Detecting broiler chickens on litterﬂoor with the YOLOv5-CBAM deeplearning model
Yangyang Guoa,b,S a m u e lE .A g g r e yb, Xiao Yangb, Adelumola Oladeindec,Y o n g l i a n gQ i a od,L i l o n gC h a ib,⁎
aSchool of Internet, Anhui University, Hefei, Anhui 230039, China
bDepartment of Poultry Science, University of Georgia, Athens, GA 30602, USA
cU.S. National Poultry Research Center, USDA-ARS, Athens, GA 30605, USA
dAustralian Centre for Field Robotics (ACFR), Faculty of Engineering, The University of Sydney, NSW 2006, Australia
abstract article info
Article history:Received 5 April 2023Received in revised form 16 July 2023Accepted 8 August 2023Available online 9 August 2023For commercial broiler production, about 20,000 –30,000 birds are raised in each conﬁned house, which has caused growing public concerns on animal welfare. Cu rrently, daily evaluation of broiler wellbeing and growth is conducted manually, which is labor-intensi ve and subjectively subject to human error. Therefore, there is a need for an automatic tool to detect and analyze the behaviors of chickens and predict their wel-fare status. In this study, we developed a YOLOv5-CBA M-broiler model and tested its performance for de- tecting broilers on litterﬂoor. The proposed model consisted of two parts: (1) basic YOLOv5 model forbird or broiler feature extraction and object detectio n; and (2) the convolutional block attention module (CBAM) to improve the feature extraction capabilit y of the network and the problem of missed detection of occluded targets and small targets. A complex dataset o f broiler chicken images at different ages, multiple pens and scenes (fresh litter versus reused litter) was constructed to evaluate the effectiveness of the newmodel. In addition, the model was compared to the Faster R-CNN, SSD, YOLOv3, Ef ﬁcientDet and YOLOv5 models. The results demonstrate that the precisi on, recall, F1 score and an mAP@0.5 of the proposed method were 97.3%, 92.3%, 94.7%, and 96.5%, which wer e superior to the comparison models. In addition, comparing the detection effects in different scenes, the YOLOv5-CBAM model was still better than the com-parison method. Overall, the proposed YOLOv5-CBA M-broiler model can achieve real-time accurate and fast target detection and provide technical support f or the management and monitoring of birds in commer- cial broiler houses.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Poultry productionDeep learningYOLOv5Attention mechanism
1. IntroductionPoultry provides a valuable source of proteins but face a numberof challenges worldwide. Among the many challenges is the welfareconcerns of the birds under intensive management systems ( Chai et al., 2018, 2019). Due to the large number of chickens reared atany given time in a house, accurate and ef ﬁcient monitoring of birds can improve their health and welfare status ( Li et al., 2021; Okinda et al., 2020). Currently, most broiler houses are monitoredm a n u a l l y ,h o w e v e r ,t h i sa p p r o ach could be both laborious anderroneous. Automatic broiler monitoring system could collectindividual bird data within aﬂock and provide critical informationto aid digital management (Subedi et al., 2023a, 2023b;Yang et al., 2022).Computer vision technology (CVT) is widely used to monitorfarm animals because it is non-invasive ( Qin et al., 2021;Wang et al., 2022;He et al., 2016). The CVT together with machine visionmethods have achieved target detection based on the features oft h et a r g e ta r e a( e . g . ,c o l o r ,s h a p e ,t e x t u r e ) .T h ea b i l i t yt oe f f e c t i v e l yacquire these visual features will affect the accuracy of target detec-tion (T h a r w a te ta l . ,2 0 1 4;Awad et al., 2013;Andrew et al., 2017). The feature acquisition method, external environment (e.g., lightintensity and occlusion), shooting angle and image quality chosenare crucial parameters that will affect target detection. Therefore,it is important to innovate animal target detection algorithms thatare less affected by the natural environment.Deep learning technology has powerful feature representation capa-bilities, fast processing speed, and can resolve problems associated withexternal interferences. Thus, deep learning algorithms are appropriatemodels for developing an automatic, efﬁcient and intelligent tool for an- imal farming (Qiao et al., 2021). Deep learning technologies have beenapplied to the study of large animals (pigs, sheep, cattle), such as objectArtiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
⁎Corresponding author.E-mail address:lchai@uga.edu(L. Chai).
https://doi.org/10.1016/j.aiia.2023.08.0022589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/detection, individual tracking, behavior recognition and body conditionevaluation (Chen et al., 2021;Xue et al., 2021;Tian et al., 2019;Qiao et al., 2019;Shen et al., 2020;Alvarez et al., 2019;Guo et al., 2021a, 2022). However, the size of the chicken and the sheer numbers thatare raised in a single house (e.g., 20,000 –30,000 chickens on litter ﬂoor of 2000–2500 m
2)p o s ec h a l l e n g e si na p p l y i n gd e e pl e a r n i n gt e c h -niques in monitoring individual chickens ( Guo et al., 2020).Yang et al. (2022)built a YOLOv5x-hens model, which detection is highly ef ﬁcient and over 95% accurate.Fang et al. (2020)proposed poultry tracking al- gorithm TBroiler tracker which has good performance in overlap rate,pixel error and failure rate, and its hybrid tracking performance evalua-tion (MTPE) is 0.730.Fang et al. (2021)analyzed the behavior of broiler using DNNs.The tests showed that the accuracy of standing, walking, running, eat-ing, resting and tidying behavior recognition was 0.7511, 0.5135, 0.6270,0.9361, 0.9623 and 0.9258, respectively. Although the above research hasmade some progresses, however, there is a lack of poultry research at dif-ferent ages, feeding environments, and densities. And object detection isthe premise of behavior recognition and target tracking, and it is also thedata basis for providing target area information. Therefore, target detec-tion of broiler groups in multiple scenarios is of great signi ﬁcance. Among the deep learning algorithms, YOLO series is one of the fast andhigh precision algorithms for multi-target detection at present ( Subedi et al., 2023a, 2023b;Ge et al., 2021;Bochkovskiy et al., 2020). When tar- gets are small or occluded YOLO can res ult in missed or false detections. There are attention mechanisms in deep learning that can reduce infor-mation loss and improve the detection performance of occlusion andsmall object.(Yang et al., 2023;Li et al., 2020;Fukui et al., 2019). In the current study, we incorporated a convolutional block atten-tion module (CBAM) into YOLOv5 to enhance the algorithm's abilityto extract image features. To do this, we used a complex dataset ofbroiler images (e.g., birds at different ages, fresh and reused litter andmultiple pens) to train and test the model. The proposed YOLOv5-CBAM improved the acquisition ability of small object features and theaccuracy of small target detection.2. Material and methods2.1. Data acquisitionThe data for this study came from an experimental broiler houseat the Poultry Research Center of the University of Georgia, USA(Guo et al., 2020;Guo et al., 2021b). Two different litter types (fresh pine shavings and reused litter previously used to raise threeﬂocks of broilers) were selected as application scenes for broiler de-tection. For the two litter scenes, 70 images were selected from d2,d9, d16, and d23, respectively, for a total of 560 images. In addition,to evaluate the detection performance of the model under multiplepens scenes, the image samples shown in Fig. 1c were constructed, in which 70 images were selected for d16 and d23. Finally, 700 im-ages were obtained and randomly assigned at a ratio of 5:2 intotraining and testing set, respectively. Fig. 1are examples of broiler images from different scenes.2.2. YOLOv5-CBAM model for broiler detectionIn the current study, we propose a YOLOv5-CBAM-broiler model(Fig. 2). This method added CBAM attention modules to the backboneand neck layers of YOLOv5 to improve the feature representationcapability.2.2.1. The YOLOv5 networkJocher et al. (2020)developed the YOLOv5 algorithm and demon-strated that it was more accurate and faster compared to the previousYOLO model. The YOLOv5s network consists of three parts: backbone,neck, and prediction. Broiler chicken images were used as input forthe backbone to obtain image features, the neck part was used to inte-grate the extracted feature information and generate feature maps,and the prediction part was used to generate bounding boxes and pre-dict categories for the generated feature maps. The detailed process isprovided as a supplementary material.
Fig. 1.Examples of broiler images from different scenes.Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
37The Backbone part of the model is used to extracted different ﬁne- grained features from images, as shown in Fig. 2, the original backbone network is composed of Focus, Conv, C3 and Spatial Pyramid Pooling(SPP). The bird image size in the YOLOv5s model was 416 × 416 × 3,which became 208 × 208 × 12 through the focus slicing operation,and theﬁnal feature map size became 208 × 208 × 32 after convolutionwith 32 convolution operation kernels. C3 is used to extract broilerimage features. In the Backbone part, the C3 module contains detailedlocation information, but less semantics. The SPP is used to concatenatefeature maps of different sizes together as an output ( He et al., 2015). In the Neck part, the C3 module extracts features, which contains less loca-tion information, but more semantics. After the feature information ofoccluded or small targets are processed by C3 modules, the target posi-tion information is rough, and the feature information can be easily lost.The Head part predicts the processed broiler image features in three dif-ferent scales, generates bounding boxes and predicts the class of objects.The Head part in YOLOv3 was used as YOLOv5 Head.To improve the detection accuracy of the original model for broilertargets at different growth stages and feeding scenes, we herein proposean improved YOLOv5 network model, as shown in Fig. 2. The CBAM module was added to Backbone and Neck and placed after the C3 mod-ule. The CBAM module can strengthen the learning of occlusion or smalltarget feature information during the network training process throughthe channel and spatial attention modules.2.2.2. The CBAM attention moduleIn the target detection task of broiler chickens at different growthstages and in different scenes, occlusion or small targets occupy fewerpixels, and their feature information is easily lost in the deep network,which leads to missed and false detection of targets. The CBAM modulecan effectively increase the weight of the occlusion or small targets inthe entire feature map through channel and spatial attention modules,making the information easier to be learned by the network ( Woo et al., 2018). The broiler image features extracted from the C3 modulewere denoted as F, and the channel attention map was generated byusing the channel relationship between features, which was multipliedby F to form a new feature F ′to enhance the features related to the tar-get area of bird. Then, the spatial attention feature map was generatedby using the internal spatial relationship between features, which mul-tiplied with F′to obtain F″, which strengthened the weight of broiler tar-get area features from the channel and spatial relationship betweenfeatures. As shown inFig. 3.2.2.2.1. Channel attention module.The channel attention module infor-mation is extracted using max pooling and average pooling, respec-tively, and thenﬁltered, activated and normalized to improve theability to extract channel information.As shown inFig. 3a. First, the feature mapF∈R
(C×H×W)is inputted, and the feature map of sizeC×H×Wis transformed intoC×1×1 using maximum pooling and average pooling. Then the feature map isentered into the neural network MLP, the number of neurons in theﬁrst layer isC/r,ris the decline rate, and the activation function isRelu. The number of neurons in the second layer is C, and then the re- sults are combined through the addition operation. The weight coef ﬁ- cientM
c∈R(C×1×1)is obtained through thesigmoidfunction, as shown in Eq.(1).M
cFðÞ ¼σW 1W2Fcavg/C18/C19/C18/C19þW
1W2Fcmax/C18/C19/C18/C19/C18/C19 ð1Þwhere,σis the sigmoid function;F
cavgandFcmaxrepresent the feature maps after average and maximize pooling; W
1andW 2represent the weights of two layers of a multilayer perception. Then, the channelattention feature mapF′is obtained by multiplyingM
cwith the original feature mapF.2.2.2.2. Spatial attention module.The spatial attention mechanism fo-cuses on local information. The information is ﬁltered by pooling, and then the important information is extracted by convolution from the ﬁl- tered information. As shown inFig. 3b. UsingF′as input into the spatial attention module, it is also pooled with maximum and average, stackedby theConcatoperation, and then the weight coefﬁcientM
s∈R(1×H×W)is obtained by convolution operation and sigmoid, as shown in eq.(2).M
sF′ðÞ ¼σf7/C27F′Savg;F′Smaxhi/C16/C17/C16/C17 ð2Þ
Fig. 2.The overall structure of YOLOv5-CBAM network.Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
38where,σis the sigmoid function;F′savgandF′smaxrepresent the feature maps of size 1 × H × W after average and maximize pooling; f
7/C27repre- sents 7 × 7 convolution. Lastly, theM
sand F′are multiplied to obtain the ﬁnal attention feature mapF″.2.3. Performance evaluationIn the current study, precision, recall, F1 score, mean average preci-sion (mAP) and Frames Per Second (FPS) were adopted as the metrics ofthe detection accuracy, as shown in the following equations:Precision¼
TPTPþFP/C2100% ð3ÞRecall¼
TPTPþFN/C2100% ð4ÞF1¼2/C2
Precision/C2RecallPrecisionþRecall/C2100%ð5ÞAP¼Z10PrðÞdr ð6ÞmAP¼
1n∑ni¼1AP iðÞ ð 7Þwhere, TP, FP and FN are the numbers of true positive samples, false pos-itive samples and false negative samples, respectively. The mAP is themean of all classiﬁed AP (Average Precision).nrepresents the number of object categories (n= 1). FPS refers to the number of images identi-ﬁed within 1 s.2.4. Network training parametersIn this study, all model tests were performed on a computerequipped with a GeForce GTX 1080 Ti GPU, I9-7920× CPU@2.9 GHz.Parameter settings: 416 × 416 × 3 input size, 1000 training period, 16batch size, 0.0013 learning rate. Other parameters are their defaultsettings.IF a s t e rR - C N N(Ren et al., 2015), SSD (Liu et al., 2016), YOLOv3 (Redmon and Farhadi, 2018), EfﬁcientDet (Tan et al., 2020)a n d YOLOv5s (Liu et al., 2021) serve as comparison models.3. Results3.1. Performance of new detection modelWe used datasets consisting of broiler images at different ages,raised on two types of litter and multiple pens to test the performanceof YOLOv5-CBAM. The detection results of broiler with different models
Fig. 3.The structure of the CBAM module.
Table 1Performance comparison of different algorithms (%).Method Precision Recall F1 mAP@0.5 FPS (Frame/s)Faster-rcnn 79.7 95.4 86.8 90.6 2.6SSD 60.8 94.0 73.8 88.5 3.1YOLOv3 83.7 83.0 83.3 70.6 18.9EfﬁcientDet 97.0 47.0 64.0 59.6 36YOLOv5 96.6 92.1 94.3 96.3 62YOLOv5-CBAM 97.3 92.3 94.7 96.5 55
Table 2Comparison of detection accuracy in different scenes.Method Precision Fresh pine shavings Reused litter Multiple pensd2 d9 d16 d23 d2 d9 d16 d23 d16 d23Precision of YOLOv5 95.1 99 98.6 92.8 94.3 99 99.1 98.7 92.2 98.6Precision of YOLOv5-CBAM 96.1 99.3 99.3 94 95.2 98.9 99.3 99.3 92.5 98.8Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
39are shown inTable 1.F r o mTable 1, the precision, recall, F1 and mAP@0.5 of YOLOv5-CBAM were 97.3%, 92.3%, 94.7% and 96.5%, which washigher than that of YOLOv5 (96.6%, 92.1%, 94.3% and 96.3%), Faster R-CNN (79.7%, 95.4%, 86.8% and 90.6%), SSD (60.8%, 94.0%, 73.8% and88.5%), YOLOv3 (83.7%, 83.0%, 83.3% and 70.6%) and Ef ﬁcientDet (97.0%, 47.0%, 64.0% and 59.6%). Adding the CBAM module to YOLOv5network improved the performance of the broiler detection model. Italso showed that the model YOLOv5-CBAM was suitable for the detec-tion of broilers at different growth stages, in different litters type andmultiple pens. IThe FPS of YOLOv5-CBAM was 55 Frame/s, which waslower than YOLOv5 (62 Frame/s), but higher than Faster R-CNN (2.6Frame/s),SSD (3.1 Frame/s), YOLOv3 (18.9 Frame/s) and Ef ﬁcientDet (36 EfﬁcientDet). It can be seen that the accuracy of YOLOV5-CBAMhas also been improved while maintaining a high processing speed,and it can be applied to target detection or small target detection ofbirds at different feeding densities.3.2. Detection results in different scenesTable 2lists the detection precision of YOLOv5 and YOLOv5-CBAM atdifferent growth stages, on different litter types and in multiple pens. Inthis sample dataset, the precision of YOLOv5-CBAM in each scene wasslightly higher than that of YOLOv5 (Table 2). The precision of detection at d2 in fresh and reused litter was lower than d9 and d16. This is be-cause broilers on d2 were small, and the feature extraction was not suf-ﬁcient for crowded and occluded targets. This may have resulted inmissed detections or false positive and negative detections. Neverthe-less, the precision of YOLOv5-CBAM (96.1%, 95.2%) at d2 was slightlyhigher than YOLOv5 (95.1%, 94.3%). The detection precision of hens ond23 was lower in the scene of fresh pine shavings than the reused litterscene (as shown inFig. 1), which could be caused by changes inchickens' crowding or pilling behaviors on different litter ﬂoors. When overcrowded, the target information of broilers could be lost, and thus
Fig. 4.Detection results using YOLOv5 and YOLOv5-CBAM in fresh pine shavings.Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
40lead to missed or false detection. In addition, the selection of sampleswill also affect the performance of the test, and the accuracy of themodel detection will be reduced when the birds in the sample are toocrowded or heavily occluded. This is also the reason for the reduced ac-curacy of YOLOv5-CBAM in d23. Although the precision of YOLOv5-CBAM in each scene was slightly higher than YOLOv5 ( Table 2), the overall performance was better than YOLOv5 ( Table 1). YOLOv5- CBAM has been improved at different ages, different litter and differentpopulation densities, and the generalization performance of the modelhas also been improved, which can be applied to object detection in dif-ferent feeding environments. It also provides technical support for theaccurate detection of commercial broiler breeding.Fig. 4,Fig. 5,a n dFig. 6shows detection results of YOLOv5 andYOLOv5-CBAM in different scenes (ﬂoor types). InFigs. 4 to 6,t h e ﬁrst column is the detection results of YOLOv5, the second columnis the original images, and the third column is the detection resultsof YOLOv5-CBAM. i→ji nt h eFigs. 4 to 6, i is the actual number of broilers, and j is the number of broilers detected broilers. It can be ob-served fromFigs. 4 to 6that in different scenes, YOLOv5-CBAM candetect broilers better than YOLOv5, and in the case of crowded orsmall targets, it can still provide better detection results. For example,inFig. 4, YOLOv5 will falsely detect broilers under crowded condi-tions, while YOLOv5-CBAM performed better under crowded condi-tion. However, when the broilers were overcrowded, that is, thebroilers overlap and block each other signi ﬁcantly, YOLOv5-CBAM also has false detection (d23 inFig. 5), but it was lower than YOLOv5. In the case of multiple pens, the edge of the sample imageis distorted, and the broiler appears smaller in the ﬁeld of view, and the occlusion is more substantial and resulted in false detections bythe model (Fig. 6).
Fig. 4(continued).Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
414. DiscussionThe YOLOv5-CBAM-broiler modle in the current study has a preci-sion of 97.3% for broiler detection on the litter ﬂoor, which is higher than that of Faster R-CNN, SSD and YOLOv5 models. The introductionof CBAM attention mechanism can suppress the general features andenhance the important features, thus effectively reducing the missedor false detections. FromTable 2andFigs. 4 to 6, it can be found that adding the CBAM module to YOLOv5 network can improve thedetection performance of the model against small or blocked targets.However, when the broilers are signiﬁcantly blocked or there is a sub- stantial distortion in the image, YOLOv5-CBAM has the phenomenonof false detection or missing detection.The datasets samples used for model development consisted of dif-ferent image scenes of broilers at different ages, raised on litter typesand multiple pens. Therefore, the overall sample contains broilers of dif-ferent sizes, crowding, occlusion, equipment interference, etc., whichwill affect the detection performance. In addition, broilers have multipleangles and poses in the scene, which will also affect the detection accu-racy, as shown inFigs. 4 to 6. To sum up, it can be concluded that the se-lection and number of samples will also affect the results.YOLOv5-CBAM has a small model, high detection accuracy, and FPSof 55 frames/s. It has good real-time performance and can be installedon portable embedded platforms to develop mobile object detectionequipment, such as mobile robots. In addition, this method has achievedgood detection results in different scenarios, and the samples of differ-ent varieties can be further enriched to further train the model in thelater stage, which is expected to achieve multi-target detection underdifferent varieties.5. ConclusionsTo detect broilers in different scenes (e.g., different ages, raised ondifferent litter types and multiple pens), we proposed using the
Fig. 5.Detection results using YOLOv5 and YOLOv5-CBAM in reused litter.Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
42YOLOv5-CBAM-broiler model. The proposed approach integrates CBAMinto YOLOv5 and improved the overall detection performance, espe-cially in the case of small targets or occlusions. In addition, the resultsshow that YOLOv5-CBAM could detect broilers of different ages effec-tively and provides the basis for real-time target detection for intelligentpoultry management.CRediT authorship contribution statementYangyang Guo:Data curation, Investigation, Writing - original draft.Samuel E. Aggrey:Resources, Supervision.Xiao Yang:Investigation. Adelumola Oladeinde:Resources, Supervision.Yongliang Qiao:Data curation.Lilong Chai:Conceptualization, Resources, Supervision.
Fig. 5(continued).Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
43Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgementsThis study was supported by a cooperative grant 58-6040-6-030(Lilong Chai) and 58-6040-8-034 (S. E. Aggrey) from the United StateDepartment of Agriculture-Agriculture Research Service; USDA-NIFAHatch Project (GEO00895): Future Challenges in Animal ProductionSystems-Seeking Solutions through Focused Facilitation; UGA CAESDean's Ofﬁce Research Fund; and Georgia Research Alliance - VentureFund.References
Alvarez, J.R., Arroqui, M., Mangudo, P., Toloza, J., Jatip, D., Rodriguez, J.M., Teyseyre, A.,Sanz, C., Zunino, A., Machado, C., Mateos, C., 2019. Estimating body condition scorein dairy cows from depth images using convolutional neural networks, transfer learn-ing and model ensembling techniques. Agronomy 9 (2), 90. https://doi.org/10.3390/ agronomy9020090.Andrew, W., Greatwood, C., Burghardt, T., 2017. Visual localisation and individual identi- ﬁcation of Holstein friesian cattle via deep learning. Proceedings of the IEEE Interna-tional Conference on Computer Vision Workshops, pp. 2850 –2859. Awad, A.I., Zawbaa, H.M., Mahmoud, H.A., Nabi, E.H.H.A., Fayed, R.H., Hassanien, A.E.,2013.A robust cattle identiﬁcation scheme using muzzle print images. In 2013 Fed-erated Conference on Computer Science and Information Systems. IEEE, pp. 529 –534. Bochkovskiy, A., Wang, C. Y., & Liao, H. Y. M. (2020). Yolov4: optimal speed and accuracyof object detection. arXiv preprint arXiv:2004.10934. Doi:https://doi.org/10.48550/ arXiv.2004.10934.Chai, L., Zhao, Y., Xin, H., Wang, T., Soupir, M.L., 2018. Mitigating airborne bacteria gener- ations from cage-free layer litter by spraying acidic electrolysed water. Biosyst. Eng.170, 61–71.Chai, L., Xin, H., Wang, Y., Oliveira, J., Wang, K., Zhao, Y., 2019. Mitigating particulate mat- ter generation in a commercial cage-free hen house. Trans. ASABE 62 (4), 877 –886. Chen, C., Zhu, W., Norton, T., 2021. Behaviour recognition of pigs and cattle: journey fromcomputer vision to deep learning. Comput. Electron. Agric. 187, 106255. https://doi. org/10.1016/j.compag.2021.106255 . Fang, C., Huang, J., Cuan, K., Zhuang, X., Zhang, T., 2020. Comparative study on poultry tar- get tracking algorithms based on a deep regression network. Biosyst. Eng. 190,176–183.Fang, C., Zhang, T., Zheng, H., Huang, J., Cuan, K., 2021. Pose estimation and behavior clas- siﬁcation of broiler chickens based on deep neural networks. Comput. Electron. Agric.180, 105863.Fukui, H., Hirakawa, T., Yamashita, T., Fujiyoshi, H., 2019. Attention branch network: learning of attention mechanism for visual explanation. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 10705 –10714. Ge, Z., Liu, S., Wang, F., Li, Z., Sun, J., 2021. Yolox: exceeding yolo series in 2021. arXiv pre-print.https://doi.org/10.48550/arXiv.2107.08430 arXiv:2107.08430. Guo, Y., Chai, L., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., 2020. A machine vision-based method for monitoring broiler chicken ﬂoor distribution. Sensors 20 (11), 3179.https://doi.org/10.3390/s20113179 . Guo, Y.Y., Qiao, Y.L., Sukkarieh, S., Chai, L.L., He, D.J., 2021a. Bigru-attention based cow be- havior classiﬁcation using video data for precision livestock farming. Trans. ASABE 64(6), 1823–1833.Guo, Y., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., Chai, L., 2021b. A machine vision-based method optimized for restoring broiler chicken images occluded by feedingand drinking equipment. Animals 11 (1), 123. https://doi.org/10.3390/ani11010123 . Guo, Y., Aggrey, P., Wang, S.E., Chai, L., 2022. Monitoring behaviors of broiler chickens atdifferent ages with deep learning. Animals 12 (23), 3390. https://doi.org/10.3390/ ani12233390.He, K., Zhang, X., Ren, S., Sun, J., 2015. Spatial pyramid pooling in deep convolutional net-works for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37 (9),1904–1916.https://doi.org/10.1109/TPAMI.2015.2389824 . He, D.J., Liu, D., Zhao, K.X., 2016. Review of perceiving animal information and be-havior in precision livestock farming. Transactions of the Chinese Society forAgricultural Machinery 47 (5), 231 –244.
https://doi.org/10.6041/j.issn.1000- 1298.2016.05.032.Jocher, G., Nishimura, K., Mineeva, T., Vilariño, R., 2020. yolov5. Code repository. Li, X., Jia, X., Wang, Y., Yang, S., Zhao, H., Lee, J., 2020. Industrial remaining useful life pre-diction by partial observation using deep learning with supervised attention. IEEE/ASME Transactions on Mechatronics 25 (5), 2241 –2251.https://doi.org/10.1109/ TMECH.2020.2992331.
Fig. 6.Detection results using YOLOv5 and YOLOv5-CBAM in multiple pens.Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
44Li, G., Huang, Y., Chen, Z., Chesser Jr., G.D., Purswell, J.L., Linhoss, J., Zhao, Y., 2021. Practicesand applications of convolutional neural network-based computer vision systems inanimal farming: a review. Sensors 21 (4), 1492. https://doi.org/10.3390/s21041492 . Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., 2016. Ssd: Singleshot multibox detector. European Conference on Computer Vision. Springer, Cham,pp. 21–37.https://doi.org/10.1007/978-3-319-46448-0-2 . Liu, K., Tang, H., He, S., Yu, Q., Xiong, Y., Wang, N., 2021. Performance validation of YOLOvariants for object detection. Proceedings of the 2021 international conference onbioinformatics and intelligent computing, pp. 239 –243.https://doi.org/10.1145/ 3448748.3448786.Okinda, C., Nyalala, I., Korohou, T., Okinda, C., Wang, J., Achieng, T., Wamalwa, T., Manga,T., Shen, M., 2020. A review on computer vision systems in monitoring of poultry: awelfare perspective. Artiﬁcial Intelligence in Agriculture 4, 184 –208.https://doi.org/ 10.1016/j.aiia.2020.09.002. Qiao, Y., Truman, M., Sukkarieh, S., 2019. Cattle segmentation and contour extractionbased on mask R-CNN for precision livestock farming. Comput. Electron. Agric. 165,104958.https://doi.org/10.1016/j.compag.2019.104958 . Qiao, Y., Kong, H., Clark, C., Lomax, S., Su, D., Eiffert, S., Sukkarieh, S., 2021. Intelligent per-ception for cattle monitoring: a review for cattle identi ﬁcation, body condition score evaluation, and weight estimation. Comput. Electron. Agric. 185, 106143. https://doi. org/10.1016/j.compag.2021.106143 . Qin, Q., Liu, Z., Zhao, C., Zhang, C., Dai, D., Sun, J., Wang, Z., Li, J., 2021. Application of ma- chine vision Technology in Livestock and Poultry. Agric. Eng. 11 (07), 27 –33. Redmon, J., Farhadi, A., 2018. Yolov3: An incremental improvement. arXiv preprint.https://doi.org/10.48550/arXiv.1804.02767 . Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: towards real-time object detection with region proposal networks. Adv. Neural Inf. Proces. Syst. 28.Shen, W., Hu, H., Dai, B., Wei, X., Sun, J., Jiang, L., Sun, Y., 2020. Individual identiﬁcation of dairy cows based on convolutional neural networks. Multimed. Tools Appl. 79 (21),14711–14724.Subedi, S., Bist, R.B., Yang, X., Chai, L., 2023a. Tracking pecking behaviors and damages of cage-free laying hens with machine vision technologies. Comput. Electron. Agric. 204(1), 107545.Subedi, S., Bist, R.B., Yang, X., Chai, L., 2023b. Trackingﬂoor eggs with machine vision in cage-free hen houses. Poult. Sci. 102637.Tan, M., Pang, R., Le, Q.V., 2020. Efﬁcientdet: scalable and efﬁcient object detection. Pro- ceedings of the IEEE/CVF conference on computer vision and pattern recognition,pp. 10781–10790.Tharwat, A., Gaber, T., Hassanien, A.E., 2014. Cattle identiﬁcation based on muzzle images using gabor features and SVM classi ﬁer. International Conference on Advanced Ma- chine Learning Technologies and Applications. Springer, Cham, pp. 236 –247. Tian, M., Guo, H., Chen, H., Wang, Q., Long, C., Ma, Y., 2019. Automated pig counting usingdeep learning. Comput. Electron. Agric. 163, 104840. https://doi.org/10.1016/j. compag.2019.05.049.Wang, Z., Song, H., Wang, Y., Hua, Z., Li, R., Xu, X., 2022. Research progress on intelligent morning methods of dairy Cow ’s motion behavior. Smart Agriculture 1 –
18 (2022-06- 12).Woo, S., Park, J., Lee, J.Y., Kweon, I.S., 2018. Cbam: convolutional block attention module. Proceedings of the European Conference on Computer Vision (ECCV), pp. 3 –19. Xue, H., Qin, J., Quan, C., Ren, W., Gao, T., Zhao, J., 2021. Open set sheep face recognitionb a s e do ne u c l i d e a ns p a c em e t r i c .M a t h .P r o b l .E n g .1 - 5 . https://doi.org/10.1155/ 2021/3375394.Yang, X., Chai, L., Bist, R.B., Subedi, S., Wu, Z., 2022. A deep learning model for detectingcage-free hens on the litterﬂoor. Animals 12 (15), 1983.https://doi.org/10.3390/ ani12151983.Yang, X., Bist, R., Subedi, S., Chai, L., 2023. A deep learning method for monitoring spatialdistribution of cage-free hens. Arti ﬁcial Intelligence in Agriculture 8, 20 –29.https:// doi.org/10.1016/j.aiia.2023.03.003 .Y. Guo, S.E. Aggrey, X. Yang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 36 –45
45