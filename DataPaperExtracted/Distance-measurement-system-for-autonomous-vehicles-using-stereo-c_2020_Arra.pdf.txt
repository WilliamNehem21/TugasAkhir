Distance measurement system for autonomous vehicles using stereo camera
Abdelmoghit Zaarane*, Ibtissam Slimani , Wahban Al Okaishi , Issam Atouf , Abdellatif Hamdoun
LTI Lab, Department of Physics, Faculty of Sciences Ben M ’Sik, University Hassan II Of Casablanca, Morocco
ARTICLE INFO
Keywords:Distance measurementVehicle detectionStereo visionImage processingStereo cameraABSTRACT
The focus of this paper is inter-vehicles distance measurement which is a very important and challenging task inimage processing domain. Where it is used in several systems such as Driving Safety Support Systems (DSSS),autonomous driving and trafﬁc mobility. In the current paper, we propose an inter-vehicle distance measurementsystem for self-driving based on image processing. The proposed system uses two cameras mounted as one stereocamera, in the hosting vehicle behind the rear-view mirror. The detection of vehicles is performed ﬁrst in a single camera using a recent powerful work from the literature. Then, the same vehicle is detected in the image capturedby the second camera using template matching technique. Thus, the inter-vehicle distance is calculated using asimple method based on the position of the vehicle in both cameras, geometric derivations and additionaltechnical data such as distance between the cameras and some other speci ﬁc angles (e.g. the cameras viewﬁeld angle). The results of the extensive experiments showed the high accuracy of the proposed method compared tothe previous works from literature and it allows to measure ef ﬁciently the distances between the vehicles and the hosting vehicle. In addition, this method could be used in several systems of various domains in real timeregardless of the object types. The experiments results were done on a Hardware Processor System (HPS) locatedin a VEEK-MT2S provided by TERASIC.
1. IntroductionIn the last twenty years, the self-driving cars have obtained a hugeimportance in the research domain, they are expected to take places ofhumans in differentﬁelds by performing several missions. The autono-mous vehicles development has been one of the most important subjectsin theﬁeld of automotive research due to the growth of the traf ﬁc problems in most of the world. Therefore, the expectance for increasingsafety of the road and comfort of driving are high by relieving the driversof driving tasks in partial or complete way. Because the automation of thedriver responsibilities may signiﬁcantly reduce the collisions and in-crease the road safety.The researchers face a lot of difﬁculties in self-drivingﬁeld due to the dynamic and complex environment and the complex movement in fastway. The automated vehicles need to detect the other vehicles whatevertheir shape and type[1–3,16]. Thus, several algorithms should be per-formed such as vehicle detection, license plate detection [15]and speed and distance estimation. The extracted information using these algo-rithms are used by the automated vehicles for making some decisions forexample bypassing other vehicles or changing their path or their speed.The distance measurement between vehicles is very important subjectin autonomous vehicles. Therefore, detecting surroundings vehicles in-formation accurately (e.g. distance between vehicles) in real time is veryimportant and challenging task. In the literature, two main methods existfor the distance measurement, active methods and passive methods.The active methods measure the distance by sending signals to thetarget. These systems are based generally on computing the time of ﬂight of laser beams, ultrasound, or radio signals, to measure and search for theobjects. The time ofﬂight systems are used to estimate the object distanceusing speciﬁc sensors by measuring the needed time of a signal pulse totransmit to the object and reﬂect by it. Their main inconvenient are thepotential confusion of echoes from previous or subsequent pulses andalso the accuracy range of distance for these systems is usually boundedbetween one to 4 m. Carullo and Parvis [ 4], presented an ultrasonic system that can measure the distance of selected points, where the ul-trasonic sensor is based on measuring the time of ﬂight of an ultrasonic pulse, which is reﬂected by the object. Nakahira et al. [5], presented an ultrasonic system using a pulse time-of-ﬂight estimation, by combining frequency-modulated emissions and correlation detection for time-ofﬂight estimation in real-time from the noisy echoes. Their purpose is totackle with confusion of echoes from previous or subsequent pulses,those of other systems, or from other objects.
* Corresponding author.E-mail addresses:z.abdelmoghit@gmail.com(A. Zaarane),ibtissamslimani7@gmail.com(I. Slimani).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100016Received 30 May 2019; Received in revised form 26 December 2019; Accepted 7 January 2020Available online 8 January 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 5 (2020) 100016However, the passive methods, measure the distance by receivinginformation about the position of the object. These systems are basedgenerally on cameras and computer vision techniques. In principal, twotype of systems exist for the passive method, Mono Vision systems andStereo Vision systems.The mono vision systems use one camera to estimate the distancebased on reference points in the camera view ﬁeld and there are usually used for visual servoing purposes. Zhang et al. [ 6], presented an absolute localization of an object in the camera coordinate frame using the dis-tance estimation between principal point and feature point based on thecalculated area in the frame. Their process follows three parts. The ﬁrst part is about the calibration of the camera, in other words the intrinsicparameters calibration. The second part is to constitute a model for dis-tance measurement over the optical axis direction according to themapping relationship between the objects in the camera coordinatesframe and their projection in the pixel coordinate frame and the ﬁnal part is about the absolute distance estimation. Aswini et al. [ 7], proposed an obstacle avoidance and distance measurement system using mono visionmethod. They measure the distance between the vehicle and the obstaclebased on camera calibration techniques and the pixel variation inconsecutive video frames using the key points extracted by SIFT andSURF algorithms. Huang et al. [8], proposed a mono vision system withinstance segmentation and camera focal length to detect the cars distancein front of the current car. Their proposed system composed of threestages. In theﬁrst stage, the locations of the cars are extracted. In thesecond stage, the located cars are classiﬁed to get their types and their mask values using a model that is trained by the CompCars dataset toclassify car types. Then, a new instance segmentation model by theCityscapes dataset is used to get the car mask. In the third stage, the carsdistances are calculated based on the relationship between the size in-formation of the different car types and their mask values. The incon-venient of the mono vision methods in estimating distances is that weshouldﬁrst not only detect the objects but also extract their types.Therefore, to recognize the detected objects types we need a huge datasetcontains all types of objects (models of brands) and their dimensions, andeven using a huge dataset it will be always an issue in estimating dis-tances of unknown objects. another inconvenient reside in the highcomplexity of the used algorithm to classify the objects types and also tomatch the real dimensions of the objects and the dimensions of the sameobjects in the images in different positions, especially when there isoverlapping.The stereo vision system is a computer vision system that is based onstereoscopic ranging techniques to calculate the distance. This system usetwo cameras as one camera, trying to give the impression of depth anduse the disparity of the objects between the cameras to compute thedistance with high accuracy. Salman et al. [ 9], presented a distance estimation method based on stereoscopic measurement using trigono-metric equations, their method is divided into three parts. The ﬁrst part is about applying some image processing methods to improve the compu-tational speed such as reducing the input image resolution and con-verting the input image from RGB domain to grayscale domain. Thesecond part is about extracting the object position from the two cameras.The third part is aboutﬁnding out the state where the object is,depending on the object position. Then estimating the distance usingstate equation based on the trigonometric method. Hsu and Wang [ 10], proposed a stereo vision system for estimating an object distance basedon the cameras focal length and the disparity between the images. Theirproposed method is composed of four stages, the ﬁrst stage is about applying some pre-processing methods on the images to reduce thecomputational speed like down scaling the images size to certain level.The second stage is the region segmentation where they divided theimages into small blocks and they applied the local threshold selectionalgorithm to isolate the objects from the background. The third stage isabout looking for the disparity information of each object between thetwo images by extracting their features then matching them, using spe-ciﬁc descriptors. Theﬁnal stage is about computing the object distancebased on the object’s disparity values, the cameras focal length and othertechnical parameters. Nurnajmin et al. [
11], are based on stereo vision method to estimate the distance and use a novel image templatematching approach to increase the accuracy of the system. They used theSimulated Kalman Filter (SKF) algorithm for template matching whichshows more efﬁcient to solve the distance measurement problem.Mrovlje et al. [12], estimate the distance from the differences betweenthe pictures taken by the two cameras and additional technical data likefocal length and distance between the cameras.Even if there are many existing works related to the object distancemeasurement, their distance measurement methods have used profes-sional cameras and their calculation formulas have contained complexcomputational terms that make the process time consuming. However, inthis paper, we proposed an algorithm based on stereo vision for distancemeasurement using only web cameras, and the calculation formulacontains simple terms based on web cameras criteria obtained by manualmeasurement (or noted in cameras package). With the proposed algo-rithm, the distance measurement accuracy is better than those of previ-ous works. The results of the experiment showed that by using theproposed method, we can accurately obtain inter-vehicles distances. Ourmethod starts with capturing the images from the scene using bothcameras. Then, a vehicle detection algorithm is applied to only oneimage. Next, a stereo matching algorithm is applied to detect and matchthe same vehicles in the other image. Finally, the horizontal centroids ofvehicles in both images are used to measure the inter-vehicles distances.Fig. 1shows the overallﬂow diagram of the method.2. Stereo vision methodStereo vision is a well-known technique that aimed at extractingdepth information of a scene from two cameras horizontally aligned andvertically displaced from one to another, to obtain two different views ofthe same scene at the same time, similarly to our own eyes. The principalidea is to record a scene from two different viewpoints and to utilize thedisparity to indicate the position, relation and structure of objects inscene. The difference between pixel positions in two images produces theimpression of depth. The object ’s distance is measured when it is in theoverlappingﬁeld of views of the two cameras.As shown inFig. 2, the two cameras are horizontally mounted andseparated by distance A. where h is the desired distance between theobject and the cameras. To measure the distance h, we need theseparameters:B: the distance separates the object and the left camera.C: the distance separates the object and the right camera.
α;ϕ;θ:the angles of the triangle formed by the object and the twocameras, as shown inFig. 2.Depending on the trigonometric functions, we have:sinφ¼
hB (1)sinθ¼
hC (2)So: h¼B⋅sinφ¼C⋅sinθ (3) According to the law of sines, we have:
Asin
α¼BsinθSo: B¼ Asinθsin
αEq.4In the end, from Eq.3 and Eq.(4), we obtain:h¼
Asinθsinϕsin
αEq.5A. Calculation ofθ;ϕand
α:In Euclidean geometry, the sum of the angles of a triangle is invari-A. Zaarane et al. Array 5 (2020) 100016
2ably equal to the straight angle, so we have:θþ
αþϕ¼180∘Eq.6 According to Eq.(6), when we getθandϕwe can conclude
α. Based onFig. 3, we calculate the angles in question.According toFig. 3:
ω1;ω2: the view angles of the two cameras respectively.H
1,H 2: the number of horizontal pixels of the two camerasrespectively.P1, P2: the position of the object in both cameras, where P1 is thedistance in pixel between the centroid of the object and the end of theoverlap area for the camera on the left.P2 is the distance in pixel between the centroid of the object and thebeginning of the overlap area for the right camera.According toFig. 3, we have:ϕ¼01þβ1 Eq.7andθ¼0
2þβ2 Eq.8 According toFig. 4:β¼
180/C0ω
2
So,β1¼180/C0ω1
2and.β2¼180/C0ω2
2
Nowβ1andβ2are known, so we still need O 1and O 2. These two angles can be obtained by multiplying the position of the object in bothcameras (P1 and P2) by angles that correspond to each pixel in the twocameras (Ap1 and Ap2), as shown below:O
1¼P1:Ap1 Eq.9O
2¼P2:Ap2 Eq.10 Therefore, we must calculate the angles Ap1 and Ap2. We have theangle
ω1correspond to H 1pixels for theﬁrst camera and the angle ω2
correspond to H 2pixels for the second camera. So, AP1 and AP2 aredeﬁned by:Ap1¼
ω1
H1Eq.11Ap2¼
ω2
H2Eq.12So, according to Eq.(7), Eq.(8), Eq.(9), Eq.(10), Eq.(11)and Eq.
Fig. 1.The Overallﬂow diagram of the proposed method.
Fig. 2.Example of two cameras mounted as a stereo camera.
Fig. 3.Illustration of the angles used for computing the distance.
Fig. 4.The angles of the camera.A. Zaarane et al. Array 5 (2020) 100016
3(12):ϕ¼P1:
ω1
H1þβ1 Eq.13θ¼P2:
ω2
H2þβ2 Eq.14Now, we haveϕandθ. However, we still need
α. According to Eq.(6) we get:
α¼180/C0/C18ϕþθ/C19¼180/C0/C18/C18P1:ω1
H1þβ1/C19þ/C18P2:ω2
H2þβ2/C19/C19Eq.15Finally, according to Eq.(5), Eq.(13), Eq.(14)and Eq.(15)the dis- tance h is deﬁned as below:h¼
Asin/C18P2:ω2
H2þβ2/C19sin/C18P1:
ω1
H1þβ1/C19sin/C18180/C0/C18P2:
ω2
H2þβ2þP1:ω1
H1þβ1/C19/C19Eq.16The distance to the object can be calculated easily as given in Eq. (16) by considering view angles of both cameras, distance between camerasand the object positions in both cameras, which are the only terms in thedistance calculation formula (Eq.(16)) that has to be calculated while all the other terms are already known.3. Object recognition3.1. Object detectionDetecting objects is an important task in distance measurement sys-tems where the performance of vehicle detection algorithm acts in pro-portion to the distance measurement performance. Therefore, beforemeasuring the vehicle distance, an efﬁcient vehicle detection algorithm is applied [1]. This algorithm is composed of two steps: hypothesisgeneration step and hypothesis veriﬁcation step. In the hypothesis gen- eration step, potential locations of vehicles (hypotheses) are generated,this generation is based on matching vehicles templates with the imagesusing cross-correlation [13] after performing a pre-processing using edgedetection. In the hypothesis veriﬁcation, the generated hypotheses in theﬁrst step are veriﬁed by performing two operations: features extractionand classiﬁcation. The third level of two-dimensional discrete wavelettransform [14] is performed to extract features from the generated hy-potheses then use them to classify the hypotheses as vehicles ornon-vehicles using AdaBoost classiﬁer. In stereo vision system for dis-tance measurement, the object detection methods are applied to imagescaptured by both cameras, which consume time. However, in our pro-posed method, object detection method is applied only to imagescaptured by one camera then stereo matching method is performedwhich obviously reduce the treatment time. The Fig. 5shows the overall ﬂow diagram of this process.3.2. Stereo matchingThe problem we may face in such systems is how to know that theselected object in the left camera is the same one in the right camera,when there are multiple objects. Therefore, before measuring the objectsdistances, we need to deﬁne the location of the same object in the twoimages. In such systems, objects detection methods are applied to imagescaptured by both cameras. Then, they apply some stereo matching al-gorithms to match the detected objects in both cameras, which consumetime. However, the main idea in this paper is to detect vehicles byapplying the vehicle detection method [ 1] to the images captured by single camera. Then, match them with the same vehicles in the imagescaptured by the other camera. This matching is done by performing thecross-correlation technique between the detected vehicle in the imagestaken by theﬁrst camera and the same horizontal position in the imagestaken by the second camera, as shown in Fig. 6. In principle, the cross-correlation function varies betweenþ1 and/C01, where the best correlation state is identiﬁed when the cross-correlation function takesvalues close toþ1. Therefore, the best match is detected when the resultof performing the cross-correlation technique takes the maximum valuegreater than a predeﬁned threshold. However, no match is detected whenthe result takes a value less than the prede ﬁned threshold. In other words, the vehicle is detected outside of the overlapping ﬁeld of views of both cameras.4. Experiment results
4.1. Equipment setupStereoscopy is an important technique used to obtain the illusion ofdepth by using two images from two slightly offset positions (stereo-scopic images), which permit us to measure the distance between thestereo camera and the chosen object using the proposed method.The stereoscopic images may be captured using two cameras (stereocamera) mounted similarly as human eyes. The most important thing tocapture stereoscopic images is how are the two cameras mounted? Hereare the criteria should be respected while mounting the two cameras:/C15The cameras should be mounted at the same level./C15The cameras should be mounted at the same horizontal position./C15The cameras should be vertically displaced by a prede ﬁned distance. /C15The pictures should be captured from both cameras at the same time.This paper uses Cþþand OpenCV as programing language, to test theproposed method. The device used in the implementation is 1.2 GHzDual-core ARM Cortex-A9 (HPS) that runs under LXDE desktop with 1.0GB memory DDR3. The HPS is located in a VEEK-MT2S that is composedof DE10 standard FPGA and the MTLC2 module provided by TERASIC.4.2. Performance metricsThe used cameras are two web cameras contain color CMOS imagesensor that emit color images with resolution of 640x480 up to 30 framesper second their horizontal view angle is 60
/C14degree. The experiments led us to test the proposed method accuracy formeasuring the distances of objects with the impact of changing the base(distance between the cameras). The proposed system has been tested inseveral scenes in cars parking. Therefore, we took several shots of eachscene by changing base. The followingﬁgure (Fig. 7) shows some scenes used as test.TheTable 1shows some measured distances compared to the realdistance.The results presented in theTable 1shows that the distance mea- surement error depends on the chosen base and also depends implicitlyon object detection quality. The use of several values in the base gavegood results. However, we observe the distance was computed accuratelywith low error using 0,6 m in the base.The experiments led us also to measure the rapidity of the proposedmethod by computing the number of frames treated per second.
Fig. 5.The Overallﬂow diagram of the vehicle detection process.A. Zaarane et al. Array 5 (2020) 100016
4To ensure this part of experiment, we have used three different videossequences taken from road. TheFig. 8shows some used scenes.The followingﬁgure shows some statistics of the number of framestreated per second.TheFig. 9shows that the proposed method could treat up to 23frames per second. the average of frames per second through all the ex-periments is 20.57 frames per second which is enough for real timetreatments.4.3. Evaluation resultsTo evaluate our work, we compared it with three works that we haveimplemented and adapted to our dataset. The method proposed by Hsuand Wang [10] is based on the focal length of the camera, the disparityand the base which is aﬁxed parameter. Mrovlje and Vran /C20ci/C19c[12] pre- sented a method measures the distance using formula based on the baseand the tangent of angle formed by the view angle bisector and the ob-ject. Salman et al. [9] presented a method of measuring distance based ontrigonometric calculations depending on which state is the detectedobject.Table 2shows the results of these three works compared to the resultsof our work in different scenes. This comparison shows that even ourmethod is simple, it has the least error and that our results are moreaccurate than the other results.5. ConclusionA Real-time distance measurement method for self-driven system isintroduced in this paper. The utilized method is based on using stereocamera, which is two cameras mounted in the same horizontal positionand displaced vertically by a predeﬁned distance (the base). To measurethe distance to vehicles, a vehicle detection method is performed ﬁrst following two steps: hypothesis generation and hypothesis veri ﬁcation. In theﬁrst step, the hypotheses are generated using cross-correlationafter performing an edge detection method. In the second step, thegenerated hypotheses are veriﬁed by extracting the desired featuresusing the third level of 2D-DWT and then classify them using AdaBoostclassiﬁer. Several methods apply the detection task on both images,which is time consuming. However, in this paper the vehicles aredetectedﬁrst in only one camera then the similar vehicles are detected onthe other camera using a stereo matching method. After detecting andmatching the same vehicles in both cameras, the distance measurement
Fig. 6.Stereo matching process.
Fig. 7.Some examples of test scenes.
Table 1The measured distance in various base length.
BaseVehiclesMeasured distance (m) Real distance (m)0,1 0,2 0,3 0,4 0,5 0,6 0,7 0,81 7,90 7,94 8,27 8,22 8,16 8,108,08 8,13 8,10 2 9,32 9,30 9,27 9,10 9,22 9,18 9,18 9,21 9,15 3 18,20 18,41 18,37 18,40 18,32 18,32 18,29 18,27 18,34 4 12,30 12,28 12,34 12,33 12,35 12,39 12,3812,37 12,38 5 5,15 5,11 5,32 5,28 5,14 5,225,23 5,23 5,20 6 22,96 22,56 22,66 22,81 22,78 22,73 22,73 22,66 22,70 7 13,89 13,93 14,09 14,06 13,96 13,9814,03 14,04 14 8 15,21 15,20 15,26 15,24 15,33 15,28 15,31 15,3015,30 9 17,16 16,96 17,11 17,11 16,99 17,0617,03 17,02 17,05A. Zaarane et al. Array 5 (2020) 100016
5method based on the distance between the two cameras, the position ofvehicles in both cameras and certain geometric angles, is performed.Although the method is based on relatively simple algorithm, the dis-tance is measured accurately. Furthermore, a comparison between theproposed method and some other methods from literature was performedto evaluate the proposed method, where it showed that despite thesimplicity of the proposed method, it measures the distance with highaccuracy. The proposed method may be used to perform several tasks inseveral systems such as computing safety distance between vehicles andvehicles speed and it may also be used to measure objects distancesregardless of their types by simply changing the detection algorithm.Data availabilityThe data used to support theﬁndings of this study are available fromthe corresponding author upon request.Declaration of competing interestThe authors declare that there are no conﬂicts of interests exist.CRediT authorship contribution statementAbdelmoghit Zaarane:Conceptualization, Methodology, Resources,Software, Formal analysis, Writing - original draft, Writing - review & editing.Ibtissam Slimani:Methodology, Software, Formal analysis,Writing - original draft, Writing - review&editing.Wahban Al Okaishi: Investigation, Resources, Writing - review&editing.Issam Atouf: Validation, Visualization, Supervision. Abdellatif Hamdoun:Valida- tion, Visualization, Supervision.
Fig. 8.Some examples of test scenes from the road.
Fig. 9.Statistics of the frames treated per second according to the detec-ted vehicles.
Table 2The evaluation results of four distance measurement methods.
Vehicles MethodsProposedmethod(m)Hsu et al.[10](m)Mrovljeet al. [12](m)Salmanet al. [9](m)Realdistance(m)18,108,03 7,05 8,20 8,1029,189,11 9,10 9,27 9,153 18,32 18,38 18,3518,40 18,34 412,39 12,37 12,36 12,44 12,38 5 5,225,195,22 5,28 5,20 622,7322,86 22,65 22,86 22,70713,9813,94 13,97 14,09 14815,2815,35 15,27 15,23 15,30917,0617,13 16,99 17,17 17,05A. Zaarane et al. Array 5 (2020) 100016
6References
[1] Zaarane Abdelmoghit, Slimani Ibtissam, Hamdoun abdellatif, et al. Real-timevehicle detection using cross-correlation and 2D-DWT for feature extraction.J. Electrical and Computer Eng 2019;2019. https://doi.org/10.1155/2019/ 6375176.[2]Slimani Ibtissam, Zaarane Abdelmoghit, Hamdoun Abdellatif, et al. Traf ﬁc surveillance system for vehicle detection using discrete wavelet transform. J TheorAppl Inf Technol 2018;96(17) . [3]et Prakoso Puguh Budi, Sari Yuslena. Vehicle detection using backgroundsubtraction and clustering algorithms. Telkomnika 2019;17(3) . [4]et Carullo Alessio, Parvis Marco. An ultrasonic sensor for distance measurement inautomotive applications. IEEE Sens J 2001;1(2):143 . [5]Nakahira Kenji, Kodama Tetsuji, Morita Shin, et al. Distance measurement by anultrasonic system based on a digital polarity correlator. IEEE Trans onInstrumentation and Measurement 2001;50(6):1748 –52. [6]Zhang Zhisheng, Han Yanxiang, Zhou Yifan, et al. A novel absolute localizationestimation of a target with monocular vision. Optik-Int J Light and Electron Optics2013;124(12):1218–23.[7] Aswini N, Uma SV. Obstacle avoidance and distance measurement for unmannedaerial vehicles using monocular vision. Int J Electr Comput Eng 2019;9(5):3504.https://doi.org/10.11591/ijece.v9i5.pp%25p . [8]Huang Liqin, Chen Yanan, Fan Zhengjia, et al. Measuring the absolute distance of afront vehicle from an in-car camera based on monocular vision and instancesegmentation. J Electron Imaging 2018;27(4). 043019 .[9]et Salman, Dawood Yasir, Ku-Mahamud, Ku Ruhana, Kamioka Eiji. Distancemeasurement for self-driving cars using stereo camera. In: Proceedings of the 6thinternational conference on computing and informatics, ICOCI; 2017 . [10]et Hsu Tsung-Shiang, Wang Ta-Chung. An improvement stereo vision imagesprocessing for object distance measurement. Int J Automation and Smart Technol2015;5(2):85–90.[11]Ann Nurnajmin Qasrina, Pebrianti Dwi, Bayuaji Luhur, et al. SKF-based imagetemplate matching for distance measurement by using stereo vision. In: Intelligentmanufacturing&mechatronics. Singapore: Springer; 2018. p. 439 –47. [12]et Mrovlje Jernej, Vrancic Damir. Distance measuring based on stereoscopicpictures. In: 9th international PhD workshop on systems and control: younggeneration viewpoint; 2008. p. 1 –6. [13]Wei S-D, Lai S-H. Fast template matching based on normalized cross correlationwith adaptive multilevel winner update. IEEE Trans Image Process 2008;17(11):2227–35.[14]Slimani I, Zaarane A, et Hamdoun A. Convolution algorithm for implementing 2Ddiscrete wavelet transform on the FPGA. In: Computer systems and applications(AICCSA), 2016 IEEE/ACS 13th international conference of. IEEE; 2016. p. 1 –3. [15]Slimani I, Zaarane A, Hamdoun A, April Atouf I. Vehicle License Plate Localizationand Recognition System for Intelligent Transportation Applications. IEEE; 2019.p. 1592–7.[16]Al OW, Zaarane A, Slimani I, Atouf I, Benrabh M. Vehicular queue lengthmeasurement based on edge detection and vehicle feature extraction. J Theor ApplInf Technol 2019;97(5).A. Zaarane et al. Array 5 (2020) 100016
7