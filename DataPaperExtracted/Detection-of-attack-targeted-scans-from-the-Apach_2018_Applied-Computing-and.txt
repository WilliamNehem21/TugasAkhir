Detection of attack-targeted scans from the Apache HTTP Server accesslogs
Merve Bas/C223Seyyara,⇑, Ferhat Özgür Çatakb, Ensar Güla
a_Istanbul S/C223ehir University, _Istanbul, Turkey
bTubitak/Bilgem, Kocaeli, Turkey
article info
Article history:Received 8 January 2017Revised 25 April 2017Accepted 26 April 2017Available online 28 April 2017Keywords:Rule-based modelLog analysisScan detectionWeb application securityXSS detectionSQLI detectionabstract
A web application could be visited for different purposes. It is possible for a web site to be visited by aregular user as a normal (natural) visit, to be viewed by crawlers, bots, spiders, etc. for indexing purposes,lastly to be exploratory scanned by malicious users prior to an attack. An attack targeted web scan can beviewed as a phase of a potential attack and can lead to more attack detection as compared to traditionaldetection methods. In this work, we propose a method to detect attack-oriented scans and to distinguishthem from other types of visits. In this context, we use access log ﬁles of Apache (or ISS) web servers andtry to determine attack situations through examination of the past data. In addition to web scan detec-tions, we insert a rule set to detect SQL Injection and XSS attacks. Our approach has been applied on sam-ple data sets and results have been analyzed in terms of performance measures to compare our methodand other commonly used detection techniques. Furthermore, various tests have been made on log sam-ples from real systems. Lastly, several suggestions about further development have been also discussed./C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Contents1. Introduction.......................................................................................................... 2 92. Related work.......................................................................................................... 2 93. System model . . . . . . . . . . . . . . . .......................................................................................... 3 0 3.1. Assumptions . . . ................................................................................................. 3 03.2. Data and log generation . . . . . . . . . . . . . . ............................................................................. 3 0 3.2.1. Web servers . . . . . . . . . . . .................................................................................. 3 0 3.2.2. Damn Vulnerable Web Application (DVWA) . .................................................................. 3 0 3.2.3. Web vulnerability scanners . . . . . . . . . . . . . . . .................................................................. 3 0 3.3. Rules and methodology. . . . . . . . . . . . . . . ............................................................................. 3 1 4. Results. . . . ........................................................................................................... 3 54.1. Experimental setup . . . . . . . . . . . . . . . . . . ............................................................................. 3 5 4.2. Model evaluation................................................................................................. 3 54.3. Scan detection on live data. . . . . . . . . . . . ............................................................................. 3 5 5. Conclusion ........................................................................................................... 3 6Appendix A. Supplementary material . . . . . . . . . . . . . . . . . ....................................................................... 3 6 References ........................................................................................................... 3 6
http://dx.doi.org/10.1016/j.aci.2017.04.0022210-8327//C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author.E-mail addresses:merveseyyar@std.sehir.edu.tr (M. Bas/C223Seyyar),ozgur.catak@tubitak.gov.tr(F.Özgür Çatak),ensargul@sehir.edu.tr(E. Gül). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 14 (2018) 28–36
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
1. IntroductionThe dependency to web systems; consisting of web services andweb applications, is growing with time. From health sector to elec-tronic commerce (e-commerce), internet usage is needed in allareas of life. Due to incremental utilization of cloud technology;there is no doubt that this dependency will increase even more.However, web environment is hosting billions of users includingmalicious ones like script kiddies and cyber terrorists. Malicioususers misuse highly efﬁcient automated scan tools to detect vul-nerabilities in web applications. Obtaining diagnostic informationabout web applications and speciﬁc technologies thanks to thesetools is known as ‘‘Reconnaissance” in penetration testing method-ologies and standards like The Penetration Testing Execution Stan-dard (PTES). This information gathering phase is the ﬁrst phase ofall attacks just before exploitation because security vulnerabilitieswould lead to threats either directly or indirectly [1]. Because, an overlooked vulnerability scan may result in a large scale problemssuch as information leakage and privacy violation. As a matter offact, the detection of these malicious scans becomes very crucialto prevent web applications from exploitation and to take effectivecountermeasures almost immediately.According to European Network and Information SecurityAgency (ENISA) Threat Landscape 2015 (ETL 2015) [2], web based and web applications attacks are ranked as number two and threein cyber-threat environment, and their rankings have remainedunchanged between 2014 and 2015. Since web security relatedthreats have been perpetually evolving, web applications are moredisposed to security risks[3]. Also, attack methods to web applica-tions are very diverse and their trends continue for a long time. Forinstance, although Structured Query Language (SQL) injection andCross-Site Scripting (XSS) seem to be at a decreasing rate in 2014,an increase in their exposures is seen in 2015. Therefore, one mayeasily deduce that web systems are in the focus of cyber criminals.To detect all of mentioned attacks and scans, analyzing the logﬁles is usually preferred, because anomalies in users’ requests andrelated server responses could be clearly identiﬁed. Two primaryreasons for this preference are that log ﬁles are easily available,and there is no need for expensive hardware for analysis [4].I n addition, logs may provide successful detection especially forencrypted protocols such as Secure Sockets Layer (SSL) and SecureShell Daemon (SSHD)[5]. However, the heavier the website’s traf-ﬁc is, the more difﬁcult the examination of the log ﬁles gets. There-fore, the need for an user-friendly web vulnerability scan detectiontool by analyzing log ﬁles seems pretty obvious.Therefore, the objectives of this study can be summarized asfollows:/C15to detect vulnerability scans./C15to detect XSS and SQLI attacks./C15to examine access log ﬁles for detections.Accordingly, the contributions of the work can be expressed asfollows:/C15The motivation of the relevant work is quite different, typicallyfocusing on machine-learning based predictive detection ofmalicious activities. Actually, all machine learning algorithmshave training phase and training data to built a classiﬁcationmodel. In order to increase accuracy of machine learning classi-ﬁer model, a large scale input training data is needed. In turn, anincrease in memory consumption would occur. As a result,either the model would turn out to be not trainable, or trainingphase would last for days. On the other hand, executing the pro-posed rule set on access logs does not cause any memory con-sumption problems. Our script simply runs on Ubuntuterminal with a single line of code./C15Another negative aspect of focusing on machine learning isoverﬁtting; referring to a model that models the training datatoo well. Using a very complex models may result in overﬁttingthat may negatively inﬂuence the model’s predictive perfor-mance and generalization ability[6]. Nevertheless, we design our rules to operate on past data which allows a detailed anal-ysis of a user’s actions[4]so that the complexity of ourapproach is not too high./C15The proposed model addresses the detection of web vulnerabil-ity scans on web applications by analyzing log ﬁles retrievedfrom web servers. Since most of the web servers log HTTPrequests by default, data is easily available to be analyzed. Thus,any extra conﬁguration, installation, purchase or data formatmodiﬁcation are not needed. Furthermore, our analysis is basedupon rule-based detection strategy and we built our rule set onseveral features of log entries. As opposed to relevant work, thenumber of these features is low enough to make input data lesscomplex./C15Finally, our work contributes to a better understanding of cur-rent web security vulnerabilities. For example, we can detectweb vulnerability scanners and learn about vulnerability itself
at the same time.The rest of the paper is organized as follows: The related work ispresented inSection 2.Section 3presents our system model indetails. Our model evaluation and real system test results are pre-sented inSection 4. The concluding remarks are given in Section 5.2. Related workWithin this section, the most related researches for vulnerabil-ity scan detection have been reviewed.Auxilia and Tamilselvan suggest a negative security model forintrusion detections in web applications [7]. This method is one of the dynamic detection techniques that is anomaly-based. Theauthors propose to use Web Application Firewall (WAF) with a ruleset protecting web applications from unknown vulnerabilities.When analyzed their rules for Hypertext Transfer Protocol (HTTP)attacks detection, the rules appears to be generated by checkingthe values of some important HTTP header ﬁelds, UniformResource Identiﬁer (URI) strings, cookies, etc. Associating WAF,Intrusion Detection System (IDS), rule engine reasoning togethermakes this article interesting.Goseva-Popstojanova et al.[8]propose a method to classifymalicious web sessions through web server logs. Firstly, theauthors constitute four different data sets from honeypots; onwhich several web applications were installed. Afterwards, 43 dif-ferent features were extracted from web sessions to characterizeeach session and three machine learning methods that are SupportVector Machine (SVM), J48 and Partial Decision Trees (PART) wereused to make the classiﬁcations. The authors assert that when all43 features used in learning period, their method to distinguishbetween attack and vulnerability scan sessions attains high accu-racy rates with low probability of false alarms. This comprehensiveresearch provides signiﬁcant contribution in the area of websecurity.Different from log analysis, Husák et al. [9]analyze extended network ﬂow and parse HTTP requests. In addition to some OpenSystems Interconnection (OSI) Layer 3 and Layer 4 data, theextracted HTTP information from network ﬂow includes hostname, path, user agent, request method, response code, referrerand content type ﬁelds. To group network ﬂow in three classessuch as repeated requests, HTTP scans, and web crawlers; sourceInternet Protocol (IP), destination IP, and requested UniformResource Locator (URL) split into domain and path are used. OneM. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36 29of the interesting results they obtain is that the paths requested forHTTP scans are also requested for brute-force attack as well. How-ever, not only HTTP requests but also HTTP responds should also beanalyzed to get more effective results.After a learning period of non-malicious HTTP logs, Zolotukhinet al.[10]analyze HTTP requests in an on-line mode to detect net-work intrusions. Normal user behavior, anomalies related featuresand intrusions detection are extracted from web resources, queriesattributes and user agent values respectively. The authors compareﬁve different anomaly-detection methods; that are Support VectorData Description (SVDD), K-means, Density-Based Spatial Cluster-ing of Applications with Noise (DBSCAN), Self-Organizing Map(SOM) and Local Outlier Factor (LOF), according to their accuracyrates in detecting intrusions. It is asserted that simulations resultsshow higher accuracy rates compared to the other data-miningtechniques.Session Anomaly Detection (SAD) is a method developed by Choand Cha[11]as a Bayesian estimation technique. In this model,web sessions are extracted from web logs and are labelled as ‘‘nor-mal” or ‘‘abnormal” depending on whether it is below or above theassigned threshold value. In addition, two parameters that are pagesequences and their frequency are investigated in training data. Inorder to test their results; the authors use Whisker v1.4 as a toolfor generating anomalous web requests and it is asserted thatThe Bayesian estimation technique has been successful for detect-ing 91% of all anomalous requests. Therefore, two points makingthis article different from the others are that SAD can be cus-tomized by choosing site-dependent parameters; and the falsepositive rates gets lower with web topology information.Singh et al.[12]have presented an analysis of two web-basedattacks which are i-frame injection attacks and buffer overﬂowattacks. For analysis, log ﬁles created after attacks are used. Theycompare the size of the transferred data and the length of inputparameters for normal and malicious HTTP requests. As a result,they just have carried out descriptive statistics and have not men-tioned any detection techniques.In their work, Stevanovic et al.[13]use SOM and Modiﬁed Adaptive Resonance Theory 2 (Modiﬁed ART2) algorithms fortraining and 10 features related to web sessions for clustering.Then, the authors label these sessions as human visitors, well-behaved web crawlers, malicious crawlers and unknown visitors.In addition to classifying web sessions, similarities among thebrowsing styles of Google, MSN, and Yahoo are also analyzed inthis article. The authors obtain lots of interesting results, one ofwhich is that 52% of malicious web crawlers and human visitorsare similar in their browsing strategies; which means that it is hardto distinguish each other.Another completely different propose a semantic model that isnamed ontological model[14]. They assert that attack signaturesare not independent from programming languages and platforms.As a result, signatures may become invalid after some changes inbusiness logic. In contrary, their model is extendible and reusableand could detect malicious scripts in HTTP requests and response.Also, thanks to ontological model, zero day attacks could be effec-tively detected. Their paper also includes a comparison betweenthe proposed Semantic Model and ModSecurity.There are several differences between our work and the abovementioned works. Firstly, as in the most of the related works,checking only the user-agent header ﬁeld from a list is not enoughto detect web crawlers in the correct way. Correspondingly, we addextra ﬁelds to check to make the web crawler detection more accu-rate. Additionally, unlike machine learning and data-mining,rule-based detection has been used in the proposed model. Finally,in contrast to other works, we prefer to use combined log format inorder to make the number of features larger and to get more con-sistent results.3. System modelIn this section, we describe how we construct and design theproposed model in detail. Also, we present our rules with underly-ing reasons.3.1. Assumptions/C15In access logs, POST data can not get logged. Thus, the proposedmethod cannot capture this sort of data./C15Browsers or application servers may support other encodings.Since only two of them are in the context of this work, our scriptcannot capture data encoded in other styles./C15Our model is designed for detection of two well-known webapplication attacks and malicious web vulnerability scans, notfor prevention. Thus, working on-line mod is not included inthe context of our research.3.2. Data and log generationIn this section, tools, applications, virtual environment usedthroughout this work and their installation and conﬁguration set-tings are explained.3.2.1. Web servers3.2.1.1. HTTP Server.As mentioned earlier, Apache/2.4.7 (Ubuntu)Server is chosen as a web server. Apache is known to be the mostcommonly used web server. According to the W3Techs (Web Tech-
nology Surveys)[15], as of December 1, 2016; Apache is used by51.2 percent of all web servers. In addition, it is open source, highlyscalable and has a dynamically loadable module system. Apacheinstallation is made via apt-get command-line package manager.Any extra conﬁguration is not necessary for the scope of this work.3.2.1.2. Apache Tomcat.The Apache Tomcat being an implementa-tion of the Java Servlet, JavaServer Pages, Java Expression Languageand Java WebSocket technologies, is an open source software [16]. In this work, Apache Tomcat Version 8.0.33 is used. Atlassian JIRAStandalone Edition (Jira 3.19.0-25-generic #26) is used as a webapplication. Access log conﬁguration of Tomcat is set to be similarto access log entries in Apache.3.2.2. Damn Vulnerable Web Application (DVWA)DVWA is a vulnerable PHP/MySQL web application. It isdesigned to help web developers ﬁnd out critical web applicationsecurity vulnerabilities by hands on activity. Different from illegalwebsite-hacking, it offers a totally legal environment to exploit forsecurity people. Thanks to DVWA; Brute Force, Cross Site RequestForgery (CSRF), Command Execution, XSS (reﬂected) and SQL Injec-tion vulnerabilities could be tested for three security levels; low,medium, high.In this work, DVWA 1.0.8 version (Release date: 11/01/2011) isused. To install this web application, Linux Apache MySQL PHP(LAMP) Server; including MySql, PHP5, and phpMyAdmin, hasbeen installed. The reasons for studying with DVWA are to betterunderstand XSS and SQL Injection attacks and to ﬁnd out relatedpayloads substituted in query string part of URIs. In this way, ruleselection to detect these attacks from access logs could be correctlydetermined. Also, web vulnerability scanners used in this work,have scanned this web application for data collection purposes.3.2.3. Web vulnerability scanners3.2.3.1. Acunetix.Acunetix is one of the most commonly used com-mercial web vulnerability scanners. Acunetix scans a web siteaccording to the determined conﬁgurations, produces a reportabout the existing vulnerabilities, groups them as high, medium,30 M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36low and informational; and identiﬁes the threat level of the webapplication with the related mitigation recommendations. In thecontext of this work, Acunetix Web Vulnerability Scanner (WVS)Reporter v7.0 has been used with default scanning conﬁgurationsin addition to site login information.3.2.3.2. Netsparker.Netsparker is a web application security scan-ner that is commercial too. Netsparker detects security vulnerabil-ities of a web application and produces a report includingmitigation solutions. In addition, detected vulnerabilities couldbe exploited to conﬁrm the report results. In the context of thiswork, Netsparker Microsoft Software Library (MSL) Internal Build4.6.1.0 along with Vulnerability Database 2016.10.27.1533 hasbeen used with special scanning conﬁgurations including customcookie information.3.2.3.3. Web Application Attack and Audit Framework (W3AF). W3AF is an open source web application security scanner. W3AF is devel-oped using Python and licensed under General Public License (GPL)v2.0. Framework is designed to help web administrators secure theweb applications. W3AF could detect more than 200 vulnerabilities[17]. W3AF has several plug-ins for different operations such ascrawling, brute forcing, and ﬁrewall bypassing. W3AF comes bydefault in Kali Linux and could be found in ‘‘Applications/WebApplication Analysis/Web Vulnerability Scanners”. W3AF version1.6.54 has been used with ‘‘fast-scan” proﬁle through audit, crawl,grep and output plugins.3.3. Rules and methodologyAs mentioned earlier, our script runs on access log ﬁles. Themain reason for this choice is the opportunity for detailed analysisabout users actions. By examining past data, information securitypolicies for the web applications could be correctly created andimplemented. Additionally, further exploitations could be pre-vented in advance. Unlike the proposed model, Network Intrusion
Fig. 1.Flow chart of the proposed rule-based model.M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36 31Detection System (NIDS) may not detect attacks when HTTPS isused[4]. However, working with logs has some disadvantages.Since log ﬁles do not contain all data of HTTP request and response,some important data could not be analyzed. For example, POSTparameters that are vulnerable to injections attacks could not belogged by web servers. Another negative aspects are the size of logsand parsing difﬁculty. Nevertheless, to solve this problem, we sep-arate the access log ﬁles on a daily basis. Therefore, web adminis-trators might run our script every day to check for an attack. Lastly,real-time detection and prevention is not possible with the pro-posed method which runs off-line. Thus, we could not guaranteeto run on-line. In fact, this approach is conceptually sufﬁcient forthe scope of this work. Differently from the test environment; anextra module that directly accesses logs, or a script that analyseslogs faster could be developed to use our approach in a live or realenvironment.Our method could be described as rule-based detection. Unlikeanomaly based detection, our rules are static including both black-list and whitelist approaches. In detail, XSS and SQL injectiondetection part of our method is a positive security model; on theother hand, the rest is a negative security model. Thus, data eva-sion is tried to be kept at a minimum level. In order to classify IPaddresses in the access log ﬁle, we identify three different visitortypes as follows:
 10000 20000 30000 40000 50000 60000 70000 80000 90000100000110000Log Lines020040060080010001200Running Time (in seconds)
Fig. 2.Time performance of the proposed method.
Table 9Details of log samples.
Log ﬁle Log duration File size Line number IP numberData Set 1 5 days 43 MB 202,145 3910Data Set 2 210 days 13.4 MB 34,487 9269Data Set 3 270 days 7.2 MB 36,310 4719Data Set 4 90 days 1.3 MB 5936 1795Data Set 5 90 days 0.48 MB 3554 579Total 665 days 65.37 MB 282,432 20,272Table 8Summary of results for general data set.
IP number Accuracy Precision Recall F 1
162 99.38% 100.00% 75.00% 85.71%Table 1HTTP methods in Acunetix.
HTTP method NumberConnect 2Get 2758Options 2Post 668Trace 2Track 2Total 3434
Table 2HTTP methods in Netsparker.
HTTP method NumberGet 3059Head 590Netsparker 1Options 14Post 956Propﬁnd 14Total 4634
Table 3HTTP status codes in Netsparker.
HTTP status code Number200 177301 1302 23404 494500 6Total 701
Table 5HTTP status codes in Acunetix.
HTTP status code Number200 598301 38302 686400 44403 16404 2022405 4406 2417 2500 20501 2Total 3434Table 6Details of classiﬁed data sets.
Visitor type Log ﬁle Line number IP numberType 1 Normal 62,539 15Type 2 Web robot 28,804 143Type 3 Acunetix 6539 1Type 3 Netsparker 7314 1Type 3 W3AF 3996 2Type 1, 2 and 3 Total 109,192 162
Table 4HTTP status codes in W3AF.
HTTP status code Number200 91302 8404 30500 6Total 135Table 7Confusion matrix.
Actual: Type 3 Actual: Type 1 or 2Predicted: Type 3 TP = 3 FN = 1Predicted: Type 1 or 2 FP = 0 TN = 15832 M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36/C15Type 1:Regular (normal) users with a normal (natural) visit./C15Type 2:Crawlers, bots, spiders or robots./C15Type 3:Malicious users using automated web vulnerabilityscanners.As shown inFig. 1in Phase 1, our ﬁrst step is to detect SQLinjection and XSS attacks. Although different places of HTTP (theHTTP body, URI) could be used to exploit a vulnerability [4];w e will analyze path and query parts of the requested URI fordetection.In detail; for XSS, we use regular expressions to recognize somepatterns such as HTML tags, ‘src’ parameter of the ‘img’ tag andsome Javascript event handlers. Likewise; for SQL injection, wecheck the existence of the singlequote, the doubledash, ‘#’, exec()function and some SQL keywords. In addition, since there is a pos-sibility for URL obfuscation, Hex and UTF-8 encodings of these pat-terns are also taken in consideration.Afterwards, we continue by separating IP addresses of Type 2from the rest of the access log ﬁle in Phase 2. To do this, two differ-ent approaches are used. Firstly, user-agent part of all log entries iscompared with the user-agent list from robots database that ispublicly available in[18]. However, since this list may not be up-to-date, another bot detection rules are added. In order to identifythese rules, we use the following observations about web robots:1. Most of the web robots make a request for ‘‘/robots.txt” ﬁle [19]. 2. Web robots have higher rate of ‘‘4xx” requests since they usu-ally request unavailable pages[20–23]. 3. Web robots have higher unassigned referrer (‘‘–”) rates [23–25].Table 10Data sets test results.
Data set Period IP number Type 3 IP number Type 3 percentage (%)Data Set 1 2004 10/March 370 13 3.51 11/March 786 20 2.5412/March 1002 22 2.2013/March 1960 39 1.9914/March 1079 21 1.95 Data Set 2 2004 April 3140 1 0.03 May 4546 3 0.07June 701 6 0.86July 735 4 0.54August 189 1 0.53September 280 0 0.00October 106 1 0.94 Data Set 3 2005 June 663 1 0.15 July 755 1 0.13August 577 0 0.00September 731 1 0.14October 452 0 0.00November 623 19 3.05December 181 1 0.55January 652 45 6.90February 802 34 4.24 Data Set 4 2005 1–15/June 160 1 0.63 16–30/June 497 0 0.001–15/July 503 0 0.0016–30/July 280 1 0.361–15/August 284 0 0.0016–30/August 282 0 0.00 Data Set 5 2005 16–31/January 28 0 0.00 1–15/February 176 0 0.0016–28/February 112 0 0.001–15/March 225 3 1.3316–30/March 28 0 0.00
10.Mar.0411.Mar.0412.Mar.0413.Mar.0414.Mar.04 Days00.511.522.533.54Type 3 (%)
Fig. 3.Data Set 1 test results.Apr May Jun Jul Aug Sep Oct
Months00.10.20.30.40.50.60.70.80.91Type 3 (%)
Fig. 4.Data Set 2 test results.M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36 334. According to the access logs that we analyzed, user-agentheader ﬁeld of web robots may contain some keywords suchas bot, crawler, spider, wanderer, and robot.As a result of above mentioned observations, we add some extrarules to correctly distinguish Type 2 from other visitors.For the rest our rule set as indicated at Phase 3 in Fig. 1, we con- tinue by investigating our access log ﬁles formed as a result of vul-nerability scanning mentioned in the previous section. As shown inTables 1 and 2, our ﬁrst immediate observation is that as comparedto Type 2 and Type 1, Type 3’s requests include different HTTPmethods; such as Track, Trace, Netsparker, Pri, Propﬁnd and Quit.Secondly, as shown inTable 3,Tables 4 and 5; we deduct thatstatus codes of Type 3 differ from Type 2 and Type 1. In fact, Type3 has higher rate of ‘‘404” requests, average of which for Acunetix,Netsparker and W3AF is 31% in our data set. Thus, we generate arule to check the presence of these HTTP methods and the percent-age of ‘‘404” requests. User-agent header ﬁelds of Type 3 couldgenerally be modiﬁed and obfuscated manually at the conﬁgura-tion phase before vulnerability scan. Even so, we made a list ofwell-known automated web vulnerability scanners, and compareit with user-agent header ﬁelds. Finally, we notice that these scan-ners make at least more than 100 HTTP requests in a certain time,we select this value as a threshold for Type 3 detection.The pseudo code of the proposed model is shown in Algorithm 1:Algorithm 1.Pseudo-Code for Proposed Model.
∈
←−∈
⊿
←−∈
←−←−←−←−””∈”” ” ” ” ” ” ” ” ”””∈
←−∈
⊿
←−←−←−∈”” ”” ” ” ” ” ” ” ” ”
⊿34 M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–364. ResultsThis section is based on the evaluation of our model againstsome important metrics. Moreover, test results of attack detectionon live data are also included.4.1. Experimental setupTo implement our rules, Python programming language version3.5 has been chosen. Script is executed on Ubuntu operating sys-tem mentioned in Section3.2.2via terminal. To parse log lines,‘‘apache-log-parser 1.7.0” which is a Python package has beenused. As well as, we beneﬁt from python libraries that are collec-tions, datetime, numpy, ua-parser and argparse.Since there are not any actual, publicly available and labelled datasets to evaluate our model, we create our data sets. In fact, we deploytwo different web applications on two different web servers to formType 1 and Type 3 trafﬁcs. Details are expressed in Section 3.2.2. Type 1 (normal trafﬁc) is the data set collected from Jira Soft-ware as a web application running on Tomcat web server during4 days.The size of the related access log ﬁle is 16.3 MB. As shownTable 6, log ﬁle contains 62,539 log entries from 15 different IPaddresses. These requests are generated in a local network.For Type 2 trafﬁc, an external trafﬁc that is open to the internetis needed. To this end, we make use of three different access logﬁles retrieved from a company website. In detail, log ﬁles containcrawling data collected during 13 days from requests of severalweb robots. The size of the related access log ﬁles is totally6.4 MB, and log ﬁles contain 28,804 log entries from 143 differentIP addresses as shownTable 6.To generate Type 3 trafﬁc, DVWA running on Apache HTTP Ser-ver is used as a web application. Before scanning, the security levelof DVWA is conﬁgured as low security. Moreover, we scan thisapplication via Acunetix, Netsparker and W3AF as web vulnerabil-ity scanners. Firstly, DVWA is scanned for 22 min and 22 s withAcunetix. Secondly, DVWA is scanned for 19 min and 56 s withNetsparker. Lastly, DVWA is scanned for 2 min and 6 s withW3AF. The details of the related access log ﬁles are summarizedas Type 3 inTable 6.For the evaluation of the proposed model, we combine all men-tioned access log ﬁles into one ﬁle that is our general data set.Then, we run our Python script on the mentioned data set.4.2. Model evaluationInitially, to evaluate the proposed model, we compute the con-fusion matrix where TP, FN, FP, and TN denote true negatives, falsenegatives, false positives, and true negatives respectively as showninTable 7.After, we evaluate the following measures:
accuracyðaccÞ¼ðTNþTPÞðTNþFNþFPþTPÞprecisionðprecÞ¼
ðTPÞðTPþFPÞrecallðrecÞ¼
ðTPÞðTPþFNÞF
1score¼ð2TPÞð2TPþFPþFNÞð1Þ
More speciﬁcally, the accuracy provides the percentage of Type 3that are detected correctly. The precision determines the fractionof IP addresses correctly classiﬁed as Type 3 over all IP addressesclassiﬁed as Type 3. The recall (a.k.a. sensitivity) is the fraction ofIP addresses correctly classiﬁed as Type 3 over all IP addresses ofType 3. Finally, the F
1-score is a harmonic mean of precision andrecall. As a result, our model has 99.38% accuracy, 100.00% preci-sion, 75.00% recall and ﬁnally 85.71% F1score as we can see in Table 8.
Fig. 2illustrates the relation between the line number of the logﬁles and the running time. It is clear that the running time risessteadily as the number of the lines increases.4.3. Scan detection on live dataWe have built or model according to the data sets mentioned inSection4.1. Additionally, we test our model according to severallarge-scale, live, not labelled and publicly available data sets. Inthis section, we share our test results illustrated in tables andgraphs.In accordance with this purpose, we have used log samplesfrom real systems[26]. As stated in the related web source, thesesamples are collected from various systems, security devices,applications, etc.; and neither Chuvakin nor we did not sanitize,anonymized or modiﬁed them in any way. Since they include HTTPaccess logs, we have chosen the log samples named Bundle 9, Bun-dle 7, Bundle 1, Bundle 4 and Bundle 3. For the rest of the work,these bundles are expressed as Data Set 1, Data Set 2, Data Set 3,Data Set 4 and Data Set 5 respectively. Details of these data setsare shown inTable 9.In order to test the log samples, Data Set 1, Data Set 2, Data Set3, Data Set 4 and Data Set 5 are divided into daily, monthly,
123456
15-Day Period00.10.20.30.40.50.60.7Type 3 (%)
Fig. 6.Data Set 4 test results.Jun Jul Aug Sep Oct Nov Dec Jan Feb
Months01234567Type 3 (%)
Fig. 5.Data Set 3 test results.M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36 35monthly, 15-day and 15-day periods respectively. Related detailsare expressed inTable 10.Type 3 percentage of each data set is shown in Figs. 3–7.5. ConclusionIn this work, we studied web vulnerability scans detectionthrough access log ﬁles of web servers in addition to detection ofXSS and SQLI attacks. In accordance with this purpose, we usedrule-based methodology. Firstly, we examined the behavior ofthe automated vulnerability scanners. Moreover, we implementedour model with a Python script. Afterwards, our model has beenevaluated based on data we have collected. Finally, we tested ourmodel on the log samples from real systems.It is clear that our method has very high probability of detectionand low probability of false alarm. More speciﬁcally, the accuracyand the precision rates of our model are 99.38%, 100.00% respec-tively. More importantly, malicious scans can be captured moreprecisely because different types of scanning tools including bothopen source and commercial tools were examined. Therefore, ourresults indicates that static rules can detect successfully web vul-nerability scans. Besides, we have observed that our model func-tions properly with larger and live data sets and correctly detectsType 3 IP addresses.As shown in theFig. 2, the relation between the number of linesof the log ﬁles and the running time is linear. As a result, how longa log ﬁle would be analyzed, could be predicted in advance.The results presented in this work may enhance researchesabout malicious web scans and may support the development ofattack detection studies. Also, if security analysts or administratorsexecute the proposed python script several times within the sameday, he/she could prevent most of the web related attacks.Future work considerations related to this work are twofold. Inthe ﬁrst place, one could make our model possible to analyze otherlog ﬁles such as audit log and error log. Secondly, in addition to thescope of this work; different from SQLI and XSS attacks, other well-known web application attacks like CSRF could be addressed too.Appendix A. Supplementary materialSupplementary data associated with this article can be found, inthe online version, athttp://dx.doi.org/10.1016/j.aci.2017.04.002 .References
[1] E.M. Hutchins, M.J. Cloppert, R.M. Amin, Intelligence-driven ComputerNetwork Defense Informed by Analysis of Adversary Campaigns andIntrusion Kill Chains, vol. 1, API, 2011, URL <https://books.google.com. tr/books?id=oukNfumrXpcC> . [2] European Union Agency for Network and Information Security (ENISA), ENISAThreat Landscape 2015. URL < https://www.enisa.europa.eu/publications/ etl2015>, 2016 (accessed November 29, 2016).[3] D.V. Bernardo, Clear and present danger: interventive and retaliatoryapproaches to cyber threats, Appl. Comput. Infor. 11 (2) (2015) 144–157,http://dx.doi.org/10.1016/j.aci.2014.11.002 , URL<http:// www.sciencedirect.com/science/article/pii/S2210832714000386> . [4] R. Meyer, Detecting Attacks on Web Applications from Log Files. URL < https:// www.sans.org/reading-room/whitepapers/logging/detecting-attacks-web-applications-log-ﬁles-2074>, 2008 (accessed December 12, 2016). [5] D.B. Cid, Log Analysis using OSSEC. URL < http://www.academia.edu/8343225/ Log_Analysis_using_OSSEC>, 2007 (accessed November 29, 2016). [6] Wikipedia, Overﬁtting. URL < https://en.wikipedia.org/wiki/Overﬁtting >, 2016 (accessed December 27, 2016).[7] M. Auxilia, D. Tamilselvan, Anomaly detection using negative security model inweb application, in: 2010 International Conference on Computer InformationSystems and Industrial Management Applications (CISIM), 2010, pp. 481–486,http://dx.doi.org/10.1109/CISIM.2010.5643461 . [8] K. Goseva-Popstojanova, G. Anastasovski, R. Pantev, Classiﬁcation of maliciousweb sessions, in: 2012 21st International Conference on ComputerCommunications and Networks (ICCCN), 2012, pp. 1–9, http://dx.doi.org/ 10.1109/ICCCN.2012.6289291 . [9] M. Husák, P. Velan, J. Vykopal, Security monitoring of http trafﬁc usingextended ﬂows, in: 2015 10th International Conference on Availability,Reliability and Security, 2015, pp. 258–265, http://dx.doi.org/10.1109/ ARES.2015.42.[10] M. Zolotukhin, T. Hämäläinen, T. Kokkonen, J. Siltanen, Analysis of httprequests for anomaly detection of web attacks, in: 2014 IEEE 12thInternational Conference on Dependable, Autonomic and Secure Computing,2014, pp. 406–411,http://dx.doi.org/10.1109/DASC.2014.79 . [11] S. Cho, S. Cha, Sad: web session anomaly detection based on parameterestimation, Comput. Secur. 23 (4) (2004) 312–319, http://dx.doi.org/10.1016/ j.cose.2004.01.006, URL<http://www.sciencedirect.com/science/article/pii/S0167404804000264>.[12] N. Singh, A. Jain, R.S. Raw, R. Raman, Detection of Web-Based Attacks byAnalyzing Web Server Log Files, Springer India, New Delhi, 2014, http://dx.doi. org/10.1007/978-81-322-1665-0_10 , pp. 101–109. [13] D. Stevanovic, N. Vlajic, A. An, Detection of malicious and non-maliciouswebsite visitors using unsupervised neural network learning, Appl. SoftComput. 13 (1) (2013) 698–708, http://dx.doi.org/10.1016/j. asoc.2012.08.028, URL<http://www.sciencedirect.com/science/article/pii/S1568494612003778>.[14] A. Razzaq, Z. Anwar, H.F. Ahmad, K. Latif, F. Munir, Ontology for attackdetection: an intelligent approach to web application security, Comput. Secur.45 (2014) 124–146,http://dx.doi.org/10.1016/j.cose.2014.05.005 , URL<http:// www.sciencedirect.com/science/article/pii/S0167404814000868> . [15] W3Techs (Q-Success DI Gelbmann GmbH), Usage Statistics and Market Shareof Apache for Websites. URL < https://w3techs.com/technologies/details/ws- apache/all/all>, 2009–2017 (accessed December 12, 2016).[16] The Apache Software Foundation, Apache Tomcat. URL < http://tomcat.apache. org> (accessed December 24, 2016).[17] w3af.org, w3af. URL <http://w3af.org>, 2013 (accessed December 12, 2016). [18] The Web Robots Pages, Robots Database. URL < http://www.robotstxt.org/db. html> (accessed September 4, 2016).[19] M.C. Calzarossa, L. Massari, D. Tessera, An extensive study of web robotstrafﬁc, in: Proceedings of International Conference on InformationIntegration and Web-based Applications &#38; Services, IIWAS ’13, ACM,New York, NY, USA, 2013, pp. 410:410–410:417, http://dx.doi.org/10.1145/ 2539150.2539161.[20] M.D. Dikaiakos, A. Stassopoulou, L. Papageorgiou, An investigation of webcrawler behavior: characterization and metrics, Comput. Commun. 28 (8)(2005) 880–897,http://dx.doi.org/10.1016/j.comcom.2005.01.003 , URL <http://www.sciencedirect.com/science/article/pii/S0140366405000071> . [21] M. Dikaiakos, A. Stassopoulou, L. Papageorgiou, Characterizing CrawlerBehavior from Web Server Access Logs, Springer Berlin Heidelberg, Berlin,Heidelberg, 2003,http://dx.doi.org/10.1007/978-3-540-45229-4_36 . [22] M.C. Calzarossa, L. Massari, Analysis of Web Logs: Challenges and Findings,Springer Berlin Heidelberg, Berlin, Heidelberg, 2011, http://dx.doi.org/ 10.1007/978-3-642-25575-5_19 , pp. 227–239. [23] D. Stevanovic, A. An, N. Vlajic, Feature evaluation for web crawler detectionwith data mining techniques, Expert Syst. Appl. 39 (10) (2012) 8707–8717,http://dx.doi.org/10.1016/j.eswa.2012.01.210 , URL<http:// www.sciencedirect.com/science/article/pii/S0957417412002382> . [24] A.G. Lourenço, O.O. Belo, Catching web crawlers in the act, in: Proceedings of the6th International Conference on Web Engineering, ICWE ’06, ACM, New York,NY, USA, 2006, pp. 265–272,http://dx.doi.org/10.1145/1145581.1145634 . [25] D. Stevanovic, A. An, N. Vlajic, Detecting Web Crawlers from Web Server AccessLogs with Data Mining Classiﬁers, Springer Berlin Heidelberg, Berlin, Heidelberg,2011,http://dx.doi.org/10.1007/978-3-642-21916-0_52 , pp. 483–489. [26] A. Chuvakin, Public Security Log Sharing Site. URL < http://log- sharing.dreamhosters.com>, 2009 (accessed December 15, 2015).12345
15-Day Period00.20.40.60.811.21.4Type 3 (%)
Fig. 7.Data Set 5 test results.36 M. Bas/C223Seyyar et al. / Applied Computing and Informatics 14 (2018) 28–36