Explainable artiﬁcial intelligence and interpretable machine learningfor agricultural data analysis
Masahiro Ryoa,b,⁎
aLeibniz Centre for Agricultural Landscape Research (ZALF), Eberswalder Str. 84, 15374 Müncheberg, Germany
bBrandenburg University of Technology Cottbus –Senftenberg, Platz der Deutschen Einheit 1, 03046 Cottbus, Germany
abstract article info
Article history:Received 22 September 2022Received in revised form 14 November 2022Accepted 15 November 2022Available online 17 November 2022Artiﬁcial intelligence and machine learning have been increasingly applied for prediction in agricultural science.However, many models are typically black boxes, meaning we cannot explain what the models learned from thedata and the reasons behind predictions. To address this issue, I introduce an emerging subdomain of arti ﬁcial intelligence, explainable artiﬁcial intelligence (XAI), and associated toolkits, interpretable machine learning.This study demonstrates the usefulness of several methods by applying them to an openly available dataset.The dataset includes the no-tillage effect on crop yield relative to conventional tillage and soil, climate, and man-agement variables. Data analysis discovered that no-tillage management can increase maize crop yield whereyield in conventional tillage is <5000 kg/ha and the maximum temperature is higher than 32°. These methodsare useful to answer (i) which variables are important for prediction in regression/classi ﬁcation, (ii) which var- iable interactions are important for prediction, (iii) how important variables and their interactions are associatedwith the response variable, (iv) what are the reasons underlying a predicted value for a certain instance, and(v) whether different machine learning algorithms offer the same answer to these questions. I argue that thegoodness of modelﬁt is overly evaluated with model performance measures in the current practice, whilethese questions are unanswered. XAI and interpretable machine learning can enhance trust and explainabilityin AI.© 2022 The Author. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Interpretable machine learningExplainable artiﬁcial intelligenceAgricultureCrop yieldNo-tillageXAI
1. IntroductionArtiﬁcial intelligence (AI) and machine learning are increasinglyused for prediction in agriculture ( Benos et al., 2021;Liakos et al., 2018). They often outperform conventional statistical parametricmodels like generalized linear models in predictive performance(Breiman, 2001a). A general linear regression, for example, needs thevariables to follow normality and linearity; therefore, data transforma-tion is often needed. Meanwhile, random forests and arti ﬁcial neural networks do not need such transformation procedures. In addition, ma-chine learning algorithms can automatically discover nonlinearity andvariable interactions (Ryo and Rillig, 2017). These tools are now easy to learn because various online courses are nowadays available, lower-ing the hurdle for students and researchers to start using machine learn-ing in their projects.AI and machine learning make statistical modeling more predictive,but it comes at a cost. It sacriﬁces interpretability. Machine learningalgorithms that achieve a higher predictive performance tend to bemore complex, like random forests, gradient boosting, and arti ﬁcial neu- ral networks (Breiman, 2001a). Increasing model complexity (with reg-ularization) is key to enhancing predictability. However, the mostaccurate model is often too complex for human beings to interpret thelogic behind a prediction, the so-called black box. We cannot explainwhat the model learned from the data, why it predicts a certain valuefor a given instance, and when it tends to make a mistake. In general,there is a trade-off between the accuracy and interpretability of statisti-cal models (Breiman, 2001a).Achieving both high accuracy and interpretability is challenging(Breiman, 2001a), but most researchers would agree that the employedmodel should be both accurate and easy to interpret. Providing inter-pretable predictions is more important than providing accurate predic-tions with a black-box model for decision-making ( Rudin, 2019;Rudin et al., 2022). For instance, an AI model suggests a farmer change the cur-rentﬁeld management from conventional tillage to no-tillage so thatyield can increase by 10%. Surely, the farmer wants to know why themodel predicted so. The model developer should also know if themodel learned agriculturally meaningful patterns from the data and ifthe reasons behind each prediction make sense. What if the modelArtiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
⁎Corresponding author at: Leibniz Centre for Agricultural Landscape Research (ZALF),Eberswalder Str. 84, 15374 Müncheberg, Germany.E-mail address:Masahiro.Ryo@zalf.de(M. Ryo).
https://doi.org/10.1016/j.aiia.2022.11.0032589-7217/© 2022 The Author. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the C C BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/discovers a strange but interesting pattern? One can investigate it fur-ther to evaluate if the discovery is important or not. For these purposes,AI and machine learning need to be interpretable and explainable(Meske and Bunde, 2020;Ribeiro et al., 2016). To fulﬁll this demand, we can make use of the emerging sub ﬁeld of the AI domain, explainable AI (XAI), especially a set of tools, interpret-able machine learning (Adadi and Berrada, 2018;Doshi-Velez and Kim, 2017;Molnar, 2019;Murdoch et al., 2019;Rudin et al., 2022). XAI aims to develop tools for enhancing the interpretability of complexalgorithms without sacriﬁcing predictability (Carvalho et al., 2019). The XAI domain has been gaining much attention in the past decade, and itspotential has been disseminated to several natural science ﬁelds, such as biodiversity research (Ryo et al., 2021), geoscience (Mamalakis et al., 2022), and hydrological/climatic science ( Başağaoğlu et al., 2022). In the agricultural domain, several previous studies have started applyingthe techniques since 2020 (Fig. 1): Crop yield estimate (Sihi et al., 2022; Wolanin et al., 2020); crop type and trait classiﬁcation using satellite (Newman and Furbank, 2021;Orynbaikyzy et al., 2020); soil texture classiﬁcation (Zhou et al., 2022); leaf disease classiﬁcation (Wei et al., 2022); waterﬂux and quality assessment (Garrido et al., 2022;Zhang et al., 2022); IoT based smart agriculture system ( Sabrina et al., 2022); biomethane production (De Clercq et al., 2020); agricultural land iden- tiﬁcation (Viana et al., 2021). However, these studies use only a few par-ticular methods. Moreover, potentially several articles are using XAImethods without emphasizing the usage. Nevertheless, I argue thatthe XAI concept and several useful techniques remain largelyunintroduced to the agricultural domain.This article aims to demonstrate the potential of XAI, especially in-terpretable machine learning techniques, for analyzing agriculturaldatasets. After a brief introduction to the concept of interpretable ma-chine learning, I show how interpretable machine learning methodscan be used for discovering novel patterns from a tabular dataset. As a
case study, I use the global dataset for crop production under conven-tional tillage and no-tillage systems openly available from Su et al. (2021). The analysis gives a novel insight into under which conditionsno-tillage management can improve Maize crop yield compared to con-ventional tillage management (seesection 2.2for the detailed descrip- tion of the dataset). I made the analysis fully reproducible with thedata and R script available on GitHub, hoping that to facilitate readers'hands-on learning (https://github.com/masahiroryo/2022_IML_Agriculture.git).[(“machine learning”OR“artiﬁcial intelligence”)A N D“agricul*”] (topic) and [(“interpretable machine learning”OR“explainable artiﬁcial intelligence”OR“explainable machine learning”OR“XAI”OR“inter- pretable ML”OR“explainable AI”)A N D“agricul*”] (topic), respectively. The search was done on 30.08.2022, and the number in 2022 wasmultiplied by 1.5 so that it can be an estimate for the end of the year,compatible with the past years.2. Methods2.1. Interpretable machine learning: An overviewMachine learning algorithms can make accurate predictions, but un-derstanding the rationales behind predictions is often dif ﬁcult. The lack of interpretability makes scientists and stakeholders wonder how muchthey should trust what the models predict regardless of accuracy(Meske and Bunde, 2020;Ribeiro et al., 2016). This problem developed the idea of XAI and various tools, namely, interpretable machine learn-ing (Murdoch et al., 2019). XAI aims to develop tools for enhancing theinterpretability of complex machine learning algorithms withoutsacriﬁcing accuracy (Carvalho et al., 2019). XAI has been gaining popu- larity rapidly in recent years, and many new interpretable machinelearning methods have been proposed, reviewed, and applied in variousscientiﬁcﬁelds recently (Boehmke and Greenwell, 2020;Molnar, 2019; Murdoch et al., 2019;Ryo et al., 2021).Most interpretable machine learning methods are categorized inmodel selection, method generality, and explanation scale ( Adadi and Berrada, 2018;Molnar, 2019;Murdoch et al., 2019). Firstly, model selec- tion is either model-based or post-hoc. Model-based means that a ma-chine learning algorithm used for the study is rather simple anddirectly interpretable (e.g., decision tree and generalized additivemodel), while post-hoc means that a complex machine learning algo-rithm (e.g., random forests and gradient boosting) is used for thestudy. Then theﬁtted model is analyzed with some statistical methods.Secondly, method generality is either model-speci ﬁco rm o d e l - a g n o s t i c . Some methods can be used only for the corresponding algorithm(e.g., Gini importance for tree-based algorithms), but many methodsare developed and can be used for any algorithm, so-called model-agnostic. Thirdly, explanation scale is either global or local. Globalmeans interpreting what the model learned from the entire variable dis-tribution (e.g., if predictor X is positively associated with the response).Local means interpreting the rationale behind every single predictiongiven by the model (e.g., the model predicts this plant is sick, but whydoes it predict so?). Note that different terminology may also be usedfor method classiﬁcation in other studies because the XAI domain isstill at emergence and dynamic.2.2. DatasetI use the global dataset for crop production under conventional till-age and no-tillage systems, which are openly available from Su et al. (2021). The dataset contains paired yield observations comparing con-ventional tillage and no-tillage conditions for eight major staple cropsin 50 countries. The dataset reports crop yield, crop growing season,management practices, soil characteristics, and key climate parametersthroughout the experimental year.Conventional tillage is a tillage system using cultivation as the pri-mary means of seedbed preparation and weed control with emphasison soil preparation, including a sequence of soil tillage, such as plowingand harrowing, and the removal of most of the plant residue from theprevious crop (OECD, 2001). No-tillage (also zero tillage) is a minimumtillage practice where the crop is sown directly into the soil not tilledsince the harvest of the previous crop, weed control is achieved byusing herbicides, and stubble is retained for erosion control ( OECD, 2001). No-tillage is recognized as one of the key conservational agricul-tural strategies without compromising crop yield ( Phillips et al., 1980),
Fig. 1.Publication trend in“AI and machine learning”and“XAI and interpretable machine learning”in agricultural science according to the Web of Science Core Collection. XAI:Explainable artiﬁcial intelligence. The search queries were.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
258but this statement is controversial. A recent global meta-analysis studysynthesizing 678 studies across 50 crops with 6005 paired observationsconcluded that no-tillage reduces crop yield by 5%, and especially thenegative impact of no-tillage was the largest for maize (−7.6%) (Pittelkow et al., 2015).As a case study, I analyzed maize. Although the largest negative ef-fect was found for maize, it is just a global average across various condi-tions. I hypothesized that the effect of no-tillage can be positive undersome conditions, and the conditions can be identi ﬁed using interpret- able machine learning. If the conditions were discovered, our scienti ﬁc knowledge would improve:“On average, no-tillage reduces maizeyield; however, no-tillage can increase yield if the condition is ….”Ia n - alyzed the relative yield change in maize (%) that was quanti ﬁed by comparing no-tillage to conventional tillage in a paired experimentalsetup.the most dominant crop type in the dataset with global coverage(n=1 2 7 1 ;Fig. 2a). A relative change in crop yield from conventionalto no-tillage was random (Fig. 2b; mean =−0.02, standard deviation = 0.25; note that the extreme values of 97.5th percentile or higherwere removed), indicating that whether no-tillage increases ordecreases crop yield compared to conventional tillage is quite contro-versial. With machine learning modeling, I explored under which condi-tions the effect tends to be positive.2.3. ModelingA relative change in Maize crop yield from conventional to no-tillagewas regressed with 17 variables: Crop yield under conventional tillageas baseline (Yield_CT) [kg/ha]; latitude and longitude of experimentalsites accounting for spatial dependence [degree]; Years since no-tillage started accounting for lagged effect (Years_NT); crop rotationwith at least three crops involved in conventional tillage and no-tillage for temporal dependency (Crop_rotation_CT and _NT) [yes/no];soil texture (ST) [seven categories related to sand, silt, clay composi-tion]; soil cover (Soil_cover_CT and _NT) [yes/no/mixed]; weed andpest control (Weed_pest_control_CT and _NT) [yes/no]; Precipitationand potential evapotranspiration over the growing season and their dif-ference for water availability (P, E, PB, respectively) [mm]; average,maximum, and minimum air temperature during the growing season(Tave, Tmax, Tmin, respectively) [degree Celsius].The modeling process is illustrated in Fig. 3. The sample (n = 1271) was split randomly into a training and test dataset (80:20 split). Fourmachine learning algorithms were used: linear model with AIC stepwisevariable selection, decision tree (conditional inference tree; Hothorn et al., 2006), random forests (Breiman, 2001b), and gradient boosting (Friedman, 2001). The former two algorithms are relatively simplemodels with high model-based interpretability. The latter two are com-plex models combining 100–10,000 models (weak learners), and there-fore they require post-hoc interpretable methods for understandingmodel behavior. These four models were compared to show how themodels learn differently. Note that the interpretable machine learningmethods I introduce can be used with any other machine learningmethods like support vector machines and arti ﬁcial neural network. A 5-fold cross-validation was employed for ﬁnding the best hyperparameter set for decision tree (mincriterion = 0.01), randomforests (mtry = 12) and gradient boosting (n.trees = 1000, interac-tion.depth = 3) in terms of root mean squared error (RMSE). Modelperformance was evaluated with R-squared (R
2)a n dR M S E .2.4. Interpretable machine learning methodsI use a set of post-hoc, model-agnostic methods (3 global and 1 local)so that model behavior can be compared among algorithms in a stan-dard way: Permutation-based variable importance (global), pairwiseinteraction importance (global), partial dependence plot (global), andLIME local variable importance (Fig. 3).Permutation-based variable importance measure: This is a measureto rank the relative importance of predictor variables for prediction. Thefundamental idea is that if one randomly permutes the values of an im-portant variable in the training data, the model performance would
Fig. 2.Collection of experiments comparing Maize crop yield in conventional and no-tillage conditions ( n= 1271): (a) experimental site distribution and (b) histogram of yield change in no-tillage relative to conventional tillage. Data is available from Su et al. (2021), and extreme values (97.5 percentile) were removed.
Fig. 3.Study framework for novel pattern discovery using interpretable machine learningmethods after implementing machine learning algorithms (i.e., post-hoc analysis). LIME:Local Interpretable Model-Agnostic Explanations.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
259degrade because permutation destroys the relationship between thevariable and the response variable ( Breiman, 2001b). The larger the loss in model performance, the larger its importance. The importancemeasure is based on the difference between a baseline performancemeasure (R
2in this study) and the same performance measure obtainedafter permuting the values of a particular variable in the training data.To account for random variability due to permutation, I calculatedpermutation-based importance thirty times and took an average.Pairwise interaction importance: This measure is used to quantifythe strength of two-way interaction effects that affects model predic-tion. The fundamental idea is that if a certain variable pair (X
i,Xj) has a strong interaction strength, the modeled association between X
iand the response variable would strongly depend on the other variable'svalue, X
j. I used the method inGreenwell et al. (2018).I te v a l u a t e s how much theﬂatness of the modeled association of X
ito the response variable changes by changing the value of X
j, calculating the standard deviation of aﬂatness score. This procedure is also done byﬂipping X
iand X jto take an average. Another popular approach forquantifying interaction strength is Friedman's H-statistic ( Friedman and Popescu, 2008). But, I did not use this approach becauseGreenwell et al. (2018)warned that Friedman's H-statistic may not ad-equately discover strong interactions (yet, Greenwell et al. did not argueany potential reasons).”Partial dependence plot: This method helps visualize the modeledassociation between a subset of the predictors (conventionally, 1 –2v a r - iables) and the response while accounting for the average effect of theother predictors in the model (Friedman, 2001). To estimate the associ- ation of X
iwith the response, the model gives predictions given a ﬁxed value of X
iwhile changing the values of all the other predictors availablein the training dataset. This procedure is done for the entire range of X
i.I refer to the method (Greenwell, 2017), while many other approaches are available. This is becauseGreenwell (2017)offers thepdppackage, the most generalized implementation in R with a clear documentationfor practical usage.LIME variable importance: LIME stands for Local InterpretableModel-agnostic Explanations (Ribeiro et al., 2016), a technique to eval- uate the variable importance for each prediction. LIME assumes thateven though a complex machine learning model shows a nonlinear,non-additive behavior, the behavior can be approximated with a sim-pler model like a linear model (so-called local surrogate model). I imple-mented the version byMolnar (2019). In short, when a prediction is made with the machine learning model, LIME generates many datapoints by slightly perturbing the predicted case. It ﬁts a locally weighted linear regression model with L1-regularization to the points whereweights are based on their proximity to the predicted case. Then, thevariable importance of the linear model is reported. In this study, Iused the Canberra distance with the kernel width of 2 because of agood modelﬁt, while other distance measures with a different widthcan be used.2.5. Programming language and reproducibilityAll data handling and analysis were done in R version 4.2.1 ( RC o r e Team, 2022) with the following libraries: For data handling and visual-ization, tydiverse (Wickham et al., 2019), patchwork (Pedersen, 2022), stars (Pebesma et al., 2022), rnaturalearth (South, 2017); for machine learning implementation, caret (Kuhn, 2008); for interpretable machine learning methods, vip (Greenwell et al., 2020), pdp (Greenwell, 2017), iml (Molnar and Schratz, 2022). The script and data are available inthe GitHub repository (https://github.com/masahiroryo/2022_IML_Agriculture.git).3. ResultsThe model performance revealed random forests as the best algo-rithm (R
2= 0.42; RMSE = 0.199), followed by gradient boosting(0.33; 0.200), decision tree (0.18; 0.225), and linear model (0.11;0.236) (Fig. 4).In terms of variable importance, the random forests and gradientboosting commonly selected yield in conventional tillage as the bestpredictor, followed by temperature-related variables ( Fig. 5c, d). The decision tree and linear model also selected yield in conventional tillageas one of the top predictors but regarded it as less important than soiltexture (Fig. 5a, b). On the contrary, random forests and gradientboosting did not select soil texture within the top ten predictors.Variable importance was also evaluated for discovering key variableinteractions. Interaction strength was investigated for all possible pairsamong the variables that were selected within the top 3 in variable im-portance by at least one algorithm (Fig. 5). In total, six variables were in- vestigated, accounting for theﬁfteen pairwise combinations: Yield_CT,Tmax, Tave, Tmin, ST, Soil_cover_NT. Overall, different algorithmslearned different interactions as important for prediction. The linearmodel showed no importance for any pairs because no interactionswere included in the formula (Fig. 6a). Both random forests and gradi-ent boosting identiﬁed the interaction of Yield_CT and Tmax as thestrongest one (Fig. 6c, d). The decision tree selected this interactionpair as the top 3 (Fig. 6b). The top 3 pairs identiﬁed by each algorithm included one of the top 3 important variables in Fig. 5. Hereafter, I decided to investigate the effects of Yield_CT and Tmaxmore because random forests and gradient boosting selected these var-iables within the top 3 in variable importance ( Fig. 5) and the strongest combination (Fig. 6c, d). Partial dependence plots were depicted for di-agnosing how the associations between Yield_CT and relative yieldchange were modeled by each of the four algorithms ( Fig. 7a). All models suggest a negative relationship. However, the strength andcurve shape differed among the models. The linear model suggested alinear relationship, the decision tree suggested a unimodal curve, andboth random forests and gradient boosting suggested a negative butnonlinear relationship where the slope of the curve gets milder alongwith Yield_CT. The models except the linear one suggested no associa-tion with Yield_CT > 15,000 because the data points were scarce(Fig. 7c). In terms of Tmax (Fig. 7b), the linear model, decision tree,and random forests suggested a positive relationship with relativeyield change. Both decision tree and random forests identi ﬁed a sharp stepwise relationship around Tmax = 32 ( Fig. 7b). Gradient boosting did not show a clear association.Two-dimensional partial dependence plots were depicted to visuallyconﬁrm the interaction effects of Yield_CT and Tmax ( Fig. 8). All models except the linear one suggest that relative yield change is conditional to
Fig. 4.Model performance.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
260Fig. 5.Permutation-based variable importance.
Fig. 6.Pairwise variable interaction importance.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
261both Tmax and Yield_CT. The patterns identi ﬁed with all models but lin- ear one show a clear split in the patterns along Tmax around 32° Celsiusand Yield_CT around 5000, suggesting the interaction effect of thesevariables. This interaction effect was further con ﬁrmed by depictingpartial dependence plots of Yield_CT conditional to a Tmax 32-degthreshold (Fig. 9). It is visible that the association of Yield_CT is strongerif Tmax is higher than 32°. Note that this interaction pattern could beidentiﬁed only by the previous data analysis procedure. It is not
Fig. 7.Partial dependence plots for Yield_CT (a) and Tmax (b) with the data distributions (c, d).
Fig. 8.Partial dependence plot (2D). A brighter yellow region (top-left) indicates that crop yield in no-tillage is higher than conventional tillage, whil e a darker blue region (bottom-right) indicates the opposite.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
262discoverable when only partial dependence plots for a single variableare investigated, as seen inFig. 7a.Until here (Figs. 4–9), the focus was to explain global model behav-ior to understand what the models learned from the data. However, itdoes not explain local model behavior, which is important for answer-ing what the models consider important when predicting a valuegiven a speciﬁc instance. To showcase a local model behavior diagnostic,I used the LIME method for evaluating the variable importance of a ran-domly selected local site. The site was an experimentation ﬁeld in Rwanda (Fig. 10e), where the value of relative yield change was−0.239 (Yield_CT = 9200; Yield_NT = 7000). At the site, all modelsbut the linear one suggested that evapotranspiration (E = 520 mm)had a positive effect and soil type of clay (ST = Clay) had a negative ef-fect (Fig. 10a-d). These variables were more important than Yield_CTand Tmax, the most important variables for regulating the globalmodel behavior (Fig. 4), indicating that globally important variablesare not necessarily important locally because of context dependence.4. DiscussionAnalyzing the global dataset of maize crop yield as a case study, Idemonstrated how a set of interpretable machine learning tools couldbe used for agricultural data analysis. All methods are post-hoc andmodel-agnostic, meaning they apply to any machine learning algo-rithms after training with the data. I used permutation-based variableimportance, pairwise variable interaction importance, and partial de-pendence plot for global model interpretation. I identi ﬁed that relative yield change can be positive where yield in conventional tillage issmaller than 5000 [kg/ha], and the maximum temperature is higherthan 32° Celcius. For local model behavior, I used the LIME method, re-vealing that locally important variables can differ from the global onesbecause the conditions are different site by site. While machine learningapplications are increasingly popular in agriculture, they often do notuse these methods or just a few. These methods can be applied for pat-tern discovery from any structured (i.e., tabular) dataset and test the re-liability of machine learning methods while addressing nonlinearity,variable interactions, and context dependency.The discovered pattern can be interesting for agronomists workingwith maize, although explaining the pattern agriculturally is beyondthe aim of this study. The most similar work is a global meta-analysisof crop yield under conventional and no-tillage conditions ( Pittelkow et al., 2015). Analyzing 6005 paired observations from 678 studies for50 crops, they concluded that no-tillage reduces yields on average by5.1%, and the reduction rate was the worst for maize crops (−7.6%; −2.7% in this study).Pittelkow et al. (2015)also explored some reasons based on previous reviews and meta-analyses, concluding that maizeyield decreases, especially in cooler climates and areas with high precip-itation (Ogle et al., 2012;Rusinamhodzi et al., 2011;Toliver et al., 2012; Van den Putte et al., 2012). My case study analysis suggests yield can de-crease in non-hot climates (32° as a threshold), which is in line withthem, while precipitation was not a key factor. However, no previousstudies found yield in conventional tillage as the strongest factorinteracting with temperature: Therefore, this pattern is a new piece ofknowledge discovery. Moreover, a meta-analysis tends to focus on theeffect (positive or negative) on the mean value of the entire (global)data. Identifying locally speciﬁc effects, nonlinear thresholds, and stronginteractions from data is a promising avenue for data synthesis andexploration.
Fig. 9.Partial dependence plot of Yield_CT conditional to Tmax value (higher or lower than 32° Celsius). It suggests that relative yield change becomes high er where yield in conventional tillage is lower than 5000, and the maximum temperature is higher than 32°.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
263As the next step, an emerging, exciting question can be “why”–why do the sites with a lower yield in conventional tillage and a maximumtemperature over 32° are more likely to increase Maize crop yieldwith no-tillage in comparison to conventional tillage? Here, the userof interpretable machine learning needs to communicate with a domainexpert to explore the potential reasons behind the pattern. If one comesup with a potential reason (without supporting evidence), it is so-called“hypothesis generation”, where the hypothesis can be tested based onan experiment for causality.Testing causality is necessary for understanding the mechanismregardless of the strength of a discovered pattern because machinelearning methods can only explore correlation but not causation ( Ryo et al., 2021). Correlation can emerge without causation, and causationcan also emerge without correlation. Correlation should be carefullyinterpreted with the potential existence of any confounding factor. Astrong correlation is useful for prediction as a proxy for any underlyingmechanisms, but caution is needed because this approach is invalidwhen the underlying mechanisms change over time ( Dormann et al., 2013).I believe that XAI and interpretable machine learning can bring sub-stantial beneﬁts to agricultural science. However, I also elaborate majorcaveats. The largest, fundamental question is if we should ever use post-hoc model-agnostic methods for explaining complex models or just usesimpler models that can be more directly interpreted ( Krishnan, 2020; Molnar et al., 2020;Rudin, 2019). Basically,“explaining the modeled as- sociations”is not the same as“explaining the real causal associations ” (Lipton, 2018). In particular, high stakes decision making needs inter-pretable models instead of explaining black box models (post-hoc)(Rudin, 2019;Rudin et al., 2022). Some post-hoc methods have param-eters that affect the results, meaning that the explanation changes quiteeasily. For instance, the LIME method requires the user to specify thedistance measure, kernel width, the number of predictors used, andthe proximity method. The result can differ depending on the setting.Also, one needs to pay attention to bias in the data. Globally collecteddatasets are often biased to information from developed countries orcertain regions. Spatially extrapolating the modeled associations to anovel environment can be highly misleading, especially when the con-dition of the predicted environment does not ﬁt in the probability distri- bution of the training data (Meyer and Pebesma, 2022). In conclusion, I hope that this article encourages applications of XAIand interpretable machine learning tools in the agriculture domain. Thescript is available, so one can learn how the methods were implementedfrom the code. Opening the black box is a promising next step for AIapplications in agriculture.Credit author statementMasahiro Ryo: This single author covered all processes from con-ceptualization to writing.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgmentThis study was supported by ZALF Integrated Priority Project(IPP2022)“Co-designing smart, resilient, sustainable agricultural
Fig. 10.Local Interpretable Model-Agnostic Explanations (LIME) method for explaining the variable importance at the randomly selected local experimenta tion site (red point in panel e; Bugusera, Rwanda; latitude = 2°21 ′S, longitude = 30°15′E). Tave: average temperature; Tmax: maximum temperature; E: Evapotranspiration; P: Precipitation; CT: conventional tillage; N T :n o - t i l l a g e ;S T :S o i lt y p e .M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
264landscapes with cross-scale diversiﬁcation”, Bundesministerium für Bildung und Forschung (BMBF) Land-Innovation-Lausitz project“Landschaftsinnovationen in der Lausitz für eine klimaangepassteBioökonomie und naturnahen Bioökonomie-Tourismus ”(03WIR3017A), BMBF project“Multi-modale Datenintegration, domänenspezi ﬁsche Methoden und KI zur Stärkung der Datenkompetenz in derAgrarforschung”(16DKWN089), and Brandenburgische TechnischeUniversität Cottbus-Senftenberg GRS cluster project “Integrated analysis of Multifunctional Fruit productionlandscapes to promote ecosystem ser-vices and sustainable land-use under climate change ”(GRS2018/19). I thank two anonymous reviewers for constructive comments.References
Adadi, A., Berrada, M., 2018.Peeking inside the black-box: a survey on explainable arti ﬁ- cial intelligence (XAI). IEEE Access 6, 52138 –52160. Başağaoğlu, H., Chakraborty, D., Lago, C.D., et al., 2022. A review on interpretable and ex- plainable artiﬁcial intelligence in hydroclimatic applications. Water. 14, 1230.Benos, L., Tagarakis, A.C., Dolias, G., et al., 2021. Machine learning in agriculture: a compre- hensive updated review. Sensors 21, 3758.Boehmke, B., Greenwell, B., 2020. Hands-On Machine Learning with R Available at. Breiman, L., 2001a.Statistical modeling: the two cultures. Stat. Sci. 16, 199 –215. Breiman, L., 2001b.Random Forests. Mach. Lang. 45, 5 –32. Carvalho, D.V., Pereira, E.M., Cardoso, J.S., 2019. Machine learning interpretability: a sur- vey on methods and metrics. Electronics 8, 832.De Clercq, D., Wen, Z., Fei, F., et al., 2020. Interpretable machine learning for predicting biomethane production in industrial-scale anaerobic co-digestion. Sci. Total Environ.712, 134574.Dormann, C.F., Elith, J., Bacher, S., et al., 2013. Collinearity: a review of methods to deal with it and a simulation study evaluating their performance. Ecography 36, 27 –46. Doshi-Velez, F., Kim, B., 2017. Towards a rigorous science of interpretable machine learn-ing. ArXiv 1702, 08608.Friedman, J.H., 2001.Greedy function approximation: a gradient boosting machine. Ann.S t a t .2 9( 5 ) ,1 1 8 9–1232.Friedman, J.H., Popescu, B.E., 2008. Predictive learning via rule ensembles. Ann. Appl. Stat. 2( 3 ) ,9 1 6–954.Garrido, M.C., Cadenas, J.M., Bueno-Crespo, A., et al., 2022. Evaporation forecasting through interpretable data analysis techniques. Electronics 11, 536.Greenwell, B.M., 2017.Pdp: an R package for constructing partial dependence plots. R J. 9,421–436.Greenwell, B.M., Boehmke, B.C., McCarthy, A.J., 2018. A simple and effective model-based variable importance measure. ArXiv 1805, 04755.Greenwell, B.M., Boehmke, B., Gray, B., 2020. vip: Variable Importance Plots. https://cran. r-project.org/web/packages/vip/index.html . H o t h o r n ,T . ,H o r n i k ,K . ,Z e i l e i s ,A . ,2 0 0 6 . Unbiased recursive partitioning: a conditional in- ference framework. J. Comput. Graph. Stat. 15 (3), 651 –674. Krishnan, M., 2020.Against interpretability: a critical examination of the interpretabilityproblem in machine learning. Philos. Technol. 33, 487 –502. Kuhn, M., 2008.Building predictive models in R using the caret package. J. Stat. Softw. 28,1–26.Liakos, K.G., Busato, P., Moshou, D., et al., 2018. Machine learning in agriculture: a review. Sensors 18, 2674.Lipton, Z.C., 2018.In machine learning, the concept of interpretability is both importantand slippery. Queue 16, 28.Mamalakis, A., Barnes, E.A., Ebert-Uphoff, I., 2022. Investigating the Fidelity of explainable artiﬁcial intelligence methods for applications of convolutional neural networks ingeoscience. Artif. Intell. Earth Syst. 1 (4), e220012.Meske, C., Bunde, E., 2020.Using explainable artiﬁ
cial intelligence to increase trust in computer vision. ArXiv 2002, 01543.Meyer, H., Pebesma, E., 2022.Machine learning-based global maps of ecological variablesand the challenge of assessing them. Nat. Commun. 13, 2208.Molnar, C., 2019. Interpretable Machine Learning. A Guide for Making Black Box ModelsExplainable. 2nd Ed.https://christophm.github.io/interpretable-ml-book/index.html . Molnar, C., Schratz, P., 2022. iml: Interpretable Machine Learning. https://cran.r-project. org/web/packages/iml/index.html .Molnar, C., König, G., Herbinger, J., et al., 2020. Pitfalls to avoid when interpreting machine learning models. ArXiv 2007, 04131.Murdoch, W.J., Singh, C., Kumbier, K., et al., 2019. Deﬁnitions, methods, and applications in interpretable machine learning. Proc. Natl. Acad. Sci. 116, 22071 –22080. Newman, S.J., Furbank, R.T., 2021. Explainable machine learning models of major crop traits from satellite-monitored continent-wide ﬁeld trial data. Nat. Plants 7, 1354. OECD, 2001.Environmental Indicators for Agriculture –Vol. 3: Methods and Results (glos- sary: p399-400). OECD Publ. Serv. 409.Ogle, S.M., Swan, A., Paustian, K., 2012. No-till management impacts on crop productivity, carbon input and soil carbon sequestration. Agric. Ecosyst. Environ. 149, 37 –49. Orynbaikyzy, A., Gessner, U., Mack, B., et al., 2020. Crop type classiﬁcation using fusion of Sentinel-1 and Sentinel-2 data: assessing the impact of feature selection, optical dataavailability, and parcel sizes on the accuracies. Remote Sens. 12, 2779.Pebesma, E., Sumner, M., Racine, E., et al., 2022. Stars: spatiotemporal arrays. RasterVector Data Cubes.https://cran.r-project.org/web/packages/stars/index.html . Pedersen, T.L., 2022. Patchwork: the Composer of Plots. https://cran.r-project.org/web/ packages/patchwork/index.html . Phillips, R.E., Thomas, G.W., Blevins, R.L., et al., 1980. No-tillage agriculture. Science 208, 1108–1113.Pittelkow, C.M., Linquist, B.A., Lundy, M.E., et al., 2015. When does no-till yield more? A global meta-analysis. Field Crop Res. 183, 156 –168. R Core Team, 2022. R: A Language and Environment for Statistical Computing. Austria,Vienna. Retrieved fromhttps://www.R-project.org/. Ribeiro, M.T., Singh, S., Guestrin, C., 2016. “Why should I trust you?”: explaining the predictions of any classiﬁer. ArXiv 1602, 04938.Rudin, C., 2019.Stop explaining black box machine learning models for high stakesdecisions and use interpretable models instead. Nat. Mach. Intell. 1, 206 –215. Rudin, C., Chen, C., Chen, Z., et al., 2022. Interpretable machine learning: fundamental principles and 10 grand challenges. Stat. Surv. 16 (1 –85), 3. Rusinamhodzi, L., Corbeels, M., van Wijk, M.T., et al., 2011. A meta-analysis of long-term effects of conservation agriculture on maize grain yield under rain-fed conditions.Agron. Sustain. Dev. 31, 657.Ryo, M., Rillig, M.C., 2017.Statistically reinforced machine learning for nonlinear patternsand variable interactions. Ecosphere 8, e01976.Ryo, M., Angelov, B., Mammola, S., et al., 2021. Explainable artiﬁcial intelligence enhances the ecological interpretability of black-box species distribution models. Ecography 44,199–205.Sabrina, F., Sohail, S., Farid, F., et al., 2022. An interpretable artiﬁcial intelligence based smart agriculture system. Cmc-Comput. Mater. Contin. 72, 3777 –3797. Sihi, D., Dari, B., Kuruvila, A.P., et al., 2022. Explainable machine learning approach quan- tiﬁed the long-term (1981-2015) impact of climate and soil properties on yields ofmajor agricultural crops across CONUS. Front. Sustain. Food Syst. 6, 847892.South, A., 2017. Rnaturalearth: World Map Data from Natural Earth. https://cran.r- project.org/web/packages/rnaturalearth/index.html . Su, Y., Gabrielle, B., Makowski, D., 2021. A global dataset for crop production under con- ventional tillage and no tillage systems. Sci. Data 8, 33.Toliver, D.K., Larson, J.A., Roberts, R.K., et al., 2012. Effects of no-till on yields as inﬂuenced by crop and environmental factors. Agron. J. 104, 530 –541. Van den Putte, A., Govers, G., Diels, J., et al., 2012. Soil functioning and conservation tillage in the Belgian Loam Belt. Soil Tillage Res. 122, 1 –11. Viana, C.M., Santos, M., Freire, D., et al., 2021. Evaluation of the factors explaining the use of agricultural land: a machine learning and model-agnostic approach. Ecol. Indic.131, 108200.Wei, K., Chen, B., Zhang, J., et al., 2022. Explainable deep learning study for leaf disease classiﬁcation. Agron.-Basel 12, 1035.Wickham, H., Averick, M., Bryan, J., et al., 2019. Welcome to the Tidyverse. J. Open Source Softw. 4, 1686.Wolanin, A., Mateo-Garcia, G., Camps-Valls, G., et al., 2020. Estimating and understanding crop yields with explainable deep learning in the Indian Wheat Belt. Environ. Res.Lett. 15, 024019.Zhang, Z., Huang, J., Duan, S., et al., 2022. Use of interpretable machine learning to identify the factors inﬂuencing the nonlinear linkage between land use and river water qual-ity in the Chesapeake Bay watershed. Ecol. Indic. 140, 108977.Zhou, Y., Wu, W., Wang, H., et al., 2022. Identiﬁcation of soil texture classes under vege- tation cover based on Sentinel-2 data with SVM and SHAP techniques. IEEE J. Sel.Top. Appl. Earth Obs. Remote Sens. 15, 3758 –3770.M. Ryo Artiﬁcial Intelligence in Agriculture 6 (2022) 257 –265
265