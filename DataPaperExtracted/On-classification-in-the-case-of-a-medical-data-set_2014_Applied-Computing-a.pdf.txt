ORIGINAL ARTICLE (INVITED)
On classiﬁcation in the case of a medicaldata set with a complicated distribution
Martti Juholaa,*, Henry Joutsijokia, Heikki Aaltob,Timo P. Hirvonen
b
aComputer Science, School of Information Sciences, University of Tampere, Tampere 33014, Finland
bDepartment of Otorhinolaryngology & Head and Neck Surgery, University of Helsinki and HelsinkiUniversity Central Hospital, Helsinki 00029 HUS, FinlandReceived 27 February 2014; accepted 14 March 2014
Available online 25 March 2014
KEYWORDSData mining;Data cleaning;Complicated datadistributions;ClassiﬁcationAbstract In one of our earlier studies we noticed how straightforward cleaningof our medical data set impaired its classiﬁcation results considerably with somemachine learning methods, but not all of them, unexpectedly and against intui-tion compared to the original situation without any data cleaning. After a moreprecise exploration of the data, we found that the reason was the complicatedvariable distribution of the data although there were only two classes in it. Inaddition to a straightforward data cleaning method, we used an efﬁcient waycalled neighbourhood cleaning that solved the problem and improved our clas-siﬁcation accuracies 5–10%, at their best, up to 95% of all test cases. This showshow important it is ﬁrst very carefully to study distributions of data sets to beclassiﬁed and use different cleaning techniques in order to obtain best classiﬁca-tion results.
ª2014 King Saud University. Production and hosting by Elsevier B.V. All rights reserved.
*Corresponding author. Tel.: +358 40 1901716; fax: +358 3 2191001.E-mail address:Martti.Juhola@sis.uta.ﬁ(M. Juhola). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2014) 10, 52–67
King Saud University
Applied Computing and Informatics
www.ksu.edu.sawww.sciencedirect.com
2210-8327ª2014 King Saud University. Production and hosting by Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.aci.2014.03.0011. Introduction
In our earlier research we developed a signal analysis method for nystagmic eyemovements investigated in otoneurological tests (Juhola et al., 2009, 2011). For the automatic analysis of such signals poor or invalid nystagmic eye movementsshould correctly be separated from valid nystagmic eye movements, because valideye movements can only be used for the data analysis needed for the diagnostics ofotoneurological patients. Typically, invalid nystagmic eye movements are cor-rupted by noise or artefacts. Thus, we have also studied the classiﬁcation of nys-tagmic eye movement candidates into invalid and valid, hereafter called therejected and accepted, on the basis of machine learning methods ( Juhola et al., 2013). We then observed how their complicated distribution made the classiﬁca-tion task difﬁcult and attempted to reduce the greater subset (class) of the rejectedeye movement candidates in a learning set, which was performed by cleaning awaya part from them. Surprisingly, a simple cleaning process impaired classiﬁcationresults of some of the machine learning methods applied. We realized that the rea-son for such a seemingly conﬂicting situation originated from the complicated var-iable distribution of the data (Juhola et al., 2013).
In order to deﬁne which distribution of two classes is seen as simple or compli-cated we refer toFig. 1. A simple distribution is where the centre (computed asmeans for all variables) of each class is located inside its own area including mostelements of that class. This is described inFig. 1(a). A complicated distribution isdepicted inFig. 1(b), where the centre of one class is outside its own area. Suchcomplicatedness could be deﬁned in various ways, but it is essential that we cannotthen base data cleaning on distances from the class centres.
Nystagmus is formed from repeated, reﬂexive eye movements occurring asto-and-for beats that can be measured in the horizontal, vertical and torsionaldirections with two eye movement video cameras, one for each eye. A hypotheticnystagmic eye movement beat is seen inFig. 2and an actual signal of severalnystagmic beats inFig. 3. A nystagmic beat includes the slow phase immediatelyfollowed by the fast phase in order to return the eye in the opposite direction.Nystagmic beats are repetitive and their conﬁgurations vary even in the courseof short measurement times. A healthy subject performs nystagmic eye move-ments, for example, when he or she is sitting in a moving train and looks at chang-ing (relatively close) views through a window. This is called optokinetic nystagmusbecause of the stimulation. Caloric nystagmus is induced by injecting a smallquantity of cool or warm (37 ± 7/C176C) air or water into the ear canal of a subject.In regard to some otoneurological disorders or diseases, head shaking or headmovement can provoke nystagmus and even spontaneous nystagmus may appearin vestibular patients. Congenital nystagmus also exists ( Hertle and Dell’Osso, 1999). The slow phase features (variables) of nystagmus are important for thediagnostics of vestibular neuritis, positional vertigo, vestibular schwannoma andMenie`re’s disease. The fast phase features of nystagmus are important forClassiﬁcation of medical data set 53investigations of the central origin, i.e., the brain. Our current data included mea-surements from 107 patients mainly with spontaneous nystagmus. Here the slowphase variables were medically essential.
Since the 1970s, dozens of different nystagmus detection algorithms have beenpresented (Abel et al., 2008; Augustyniak, 1996; Hertle and Dell’Osso, 1999;Hosokawa et al., 2004; Juhola, 1988; Tominaga and Tanaka, 2010; Wall andBlack, 1982). Most of them have been on the basis of applying digital ﬁlters,(a) 
(b) 
A
Fig. 1Let us assume that there are two hypothetical classes A and B in a variable space. Elements of themajority class A close to B and possibly from their overlapping area would be useful to be cleaned out. (a) Asimple distribution in which the majority class A (more elements) and minority class B partially overlap and inwhich elementxfrom A is closer to the centre of B (square) than to the centre (circle) of its own class A. Thus, x can clearly be cleaned on the basis of the distance criterion. (b) A complicated distribution in which xfrom A is closer to its own centre than to that of B. Further, the centre of A is outside its actual area. Element xcannot be cleaned on the basis of the distance criterion.54 M. Juhola et al.actual slow phase
y = line between samples x(s) and x(f)actual fast phase amplitude
durationx(s)durationx(e)x(f)
s fefastphase
amplitude [º]
time [s] amplitudeslow phase 
Fig. 2A hypothetic nystagmic beat describing its main variables and shape from which some other variablesare derived. A beat is composed of the slow and fast phases represented with their amplitudes given in angulardegrees and durations in seconds. In reality, there may be noise or other unevenness in nystagmus signals. Suchphenomena are estimated by computing, for example, linear correlation between an actual slow phase and itsideal component, straight liney.
0 50 100 150 200 250 300 350 400 450 500-20-15-10-505101520amplitude [°]left eye
samples
(1)(2)(3)(5)verticaltorsional
horizontal
(4)
Fig. 3Here a three-dimensional nystagmic eye movement signal is depicted as three one-dimensionalcomponent signals so that their features can be visualized and understood. The component signals are horizontal(blue), vertical (green) and torsional (red). The signal is 10 s long sampled at 50 Hz. Segments (1)-(5) were deemeddue to be rejected nystagmic beats because of signal corruption. Segment (1) was corrupted both in the horizontaland torsional components. Segments from (2) to (5) were dropouts of video camera images in the torsionalcomponent, in other words, the camera system had momentarily failed to identify the eye in its successive images.Peaks with lower amplitudes than 1/C176were seen noise or unevenness being neither valid nor invalid nystagmicbeats.Classiﬁcation of medical data set 55thresholding, other signal analysis and tuning parameters. Nevertheless, it is notonly to detect nystagmic beat candidates, but the separation of acceptable beatsfrom those corrupted or noisy is a necessary stage in processing before computingnystagmus variable values from all accepted beats. Typically, this stage has beengrounded on conventional signal analysis methods and separately on every singlenystagmic eye movement. However, this is difﬁcult, because poor nystagmic beatsmay vary remarkably between patients and also depend on measurement devices.
Recently, we developed an approach based on machine learning and collected adata set of nystagmic eye movement beats, both accepted and rejected, in order toform a training set for machine learning and classiﬁcation of nystagmus signals(Juhola et al., 2013). To our knowledge, no such attempt has previously beenmade for nystagmus data. Nonetheless, the classiﬁcation task was difﬁcult becausethe data distribution appeared to be of a rare form in which the two classes of thedata were very close to each other or partially overlapped so that accepted nystag-mic beats were, in a way, surrounded by those rejected beat candidates ( Juhola et al., 2013). This resulted in an extraordinary outcome that our then straightfor-ward data cleaning technique impaired the classiﬁcation results computed withsome machine learning methods. In the present study we applied a more efﬁcientmethod (Laurikkala, 2001) to overcome this problem.
In the present description we do not express in detail how nystagmic beat can-didates were detected and how they were determined to be acceptable, because allthese are preprocessing phases for the study to be described and because they arepresented precisely in our earlier articles (Juhola et al., 2009, 2011, 2013).
2. The data set
Our data set included 19 variables that were computed for nystagmic beat candi-dates in order to advance their identiﬁcation into accepted and rejected beats. Notall of these variables are medically useful, i.e., it is probable that otoneurologicaldisorders do not affect all of them. In our previous research we, however, observedthat all 19 variables are useful for the present classiﬁcation task ( Juhola et al., 2013). The variables are given inTable 1. See alsoFig. 2for symbols. The variable distributions are given in detail in (Juhola et al., 2013).
The data set included one signal from each of 107 patients suffering chieﬂy fromacute, unilateral, peripheral loss of vestibular function, in other words, vestibularneuritis or having had a surgery for acoustic neuroma. Nystagmus of a patient sit-ting in a chair in the darkened room was measured with two small eye movementvideo cameras that detected eye movements with an image processing system.Each signal was 30 s long and included 20–80 acceptable nystagmic beats.
Every signal was analysed with a nystagmus detection algorithm ( Juhola et al., 2011) and nystagmic beat candidates were, at the same time, separated either intoaccepted or rejected beats according to our algorithm ( Juhola et al., 2013). Inde- pendent of this automatic separation, an expert manually explored all nystagmic56 M. Juhola et al.beat candidates, signal by signal, accepting or rejecting nystagmic beats based onvisual screening. The purpose of applying both manual and automatic ways was tocompare their results and also to use both of them jointly to see whether classiﬁ-cation results could further be improved this way.
3. Data cleaning procedures
3.1. Straightforward cleaning procedure
In our original cleaning procedure (Juhola et al., 2013) we assumed that there would be two classes in the data having their class centres in different areas. Thisassumption was reasonable in the sense that the poor nystagmic beat candidateswere marked to be rejected (manually or automatically) typically since some oftheir variable values were above some upper bounds. For instance, segments(2)–(5) inFig. 3would create too high torsional mean velocities for slow or fastphases of nystagmus because of the very steep spikes in the torsional signal. Clean-ing was made ﬁrst by computing the class centres and second by deleting those re-jected nystagmic beats that were the closest to the class centre of the opposite classuntil the class size of the rejected nystagmic beat candidates became as small as theopposite class.
A more precise data analysis showed, however, that the class distribution wascomplicated so that no two clearly separate class centres appeared. The distribu-tions computed with principal component analysis are given after the automaticrejection and acceptance of nystagmic beat candidates in Fig. 4(a) and similarly inFig. 4(b) after ﬁrst reducing the larger class of the rejected candidates accordingTable 1Variables of the nystagmus data set deﬁned with symbols associated with Fig. 2. In the variable names,spandfpare slow and fast phases of nystagmus, adenotes amplitude,dduration,vmean velocity,c correlation,mvmaximum velocity,qquality signal of the torsional component, and subscripts h,vandt indicate horizontal, vertical and torsional component signals.Variables Explanation Deﬁnitionspa
h,spa v,spa tAmplitudes of slow phase |x(f)/C0x(s)| fpa
h,fpa v,fpa tAmplitudes of fast phase |x(e)/C0x(f)| spd
h Duration of slow phase (f/C0s)/fr,frsampling frequency fpd
h Duration of fast phase (e/C0f)/fr,frsampling frequency spv
h,spv v,spv tMean velocities of slow phase Slope according to linear regression from samples { x(s), x(s+ 1),...,x(f)} fpv
h,fpv v,fpv tMean velocity of fast phase Slope according to linear regression from samples { x(f), x(f+ 1),...,x(e)} spc
h,spc v,spc tCorrelations of slow phase Correlation coeﬃcient between samples { x(s),x(s+ 1), ...,x(f)} and {y(s),y(s+ 1),...,y(f)} fpmv
h Maximum velocity of horizontalfast phaseSlopesz(i) according to linear regression from samples{x(i/C02),x(f/C01),x(i),x(i+ 1),x(i+ 2)},i=f+2 ,..., e/C02, and then by taking max
i2ffþ2;...;e/C02gfzðiÞg q
t Mean of torsional signal qualityvaluesMean of quality values during slow phase of interval [ s,f]Classiﬁcation of medical data set 57to the cleaning procedure called neighbourhood cleaning ( Laurikkala, 2001) de- scribed below.Fig. 4(a) shows how heavily the distributions of two classes wereoverlapping and that how cleaning given inFig. 4(b) alleviated this difﬁculty.
After the manual selection of nystagmic beat candidates there were 2171 ac-cepted and 3818 rejected beats. After the automatic selection these numbers were2517 and 3472, and after using the both ways jointly 1645 and 4344, respectively.Thus, after cleaning, i.e., reducing the larger class of the rejected beats, the num-bers were 2171, 2517 and 1645 for each of the classes in these three situations.-200 0 200 400 600 800 1000 1200 1400-600-400-2000200400600800
1st principal component2nd principal componentaccepted
rejected
-500 0 500 1000 1500 2000 2500-1000-800-600-400-20002004006008001000
1st principal component2nd principal componentaccepted
rejected(a)
(b)
Fig. 4(a) Before neighbourhood cleaning and (b) after this. The two ﬁrst (most important) principalcomponents were used to show the distributions of the accepted and rejected nystagmic beats after the automaticselection to the accepted and rejected beats. To make the scatter plots clear, one tenth of all beats only were drawnsince their occurrences overlap considerably, particularly in the ‘‘left corner’’ of either distribution. The two ﬁrstprincipal components accounted for 80% of variance in (a) before cleaning and 87% in (b) after cleaning.58 M. Juhola et al.3.2. Neighbourhood cleaning procedure
The above straightforward cleaning procedure even impaired the classiﬁcationaccuracies from 80–90% (without cleaning) down to half for nearest neighboursearching, naı¨ve Bayes rule and logistic discriminant analysis (Juhola et al., 2013), whereas linear and quadratic discriminant analysis and support vectormachines with the linear and quadratic kernels obtained minor improvements.For the sake of these unexpected negative effects we abandoned the precedingcleaning procedure in the present study and used another, more sophisticatedmethod (Laurikkala, 2001) that was based on nearest neighbour searching todetermine which rejected beat candidates were the best to drop out from thatlarger class in order to balance the class sizes and, most of all, to use cleaningin order to improve classiﬁcation.
According to (Laurikkala, 2001) the class of the rejected nystagmic beats wasreduced by leaving out such rejected beat candidates that were found on the basisof two consecutively executed rules.3.2.1. Neighbourhood cleaning procedure
(1) Iterate every nystagmic eye movement candidate cone by one as follows. (2) Computek= 3 nearest neighbours ofcalong with the Euclidean distancethroughoutnbeats of the whole data set.(3) If candidatecwas from the class of the rejected beats, rule (3.1.) is tested.Otherwise, it was from the class of the accepted nystagmic beats and then rule(3.2.) is tested.
(3.1.) If majority, in this case either 2 or 3 nearest neighbours were from the
opposite class, accepted beats, the current candidate cis marked to be removed from the data set.(3.2.) Correspondingly, if the majority ofknearest neighbours were from theopposite class, rejected beats, these 2 or 3 rejected beats are marked to beremoved from the data set.(4) All those selected to be removed are left out from the data set.
The cleaning procedure acts locally, not globally as the previous that was con-structed on the basis of the class centres. Thus, the neighbourhood cleaning meth-od of (Laurikkala, 2001) is not hampered by a complicated distribution of more orless overlapping, mixing classes or one class surrounding the other. Since thecleaning procedure processes data cases on the basis of their nearest neighbour-hood approach, it functions locally, independent of the ‘‘global’’ properties ofdata. It also cleans not only according to the majority class of the rejected beats,but also by means of the minority class. Nevertheless, in our nystagmus datacleaning is only directed to the majority class. This is very natural particularlyClassiﬁcation of medical data set 59for the current data set, because the majority class represents rejected nystagmiceye movement beats and the aim was also to enhance dissimilarity between theclasses of the accepted and rejected beats in order to improve classiﬁcation results.
Note that the present cleaning procedure when directed to the majority classdoes not necessarily balance much the class sizes, but the number of removeditems depends on the data. On the other hand, since small k= 3 was used, the two cleaning rules of the method explore tiny areas from the distribution spaceat a time and may, therefore, mark relatively many items to be removed, becausethe majority condition may be frequently satisﬁed.
3.3. Artiﬁcial extension of training data
Not pure cleaning is the only useful data reﬁnement technique. Sometimes the useof artiﬁcial extension of data cases may also be useful, especially for balancing aclass distribution (Swingler, 1996; Autio et al., 2007). In our data, class balancingwas not necessary, because there were only two classes with frequent nystagmicbeat candidates. Nevertheless, we experimented with the artiﬁcial extension byextending the same number of artiﬁcial nystagmic beats in the class of the rejectedas was cleaned out. Such artiﬁcial cases should naturally resemble very much thoseof the same class. A simple way is just to copy existing cases. However, this is notperhaps fully suitable for all classiﬁcation methods that may take advantage of theproperty that all cases are more or less different, i.e., multiple cases do not bringnew information for training a model. Therefore, we created artiﬁcial rejected nys-tagmic beat candidates by calculating means of pairs of two successive rejectedbeats in the data set. (After cleaning, more nystagmic beat candidates still remainedthan were discarded from the class of the rejected beats.) We may assume that sincetwo successive rejected beats are temporarily quite close to each other in a time-dependent signal, their nystagmus variable values may resemble each other. Con-sequently, computing the means of all their variable value pairs we may assume thatsuch an average, artiﬁcial case would also be in the class of the rejected beats if itwere real. Of course, several, more complicated, but perhaps more productiveextension ways could be designed. We used this simple way because we assumedwhatsoever that artiﬁcial extension could not improve results essentially since clas-siﬁcation results obtained after neighbourhood cleaning were already very high.
It is important to know that the artiﬁcial cases were only used to extend trainingsets. It would not be sensible to include them in test sets.
4. Results
4.1. Classiﬁcation procedure
As mentioned, the data set was computed according to three selections ( Juhola et al., 2013): manual selection to accepted or rejected nystagmic beats, automatic60 M. Juhola et al.selection to the preceding two classes and both selections jointly. In addition, theneighbourhood cleaning procedure was run after each of the three selections.
InFig. 5, there is a diagram how classiﬁcations were performed. To clarify thefollowing descriptions of tests made, there is a round-up for the classiﬁcation runsinFig. 6.
We tested with nearest neighbour searching, Naı¨ve Bayes rule, and linear, qua-dratic and logistic discriminant analysis, and support vector machines with differ-ent kernel functions. Results are presented in percents as classiﬁcation accuracies,i.e., ratios of true positive added with true negative rates to the number of all testcases. Since the leave-one-out method was applied to tests, the number of testedcases was equal to all nystagmic beat candidates, 5989, or less when the data werecleaned.
Nearest neighbour searching and logistic discriminant analysis gave better clas-siﬁcation results when the data were ﬁrst standardized by subtracting the mean ofeach variable and then by dividing with its standard deviation. For those otherclassiﬁcation methods, standardization (being not useful) was not used. Withoutstandardization, the cleaning procedure removed 1313, 1680 and 1364 nystagmicbeat candidates from the class of the rejected, respectively, after the three selec-tions. With standardization, these were only 998, 717 and 732. Consequently,the class of the rejected beats remained as the majority class for all other situationsthan that without standardization for the automatic selection.
Data of n nystagmic beats with two classes: the accepted and rejected beats (the latter as the majority class)
Build a model with a selected algorithm according to leave-one-out: use n-1 beats for training and 1 beat for a classification test 
Store the result of a single test 
Compute classification accuracyrepeat
ntimesClean the data of the rejected on the basis of neighbourhood cleaning or other method
Fig. 5The diagram of the classiﬁcation procedure.Classiﬁcation of medical data set 614.2. Classiﬁcation with nearest neighbour searching
InTable 2, the results produced withknearest neighbour searching are shown.The nearest neighbour number ofkequal to 15 was used here since earlier it gaveus at least 1% better results than odd values of less than 10 or greater than 20(Juhola et al., 2013). To experimentally study the inﬂuence of the neighbourhoodcleaning procedure (Laurikkala, 2001) described above, we also ran cleaning by(1) Classification: Nearest neighbour searching with k=15 (results in Table 2) no cleaning random cleaning (1/4 out of the rejected)randomcleaning (1/2 out of the rejected)neighbourhoodcleaning (k=3)neighbourhoodcleaning (k=3) followed by artificialextension
(2) Classification: Linear (LD), quadratic (QD) and logistic discriminant (LogD) analysis, and naïve Bayes (NB) rule without or with neighbourhood cleaning withk=3 (results in Table 3) LD, no cleaningLD,cleaningQD, no cleaningQD,cleaningLogD,nocleaningLogD,cleaningNB, no cleaningNB,cleaning
(3) Classification: Support vector machines with linear (L), quadratic (Q) and radial basis function (RBF) kernels without or with neighbourhood cleaning withk=3 (results in Table 4) L, no cleaningL, cleaning Q, no cleaningQ,cleaningRBF, no cleaningRBF,cleaning
(4) Classification: Nearest neighbour searching ( k=15) after running neighbourhood cleaning procedure with different k values (results in Table 5) k equal to 5 7 9 11 13 
Fig. 6The round-up of all classiﬁcation alternatives accomplished.62 M. Juhola et al.leaving out a half or quarter from the class of the rejected beats as systematicallydropping out every two or four beat candidates from those of the rejected. Let uscall these ‘random cleaning’ because no assessment criterion for cleaning was em-ployed. The last row inTable 2includes the results when, after neighbourhoodcleaning, artiﬁcial beats as many as cleaned beats from the rejected were insertedinto the class of the rejected.
According toTable 2, we see that it is quite useless to run random cleaning, atleast for this complicated data distribution, since the results were not better thanthose original of the uppermost row. (Random cleaning was made only to showthat more efﬁcient cleaning is necessary.) When strong cleaning that left the halfout of the rejected was run, we even obtained partially poorer results. Of course,we also have to notice the different class sizes here that affect a priori probabilitiesin classiﬁcation. Doubtless did neighbourhood cleaning affect positively byincreasing classiﬁcation accuracies by 5–6% for all three selection ways of nystag-mic beat candidates. Instead, the extension of artiﬁcial, rejected beats after neigh-bourhood cleaning had only a minor effect, less than 0.5%. In principle, it mightsometimes even impair results because there is no guarantee that such extensionwould always ‘‘improve the quality of data’’. Therefore, it is not reasonable to ap-ply for tests of the subsequent tables.
4.3. Classiﬁcation with discriminant analysis and Naı ¨ve Bayes rule
InTable 3there are results of linear, quadratic and logistic discriminant analysisand Naı¨ve Bayes rule. The different sizes of the classes of the rejected beats for thedifferent classiﬁcation methods came from whether the data standardization wasused or was not used. Again neighbourhood cleaning was effective improvingaccuracies by 4–10% compared to the results of not cleaned situations.Table 2Accuracy results ofk= 15 nearest neighbour searching after data standardization and differentcleaning procedures when the sizes of the classes of the accepted nystagmic beats were 2171, 2517 and 1645 forthe three methods of the nystagmic beat selection. The accuracies of the best alternatives were marked in bold.Cleaning procedure Selection method of acceptance or rejection Manual Automatic Manual and automatic No cleaning Class size of the rejected 3818 3472 4344 Accuracy% 84.2 89.3 88.4 Random cleaning (quarter) Class size of the rejected 2864 2604 3258 Accuracy% 84.0 89.1 87.9 Random cleaning (half) Class size of the rejected 1909 1736 2172 Accuracy% 80.0 89.6 86.6 Neighbourhood cleaning Class size of the rejected 2820 2755 3612 Accuracy%91.7 94.9 94.4 Neighbourhood cleaning and extension Class size of the rejected 2820 2755 3612 Accuracy%91.9 95.2 94.7Classiﬁcation of medical data set 634.4. Classiﬁcation with support vector machines
We also experimented with support vector machines the results of which are giveninTable 4. Suitable parameter values were extensively studied as in ( Juhola et al., 2013) and those giving the best average accuracy results were chosen for the threeselection methods of nystagmic beat candidates (three rightmost columns in Table 4): (1) box constraint 8.9 for linear kernel and 0.1 for quadratic kernel, and boxTable 4Accuracy results of support vector machines without and with neighbourhood cleaning when thesizes of the classes of the accepted nystagmic beats were 2171, 2517 and 1645 for the three selection methods.Three kernel functions were used. The accuracies of the best alternatives were marked in bold.Kernel, no cleaning or with it Class size or accuracy Selection method of acceptance or rejection Manual Automatic Manual and automatic Linear kernel, no cleaning Rejected 3818 3472 4344Accuracy% 75.6 83.8 79.3Linear kernel with cleaning Rejected 2505 1792 2980Accuracy% 83.7 89.7 84.0Quadratic kernel, no cleaning Rejected 3818 3472 4344Accuracy% 81.9 88.4 85.5Quadratic kernel with cleaning Rejected 2505 1792 2980Accuracy%88.0 92.6 89.6 RBF kernel, no cleaning Rejected 3818 3472 4344Accuracy% 63.8 57.9 72.5RBF kernel with cleaning Rejected 2505 1792 2980Accuracy%88.7 88.2 88.0Table 3Accuracy results of discriminant analysis (with data standardization for the logistic one) and Naı ¨ve Bayes rule when the sizes of the classes of the accepted nystagmic beats were 2171, 2517 and 1645 for the threeselection methods. The accuracies of the best alternative were marked in bold.Classiﬁcation method Class size or accuracy Selection method of acceptance or rejection Manual Automatic Manual and automatic Linear discr. anal., no cleaning Rejected 3818 3472 4344Accuracy% 75.9 83.7 79.2Linear discr. anal. with cleaning Rejected 2505 1792 2980Accuracy% 83.7 89.6 83.9Quadratic discr. anal., no cleaning Rejected 3818 3472 4344Accuracy% 63.4 75.9 72.4Quadratic discr. anal. with cleaning Rejected 2505 1792 2980Accuracy% 80.3 86.0 79.8Logistic discr. anal., no cleaning Rejected 3818 3472 4344Accuracy% 80.5 87.2 86.9Logistic discr. anal. with cleaning Rejected 2820 2755 3612Accuracy%85.4 92.7 91.3 Naı¨ve Bayes rule, no cleaning Rejected 3818 3472 4344Accuracy% 63.3 76.8 74.3Naı¨ve Bayes rule with cleaning Rejected 2505 1792 2980Accuracy% 70.4 86.1 80.264 M. Juhola et al.constraint 1.2 and sigma 9.9 for radial basis function (RBF) kernel; (2) box con-straint 1.2 for linear kernel and 1.0 for quadratic kernel, and box constraint 8.0and sigma 10.0 for RBF kernel; (3) box constraint 0.2 for linear kernel and 0.1quadratic kernel, and box constraint 0.7 and sigma 10.0 for RBF kernel; Of thethree kernels, the quadratic kernel yielded 0–5% better accuracies than the othertwo. For RBF kernel, cleaning was essential, because without cleaning it lost alltest cases stemming from the class of the accepted nystagmic beats and was even25% worse than with cleaning. With cleaning it was 4–5% worse than the othertwo kernels.
4.5. Classiﬁcation with nearest neighbour searching after neighbourhood cleaningwith different k values
Ultimately, we still studied the use of greaterkvalues than 3 for the neighbour-hood cleaning procedure. Results obtained are given in Table 5. These are only shown for the classiﬁcation set-up where nearest neighbour searching was appliedto classiﬁcation since similar phenomena were also expected for the other classiﬁ-cation methods. Increasing the number up to 11 or 13 of the nearest neighbourssearched for in the cleaning procedure improved classiﬁcation accuracies slightly,by 1–2%. This came from the stronger cleaning, in other words, with greater kval- ues more elements were left out from the class of the rejected beats. Therefore, themore intensive data cleaning may be a reasonable approach if there are abundantelements in the majority class as were here.
It was obvious that increasingkin neighbourhood cleaning did not much im-prove the results obtained compared to those ofkequal to 3, because the latter werealready high, over 90%, although more and more elements of the majority classwere left out. Note two lowest rows inTable 5. According to them, it seemed inthe present data that increasingkover 11 did not improve classiﬁcation accuracies.
Table 5Classiﬁcation accuracy results ofk= 15 nearest neighbour searching after the neighbourhood cleaning procedure when the sizes of the classes of the accepted nystagmic beats were 2171, 2517 and 1645 forthe three methods of the nystagmic beat selection and when in cleaning 5, 7, 9, 11 or 13 nearest neighbourswere utilized to determine elements to be cleaned. The accuracies of the best alternatives were marked in bold.Neighbours in cleaning Selection method of acceptance or rejection Manual Automatic Manual and automatic 5 Class size of the rejected 2607 2633 3469Accuracy% 92.4 95.8 95.07 Class size of the rejected 2455 2536 3365Accuracy% 92.8 96.3 95.39 Class size of the rejected 2325 2467 3288Accuracy% 93.1 96.4 95.611 Class size of the rejected 2184 2407 3196Accuracy%94.1 97.0 96.0 13 Class size of the rejected 2086 2362 3113Accuracy%94.2 96.9 96.0Classiﬁcation of medical data set 655. Discussion and conclusions
In all of our results, neighbourhood cleaning was an efﬁcient way to reﬁne dataand reduced the size of the majority class, rejected nystagmic beats. Simpler clean-ing ways could hardly function as effectively, at least neither that was used earlier(Juhola et al., 2013) nor inTable 2.
Logistic discriminant analysis after cleaning was 2–12% better than the othersinTable 3and 2–6% worse than the accuracies of the nearest neighbour searchingafter neighbourhood cleaning as inTable 2. However, the best support vector ma-chines (with the quadratic kernels) were 2–5% poorer than the nearest neighbouraccuracies after neighbourhood cleaning as inTable 2. This is a slightly surprisingconclusion since our experience among some other data sets has been the opposite,frequently support vector machines have been subtly better than others. We as-sume that the possible reason was now that neighbourhood cleaning favouredthe nearest neighbour searching classiﬁcation. Since the nearest neighbour search-ing was applied in both, cleaning could ‘‘favour’’ its ‘‘relative classiﬁcation meth-od’’ more than the others.
The automatic selection was better in some cases than the manual and auto-matic ones together. Apparently, this stemmed from the fact that manual selectioncriteria may vary a little from time to time. Instead, the automatic selection alwaysfunctions stably.
The use of greater nearest neighbour numbers (5, 7, 9 or 11) than 3 originallyused improved the classiﬁcation results slightly further since more elements werecleaned out from the majority class compared to the situation of 3 nearest neigh-bours. However, no such conclusion could be drawn that this phenomenon wouldtypically be present. After all, the properties of data and their distribution areessential.
A future research detail in using the neighbourhood cleaning method could beto attempt to also clean the minority class. In general this is not perhaps sensible,but for such data sets as ours here where both classes were fairly large, it might beuseful since the classes were ‘‘mutually overlapping’’, some rejected nystagmicbeats among the accepted and vice versa, as seen in Fig. 4. On the other hand, cleaning accepted beats should be made very carefully and ‘‘conservatively’’, notto deteriorate the validity of the data set as a training set for nystagmus analysisviz., the accepted nystagmic beats represent physiologically plausible and possiblenystagmus variable values, whereas the rejected beats are more or less values out-side physiologically possible boundaries. However, these boundaries are not exact,because they may vary between subjects.
We can conclude that neighbourhood cleaning reﬁned data efﬁciently and im-proved accuracies throughout the tests accomplished. Its character is rather localthan global and it can purify noise-like occurrences from data. It had also goodinﬂuence on the current data set with the complicated distribution. To clean datasets with unknown distributions, it is best to ﬁrst explore their data distributions66 M. Juhola et al.– as this is useful in general – before cleaning data and to choose a cleaning proce-dure carefully.
Acknowledgements
The second author is thankful to Maj and Tor Nessling Foundation for thesupport.
References
Abel, L.A., Wang, Z.I., Dell’Osso, L.F., 2008. Wavelet analysis in infantile nystagmus syndrome. Invest.Ophthalmol. Vis. Sci. 49 (8), 3413–3423
.
Augustyniak, P., 1996. Advanced method of nystagmus-phase separation using adaptive modiﬁcation of time-frequency signal representation. In: 3rd International Conference on Signal Processing (ICSP’96), Beijing,China. pp. 351–354.
Autio, L., Juhola, M., Laurikkala, J., 2007. On the neural network classiﬁcation of medical data and anendeavour to balance non-uniform data sets with artiﬁcial data extension. Comp. Biol. Med. 37, 388–397
.
Hertle, R.W., Dell’Osso, L.F., 1999. Clinical and ocular motor analysis of congenital nystagmus in infancy. J.Am. Assoc. Pediatr. Ophthalmol. Strabismus 3 (2), 70–79
.
Hosokawa, M., Hasebe, S., Ohtsuki, H., Tsuchida, Y., 2004. Time-frequency analysis of electronystagmogramsignals in patients with congenital nystagmus. Jpn. J. Ophthalmol. 48, 262–267
.
Juhola, M., 1988. Detection of nystagmus eye movements using a recursive digital ﬁlter. IEEE Trans. Biomed.Eng. 35 (5), 389–394
.
Juhola, M., Aalto, H., Hirvonen, T., 2009. On signal analysis of three-dimensional nystagmus. In: Adlassning,K.-P., Blobel, B., Mantas, J., Masic, I. (Eds.), Proceedings of Medical Informatics in Europe 2009 (MIE2009),Sarajevo, Bosnia and Herzegovina. IOS Press, pp. 846–850
.
Juhola, M., Aalto, H., Jutila, T., Hirvonen, T., 2011. Signal analysis of three-dimensional nystagmus forotoneurological investigations. Ann. Biomed. Eng. 39 (3), 973–982
.
Juhola, M., Aalto, H., Joutsijoki, H., Hirvonen, T.P., 2013. The classiﬁcation of valid and invalid beats of three-dimensional nystagmus eye movement signals using machine learning methods. Adv. Artif. Neural Syst..http://dx.doi.org/10.1155/2013/972412. Article ID 972412.
Laurikkala, J., 2001. Improving identiﬁcation of difﬁcult small classes by balancing class distribution. In:Quaglini, S., Barahona, P., Andreassen, S. (Eds.), . In: Artiﬁcial Intelligence in Medicine: Eight EuropeanConference on Artiﬁcial Intelligence in Medicine in Europe, Lecture Notes in Artiﬁcial Intelligence, vol. 2101.Springer, Berlin, pp. 63–66
.
Swingler, K., 1996. Applying Neural Networks, A Practical Guide. Academic Press, London .
Tominaga, S., Tanaka, T., 2010. 3-Dimensional analysis of nystagmus using video and image processing. In: TheSociety of Instrument and Control Engineers, SICE Ann. Conf., Taipei, Taiwan. pp. 89–91.
Wall III, C., Black, F.O., 1982. Algorithms for the clinical analysis of nystagmus eye movements. IEEE Trans.Biomed. Eng. 28, 638–646
.Classiﬁcation of medical data set 67