Emotion recognition from EEG-based relative power spectral topographyusing convolutional neural network
Md. Asadur Rahmana, Anika Anjumb, Md. Mahmudul Haque Miluc, Farzana Khanamc, Mohammad Shorif Uddin
d,*, Md. Nurunnabi Mollahe
aDepartment of Biomedical Engineering, Military Institute of Science and Technology (MIST), Dhaka, 1216, Bangladesh
bDepartment of Biomedical Engineering, Khulna University of Engineering &Technology (KUET), Khulna, 9203, Bangladesh
cDepartment of Biomedical Engineering, Jashore University of Science and Technology (JUST), Jashore, 7408, Bangladesh
dDepartment of Computer Science and Engineering, Jahangirnagar University (JU), Dhaka, 1342, Bangladesh
eDepartment of Electrical and Electronic Engineering, Khulna University of Engineering &Technology (KUET), Khulna, 9203, Bangladesh
ARTICLE INFO
Keywords:Electroencephalography (EEG)Emotion recognitionRelative power spectral densityConvolutional neural network (CNN)SEED datasetABSTRACT
Emotion recognition, a challenging computational issue, ﬁnds interesting applications in diverse ﬁelds. Usually, feature-based machine-learning methods have been used for emotion recognition. However, these conventionalshallow machine learning methods often ﬁnd unsatisfactory results as there is a tradeoff between feature di- mensions and classiﬁcation accuracy. Besides, extraction and selection of features from the spatial and frequencydomains could be an additional issue. This work proposes a method that transforms EEG (electroencephalog-raphy) signals to topographic images that contain the frequency and spatial information and utilizes a convolu-tional neural network (CNN) to classify the emotion, as CNN has improved feature extraction capability.According to the proposed method, the topographic images are prepared from the relative power spectral densityrather than power spectral density that shows remarkable improvement in classi ﬁcation accuracy. The proposed method is applied to the well-known SEED database and has given outperforming results than the current state-of-the-art.
1. IntroductionEmotion is a complex state of mind that is associated with someone'ssurroundings, thoughts, feelings, and circumstances and it results inphysical and psychological changes. Some researchers have found thatemotion is a cognitive task [1–3]. A human can understand emotionthrough the behavior, mood, and temperament of another person, but itis hard for a machine to understand human emotion unless the humanmakes the machine intelligent enough to decode emotions. This methodfor the machines can be named as human emotion recognition. In thecase of the treatment of patients with expression problems, recognition ofa real emotional state by a machine is helpful for providing bettermedical treatment. There are some psychophysiological studies [ 4–6] where they found a strong correlation between emotion recognition andbrain activities.In recent years, EEG modality has gained much attention formeasuring brain activity in a precise manner along with setting acommunicational pathway between humans and machines. Amongvarious modalities, this method is very much promising because of itshigh accuracy and objective evaluation comparing with other externalappearances like facial expression and gesture [ 7]. But, the raw EEG signal is contaminated with artifacts and this signal is complex due to itsvariation with time and space. The feature extraction and classi ﬁcation of EEG signal have two important aspects which must be ensured inemotional state recognition. Speciﬁed domain knowledge is required forconventional manual feature extraction and feature selection. However,the cost of conventional feature selection increases at a quadratic ratewith the increase of the number of features [ 8]. In most of the studies for feature extraction, researchers have only focused on time [ 9–14], fre- quency [15–17], or time-frequency [18,19] domains of the EEG signal, and rarely focused on the spatial dimension. A multichannel EEG systemacquires data from the different spatial locations of the human brain. So,the motivation of this work is to combine time, frequency, and spatialdomains to classify the emotional state from the multichannel EEG
* Corresponding author.E-mail addresses:bmeasadur@gmail.com(Md.A. Rahman),anikaanjum123@gmail.com(A. Anjum),mahmudhmilu@gmail.com(Md.M.H. Milu),farzanabme@ just.edu.bd(F. Khanam),shorifuddin@gmail.com,shorifuddin@juniv.edu(M.S. Uddin),nurunnabim12@gmail.com(Md.N. Mollah).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100072Received 24 October 2020; Received in revised form 7 May 2021; Accepted 31 May 2021Available online 10 June 20212590-0056/©2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 11 (2021) 100072signals with high accuracy.Several studies have considered body movement, voice tone or facialexpressions, and physiological activities to recognize emotions [ 20–23]. These physical changes had some common complications as they did notshow reliable emotion. Our state of mind could only be a reliable way tounderstand the emotional state that could have been revealed by the brainsignal like EEG. Some research works [ 24–26] are accomplished so far to construct machine learning-based predictive models to classify theemotional states from the EEG signals. A study led by Wang et al. [ 24] reported the alpha, beta, and gamma bands as features to classify fouremotions (joy, relax, sad, and fear) and found 66.5% (in average) accuracyby support vector machine (SVM) using different kinds of emotional videoas stimuli. Another study done by Lin et al. [ 25] achieved 82.29% accuracy using the same classiﬁer. Their experiment included 26 subjects and used30 s long music samples as stimuli. To achieve a highly accurate predictivemodel, recent studies prefer the deep neural network as a feature extractorand classiﬁer. Zheng et al. [26] extracted density entropy of the multi-channel EEG signals using a short-time Fourier transform and used deepbelief network (DBN) as a classiﬁer to classify positive, negative, andneutral emotions and obtained 86.65% accuracy. A recent study [ 27]h a s proposed to construct topographic images from the multichannel EEGsignals to feed them into the deep neural network which proves to achieveah i g hc l a s s iﬁcation accuracy than the current state of the arts. The methoddescribes in Ref. [27] used power spectral density (PSD) to form thetopographic image which is slightly questionable to attain high-level ac-curacy, as the value of the PSD can vary with the individuals, time, andsystem. Therefore, using relative PSD (RPSD) is more technically sound toensure the power level variation measurement of the concerned band withrespect to the full band power in case of EEG signals [ 27,28]. Therefore, feature extraction, feature selection, and setting of the proper classi ﬁer are the most important issues to construct a machine learning-based predictivemodel for emotional state classiﬁcation.To overcome these issues, the present work proposes a mechanism tocombine the time, frequency, and spatial domain of the multichannelEEG emotional signals into topographic images. The topographic imagerepresents the RPSD of each trial of an emotional state that needs time,frequency, and spatial information. There may arise an argument forusing the RPSD instead of PSD. Although a beautiful explanation is givenin Refs. [27,28], a graphical realization is presented in the Results andDiscussions Section to explain the effect of PSD and RPSD through powerlevel variation. This graphical presentation clari ﬁes how RPSD improves the feature quality of the EEG signal power distribution.Many recent research works [29–31] recommend that the convolu- tional neural network (CNN) method is very effective in feature extrac-tion and classiﬁcation tasks. For this, utilizing the automated featureextraction and classiﬁcation facilities of the CNN, the topographicalimages are classiﬁed. The main contributions of this work are as follows:/C15A method is proposed that utilizes the RPSD of each spatial position ofEEG data on the brain (channel) to prepare the topographic images./C15A CNN has been applied for the feature extraction and emotion
classiﬁcation from the topographic images./C15We compared our work with the other recent works on emotionrecognition using the SEED dataset.The article is organized as follows: Section 2describes the data collection, representation, and the proposed methodology, Section 3 presents the results of this research work and compares its ﬁndings with theﬁndings of the recent works,ﬁnally, Section4concludes the paper with future research directions.2. Materials and methods2.1. Dataset descriptionThe proposed methodology of this work has been conducted on theSEED dataset [32]. In the data acquisition, there areﬁfteen subjects, among them eight are females and seven are males (Age ¼23.27/C62.37 years). All participants are right-handed native Chinese students ofShanghai Jiao Tong University. They are reported as normal or correctedto normal vision and normal hearing.Various stimuli can be utilized in emotion-related research such asmovie clips, music, and verbal command, etc. Among them, the movieclip has greater efﬁciency and reliability [33] as it contains both audio and video. In this experiment, Chinese movie clips were selected since allthe participants were native Chinese. The videos are of 4 min long andcan be categorized as: positive, negative, and neutral. The following se-lection criteria were maintained-/C15The total time of the experiment should be small otherwise subjectsmight become fatigued./C15The movie clips should be well understood without clari ﬁcation. /C15The clips should stimuli a single target emotion. It should not containmixed emotions.Eachﬁlm is edited to create coherent emotion-eliciting and maximizeemotional meanings. The details of theﬁlm clips used in the experiments are given inTable 1.Each subject experienced three experiments: positive, negative, andneutral. So, 15 subjects experienced 45 experiments in total. There is atotal of 15 trials for each experiment. Each trial is started with 5 s hint ofstarting. Then,ﬁlm clips are played for 4 min and 45 sec is allotted aftereach clip for feedback. Within this period, participants are asked to reporttheir emotional reactions toward the shown video clips. The order ofpresentation is arranged in such a way that two clips targeting the sameemotion are not shown consecutively. The detailed protocol is given inFig. 1.2.2. Data acquisition and processingThe experiments were performed in three sessions for each subject.The time interval between each session is one week or longer. Thisprocess ensured stable patterns of neural activities across sessions andindividuals. Facial videos and EEG data are recorded simultaneously.Subjects are seated in front of a big screen where movie clips are shown.EEG data is recorded using an ESI NeuroScan System at a sampling rate of1000 Hz from a 62-channel active AgCl electrode cap following the In-ternational 10–20 System [34]. The SEED dataset contains a down-sampled, preprocessed, and segmented version of EEG data. It isdownsampled to 200 Hz. A bandpassﬁlter of 0–75 Hz is applied to remove any artifacts. There are 45 matﬁles in total, one per experiment. Each subjectﬁle contains 15 arrays. In this work, we have used aband-passﬁlter (3–30 Hz) to remove unnecessary frequencies. In the EEGsignal, up to 3 Hz represents the Delta Band, which is present in deepsleep or coma. As subjects were awake and alert during trials, Delta Bandis rejected in our processing. Frequencies higher than 30 Hz are alsoremoved as it does not represent any brain activity regarding emotionaleffects.2.3. Base construction of the EEG topographyAn illustration of 62-electrode placement regarding the SEED datasetis given inFig. 2. It can be generalized through a matrix of m/C2n. Here,m andnrepresent the maximum-point number in horizontal test points andvertical test points, respectively. In this work, m/C2nequals to 9/C28 matrix. The red circled points inFig. 3are the EEG electrodes of the SEED dataset. These points indicate the value of relative PSD of the EEG signalsof corresponding electrodes. The gray points are added to form a com-plete 9/C28 matrix. These points are the interpolation of the surroundingred points. The method of EEG topography construction is shown step-wise inFig. 4. A pseudo-code for the topographic image formationfrom the multichannel EEG signals is added in Appendix A.Md.A. Rahman et al. Array 11 (2021) 100072
2The construction of some new points using speci ﬁc neighborhood discrete data points is found by interpolation. To make 72 points of the 9/C28 matrix, 10 additional points are calculated through interpolating theneighborhood points. The interpolation process of gray points can becalculated as:Γðm;nÞ¼
Γ0ðmþ1;nÞþΓ0ðm/C01;nÞþΓ0ðm;nþ1ÞþΓ0ðm;n/C01ÞK; ð0/C20m;n/C208;m;m2NÞ (1)where,Γ(m,n) presents the required value of the gray points, Γ
’(m,n) are the values of the point surrounding Γ(m,n). Here,Kis the number of non-zero elements in the numerator.2.4 RPSD calculation and normalizationPSD of a signal means the distribution of power over its frequencycomponents that represents the impact of the frequency componentsincluded in the signal. Let,Pis the average power of a signalx(t), then the power for the total time periodTis,P¼lim
T→∞1TZT
0
jxðtÞj2dt (2)The frequency content of the signal xðtÞisx_ðωÞwhich is calculated by Fourier transformation. Then, the PSD can be calculated as follows [ 35, 36]:S
xxðωÞ¼lim
T→∞E/C2jbxðωÞj2/C3 (3)The ratio of the PSD of the band of interest ( PSD
BOI) and the PSD of the total frequency band (PSD
total) is called RPSD. The value of the PSDvaries from person to person and in the case of the same individual, itvaries from time to time. Thus PSD is not a reliable source of informationirrespective of person and time [27,28]. RPSD solves this problem by comparing the PSD of the concerned band with respect to the PSD valueof the total frequency range of the signal [ 27,28]. Therefore, RPSD can be presented as (4).RPSD¼
PSD BOI
PSD total(4)To reduce inter-participant variability, the RPSD of each subject canbe normalized by scaling between 0 and 1 [ 34] as given in(5).
π0¼π/C0πmin
πmax/C0πmin(5)Where
π0is the normalized value of the feature; πmax,πminare the maximum and minimum value of the subject features, respectively. Using
Fig. 1.Each subject faced 45 trials in total. No consecutive trial was targeted at the same emotion.Table 1Exemplary movie clips for the positive, negative, and neutral emotional stimulation used in the experiment.
Serial no Emotion label Film clips' sources01 Negative Tangshan Earthquake02 Negative Back to 1942 (War movie)03 Positive Lost in Thailand04 Positive Flirting Scholar05 Positive Just Another Pandora's Box06 Neutral World Heritage in ChinaExamples of the movie clips of Positive, Negative, and Neutral emotionsPositive Clips Negative Clips Neutral Clips
Flirting Scholar
 Back to 1942
 World Heritage in China
Lost in Thailand
 Tangshan Earthquake
 World Heritage in ChinaMd.A. Rahman et al. Array 11 (2021) 100072
3Fig. 3.Feature matrix of EEG is made considering RPSD as a feature. The shown gray points are interpolated from the surrounding red points. Ten new points areinterpolated during this topographic image construction.
Fig. 2.The positions of the 62 electrodes of the data acquiring device according to the international 10 –20 method.Md.A. Rahman et al. Array 11 (2021) 100072
4normalized RPSD values of 72 positions (as shown in Fig. 3) the EEG topographic images were constructed.2.5. Construction of CNN-based classiﬁerFor automatic feature extraction and classiﬁcation CNN is used in thiswork. CNN is a type of machine learning system in which a model learnsautomatically to classify objects from images, numerical values, orvideos. It is highly capable of learning from the input data by optimizingthe weight parameters of eachﬁlter by minimizing the classiﬁcation error. CNN consists of an input layer and an output layer, along withmultiple hidden layers. It takes an image as an input, then processes it
Fig. 4.Flow diagram of the procedure to construct the topographic image from multichannel EEG.Md.A. Rahman et al. Array 11 (2021) 100072
5through multiple hidden layers. It gives the output as a probable classname. The hidden layers of CNN typically consist of convolutional layers,ReLU layers, pooling layers, a fully connected layer, a batch normaliza-tion layer [37]. A generalized pattern of the layers in a CNN is presentedinFig. 5. The design details of the different layers regarding this work arediscussed brieﬂy in the following.In this work, topographic images are considered as the input of theCNN. A color image can be represented as m/C2n/C2cwheremis the width andnis the height of the image andcis the number of channels e.g. RGB image hasc¼3. Here, the input image size is 192 /C2192/C23. The convolutional layer works for feature extraction. It is considered the corebuilding block of the CNN architecture [ 38]. Convolutional layers
Fig. 7.Comparison between the raw EEG data and the ﬁltered EEG signal. EEG signal isﬁltered with a band-passﬁlter of band 8–32 Hz.
Fig. 6.The values of the different parameters of the layers in the proposed CNN along with the clari ﬁcation of the regarding layers.
Fig. 5.Different layers of a convolutional neural network consisting of an image as an input layer, different ﬁlters in convolution, ReLU and pooling layer, and ﬁnally output layer with theﬁnal score.Md.A. Rahman et al. Array 11 (2021) 100072
6Fig. 9.Comparison among topographic images of (a) negative, (b) neutral, and (c) positive emotions. The center part of the brain which contains a limbic syst em is always active during these emotions as this part is responsible for creating the emotions.
Fig. 8.An example of an EEG topographic image. AF7, AF5, AF6, AF8, PO9, PO10, CB5, CB3, CB4, and CB6 are the interpolated points from surrounding points.Md.A. Rahman et al. Array 11 (2021) 100072
7transform the input data by using a patch of locally connecting neuronsfrom the previous layer. This convolutional layer convolves the 192 /C2 192/C23 image by moving aﬁlter along with the vertical and horizontalinput image. While creating a convolutional two-dimensional layer, theﬁlter size of the input argument is speciﬁed as 8/C28. Stride is known as the step size with which theﬁlter moves. For 8/C28ﬁlter scanning through the input image, the stride of 1 is used in this research.In dilated convolution, theﬁlters are expanded by inserting spacesbetween the elements without increasing the number of parameters orcomputation. We apply ([3,3,4,4]) zero padding to input image bordersto add zero values vertically and horizontally. The output size of thisdesigned convolutional layer can be equated by:Output Size¼
1Sþ1ðIS/C0ð ðFS/C01Þ*DFþ1Þþ2PÞ(6)Here,IS¼Input size of image;S¼Stride;F S¼Filter size;D F¼ Dilation factor;P¼Padding.So, the mathematical formula of the two-dimensional convolutional layer is:G½m;n/C138¼X
∞i¼/C0∞X∞j¼/C0∞h½i;j/C138:g½m/C0i;n/C0j/C138(7)Here,grepresents the input image matrix to be convolved with thekernel matrixhto result in a new matrixG. In our proposed CNN structure, the input channel is normalized by abatch normalization layer. It is generally used between the convolutionallayer and the ReLU layer to speed up the training process of CNN andreduce the sensitivity to network initialization. A batch normalizationlayer normalizes its inputs by calculating the mean and variance over amini-batch and each input channel. Then it calculates the normalizedactivations as [39],bpi¼pi/C0μBﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2Bþεp (8)Here,p
i¼inputs; μB¼mean; σ2B¼variance; εimproves numerical stability when the mini-batch variance is very small. The recti ﬁed linear unit or ReLU was used as an element-wise activation function over theinput data thresholding. Running ReLU over the input volume changesthe pixel values but does not change the spatial dimension of the inputdata in the output. ReLU is more preferred than other functions because itmakes the neural network faster [39].The details of these layers in our proposed CNN model are shown inFig. 6. The fully connected layer is used to compute class scores that can
Fig. 11.(a) Accuracy and (b) loss of the training and validation of the proposed CNN structure with respect to iterations.
Fig. 10.RPSD- and PSD-based topographs presenting the variation of EEG signals' spatial power level distribution for positive, negative, and neutral emoti ons. These are the corresponding results of Subject 3.Md.A. Rahman et al. Array 11 (2021) 100072
8be used as the output of the network. The dimension of the output vol-ume is 1/C21/C2N, whereNis the number of output classes (here thenumber of classesN¼3).3. Results and discussionsAll steps of the processing, image formation, feature extraction, anddata classiﬁcation were performed in Matlab 2018a. Data utilized in thisresearch work has been collected from the SEED dataset. Some pre-processing was already applied to the dataset e.g. data were down-sampled to 200 Hz, bandpassﬁlter was applied from 0 to 75 Hz to removeartifacts. Despite this preprocessing, some other processing were alsodone. A bandpassﬁlter was applied from 8 to 32 Hz to remove noise. Agraphical representation of raw and bandpass ﬁltered EEG signals is shown inFig. 7.Fig. 8shows one sample of EEG topographic image, where 10 newdata points are interpolated from the neighbor points: AF7, AF5, AF6,AF8, PO9, PO10, CB5, CB3, CB4, and CB6. The RPSD of each electrode iscalculated and normalized within the range of 0 –1. Each RPSD value is mapped in a two-dimensional plot according to the locations of eachelectrode. These locations are given speciﬁc names. A color bar is also shown inFig. 8to represent the activity level of the different positions ofour brain during different emotions.Different emotions create different effects on the human brain. A sadmovie can make someone emotionally unstable while a natural scene cancreate a soothing effect on our brain. The state of an emotional conditionchanges the EEG result taken during different emotions. A comparisonamong topographic images of negative, neutral, and positive emotions isshown inFig. 9. One thing to notice in those images is that the centralpart of the brain is always active in all three emotions. The central part ofthe brain or medial temporal lobe is consists of the limbic system [ 40]. It has been said that the limbic system is responsible for emotion formation[41]. Thus, despite the emotion type, the limbic system is always active.Another important issue is to use the RPSD instead of PSD for makingthe topographical images. It is a hypothesis that power level distribution-based images are better in the case of RPSD than that of the PSD. Basedon this argument, we have presented PSD- and RPSD-based topograph-ical images inFig. 10. It is clearly observable that power level distribu-tion is more precise in the case of RPSD than that of the PSD.The topographic images are given as input to the CNN for featureextraction and emotion classiﬁcation. To feed the CNN, there were threetypes of EEG data corresponding to three emotional states of the brain.Since each participant took part in 3 different emotional states (positive,negative, and neutral) and there are 15 trials for an individual state. So,there are 15/C23¼45 multichannel EEG data from each participant.The accuracy and loss of the proposed CNN structure with respect toiterations are presented inFig. 11(a) andFig. 11(b), respectively. Here, the maximum epoch is considered 15. We have selected the ﬁlter size of the convolutional layer based on the trial and error method. Each result ischecked using the Matlab simulator several times to select the bestpattern. The convolutional layer, batch normalization layer, ReLU layer,pooling layer are used twice in this work. The ﬁnal results are the clas- siﬁ
cation accuracies against the three classes: positive, negative, andneutral emotional states. Therefore, the fully connected layer produces 3output classes.We have done experimentation with the proposed CNN in two sce-narios. In theﬁrst scenario, 25% of data are used for training, and the rest75% are used for testing. Similarly, in the second scenario, 50% of dataare used for training, and the rest 50% are used for testing. The trainingand testing data were chosen randomlyﬁve times for each participant and the resultant classiﬁcation accuracy is tabulated from the averagevalue of the correspondingﬁve classiﬁcation accuracy of each partici- pant. The classiﬁcation accuracy of the proposed method is presented inTable 2. We found that the average classiﬁcation accuracy achieved by our proposed work is 89% for theﬁrst scenario and 94% for the secondscenario. This outcome is the highest classi ﬁcation accuracy with respect to the other recent methods applied to the SEED dataset so far. Thecomparison of the methods and their classi ﬁcation accuracies are shown inTable 3.4. ConclusionIn this research work, human emotion has been detected frommultichannel EEG signals. The multichannel emotional EEG signals weremapped into two-dimensional tomographic images using RPSD. Theseimages combined frequency and spatial domain information of the EEG
Table 3Comparison with other recent methods with SEED dataset.
Author&Study Method Classi ﬁer Average Accuracy (%)W. Zheng et al., 2017 [13] Group Sparse Canonical Correlation Analysis 86.65W. L. Zheng et al., 2017 [14] Differential Entropy as Features Discriminative Graph regularized Extreme Learning Machine 79.28W. L. Zheng et al., 2015 [17] Critical Frequency Band Investigation Deep Belief Network 86.08 /C68.34 Y. M. Jin et al., 2020 [30] Differential Entropy Domain Adaptation Network 79.19Y. Yang, 2018 [42] Differential Entropy as Features Hierarchical Network with Subnetwork Nodes 86.42M. A. Rahman, 2019 et al.[43]PCA and t-statistics-Based Feature SelectionMethod SVM and ANN 85.85 /C65.72 and 86.57/C64.08Proposed Method RPSD-Based Topographic Image for Emotional EEG Data CNN 89.056 /C64.32 (25% data fortraining)94.63/C63.68 (50% data fortraining)
SVM: Support Vector Machine[44,45],ANN: Artiﬁcial Neural Network[44,45].Table 2Classiﬁcation accuracy for 15 subjects using CNN.
Subject Accuracy with 25% data fortraining and 75% data fortesting (%)Accuracy with 50% data fortraining and 50% data fortesting (%)1 82.79 93.482 90.91 1003 84.85 92.484 90.85 95.245 84.70 1006 90.91 90.717 83.82 90.718 84.85 95.249 88.82 10010 93.94 90.4811 92.91 95.2412 85.73 88.6713 90.88 94.4814 96.97 94.4815 92.91 98.24Average/C6StandardDeviation89.056/C64.32 94.63 /C63.68Md.A. Rahman et al. Array 11 (2021) 100072
9signals. For feature extraction and classiﬁcation CNN was used in this work and found a signiﬁcant enhancement in the classiﬁcation accuracy. Compared with the other recent methods that were applied on the SEEDdataset, the proposed method achieved the highest classi ﬁcation accu- racies 89.056%/C64.32 (using 25% data in training and the rest 75% intesting) and 94.63%/C63.68 (using 50% data in training, and rest 50% intesting). From this convincing output, it is expected that the proposedexpert system would work efﬁciently in other types of EEG signal clas-siﬁcation, which will be our next focus.Declaration of competing interestThe authors have no conﬂict of interest regarding this publication.Funding AcknowledgementNo funding organization/institute supported this research work.Appendix APseudo Code:EEG topographic image (used in MATLAB R2018a):data¼load the RPSD values of total estimated channel; % Load your2D EEG dataxc¼[horizontal axis coordinates]; % get the x-axisyc¼[vertical axis coordinates]; % get y-axis pointstrlen¼normalized value of each electrode; %ﬁnd the normalized valuexi¼linspace(min(xc),max(xc),30);yi¼linspace(min(yc),max(yc),30);[XI, YI]¼meshgrid(xi,yi); % Create a mesh with xi and yizc¼griddata(xc,yc,trlen,XI,YI,'natural ’); % relative power of each electrode[cs,hh]¼contourf(XI,YI,ZI,20,'LineStyle','none ’); % contour plot of the dataset(hh,'EdgeColor','none')shading interpset(gca,'Visible','off’); colormap(jet);set(gcf,'PaperUnits','inches','PaperPosition',[0 0 2 2]);print(1,'-dpng’, '.png','-r0
0);Author statementMd. Asadur Rahman: Conceptualization, Methodology, Formal anal-ysis, Software, Investigation, Resources, Draft preparation. Anika Anjum:Methodology, Formal analysis, Software, Investigation, Draft prepara-tion. Md. Mahmudul Haque Milu: Formal analysis, Data curation,Investigation, Draft preparation. Farzana Khanam: Formal analysis, Datacuration, Investigation, Draft preparation. Mohammad Shorif Uddin:Validation, Visualization, Review&Editing. Md. Nurunnabi Mollah: Supervision, Review&EditingFundingNo funding was received for this research.Informed consentThe studies reported in this work did not require any informedconsent.Ethical approvalThis article does not contain any studies with human participants oranimals performed by any of the authors. It used a publicly availabledataset. Proper acknowledgments with citation guidelines are main-tained for the use of this dataset.References[1]Lazarus R. Emotion and adaptation. USA: Oxford University Press; 1991 . [2] Mueller SC. The inﬂuence of emotion on cognitive control: relevance fordevelopment and adolescent psychopathology. Front Psychol 2011;2:327. https:// doi.org/10.3389/fpsyg.2011.00327 . [3] Tyng CM, Amin HU, Saad MNM, Malik AS. The in ﬂuences of emotion on learning and memory. Front Psychol August 2017;8(1454):24. https://doi.org/10.3389/ fpsyg.2017.01454.[4] Sammler D, Grigutsch M, Fritz T, Koelsch S. Music and emotion:electrophysiological correlates of the processing of pleasant and unpleasant music.Psychophysiology 2007;44(2):293 –304.https://doi.org/10.1111/j.1469- 8986.2007.00497.x.[5] Knyazev GG, Slobodskoj-Plusnin JY, Bocharov AV. Gender differences in implicitand explicit processing of emotional facial expressions as revealed by event-relatedtheta synchronization. Emotion 2010;10(5):678 –87.https://doi.org/10.1037/ a0019175.[6] Mathersul D, Williams LM, Hopkinson PJ, Kemp AH. Investigating models of affect:relationships among EEG alpha asymmetry, depression, and anxiety. Emotion 2008;8(4):560–72.https://doi.org/10.1037/a0012811 . [7] Ahern GL, Schwartz GE. Differential lateralization for positive and negative emotionin the human brain: EEG spectral analysis. Neuropsychologia 1985;23(6):745 –55. https://doi.org/10.1016/0028-3932(85)90081-8 . [8] Dash M, Liu H. Feature selection for classi ﬁcation. Intell Data Anal 1997;1(3): 131–56.https://doi.org/10.1016/S1088-467X(97)00008-5 . [9] Murugappan M, Ramachandran N, Sazali Y. Classi ﬁcation of human emotion from EEG using discrete wavelet transform. J Biomed Sci Eng 2010;3:390 –6.https:// doi.org/10.4236/jbise.2010.34054 . [10]Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from EEG using higher-order crossings. IEEE Trans Inf Technol Biomed 2010;14:186 –97. 0.1109/ TITB.2009.2034649.[11] Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from brain signals usinghybrid adaptiveﬁltering and higher-order crossings analysis. IEEE Transaction onAffective Computing 2010;1:81 –97.https://doi.org/10.1109/T-AFFC.2010.7 . [12]Murugappan M, Rizon M, Nagarajan R, Yaacob S. Inferring of human emotionalstates using multichannel EEG. Eur J Sci Res 2010;48(2):281 –99. [13] Zheng W. Multichannel EEG-based emotion recognition via group sparse canonicalcorrelation analysis. IEEE Transactions on Cognitive and Developmental SystemsSept. 2017;9(3):281–90.https://doi.org/10.1109/TCDS.2016.2587290 . [14] Zheng WL, Zhu JY, Lu BL. “Identifying stable patterns over time for emotionrecognition from EEG. IEEE Transaction on Affective Computing 2017;1. https:// doi.org/10.1109/TAFFC.2017.2712143 . [15] Thammasan N, Moriyama K, Fukui K, Numao M. Continuous music-emotionrecognition based on electroencephalogram99. ”
IEICE Transaction on Information System; 2016. p. 1234–41.https://doi.org/10.1587/transinf.2015EDP7251 . [16] Jirayucharoensak S, Pan-Ngum S, Israsena P. ““EEG-based emotion recognition using deep learning network with principal component based covariate shiftadaptation. Sci World J 2014:1 –10.https://doi.org/10.1155/2014/627892 . 2014, 627892.[17] Zheng WL, Lu BL. Investigating critical frequency bands and channels for EEG-basedemotion recognition with deep neural networks. IEEE Transactions on AutonomousMental Development 2015;7(3):162 –75.https://doi.org/10.1109/TAMD.2015 . [18] Yin Z, Wang Y, Liu L, Zhang W, Zhang J. Cross-subject EEG feature selection foremotion recognition using transfer recursive feature elimination. Front Neurorob2017;11:19.https://doi.org/10.3389/fnbot.2017.00019 . [19] Li X, Qi XY, Sun XQ, Xie JL, Fan MD, Kang JN. An improved multi-scale entropyalgorithm in emotion EEG features extraction. Journal of Medical Imaging &Health Informatics 2017;7:436 –9.https://doi.org/10.3772/j.issn.1002-0470.2015.10-11.001.[20] Schirmer A, Adolphs R. Emotion perception from face, voice, and touch:comparisons and convergence. Trends Cognit Sci 2017;21(3):216 –28.https:// doi.org/10.1016/j.tics.2017.01.001 . [21] Rigoulot S, Pell MD. Seeing emotion with your ears: emotional prosody implicitlyguides visual attention to faces. PloS One 2012;7(1):1 –11.https://doi.org/ 10.1371/journal.pone.0030740 . [22] Banziger T, Grandjean D, Scherer KR. Emotion recognition from expressions in face,voice, and body: the multimodal emotion recognition test (MERT). Emotion 2009;9(5):691–704.https://doi.org/10.1037/a0017088 . [23] Stathopoulou IO, Tsihrintzis GA. “Emotion recognition from body movements and gestures,”Intelligent Interactive Multimedia Systems and Services. July 2011.p. 295–303.https://doi.org/10.1007/978-3-642-22158-3_29 . [24] Wang XW, Nie D, Lu BL. EEG based emotion recognition using frequency domainfeatures and support vector machines. Neural Information Processing 2011;7062:734–43.https://doi.org/10.1007/978-3-642-24955-6_87 . [25] Lin YP, Wang CH, Jung TP. EEG-based emotion recognition in music listening. IEEE(Inst Electr Electron Eng) Trans Biomed Eng 2010;57(7):1798 –806.https:// doi.org/10.1109/TBME.2010.2048568 . [26] Li Y, Huang J, Zhou H, Zhong N. Human emotion recognition withelectroencephalographic multidimensional features by hybrid deep neuralnetworks. Appl Sci 2017;7(10). https://doi.org/10.3390/app7101060 . [27] Rahman MA, Rashid MMO, Khanam F, Alam MK, Ahmad M. EEG based brainalertness monitoring by statistical and arti ﬁcial neural network approach. Int J AdvMd.A. Rahman et al. Array 11 (2021) 100072
10Comput Sci Appl January 2019;10(1). https://doi.org/10.14569/ IJACSA.2019.0100157.[28] Khanam F, Rahman MA, Ahmad M. Evaluating alpha relative power of EEG signalduring psychophysiological activities in Salat. In: International conference oninnovations in science, engineering and Technology 2018 (ICISET). Bangladesh:International Islamic University Chittagong (IIUC); 2018. p. 1 –6.https://doi.org/ 10.1109/ICISET.2018.8745614 . 27-28 October. [29] Mahmud M, Kaiser MS, Hussain A, Vassanelli S. Applications of deep learning andreinforcement learning to biological data. in IEEE Transactions on Neural Networksand Learning Systems June 2018;29(6):2063 –79.https://doi.org/10.1109/ TNNLS.2018.2790388.[30] Noor MBT, Zenia NZ, Kaiser MS, Mamun SA, Mahmud M. “Application of deep learning in detecting neurological disorders from magnetic resonance images: asurvey on the detection of Alzheimer's disease, Parkinson's disease andschizophrenia. Brain Informatics 2020;7(11). https://doi.org/10.1186/s40708- 020-00112-2.[31] Rahman MA, Uddin MS, Ahmad M. Modeling and classi ﬁcation of voluntary and imagery movements for brain-computer interface from fNIR and EEG signalsthrough convolutional neural network. Health Inf Sci Syst 2019;7(22). https:// doi.org/10.1007/s13755-019-0081-5 . [32] Emotional EEG Dataset. Available in: http://bcmi.sjtu.edu.cn/home/seed/index.ht ml.[33] Jin Y, Luo Y, Zheng W, Lu B. EEG-based emotion recognition using domainadaptation network. International Conference on Orange Technologies. Singapore:ICOT); 2017. p. 222–5.https://doi.org/10.1109/ICOT.2017.8336126 . [34] Homan RW, Herman J, Purdy P. “Cerebral location of international 10 –20 system electrode placement. Electroencephalogr Clin Neurophysiol 1987;66(4):376 –82. https://doi.org/10.1016/0013-4694(87)90206-9 . [35]Rieke F, Bialek W, Warland D. Spikes: Exploring the Neural Code (ComputationalNeuroscience). MIT Press; 1999, ISBN 978-0262681087 . [36]Scott Millers, Childers Donald. Probability and random processes. Academic Press;2012. p. 370–5.[37] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classi ﬁcation with deep convolutional neural networks. Adv Neural Inf Process Syst 2012:1097 –105. https://doi.org/10.1145/3065386 . [38]Cun YL, Bengio Y.“Convolutional networks for images, speech, and time series, ” The handbook of brain theory and neural networks. 1995 . [39] Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adeli H. Deep convolutional neuralnetwork for the automated detection and diagnosis of seizure using EEG signals.Comput Biol Med 2018;100:270 –8.https://doi.org/10.1016/ j.compbiomed.2017.09.017. [40] Morgane PJ, Galler JR, Mokler DJ. A review of systems and networks of the limbicforebrain/limbic midbrain. Prog Neurobiol 2005;75(2):143 –60.https://doi.org/ 10.1016/j.pneurobio.2005.01.001 . [41] Pessoa L.“Emotion and cognition and the amygdala: from
“what is it?”to“what's to be done”. Neuropsychologia 2010;48(12):3416 –29.https://doi.org/10.1016/ j.neuropsychologia.2010.06.038 . [42] Yang Y, Wu QMJ, Zheng W, Lu B. EEG-based emotion recognition using hierarchicalnetwork with subnetwork nodes. IEEE Transactions on Cognitive andDevelopmental Systems June 2018;10(2):408 –19.https://doi.org/10.1109/ TCDS.2017.2685338.[43] Rahman MA, Hossain MF, Hossain M, Ahmmed R. Employing PCA and t-statisticalapproach for feature extraction and classi ﬁcation of emotion from multichannel EEG signal. Egyptian Informatics Journal 2019. https://doi.org/10.1016/ j.eij.2019.10.002.[44] Rahman MA, Khanam F, Ahmad M, Uddin MS. “Multiclass EEG signal classiﬁcation utilizing R/C19enyi min-entropy-based feature selection from wavelet packettransformation. Brain Informatics 2020;7(7). https://doi.org/10.1186/s40708-020- 00108-y.[45] Rahman MA, Rashid MA, Ahmad M, Kuwana A, Kobayashi H. Modeling andclassiﬁcation of voluntary and imagery movements from the prefrontal fNIRSsignals. IEEE Access 2020;8:218215 –33.https://doi.org/10.1109/ ACCESS.2020.3042249.Md.A. Rahman et al. Array 11 (2021) 100072
11