A review: Deep learning for medical image segmentation usingmulti-modality fusion
Tongxue Zhoua,b,*, Su Ruana,S t/C19ephane Canub
aUniversit/C19e de Rouen Normandie, LITIS - QuantIF, Rouen, 76183, France
bINSA de Rouen, LITIS -Apprentissage, Rouen, 76800, France
ARTICLE INFO
Keywords:Deep learningMedical image segmentationMulti-modality fusionReviewABSTRACT
Multi-modality is widely used in medical imaging, because it can provide multiinformation about a target (tumor,organ or tissue). Segmentation using multimodality consists of fusing multi-information to improve the seg-mentation. Recently, deep learning-based approaches have presented the state-of-the-art performance in imageclassiﬁcation, segmentation, object detection and tracking tasks. Due to their self-learning and generalizationability over large amounts of data, deep learning recently has also gained great interest in multi-modal medicalimage segmentation. In this paper, we give an overview of deep learning-based approaches for multi-modalmedical image segmentation task. Firstly, we introduce the general principle of deep learning and multi-modalmedical image segmentation. Secondly, we present different deep learning network architectures, then analyzetheir fusion strategies and compare their results. The earlier fusion is commonly used, since it ’s simple and it focuses on the subsequent segmentation network architecture. However, the later fusion gives more attention onfusion strategy to learn the complex relationship between different modalities. In general, compared to the earlierfusion, the later fusion can give more accurate result if the fusion method is effective enough. We also discusssome common problems in medical image segmentation. Finally, we summarize and provide some perspectives onthe future research.
1. IntroductionSegmentation using multi-modality has been widely studied with thedevelopment of medical image acquisition systems. Different strategiesfor image fusion, such as probability theory [1,2], fuzzy concept[3,4], believe functions[5,6], and machine learning[7–10]have been devel- oped with success. For the methods based on the probability theory andmachine learning, different data modalities have different statisticalproperties which makes it difﬁcult to model them using shallow models.For the methods based on the fuzzy concept, the fuzzy measure quanti ﬁes the degree of membership relative to a decision for each source. Thefusion of several sources is achieved by applying the fuzzy operators tothe fuzzy sets. For the methods based on the belief function theory, eachsource isﬁrst modeled by an evidential mass, the DempsterShafer rule isthen applied to fuse all sources. The main dif ﬁculty to use the belief function theory and the fuzzy set theory relates to the choice of theevidential mass, the fuzzy measure and the fuzzy conjunction function.However, a deep learning-based network can directly encode the map-ping. Therefore, the deep learning-based method has a great potential toproduce better fusion results than conventional methods. Since 2012,several deep convolutional neural network models have been proposedsuch as AlexNet[11], ZFNet[12], VGG[13], GoogleNet[14], Residual Net[15], DenseNet[16], FCN[17]and U-Net[18]. These models have not only provided state-of-the-art performance for image classi ﬁcation, segmentation, object detection and tracking tasks, but also provide a newpoint of view for image fusion. There are mainly four reasons contrib-uting to their success: Firstly, the main reason behind the amazing suc-cess of deep learning over traditional machine learning models is theadvancements in neural networks, it learns high-level features from datain an incremental manner, which eliminates the need of domain expertiseand hard feature extraction. And it solves the problem in an end to endmanner. Secondly, the appearance of GPU and GPU-computing librariesmake the model can be trained 10 to 30 times faster than on CPUs. Andthe open source software packages provide ef ﬁcient GPU implementa- tions. Thirdly, publicly available datasets such as ImageNet, can be usedfor training, which allow researchers to train and test new variants ofdeep learning models. Finally, several available ef ﬁcient optimization techniques also contributes theﬁnal success of deep learning, such as
* Corresponding author.E-mail address:tongxue.zhou@insa-rouen.fr(T. Zhou).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2019.100004Received 20 May 2019; Received in revised form 22 July 2019; Accepted 27 August 2019Available online 31 August 20192590-0056/©2019 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 3-4 (2019) 100004dropout, batch normalization, Adam optimizer and others, ReLU acti-vation function and its variants, with that, we can update the weights andobtain the optimal performance.Motivated by the success of deep learning, researches in medicalimageﬁeld have also attempted to apply deep learning-based approachesto medical image segmentation in the brain [19–21], lung[22], pancreas [23,24], prostate[25]and multi-organ[26,27]. Medical image seg- mentation is an important area in medical image analysis and is neces-sary for diagnosis, monitoring and treatment. The goal is to assign thelabel to each pixel in images, it generally includes two phases, ﬁrstly, detect the unhealthy tissue or areas of interest; secondly, decliner thedifferent anatomical structures or areas of interest. These deeplearning-based methods have achieved superior performance comparedto traditional methods in medical image segmentation task. In order toobtain more accurate segmentation for better diagnosis, usingmulti-modal medical images has been a growing trend strategy. A thor-ough analysis of the literature with the keywords ‘deep learning’, ‘medical image segmentation ’and‘multi modality’on Google Scholar search engine is performed inFig. 1, which is queired on July 17, 2019.We can observe that the number of papers increases every year from2014 to 2018, which means multi-modal medical image segmentation indeep learning are obtaining more and more attention in recent years. Tohave a better understanding of the dimension of this research ﬁeld, we compare the scientiﬁc production of the image segmentation community,the medical image segmentation community, and the medical imagesegmentation using multi-modality fusion with and without deeplearning inFig. 2. From theﬁgure we can see, the amount of papers has adescent or even tendency in the methods without deep learning, butthere is an increase number of papers using deep learning method inevery researchﬁeld. Especially in medial image segmentation ﬁeld, due to the limited datasets, classical methods take still a more dominantposition, but we can see an obvious increasing tendency in the methodsusing deep learning. The principal modalities in medical images analysisare computed tomography (CT), magnetic resonance imaging (MRI) andpositron emission tomography (PET). Compared to single images,multi-modal images help to extract features from different views andbring complementary information, contributing to better data represen-tation and discriminative power of the network. As pointed out inRef.[28], the CT image can diagnose muscle and bone disorders, such asbone tumors and fractures, while the MR image can offer a good softtissue contrast without radiation. Functional images, such as PET, lackanatomical characterization, while can provide quantitative metabolicand functional information about diseases. MRI modality can providecomplementary information due to its dependence on variable acquisi-tion parameters, such as T1-weighted (T1), contrast-enhancedT1-weighted (T1c), T2-weighted (T2) and Fluid attenuation inversionrecovery (Flair) images. T2 and Flair are suitable to detect the tumor withperitumoral edema, while T1 and T1c to detect the tumor core withoutperitumoral edema. Therefore, applying multi-modal images can reducethe information uncertainty and improve clinical diagnosis and seg-mentation accuracy[29]. Several widely used multi-modal medical im-ages are described inFig. 3. The earlier fusion is simple and most worksuse the fusion strategy to do the segmentation, it focuses on the subse-quent complex segmentation network architecture designs, but it doesn ’t
consider the relationship between different modalities and doesn ’t analyze how to fuse the different feature information to improve thesegmentation performance. However, the later fusion pays more atten-tion on the fusion problem, because each modality is employed as aninput of one network which can learn complex and complementaryfeature information of each modality. In general, compared to the earlierfusion, the later fusion can achieve better segmentation performance ifthe fusion method is ffective enough. And the selection of fusion methoddepends on the speciﬁc problem.There are also some other reviews on medical image analysis usingdeep learning. However, they don ’t focus on the fusion strategy. Forexample, Litjens et al.[30]reviewed the major deep learning concepts inmedical image analysis. Bernal et al. [31]gave an overview in deep CNN for brain MRI analysis. In this paper, we focus on fusion methods ofmulti-modal medical images for medical image segmentation.The rest of the paper is structured as followed. In Section 2we introduce the general principle of deep learning and multi-modal medicalimage segmentation. In Section3, we present how to prepare the databefore feeding to the network. In Section 4, we describe the detailed multi-modal segmentation network based on different fusion strategies.In Section5, we discuss some common problems appeared in the ﬁeld. Finally, we summarize and discuss the future perspective in the ﬁeld of multi-modal medical image segmentation.2. Deep learning based methods2.1. Deep learningDeep learning refers to a neural network with multiple layers ofnonlinear processing units[32]. Each successive layer uses the outputfrom the previous layer as input. The network can extract the complexhierarchy features from a large amount of data by using these layers. In
Fig. 1.The tendency of multi-modal medical image segmentation in deep learning.T. Zhou et al. Array 3-4 (2019) 100004
2recent years, deep learning has made signiﬁcant improvements in image classiﬁcation, recognition, object detection and medical image analysis,where they have produced excellent results comparable to or sometimessuperior to human experts. Among the known deep learning algorithms,such as stacked auto-encoders[33], deep Boltzmann machines[34], and convolutional neural networks[35], the most successful one for imagesegmentation is convolutional neural networks (CNN). It was ﬁrst pro- posed in 1989 by LeCun and theﬁrst successful real-world application[36]is the hand-written digit recognition in 1998 by LeCun, where hepresented aﬁve-layer fully-adaptive architecture. Due to its accuracyresults (1% error rate and 9% reject rate from a dataset of 2007 hand-written characters), the neural networks can be applied into a real-worldproblem. However, it did not gather much attention until the contribu-tion of Krizhevsky et al. to the ImageNet challenge in 2012. The proposedAlexNet[11], similar to LeNet but deeper, outperformed all the com-petitors and won the challenge by reducing the top-5 error (the per-centage of test examples for which the correct class was not in the top 5predicted classes) from 26% to 15.3%. In the subsequent years, otherbased on CNN architectures are proposed, including VGGNet [13], GoogleNet[14], Residual Net[15]and DenseNet[16],Table 1describes the details of these network architectures.CNN is a multi-layer neural network containing convolution, pooling,activation and fully connected layers. Convolution layers are the core ofCNNs and are used for feature extraction. The convolution operation canproduce different feature maps depending on the ﬁlters used. Pooling layer performs a downsampling operation by using maximum or averageof the deﬁned neighborhood as the value to reduce the spatial size of eachfeature map. Non-linear rectiﬁed layer (ReLU) and its modiﬁcations such as Leaky ReLU are among the most commonly used activation functions[37], which transforms data by clipping any negative input values to zerowhile positive input values are passed as output. Neurons in a fullyconnected layer are fully connected to all activations in the previouslayer. They are placed before the classiﬁcation output of a CNN and are used toﬂatten the results before a prediction is made using linear clas-siﬁers. While training the CNN architecture, the model predicts the classscores for training images, computes the loss using the selected loss
Fig. 2.The tendency of relative researchﬁeld with/without deep learning.
Fig. 3.The multi-modal medical images, (a) –(c) are the commonly used multi-modal medical images and (d) –(g) are the different sequences of brain MRI.T. Zhou et al. Array 3-4 (2019) 100004
3function andﬁnally updates the weights using the gradient descentmethod by back-propagation. The cross-entropy loss is one of the mostwidely used loss functions and stochastic gradient descent (SGD) is themost popular method to operate gradient descent.2.2. Multi-modal medical image segmentationDue to the variable size, shape and location of target tissue, medicalimage segmentation is one of the most challenging tasks in the ﬁeld of medical image analysis. Despite the variety of proposed segmentationnetwork architectures, it is still hard to compare the performance ofdifferent algorithms, because most of the algorithms are evaluated ondifferent sets of data and reported in different metrics. In order to obtainaccurate segmentation and compare different state-of-the-art methods,some well-known publicly challenges for segmentation are created, suchas Brain tumor Segmentation (BraTS) [21], Ischemic Stroke Lesion Seg- mentation(ISLES),
1MR Brain Image Segmentation (MRBrainS) [38], Neonatal Brain Segmentation (NeoBrainS) [39], Combined (CT-MR) Healthy Abdominal Organ Segmentation (CHAOS),
26-month infant brain MRI Segmentation (Iseg-2017) [40]and Automatic intervertebral disc localization and segmentation from 3D Multi-modality MR (M3)Images (IVDM3Seg).
3Table 2describes the detailed dataset informationmentioned above.Table 3shows the main evaluation metrics in thesedatasets.We describe a pipeline of multi-modal medical image segmentationbased on deep learning, shown inFig. 4. The pipeline consists of four parts: data preparation, network architecture, fusion strategy and datapost-processing. In the data preparation stage, the data dimension isﬁrstly chosen, and the pre-processing is used to reduce the variationbetween images, and data augmentation strategy can also be used toincrease the training data to avoid the over- ﬁtting problem. In the network architecture and fusion strategy stages, the basic network anddetailed multi-modal images fusion strategies are presented to train thesegmentation network. In the data post-processing stage, some post-pressing techniques such as morphological techniques and conditionalrandomﬁeld are implanted to reﬁne theﬁnal segmentation result. In the task of multi-modal medical image segmentation, fusing multiple mo-dalities is the key problem of the task. According to the level in thenetwork architecture where the fusion is performed, the fusion strategiescan be categorized into three groups: input-level fusion, layer-levelfusion, and decision-level fusion, the details refers to Section 4.3. Data processingThis section will describe the data processing including data dimen-sion selection, image pre-processing, data augmentation and post-processing techniques. This step is important in deep learning-basedsegmentation network.3.1. Data dimensionMedical image segmentation usually deals with 3D images. Somemodels directly use the 3D images to train models [41–44], while some models process the 3D image slice by slice [20,45–48]. The 3D approach takes the 3D image as input and applies the 3D convolution kernel toexploit the spatial contextual information of the image. The maindrawback is its expensive computational cost. Compared to utilizing thewhole volume image to train the model, some 3D small patches can beused to reduce the computational cost. For instance, Kamnitsas et al. [49] extracts 10 k random 3D patches at regular intervals for training tosegment the brain lesion. The 2D approach takes the image slice or patchextracted from the 3D image as input and applies the 2D convolutionalkernel, the 2D approach can efﬁciently reduce the computational cost,while it ignores the spatial information of the image in z direction. Forexample, Zhao, et al.[50]trainedﬁrstly FCNNs using image patches andthen CRFs as Recurrent Neural Networks using image slices with theparameters of FCNNsﬁxed,ﬁnally theyﬁne-tuned the FCNNs and the CRF-RNN using image slices. To exploit the feature information of the 2Dimage and 3D image, Mlynarski, et al. [51]described a CNN-based model for brain tumor segmentation, itﬁrst extracts the 2D features of the imagefrom axial, coronal and sagittal views and then takes them as the addi-tional input of the 3D CNN-based model. The method can learn richfeature information in three dimensions, which achieve good perfor-mance with median Dice scores of 0.918 (whole tumor), 0.883 (tumorcore) and 0.854 (enhancing core).3.2. Pre-processingPre-processing plays an important role in subsequent segmentationtask, especially for the multi-modal medical image segmentation becausethere are variant intensity, contrast and noise in the images. Therefore, tomake the images appear more similar and make the network trainingsmooth and quantiﬁable, some pre-processing techniques are appliedbefore feeding to the segmentation network. The typical pre-processingtechniques consist of image registration, bias
ﬁeld correction and in- tensity normalization. For BraTS dataset, the image registration hasalready done before provided to the public [20,45,49,50,52]. used the N4ITK method to correct the distortion of MRI data [19,20,41,45,48,49]. proposed to normalize each modality of each patient independently bysubtracting the mean and dividing by the standard deviation of the brainregion.3.3. Data augmentationMost of the time, a large number of labels for training is not availablefor several reasons. Labelling the dataset requires an expert in this ﬁeld which is expensive and time-consuming. When training large neuralnetworks from limited training data, the over- ﬁtting problem needs to be considered[53]. Data augmentation is a way to reduce over- ﬁtting and increase the amount of training data. It creates new images by trans-forming (rotated, translated, scaled,ﬂipped, distorted and adding some noise such as Gaussian noise) the ones in training dataset. Both theoriginal image and created images are fed into the neural network. Forexample, Isensee, et al.[41]proposed to address over-ﬁtting by utilizing a large variety of data augmentation techniques like random rotations,random scaling, random elastic deformations, gamma correctionaugmentation and mirroring on theﬂy during training.Table 1Summary of deep learning network architectures, ILSVRC: ImageNet Large ScaleVisual Recognition Challenge.
Architecture Article Rank on ILSVRCTop-5error rateNumber ofparametersLeNet[36]LeCun et al.,1998N/A N/A 60 thousandAlexNet[11]Krizhevskyet al., 20121st 16.4% 60 millionZFNet[12]Zeiler et al.,20131st 11.7% N/AVGG Net[13]Simonyan et al.,20142nd 7.3% 138 millionGoogleNet[14]Szegedy et al.,20151st 6.7% 5 million (V1) &23million (V2) ResNet[15]He. Kaiminget al., 20161st 3.57% 25.6 million(ResNet-50) DenseNet[16]Huang et al.,2017N/A N/A 6.98 million(DenseNet-100, k¼12)
1http://www.isles-challenge.org.
2https://chaos.grand-challenge.org .
3https://ivdm3seg.weeblys.com.T. Zhou et al. Array 3-4 (2019) 100004
43.4. Post-processing[54]Post-processing is applied to reﬁne theﬁnal result in segmen- tation network. The isolated segmentation labels with small size areprone to artefacts and the largest volume are usually kept in the ﬁnal segmentation. In this case, morphological techniques are preferred toremove incorrect small fragments and keep the largest volume. And somepost-processing techniques can be designed according to the structure ofdetected region. For example, considering LGG patients may don ’t have enhancing tumor, Isensee, et al.[42]proposed to replace all enhancing tumor voxels with necrosis if the number of predicted enhancing tumor isless than a threshold. Because if there is a false positive voxel in predictedsegmentation where no enhancing tumor presents in the ground truthwill result in a Dice score of 0. Another case in Ref. [49], a 3D fully connected Condition Random Field (CRF) is applied for post-processingto effectively remove false positives to reﬁne the segmentation result.4. Multi-modal segmentation networksOver the years, various semi-automated and automated techniqueshave been proposed for multi-modal medical image segmentation usingdeep learning-based methods, such as CNN [36]and FCN[17]especially U-Net[18]. According to the multi-modal fusion strategies, we categorythe network architectures into input-level fusion network, layer-levelfusion network and decision-level fusion network, for each fusion strat-egy we conclude some common used methods, shown in Fig. 5.4.1. Input-level fusion networkIn the input-level fusion strategy, multi-modality images are fusedchannel by channel as the multi-channel inputs to learn a fused featurerepresentation, and then to train the segmentation network. Most of theexisting multi-modal medical image segmentation networks adopt theinput-level fusion strategy, which directly integrates the multi-modalTable 2Summary of the multi-modal medical image segmentation datasets.
Dataset Train Validation Test Segmentation Task Modality Image SizeBrats2012 35 N/A 15 Brain tumor T1, T1C, T2, Flair 160 /C2216/C2176176/C2176/C2216 Brats2013 35 N/A 25 Brain tumor T1, T1C, T2, Flair 160 /C2216/C2176 176/C2176/C2216 Brats2014 200 N/A 38 Brain tumor T1, T1C, T2, Flair 160 /C2216/C2176 176/C2176/C2216 Brats2015 200 N/A 53 Brain tumor T1, T1C, T2, Flair 240 /C2240/C2155 Brats2016 200 N/A 191 Brain tumor T1, T1C, T2, Flair 240 /C2240/C2155 Brats2017 285 46 146 Brain tumor T1, T1C, T2, Flair 240 /C2240/C2155 Brats2018 285 66 191 Brain tumor T1, T1C, T2, Flair 240 /C2240/C2155 ISLES2015 28 N/A 36 Ischemic stroke lesion T1, T2, TSE, Flair, DWI, TFE/TSE 230 /C2230/C2154 30 N/A 20 T1c, T2, DWI, CBF, CBV, TTP, Tmax N/AMRBrainS13 5 N/A 15 Brain Tissue T1, T1_1 mm, T1_IR, Flair 256 /C2256/C2192 240/C2240/C248 NeoBrainS12 20 N/A 5 Brain Tissue T1, T2 384 /C2384/C250 512/C2512/C2110 512/C2512/C250 iSeg-2017 10 N/A 13 Brain Tissue T1,T2 N/ACHAOS 20 N/A 20 Abdominal Organs CT, T1-DUAL, T2-SPIR N/AIVD 16 N/A 8 Intervertebral Disc In-phase, Opposed-phase, Fat, Water N/A
Table 3Summary of the evaluation metrics commonly used for these datasets. Withrespect to the number of false positive ( FP), true positive (TP), false negative (FN) and true negative (TN),
∂Sand ∂Rare the sets of lesion border pixels/voxels for the predicted and the truth segmentations, and d
mðv;vÞis the minimum of the Euclidean distances between a voxel vand voxels in a setv.jXjis the number of voxels in the reference segmentation and jYjis the number of voxels in the al- gorithm segmentation,X
sandY sare the sets of surface points of the reference and algorithm segmentations respectively. The operator dis the Euclidean dis- tance operator.
Evaluation metric Mathematical descriptionDice score(DSC) DSC¼
2TP2TPþFPþFN Sensitivity Sensitivity¼
TPTPþFN Speciﬁcity Specificity¼
TNTNþFP Hausdorff distance(HD) HD¼maxfsup
rε∂Rdmðs;rÞ;sup sε∂Sdmðs;rÞg Absolute relative volumedifference(ARVD) ARVDðX;YÞ¼/C12/C12/C12/C12100/C2/C18
jXjjYj/C01/C19/C12/C12/C12/C12 Average boundary distance(ABD) ABDðX
s;YsÞ¼1N
XsþN Ys/C16X
x2X smin y2Y sdðx;yÞþP
y2Y smin x2X sdðy;xÞ/C17
Fig. 4.The pipeline of multi-modal medical image segmentation based on deep learning.
Fig. 5.The generic categorization of the fusion strategy.T. Zhou et al. Array 3-4 (2019) 100004
5images in the original input space [20,41,42,45,48–50,55,56].Fig. 6 describes the generic network architecture of the input-level fusionsegmentation network. We take CT and MRI as two input modalities,convolutional neural network as the segmentation network and the braintumor segmentation as the segmentation task. By using the input-levelfusion strategy, the rich feature information from different modalitiescan be fully exploited in all layers, from the ﬁrst layer to the last one. This kind of fusion uses usually four techniques, multi-task segmentation,multi-view segmentation, multi-scale segmentation and GAN-basedsegmentation.To name a few, Wang, et al.[48]proposes a multi-modal segmenta- tion network using BraTS dataset to segment the brain tumor into threesubregions including the whole tumor, tumor core and enhancing tumorcore. It uses multi-task and multi-view techniques. In order to obtain aunited feature set, it directly integrates the four modalities (T1, T1c, T2and Flair of MRI) as the multi-channel inputs in the input space. Then itseparates the complex multi-class segmentation task into several simplersegmentation tasks according to the hierarchical structure of the braintumor. The whole tumor isﬁrstly segmented and then the bounding boxincluding the whole tumor is used for the tumor core segmentation.Based on the obtained bounding box of the tumor core, the enhancingtumor core isﬁnally segmented. Furthermore, to take advantage of 3Dcontextual information, for each individual task, they fused the seg-mentation results from three different orthogonal views (axial, coronaland sagittal) by averaging the softmax outputs of the individual task.Experiments with the testing set of BraTS 2017 data show that the pro-posed method achieves an average Dice scores of 0.7831, 0.8739, and0.7748 for enhancing tumor core, whole tumor and tumor core, respec-tively, which won the second place on BraTS 2017 challenge. Themulti-task segmentation separates the complex task of multiple classsegmentation into several simpler segmentation tasks and takes advan-tage of the hierarchical structure of tumour subregions to improve seg-mentation accuracy.Zhou et al.[57]also proposes a multi-task segmentation network onBraTS dataset, it fuses the multi-modal MR images channel by channel inthe input space to learn a fused feature representation. Compared tosegmentation of[48]which suffers from network complexity and ignoresthe correlation between the three sequential segmentation tasks, it de-composes brain tumor segmentation into three different but related tasks.Each task has an independent convolutional layer, one classi ﬁcation layer, one loss layer and different input data. Based on curriculumlearning[58], which means gradually increasing the difﬁculty of training tasks, they applied an effective strategy to improve the convergencequality of the model by training theﬁrst task only until the loss curve tends toﬂatten, then theﬁrst data and the second data are concatenatedalong the batch dimension as the input for the second task. The operationof the third task is like the second one. In this way, not only the modelparameters but also the training data are transferred from an easier taskto a more difﬁcult task. The proposed approach ranksﬁrst on the BRATS 2015 test set and achieves top performance on the BRATS 2017 dataset.It’s likely to require different receptiveﬁeld when segmenting different regions in an image. For example, large regions may need alarge receptiveﬁeld at the expense ofﬁne details, while small regions may require high resolution local information. Qin et al. [43]proposed the autofocus convolutional layer to enhance the abilities of neural net-works by using multi-scale processing. After integrating the multi-modalimages in the input space, they applied an autofocus convolutional layerby using multiple convolutional layers with different dilation rates tochange the size of the receptive
ﬁeld. Autofocus convolutional layer canindicate the importance of each scale when processing different locationsof an image. Also, they used an attention mechanism to choose theoptimal scale. The proposed autofocus layer can be easily integrated intoexisting networks to improve a model ’s performance. The proposed method gained promising performance on the challenging tasks ofmulti-organ segmentation in pelvic CT and brain tumor segmentation inMRI.Motivated by the success of Generative Adversarial Network (GAN)[59], which models a mini-max game between the generator and thediscriminator, some methods propose to apply the discriminator as theextra constraint to improve the segmentation performance [60,61].I n Ref.[60], by fusing the multi-modal images as multi-channel inputs, theytrained two separate networks: a residual U-net as the generativenetwork and a discriminator network, the segmentation network willgenerate a segmentation, while the discriminator network will distin-guish between the generated segmentations and ground truth masks. Thediscriminator is a shallow network containing three 3D convolutionblocks, each followed by a max-pooling layer. In order to obtain a robustsegmentation, they introduced extra constraints via contours to themodel. Hausdorff distance between ground truth contours and predictioncontours is used as a measure of dissimilarity. The proposed method wasevaluated on the BraTS 2018 dataset and achieved competitive results,demonstrating that raw segmentation results can be improved by incor-porating extra constraints in contours and adversarial training. Huo et al.[61]employed the PatchGAN[62]as an additional discriminator to su-pervise the training procedure of the network. The method based on GANcan obtain a robust segmentation due to the extra constrain of discrim-inator, but it costs more memory to train the extra discriminator.The input-level fusion strategy can maximumly keep the originalimage information and learn the intrinsic image feature. Using sequentialsegmentation networks allows to take different strategies, such as multi-task, multi-view, multi-scale and GAN-based segmentation network, tofully exploit the feature representation from multi-modal images.4.2. Layer-level fusionIn the layer-level fusion strategy, single or two modal images are usedas the single input to train individual segmentation network, and thenthese learned individual feature representations will be fused in thelayers of the network,ﬁnally the fused result will be fed to the decisionlayer to obtain theﬁnal segmentation result. The layer-level fusionnetwork can effectively integrate and fully leverage multi-modal images[44,46,63,64].Fig. 7describes the generic network architecture oflayer-level fusion segmentation work.To name a few, we also take the brain tumor segmentation in multi-sequence of MRI to illustrate this kind of fusion. It is well known that T1weighed MRI and T1c are suitable to segment the tumor core without theperitumoral edema, while T2 and Flair are suitable to segment the per-itumoral edema. Chen et al.[63]proposes a dual-pathway multi-modalbrain tumor segmentation network. Theﬁrst pathway uses the T2 and Flair to extract the relative feature to segment the whole tumor from thebackground, and the second pathway uses the T1 and T1c to train thesame segmentation network to learn other relative feature
Fig. 6.The generic network architecture of the input-level fusion.T. Zhou et al. Array 3-4 (2019) 100004
6representation, and then the features from the both pathways are fusedandﬁnally fed into a four-class softmax classiﬁer to segments the back- ground, ED, ET and NCR/NET. The dual-pathway segmentation networkcan exploit the effective feature information of different modalities andachieve an accurate segmentation result.Dolz et al.[44]proposes a 3D fully convolutional neural networkbased on DenseNets that extends the deﬁnition of dense connectivity to multi-modal segmentation. Each imaging modality has a path and denseconnections exist both in the layers within the same path and in thedifferent paths. Therefore, the proposed network can learn more complexfeature representations between the modalities. The extensive experi-ment results on two different and highly competitive multi-modal braintissue segmentation challenges: iSEG 2017 [40]and MRBrainS 2013 [38], show that the proposed method yielded signi ﬁcant improvements over many other state-of-the-art segmentation networks, ranking at thetop on both benchmarks.Inspired by Ref.[44], Dolz, et al.[46]proposes an architecture for IVD (Intervertebral Disc) localization and segmentation in multi-modalMRI. Each MRI modality is processed in a corresponding single path tobetter exploit its feature representation. The network is densely con-nected both within each path and across different paths, granting thenthe freedom of the model to learn where and how the different modalitiesshould be processed and combined. It also improves the standard U-Netmodules by extending inception modules using two convolutional blockswith dilated convolutions of a different scale to help handle multi-scalecontext information.To summarize, in the layer-level fusion segmentation network, Den-seNets are the commonly used networks which bring the three followingbeneﬁts. First, direct connections between all layers help to improve theﬂow of information and gradients through the entire network, alleviatingthe problem of vanishing gradient. Second, short paths to all the featuremaps in the architecture introduce implicit deep supervision. Third,dense connections have a regularizing effect, which reduces the risk ofover-ﬁtting on tasks with smaller training sets. Therefore, DenseNetsallow to improved effectiveness and efﬁciency in the layer-level fusion segmentation network. In the layer-level fusion segmentation network,the connection among the different layers can capture complex re-lationships between modalities, which fully exploit the feature repre-sentation of multi-modal images.4.3. Decision-level fusionIn decision-level fusion segmentation network, like the layer-levelfusion, each modality image is used as the single input of single seg-mentation network. The single network can better exploit the uniqueinformation of the corresponding modality. The outputs of the individualnetworks will then be integrated to get the ﬁnal segmentation result. The decision-level fusion segmentation network is designed to independentlylearn the complementary information from different modalities, sincemulti-modal images have little direct complementary information intheir original image spaces due to different image acquisition techniques.Fig. 8describes the generic network architecture of layer-level fusionsegmentation work.For example, to effectively employ multi-modalities from T1, T2 andfractional anisotropy (FA) modality, Nie, et al. [47]proposes a new multi-FCNs network architecture for the infant brain tissue segmentation(white matter (WM), gray matter (GM), and cerebrospinal ﬂuid (CSF)). Instead of simply combining three modality data from the input space,they trained one network for each modality and then fused multiplemodality features from high-layer of each network. Results showed thatthe proposed model signiﬁcantly outperformed previous methods interms of accuracy.For the decision-level fusion, many fusion strategies have been pro-posed[64]. The most of them are based on averaging and majorityvoting. For averaging strategy, Kamnitsas, et al. [52]trains three net- works separately and then averaged the con ﬁ
dence of the individual networks. Theﬁnal segmentation is obtained by assigning each voxelwith the highest conﬁdence. For majority voting strategy, theﬁnal label of a voxel depends on the majority of the labels of the individualnetworks.The statistical properties of the different modalities are different,which make it difﬁcult for a single model to directlyﬁnd correlations
Fig. 7.The generic network architecture of the layer-level fusion.
Fig. 8.The generic network architecture of the decision-level fusion.T. Zhou et al. Array 3-4 (2019) 100004
7across modalities. Therefore, in decision-level fusion segmentationnetwork, the multiple segmentation networks can be trained to fullyexploit the multi-modal features. Aygn et al. [65]investigates different fusion methods on the brain tumor segmentation problem in terms ofmemory and performance. In terms of memory usage, the decision-levelfusion strategies require more memory since the model fuses the featureslater and more parameters are needed for layers to perform convolutionand other operations. However, the later fusion can achieve better per-formance, because each modality is employed as input of one networkwhich can learn complex and complementary feature informationcompared to input-level fusion network.5. Common problems5.1. Over-ﬁttingOne limitation in medical image segmentation is data scarcity, usu-ally leading to the over-ﬁtting which refers to a model that has a goodperformance on the training dataset but does not perform well on newdata. Most of the time, a large number of labels for training is notavailable for medical image analysis, because labelling the dataset re-quires experts in thisﬁeld, and it is time-consuming and sometimes proneto error. When training complex neural networks with limited trainingdata, special care must be taken to prevent the over- ﬁtting. The complexity of a neural network model is de ﬁned by both its structure and the parameters. Therefore, we can reduce the complexity of the networkarchitecture by reducing the layers or parameters or focus on methodsthat artiﬁcially increase the number of training data instead of changingthe network architecture[32,66]. The latter is commonly used to pro-duce new synthetic images by performing data transformations and thecorresponding ground truth that include operations of scaling, rotation,translation, brightness variation, elastic deformations, horizontal ﬂip- ping and mirroring (for details refer to Section 3.3data augmentation).5.2. Class imbalanceOne of the major challenges in medical image analysis is to deal withimbalanced data. In medical imagingﬁeld, the problem is even more glaring. For example, for a segmentation of brain tumor or that of whitematter lesion, the normal brain region is larger than the abnormal region.Training with the class imbalanced data can cause an unstable segmen-tation network, which is biased towards the class with a large region.Table 4illustrate the distribution of the classes in the training data ofBraTS 2017, the number of positives (NEC/NET, ED and ET) and nega-tives (Background) are highly imbalanced and the background is over-whelmingly dominant. As result, the choice of the loss functions is crucialin segmentation networks, especially when dealing with highly unbal-anced problems. We present several types of loss function which arewidely used individually or combined in medical image segmentationnetworks. From the data-level, the problem can be addressed by resam-pling the data space. There are three main approaches: under-samplingthe negative class[67]or upsampling the negative class [68]and SMOTE (Synthetic Minority Over-sampling Technique) [69]generating synthetic samples along the line segment that joins minority class sam-ples. These approaches are simple to follow but they may remove someimportant data or add redundant data to the training set.The patch sampling-based method can also mitigate the imbalanceddata problem. For example, Kamnitsas, et al. [49]proposes the balanced strategy to alleviate class imbalance problem. They extract the trainingpatches with 50% probability being cantered either on the lesion orhealthy voxels. Clrigues et al.[56]uses the lesion centered strategy, inwhich all training patches are extracted from the region centered on alesion voxel. Additionally, a random offset is added to a sampling 475point to avoid location bias, where a lesion voxel is always expected atthe patch center, contributing then to some data augmentations.As for the algorithm-level, Havaei, et al. [19]proposes a two-phase training procedure. Itﬁrst constructs a patch dataset such that all la-bels are equiprobable by taking into account the diversity in all classes,and then retrains only the output layer to calibrate the output probabil-ities correctly. In this way, the class imbalance problem is overcome.
Another approach consists of using a multi-task segmentation [48,57,63, 70]that decomposes the complex multi-class segmentation task intoseveral simple tasks, since each training task segments only one region,where the label distribution will be less unbalanced than segmentationmultiple classes at one time. Some approaches address class imbalanceproblem through an ensemble learning by combining same or differentclassiﬁers to improve their generalization ability [71]. Otherwise, the loss functions can also alleviate this problem by modifying the distribu-tions of the training data. We present them as below.Cross-Entropy (CE) loss:Cross-entropy loss function is the mostcommonly used for the task of image segmentation. It is calculated byequation(1). Because the cross-entropy loss evaluates individually theclass predictions for each pixel vector and then averages all pixels, thiscan lead some error if an unbalanced class representation exists in theimage. Long et al.[17]proposes to weight or sample the loss function foreach output channel in order to alleviate the class imbalance problem.Loss
CE¼/C0X
i2NX
l2LyðlÞilogbyðlÞi(1)whereNis the set of all examples andLthe set of all labels,y
ðlÞiis the one- hot encoding (0 or 1) for example and label l,by
ðlÞiis the predicted probability for the same example/label pair.Weighted Cross Entropy (WCE): Since the background regions dominate the training set, it is reasonable to incorporate the weights ofmultiple classes into the cross-entropy as de ﬁned as follows[32]:Loss
WCE¼/C0X
iεNX
l2LwiyðlÞilogbyðlÞi(2)wherew
irepresents the weight assigned to the ith label. Dice Loss (DL):Dice loss is a popular loss function for medical imagesegmentation which is a measure of overlap between the predictedsample and real sample. This measure ranges from 0 to 1 where a Dicescore of 1 denotes the complete overlap as de ﬁned as follows[72]:Loss
DL¼1/C02P
l2LP
i2NyðlÞibyðlÞiþε
P
l2LP
i2N/C16yðlÞiþbyðlÞi/C17þ
ε(3)where
εis a small constant to avoid dividing by 0.Generalized Dice (GDL):Sudre et al.[73]proposed to use the class rebalancing properties of the Generalized Dice overlap, de ﬁned in (4), as a robust and accurate deep-learning loss function for unbalanced tasks.The authors investigate the behavior of Dice loss, cross-entropy loss andgeneralized dice loss functions in the presence of different rates of labelimbalance across 2D and 3D segmentation tasks. The results demonstratethat the GDL is more robust than the other loss functionsLoss
GDL¼1/C02P
l2LwiP
i2NyðlÞibyðlÞiþε
P
l2LwiP
i2N/C16yðlÞiþbyðlÞi/C17þ
ε(4)Focal Loss (FL):Focal loss was originally introduced for the detectiontask. It encourages the model to down-weight easy examples and focusestraining on hard negatives. Formally, the Focal loss is de ﬁned by intro- ducing a modulating factor to the cross-entropy loss and a parameter forTable 4The distribution of classes on BraTS 2017 training set, NET: Non EnhancingTumor, NCR: Necrotic.
Region Background NET/NCR Edema Enhancing tumorPercentage 99.12 0.28 0.40 0.20T. Zhou et al. Array 3-4 (2019) 100004
8class balancing[74]:Loss
FLðptÞ¼ /C0 αtð1/C0p tÞγlogðp tÞ (5)p
t¼/C26pt if y¼11/C0p
totherwise (6)wherey2f/C01;þ1gis the ground-truth class, andp
t2½0;1/C138is the estimated probability for the class with label y¼1.The focusing parameterγsmoothly adjusts the rate at which easy examples are down-weighted, settingγ>0 can reduce the relative loss for well-classi ﬁed examples, putting focus on hard and misclassi ﬁed examples, the focal loss is equal to the original cross entropy loss when γ¼0.6. Discussion and conclusionIn the above sections, we presented a large set of state-of-the-artmultimodal medical image segmentation networks based on deeplearning. They are summarized inTable 5. For BraTS challenge, these methods are concluded since 2013, because deep learning methods areapplied since 2013. Publicly available multi-modal medical image data-sets for segmentation task are rare, the most used dataset is the BraTSdataset having proposed since 2012. For their segmentation, the currentbest method is proposed in Ref.[55], they use the input-level fusionstrategy to directly integrate the different modalities in the input space,they apply the encoder-decoder structure of CNN combined with anadditional VAE (variational autoencoder) branch to the encoder part. TheVAE branch can reconstruct the input image and exploit better the fea-tures of the encoder endpoint. It also provides an additional guidance anda regularization to the encoder part. The authors demonstrate that moresophisticated data augmentation techniques, data post-processing tech-niques, or deeper network will not further improve the network perfor-mance, which means the network architecture plays a crucial role in thesegmentation network than other data processing operations.For multi-modal medical image segmentation, the fusion strategytakes an important role in order to achieve an accurate segmentationresult. Conventional image fusion strategy learns a direct mapping be-tween source images and target images, the fusion strategy consists oftwo basic stages: activity level measurement and fusion rule [79]. Ac- tivity level measurement is implemented by designing local ﬁlters to extract high-frequency details, and the calculated clarity information ofdifferent source images are then compared using some designed rules toobtain a clarity image. To achieve better performance, these issuesbecome more and more complicated, so it is dif ﬁcult to manually propose an ideal fusion strategy which fully concerns the important issues. To thisend, a deep learning-based network can directly encode the mapping.Deep learning-based methods outperform in three aspects. First, deep
Table 5Summary of the deep learning approaches for multi-modal medical image segmentation, the bold presents the best performance in the challenge. The acr onyms in results are: cerebrospinalﬂuid (CSF), gray matter (GM), white matter (WM), the symbol * indicates the method has available code.
Article Pre-processing Data Network Fusion level Results (DSC) Database[49]* Normalization 3D CNN Input whole/core/enhanced BraTS15 Bias Field Correction Patch CRF 0.84/0.66/0.63[20]Normalization 2D CNN Input whole/core/enhanced BraTS13Bias Field Correction Patch 0.84/0.71/0.57[41]* Normalization 3D U-Net Input whole/core/enhanced BraTS15Data Augmentation Patch ResNet 0.85/0.74/0.64 BraTS17 0.85/0.77/0.64 [50]Normalization 3D FCN Input whole/core/enhanced BraTS13Bias Field Correction Patch CRF 0.86/0.73/0.62 BraTS15 RNN 0.84/0.73/0.62 BraTS16 4/3/2(rank) [57]Normalization 3D U-Net Input whole/core/enhanced BraTS15 ResNet 0.87/0.75/0.64 [48]Normalization 2D U-Net Input whole/core/enhanced BraTS17Bias Field Correction Slice ResNet 0.87/0.77/0.78[42]Normalization 3D U-Net Input whole/core/enhanced BraTS18Data Augmentation Patch ResNet 0.87/0.80/0.77[45]Normalization 2D CNN Input whole/core/enhanced BraTS15Data Augmentation Patch FCN 0.89/0.77/0.80Bias Field Correction[20]Normalization 3D CNN Input whole/core/enhanced BraTS13 Bias Field Correction 0.88/0.81/0.76 [75]Normalization 2D FCN Input whole/core/enhanced BraTS16 Data Augmentation 0.87/0.81/0.72 [52]* Normalization 3D U-Net Input whole/core/enhanced BraTS17 Bias Field Correction FCN 0.88/0.78/0.72 DeepMedic [55]* Normalization 3D U-Net Input whole/core/enhanced BraTS18 Data Augmentation VAE 0.88/0.81/0.76[76]N/A 2D CNN Input 0.9112 IVD Slice ResNet [56]* Normalization 3D U-Net Input 0 :59/C60:31ISLES15 Patch ResNet 0 :84/C60:10 (SISS/SPES) [77]* Normalization 3D SVM Input CSF/WM/GM MRBrainS13 Patch 0.78 0.88 0.84 [44]* N/A 3D CNN Layer CSF/WM/GM iSEG-2017 Patch DenseNet 0.95/0.91/0.90 MRBrainS13 0.84/0.90/0.86 [78]* Normalization 3D DenseNet Layer CSF/WM/GM iSEG-2017 Patch 0.96/0.91/0.90 [46]N/A 2D U-Net Layer 0 :9191/C60:0179 IVD Slice DenseNet [47]N/A 2D FCN Decision CSF/WM/GM Private data Patch 0.85/0.88/0.87T. Zhou et al. Array 3-4 (2019) 100004
9learning-based networks learn a complex and abstract hierarchicalfeature representation for image data to overcome the dif ﬁculty of manual feature design. Second, deep learning-based networks can pre-sent the complex relationships between different modalities by using thehierarchical network layer, such as the layer-level fusion strategy. Third,the image transform and fusion strategy in the conventional fusionstrategy can be jointly generated by training a deep learning model, inthis way some potential deep learning network architectures can beinvestigated for designing an effective image fusion strategy. Therefore,the deep learning-based method has a great potential to produce betterfusion results than conventional methods.Choosing an effective deep learning fusion strategy is still an impor-tant issue. In 2013–2018 BraTS Challenge, all the methods applied theinput-level fusion to directly integrate the different MR images in theinput space, which is simple and can remain the intrinsic image featureand allow the method to focus on the subsequent segmentation networkarchitecture designs, such as multi-task, multi-view, multi-scale andGAN-based strategies. While the strategy just concatenates the modalitiesin the input space, but it does not exploit the relationships among thedifferent modalities. For layer-level fusion, with the dense connectionamong the layers, the fusion strategy often takes the DenseNet as thebasic network. The connection among the different layers can capturecomplex relationships between modalities, which can help the segmen-tation network learn more valuable information and achieve better per-formance than directly integrating different modalities in the inputspace. For decision-level fusion strategy, it can achieve better perfor-mance compared to the input-level fusion, because each modality isemployed to train a single network to learn independent feature repre-sentation, while this requires much memory and computational time.Compared the last two fusion strategies, the layer fusion strategy seemsbetter, since the dense connection among the layers can exploit morecomplex and complementary information to enhance the networktraining, while the decision-level fusion only learns the independentfeature representation in single modality. Since the results of the threefusion strategies are not obtained from the same data, their comparisonin terms of performance is difﬁcult. Methodologically, each strategy hasits advantages and disadvantages.Although we observed the advantages of these fusion strategies basedon deep learning, based on the previous works, we can still observe thatthere are some locks to lift in multi-modal medical image segmentationbased on deep learning. It is known that multi-modal fusion networksgenerally perform better than single-modal network for segmentationtask. The problem is how to fuse different modalities to get the bestcompromise for a precise segmentation. Hence, how to design multi-modal networks to efﬁciently combine different modalities, how toexploit the latent relationship between different modalities, and how tointegrate the multi-information into the segmentation network toimprove the segmentation performance can be the topics of future works.Other problem concerns the data. First, since it is dif ﬁcult to obtain a large number of medical image data, the limited training data can easilylead to over-adjustment. To deal with it, reducing the complexity of thenetwork architecture or increasing the number of training data has beenproved to alleviate the problem. Second, training with the imbalanceddata can cause an unstable segmentation network especially with smalllesion or structure segmentation. Resampling the data space, using two-phase training procedure, careful path sampling and appropriate lossfunction are the proposed strategies to overcome the problem. Third, likecommon problems for deep learning, it ’s difﬁcult to train a deep network with original limited data without data augmentation or other optimizedtechniques. Therefore, designing faster methods to perform convolutionsand appropriate optimization methods can help to train an effectivesegmentation network. It is becoming a widespread practice in thecomputer vision community to release source codes to the public. Wehave indicated the available code in Table 5. This practice helps to expedite the research in theﬁeld. Another recommended practice isvalidating the model on different datasets, which can open the door todesign a robust model that can be applied to datasets of similarapplications.Declaration of Competing InterestThe authors declare no conﬂict of interest.References
[1]Dubois D, Prade H. Combination of fuzzy information in the framework ofpossibility theory. Data Fusion Robot. Mach. Intell. 1992;12:481 –505. [2]Lapuyade-Lahorgue J, Xue J-H, Ruan S. Segmenting multi-source images usinghidden markovﬁelds with copula-based multivariate statistical distributions. IEEETrans Image Process 2017;26(7):3187 –95. [3]Das S, Kundu MK. A neuro-fuzzy approach for medical image fusion. IEEE TransBiomed Eng 2013;60(12):3347 –53. [4]Balasubramaniam P, Ananthi V. Image fusion using intuitionistic fuzzy sets. InfFusion 2014;20:21–30.[5]Smets P. The combination of evidence in the transferable belief model. IEEE TransPattern Anal Mach Intell 1990;12(5):447 –58. [6]Lian C, Ruan S, Denœux T, Li H, Vera P. Joint tumor segmentation in pet-ct imagesusing co-clustering and fusion based on belief functions. IEEE Trans Image Process2019;28(2):755–66.[7]Vazquez-Reina A, Gelbart M, Huang D, Lichtman J, Miller E, P ﬁster H. Segmentation fusion for connectomics. In: 2011 international conference oncomputer vision. IEEE; 2011. p. 177 –84. [8]Srivastava N, Salakhutdinov RR. Multimodal learning with deep Boltzmannmachines. In: Advances in neural information processing systems; 2012.p. 2222–30.[9]Cai H, Verma R, Ou Y, Lee S-k, Melhem ER, Davatzikos C. Probabilisticsegmentation of brain tumors based on multi-modality magnetic resonance images.In: 2007 4th IEEE international symposium on biomedical imaging: from nano tomacro. IEEE; 2007. p. 600 –3. [10]Zhang N, Ruan S, Lebonvallet S, Liao Q, Zhu Y. Kernel feature selection to fusemulti-spectral mri images for brain tumor segmentation. Comput Vis ImageUnderstand 2011;115(2):256 –69. [11]Krizhevsky A, Sutskever I, Hinton GE. Imagenet classi ﬁcation with deep convolutional neural networks. In: Advances in neural information processingsystems. Advances in neural information processing systems; 2012. p. 1097 –105. [12]Zeiler MD, Fergus R. Visualizing and understanding convolutional networks. In:European conference on computer vision. Springer; 2014. p. 818 –33. [13] K. Simonyan, A. Zisserman, Very deep convolutional networks for largescale imagerecognition, arXiv preprint arXiv:1409.1556.[14]Szegedy C, Liu W, Jia Y, Sermanet P, Reed S, Anguelov D, Erhan D, Vanhoucke V,Rabinovich A. Going deeper with convolutions. In: Proceedings of the IEEEconference on computer vision and pattern recognition; 2015. p. 1 –9. [15]He K, Zhang X, Ren S, Sun J. Identity mappings in deep residual networks. In:European conference on computer vision. Springer; 2016. p. 630 –45.
[16]Huang G, Liu Z, Van Der Maaten L, Weinberger KQ. Densely connectedconvolutional networks. In: Proceedings of the IEEE conference on computer visionand pattern recognition; 2017. p. 4700 –8. [17]Long J, Shelhamer E, Darrell T. Fully convolutional networks for semanticsegmentation. In: Proceedings of the IEEE conference on computer vision andpattern recognition; 2015. p. 3431 –40. [18]Ronneberger O, Fischer P, Brox T, U-net. Convolutional networks for biomedicalimage segmentation. In: International Conference on Medicalimage computing andcomputer-assisted intervention. Springer; 2015. p. 234 –41. [19]Havaei M, Davy A, Warde-Farley D, Biard A, Courville A, Bengio Y, Pal C, Jodoin P-M, Larochelle H. Brain tumor segmentation with deep neural networks. Med ImageAnal 2017;35:18–31.[20]Pereira S, Pinto A, Alves V, Silva CA. Brain tumor segmentation using convolutionalneural networks in mri images. IEEE Trans Med Imaging 2016;35(5):1240 –51. [21]Menze BH, Jakab A, Bauer S, Kalpathy-Cramer J, Farahani K, Kirby J, Burren Y,Porz N, Slotboom J, Wiest R, et al. The multimodal brain tumor image segmentationbenchmark (brats). IEEE Trans Med Imaging 2015;34(10):1993 –2024. [22] A. Kalinovsky, V. Kovalev, Lung image ssgmentation using deep learning methodsand convolutional neural networks.[23]Fu M, Wu W, Hong X, Liu Q, Jiang J, Ou Y, Zhao Y, Gong X. Hierarchicalcombinatorial deep learning architecture for pancreas segmentation of medicalcomputed tomography cancer images. BMC Syst Biol 2018;12(4):56 . [24]Roth HR, Lu L, Farag A, Shin H-C, Liu J, Turkbey EB, Summers RM. Deeporgan:multi-level deep convolutional networks for automated pancreas segmentation. In:International conference on medical image computing and computer-assistedintervention. Springer; 2015. p. 556 –64. [25]Yu L, Yang X, Chen H, Qin J, Heng PA. Volumetric convnets with mixed residualconnections for automated prostate segmentation from 3d mr images. In: Thirty-ﬁrst AAAI conference on arti_cial intelligence. AAAI; 2017 . [26]Zhou X, Takayama R, Wang S, Hara T, Fujita H. Deep learning of the sectionalappearances of 3d ct images for anatomical structure segmentation based on an fcnvoting method. Med Phys 2017;44(10):5221 –33. [27]Trullo R, Petitjean C, Nie D, Shen D, Ruan S. Joint segmentation of multiple thoracicorgans in ct images with two collaborative deep architectures. In: Deep learning inT. Zhou et al. Array 3-4 (2019) 100004
10medical image analysis and multimodal learning for clinical decision support.Springer; 2017. p. 21–9. [28]Bhatnagar G, Wu QJ, Liu Z. A new contrast based multimodal medical image fusionframework. Neurocomputing 2015;157:143 –52. [29]Guo Z, Li X, Huang H, Guo N, Li Q. Deep learning-based image segmentation onmultimodal medical imaging. IEEE Trans Radiat Plasma Med Sci 2019;3(2):162 –9. [30]Litjens G, Kooi T, Bejnordi BE, Setio AAA, Ciompi F, Ghafoorian M, Van DerLaak JA, Van Ginneken B, S_anchez CI. A survey on deep learning in medical imageanalysis. Med Image Anal 2017;42:60 –88. [31] J. Bernal, K. Kushibar, D. S. Asfaw, S. Valverde, A. Oliver, R. Martí, X. Llad /C19oo, Deep convolutional neural networks for brain image analysis on magnetic resonanceimaging: a review, Artif. Intell. Med..[32] Y. LeCun, Y. Bengio, G. Hinton, Deep learning. Nature 521 (7553): 436,GoogleScholar.[33]Bengio Y, Lamblin P, Popovici D, Larochelle H. Greedy layer-wise training of deepnetworks. In: Advances in neural information processing systems; 2007. p. 153 –60. [34]Salakhutdinov R, Hinton G. Deep Boltzmann machines. In: Arti ﬁcial in- telligence and statistics; 2009. p. 448 –55. [35]LeCun Y, Boser B, Denker JS, Henderson D, Howard RE, Hubbard W, Jackel LD.Backpropagation applied to handwritten zip code recognition. Neural Comput1989;1(4):541–51.[36]LeCun Y, Bottou L, Bengio Y, Haffner P, et al. Gradient-based learning applied todocument recognition. Proc IEEE 1998;86(11):2278 –324. [37]He K, Zhang X, Ren S, Sun J. Delving deep into recti ﬁers: surpassing human-level performance on imagenet classi_cation. In: Proceedings of the IEEE internationalconference on computer vision. IEEE; 2015. p. 1026 –34. [38]Mendrik AM, Vincken KL, Kuijf HJ, Breeuwer M, Bouvy WH, De Bresser J,Alansary A, De Bruijne M, Carass A, El-Baz A, et al. Mrbrains challenge: onlineevaluation framework for brain image segmentation in 3t mri scans. Comput IntellNeurosci 2015;2015:1.[39]I/C20sgum I, Benders MJ, Avants B, Cardoso MJ, Counsell SJ, Gomez EF, et al.Evaluation of automatic neonatal brain segmentation algorithms: the neobrains12challenge. Med Image Anal 2015;20(1):135 –51. [40]Wang L, Nie D, Li G, Puybareau /C19E, Dolz J, Zhang Q, et al. Benchmark on automatic 6-month-old infant brain segmentation algorithms: the iseg-2017 challenge. IEEETrans Med Imaging 2019. [41]Isensee F, Kickingereder P, Wick W, Bendszus M, Maier-Hein KH. Brain tumorsegmentation and radiomics survival prediction: contribution to the brats 2017challenge. In: International MICCAI brainlesion workshop. Springer; 2017.p. 287–97.[42]Isensee F, Kickingereder P, Wick W, Bendszus M, Maier-Hein KH. No new-net. In:International MICCAI brainlesion workshop. Springer; 2018. p. 234 –44.
[43]Qin Y, Kamnitsas K, Ancha S, Nanavati J, Cottrell G, Criminisi A, Nori A. Autofocuslayer for semantic segmentation. In: International conference on medical imagecomputing and computer-assisted intervention. Springer; 2018. p. 603 –11. [44] J. Dolz, K. Gopinath, J. Yuan, H. Lombaert, C. Desrosiers, I. B. Ayed, Hyperdense-net: a hyper-densely connected cnn for multi-modal image segmentation, IEEETrans Med Imaging.[45]Cui S, Mao L, Jiang J, Liu C, Xiong S. Automatic semantic segmentation of braingliomas from mri images using a deep cascaded neural network. J Healthc Eng2018.[46]Dolz J, Desrosiers C, Ayed IB. Ivd-net: intervertebral disc localization andsegmentation in mri with a multi-modal unet. In: International workshop andchallenge on computational methods and clinical applications for spine imaging.Springer; 2018. p. 130 –43. [47]Nie D, Wang L, Gao Y, Sken D. Fully convolutional networks for multimodalityisointense infant brain image segmentation. In: 2016 IEEE 13
thinternational symposium on biomedical imaging (ISBI). IEEE; 2016. p. 1342 –5. [48]Wang G, Li W, Ourselin S, Vercauteren T. Automatic brain tumor segmentationusing cascaded anisotropic convolutional neural networks. In: International MICCAIbrainlesion workshop. Springer; 2017. p. 178 –90. [49]Kamnitsas K, Ledig C, Newcombe VF, Simpson JP, Kane AD, Menon DK, et al.Efﬁcient multi-scale 3d cnn with fully connected crf for accurate brain lesionsegmentation. Med Image Anal 2017;36:61 –78. [50]Zhao X, Wu Y, Song G, Li Z, Zhang Y, Fan Y. A deep learning model integratingfcnns and crfs for brain tumor segmentation. Med Image Anal 2018;43:98 –111. [51]Mlynarski P, Delingette H, Criminisi A, Ayache N. 3d convolutional neural networksfor tumor segmentation using long-range 2d context. Comput Med Imag Graph2019;73:60–72.[52]Kamnitsas K, Bai W, Ferrante E, McDonagh S, Sinclair M, Pawlowski N, Rajchl M,Lee M, Kainz B, Rueckert D, et al. Ensembles of multiple models and architecturesfor robust brain tumour segmentation. In: International MICCAI brainlesionworkshop. Springer; 2017. p. 450 –62.[53] L. Perez, J. Wang, The effectiveness of data augmentation in image classi ﬁcation using deep learning, arXiv preprint arXiv:1712.04621.[54]Hua R, Huo Q, Gao Y, Sun Y, Shi F. Multimodal brain tumor segmentation usingcascaded v-nets. In: International MICCAI brainlesion workshop. Springer; 2018.p. 49–60.[55]Myronenko A. 3d mri brain tumor segmentation using autoencoder regularization.In: International MICCAI brainlesion workshop. Springer; 2018. p. 311 –20. [56] A. Cl/C18erigues, S. Valverde, J. Bernal, J. Freixenet, A. Oliver, X. Llad_o, Sunet: a deeplearning architecture for acute stroke lesion segmentation and outcome predictionin multimodal mri, arXiv preprint arXiv:1810.13304.[57]Zhou C, Ding C, Lu Z, Wang X, Tao D. One-pass multi-task convolutional neuralnetworks for effcient brain tumor segmentation. In: International conference onmedical image computing and computer-assisted intervention. Springer; 2018.p. 637–45.[58]Bengio Y, Louradour J, Collobert R, Weston J. Curriculum learning. In: Proceedingsof the 26th annual international conference on machine learning. ACM; 2009.p. 41–8.[59]Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S,Courville A, Bengio Y. Generative adversarial nets. In: Advances in neuralinformation processing systems; 2014. p. 2672 –80. [60]Yang H-Y, Yang J. Automatic brain tumor segmentation with contour awareresidual network and adversarial training. In: International MICCAI brainlesionworkshop. Springer; 2018. p. 267 –78. [61] Y. Huo, Z. Xu, S. Bao, C. Bermudez, H. Moon, P. Parvathaneni, T. K. Moyo, M. R.Savona, A. Assad, R. G. Abramson, et al., Splenomegaly segmentation on multi-modal mri using deep convolutional networks, IEEE Trans Med Imaging.[62]Isola P, Zhu J-Y, Zhou T, Efros AA. Image-to-image translation with conditionaladversarial networks. In: Proceedings of the IEEE conference on computer visionand pattern recognition; 2017. p. 1125 –34. [63]Chen L, Wu Y, DSouza AM, Abidin AZ, Wismüller A, Xu C. Mri tumor segmentationwith densely connected 3d cnn. In: Medical imaging 2018: image processing, vol.10574. International Society for Optics and Photonics; 2018. 105741F . [64]Rokach L. Ensemble-based classiﬁers. Artif Intell Rev 2010;33(1 –2):1–39. [65] M. Aygün, Y. H. S¸ahin, G. Ünal, Multi modal convolutional neural networks forbraintumor segmentation, arXiv preprint arXiv:1809.06191.[66]Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: asimple way to prevent neural networks from over ﬁtting. J Mach Learn Res 2014; 15(1):1929–58.[67]Jang J, Eo T-j, Kim M, Choi N, Han D, Kim D, Hwang D. Medical image matchingusing variable randomized undersampling probability pattern in data acquisition.In: 2014 international conference on electronics, information and communications(ICEIC). IEEE; 2014. p. 1 –2. [68]Douzas G, Bacao F. Effective data generation for imbalanced learning usingconditional generative adversarial networks. Expert Syst Appl 2018;91:464 –71. [69]Chawla NV, Bowyer KW, Hall LO, Kegelmeyer WP. Smote: synthetic minority over-sampling technique. J Artif Intell Res 2002;16:321 –57. [70]Shen H, Wang R, Zhang J, McKenna S. Multi-task fully convolutional network forbrain tumour segmentation. In: Annual conference on medical image understandingand analysis. Springer; 2017. p. 239 –48. [71]Sun Y, Kamel MS, Wong AK, Wang Y. Cost-sensitive boosting for classi ﬁcation of imbalanced data. Pattern Recognit 2007;40(12):3358 –78. [72]Milletari F, Navab N, Ahmadi S-A, V-net. Fully convolutional neural networks forvolumetric medical image segmentation. In: 2016 fourth international conferenceon 3D vision (3DV). IEEE; 2016. p. 565 –71. [73]Sudre CH, Li W, Vercauteren T, Ourselin S, Cardoso MJ. Generalised dice overlap asa deep learning loss function for highly unbalanced segmentations. In: Deeplearning in medical image analysis and multimodal learning for clinical decisionsupport. Springer; 2017. p. 240 –8. [74]Lin T-Y, Goyal P, Girshick R, He K, Doll /C19ar P. Focal loss for dense object detection. In: Proceedings of the IEEE international conference on computer vision. IEEE; 2017.p. 2980–8.[75]Chang PD, et al. Fully convolutional neural networks with hyperlocal features forbrain tumor segmentation. In: Proceedings MICCAI-BRATS workshop; 2016. p. 4 –9. [76]Georgiev N, Asenov A. Automatic segmentation of lumbar spine mri using ensembleof 2d algorithms. In: International workshop and challenge on computationalmethods and clinical applications for spine imaging. Springer; 2018. p. 154 –62. [77]
van Opbroek A, van der Lijn F, de Bruijne M. Automated brain-tissue segmentationby multi-feature svm classiﬁcation. In: Proceedings of the MICCAI workshops theMICCAI grand challenge on MR brain image segmentation; 2013. MRBrainS13 . [78] T. D. Bui, J. Shin, T. Moon, 3d densely convolutional networks for volumetricsegmentation, arXiv preprint arXiv:1709.03199.[79]Liu Y, Chen X, Peng H, Wang Z. Multi-focus image fusion with a deep convolutionalneural network. Inf Fusion 2017;36:191 –207.T. Zhou et al. Array 3-4 (2019) 100004
11