A study on deep learning algorithm performance on weed and cropspecies identiﬁcation under different image background
Sunil G Ca,C e n g i zK o p a r a na,M o h a m m e dR a j uA h m e da,Y uZ h a n ga, Kirk Howattb,X i nS u na,⁎
aDepartment of Agricultural and Biosystems Engineering, North Dakota State University, Fargo, ND 58108, USA
bDepartment of Plant Sciences, North Dakota State University, PO Box 6050, Fargo, ND 58108-6050, USA
abstract article info
Article history:Received 26 September 2022Received in revised form 8 November 2022Accepted 9 November 2022Available online 12 November 2022Weed identiﬁcation is fundamental toward developing a deep le arning-based weed control system. Deep learning algorithms assist to build a weed detection model by using weed and crop images. The dynamic environmentalconditions such as ambient lighting, moving cameras, or varying image backgrounds could affect the performance of deep learning algorithms. There are limited studies o n how the different image backgrounds would impact the deep learning algorithms for weed identi ﬁcation. The objective of this research was to test deep learning weed identiﬁcation model performance in images with potting mix (non-uniform) and black pebbled (uniform) back-grounds interchangeably. The weed and crop images were acquired by four canon digital cameras in the green-house with both uniform and non-uniform background conditions. A Convolutional Neural Network (CNN),Visual Group Geometry (VGG16), and Residual Network (ResNet50) deep learning architectures were used tobuild weed classiﬁcation models. The model built from uniform background images was tested on images witha non-uniform background, as well as model built from non-uniform background images was tested on imageswith uniform background. Results showed that the VGG16 and ResNet50 models built from non-uniform back-ground images were evaluated on the uniform background, achieving models' performance with an average f1-score of 82.75% and 75%, respectively. Conversely, the VGG16 and ResNet50 models built from uniform back-ground images were evaluated on the non-uniform background images, achieving models' performance with anaverage f1-score of 77.5% and 68.4% respectively. Both the VGG16 and ResNet50 models' performances were im-proved with average f1-score values between 92% and 99% when both uniform and non-uniform background im-ages were used to build the model. It appears that the mo del performances are reduced when they are tested with images that have different object background than the ones used for building the model.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Computer visionDeep learningImage backgroundNeural networkWeed classiﬁcation
1. IntroductionOne of the major causes of yield loss in agriculture is due to weeds.Oerke (2006)reported that weeds (34%) caused approximately doubleyield losses than that of animal pests (18%), and pathogens (16%)(Oerke, 2005). A recent study bySoltani et al. (2017)estimated that soybean yield would be reduced due to weed interference by approxi-mately 52%, which is equivalent to $16 billion annually in the US. Be-sides yield loss, weeds make harvesting dif ﬁcult, give undesired color and test on crops, and act as hosts for disease and pests ( Holzner, 1982). Hence, a number of studies have been performed to develop aprecision weed control system by the use of computer vision-baseddeep learning architecture, and robotics, which has been depicted in arecent review article byLi et al. (2022). Deep learning is the subﬁeldof machine learning, which is inspired by the arti ﬁcial neural network algorithm. It is one of the breakthroughs in the area of computer visionfor image classiﬁcation, object detection, and localization, which hasbeen effectively used in weed detection for robotic weed control re-search (Hasan et al., 2021).Deep learning architectures and algorithms were able to achieve near98% performance accuracy in weeds classi ﬁcation (Espejo-Garcia et al., 2020;Hu et al., 2020). However, most of the research is location-speci ﬁc, crop-speciﬁc, and weed-speciﬁc(Espejo-Garcia et al., 2020;Hasan et al., 2021;Khan et al., 2021;Olsen et al., 2019). The deep learning model built from a certain section of the weed and crop images may not be ableto perform well in another section due to the different soil types, lightningcondition, or image background situations. However, there is limited studyon weed classiﬁcation deep learning model evaluation on completely un- seen nature of images with completely different image backgrounds inthe area of sites-speciﬁc precision weed control images lacking generalityand robustness (Espejo-Garcia et al., 2020;Wu et al., 2021). Such type ofArtiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
⁎Corresponding author.E-mail address:xin.sun@ndsu.edu(X. Sun).
https://doi.org/10.1016/j.aiia.2022.11.0012589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/study is important because, during the deep learning computer visionmodel training, crop and weeds pixels along with soil background pixelsare used by convolutional neural network architecture to generalize theweed detection model (Kulawardhana, 2011). The changes in image pixels between the model training images and testing images might lead to themodel performance degradation (Velumani et al., 2021). A recent study cross evaluated the deep learning model in differentbackground condition in disease detection area of agriculture ( Ferentinos, 2018). In a previous study byFerentinos (2018), model built from labrotory condition images was tested on images obtained in ﬁeld, which caused de- cline in model performance due to having completely different backgroundimage pixel than that of labrotory images.There is higher probability of hav-ing different background in disease detection studies images than weed de-tection images, because weed detection images capture whole plantimages whereas, disease detection images capture s speci ﬁcr e g i o no f plant affected by disease. Similar studies are important to ﬁll the existing g a po nr e s e a r c ha b o u te f f e c t so fb a c k g r o u n di nw e e di d e n t i ﬁcation which could assist toward the development of robust real time deep learningweed detection model in agriculture industries.The objective of this study was 1) to build comprehensive weed clas-siﬁcation models using deep learning algorithms on weed species ofHorseweed, Palmer Amaranth, Redroot Pigweed, Kochia, Ragweed, andWaterhemp among crop species of Canola and Sugar beet; 2) to evaluatethe deep learning model performance on two different backgrounds ofthe acquired weed and crop image pictures in greenhouse condition.2. Material and methods2.1. Weed and crop greenhouse layout design, image acquisition, and imagelabelingThe crops and weeds were grown in 3.5 cubic inch size pots in agreenhouse environment for indoor data collection. The weed seeds ofHorseweed, Palmer Amaranth, Redroot Pigweed, Ragweed,Waterhemp, and Kochia and crop species of Canola and Sugar beetswere planted in the pots for weed identi ﬁcation study. The pots were randomly placed on a bench. The selected weed and crop species areprominent weed species and major crops of the Midwest region of theUnited States (Bryson and DeFelice, 2010). To create a two different image background scenario, the black lava pebble gravel material withan average of 2-5 mm diameter was placed at the surface of the pottingmix approximately a 6 mm thick layer to ensure proper surface cover-age. The average weight of the gravel material was 150 g for each squarepot.Fig. 1depict the weed and crop pots with potting mix and gravelmaterial to create background scenario-1 (non-uniform) in Fig. 1aa n d background scenario-2 (uniform) inFig. 1b, respectively. Four Canon EOS T7 digital cameras (Canon Inc., Tokyo, Japan) weremounted over the table in aﬁxed position at a height (≈48 ± 0.5 in.), where camera captures the total width of the bench. The reason forusing four cameras was to cover entire bench area (≈48.8 square foot). The cloud based automatic image acquisition system was usedto acquire weed and crop images. Images were captured for threedays with non-uniform background and uniform background to mini-mize the error that could cause due to the plant growth. The imageswere captured within three days of 30 March 2021 with natural lightingduring the day and artiﬁcial lighting during the evening in the green-house.After images were acquired, the next step was extracting the indi-vidual weed and crop species from the single image. A Python (PythonSoftware Foundation, Wilmington, DE) script was developed to label asingle image of a day using an OpenCV library ( Bradski, 2000), which saves the bounding box coordinate in order to perform automaticcropping of the remaining images for the same day. Before applyingthe python script, images were manually checked if there were any dis-turbances, such as human intervention, external objects in the image,and changed illumination. The images that included disturbances
Fig. 1.Image captured; (a) before adding gravel (non-uniform background), and (b) after adding the gravel (uniform background).S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
243were removed before processing the dataset. Fig. 2shows a detailed ﬂowchart of image labeling and cropping python script.After cropping the images for all the species, images were checked ifany plant changed its position or only a small part within the images;such images were removed from the datasets. Fig. 3shows the cropped images for all the species of weeds and crops for both non-uniformbackground and uniform background. The cropped images were labeledby making a directory for each species and placing crop images into therespective directory by the python script. Two directories were createdfor the 2868 images with non-uniform background datasets and 3488images with uniform background datasets. Table 1shows the detail of the number of each species images for both background scenario data.2.2. Crops and weed image classiﬁcation using convolutional neural networkIn this study, Convolutional Neural Networks (CNN) based VisualGeometry Group 16 (VGG16) deep learning architecture was used tocompare the classiﬁcation of the weed and crop images for the non-uniform background scenario (S1) and uniform background scenario(S2). VGG16 architecture already trained on ImageNet 1000 classeswas used toﬁnetune the model with weed and crop image datasets(Abdalla et al., 2019;Espejo-Garcia et al., 2020). VGG16 model was trained up to 50 epochs with 32 steps in each epoch. This technique iscalled transfer learning, where convolution and pooling layers were fro-zen and fully connected layers were modi ﬁed for new sets of problems. Instead of training and optimizing frozen layers, weights and biases,weight, and biases from previously trained VGG16 models on ImageNetdatasets were used. Only the fully connected layer was trained witheight classes as the output classes. Transfer learning was used becauseit makes training faster and is able to achieve outstanding performancewith smaller numbers of data (GC et al., 2021).Fig. 4a shows the com- plete block diagram for VGG16 architecture model layers used in thisstudy with their outputs and input dimension ( Simonyan and Zisserman, 2014). The complete diagram depicts the ResNet50 architec-ture with its trainable and frozen layers for the transfer learning ap-proach (He et al., 2015). There wereﬁve blocks of convolution and pooling layers that were frozen during the training. In VGG16 model,there were a total of 14,915,400 parameters, out of which 200,712 pa-rameters were trainable, and the remaining 14,714,688 parameterswere not trainable because only few outermost layers were trained intransfer learning model training approach.Similar to VGG16, CNN based Residual Networks 50 (ResNet50)deep learning architecture was also used to train the non-uniform back-ground scenario (S1) data, uniform background scenario (S2) data, andcombined-datasets scenario (C) obtained after merging both scenarios'data. For this training, a ResNet50 model already trained on theDeepWeeds image dataset byOlsen et al. (2019)was used. A transfer learning technique was used for ResNet50, where only 20,488 parame-ters were trained, and the remaining 23,583,616 parameters were fro-zen. ResNet50 was trained for 70 epochs, with 32 steps in each epoch.An outermost layer with 9 output classes and sigmoid activation func-tion was removed and a new outer layer was added for 8 output classesa with softmax activation function. Softmax assigns the decimal
Fig. 2.Flowchart showing labeling of each image for weeds and crops species from the single image captured by the camera.S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
244probabilities to each class of weeds and crops, where the class with thehighest probability was considered to be the predicted class ( Alzubaidi et al., 2021).VGG16 and ResNet50 model training and validation were performedon both S1, S2, and C datasets. Labeled images were randomly dividedinto the training, validation, and testing datasets for both scenario datain the ratio of 60%, 20%, and 20% respectively. There were 1719 and1149 number of crop and weed images for S1, respectively. Whereas,S2 datasets had 1984 and 1504 number of crop and weed images respec-tively. The more details on number of images for individual weed andc r o ps p e c i e si sb r i eﬂys h o w ni nTable 1.F i r s t l y ,am o d e lw i t hS 1a n dS 2 datasets was developed. Later, both training and validation datasetswere mixed, and a third model was developed with combined data tocreate C. A detailedﬂowchart for the steps of VGG16 and ResNet50
Fig. 3.Copped images of crop and weed plants from the image captured by canon camera after the application of labeling and cropping python scripts (a). Weed an dc r o pi m a g e sb e f o r et h e application of black gravel (non-uniform background) (b). Weeds and crop images after the application of black gravel (uniform background).S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
245model development, validation, and testing steps is depicted in Fig. 5. Adam optimizer (Kingma and Ba, 2015) and categorical cross-entropy were used as optimizer and cost function respectively for both VGG16and ResNet50 model training and validation. Training datasets wereonly augmented to increase the number of datasets, which also helpsto reduce the model overﬁtting when training with models with largernumbers of parameters. Image augmentation techniques used were re-scale, shear, shift,ﬂip, rotation, and zoom. Finally, the models developedfrom training and validation datasets were tested with the testingdatasets to evaluate the performance of the models. Both models werebuilt and tested by using deep learning TensorFlow ( TensorFlow.org) and Keras (keras.io) python Application Program Interface (API). Thetraining and testing were performed on desktop computer with Intel®Core™i7–3770 CPU @ 3.40 GHz processor, 8.00 GB RAM memory (Dell7040, Dell Technologies Inc., Round Rock, TX).2.3. Crops and weeds data analysisIn this study, to see the effects of background scenarios on model per-formance, models were tested with images with different backgroundthan the images used for building the model. Test datasets from both S1and S2 were tested on three models trained and validated with S1 model,S2 model, and C model datasets. This helped to cross-check the model per-formance when the test dataset's background scenario was completely dif-ferent from the scenario on which the model was developed.VGG16 and ResNet50 model performance was measured with preci-sion, recall (sensitivity), f1-score, and accuracy parameters. The f1-scoremetric was used over accuracy because it works well when there is animbalance in the number of images in each class ( Johnson and Khoshgoftaar, 2019). Accuracy is the number of correctly classi ﬁedTable 1The total number of training, validation, and testing images of crop and weed species fornon-uniform background scenario (S1) and uniform background scenario (S2) data.Training Validation Testing Total Grand total S1 S2 S1 S2 S1 S2 S1 S2 BothHorseweed 132 144 45 48 45 48 222 240 462PalmerAmaranth124 180 42 60 42 60 208 300 508Redroot Pigweed 62 87 22 30 22 30 106 147 253Ragweed 140 191 47 65 47 65 234 321 555Waterhemp 134 182 45 62 45 62 224 306 530Canola 522 640 175 214 175 214 872 1068 1940Kochia 93 114 31 38 31 38 155 190 340Sugar beets 507 548 170 184 170 184 847 916 1763Total 1714 2086 577 701 577 701 2868 3488 6356
Fig. 4.VGG16 and ResNet50 architecture showing input layer, convolution layer, pooling layer, and output layer. Frozen layers parameters are not trained, and trainable layer parameters are optimized during the training. (a). VGG16 architecture (b). ResNet50 architecture.
Fig. 5.Diagram showing VGG16 and ResNet50 model development ﬂowchart for training, validation, and testing steps.S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
246data instances over the total number of data instances. Accuracy maynot be a good measure when datasets are imbalanced because itmight not show a clear picture of model performance. However, thef1-score, which is also the geometric mean of the precision and recalldepict clear pictures of model performance in terms of each class. Preci-sion is the percentage of correctly predicted images out of the totalnumber of predicted images, whereas recall is the percentage of cor-rectly predicted images out of the total number of images in the actualclass. Ideally, for good classiﬁers, both precision and recall values shouldbe 1, but they counter each other, and increasing one usually reducesthe other (Arya et al., 2020). The macro and weighted average were cal-culated for the precision, recall, and f1-score. A confusion matrix wasalso used to visualize the summary of the prediction results of themodel. For calculating all the metrics and visualizing the confusion ma-trix, python sklearn API was used (Pedregosa et al., 2011). The decline in model performance for S1 and S2 was obtained by cal-culating the percentage of error (PE). It was calculated by ﬁnding the difference between the f1-score or accuracy value of the model whenit was tested on the same background scenario test datasets and the dif-ferent background scenario test data sets. Furthermore, the percentageof improvement (PI) was calculated for both background scenariomodels when the models were built with combined datasets. The signif-icance of the percentage of error PE and PI was also tested with a one-tailed pairedt-test.3. Results3.1. The model performances from training and validation steps for cropand weed classiﬁcationThe model performance evaluation of VGG16 and ResNet50 modelhas been made for S1, S2, and C datasets during the model trainingand validation to check the model generalization. The training and val-idation accuracy of VGG16 model was 97% for both S1 and S2 data. Fig. 6 shows the training accuracy, validation accuracy, training loss, and val-idation loss from the VGG16 and ResNet50 model training. When theepoch was increased from epoch 1 to 50 as training continued, trainingloss decreased from 1.6501 to 0.0469, training accuracy increased from54.42% to 97.85%, validation loss decreased from 0.6675 to from 0.0176,and validation accuracy increased from 75% to 99.02%. It appeared thatthe accuracy was increased, and loss was decreased for VGG16-S1model (Fig. 6). A similar trend was found for the VGG16-S2 model. Forthe VGG16-S2 model training (Fig. 6), training loss decreased from 1.8856 to 0.0262, training accuracy increased from 49.02% to 99.22%,validation loss decreased from 0.6767 to 0.0562; validation accuracy in-creased from 79.10% to 97.85%. This showed validation accuracy washigher for the VGG16-S1 model than the VGG16-S2 model.Similar to the VGG16, training and validation accuracy was in-creased, and training and validation loss was decreased for ResNet50model training for both background scenarios, when epoch was in-creased from 1 to 70 (Fig. 6). During the ResNet50-S1 model training,training loss decreased from 1.3279 to 0.0698, training accuracy in-creased from 57.62% to 98.63%; validation loss decreased from 0.9317to 0.1018, and validation accuracy increased from 68.36% to 97.27%.Whereas, during the ResNet50-S2 model training, training loss de-creased from 1.3578 to 0.1217, training accuracy increased from55.27% to 97.57%,validation loss decreased from 0.9924 to 0.1372, andvalidation accuracy increased from 64.65% to 95.31%.This showed bothS1 and S2 ResNet50 models achieved a validation accuracy of greaterthan 95%. However, the ResNet50-S1 model achieved higher validationaccuracy than the ResNet50-S2 model.Combined datasets from both S1 and S2 were used to train and val-idate the VGG16 and ResNet50 models. During the VGG16 model train-ing on VGG16-C model (Cmodel) from epoch 1 to 50, as trainingcontinued accuracy and loss values changed as shown in Fig. 6.T r a i n i n g loss decreased from 1.9313 to 0.0873, training accuracy increased from51.95% to 97.66%, validation loss decreased from 1.0262 to 0.0339, andvalidation accuracy increased from 70.51% to 98.24%. Similarly, forResNet50-C model development, as the epoch increased from 1 to 70,training loss decreased from 1.2242 to 0.1380, training accuracyincreased from 56.84% to 95.31%, validation loss decreased from1.0244 to 0.1563, and validation accuracy increased from 64.26% to93.55%. Training and validation accuracy for both VGG16-C andResNet50-C models were obtained above 95% (ResNet50: epoch 66).VGG16-C model training and validation accuracy were higher thanResNet50-C model for weed, and crop datasets used in this study. Allsix training graphs inFig. 6shows model was not overﬁtted during the training steps.3.2. The VGG16 model prediction performance on test images for weed andcrop classiﬁcationFor deep learning model performance evaluation, precision, recall,
f1-score, and confusion matrix were used to depict the VGG16 modelperformance on S1 test data (S1tedata) and S2 test data (S2tedata).The VGG16-S1, VGG16-S2, and VGG16-C models were tested on bothS1tedata and S2tedata.Table 2shows the performance of the VGG16-S1 model in terms of precision, recall, and f1-score. Model performancewas the highest with an average f1-score of 99% when the VGG16-S1model was tested with S1tedata. This result would be expected becausethe model learned from images with similar backgrounds. However,when the VGG16-S1 model was tested with S2tedata, the macro aver-age f1-score declined to 83% and the weighted average f1-score de-clined to 87%. Among two crops and six weed species, the top threespecies for which f1-score severely declined were Redroot Pigweed(100% to 46%), Horseweed (100% to 79%), and Palmer Amaranth (99%to 80%). Similar type of performance degradation was observed byFerentinos (2018)in plant disease detection when test images werechanged, where model success rate was 32% to 66% when model trainedon laboratory andﬁeld data was tested withﬁeld and laboratory data, respectively. The decline in the f1-score value can be attributed to thelow recall value, which was due to the classi ﬁcation of a smaller number of Redroot Pigweed images correctly out of the total number of imagesin the actual class.Fig. 7a and b shows the confusion matrix for the VGG16-S1 model,which underpin the results inTable 2. When VGG16-S1 model was tested with S1tedata, 100% of images were predicted correctly for allclasses except Sugar beet with 96.5% accuracy. However, whenVGG16-S1 model was tested with S2tedata, prediction accuracy de-clined from 98.96% to 88% due to a sharp decline in the prediction ofRedroot Pigweed (100% to 33%) and Horseweed (100% to 66.7%),which is clearly depicted inFig. 7b. While investigating the S1modelS2tedatad confusion matrix, 14.6% of Horseweed wasmisclassiﬁed as Ragweed and Canola. Whereas 33% and 20% of RedrootPigweed were misclassiﬁed as Canola and Sugar beet, respectively.These misclassiﬁcations contributed in VGG16-S1 model performancedegradation when the model was tested with S2tedata. The RedrootPigweed was not promising, which may be due to the different back-ground scenarios and lower number of training images. The 10% of Red-root Pigweed was misclassiﬁed as Palmer Amaranth, which may be dueto the similar morphological characteristics of their leaves ( Ma et al., 2015).The VGG16-S2 model was tested with S1tedata and S2tedata ofwhich performance is depicted inTable 3. The VGG16-S2 model perfor- mance seems promising with macro and weighted average f1-scorevalue of 98% and 99%, respectively for S2tedata. For S2tedata testing,out of eight classes, Waterhemp had the lowest f1-score value of 95%whereas Canola and Ragweed had the highest f1-score value of 100%.While investigating the model performance on confusion matrix(Fig. 7c) for S2tedata, all the classes were predicted 100% correctly ex-cept Redroot Pigweed, Kochia, and Sugar beet with 93.3%, 97.4%, and96.2% accuracies, respectively. However, the VGG16-S1 model did notS. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
247Fig. 6.Training and validation progress during the VGG16 and ResNet50 model development showing training and validation accuracy (train_acc, valid_acc) and loss (train_loss,val_loss). VGG16_S1 is the VGG16 model trained on background scenario-1 (S1) data, VGG16_S2 is the VGG16 model trained on background scenario-2 (S2) data, VGG16 _C is the VGG16 model trained on combined (C) data from both background scenarios, ResNet50_S1 is the ResNet50 model trained on S1 data, ResNet50_S2 is the ResNet50 model t rained on S2 data, and ResNet50_C is the ResNet50 model trained on combined (C) data from both background scenarios.
Table 2VGG16-S1 model performance in terms of precision, recall, and f1-score metrics from the test on S1tedata ( S1modelS1tedata) and S2tedata (S1modelS2tedata
).precision recall f1-score number of test dataS1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedataHorseweed 1.00 0.97 1.00 0.67 1.00 0.79 45 48Palmer Amaranth 0.98 0.77 1.00 0.83 0.99 0.80 42 60Redroot Pigweed 1.00 1.00 1.00 0.30 1.00 0.46 22 30Ragweed 1.00 0.86 1.00 1.00 1.00 0.92 47 65Waterhemp 1.00 0.93 1.00 0.90 1.00 0.92 45 62Canola 0.97 0.88 1.00 0.98 0.99 0.93 175 214Kochia 1.00 0.94 1.00 0.89 1.00 0.92 31 38Sugar beet 1.00 0.88 0.96 0.88 0.98 0.88 170 184macro avg 0.99 0.90 1.00 0.81 0.99 0.83 577701 weighted avg 0.99 0.89 0.99 0.88 0.99 0.87 577701S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
248Fig. 7.VGG16 model test results in terms of the confusion matrix. (a). S1model tested with S1tedata (b). S1model tested with S2tedata (c). S2model tested wit h S2tedata (d). S2model tested with S1tedata (e). Cmodel tested with S1tedata (f). Cmodel tested with S2tedata (HW: Horseweed, PA: Palmer Amaranth, RRPW: Redroot Pigweed, R W: Ragweed, WH: Waterhemp, SUB: Sugar beet).S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256perform well for S1tedata with macro and weighted average f1-scorevalue of 78% and 75%, respectively. For S2modelS1tedata testing, Red-root Pigweed, Sugar beet, and Palmer Amaranth had the lowest f1score value of 57%, 57%, and 66% respectively. F1-score value waslower in Redroot Pigweed and Sugar beet due to low recall values of45% and 40%, respectively; however, Palmer Amaranth had lower f1-score value due to low precision value of 54%.Moreover, while looking into the confusion matrix for S2modelS1tedata(Fig. 7d), the prediction accuracy decreased severely to 45.5% for RedrootPigweed and 40% for Sugar beet. The 31.8% of all the Redroot Pigweed and Sugar beet images (31.8%) were misclassi ﬁed as Waterhemp and Ca- nola, respectively. This may be due to the different test images backgroundscenario, a low number of Redroot Pigweed images, and similar morphol-ogy between the Redroot Pigweed and Waterhemp.The VGG16-C model was tested with S1tedata and S2tedata of whichthe model performance is clearly depicted in Table 4. The VGG16-C model performance seems promising with both macro average andweighted average f1-score values over 98% for both S1tedata andS2tedata. For S1tedata, f1-score was either 99% or 100% for eight classes,however, for S2tedata, the f1-score value was between 96% (Kochia)and 100% (Ragweed). Moreover,Fig. 7ea n dFig. 7f show the confusion matrix for CmodelS1tedata and CmodelS2tedata testing. For S1tedata,all the classes were predicted 100% correctly except Sugar beet(98.2%), whereas for S2tedata, Redroot Pigweed, Ragweed, Waterhemp,and Canola were predicted 100% correctly. The lowest prediction per-centage of the S2tedata was for Horseweed images (94%). Model perfor-mance was improved when combined datasets were used to train andvalidate the model. Researchers have suggested using a larger datasetwith a large variety of images while training the deep learning model(Hasan et al., 2021).3.3. The Resnet50 model prediction performance on test images for weedand crop classiﬁcationThe ResNet50 model was also used in this study to see if the modelperformance under different background conditions follows the similarpattern as that of VGG16. The model trained from S1 data (S1model)was tested with both S1 data (S1tedata) and S2 data (S2tedata).Table 5shows the precision, recall, and f1-score for ResNet50-S1model tested on both S1tedata and S2tedata. When the ResNet50-S1 model was tested with S1tedata, the model performed well with aweighted and macro average f1-score of 98%. However, whenResNet50-S1 model was tested with S2tedata, model performance de-clined sharply with macro and weighted average f1-score of 75% and80%. While drilling down intoTable 5, S1modelS1tedata had f1-score of 100% for Horseweed, Ragweed, and Kochia, however none ofS1modelS2tedata classes had 100% f1-score. S1modelS1tedata RedrootPigweed had the lowest f1-score value of 93%.In S1modelS2tedata, Ragweed had the highest f1-score of 96% andPalmer Amaranth had the lowest f1-score of 44%, where the bottom-3classes with the lowest f1-score were Palmer Amaranth (44%), RedrootPigweed (49%), and Waterhemp (75%). Palmer Amaranth had 94% pre-cision but 28% recall value, which had caused severe decline in f1-score.However, Redroot Pigweed had both lower precision (63%) and recall(40%) value causing sharp decline in f1-score in S2tedata from that ofS1tedata. The pattern of degradation in the model performance is simi-lar for both VGG16 and ResNet50 models when the background of thetest data was different from the background of the trained model data.The clearer picture of the ResNet50-S1 model performance onS1tedata and S2tedata is depicted inFig. 8a and b confusion matrix. For S1modelS1tedata, the model predicted Horseweed, Palmer Amaranth,Ragweed, and Kochia 100% correctly. The lowest prediction percentagewas obtained for Redroot Pigweed (90.9%), out of which 9.09% of imageswere predicted incorrectly as Sugar beet. However, the S1modelS2tedataconfusion matrix did not have classes with 100% prediction accuracy,where Horseweed had the highest prediction accuracy of 97.9% andPalmer Amaranth had the lowest prediction accuracy of 28.3%. More-over, Redroot Pigweed (40%) also had the poorest performance afterPalmer Amaranth. For both Palmer Amaranth and Redroot Pigweed, alarge percentage of images were wrongly predicted as Sugar beetscausing lower prediction accuracy (Fig. 8b). Besides these species, the remaining species had a prediction accuracy in between 79% and 98%.Table 3The VGG16-S2 model performance in terms of precision, recall, and f1-score metrics for S1tedata ( S2modelS1tedata) and S2tedata (S2modelS2tedata
).precision recall f1-score number of test dataS2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedataHorseweed 0.98 0.80 1.00 0.91 0.99 0.85 48 45Palmer Amaranth 0.98 0.54 1.00 0.86 0.99 0.66 60 42Redroot Pigweed 1.00 0.77 0.93 0.45 0.97 0.57 30 22Ragweed 1.00 0.92 1.00 1.00 1.00 0.96 65 47Waterhemp 0.90 0.70 1.00 1.00 0.95 0.83 62 45Canola 1.00 0.74 1.00 1.00 1.00 0.85 214 175Kochia 1.00 1.00 0.97 0.84 0.99 0.91 38 31Sugar beet 1.00 1.00 0.96 0.40 0.98 0.57 184 170macro avg0.980.81 0.98 0.81 0.98 0.78 701 577 weighted avg 0.99 0.83 0.99 0.78 0.99 0.75 701 577
Table 4The VGG16-C model performance in terms of precision, recall, and f1-score metrics for S1tedata ( CmodelS1tedata) and S2tedata (CmodelS2tedata
).precision recall f1-score number of test dataCmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedataHorseweed 1.00 0.98 1.00 0.94 1.00 0.96 45 48Palmer Amaranth 0.98 0.97 1.00 0.97 0.99 0.97 42 60Redroot Pigweed 1.00 0.97 1.00 1.00 1.00 0.98 22 30Ragweed 1.00 1.00 1.00 1.00 1.00 1.00 47 65Waterhemp 0.98 0.98 1.00 1.00 0.99 0.99 45 62Canola 0.99 0.99 1.00 1.00 1.00 0.99 175 214Kochia 1.00 0.95 1.00 0.97 1.00 0.96 31 38Sugar beets 1.00 0.99 0.98 0.97 0.99 0.98 170 184macro avg 0.99 0.98 1.00 0.98 1.00 0.98 577701 weighted avg 0.99 0.98 0.99 0.98 0.99 0.98 577701S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
250The ResNet50-S2 model performance for both S1tedata and S2tedatain terms of f1-score is depicted inTable 6. When the ResNet50-S2 model was evaluated on S2tedata, the macro average f1-score and weightedf1-score was found to be 95% and 96%, respectively. However, whenthe model was evaluated on S1tedata, performance declined signi ﬁ- cantly with a macro and weighted f1-score value of 68% and 63%.There could be several reasons for this, but the major reason behindthis was due to completely different backgrounds on test images thanthat of training images. When evaluating the ResNet50-S2 model withS2tedata, the highest f1-score value was obtained for Horseweed(100%) and the lowest f1-score value was obtained for Redroot Pigweed(87%). The severe decline in performance of the ResNet50-S2 model onS1tedata was due to the signiﬁcant decline of f1-score value in Sugarbeet, Palmer Amaranth, and Redroot Pigweed below 45% ( Table 6). For Sugar beet, the decline in the f1-score value is due to the declineof the recall value to 21%, whereas for Redroot Pigweed and Palmer Am-aranth, both the precision and recall values declined below 51%.The confusion matrix of the ResNet50-S2 model for S2tedata andS1tedata is shown inFig. 8c and d, respectively. For S2modelS2tedata,Horseweed, Waterhemp, and Canola test images were predicted 100%correctly, where all the classes had greater than 90% prediction accu-racy, except Palmer Amaranth (80%). For Palmer Amaranth, 6.67% ofthe images were misclassiﬁed as Sugar beet and 5% of the imageswere misclassiﬁed as Redroot Pigweed. When the ResNet50-S2 modelwas evaluated on the S1tedata, the model performance declined com-pared to the S2tedata. In the confusion matrix for S1tedata in Fig. 8d, Sugar beet prediction accuracy was signiﬁcantly lower due to misclassi- ﬁcation of Sugar beet images as Canola. Similarly, the prediction accu-racy for Palmer Amaranth and Redroot Pigweed was below 51%.However, Horseweed, Canola, and Ragweed had an outstanding perfor-mance with a prediction accuracy over 97%. The overall pattern of per-formance degradation on ResNet50-S2 model was similar to that ofResNet50-S1 model when the model was tested on different back-ground test data.The ResNet50-C model was evaluated on both S1tedata and S2tedatato see the performance difference in CmodelS1tedata andCmodelS2tedata.Table 7shows the precision, recall, and f1-score valuesfor ResNet50-C model when tested with both S1tedata and S2tedata.Macro average f1-score for both S1tedata and S2tedata was obtainedas 93% and weighted average f1-score was obtained as 95% for bothtest data. Therefore, there was no signiﬁcant performance difference when using two test data because the model was trained using bothbackground scenario training and validation data. For CmodelS1tedata,f1-score values for eight classes range in between 79% (Palmer Ama-ranth) and 100% (Horseweed, Ragweed, and Kochia). Similarly, inCmodelS2tedata, f1-score value was in between 79% (Palmer Ama-ranth) and 100% (Horseweed). Palmer Amaranth had the lowest f1-score due to having recall value below 70% for CmodelS1tedata andCmodelS2tedata.Furthermore,Fig. 8e and f shows the confusion matrix with percent-age of the images correctly classiﬁed and misclassiﬁed for ResNet50-Cmodel. For ResNet50 CmodelS1tedata, 100% of the Horseweed, Rag-weed, and Kochia images were classiﬁed correctly. Palmer Amaranth had the poorest prediction accuracy of 69% and most of Palmer Ama-ranth (14.25%) images were misclassiﬁed as the Waterhemp. In addi- tion to Palmer Amaranth, Redroot Pigweed also had poor predictionaccuracy of 72.7% because 22.7% of the images were misclassi ﬁed as Sugar beet. However, for ResNet50 CmodelS2tedata, only Palmer Ama-ranth had poor performance with prediction accuracy of 65% ( Fig. 8f). Remaining classes had prediction accuracy of over 96% percent exceptRedroot Pigweed with prediction accuracy of 86.7%. This shows overallprediction accuracy was impressive when both background data sce-narios were mixed.3.4. Model's comparisons and statistical signi ﬁ
cance under two background scenariosModel performance statistical comparison based on average f1-scores and average accuracies was performed. The average accuracy ofthe six testing scenarios for both VGG16 and ResNet50 is depicted inFig. 9. The graph exhibits the performance degradation of S1modeland S2model in both deep learning CNN architectures when themodel was tested on test data with different background from thedata used in building a model. The graph in Fig. 9shows how each clas- ses' accuracy deviated from the average accuracy for both CNN architec-tures. There was higher deviation in the accuracy in the classes from themean for both S1 model and S2 model when the model was tested ontest data with a different background from the model building data.Conversely, S1 model and S2 model tested on test data with a similarbackground as the model building data performed well, and the devia-tion from mean accuracy was quite small.The percentage of model performance degradation is depicted inTable 8with its statistical signiﬁcance. In the VGG16 model, mean accu-racy declined by 18.97% for S1 model and 17.88% for S2 model whenmodels were tested on test data with different background scenariosfrom the model building data. The paired- t-test results for both VGG16-S1 model (t(7) = 2.3,p= 0.027), and VGG16-S2 model (t(7) = 2.24,p= 0.03) showed that the differences in mean accuracieswere signiﬁcant at an alpha level of 0.05. The accuracy model resultsshowed that VGG16-S2 model was able to generalize well oncompletely different sets of test data. However, accuracy may not givean exact picture of the model performance since model performancewas compared based on f1-score to make a conclusion. The ResNet50model also shows similarities with VGG16 model. The average accuracyof the ResNet50-S1 model and ResNet50-S2 model declined by 23.52%and 25.56% for S2tedata and S1tedata, respectively. These values sug-gest a greater decline of the model performance in ResNet50 than theVGG16.The average accuracy explains the percentage of the correct classi ﬁ- cation of images but does not depict the level of wrong classi ﬁcation of images into certain classes. Hence, the average f1-score was used as themain performance metric. The average f1-score error bar plot in Fig. 10Table 5The ResNet50-S1 model performance in terms of precision, recall, and f1-score metrics for S1tedata ( S1modelS1tedata) and S2tedata (S1modelS2tedata
).precision recall f1-score number of test dataS1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedata S1modelS1tedata S1modelS2tedataHorseweed 1.00 0.85 1.00 0.98 1.00 0.91 45 48Palmer Amaranth 0.95 0.94 1.00 0.28 0.98 0.44 42 60Redroot Pigweed 0.95 0.63 0.91 0.40 0.93 0.49 22 30Ragweed 1.00 1.00 1.00 0.92 1.00 0.96 47 65Waterhemp 1.00 0.72 0.91 0.79 0.95 0.75 45 62Canola 0.98 0.96 0.99 0.83 0.99 0.89 175 214Kochia 1.00 0.72 1.00 0.82 1.00 0.77 31 38Sugar beet 0.98 0.68 0.99 0.94 0.98 0.79 170 184macro avg 0.98 0.81 0.97 0.75 0.98 0.75 577701 weighted avg 0.98 0.84 0.98 0.81 0.98 0.80 577701S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
251Fig. 8.The ResNet50 model test results in terms of confusion matrix. (a). S1model tested with S1tedata (b). S1model tested with S2tedata (c). S2model tested with S2tedata (d). S2model tested with S1tedata (e). Cmodel tested with S1tedata (f). Cmodel tested with S2tedata (HW: Horseweed, PA: Palmer Amaranth, RRPW: Redroot Pigweed, R W: Ragweed, WH: Waterhemp, SUB: Sugar beet).S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256depicts the pattern of decline in the performance for both VGG16 andResNet50 models when the models were tested on test data with differ-ent backgrounds than that of the model building data. Besides this, therewas higher deviation of the f1-score values of classes from the mean forboth S1model and S2model when the model was tested on test datawith a different background from the model building data. Fig. 10also shows a lower performance of the ResNet50-S1 model and ResNet50-S2 model than that of the VGG16.Table 8shows the average percentage of decline in f1-scores whenmodelsVGG16-S1, VGG16-S2, ResNet50-S1, and ResNet50-S2 weretested on test data with a different background from the model buildingdata. The average f1-score of VGG16-S1 model and VGG16-S2 modeldeclined by 16.83% and 21.22%, respectively, whereas the average f1-score of the ResNet50-S1 model and ResNet50-S2 model declined by23.37% and 28.40%, respectively. This means S1model was more gener-alized than that of S2model in both VGG16 and ResNet50 because of itshigher performance on unseen test data with a different background.Moreover, the paired-t-test results shows that the differences in f1-scores between models VGG16-S1 (t(7) = 2.95, p= 0.01), VGG16-S2 (t(7) = 3.99,p= 0.0026), ResNet50-S1 (t(7) = 3.689, p= 0.0039), and ResNet50-S2 (t(7) = 3.42,p= 0.0056) were signiﬁcant using an alpha level of 0.05 (Table 8).The decline in model performance between VGG16-S1, VGG16-S2,ResNet50-S1, and ResNet50-S2 was addressed by the training of themodel using both background scenarios training data to develop theVGG16-C model and ResNet50-C model. The improvement of themodel performance of Cmodel from S1model and S2model is depictedfor both VGG16 and ResNet50 models in Fig. 9andFig. 10. However,Table 6The ResNet50-S2 model performance in terms of precision, recall, and f1-score metrics for S1tedata ( S2modelS1tedata) and S2tedata (S2modelS2tedata
).precision recall f1-score number of test dataS2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedata S2modelS2tedata S2modelS1tedataHorseweed 1.00 0.85 1.00 1.00 1.00 0.92 48 45Palmer Amaranth 1.00 0.41 0.80 0.48 0.89 0.44 60 42Red Root Pigweed 0.84 0.33 0.90 0.50 0.87 0.40 30 22Ragweed 1.00 1.00 0.98 0.98 0.99 0.99 65 47Waterhemp 0.95 0.83 1.00 0.64 0.98 0.73 62 45Canola 0.96 0.58 1.00 0.99 0.98 0.73 214 175Kochia 0.97 0.96 0.97 0.87 0.97 0.92 38 31Sugar beet 0.96 1.00 0.95 0.21 0.96 0.34 184 170macro avg 0.96 0.81 0.95 0.81 0.95 0.68 701 577 weighted avg 0.96 0.83 0.96 0.78 0.96 0.63 701 577
Table 7The ResNet50-C model performance in terms of precision, recall, and f1-score metrics for S1tedata ( CmodelS1tedata) and S2tedata (CmodelS2tedata
).precision recall f1-score number of test dataCmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedata CmodelS1tedata CmodelS2tedataHorseweed 1.00 1.00 1.00 1.00 1.00 1.00 45 48Palmer Amaranth 0.94 1.00 0.69 0.65 0.79 0.79 42 60Redroot Pigweed 0.89 0.90 0.73 0.87 0.80 0.88 22 30Ragweed 1.00 1.00 1.00 0.98 1.00 0.99 47 65Waterhemp 0.86 0.91 0.93 0.97 0.89 0.94 45 62Canola 0.96 0.97 0.99 1.00 0.97 0.98 175 214Kochia 1.00 0.95 1.00 0.97 1.00 0.96 31 38Sugar beet 0.94 0.90 0.97 0.97 0.95 0.93 170 184macro avg 0.95 0.95 0.91 0.93 0.93 0.93 577 701weighted avg 0.95 0.95 0.95 0.95 0.95 0.95 577 701
Fig. 9.The VGG16 and ResNet50 average accuracy error bar graph plot for six types of testing: S1modelS1tedata: S1model tested on S1tedata, S1modelS2tedata ; S1model tested on S2tedata, S2modelS2tedata; S2model tested on S2tedata S2modelS1tedata; S2model tested on S1tedata, CmodelS1tedata; Cmodel tested on S1tedata, a nd CmodelS2tedata; Cmodel tested on S2tedata.S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
253the deviation of the accuracy and f1-score from the mean was higher forthe ResNet50 than the VGG16 model. Moreover, the performance im-provement is clearly depicted onTable 9for both VGG16 and ResNet50 in terms of accuracy and f1-score. The improvement of the model per-formance of the VGG16-C model on S2tedata were 18.28% in terms off1-score and 21.66% in terms of accuracy from the VGG16-S1 model.Whereas the improvement of the model performance of the VGG16-Cmodel on S1tedata were 28.55% in terms of f1-score and 23.52% interms of accuracy from the VGG-S2 model. The performance improve-ment pattern in the ResNet50 model was also same as that of VGG16.The improvement of the model performance of the ResNet50-C modelon S2tedata were 24.5% in terms of f1-score and 24.20% in terms of ac-curacy from the ResNet50-S1 model. Whereas the improvement of themodel performance of the ResNet50-C model on S1tedata were35.28% in terms of f1-score and 29.04% in terms of accuracy from theResNet50-S2 model. The performance improvement was higher for S2model than S1model for both VGG16 and ResNet50, which was ex-pected as the S2 model performance was lower than the S1 model inboth VGG16 and ResNet50. The model developed from non-uniform(S1) image background dataset performed better than uniform (S2)image background dataset. When the model from combined imagebackground dataset was tested on uniform and non-uniform testdatasets, the weed identiﬁcation performance improved better at non-uniform image background test dataset.4. DiscussionIn this research article, an applications of deep learning architectureto study the performance of the model with different image backgroundconditions under greenhouse condition was performed on weed andcrop classiﬁcation task. Same crop and weed plants were planted inpots and two different image backgrounds scenarios as non-uniformbackground scenario (S1) and uniform background scenario (S2) wascreated. The Deep Learning (DL) models that were developed fromthe two scenarios was compared in terms of developing a more robustmodel and to understand how the models perform on different imageanalysis conditions than the condition the model was created. The DLrobustness was checked for completely unseen images with differentimage background than that of images used for model training. The re-sults conﬁrmed that model performance declined in both types of back-ground scenarios when model was cross-tested on images withdifferent background scenario than that of images used during modeltraining. In weed classiﬁcation study,Espejo-Garcia et al. (2020)per- formed plant segmentation to check if deep learning model afterTable 8Model performance degradation when model was tested on completely different back-ground scenario data; n is the number of classes, SD is the standard deviation, PE is per-centage of error when S1model and S2model model were tested with differentbackground test data, andp-value gives the signiﬁcance of the model performance degra- dation using paired-t-test.n Mean(%)SD(%)n Mean(%)SD(%)PE(%)p-valueVGG16S1modelS1tedata S1modelS2tedataF1-score 8 99.50 0.71 8 82.75 15.86 16.83 0.011Accuracy 8 99.56 1.16 8 80.66 22.88 18.97 0.027S2modelS2tedata S2modelS1tedataF1-score Accuracy8 98.38 1.69 8 77.5 15.31 21.22 0.0038 98.36 2.53 8 80.78 24.37 17.88 0.030ResNet50S1modelS1tedata S1modelS2tedataF1-score Accuracy8 97.88 2.59 8 75 19.09 23.37 0.0048 97.46 4.02 8 74.54 25.95 23.52 0.018S2modelS2tedata S2modelS1tedataF1-score Accuracy8 95.50 4.81 8 68.38 25.85 28.40 0.0068 95.13 6.10 8 70.81 29.70 25.56 0.017
Fig. 10.The VGG16 and ResNet50 accuracy f1-score error bar graph plot for six types of testing. S1modelS1tedata: S1model tested on S1tedata, S1modelS2tedat a; S1model tested on S2tedata, S2modelS2tedata; S2model tested on S2tedata S2modelS1tedata; S2model tested on S1tedata, CmodelS1tedata; Cmodel tested on S1tedata, a nd CmodelS2tedata; Cmodel tested on S2tedata.Table 9Model performance improvement when model was tested on combined data from bothbackground scenarios; n is the number of classes, SD is the standard deviation, PI is thepercentage of improvement when model was developed with combined dataset of twobackground scenario, andp-value gives the signiﬁcance of the model performance im- provement using paired-t-test.n Mean (%) SD (%) n Mean (%) SD (%) PI (%)VGG16S1modelS2tedata CmodelS2tedataF1-score 8 82.75 15.86 8 97.88 1.46 18.28Accuracy 8 80.66 22.88 8 98.15 2.27 21.66S2modelS1tedata CmodelS1tedataF1-score Accuracy 8 77.5 15.31 8 99.63 0.52 28.558 80.78 24.37 8 99.78 0.64 23.52ResNet50S1modelS2tedata CmodelS2tedataF1-score Accuracy 8 75.00 19.09 8 93.38 6.97 24.58 74.54 25.95 8 92.58 11.90 24.20S2modelS1tedata CmodelS1tedataF1-score Accuracy 8 68.38 25.85 8 92.5 8.83 35.288 70.81 29.70 8 91.375 12.90 29.04S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
254background removal performs better and found that result did not im-prove. The study performed in this article is different from the studyperformed byEspejo-Garcia et al. (2020), where different background condition was created by the application of the black pebble on thetop of the promix mixture. In this study, two different background im-ages was created by application of black gravels in same crop andweed plants which is different fromEspejo-Garcia et al. (2020)study. Beside the background in image, black gravel also affects the lightingaround the plant due to more absorption of light by black surfacesthan other color surfaces (Stuart-Fox et al., 2017). The study's results performed for plant disease detection by Ferentinos (2018)was in cor- respondence to our study results in weed classi ﬁcation. In their re- search, model performance decline from 99% to 68% when modeltrained withﬁeld condition image was tested with laboratory-conditions images. The performance decline even more to 33% whentypes of images for model training and testing was reversed.The VGG16S1 model (model trained on non-uniform (S1) data) interms of f1-score declined by 16.83% when tested with S2tedata (uni-form (S2) test data), whereas the VGG16 S2model (model trained onuniform (S2) data) performance declined by 21%, when the modelwas tested with S1tedata (non-uniform (S1) test data). Similar patternof declination was observed in ResNet50, where ResNet50-S1 modelperformance in terms of F1-score declined by 23.37% and ResNet50-S2model performance declined by 28.40%. This decline in performancewas due to testing the models on image sets with different backgroundsthan that of images used in model training. This result also shows thecause of difﬁculty of the implementation of deep learning models inreal time weed detection applications due to different soil backgroundconditions such as color and texture. The model's performance was im-proved by 18% to 35% in both CNN architectures when both types of im-ages were used to build a model. This suggests to used wider variety oftraining weed and training data collected from different geographicalarea, cultivation condition, imaging sensors, and image capturingmode as suggested byFerentinos (2018)in plant disease detection study.In addition, studies in research and reviews on the use of digital im-ages on weed identiﬁcation using DL have shown no use of six weedspecies (Horseweed, Palmer Amaranth, Redroot Pigweed, Kochia, Rag-weed, and Waterhemp) and two crop species (Canola and Sugar beet)together to build a model (Espejo-Garcia et al., 2020;Hasan et al., 2021;Khan et al., 2021;Olsen et al., 2019). However, these studies have limitation focus on the performance of the DL model when imageshave different background conditions and did not involve popular weedand crop species that found popular in the Midwest United States Re-gion. In our study, the average f1-score values from 92% to 99% was ob-served, when the model was developed with combined datasets whichis in accordance with the researchﬁnding byEspejo-Garcia et al. (2020), Hu et al. (2020),Khan et al. (2021),Le et al. (2020),Olsen et al. (2019), andRazfar et al. (2022)in weed detections. In comparison, our researchwas performed for six species of weeds that are prevalent on North Da-kota and two important crop species grown on North Dakota. Study onuse of machine learning and deep learning in North Dakota based cropand weed species classiﬁcation was performed in our previous researcharticle (GC et al., 2022), which was mainly focused on weed classi ﬁca- tion under different crop production system and did not involve anystudy about impact of image background on model performance. The
result with combined datasets seems outstanding despite three weedspecies (Waterhemp, Palmer Amaranth, and Redroot Pigweed) withsimilar characteristics being used for building a model.5. ConclusionThis research investigated the effects of image background in DeepLearning (DL) models, speciﬁcally compared Convolutional Neural Net-works (CNNs) of VGG16 and ResNet16 on non-uniform and uniformimage background conditions. The analysis exhibited a decline inweed identiﬁcation performance by 16% to 28% in the VGG16 andResNet50 model, when models were tested on images with differentbackground than that of model training. However, a model trainedwith combined datasets of two background scenarios, was able toachieve the f1-score value of 92% to 99% with eight classes of weedand crop images. The performance parameters for each weed and cropspecies in terms of accuracy, f1-score, and confusion matrix weregreater than expected. Theﬁndings of this study suggest that DL modelsdeveloped from uniform image background could not improve the per-formance of deep learning models to identify weeds in unseen environ-mental conditions. From this study we suggest that similar kind ofbackground soil environment as that ofﬁeld should be created inside the greenhouse to be able to apply the deep learning-based weed detec-tion model inﬁeld. In addition, it seems including greater variances inimage background for the models seems working better when testedin individual conditions (when there is single variable such asnon-uniform, or uniform, or clay, or sandy soil, or volcanic ash) thanjust developing the model in same condition and testing in samecondition. Future work will focus on fusion of greenhouse and ﬁeld images to create aﬁeld environment background condition to buildrobust weed detection deep learning model with localization.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgmentThis material is based upon work partially supported by the USDA -Agricultural Research Service, agreement number 58-6064-8-023. Anyopinions,ﬁndings, conclusions or recommendations expressed in thispublication are those of the author(s) and do not necessarily re ﬂect the view of the U.S. Department of Agriculture. This work is/was sup-ported by the USDA-National Institute of Food and Agriculture, Hatchproject number ND01487. In addition, we want to thank NDSU PlantScience department's research specialists and Agricultural andBiosystems Engineering department graduate students (ArjunUpadhyay and Billy Ram) for helping with some of the greenhouse sam-ple processes in this study.ReferencesAbdalla, A., Cen, H., Wan, L., Rashid, R., Weng, H., Zhou, W., He, Y., 2019. Fine-tuningconvolutional neural network with transfer learning for semantic segmentation ofground-level oilseed rape images in a ﬁeld with high weed pressure. Comput. Elec- tron. Agric. 167.https://doi.org/10.1016/J.COMPAG.2019.105091/FINE_TUNING_CONVOLUTIONAL_NEURAL_NETWORK_WITH_TRANSFER_LEARNING_FOR_SEMAN-TIC_SEGMENTATION_OF_GROUND_LEVEL_OILSEED_RAPE_IMAGES_IN_A_FIELD_WITH_HIGH_WEED_PRESSURE.PDF . Alzubaidi, L., Zhang, J., Humaidi, A.J., Al-Dujaili, A., Duan, Y., Al-Shamma, O., Santamaría, J.,Fadhel, M.A., Al-Amidie, M., Farhan, L., 2021. Review of deep learning: concepts, CNNarchitectures, challenges, applications, future directions. J. Big Data 81 (8), 1 –74. https://doi.org/10.1186/S40537-021-00444-8 . Arya, D., Maeda, H., Ghosh, S.K., Toshniwal, D., Mraz, A., Kashiyama, T., Sekimoto, Y., 2020.Transfer Learning-Based Road Damage Detection for Multiple Countries.Bradski, G., 2000.The OpenCV Library. Dr. Dobb ’sJ .S o f t w .T o o l s . Bryson, C.T., DeFelice, M.S., 2010. Weeds of the Midwestern United States and Central Canada, A Wormsloe Foundation Nature Book. University of Georgia Press, Athens.Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., Vasilakoglou, I., 2020. Towardsweeds identiﬁcation assistance through transfer learning. Comput. Electron. Agric.171, 105306.https://doi.org/10.1016/J.COMPAG.2020.105306 . Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.https://doi.org/10.1016/J.COMPAG.2018.01. 009.GC, S., Saidul Md, B., Zhang, Y., Reed, D., Ahsan, M., Berg, E., Sun, X., 2021. Using deeplearning neural network in artiﬁcial intelligence technology to classify beef cuts. Front. Sensors 0, 5.https://doi.org/10.3389/FSENS.2021.654357 . GC, S., Zhang, Y., Koparan, C., Ahmed, M.R., Howatt, K., Sun, X., 2022. Weed and crop spe-cies classiﬁcation using computer vision and deep learning technologies in green-house conditions. J. Agric. Food Res. 9, 100325. https://doi.org/10.1016/J.JAFR.2022. 100325.S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
255Hasan, A.S.M.M., Sohel, F., Diepeveen, D., Laga, H., Jones, M.G.K., 2021. A survey of deeplearning techniques for weed detection from images. Comput. Electron. Agric. 184,106067.https://doi.org/10.1016/J.COMPAG.2021.106067 . He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition. Proc.IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2016-December, pp. 770 –778 https://doi.org/10.1109/CVPR.2016.90 . Holzner, W., 1982. Concepts, categories and characteristics of weeds. Biology and Ecologyof Weeds. Springer, Netherlands, pp. 3 –20https://doi.org/10.1007/978-94-017- 0916-3_1.Hu, K., Coleman, G., Zeng, S., Wang, Z., Walsh, M., 2020. Graph weeds net: a graph-baseddeep learning method for weed recognition. Comput. Electron. Agric. 174, 105520.https://doi.org/10.1016/J.COMPAG.2020.105520 . Johnson, J.M., Khoshgoftaar, T.M., 2019. Survey on deep learning with class imbalance.J. Big Data 6, 1–54.https://doi.org/10.1186/S40537-019-0192-5/TABLES/18 . Khan, S., Tufail, M., Khan, M.T., Khan, Z.A., Anwar, S., 2021. Deep learning-based identi ﬁ- cation system of weeds and crops in strawberry and pea ﬁelds for a precision agricul- ture sprayer. Precis. Agric. 22, 1711 –1727.https://doi.org/10.1007/S11119-021- 09808-9/TABLES/3.Kingma, D.P., Ba, J., 2015.Adam: A Method for Stochastic Optimization. CoRR abs/1412.6.Kulawardhana, R.W., 2011. Remote sensing of vegetation: principles, techniques and ap-plications. By Hamlyn G. Jones and Robin a Vaughan. J. Veg. Sci. 22, 1151 –1153. https://doi.org/10.1111/J.1654-1103.2011.01319.X . Le, V.N.T., Ahderom, S., Alameh, K., 2020. Performances of the LBP based algorithm overCNN models for detecting crops and weeds with similar morphologies. Sensors(Basel). 20.https://doi.org/10.3390/S20082193 . Li, Y., Guo, Z., Shuang, F., Zhang, M., Li, X., 2022. Key technologies of machine vision forweeding robots: a review and benchmark. Comput. Electron. Agric. 196, 106880.https://doi.org/10.1016/J.COMPAG.2022.106880 . Ma, X., Wu, H., Jiang, W., Ma, Yajie, Ma, Yan, 2015. Interference between redroot pigweed(Amaranthus retroﬂexusL.) and Cotton (Gossypium hirsutumL.): growth analysis. PLoS One 10.https://doi.org/10.1371/JOURNAL.PONE.0130475 .Oerke, E., 2005.Crop losses to pests. J. Agric. Sci. 144, 31 –43. Oerke, E., 2005.Crop losses to pests. J. Agric. Sci. 144 (1), 31 –43. Olsen, A., Konovalov, D.A., Philippa, B., Ridd, P., Wood, J.C., Johns, J., Banks, W., Girgenti, B.,Kenny, O., Whinney, J., Calvert, B., Azghadi, M.R., White, R.D., 2019. DeepWeeds: amulticlass weed species image dataset for deep learning. Sci. Report. 91 (9), 1 –12. https://doi.org/10.1038/s41598-018-38343-3 . Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-learn: machine learning in {P} ython. J. Mach. Learn. Res. 12, 2825 –2830. Razfar, N., True, J., Bassiouny, R., Venkatesh, V., Kashef, R., 2022. Weed detection in soy-bean crops using custom lightweight deep learning models. J. Agric. Food Res. 8,100308.https://doi.org/10.1016/J.JAFR.2022.100308 . Simonyan, K., Zisserman, A., 2014. Very Deep Convolutional Networks for Large-Scale Image Recognition. 3rd Int. Conf. Learn. Represent. ICLR 2015 - Conf. Track Proc.Soltani, N., Dille, J.A., Burke, I.C., Everman, W.J., Vangessel, M.J., Davis, V.M., Sikkema, P.H.,2017. Perspectives on potential soybean yield losses from weeds in North America.Weed Technol. 31, 148 –
154.https://doi.org/10.1017/WET.2016.2 . Stuart-Fox, D., Newton, E., Clusella-Trullas, S., 2017. Thermal consequences of colour andnear-infrared reﬂectance. Philos. Trans. R. Soc. B Biol. Sci. 372. https://doi.org/10. 1098/RSTB.2016.0345.Velumani, K., Lopez-Lozano, R., Madec, S., Guo, W., Gillet, J., Comar, A., Baret, F., 2021. Es-timates of maize plant density from UAV RGB images using faster-RCNN detectionmodel: impact of the spatial resolution. Plant Phenomics (Washington, D.C.). Doi:10.34133/2021/9824843.Wu, Z., Chen, Y., Zhao, B., Kang, X., Ding, Y., 2021. Review of weed detection methodsbased on computer vision. Sensors (Basel). 21. https://doi.org/10.3390/S21113647 .S. G C, C. Koparan, M.R. Ahmed et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 242 –256
256