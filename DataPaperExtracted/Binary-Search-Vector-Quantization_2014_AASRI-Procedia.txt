 AASRI Procedia   8  ( 2014 )  112 – 117 Available online at www.sciencedirect.com
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institutedoi: 10.1016/j.aasri.2014.08.019 
ScienceDirect
2
Abst
Thistrad
e
the tr
achie
VQ(
98.15
© 20
Sele
Keyw
1.In
V
exce
com
gain
A
the V
*
E2014 AAS R
N
tract
paper propose s
e-off and learni n
rade-off aspec t
eved. In the le a
(FSVQ) as an i
5% were achie v
014. Publishe d
ection and/or p
words: binary sea r
ntroduction 
Vector quanti z
ellent rate-dis t
mputation req u
ned by using t h
Among existi n
VQ search s p
Corresponding a
E-mail address: s 9RI Confere n
Bin
Ning-Yun 
a NTUT, Elec t
s a fast search 
ng aspects (TL A
t, a slight loss 
arning aspect, t h
inferred functi o
ved, which con f
d by Elsevier 
peer review u n
rch; G.729; line s
zation (VQ) i s
tortion perfor m
uirements. Th u
he VQ appro a
ng fast search 
pace. The sea r
author. Tel.: +88 6
94310393s@hot mnce on Spo r
nary Sear
Kua, Shu n
trical Engineerin g
algorithm for v
A) were used t o
occurred in th e
he binary searc h
on. In the expe r
firmed the exc e
B.V.
nder responsi b
pectrum pair (L S
s a lossy and 
mance and si m
us, numerous
ach.methods, th
e
rch spaces o f
6930204341. 
mail.com. rts Enginee r
rch Vec t
n-Chieh C h
g #1, Sec. 3, Chu
vector quantiza t
o enhance the l
e quantization q
h space was de v
riment, compu t
ellent performa n
bility of Ame r
SP); speech code c
powerful me t
mple structu r
 studies hav e
e tree-structur e
fthe TSVQ c oring and Co m
tor Qua n
hanga, Sha w
ng-hsiao East R d
tion (VQ) baseine spectrum p
a
quality; howe v
veloped using t
tational saving s
nce of the BSS -
rican Applied 
c; vector quantiz a
thod for mul t
re. However, f
e focused on t
ed VQ (TSV Q
ode-book are mputer Sci e
ntizatio n
w-Hwa H w
d., Taipei, Taiwa n
d on a binary s
air (LSP) enco d
er, substantial 
the learning pr o
s of 86.19% an d
-VQ approach.
Science Res e
ation (VQ); 
timedia com m
full search al g
the computat i
Q) method [2 ]
reduced to 2ence (SEC S
n
wanga,*
n, ROC 
search space ( B
der of the G.72 9
computational 
ocess, which u s
d a quantizatio n
earch Institute 
munications b e
gorithms hav e
ional savings 
] was propos e
log2(N) whic hS 2014) 
BSS-VQ). The 
9 standard. In savings were 
ses full search 
n accuracy of 
ecause of its 
e substantial that can be 
ed to reduce 
h makes the 
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute113  Ning-Yun Ku et al.  /  AASRI Procedia   8  ( 2014 )  112 – 117 
method powerful; however, the meth-od’s loss of quantization quality is problematic. The TSVQ is a binary search method that uses a single path to traverse the appropriate codewords. Thus, the TSVQ encoder does not ensure that the selected codeword is the closest to the input vector. A 
previous study [1] applied triangular inequality elimination (TIE) to VQ-based image coding, achieving computational savings of more than 90%. However, the TIE approach is dependent on the correlation characteristics of input signals. The noise-like (weak co rrelation characteristic) input vector reduces the performance of the TIE approach. In [3], the quasi-binary search (QBS) algorithm and trade-off aspect between TSVQ and TIE were proposed to reduce the computational complexity of the VQ algorithm. The performance was superior to that of TIE, particularly regarding the noise-like input signa l. The QBS algorithm is not dependent on the correlation characteristics of input signals. Although the quantization accuracy of the QBS algorithm was 99.16%, which was reasonably good, the computational savings of 59.43% for the LSP encoder of the G.729 standard were unsatisfactory. Although VQ is a powerful method, it confronts a num ber of problems in gaining computational savings for the LSP encoder of the G.729 standard. First, a moving average (MA) filters the LSP parameter, and the redundant source data is then extracted and removed. The MA filter thereby eliminates the correlation characteristics of the bias of LSP. Thus, the performance of these TIE-based algorithms decreases in the G.729 standard. Second, a small codebook size is prov ided in the multistage VQ of the G.729 standard. All codewords are adjacent and close to each other; therefore, the TIE algorithm cannot work efficiently. Third, th
e TSVQ achieves considerable computational savings, but loses substantial quantization quality. The QBS approach loses slight quantization quality and achieves unsatisfactory computational savings. Another study [4] proposed the fast search method for VQ. However, none of those methods focus on the LSP encoder of the G.729 standard. This paper prop oses a binary search space VQ (BSS-VQ) approach, which balances computational savings and quantization quality, and achieves considerable computational savings with only a slight loss in quantization quality. Moreover, the learning aspects of neural networks and VQ were also employed to improve the performance of the BSS-VQ approach. The LSP encoder with the G.729 standard was used to verify the performance of the BSS-VQ. The following section describes the BSS- VQ algorithm. The last two sections present the experimental results and conclusion. 2. BSS-VQ algorithm This section presents a fast search BSS-VQ algorithm that employs a fast locating approach to determine a small search space and fully search it to obtain an optimally matched codeword. A learning algorithm was proposed 
 to build a BSS, which uses full search VQ (FSVQ) as an inferred function. The BSS was trained using large amounts of training data, which efficiently obtained codewords for each subspace. However, some trifling regions existed between splitting boundaries and codeword boundaries, which implied that the training data fell in trifling regions with little possi-bility. Th erefore, a learning algorithm with insufficient training may lead to missed encoding in the trifling region which no training data fall in when the training process, resulting in a decline in quantization accuracy; conversely, each subspace can be fully trained using a learning algorithm
 with sufficient training data, but it with highly overlapped codewords; a BSS with highly overlapped codewords results in a decline in computa tional savings. Based on the aforementioned analysis, Algorithm 1 de-scribes the BSS generated by the learning process in the BSS-VQ algorithm. Algorithm 2, concerning the BSS-VQ encoding process, is also presented in detail. 114   Ning-Yun Ku et al.  /  AASRI Procedia   8  ( 2014 )  112 – 117 
2.1. Algorithm 1: BSS generation for BSS-VQ Step1: BSS determination. Step 1.1: Select ܤܥ ൌ ൛ݓܿ
௜ǡ௝ȁ ͳ൑݅൑ܰ ǡͳ൑݆൑ܦൟ  (1) which is the original codebook of VQ, to train the BSS. The term N is the codebook size and D is the code
 word dimension. Step 1.2: Calculate ܥ
௝ൌଵேσݓܿ௜ǡ௝ே௜ୀଵǡͳ൑݆൑ܦ(  2) as the centroid of CB in the jth dimension. Step 1.3: Determine ܵܵܤ ൌሼݏݏܾ
௞ȁ ͳ൑݇൑ʹ஽ሽ  (3) as the BSS, which is generated from dichotomy splitting according to the centroid Cj. The BSS contains 2
D
subspaces, which are empty of codewords at the initialization stage. Step 1.4: Empty ܵܲܶ ൌ ൛ݏ݌ݐ
௞ǡ௜ȁ ͳ൑݇൑ʹ஽ǡͳ൑݅൑ܰ ൟ  (4) as the training placement statistics, which are used to determine how often the inferred values fall into cw
i of bss
k.Step 2: Learning process. Step 
2.1: The term ܵൌሼݏ
௜ȁ ͳ൑݅൑ܯሽ  (5) is adop ted as the training database, S is used to build the content codeword of the BSS,
  where s i is the D- dimensional vector, and M is the data number. The term M is a large number, indicating 
that the training data are sufficient to fully train each binary search subspace. Step 2.2: Obtain ݓܿ
݊ൌܸܳܵܨ ሺ ݅ݏሻ  (6) 115  Ning-Yun Ku et al.  /  AASRI Procedia   8  ( 2014 )  112 – 117 
The term FSVQ(s i) denotes a full search VQ algorithm. It is us ed to determine the optimally matched codeword, cw
n, from CB for input vector s i.Step 2.3:Determine ݏݏܾ
݌ൌ ܸܳܵܵܤሺ݅ݏሻ  (7) ݌ൌσʹ
௝ȉ݂ሺݏ௜ሻ஽௝ୀଵǡ݂ሺݏ௜ሻൌ൜Ͳǡݏ௜ǡ௝൒ܥ௝
ͳǡݏ௜ǡ௝൏ܥ௝  (8) The BSSVQ(s
i) is proposed in this paper, and is used to determine the nearest subspace bs s p for input vector s
i. The BSSVQ(.) requires only D times comparison. Step 2. 4: Set ݓܿ
݌ݏݏܾא݊ (  9) and increase tps
p,n by one. The optimal matched codeword cw n is inserted into bss p if cw n does not belong to bss
p.Step 2.5: Repeat Steps 2.2 to 2.4 with each input vector s
i until i=M. The sum of TPS is M in the end. Step 2.6: Normalize ܵܲܶ ൌ ൛ݏ݌ݐ
௞ǡ௟݉ݑܵሺݏ݌ݐ ௞ሻΤȁ ͳ൑݇൑ʹ஽ǡͳ൑݈൑ܰൟǡ݉ݑܵሺݏ݌ݐ ௞ሻൌσݏ݌ݐ௞ǡ௟ே௟ୀଵ (10) Step 3: BSS screening. Step 3.1: Denote ܵܵܤ ൌ ൛ݏݏܾ
௜ǡ௝ȁͳ ൑ ݅ ൑ ʹ஽ǡͳ൑݆൑ܰ ௜ൟ  (11) as the  learned binary search space, where N
i is the number of codewords in bss i. Set the optimal thresholds ı
gsn and ı vss.Step 3.2:Sort bss
i.The sorted ݏݏܾ
݅ൌቄ݅ݏݏܾǡ݆ȁ݅ݏ݌ݐǡ݆൒݅ݏ݌ݐ ǡ݆൅ͳǡͳ൑݆൑݅ܰቅ  (12) The codewords of bss
i are sorted in descending order by tps i. Step 3.3: Screen bss
i. The new ݏݏܾ
݅ൌ൛݅ݏݏܾǡ݆หͳ൑݆൑݅ܯൟ  (13) 116   Ning-Yun Ku et al.  /  AASRI Procedia   8  ( 2014 )  112 – 117 
The term ܯ
௜ൌቊܰ௜ǡܰ௜൑ߪ௚௦௡
݆หݏ݌ݐ௜ǡ௝൒ߪ௩௦௦൐ݏ݌ݐ ௜ǡ௝ାଵǡܰ௜൐ߪ௚௦௡  (14) is the effective search subspace. The dismissed subspace ݏݏܾ݀
݅ൌ൛݅ݏݏܾǡ݆ห݅ܯ൑݆൑݅ܰൟ is filtered out by threshold ı
vss, which contains less likely codewords of bss i. Step 3.4: Repeat Steps 3.2 to 3.3 until i=2
D.2.2. Algorithm 2: A BSS-VQ encoding process Step 1: 
Denote ܺൌሼݔ
௜ȁ ͳ൑݅൑ݐሽ  (15) as the i nput signals, where x
i is a D-dimensional vector. Step 2: Determine ݏݏܾ
௤ൌ ܸܳܵܵܤሺݔ ௜ሻǡݍൌσʹ௝ȉ݂ሺݔ௜ሻ஽௝ୀଵ  (16) where ݂ሺݔ
௜ሻൌ൜Ͳǡݔ௜ǡ௝൒ܥ௝
ͳǡݔ௜ǡ௝൏ܥ௝  (17) TheBSSVQ(.) function is used to determine the subspace bss
q, and costs D times comparison. Step 3: Obtain ݓܿ
௠ൌܸܳܵܨ ሺ ݔ ௜ሻ  ( ) All codewords within subspace bss
q are set as the new codebook, and the FSVQ(.) is subsequently used to obtain optimally matched codeword cw
m from bss q. The codeword number of subspace bss q is lower than that of the original codebook. Thus, computational complexity is substantially reduced. Step 4: Repeat Steps 2 and 3 with each input vector x
i until i = t. 3. Experimental results This st
udy used the LSP encoder in the G.729 standard with a 10-D codebook and 128 codewords to verify the performance of BSS-VQ. A speech database, which was recorded at a sampling rate of 8 kHz and a resolution of 16 bits, was used on the inside test; it contained male speech, female speech, background noise, and silence, and its data number M was 601 422 (45.8 MB). The trained binary search space whose average, minimum, and maximum number of codewords were 16.53, 8, and 26, respectively. Moreover, the 15.3 MB 117  Ning-Yun Ku et al.  /  AASRI Procedia   8  ( 2014 )  112 – 117 
speech data for the outside test data were used to verify the performance of BSS-VQ. As shown in Table 1, computational savings of 86.19% and a quantization accuracy of 98.15% were achieved. The computational savings of BBS-VQ outperform that of the QBS-VQ approach when their quantization accuracies are similar. More
over, the quantization accuracy of BSS-VQ outperfo rms that of TSVQ when their computational savings are similar. These experimental results confirmed the excellent performance of the BSS-VQ approach. 
Table 1. The computational saving of LSP encoder with G.729 standard for outside test with FSVQ, TIE, QBS, BSSVQ, and TSVQ algorithms 
 Computational SavingsQuantization Accurac y
FSVQ0% 100% 
TIEVQ 23.65% 100% TSVQ89.06% 46.61% QBSVQ 59.43% 99.16% BSS-VQ 86.19% 98.15% 
4. Conclusion This paper presents a BSS-VQ algorithm to enhance the LSP encoder of the G.729 standard. The trade-off and learning aspects were used to achieve optimal perfo rmance. The BSS-VQ algorithm is not dependent on the correlation characteristics of input signals and outperforms most existing algorithms in the LSP encoder of the G.729 standard. References [1] Choi, S.Y. and Chae, S.I. Incremental-Search Fast Vector Quantiser Using Trian gular Inequalities for Multiple Anchors. Electronics Letters, 1192-1193; 1998. [2] Djamah, M. and O’S, D. An efficient tree-structured codebook design for embedded vector quantization. IEEE International Conference onAcoustics Speech and Signal Processing (ICASSP), 4686-4689; 2010. [3] Yan, L.J. and Hwang, S.H. The binary vector quantisation. Proc. Third IEEE Int. Symp. on Communication, Control, Signal Processing (ISCCSP), 604–607; 2008. [4] Chen, S.X. and Li, F.W. Fast encoding method  for vector quantisation of images using subvector characteristics and Hadamard transform.IET Image Processing, 18-24; 2011. 