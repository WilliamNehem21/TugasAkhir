Lightweight convolutional neural network models for semanticsegmentation of in-ﬁeld cotton bolls
Naseeb Singha,⁎,V . K .T e w a r ia, P.K. Biswasb,L . K .D h r u wa
aDepartment of Agricultural and Food Engineering, IIT Kharagpur, Kharagpur 721 302, India
bDepartment of Electronics and Electrical Communication Engineering, IIT Kharagpur, Kharagpur 721 302, India
abstract article info
Article history:Received 30 December 2022Received in revised form 1 March 2023Accepted 3 March 2023Available online 7 March 2023Robotic harvesting of cotton bolls will incorporate the bene ﬁts of manual picking as well as mechanical harvest- ing. For robotic harvesting, in-ﬁeld cotton segmentation with minimal errors is desirable which is a challengingtask. In the present study, three lightweight fully convolutional neural network models were developed for thesemantic segmentation of in-ﬁeld cotton bolls. Model 1 does not include any residual or skip connections,while model 2 consists of residual connections to tackle the vanishing gradient problem and skip connectionsfor feature concatenation. Model 3 along with residual and skip connections, consists of ﬁlters of multiple sizes. The effects ofﬁlter size and the dropout rate were studied. All proposed models segment the cotton bollssuccessfully with the cotton-IoU (intersection-over-union) value of above 88.0%. The highest cotton-IoU of91.03% was achieved by model 2. The proposed models achieved F1-score and pixel accuracy values greaterthan 95.0% and 98.0%, respectively. The developed models were compared with existing state-of-the-art net-works namely VGG19, ResNet18, EfﬁcientNet-B1, and InceptionV3. Despite having a limited number of trainable parameters, the proposed models achieved mean-IoU (mean intersection-over-union) of 93.84%, 94.15%, and94.65% against the mean-IoU values of 95.39%, 96.54%, 96.40%, and 96.37% obtained using state-of-the-art net-works. The segmentation time for the developed models was reduced up to 52.0% compared to state-of-the-art networks. The developed lightweight models segmented the in- ﬁeld cotton bolls comparatively faster and with greater accuracy. Hence, developed models can be deployed to cotton harvesting robots for real-time recog-nition of in-ﬁeld cotton bolls for harvesting.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:CottonSemantic segmentationConvolutional neural networkRobotic harvestingImage segmentationDeep learning
1. IntroductionCotton (Gossypium hirsutumL.) is an important cash crop for thefarmers of developed as well as developing countries and is presentlyharvested using machines like stripper or spindle pickers in developedcountries and manually in developing countries ( Bakhsh et al., 2017). Hand-picking of cotton bolls has certain advantages like better preser-vation ofﬁber characteristics, less trash content, and high harvesting ef-ﬁciency but causes health hazards due to pesticide residues ( Bakhsh et al., 2016, 2017;Memon et al., 2019), as well as it is a time- consuming and labor-intensive job. In the wake of a reduction in agri-cultural workers as well as an increase in agricultural labor costs innear future (Mehta et al., 2019), mechanical harvesting of cotton is nec-essary. But currently, cotton harvesting machines are the non-selectivetype which has many drawbacks as compared to manual hand picking.For instance, three times increase in trash content ( Shukla et al., 2017)which decreases the quality ofﬁber (Tian et al., 2018), necessarily use of defoliation chemicals (Snipes and Baskin, 1994) which increases the harvesting cost, soil compaction due to movement of heavy machineson cottonﬁelds (Braunack and Johnston, 2014) which decreases water and nutrients use efﬁciency (Colombi et al., 2018), having less picking efﬁciency (85.0–90.0% for spindle pickers) (Williford et al., 1994), and higherﬁe l dl o s s e sd u r i n gh a r v e s t i n g(Hughs et al., 2008), etc. Thus so, harvesting cotton manually and using presently available machineshas its advantages and disadvantages. Using harvesting robots for cot-ton harvesting can be a better alternative as cotton harvesting robotscan incorporate the advantages of hand picking as well as machinesby replacing humans with machines for performing selective pickingof cotton as performed by humans. The prospect of using cotton-picking robots has increased because of recent studies on agriculturalautonomous platforms (Raikwar et al., 2022;Roshanianfard et al., 2020;Zaidner and Shapiro, 2016), which are well-suited to autonomousagricultural robots.In the development of a cotton harvesting robot, the ﬁrst major task is the in-ﬁeld detection of cotton bolls which is a challenging taskArtiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
⁎Corresponding author.E-mail address:naseeb501@gmail.com(N. Singh).
https://doi.org/10.1016/j.aiia.2023.03.0012589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/because of the non-uniformity in visual characteristics of cotton bolls,varying illumination conditions during harvesting operation, and com-plex background around the harvesting scene, due to which errorsarises in cotton recognition. For the segmentation of cotton bolls, inpast, numerous researchers used various image processing techniquesfor which color, shape, and texture features were extracted as showninTable 1, and used as an individual basis or as a combination of multi-ple features. But, features extracted using hand-engineered-basedmethods are often low-dimensional ( Wang et al., 2021). For the seg- mentation of cotton bolls with minimal errors in natural illuminationwith a complex background, high dimensional features need to be ex-tracted which is a difﬁcult task using present conventional image seg-mentation methods. Thus, a new technique barring conventionalimage segmentation methods needs to be implemented for cotton im-ages which can extract high dimensional features automatically. Aconvolutional neural network is a technique that can extract high-dimensional features automatically through self-learned features.Using this convolutional neural network technique, numerous re-searchers achieved promising results for classi ﬁcation (Dyrmann et al., 2016;Jiang et al., 2019;Khanramaki et al., 2021;Waheed et al., 2020; Xie et al., 2021;Zhang et al., 2019) as well as for semantic segmentation(Azizi et al., 2020;Chen et al., 2020;Kang et al., 2021;Kestur et al., 2019; Tassis et al., 2021;Zabawa et al., 2020;Zou et al., 2021) tasks. For classi- ﬁcation tasks, at the end of convolutional layers, dense layers are addedwhile for semantic segmentation, fully convolutional neural networksare used in which a speciﬁc class is assigned to each pixel based onhigh features learned during the training of convolutional neural net-work models. For semantic segmentation of agricultural images, Tassis et al. (2021),Zou et al. (2021),Chen et al. (2020),Azizi et al. (2020), andMajeed et al. (2018)used fully convolutional neural networks andachieved a mean intersection over union (mean-IoU) value of 94.25%,92.91%, 89%, 80.50%, and 59.0%, respectively. Regarding the use ofconvolutional neural networks for cotton segmentation, in the past, Li et al. (2017)andLi et al. (2016)employed fully convolutional neuralnetworks in their studies to recognize in-ﬁeld cotton bolls and achieved an intersection over union (IoU) score of 70.4% and 73.5%, respectively.Using convolutional neural networks, Tedesco-Oliveira et al. (2020) predicted cotton yield with an accuracy of 80.0%, while Singh et al. (2022)used existing state-of-the-art deep learning models to discrimi-nate the cotton pixels from the sky, and achieved an IoU score of above80.0%. Further applications of CNN in cotton include the prediction ofcotton yarn intensity (Zhenlong et al., 2018) and the identiﬁcation of in- fected cotton leaves (
Yan et al., 2021).For the working of cotton harvesting robots in real-time with higherproductivity and lesserﬁeld losses, a high recognition rate with minimalerrors as well as lower harvesting time is imperative. At present, the ac-curacy of cotton segmentation using a convolutional neural network isbelow 80.0% (Li et al., 2016, 2017), therefore, this study aimed to de-velop light-weight convolutional neural network (CNN) models whichwill be able to segment the cotton bolls with higher accuracy, minimalerrors and comparatively a lower inference time. The performance ofdeveloped models were evaluated in terms of intersection over union,F1-score, accuracy, precision, and recall values. For a comprehensiveevaluation, the performance of developed models was also comparedwith the cotton segmentation results obtained using existing state-of-the-art networks.The major contributions of the present study are as follows:1. Low-level and high-level features were combined with various ﬁlter sizes to create a neural network model for in- ﬁeld cotton boll detec- tion that can be deployed on small single-board computers like Rasp-berry Pi, etc.2. The impact of the dropout rate in the convolutional layer as well asﬁlter size on the performance of the convolutional neural networkis discussed.3. The ablation study of various proposed models provides insight intothe implications of model pruning on their performance.The rest of the paper is organized as follows. In Section 2,i m a g e dataset preparation, the methodology followed for CNN model develop-ment, their training, and quality criteria used for performance evalua-tion of CNN models is presented. The results obtained by the trainedmodels to segment cotton bolls are presented in Section 3. In the end, Section 4concludes theﬁndings of this study.2. Materials and methodsThis section describes the image acquisition, labeling, pre-processing, and augmentation process. This section further describesthe process followed for the development of three new architecturesof CNNs. The training procedure of developed architectures is discussedalong with the explanation of criteria matrices used in this study for theperformance evaluation of developed CNN models.2.1. Cotton image acquisitionHaryana is a major cotton-growing state in Northern India ( Seidu, 2018) and the cotton images used in the present study were capturedfrom a cottonﬁeld ofKharantivillage, Rohtak district, India (29°01 ′ 30″N, 76°27′12″E). A total of one hundred RGB color images (having aresolution of 480 × 640 pixels) of cotton plants from the ﬁeld with cot- ton bolls due for harvesting in a week were acquired under natural illu-mination conditions using a 0.9-megapixel digital camera (LogitechWebcam C270) having aﬁxed focal length of 4.6 mm and diagonalﬁeld of view of 55°. As the distance and angle of the captured scene ofthe cottonﬁeld will vary based on the position of vision sensorsmounted on the robotic arm, therefore, for this study, with a purposeto make the image dataset heterogeneous and practical for in- ﬁeld application of developed models, images were captured at randomdistances and angles to cotton plants.2.2. Dataset construction2.2.1. Image labelingA labeled image is one in which each pixel is assigned a speci ﬁcc l a s s manually and this process is called image labeling which is a fundamen-tal pre-requisite to train the CNNs if the developed CNNs are for seman-tic segmentation tasks. In this study, image labeling was done for two-class binary classiﬁcation for which pixels of cotton bolls were assigned
a value of‘1’, while pixels of the region other than cotton bolls wereassigned a value of‘0’using an annotation app ‘apeer’provided by ZEISS microscopy(apeer [WWW Document], 2021).2.2.2. Data augmentationThe training of CNNs required a sizeable number of labeled images(Zou et al., 2021), moreover, to achieve better semantic predictionsusing CNNs, image labeling should be carried out with maximal accu-racy, and hence, labeling of images consumes a lot of time, human ef-forts, etc. Therefore, due to the limitation of resources in the labelingof images, an image augmentation technique in which synthetic imagesand their corresponding labeled images were produced arti ﬁcially, was successfully implemented by various researchers ( Azizi et al., 2020;Table 1Features extracted by researchers for cotton recognition.Features extracted Accuracy ReferenceColor features (YCbCr color space) 90.44% Liu et al., 2011 Color features (RGB color space) 88.09% Wang et al., 2008 Spatial features 88.0% Yeom et al., 2018 Color features (RGB and YCbCr color space) 91.05% Singh et al., 2021 Color and spatial features 84.60% Sun et al., 2019N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
2Chen et al., 2020;Kang et al., 2021;Zou et al., 2021) and achieved im- pressive accuracy in semantic segmentation tasks. Previous studies(Kang et al., 2021;Sun et al., 2021;Tassis et al., 2021;Zabawa et al., 2020;Zou et al., 2021) have shown that with the right data augmenta-tion techniques, researchers may get good results with a limited num-ber of images instead of a much larger one. Therefore, to increase thenumber of image datasets for the training and validation of CNNs, theimage augmentation technique was implemented in this study. Out ofhundred captured images, ninety images were taken randomly andthe image augmentation technique was applied to each image. To ac-complish this, images with random values were rotated from 10° to60°, moved horizontally by 100 pixels, moved vertically by 100 pixels,ﬂipped horizontally as well as vertically, upon which ﬁve new synthetic images and their corresponding labeled images were obtained. Fig. 1 shows the sample of augmented cotton images and correspondingmasked images. Thus, the cotton dataset that was used for the trainingof CNNs consists of a total of 540 images out of which 405 images(75%) were used for the training of CNNs while the remaining 135 im-ages (25%) were used for validation purposes. The remaining ten rawimages were used for the testing of the proposed models.2.3. Building convolutional structure of proposed modelsInspired by various state-of-the-art convolutional networks, threedifferent custom architectures were designed in this study to recognizethe cotton bolls using end-to-end fully convolutional networks. Allthree designed architectures have the same baseline which is as follows:(a) unless speciﬁed, all convolutions were performed using a 3 × 3 ﬁlter size with a stride of 1, (b) all max-pooling operations were performedusing a 2 × 2ﬁlter size with a stride of 2, (c) all transposed convolutionsoperations were performed using 2 × 2 with a stride of 2, (d) for allconvolutional, transposed convolutional and max-pooling layers, pad-ding was applied such that the output dimensions of feature map resultin the same dimensions as of input feature map.A non-linear activation function was applied after eachconvolutional layer, with the purpose to extract non-linear featuresfrom computed feature maps as well as increasing the accuracy of pro-posed convolutional networks (Jarrett et al., 2009). In the present study, the ReLU activation function proposed by Nair and Hinton (2010)was used as it leads to faster convergence ( Glorot et al., 2011;Krizhevsky et al., 2012) and do not suffer from vanishing gradient problems.
Fig. 1.Sample of augmented raw images and corresponding masked images.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
3The ReLU activation function retains only the positive values of the com-puted feature maps while making the negative values equal to zero andmathematically, can be deﬁned by Eq.(1).fxðÞ ¼maxx,0ðÞ ð 1ÞIn the last layer of each model, the soft-max activation function wasused. During the training process of convolutional neural networks, atﬁrst, initial weights need to be provided to each layer after whichthese weights will be updated iteratively during the backpropagationprocess. This process of generation of initial weights according to thedimensions of layers is calledweight initializationwhich is a vital opera- tion for the network convergence ( Kumar, 2017) as too-large or too-small weights initialization may lead to exploding and vanishinggradients issues, respectively (Bengio et al., 1994) during the training of the convolutional network. As in the present study, the ReLUactiva- tion function was used in neural layers, hence, as recommended by He et al. (2015), weights were initialized usingHeinitialization for the cus- tom proposed models. All models proposed in this study, are essentiallycomprised of encoder and decoder parts.2.3.1. Building convolutional structure of model 1For model 1, each convolutional block in the encoder part consists ofone convolutional layer followed by batch normalization and a ReLU ac-tivation layer. After the successive inclusions of two such convolutionblocks in architecture, a max-pooling layer was added to reduce the spa-tial dimensionality. This arrangement is repeated three times. At its endportion, the encoder consists of a max-pooling layer followed by a singleconvolutional block. Each block in the decoder part consists of a singlenumber of transposed convolution layers, convolutional layer, batchnormalization layer, and activation layer, sequentially as mentioned.This decoder block in the decoder up-sampled the input feature mapsusing the 2 × 2 transposed convolution. After the inclusion of foursuch blocks in architecture, at the end of the decoder, an additional1 × 1 convolution is applied to reduce the depth of feature maps tothe required number of classes which is two in the present studyi.e., background and cotton classes, and produce the pixel-wise seg-mented image. At each convolutional block in the encoder, the numberof feature channels increases progressively, while in the decoder at eachdecoder block, the number of feature channels decreased progressively.The architecture of proposed model 1 is illustrated in Fig. 2along with the dimensions of feature maps and the number of ﬁlters.2.3.2. Building convolutional structure of model 2The pooling operation used in the encoder not only reduces the spa-tial dimensionality of feature maps but also results in the elimination ofspatial information (Ronneberger et al., 2015)a sw e l la sg e n e r a t e s checkerboard artifacts during up-sampling ( Sugawara et al., 2019). In- spired byRonneberger et al. (2015), in proposed model 2 and model3, skip connections were used to pass features from the encoder to thedecoder with the purpose to regain spatial information. For this, fea-tures maps in encoders were concatenated channel-wise to the featuresmaps of identical dimensions in the decoder. As compared to model 1,layers in model 2 were increased to enhance the segmentation accuracy.But, the inclusion of additional layers in architecture may lead to an in-crease in training error due to degradation and vanishing gradient prob-lems (He et al., 2015). Therefore, in the present study, the probableproblem of increased training error has been alleviated by using the re-sidual connection. Using the residual connection, two layers were di-rectly connected while skipping one or more layers in between. Fig. 3 shows the residual block used in the architecture of model 2 to allevi-ated the training error issues. In this, the residual connection simplyconvolutes the input feature maps with kernel size of 1 × 1, output ofwhich will be added to the output of the other stacked layer as showninFig. 3.The encoder part of model 2 comprises three convolutional blocks,two residual blocks, and four max-pooling layers. Two residual blockswere successively added after theﬁ
rst convolutional block as showninFig. 4. Max-pooling layer was placed after each convolutional blockand residual block except the last convolutional block. The decoderpart of model 2 consists of four transposed convolution layers, witheach of which feature maps from the encoder concatenated using skipconnections. The concatenation of transposed convolutional and skipfeatures were followed by a residual block and this arrangement was re-peated four times in the decoder. The complete architecture of proposedmodel 2 is illustrated inFig. 4along with the dimensions of feature mapsand the number ofﬁlters.2.3.3. Building convolutional structure of model 3Practically, during the operation of robotic harvesting in a cottonﬁeld, a visual sensor will capture cotton bolls at different viewing dis-tances and angles due to which spatial layout, as well as the scale of cot-ton bolls, will be random. As mentioned by Li et al. (2017), single- instance cotton bolls (present globally in an image) are easier to seg-ment as compared to multi-instance cotton bolls (present locally in animage). Hence, choosing a kernel size in such cases is always a challeng-ing task as a larger kernel size is preferred for information that is present
Fig. 2.Conceptual diagram of CNN model 1 (For yellow box refer to ablation experiments section).N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
4globally while a smaller kernel size is preferred for the informationwhich is present locally in an image. Therefore, inspired by Szegedy et al. (2014), an Inception-Residual block (IRB) as shown in Fig. 5was built in the present study in which convolution on input with three dif-ferent sizes ofﬁlters i.e., 1 × 1, 3 × 3, 5 × 5 was performed. A max-pooling operation was also performed with a kernel size of 3 × 3 withstride 2. Noteworthy, the 1 × 1 convolution proposed by Lin et al. (2014)was added before the 3 × 3 and 5 × 5 convolutions to limit thenumber of input channels, and so the computational expensiveness ofInception-Residual block (IRB). Additionally, a shortcut/residual con-nection is implemented in IRB to exploit the advantages of residual con-nections. The outputs from all these operations were concatenated andthat was the output of the IRB. The output of IRB acts as input to the nextlayer.The encoder part of model 3 consists of three convolutional blockswhich comprised convolutional, batch normalization, and activationlayers serially, three Inception-Residual blocks, and four max-poolinglayers which were arranged as shown in Fig. 6. The decoder part consists of four transposed convolutions blocks which comprised of transposedconvolutional, convolutional, batch normalization, and activation layers,serially to which feature maps from the encoder were concatenatedusing skip connections. The complete architecture along with the di-mensions of feature maps and the number of ﬁlters is illustrated in Fig. 6.2.4. Training of CNN modelsOut of the total captured in-ﬁeld cotton images, 10% of images werekept separate for the testing of developed CNN models, while the re-maining 90% were used to create synthetic images which later utilizefor training and validation purposes in 75:25 proportion, respectively.The optimization algorithm, learning rate, number of epochs, batch
Fig. 3.Conceptual residual block for CNN model 2.
Fig. 4.Conceptual diagram of CNN model 2 (For yellow box refer to ablation experiments section).
Fig. 5.Conceptual Inception-Residual block for CNN model 3.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
5size, loss function, and weight initialization are some crucialhyperparameters that need to be tuned before training of neural net-work to achieve better results. The improper selection of thesehyperparameters may result in poor generalization performance ( Xing et al., 2018), overﬁtting, excess training time, over-consumption ofcomputational resources (Xie et al., 2021), and may effects regulariza- tion signiﬁcantly (Wilson and Martinez, 2003), etc. In the past, numer- ous researchers implemented the Adam optimizer ( Bao et al., 2021; Gonzalez-Huitron et al., 2021;Waheed et al., 2020), used a learning rate of 0.001 (Gonzalez-Huitron et al., 2021;Kolhar and Jagtap, 2021; Zabawa et al., 2020), trained the models for one hundred epochs(Abdalla et al., 2019;Kolhar and Jagtap, 2021;Tassis et al., 2021;Zou et al., 2021) and achieved impressive results for agricultural images.Therefore, in the present study, models were trained for one hundredepochs having Adam optimizer (Kingma and Ba, 2017)a sa no p t i m i z a - tion algorithm with a learning rate of 0.001 and a small batch size of ﬁve images as recommended by Kandel and Castelli ( Kandel and Castelli, 2020).Because of the existence of a higher class imbalance in the trainingdataset due to the substantial pixel ratio gap between cotton and back-ground pixels, the accuracy of the trained model may not be satisfactory(Fujii et al., 2021). To deal with the class imbalance issue, multiple re-searchers (Badrinarayanan et al., 2017;Ngugi et al., 2020;Tang et al., 2020) successfully used the dice coefﬁcient as a loss function in seman- tic segmentation tasks for agricultural images. Therefore, in the presentstudy, the dice coefﬁcient deﬁned by Eq.(2)w a su s e da sal o s sf u n c t i o n in the training of proposed neural networks.Dice
Loss¼1/C02/C2pic/C2qicþϵp
icþqicþϵð2Þwhere, i = i
thsample in training data; c = cthclass;p ci=o n eh o te n - coder of ground truth;q
ci= probability of class c for ithsample; ϵ=1×1 0
−6.A python-based open-source deep learning application program-ming interface named Keras v2.4.0 ( Chollet, 2015) with an open- source python library called TensorFlow v2.6.1 ( TensorFlow Developers, 2021) as its backend was used in this study to build the pro-posed models. Google Colaboratory (Google Colaboratory, 2021), an on- line cloud-based Jupyter notebook environment, was used to trainproposed convolutional neural networks using available powerful hard-ware options as described inTable 2.2.5. Selection of dropout rateDropout is a regularization technique, used to avoid over ﬁtting in the neural networks by dropping out activation units randomly duringtraining. This technique of regularization was successfully implementedin the fully connected layer of convolutional networks by various re-searchers for agricultural images (Khanramaki et al., 2021;Rahman et al., 2020;Waheed et al., 2020;Xie et al., 2021).You et al. (2020) usedDropBlock(Ghiasi et al., 2018)a n dLi et al. (2017)added the drop- out layer after the convolutional layer to avoid the over ﬁtting of neural networks which build for weed detection and cotton segmentation, re-spectively. For analyzing the effects of the dropout layer on the perfor-mance of fully convolutional neural networks as well as to get theoptimum value of dropout rate, proposed models having a ﬁxedﬁlter size of 3 × 3 in convolutional layers were trained with different dropoutrates of 0%, 10%, 20%, 30%, and 40%. Except for the dropout rate, all otherhyperparameters were kept the same for each case during training.2.6. Selection ofﬁlter sizeThe size of the convolutionﬁlters determines the size of the recep-tiveﬁeld from where information is extracted in convolutional layers.If the size of convolutionalﬁlters is small, then the extracted featureswill be highly local, while extracted features will be more expressivein case of a largerﬁlter size (Szegedy et al., 2015). The in-ﬁeld cotton bolls in captured images are present globally as well as locally forwhich largerﬁlter size and smallerﬁlter size should be preferred, re- spectively. To achieve the scale invariance for better performancefrom trained CNNs, the optimalﬁlter size for convolutional layers wasselected experimentally. For this, proposed models were trained withaﬁlter size of 3, 5, and 7, while keeping other hyperparameters the
Fig. 6.Conceptual diagram of CNN model 3.
Table 2Software and hardware speciﬁcations.Name SpeciﬁcationsGPU 1xTesla K80, having 2496 CUDA cores, compute 3.7, 12 GBGDDR5 VRAMCPU 1xsingle core hyperthreaded Xeon Processors @2.3GhzRAM 25 GB AvailableProgrammingLanguagePythonN. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
6same for each case during training. Theﬁlter size which resulted in the best performance of the CNNs was selected to build the models.2.7. Ablation study of architecturesA typical convolutional neural network consists of millions of train-able parameters (He et al., 2015;Ronneberger et al., 2015;Szegedy et al., 2015) out of which possibly some parameters may contributemeager or negligible to the output of the network and hence, can be re-moved without affecting the efﬁciency of the network (Molchanov et al., 2017). To remove the non-performing parameters of a CNN, abla-tion experiments were conducted by multiple researchers ( Adhikari et al., 2019;Bao et al., 2021;Xie et al., 2021;You et al., 2020) in which certain components/layers of the network were pruned. The goal ofthe ablation study is to reduce the network size, computational expen-siveness, training time, and inference time, providing the actual ef ﬁ- ciency of the network is retained or degraded insigni ﬁcantly. In the present study, ablation experiments were conducted on pro-posed networks to reduce the size of networks and inference timewhile retaining the performance of networks. The ablation of neuralnetworks in the present study was conducted in two ways: (i) by re-moving the number of layers, and (ii) by changing the number of ﬁlters in models. In model 1, from the encoder, the last max-pooling layer andthe convolutional block was removed, while from the decoder, the ﬁrst deconvolutional block was removed. The removed portion from model1 is highlighted by yellow dotted lines in Fig. 2. This ablated model is re- ferred to as model v1.1 in this study. In model 2, from the encoder, thelast max-pooling layer and the convolutional block was removed,while from the decoder, theﬁrst deconvolutional layer, concatenatelayer and residual block were removed. The removed portion frommodel 2 is highlighted by yellow dotted lines in Fig. 4.T h i sa b l a t e d model is referred to as model v2.1. In model 3, Inception-Residualblock number 3 (IRB3) was replaced with one convolutional blocklayer and referred to as model v3.1. For another ablation using model3, IRB2 and IRB3 were replaced with a single convolutional block foreach IRB and referred to as model v3.2. Regarding ablation experimentsthrough the number ofﬁlters, for each model and its variants, theﬁlters were decreased to 80%, 60%, and 40% from the numbers of ﬁlters stated inF i g s .2 ,4 ,a n d6. The performance of models and their variants wereevaluated using evaluation matrices to select the best models amongthese in terms of segmentation accuracy, network size, and inferencetime.2.8. Evaluation indexesFor the performance evaluation of proposed neural networks andcomparisons among these, segmentation results obtained using net-works were quantiﬁed using well-known evaluation metrics for agricul-tural images: F1-score (Chen et al., 2021;Kestur et al., 2019;Kolhar and Jagtap, 2021), intersection-over-union (IoU) (Sun et al., 2021;Zabawa et al., 2020;
Zou et al., 2021), mean intersection-over-union (mean-IoU) (Long et al., 2015;Tassis et al., 2021;Xu et al., 2020), pixel accuracy (Kestur et al., 2019;Kolhar and Jagtap, 2021;Tassis et al., 2021), preci- sion (Kestur et al., 2019;Zabawa et al., 2020) and recall (Chen et al., 2020;Tassis et al., 2021;Zou et al., 2021) metrics. IoU metric as deﬁned by Eq.(3), measures the ratio of the number of pixels common betweenthe predicted, and ground truth image and the total number of pixelspresent across both images. Mean-IoU is the average of IoU scoresover semantic classes and is deﬁned by Eq.(4). Pixel accuracy as deﬁned by Eq.(5), measures the percent of pixels that were classi ﬁed correctly. For a given class i, a precision metric can be de ﬁned as the fraction of correctly classiﬁed pixels of class i among the total pixels classi ﬁed as class i pixels, whereas recall metric measures the fraction of correctlyclassiﬁed pixels of class i among the actual number of pixels of class ipresent in ground truth image. Precision and recall metrics are de ﬁned by Eqs.(6) and (7), respectively. The F1-score combine the precisionand recall metrices into a single parameter for better analyzing anddeﬁned by Eq.(8).IoUi¼TPi
TPiþFN iþFP ið3Þmean/C0IoU¼
1k∑ki¼1 pii
∑kj¼1pijþ∑kj¼1pji/C0piið4Þwhere p
ijrepresents the number of pixels of the class ithat were predicted to be class j and k represents the number of semantic classes.Pixel Accuracy
i¼TPiþTN i
TPiþFN iþFP iþTN ið5ÞPrecision
i¼TPi
TPiþFP ið6ÞRecall
i¼TPi
TPiþFN ið7ÞF1/C0Score
i¼2/C2Precisioni/C2Recalli
PrecisioniþRecallið8Þwhere true positive/true negative (TP
i/TN i) are the pixels belonging to classithat is predicted correctly, false positive (FP
i) are pixels that are predicted as classipixels but do not belong to classi, false negative (FN
i) are pixels that belong to classibut incorrectly classiﬁed as pixels of another class.2.9. Comparison with the state-of-the-art networksOn considering the success of convolutional neural networks forclassiﬁcation tasks,Long et al. (2015)developed theﬁrst end-to-end trainable fully conventional network for image segmentation tasks.Later,Ronneberger et al. (2015)developed a novel CNN model for bio-medical image segmentation. The architecture of their proposedmodel mainly consists of encoder and decoder portions in which the en-coder learned the low-level features from the input image, while the de-coder semantically maps those learned features onto the image pixels ofthe same size as of input image. Subsequently, numerous researchers(Adhikari et al., 2019;Badrinarayanan et al., 2017;Kolhar and Jagtap, 2021;Milioto et al., 2018;Peng et al., 2019) used convolutional encoder-decoder networks for semantic segmentation. The Visual Ge-ometry Group (VGG) (Simonyan and Zisserman, 2015), ResNet (He et al., 2015), and InceptionV3 (Szegedy et al., 2015) networks achieve top-5 accuracy of 92.7%, 93.3%, and 93.9%, respectively on ImageNetdataset (Deng et al., 2009), which proves that these networks havegood features extraction ability and often ( Gao et al., 2020;Hecht et al., 2020;Majeed et al., 2018;Ou et al., 2019;Panda et al., 2022; Shah et al., 2022;Zou et al., 2021) used as a backbone in various CNN ar-chitectures developed for semantic segmentation. This study comparedthe proposed models to the aforementioned models while using theaforementioned networks as the encoder backbone. For this purpose,fully connected dense layers were pruned from these CNN models andan architecture similar to the U-Net model ( Ronneberger et al., 2015) was constructed using up-sampling layers. Additionally, rather thanstarting the training process by randomly initializing the weights,weights of respective networks, trained on the ImageNet dataset(Deng et al., 2009) were used, while keeping the other hyperparametersunchanged from the training of the proposed models in the study. Tan and Le (2019)proposed a new model named EfﬁcientNet with which they achieved better performance by balancing network depth, width,and resolution. For agricultural images too, multiple researchers ( Atila et al., 2021;Duong et al., 2020;Yin et al., 2020;Zhang et al., 2020)N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
7Fig. 7.Effects of dropout rate on the performance of proposed models.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
8Fig. 8.Effects ofﬁlter size on the performance of proposed models.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
9achieved desirable results by implementing the various models ofEfﬁcientNet family in their studies. The EfﬁcientNet models are smaller and faster as compared to other existing state-of-the-art models ( Tan and Le, 2019), therefore, for comprehensiveness in evaluation, light-weight models proposed in the present study were also comparedwith the EfﬁcientNet-B1 model.3. Results and discussion3.1. Effects of dropout rate on the performance of convolutional networksThe segmented results from models that were trained using differ-ent dropout rates were compared. From Fig. 7, it can be observed that with the increase in the dropout rate, the performance of the proposedmodels signiﬁcantly decreased. The mean IoU is an important and com-monly used parameter to evaluate the performance of models devel-oped for semantic segmentation. It was observed that when thedropout rate increased from 0% to 40%, the mean- IoU value decreasedby 31.60%, 14.32%, and 26.85% for model 1, model 2, and model 3,respectively. In addition, to mean-IoU values, cotton-IoU (Intersection-over-Union), F1-score, and pixel accuracy values also decreasedsigniﬁcantly when dropout rates increased. Our analysis in this studyshows that the dropout layer present in a fully convolutional neuralnetwork is ineffective and may decrease its overall performance.The reason for the failure of the dropout technique is that the pro-posed models are fully conventional neural networks and cotton ﬁeld images have strong spatial correlation and hence the extracted featuremaps too. As feature maps activations are spatially correlated, therefore,despite the dropping-off of some random activations' units from featuremaps, information stillﬂows through the convolutional layers and thefully convolutional neural network shows no positive effect of dropouton its performance. Similar observations were also mentioned byGhiasi et al. (2018)andTompson et al. (2015)in their respective studies.
Fig. 9.Ablation study results in terms of evaluation matrices for model 1 and its variant.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
103.2. Effects ofﬁlter size on the performance of convolutional networksThe segmented results from the models that were trained using ﬁlter sizes of 3, 5, and 7 in convolutional layers were compared to select thebestﬁlter size. FromFig. 8, it can be observed that there is an insigni ﬁ- cant difference between the performance of proposed models whiletrained with differentﬁlter sizes. For theﬁlter size of 3, 5, and 7, the mean IoU value for model 1, model 2, and model 3 was changed margin-ally while the computational expensiveness increased by 2.78 times and5.45 times (Szegedy et al., 2015)f o r5×5a n d7×7ﬁlter size. Moreover, in terms of F1-score, accuracy, and cotton-IoU values too, no signi ﬁcant advantage was observed in favor of a larger ﬁlter size for the proposed models. Therefore, a 3 × 3ﬁlter size was used in the convolutional neu-ral networks of this study.3.3. Results of ablation experimentsTo reduce the size of models and their inference time, ablation ex-periments on three proposed models were conducted for which layer/components were pruned from the networks in the ﬁrst approach while in the second approach, the number of ﬁlters was altered in models as well as in their derived variants. Figs. 9, 10, and 11show the performance comparison of model 1, model 2, and model 3, respec-tively which were trained with 100%, 80%, 60%, and 40% of the totalnumber ofﬁlters used in the proposed models as mentioned in Figs. 2, 4, and 6. The value of 100% denotes no change in the number of ﬁlters, while 80% denotes the 20% lesser number of ﬁlters as compared to the number ofﬁlters stated inFigs. 2, 4, and 6. A comparison was also made between the proposed models and their respective variants hav-ing a different number ofﬁlters.Zou et al. (2021)developed a convolutional neural network that outperform the U-net model by1.78%, the DeepLabv3 network inChen et al. (2021)outperform the U-net by 0.7%, and the AM-DeepLabv3 network in Kang et al. (2021) outperform the Sub-pixel-DeepLabv3+ network by 0.77% in terms ofIoU score. Therefore, a threshold value of 0.80% was used in this studyto categorize the change in the IoU score into signi ﬁcant or insigniﬁcant. If a change in the IoU score is greater than 1.0%, then only the changewas considered signiﬁcant.
Fig. 10.Ablation study results in terms of evaluation matrices for model 2 and its variant.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
11The mean-IoU is the most important parameter to evaluate the per-formance of semantic segmentation. The value of the mean-IoU score, asobserved inFigs. 9, 10, and 11, was decreased by 2.18%, 4.98%, and 3.62%on reducing the number ofﬁlters by 60% in model 1, model 2, and model3, respectively. For 100% of the number of ﬁlters, a difference of 3.7% be- tween model 1 and model v1.1, 3.23% between model 2 and model v2.1,and 3.22% between model 3 and model v3.2 was observed in the mean-IoU score. Similar patterns were also observed for precision, recall, F1-score, pixel accuracy, and cotton-IoU score of models and their respec-tive variants. So, fromFigs. 9, 10, and 11we can conclude that either the number of layers/components gets pruned from the network orthe number ofﬁlters gets reduced, and the performance of modelsand their respective variants gets decreased.In addition to segmentation accuracy, inference time is also an impor-tant parameter that should be considered for the evaluation of models.Therefore, along with the cotton-IoU score, the inference time of modelswas compared as shown inFig. 12.I tc a nb eo b s e r v e df r o mFig. 12that for100% of the number ofﬁlters, there is a 7.13% reduction in cotton-IoUscore for model v1.1 as compared to model 1 against a 15.34% reductionin inference time. A comparison between model 1 with no reduction intheﬁlter and a 20% reduction in the number of ﬁlters shows that the cotton-IoU score decreased by 0.76% against a 20.45% reduction in infer-ence time. The inference time will be further reduced to 298 ms by re-ducing the number ofﬁlters to 60%, but the cotton-IoU score alsodropped by 2.0%. Hence, model 1 with a 20% fewer number of ﬁlters was selected for furtherﬁne-tuning. In the case of model v2.1 with100%ﬁlters, there is a 6.44% reduction in cotton-IoU score as comparedto model 2 against a 9.97% reduction in inference time. Comparingmodel 2 with no reduction inﬁlters and a 20% reduction in the numberofﬁlters shows that the cotton-IoU score decreased by 1.74% which isconsidered to be a signiﬁcant change based on the threshold value of0.80% as described earlier. Hence, model 2 with a 100% number of ﬁlters was selected for furtherﬁne-tuning. A comparison of model 3 having noreduction in the number ofﬁlters with model v3.1 and model v3.2 shows
Fig. 11.Ablation study results in terms of evaluation matrices for model 3 and its variant.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
12Fig. 12.Comparison of models and their variants in terms of cotton-IoU score and inference time.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
13the 1.78% and 6.04% reduction in cotton-IoU score against 11.9% and13.2% reduction in inference time. A comparison between model 3with no reduction inﬁlters and a 20% reduction in the number of ﬁlters shows that the cotton-IoU score decreased by 0.84% against a 10.53%reduction in inference time. In this case, the reduction in inferencetime is marginal plus the reduction in cotton-IoU score crossed thethreshold value, and therefore, model 3 with a 100% number of ﬁlters was selected for furtherﬁne-tuning.
Fig. 13.Comparison ofﬁnally selected, andﬁne-tuned models in terms of evaluation matrices.
Fig. 14.Qualitative visualization of semantic segmentation results of cotton bolls obtained using developed models.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
14Fig. 15.Qualitative performance comparison of developed models with state-of-the-art convolutional networks for semantic segmentation of cotton bolls ( The green, white, and red colors indicate the correct classiﬁcation, false positive and false negative, respectively).N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
153.4. Performance evaluation of neural networksAfter analyzing the models in terms of segmentation accuracy andinference time inSection 3.3, it was observed that with a 20% reductionofﬁlters in model 1, inference time was reduced by 20.45% against mar-ginal (<0.77%) decrease in cotton-IoU score, while in case of model 2and model 3, reduction in the number ofﬁlters resulted into a substan- tial reduction in cotton-IoU score. Therefore, considering the segmenta-tion accuracy as well as inference time of proposed models and theirvariants, three models were selected for further ﬁne-tuning. For ease of reference, these selected models will be designated as: Model 1: model 1 with 80% of the number ofﬁlters;Model 2: model 2 with 100% of the number ofﬁlters;Model 3:m o d e l3w i t h1 0 0 %o ft h en u m - ber ofﬁlters. These three selected models were trained for one-hundredepochs having Adam optimizer as an optimization algorithm with alearning rate of 0.001. The performance of these three trained modelswas evaluated using evaluation indexes as discussed earlier. FromFig. 13, it can be observed that the mean-IoU and cotton-IoU scoresare above 93% and 88%, respectively for all three models. Model 3 out-performs the other two models with a pixel accuracy value of 98.97%and a mean-IoU score of 94.65%. In terms of the cotton-IoU score, thehighest value of 91.03% was achieved by model 2. Li et al. (2016)em- ployed a region-based semantic segmentation method to detect thein-ﬁeld cotton bolls and achieved accuracy and IoU value of 97.0% and73.5% respectively, while in another study, Li et al. (2017)achieved ac- curacy and cotton-IoU value of 97.14% and 70.5%, respectively usingfully convolutional neural network for cotton segmentation. An openboll detection algorithm was proposed by Yeom et al. (2018)in whichthey achieved the highest values of 92.5%, 96.6%, 95.6%, and 96.1% forIoU, precision, recall, and F1-score, respectively. Singh et al. (2021) achieved precision and recall values of 97.53% and 88.69%, respectivelyusing their proposed algorithms which were developed using color fea-tures. After analyzing the performance of the proposed modelsquantitively, it can be concluded that all three developed models cansegment the in-ﬁeld cotton bolls with higher accuracy. Fig. 14shows the semantic segmentation results of cotton bolls obtained usingmodel 1, model 2, and model 3. It can be visually observed that the pro-posed models segmented the single as well as multiple instances ofin-ﬁeld cotton bolls successfully.3.5. Performance comparison with the state-of-the-art CNN modelsFor a comprehensive evaluation of the proposed models based onsegmentation accuracy and inference time, four state-of-the-artconvolutional networks namely InceptionV3, ResNet18, VGG19, andEffecientNet-B1 were used for the semantic segmentation of cottonbolls and segmentation results were compared with the proposedmodels.Fig. 15shows the qualitative performance comparison of devel-oped models with state-of-the-art convolutional networks. For a bettercomparison of the segmentation performance of networks, false posi-tive and false negative errors for each model are shown in Fig. 15.T h e false-positive and false-negative errors were obtained using groundtruth images. The green, white, and red colors in Fig. 15indicate the cor- rect classiﬁcation, false positive and false negative, respectively. It can beobserved visually fromFig. 15that VGG19, ResNet18, EffecientNet-B1,and InceptionV3 perform slightly better than the proposed models asthe number of white color pixels (false positive) and red color pixels(false negative) are comparably high in segmented results obtained
using the proposed models. The slightly superior performance ofstate-of-the-art networks as compared to the proposed models is justi-ﬁable as the number of trainable parameters in state-of-art networks ismultiple times higher than the proposed models. Table 3shows the quantiﬁed comparison between state-of-the-art networks and pro-posed networks along with trainable parameters. Model 2 achievedthe mean-IoU value of 94.15% while ResNet18 achieved the highestvalue of 96.54% among the state-of-the-art models. Therefore, despitehaving 76.06% fewer trainable parameters compared to ResNet18, theperformance of model 2 in terms of mean-IoU and accuracy declinedby 1.39% and 1.17%, respectively which is a marginal decline as com-pared to the decline in the number of trainable parameters. Model 3Table 3Quantitative evaluation of segmentation results obtained using proposed models andstate-of-the-art convolutional networks along with trainable parameters.Model Trainableparameters,MMean-IoUscore, %Precision,%Recall,%F-1 score,%VGG19 29.05 95.39 93.47 96.13 94.79ResNet18 14.33 96.54 96.10 95.84 95.96InceptionV3 29.89 96.37 96.68 95.78 96.22EfﬁcientNet-B1 12.58 96.40 96.56 96.42 96.48Model 1 3.48 93.84 94.57 95.67 95.12Model 2 3.43 94.15 97.57 93.63 95.56Model 3 8.17 94.65 95.16 96.61 95.88
Fig. 16.Inference time of developed models and state-of-the-art convolutional networks for semantic segmentation of cotton bolls.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
16which has the highest number of trainable parameters among proposedmodels, achieved a mean-IoU of 94.65% which is just 1.89%, 1.72%,1.75%, and 0.74% lesser than the values achieved using ResNet18,InceptionV3, EfﬁcientNet-B1, and VGG19 networks, respectively, whilethe number of trainable parameters in these networks is higher bymany folds.For the neural networks developed for real-time application, infer-ence time should be taken into account before its deployment. The seg-mentation algorithm with a higher inference time is not suitable forharvesting robots. Because, the higher inference time restricted itsreal-time application to some extent ( De-An et al., 2011) as the theoret- icalﬁeld capacity of harvesting robots (Sakai et al., 2008) decreased pro- portionally with an increase in inference time. Therefore, the inferencetime required by networks for semantic segmentation of cotton bollsper image was compared in the present study. The combination of aver-age time needed for model loading, image loading, pre-processing, andinference time is termed inference time in this study. To measure the in-ference time, in the present study, 15 numbers of images were used forpredictions and the average time taken for their prediction was re-corded. This procedure was repeatedﬁve times and the average ofﬁve rounds was considered as inference time. For this, a laptop having spec-iﬁcations of 8 GB RAM, Intel Core i5 CPU (without GPU), and Windows10 operating system was used. Proposed models will be deployed tohardware having similar speciﬁcations as speciﬁed above, therefore, in- ference time was calculated using the same hardware. Fig. 16shows the inference time of each network. The inference time for all proposedmodels is at least 22.0% less as compared to state-of-the-art networks.Among all state-of-the-art networks, Ef ﬁcientNet-B1 consumed the lowest inference time of 867 ms, followed by the ResNet18 modelwith 987 ms. The proposed model 1 consumed the lowest inferencetime of 572 ms which is 52.0% less as compared to the Ef ﬁcientNet-B1 network.Table 4shows the comparison of inference time between theproposed models as well as state-of-the-art models. In comparing theEfﬁcientNet-B1 model (smallest among the state-of-the-art modelsused in the present study) with the proposed Model 3 (biggest amongthe proposed models), it can be observed from Table 4that the infer- ence time for state-of-the-art models is at least 1.22 times higher thanthe proposed models. The inference time for VGG19 is 3.06 times higherwhen compared to model 1 against the marginal drop (1.55%) in themean-IoU score for model 1. We can conclude that all proposed modelsare signiﬁcantly faster than the state-of-the-art networks and hence,deployment of these developed models will increase the ﬁeld capacity of harvesting robots with marginal losses in segmentation accuracy.4. ConclusionsThe detection of in-ﬁeld cotton bolls is a challenging task due to var-ious factors associated such as environment lighting, complex back-ground, and non-uniformity in visual characteristics of cotton bolls.This study proposed three fully convolutional neural network modelsfor the semantic segmentation of in-ﬁeld cotton bolls. In these models, residual connections, skip connections, and multiple ﬁlter sizes were in- corporated to enhance the segmentation performance. Effects ofdropout rate in the convolutional layer and ﬁlter size were evaluated and found that in a convolutional layer, dropout is ineffective, whilean increase inﬁlter size reduced the segmentation accuracy for the cot-ton dataset. The cotton-IoU for all proposed models was found to beabove 88.0%, in model 2 achieved the highest cotton-IoU of 91.03%.The proposed models achieved an F1-score and pixel accuracy valuesgreater than 95.0% and 98.0%, respectively. Developed models were
also compared with state-of-the-art networks such as VGG19,ResNet18, EfﬁcientNet-B1, and InceptionV3. Despite having a limitednumber of trainable parameters as compared to state-of-the-art net-works, the proposed models performed well with mean-IoU of 93.84%,94.15%, and 94.65% against the mean-IoU value of 95.39%, 96.54%,96.40%, and 96.37% obtained using VGG19, ResNet18, Ef ﬁcientNet-B1, and InceptionV3 models, respectively. The inference time for the pro-posed models, which is an important factor for the real-time applicationof neural networks, was reduced up to 52.0% compared to state-of-the-art networks. Hence, it can be concluded that the three lightweight fullyconvolutional neural network models proposed in this study segmentthe in-ﬁeld cotton bolls faster with greater accuracy and can be de-ployed to the cotton harvesting robots for real-time recognition of cot-ton bolls for harvesting.CRediT authorship contribution statementNaseeb Singh:Software, Conceptualization, Methodology, Investi-gation, Writing–original draft, Data curation.V.K. Tewari:Supervision, Resources, Writing–review & editing.P.K. Biswas:Supervision, Writing–review & editing.L.K. Dhruw:Software, Validation, Visualization, Data curation.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgmentsTheﬁnancial support was received from the Indian Institute of Tech-nology Kharagpur, Kharagpur, West Bengal, India.References
Abdalla, A., Cen, H., Wan, L., Rashid, R., Weng, H., Zhou, W., He, Y., 2019. Fine-tuningconvolutional neural network with transfer learning for semantic segmentation ofground-level oilseed rape images in a ﬁeld with high weed pressure. Comput. Elec- tron. Agric. 167, 105091.https://doi.org/10.1016/j.compag.2019.105091 . Adhikari, S.P., Yang, H., Kim, H., 2019. Learning semantic graphics using convolutionalencoder–decoder network for autonomous weeding in paddy. Front. Plant Sci. 10,1404.https://doi.org/10.3389/fpls.2019.01404 . apeer [WWW Document], 2021. Apeer by ZEISS. https://www.apeer.com/home (accessed 11.1.21).Atila, Ü., Uçar, M., Akyol, K., Uçar, E., 2021. Plant leaf disease classi ﬁcation using EfﬁcientNet deep learning model. Ecol. Inform. 61, 101182. https://doi.org/10.1016/ j.ecoinf.2020.101182.Azizi, A., Abbaspour-Gilandeh, Y., Vannier, E., Dusséaux, R., Mseri-Gundoshmian, T.,Moghaddam, H.A., 2020. Semantic segmentation: a modern approach for identifyingTable 4Comparison of inference time among the CNN models. ⁎Model InceptionV3 VGG19 ResNet Ef ﬁcientNet-B1 Model 1 Model 2 Model 3InceptionV3 1.00VGG19 1.55 1.00ResNet18 0.87 0.56 1.00EfﬁcientNet-B1 0.77 0.50 0.88 1.00Model 1 0.51 0.33 0.58 0.66 1.00Model 2 0.56 0.36 0.64 0.73 1.10 1.00Model 3 0.63 0.41 0.72 0.82 1.24 1.13 1.00⁎Read as: Computationally, based on inference time, the VGG19 model is 1.55 times more expensive compared to the Inception model.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
17soil clods in precision farming. Biosyst. Eng. 196, 172 –182.https://doi.org/10.1016/j. biosystemseng.2020.05.022. Badrinarayanan, V., Kendall, A., Cipolla, R., 2017. SegNet: a deep convolutional encoder-decoder architecture for image segmentation. IEEE Trans. Pattern Anal. Mach. Intell.39, 2481–2495.https://doi.org/10.1109/TPAMI.2016.2644615 . Bakhsh, K., Ahmad, N., Kamran, M.A., Hassan, S., Abbas, Q., Saeed, R., Hashmi, M.S., 2016.Occupational hazards and health cost of women cotton pickers in Pakistani Punjab.BMC Public Health 16, 961.https://doi.org/10.1186/s12889-016-3635-3 . Bakhsh, K., Ahmad, N., Tabasum, S., Hassan, S., Hassan, I., 2017. Health hazards and adop-tion of personal protective equipment during cotton harvesting in Pakistan. Sci. TotalEnviron. 598, 1058–1064.https://doi.org/10.1016/j.scitotenv.2017.04.043 . Bao, W., Yang, Xinghua, Liang, D., Hu, G., Yang, Xianjun, 2021. Lightweight convolutionalneural network model forﬁeld wheat ear disease identiﬁcation. Comput. Electron. Agric. 189, 106367.https://doi.org/10.1016/j.compag.2021.106367 . Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with gradientdescent is difﬁcult. IEEE Trans. Neural Netw. 5, 157 –166.https://doi.org/10.1109/72. 279181.Braunack, M.V., Johnston, D.B., 2014. Changes in soil cone resistance due to cotton pickertrafﬁc during harvest on Australian cotton soils. Soil Tillage Res. 140, 29 –39.https:// doi.org/10.1016/j.still.2014.02.007 . Chen, C., Li, B., Liu, J., Bao, T., Ren, N., 2020. Monocular positioning of sweet peppers: aninstance segmentation approach for harvest robots. Biosyst. Eng. 196, 15 –28. https://doi.org/10.1016/j.biosystemseng.2020.05.005 . Chen, Z., Ting, D., Newbury, R., Chen, C., 2021. Semantic segmentation for partially oc-cluded apple trees based on deep learning. Comput. Electron. Agric. 181, 105952.https://doi.org/10.1016/j.compag.2020.105952 . Chollet, F., 2015.Keras.Colombi, T., Torres, L.C., Walter, A., Keller, T., 2018. Feedbacks between soil penetration re-sistance, root architecture and water uptake limit water accessibility and crop growth–a vicious circle. Sci. Total Environ. 626, 1026 –1035.https://doi.org/10.1016/j. scitotenv.2018.01.129.De-An, Z., Jidong, L., Wei, J., Ying, Z., Yu, C., 2011. Design and control of an apple harvestingrobot. Biosyst. Eng. 110, 112 –122.https://doi.org/10.1016/j.biosystemseng.2011.07. 005.Deng, J., Dong, W., Socher, R., Li, L.-J., Li, K., Fei-Fei, L., 2009. ImageNet: a large-scale hier-archical image database. 2009 IEEE Conference on Computer Vision and Pattern Rec-ognition. Presented at the 2009 IEEE Conference on Computer Vision and PatternRecognition, pp. 248 –255https://doi.org/10.1109/CVPR.2009.5206848 . Duong, L.T., Nguyen, P.T., Di Sipio, C., Di Ruscio, D., 2020. Automated fruit recognitionusing EfﬁcientNet and MixNet. Comput. Electron. Agric. 171, 105326. https://doi. org/10.1016/j.compag.2020.105326 . Dyrmann, M., Karstoft, H., Midtiby, H.S., 2016. Plant species classi ﬁcation using deep convolutional neural network. Biosyst. Eng. 151, 72 –80.https://doi.org/10.1016/j. biosystemseng.2016.08.024. Fujii, H., Tanaka, H., Ikeuchi, M., Hotta, K., 2021. X-net with different loss functions for cell
image segmentation. 2021 IEEE/CVF Conference on Computer Vision and PatternRecognition Workshops (CVPRW). Presented at the 2021 IEEE/CVF Conference onComputer Vision and Pattern Recognition Workshops (CVPRW), pp. 3788 –3795 https://doi.org/10.1109/CVPRW53098.2021.00420 . Gao, F., Wu, T., Chu, X., Yoon, H., Xu, Y., Patel, B., 2020. Deep residual inception encoder – decoder network for medical imaging synthesis. IEEE J. Biomed. Health Inform. 24,39–49.https://doi.org/10.1109/JBHI.2019.2912659 . Ghiasi, G., Lin, T.-Y., Le, Q.V., 2018. DropBlock: A Regularization Method for Convolutional Networks. arXiv:1810.12890 [cs].Glorot, X., Bordes, A., Bengio, Y., 2011. Deep sparse rectiﬁer neural networks. Proceedings of the Fourteenth International Conference on Arti ﬁcial Intelligence and Statistics. Presented at the Proceedings of the Fourteenth International Conference on Arti ﬁcial Intelligence and Statistics, JMLR Workshop and Conference Proceedings, pp. 315 –323. Gonzalez-Huitron, V., León-Borges, J.A., Rodriguez-Mata, A.E., Amabilis-Sosa, L.E.,Ramírez-Pereda, B., Rodriguez, H., 2021. Disease detection in tomato leaves via CNNwith lightweight architectures implemented in Raspberry Pi 4. Comput. Electron.Agric. 181, 105951.https://doi.org/10.1016/j.compag.2020.105951 . Google Colaboratory, 2021. Google Colaboratory [WWW Document]. https://colab. research.google.com/notebooks/basic_features_overview.ipynb (accessed 11.14.21). He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep Residual Learning for Image Recognition. arXiv:1512.03385 [cs].Hecht, H., Sarhan, M.H., Popovici, V., 2020. Disentangled autoencoder for cross-stain fea-ture extraction in pathology image analysis. Appl. Sci. 10, 6427. https://doi.org/10. 3390/app10186427.Hughs, S.E., Valco, T.D., Williford, J.R., 2008. 100 years of cotton production, harvesting,and ginning systems engineering: 1907-2007. Trans. ASABE 51, 1187 –1198.https:// doi.org/10.13031/2013.25234. Jarrett, K., Kavukcuoglu, K., Ranzato, M., LeCun, Y., 2009. What is the best multi-stage ar-chitecture for object recognition? 2009 IEEE 12th International Conference on Com-puter Vision. Presented at the 2009 IEEE 12th International Conference onComputer Vision, pp. 2146 –2153https://doi.org/10.1109/ICCV.2009.5459469 Jiang, B., He, J., Yang, S., Fu, H., Li, T., Song, H., He, D., 2019. Fusion of machine vision tech-nology and AlexNet-CNNs deep learning network for the detection of postharvestapple pesticide residues. Artif. Intell. Agric. 1, 1 –8.https://doi.org/10.1016/j.aiia. 2019.02.001.Kandel, I., Castelli, M., 2020. The effect of batch size on the generalizability of theconvolutional neural networks on a histopathology dataset. ICT Express 6, 312 –315. https://doi.org/10.1016/j.icte.2020.04.010 . Kang, J., Liu, L., Zhang, F., Shen, C., Wang, N., Shao, L., 2021. Semantic segmentation modelof cotton roots in-situ image based on attention mechanism. Comput. Electron. Agric.189, 106370.https://doi.org/10.1016/j.compag.2021.106370 .Kestur, R., Meduri, A., Narasipura, O., 2019. MangoNet: a deep semantic segmentationarchitecture for a method to detect and count mangoes in an open orchard. Eng.Appl. Artif. Intell. 77, 59 –69.https://doi.org/10.1016/j.engappai.2018.09.011 . Khanramaki, M., Askari Asli-Ardeh, E., Kozegar, E., 2021. Citrus pests classi ﬁcation using an ensemble of deep learning models. Comput. Electron. Agric. 186, 106192.https://doi.org/10.1016/j.compag.2021.106192 . Kingma, D.P., Ba, J., 2017.Adam: A Method for Stochastic Optimization. arXiv:1412.6980[cs].Kolhar, S., Jagtap, J., 2021. Convolutional neural network based encoder-decoder architec-tures for semantic segmentation of plants. Ecol. Inform. 64, 101373. https://doi.org/ 10.1016/j.ecoinf.2021.101373. Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classiﬁcation with deep convolutional neural networks. Adv. Neural Inf. Process. Syst. 25, 1097 –1105. Kumar, S.K., 2017.On Weight Initialization in Deep Neural Networks. arXiv:1704.08863[cs].Li, Y., Cao, Z., Lu, H., Xiao, Y., Zhu, Y., Cremers, A.B., 2016. In- ﬁeld cotton detection via region-based semantic image segmentation. Comput. Electron. Agric. 127, 475 –486. https://doi.org/10.1016/j.compag.2016.07.006 . Li, Y., Cao, Z., Xiao, Y., Cremers, A.B., 2017. DeepCotton: in- ﬁeld cotton segmentation using deep fully convolutional network. J. Electron. Imaging 26, 1. https://doi.org/10.1117/ 1.JEI.26.5.053028.Lin, M., Chen, Q., Yan, S., 2014. Network in Network. arXiv:1312.4400 [cs]. Liu, J.-S., Lia, H.-C., Jia, Z.-H., 2011. Image segmentation of cotton based on YCbCr color space andﬁsher discrimination analysis. Acta Agron. Sin. 37, 1274 –1279. Long, J., Shelhamer, E., Darrell, T., 2015. Fully Convolutional Networks for Semantic Seg- mentation. arXiv:1411.4038 [cs].Majeed, Y., Zhang, J., Zhang, X., Fu, L., Karkee, M., Zhang, Q., Whiting, M.D., 2018. Appletree trunk and branch segmentation for automatic trellis training using convolutionalneural network based semantic segmentation. IFAC-PapersOnLine, 6th IFAC Confer-ence on Bio-Robotics BIOROBOTICS 2018. 51, pp. 75 –80.https://doi.org/10.1016/j. ifacol.2018.08.064.Mehta, C., Chandel, N., Jena, P., Jha, A., 2019. Indian agriculture counting on farm mecha- nization. Agric. Mech. Asia Africa Latin Am. 50, 84 –89. Memon, Q.U.A., Wagan, S.A., Chunyu, D., Shuangxi, X., Jingdong, L., Damalas, C.A., 2019.Health problems from pesticide exposure and personal protective measures amongwomen cotton workers in southern Pakistan. Sci. Total Environ. 685, 659 –666. https://doi.org/10.1016/j.scitotenv.2019.05.173 . Milioto, A., Lottes, P., Stachniss, C., 2018. Real-Time Semantic Segmentation of Crop andWeed for Precision Agriculture Robots Leveraging Background Knowledge in CNNs.https://doi.org/10.48550/arXiv.1709.06764 . Molchanov, P., Tyree, S., Karras, T., Aila, T., Kautz, J., 2017. Pruning Convolutional Neural Networks for Resource Efﬁcient Inference. arXiv:1611.06440 [cs, stat]. Nair, V., Hinton, G.E., 2010. Rectiﬁed Linear Units Improve Restricted Boltzmann Machines. Undeﬁned.Ngugi, L.C., Abdelwahab, M., Abo-Zahhad, M., 2020. Tomato leaf segmentation algorithmsfor mobile phone applications using deep learning. Comput. Electron. Agric. 178,105788.https://doi.org/10.1016/j.compag.2020.105788 . Ou, X., Yan, P., Zhang, Y., Tu, B., Zhang, G., Wu, J., Li, W., 2019. Moving object detectionmethod via ResNet-18 with encoder –decoder structure in complex scenes. IEEE Access 7, 108152–108160.https://doi.org/10.1109/ACCESS.2019.2931922 . Panda, M.K., Sharma, A., Bajpai, V., Subudhi, B.N., Thangaraj, V., Jakhetiya, V., 2022. En-coder and decoder network with ResNet-50 and global average feature pooling forlocal change detection. Comput. Vis. Image Underst. 222, 103501. https://doi.org/ 10.1016/j.cviu.2022.103501. Peng, D., Zhang, Y., Guan, H., 2019. End-to-end change detection for high resolution sat-ellite images using improved UNet++. Remote Sens. 11, 1382. https://doi.org/10. 3390/rs11111382.Rahman, C.R., Arko, P.S., Ali, M.E., Iqbal Khan, M.A., Apon, S.H., Nowrin, F., Wasif, A., 2020.
Identiﬁcation and recognition of rice diseases and pests using convolutional neuralnetworks. Biosyst. Eng. 194, 112 –120.https://doi.org/10.1016/j.biosystemseng.2020. 03.020.Raikwar, S., Fehrmann, J., Herlitzius, T., 2022. Navigation and control development for afour-wheel-steered mobile orchard robot using model-based design. Comput. Elec-tron. Agric. 202, 107410.https://doi.org/10.1016/j.compag.2022.107410 . Ronneberger, O., Fischer, P., Brox, T., 2015. U-Net: Convolutional Networks for Biomedical Image Segmentation. arXiv:1505.04597 [cs].Roshanianfard, A., Noguchi, N., Okamoto, H., Ishii, K., 2020. A review of autonomous agri-cultural vehicles (the experience of Hokkaido University). J. Terrramech. 91,155–183.https://doi.org/10.1016/j.jterra.2020.06.006 . Sakai, S., Iida, M., Osuka, K., Umeda, M., 2008. Design and control of a heavy material han-dling manipulator for agricultural robots. Auton. Robot. 25, 189 –204.https://doi.org/ 10.1007/s10514-008-9090-y. Seidu, S.M., 2018. Growth and Instability in Cotton Cultivation in Northern India. EA 63.https://doi.org/10.30954/0424-2513.2.2018.20 . Shah, J., Gao, F., Li, B., Ghisays, V., Luo, J., Chen, Y., Lee, W., Zhou, Y., Benzinger, T.L.S.,Reiman, E.M., Chen, K., Su, Y., Wu, T., 2022. Deep residual inception encoder-decoder network for amyloid PET harmonization. Alzheimers Dement. https://doi. org/10.1002/alz.12564alz.12564.Shukla, S., Arude, V., Deshmukh, S., Patil, P., Mageshwaran, V., Sundaramoorthy, C., 2017.Mechanical harvesting of cotton: a global research scenario and Indian case studies.Cotton Res. J. 8, 46–57.Simonyan, K., Zisserman, A., 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition. arXiv:1409.1556 [cs].Singh, N., Tewari, V.K., Biswas, P.K., Pareek, C.M., Dhruw, L.K., 2021. Image processing al-gorithms for in-ﬁeld cotton boll detection in natural lighting conditions. Artif. Intell.Agric. 5, 142–156.https://doi.org/10.1016/j.aiia.2021.07.002 .N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
18Singh, N., Tewari, V.K., Biswas, P.K., Dhruw, L.K., Pareek, C.M., Singh, H.D., 2022. Semanticsegmentation of in-ﬁeld cotton bolls from the sky using deep convolutional neuralnetworks. Smart Agric. Technol. 2, 100045. https://doi.org/10.1016/j.atech.2022. 100045.Snipes, C.E., Baskin, C.C., 1994. Inﬂuence of early defoliation on cotton yield, seed quality,andﬁber properties. Field Crop Res. 37, 137 –143.https://doi.org/10.1016/0378-4290 (94)90042-6.Sugawara, Y., Shiota, S., Kiya, H., 2019. Checkerboard artifacts free convolutional neuralnetworks. APSIPA Trans. Signal Inf. Process. 8, e9. https://doi.org/10.1017/ATSIP. 2019.2.Sun, S., Li, C., Paterson, A.H., Chee, P.W., Robertson, J.S., 2019. Image processing algorithmsfor inﬁeld single cotton boll counting and yield prediction. Comput. Electron. Agric.166, 104976.https://doi.org/10.1016/j.compag.2019.104976 . Sun, K., Wang, X., Liu, S., Liu, C., 2021. Apple, peach, and pear ﬂower detection using semantic segmentation network and shape constraint level set. Comput. Electron.Agric. 185, 106150.https://doi.org/10.1016/j.compag.2021.106150 . Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,Rabinovich, A., 2014.Going Deeper with Convolutions. arXiv:1409.4842 [cs].Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2015. Rethinking the Inception Architecture for Computer Vision. arXiv:1512.00567 [cs].Tan, M., Le, Q.V., 2019. EfﬁcientNet: Rethinking Model Scaling for Convolutional NeuralNetworks.https://doi.org/10.48550/ARXIV.1905.11946 . Tang, H., Wang, B., Chen, X., 2020. Deep learning techniques for automatic butter ﬂys e g - mentation in ecological images. Comput. Electron. Agric. 178, 105739. https://doi.org/ 10.1016/j.compag.2020.105739 . Tassis, L.M., Tozzi de Souza, J.E., Krohling, R.A., 2021. A deep learning approach combininginstance and semantic segmentation to identify diseases and pests of coffee leavesfrom in-ﬁeld images. Comput. Electron. Agric. 186, 106191. https://doi.org/10.1016/ j.compag.2021.106191.Tedesco-Oliveira, D., Pereira da Silva, R., Maldonado, W., Zerbato, C., 2020. Convolutionalneural networks in predicting cotton yield from images of commercial ﬁelds. Comput. Electron. Agric. 171, 105307. https://doi.org/10.1016/j.compag.2020. 105307.TensorFlow Developers, 2021. TensorFlow. https://doi.org/10.5281/ZENODO.4724125 . Tian, J., Zhang, X., Zhang, W., Li, J., Yang, Y., Dong, H., Jiu, X., Yu, Y., Zhao, Z., Xu, S., Zuo, W.,2018. Fiber damage of machine-harvested cotton before ginning and after lintcleaning. J. Integr. Agric. 17, 1120 –1127.https://doi.org/10.1016/S2095-3119(17) 61730-1.Tompson, J., Goroshin, R., Jain, A., LeCun, Y., Bregler, C., 2015. Efﬁcient Object Localization using Convolutional Networks. arXiv:1411.4280 [cs].Waheed, A., Goyal, M., Gupta, D., Khanna, A., Hassanien, A.E., Pandey, H.M., 2020. Anoptimized dense convolutional neural network model for disease recognition andclassiﬁcation in corn leaf. Comput. Electron. Agric. 175, 105456. https://doi.org/10. 1016/j.compag.2020.105456. Wang, Y., Zhu, X., Ji, C., 2008. Machine vision based cotton recognition for cotton harvest-ing robot. In: Li, D. (Ed.), Computer and Computing Technologies in Agriculture, Vol-ume II, The International Federation for Information Processing. Springer US, Boston,MA, pp. 1421–
1425https://doi.org/10.1007/978-0-387-77253-0_92 . Wang, S., Li, Y., Yuan, J., Song, L., Liu, Xinghua, Liu, Xuemei, 2021. Recognition of cottongrowth period for precise spraying based on convolution neural network. Inf. Process.Agric. 8, 219–231.https://doi.org/10.1016/j.inpa.2020.05.001 .Williford, J., Brashears, A., Barker, G., 1994. Harvesting. Cotton Ginners Handbook. USDA Agricultural Research Service, Washington, D.C, pp. 11 –16. Wilson, D.R., Martinez, T.R., 2003. The general inef ﬁciency of batch training for gradient descent learning. Neural Netw. 16, 1429 –1451.https://doi.org/10.1016/S0893-6080 (03)00138-2.Xie, W., Wei, S., Zheng, Z., Yang, D., 2021. A CNN-based lightweight ensemble model fordetecting defective carrots. Biosyst. Eng. 208, 287 –299.https://doi.org/10.1016/j. biosystemseng.2021.06.008. Xing, C., Arpit, D., Tsirigotis, C., Bengio, Y., 2018. A Walk with SGD. arXiv:1802.08770 [cs, stat].Xu, L., Li, Y., Xu, J., Guo, L., 2020. Two-level attention and score consistency network forplant segmentation. Comput. Electron. Agric. 170, 105281. https://doi.org/10.1016/j. compag.2020.105281.Yan, T., Xu, W., Lin, J., Duan, L., Gao, P., Zhang, C., Lv, X., 2021. Combining multi-dimensional convolutional neural network (CNN) with visualization method fordetection ofAphis gossypiiglover infection in cotton leaves using hyperspectral imag-ing. Front. Plant Sci. 12, 604510. https://doi.org/10.3389/fpls.2021.604510 . Yeom, J., Jung, J., Chang, A., Maeda, M., Landivar, J., 2018. Automated open cotton bolldetection for yield estimation using unmanned aircraft vehicle (UAV) data. RemoteSens. 10, 1895.https://doi.org/10.3390/rs10121895 . Yin, X., Wu, D., Shang, Y., Jiang, B., Song, H., 2020. Using an Ef ﬁcientNet-LSTM for the recognition of single cow ’s motion behaviours in a complicated environment.Comput. Electron. Agric. 177, 105707. https://doi.org/10.1016/j.compag.2020. 105707.You, J., Liu, W., Lee, J., 2020. A DNN-based semantic segmentation for detecting weed andcrop. Comput. Electron. Agric. 178, 105750. https://doi.org/10.1016/j.compag.2020. 105750.Zabawa, L., Kicherer, A., Klingbeil, L., Töpfer, R., Kuhlmann, H., Roscher, R., 2020. Countingof grapevine berries in images via semantic segmentation using convolutional neuralnetworks. ISPRS J. Photogramm. Remote Sens. 164, 73 –83.https://doi.org/10.1016/j. isprsjprs.2020.04.002.Zaidner, G., Shapiro, A., 2016. A novel data fusion algorithm for low-cost localisation andnavigation of autonomous vineyard sprayer robots. Biosyst. Eng. 146, 133 –148. https://doi.org/10.1016/j.biosystemseng.2016.05.002 . Zhang, Shanwen, Zhang, Subing, Zhang, C., Wang, X., Shi, Y., 2019. Cucumber leaf diseaseidentiﬁcation with global pooling dilated convolutional neural network. Comput.Electron. Agric. 162, 422 –430.https://doi.org/10.1016/j.compag.2019.03.012 . Zhang, P., Yang, L., Li, D., 2020. Ef ﬁcientNet-B4-ranger: a novel method for greenhouse cucumber disease recognition under natural complex environment. Comput. Elec-tron. Agric. 176, 105652.https://doi.org/10.1016/j.compag.2020.105652 . Zhenlong, H., Qiang, Z., Jun, W., 2018. The prediction model of cotton yarn intensity basedon the CNN-BP neural network. Wirel. Pers. Commun. 102, 1905 –1916.https://doi. org/10.1007/s11277-018-5245-0 . Zou, K., Chen, X., Wang, Y., Zhang, C., Zhang, F., 2021. A modi ﬁed U-net with a speciﬁcd a t a argumentation method for semantic segmentation of weed images in the ﬁeld. Comput. Electron. Agric. 187, 106242. https://doi.org/10.1016/j.compag.2021. 106242.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 1 –19
19