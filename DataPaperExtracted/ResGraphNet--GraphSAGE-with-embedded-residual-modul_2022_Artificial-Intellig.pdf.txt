Artificial Intelligence in Geosciences 3 (2022) 148â€“156
Available online 8 December 2022
2666-5441/Â© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC
BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available at ScienceDirect
Artificial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artificial-intelligence-in-geosciences
Original research articles
ResGraphNet: GraphSAGE with embedded residual module for prediction of
global monthly mean temperature
Ziwei Chena,b, Zhiguo Wanga,âˆ—, Yang Yangb, Jinghuai Gaob
aSchool of Mathematics and Statistics, Xiâ€™an Jiaotong University, Xiâ€™an, 710049, Shaanxi, PR China
bSchool of Information and Communications Engineering, Xiâ€™an Jiaotong University, Xiâ€™an, 710049, Shaanxi, PR China
A R T I C L E I N F O
Keywords:
Graph neural network
GraphSAGE
ResNet
Global temperature predictionA B S T R A C T
Data-driven prediction of time series is significant in many scientific research fields such as global climate
change and weather forecast. For global monthly mean temperature series, considering the strong potential
of deep neural network for extracting data features, this paper proposes a data-driven model, ResGraphNet,
which improves the prediction accuracy of time series by an embedded residual module in GraphSAGE layers.
The experimental results of a global mean temperature dataset, HadCRUT5, show that compared with 11
traditional prediction technologies, the proposed ResGraphNet obtains the best accuracy. The error indicator
predicted by the proposed ResGraphNet is smaller than that of the other 11 prediction models. Furthermore,
the performance on seven temperature datasets shows the excellent generalization of the ResGraphNet. Finally,
based on our proposed ResGraphNet, the predicted 2022 annual anomaly of global temperature is 0.74722â—¦C,
which provides confidence for limiting warming to 1.5â—¦C above pre-industrial levels.
1. Introduction
The Global monthly mean temperature is a key climate indicator,
which is usually expressed as a temperature â€˜â€˜anomalyâ€™â€™ that is just
the difference from the average over a fixed period. Since 1850, as a
typical time series, temperature series has been measured at weather
stations, by ships and buoys and by satellites. The Paris Agreement is
a legally binding international treaty on climate change, and its goal
is to limit global warming to well around 2â—¦C, preferable to 1.5â—¦C,
compared to pre-industrial levels. Thus, as a typical prediction problem
of time series, it is significant to predict global mean temperature,
which demonstrates the megatrend of global warming and climate
changing. In the past few decades, prevalent climate models of the
Earth have been proposed and worked well for weather and climate
forecasting ( Gordon et al. , 2000 ; Stott and Kettleborough , 2002 ; Smith
et al. , 2007 ). However, not all significant physical and chemical pro-
cesses can be explicitly resolved by these deterministic models ( Bauer
et al. , 2015 ). Instead, in this study, the data-driven models ( Pathak
et al. , 2018 ; Arcomano et al. , 2020 ; Taylor et al. , 2022 ) are focused
on the near-term climate prediction ( Kushnir et al. , 2019 ) using time
series of the global monthly mean temperature.
Nowadays, there are myriad data-driven methods to predict time
series including global mean temperature series, such as the autore-
gressive integrated moving average (ARIMA) ( Contreras et al. , 2003 ),
the seasonal autoregressive integrated moving average with exogenous
âˆ—Corresponding author.
E-mail address: emailwzg@mail.xjtu.edu.cn (Z. Wang).(SARIMAX) ( Guin, 2006 ), the random forest ( Breiman , 2001 ), the
support vector regression ( Smola and SchÃ¶lkopf , 2004 ), the recurrent
neural networks (RNN) ( Hopfield , 1982 ), temporal convolutional net-
work ( Li et al. , 2021 ), etc. In particular, as a deep learning model, the
RNN and its variants, such as long-short term memory (LSTM) ( Hochre-
iter and Schmidhuber , 1997 ) and gated recurrent unit (GRU) ( Chung
et al. , 2014 ), have been widely applied in time series prediction in
the Euclidean domain. However, different from many other time series,
the global temperature generally shows a rising trend due to increas-
ing accumulation of CO2in the atmosphere as a result of fossil fuel
consumption, which leads to some bottlenecks in the performance of
traditional data-driven methods. For example, because the global mean
temperature series show rich relational structure in the long sequences,
the inherent architecture of the RNN limits its representation ability
with slow and complex training procedures. To improve the accuracy
and efficiency of prediction, the graph neural network (GNN) ( Scarselli
et al. , 2009 ) is an emerging approach to expand the representation of
artificial neural network with the help of relevant concepts of graph
theory in the non-Euclidean domain.
The current GNN models are mainly categorized as recurrent GNNs,
convolutional GNNs, graph autoencoders, and spatialâ€“temporal GNNs
(Wu et al. , 2021 ). As a pioneering work, Kipf and Welling proposed
a simplified graph neural network model, graph convolution network
https://doi.org/10.1016/j.aiig.2022.11.001
Received 28 October 2022; Received in revised form 26 November 2022; Accepted 26 November 2022Artificial Intelligence in Geosciences 3 (2022) 148â€“156
149Z. Chen et al.
Fig. 1. (a) The global surface temperature dataset, HadCRUT5, includes a global mean temperature series from 1850 to 2022 in every month, and (b) its temperature anomalies
distribution.
(GCN), that operates directly on a graph and induces embedding vec-
tors of nodes based on the properties of their neighborhoods ( Kipf and
Welling , 2016 ). The major limitation of GCN is inherently transductive,
GraphSAGE is an inductive framework that can quickly generate em-
beddings and predict for unseen information ( Hamilton et al. , 2017 ).
Whereas, due to over-smoothing problem, it is difficult to improve
the predictive performance with stacked up many GraphSAGE layers.
Meanwhile, as a classical deep convolution neural network (CNN) with
strong feature extraction, the residual neural network (ResNet) helps in
tackling the vanishing gradient problem using identity mappings ( He
et al. , 2016 ). Thus, the embedded ResNet module also can act as a
buffer layer to reduce the over-smoothing problem of the GraphSAGE
layers ( Oono and Suzuki , 2020 ).
Therefore, we propose a specific deep neural network, called Res-
GraphNet, for the prediction of the global monthly mean temperature
series. The ResGraphnet is an end-to-end architecture that consists of
two GraphSAGE layers, an embedded ResNet layer and a full-connected
(FC) layer. Meanwhile, the path graph is selected to represent the
temperature time series, as the underlying topology of GraphSAGE.
The subsequent experimental results show that compared with simply
using the GraphSAGE or the ResNet as the network layers, our proposed
ResGraphNet has a stronger accuracy and a generalization ability in
predicting time series, especially monthly mean temperature series. Our
source code is available at https://doi.org/10.5281/zenodo.7213337 .
Compared with the previous models which has been commonly
used in prediction of time series, our contributions of the proposed
ResGraphNet mainly includes the following points:
1. To the best of our limited knowledge, this is the first study
to model a temperature series as a path graph and learn the
characteristics of the global temperature between the connection
weights of each node in the GNN with embedded ResNet.
2. Results on several temperature time series datasets demonstrate
that our method outperforms state-of-the-art temperature series
methods, with possessing a shorter training time and faster
convergence speed.
3. Our proposed ResGraphNet predicts that the 2022 annual
anomaly of global temperature is 0.74722â—¦C, which provides a
confidence for limiting warming to 1.5â—¦C above pre-industrial
levels.The rest of this paper is organized as follows. The Chapter II briefly
introduces the basic concepts of the ResNet and the GraphSAGE, and
then elaborates the architecture of the proposed ResGraphNet. The
Chapter III shows experiments of datasets that reveal the performance
of the proposed ResGraphNet. Finally, conclusions are drawn in the
Chapter IV.
2. Data and method
2.1. Datasets of temperature time series
For understanding and predicting global surface temperature
changes, a public time series HadCRUT5 (Morice et al. , 2021 ), the
global surface temperature dataset from 1850.Jan to 2022.Mar in every
month is analyzed mainly in this study. As shown in Fig. 1, since 1936,
because of increasing accumulation of CO2in the atmosphere as a result
of huge fossil energy consumption, the global temperature is increasing
gradually ( Arrow , 2007 ). This is a challenge to those methods that use
past data to predict the future information ( Dougherty et al. , 2005 ;
Hastings et al. , 2014 ; Terando et al. , 2014 ), because it is difficult to
learn the drastic changes in temperature data in the last few years.
Therefore, to tackle this problem, we build the novelty architecture of
ResGraphNet.
In additional, ERSSTv4 ( Smith et al. , 2008 ), ERSSTv3b ( Smith et al. ,
2008 ), Berkeley-Earth ( Rohde et al. , 2013 ), HadSST3 ( Kennedy et al. ,
2011 ), and ERA5 ( Hersbach et al. , 2020 ) also are temperature data
measured in different ways and in different areas. These datasets
also are applied to verify the generalization ability of the proposed
ResGraphNet.
2.2. Residual neural network
ResNet is a type of neural networks that applies identity mapping.
It is known that simply stacking up CNN layers cannot indefinitely
improve the performance of the CNN model, due to the vanishing
gradient problem ( Glorot and Bengio , 2010 ). To solve this problem
effectively, the concept of a highway network is proposed as ( Srivastava
et al. , 2015 )
ğ‘¦=îˆ²(ğ‘¥,ğ‘¤ğ‘–)â‹…ğœ(ğ‘¥,ğ‘¤ğ‘¡) +ğ‘¥â‹…îˆ¯(ğ‘¥,ğ‘¤ğ‘) (1)Artificial Intelligence in Geosciences 3 (2022) 148â€“156
150Z. Chen et al.
Fig. 2. The architecture of a ResNet module that is applied in this study.
whereğ‘¥andğ‘¦are the input and output of the layers aimed, and the
ğœ(ğ‘¥,ğ‘¤ğ‘¡)andîˆ¯(ğ‘¥,ğ‘¤ğ‘)are the transform gate and the carry gate, respec-
tively. For simplifying parameters and reducing the risk of over fitting,
the ResNet set these two gates as the identical mapping(i.e., ğœ(ğ‘¥,ğ‘¤ğ‘¡) =
1,îˆ¯(ğ‘¥,ğ‘¤ğ‘) = 1) (He et al. , 2016 ), then the forward propagation formula
is
ğ‘¦=îˆ²(ğ‘¥,ğ‘¤ğ‘–) +ğ‘¥ (2)
where the function îˆ²(ğ‘¥,ğ‘¤ğ‘–)represents the residual mapping to be
learned. As shown in Fig. 2, in this study, the ResNet module is mainly
composed of 4 CNN layers, which can be expressed as
â§
âª
âª
âª
âª
âª
â¨
âª
âª
âª
âª
âªâ©â„1=ğ¶ğ‘ğ‘1(ğ‘¥)
â„2=ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (â„1)
â„3=ğ¶ğ‘ğ‘2(â„2)
â„4=ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (â„3)
â„5=ğ¶ğ‘ğ‘3(â„3)
â„6=â„5+â„1
ğ‘¦=ğ¶ğ‘ğ‘4(â„6)(3)
whereğ‘¥andğ‘¦is the input and output of our ResNet module ( Fig. 2)
andğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ is the dropout layer. In the following content, the ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡
also denotes the forward propagation process in Eq. (3).
2.3. GraphSAGE neural network
In this study, a graph is defined as ğº= (ğ‘‰,ğ¸ ), whereğ‘‰=
{ğ‘£0,ğ‘£1,â€¦,ğ‘£ğ‘âˆ’1}denotes the nodes and ğ¸denotes the edges. ğ´âˆˆRğ‘Ã—ğ‘
is the adjacency matrix. If there is an edge from ğ‘£ğ‘–toğ‘£ğ‘—, that is
âˆƒ(ğ‘£ğ‘–,ğ‘£ğ‘—) âˆˆğ¸, thenğ´ğ‘–ğ‘—= 1otherwiseğ´ğ‘–ğ‘—= 0. And then the degree
matrixğ·is defined as ğ·ğ‘–ğ‘—=âˆ‘ğ‘âˆ’1
ğ‘—=0ğ´ğ‘–ğ‘—. Thus, the Laplacian matrix ğ¿
is defined as ğ¿=ğ·âˆ’ğ´. In addition, the normalized form of Laplacian
matrix îˆ¸=ğ·âˆ’1âˆ•2ğ¿ğ·âˆ’1âˆ•2is often used in practical applications. If datainformation of node ğ‘£ğ‘–isğ’™ğ‘–, then GraphSAGE updates node information
by sampling and aggregating adjacent points of each node ğ‘£ğ‘–, that is:
ğ’™â€²
ğ‘–=ğ‘¾1ğ’™ğ‘–+ğ‘¾2â‹…ğ‘šğ‘’ğ‘ğ‘›ğ‘—âˆˆğ‘(ğ‘—)ğ’™ğ‘— (4)
whereğ‘(ğ‘—)is the set of adjacent points of a node ğ‘£ğ‘–, which can be
obtained by the adjacency matrix ğ‘¨of a graph ğº;ğ‘¾1andğ‘¾2are
the learnable network weights; ğ‘šğ‘’ğ‘ğ‘› demonstrates that the aggregation
method adopts the mean aggregation. Fig. 3 shows the running process
of the GraphSAGE. For the random sampling process of nodes, the
number of the first-order sampling nodes is 1 and the number of the
second-order sampling nodes is 1. As shown in Fig. 3(b), the node
aggregation takes the mean value after weighted summation of the
information of adjacent points. Then, we update the information of the
current central node, as shown in Fig. 3(c).
2.4. Proposed ResGraphNet
To pay attention to rich correlation in the temperature time di-
mension, we propose that the ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ module is embedded in two
GraphSAGE layers. Assuming that the input and the output of the
network are ğ’™andğ’š, its forward propagation can be expressed as:
â§
âª
â¨
âªâ©ğ‘“1=ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸1(ğ’™,ğ‘¨)
ğ‘“2=ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ (ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (ğ‘“1))
Ì‚ğ’š=ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸2(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (ğ‘“2),ğ‘¨)(5)
whereğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸ğ‘–is theğ‘–th GraphSAGE layer, ğ‘–= 1,2.ğ‘¨is the
adjacency matrix of the graph.
Here, notice that the GraphSAGE layers (including most other GNN
layers) need a given topology structure of a graph in advance (i.e. the
adjacency matrix ğ‘¨). For a specific time series, it is an open topic to
build an available graph ( Lacasa et al. , 2008 , 2009 ; Wang et al. , 2019 ;
Chen et al. , 2022 ). In this study, to preserve the inherent correlation
and causality of time series, we consider adjacency matrix ğ‘¨ğ‘ğ‘ğ‘¡â„of the
path graph which satisfies
ğ‘¨ğ‘ğ‘ğ‘¡â„=â¡
â¢
â¢
â¢
â¢
â¢
â¢â£0 1 0 â‹¯ 0 0
0 0 1 â‹¯ 0 0
0 0 0 â‹¯ 0 0
â‹® â‹® â‹® â‹± â‹® â‹®
0 0 0 â‹¯ 0 1
0 0 0 â‹¯ 0 0â¤
â¥
â¥
â¥
â¥
â¥
â¥â¦(6)
For the path graph, each node ğ‘£ğ‘–has an edge ğ‘’ğ‘–,ğ‘–+1pointing to the next
nodeğ‘£ğ‘–+1, and the time series prediction task studied in this paper is to
utilize the past data to predict future data. Based on this idea, a node
ğ‘£ğ‘¡can be constructed for each time ğ‘¡, setğ’™= [ğ‘¥0,ğ‘¥1,â€¦,ğ‘¥ğ¿âˆ’1]ğ‘‡,ğ‘¥ğ‘¡âˆˆ
Rğ‘€Ã—1,ğ‘¡= 0,1,â€¦,ğ¿âˆ’ 1, thenğ‘¥ğ‘¡is the value of node ğ‘£ğ‘¡,ğ‘¨ğ‘ğ‘ğ‘¡â„âˆˆRğ¿Ã—ğ¿.
So the role of ğ‘¨ğ‘ğ‘ğ‘¡â„, is to make the value ğ‘¥ğ‘¡of time series ğ’™at timeğ‘¡
(the value of node ğ‘£ğ‘¡) be related to the value ğ‘¥ğ‘¡+1of next time ğ‘¡+1(the
value of node ğ‘£ğ‘¡+1).
Consequently, to further alleviate the gradient disappearance prob-
lem, a FC layer is applied to mitigate the instability of results due to
the residual connection. From Fig. 4, the proposed ResGraphNet can be
written as
â§
âª
âª
â¨
âª
âªâ©ğ‘“1=ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸1(ğ’™,ğ‘¨)
ğ‘“2=ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ (ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (ğ‘“1))
ğ‘“3=ğ¹ğ¶(ğ‘“1+ğ‘“2)
Ì‚ğ’š=ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸2(ğ·ğ‘Ÿğ‘œğ‘ğ‘œğ‘¢ğ‘¡ (ğ‘“3),ğ‘¨)(7)
Ideally, for the proposed ResGraphNet, the main features of time series
will be learned by the shallow layers ( ğºğ‘Ÿğ‘ğ‘â„ğ‘†ğ´ğºğ¸ ), and more detailed
features will be extracted by the deeper module ( ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ ). To verify
performance of the ResGraphNet, the experimental results are shown
in Chapter 3.Artificial Intelligence in Geosciences 3 (2022) 148â€“156
151Z. Chen et al.
Fig. 3. The operation process of the GraphSAGE, involves (a) the sampling process, (b) the aggregation process and (c) the updating nodes.
Fig. 4. Architecture of the proposed ResGraphNet is defined in Eq. (7), where the
ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ module is defined in Eq. (3).
3. Experiment and result
This chapter first shows the training dataset and the test dataset
based on real time series data. Then, we build models based on different
algorithms and set the parameters of these different models. Finally, the
training dataset are applied to train each model, while the test dataset
are used to evaluate the performance of models.
3.1. Dataset samples
Now we build training datasets and test datasets based on Had-
CRUT5 . At the beginning, if we set the length of time series as ğ‘™, then
it can be denoted as ğ‘«= [ğ‘‘0,ğ‘‘1,â€¦,ğ‘‘ğ‘™âˆ’1]. The first 50% of the data are
selected as the training dataset. ğ‘«ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and the last 50% are selected
as the test dataset ğ‘«ğ‘¡ğ‘’ğ‘ ğ‘¡, whose lengths are ğ‘™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘™ğ‘¡ğ‘’ğ‘ ğ‘¡respectively.
The target task of this paper is to predict time series. We hope to
use the data before time ğ‘¡to predict the data at time ğ‘¡and the error
between the prediction result and the real data will be as small as
possible. To achieve this purpose, for each time ğ‘¡, we set the input data
ğ‘¥ğ‘¡= [ğ‘‘ğ‘¡âˆ’ğ‘™ğ‘¥,ğ‘‘ğ‘¡âˆ’ğ‘™ğ‘¥+1,â€¦,ğ‘‘ğ‘¡âˆ’1]and the input label ğ‘¦ğ‘¡= [ğ‘‘ğ‘¡], and each pair of
ğ‘¥ğ‘¡andğ‘¦ğ‘¡constitute a sample ğ‘ ğ‘¡, thenğ‘ ğ‘¡= {ğ‘¥ğ‘¡,ğ‘¦ğ‘¡}, whereğ‘™ğ‘¥is the length
of input data ğ‘¥. In other words, we use ğ‘™ğ‘¥data before time ğ‘¡to predict
the data at time ğ‘¡. According to the above steps, the training dataset
ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›= [ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
0,ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
1,â€¦,ğ‘ ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›âˆ’1]can be constructed for ğ‘«ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and thetest dataset ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡= [ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡
0,ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡
1,â€¦,ğ‘ ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡âˆ’1]can be constructed for ğ‘«ğ‘¡ğ‘’ğ‘ ğ‘¡,
whereğ‘šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡are the number of samples of ğ‘«ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘«ğ‘¡ğ‘’ğ‘ ğ‘¡,
respectively. Meanwhile, ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡can be divided into training
datağ’™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, training labels ğ’šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, test data ğ’™ğ‘¡ğ‘’ğ‘ ğ‘¡and test labels ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡. The
construction process of samples is shown in Fig. 5.
3.2. Parameters and indicator
For any sample ğ‘ ğ‘¡,ğ‘¥ğ‘¡is the input of the model and Ì‚ ğ‘¦ğ‘¡denotes the
corresponding output. The loss function of the proposed model is the
mean square error and the network weights are updated by the negative
direction of the gradient decline of the loss function.
For regression problems, the Root Mean Square Error (RMSE) is
generally selected as the performance evaluation indicator ( Bellocchio
et al. , 2012 ). However, the RMSE is easily affected by the data itself.
If there are great differences in the value distribution of different time
series, the magnitude of their RMSE may be completely different, which
makes it impossible to compare the testing results of these time series.
Therefore, we further apply the determination coefficient (also named
as R2score) as our indicator. For any model, we denote the input data
and predicted results as ğ’™ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›andÌ‚ğ’šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›respectively, then we set ğ’™ğ‘¡ğ‘’ğ‘ ğ‘¡
andÌ‚ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡for test set. Their R2score is defined as rğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, rğ‘¡ğ‘’ğ‘ ğ‘¡:
â§
âª
âª
â¨
âª
âªâ©rğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›=âˆ‘
ğ‘–(Ì‚ ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘–âˆ’ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›)2
âˆ‘
ğ‘–(ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘–âˆ’ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›)2
rğ‘¡ğ‘’ğ‘ ğ‘¡=âˆ‘
ğ‘–(Ì‚ ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ‘–âˆ’ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡)2
âˆ‘
ğ‘–(ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ‘–âˆ’ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡)2(8)
whereğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›=âˆ‘ğ‘¦ğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›
ğ‘–
ğ‘šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›,ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡=âˆ‘ğ‘¦ğ‘¡ğ‘’ğ‘ ğ‘¡
ğ‘–
ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡are mean values of the ğ’šğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›and the
ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡, respectively. If ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡of one model is closer to 1, this model has a
stronger time series prediction ability.
3.3. Performances on different models
Now we train all the models based on ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, and use ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡for
performance testing where we set ğ‘™ğ‘¥= 60 andğ‘™ğ‘¦= 1. In Table 1,
there are 12 models are applied, including the proposed ResGrapNet,
the ResNet, the GraphSAGE, the GNN with Unified Message Passaging
Model (UniMP) ( Shi et al. , 2020 ), the GCN ( Kipf and Welling , 2016 ),
the Graph Isomorphic Network (GIN) ( Morris et al. , 2019 ), the RNN
with LSTM, the RNN with GRU, the Random Forest Regression (Forest),
the Linear Regression (Linear), the ARIMA and the SARIMAX. We
calculate R2score of each model, as shown in Table 1. Using climate
spirals, we draw the true data and predicted results of the ğ‘ºğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›, and
the true data and predicted results of the ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡of 12 models, as shown
in Fig. 6. From Table 1, theğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡of the proposed ResGraphNet is 0.9980,
which is the best in all of 12 models. As shown in Fig. 6, the predicted
climate spirals of the proposed ResGraphNet is closest to the true data,
but the prediction of the Forest is the worst with the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡= 0.6397.
Moreover, because the time series is too long, it is not intuitive to
directly observe the curve between the prediction results and the true
data. Hence, we define the error index ğ’†= [ğ‘’0,ğ‘’1,â€¦,ğ‘’ğ‘šğ‘¡ğ‘’ğ‘ ğ‘¡âˆ’1]as
ğ’†=Ì‚ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡âˆ’ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡(9)Artificial Intelligence in Geosciences 3 (2022) 148â€“156
152Z. Chen et al.
Fig. 5. Samples construction process of the time series (When ğ‘™ğ‘¥= 3andğ‘™ğ‘¦= 1).
Table 1
R2scores calculated by different models.
Model ResGraphNet ResNet GraphSAGE UniMP GCN GIN
rğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›0.9975 0.9138 0.8597 0.8430 0.8955 0.8621
rğ‘¡ğ‘’ğ‘ ğ‘¡0.9980 0.9039 0.8656 0.8440 0.8693 0.8664
Model LSTM GRU Forest Linear ARIMA SARIMAX
rğ‘¡ğ‘Ÿğ‘ğ‘–ğ‘›0.8871 0.8225 0.9464 0.9037 0.8782 0.8329
rğ‘¡ğ‘’ğ‘ ğ‘¡0.8030 0.8249 0.6397 0.9010 0.8214 0.8124
Table 2
Training time of different models.
Model Time[s]
ResGraphNet 78.78
ResNet 123.57
GraphSAGE 11.87
LSTM 149.08
GRU 126.70
Forest 1.17
Linear 0.04
Then we can count and plot the frequency distribution of all elements
ğ‘’ğ‘–in the ğ’†as shown in Fig. 7. Most of the errors ğ‘’ğ‘–computed by the
ResGraphNet are distributed within the interval [âˆ’0.03,0.06], but errors
ğ‘’ğ‘–of other models are outside this interval. In other words, the error ğ‘’ğ‘–
computed by the ResGraphNet is smaller than that of other models.
Furthermore, to compare the computation time, we record the time
spent on training different models, as shown in Table 2. From Table 2
it turns out that ResGraphNet can indeed converge faster than ResNetTable 3
ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡in different ğ‘™ğ‘¦and different models.
ğ‘™ğ‘¦ ResGraphNet GraphSAGE Linear GRU
2 0.9966 0.8735 0.8513 0.8241
5 0.9891 0.8324 0.8273 0.7995
12 0.9858 0.7988 0.7768 0.7690
and RNN with LSTM and GRU. Due to the addition of ğ‘…ğ‘’ğ‘ ğ‘ğ‘’ğ‘¡ module,
the training time of ResGraphNet is longer than that of GraphSAGE.
Because the traditional machine learning including Forest and Linear
adopts a simpler calculation process than the Neural Network, the
running time of them is also quite shorter. In this Scenario, the pro-
posed ResGraphNet makes a certain and acceptable sacrifice for the
computation time, in exchange for the highest prediction accuracy.
In the previous analysis, we set ğ‘™ğ‘¥= 60 andğ‘™ğ‘¦= 1. Actually, we
hope that ResGraphNet can still maintain its good performance, when
the length of predicted time series is becoming longer. The results ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡
based on different ğ‘™ğ‘¦and different models are shown in Table 3.ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡
decreases with the increase of ğ‘™ğ‘¦, but ResGraphNet outperforms other
models. In other words, the length of time series, that can be predicted
by the ResGraphNet, is limited in a way. When the temperature change
of 12 months (i.e. ğ‘™ğ‘¦= 12) in 2021 year is expected to be predicted, the
indicatorğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡between ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡andÌ‚ğ’šğ‘¡ğ‘’ğ‘ ğ‘¡can remain above 0.98. Therefore,
for the prediction of future temperature, the length of predicted time
seriesğ‘™ğ‘¦= 12 is available.Artificial Intelligence in Geosciences 3 (2022) 148â€“156
153Z. Chen et al.
Fig. 6. Climate spirals of (a) true data and predicted results by (b) ResGraphNet, (c) ResNet, (d) GraphSAGE, (e) UniMP, (f) GCN (g) GIN, (h) LSTM, (i) GRU, (j) Forest, (k)
Linear, (l) ARIMA, (m) SARIMAX models in ğ‘ºğ‘¡ğ‘’ğ‘ ğ‘¡.
3.4. Prediction of future temperature
To show the prediction performance of the ResGraphNet for un-
known future temperature, we firstly list the known true data and pre-
dicted results of 2021 in Table 4. As shown in Table 4, for HadCRUT5
dataset, the predicted result of annual mean temperature (0.76863â—¦C)
is close to the measured value (0.76185â—¦C). Similarly, the tempera-
tures in northern hemisphere HadCRUT5-N and southern hemisphere
HadCRUT5-S are also tested.
Next, Table 5 and Fig. 8 show the results in 2022 year. The corre-
spondingğ‘šğ‘ ğ‘’between the predicted results and measured data in 2022
January, February and March are 0.0015, 0.0109, 0.0048 respectively,
as well as the high similarity of annual temperature. Thus, the results
of prediction show the proposed ResGraphNet can accurately predict
the unknown temperature data in the future to some extent. Note that,based on our proposed model, the predicted 2022 annual anomaly of
global temperature is 0.74722â—¦C, which is smaller than that of 2021.
The predicted results of 2022 provide more confident to limit global
warming to well below 2â—¦C, preferably to 1.5â—¦C above pre-industrial
levels (red circle in Fig. 8).
3.5. Other datasets
Finally, to verify the generalization ability of the proposed Res-
GraphNet, we test the prediction performance of models based on
other different public datasets, including HadCRUT5-N, HadCRUT5-S,
ERSSTv4, ERSSTv3b, Berkeley-Earth, HadSST3 and ERA5. Meanwhile,
we additionally consider some other time-series prediction models,
i.e. the GraphSAGE, the Linear, the GRU, the ARIMA and the SARIMAX,Artificial Intelligence in Geosciences 3 (2022) 148â€“156
154Z. Chen et al.
Fig. 7. The distributions of error ğ‘’ğ‘–based on (a) ResGraphNet, (b) ResNet, (c) GraphSAGE, (d) UniMP, (e) GCN (f) GIN, (g) LSTM, (h) GRU, (i) Forest, (j) Linear, (k) ARIMA, (l)
SARIMAX models, where marked red and green bars represent different ğ‘’ğ‘–intervals.
Fig. 8. The temperature of true data and predicted (a) HadCRUT5, (b) HadCRUT5-N, and (c) HadCRUT5-S results in 2022, where the white-dashed-lines are â€˜â€˜Predictâ€™â€™ and the
yellow-solid-lines are â€˜â€˜Knownâ€™â€™. Annual predicted results provide more confident to limit warming to below 1.5â—¦C (red circle).
as the comparison methods. Table 6 reveals that the performance of the
proposed ResGraphNet is the best among six prediction models.
When applied to seven different datasets, the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡calculated by
using the proposed ResGraphNet is still closer to 1 than that of other
models. Therefore, the proposed ResGraphNet has a strong general-
ization ability in temperature prediction, and has potential to predict
temperature data in many scenarios.4. Conclusion
This paper proposes a specific ResGraphNet that consists of two
GraphSAGE layers, an embedded ResNet layer and a FC layer, which
is an end-to-end architecture for time series prediction of the global
monthly mean temperature. Based on some global mean temperature
datasets, the performance of the proposed ResGraphNet is evaluated.Artificial Intelligence in Geosciences 3 (2022) 148â€“156
155Z. Chen et al.
Table 4
The true and predicted temperature values in 2021.
Month HadCRUT5 HadCRUT5-N HadCRUT5-S
Predict Known Predict Known Predict Known
January 0.6845 0.7008 1.0578 1.1269 0.3087 0.2747
February 0.6703 0.5645 1.2658 0.8643 0.2965 0.2647
March 0.8831 0.7262 1.2226 1.0232 0.4923 0.4292
April 0.7804 0.7601 1.1715 1.1503 0.4793 0.3698
May 0.7806 0.7064 1.1965 1.0403 0.4164 0.3724
June 0.7198 0.7132 1.1938 1.1113 0.3479 0.3151
July 0.7335 0.7916 1.0732 1.0753 0.5140 0.5079
August 0.7894 0.7994 1.0810 1.0170 0.5751 0.5818
September 0.7658 0.8674 1.0130 1.0672 0.6145 0.6676
October 0.9283 0.9073 0.9592 1.2139 0.5632 0.6008
November 0.7504 0.8536 1.1245 1.2119 0.4870 0.4952
December 0.7368 0.7513 1.0862 1.0331 0.4197 0.4694
Annual 0.7686 0.7618 1.1204 1.0779 0.4595 0.4457
Table 5
The true and predicted temperature values in 2022.
Month HadCRUT5 HadCRUT5-N HadCRUT5-S
Predict Known Predict Known Predict Known
January 0.8300 0.7788 1.1400 1.1154 0.5140 0.4423
February 0.7628 0.7634 1.1290 1.0638 0.4636 0.4631
March 0.8569 0.8887 1.1028 1.2554 0.5425 0.5219
April 0.7702 1.1328 0.4518
May 0.9628 1.5161 0.5386
June 0.6580 1.2281 0.3322
July 0.6843 1.1572 0.2894
August 0.5555 0.9272 0.2483
September 0.7179 1.0627 0.4310
October 0.7553 1.1633 0.3561
November 0.6440 1.0605 0.4123
December 0.7684 1.0955 0.3096
Annual 0.7472 0.8103 1.1429 1.1449 0.4236 0.4758
Table 6
For seven datasets, the ğ‘Ÿğ‘¡ğ‘’ğ‘ ğ‘¡scores are calculated by different models.
Dataset ResGraphNet GraphSAGE Linear GRU ARIMA SARIMAX
HadCRUT5-N 0.9589 0.8273 0.8591 0.8214 0.8533 0.8244
HadCRUT5-S 0.9569 0.8019 0.8245 0.8123 0.8391 0.8182
ERSSTv4 0.9492 0.8135 0.8221 0.8156 0.8380 0.8364
ERSSTv3b 0.9389 0.7883 0.8279 0.7915 0.8135 0.8124
Berkeley-Earth 0.9624 0.8609 0.8943 0.8538 0.8986 0.8809
HadSST3 0.9923 0.9658 0.9754 0.9454 0.9228 0.8681
ERA5 0.9162 0.8112 0.7927 0.8018 0.7756 0.7540
The results of the comparison methods show that the proposed Res-
GraphNet has the best accuracy and generalization to predict monthly
anomaly of temperature series. In addition, the predicted 2022 annual
anomaly of global mean temperature is 0.74722â—¦C, which helps us
understand the trend of global warming of 1.5â—¦C above pre-industrial
levels.
However, there are still two problems that have not been well
solved in this paper. First, there is still no better method to choose
the best graph structure for specific data such as temperature time
series. Second, this paper does not solve the problem of long-term
prediction of temperature series. Therefore, We hope to further improve
the performance of the current model.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Funding
This work was supported by the National Natural Science Founda-
tion of China under Grant 41974137.References
Arcomano, T., Szunyogh, I., Pathak, J., Wikner, A., Hunt, B.R., Ott, E., 2020. A
machine learning-based global atmospheric forecast model. Geophys. Res. Lett. 47
(9), e2020GL087776.
Arrow, K.J., 2007. Global climate change: A challenge to policy. Econ. Voice 4 (3),
http://dx.doi.org/10.2202/1553-3832.1270.
Bauer, P., Thorpe, A., Brunet, G., 2015. The quiet revolution of numerical weather
prediction. Nature 525 (7567), 47â€“55.
Bellocchio, F., Ferrari, S., Piuri, V., Borghese, N.A., 2012. Hierarchical approach for
multiscale support vector regression. IEEE Trans. Neural Netw. Learn. Syst. 23 (9),
1448â€“1460.
Breiman, L., 2001. Random forests. Mach. Learn. 45 (1), 5â€“32.
Chen, Z., Wang, Z., Wu, S., Wang, Y., Gao, J., 2022. MagInfoNet: Magnitude estimation
using seismic information augmentation and graph transformer. Earth Space Sci. 9
(12), http://dx.doi.org/10.1029/2022EA002580, e2022EA002580.
Chung, J., Gulcehre, C., Cho, K., Bengio, Y., 2014. Empirical evaluation of gated
recurrent neural networks on sequence modeling. arXiv preprint arXiv:1412.3555.
Contreras, J., Espinola, R., Nogales, F., Conejo, A., 2003. ARIMA models to predict
next-day electricity prices. IEEE Trans. Power Syst. 18 (3), 1014â€“1020. http:
//dx.doi.org/10.1109/TPWRS.2002.804943.
Dougherty, M.R., Scheck, P., Nelson, T.O., Narens, L., 2005. Using the past to predict
the future. Memory Cogn. 33 (6), 1096â€“1115.
Glorot, X., Bengio, Y., 2010. Understanding the difficulty of training deep feedforward
neural networks. In: Proceedings of the Thirteenth International Conference on
Artificial Intelligence and Statistics. JMLR Workshop and Conference Proceedings,
pp. 249â€“256.
Gordon, C., Cooper, C., Senior, C.A., Banks, H., Gregory, J.M., Johns, T.C., Mitchell, J.F.,
Wood, R.A., 2000. The simulation of SST, sea ice extents and ocean heat transports
in a version of the Hadley centre coupled model without flux adjustments. Clim.
Dynam. 16 (2), 147â€“168.
Guin, A., 2006. Travel time prediction using a seasonal autoregressive integrated
moving average time series model. In: 2006 IEEE Intelligent Transportation Systems
Conference. pp. 493â€“498. http://dx.doi.org/10.1109/ITSC.2006.1706789.
Hamilton, W., Ying, Z., Leskovec, J., 2017. Inductive representation learning on large
graphs. Adv. Neural Inf. Process. Syst. 30.
Hastings, S.N., Whitson, H.E., Sloane, R., Landerman, L.R., Horney, C., Johnson, K.S.,
2014. Using the past to predict the future: Latent class analysis of patterns of health
service use of older adults in the emergency department. J. Am. Geriatr. Soc. 62
(4), 711â€“715.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recog-
nition. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition. pp. 770â€“778.
Hersbach, H., Bell, B., Berrisford, P., Hirahara, S., HorÃ¡nyi, A., MuÃ±oz-Sabater, J.,
Nicolas, J., Peubey, C., Radu, R., Schepers, D., et al., 2020. The ERA5 global
reanalysis. Q. J. R. Meteorol. Soc. 146 (730), 1999â€“2049.
Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8),
1735â€“1780. http://dx.doi.org/10.1162/neco.1997.9.8.1735.
Hopfield, J.J., 1982. Neural networks and physical systems with emergent collective
computational abilities. Proc. Natl. Acad. Sci. 79 (8), 2554â€“2558.
Kennedy, J.J., Rayner, N.A., Smith, R.O., Parker, D.E., Saunby, M., 2011. Reassessing
biases and other uncertainties in sea surface temperature observations measured
in situ since 1850: 1. Measurement and sampling uncertainties. J. Geophys. Res.:
Atmos. 116 (D14), http://dx.doi.org/10.1029/2010JD015218.
Kipf, T.N., Welling, M., 2016. Semi-supervised classification with graph convolutional
networks. arXiv preprint arXiv:1609.02907.
Kushnir, Y., Scaife, A.A., Arritt, R., Balsamo, G., Boer, G., Doblas-Reyes, F., Hawkins, E.,
Kimoto, M., Kolli, R.K., Kumar, A., et al., 2019. Towards operational predictions
of the near-term climate. Nature Clim. Change 9 (2), 94â€“101.
Lacasa, L., Luque, B., Ballesteros, F., Luque, J., Nuno, J.C., 2008. From time series to
complex networks: The visibility graph. Proc. Natl. Acad. Sci. 105 (13), 4972â€“4975.
Lacasa, L., Luque, B., Luque, J., Nuno, J.C., 2009. The visibility graph: A new method
for estimating the Hurst exponent of fractional Brownian motion. Europhys. Lett.
86 (3), 30001.
Li, J., Wu, Y., Li, Y., Xiang, J., Zheng, B., 2021. The temperature prediction of
hydro-generating units based on temporal convolutional network and recurrent
neural network. In: 2021 40th Chinese Control Conference. CCC, pp. 8228â€“8233.
http://dx.doi.org/10.23919/CCC52363.2021.9549853.
Morice, C.P., Kennedy, J.J., Rayner, N.A., Winn, J., Hogan, E., Killick, R., Dunn, R.,
Osborn, T., Jones, P., Simpson, I., 2021. An updated assessment of near-surface
temperature change from 1850: The HadCRUT5 data set. J. Geophys. Res.: Atmos.
126 (3), e2019JD032361.
Morris, C., Ritzert, M., Fey, M., Hamilton, W.L., Lenssen, J.E., Rattan, G., Grohe, M.,
2019. Weisfeiler and leman go neural: Higher-order graph neural networks. In:
Proceedings of the AAAI Conference on Artificial Intelligence, Vol. 33, no. 01. pp.
4602â€“4609.
Oono, K., Suzuki, T., 2020. Graph neural networks exponentially lose expressive
power for node classification. In: 8th International Conference on Learning
Representations. ICLR 2020, Addis Ababa, Ethiopia, April 26-30, 2020.Artificial Intelligence in Geosciences 3 (2022) 148â€“156
156Z. Chen et al.
Pathak, J., Hunt, B., Girvan, M., Lu, Z., Ott, E., 2018. Model-free prediction of large
spatiotemporally chaotic systems from data: A reservoir computing approach. Phys.
Rev. Lett. 120 (2), 024102.
Rohde, R., Muller, R., Jacobsen, R., Perlmutter, S., Rosenfeld, A., Wurtele, J., Curry, J.,
Wickham, C., Mosher, S., 2013. Berkeley earth temperature averaging process.
Geoinform. Geostat. An Overview 1 (2), 1â€“13.
Scarselli, F., Gori, M., Tsoi, A.C., Hagenbuchner, M., Monfardini, G., 2009. The graph
neural network model. IEEE Trans. Neural Netw. 20 (1), 61â€“80. http://dx.doi.org/
10.1109/TNN.2008.2005605.
Shi, Y., Huang, Z., Feng, S., Zhong, H., Wang, W., Sun, Y., 2020. Masked label
prediction: Unified message passing model for semi-supervised classification. arXiv
preprint arXiv:2009.03509.
Smith, D.M., Cusack, S., Colman, A.W., Folland, C.K., Harris, G.R., Murphy, J.M., 2007.
Improved surface temperature prediction for the coming decade from a global
climate model. Science 317 (5839), 796â€“799.
Smith, T.M., Reynolds, R.W., Peterson, T.C., Lawrimore, J., 2008. Improvements to
NOAAâ€™s historical merged landâ€“ocean surface temperature analysis (1880â€“2006).
J. Clim. 2283â€“2296.Smola, A.J., SchÃ¶lkopf, B., 2004. A tutorial on support vector regression. Stat. Comput.
14 (3), 199â€“222.
Srivastava, R.K., Greff, K., Schmidhuber, J., 2015. Highway networks. arXiv preprint
arXiv:1505.00387.
Stott, P.A., Kettleborough, J.A., 2002. Origins and estimates of uncertainty in
predictions of twenty-first century temperature rise. Nature 416 (6882), 723â€“726.
Taylor, J.A., Larraondo, P., de Supinski, B.R., 2022. Data-driven global weather
predictions at high resolutions. Int. J. High Perform. Comput. Appl. 36 (2),
130â€“140. http://dx.doi.org/10.1177/10943420211039818.
Terando, A.J., Costanza, J., Belyea, C., Dunn, R.R., McKerrow, A., Collazo, J.A., 2014.
The southern megalopolis: using the past to predict the future of urban sprawl in
the southeast US. PLoS One 9 (7), e102261.
Wang, Z., Zhang, B., Gao, J., 2019. How to transform the seismic time series to a
graph? In: SEG Technical Program Expanded Abstracts 2019. Society of Exploration
Geophysicists, pp. 3459â€“3463.
Wu, Z., Pan, S., Chen, F., Long, G., Zhang, C., Yu, P.S., 2021. A comprehensive survey
on graph neural networks. IEEE Trans. Neural Netw. Learn. Syst. 32 (1), 4â€“24.
http://dx.doi.org/10.1109/TNNLS.2020.2978386.