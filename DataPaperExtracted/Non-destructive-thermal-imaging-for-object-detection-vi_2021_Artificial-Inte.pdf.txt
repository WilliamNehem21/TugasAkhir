Non-destructive thermal imaging for object detection via advanced deeplearning for robotic inspection and harvesting of chili peppers
Steven C. Hespeler, Hamidreza Nemati, Ehsan Dehghan-Niri ⁎
Intelligent Structures and Nondestructive Evaluation (ISNDE), Civil Engineering Department, New Mexico State University, Las Cruces, NM, USA
abstract article info
Article history:Received 15 March 2021Received in revised form 11 May 2021Accepted 11 May 2021Available online 15 May 2021
Keywords:Deep learningYou only look once (YOLO) v3Object detectionChili pepper fruitDeep Learning has been utilized in computer vision for object detection for almost a decade. Real-time object de-tection for robotic inspection and harvesting has gained interest during this time as a possible technique for high-quality machine assistance during agriculture applications.We utilize RGB and thermal images of chili peppers inan environment of various amounts of debris, pepper overlapping, and ambient lighting, train this dataset, andcompare object detection methods. Results are presented from the real-time and less than real-time objectdetection models. Two advanced deep learning algorithms, Mask-Regional Convolutional Neural Networks(Mask-RCNN) and You Only Look Once version 3 (YOLOv3)are compared in terms of object detection accuracyand computational costs. When utilizing the YOLOv3 architecture, an overall training mean average precision(mAP) value of 1.0 is achieved. Most testing images from this model score within a range from 97 to 100% con-ﬁdence levels in natural environment. It is shown that the YOLOv3 algorithm has superior capabilities to theMask-RCNN with over 10 times the computational speed on the chili dataset. However, some of the RGB test im-ages resulted in low classiﬁcation scores when heavy debris is present in the image. A signi ﬁcant improvement in the real-time classiﬁcation scores was observed when the thermal images were used, especially with heavy de-bris present. We found and report improved prediction scores with a thermal imagery dataset where YOLOv3struggled on the RGB images. It was shown that mapping temperature differences between the pepper andplant/debris can provide signiﬁcant features for object detection in real-time and can help improve accuracy ofpredictions with heavy debris, variant ambient lighting, and overlapping of peppers. In addition, successful ther-mal imaging for real-time robotic harvesting could allow the harvesting period to become more ef ﬁcient and open up harvesting opportunity in low light situations.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionDue to its lightly pungent, crisp, and smoky taste, the New Mexicochili pepper (Capsicum annuum)i sw i d e l yp o p u l a ri nt h eS o u t h w e s tr e -gion of the USA. Sometimes referred to as chiles, the pepper in NewMexico is a cash crop with an annual harvesting of approximately8000 to 10,000 acres and is used for consumption, processing intodried spice, or decorations (strung on ristras) ( Bosland et al., 1991). The agricultural production of chili peppers is the most consumedspicy crop throughout the world (Jiang et al., 2018). Throughout the world, the USA and other countries lack in total Capsicum annuumpro- duction compared to agricultural production achievements made byChina in 2019.Fig. 1displays the production quantities of the top 11countries worldwide.Total world production and area harvested have steadily increasedeach year since 1994. From 2009 to 2019, worldwide production ofthe crop increased by 10 million tonnes per year, from 28 to 38 milliontonnes (FAO, 2019).Fig. 2visualizes the worldwide trend. While de-mand for the chili pepper has increased, harvest production has roomfor growth. Agriculture producer prices (prices for crops at the initialsale point) do not indicate quantity values of production. Althoughone of the smaller total producing countries with the least amount ofland harvested, Netherlands achieved the highest selling point per hect-are (ha) of chili peppers among the entire group highlighted. Table 1 displays data from 2019 that showcases the production amount, landarea harvested, producer price at the initial sale, and how much priceeach hectare produces for these countries ( FAO, 2019). Using this data as a blueprint, countries with less land harvested could increase produc-tion to become more competitive and drive up their production yieldper hectare. We aim to illuminate and provide insight into this issue.Nondestructive imaging for robotic harvesting has been an effectivetool for assistance in the agriculture industry to harvest the fruit withdebris present while not harming the plant ( Gao et al., 2020). Due to the complexity of this task, an efﬁcient object detection and inspectionalgorithm are necessary for a robotic platform to be used in pepperArtiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
⁎Corresponding author.E-mail address:niri@nmsu.edu(E. Dehghan-Niri).
https://doi.org/10.1016/j.aiia.2021.05.0032589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/harvesting. Nondestructive quality assessment of peppers has beenstudied successfully using near-infrared (NIR) hyper-spectral imagingto not harm the plant (Jiang et al., 2018).Real-time robotic harvesting is a promising development in agricul-ture with some challenges (Kang et al., 2020). Challenges in this area re- sult from complex machine tasks that require powerful computationabilities, complex model architectures, and non-destructive measures,all while the machine is detecting the fruit. With advances in Convolu-tion Neural Network (CNN) techniques, object detection performancehas rapidly increased in recent years and is a suitable solution for real-time robotic harvesting. Gu et al. showcase the recent growth of CNNsthrough a broad scope of literature (Gu et al., 2018). The survey high- lights several improvements that have propelled the expansion ofCNN applications. A critical development is applying several new activa-tion functions that have increased in model performance. One ex-tremely effective activation function is the Recti ﬁed Linear Unit (ReLU). The transition to ReLU from sigmoid function solved the famousissue of vanishing gradient, which was impeding traditional Neural Net-works (NN) from learning on large datasets. ReLU can be mathemati-cally represented as:ai,j,k¼max z i,j,k,0/C0/C1 ð1Þwherez
i,j,kis the input of the activation function located at ( i,j). Other variants include Leaky ReLU (and many other variants), ELU, Maxout,and Probout. We refer the reader to the following reference for an in-depth analysis of CNN components (Gu et al., 2018). Thermal imaging is used to convert invisible radiation patterns of aparticular object to visible imaging for feature extraction ( Vadivambal and Jayas, 2011). This type of imaging can be utilized as a non-destructive method during real-time robotic harvesting since it is non-contact, non-invasive, and rapid (Vadivambal and Jayas, 2011). We be- lieve mapping temperature differences between the pepper and plant/de-bris can provide signiﬁcant features for object detection in real-time andcan help improve accuracy predictions with heavy debris, variant ambientlighting, and overlapping of peppers. In addition, successful thermal imag-ing for real-time robotic harvesting could allow the harvesting period tobecome more efﬁcient and open up harvesting opportunity in the eveninghours or low light situations.Fig. 3highlights how prominent thermal im-aging can be for extracting features than an RGB image.Modern architectures (one-stage and two-stage) for object detec-tion is implemented on a custom dataset to detect chili peppers inreal-time for robotic harvesting. In this investigation, we 1) compare
Fig. 1.Chili pepper production in 2019 ( FAO, 2019).
Fig. 2.Chili area harvested (blue) in ha and chili production (red) in tonnes ( FAO, 2019).
Table 1Chili pepper data 2019 (FAO, 2019).Country Production (tonnes) Area Harvested (ha) Producer Price (USD/t) Price per haNetherlands 375,000 1500 1177 $ 294,150.00 Tunisia 443,632 20,103 382 $ 8425.54USA 624,982 19,627 941 $ 29,964.24Algeria 675,168 21,767 643 $ 19,953.86Nigeria 753,116 99,715 1077 $ 8130.47Egypt 764,292 40,422 127 $ 2395.62Spain 1,402,380 21,430 927 $ 60,662.91Indonesia 2,588,633 300,377 1206 $ 10,388.93Turkey 2,625,669 92,089 432 $ 12,308.76Mexico 3,238,245 149,577 525 $ 11,370.24China 19,007,248 798,877 748 $ 17,801.52S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
103imagery features on RGB and thermal datasets, 2) compare two modernobject detection techniques on an RGB dataset for locating and identify-ing chili fruit, and 3) develop a thermal imaging technique to improveobject detection on images with high debris, various ambient lighting,and overlapping peppers using YOLOv3.2. Literature review2.1. Object detectionObject detection has a rich history of literature with vast improve-ments made recently that utilize computer vision to identify whereand what an object is in space. Literature generally accepts the notationthat two eras exist for the progression of object detection; traditionalobject detection before 2014 and the introduction of Deep Learning(DL) for object detection after 2014 (Zou et al., 2019). Before DL was uti- lized for object detection, many investigations were limited due to com-putation power and trainable image representation. We refer the readerto (Zou et al., 2019) for a comprehensive survey on object detection.2.1.1. Traditional object detectionViola and Jones (2001), introduced the Viola-Jones detector with thepurpose of robust rapid and real-time detection. In this work, the au-thors focus on simple features rather than pixels to speed the detectionprocess up. Three main contributions were made in the study. 1) IntegralImage which is a method for using intermediate image representationfor rapid rectangle computation, 2) an algorithm based on AdaBoostfor feature extraction by combing some features to form an effectiveclassiﬁer, and 3) a cascade method for classiﬁcation to discard or pass on viable information for further analysis and ultimately, increased de-tection performance.Viola and Jones (2004)demonstrated the robust- ness of this method by applying the techniques to facial recognition.Viola et al. (2005), utilized the cascade method for moving person de-tection along with AdaBoost for increasingly complex rejection regionsbased on a rule system.Another cornerstone in object detection was introducing the Histo-grams of Oriented Gradient (HOG) detection system presented inDalal and Triggs (2005). In this investigation, Dalal et al. created amethod that utilizes edge direction and gradient intensity to determinethe appearance of an image. This task is accomplished by separating thewindow into cells (spatial regions) and combining localized histogramsof gradients directions and edge orientations over every pixel. After theHOG descriptors are calculated, they are fed to a linear Support VectorMachine (SVM) for classiﬁcation purposes.The Deformable Part-based Model (DPM) was developed byFelzenszwalb et al. (2008)and builds off of the architecture from(Dalal and Triggs, 2005). Using the underlying blocks, the model estab-lishes HOG descriptors via histogram gradient magnitudes within each1D pixel. Filters are used to dictate weights in the detection window.Learning is accomplished on the PASCAL training data with a latentSVM. Using DPM, the authors won 2007, 2008, and 2009 Pascal VisualObject Classes (VOC) detection challenge ( Everingham et al., 2007).2.1.2. Deep learning for object detectionFor a comprehensive survey of Convolutional Neural Networks(CNN), we refer the reader to (Gu et al., 2018). DL approaches advanced signiﬁcantly in 2012 with the introduction of AlexNet that was based onLeNet-5 architecture, just with a deeper structure ( Krizhevsky et al., 2012).Two -step-based detectors examine the input (image), apply re-gional proposals, and detect objects. Then, those detected objects arecropped and processed by an entirely separate network to estimate.Two-step methods typically require more computation time becauseeach step requires resampling (three in total) ( Poirson et al., 2016). Typ- ically, one-stage detector networks divide regions and apply predictionbounding boxes with probabilities inside each region simultaneously.Fig. 4illustrates the two-stage (4a) and one-stage process (4b).2.1.3. CNN two-stage based detectorsGirshick et al. (2014)introduced the Regional CNN (R-CNN). Theyreported drastic improvements (at the time) to mean precision aver-age (mPA) metrics compared to the best performing method onVOC2012. It begins with extracting around 2000 bottom-up region
proposals (object candidate boxes) by Selective Search ( Uijlings et al., 2013). Next, each regional proposal was rescaled to ﬁxed image size (227 × 227) and used for model training via a CNN onan image database like ImageNet (Deng et al., 2009) to extract fea- tures. Lastly, a binary SVM classiﬁer is used to predict/detect an ob-ject(s) within each region.
Fig. 3.(a) RGB image, (b) Thermal image.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
104Girshick (2015), improved on the R-CNN by simplifying the learningprocess. The authors realized that the R-CNN is slow due to the networkperforming a“forward pass for each object proposal ”. Spatial PyramidPooling networks (SPP or SPPnet) were introduced by He et al. (2015) in an attempt to speed up training.Gu et al. (2018), utilized SPP to up- date all network layers duringﬁne-tuning.Ren et al. (2015), made
Fig. 6.YOLO architecture (Redmon et al., 2016.
Fig. 4.Two/One-stage detectors (Poirson et al., 2016): (a) Two-stage, (b) One-stage.
Fig. 5.Head architecture for Mask R-CNN ( He et al., 2017).S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
105improvements on the fast R-CNN design in the same year to move thealgorithm closer to real-time detection speeds. This version becametheﬁrst end-to-end and close to real-time object detector ( Zou et al., 2019). In theﬁrst stage, a Region Proposal Network (RPN) considers acandidate bounding box. In the second stage, feature extraction is com-pleted using Region of Interest (RoI) pooling (RoIPool) from each candi-date which executes classiﬁcation and bounding box regression ( Ren et al., 2015).Mask R-CNN is an extension of the fast R-CNN architecture and gen-erates a high-quality segmentation mask for each instance while ef ﬁ- ciently detecting objects in an image ( He et al., 2017). Using various Feature Pyramid Network (FPN) frameworks, R-50-FPN, R-101-FPN,X-101-FPN, and X-152-FPN, AP performance was measured at 40.8and 70.4 for enhanced detection and enhanced keypoint results, respec-tively.Fig. 5details the head architecture of the algorithm and high-lights the exact improvements made to the Faster R-CNN.Here, we see the algorithm applied with the ResNet (left) and FPNbackbones.2.1.4. CNN one-stage based detectorsYou Only Look Once (YOLO) is a real-time detecting system createdbyRedmon et al. (2016)with several incremental improvementsthrough the years (Redmon and Farhadi, 2017;Redmon and Farhadi, 2018). Most recent improvements have been made by Bochkovskiy et al. (2020). The original system design gets its name from only lookingat an image once, therefore treating every image (input) as a regressionproblem. Compared to R-CNN architectures that are complex pipelines,this improvement in simplicity might be the most important aspect ofthe YOLO system. The system consists of 3 main steps. First, inputs areresized to 448 × 448. Next, a single CNN is run. Finally, the Non-maxSuppression (NMS) technique attempts to correct multiple detectionson the same image.The YOLO architecture consists of 24 convolutional layers followedby 2 fully connected layers.Fig. 6shows the details of each layer inthe basic YOLO architecture. The network outputs a 7 × 7 × 30 tensorof predictions.In 2018 Redmon et al. established an incremental improvement tothe network design to include 53 convolutional layers. This versiondemonstrated exceptional accuracy due to the more robust design andis 3.8 times faster than RetinaNet. At the time of publication (2016),the YOLO system achieved more than twice the mPA compared to thenext best real-time system, 63.4% mAP to 26.1%, respectively. It also ob-serves the entire image, unlike region proposal-based architectures.Table 2summarizes the performance of YOLO compared to other real-time and less than real-time systems ( Redmon et al., 2016). Another critical aspect of YOLO performance is that when comparedto Fast R-CNN, YOLO had a smaller percentage of background errors butmore signiﬁcant percentage of localization errors. DL, especially CNNs,have been thrust upon the robotic harvesting research area for assis-tance during agriculture applications. The research area has seen asurge in investigations in 2019 and 2020 ( Naranjo-Torres et al., 2020). InHameed et al. (2018), investigators present a critical comparison ofmultiple modern computer vision techniques investigated by re-searchers to classify fruit and vegetables. In the study, Support VectorMachine (SVM), K-Nearest Neighbour (KNN), Decision Trees, Arti ﬁcial Neural Networks (ANN), and CNN's are reviewed for a variety of fruitand vegetable classiﬁcation. However, the Mask R-CNN and YOLOv3methods are not evaluated in the investigation.Li et al. (2019), reviewed non-destructive optical techniques forberries, mainly strawberries and blueberries. The researchers identi ﬁed and reviewed 13 methods, one of which was computer vision. The re-
view did not detail speciﬁc computer vision techniques; however, sev-eral studies are mentioned that detail computer vision as a viablemethod for berry classiﬁcation. The researchers state that one mainissue with computer vision is “inconsistent ambient illumination andcomplex background.”For these reasons, we have selected theYOLOv3 for real-time object detection.2.2. Object detection in agricultureIn the past decade, different object detection methods have beensuccessfully adopted in several agricultural applications. Hameed et al.
Fig. 7.HOGﬁlter: (a) Horizontal kernel, (b) Vertical kernel.
Fig. 8.Mask R-CNN block diagram.Table 2YOLO performance (Redmon et al., 2016).Real-Time Detectors Train mAP FPS100 Hz DPM (Sadeghi and Forsyth, 2014) 2007 16.0 100 30 Hz DPM (Sadeghi and Forsyth, 2014) 2007 26.1 30 Fast YOLO 2007 + 2012 52.7 155 YOLO 2007 + 2012 63.445 Less Than Real-TimeFastest DPM (Yan et al., 2014) 2007 30.4 15 R-CNN Minus R (Lenc and Vedaldi, 2015) 2007 53.5 6 Fast R-CNN (Girshick, 2015) 2007 + 2012 70.0 0.5Faster R-CNN VGG-16 (Ren et al., 2015) 2007 + 2012 73.2 7 Faster R-CNN ZF (Ren et al., 2015) 2007 + 2012 62.1 18 YOLO VGG-16 2007 + 2012 66.4 21S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
106highlighted the complexities of classifying fruit and vegetables. Speci ﬁ- cally, they looked at the issue of identifying fruits and vegetables at su-permarket self-checkout systems (Hameed et al., 2020a). The authors applied pre-trained weights via transfer learning and an ensemblemethod to weights of GoogleNet and MobileNet for an optimized aver-age weight. Results demonstrated that these pre-trained weights posi-tively affected the model, with the ensemble method reachingaccuracy measurements higher than training and testing datasets.Hameed et al. (2020b), proposed a novel technique to the supermarketfruit and vegetable classiﬁcation issue by incorporating a progressivefruit and vegetable classiﬁer with AdaBoost-based optimization of aCNN. 15 classes, 1000 images in each class, of fruits and vegetableswere considered for training. The method began by coarsely classifyingfruits and vegetables into three categories via the Jenks Natural Breaksclassiﬁcation method. From there, the classes were implemented tothree CNNs (GoogleNet, MobileNet-v2, and a custom-designed net-work) for more detailed classiﬁcation. The optimized AdaBoost CNNoutperformed the custom CNN. Accuracy of the CNN with AdaBoostoptimization ranged from 97.60% (navel orange) to 99.87% (lady ﬁnger banana) with less than 3% error for all classes. The custom 15 layerCNN achieved test accuracy ranges of 80.13% (10 epochs) to 93.97%(22 epochs).
Fig. 9.YOLOv3 block diagram.
Fig. 10.RGB HOG images: (a) Original RGB image, (b) Resized RGB image, (c) HOG RGB.
Fig. 11.Gradients of images.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
107Several recent studies have attempted to improve or test modernobject detection methods to use real-time robotic harvesting. Wan and Goudos (2020), utilized an improved Faster R-CNN architecture todetect fruits (apples, mango's, oranges) for robot picking applications.In the investigation, the authors reported on 1) how their model up-dates hyperparameters during the training phase of their model,2) how the model augments data on high-quality images, and 3) optimi-zation of the convolutional and pooling layers of their Faster R-CNNmodel. Speciﬁcally, feature extraction was completed with a CNNmodel VGG-16 using 13 convolutional layers, 13 ReLu layers, and fourpooling layers. Two loss functions were added to optimize theconvolutional and pooling layers, allowing the parameters to adjust au-tomatically during training. They found that automatically tuning pa-rameters resulted in 58 ms/image detection speed with mAP % of90.72. These results were compared to YOLO, YOLOv2, YOLOv3, Fast R-CNN, and Faster R-CNN, of which their model outperformed YOLO,Fast R-CNN, and Faster R-CNN in terms of detection speed and mAP%.When the authors fed their dataset to YOLOv3 they achieved a 40 ms/image detection speed and mAP % of 90.03. So, they were able to softenthe gap of performance between the Faster R-CNN and YOLOv3architectures.There have been studies that focus on low harvesting rates and howto improve picking point accuracy in real-time ( Yu et al., 2020). This in- vestigation, completed by Yu et al., involved creating an end-effector as-sembled on a servo control system of a robotic harvesting machine. Thismechanism effectively allowed the robot to emit and receive a laserbeam instead of measuring the depth distance in real-time. A rotatedYOLO (R-YOLO) was proposed to improve to the YOLOv3 architectureto be compatible with the mechanism. A highlight of R-YOLO was theprocess of rotating the annotation bounding boxes and employing a ro-tational parameter,α, during the labeling phase. This process aids in thelocalization of the picking point of the fruit. They reported precision andrecall rates of 94.4 and 93.4%, respectively. The average computationalspeed on a 640 × 480 image with R-YOLO was 0.056 s.Robust and efﬁcient non-destructive testing methods play an essen-tial role in robotic harvesting and quality control of agricultural prod-ucts. Throughout literature, many novel techniques have beenimplemented for accomplishing this task. Jiang et al. (2018), employed near-infrared hyperspectral imaging for non-destructive quality assess-ment of chili peppers. In combination with regression models, NIR wasused for evaluating capsaicinoid concentrations and the water contentof peppers. Besides, a radial basis function neural network (RBFNN)was used for classifying pungent and non-pungent peppers. The inves-tigation demonstrated that capsaicin and dihydrocapsaicin concentra-tions of chili peppers are visually different. Pungency was classi ﬁed with an accuracy of 98.7 and 98.0% (full spectra and successive projec-tions algorithm, respectfully).Gao et al. (2020), implemented a real- time hyperspectral imaging forﬁeldwork in classifying the ripeness offruit. This investigation reported a technique created by the authors
Fig. 12.Thermal HOG images: (a) Original thermal image, (b) Resized thermal image, (c) HOG thermal.
Fig. 13.Gradients of thermal images.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
108that utilized a portable hyperspectral imagery machine to estimate theripeness of strawberries. Two wavelengths were selected based on aforward SFS algorithm and feed to an SVM. Finally, a CNN was imple-mented to predict deep spatial features, and the model achieved an ac-curacy of 98.6%.3. Methodology3.1. Image representationVision-based fruit detection is a critical component for in- ﬁeld auto- mation and robotic fruit harvesting. However, this technology couldalso be leveraged in other applications such as disease detection, matu-rity detection, crop health status monitoring ( Patel et al., 2011).The datasets in this study were constructed as thermal and RGBcolor images to detect the chili peppers. To this end, we monitoredtwo different chili plants during their growth period. The ﬁrst chili pep- per was a raw, green one, and the second was mature. The RGB imageswere acquired from conventional digital cameras with no preprocessingand saved on a standard JPEGﬁle format. At the same time, thermal im-ages of chili peppers were collected using a Forward Looking Infraredcamera (FLIR A615). The camera is highly accurate with easy setupand automated inspection software. It is sensitive enough to detect tem-perature changes as small as 50 mK. The spectral range of the thermalcamera is 7.5−14μm with an absolute thermal accuracy of ±2°C or±2%, and thermal sensitivity of <0.05° at 30 °C. The camera producesthermal images with up to 640 by 480 resolution at 60 frames persecond.
Fig. 14.Images with masks: (a)Mask 1, (b)Mask 2, (c) Mask 3, (d) Mask 4.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
109Both cameras were mounted over the chili plants at a ﬁxed dis- tance of 130 cm with a light background. The thermal camera wasfully controlled with the FLIR Tools; In fact, pre or post-processingcould be done on an individual image, using ResearchIR software.Using an IR lens with a focal length of 13.1 mm, 112 thermal images(Besides 112 RGBs) were captured every other day over threemonths. On each day, four different images were taken from differentdirections for a single plant. Some of the images were captured in anoutdoor setting and in natural light to validate the proposedtechnique.3.2. Performance metricsUnlike other ML applications, object detection classi ﬁcation is not bi- nary, as in true or false. Therefore, the performance metrics used havebeen tailored for these exact operations. Intersection of Union (IoU)gives the user the ability to measure how well bounding boxes arebeing predicted with respect to truth boxes greater than a user-identiﬁed threshold. In other words, IoU is a measurement of the objectlocalization accuracy between the truth boxes and predicted boxes ( Zou et al., 2019). Eq.2deﬁnes IoU:IoU¼
Area of OverlapArea of Union ð2ÞHere, we are measuring the magnitude of overlap between theground truth box and predicted box. In literature, an accepted thresholdtends to be 0.5. That is:IoU≥0:5TPIoU<0:5FP/C26So, if the measurement is at or higher than 0.5 the object will be suc-cessfully identiﬁed. Precision and recall are used as a metric to evaluateperformance. We deﬁne precision and recall in Eqs.3a n d4below.Precision¼
TPTPþFP ð3ÞRecall¼
TPTPþFN ð4ÞWhere, TP is true positive, FP is false positive, and FN is false neg-ative. True positives are when the model correctly classi ﬁes the object detection. False positive occurs when there is incorrect object detec-tion. False negative occurs when ground truth bounding boxes arepresent in the image and the model fails to detect the object in theimage. True negative (TN) is not considered for this application sinceit would be correctly not labeling parts of the image that do nothave truth bounding boxes.3.3. Implementation of object detection modelsAnnotationﬁles were generated for each image with the LabelImgsoftware (tzutalin, 2017). Mask R-CNN was implemented in a Conda en-vironment and operated with Python 3.6, Tensor ﬂow 1.14, and Keras 2.24 packages. Next, we cloned the Github repository Mask R-CNNfrom (Abdulla, 2017).The YOLOv3 model was computed on GPU with CUDA version 10.1,V10.1.243. The architecture was cloned from the Github repository cre-ated byBochkovskiy (2020).3.3.1. HOG implementation in PythonA prerequisite for using the HOG method is that all images must bean array of shape (128,64). Therefore, all images were dropped thentransformed using skimage resize tool in python ( van der Walt et al., 2014). The HOG descriptors work by applying a ﬁlter to the image. The vertical and horizontal gradients are calculated with the kernelsshown inFig. 7.Magnitude and direction are calculated by Eqs. 5a n d6, respectively:g¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃg2xþg2yq ð5Þθ¼arctan
gy
gxð6Þ3.3.2. Implementation of one and two-stage detection modelsWe select two top-performing algorithms from two-stage and one-stage detectors for implementation on our dataset. After the data wasprepared, we split our test/train sets to 20/80% to train on as many im-ages as possible due to the dataset being small. As stated in Section 2.1.2, the Mask R-CNN utilizes FPN and a ResNet101 backbone. For the MaskR-CNN model, weﬁrst loaded bounding boxes and then masks foreach image. We utilized a python tool to ensure that the masks were ap-plied correctly to the dataset. Then, we de ﬁned andﬁt the model using the MaskRCNN function supplied from Abdulla (2017).Fig. 8depicts how our image dataset walks through the Mask R-CNN.For our implementation of the Mask R-CNN architecture, we utilizeFPN for feature extraction which is leveraged from a ConvNet's pyrami-dal feature hierarchical system (Lin et al., 2017). To do this, a region- based object detector or Region-of-Interest (RoI) pooling extracts fea-tures from the matrix. The RoI is assigned as:k¼
⌊K0þlog2ﬃﬃﬃﬃﬃﬃﬃwhp=224/C16/C17⌋
wherewandhare the width and height (respectively), on the input im-age to the network andK
0is the target level. During theﬁrst stage, re- gions are proposed with anchors. Then the object class is predicted inthe second stage. During the second stage, anchors are not used. Instead,ROIAlign is performed toﬁx misalignments (He et al., 2017). The ROIAlign generates masks for each object at the pixel level while theRoI classiﬁcation branch is used to predict categories ( He et al., 2017). Results are displayed inSection 4.2.YOLOv3 consists of 53 convolutional layers that contain a batch nor-malization layer with a ReLU activation function. Pooling is not utilizedfor the model. Since this model is invariant to image size, cropped
Fig. 15.Mask R-CNN prediction box.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
110images were similar in size but not the same. Feature maps aredownsampled via a convolutional layer with a stride of 2. Fig. 9walks through the process of how our data is implemented into YOLOv3.Once the dataset is prepared, the YOLOv3 model extracts featureswith Darknet-53. During feature extraction, three feature matrices arecreated of sizes 1) 52 × 52, 2) 26 × 26, and 3) 13 × 13. These feature ma-trices are fed to the multiscale convolutional detector where the fea-tures are concatenated through multi-step convolutions.We batch and input images with size ( m,416,416,3) and output bounding boxes labeled (p
c,bx,by,bh,bw,c). The bounding box predic- tions are calculated by:bx¼σt xðÞ þc x
by¼σt y/C0/C1þc
y
bw¼pwetw
bh¼pheth
where,b x,by,bh,andb ware bounding box center coordinates at x, y andwidth and height of the prediction box. t
x,y,w,h are network outputs. Object scores are calculated as the probability (between 0 and 1) thatan object is inside a bounding box from a sigmoid activation function.We report allﬁndings inSection 4.3.
Fig. 16.Actual vs predicted plots for Mask-RCNN: (a) Actual plot image 1, (b) Predicted plot image 1, (c) Actual plot image 2, (d) Predicted plot image 2.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
1114. Results and discussionThis section provides the results of all experimentation completedduring training. Here we show the viability of certain techniques ap-plied to a chili pepper dataset in an environment with complex back-grounds and good ambient illumination. Section 4.1details the HOG algorithm results,Section 4.2provides results for the Mask R-CNN archi-tecture, andSection 4.3shows results from real-time detection usingthe YOLOv3 architecture. The RGB dataset was considered for experi-mentation on the HOG, Mask R-CNN, and YOLOv3 models.Computer vision is a highly researched area with many new andcutting-edge technology. As a result, terminology can sometimes getconfusing. AP wasﬁrst introduced by VOC2007 (Everingham et al., 2007) and the calculation is applied to every image within the labeledset and measures the average AP of the entire set. We follow theCOCO deﬁnition for mAP terminology as:AP is averaged over all categories. Traditionally, this is called “mean average precision”(mAP). We make no distinction between AP andmAP (and likewise AR and mAR) and assume the difference is clearfrom context (Common Objects in Context (COCO) (2021) ). Therefore, we deﬁne mAP as:mAP¼
1N∑Ni¼1APi ð7Þwhere,AP
iis the average precision at imageiandNis the total number of images. The mAP metric is usually used as a ﬁnal metric to compare performance throughout all object categories ( Zou et al., 2019), which we use to evaluate experiments with both the Mask R-CNN and YOLOv3models.4.1. HOG resultsThe HOG method allows for visual feature extraction and compari-son, which can be seen inFigs. 10, 11, 12, and 13. The purpose of using the HOG method in this experiment was to visualize gradient di-rection and intensity between RGB and thermal images of chili peppers.InFig. 10, we show the process of transforming the original image inFig. 10a, resizing the image inFig. 10b, andﬁnally plotting gradientdirections inFig. 10c. When plotting the normalized histograms inFig. 10c, there is visual evidence (intensity) of the HOG detector recog-nizing the shape of the entire plant and even demonstrating a connec-tion to the chili pepper centralized in the original image.Gradient intensity of the histograms is visually represented as theyincrease in color in the image from the color map in Fig. 11. Gradient values range from 0 to 0.35 and visualize distinct features of the chiliplant and pepper. Edges of the pepper and leaves have lower gradientvalues when compared to the background of the image. These valuesare close to zero (purple), while the center of the pepper display gradi-ents between 0.30 and 0.25. The background displays gradient valuesranging between 0.20 and 0.35. Here we show the regional location ofwhere the chili pepper is within the normalized histogram plot. A plotof the gradients demonstrates evidence that the HOG method can out-line the chili pepper inFig. 11.When we move to a thermal image, HOG detectors appear to haveless visual evidence of detecting a pepper. Fig. 12depicts features of a thermal image. The feature extraction process is represented with theoriginal image inFig. 12a, the resized thermal image inFig. 12b, and the gradient directions inFig. 12c. Gradient directions from the 12cplot does not show the distinct region of the pepper, but it does demon-strate the shape of the entire plant.Gradients range from 0 to 0.25 inFig. 13and display the gradient values of the thermal image fromFig. 12a. The plot of gradient values does not demonstrate a clear feature pattern and the variation betweengradient values is much less inFig. 13compared toFig. 11(<0.1). The HOG algorithm was an effective technique to visualize how featuresmay be represented in computer vision. Figs. 11 and 13demonstrate through gradient intensities that RGB images have stronger featureswhen compared to thermal images.4.2. Mask RCNNDue to its ability to produce highly accurate mAP responses, we uti-lized the Mask R-CNN architecture to analyze and compare results fromthe Mask R-CNN to the YOLOv3 model. Before training, we ensure thatimage and mask arrays have identical width and height. Fig. 14shows four images with masks overlaid to con ﬁrm masks were loading
Fig. 17.Predictions from pre-trained YOLO Weights: (a) Out-of-Box chili pepper prediction, (b) TRAINED chili pepper prediction.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
112properly. For example,Fig. 14d output an image shape of (881,562,3)and a mask shape of (881,562,1). We utilize the following tool:visualize.display_instanceThis python tool is used to call the plot in Fig. 15. Here, we demon- strate the ability of this particular architecture to detect chili peppersin an environment with debris. We visualize that the Mask R-CNNalgorithm has identiﬁed and located all chili peppers in the image(with different colors), assigned a prediction bounding box, and labeledit with the appropriate label (chili pepper). As covered in Section 3.2, precision and recall are determined by evaluating truth boundingboxes and predicted bounding boxes. While training on the RGB dataset,the Mask R-CNN performed with a train mAP of 0.872 and test mAP of0.896 with an overall computational time of 40.79 s per test image.
Fig. 18.Additional predictions on validation dataset with YOLOv3: (a) Prediction test image 2, (b) Prediction Test Image 3, (c) Prediction Test Image 4, (d) P rediction test image 5.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
113Fig. 16visualizes the truth and prediction bounding boxes for 2 im-ages of chili peppers during testing. Overall, the Mask R-CNN model ap-pears to place predicting boxes well. Fig. 16b, shows that the model failed to detect (False Negative) a chili pepper covered in debris.4.3. YOLOv3 for real-time detectionWe utilize the YOLOv3 architecture for real-time application due toits high performance and fast computing capabilities. We noticed that
Fig. 19.Predictions on green chili test set via YOLOv3: (a) Prediction test image 2, (b) Prediction test image 3, (c) Prediction test image 4.
Fig. 20.Computational time (sec) for two-stage and one-stage algorithms.Table 3Mask R-CNN mAP values.RGB ThermalTrain Test Train Test0.872 0.896 0.391 0.298S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
114training loss drops below 2.0 roughly at the 250th epoch. After trainingwas complete, we evaluated the model using mAP performance metrics.Both training and testing resulted in 100% precision accuracy (mAP)with a computational time of 3.64 s per test image.An initial test was conducted using out-of-box pre-trained weightsand classes on the chili dataset.Fig. 17compares the out-of-box likeness scores of a chili pepper to that of a chili pepper image after training wascomplete.Fig. 18displays conﬁdence levels of additional test images. As statedearlier in the text, conﬁdence scores mostly evaluate test images be-tween 97 and 100% as the predeﬁned class of‘chile pepper’. Conﬁdence scores suggest that the YOLOv3 model is functioningwith a high level of precision. There does appear to be an issue of debrisinFig. 19a). Here, we can see that one of the peppers is only detectedwith a conﬁdence of 33% which indicates a possible impediment duringreal-time robotic harvesting where the environment may be dense withdebris.Fig. 20compares the computational capabilities of Mask-RCNN (red)to YOLOv3 (blue). The YOLOv3 algorithm has superior capabilities to theMask-RCNN with over ten times the computational speed on the chilidataset. For this reason, we believe the YOLOv3 algorithm will be suit-able in a harvesting robot for real-time object detection. Computationalspeeds for the YOLOv3 and Mask R-CNN are the same on the RGB andthermal images. The followingsection reports mAP values and com-pares classiﬁcation performance for the two models with the RGB andthermal datasets.In this section, using the RGB dataset, it was shown that accuracy ofprediction can signiﬁcantly decrease when there are heavy debris andoverlapping of peppers. In addition, the variant ambient lighting canhamper the performance of detection algorithms based on RGB data.
Fig. 21.Thermal images with Masks: (a)Mask 1, (b)Mask 2, (c) Mask 3, (d) Mask 4.
Fig. 22.Comparison of thermal Mask R-CNN: (a) Actual plot, (b) Predicted plot.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
115In the following section, non-destructive thermal imaging will be usedto resolve these issues as well as highlight results of the Mask R-CNNand YOLOv3 models used with the thermal dataset.4.4. Thermal imagingThe same process conducted with the RGB dataset was used with thethermal dataset. 80% of the images were used for training with 20% heldfor testing. Predictions from the Mask R-CNN model did not bene ﬁt from thermal imaging and Prediction performance was decreased dras-tically.Table 3displays the comparison of mAP values from the RGB andthermal imagery datasets showing precision reduced over half. We seetesting mAP values drop by almost 0.6 for the Mask R-CNN, which isconcerning since these models only predict on one class.Fig. 21shows how the masks were applied during training to a vari-ety of different images.Fig. 21a demonstrates an image in heavy debrisdue to the pepper location directly behind the plant stem. Figs. 21b through21d demonstrate peppers in light debris. We see how debrisnegatively affects prediction for the Mas R-CNN in Fig. 22. A detailed in- spection of theﬁgure reﬂects the tabular results by comparing actualand predicted plots of a test image. In Fig. 22a, the image shows one bounding box applied to a single pepper however, the Mask R-CNNmodel predicted 7 bounding boxes on the test image which decreasesthe mAP value signiﬁcantly.Performance of the Mask R-CNN during this portion of our investiga-tion leads us to doubt the possibility of using the model for real-time ap-plications with thermal imagery. The YOLOv3 model, however showedpromising results.Table 4displays RGB and thermal mAP values forthe YOLOv3 architecture. There is a slight decline in performance fromRGB to thermal; however compared to the decline that occurred withthe Mask R-CNN architecture, this decline is acceptable.
Fig. 23demonstrates how thermal imaging can be useful for predic-tions made with debris. When we compare test images 2, 3, and 5 inFigs. 18 and 23we can see that prediction scores do not signi ﬁcantly change. However, test image 4 shows a dramatic improvement inFig. 23.I nFig. 18(c) where there is overlapping of peppers, predictionscores for the peppers are reported as 1.00 and 0.67. Compared to thesame pepper conﬁguration,Fig. 23(c) shows that both peppers are pre-dicted at scores of 0.97.Prediction performance of YOLOv3 with the thermal image datasetshowcases the possibility of utilizing object detection for robotic har-vesting with debris, paper overlapping, and low light present. Furthertesting should be conducted to determine the possibility of using objectdetection for robotic harvesting during evening and night hours.5. ConclusionIn this study, utilized RGB and thermal images of chili peppers in anenvironment of various amounts of debris, pepper overlapping, and am-bient lighting, train this dataset, and compare object detection methods.Results of feature visualization with HOG and object detection withMask R-CNN and YOLOv3 architectures for a novel chili pepper imagedataset were reported. HOG helped display gradient edge directions,and the Mask R-CNN model provided suitable prediction performancefor less than real-time detection on the RGB dataset. The YOLOv3Table 4YOLOv3 mAP values.RGB ThermalTrain Test Train Test1.0 1.0 0.99 0.97
Fig. 23.Thermal chili predictions on validation dataset with YOLOv3: (a) Prediction test image 2, (b) Prediction test image 3, (c) Prediction test image 4, (d ) Prediction test image 5.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
116algorithm has superior capabilities to the Mask-RCNN with over tentimes the computational speed on the chili dataset. The YOLOv3model introduces a spike in model performance for real-time detection(~3 s predictions) and demonstrates the possibility of being utilizedduring real-time robotic harvesting. However, we reported a lack ofconﬁdence in detection when heavy amounts of debris and paper over-lapping are present in RGB images. In this study, it was shown that non-destructive thermal imaging can resolve these issues. In particular,YOLOv3 demonstrated an ability to detect peppers in areas with highdebris and pepper overlapping with the thermal imagery; howeverthe Mask R-CNN model did not detect well on the thermal imagery. Itwas shown that mapping temperature differences between the pepperand plant/debris can provide signiﬁcant features for object detection inreal-time and can help improve accuracy predictions with heavy debris,variant ambient lighting, and overlapping of peppers. In addition, suc-cessful thermal imaging for real-time robotic harvesting could allowthe harvesting period to become more efﬁcient and open up harvesting opportunity in the evening hours or low light situations.Further investigation should be pursued to improve object detectionwhen dense debris and poor ambient light are present. One issue in ourexperimentation is that only one class was used during training. Thisissue could lead to overﬁtting and could be the reason that YOLOv3 per-formed so well on the datasets. Additional classes and variations in im-agery type should be utilized during training and testing to avoidoverﬁtting. We plan to expand the number and type of images in thedataset and determine if the YOLOv3 (or other models) can exceed per-formance expectations during real-time robotic harvesting in dense de-bris and poor lighting environments with peppers in naturalenvironment.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Abdulla, W., 2017. Mask r-cnn for Object Detection and Instance Segmentation on Kerasand Tensorﬂow.https://github.com/matterport/MaskRCNN . Bochkovskiy, A., 2020. Yolov3 With Darknet Framework. https://github.com/AlexeyAB/ darknet.Bochkovskiy, A., Wang, C.Y., Liao, H.Y.M., 2020. Yolov4: optimal speed and accuracy of ob- ject detection. arXiv 2004.10934 (preprint arXiv:2004.10934).Bosland, P.W., Bailey, A.L., Cotter, D.J., 1991. Growing Chiles in New Mexico. Guide H-New Mexico State University, Cooperative Extension Service (USA).Common Objects in Context (COCO), 2015. Detection Evaluate. https://cocodataset.org/ detection-eval. Online; accessed 19 January 2021.Dalal, N., Triggs, B., 2005. Histograms of oriented gradients for human detection. 2005IEEE computer society conference on computer vision and pattern recognition(CVPR’05). 886–893. IEEE.https://doi.org/10.1109/CVPR.2005.177 . Deng, J., Dong, W., Socher, R., Li, L.J., Li, K., Fei-Fei, L., 2009. ImageNet: a large-scale hierar- chical image database. CVPR09, pp. 248 –255. Everingham, M., Van Gool, L., Williams, C.K.I., Winn, J., Zisserman, A., 2007. The PASCALVisual Object Classes Challenge 2007 (VOC2007) Results. http://www.pascal-net- work.org/challenges/VOC/voc2007/workshop/index.html . FAO, 2019. Food and Agriculture Organization of the United Nations Statistics Division.(ACCESSED 23 FEBRUARY 2021). http://www.fao.org/faostat/en/data/QC/visualize/ . Felzenszwalb, P., McAllester, D., Ramanan, D., 2008. A discriminatively trained, multiscale,deformable part model. 2008 IEEE Conference on Computer Vision and Pattern Rec-ognition. 1–8. IEEE.https://doi.org/10.1109/CVPR.2008.4587597 . Gao, Z., Shao, Y., Xuan, G., Wang, Y., Liu, Y., Han, X., 2020. Real-time hyperspectral imagingfor the in-ﬁeld estimation of strawberry ripeness with deep learning. Artif. Intell.Agric.https://doi.org/10.1016/j.aiia.2020.04.003 . Girshick, R., 2015. Fast r-cnn. Proceedings of the IEEE International Conference on Com-puter Vision, pp. 1440–1448.https://doi.org/10.1109/ICCV.2015.169 . Girshick,R.,Donahue,J.,Darrell,T.,Malik,J.,2014.Richfeaturehierarchiesforaccurateobjectdetectionandsemanticsegmentation.ProceedingsoftheIEEEConferenceonComputerVision and Pattern Recognition, pp. 580 –587.https://doi.org/10.1109/CVPR.2014.81 . Gu, J., Wang, Z., Kuen, J., Ma, L., Shahroudy, A., Shuai, B., Liu, T., Wang, X., Wang, G., Cai, J.,et al., 2018. Recent advances in convolutional neural networks. Pattern Recogn. 77,354–377.https://doi.org/10.1016/j.patcog.2017.10.013 .Hameed, K., Chai, D., Rassau, A., 2018. A comprehensive review of fruit and vegetable clas-siﬁcation techniques. Image Vis. Comput. 80, 24 –44.https://doi.org/10.1016/j. imavis.2018.09.016.Hameed, K., Chai, D., Rassau, A., 2020a. A progressive weighted average weight optimisa- tion ensemble technique for fruit and vegetable classi ﬁcation. 2020 16th International Conference on Control, Automation, Robotics and Vision (ICARCV). IEEE, pp. 303 –308. Hameed, K., Chai, D., Rassau, A., 2020b. A sample weight and adaboost cnn-based coarsetoﬁne classiﬁcation of fruit and vegetables at a supermarket self-checkout. Appl. Sci.10, 8667.https://doi.org/10.3390/app10238667 . He, K., Zhang, X., Ren, S., Sun, J., 2015. Spatial pyramid pooling in deep convolutional net-works for visual recognition. IEEE Trans. Pattern Anal. Mach. Intell. 37, 1904 –1916. https://doi.org/10.1109/TPAMI.2015.2389824 . He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn. Proceedings of the IEEE Inter-national Conference on Computer Vision, pp. 2961 –2969.https://doi.org/10.1109/ ICCV.2017.322.Jiang, J., Cen, H., Zhang, C., Lyu, X., Weng, H., Xu, H., He, Y., 2018. Nondestructive qualityassessment of chili peppers using near-infrared hyperspectral imaging combinedwith multivariate analysis. Postharvest Biol. Technol. 146, 147 –154.https://doi.org/ 10.1016/j.postharvbio.2018.09.003 . Kang, H., Zhou, H., Wang, X., Chen, C., 2020. Real-time fruit recognition and grasping es-timation for robotic apple harvesting. Sensors 20, 5670. https://doi.org/10.3390/ s20195670.Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. Imagenet classi ﬁcation with deep convolutional neural networks. In: Pereira, F., Burges, C.J.C., Bottou, L., Weinberger,K.Q. (Eds.), Advances in Neural Information Processing Systems. Curran Associates,Inc, pp. 1097–1105.https://doi.org/10.1145/3065386 . Lenc, K., Vedaldi, A., 2015. R-cnn minus r. arXiv https://doi.org/10.5244/C.29.5 preprint arXiv:1506.06981.Li, S., Luo, H., Hu, M., Zhang, M., Feng, J., Liu, Y., Dong, Q., Liu, B., 2019. Optical non-destructive techniques for small berry fruits: a review. Artif. Intel. Agric. 2, 85 –98. https://doi.org/10.1016/j.aiia.2019.07.002 . Lin, T.Y., Dollár, P., Girshick, R., He, K., Hariharan, B., Belongie, S., 2017. Feature pyramid networks for object detection. Proceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition, pp. 2117 –2125. Naranjo-Torres, J., Mora, M., Hernández-García, R., Barrientos, R.J., Fredes, C., Valenzuela,A., 2020. A review of convolutional neural network applied to fruit image processing.Appl. Sci. 10, 3443.https://doi.org/10.3390/app10103443 . Patel, H.N., Jain, R., Joshi, M.V., et al., 2011. Fruit detection using improved multiple featuresbased algorithm. Int. J. Comput. Appl. 13, 1 –5.https://doi.org/10.5120/1756-2395 . Poirson, P., Ammirato, P., Fu, C.Y., Liu, W., Kosecka, J., Berg, A.C., 2016. Fast single shot de-tection and pose estimation. 2016 Fourth International Conference on 3D Vision(3DV). IEEE, pp. 676–684.https://doi.org/10.1109/3DV.2016.78 . Redmon, J., Farhadi, A., 2017. Yolo9000: better, faster, stronger. Proceedings of the IEEEConference on Computer Vision and Pattern Recognition, pp. 7263 –7271.https:// doi.org/10.1109/CVPR.2017.690 . Redmon, J., Farhadi, A., 2018. Yolov3: An incremental improvement. Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition (CVPR). 779 –788. arXiv preprint arXiv:1804.02767.Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: uni ﬁed, real- time object detection. Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 779 –788.https://doi.org/10.1109/CVPR.2016.91 . Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: towards real-time object detectionwith region proposal networks. arXiv https://doi.org/10.1109/TPAMI.2016.2577031 preprint arXiv:1506.01497.Sadeghi, M.A., Forsyth, D., 2014. 30hz object detection with dpm v5. European Conferenceon Computer Vision. 65–79. Springer.
https://doi.org/10.1007/978-3-319-10590-1-5 . tzutalin, 2017. Graphical Image Annotation Tool. https://github.com/tzutalin/labelImg . Uijlings, J.R., Van De Sande, K.E., Gevers, T., Smeulders, A.W., 2013. Selective search for ob-ject recognition. Int. J. Comput. Vis. 104, 154 –171.https://doi.org/10.1007/s11263- 013-0620-5.Vadivambal, R., Jayas, D.S., 2011. Applications of thermal imaging in agriculture and foodindustry—a review. Food Bioprocess Technol. 4, 186 –199.https://doi.org/10.1007/ s11947-010-0333-5.Viola, P., Jones, M., 2001. Rapid object detection using a boosted cascade of simple fea-tures. Proceedings of the 2001 IEEE Computer Society Conference on Computer Vi-sion and Pattern Recognition. CVPR 2001. IEEE https://doi.org/10.1109/ CVPR.2001.990517Pp. I–I.Viola, P., Jones, M.J., 2004. Robust real-time face detection. Int. J. Comput. Vis. 57, 137 –154. https://doi.org/10.1023/B:VISI.0000013087.49260.fb . Viola, P., Jones, M.J., Snow, D., 2005. Detecting pedestrians using patterns of motion andappearance. Int. J. Comput. Vis. 63, 153 –161.https://doi.org/10.1109/ ICCV.2003.1238422.van der Walt, S., Schönberger, J.L., Nunez-Iglesias, J., Boulogne, F., Warner, J.D., Yager, N.,Gouillart, E., Yu, T., the scikit-image contributors, 2014. scikit-image: image process-ing in Python. PeerJ 2, e453.https://doi.org/10.7717/peerj.453 . Wan, S., Goudos, S., 2020. Faster r-cnn for multi-class fruit detection using a robotic visionsystem. Comput. Netw. 168, 107036. https://doi.org/10.1016/j.comnet.2019.107036 . Yan, J., Lei, Z., Wen, L., Li, S.Z., 2014. The fastest deformable part model for object detec-tion. Proceedings of the IEEE Conference on Computer Vision and Pattern Recogni-tion, pp. 2497–2504.https://doi.org/10.1109/CVPR.2014.320 . Yu, Y., Zhang, K., Liu, H., Yang, L., Zhang, D., 2020. Real-time visual localization of the pick-ing points for a ridge-planting strawberry harvesting robot. IEEE Access 8,116556–116568.https://doi.org/10.1109/ACCESS.2020.3003034 . Zou, Z., Shi, Z., Guo, Y., Ye, J., 2019. Object detection in 20 years: a survey. arXiv 1905.05055 preprint arXiv:1905.05055.S.C. Hespeler, H. Nemati and E. Dehghan-Niri Artiﬁcial Intelligence in Agriculture 5 (2021) 102 –117
117