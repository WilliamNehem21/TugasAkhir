t-SNE: A study on reducing the dimensionality of hyperspectral datafor the regression problem of estimating oenological parameters
Rui Silvaa,⁎, Pedro Melo-Pintoa,b,⁎⁎
aCITAB - Centre for the Research and Technology of Agro-Environmental and Biological Sciences, Inov4Agro-Institute for Innovation, Capacity Build ing and Sustainability of Agri-Food Production, Universidade de Trás-os-Montes e Alto Douro, Quinta dos Prados, Vila Real 5000-801, Portugal
bDepartamento de Engenharias, Escola de Ciências e Tecnologia, Universidade de Trás-os-Montes e Alto Douro, Quinta dos Prados, Vila Real 5000-801, P ortugal
abstract article info
Article history:Received 22 September 2022Received in revised form 20 February 2023Accepted 21 February 2023Available online 6 March 2023In recent years there is a growing importance in using machine learning techniques to improve procedures inprecision agriculture: in this work we perform a study on models capable of predicting oenological parametersfrom hyperspectral images of wine grape berries, a specially relevant topic to boost production tasks forwinemakers. Speciﬁcally, we explore the capabilities of a novel technique mostly used for visualization,t-Distributed Stochastic Neighbor Embedding (t-SNE), for reducing the dimensionality of the highly complexhyperspectral data and compare its performance with Principal Component Analysis (PCA) method, which de-spite the introduction of many nonlinear dimensionality reduction techniques over the years, had achieved thebest results for real-world data across several studies in literature. Additionally we explore the potential of Kernelt-SNE, an extension to the t-SNE method that allows for the usage of the technique in streaming data or onlinescenarios. Our results show that, in a direct comparison, t-SNE achieves better metrics than PCA for most ofthe data sets in this work and that the regressor (Support Vector Regression, SVR) performs better with thet-SNE reduced features as inputs, accomplishing better predictions with lower error rates. Comparing the resultswith current literature, our shallow learning model paired with t-SNE achieves either better or on par results thanthose reported, even competing with more advanced models that use deep learning techniques, which shouldpropel the introduction of t-SNE in more studies that require dimensionality reduction.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
Keywords:Hyperspectral imagesDimensionality reductionRegressionT-SNESupport vector machinesWine grape berries
1. IntroductionIn recent years the wine industry has evolved in a way to introduceas many new technologies as possible to improve production proce-dures: one of its main focuses is to obtain grapes for wine productionin an environmentally friendly manner, without destroying them inthe process and choosing them according to quality features. The tradi-tional approach relies on laboratorial analysis to assess some select oe-nological parameters which, alongside destroying the grapes used foranalysis, is a cost and time-heavy method. The logical next step was toﬁnd some sort of imaging technique that allowed to obtain clean infor-mation about the grapes, and hyperspectral imaging found the mostsuccess in this task.Hyperspectral imaging (Gowen et al., 2007;Hall et al., 2002)i sa technique that shows the light reﬂection and absorption on an objectas a function of wavelength: it collects both spatial and spectral infor-mation, requiring robust models that are capable of extracting knowl-edge from the patterns present in the spectra. Our recent studies(Fernandes et al., 2011, 2015;Gomes et al., 2014a, 2014b, 2017b,2021a, 2021b;Gomes and Melo-Pinto, 2021;Silva et al., 2018) found some success in combining these images with machine/deep learningalgorithms that are capable of regression from hyperspectral images;however, one of the biggest issues to tackle in order to obtain generali-zation capacity from these models is the ability to reduce the inputspace, which allows to consistently identify the main features in thespectra and accurately predict oenological parameters. This led to astudy of a wide variety of dimensionality reduction methods ( Silva and Melo-Pinto, 2021) that showed that despite several advances andnew techniques, Principal Component Analysis (PCA) ( Wold et al., 1987) was still the method that allowed the algorithm to achieve thebest predictions and the best generalization capacity for the case ofpredicting oenological parameters from hyperspectral images of winegrape berries. Following this line of research, arose the need of applyingArtiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
⁎Corresponding author.⁎⁎Corresponding author at: CITAB - Centre for the Research and Technology of Agro-Environmental and Biological Sciences, Inov4Agro-Institute for Innovation, CapacityBuilding and Sustainability of Agri-Food Production, Universidade de Trás-os-Montes eAlto Douro, Quinta dos Prados, Vila Real 5000-801, Portugal.E-mail addresses:ruimsilva@utad.pt(R. Silva),pmelo@utad.pt(P. Melo-Pinto).
https://doi.org/10.1016/j.aiia.2023.02.0032589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique forthe dimensionality reduction of hyperspectral images.t-SNE (Van der Maaten and Hinton, 2008) is a technique that visual- ises high-dimensional data by giving each data point a location in a twoor three-dimensional map, reducing the tendency to crowd points to-gether and therefore creating more structured visualisations of thedata. We decided to conduct a study of the application of t-SNE tohyperspectral imaging data since, as concluded in ( Silva and Melo- Pinto, 2021) and mentioned in (Van der Maaten and Hinton, 2008), other dimensionality reduction techniques showcase a strong perfor-mance on artiﬁcial data sets but fail to translate that performance toreal-world data, mostly because of their failure to retain both the localand global structure of the data in a single map: t-SNE has shown tobe capable of capturing much of the local structure of the high dimen-sional data as well as revealing a global structure in it.Furthermore, it is also mentioned in ( Van der Maaten and Hinton, 2008) that it is unclear how t-SNE will perform on general dimensional-ity reduction tasks, and we decided to evaluate its performance onhyperspectral imaging data - reviewing the current state-of-the-artshows some applications of t-SNE to hyperspectral data:•in (Miao et al., 2018) the authors use t-SNE to reduce the dimension-ality of hyperspectral images of maize kernels and performclassiﬁcation.•in (Hariharan, 2021) t-SNE is applied to reduce the dimensionality ofhyperspectral open-access data sets (i.e.: Indian Pines data set) andperform classiﬁcation while tackling the curse of high dimensionalityon a limited number of training samples.•in (Zhang et al., 2018) t-SNE is combined with a Deep ConvolutionalGenerative Adversarial Network (DCGAN) to extract spectra-spatialfeatures and perform dimensionality reduction on hyperspectralimages.•in (Devassy and George, 2020) t-SNE is used to perform clustering andobtain a better visualization on a hyperspectral database of inks from60 different pens.•in (Gao et al., 2019) the authors combine t-SNE with a ConvolutionalNeural Network (CNN) to reduce the dimensionality and performclassiﬁcation on hyperspectral images with a large number of bandsbut an insufﬁcient number of sample pixels for each class.•in (Pouyet et al., 2018) t-SNE is also used to obtain visualisations ofhyperspectral images in 2D scatter plots.Additionally, there are also relevant applications of t-SNE outside ofhyperspectral images:•in (G i s b r e c h te ta l . ,2 0 1 2) the authors pave the way towards efﬁcient nonlinear dimensionality reduction proposing an extension of t-SNEwith the linear basis function.•in (Alibert, 2019) t-SNE is applied to better visualise and representplanetary systems in a two-dimensional space.•in (Anowar et al., 2021) the authors compare the performance ofseveral dimensionality reduction methods on open-access data sets.However, and as far as we are aware up to date, there is not a studyof the capability of t-SNE to reduce the dimensionality of a data set tothen perform predictions with a machine learning algorithm (regres-sion problem); additionally, we also perform this study on real-worldsamples (most of the studies are performed on arti ﬁ
cial data sets) of hyperspectral images of wine grape berries, a problem with extremelyhigh variability between harvest years and vintages of wine grapes;our study is also relevant because we perform t-SNE on small data sets(due to the inherent difﬁculties in acquiring training samples) in con-trast to most works that have great amounts of data: combining allthese aspects, we believe we perform a true test to the capability of t-SNE to reduce the dimensionality of real-world hyperspectral imagesin a problem with real-world applications. To further enhance ourstudy of the t-SNE technique, we also decided to apply a variation ofthis method, Kernel t-SNE. Kernel t-SNE ( Gisbrecht et al., 2015)e x t e n d s the non-parametric dimensionality reduction technique to an explicitmapping byﬁxing the parametric formx→fw(x)=yand optimizing the parameters off
winstead of the projection coordinates: thisenables to map large data sets in linear time by training a mapping ona small sub-sample only, with good generalization capacity. However,since Kernel t-SNE is applied to a subset of the training samples only,the results may differ when compared to t-SNE that is applied to thefull data set, due to missing information in the data used for trainingof the map. In order to close this information gap, the authors ( Schulz and Hammer, 2015) introduced Fisher Kernel t-SNE: a set of data pointsx
iis equipped with the pairwise Fisher metric, estimated based on theclass labels taking simple linear approximations for the path integralsand, using t-SNE, a new training setX′is obtained by taking the auxiliarylabel information into account (the calculated pairwise distances of datacomputed based on the Fisher metric); the authors then infer a kernel t-SNE mapping which is adapted to the label information due to the infor-mation inherent in the training set, and the resulting map is adapted tothe information encoded in the training set - this technique can be re-ferred to as Fisher t-SNE or Fisher kernel t-SNE. Both these techniquesattempt to further optimize the capability of t-SNE in real-world appli-cations and while we decided to study kernel t-SNE, we opted to not im-plement Fisher kernel-SNE due to the high computational cost ofcalculating the Fisher information matrices, which renders the solutionunﬁtting for a possible real-time analysis in the vineyards.As for the rest of this paper, inSub-Section 2.1we describe the hyperspectral imaging procedure, with an in-depth look into our exper-imental setup and to the way we perform the re ﬂectance measurements to construct the training sets;Sub-Section 2.2provides a brief theoreti- cal background of the dimensionality reduction methods applied;Sub-Section 2.3introduces the algorithm chosen to perform regression,the Support Vector Regression (SVR) technique; in Sub-Section 2.4we discuss our approach to avoid over-ﬁtting and achieve maximum gener-alization capacity via the usage of a cross-validation technique; inSub-Section 2.5we give insights about the grape sampling procedureand provide the data sets description to give a better understanding ofthe data and its high variability;Section 3presents the results obtained for the prediction of the oenological parameters for each dimensionalityreduction technique and for each oenological parameter, alongside adiscussion of said results and how they fare when compared to otherstate-of-the-art publications;Section 4summarizes ourﬁndings and concludes about the work, while also denoting future guidelinesfor improvement.2. Materials and methods2.1. Hyperspectral images2.1.1. Experimental setupHyperspectral measurements were performed in line with our pre-vious work (Silva and Melo-Pinto, 2021) using the following image ac- quisition system (Fig. 1): a hyperspectral camera, composed of a JAIPulnix (JAI, Yokohama, Japan) black and white camera and a SpecimImspector V10E spectrograph (Specim, Oulu, Finland); lighting, bymeans of a lamp holder with 300 × 300 × 175 mm
3(length × width × height) that held four 20W,1 2Vhalogen lamps and two 40W, 220V blue reﬂector lamps (Spotline, Philips, Eindhoven, The Netherlands).The halogen lamps were powered by continuous current power sup-plies to avoid lightﬂickering and the reﬂector lamps were powered at only 110Vto reduce lighting and prevent camera saturation. Each com-ponent was bough directly from their respective manufacturers and theimage acquisition system was assemble by the authors. The resultingimages have a spatial resolution of 1040 × 1392 pixels, where the1040 pixels correspond to the wavelength channels, ranging between380 and 1028nm, with approximately 0.6nmwidth for each channel,R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
59and the 1392 pixels stand for the spatial dimension (one line over thesamples) with approximately 110mmof width. The distance between the camera and the sample base was set to 420 mmand the camera was controlled with the Coyote software from JAI. All the hyperspectralmeasurements were done inside a dark room and at room temperature(20 °C).Each hyperspectral image is acquired for six grape berries and inthree different berry rotations of approximately 120° between posi-tions, with a single line taken solely over the berry equator when con-sidering the pedicel as the pole, as seen below in Fig. 2. Theﬁnal hyperspectral image for each rotation and position is theaverage of 32 different hyperspectral images acquired during a periodof time of 4 s, with the camera acquiring 8 images per second: thismethod reduces measurement noise, and also provides with some in-formation of the spatial dimensions (since we are not using it directlyon line-scan hyperspectral images) that is re ﬂected on the averaged value. As expected, there are observable patterns of light re ﬂection and absorption in the main areas where the wine grape berries lie inthe Spectralon (full reﬂection surface) and, to get individual imagesfor each wine grape berry, we use a threshold-based segmentationmethod.Fig. 3shows an example of a hyperspectral image capturedby the aforementioned experimental setup before segmentation.ObservingFig. 3it is possible to denote that the spots where thegrapes lie for imaging are clear to absorb more light (black strips)while the spots with no grapes (the remainder of the Spectralon) re ﬂectmore light (white strips). Afterﬁnishing the process of obtaining anindividual hyperspectral image for each wine grape berry we carry outreﬂectance measurements to build theﬁnal data sets.2.1.2. Reﬂectance measurementsReﬂectance is a function of the light wavelength and is de ﬁned as the ratio between the light intensity reﬂected by an object and the light thatilluminates that object. Despite the fact that the measurements could becarried out using other modalities, like transmittance or interactance,we chose the reﬂectance mode as input because the different patternsof reﬂectance and absorption across wavelengths will allow for theidentiﬁcation of chemical compounds and because, contrary to theother referred modes, imaging is possible without the need for the spec-trometer/camera to touch the samples ( Gomes et al., 2021a, 2021b; Gomes and Melo-Pinto, 2021;Silva and Melo-Pinto, 2021). For a posi- tion represented by vectorxand at wavelengthλ, the reﬂectanceR can be expressed as:Rx,λðÞ ¼
αx,λðÞ /C0σx,λðÞμx,λðÞ /C0σx,λðÞ ð1Þwhereαis the light intensity reﬂected from the grape;μis the light in- tensity reﬂected from a reference total reﬂectance target; andσis the dark current signal, which is electronic noise. For each wine grape berrywe took 32 hyperspectral images, with three different berry rotations,and the resulting spectrum was then normalized (using max-min nor-malization) to avoid variations in the measured light intensities. Fig. 4 shows the result of the reﬂectance measurements in one of the data setsthat will be used in the present work.2.2. Dimensionality reduction2.2.1. t-distributed stochastic neighbor embeddingt-SNE (Van der Maaten and Hinton, 2008) is a technique that is capa- ble of capturing the local structure of the high-dimensional data whilealso revealing global structure, such as the presence of clusters at sev-eral scales. When a problem requires dimensionality reduction commongoals between different applications can be highlighted, such as
Fig. 1.Mock-up of the setup used for hyperspectral imaging ( Fernandes et al., 2011).
Fig. 2.Imaging line over each berry ( Gomes et al., 2017a).
Fig. 3.Hyperspectral image of a wine grape berry sample before segmentation andreﬂectance measurements.R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
60preserving as much signiﬁcant structure or information in the high-dimensional data as in the low-dimensional representation/projection;increase the means of interpretation of the data in the lower dimension;and minimize the information loss in the new, low-dimensional repre-sentation of the data. In our previous studies ( Silva and Melo-Pinto, 2021) PCA established itself as the technique with the best results interms of easing the interpretation of the data by the regressor, obtainingthe representations that contained the most preserved information and,in most cases, the representations that led to less erroneous predictionsby the machine learning algorithm. When we compare these two tech-niques, we can denote some relevant differences:•PCA is a deterministic algorithm commonly used for feature extrac-tion, while t-SNE is a randomized algorithm that is mostly used for vi-sualization purposes only;•PCA applies a linear technique where the focus is on keeping the dis-similar points apart in a lower-dimensional space, while t-SNE appliesa non-linear technique that attempts to keep the similar data pointsclose together in the lower-dimensional space;•PCA transforms the original data by preserving the variance in thedata using eigenvalues matrices and is very affected by outliers,while t-SNE preserves the local structure of the data by using studentt-distributions to compute the similarity between two points in thelower-dimensional space (which helps address crowding and optimi-zations problems) and is not as strongly impacted by outliers.Henceforth, since t-SNE is mostly used for visualization purposes it isunclear how it will perform in a general dimensionality reduction task,and by having such contrasting characteristics to the technique that ob-tained the best results thus far, we were intrigued to conduct this appli-cation and study the outcome since we concluded previously ( Silva and Melo-Pinto, 2021) that, in non-linear techniques, local learners seemedto have better results than global learners, but we never tested a non-linear technique capable of preserving both local and global structure;and also, since the problem of estimating oenological parameters fromwine grape berries possesses high variability due to the hundreds of dif-ferent varieties and the different harvest years, a technique capable ofreducing the impact of outliers could lead to a superior generalizationcapacity.t-SNE originates from Stochastic Neighbor Embedding (SNE)(Hinton and Roweis, 2002), but uses a student t-distribution with aheavy-tailed probability distribution to address the crowding problemin the original technique: in summary, t-SNE minimizes the Kullback-Leibler divergences between the high-dimensional and the latentspaces with the cost function:C¼∑iKL P i∥QiðÞ ¼∑
i∑
jpj∣ilogpj∣i
qj∣ið2ÞwherePis the conditional probability distribution in the original spaceandQ
iis the conditional probability distribution in the latent space.Since the Kullback-Leibler divergence is not symmetric, the error inthe pairwise distances in the low-dimensional representation will beweighted differently: the usage of widely separated map points to rep-resent nearby data points will have a larger cost while the usage ofnearby map points to represent widely separated data points will havea smaller cost - this means that there is a focus on retaining the localstructure of the data in the low-dimensional representation.Hence, the variance of the Gaussian that is centered on data point x
i, the parameterσ
i, will never be optimally represented for all data pointsin the data set because the density of the data is likely to vary: in denserregions a smaller value ofσwill be more appropriate than in sparserregions. t-SNE will then perform a binary search for the value of σ
i
that produces aP iwith aﬁxed perplexity that is user-speciﬁed and deﬁned as:PerpP
iðÞ ¼2HP iðÞð3ÞwhereH(P
i)i st h eS h a n n o ne n t r o p yo fP imeasured in bits. One can then interpret perplexity as a simple measure of the effective number ofneighbors with typical values varying between 5 and 50: in this work,we used the same range of values for all the experiments, picking theperplexity value that originated the predictions with the least errorvalue.2.2.2. Kernel t-SNEt-SNE is a non-parametric technique that provides a high-to-low di-mensional representation of a given data set without a mapping for-mula on how to project further points not included in the original set:while it grants a higher degree ofﬂexibility (since no constraints haveto be met) it means that the result of the visualization step entirely de-pends on the formalization of the mapping procedure and that there isnot a direct way to map additional points after obtaining the projectionsof the data set; this fact renders t-SNE unsuitable for the visualization ofstreaming data or online scenarios. Additionally, non-parametric tech-niques are unﬁtting for large data sets since they display at least a qua-dratic complexity. To tackle these drawbacks ( Gisbrecht et al., 2015) introduces Kernel t-SNE.Kernel t-SNE is an approach that maintains the ﬂexibility of t-SNE while displaying generalization ability within out-of-sample exten-sions: it extends the non-parametric t-SNE to an explicit mapping byﬁxing the parametric formx→f
w(x)=yand optimizing the parameters off
winstead of the projection coordinates. The mappingf
w=yunderlying kernel t-SNE follows the form:x!yxðÞ ¼∑
jαj⋅kx,x j/C0/C1∑
ikx,x lðÞð4Þwherea
j∈Yare parameters corresponding to points in the projectionspace and the datax
jare taken as aﬁxed sample.kis the Gaussian kernel parameterized by the bandwidthσ
j:kx;x
j/C0/C1¼exp−0:5x−x
j/C13/C13/C13/C13 2=σ2j/C16/C17 ð5ÞThis generalized linear mapping allows training to be performed in asimple way (given a set of samplesx
iandy(x i) is available). Parameters
Fig. 4.Reﬂectance values obtained for a complete data set ( Silva and Melo-Pinto, 2021).R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
61αjcan be determined by a least squares solution of the mapping. Thus, inkernel t-SNE a standard t-SNE is applied to the subset X′to obtain a training set and afterwards the previous analytical solution is used toobtain the parameters of the mapping: once this is done the full set X can be projected in linear time by applying the mapping y.I nt h i s work, the bandwidthσwas deﬁned as 0.5 for all experiments, whilewe used the same range for the perplexity value in kernel t-SNE as theone used in t-SNE.2.3. Regression model: Support vector regressionAfter capturing the hyperspectral images and obtaining the re-ﬂectance measurements, dimensionality reduction techniques(such as t-SNE) are applied to the data sets to obtain a low-dimensional representation of the points that is better suited toserve as an input for a classiﬁcation or, for the case of this work, re-gression model: in this paper we chose the Support Vector Regres-sion (SVR) algorithm (Vapnik, 1999) since it obtained state-of-the- art results (Silva et al., 2018) for the particular application ofpredicting oenological parameters from hyperspectral images ofwine grape berries when compared to Neural Networks (NN) ( Janik et al., 2007), Partial Least Squares (PLS) (Arana et al., 2005)o r Least-Squares Support Vector Machines (LSSVM) ( Cao et al., 2010) models; additionally, to guarantee that for the case studies presentin this work the SVR algorithm obtained superior results and thatthe behaviour exhibited by PCA, t-SNE and Kernel t-SNE followedsimilar tendencies, we also applied NN for single vintage data sets -results can be found inAppendix A.The support vector algorithm is described as follows ( Smola and Schölkopf, 2004): given a set of training data {( x
1,y1),…,(x n, y
n)}⊂χ∈ℝ,w h e r eχrepresents the space of input patterns, thegoal is to determine a function f(x)=〈w,x〉+b,w∈χ,b∈ℝ that deviates a maximum of ɛfrom the real measured targetsy
ifor the entire training set while, simultaneously, being as ﬂat as possible. This convex optimization problem can be written as theminimization of the Euclidean norm for the cases of a linear function;to extend the SV machine to nonlinear functions the so-called “SV expansion”is introduced:fxðÞ ¼∑
ni¼1αi/C0α∗i/C0/C1x
i,xðÞ þbð6Þwhereωis described as a linear combination of the training patternsand cases withα
i> 0 are the support vectors. This SV expansion comesto light by writing the optimization problem in its dual formulationfrom both the objective function and the corresponding constraints. Ad-ditionally, a computationally efﬁcient form of mapping the input vectorsinto a high-dimensional feature space with a nonlinear mapping comeswith the usage of a suitable kernel function, obtaining the nonlinearregression functions of the form:fxðÞ ¼∑
ni¼1α∗i/C0α i/C0/C1⋅κx
i,xðÞ þbð7Þwhereκis the chosen kernel function. This transformation allows themodel to solve the optimization problem in a more suitable featurespace; there is a wide variety of kernels to choose from that are suitedfor different types of applications, with local kernels being based on dis-tance (only the data points near each other in ﬂuence the kernel values) and global kernels being based on dot product (data points far awayfrom each other still inﬂuence the kernel values): with the differenttests performed in previous works (Silva et al., 2018) we chose a Gauss- ian kernel for the current paper since it achieved the best results for ourapplication; as for the well-known hyperparameters Candγ, we per- formed a grid search for each experiment with Cvalues ranging from 80 to 120 andγranging from 1e
−5−1e−1.2.4. Cross-validation and evaluation metrics2.4.1. n-Fold Cross-ValidationAt this point of the models pipeline it is of paramount importance toguarantee that the results obtained are not skewed in any form by thetraining stage, since it is very common for these algorithms to sufferfrom“overﬁtting”- the statistical model achieving a perfect ﬁta g a i n s t its training data and being unable to perform accurately against unseendata - to tackle this issue a cross-validation approach was implemented.The n-fold cross-validation method (Lendasse et al., 2003;Remesan and Mathew, 2015) splits the data setXintoKparts of equal size with thekth set forming the validation set Xvaland the remaining sets forming the training setX
learn: the training of a modelgis performed usingX
learnand the errorE k(g)i sc a l c u l a t e da s :E
kgðÞ ¼∑NK
i¼1gxvali/C0/C1/C0yvali/C0/C1 2
N=K ð8Þwith (x
ival,yival) the elements ofX valandg(x ival) the estimation ofy ivalby modelg. This process loops forkvarying from 1 toKand the average error is computed as:bE
gengðÞ ¼∑Kk¼1EkgðÞK ð9ÞIn this work, we choose the number of folds Kvarying from a range of 5–10: we choose a smaller number of folds for smaller data sets and ahigher number for data sets with more samples.2.4.2. Trustworthiness and continuityIn order to evaluate the performance of t-SNE and kernel t-SNE andcompare it to a standard PCA approach for dimensionality reductionsome metric that measures the quality of the low-dimensional embed-dings must be employed: based on (Du, 2019;Venna and Kaski, 2006) we implemented the trustworthiness and continuity metrics.Trustworthiness and Continuity are metrics that attempt to measurethe degree of similarity of the local structure of the data between itsoriginal high-dimensional state and its low-dimensional visualizationobtained via the application of a dimensionality reduction technique.In particular, trustworthiness evaluates if the neighbors chosen are thesame in both representations and is deﬁned by:TkðÞ ¼1/C0AkðÞ∑
Ni¼1∑
j∈U kiðÞrTi,jðÞ /C0kð10ÞwhereNis the sample size,kis the number of nearest neighbors, A(k)i s a scaling function andU
k(i) is the set of points that are among the k nearest neighbors ofiin the low-dimensional space but not in thehigh-dimensional one.r
T(i,j) is the rank of pointjinU k(i)a c c o r d i n gt o the pairwise distance fromiin the original high-dimensional space.On the other hand, continuity attempts to quantify how well the localstructure was maintained after its transformation to a low-dimensional visualization and is deﬁned by:CkðÞ ¼1/C0AkðÞ∑
Ni¼1∑
j∈V kiðÞrCi,jðÞ /C0kð11ÞwhereV
k(i) is the set of points that are among the knearest neighbors of data pointiin the original high-dimensional space but are not neighborsin the visualization.r
C(i,j) is the rank of the pointjinV k(i)o r d e r e db y the pairwise distances betweenjandiin the low-dimensional visualiza- tion.2.4.3. Root mean square error (RMSE) and determination coef ﬁcient R
2
Concerning the evaluation of the performance of the dimensionalityreduction techniques we also measure the generalization error of theR. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
62regressor trained on the low-dimensional data representation(Sanguinetti, 2008): this process allows the comparison of the predic-tion results with other state-of-the-art publications. Root mean squaree r r o r( R M S E )i sd eﬁned as:RMSE¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
∑Ni¼1byi/C0yi/C0/C12
N/C01s ð12Þwherey
iis the reference value andbyiis the model estimate. The generalization error is the only well-established practice in literatureto evaluate the accuracy of a model and we measure the RMSE for thehold-out test set; however, it seems that for this particular area of re-search the usage of the determination coef ﬁcient (R
2) as an indicator of quality of prediction is very common. R
2is deﬁned as:R
2¼σy^y
σyσy/C18/C192
ð13Þwhereσ
y,^yis the covariance betweenyand^yandσ i,σ^yare the respective standard deviations. While we present the R
2indicator in the results section to serve as a sort of comparison benchmark toother works in literature for the prediction of oenological parametersin hyperspectral images of wine grape berries, it is important todenote that, according to (Spiess and Neumeyer, 2010),R
2is not well- deﬁned for non-linear regression since it shows extreme bias to higherparameterized models and, in a background of low and medium exper-imental noise, this indicator cannot compensate the effect of the num-ber of increasing parameters.2.5. Grape sampling and data set analysisRegarding the variety of wine grape berries used to frame the datasets for this work, we chose grapes of the native Portuguese varietyTouriga Franca (TF) mainly due to its' importance for Port wine produc-tion in the Portuguese Douro region, as well as due to its' importance toour industrial partner Symington Family Estates: the TF variety is so im-portant for both producers and oenologists due to its' resistance to mostplant diseases, the strongﬂavour and aroma of the resulting wine anddue to its' high concentration of tannins, which guarantees that thewine will age well. The grapes were harvested from Quinta do Bon ﬁm in Pinhão, Portugal in the years (refers to the years of sample collecting,do not mistake for year of vine tree plantation) of 2012 (240 samples),2013 (81 samples), 2014 (120 samples), 2016 (407 samples) and2017 (540 samples) from different regions in the vineyard and with dif-ferent levels of maturity; each sample is composed of six grape berriescollected from a single bunch and the ground-truth results are obtainedvia laboratory analysis using validated standard methods ( Carbonneau and Champagnol, 1993;Ofﬁce International de la Vigne and du Vin,1990).Henceforth, we conducted a study of the models ability to predictthe pH index and sugar content on different vintages of wine grapeberries from the TF variety: we chose to evaluate these oenological pa-rameters since they are highly researched due to their correlationwithﬂavour, colour and overall grape ripeness stage; additionally, dueto several factors such as climate change, soil quality, sun exposition,water assessment, altitude and harvest time, a large variability is pres-ent in the vineyards and consequently in the quality of the grapes andtheir oenological proﬁle, which makes the evaluation of the models' ca-pacity across different vintages a true test to its' generalization potential.Appendix Bcontains a descriptive statistics analysis of the ground-truthresults for each oenological parameter in the different TF data sets used;7 presents ANOVA (one-way analysis of variance) tests to verify signif-icant differences between the means of the different data sets.In order to achieve a more comprehensible form of presenting theresults, we decided to split the analysis according to the characteristicsof the training set and the hold-out test set for each run:•Case An: cases where the training and test sets use the same singlevintage of TF wine grape berries;•Case Bn: cases where the training set employs multiple vintages of TFwine grape berries and the hold-out test set uses the same multiplevintages of TF wine grape berries.•Case Cn: cases where the training set uses single/multiple vintages ofTF wine grape berries and the hold-out test set uses a different vintageof TF wine grape berries.For each case study we create a hold-out test set with 10% of the sizeof the correspondent training set; for each training set we performcross-validation with a 90/10% split into training/validation folders.Table 1provides details on all the experiments performed.In each of the experiments previously described we present the RMSEandR
2obtained by the regressor for the hold-out test set for each oeno-logical parameter (pH index and sugar content) and for each dimension-ality reduction method employed (PCA, t-SNE and kernel t-SNE) - asmentioned previously, we compare the performance of t-SNE and kernelt-SNE against PCA since in our previous work ( Silva and Melo-Pinto, 2021), for these same data sets and case studies, PCA outperformed awide variety of nonlinear methods establishing itself as the best techniqueto reduce our inputs, which goes in accordance to several studies in liter-ature that show that, for real-world data, PCA still outperforms the mostadvanced techniques introduced in recent years; the trustworthinessand continuity metrics are calculated for each data set for every dimen-sionality reduction method implemented. Appendix Dsummarizes the best published results in literature and how they compare with the bestresults in this work, split by corresponding case studies.3. Results and discussion3.1. AnalysisTable 2andTable 3present the results (best results are underlined)obtained for the trustworthiness and continuity measures for all datasets. Observing these tables, it is noticeable that t-SNE achieves thebest results in trustworthiness across the different data sets but the con-tinuity results are a bit closer: this means that while t-SNE does a betterjob of choosing the same neighbors in both the higher and lower dimen-sional representations, how well the local structure was maintainedafter its transformation (deﬁned by continuity) is on par betweenboth methods; however, it is important to denote that for the dataTable 1Experiment details for each case study.Case Training/Validation set Test setA1 TF 2012 TF 2012A2 TF 2013 TF 2013A3 TF 2014 TF 2014B1 TF 2012/13 TF 2012/13B2 TF 2012/13/14 TF 2012/13/14B3 TF 2012/13/14/16 TF 2012/13/14/16C1 TF 2012 TF 2013C2 TF 2012/13 TF 2014C3 TF 2012/13/14/16 TF 2017
Table 2TrustworthinessT(20) for each dimensionality reduction technique in each individualdata set.TrustworthinessMethod TF 2012 TF 2013 TF 2014 TF 2016 TF 2017PCA 0.9949 0.9935
0.9972 0.9832 0.9702 t-SNE
0.9966 0.9947 0.9935 0.9975 0.9981 Kernel t-SNE 0.9871 0.9761 0.9649 0.9840 0.9866R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
63sets with a higher number of samples and a higher intrinsic dimension-ality, t-SNE gets the best results which might be an indicator of thismethod being better suited to reduce the dimensionality of more com-plex samples, a trait that is also present on Kernel t-SNE results.Table 4showcases the results obtained in the prediction of the pHindex for the different case studies and dimensionality reductionmethods. In a direct comparison, the t-SNE technique gets the best re-sults for almost every single case study, with smaller error rates andhigher determination coefﬁcient values. As highlighted previously bythe trustworthiness and continuity measures, it is also important to no-tice that the Kernel t-SNE method achieves smaller error rates in the lastcase studies in comparison to the PCA method: these are the caseswhere training sample number increases signi ﬁcantly and where the in- trinsic dimensionality of the data rises. While the regressor perfor-mance is quite stable across the different case studies, it is stillnoticeable that the smallest error rates are obtained in cases wherethe training and test sets use the same single vintage of TF wine grapeberries: this shows that despite having a very strong generalization ca-pacity, the model still cannot predict the pH index for cases of unseenvintages in the testing phase with such certainty as it does for whenthe training set includes samples of that same vintage: this can be con-sidered normal, since the complexity of the prediction signi ﬁcantly in- creases (Appendix C Table C.1shows that there is a signiﬁcant difference in the means between almost every single vintage) and aneven bigger drop-off in results would not be surprising, since we areusing a shallow learning regressor to operate the predictions and pHindex values are extremely vulnerable to even the smallest changes inexternal factors (such as weather, water availability, etc.). Additionally,we conducted a pairedt-test between the predictions obtained by eachof the dimensionality reduction methods and no difference in themeans between the sets was found.Table 5presents the results obtained for the prediction of sugar con-tent for the different case studies and dimensionality reduction methods:once again, the t-SNE technique achieves the best results for almost everycase study, with smaller error rates and higher determination coefﬁcient values; additionally, it is once again important to emphasize that both t-SNE and Kernel t-SNE results for cases where the intrinsic dimensionalityand number of samples rises are better than the ones obtained by PCA. Asfor the generalization capacity of the regressor, for the sugar content pre-diction the drop-off in results (higher error rate) is more noticeable thanfor the case of the pH index, which could be expected since we areworking with more sparsely distributed sample points ( Appendix Bpre- sents descriptive statistics of the samples for each data set in Table B.1 andTable B.2); this increase in error rate can be even more perceptiblein cases where the TF 2014 vintage is employed in training or testing(cases A3, B2, B3, C2 and C3), which can be explained by the fact thatthis particular vintage had particularly low values for the sugar content,with a much smaller mean value when compared to the other vintages,causing the regressor to have bigger problems in generalizing its predic-tions - however, we still consider the generalization capacity of themodel to be very acceptable, specially considering we are using a shallowlearning regressor that lacks the capability of more powerful deep learn-ing techniques. The paired t-test to investigate statistical signi ﬁcant differ- ences between the means of the predictions given by each dimensionalityreduction method showed that there were signi ﬁcant differences in the means of the predictions obtained by the PCA and Kernel t-SNE methodswhen compared to the PCA for case studies A3 and C3, which possibly jus-tiﬁes the difference in the results with the fact that the regressor high-lighted different features for learning in these sets.3.2. ComparisonComparing our results with top state-of-art works ( Appendix D Table D.1provides a comparative review table), t-SNE obtained thebest RMSE andR
2values for sugar content estimation out of all studieswhere the training and test sets use the same single vintage of winegrape berries and also out of all studies where the test set uses a differ-ent vintage of wine grape berries; regarding case study B (same multi-ple vintages of wine grape berries employed in both the training andhold-out test sets) t-SNE achieved on par and competitive results, onlybeing outdone by the Convolutional Neural Networks (CNN) model in(Gomes et al., 2021a) and by the Partial Least Squares (PLS) models pre-sented by (Caballero et al., 2011) and (Fadock et al., 2016): in the au- thors opinion, this emphasizes even more the capabilities of the t-SNEtechnique for dimensionality reduction, since we are able to obtaincompetitive or superior results with a shallow learning regressor versusmodels that already employ more advanced, deep learning solutionsfor prediction; considering the differences in methodology betweendifferent research groups (different data sets, different setups forhyperspectral imaging, different modes for re ﬂectance/transmittance/ interactance measurements, different techniques for prediction, etc.)we also consider that a direct comparison, with the same pipeline andsamples, between different techniques provide better conclusionsabout the dimensionality reduction step and, in this matter, we show-case that t-SNE achieves better results than PCA for the same case stud-ies, which is a novelty since no other works can be found that report thissuperiority in real-world data.As for the pH index estimation, t-SNE achieved very competitive re-sults, even obtaining the best RMSE and R
2values of all studies where the training and test sets use the same single vintage of wine grapeberries; when comparing case studies with multiple vintages of wineTable 4Results obtained for pH index in the hold-out test set for each case study.pH IndexCasePCA t-SNE Kernel t-SNER
2RMSER2RMSER2RMSEA1 0.786 0.186
0.870 0.148 0.825 0.177 A2 0.810 0.199
0.947 0.106 0.899 0.139 A3 0.753 0.139
0.829 0.120 0.788 0.148 B1 0.814 0.171
0.876 0.158 0.813 0.170 B2 0.818
0.163 0.863 0.171 0.803 0.168 B3 0.783 0.174
0.832 0.162 0.757 0.185 C1 0.845 0.229
0.953 0.153 0.845 0.189 C2
0.818 0.195 0.808 0.152 0.738 0.161 C3 0.797 0.245
0.869 0.120 0.860 0.133Table 5Results obtained for sugar content in the hold-out test set for each case study.Sugar Content (°Brix)CasePCA t-SNE Kernel t-SNER
2RMSER2RMSER2RMSEA1 0.922 1.146
0.949 0.955 0.926 1.113 A2 0.940 1.470
0.985 0.737 0.982 0.950 A3 0.898 2.155
0.958 1.279 0.817 1.288 B1 0.935 1.791
0.944 1.180 0.918 1.492 B2
0.916 1.601 0.907 1.545 0.851 1.593 B3 0.876
1.304 0.890 1.430 0.866 1.746 C1 0.940 1.544
0.964 1.510 0.951 1.726 C2 0.839
3.558 0.940 3.789 0.868 3.559 C3 0.830 2.964
0.840 1.863 0.809 1.776Table 3ContinuityC(20) for each dimensionality reduction technique in each individual data set.ContinuityMethod TF 2012 TF 2013 TF 2014 TF 2016 TF 2017PCA 0.9965
0.9944 0.9978 0.9912 0.9866 t-SNE
0.9966 0.9942 0.9910 0.9954 0.9926 Kernel t-SNE 0.9857 0.9539 0.9571 0.9852 0.9773R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
64grape berries or with a different vintage used in the hold-out test set,despite showcasing a very strong performance, our model still falls abit short out of the results obtained by the CNN in ( Gomes et al., 2021a, 2021b): as mentioned previously, we believe that a direct com-parison between techniques provides more meaningful information re-garding the dimensionality reduction step, since it is hard to pinpoint ifthe difference in results derives from a weaker dimensionality reductiontechnique, from a weaker regressor, or from the overall differences inmethodology and samples - in this matter and in our case studies, weonce again highlight the superiority of t-SNE when compared to PCA, in-dependently of the regressor employed ( Appendix Ashows that while using a NN regressor the t-SNE still outperforms PCA).4. ConclusionsAn evaluation of the performance of t-SNE and Kernel t-SNE tech-niques for dimensionality reduction was carried out in real-world dataof hyperspectral images of wine grape berries. Our study combinedthese techniques with a machine learning regressor (Support Vector Re-gression) to predict sugar content and pH index values in different vin-tages of wine grape berries, a highly complex problem due to highvariability in samples across different vintages and varieties of winegrapes. Our results show that not only t-SNE is a technique suited for di-mensionality reduction tasks (it is normally used for visualization pur-poses only), but it even surpasses the results obtained by PCA foralmost every single case study, a very rare achievement since most re-cent dimensionality reduction techniques introduced showcase a strongperformance on artiﬁcial data sets but fail to replicate it in real-worlddata, usually falling short of the performance obtained by PCA. Addition-ally, Kernel t-SNE also showed a strong performance, opening the possi-bility of using a t-SNE extension for dimensionality reduction tasks instreaming data or online scenarios.Comparing our results with state-of-art works, our model combinedof a shallow learning regressor with t-SNE to reduce the dimensionalityof the inputs achieved either better or on par results with the ones pre-sented in literature, even competing with works using more re ﬁned deep learning models (like CNNs); this highlights the capability of t-SNE, since previous models that combined a shallow learning regressorwith PCA could not compete with the results obtained by more ad-vanced models. Additionally, the development of accurate shallowlearning models for oenological parameter estimation fromhyperspectral images of wine grapes is extremely important, since ac-quiring the necessary number of samples to allow for the usage of adeep learning model that does not overﬁt to the training set and has generalization capacity is very costly and time-consuming.Future works should expand on the possibility of using t-SNE in thedimensionality reduction step when combined with deep learningmodels, since it is possible that an increase in performance could beachieved: additionally, we also believe that further work in this areashould be directed into deep learning models and arti ﬁcial data synthe- sis, since it is very difﬁcult and costly to obtain new samples and there isa huge variability present in the spectra of different varieties and vin-tages of wine grapes.CRediT authorship contribution statementRui Silva:Conceptualization, Software, Validation, Formal analysis,Investigation, Writing–original draft, Visualization.Pedro Melo- Pinto:Methodology, Resources, Data curation, Writing –review & editing, Supervision, Project administration, Funding acquisition.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgementsThis work is supported by National Funds by FCT —Portuguese Foundation for Science and Technology, under the project UIDB/04033/2020. The authors also gratefully acknowledge the support fromNational funding by FCT, Portuguese Foundation for Science and Tech-nology, through the individual research grant (SFRH/BD/137216/2018) and from NVIDIA Corporation with the donation of the Titan XPascal GPU used for this research.Appendix A
Table A.1Results obtained for pH index in the hold-out test set for case studies A with SVR and NN.pH indexModel Case PCA t-SNE Kernel t-SNER
2RMSER2RMSER2RMSESVRA1 0.786 0.186
0.870 0.148 0.825 0.177 A2 0.810 0.199
0.947 0.106 0.899 0.139 A3 0.753 0.139
0.829 0.120 0.788 0.148NNA1 0.764 0.202 0.830 0.168 0.790 0.197A2 0.787 0.496 0.870 0.187 0.865 0.231A3 0.738 0.176 0.802 0.120 0.761 0.150
Table A.2Results obtained for sugar content in the hold-out test set for case studies A with SVR and NN.Sugar contentModel Case PCA t-SNE Kernel t-SNER
2RMSER2RMSER2RMSESVRA1 0.922 1.146
0.949 0.955 0.926 1.113 A2 0.940 1.470
0.985 0.737 0.982 0.950 A3 0.898 2.155
0.958 1.279 0.817 1.288NNA1 0.926 1.695 0.928 1.036 0.904 1.404A2 0.936 1.641 0.981 1.392 0.947 1.452A3 0.897 2.220 0.904 1.876 0.798 2.287R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
65Appendix B
Table B.1Descriptive statistics for the pH index of all TF vintages.pH indexVariety N Mean 95% CI St.Dev. 95% CI Min Median MaxTF 2012 240 3.55 (3.51; 3.60) 0.35 (0.32; 0.38) 2.85 3.58 4.23TF 2013 81 3.72 (3.64; 3.80) 0.35 (0.31; 0.42) 3.05 3.74 4.44TF 2014 120 3.49 (3.45; 3.54) 0.26 (0.23; 0.30) 2.93 3.51 3.97TF 2016 404 3.85 (3.83; 3.88) 0.28 (0.26; 0.30) 3.09 3.84 4.60TF 2017 538 3.90 (3.88; 3.92) 0.19 (0.18; 0.20) 3.29 3.92 4.97
Table B.2Descriptive statistics for the sugar content of all TF vintages.Sugar content (°Brix)Variety N Mean 95% CI St.Dev. 95% CI Min Median MaxTF 2012 240 16.93 (16.50; 17.35) 3.34 (3.07; 3.67) 9.06 17.06 24.71TF 2013 82 19.45 (18.66; 20.23) 3.58 (3.11; 4.24) 8.10 20.03 25.00TF 2014 120 13.56 (12.89; 14.21) 3.66 (3.25; 4.19) 7.87 13.00 25.66TF 2016 404 17.83 (17.57; 18.09) 2.62 (2.45; 2.82) 10.07 17.54 26.03TF 2017 538 19.86 (19.57; 20.14) 3.37 (3.18; 3.58) 10.94 20.34 30.08
Appendix C
Table C.1One-way ANOVA for the pH index of the laboratory results.Tukey simultaneous tests for differences of meanspH indexDiff. of Levels Diff. of Means SE. of Diff 95% CI T-Value ρ-valueTF2013 - TF2012 0.17 0.03 (0.07; 0.26) 4.85 0.000TF2014 - TF2012 −0.06 0.03 ( −0.14; 0.02) −1.99 0.272 TF2016 - TF2012 0.30 0.02 (0.24; 0.37) 13.92 0.000TF2017 - TF2012 0.35 0.02 (0.29; 0.40) 16.84 0.000TF2014 - TF2013 −0.22 0.04 ( −0.33;−0.12)−5.88 0.000 TF2016 - TF2013 0.14 0.03 (0.05; 0.22) 4.20 0.000TF2017 - TF2013 0.18 0.03 (0.10; 0.27) 5.74 0.000TF2016 - TF2014 0.36 0.03 (0.29; 0.44) 13.05 0.000TF2017 - TF2014 0.40 0.03 (0.33; 0.48) 15.15 0.000TF2017 - TF2016 0.05 0.02 (0.00; 0.09) 2.63 0.065H
0: All means are equal; Signiﬁcance:α= 0.05; Individual Conﬁdence = 99.55%.Table C.2One-way ANOVA for the sugar content of the laboratory results.Tukey simultaneous tests for differences of meansSugar content (°Brix)Diff. of levels Diff. of means SE. of Diff 95% CI T-value ρ-valueTF2013 - TF2012 2.52 0.41 (1.40; 3.64) 6.15 0.000TF2014 - TF2012 −3.37 0.35 ( −4.35;−2.40)−9.41 0.000 TF2016 - TF2012 0.90 0.26 (0.19; 1.62) 3.46 0.005TF2017 - TF2012 2.93 0.25 (2.25; 3.61) 11.77 0.000TF2014 - TF2013 −5.89 0.46 ( −7.15;−4.64)−12.83 0.000 TF2016 - TF2013 −1.62 0.39 ( −2.68;−0.58)−4.16 0.000 TF2017 - TF2013 0.41 0.38 ( −0.63; 1.45) 1.08 0.819 TF2016 - TF2014 4.28 0.33 (3.37; 5.19) 12.83 0.000TF2017 - TF2014 6.30 0.32 (5.42; 7.19) 19.47 0.000TF2017 - TF2016 2.03 0.21 (1.45; 2.60) 9.60 0.000H
0: All means are equal; Signiﬁcance:α= 0.05; Individual Conﬁdence = 99.55%.R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
66Appendix D
Table D.1Summary of the best results published in literature for the prediction of oenological parameters on hyperspectral images of wine grape berries.RMSE R
2
Cases Authors Model Sugar (°Brix) pH Index Sugar pH IndexAPW SVR 0.737 0.106 0.9850.947 B 1.180 0.158 0.944 0.876 C 1.510 0.1530.9640.953
A(Arana et al., 2005) PLS 1.270 –0.710– (Bueno et al., 2014) MPLS 1.370
0.120 0.990 0.940 (Cao et al., 2010) LSSVM 0.960 –0.907– (Cao et al., 2010) PLS 0.925 –0.914– (Costa et al., 2019) PLS 1.150 –0.950– (Cozzolino et al., 2004) MPLS – 0.150–0.850 (Fernandes et al., 2015) NN 0.950 0.180 0.920 0.730(Gomes et al., 2017a) NN 0.955 –0.924– (Gomes et al., 2017a) PLS 0.939 –0.929– (Silva et al., 2018) SVR
0.804 0.142 0.964 0.887 (Silva and Melo-Pinto, 2021) SVR 1.100 –0.930–
B(Caballero et al., 2011) MPLS 1.000 0.120 0.910
0.870 (Fadock et al., 2016) PLS 1.090
0.060 0.700 0.720 (Gomes et al., 2017b)N N – 0.191–0.723 (Gomes et al., 2017a) NN 1.355 –0.917– (Gomes et al., 2017a) PLS 1.344 –0.948– (Gomes et al., 2021a) CNN
0.755 0.110 –– (Gomes et al., 2021b) CNN 0.970 –0.920– (Silva et al., 2018) SVR 1.048 0.170 0.948 0.830(Silva and Melo-Pinto, 2021) SVR 0.922 –
0.969–C(Gomes et al., 2021a) CNN 1.085
0.183–– (Silva and Melo-Pinto, 2021)N N
1.060 – 0.986–PW: Present Work.
References
Alibert, Y., 2019.New metric to quantify the similarity between planetary systems: appli-cation to dimensionality reduction using t-SNE. Astron. Astrophys. 624, A45.Anowar, F., Sadaoui, S., Selim, B., 2021. Conceptual and empirical comparison of dimen- sionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t-SNE). Comput. Sci. Rev. 40, 100378.Arana, I., Jarén, C., Arazuri, S., 2005. Maturity, variety and origin determination in white grapes (vitis vinifera l.) using near infrared re ﬂectance technology. J. Near Infrared Spectrosc. 13, 349–357.Bueno, J., Hernández-Hierro, J., Rodrguez-Pulido, F., Heredia, F., 2014. Determination of technological maturity of grapes and total phenolic compounds of grape skins inred and white cultivars during ripening by near infrared hyperspectral image: a pre-liminary approach. Food Chem. 152, 586 –591. Caballero, V., Pérez-Marn, D., López, M., Sánchez, M., 2011. Optimization of nir spectral data management for quality control of grape bunches during on-vine ripening.Sensors 11, 6109–6124.Cao, F., Wu, D., He, Y., 2010.Soluble solids content and ph prediction and varieties dis-crimination of grapes based on visible –near infrared spectroscopy. Comput. Electron. Agric. 71, S15–S18.Carbonneau, A., Champagnol, F., 1993. Nouveaux systèmes de culture integré du vignoble. Programme AIR.Costa, D., Mesa, N., Freire, M., Ramos, R., Mederos, B., 2019. Development of predictive models for quality and maturation stage attributes of wine grapes using Vis-nirreﬂectance spectroscopy. Postharvest Biol. Technol. 150, 166 –178. Cozzolino, D., Cynkar, W., Janik, L., Dambergs, B., Francis, L., Gishen, M., 2004. Measure- ment of colour, total soluble solids and ph in whole red grapes using visible andnear infrared spectroscopy. Proceedings of the 12th Australian Wine Industry Techni-cal Conference, Melbourne, Australia, pp. 24 –29. Devassy, B., George, S., 2020.Dimensionality reduction and visualisation of hyperspectralink data using t-SNE. Forensic Sci. Int. 311, 110194.Du, T., 2019.Dimensionality reduction techniques for visualizing morphometric data:comparing principal component analysis to nonlinear methods. Evol. Biol. 46,106–121.Fadock, M., Brown, R., Reynolds, A., 2016. Visible-near infrared reﬂectance spectroscopy for nondestructive analysis of red wine grapes. Am. J. Enol. Vitic. 67, 38 –46. Fernandes, A., Oliveira, P., Moura, J., Oliveira, A., Falco, V., Correia, M., Melo-Pinto, P., 2011.Determination of anthocyanin concentration in whole grape skins usinghyperspectral imaging and adaptive boosting neural networks. J. Food Eng. 105,216–226.Fernandes, A., Franco, C., Mendes-Ferreira, A., Mendes-Faia, A., da Costa, P., Melo-Pinto, P.,2015.Brix, pH and anthocyanin content determination in whole port wine grapeberries by hyperspectral imaging and neural networks. Comput. Electron. Agric.115, 88–96.Gao, L., Gu, D., Zhuang, L., Ren, J., Yang, D., Zhang, B., 2019. Combining t-distributed sto- chastic neighbor embedding with convolutional neural networks for hyperspectralimage classiﬁcation. IEEE Geosci. Remote Sens. Lett. 17, 1368 –1372. Gisbrecht, A., Mokbel, B., Hammer, B., 2012. Linear basis-function t-SNE for fast nonlinear dimensionality reduction. The 2012 International Joint Conference on Neural Net-works, pp. 1–8.Gisbrecht, A., Schulz, A., Hammer, B., 2015. Parametric nonlinear dimensionality reduc- tion using kernel t-SNE. Neurocomputing 147, 71 –82. Gomes, V., Melo-Pinto, P., 2021. Towards robust Machine Learning models for grape ripe-
ness assessment. IEEE 18th International Joint Conference on Computer Science andSoftware Engineering (JCSSE), pp. 1 –5. Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2014a. Comparison of different approaches for the Prediction of Sugar Content in Whole Port Wine Grape Berriesusing Hyperspectral Imaging. ENBIS 14: 14th Annual Conference of the EuropeanNetwork for Business and Industrial Statistics, p. 1.Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2014b. Determination of Sugar Content in Whole Port Wine Grape Berries Combining Hyperspectral Imaging with NeuralNetworks Methodologies. IEEE Symposium Series on Computational Intelligence.Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2017a. Comparison of different approaches for the prediction of sugar content in new vintages of whole port winegrape berries. Comput. Electron. Agric. 140, 244 –254. Gomes, V., Fernandes, A., Martins-Lopes, P., Pereira, L., Faia, A., Melo-Pinto, P., 2017b.Characterization of neural network generalization in the determination of pH and an-thocyanin content of wine grape in new vintages and varieties. Food Chem. 218,40–46.Gomes, V., Mendes-Ferreira, A., Melo-Pinto, P., 2021a. Application of hyperspectral imag- ing and deep learning for robust prediction of sugar and pH levels in wine grapeberries. Sensors 21, 3459.Gomes, V., Reis, M., Rovira-Más, F., Mendes-Ferreira, A., Melo-Pinto, P., 2021b. Prediction of sugar content in port wine vintage grapes using machine learning andhyperspectral imaging. Processes 9, 1241.Gowen, A., O’Donnell, C., Cullen, P., Downed, G., Frias, J., 2007. Hyperspectral imaging–an emerging process analytical tool for food quality and safety control. Trends Food Sci.Technol. 18, 590–598.Hall, A., Lamb, D., Holzapfel, B., Louis, J., 2002. Optical remote sensing applications in viti- culture - a review. Aust. J. Grape Wine Res. 36 –47. Hariharan, S., 2021.Analysing effect of t-SNE and 1-D CNN on performance ofhyperspectral image classiﬁcation. Turk. J. Comput. Math. Educ. 12, 1828 –1833. Hinton, G., Roweis, S., 2002.Stochastic neighbor embedding. Advances in Neural Informa-tion Processing Systems, pp. 833 –840. Janik, L., Cozzolino, D., Dambergs, R., Cynkar, W., Gishen, M., 2007. The prediction of total anthocyanin concentration in red-grape homogenates using visible-near-infraredspectroscopy and artiﬁcial neural networks. Anal. Chim. Acta 594, 107 –118. Lendasse, A., Wertz, V., Verleysen, M., 2003. Model selection with cross-validations and bootstraps—Application to time series prediction with rbfn models. Arti ﬁcial NeuralR. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
67Networks and Neural Information Processing —ICANN/ICONIP 2003. Springer, pp. 573–580.Miao, A., Zhuang, J., Tang, Y., He, Y., Chu, X., Luo, S., 2018. Hyperspectral image-based variety classiﬁcation of waxy maize seeds by the t-SNE model and procrustesanalysis. Sensors 18, 4391.Ofﬁce International de la Vigne and du Vin, 1990. Recueil des méthodes internationales d’analyse des vins et des moûts. OIV.Pouyet, E., Rohani, N., Katsaggelos, A., Cossairt, O., Walton, M., 2018. Innovative data reduction and visualisation stratagy for hyperspectral imaging datasets using t-SNEapproach. Pure Appl. Chem. 90, 493 –506. Remesan, R., Mathew, J., 2015. Model data selection and data pre-processing approaches.Hydrological Data Driven Modelling. Springer, pp. 41 –70. Sanguinetti, G., 2008.Dimensionality reduction of clustered data sets. IEEE Trans. PatternAnal. Mach. Intell. 30, 535–540. Schulz, A., Hammer, B., 2015. Discriminative dimensionality reduction for regressionproblems using theﬁsher metric. International Joint Conference on Neural Networks,pp. 1–8.Silva, R., Melo-Pinto, P., 2021.A review of different dimensionality reduction methods forthe prediction of sugar content from hyperspectral images of wine grape berries.Appl. Soft Comput. 113.Silva, R., Gomes, V., Mendes-Faia, A., Melo-Pinto, P., 2018. Using support vector regression and hyperspectral imaging for the prediction of oenological parameters on differentvintages and varieties of wine grape berries. Remote Sens. 10, 312.Smola, A., Schölkopf, B., 2004. A tutorial on support vector regression. Stat. Comput. 14,199–222.Spiess, A., Neumeyer, N., 2010. An evaluation of r2 as an inadequate measure for nonlin-ear models in pharmacological and biochemical research: a Monte Carlo approach.BMC Pharmacol. 10, 1–11.Van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9. Vapnik, V., 1999.The Nature of Statistical Learning Theory. Springer Science & BusinessMedia.Venna, J., Kaski, S., 2006.Visualizing Gene Interaction Graphs with Local Multidimen-sional Scaling. ESANN, Citeseer, pp. 557 –562. Wold, S., Esbensen, K., Geladi, P., 1987. Principal component analysis. Chemom. Intell. Lab. Syst. 2, 37–52.Zhang, J., Chen, L., Zhou, L., Liang, X., Li, J., 2018. An efﬁcient hyperspectral image retrieval method: deep spectral-spatial feature extraction with DCGAN and dimensionalityreduction using t-SNE-based NM hashing. Remote Sens. 10, 271.R. Silva and P. Melo-Pinto Artiﬁcial Intelligence in Agriculture 7 (2023) 58 –68
68