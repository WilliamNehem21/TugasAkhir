A heuristic approach for load balancing the FP-growth algorithmon MapReduce
Sikha Bagui*, Keerthi Devulapalli, John Coffey
Department of Computer Science, University of West Florida, Pensacola, FL, USA
ARTICLE INFO
Keywords:Association rule miningFrequent pattern growth algorithmLoad balancingMapReduceHadoopABSTRACT
Frequent itemset discovery is an important step in Association Rule Mining. The Frequent Pattern (FP) growthalgorithm, often used for discovering frequent itemsets, cannot scale directly to today ’s Big Data, especially for large sparse datasets. Hence there is a need to distribute and parallelize the FP-growth algorithm. Parallel FP-growth (PFP) is a parallel implementation of the FP-growth algorithm on Hadoop ’s MapReduce execution framework. Though PFP scales to large datasets, it suffers from imbalanced load across processing units. In thispaper we propose a heuristic based, lower order of complexity, load balancing strategy for the PFP algorithm,called Heuristic Based PFP (HBPFP). Our results show that HBPFP distributes the load more evenly across theHadoop cluster nodes, runs faster than the PFP algorithm, and uses cluster resources more ef ﬁciently, especially for large sparse datasets.
1. IntroductionAssociation Rule Mining (ARM), most commonly used in trans-actional databases, is a process of discovering relationships among itemsor itemsets. Association rules are formed from frequent itemsets, hencetheﬁrst step of ARM is frequent itemset discovery. A well-known algo-rithm for discovering frequent itemsets is the Apriori algorithm [ 2]. This algorithm requires multiple scans of a database, which is inef ﬁcient in today’s Big Data era. The FP-growth [3] algorithm is another well-known algorithm for discovering frequent itemsets. It is based on the FP-Treedata structure, which stores frequency information of items in aconcise form. Unlike the Apriori algorithm, the FP-growth algorithm onlyrequires two scans of the database and it does not generate candidateitemsets. Hence the FP-growth algorithm is considered more ef ﬁcient than the Apriori algorithm [3].But, the FP growth algorithm has performance and scalability issues[3]. The FP-Tree, for very large data sets, will not ﬁt in memory, making it difﬁcult to process Big Data. A distributed and parallel implementationof the FP growth algorithm is needed to scale to large data sets. Hadoop[4] is a generic distributed framework that provides two features mostimportant for Big Data solutions–a distributedﬁle system, Hadoop’s Distributed File System (HDFS), for efﬁciently storing large data sets in the size of petabytes, and an efﬁcient distributed and parallel executionframework called MapReduce to process data stored in HDFS. Data storedin HDFS is divided into data blocks and each block is stored on a differentcluster node, allowing for parallel processing on the nodes.There have been several distributed implementations [ 9–15] of the FP-growth algorithm on different platforms. It can be observed fromthese implementations that apart from implementing this data miningalgorithm on different platforms, there is an overhead of task distribu-tion, task scheduling, effective resource utilization, task synchronization,etc. These tasks are related to maintaining a distributed system. Inspite ofthe advantages of using a parallel and distributed ﬁle system, [1] implemented a parallel version of the FP growth algorithm called ParallelFP (PFP) growth on Hadoop’s MapReduce framework, and this algorithmdid not produce optimized results due to imbalanced load in the nodes.The main motivation of this paper is in the need for load balancing inthe MapReduce environment in order to ef ﬁciently handle memory intensive algorithms like FP-Tree, especially for large sparse data setswhich would be even more memory intensive. Dividing the data equallybetween the nodes in the cluster, which seems like the most logical thingto do, does not necessarily mean that the load is balanced for every al-gorithm that is being run on Hadoop ’s MapReduce environment. So, what is meant byloadbalancing? This can be explained by deﬁning load. Load is the work to be performed by a node. Load in ﬂuences the time taken by a node to complete its share of an overall task. When some nodes
* Corresponding author.E-mail addresses:bagui@uwf.edu(S. Bagui),keerthi0589@gmail.com(K. Devulapalli),jcoffey@uwf.edu(J. Coffey).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100035Received 17 February 2019; Received in revised form 2 May 2020; Accepted 2 July 2020Available online 8 August 20202590-0056/©2020 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 7 (2020) 100035are assigned more work than others, the load is not balanced. This willlead to an increase in the overall run-time of a distributed algorithm sincethe distributed algorithm waits for the node taking the longest runtime(the node with the highest load) to complete.In this paper, we propose a heuristic load balancing strategy, that willbe particularly useful for sparse datasets, that distributes the load on allcluster nodes in a such way that the load on the nodes is more or less thesame for the FP-growth’s parallel implementation. We will refer to this asthe Heuristic Based Balanced Parallel FP (HBPFP) growth algorithm. Theheuristic behind this load balancing strategy is to identify what consti-tutes load or work in a given algorithm, and group items such that eachgroup gets a fair share of items for which more work has to be performedand items for which less work has to be performed, hence balancing theoverall load.HBPFP is compared with two established load balancing algorithms,Mahout’s PFP (MPFP) [7] and Balanced PFP (BPFP) [6], using a dataset that can be considered sparse, the transactional dataset from theFrequent Itemset Mining Dataset Repository [ 19]. Our proposed HBPFP outperforms both MPFP and BPFP in terms of run-time performance, loadbalancing, memory performance, and scalability, showing that it usescluster resources more efﬁciently.The rest of the paper is organized as follows: section two provides abackground on FP-Growth on MapReduce and load balancing; sectionthree discusses related work; section four provides the background forthe current work; sectionﬁve explains the heuristic based load balancinggrouping algorithm along with a working example; section six providesimplementation details; section seven describes the experimental setup;section eight presents the results and discussion; and ﬁnally, section nine presents the conclusion.2. FP-growth on MapReduce and loadbalancingHadoop’s MapReduce environment has two main phases, map andreduce. Data in read into the map phase, each mapper processes this dataand emits key-value pairs. These key-value pairs then become input forthe reduce phase. In the reduce phase, key-value pairs from differentmappers are combined to produce theﬁnal result. The FP-Growth algorithm has two phases –the tree construction phase and the growth phase. In the PFP [ 1] algorithm, the mapper gen- erates a transaction tree containing the frequent items for each trans-action and emits it as intermediate data for each group. In the reducer, foreach group, all the emitted transaction trees are combined to form theFP-tree of that group. The FP-tree construction is an O(n*m) operation,wherenis the number of transaction trees to be combined and mis the size of the transaction trees. The number of transaction trees generatedfor a group is equal to the number of transactions present in the databasethat contain at least one of the group items. The number of transactiontrees is reduced by using a combiner. The transaction trees with leastfrequent items will be larger in size than the transaction trees with mostfrequent items. So a group containing least frequent items will have tocombine larger transaction trees and hence will spend more time in thetree construction phase.In the growth phase, the conditional pattern base is constructed for anitem and mined to generate the frequent patterns for the item. Hence thetime spent in the growth phase is directly proportional to the size of theconditional pattern base of the item. Items that occur deep in the FP-Tree,that is, towards the leaves, have long preﬁx paths and hence will have bigger conditional pattern bases. More time will be spent on constructingand mining the conditional FP-trees for such items. For the items thatoccur towards the root of the FP-tree, the pre ﬁx paths are shorter and their conditional pattern bases are smaller. Less time is spent on miningthese small conditional pattern bases. Therefore, in the growth phase,more time is spent on less frequent items that occur deep in the FP-treeand less time is spent on more frequent items that occur towards theroot of the FP-tree. Overall the FP-Growth algorithm spends more time onless frequent items than on more frequent items. So, overall, the FP-Growth algorithm will have more of a problem in sparse databases,which will have more less frequent items.To distribute the FP growth algorithm across all reducer tasks, eachdivision must be assigned to a reduce task to construct the FP tree andmine it for frequent itemsets. Let I¼
(a1;a2;…;a m) be a set of items, and DB be the transaction database, denoted by DB¼(T
1;T2;…;T n), where each T
i⊆I( 1/C20i/C20n) is a transaction. Let the set of items, I, be dividedinto different disjoint non-empty subsets, I
1,I2,I3, etc. Each subset I iis assigned to a reducer to be processed. For each subset I
i, an FP tree is constructed from all the transactions in DB and the FP growth algorithmis used to obtain the frequent patterns for the items in the given subset.Load balancing occurs when the time taken for each subset I
i,in the FP tree construction and FP growth phases, are almost equivalent, that is,time taken for I
1/C25time taken for I 2/C25time taken for I 3/C25…time taken for I
i.3. Related workSeveral works have focused on the performance issues on the serialversion of the FP growth algorithm. These related works can be dividedinto two broad areas: (i) works that focused on distributing and paral-lelizing the FP growth algorithm on different execution platforms; and(ii) works that focused on tasks related to maintaining distributed sys-tems, that is, algorithms that focused on the overhead due to the distri-bution of tasks, task scheduling, effective resource utilization and tasksynchronization, etc.3.1. Works that focused on distributing and parallelizing the FP growthalgorithm on different execution platforms[9] presented a multi-threaded algorithm, Multiple Local FrequentPattern Tree (MLFPT), that runs on SMP processors with shared memory.In this algorithm, multiple local FP-trees are constructed, one by eachprocessor. These local FP-Trees are shared among all processors in thegrowth phase. The load on the processors is balanced using the averagenumber of occurrences that need to be traversed by each processor, andthis is considered the load factor. MLFPT scales up well for very largedatasets.[10] describes the FP growth algorithm underutilizing the modernprocessor’s architectural features. Hence, the FP tree was implemented asa cache-conscious data structure which improved performance by takingadvantage of hardware cache prefetching. It was implemented as amulti-threaded program where the threads were co-scheduled to maxi-mize cache reuse.[11] stated some parallelization techniques based on shared memorySMP architectures for data mining algorithms. Using these techniques,they parallelized the FP growth algorithm and scaled it to process data-sets of few hundred megabytes (MBs). Similarly, [ 12] used two tech- niques: (i) a cache-conscious FP-array, which is a data reorganization ofthe FP tree and, and (ii) lock-free FP-tree construction, for parallelizingthe FP growth algorithm on multi-core processors. These techniques aimto solve problems like poor data locality and insuf ﬁcient parallelism of data mining algorithms on multi-core processors. [ 13] implemented a multithreaded solution on the multi-core CPU and GPU. Both imple-mentations were based on FP-arrays [ 12]. [14] implemented a parallel FP-growth algorithm on a distributedmemory multiprocessor system using message passing. It is based on amaster-slave approach. In this solution, each processor is assigned apartition of the database and performs the FP-growth algorithm in par-allel on the local partition. The processors initially compute local fre-quency lists, from which the global frequency list is created anddistributed to all processors. Using the global frequency list, each pro-cessor builds local FP trees. [15] implemented an architecture-awaresolution, Distributed FP growth (DFP), for parallelizing the FP growthalgorithm on a message-passing cluster.S. Bagui et al. Array 7 (2020) 100035
23.2. Works that focused on tasks related to maintaining distributed systemsPFP [1] is a distributed and parallel implementation of the FP growthalgorithm on Hadoop’s MapReduce framework. The grouping strategyused in the PFP [1] algorithm is: compute- the size of each group ( n)b y dividing the total number of items in the frequency list by the number ofgroups. Starting from the most frequent item, nconsecutive items are grouped into one group. For each group, the group dependent trans-actional database is generated. Each group is assigned to a differentreducer. The grouping strategy does not take load balancing intoconsideration. Hence, the processing needed for different groups can bevaried and the load on the nodes performing FP growth on the groups canbe different. The overall running time of the PFP algorithm can beextended by the time taken by the reducer with the largest load. In thePFP [1] algorithm, the group dependent transactional databases gener-ated for each reducer key cannot be split. Hence the PFP [ 1]i sa non-distributive MapReduce program and cannot use tools like PerfectBalance for load balancing. The program implementation must take theresponsibility of load balancing.A load balancing strategy was introduced for PFP called Balanced PFP(BPFP) [6]. This algorithm performs load balancing based on a loadfactor which was measured in terms of the log of the location of the itemin the reverse sorted frequency list. The load factor was based on esti-mations for measuring the load of an item in the FP growth phase. The FPtree construction phase, which also contributes to the load of an item,was not part of the estimations.Improved Parallel FP growth (IPFP) algorithm [ 16] integrates a small ﬁle processing strategy into the PFP algorithm. IPFP works ef ﬁciently for datasets that have a very large number of small ﬁles, where eachﬁle size can be as small as 64 KB. Such datasets are not suitable for Hadoop (bothwith respect to HDFS and MapReduce), as they tend to have high memorycost, high I/O overhead, and low computing performance. IPFP tries toaddress these problems by merging the small ﬁles as a preparatory step before applying the PFP algorithm. The IPFP algorithm does not performany load balancing.PARMA [17] is a combination of parallelization and randomization. Itmines, in parallel, a set of small random samples and then ﬁlters and aggregates the collections of frequent itemsets or association rules ob-tained from each sample.[20] created a closed FIM algorithm basing the grouping method on agreedy strategy, such that all groups receive an equivalent number oftransactions.FiDoop-DP [21] balances the load on the reducer nodes by using avoronoi based partitioning technique. The partitioning technique almostcreates clusters of transactions and mines the clusters in parallel indifferent reducers. FiDoop-DP not only aims to balance the load of thereducers, but also avoids duplicate transactions in the intermediate data.But FiDoop-DP needs a pre-processing step. Their experimental results donot account for this preprocessing step and hence the overhead of thepre-processing step is not known.[22] introduced two algorithms that used the MapReduce frameworkto deal with two aspects of the challenges of Frequent Itemset Mining ofBig Data. They introduced Dist-Eclat to optimize for speed and BigFIMoptimized for a truly Big Data implementation using a hybrid algorithm.[23] proposed MapFIMþ, a two-phase approach to frequent itemsetmining in very large datasets beneﬁting both from a MapReduce-baseddistributed Apriori method and local in-memory Frequent Itemset Min-ing methods. In this approach,ﬁ
rst, MapReduce was used to generatefrequent itemsets, then an optimized local in-memory mining processwas used to generate all remaining frequent itemsets from eachpreﬁx-projected database on individual processing nodes.Our heuristic approach to balance groups, which seems to be naturaland simple for a parallel implementation of the FP growth algorithm,especially for large sparse datasets, has not been attempted or tested byany of the earlier works. In our work, every group has an equal share ofmore frequent items and less frequent items, and the algorithm of themappers and reducers is unaffected by the balancing strategy.And although there are some commercially available tools like OracleBig Data Appliance’s Perfect Balance [5] that perform load balancing in the reducer phase of the MapReduce programs, these tools are notapplicable to all MapReduce programs. For example, Perfect Balance canonly be used with distributive MapReduce programs, that is, programswhere the group of records associated with a reduce key can be split.Hence, for algorithms that do not meet this criterion, program imple-mentation must take the responsibility of load balancing.4. Background for current workThis section describes the two Parallel FP-Growth Algorithms, Ma-hout’s PFP [7] and Balanced PFP [6], that our HBPFP will be comparedwith.4.1. Mahout PFPMahout [7] is an open-source scalable machine learning librarywhich contains an implementation of the PFP algorithm [ 1] on the Hadoop platform, which we will refer to as MPFP. The MPFP algorithmuses two MapReduce jobs: theﬁrst MapReduce job generates the reversesorted frequency list of frequent items and the second job generatesgroup dependent transactional databases and executes the serial FPgrowth algorithm on all the group dependent transactional databases inparallel. Before running the second MapReduce job, the MPFP algorithmgroups the items of the frequency list into multiple groups. The numberof groups to be created is a user given constant.The grouping strategy used in MPFP computes the size of each group(n) by dividing the total number of items in the frequency list by thenumber of groups. Starting from the most frequent item, nconsecutive items are placed in one group. Following is an example of how the MPFPgrouping algorithm works based on the transactions shown in Table 1. Let the minimum support be 3 and the number of groups be 3.As theﬁrst step of FP-growth algorithm, the reverse sorted frequencylist is computed as shown inTable 2. The number of items in the fre- quency list is 6. The number of items per group is calculated as number ofitems in the frequency list divided by the number of groups. For thisexample it is 6/3, i.e. 2 items per group.In the MPFP growth algorithm, theﬁrst 2 items will be placed in group1, the next 2 items will be placed in group2 and the last 2 items willbe placed in group3, as shown inTable 3. The length of the preﬁx path is a measure of the size of the conditional pattern base of an item. If thelength of the preﬁx path is small, the conditional pattern base will also besmall and hence the time taken to mine the small conditional pattern basewill be less.Table 1Example transaction dataset.
Transaction-ID Transaction1 102, 97, 99, 100, 103, 105, 109, 1122 97, 98, 99, 102, 108, 109, 1113 98, 102, 104, 106, 1114 98, 99, 107, 115, 1125 97, 102, 99, 101, 108, 112, 109, 110
Table 2Frequency list.
Item-id Frequency102 499 4109 3112 397 398 3S. Bagui et al. Array 7 (2020) 100035
3Fig. 1shows the FP-Tree for group1, as constructed by the MPFPgrowth algorithm. The depth of the FP-Tree is 2. The length of item 102 ’s preﬁx path is 0. Hence there is no frequent pattern generation for item102. Item 99 has two paths in the tree, one with pre ﬁx path length of 1 and the other with preﬁx path length of 0. Hence, only one frequentpattern is generated for item 99.Fig. 2shows the FP-Tree for group2, as constructed by the MPFPgrowth algorithm. The depth of the FP-Tree is 4. The length of the pre ﬁx path for item 109 is 2. Item 112 has two paths in the tree, one with apreﬁx path length of 3 and the other with a preﬁx path length of 1. Hence, the size of the conditional pattern bases for items 109 and 112 is greaterthan the size of the conditional pattern base for group1 ’s items 102 and 99. Hence, group2 will take more time than group1 to generate thefrequent items.Fig. 3shows the FP-Tree for group3, as constructed by the MPFPgrowth algorithm. The depth of the FP-Tree is 5. Item 97 has two paths inthe tree, one with preﬁx path length of 4 and the other with preﬁx path length of 3. Item 98 has three paths in the tree with pre ﬁx path lengths of 4, 2 and 1 respectively. Hence, the conditional pattern bases for items 97and 98 are larger in size than the conditional pattern bases of items ingroup1 and group2. Therefore, group3 will take the longest time togenerate the frequent patterns.As stated by Ref. [6], the local FP growth algorithm running in thereducer phase of the second MapReduce job consumes 50% of the totalmining time. For the group with the less frequent items, the local FPgrowth algorithm will spend more time on tree construction and alsomore time in the growth phase. The time spent by the groups are notequivalent and hence they are not balanced. Though the local FP growthalgorithm takes less time for some groups, the overall running time of theMPFP algorithm is extended by the local FP growth algorithm that takesthe longest time to execute. This difference in running time is dueto the load not being balanced among the reducers. Hence the local FP growthinﬂuences the overall performance of the MPFP algorithm. Any perfor-mance savings at this stage can result in signi ﬁcant performance gains. The time taken by the local FP growth in the reduce phase can beinﬂuenced by the grouping strategy.4.2. Balanced PFPBalanced PFP (BPFP) [6] attempts to balance the load among thereducers in the second MapReduce job of the PFP algorithm by using aload factor that measures the log of the location of the item in the reversesorted frequency list. BPFP uses a priority queue for grouping items. Thispriority queue consumes more time for grouping the items.5. Heuristic based load balancingHBPFP aims to balance the load on all reducers performing the localFP growth algorithm based on a heuristic that accounts for all work donefor every item. When the reducers’load is balanced, it will complete thelocal FP growth algorithm in almost an equivalent amount of time,therefore reducing the overall run time of the HBPFP algorithm.Items with high frequencies occur closer to the root of an FP tree.Hence, with respect to the tree construction phase, more time is spent onless frequent items and less time is spent on more frequent items. For theitems that occur towards the root of the FP-tree, the pre ﬁx paths are shorter and their conditional pattern bases are smaller. Sparse datasetswill typically have more less frequent items, so grouping them with themore frequent items makes this algorithm especially suitable for sparsedatasets.Our heuristic approach to balance the groups, is to combine the lessfrequent items with the more frequent items. The balanced groups areformed from the reverse sorted frequency list as follows:1. The top
‘n’most frequent items are each placed in ‘n’groups. 2. For the remaining items in the frequency list, the items are placed inthe groups in a round robin fashion, starting from the least frequentitem, that is, from the end of the frequency list.Hence, less frequent items and more frequent items appear in theTable 3Grouping by MPFP.
Group Members1 102, 992 109, 1123 97, 98
Fig. 1.MPFP: Group1 FP-Tree.
Fig. 2.MPFP: Group2 FP-Tree.
Fig. 3.MPFP: Group3 FP-Tree.S. Bagui et al. Array 7 (2020) 100035
4same group. Every group should have an almost equal share of mostfrequent items and least frequent items. We will demonstrate our loadbalancing strategy using the transactions presented in Table 1and the frequency list presented inTable 2.With the heuristic based balancing strategy, the groups will be formedas shown inTable 4.Figs. 4–6shows the FP-Trees for the groups formedby heuristic based grouping strategy. As can be seen, the FP-Trees for allthe groups have almost the same depth.For group1 (Fig. 4), the depth of the FP-Tree is 5. The length of item102’s preﬁx path is 0. Hence there is no frequent pattern generation foritem 102. Item 98 has three paths in the tree, with pre ﬁx path lengths of 4, 2 and 1 respectively. Though more work has to be done for miningfrequent patterns for item 98, there are no frequent patterns to begenerated for item 102. Hence the longer time spent for item 98 isbalanced by the shorter time spent for item 102.For group2 (Fig. 5), the depth of the FP-Tree is 5. Item 99 has twopaths in the tree, with preﬁx path lengths of 1 and 0. Item 97 has twopaths in the tree, with preﬁx path lengths of 4 and 3. The conditionalpattern base of item 97 is larger than that of item 99. Hence, the longertime spent for item 97 is balanced by the shorter time spent for item 99.For group3 (Fig. 6), the depth of the FP-Tree is 4. The length of thepreﬁx path for item 109 is 2. Item 112 has two paths in the tree, withpreﬁx path lengths of 4 and 1. The conditional pattern base of item 112 islarger than that of item 109. Hence, the longer time spent for item 112 isbalanced by the shorter time spent for item 109. Hence, all groups takealmost the same amount of time to generate frequent patterns. As can beseen from this example, our heuristic based grouping strategy groups theitems into balanced groups such that the time taken to generate thefrequent patterns is almost equivalent for all groups. Hence the load isbalanced across the groups.6. Implementation details of the Heuristic based balancedparallel FP growth algorithm6.1. Grouping algorithm used for the Heuristic based balanced parallel FPgrowth algorithmThe grouping algorithm is presented in Algorithm 1. It uses a First InFirst Out (FIFO) queue of groups. A hash map is maintained which maps agroup id to its member items. A reverse hash map is also maintained,which maps the items to their group ids. The hash map and reverse hashmap help to efﬁciently look up an item’s group id or the member items of a group. Theﬁrst while loop (steps 3 to 9) performs step 1 mentioned inSection4. It createsngroups and adds the topnitems of the frequency list to these groups. The second while loop (steps 11 to 18) performs step 2mentioned in Section4, that is, starting from the end of the frequency list,it adds the remaining items to the groups in FIFO order.Since the entire fList has to be scanned for grouping, the complexity ofthe grouping algorithm is O(n), that is, it is proportional to fList size. TheBPFP algorithm, however, uses a priority queue for grouping and alsotraverses the entire fList. The complexity of their grouping algorithm isO(n*log(n)). By comparison of complexities, it can be seen that HBPFP ’s grouping algorithm is more efﬁcient than the grouping algorithm ofBPFP. The results section clearly demonstrates that the overall runningtime of HBPFP is less than that of BPFP. Also, the groups produced by
HBPFP are more balanced than the groups produced by BPFP. Both BPFPand HBPFP store the groups in a map and thus have a complexity of O(1)for group lookup.In MPFP, the fList items are grouped by using hash partitioning. Forevery item, the group id is determined by dividing the item position inTable 4Grouping by heuristic balancing strategy.
Group Members1 102, 982 99, 973 109, 112
Fig. 4.HBPFP: Group1 FP-Tree.
Fig. 5.HBPFP: Group2 FP-Tree.
Fig. 6.HBPFP: Group3 FP-Tree.S. Bagui et al. Array 7 (2020) 100035
5the fList by the number of items per group. Hence, complexity ofgrouping in MPFP is O(1). Though MPFP does not spend much time ingrouping, it creates unbalanced groups. This penalizes some of the reducetasks by slowing down the entire MapReduce job.
Algorithm 1Heuristic Based Grouping.
6.2. FP-growth algorithm implementation for the Heuristic Based BalancedParallel FP growth algorithmThe FP-Growth algorithm is implemented as a MapReduce job. It issimilar to the Parallel FP Growth step of the MPFP algorithm. The fListand the map containing the groups are copied to all map tasks and reducetasks using the distributed cache feature of the MapReduce framework.Every map and reduce task reads them from the distributed cache. Thegroup map is used in the map task to determine the group id for a givenitem, and is used in the reduce task to determine the items that belong toa given group id.The map tasks parse the transactions. They eliminate the transactionitems not found in the fList and sort the remaining items in the sameorder as the fList. Starting from the most frequent transaction item, themap tasks determine the group id for each item and emit the group id asthe key, and the transaction is emitted in the form of a transaction tree asthe value. The map tasks emit a transaction only once for a group id.Hence all transactions that belong to the items of a group are sent to thesame reducer. The reducer task combines all the transaction trees for agroup and constructs the FP-Tree for the group items. This FP-Tree iscomplete with respect to the group items, hence it can only be mined forgenerating frequent itemsets of the group items. The FP-Tree is minedrecursively and the frequent item sets for the group items are generated.The generated frequent itemsets are emitted as the value, with the groupid as the key.7. Experimental setupHBPFP was implemented on an in-house Hadoop cluster that consistsof three management nodes and six data nodes. The cluster runs Hadoop2.6.0-CDH5.5.1 with JDK 1.7. It uses the MR2 (YARN) MapReduceframework where the six data nodes have node managers, and hence runthe map and reduce tasks. The data nodes are identical in con ﬁguration, where each node has two Xeon E5-2650 processors and 128 GB RAM.For this work, a transactional dataset based on spidered collection ofweb html documents from the Frequent Itemset Mining Dataset Re-pository [19] was used. This dataset, which can be considered a sparsedataset, was built from about 1.7 million web documents. The html tagsand the most common words (stop words) were ﬁltered out of the web documents. Each transaction represents a web document and the items inthe transactions represent the set of distinct terms that appear in thatdocument. The resulting dataset was 1.48 GB in size, containing 1,692,082 transactions with 5,267,656 distinct items. The maximal length of atransaction was 71,472.The three algorithms, MPFP [8], BPFP [6] and our HBPFP, were executed on the webdocs dataset [18] for different minimum support values. They were compared for run-time performance, their ability tobalance the load across the cluster as well as memory performance. Therun-time performance is measured in terms of overall time taken togenerate the frequent itemsets. The ability to balance the load isexpressed by the standard deviation of the running time taken to processeach group dependent transactional database. The memory performanceis expressed by the data size that can be processed for a given memoryspace, and also the time taken for a given minimum support thresholdand memory space.8. Results and discussionResults are presented, for the three algorithms, MPFP, BPFP, andHBPFP, in terms of run-time performance, load balancing, memory per-formance and scalability.8.1. Run-time performanceThe three algorithms were run for different minimum supportthresholds, from 7.5% to 12%, in increments of 0.5%. The reducememory size was set at 5,354 MB for all three algorithms. The groupingalgorithm was dependent on the size of the fList, shown in Table 5, not the size of the transaction database.FromTable 5, it can be observed that, as the minimum supportthreshold decreases, the size of the fList increases. When the fList in-creases, more items are considered to be frequent. These need to begrouped and included in the transaction trees emitted as the intermediatevalues. Hence, a lower minimum support threshold results in an increasein the intermediate data size, which in turn increases the load on thereducers. The fList size, however, is negligible when compared to the sizeof the transaction database and the intermediate data size which has tobe processed by the reducer phase.Fig. 7shows the average running times (in minutes) for HBPFP, MPFPand BPFP for the various minimum support values. The running timeincludes the time taken for creating the fList, grouping the items, con-structing the group-speciﬁc FP-Trees and mining them. To get robustresults for the average running times, each run was performed 10 timesand averaged. The running time for the algorithms decrease as theminimum support increases. This is because, as the minimum supportincreases, the size of the frequency list, that is, the number of items forwhich the frequent patterns have to be mined, decreases. For all thedifferent minimum support thresholds, HBPFP runs the fastest amongstthe three PFP algorithms.
Table 5fList Size for various Minimum Support Threasholds.
Minimum Support Threshold(%) fList Size7.5 3968 3648.5 3319 3079.5 28310 26210.5 24511 22111.5 20412 189S. Bagui et al. Array 7 (2020) 100035
6As can be seen fromFig. 7, that we do not have data for MPFP forminimum support of 7.5%, 8% and 8.5%, and for BPFP for minimumsupport of 7.5%. Both MPFP and BPFP run out of memory when miningfrequent itemsets with lower minimum support values because of theincreased load on the reducers, which was not well-balanced. Some of thereducers needed to mine larger FP-Trees. The HBPFP algorithm runssuccessfully for all values of minimum support from 7.5% and above.This indicates that our HBPFP algorithm balances the load better thanMPFP and BPFP. With HBPFP, none of its reducers are burdened withvery large FP-Trees. The average percent improvement in running time ofHBPFP over MPFP is 23.82%. On the average, BPFP runs faster thanMPFP, but slower than HBPFP. HBPFP is 8.59% faster than BPFP.8.2. Load balancingTo illustrate the load balancing performed by the three algorithms,the time taken for generating frequent itemsets, at a minimum support of10%, for individual groups, 1 to 79 groups, was recorded, as shown inFig. 8. The minimum support threshold was arbritarily selected fromFig. 7, trying to strike a balance between best running time and lowestminimum support threshold.Fig. 8shows that MPFP is the most unbalanced, that is, the variationin running times for the different groups is high for MPFP. For BPFP, thevariations in running times of different groups is less than that of MPFP.This is because BPFP does a better job of balancing the load of the groupsthan MPFP. When compared with the BPFP and MPFP, HBPFP has theleast variation in running times for the different groups, demonstrating amore balanced load than the other two algorithms, no matter what groupsize is used.Fig. 9presents the standard deviation of the running times of allgroups (averaged) for the three algorithms at different minimum supportvalues. Again, compared to the BPFP and MPFP, for all minimum supportvalues, HBPFP has the lowest standard deviation. Hence HBPFP distrib-utes the load almost equivalently among all the groups, irrespective ofthe size of the group.8.3. Memory performanceFP-growth is a memory intensive algorithm. A deep FP-Tree willrequire more memory, and mining a deep FP-Tree involves manyrecursive calls in the growth phase. This can generate a large number ofpattern trees. The load on the reducers determines the memory re-quirements of the reducers. When the load on the reducers is notbalanced, some reducers will have to construct and mine very large FP-Trees, while other reducers will have to only construct and mine smallFP-trees. Reducers handling smaller FP-trees may complete the FP-growth mining successfully, while reducers handling larger FP-Treesmay run out of memory and not be able to complete the job. AFig. 7.Comparison of average running times of MPFP, BPFP andHBPFP algorithms.
Fig. 8.Group Running Times for minimum support of 10%.Fig. 9.Standard Deviation of Group Running Times (all groups averaged).S. Bagui et al. Array 7 (2020) 100035
7MapReduce job fails when one of its reducers fails. Thus the load on thereducers needs to be balanced.To analyze memory performance, the three algorithms, MPFP, BPFPand HBPFP, were run with different reducer memory sizes at variousminimum support thresholds. In Hadoop, the minimum memory for areduce task is 1 GB (1024 MB), and the maximum size that can be allo-cated to a reduce task in our in-house Hadoop cluster is 8 GB (8192 MB).Hence we ran the three algorithms, MPFP, BPFP and HBPFP, varing thereduce task memory size from 8 GB to 1 GB in decrements of 1 GB, fordifferent minimum support thresholds, 6.5% –12% in increments of 0.5%. None of the algorithms ran for reduce task memory of 1 GB with ourminimum support threshholds of up to 12%. Only HBPFP ran with reducetask memory of 2 GB for a minimum support threshold of 12%, but sincethe other two algorithms, MPFP and BPFP, did not run on a reduce taskmemory size of 2 GB either, we did not present in these results in Table 6. Table 6presents the running times for the algorithms (in minutes) for thevarious reduce memory sizes, 8 GB–3 GB, in decrements on 1 GB, at thevarious minimum support threshholds.FromTable 6, we can note: (i) As the minimum support threshold isincreased, the running time goes down for all algorithms; (ii) As thereduce memory size is increased, the running time goes up for all threealgorithms, if the minimum support threshold is kept the same; in orderto get faster running times, as the memory size is reduced, the minimumsupport needs to be increased too; (iii) HBPFP outperforms both BPFPand MPFP, both in terms of being able to run with lower reduce memorysize and minimum support threshold. That is, of the three algorithms,HBPFP ran at the lowest minimum support threshold for each taskmemory size. BPFP performed the second best.For example, for the reduce memory size of 8 GB, both BPFP andHBPFP successfully complete the jobs for the minimum support thresholdof 6.5%, and HBPFP runs faster than BPFP for all minimum supportthresholds except the 10% and 10.5% thresholds. MPFP cannot handlelarge datasets for minimum support thresholds of 6.5%, 7% and 7.5%,even at the 7 or 8 GB reduce task memory size. At the reduce taskmemory size of 7 GB, BPFP could not even handle large datasets ofminimum support thresholds of 7%.At the low 3 GB reduce task memory size, both HBPFP and MPFP ransuccessfully at a minimum support threshold of 11%. However, HBPFPran faster than both BPFP and MPFP. BPFP could handle large datasetsfor minimum support thresholds of 11% and 11.5%.So, fromTable 6it is also clear that the reduce task memorydetermines the data size that can be processed by the FP-growth algo-rithm in the reduce phase. As the reduce task memory increases, largerdata sizes can be processed. These experiments show that HBPFP ’s load balancing technique distributes the load equivalently, such that indi-vidual groups can be processed with less memory. Hence, HBPFP not onlyimproves the run time performance of the PFP algorithm, but also thespace requirements.8.4. ScalabilityTo demonstrate scalability, the three PFP based algorithms wereexecuted on the AWS EMR cluster, after increasing the data size to 100GB. The EC2 instance type m3.xlarge was chosen as it was best suited forgeneral purpose applications. The Hadoop version used was the same asthe Hadoop version of the home-grown cluster, i.e. Hadoop-2.6.0. Aminimum support of 9% was chosen since this was the lowest minimumsupport that would run for all the three algorithms for this data sizewithout throwing a heap size exception. The number of groups was keptat 79, that is, the same as the number of groups used while running thealgorithms on the home-grown cluster. The number of reducers was set tobe the same as the number of groups, that is, 79. 22 mappers were used.As per the above speciﬁcations, the three algorithms, MPFP, BPFP andHBPFP, were run on various number of nodes: 6, 8, 10, 12, 14 and 16.Fig. 10presents the running times (in minutes) for the different numberof nodes in the cluster. It can be noted that HBPFP consistently had thelowest running times, and of course the running times reduced as thenumber of nodes increased. MPFP consistenly had the highest runningtimes of all three algorithms.8. ConclusionsIn a distributed and parallel environment like Hadoop ’s MapReduce framework, balancing the load on all the cluster nodes plays an importantrole on the overall performance of the algorithm. Our results show thatour heuristic based strategy, which has a lower order of complexity,balances the load among reducers better than the mahout implementa-tion of PFP and BPFP’s load factor based balancing strategy, especiallyfor large sparse datasets. The total running time of our algorithm is foundto be less than that of both MPFP and BPFP. In addition to scaling wellwith cluster size, HBPFP utilizes cluster resources like task memory moreefﬁciently. The HBPFP algorithm processes the largest dataset size for aTable 6Average Running Times for various Reduce Task Memory Size at different Minimum Support Threshholds.
Min. Support (%) 6.5 7 7.5 8 8.5 9 9.5 10 10.5 11 11.5 128G BMPFP–– – 89.85 73.35 43.53 35.54 29.87 25.40 14.50 12.70 10.70 BPFP 145.90 80.30 52.35 43.22 36.00 28.97 23.79 18.83 16.36 14.14 12.50 11.50HBPFP 102.00 72.94 49.84 42.01 34.39 28.17 23.36 19.48 17.01 12.85 10.80 9.687G BMPFP–– – 121.12 87.75 37.84 30.97 25.79 21.91 13.26 11.67 9.80 BPFP––50.73 37.66 31.18 26.06 21.16 17.12 14.98 12.69 11.68 10.84 HBPFP–79.36 46.69 36.80 30.69 25.15 20.56 17.58 15.29 11.86 10.27 9.05 6G BMPFP–– – –– 38.74 31.26 26.22 22.17 13.46 11.55 10.05 BPFP–– – 39.60 31.86 26.76 21.59 16.86 15.04 12.76 11.61 10.69 HBPFP––57.65 40.64 30.54 24.99 20.86 17.40 15.32 11.93 10.12 9.05 5G BMPFP–– – –– 69.13 38.61 27.46 22.73 13.15 11.74 9.98 BPFP– ––– ––– 17.26 14.94 12.91 11.84 10.97 HBPFP–– – – 35.95 25.26 21.01 18.00 15.17 11.93 10.19 9.12 4G BMPFP– ––– ––––– 10.00 8.98 7.91 BPFP– ––– ––– 19.14 11.86 10.47 9.54 8.85 HBPFP– ––– –– 16.59 13.97 11.86 9.37 8.53 7.75 3G BMPFP– ––– ––––– 11.34 9.04 7.52 BPFP– ––– ––––––– 8.92 HBPFP– ––– ––––– 9.73 8.52 7.39S. Bagui et al. Array 7 (2020) 100035
8given memory space and runs fastest for any given dataset size andmemory space. Theﬁnal recommendation would be to use HBPFP forsparse datasets.Credit author statementDr. Sikha Bagui and Keerthi Devulapalli conceptualized the paper.Keerthi did most of the programming and both Dr. Bagui and Keerthiwere involved in the writing of the paper. Dr. Coffey was involved in theediting of the paper.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgementsThis work has been partially supported by the Askew Institute at TheUniversity of West Florida.References
[1]Li H, Wang Y, Zhang D, Zhang M, Chang EY. Pfp: parallel fp-growth for queryrecommendation. In: Proceedings of the 2008 ACM conference on Recommendersystems. ACM; 2008. p. 107–14.[2]Agrawal R, Srikant R.“Fast algorithms for mining association rules ”, InProc. 20th int. conf. very large data bases. VLDB 1994 Sep 12;1215:487 –99. [3]Han J, Pei J, Yin Y. Mining frequent patterns without candidate generation. In: ACMSigmod Record, vol. 29; 2000. p. 1 –12. 2. [4] Apache hadoop.http://hadoop.apache.org/. [5] Perfect balance.https://docs.oracle.com/cd/E41604_01/doc.22/e41241/balance.htm#BIGUG279.[6]Zhou L, Zhong Z, Chang J, Li J, Huang J, Feng S. “Balanced parallel fp-growth with mapreduce,”in information computing and telecommunications (YC-ICT), 2010IEEE youth conference on. IEEE; 2010. p. 243 –6. [7] Apache mahout.http://mahout.apache.org. [8] Mahout PFP.http://grepcode.com/ﬁle/repo1.maven.org/maven2/org.apache.mah out/mahout-core/0.9/org/apache/mahout/fpm/pfpgrowth/package-info.java?av¼f.[9]Zaïane OR, El-Hajj M, Lu P. Fast parallel association rule mining without candidacygeneration. In: Data mining, ICDM 2001, proceedings IEEE internationalconference; 2001. p. 665–8. [10]Ghoting A, Buehrer G, Parthasarathy S, Kim D, Nguyen A, Chen YK, Dubey P.Cache-conscious frequent pattern mining on a modern processor. In: Proceedings ofthe 31st international conference on Very large data bases; 2005. p. 577 –88. [11]Jin R, Yang G, Agrawal G. Shared memory parallelization of data miningalgorithms: techniques, programming interface, and performance. IEEE TransKnowl Data Eng 2005;17(1):71 –89. [12]Liu L, Li E, Zhang Y, Tang Z. Optimization of frequent itemset mining on multiple-core processor. In: Proceedings of the 33rd international conference on Very largedata bases; 2007. p. 1275–85. [13]Arour K, Belkahla A.“Frequent pattern-growth algorithm on multicore CPU andGPU processors”. CIT. J Comput Inf Technol 2014;22(3):159 –69. [14]Moonesinghe HDK, Chung MJ, Tan PN. Fast parallel mining of frequent itemsets.Michigan State University; 2006 . [15]Buehrer G, Parthasarathy S, Tatikonda S, Kurc T, Saltz J. Toward terabyte patternmining: an architecture-conscious solution. In: Proceedings of the 12th ACMSIGPLAN symposium on Principles and practice of parallel programming; 2007.p. 2–12.[16]Xia DA, Rong ZH, Zhou YA, Li Y, Shen Y, Zhang Z. A novel parallel algorithm forfrequent itemsets mining in massive small ﬁles datasets. ICIC Express Lett B: Appl 2014;5(2):459–66.[17]Riondato M, DeBrabant JA, Fonseca R, Upfal E. Parma: a parallel randomizedalgorithm for approximate association rules mining in mapreduce. In: Proceedingsof the 21st ACM international conference on Information and knowledgemanagement. ACM; 2012. p. 85 –94. [18]Vavilapalli VK, Murthy AC, Douglas C, Agarwal S, Konar M, Evans R, Graves T,Lowe J, Shah H, Seth S, Saha B. Apache hadoop yarn: yet another resourcenegotiator. In: Proceedings of the 4th annual symposium on cloud computing; 2013Oct 1. p. 5.[19] FIMI.http://ﬁmi.cs.helsinki.ﬁ/data. [20]Chen GP, Yang YB, Zhang Y. “Mapreduce-based balanced mining for closed frequent Itemset”in Web Services (ICWS). In: 2012 IEEE 19th internationalconference on. IEEE; 2012, June. p. 652 –3. [21]Xun Y, Zhang J, Qin X, Zhao X. FiDoop-DP: data partitioning in frequent itemsetmining on hadoop clusters. IEEE Trans Parallel Distr Syst 2017;28(1):101 –14. [22]Moens S, Aksehirli E, Goethals B. Frequent itemset mining for Big data. Proc BigData 2013;111–118.[23] Doung, K-C., Bamba, M., Giacometti, A., Li, D., Soulet, A., &Vrain, C.“MapFIMþ: memory aware parallelized frequent itemset mining in very large datasets, ” Transactions on large-scale data- and knowledge-centered systems XXXiX. Lecturenotes in computer science, vol. 11310, Springer, Berlin, Heidelberg.Fig. 10.Average Running Time (in minutes) Vs Number of cluster nodes.S. Bagui et al. Array 7 (2020) 100035
9