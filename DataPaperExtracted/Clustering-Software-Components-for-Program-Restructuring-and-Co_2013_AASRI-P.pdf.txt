 AASRI Procedia   4  ( 2013 )  319 – 328 
2212-6716 © 2013 The Authors. Published by Elsevier B.V.
Selection and/or peer review under responsibility of American Applied Science Research Institute
doi: 10.1016/j.aasri.2013.10.047 ScienceDirect
* 
E
 Cl
 
Abst
Com
sever
impo
toget
instig
new 
sets 
docu
comp
dyna
cohe
effici
softw
 
© 20
Sele
 
Keyw
1. In
C
also 
Corresponding a
E-mail address:  ralusterin g
Comp o
Chin
      Departmen t
                        
tract 
mponent based s
ral researchers 
ortant problem s
ther thus ensur i
gate a new a g e
similarity func t
or software co m
uments or co m
ponent or doc u
amically as co m
sive pattern gr o
ient in terms o f
ware compone n
013. Publishe d
ection and/or p
words :  Clusterin g
ntroduction 
Clustering is o
from the per
author. Tel.: +97 0
adhakrishna_v@ v2013 AA S
g Softw a
onent Re u
ntakindi Sr i
Associate Profe s
t of Information T
Professor of Co m
oftware develo
and also fro m
s aimed by rese a
ing reduced ti m
eneralized a ppr
tion called hyb r
mponents. We 
mponents or pa t
ument clusteri n
mpared to other 
oups or docum e
f processing w i
nt clustering.  
d by Elsevier 
peer review u n
g, hybrid XOR;fr e
one of the top i
rspective of t h
00684242 
vnrvjiet.in  SRI Confer e
are Com p
use Usi n
inivas,Va n
ssor of CSE, Kaka
Technology, VN R
mputer Science a n
pment has gai n
m industry pers p
archers. Cluste r
me complexity a
roach for clust e
rid XOR funct i
construct a m a
tterns by appl y
ng which has t
clustering alg o
ents. The appr o
ith reduced se a
B.V. 
nder responsi b
equent itemsets; m
ics which ha v
he software i nence on Int e
ponents 
ng Hybri
ngipuram R
atiya Institute of T
R Vignana Jyothi I
nd Engineering, S
ned a lot of pra c
pective. Findi n
ring reduces th
as it reduces th e
ering a given s e
ion for the pur p
atrix called si m
ying hybrid X
the input as si m
orithms that pre
oach can be jus t
arch space and c
bility of Ame r
mining;classifica t
ve achieved a 
ndustry. The selligent Sys t
for Pro g
id XOR 
Radhakrish n
Technology and S
Institute of Engi n
SR Engineering C
ctical importan c
ng components 
e search space 
e search time f o
et of document
pose of finding 
milarity matrix 
XOR function.
milarity matri x
edefine the co u
tified as it carr i
can be also be 
rican Applied 
tion,components
lot of practic a
significance fotems and C o
gram Re s
Similar i
na, Dr.C. V
Science, Warang a
neering and Tech n
College,Waranga
ce in the field o
for efficient s
of components
or component r
s or text files o
degree of simil
of the or der n-
We define a n
x and output b
unt of cluste rs. T
es out very si m
used in gener a
Science Res e
al importanc e
for clustering ontrol 
structur i
ity Func
V.Guru Ra o
al, INDIA 
nolog, Hyderaba
l,INDIA 
of software eng i
software reuse 
 by grouping s i
retrieval. In thi s
or components 
arity between t
-1 by n for a g
nd design the a
eing set of cl u
The output is a
mple computati o
al for documen t
earch Institute 
e from the res
approach co ming and 
ction 
o 
d,INDIA 
    
ineering from 
is one of the 
imilar entities 
s research, we 
by defining a 
two document 
given set of n 
algorithm for 
usters formed 
a set of highly 
onal logic and 
t clustering or 
earchers and 
mes from the 
Available online at www.sciencedirect.com
© 2013 The Authors. Published by Elsevier B.V.
Selection and/or peer review under responsibility of American Applied Science Research InstituteOpen access under CC BY -NC-ND  license.
Open access under CC BY -NC-ND  license.320   Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
need of decision making such as classification, prediction , component search and retrieval and is thus widely used in many practical domains such as text classification , bioinformatics, medicine , image processing. We can define Clustering as the process of grouping similar set of patterns together [11]. The input to clustering algorithm may be any set of entities or patterns or text files or software components. The output of clustering algorithm will be a partition of cohesive groups. The abstract representation of clustering process is shown in figure 1 below. 
 Figure 1:  Abstract view of clustering process 
The representations or descriptions of clusters so form ed shall be used in decision making such as which software component or pattern need to be selected. One important feature of clustering is that all the patterns within a cluster share common or same properties in some sense and patterns in different clusters are dissimilar in corresponding sense. In view of software engineering, all the components within the same cluster have high cohesion and low coupling. Software component clusters can be treated as highl y cohesive groups with low coupling which is the desired feature. One disadvantage of existing data cluste ring methods is that they do not adequately address the problem of processing large datasets with a limited  amount of resources. Using these limitations as our motivation, so if we can try to reduce the dataset for training process it can help in reducing the cost of training which in turn improves efficiency of clustering. If done so, clustering takes less amount of space and hence forms a compact storage of patterns. Clustering is not any one specific algorithm that we can stick firm to, but it must be viewed as the general task to be solved. Clustering algorithms may unsupervised or supervised [11 ]. In unsupervised clustering the partitions are viewed as the unlabelled patterns or components. Superv ised clustering algorithms label the patterns which can be used to classify the components for decision making.  Hence the partitions obtained by clustering process may be labeled or unlabeled.  A new method called Maximum Capturing is propos ed for document clustering [3].Maximum Capturing includes two procedures:  1. constructing document clus ters and 2. Assigning cluster topics. The search complexity can be reduced by using the algorithm [10] wh ere ever necessary as part of component retrieval.  2. Taxonomy The problem of finding frequent itemsets gets bi rth from [8] which uses frequent itemsets to find association rules of items in large transactional databases. In  [1] clustering a given set of text documents from neighbour set is proposed. In [2] the authors propose a method for discovering maximum length frequent item sets. In [6], the classification of text files or documents is done by considering Gaussian membership function and making use of it to obtain clusters by finding word patterns. Each cluster is identified by its word pattern calculated using fuzzy based Gaussian member ship function once clusters are formed.  In this paper the idea is to first obtain frequent item sets for each document using existing association rule mining algorithms either by horizontal or vertical approach. Once we find frequent itemsets in each document 
321  Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
then we form a Boolean matrix with rows indicating documents and columns indicating unique frequent item sets from each document.  This is followed by the computation of a binary feature vector for each document pair, represented as a 2D array or 2D matrix by rede fining the XOR function as hybrid XOR logic with slight modification in the function introducing high impedance variable as Z. The idea of maximum capturing is taken as the base framework for clustering. 3. Proposed Work To design a clustering algorithm we must first design  the similarity function which is the heart of any clustering algorithm. We define a generalized similar ity function called Hybrid XOR function which may be used to compute similarity feature between any pair of entities which may be software components or software patterns or documents. The documents may be text files to be classified or software product documents of various phases in software life cycle. We define the similarity function S as a function of any two entities A and B which is a tri state function as shown below in the truth table 1. 
Table 1.  Truth Table of hybrid XOR Similarity Function 
A B S(A,B) 
0 0 Z  
0 1 1 
1 0 1 
1 1 0 
 Hypothesis-1:  If a frequent item set exists in the docu ment, then the cell value of the matrix corresponding to  D [d
i,wk ] is made 1 else the corresponding cell value of the matrix is made as zero. The algorithm for document clustering has its input as documents with frequent item sets and output as set of clusters formed dynamically. The approach followed is a tabular approach. Similarly the algorithm for component clustering has its input as software componen ts with properties predefined and the output is a set of highly cohesive components with low coupling feature. 3.1 Algorithm for Clustering  // may be used for software component clustering or do cument clustering or pattern clustering in general Document_Clustering (Document set, frequent item sets)   Begin of Algorithm Step1:For each document D do  Begin   Step1.    Remove stop words and stemming words from each document. Step2.    Find unique words in each document and count of the same. Step3.    Find frequent itemsets of each document End for  Step 2:  Form a word set W consisting of each word in frequent item sets of each document. Step 3: Form Dependency Boolean Matrix with ea ch row and column corresponding to each Document and each word respectivelyFor each document in document set do   Begin   For each word in word set do  322   Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
Begin  If (word W
k in Word set W is in document D i) Begin  Set D[D
i, Wk] = 1 Else  Set D[D
i, Wk] = 0 End if  End for  End for  Step 4 : Find the Feature vector similarity matrix by  evaluating similarity value for each document pair applying Hybrid XOR Function defined in table 1 to obtain the matrix with feature vectors for each document pair. Step 5: Replace the corresponding cells of matrix by c ount of number of zeroes in tri state feature vector. Step 6:  At each step, find the cell with maximum va lue and document pairs containing this value in the matrix.Group such document pairs to form clusters. Al so if document pair (I, J) is in one cluster and document pair  (J, K) is in another cluster, form a new cluster containing (I, J, K) as its elements.  Step 7: Repeat Step6 until no documents exist or we reach the stage of first minimum value leaving zero entry. Step 8:  Output the set of clusters obtained.  Step 9:  label the clusters by considering candidate entries. End of algorithm 3.2 Case Study showing process of document clustering  Consider the document sets with the frequent item sets obtained after mining using any of the existing association rule mining algorithms as shown below. Here we use can also use association rule mining algorithm with multiple support and confidence thresholds. We considered a set of random of 20 documents as the training set.  
Table 2.  Documents and Corresponding Frequent item sets 
 DOCUMENTS    FREQUENT ITEMSETS 
DOCUMENT 1 { ENCRYPT ,   NEURAL NETWORKS ,   CLUSTER}  
DOCUMENT 2  {SVM ,  MINING,     CLUSTER}  
DOCUMENT 3  { SVM, NEURAL NETWORKS ,  MINING ,    CLUSTER}  
DOCUMENT 4 {ENCRYPT ,   NEURAL NETWORKS ,  MINING,     CLUSTER}  
DOCUMENT 5  {SVM ,  CLUSTER}  
DOCUMENT 6  {ENCRYPT ,      NEURAL NETWORKS ,  MINING}  
DOCUMENT 7  {ENCRYPT ,  SVM, NEURAL NETWORKS}  
DOCUMENT 8  {SVM, NEURAL NETWORKS} 
DOCUMENT 9  {NEURAL NETWORKS ,  MINING,     CLUSTER}  323  Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
 We now construct a Boolean matrix with rows indicating each document and column corresponding to each unique frequent item from set of frequent item sets of all documents sets respectively. 
Table 3.  Boolean matrix Representation of Table.2 
 ENCRYPT NEURAL NETWORKS  CLUSTER SVM MINING 
D1 1 1 1 0 0 
D2 0 0 1 1 1 
D3 0 1 1 1 1 
D4 1 1 1 0 1 
D5 0 0 1 1 0 
D6 1 1 0 0 1 
D7 1 1 0 1 0 
D8 0 1 0 1 0 
D9 0 1 1 0 1 
We form a matrix D [n-1, n] for n documents and cons ider only the upper triangular region. The cells of the matrix are filled by applying the similarity functio n S for which each document pair forms the input as shown below 
Table 4.  Feature Vector Representation of document set 
 D1 D2 D3 D4 D5 D6 D7 D8 D9 
D1 x {1,1,0,1,1} = 1 {1,0,0,1,1}=2 {0,0,0,Z,1}=3 {1,1,0,1,Z=1 {0,0,1,Z,1}=2 {0,0,1,1,Z}=2 {1,0,1,1,Z}=1 {1,0,0,Z,1=2 
D2 x x {Z,1,0,0,0}=3 {0,0,0,0,1}=4 {Z,Z,0,0,=2 {1,1,1,1,0}=1 {1,1,1,0,1}=1 {Z,1,1,0,1}=1 {Z,1,0,1,0}=2 
D3 x x x {1,0,0,1,0}=3 {Z,1,0,0,1}=2 {1,0,1,1,0}=3 {1,0,1,0,1}=2 {Z,0,1,0,1}=2 {Z,0,0,1,0}=3 
D4 x x x x {1,1,0,1,1}=1 {0,0,1,0,0}=4 {0,0,1,1,1 }=2 { 1,0,1,1,1}=1 { 1,0,0,Z,0}=3 
D5 x x x x x {1,1,1,1,1}=0 {1,1,1,0,Z}=1 {Z,1,1,0,Z}=1 {Z,1,0,1,1}=1 
D6 x x x x x x {0,0,Z,1,1}=2 {1,0,Z,1,1}=1 {1,0,1,Z,0}=2 
D7 x x x x x x x {1,0,Z,0,Z}=2 {1,0,1,1,1}=1 
D8 x x x x x x x x {Z,0,1,1,1}=1 
Once we obtain the above table with feature vectors for each document pair then we replace the corresponding cells of matrix by count of number of zeroes in tri state feature vector. We call it tri-state because it can have 0 or 1 or z as the value. This is shown in the table below. 
Table 5.  Similarity Matrix with Feature Vector Replaced by Count of 0s. 
 D1 D2 D3 D4 D5 D6 D7 D8 D9 
D1 x 1 2 3 1 2 2 1 2 
D2 x x 3 4 2 1 1 1 2 
D3 x x x 3 2 3 2 2 3 
D4 x x x x 1 4 2 1 3 
D5 x x x x x 0 1 1 1 
D6 x x x x x x 2 1 2 
D7 x x x x x x x 2 1 
D8 x x x x x x x x 1 
 324   Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
Now consider only the element of the matrix with the high est value as shown in the table below. The step by step procedure is shown below in the form of tables which is self explanatory. Step1: find the first maximum value from the matrix an d target only those cells having this value to form initial cluster. 
Table 6.  Content of Similarity Matrix showing step1  
 D1 D2 D3 D4 D5 D6 D7 D8 D9  
Find max value from the above table which is 4 here and target those cells as they form
 
the best candidate solutions and replace those cell by 
x  Stage1: 
(2, 4) and (4, 6) have val as 4. So form cluster as (2, 4, 6). 
 D1 x 1 2 3 1 2 2 1 2
D2 x x 3 x 2 1 1 1 2 
D3 x x x 3 2 3 2 2 3 
D4 x x x x 1 x 2 1 3 
D5 x x x x x 0 1 1 1 
D6 x x x x x x 2 1 2 
D7 x x x x x x x 2 1 
D8 x x x x x x x x 1 
 Step 2: Find the next max value from the above table wh ich is 3 here and target those cells as they form the best candidate solutions. Now cluster {2, 4, 6} is dynamica lly changed to {1, 2, 3, 4, 6, 9} and is no more a separate cluster as shown in table.2. cell values with superscript * not considered. 
Table 7.  Content of Similarity Matrix showing step2  
 D1 D2 D3 D4 D5 D6 D7 D8 D9 Find the next max value from the above table which is 3 here and target those cells as they form the best candidate solutions.  
 Stage2: 
consider only un-clustered document set {1, 3, 5, 7, 8, 9} ad search for value 3 in corresponding columns. (1,4)-(3,4)-(3,6)-(3,9) : So form cluster {2,4,6,1,3,9} as new Cluster. Set the values as zero or x.  
Cluster 1: {1, 2, 3, 4, 6, 9} D1 x 1 2 3 1 2 2 1 2
D2 x x 3* x 2 1 1 1 2 
D3 x x x 3 2 3 2 2 3 
D4 x x x x 1 x 2 1 3*  
D5 x x x x x 0 1 1 1 
D6 x x x x x x 2 1 2 
D7 x x x x x x x 2 1 
D8 x x x x x x x x 1 
 Step 3: Find the next max value from the above table wh ich is 2 here and target those cells as they form the best candidate solutions. 
Table 8.  Content of Similarity Matrix showing step3  
 D1 D2 D3 D4 D5 D6 D7 D8 D9 Find the next max value from the above table which is 2 here and target those cells as they form the best candidate solutions.  Stage3: consider only un-clustered document set {5, 7, 8} ad search for value 2 in corresponding columns. Here (7, 8) has 2. So form cluster {7, 8} as new Cluster. Set the values as zero or x. Cluster 2: {7, 8} D1 x 1 2 x 1 2 2 1 2
D2 x x x x 2 1 1 1 2 
D3 x x x x 2 x 2 2 x 
D4 x x x x 1 x 2 1 x 
D5 x x x x x 0 1 1 1 
D6 x x x x x x 2 1 2 
D7 x x x x x x x 2 1 
D8 x x x x x x x x 1 
 Step 4: Find the next max value from the above table wh ich is 1 here and target those cells as they form the best candidate solutions.  325  Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
Table 9.  Content of Similarity Matrix showing step4  
 d1 d2 d3 d4 d5 d6 d7 d8 d9 Stage4: consider only un-clustered document set {5} and search for value 1 in corresponding columns. Here (5, 7), (5, 8), (5, 9) are all 1s. But this is next minimum value after zero if we consider initial table values before clustering. Hence 5 can’t be similar to any of those documents and we must place it as a separate cluster {5}. D1 x 1 2 x 1 2 2 1 2
D2 x x x x 2 1 1 1 2 
D3 x x x x 2 x 2 2 x 
D4 x x x x 1 x 2 1 x 
D5 x x x x x 0 1 1 1 
D6 x x x x x x 2 1 2 
D7 x x x x x x x x 1 
D8 x x x x x x x x 1 
The Set of clusters finally formed are as shown below in the following figure.   Cluster-1: {1, 2, 3, 4, 6, 9} Cluster-2 :{ 7, 8} Cluster-3 :{ 5}   
 Figure. 2. Set of Clusters formed after applying the algorithm 
4. Case Study for Software Component Clustering or Program Partitioning  Consider the following program fragment  Procedure Sum_and_Prod (n: integer; arr: int_array; var sum, prod: integer; var avg: float) var i : integer; begin 1. sum = 0; 2. prod = 1; 3. for i = 1 to n do begin 4. sum = sum + arr[i]; 5. prod = prod * arr[i]; 6. end; 
326   Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
7. avg = sum / n; end; The table below shows the matrix with rows denoting line numbers or statements, columns denoting variable names. 
Table 10.  Boolean Matrix for program module Line Numbers Sum Prod N Arr Avg 
S1 1 0 0 0 0 
S2 0 1 0 0 0 
S4 1 0 0 1 0 
S5 0 1 0 1 0 
S7 1 0 1 0 1 
 The table below shows the similarity matrix formed using algorithm. The below list of tables show the step by step process of forming clusters and final output of clusters and are self descriptive. 
Table 11.  Trace of algorithm for stage 1  
 S1 S2 S4 S5 S7 Group S1 and S4 as (S1, S4) =1 and Mark as X. Ma rk Row elements of S4 by X.  This is done to reduce overlapping of variables. So (S1, S4) forms one cluster. 
S1 X 0 1 0 1 
S2  X 0 1 0 
S4   X 1 1 
S5    X  0  
 S7     X  
 Table 12.  Trace of algorithm for stage 2 
 
Table 13.  Trace of algorithm for stage 3  
 The program module may finally be separated into  two individual cluster modules which may run separately       S1 S2 S4 S5 S7 Again we have (S1, S7) =1. So group S1 and S7 into one cluster and mark the cell as X. As already S7 row is marked no need to do so.   Since (S1, S4) are similar and (S1, S7) are similar hence (S1, S4, S7) are all similar and placed into one cluster.  
i.e Cluster1 = {S1,S4,S7} S1 X 0 X 0 1 
S2  X 0 1 0 
S4   X  X X
S5    X  0  
S7     X
 S1 S2 S4 S5 S7 Finally we have (S2, S5) =1. So group S2 and S5 into one cluster and mark the corresponding cell as X. As already S5 row is marked no need to do so.  
i.e Cluster2 = {S2,S5} S1 X 0 X 0 X 
S2  X 0 1 0 
S4   X  XX  
S5    X 0  
S7     X  327  Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
                    Cluster1                                                                                      Cluster2   
 Figure. 3.   Program partitioning to run two modules in parallel 
5. Conclusion In this paper an attempt is made to study the prob lem of clustering software components for developing reuse library files and also the method of document cl ustering. An algorithm to cluster a set of given documents or text files or software components is desi gned which uses the new similarity function defined in this paper named hybrid XOR function defined for the pu rpose of finding degree of similarity among any two entities. The Proposed algorithm has the input as similarity matrix and the output being set of clusters formed dynamically as compared to other clustering algorithms that predefine the count of clusters and documents being fit to one of those clusters or classes finally. The ap proach can be extended to classify using classifiers and applying fuzzy logic in future. The concept of Support vector machines may be used for classification once clusters are formed if required. The search complexi ty can be reduced by using the algorithm [10] where ever necessary as part of component retrieval.  Acknowledgements  We are very much thankful to all the anonymous rev iewers and mainly thankful to Sri Dr. C.V.GuruRao for his invaluable suggestions and support throughout the work. We are also thankful to Mr. V.Sreekanth, Senior Technical Lead, Business Intelligence and Wareho using architect,IBM, Bangalore for his cooperation in carrying out the work. References [1] Congnan Luo, , Yanjun Li, Soon M. Chung. Text doc ument clustering based on neighbors , Data & Knowledge Engineering (68), 2009, 1271–1288. [2] Tianming Hu,Sam Yuan Sung, Hui Xiong, Qian Fu . Discovery of maximum length frequent itemsets, Information Sciences   (178), 2008,69–87. [3] Wen Zhanga,, Taketoshi Yoshida, Xijin Tang, Qing Wang. Text clustering using frequent itemsets, Knowledge-Based Systems 23 (2010) 379–388 [4] Wen Zhanga,Taketoshi Yoshida, Xijin Tang. A compar ative study of TF*IDF, LSI and multi-words for text classification. Expert Systems  with Applications 38 (2011) 2758–2765. [5] Vincent Labatut and Hocine Cherifi. Accuracy Meas ures for the Comparison of Classifiers, ICIT 2011 The 5th International Conference on  Information Technology. [6] Jung-Yi Jiang et.al A Fuzzy Self-Constructing Feat ure Clustering  Algorithm for TextClassification, IEEE TRANSACTIONS ON KNOWLEDGE AND DATA ENGINEERING, VOL. 23, NO. 3, MARCH 2011. [7] Melita Hajdinjak, Andrej Bauer. Similarity Measures  for Relational Databases, Informatica 33 (2009) 143–149. 1. sum = 0; 4. sum = sum + arr[i]; 
7. avg =sum / n;2. prod = 0; 5. prod= prod* arr[i]; 328   Chintakindi Srinivas et al.  /  AASRI Procedia   4  ( 2013 )  319 – 328 
[8] data
b
[9] SIG
K
[10] slid
i
page
[11] Pre
s
[12] Clu
s
Con[13] clus
t
Inter
                         
Coll
post is 
D
India
Com
servAp
p
of In
and 
                
Hyd
Tech
Dr.C
inter
R. Agrawal, 
bases, ProceeF. Beil, M. 
E
KDD Interna t
Radhakrish n
ing windows ,
es 543-552. I m
V.Susheela D
ss. Salim kebir, 
ster Algorith m
ference on C o
Ronaldo.C. V
tering techni q
rnational Con
                     
                                      
 
lege, Warang a
graduate, wi t
Doctorate hol d
a. He has mo
mputer Sciencing as the E
d
plication jour n
ndia, and me m
Institution of 
                         C
derabad. Pres e
hnology and S
C.V.Guru Ra o
rnational con f
T. Imielinskidings of the 
A
Ester, X.W. X
tional Confer e
na.V,C.Sriniv a
 International
mpact factor 3
Devi, M. Nara
Abdelha k-dja
ms for Softwa r
omputer Scie n
Veras, Silvio R
ques for the o r
ference on T o
  Dr. C.V.G u
al, Andhra P r
th specializat i
der in Comp u
re than 30 pu
e & Enginee r
ditorial Board 
nal. He is a li f
mber of Instit u
Electrical & E
C.Srinivas is 
ently he is w o
Sciences, Wa r
o. He has ove r
ferences and w
, A. Swami. M
ACM SIGMO D
u, Frequent t e
ence on Kno w
as, C.V.Guru r
 Journal of C o
.85. 
asimha Murt h
amel seriai, S y
re Componen t
nce and Softw
R.L.Meira , A
rganisation of 
ools with Arti f
uruRao is cu r
radesh, India. 
ions in Electr o
uter Science &
ublications to 
ring and Info r
member for I
fe member o f 
ution of Engi n
Electronics E n
a Masters D e
orking as an A
rangal and is 
r 15years of t
workshops. H iMining associ a
D Conferenc e
erm-based tex t
wledge Discov
rao.  High Pe r
omputer Engi n
hy. Text Book 
ylvain Chardi g
t Identificatio n
are Engineeri n
driano L.I. O l
software rep o
ficial Intellig e
rrently the H e
He has near l
onic Instrum e
& Engineerin g
his credit. H e
rmation Tec h
International J
fIndian Socie t
neers, Institut i
ngineers (US A
egree holder i
Associate Pr o
a research s c
teaching exp e
is areas of int eation rules be t
e on Manage m
t clustering, i n
ery and Data 
rformance Pa t
neering and T
on Pattern R e
gny. Compar i
n, in the Proc e
ng, Pages 1-8
liveira , Brun o
ositories, in th
ence. 
ead of the D e
ly 30 Years o
entation and I n
g from India n
e also served 
hnology, Kak a
Journal of C o
ty for Techni c
ion of Electro
A). 
in Computer 
ofessor in CS
cholar a t Kak a
erience and p r
erest are Soft wtween sets of i
ment of data, 1
n: Proceeding
Mining, 200 2
ttern Search a l
Technology , V
ecognition. A n
ing and Com b
eedings of th e
, 2012. 
o J.M.Melo. C
e proceeding s
epartment of 
of teaching e x
nformation S c
n Institute o f as the Chair
m
atiya Univers i
omputational I
cal Educatio n
nics & Telec o
Science and E
E Departme n
atiya Univers i
resented pape r
ware Reuse, Citems in very 
1993, pp. 207 –
s of the 8th A
2, pp. 436–44 2
lgorithm usin g
Volume 3,iss u
n Introductio n
bining Geneti c
e ACM Fifth I n
Comparitive s t
s of 19th IEEE 
CSE at S.R.
xperience. He 
cience & Eng i
fTechnology, 
man, Board o f
ity, Waranga l
Intelligence R
n, Instrument a
ommunicatio n
Engineering f
nt at Kakatiy a
ity under the 
rs at several n
Cloud Comput ilarge 
–216. 
ACM 
2. 
g three 
ue 2, 2012 , 
n. University 
c and 
nternational 
tudy of  
Engineering is a double 
ineering. He Kharagpur, 
f Studies for 
l. He is also 
Research and 
ation Society 
ns Engineers 
from JNTU, 
a Institute of guidance of 
national and 
ing.                  