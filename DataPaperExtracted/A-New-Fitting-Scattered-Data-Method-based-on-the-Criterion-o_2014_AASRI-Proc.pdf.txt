 AASRI Procedia   6  ( 2014 )  41 – 48 
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institutedoi: 10.1016/j.aasri.2014.05.007 ScienceDirect
2013 2nd AASRI Conference on Computational Intelligence and Bioinformatics 
A New Fitting Scattered Data Method Based on the Criterion of Geometric Distance 
Guowei Yang*, Jia Xu 
College of Information Engineering, Nanchang Hangkong University, Nanchang 330063, China 
Abstract 
The traditional data fitting method based on least square method is not good for vector data fitting whose independent variable is random. So this paper proposes a new criterion of data fitting which is the least quadratic sum of geometrical distance, and brings forward the new fitting scattered data method based on the new criterion. At the same time the paper puts forward
 the optimization algorithm for the solution of the data fitting parameter. Simulation experiments show that the fitting precision of the new method is higher than the one of least square method for data fitting of vector, whose independ
 ent variable is random. 
© 2013 Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied Science Research Institute 
Keywords: Data fitting, criterion of data fitting least square method, geometrical distance; 
1. Introduction In the experimental science, social sciences, behavioral science and the actual engineering fields, such as Computer-Aided Design, Manufacturing, virtual  reality, medical imaging, the experiment, the survey or the test can frequently bring large numbers of data. In order to explain these data or according to these data to make the forecast, the judgment, provides the important basis to the policy-maker, we needs to carry on the 
* Corresponding author. Tel.: +86-791-83953432; fax: +86-791-83953432. E-mail address: ygw_ustb@163.com. 
Available online at www.sciencedirect.com
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute42   Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
linear or nonlinear fitting modelling frequently to the survey data, seeks to the function (model) which can approximately reflect the change rules of the data. There are a long history and enrich results about data fitting and its 
application
[1-3]. In linear data fitting, we frequently use the least square method in order to get data fitting parameter. The reason is that if the fitting m odel conforms to the Gauss -Markov hypothesis condition, least square method can obtain the fitting parameter with good statistical nature, like agonic, uniformity, smallest variance and so on. However, the actual  test data infinitely varied, moreover the purpose of carrying on the data fitting to the test data are also different, and the precision request are also different, thus the data fitting result which use the least squares method to carry on the data fitting can not achieve the requested purpose. For example, there are some unusual data du e to occasionally abnormal error or data probability distribution deviates normal distribution, if we use the regression analysis result of least square method, we will lose its good statistical property, one solution of this situation is to use the criterion function with steady performance
[4-5]. Moreover, during linear fitting, compare to the Scatter-point actual distributed tendency, the straight line determined by least squares method in many fields all has a bit small slope phenomenon, especially the fluctuation of the sample Scatter-point is a little bigger, this phenomenon is more obvious. The reason is that, when we use ordinary least  square method on parameter estimation, we have the following criterion: choose an equation, which the range difference sum of squares 
¦ ¦  22ˆi iiy y E of the dependent variable between the observed value and the estimated value to be smallest, from all the possible linear equation. Its geometry significance is: To find a straight line, which longitudinal distance sum of squares between the scatter-point of the observed value and this straight line to be smallest, from all possible straight lines. Just because the goal is to make the longitudinal distance sum of squares between the scatter-point and this straight line to be smallest, rather than the geometrical distance (vertical distance) sum of squares between the scatter-point and this straight line to  be smallest, which result in the estimated straight line has slope small tendency
[6].Lifts the background of least square method theory, we can discover the concealed default premise supposition when we use
  least square method on data fitting: because the dependent variable of the sample is influenced by the stochastic disturbing term, it is stochastic undulation, while the sample independent variable doesn’t stochastic, that is scatter-point deviates linear equation, completely because scatter-point undulates in the fluctuation direction of dependent variable, rather than the combined effect in the fluctuation direction of all the dependent variable. Therefore, use least square method on data fitting directly without considering the premise supposition condition, and no wonder we will get the fitting equation with deviation. The different results are produced by different methods, however different methods are established on the foundation of different premise supposition. Therefore each method itself can not be said who is right in this question, just whose prem
ise supposition more reasonable.  In practical life, there are generally two kinds of relationship of models, that is: (1) The definite relation model, that is, in scatter-point
y x,,xandyhas definite relation (that is: xis independent variable 
y is dependent variable or y is independent variable x is dependent variable) (2) The independent variable is the random variable model, that is, in scatter-point
y x,, the
xandyprimary and secondary relations are fuzzy, not clear , the independent variable and the dependent variable do not always differentiate very clear. For example, the relations of human's height and weight, the degree of two variables are completely coordinated, not only have
P  ) (x f y , but also have 
v y g x  ) (.Obviously, least square method is not suitable 
 for all linear fitting model parameter estimation. Therefore, to second kind model, we generally do not use least squares method criterion fitting. In order to solve the inde
pendent variable for the random variable model fitting, in recent years, many scientific and technology 43  Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
workers carry on much research work on the least square fitting method and obtain fruitful efforts. For example, Principal Component Regression (PCR), Partial Least Squares (PLS), all kinds of Non-Linear Principal Component Regression (NLPCR), all kinds of Non-Linear Partial Least Squares (NLPLS), Neural Network method and the syntheses of these methods ect ., which used in actual quite many in present project
[4- 9]
. In this article, we do not use the method which is mentioned by the above literature when carry on scatter-point fitting, instead of using the minimum of geom etrical distance sum of squares as new fitting standard, we propose a new data fitting method based on the new standard. In the method, including model transform, turns fitting pa
rameter solution into "constrained optimization problem", and provides the fitting parameter solution algorithm. The simulation experiment indicated that, the fitting parameter solution algorithm is feasible and effective; in the data fitting which independent variable is the random variable vector data, we can get a higher fitting precision by using the new data fitting method instead of least square method. 2. A New Data Fitting Criterion and New Data Fitting Method Question: Given the linear relation variable
, , ,2 1X XnX have experiment or observation data (has error) in 
ndimension space: ini ix x x, , ,2 1 , , , 1 iN n Nt , solve the actual linear relation formula of 
nX X X, , ,2 1 .About this question, the traditional classical procedure is suppose 
1X is dependent variable, nX X, ,2
is independent variable, establish ,1XnX X, ,2  linear relation formula 2 2 1X XE   n nXE
nEby least square method. The basic presupposition that establish nX X X, , ,2 1  linear relation formula 
n n n XX XEE E     2 2 1  by least square method is: 1X is dependent variable, nX X, ,2
is independent variable, and nX X, ,2  is fixed variable (means the experiment or observation data does not have error). It is proved that, under such presupposition, data fitting by least squares method is quite effective. However this presupposition is dissatisfy in many situ ations, sometimes independent variable 
nX X, ,2  is random variable, sometimes 
nX X X, , ,2 1  can not be distinguished which is dependent variable, which are independent variables. While independent variable 
nX X, ,2  is random variable, or nX X X, , ,2 1  can not be distinguished which is dependent variable and which are independent variables, we naturally associate to their implicit function relations, that is, the variable
nX X X, , ,2 1  linear relations in ndimension space can be shown as follows, 
02 2 1 1    b X a X a X an n                                                                                                       (1) where 
na a a2 1,  are not all 0. Data vector ini ix x x, , ,2 1 Ni, , 2 , 1  on the above or periphery of the hyperplane 
n nX a X a X a 2 2 1 1 0 b. Therefore, to solve the question which proposed above actually is to choo
 se a “proper” hyperplane 
02 2 1 1    b X a X a X an n  in ndimension space which can do vector data 
ini ix x x, , ,2 1 Ni, , 2 , 1 fitting, where b a a an, ,2 1  are some uncertain parameters. The visually and easy to excogitate hyperplane choice criterion is: the least quadratic sum of distance (or distance) from 
ini ix x x, , ,2 1 Ni, , 2 , 1  to the hyperplane 02 2 1 1    b X a X a X an n . Definition 1: Called the data point to the 
hyperplane distance 44   Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
222212 2 1 1ninniiia a ab x a x a x ae       Ni, , 2 , 1                                                                    (2) is
) , , , (2 1njj j x x x  to   n nX a X a X a2 2 1 1 0 b geometry distance. Definition 2: Define the evaluation function of data fitting is 
¦ ¦
         Ni ninni i Nii
a a ab x a x a x ae J 12222212 2 1 112   Ni, , 2 , 1                                               (3) Call
Jmin is geometrical distance criterion (new data fitting criterion), that is, the least quadratic sum of geometrical distance criterion. As follows, we will give the new data fitting method base on the least quadratic sum of geometrical distance criterion.  The new data fitting method base on the least quadratic sum of geometrical distance criterion mainly consists of the following two parts: (a)Data fitting optimization model (b)Data fitting optimization model solution Data Fitting Optimization Model Divide 
22221na a a   on each side of (1) equation synchronously, and suppose 
22221njja a aaa    n j, , 2 , 1 ,
22221na a abb   , then (1) equation can be changed into:  
02 2 1 1     b X a X a X an n                                                                                                   (4) and
122221   na a a . In a similar way, (3) equation can be changed into: 
¦¦ ¦
               Ni inniiNi ninniiNii
b x a x a x aa a ab x a x a x ae J
122 2 1 112222212 2 1 112 
Hence, data fitting model can be changed into the following optimization model: 45  Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 

0 1. .min22221122 2 1 1       ¦
 
nNi inniia a a t sb x a x a x a  Ni, , 2 , 1                                                          (5) Where 
b a a an, , , ,2 1 are some uncertain parameters. ini ix x x2 1, , N i, , 1  are Nknown data scatter-points. Data Fitting Optimization Model Solution By extremum solution theory, formula (5) can be transformed into Lagrange function: 
  1 ) , , , , , (22221 122 2 1 1 2 1          ¦
 nNi inni i na a a b x a x a x a b a a a L OO
                (6) Consider the steady point 
) , , , , , (2 1 Ob a a an . Take partial derivative to the above equation, we can obtain the following equation group. 
0 ) , , , , , (2 1  Ob a a a Fn                                                                                                                   (7)  Simply written as: 
0 F, where , ,2 1F F F Tn nF F2 1, ,  , that is 



°°°°°¯°°°°°®­
                ¦¦¦
   
0 10002222112 2 1 112 2 1 112 2 1 1 1 1
nNi inniiNi inniiinnNi innii i
a a ab x a x a x ab x a x a x a x ab x a x a x a x a
    OO
                                                                         (8) Where 
N i, , 1  are N known data scatter-points. 
¦
      Ni inniiimmm
b x a x a x a x a F
12 2 1 1 O  nm, , 2 , 1 
¦
      Ni inni in
b x a x a x a F
12 2 1 1146   Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
122221 2      n na a a F
Solve the non-linear equations group of equation(7), the fitting parameter b a a an, , , ,2 1 andO are obtained, farther solve the linear fitting equation 
02 2 1 1    b X a X a X an n , as well as solve the equation 
n nX a X a X a 2 2 1 1 0 b, thereby we can obtain the data fitting model. 3. Solution Algorithm of the Data Fitting Parameter Iteration algorithm of fitting parameter will be given as follows. For non-linear equation group
, , , , , (2 1 b a a a Fn 0 ) O , O, , , , ,2 1 b a a an are some uncertain parameters. Suppose 
) , , , , , (2 1 Ob a a a Yn , initial iteration point   ) , , , , , (0 0 002010Ob a a a Yn , the iteration point after iterate 
k times is    ) , , , , , (2 1k k knk kk b a a a YO  , by Taylor formula, we can obtain 
   111) ( ) ( |k kkkkY Y Y F Y F Y F
after rearrangement, we obtain the following equation: 
   ) ( ) () (111 kkkkkY F Y F Y Y Y F 
Definition 3: Call iteration algorithm 
  
    
 
°°¯°°®­  '        
, 1 , 0) ( ) () ( 11111 1
kY FY F Y FY F Y F Y Y Y FY F Y F Y Ykkkkkkk kkkkk
                                                                          (9) is the correction technique of solving non-linear equation group
0 ) , , , , , (2 1  Ob a a a Fn  rank .m
By equation (9), we can solve b a a an, , , ,2 1 , accordingly, can also determined the variable coefficient 
b a a an, , , ,2 1 . As well as by equation (9), first solvesna a a, , ,2 1 ,b, consequently, determines the variable coefficient 
b a a an, , , ,2 1 .4. Simulation Analysis Suppose t
he point on the straight line
0 110  y x , the observation data (has error) as follows:   
1 i2 i 3 i 4 i5 i6 i7 i8 i 9 i
x -2 -2 -0.5 -1 0.5 0 1.5 1 2.5 
y -19 -14 -9 -4 1 6 11 16 21 
Determine the fitting straight line 0  cby ax  by the above discrete point, where c b a, ,are some unknown parameter. Then solve the equation (8) with the data fitting parameter solution algorithm. Following Table 1 and Fig. 1 respectively is under the perfect condition, under the least square criterion, the straight line equation fitting parameter and the simulation figure based on the geometrical distance criterion. 47  Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
Table 1. Fitting parameter under different criterion   perfect condition the least square criterion based on the geometrical distance criterion 
a 10 8.4211 -0.99377232 
b -1 -1 0.11142971 
c 1 1 -0.1115 
Fig. 1. Simulation result under different fitting standard 
By the figure we can see, the discrete point uniform distributes on the different sides of three straight lines. It is obvious that the fitting effect by the least square criterion and based on the geometrical distance criterion is all good. However, reference to the ideal straight line we can know that the fitting straight line under new criterion obviously better than the least square fitting straight line, the fit ting straight line under new criterion located between the ideal straight line and the least square straight line, the fitting precision is higher than the least square straight line. Acknowledgements The authors would like to thank the editors and the anonymous reviewers for their valuable comments and constructive suggestions. This research is supported by the National Natural Science Foundation of China (No. 61272077, 61202319), the Natural Science Foundation of Jiangxi Province(No. 20114BAB201034) and the Scientific and Technological Project of Jiangxi Province(No. 20133BBE50022). Refere
nces [1] Lin Honghua. Data processing of dynamic m easurement. Beijing: Beijing Institute of Technology Press;1952. [2] Lin Hongwei. Adaptive data fitting by the progressive-iterative approximation. Computer Aided Geometric Design. 2012; 2( 7): 463-473. 48   Guowei Yang and Jia Xu  /  AASRI Procedia   6  ( 2014 )  41 – 48 
[3] Lapo Governi, Rocco Furferi, Matteo Palai, Yary Volpe. 3D geometry reconstruction from orthographic views: A method based on 3D image processing and da ta fitting. Computers in Industry, In Press, online 19 March 2013. [4] E. Vassiliou, I.C. Demetriou. An adaptive algorithm for least squares piecewise monotonic data fitting. Computational Statistics & Data Analysis. 2005; 49(2):591-609. [5] G. Casciola, L. Romani. A Newton-type method for constrained least-squares data-fitting with easy-to-cont
rol rational curves. Journal of Computational and Applied Mathematics. 2009; 223:672-692. [6] Huang Minjie, Ye Hao, Wang Guizeng. The regression analysis method summary based on projection, Control theory and application. 2001; 8(18):1-6. [7] Alexandru Mihai Bica. Fitting data using optimal Hermite type cubic interpolating splines. Applied Mathematics Letters. 2012; 25(12):2047-2051. [8] Philipp Reinecke, Tilman Krauß, Katinka Wolter. Cluster-based fitting of phase-type distributions to empirical data. Computers & Mathematics with  Applications. 2012; 64(12);3840-3851. [9] Akemi Gálvez, Andrés Iglesias, Andreina Avila. Immunological-based Approach for Accurate Fitting of 3D Noisy Data Points with Bézier Surfaces. Procedia Computer Science. 2013; 18:50-59. 