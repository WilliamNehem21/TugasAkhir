Generative and self-supervised domain adaptation for one-stageobject detection
Kazuma Fujii, Kazuhiko Kawamoto*
Chiba University, 1-33, Yayoicho, Inage Ward, Chiba-shi, Chiba, 263-8522, Japan
ARTICLE INFO
2010MSC:00-0199-00Keywords:Domain adaptationObject detectionUnsupervised learningABSTRACT
Unsupervised cross-domain object detection has recently attracted considerable attention because of its ability tosigniﬁcantly reduce annotation costs. For two-stage detectors, several improvements have been made in feature-level adaptations. However, this approach is not suitable for one-stage detectors that do not have access toinstance-level features. Although other approaches are often used for one-stage detectors, their performance isinsufﬁcient compared to domain adaptation methods for two-stage detectors. In this study, we propose agenerative and self-supervised domain adaptation method for one-stage detectors. The proposed method iscomposed of an adversarial generative method and a self-supervision-based method. We tested our method onthree evaluation datasets, and an improvement in the mean average precision was achieved using this method. Wealso conﬁrmed the complementary effects of an adversarial generative method and a self-supervision-basedmethod.
1. IntroductionComputer vision has attracted attention because of its applications inautomated driving, video surveillance, anomaly detection, etc. In addi-tion, the advent of deep learning has led to signi ﬁcant developments in computer vision. One of the typical tasks in this ﬁeld is object detection. In object detection, objects in an input image are classi ﬁed, and then, localized using bounding boxes. Recent studies in this area have achievedremarkable results based on advancements in deep neural networks.Object detectors can be categorized as two-stage [ 1–3] or one-stage [4–7]. One-stage detectors are superior in terms of their inference speed.Deep-learning-based object detectors are typically trained from adataset that has many real-world images with instance-label annotations,such as Pascal visual object classes (VOC) [ 8]. However, the performance often decreases signiﬁcantly when the training and test data havedifferent distributions. One possible solution is to collect labeled data fora new domain, although this is a time-consuming approach. Anothersolution is domain adaptation. The goal of domain adaptation is to adapta model from a label-rich domain (source domain) to a label-scarcedomain (target domain). In particular, unsupervised domain adaptationassumes that there are no labels available in the target domain.Unsupervised domain adaptation methods focused on visual tasks canbe divided into four categories: discrepancy-based, adversarial discrim-inative, adversarial generative, and self-supervision-based methods [ 9]. Discrepancy-based methods are designed to reduce the difference be-tween the source and target domain distributions [ 10,11]. Adversarial discriminative methods are designed to align features using the adver-sarial learning of feature extractors and domain classi ﬁers [12,13]. Adversarial generative methods use target-like images with originalsource annotations obtained from image-to-image translations [ 14–16]. Self-supervision-based methods incorporate a self-supervised learningtask in the target domain [17–19].For two-stage detectors, domain adaptation approaches are mainlybased on adversarial discriminative methods. They are often designed toalign features at several levels, and have been improved in variousmanners. For example, image- and instance-level alignments have beenused in Ref. [20]. However, it is difﬁcult to assume such an approach in one-stage detectors, predicting bounding boxes and object classesconcurrently.For one-stage detectors, the authors of [ 16] proposed a domain
* Corresponding author.E-mail addresses:fujisan8@chiba-u.jp(K. Fujii),kawa@faculty.chiba-u.jp(K. Kawamoto).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100071Received 14 February 2021; Received in revised form 26 April 2021; Accepted 26 May 2021Available online 7 June 20212590-0056/©2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-
nc-nd/4.0/ ).Array 11 (2021) 100071transfer (DT) based on adversarial generative methods in a weakly su-pervised cross-domain setting. With DT, images with instance-level an-notations are transferred from the source domain to the target domain.Under an unsupervised setting, the authors of [ 19] proposed weak self-training (WST) based on self-supervision-based methods. WST en-ables the training of unlabeled images, by reducing the negative effects ofinaccurate pseudo-labels. However, the performances of these domainadaptation methods for one-stage detectors are insuf ﬁcient compared with those for two-stage detectors.Among the four categories of domain adaptation, adversarial gener-ative and self-supervision-based methods can be easily applied to one-stage detectors. Moreover, these two methods have different advan-tages. An adversarial generative method can access accurate source la-bels; meanwhile, a self-supervision-based method can use the originaltarget images. To take advantage of both methods, we propose an un-supervised domain adaptation method that combines an adversarialgenerative method with a self-supervision-based method. For eachmethod, we use DT and WST, which have been shown to be effective forone-stage detectors [16,19]. We show that the two components com-plement each other, thereby improving the detection performance.In summary, our main contributions are as follows.●We propose an unsupervised domain adaptation approach for one-stage detectors. Our method consists of an adversarial generativemethod and a self-supervision-based method.●We show that an adversarial generative method and a self-supervision-based method complement each other.●The proposed method achieves an improvement in the mean averageprecision on three benchmark datasets.2. Related workIn this section, we review the literature on object detection anddomain adaptation.2.1. Object detectionThe development of deep convolutional neural networks (CNN) hasimproved the performance of object detection. Two-stage detectors (suchas R-CNN [1], Fast R-CNN [2], and Faster R-CNN [3]) extract region proposals, and then, classify them. One of the advantages of the two-stagedetectors is that the classiﬁer can be customized to suit a speciﬁc task [21]. In contrast, one-stage detectors, such as You Only Look Once(YOLO) [4] and Single Shot MultiBox Detector (SSD) [ 5], achieve sig- niﬁcant improvements in the inference speed using a single-stagenetwork. Furthermore, recent studies [ 6,7] have improved both the ac- curacy and inference speed.In this study, we tested our method on an SSD, which is a represen-tative one-stage detector. SSD has a simple architecture, and is wellbalanced in terms of inference speed and performance. Furthermore,because SSD has been used in related studies [ 16,19], we can make a fair comparison.2.2. Domain adaptationThe goal of domain adaptation is to adapt the information learnedfrom the source domain for use in the target domain. The authors of [ 9] divided unsupervised domain adaptation methods for visual tasks intofour categories: discrepancy-based methods, adversarial discriminativemethods, adversarial generative methods, and self-supervision-basedmethods.Discrepancy-based methods [10,11] manage to reduce the discrep- ancy between the feature distributions of the source and target domains.Adversarial discriminative methods [ 12
,13] employ adversarial learning to align features. Discrepancy-based methods and adversarial discrimi-native methods are also called feature-level adaptations, because theyaim to obtain domain-invariant features. Although these two categoriesoften perform well, they require architecture-speci ﬁc design. Adversarial generative methods [14–16] are known as pixel-level adaptations. They are based on generative adversarial nets [ 22], and produce target-like training data from the source images. This categorycan be applied regardless of the architecture type, as it only changes thetraining data. However, the performance is highly dependent on thequality of image generation.Self-supervision-based methods [17–19] employ self-supervised tasks, such as reconstruction, image rotation prediction, andself-training. This category brings the source and target domains closerby adding auxiliary tasks to the target images. However, the performanceis limited compared to the other categories.2.3. Domain adaptive object detectionRecently, numerous studies have been proposed to address theproblem of domain shifts in object detection. For two-stage detectors,adversarial discriminative methods are often used [ 20,23,24], and have shown very good performance in recent studies [ 25–27]. In many cases, they are designed to align features at several levels, including theinstance-level. One-stage detectors do not have access to instance-levelfeatures, as they predict bounding boxes and object classes simulta-neously. Therefore, adversarial discriminative methods are not suf ﬁcient for one-stage detectors.However, adversarial generative methods and self-supervision-basedmethods are suitable for both two-stage and one-stage detectors. Theauthors of [16] proposed the DT method, which is based on adversarialgenerative methods. DT transfers images with instance-level annotationsfrom the source domain to the target domain using CycleGAN [ 28], and trains a detector on domain-adapted images. The authors of [ 19] pro- posed the use of WST and adversarial background score regularization(BSR), which are based on self-supervision-based methods. With WST,reliable detections on unlabeled images were chosen, andpseudo-instance-level annotations were generated. BSR reduces thedomain shift by extracting discriminative features for target back-grounds. In this study, we combine an adversarial generative method anda self-supervision-based method for a one-stage detector.3. MethodIn this study, we propose a generative and self-supervised domainadaptation method for a one-stage detector. Fig. 1shows an overview of our method. Our method is based on SSD [ 5], and combines two methods: DT [16], which is an adversarial generative method, and WST[19], which is a self-supervision-based method.In this section, weﬁrst formulate the problem, and then, explain theeffect of combining the two methods and the details of the proposedmethod.3.1. Problem settingLetxandydenote an input image and a label, respectively. We as-sume that the source datafðx
is;yisÞgNs
i¼1are drawn from the source domainK. Fujii, K. Kawamoto Array 11 (2021) 100071
2Xs, and the target datafðxit;yitÞgNt
i¼1are drawn from the target domainX t, whereN
s.Ntis the number of source and target samples, respectively. Wedenote the distribution of domainXasP(X) andP(X
s)6¼P(X t). Therefore, the source and target data have different distributions, as shown inFig. 2(a). We do not have access to the target labels fy
itgNt
i¼1because we address unsupervised domain adaptation.3.2. Generative and self-supervised domain adaptationIn an ideal scenario where the target labels are available, supervisedlearning of the target data is possible, as shown in Fig. 2(b). We approach the unsupervised domain adaptation for one-stage detectors by bringingthe learning setting closer to the ideal case.We propose generative and self-supervised domain adaptationcomposed of an adversarial generative method and a self-supervision-based method. In the adversarial generative method ( Fig. 2(c)), the source images were converted to target-like images. Although the dis-tribution of the transferred images does not perfectly match that of thetarget images, the transferred images with source labels enable super-vised learning. In the self-supervision-based method ( Fig. 2(d)), self- supervised tasks were employed on the target images. Although super-vised learning cannot be applied, this method enables the training of theoriginal target images. These two contrasting methods are expected tocomplement each other. The proposed method ( Fig. 2(e)) is close to the ideal case (Fig. 2(b)) in terms of supervised learning near the targetdomain and using the original target images for training.In this study, we applied DT [16] to the adversarial generative method and WST [19] for the self-supervision-based method. DT usesCycleGAN [28] to transform the source images into target-like images.WST enables self-supervised learning by generating pseudo-labels on thetarget images. The pseudo-labels are assigned to reliable detections,considering the detection results of neighboring regions. Then, WSTtrains detectors using pseudo-labels, while reducing the effect of falsenegatives using weak negative mining.3.3. Training methodThe proposed method can be divided into three steps.Pre-training SSD:We pre-train the model using the source datafðx
is;yisÞgNs
i¼1. The training loss of SSD [5] can be written as follows.Lðb;c;l;gÞ¼
1NðL
confðb;cÞþ αLlocðb;l;gÞÞ:(1)wherebis a matched default box,cis the conﬁdence of multiple classes,l is a predicted box,gis a ground-truth box,Nis the number of matched default boxes,
αis the weight,L confis the conﬁdence loss, andL locis the localization loss.DT:We trained CycleGAN on the source images fx
isgNs
i¼1and target imagesfx
itgNt
i¼1. Using the trained CycleGAN, we converted the sourceimagesfx
isgNs
i¼1, which are used in pre-training, and obtained domain-adapted imagesfx
is→tgNs
i¼1that accompany labelsfyisgNs
i¼1. Examples of domain-adapted images are shown in Fig. 3. The real-world images are transferred to each target domain.Fine-tuning:The model wasﬁne-tuned using domain-adapted datafðx
is→t ;yisÞgNs
i¼1and target imagesfxitgNt
i¼1. Duringﬁne-tuning, the training batch consists of half domain-adapted data and half target images. Weapplied the loss function of the SSD, as shown in Eq. (1), for the domain- adapted data and the WST for the target images.
Fig. 1.Overview of the proposed approach.K. Fujii, K. Kawamoto Array 11 (2021) 100071
34. Experiments4.1. Datasets and evaluationIn our experiments, we used the Pascal VOC dataset [ 8] as the source domain with the Clipart1k, Watercolor2k, or Comic2k datasets [ 16]a s the target domains. Examples of target images are shown in Fig. 4. Pascal VOC is a real-world image dataset, providing instance-levelannotations. VOC2007-trainval and VOC2012-trainval datasets have16,551 images with 20 object classes. Clipart1k is a graphical imagedataset, and has the same classes as Pascal VOC. It provides 500 imagesfor the training set and another 500 images for the test set. Watercolor2kand Comic2k are unrealistic datasets, and have six classes in Pascal VOC.Each dataset provided 1000 images for the training set and 1000 imagesfor the test set. We trained a model without using the labels of the targetimages, because we tackled unsupervised domain adaptation.For all experiments, we evaluated different methods on the target testdata using average precision (AP) and mean average precision (mAP) asindicators.
Fig. 2.Visualizations of input images and learning strategies. Blue: source images; red: target images; purple: transferred images from the source domain to the target domain. (For interpretation of the references to color in this ﬁgure legend, the reader is referred to the Web version of this article.)K. Fujii, K. Kawamoto Array 11 (2021) 100071
44.2. Experimental setupDetails of training:For all experiments, SSD300 [5] was used as a base detector. Following the original study [5], we trained the model using the source data for 120,000 iterations. We used this model as thebaseline for the experiments.We also trained CycleGAN using source images and each target imagefor 20 epochs, following the original study [ 28]. Using the trained CycleGAN, we obtained domain-adapted images from the source images.We thenﬁne-tuned all layers of the model using our method. Thebaseline was applied as the initial weight. Each batch was composed of32 images—16 from the domain-adapted images and 16 from the targetimages. The model was trained for 3000 iterations with a learning rate of1.0/C210
/C05.Comparison:We compared our method with the baseline [ 5], DT [16] and WSTþBSR [19] approaches. To quantify the relative contri-butions of the adversarial generative method and self-supervision-basedmethod with our method, we trained and tested the model using DT or
Fig. 3.Examples of domain-adapted images. The images in Pascal VOC [ 8] are transferred to the Clipart1k, Watercolor2k, and Comic2k [ 16] domains.
Fig. 4.Example images in target domains.K. Fujii, K. Kawamoto Array 11 (2021) 100071
5WST alone.4.3. ResultsResults on Clipart1k:A comparison of the performance on Clipart1kis presented inTable 1. Our method outperformed the other methods interms of AP on the six classes, and improved the mAP by 12.6% from thatof the baseline and by 0.4% from that of the existing methods. ApplyingDT or WST alone also outperformed the baseline. In particular, DTimproved the mAP by 11.1% from that of the baseline.Results on Watercolor2k:A comparison of the performance onWatercolor2k is presented inTable 2. Our method outperformed the other methods for AP in all three classes, improving the mAP by 4.1%over that of the baseline and by 0.5% over that of the existing methods.Applying DT or WST alone also outperformed the baseline. However, DTimproved the mAP by only 0.2% from that of the baseline.Results on Comic2k:A comparison of the performance on Comic2kis presented inTable 3. Our method outperformed the other methods forAP in three classes, improving the mAP by 10.7% over that of the base-line and by 2.4% over that of the existing methods. Applying DT or WSTalone also outperformed the baseline. Speci ﬁcally, DT improved mAP by 9.7% from that of the baseline.4.4. Qualitative resultsThe qualitative results are shown in Fig. 5. We found that the pro- posed method detected more objects correctly compared to the baseline.Furthermore, objects detected by DT but not by WST were detected bythe proposed method, and vice versa.4.5. DiscussionComplementary effect:The experiments show that our method iseffective for unsupervised domain adaptation. Based on the improvedaccuracy compared to using DT or WST alone, the adversarial generativemethod and self-supervision-based method are considered to comple-ment each other.Measuring domain distances:To quantitatively evaluate the per-formance of each target dataset, we computed the FID [ 29] between the source and target images and between the domain-adapted and targetimages, as shown inTable 4. The FID measures the difference betweentwo distributions in the high-dimensional feature space of theInception-v3 model [30], and indicates the similarity between the twogroups. The smaller the FID between the source and target images, thebetter the performance of the baseline, as shown in Tables 1–3. DT performance:We found that DT is ineffective for Watercolor2k ascompared to Clipart1k and Comic2k. Although the FID between thedomain-adapted and target images did not differ signi ﬁcantly among the datasets, the distance difference on Watercolor2k was the smallest,which led to DT's poor performance. In contrast, DT performs better forClipart1k and Comic2k, where the distance differences are larger. Thus,the effectiveness of the adversarial generative method depends on thetarget dataset.WST performance:WST showed improvements for all three datasets.The results suggest that the self-supervision-based method is robust tovariations in the target domain.Table 1Results on adaptation from Pascal VOC to Clipart1k. AP (%) was evaluated for the target images. Column
“C”indicates which categories the method belongs to, where
“G”and“S”denote the adversarial generative methodand self-supervision-based method, respectively.
Method C aero bike bird boat bottle bus car cat chair cow table dog horse mbike person plant sheep sofa train tv mAPBase [5] 23.3 56.6 17.9 17.3 14.5 39.4 33.3 7.2 43.4 11.5 28.6 11.1 26.4 48.1 35.6 27.3 2.7 22.0 26.0 23.5 25.8DT [16] G 23.3 60.1 24.9
41.5 26.4
53.0 44.0 4.1 45.3
51.539.5 11.6
40.462.2 61.1 37.1
20.9 39.6
38.4 36.0 38.0WSTþBSR [19] S 28.0
64.523.9 19.0 21.9
64.343.516.442.2 25.9 30.5 7.9 25.5
67.654.5 36.4 10.3 31.2
57.443.5 35.7DT G 26.3 56.3 24.3 27.7 26.2 49.8 45.3 5.0
49.649.241.115.2 32.3 55.5 59.5
39.314.8 33.2 39.5 48.1 36.9WST S 24.2 55.6 18.2 20.4 18.4 41.9 38.7 5.6 45.5 18.0 32.5 7.1 29.0 53.5 45.0 26.4 3.9 25.6 28.6 30.7 28.5DTþWST(proposed) G
þS28.261.525.128.9 23.6 57.0
46.76.8 48.7 49.6 37.0
16.634.5 60.2
63.338.5 13.6 36.6 42.4
48.9 38.4K. Fujii, K. Kawamoto Array 11 (2021) 100071
65. ConclusionIn this study, we the addressed unsupervised domain adaptation forone-stage detectors. To take advantage of both the adversarial generativemethod and self-supervision-based method, we introduced a generativeand self-supervised domain adaptation method. Speci ﬁcally, we pro- posed a learning strategy for SSDs by applying DT and WST.Our experiments show that the proposed method improves thedomain adaptation performance on three benchmark datasets. Further-more, we conﬁrmed that the two components of our method complementeach other.Table 3AP for the adaptation from Pascal VOC to Comic2k(%).
Method C bike bird car cat dog person mAPBase [5] 40.5 10.1 22.1 10.2 11.8 34.5 21.5DT [16] G 43.6 13.6 30.2 16.0 26.9 48.3 29.8 WSTþBSR [19] S 50.6 13.6 31.0 7.5 16.4 41.4 26.8DT G 49.4 16.332.7 14.7 22.6 51.2 31.2 WST S 45.6 9.9 27.3 9.8 12.6 50.4 25.9DTþWST(proposed) GþS52.715.035.813.0 20.8 56.1 32.2
Fig. 5.Examples of detection results on the target domain, from top to bottom: Clipart1k, Watercolor2k, and Comic2k [ 16].
Table 4Fr/C19echet inception distance (FID) [29] between the source and target images and between the domain-adapted and target images. The source images represent thePascal VOC dataset, the target images represent the respective target dataset, andthe domain-adapted images represent the Pascal VOC dataset transferred to therespective target domain via DT. In addition, we computed the difference be-tween the two FIDs.
Targetdatasetsource images↔target imagesdomain-adapted images↔target imagesdistancedifferenceClipart1k 145.2 78.6 66.6Watercolor2k 124.3 75.9 48.4Comic2k 160.8 76.3 84.5Table 2AP for the adaptation from Pascal VOC to Watercolor2k(%).
Method C bike bird car cat dog person mAPBase [5] 81.2 45.6 39.8 29.5 27.1 57.7 46.8DT [16] G 82.8 47.0 40.2 34.6 35.3 62.5 50.4 WSTþBSR [19] S 75.6 45.8 49.334.1 30.3 64.1 49.9 DT G 80.7 45.1 41.6 29.0 27.2 58.3 47.0WST S 76.0 47.142.7 30.3 29.6 65.7 48.6 DTþWST(proposed) GþS88.6 47.1 44.1 30.6 28.1 67.1 50.9K. Fujii, K. Kawamoto Array 11 (2021) 100071
7Credit author statementKazuma Fujii: Conceptualization, Methodology, Software, Writing -Original Draft; Kazuhiko Kawamoto: Supervision, Writing - Review & Editing.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsThis work was supported by Sumitomo Construction Machinery CO.,LTD.References
[1]Girshick R, Donahue J, Darrell T, Malik J. Rich feature hierarchies for accurateobject detection and semantic segmentation. In: CVPR; 2014. p. 580 –7. [2]Girshick R. Fast r-cnn. In: ICCV; 2015. p. 1440 –8. [3]Ren S, He K, Girshick R, Sun J. Faster r-cnn: towards real-time object detection withregion proposal networks. In: NeurIPS; 2015. p. 91 –9. [4]Redmon J, Divvala S, Girshick R, Farhadi A. You only look once: uni ﬁed, real-time object detection. In: CVPR; 2016. p. 779 –88. [5]Liu W, Anguelov D, Erhan D, Szegedy C, Reed S, Fu C-Y, Berg AC. Ssd: single shotmultibox detector. In: ECCV. Springer; 2016. p. 21 –37. [6] J. Redmon, A. Farhadi, Yolov3: an incremental improvement, arXiv preprint arXiv:1804.02767.[7]Zhao Q, Sheng T, Wang Y, Tang Z, Chen Y, Cai L, Ling H, M2det. A single-shotobject detector based on multi-level feature pyramid network. In: AAAI, vol. 33;2019. p. 9259–66.[8]Everingham M, Van Gool L, Williams CK, Winn J, Zisserman A. The pascal visualobject classes (voc) challenge. IJCV 2010;88(2):303 –38. [9] S. Zhao, X. Yue, S. Zhang, B. Li, H. Zhao, B. Wu, R. Krishna, J. E. Gonzalez, A. L.Sangiovanni-Vincentelli, S. A. Seshia, et al., A review of single-source deepunsupervised visual domain adaptation, TNNLS.[10]Long M, Cao Y, Wang J, Jordan M. Learning transferable features with deepadaptation networks. In: ICML, PMLR; 2015. p. 97 –105.[11] W. Zellinger, T. Grubinger, E. Lughofer, T. Natschl €ager, S. Saminger-Platz, Central moment discrepancy (cmd) for domain-invariant representation learning, arXivpreprint arXiv:1702.08811.[12]Tzeng E, Hoffman J, Saenko K, Darrell T. Adversarial discriminative domainadaptation. In: CVPR; 2017. p. 7167 –76. [13]Long M, Cao Z, Wang J, Jordan MI. Conditional adversarial domain adaptation. In:NeurIPS; 2018. p. 1640–50. [14]Bousmalis K, Silberman N, Dohan D, Erhan D, Krishnan D. Unsupervised pixel-leveldomain adaptation with generative adversarial networks. In: CVPR; 2017.p. 3722–31.[15]Hoffman J, Tzeng E, Park T, Zhu J-Y, Isola P, Saenko K, Efros A, Darrell T. Cycada:cycle-consistent adversarial domain adaptation. In: ICML, PMLR; 2018. p. 1989 –98. [16]Inoue N, Furuta R, Yamasaki T, Aizawa K. Cross-domain weakly-supervised objectdetection through progressive domain adaptation. In: CVPR; 2018. p. 5001 –9. [17]Ghifary M, Kleijn WB, Zhang M, Balduzzi D, Li W. Deep reconstruction-classiﬁcation networks for unsupervised domain adaptation. In: ECCV. Springer;2016. p. 597–613.[18]Xu J, Xiao L, L/C19opez AM. Self-supervised domain adaptation for computer visiontasks. IEEE Access 2019;7:156694 –706. [19]Kim S, Choi J, Kim T, Kim C. Self-training and adversarial backgroundregularization for unsupervised domain adaptive one-stage object detection. In:ICCV; 2019. p. 6092–101. [20]Chen Y, Li W, Sakaridis C, Dai D, Van Gool L. Domain adaptive faster r-cnn forobject detection in the wild. In: CVPR; 2018. p. 3339 –48. [21]P/C19erez-Hern/C19andez F, Tabik S, Lamas A, Olmos R, Fujita H, Herrera F. Objectdetection binary classiﬁers methodology based on deep learning to identify smallobjects handled similarly: application in video surveillance. Knowl Base Syst 2020;194:105590.[22]Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S,Courville A, Bengio Y. Generative adversarial nets. In: NeurIPS; 2014. p. 2672 –80. [23]Xie R, Yu F, Wang J, Wang Y, Zhang L. Multi-level domain adaptive learning forcross-domain detection. In: ICCV workshops; 2019 . [24] M. Fu, Z. Xie, W. Li, L. Duan, Deeply aligned adaptation for cross-domain objectdetection, arXiv preprint arXiv:2004.02093.[25]Xu C-D, Zhao X-R, Jin X, Wei X-S. Exploring categorical regularization for domainadaptive object detection. In: CVPR; 2020 . [26]Chen C, Zheng Z, Ding X, Huang Y, Dou Q. Harmonizing transferability anddiscriminability for adapting object detectors. In: CVPR; 2020 . [27]Zheng Y, Huang D, Liu S, Wang Y. Cross-domain object detection through coarse-to-ﬁne feature adaptation. In: CVPR; 2020 . [28]Zhu J-Y, Park T, Isola P, Efros AA. Unpaired image-to-image translation using cycle-consistent adversarial networks. In: ICCV; 2017. p. 2223 –32. [29]Heusel M, Ramsauer H, Unterthiner T, Nessler B, Hochreiter S. Gans trained by atwo time-scale update rule converge to a local nash equilibrium. In: NeurIPS; 2017.p. 6626–37.[30]Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inceptionarchitecture for computer vision. In: CVPR; 2016. p. 2818 –26.K. Fujii, K. Kawamoto Array 11 (2021) 100071
8