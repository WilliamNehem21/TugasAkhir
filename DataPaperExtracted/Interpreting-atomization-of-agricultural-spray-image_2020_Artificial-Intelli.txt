Interpreting atomization of agricultural spray image patterns using latentDirichlet allocation techniques
Hongfei Lia, Steven Cryerb,⁎,J o h nR a y m o n db, Lipi Acharyab
aDepartment of Statistics, University of Illinois at Urbana-Champaign, Champaign, IL, USA
bData Science, Corteva Agriscience, Indianapolis, IN, USA
abstract article info
Article history:Received 30 January 2020Received in revised form 24 October 2020Accepted 25 October 2020Available online 28 October 2020Breakup patterns of agricultural formulations are explored using unsupervised learning techniques to elucidatethe mechanics of atomization for oil-in-water formulations. Previous researchers have shown these formulationssuccumb to a different breakup mechanism than conventional formulations, beginning with inhomogeneitieswithin the liquid sheet that nucleate holes within the material being sprayed, beginning the mechanism respon-sible for breaking the sheet into droplets. The Latent Dirichlet Allocation (LDA), a Bayesian hierarchical model, isused to explore unsupervised learning relationships between image analysis metrics on spray video data and theresulting atomization droplet size. Latent factors discovered by LDA were used for classi ﬁcation of video seg- ments and achieved 99.9% accuracy (3-fold cross validation). Seventy- ﬁve videos were used for regression where each video had a unique measured droplet size distribution (D10, D50, and D90 values) for atomization.Experiments using the features learnt by Latent Dirichlet Allocation used with regression have extremely goodresults (R
2~ 0.995 in 3-fold cross validation, R2~ 0.963 on never-seen videos), which serves as evidence for the potential use of this model in image analysis of agricultural spray patterns. LDA has huge potential in bothlearning and predicting atomization patterns [e.g., driftable ﬁnes (drops <150μm)] when used with images based on the breakup phenomena in agricultural spray. These small drop sizes that occur during atomizationhave the greatest propensity for off-target movement through wind induced drift. LDA proved useful in charac-terizing current and future formulation designs using only images as witnessed by observations and excellentpredictions summarized in this paper. In fact, these methods offer potential for use under ﬁeld conditions to ad- dress spray performance based upon images of spray patterns at the nozzle without the need for expensive lightscattering equipment often used to measure this phenomenon.© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionFlat-fan and air-induction hydraulic nozzles, mounted on spraybooms, are the primary tools used to deliver agrochemicals to their tar-get location. Nozzles are mounted on delivery booms for both groundand aerial delivery systems. Pesticide mixtures being sprayed leavethe nozzle and begin as a liquid sheet which fans outward from the noz-zle and eventually breaks up into liquid drops. Instabilities decomposethe sheet into droplets by a variety of likely mechanisms, a phenome-non known as spray atomization (Dorman (1952)). Agrochemical spray drift is an increasingly important ﬁeld of study due to increased environment stewardship, legal liabilities, and the in-direct costs of off-target effects (Palardy and Centner, 2017;Moeller, 2018;Viera et al., 2019). The resulting drop size greatly inﬂuences how well the sprayed chemical is delivered to the target. The ultimatesize of the drops dictates both efﬁcacy and potential off targetmovement (e.g., drift). Attempts at the mechanistic understandingphenomena of spray drift can be found elsewhere ( Squire, 1953; Barlow et al., 2011;Altieri et al., 2014;Cryer and Altieri, 2017;Altieri and Cryer, 2018).Atomized drop size greatly inﬂuences the chemical effectivenesswhen delivered to its target. Large (coarse) drops may not effectivelycover the target or may bounce upon impact with foliage. This iscontrasted by smaller drops that risk becoming entrained in ambientair currents and carried off target, known as droplet drift. These smalldrops that are susceptible to off target movement are known as driftableﬁnes. Driftableﬁnes is not a standardized quantity but it is on the orderof drop diameters≤150μm(Cloeter et al., 2010). Experimental observations for spray patterns for multiphase emul-sions are also reported (Li et al., 2020;Altieri et al., 2014) and used for comparison and training in unsupervised learning techniques. LatentDirichlet Allocation (LDA) models are used in this analysis which differfrom deep learning approaches and constitute a generative statisticalmodel that allows sets of observations to be explained by unobservedgroups that elucidate why parts of the data are similar.Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
⁎Corresponding author.E-mail address:steven.cryer@corteva.com(S. Cryer).
https://doi.org/10.1016/j.aiia.2020.10.0042589-7217/© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the C C BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/2. Methods and materials2.1. DataThe droplet size distribution emanating from the nozzle was ob-tained using a Sympatec laser diffraction system shown in Fig. 1(Fritz and Hoffmann, 2016). This liquid sheet area, from the nozzle tip to thedistance where the sheet breaks up into drops, was imaged by video.Spray images videos were taken by a high-speed camera (Photron®Fastcam SA3 high-speed camera and a Nikon ED AF Nikkor®80–200 mm lens) with 1024 × 1024 pixels resolution at 2000 framesper second.The dataset consists of 83 videos belonging to 44 classes. Videos ofthe same class are recordings of repeated trials with exactly the sameformulations, pressure and nozzles. Each video contains around 3000frames and a total of 263,951 frames were obtained across all videos.Since videos were taken at different times with different nozzles,shape, size, and position of nozzles as well as brightness of imagescould change. To standardize those videos, every frame waspreprocessed. Nozzles and headers at the top of each frame werealigned and then cropped from each image frame. The background ofthe centered spray patterns was denoised. Finally, each frame wasturned into gray-scale fooled by conversion into black-and-white only(i.e. intensity of a pixel is either 0 or 255) since intensity variance isnot relevant in our case. Example of a standardized frame is providedinFig. 2(b) along with the raw frame inFig. 2(a).2.2. Latent Dirichlet allocationDeep learning models using the same dataset are reported in Li et al. (2020). Better overall performance is anticipated when using averagingmethods if models are uncorrelated. Latent Dirichlet Allocation Modelsdiffer from deep learning approaches, constituting a generative statisti-cal model that allows sets of observations to be explained by unob-served groups that elucidate why some part of the data are similar.LDA wasﬁrst introduced for natural language processing by Blei et al.(2003). It is a three-level hierarchical Bayesian model, in which eachitem of a collection is modeled as aﬁnite mixture over an underlying set of topics, and each topic is modeled as an in ﬁnite mixture over an underlying set of topic probabilities. A graphic model of LDA is showedinFig. 3. Here, plate notation is a method of representing variables thatrepeat in a graphical model in Bayesian inference. A plate (or rectangle)is used to group variables into a subgraph that repeat together, and anumber is drawn on the plate to represent the number of repetitionsof the subgraph in the plate.According toBlei et al. (2003), an easy way to understand LDA is tolook at the processes it assumes for each document within adocument-term matrix (e.g., corpus).1. ChooseN~ Poisson (ξ)2. Chooseθ~D i r(α)3. For each of the N wordsw
n:a. Choose a topicz
n~ Multinomial(θ)b. Choose a wordw
nfromp(w n|zn,β), a multinomial probability con-ditions on the topicz
n.To put it simply, topics are distributions over words, and documentsare distributions over topics. Suppose we have information from manydocuments that talk about news on different topics such as social,sport,ﬁnance, politics, etc. Some words (e.g. football, win, score) aremore likely to appear in sport news than ﬁnancial news. It is natural to assume that, each topic consists of different set of words, or more ac-curately, each topic corresponds to a different probability distribution ofwords. Moreover, a document might contain several topics such asnews that talks about an event that is related to both ﬁnance and law. LDA captures this by assuming each document corresponds to a combi-nation of topics (e.g., 20% political, 50%ﬁnancial, 30% social, and so on).An image can also be treated as a document when it comes to imageprocessing. For example, an image of a natural scene is a document. Thisscene may consist of 30% sky, 20% sea, and 50% sand, where sky, sea, andsand are topics. The topic sky might assign higher probability to visual
Fig. 1.The experimental spray chamber, nozzle system, Sympatec laser, and high-speed video camera used to measure the atomization drop size distribution a nd imaging the spray sheet. Experimental setup at Corteva Indianapolis site.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
254words that are white or blue, while the topic sand might assign higherprobability of visual words that are yellow.As a result, although originally a language model, LDA has beenbrought into and widely used in computer vision. Applications of LDA in-clude object recognition (Sivic et al., 2005,Fei-Fei and Perona, 2005, Russell et al., 2006,Cao and Fei-Fei, 2010), natural scene classiﬁcation (Fei-Fei and Perona, 2005), human action classiﬁcation (Niebles et al., 2008), activity perception in complicated scene ( Wang et al., 2007), etc. Variants of LDA were developed for different tasks such as Spatial LDA(Wang and Grimson, 2008) for capturing spatial structures and Corre-spondence LDA (Blei and Jordan, 2003) for image annotation. In this work, we apply LDA to spray pattern images to obtain interpretable un-derlying features that can be subsequentially used in supervised learningtasks.2.3. Feature construction2.3.1. Bag of wordsWords, vocabulary, and documents are required to apply LDA. Thus,visual wordsare extracted (which are local areas of images) in handlingimage data. For example, in natural scenes, there is likely to be stones,mountains, sky, sun, trees, and so on. These objects are characterizedas visual words, and they form the vocabulary of natural scenes. Fei- Fei and Perona (2005)sample local regions from images by severalmethods (including evenly grid sampling) and use a k-means algorithmto cluster these local regions to get the vocabulary. In this way, a localregion of an image corresponds to a word, and an image, which is a col-lection of words, becomes a document. Other approaches employ theuse of super-pixels to over-segment images in regions of related pixels(Chen et al., 2017;Qiaojin et al., 2011).A video is a sequence of images, and in some cases human actionrecognition (e.g., spatial-temporal information) is critical. Niebles et al. (2008)represented each video sequence as a collection ofspatial-temporal words by extracting space-time interest points.However, human actions are highly structured and non-random,while the spray videos consist of ra ndom, unstable image textures, white dots and lines. Static instances of an image spray pattern tex-tures are insufﬁcient to properly categorize the information contentof such dynamic, highly random phenomena. It is the temporal dis-tribution of these image texture regions over the course of a videostream that provide discriminability. Thus, we treat videos as bagsof visual words. Each frame is an unordered collection of visualwords, and sequence of frames are still an unordered collection of vi-sual words. A sequence of frames are the collections of all visualwords that appear in any of these frames. We count how manytimes a word appears in a document, and a document is then repre-sented by the counts of words (known as a document-word matrix).A bag-of-words model is often used in document classi ﬁcation where the frequency of occurrence for each word is a feature fortraining a classiﬁer (McTear et al., 2016;Harris, 1954). For example, suppose we have two documents. The ﬁrst document is “James likes apples and Emma likes oranges. ”And the second one is “James hates oranges.”We can represent the two documents as twobags of words (BoW):
BoW1 = {“James”:1,“likes”:2,“apples”:1,“and”:1,“Emma”:1, “oranges”:1};BoW2 = {“James”:1,“hates”:1,“oranges”:1}; and the document-word matrix is constructed as follows:
Fig. 2.Single raw frame example extracted from a video showcasing the header and nozzle requiring removal (red boundary) and centering image (green boundar y) (a), followed by denoising and conversion to a black-and-white only image (b).
Fig. 3.Latent Dirichlet Allocation graphic model (taken from Blei et al. (2003)). Nodes are random variables where shaded nodes are observed and unshaded ones are unobserved.The plates indicate repetitions. The outer plate represents documents, while the innerplate represents the repeated choice of topics and words within a document.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
255James likes hates apples and Emma orangesDocument1 1 2 0 1 1 1 1Document2 1 0 1 0 0 0 1
2.3.2. CodebookSimilarly, we can construct a document-word matrix for images bythinking of an image as a document and local regions of an image aswords. We cut images into small local regions, use k-means to get a dic-tionary of visual words, and then form a bag of words representing a se-quence of images. All images are downsampled to 500 × 500 pixels forLDA. Each image is cut into 10 × 10 pixels small patches and we removeall patches that are completely empty (i.e., if every pixel in a patch is 0, itcontains the black background only).Fifteen hundred frames are randomly sampled (due to memory lim-itations and the relative simplicity of the elements in our images) to per-form k-means. A vocabulary size of 200 is selected (i.e., 200 clustercenters for the k-means) and we get a dictionary containing 200 visualwords of our“language”following the k-means step. We also obtain an-other 50 visual words using pyramid features extraction methoddescribed in section 2.3.3.Fig. 4illustrates the codebook that is learnedfrom the frames, and we use the 250 visual words in the codebook todescribe the images. We assign each local patch sampled from a frameto the visual word that is most similar. In this way, an image becomesa bag of visual words from the codebook.2.3.3. Pyramid featuresSince we resize each frame to 500 × 500 pixels and sample 10 × 10pixels local patches from these frames, each patch contains a verysmall portion of information in the original frame. One patch mightcontain a dot or part of a line (i.e., a very low-level feature about theimage). However, we would also like to have features that describeshigher level information about the image, (such as the spray angle ofsheets), and we adopt the feature extraction method used byLazebnik et al. (2006). After we extract local patches from 500 × 500pixels images, we shrink each image to 100 × 100 pixels and cutthem into 10 × 10 pixels local patches again. In this way, one patchcan contain more information in relation to the size of the image(i.e., higher-level features such as the angle of sheets in addition to adot or a short line).
Fig. 4.Codebooks sorted by frequencies. Left (a) is the codebook with 200 low level visual words. Right (b) is the codebook with 50 high level visual words.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
256Fig. 5illustrates the approach of extracting spatial pyramid featuresfrom an image of spray emanating from a nozzle in order to obtain our codebook of high-level visual words (right side, Fig. 4). We extract low- level features from high-resolution images and extract high-level
Fig. 5.Pyramid features extraction. Low level patches (a) from high resolution images of atomization process. High level patches (b) from low resolution im ages.
Fig. 6.Topic proportion of four example videos. Each subplot is a video that consists of many video segments. Each line in the plot is the topic proportion of a v ideo segment. The top (a) and bottom (b) video each belongs to different classes.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
257features from low-resolution images. We ﬁt other k-means to these higher-level patches, and learn a new codebook, where the vocabularysize is selected as 50. These 50 words and the previous 200 words con-sists of the complete vocabulary of size 250. Every frame is turned into abag of these visual words. Each word is counted to construct thedocument-word matrix.3. Results3.1. Clustering results and insightsData consists of 83 high speed videos belonging to 44 classes (wheremost classes contain two videos). Most videos contain around 3000frames. Each frame is transformed into black-and-white image. Wesplit each video into smaller video segments where each video segmentconsists of 25 consecutive frames. We treat a video segment as a docu-ment (instead of a frame as a document) where most videos containaround 100 video segments. Most frames sizes are approximately1000 × 1000 pixels, but we resize them into 500 × 500 pixels.The dataset is split randomly into a small training set (for learningthe codebook) and a test set.Approximately 200 low level visual words and 50 high level visualwords are learned in training. We set the number of topics of the LatentDirichlet Allocation model as 44. Although we use same number oftopics as classes, we do not expect one-to-one correspondence betweentopics and classes. Usually, a class is de ﬁned by multiple dominant topics.The unnormalized topic proportion for each document, or video seg-ment, is obtained afterﬁtting the Latent Dirichlet Allocation model. Aclass is deﬁned by its unique topic proportions, and this topic proportionis used to analyze similarity and difference between classes. The topicproportion for two example videos is provided in Fig. 6.E a c hl i n ei n the plot is the topic proportion of a video segment. Each subplot con-tains video segments that come from the same video. We can see thatvideo segments that come from the same video have very similartopic proportions. A video class represents the formulation attributesthat lead to how the material breaks up into droplets and the resultingdrop size. And the topic proportion of video segments of this class arealmost the same. The top and bottom video each belongs to differentclasses,Fig. 6. Thus, the topic proportions are different.The top two principle components (PCA) of topic proportion of eachvideo segment are represented inFig. 7, with different colors indicating different classes. PCA produces a low-dimensional representation of adataset byﬁnding a sequence of linear combinations of the variable hav-ing maximum variance that are mutually uncorrelated and serves as atool for data visualization. A dot is a video segment. A cloud is a video.FromFig. 7, one sees that video segments of the same class are mostlygrouped together.
Fig. 7.Principle Components Analysis of topic proportion of each video segment using the ﬁrst two principle components. A dot is a video segment. A cloud is a video. Different color indicating different classes.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
258Fig. 8shows the dendrogram of classes by Euclidian distance of theirtopic proportions (i.e., a tree diagram used to illustrate the arrangementof the clusters produced by hierarchical clustering). A dendrogram isuseful to explore the numbers of various clusters that form between dif-ferent video classes with similar formulations clustering close to one an-other. However, often dramatically different formulations can clustertogether (similar results for PCA).Topics are distributions over words and one can further examinewhat these topics are. A crude idea of topic descriptions is determinedby looking at the high frequency words of a certain topic. In Fig. 9,t h e majority ofﬁrst topic is dots, while polygons, or holes, appear in the sec-ond topic more frequently. The third topic is about vertical lines, and thelast example shows lines of different orientations. If a class has a highproportion of theﬁrst topic and low proportion of the last topic, theremight be more dots than lines in the videos of this class.3.2. Video segment classiﬁcationLatent Dirichlet Allocation discovers the topic distribution which de-ﬁne classes in an unsupervised fashion. Each class has a unique topicproportion. Supervised models can be built if topic proportions aretaken as a feature vector that describes each class. Linear DiscriminantAnalysis is chosen to perform classiﬁcation using topic distributions. There is a total of 10,461 video segments, 83 videos, 44 classes for thespray atomization videos. Video segments originating from the samevideo share the same class label. Unique classes represent video obser-vations that yield similar behavior.The topic proportion of each video segment is used as its features.Even when running videos at steady state, the dynamic nature for theﬂuid sheets suggest that it is possible for video segments to bemisclassiﬁed into different classes depending upon where in the processthe video segments is taken. Thus, the dataset is ﬁrst split into training and testing sets by video segments (i.e., 2/3 of all video segments aretrain data, 1/3 of all video segments are test data). A Linear DiscriminantAnalysis model is trained via the training set, and predictions for theclass of video segments (25 frames) are analyzed in the test set. Accu-racy of 3-fold cross validation is 99.9%. Table 1shows the high cross- validation accuracy of the classiﬁcation for this approach. A possibleexplanation to the high accuracy may be due to having multiple videosegments for a single video.For further insights into generalizability of these features, the datasetis then split into train and test set by videos. There are 31 classes con-taining two or more different videos. For those classes, video segmentscoming from one video are in train set, and video segments coming fromanother video are in test set. This approach results in 5742 video seg-ments in train set and 4719 video segments in test set. Again, a LinearDiscriminant Analysis model is used to classify these video segments,and it achieved 85.9% accuracy.Table 1summarizes the classiﬁcation results. The high classiﬁcation accuracy shows that the topic distributions extracted by Latent Dirichlet
Fig. 8.Dendrogram of classes by Euclidian distance of their topic proportions.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
259Allocation successfully captured and summarized each video segmentand proved to be good features for classiﬁcation.3.3. Drop size predictionFeature vectors are also used to do regression in addition to classi ﬁ- cation. Only 75 of the 83 videos utilized for Latent Dirichlet Allocationhad atomization drop size distributions summarized for three measure-ments (i.e., D10, D50, and D90 values representing drop size particle di-ameter corresponding to 10, 50 and 90% cumulative), leaving 9621video segments (75 videos) for the regression task. The independentvariables are the topic proportion of each video segments (smaller sub-set of main video) and are different for each video segments (althoughthe difference is extremely small if two video segments are derived fromthe same video). The dependent variables are the three measurements,D10, D50, and D90.Data is split into a train and test set by video segments (e.g., 2/3 of allthe video segments are train data, and 1/3 of all video segments are testdata). A polynomial regression with degree 2 is ﬁt to the train set and make predictions on test set.Table 2summarizes the result. The average
Fig. 9.Top 25 words of 4 example topics. Topic in (a) mostly consists of dots. Topic in (b) consists of polygons and holes. Topic in (c) consists of vertical line s and dots. Topic in (d) consists of horizontal lines, slashes and dots.
Table 1Classiﬁcation accuracy with Linear Discriminant Analysis.Split method Average classi ﬁcation accuracy3-fold cross validation by video segments 99.9%Split by videos 85.9%Table 2Drop Size Prediction.Model Mean squarederrorExplainedvarianceLatent Dirichlet Allocation + Polynomial Regression(3-fold CV) 51.6 99.5%CNN + RNN (Li et al., 2020) 39.5 99.6%H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253 –261
260R-squared value is 99.5% for a 3-fold cross validation. We also list the re-sult of a deep learning regression model that uses the same videos andD10, D50, D90 data reported earlier for comparison ( Li et al., 2020). The train-test split ratio is 2:1 for both models.In addition, similar to video segment classi ﬁcation, the dataset is also split into train and test sets by videos, where video segments com-ing from the same video are either in train set or test set. This ap-proach results in 5550 video segments in train set and 4071 videosegments in test set. A polynomial regression with degree 2 is used,mean squared error of test set is 361.2, and R-squared value of test set is 96.3%.4. ConclusionsThe mechanism for delivering many pesticides to the target site isby atomization of spray using hydraulic nozzles. Atomization showsrich diversity and is a function of formulation type, composition, thenumber of phases within the formulation, spray pressure, nozzle ge-ometry, and so on. Breakup patterns from spray nozzles are exploredusing unsupervised learning techniques to elucidate the mechanics ofatomization for oil-in-water formulations. The Latent Dirichlet Alloca-tion (LDA), a Bayesian hierarchical model, is used to perform unsuper-vised learning on video spray data for agricultural formulations. TheLDA model discovers and learns about the latent factors for the datato provide insight on information that is much more interpretablethan that obtained via black box methods. Latent factors discoveredby LDA were used for supervised learning tasks including classi ﬁcation and regression.A Linear Discriminant Analysis model was able to classify video seg-ments with 99.9% accuracy (3-fold cross-validation) based on those la-tent factors. Seventy-ﬁve videos were used for regression where eachvideo had a unique measured droplet size distribution (D10, D50, andD90 values) for atomization. The primary experiments using the fea-tures learnt by Latent Dirichlet Allocation used with regression (3-foldcross-validation) have extremely good results ( R
2~ 0.995). Further ex- periments where never-before-seen videos were held out as test setstill give good results (R
2~0 . 9 6 3 ) ,w h i c hs e r v e sa se v i d e n c ef o rt h ep o -tential use of this model in image analysis of agricultural spray patterns.The Latent Dirichlet Allocation model offers huge potential to learn andpredict atomization patterns (e.g., especially dri ﬁtableﬁnes) when used with images based on the multiphase breakup phenomena witnessed inagricultural spray drift. Small drops of sizes <150 μm offer the greatest propensity for off-target movement via wind induced drift, and LDAmethods can be used to offer insight into the drop sizes manifest withspraying oil-in-water formulations. Images can be obtained from videosobtained in spray chambers (this work) or from real time videos ofspray phenomena at the source along a boom in ﬁeld applications. Thus, process control can come into play to keep drop sizes within atargeted range (e.g., spray performance).Author StatementHongfei Li was a principle author involved in all aspects of thiswork. She was a Master's Student in Statistics at the University of Il-linois Urbana Champaign at the time of this work. Hongfei was su-pervised by Lipi Acharya and Steven Cryer of Corteva Agriscience(Data Science and Informatics) when she was ﬁnishing her Master's degree at UIUC. Dr. Acharya and Dr. Cryer provided the necessaryideas, support, feedback, guidance, and interactions for this work. Dr. John Raymond of Corteva Agriscience also provided technicalguidance.Declaration of Competing InterestNo conﬂict of interest exists.AcknowledgementsDr. Navin Elango provided critical review and feedback of this man-uscript. Hongfei Li was a master's student at the University of Illinois,Urbana-Champaign (UIUC) working with Corteva Agriscience as an in-tern at the time this work was undertaken. All research was funded byCorteva Agriscience.Appendix A. Supplementary dataSupplementary data to this article can be found online at https://doi. org/10.1016/j.aiia.2020.10.004.ReferencesAltieri, A.L., Cryer, S.A., 2018.Break-up of sprayed emulsions from ﬂat-fan nozzles using a hole kinematics model. Biosystems Engineering 169, 104 –114. Altieri, A., Cryer, S.A., Acharya, L., 2014. Mechanisms Experiment and Theory of Liquid Sheet Breakup and Drop Size from Agricultural Nozzles. Atomization and Sprays 24(8), 695–721.Barlow, N.S., Helenbrook, B.T., Lin, S.P., 2011. Transience to instability in a liquid sheet. J. Fluid Mech. 666, 358.Blei, D.M., Jordan, M.I., 2003.Modeling annotated data. Proceedings of the 26th annual in-ternational ACM SIGIR conference on Research and development in information re-trieval, pp. 127–134.Blei, D.M., Ng, A.Y., Jordan, M., 2003. Latent dirichlet allocation. J. Mach. Learn. Res. 3 (4–5), 993–1022.Cao, L., Fei-Fei, L., 2010.Spatially coherent latent topic model for concurrent object seg-mentation and classiﬁcation. Proceedings of IEEE Intern. Conf. in Computer Vision(ICCV), p. 2010.Chen, C., Zare, A., Trinh, H.N., Omotara, G.O., Cobb, J.T., Lagaunne, T.A., 2017. Partial Mem- bership Latent Dirichlet Allocation for Soft Image Segmentation. IEEE Transactions onImage Processing 26 (12), 5590 –5602. Cloeter, M.D., Qin, K., Patil, P., Smith, B., 2010. Planar Laser Induced Fluorescence (PLIF) ﬂow visualization applied to agricultural spray nozzles with sheet disintegration; In-ﬂuence of an oil-in-water emulsion. ILASS Americas 22
ndAnnual Conference on Liq- uid Atomisation and Spray Systems, Cincinnati, OH, May 2010.Cryer, S.A., Altieri, A.L., 2017. Role of large inhomogeneities in initiating liquid sheetbreakup in agricultural atomization. Biosyst. Eng. 1693, 103 –115. Dorman, R.G., 1952.The atomization of liquid in aﬂat spray. Br. J. Appl. Phys. 3, 189. Fei-Fei, L., Perona, P., 2005.A bayesian hierarchical model for learning natural scene cat-egories. Computer Vision and Pattern Recognition, CVPR 2005. IEEE Computer SocietyConference. 2, pp. 524 –531.Fritz, B.K., Hoffmann, W.C., 2016. Measuring Spray Droplet Size from Agricultural NozzlesUsing Laser Diffraction. J. Vis. Exp. 115, e54533. https://doi.org/10.3791/54533 . Harris, Z., 1954.Distributional structure. Word 10 (23), 146 –162. Lazebnik, S., Schmid, C., Ponce, J., 2006. Beyond bags of features: Spatial pyramid matching for recognizing natural scene categories. 2006 IEEE computer society con-ference on computer vision and pattern recognition (CVPR ’06). 2, pp. 2169–2178. Li, H., Cryer, S., Acharya, L., Raymond, J., 2020. Video and image classiﬁcation using atomisation spray image patterns and deep learning. Biosyst. Eng. 200, 13 –22. McTear, M.F., Callejas, Z., Griol, D., 2016. The conversational Interface, talking to smart de- vices. 6, no. 94. Springer, Cham, p. 102.Moeller, D.L., 2018.Superfund, pesticide regulation, and spray drift: rethinking the Fed-eral Pesticide Regulatory Framework to provide alternative remedies for pesticidedamage. Iowa L. Rev. 104, 1523.Niebles, J.C., Wang, H., Fei-Fei, L., 2008. Unsupervised learning of human action categories using spatial-temporal words. Int. J. Comput. Vis. 79 (3), 299 –318. Palardy, N., Centner, T.J., 2017. Improvements in pesticide drift reduction technology(DRT) call for improving liability provisions to offer incentives for adoption. LandUse Policy 69, 439–444.Qiaojin, G., Ning, L., Yubin, Y., Ganghan, W., 2011. Supervised LDA for Image Annotation. 2011 IEEE international conference on systems, Man, and Cybernetics.Russell, B.C., Freeman, W.T., Efros, A.A., Sivic, J., Zisserman, J.A., 2006. Using multiple seg- mentations to discover objects and their extent in image collections. 2006 IEEE Com-puter Society Conference on Computer Vision and Pattern Recognition (CVPR ’06). Vol. 2, pp. 1605–1614.Sivic, J., Russell, B.C., Efros, A.A., Zisserman, A., Freeman, W.T., 2005. Discovering object categories in image collections. Computer Science and Arti ﬁcial Intelligence Labora- tory Technical Report, MIT-CSAIL-TR-2005-012, Feb 25 AIM-2005-005.Squire, H.B., 1953.Investigation of the instability of a moving liquid ﬁlm. Brit. J. Appl. Phys. 4, 167–169.Viera, B.C., Luck, J.D., Amundsen, K.L., Gains, T.A., Werle, R., Kruger, G.R., 2019. Response of Amaranthus spp. following exposure to sublethal herbicide rates via spray particledrift. PloS one 14 (7), e0220014.Wang, X., Grimson, E., 2008.Spatial latent Dirichlet allocation. Adv. Neural Inf. Proces.Syst. 20, 1577–1584.Wang, X., Ma, X., Grimson, E., 2007. Unsupervised activity perception by hierarchical bayesian models. 2007 IEEE conference on computer vision and pattern recognition.IEEE., pp. 1–8.H. Li, S. Cryer, J. Raymond et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 253–261
261