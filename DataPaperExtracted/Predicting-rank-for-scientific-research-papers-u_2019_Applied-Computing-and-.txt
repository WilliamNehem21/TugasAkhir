Predicting rank for scientiﬁc research papers using supervised learning
Mohamed El Mohadab⇑, Belaid Bouikhalene, Said Saﬁ
Department of Mathematics and Informatics, Laboratory of Innovation in Mathematics and Application and Information Technologies (LIMATI), Polyd isciplinary Faculty, Sultan Moulay Slimane University, PB 592 Beni Mellal, Morocco
article info
Article history:Received 10 September 2017Revised 14 February 2018Accepted 17 February 2018Available online 6 March 2018Keywords:Scientiﬁc researchRanking scientiﬁc research papersData miningSupervised learningMultilayer perceptron algorithmabstract
Automatic data processing represents the future for the development of any system, especially in scien-tiﬁc research. In this paper, we describe one of the automatic classiﬁcation methods applied to scientiﬁcresearch as a supervised learning task. Throughout the process, we identify the main features that areused as keys to play a signiﬁcant role in terms of predicting the new rank under the supervised learningsetup. First, we propose an overview of the work that has been realized in ranking scientiﬁc researchpapers. Second, we evaluate and compare some of state-of-the-art for the classiﬁcation by supervisedlearning, semi-supervised learning and non-supervised learning. During the preliminary tests, we haveobtained good results for performance on realistic corpus then we have compared performance metrics,such as NDCG, MAP, GMAP, F-Measure, Precision and Recall in order to deﬁne the inﬂuential features inour work./C2112018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionDue to the fast development of information and communica-tions technologies, the university has ﬁrmly decided to facilitatethe access and treatment for all processes, especially in scientiﬁcresearch in order to assist PhD students, professors and adminis-trative staff to deal with digital services that they need.In recent years, research in ranking scientiﬁc research papers(SRP) from diversiﬁed ﬁelds of research has become a very impor-tant task because of the exponential growing of daily publication injournals and conferences, exceeding 50 million papers. Also, pre-dicting the future of any system represents another challenge thatwe can face generally, but mixing both problems is the case thatwe address in this research by predicting the new rank of scientiﬁcresearch paper.Using machine learning[1]in ranking scientiﬁc research papersis a crucial research direction, because it contains distinct classes ofsupervised learning algorithms[2]with regard to prediction. Between the main important algorithms used in linear classiﬁers,we choose to work with Multilayer Perceptron Algorithm [3],SMO Classiﬁer[4], and Kstar Classiﬁer[5]. The three classiﬁers rep- resent the neuron network with nodes and edges as papers andcitations between the different authors in a similar set of informa-tion. The major reason for this choice is that a research paper net-work is a concrete example of the relation where the researchercollaborates with other research communities in the scientiﬁcdomains in order to achieve their goals.The related work gives us a vision on the approaches and meth-ods for the classiﬁcation of scientiﬁc research papers, and which isgrouped into two major axes: the ﬁrst axis is ranking according tothe query and the second is ranking according to the technicalanalysis link. The limitation of these two main axes classiﬁes theexisting papers to us but it does not propose a contribution con-cerning the future classiﬁcation. The novelty brought in this workis manifested through the prediction of the future classiﬁcationbeing based on the existing papers that will offer the researcherthe paper with the highest rank in this ﬁeld.The rest of this paper is organized as follows: in the next sectionwe review the related work in ranking scientiﬁc research papers.Section3shows the state of the art for the learning methods. Sec-tion4describes Methods. Section5shows Results and discussion. Finally, in section6, we conclude and describe future researchdirections.2. Related workIn the last few years, there has been a growing interest in rank-ing scientiﬁc research papers as one of the pillars of research in
https://doi.org/10.1016/j.aci.2018.02.0022210-8327//C2112018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author.E-mail address:m.elmohadab@gmail.com(M. El Mohadab). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 15 (2019) 182–190
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
ranking in general. Among the most used tools to rank journals, weﬁnd the impact factor[6], which is an approximation of the aver-age number of citations within a year given to the set of papersbelonging to a journal published in the two preceding years.Previous studies in ranking indicate that we have two axes; theﬁrst axis represents the relevance ranking algorithm whichmatches the ranking according to the query. Among the studieswe ﬁnd:/C15Vector Space Model[7]: is the way of representing text docu-ments as vectors of identiﬁers. It is used in information retrie-val, information ﬁltering, and relevancy and indexing ranking./C15Latent semantic analysis[8]: is the way of analyzing the rela-tion between a set of concepts related to the term and a set ofdocuments. This technique is used in natural languageprocessing./C15Okapi BM25[9]: is the way of matching documents by a rankingfunction in search engines according to their relevance with asearch query. It is totally based on the probabilistic retrievalframework./C15Boolean Ranking Model[10]: is the way of searching the user’squery in the existent set based on classical set theory and Boo-lean logic.The second axis represents an important ranking model whichranks according to the link analysis technique. Among the modelswe ﬁnd:/C15HITS[11]: is an analysis algorithm, the basic idea is that a webpage serves two purposes: to provide information and or to sug-gest links to pages on a topic./C15PageRank[12]: is an algorithm working by calculating the num-ber of links to a page and also their quality to determine a closeestimation of the importance of the relevant documents.Most algorithms used to rank SRP are divergent from PageRankor HITS, we ﬁnd:/C15Topic Rank[13]: clusters papers into topics; between the mainfactors used, we have: topic, citation, date of publication, title,and keyword./C15Cite Rank[14]: is an algorithm working by ranking citation net-works based on their topology, between the main factors used,we have: citation, title, and date of publication./C15PTRA[15]: gives the paper age a higher impact, and dependshighly on time of publication to rank the papers; among themain factors used, we have: citation, publication venue, andpublication date.However, all researchers have concluded that both types ofranking algorithms (the relevant/the important) have some limita-tions, especially the relevance algorithm that is not used any morein ranking algorithms. However, studies on predicting a new rankare still lacking. In the next section, we will review the state of theart for the learning methods.3. State of the art for the learning methodsIn this section, we will provide a sort of overview of what hasbeen found in the literature concerning the models which we havebeen using in our work. There are three families of differentiallearning methods: supervised learning, which requires prior label-ing of class data so that the model can train on them; unsupervisedlearning (clustering), without a prior information input; andsemi-supervised learning mode, which jointly manipulatesunlabeled and labeled data.3.1. Supervised approachSupervised Approach is an automatic learning technique whichleads automatically to produce rules from a learning database con-taining examples of cases already dealt with. Therefore, its aim isto generalize for unknown inputs that it has been apt to learn fromthe data which are already handled by experts; the purpose is touse this to determine a compact representation of the function ofprediction, which at a new input x associates an output S (x).The three main approaches related to supervised learning are:/C15Neural Networks[16]./C15Hidden Markov Model
[17]./C15Support Vector Machines[18].The Neural Networks[16]is generally deﬁned by three types ofparameters:/C15The interconnection pattern./C15The activation function./C15The learning process.The Hidden Markov Model[17]is deﬁned by two stochastic processes: a Markov chain is deﬁned by a set of states and the tran-sitions between the different states, so-called emission probabili-ties connected with each state. We will bring into focus thedecision-making process, which is described by:/C15A ﬁnite set S of discrete states denoted s./C15A ﬁnite set A of actions denoted a./C15A transition function P: S/C2A?P (S) where P (S) is the set ofdistributions of probability on S.The Support Vector Machine[18]offers, in particular, a good approximation of the fundamental of minimization of structuralrisk. The method depends on the following ideas:/C15The data is projected in a large space by a transformation basedon a linear, polynomial or Gaussian kernel./C15The classes are disconnected by linear classiﬁers that maximizethe margin in the transformed space./C15The hyper planes can be determined by means of a few pointswhich will be called ‘‘support vectors”.The Boosting[19]is summarized as follows:/C15A large set of simple features./C15Initialization weights for training sets./C15For T rounds:o Normalize the weights.o For features from the set, train a classiﬁer with a single fea-ture and examine the training error.o Determine the classiﬁer with the lowest error.o Update the weights of the training sets./C15The ﬁnal classiﬁer is the linear combination of the T classiﬁers.3.2. Unsupervised approachThere are different reasons for choosing this type of learningsuch as the charge of developing manual labeling and the searchfor discriminatory characteristics in the ﬁrst study or characteris-tics which grow over time. Unsupervised learning [20]is oftenM. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190 183treated as a density estimation problem; the two main approachesused in unsupervised learning are:/C15K-Means Clustering[21]./C15Fuzzy C-Means[22].The k-means clustering[21]is summarized as follows:/C15Place K points into the space expressed by the objects clustered;/C15Assign each object to the near centroid;/C15Recalculate the locations of the K centroids;/C15Repeat until the centroids are ﬁxed. The metric is thencalculated.The fuzzy c-means algorithm[22]is very identical to the k- means algorithm:/C15Appoint a number of clusters./C15Each point is given a random coefﬁcient./C15Repeat the algorithm until convergence.3.3. Semi-supervised approachTo perform generic tasks of supervised learning while exploit-ing some labeled data simultaneously with multiple raw data.The ﬁrst idea is using a non-supervised context of the outputs pre-dicted by the system itself in order to construct the desired outputsby applying a supervised technique. This approach is known as thedirected decision. The second idea depends on the simultaneoususe of two classiﬁers. They alternately act as a teacher and a pupilin an algorithm, iterative learning: the output calculated by onewill be taken as the appropriate output by the other and recipro-cally until convergence.The learning criterion is here to optimize the coherencebetween the two classiﬁers. This approach is known as self-supervision. The two main approaches to semi-supervised learning[23]are:/C15Co-Training[2]: It is a machine learning algorithm used whenthere are only some labeled data and large amounts of unla-beled data. One of its uses is in text mining for search engines./C15Co-Boosting[24]: It may be seen as a combination of co-trainingand boosting.Moreover, we have compared distinct machine learning algo-rithms. In our case of study, we have chosen to work with thesupervised learning approach especially with the neuron networkrepresented by the Multilayer Perceptron classiﬁer [3]; the reasons for this choice are:/C15Presentation of a drive to the network./C15Comparison of the network output with the targeted output./C15Calculation of the error at the output of each neuron belongingto the network./C15Deﬁnition of the increase or decrease required to obtain thisvalue./C15Adjustment of the weight of each connection to the lowest localerror./C15Granting blame to all previous neurons.The reasons behind choosing SMO classiﬁer [4]are:/C15Finding a Lagrange multiplier
a1 that violates the Karush–Kuhn–Tucker (KKT) conditions for the optimization problem./C15Picking a second multiplier
a2 and optimize the pair ( a1,a2). /C15Repeating steps 1 and 2 until convergence.Also, the reasons for choosing Kstar classiﬁer [5]are:/C15Kstar operates on-the-ﬂy, which means that it does not requirethe graph to be explicitly available and stored in the main mem-ory. Portions of the graph will be generated as needed./C15Kstar can be guided using heuristic functions.In the next section we will expose our new way to predict thenew rank by using supervised learning [2].4. MethodsFrom a network viewpoint, papers can be seen as nodes in anetwork and the citations between papers as edges (see Figs. 1 and 2).As we see inFig. 3, in the network, each paper node X links toanother paper node Y through citation between themes. This net-work can help us to have more information about authors, papers,type of papers, etc. Then, like all transfer a model, the score is cal-culated by the count of the number of citations will be transferredto the referenced papers. Also, we must split our data into subsec-tions to rank each paper into its division. As a case in point, we willtreat information about different research publications in the ﬁeldof computer science exclusively for Geographic InformationSystem.In this work, we take into account the following point:/C15The papers with high number of citations reﬂect the importanceand the prestige of the author./C15Scientiﬁc Gem[25]is always in the ﬁrst rank in spite of theirdate of publication as a result of their recent citations./C15Recent publication has always less citation despite their newestcontribution.In any machine, the learning algorithm determines the goodfeatures which can provide very good results. The application ofour algorithm[26]for ranking depends on:/C15Paper Posted Time: The number of years since it has been pub-lished, by the formula:o A = Current Year-Year of Publication./C15Conference Score: The quality of any conference can beexplored by the age of the conference, the continuity for theconference, the number of papers in the proceeding and theDigital Library involved.
Fig. 1.Selection of data according to the category of learning.184 M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190/C15Author Score: The number of authors of each paper, and thenumber of publications for each author is used to calculateauthor score, which is deﬁned by:
Xni¼1AiDL1NH/C18/C19
/C15Download Rates: The number of downloads from the ofﬁcialwebsite of the journal reﬂects the importance for the givenwork in the paper./C15Keywords: The order of the keywords reﬂects the topics and theinterest for the work./C15Publication Type: In general, the papers that are published injournals are more inﬂuential than the other types of publicationvenues, and the importance of conference is less inﬂuential thanjournals and higher than workshops./C15The Average Publication/Keyword:This feature is given as below:o 0: keyword not in the title of paper.o 1: keyword in abstract.o 2: keyword in the title of paper.o 3: keyword in both title and abstractThe rank can be computed by the giving equation:Rank¼Xni¼1Ai:DL1NH/C18/C19þ0:2ðA
pþA nbrÞþ0:3ð DxþtypeÞ/C6PRðiÞ/C2DL
11þlogA/C18/C19
oA i: The number of papers published by the author.o N: The total number of all authors for the paper.o H: Constant with value 10.oA
v: The average publication/keyword.oA
nbr: The order of paper.o
Dx: Download rates.o Type: The type of paper.o PR(i): The score calculated by the PageRank algorithm.o Log (A): Used to reduce the impact of old paper having the high-est number of citations that are called ‘‘Scientiﬁc gem” [25].In this paper, we examine the possibility of predicting the newrank for scientiﬁc papers to help researchers to ﬁnd papers thatthey are looking for; we choose to work with Weka [27], as a col- lection of machine learning algorithms.5. Results and discussion5.1. DatasetWe give a great importance to the construction of training dataand test data because of their inﬂuence on our experience in thiswork; for this reason, the mobile window strategy has beenadopted.The appropriate size of test data must respect certain condi-tions. First of all, it must not be too small because it will convergeeasily; this will have consequences on the accuracy of the predic-tion because, on the one hand, it has not given enough data to sup-port the reasons enough and, on the other hand, it must not be toobig because it is not necessary to converge (see Figs. 4–6). We have chosen the moving windows strategy [28]compared to the sliding window strategy [29]because the prediction depends on the time factor, which is the case in our study. Afterthe realization of some preliminary experience, we try to choosethe two data model (test data and training data) which guaranteesthe lowest possible error rate.In this research, we have exploited the bibliographic datasetsfrom Thomson-Reuters Web of Science [30], which has information about different research publications in distinct ﬁelds of science,basic metadata for 1.935 SRPs in Research Areas of Science, andcover publications from 1996 to 2015. We use data from 1996 to2015. It is to be noted that the Thomson-Reuters dataset that weused contained citation information till the year 2015 only.
Fig. 2.Example of a neural network.
Fig. 3.The scientiﬁc papers network.M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190 1855.2. Data pre-processingThe Web of Science dataset also contains information that is notuseful in our algorithm. We need to pre-process the dataset toextract only the information that we will use in our algorithm. Indata pre-processing, after the extraction and preparation of data,the ﬁnal database contains: the title of paper, its author(s), key-words, paper posted time, the conference of publication, the givenresearch paper cites, the download rates, also the average publica-tion/keyword, article number and ﬁnally the order of paper.For our experiment, we split our data in two sections: the ﬁrst isthe data before 2012 that represent our training dataset that per-mits us to compute the rank for the papers; the second section con-tains the data that we wish to predict.From the point of view of the users who were searching in 2012for scientiﬁc research papers, the only available data is the trainingdataset, which means papers before 2012. Our system can predictfor our user the papers that he/she wants with the highest rank inhis/her subject.These experiments have been designed to ﬁnd the pertinentmetrics such as: paper-id, author score, and number of papers pub-lished, average download rates, number of citations. These metricscan be calculated from our training dataset.In our application, we apply some rules:/C15We don’t eliminate not integral data./C15We don’t rank the papers whose author doesn’t remain in thetraining dataset./C15We calculate the average of the rank that obtains all papers foreach ﬁrst author.5.3. Mathematical modelOur Mathematical model can be modeled in a set theory. Thesystem is represented as follows:S= {I, P, O}WhereS = represent the system,Where I = inputs, represent the given features,P={ P
1,P2...P n}Where P = ProcessesP
1P1= check data in local server, P2 = store data at database,O={ O
1,O2,...O n}Where O = outputs, represent the new rank for our papers.We took the papers related to the topics of Geographic Informa-tion System in our data set by our algorithm [26]as the training dataset.Rank values of our proposed algorithm are based on three prin-ciple points and depicted in table mentioned as supplementary material./C15Author and titlewhere the names of the authors and the titlesof papers are presented./C15DOI (Digital Object Identiﬁer)is a standardized method for thepermanent identiﬁcation of a published electronic object, a kindof permanent code of scientiﬁc articles. Each paper has its ownDOI./C15Journal and yearmean the date of paper publication and thejournal where is submitted.Now, our aim is to predict scientiﬁc research papers shown inFig. 9, that are in the evaluation dataset whose authors are all inthe training dataset and we calculate their futures as:We will start by deﬁning some parameters for our analysis:/C15Precision:is the fraction of retrieved instances that are rele-vant, named Positive Predictive Value./C15Recall:is the fraction of relevant instances that are retrieved,named Sensitivity./C15F-Measure:is the harmonic mean of precision and recall; inother terms, it is the measure that combines precision andrecall, and it is deﬁned as:F-Measure¼2/C2Recall/C2PrecisionRecallþPrecision
/C15Cumulative Gain:is the sum of the graded relevance values ofall results in a search result list, where rel
iis the graded rele- vance of the result at position I; it is deﬁned as:
CGp¼Xpi¼1reli
/C15Discounted Cumulative Gain[31]: is a particular rank positionp; it is deﬁned as:
DCG p¼Xpi¼1 reli
log2ðiþ1Þ
/C15Normalized Discounted Cumulative Gain [32]: search result lists vary in length depending on the query; it is deﬁned as:
NDCG p¼DCG p
IDCG p
/C15Average Precision:summarizes a precision-recall curve as theweighted mean of precisions achieved at each threshold, whereP
nandR nare the precision and recall at the nth threshold:
AP¼X
nðRn/C0R n/C01ÞPn
/C15Mean Average Precision[33]: is the average of the precisionvalue obtained for the set of k documents after the documentis retrieved.Qi s{d
1...d mj} andR jkis the set of ranked retrieval results fromthe top result until you get to document d
k, then
MAP d¼1dXdj¼11m
jXmj
k¼1PðR jkÞ
Fig. 4.The training dataset originating before 2012, and the evaluation datasetoriginating after 2012.
Fig. 5.Venn diagram of the proposed system.186 M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190/C15Geometric Mean Average Precision [34]: is the geometric mean of the average precision values for an information retrie-val system over a set of n query topics; it is deﬁned as:
GMAP¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃY
nAPnnr
The test set gives us the following results for the three classiﬁersafter making some modiﬁcation:
InTable 1, we can clearly see that the Multilayer Perceptronclassiﬁer has the highest NDCG
26[32]compared to the tow classi- ﬁers and this can provide a good performance for prediction.Our network for predicting new rank after using Multilayer Per-ceptron is shown in the ﬁgure below:As we see inFig. 7, our network contains in the input layer met-rics: paper-id, author-score, number–paper-published, average-download-rates, average-number-citation, and one hidden layerwith 17 nodes as the average between the number of inputs andoutputs. Each connection node named neuron has a weight calcu-lated from their inputs with a sigmoid function [35]as:
Wnext¼Wþ DW
DW¼/C0learning rate/C2gradientþmomentum/C2 DWprevious
Finally, we have the output layer with 29 classes representing therank of scientiﬁc research papers. The predictions give us the resultshown inTable 2.
5.4. Further comparison of the proposed algorithmNow, we are analyzing the metrics used to predict our newrank. We propose four variant APC, APD, ADC and PDC explainedas follow:/C15New Rank (APC): a proposed variant wherein paper-id, authorscore, number–paper-published and average-number-citationare the parameters used./C15New Rank (APD): a proposed variant based on the same param-eters of APC, but instead using average-number-citation we useaverage download-rates./C15New Rank (ADC): a proposed variant wherein we use paper-id,author score, average download-rates and average-number-citation./C15New Rank (PDC): a proposed variant based on paper-id,number-paper-published, average-download-rates andaverage-number-citation.In the ﬁgure below, we summarize the results for the four vari-ants of our proposed new rank.In order to deﬁne the suitable variant of our new rank the threeparameters: Precision, Recall and F-measures should have highervalues. As we see in the ﬁgure, we conclude that the APD variantsatisﬁed this condition (seeFig. 8).As we see in the previous sub-section, MAP is just an averageprecision which most frequently used in research papers. In con-trast to MAP which can be considered as an arithmetic mean,GMAP is a geometric per precision; it used to highlight improve-ment for low performing subjects.
Fig. 6.Data for prediction.
Table 1Performance for the three classiﬁers.
Multilayer Perceptron SMO KstarDCG
26 129.302 122.092 125.707IDCG
26 162.678 162.677 162.683NDCG
26 0.794 0.750 0.7727
Fig. 7.The network for prediction.M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190 187On the other hand, we should compare MAP and GMAP for thefour variants. We clearly see in the Fig. 9in term of APD, ADC and PDC the values are slightly close to each other compared to APCwhich has superior values.InFig. 10, we present a comparison between GMAP and MAP interm of the proposed new rank. We can clearly see that for eachranked paper GMAP and MAP are close to each other, instead ofsome cases wherein we ﬁnd that the values of GMAP are very lessthan MAP.MAP and GMAP may be seen as similar measures of averageranking effectiveness of a system.To sum up, theFigs. 9 and 10shows that GMAP values are lessthan MAP and this lead to a perform ranking and at the same timereducing errors in our proposed system.Table 2Scientiﬁc research papers predicted by our new rank algorithm.
Rank Author and Title DOI Journal, Year1 Wilson: On the criticality of mapping practices: Geodesign as critical GIS? https://doi.org//10.1016/j.landurbplan.2013.12.017 LANDSCAPE AND URBAN PLANNING, 20152 Brown et al.: An empirical evaluation of workshop versus survey PPGISmethods https://doi.org//10.1016/j.apgeog.2014.01.008 APPLIED GEOGRAPHY, 20143 Mukherjee: Public Participatory GIS https://doi.org//10.1111/gec3.12223 Geography Compass, 20154 Brown and Weber: A place-based approach to conservation managementusing public participation GIS (PPGIS) https://doi.org//10.1080/09640568.2012.685628 JOURNAL OF ENVIRONMENTAL PLANNING ANDMANAGEMENT, 2013 5 Brown and Weber: Using public participation GIS (PPGIS) on the Geoweb tomonitor tourism development preferences https://doi.org//10.1080/09669582.2012.693501 JOURNAL OF SUSTAINABLE TOURISM, 20136 Al-Wadaey and Ziadat: A participatory GIS approach to identify critical landdegradation areas and prioritize soil conservation for mountainous olivegroves (case study)https://doi.org//10.1007/s11629-013-2827-x JOURNAL OF MOUNTAIN SCIENCE, 20147 Mekonnen and Gorsevski: A web-based participatory GIS (PGIS) for offshorewind farm suitability within Lake Erie, Ohio https://doi.org//10.1016/j.rser.2014.08.030 RENEWABLE & SUSTAINABLE ENERGYREVIEWS, 2015 8 Young and Gilmore: The Spatial Politics of Affect and Emotion in ParticipatoryGIS https://doi.org//10.1080/00045608.2012.707596 ANNALS OF THE ASSOCIATION OF AMERICANGEOGRAPHERS, 2013 9 Thompson: Public participation GIS and neighbourhood recovery: usingcommunity mapping for economic development https://doi.org//10.1504/IJDMMM.2015.067632 INTERNATIONAL JOURNAL OF DATA MININGMODELLING AND MANAGEMENT, 2015 10 Baldwin et al.: Participatory GIS for strengthening trans boundary marinegovernance in SIDS https://doi.org//10.1111/1477-8947.12029 NATURAL RESOURCES FORUM, 201311 Pozzebon et al.: Use and consequences of participatory GIS in Mexicanmunicipality: applying a multilevel framework https://doi.org//10.1590/S0034-759020150305 RAE-REVISTA D’ADMINISTRACAO D’EMPRESAS,2015 12 Zhang et al.: Discovering Spread Mode of Public Opinions in Incidents andMapping it with GIS: a Case on Big Geospatial Data Analytics https://doi.org//10.1109/Agro-Geoinformatics.2014.6910597AGRO-GEOINFORMATICS, 201413 Sui: Opportunities and Impediments for Open GIS https://doi.org//10.1111/tgis.12075 TRANSACTIONS IN GIS, 201414 Asare-Kyei et al.: Modeling Flood Hazard Zones at the Sub-District Level withthe Rational Model Integrated with GIS and Remote Sensing Approaches https://doi.org//10.3390/w7073531 WATER, 201515 Kerski et al.: The Global Landscape of GIS in Secondary Education https://doi.org//10.1080/00221341.2013.801506 JOURNAL OF GEOGRAPHY, 201317 Levine and Feinholz: Participatory GIS to inform coral reef ecosystemmanagement: Mapping human coastal and ocean uses in Hawaii https://doi.org//10.1016/j.apgeog.2014.12.004 APPLIED GEOGRAPHY, 201518 McCall et al.: Shifting Boundaries of Volunteered Geographic InformationSystems and Modalities: Learning from PGIS /1234 ACME: An International Journal for Critical Geographies, 2015 19 Brown and Fagerholm: Empirical PPGIS/PGIS mapping of ecosystem services:A review and evaluation https://doi.org//10.1016/j.ecoser.2014.10.007 ECOSYSTEM SERVICES, 201520 Song et al.: A Participatory GIS Solution for Watershed Rehabilitation ProjectManagement in the Changjiang and Pearl River Basins https://doi.org//10.2991/rsete.2013.114 Advances in Intelligent Systems Research, 201321 Panek and Van Heerden: Participatory GIS for water provision and communityplanning - Case study Kofﬁeikraal, South Africa https://doi.org//10.5593/SGEM2013/BB2.V1/S11.030 Cartography and GIS, 2013
22 Brovelli et al.: Participatory GIS: Experimentations for a 3D social virtual globe https://doi.org//10.5194/isprsarchives-XL-2-W2-13-2013International Archives of the Photogrammetry,Remote Sensing and Spatial InformationSciences, 2013 23 Chingombe et al.: A participatory approach in GIS data collection for ﬂood riskmanagement, Muzarabani district, Zimbabwe https://doi.org//10.1007/s12517-014-1265-6 ARABIAN JOURNAL OF GEOSCIENCES, 201524 Lombard: Using participatory GIS to examine social perception towardsproposed wind energy landscapes https://doi.org//10.17159/2413-3051/2015/v26i2a2195JOURNAL OF ENERGY IN SOUTHERN AFRICA,201525 Crooks and Wise: The role of Public Participatory Geographical InformationSystems (PPGIS) in coastal decision-making processes: An example fromScotland, UKhttps://doi.org//10.1016/j.compenvurbsys.2013.05.003COMPUTERS ENVIRONMENT AND URBANSYSTEMS, 201326 Dorn et al.: GIS-Based Roughness Derivation for Flood Simulations: AComparison of Orthophotos, LiDAR and Crowdsourced Geodata https://doi.org//10.3390/rs6021739 REMOTE SENSING, 201427 Brown et al.: Which ’public’? Sampling effects in public participation GIS(PPGIS) and volunteered geographic information (VGI) systems for publiclands managementhttps://doi.org//10.1080/09640568.2012.741045 JOURNAL OF ENVIRONMENTAL PLANNING ANDMANAGEMENT, 201428 Brown et al.: Using participatory GIS to measure physical activity and urbanpark beneﬁts https://doi.org//10.1016/j.landurbplan.2013.09.006 LANDSCAPE AND URBAN PLANNING, 201429 Resch et al.: GIS-Based Planning and Modeling for Renewable Energy:Challenges and Future Research Avenues https://doi.org//10.3390/ijgi3020662 ISPRS INTERNATIONAL JOURNAL OF GEO-INFORMATION, 2014188 M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–1906. ConclusionIn this paper, we propose a new approach for predicting thenew rank for scientiﬁc research papers. Our experimental evalua-tion has shown the efﬁcient of the utilization of machine learningalgorithm in the discipline of ranking. We provide an algorithmthat use different metrics such us (including but not restricted topaper-id, author-score, number–paper-published, average-download-rates, average-number-citation) in a one network.Moreover, we provide a comparison of the metrics and rankedthem conforming to their prediction ability using metrics analysisalgorithm, we think that this work can help researcher in other dis-cipline such us: ﬁnancial sector, policies investigation, and terroristbehavior.For future work, we plan to test our algorithm on additionaldatasets in order to determine how robust it is to the different val-ues of parametrs, and in different datasets, also we plane to make asurvey to appraise the results by users.Appendix A. Supplementary materialSupplementary data associated with this article can be found, inthe online version, athttps://doi.org/10.1016/j.aci.2018.02.002 .References[1] N.W. Alkharouf, D.C. Jamison, B.F. Matthews, Online analytical processing(OLAP): a fast and effective data mining tool for gene expression databases, J.Biomed. Biotechnol. 2005 (2005) 181–188, https://doi.org/10.1155/ JBB.2005.181.[2] M. Darnstädt, H.U. Simon, B. Szörényi, Supervised learning and co-training,Algorithmic Learn. Theory 519 (2014) 68–87, https://doi.org/10.1016/j. tcs.2013.09.020.[3] H. Ramchoun, M. Amine, J. Idrissi, Y. Ghanou, M. Ettaouil, Multilayerperceptron: architecture optimization and training, Int. J. Interact. Multimed.Artif. Intell. 4 (2016) 26,https://doi.org/10.9781/ijimai.2016.415 . [4] X. Shao, K. Wu, B. Liao, Single directional SMO algorithm for least squaressupport vector machines [WWW Document], Intell. Neurosci. Comput. (2013),https://doi.org/10.1155/2013/968438 . [5] S. Lee, M. Park, J. Park, H. Na, M. Kwon, Operator interface programs for KSTARoperation, Fusion Eng. Des. 88 (2013) 2835–2841, https://doi.org/10.1016/ j.fusengdes.2013.05.008.[6] K. Slim, A. Dupré, B. Le Roy, Impact factor: an assessment tool for journals orfor scientists?, Anaesth Crit. Care Pain Med. 36 (2017) 347–348, https://doi. org/10.1016/j.accpm.2017.06.004 . [7] Y. Du, W. Liu, X. Lv, G. Peng, An improved focused crawler based on semanticsimilarity vector space model, Appl. Soft Comput. 36 (2015) 392–407, https:// doi.org/10.1016/j.asoc.2015.07.026 . [8] F. Zhuang, G. Karypis, X. Ning, Q. He, Z. Shi, Multi-view learning viaprobabilistic latent semantic analysis, Inf. Sci. 199 (2012) 20–30, https://doi. org/10.1016/j.ins.2012.02.058 . [9] S. Robertson, H. Zaragoza, M. Taylor, Simple BM25 Extension to MultipleWeighted Fields, in: Proceedings of the Thirteenth ACM InternationalConference on Information and Knowledge Management, CIKM ’04. ACM,New York, NY, USA, 2004, pp. 42–49. https://doi.org/10.1145/1031171.1031181.[10] F. Lv, H. Zhang, J.g. Lou, S. Wang, D. Zhang, J. Zhao, CodeHow: Effective CodeSearch Based on API Understanding and Extended Boolean Model (E), in: 201530th IEEE/ACM International Conference on Automated Software Engineering(ASE). Presented at the 2015 30th IEEE/ACM International Conference onAutomated Software Engineering (ASE), 2015, pp. 260–270. https://doi.org/10.1109/ASE.2015.42.[11] X. Liu, H. Lin, C. Zhang, An improved HITS algorithm based on page-querysimilarity and page popularity, J. Comput. 7 (2012), https://doi.org/10.4304/ jcp.7.1.130-134.[12] X. Tan, A new extrapolation method for PageRank computations, J. Comput.Appl. Math. 313 (2017) 383–392, https://doi.org/10.1016/j.cam.2016.08.034 . [13] A. Bougouin, F. Boudin, B. Daille, Topicrank: Graph-based topic ranking forkeyphrase extraction, in: International Joint Conference on Natural LanguageProcessing (IJCNLP), 2013. pp. 543–551.[14] P. Jomsri, S. Sanguansintukul, W. Choochaiwattana, CiteRank: combinationsimilarity and static ranking with research paper searching, Int. J. InternetTechnol. Secur. Trans. 3 (2011) 161–177, https://doi.org/10.1504/ IJITST.2011.039776.[15] M.A. Hasson, S.F. Lu, B.A. Hassoon, Scientiﬁc research paper ranking algorithmPTRA: a tradeoff between time and citation network, Appl. Mech. Mater. 551(2014) 603–611,https://doi.org/10.4028/www.scientiﬁc.net/AMM.551.603 . [16] F. Stahl, I. Jordanov, An overview of the use of neural networks for data miningtasks, Wiley Interdiscip Rev. Data Min. Knowl. Discov. 2 (2012) 193–208,https://doi.org/10.1002/widm.1052 . [17] Y. Zheng, B. Jeon, L. Sun, J. Zhang, H. Zhang, Student’s t-hidden markov modelfor unsupervised learning using localized feature selection, IEEE Trans. CircuitsSyst. Video Technol. 1–1 (2017), https://doi.org/10.1109/TCSVT.2017.2724940 . [18] F.-J. González-Serrano, Á. Navia-Vázquez, A. Amor-Martín, Training supportvector machines with privacy-protected data, Pattern Recognit. 72 (2017) 93–107,https://doi.org/10.1016/j.patcog.2017.06.01 .
Fig. 8.Comparison of the performance.
Fig. 9.MAP vs. GMAP in different variants.
Fig. 10.GMAP vs. MAP in the proposed New Rank.M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190 189[19] W.W.Y. Ng, X. Zhou, X. Tian, X. Wang, D.S. Yeung, Bagging–boosting-basedsemi-supervised multi-hashing with query-adaptive re-ranking,Neurocomputing (2017),https://doi.org/10.1016/j.neucom.2017.09.042 . [20] J. Yao, Q. Mao, S. Goodison, V. Mai, Y. Sun, Feature selection for unsupervisedlearning through local learning, Pattern Recognit. Lett. 53 (2015) 100–107,https://doi.org/10.1016/j.patrec.2014.11.006 . [21] K.M. Kumar, A.R.M. Reddy, An efﬁcient k-means clustering ﬁltering algorithmusing density based initial cluster centers, Inf. Sci. 418–419 (2017) 286–301,https://doi.org/10.1016/j.ins.2017.07.036 . [22] Z. Zainuddin, O. Pauline, An effective fuzzy C-means algorithm based onsymmetry similarity approach, Appl. Soft Comput. 35 (2015) 433–448, https:// doi.org/10.1016/j.asoc.2015.06.021 . [23] X. Zhu, Semi-Supervised Learning, in: Encyclopedia of Machine Learning,Springer, Boston, MA, 2011, pp. 892–897, https://doi.org/10.1007/978-0-387- 30164-8_749.[24] S. Chen, S. Zhu, Y. Yan, Robust visual tracking via online semi-supervised co-boosting, Multimed. Syst. 22 (2016) 297–313, https://doi.org/10.1007/s00530- 015-0459-4.[25] P. Chen, H. Xie, S. Maslov, S. Redner, Finding scientiﬁc gems with Google’sPageRank algorithm, J. Informetr. 1 (2007) 8–15, https://doi.org/10.1016/j. joi.2006.06.001.[26] M. El Mohadab, B. Bouikhlaene, S. Saﬁ, Towards an efﬁcient algorithm forranking scientiﬁc research papers, in: 2017 2nd IEEE international scientiﬁcevent on internet of things: Recent innovations and challenges (SEIT).Presented at the 2017 2nd IEEE international scientiﬁc event on internet ofthings: Recent innovations and challenges (SEIT), Rabat, Morocco.[27] S. Lievens, B. De Baets, Supervised ranking in the weka environment, Inf. Sci.180 (2010) 4763–4771,https://doi.org/10.1016/j.ins.2010.06.014 . [28] J. Valluru, P. Lakhmani, S.C. Patwardhan, L.T. Biegler, Development of movingwindow state and parameter estimators under maximum likelihood andBayesian frameworks, DYCOPS-CAB 2016 (60) (2017) 48–67, https://doi.org/ 10.1016/j.jprocont.2017.08.007 . [29] I. Chakroun, T. Haber, T.J. Ashby, SW-SGD: The Sliding Window StochasticGradient Descent Algorithm. Procedia Comput. Sci., International Conferenceon Computational Science, ICCS 2017, 12–14 June 2017, Zurich, Switzerland108, 2017, 2318–2322. https://doi.org/10.1016/j.procs.2017.05.082.[30] Web of Science library. Available at: http://www.webofknowledge.com [accessed in 2016].[31] G. Dupret, B. Piwowarski, Model Based Comparison of Discounted CumulativeGain and Average Precision. Sel. Pap. 18th Int. Symp. String Process. Inf. Retr.SPIRE 2011 18, 49–62. https://doi.org/10.1016/j.jda.2012.10.002.[32] Y. Wang, L. Wang, Y. Li, D. He, T.-Y. Liu, W. Chen. A theoretical analysis ofNDCG type ranking measures. ArXiv13046480 Cs Stat, 2013.[33] M. Thelwall, The precision of the arithmetic mean, geometric mean andpercentiles for citation data: an experimental simulation modelling approach,J. Informetr. 10 (2016) 110–123, https://doi.org/10.1016/j.joi.2015.12.001 . [34] S. Robertson, On GMAP: and other transformations. (2006), https://doi.org/ 10.1145/1183614.1183630. [35] A. Iliev, N. Kyurkchiev, S. Markov, On the approximation of the step function bysome sigmoid functions, Math. Comput. Simul., Biomath 2014 and Biomath2015 133 (2017) 223–234,https://doi.org/10.1016/j.matcom.2015.11 .190 M. El Mohadab et al. / Applied Computing and Informatics 15 (2019) 182–190