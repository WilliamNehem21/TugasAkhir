 AASRI Procedia   8  ( 2014 )  68 – 74 Available online at www.sciencedirect.com
2212-6716 © 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institutedoi: 10.1016/j.aasri.2014.08.012 
ScienceDirect
2014 AASRI Conference on Sports Engineering and Computer Science (SECS 2014) 
An Efficient Use of Principal Component Analysis in Workload Characterization-A Study 
Jyotirmoy Sarkara, Snehanshu Sahab, Surbhi Agrawalb*
aBITS PILANI & TechMahindra,,Bangalore,560100,India 
bCBIMMC & Dept. of Computer Science and Engineering, PESIT-BSC,Bangalore,560100,India  
Abstract 
PCA is a useful statistical technique that has found application in fields such as face recognition, image compression, dimensionality  reduction, Computer System performance analysis etc. It is a common technique for finding patterns in data of high dimension. In this paper, we present the basic idea of principal component analysis as a general approach that extends to various popular data analysis techniques. We state the mathematical theory behind PCA and focus on monitoring system performance using the PCA algorithm. Next, an Eigen value-Eigenvector dynamics is elaborated which aims to reduce the computational cost of the experiment. The Mathematical theory is explored and validated. For the purpose of illustration we present the algorithmic implementation details and numerical examples over real time and synthetic datasets. 
© 2014 . Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied Science Research Institute 
Keywords: PCA; Eigen Value; Eigen Vector,Workload Characterization.. 
* Corresponding author. Tel.: +91-080-66186622; fax: 91-80-. E-mail address: snehanshusaha@pes.edu. 
© 2014 The Authors. Published by Elsevier B. V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/).Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute69  Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
1. Introduction Performance evaluation helps us to give an idea how we ll a system is performing as compared to other systems. Workload is the most crucial part of any performance evaluation process. The entire process can end up in a wrong conclusion if workload is not chosen in appropriate ways. Therefore, workload selection is an integral part of performance evaluation project. Computer architectures are evaluated by running a workload on the computer and measuring the execution time. New computers are designed the same way. As the newly designed computer does not exist, it is not possible to run any workload. This is where workload characterization comes into the fore. The goal of worklo ad characterization is to describe the properties of a workload in terms of abstract performance metrics, called workload characteristics, which predict the final performance [1]. There are a couple of techniques to classify workload components. One of the widely used techniques is “weighted sum of the parameter values” that uses sum to classify the workload components into classes. But there are proper guidelines to decide the weight of pa rameters. Before PCA, an analyst running software used to assume the values of weight. Instead, one can use PCA, to calculate the value of weights. PCA is a procedure by which numbers of correlated variables are transformed into a smaller number of uncorrelated variables. It is a data analysis technique traced back to Pearson (1901). It can be used to compress a high dimensional dataset into a lower dimensional dataset. PCA can be derived from a number of starting points and optimization criteria. The most important of these are minimization of the mean square error in data compression, finding mutually orthogonal directions in the data having maximal variances and de-correlation of the data using orthogonal transformation. These uncorrel ated variables are called Principal Components. 1.1. How PCA works: For a given set of n parameters
12{ , ,... }n xxx , the Principal Component Analysis will produce a set of principal factors. The following conditions will hold true for the newly produced set - xThe principal factor 
()iy is a linear combination of initial parameters ()jx .  
1nij jjya x
  ¦
x Principal factor set is an orthogonal set. 
,0ij i k k jkyy a a!   ¦
It is an ordered set 12{ , ,... }n yyy  in the decreasing order of the percentage of variance, with 1y being the highest percentage of variance and 
ny the least. So first few factors can be used to classify the workload components.  We can find the application of Principal Component An alysis in many fields including data compression, image processing, visualization, pattern recognition and time series prediction [2].Sirvich and Kirby had efficiently used PCA in human faces representation [3-4].This approach leads to decomposition of any images into Eigen pictures so that the image can be reconst ructed using a portion of the Eigen pictures and the corresponding projection onto the Eigen picture su bspace [5]. The PCA method has also been used in handprint recognition, human made object recognition, industrial robotics and mobile robotics etc [6]. In a workload composition the choice of benchmark is very important. The selection of benchmark for inclusion in 70   Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
benchmark suit is called workload composition. Smith [7] used a metric, which is based on dynamic program characteristics for the Fortran language..They used squared Euclidean distance to measure the difference betwee
 n benchmarks. The shortcoming of this procedure is  the use of Euclidean distance for measuring the difference. To overcome this Eeckhout et al. [8] proposed Principal Componen t Analysis (PCA) to get rid of the correlation and dependence between variables. A number of program characteristics are measured for a number of benchmarks on which PCA was applied.  2. Proposed Work The most computationally expensive part of PCA is the calculation of Eigen values and Eigen vectors of the dataset. In this paper our main objective is to save the computational time of Principal Component Analysis (PCA) by skipping the Eigen vector calculation.  Here we propose to bypass Eigen vector calculation, by rather inspecting the Eigen vectors. The understanding of the dynami cs of Eigen values and Eigen vectors in the context of Linear transformations and vector spaces plays a crucial role in improving the efficiency of PCA in the workload characterization problem. This will requi re us to prove/cite important theorems in Linear Algebra. 
2.1. Algorithm x
Compute the mean and standard deviation of the parameters. 
11niix an
  ¦,211()1nxiis xxn
  ¦
x Compute the correlation of the parameters. 11() ()
iiababnaa bbixx xxx xx xnRss
  ¦
x Compute QR Decomposition of correlation matrix at every step kk kAQ R  (starting with 0k  ), where 
kQ is an orthogonal matrix and kR is an upper triangle matrix. 1kk KAR Q 
x The matrix will converge to a triangular matrix, known as the Schur form. Find out the eigen values of the matrix from the diagonal. 
x Apply the results of the theorem to choose the eigen vectors by inspection. This is possible since the matrices obtained are symmetric and the Eigen values are real and distinct. 
x Use the Eigen vectors to compute the principal factors. Next, we prove a theorem related to eigen values and eigen vectors. Let us take a linear map 
:Tu vo
() ( ) ( ) ;Tx y T x T yDED E   where ,xyu ;,uv are vector spaces of certain dimensions, &nm
say where nmz  necessarily; e.g. ,nmuR vR  RTx xOO
   , then O is an Eigen value of 
&Tx  is a corresponding Eigen vector. 
2.2. Proposition 1:
For the linear map :Tu vo , if the Eigen values “ O “are distinct, then T admits of linearly independent Eigen vectors. Proof: Linear Independence: A set of vectors 
123{ , , ,.... }n vvv v  is linearly independent if  scalars
12( , ,.... )n DDD 
10niiivD
  ¦ implies 01 , . .i inD{  i.e.iv; any vector in the set is NOT a linear 71  Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
combination of any of the other vectors in the same set e.g. 11ªº«»¬¼ & 11ªº«»¬¼ are linearly independent. Proof of the theorem: (Using the Principle of Mathematical Induction) Basis Step: 
2;n  NTS 11 2 21 2 00av av a a    
Apply11 2 2() ( 0 )TT a v a v T  ;T linear map 
11 2 2() () 0 ;aT v aT v 
11 1 2 22 0av a v OO                                                                                                                            (1) 
Also 
11 1 212 0av av OO                                                                                                                                (2) 
Therefore (1) (2) 22 1 2 2() 0 0av aOO   12 2(; 0 )vOOzz'
2100aa   ; Induction hypothesis:  Assume the proposition is true for nm  ; Induction steps: on 
1 nm   i.e. NTS  11 1 1....0mm m m av a v a v    =>12 1 ....0.mm aa a a      
Let11 1 1....0mm m m av a v a v      i.e. 11 1 1 ( ....) (0)mm m m Ta v av a v T   
so,
11 1 1 1 1....0mm m m m m av a v a vOOO                                                                                               (3)
Also, 
11 11 11 1....0mm m m av a v a vOOO                                                                                                    (4) (3)
 (4)22 1 2 1 1 1 1( ) .... ( ) 0mm m av a vOOO O      
By the hypothesis, 1{ ,... }m vv  linear independent121 ... 0mm aa a a     
? 11 1 1() 0mm mavOO  11 1 (( ) 0 & 0 )mm v OOzz'
Therefore, the Eigen vectors 11{ ,... , }mm vv v are Linearly Independent. Proposition 2: 
 A real, symmetric linear map T (matrix) admits of orthogonal Eigenvectors. 
Proof: Well established result [9]. Conclusion of proposition: The aforementioned matrix (or the linear map, T) has distinct eigen values (as 72   Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
always the case will be) and is real, symmetric. Therefore, the co rresponding eigenvectors will be linearly independent & orthogonal to each other. This enables us to find the eigenvectors by inspection rather than computing step by step via set of simultaneous equations. This saves O (n) computations, crucial computation cost!
2.3. Implication
The paper aims to improve time complexity of PC A algorithm. The following example will illustrate the principle behind PCA from initial parameters.We have collected synthetic data of the number of packets lost on two different network links. 
axis the number of packets lost on link A and bx is the number of packets lost on link B 
Table 1. Data for Principal Component Analysis Example 1 Observation Number 
ax  (Variables) bx(Variables)ay  (Principal factors)by(Principal factors)
1 300 400 -0.0027 -0.0014 
2 510 330 -0.0014  0.0028 3 212 547 -0.0021  0.0049 4 309 690  0.0028 -0.0070 5 610 410  0.0014  0.0028 6 910 150  0.0007  0.0133 7 540 320 -0.0014  0.0042 8 440 540  0.0007  -0.0021 9 219 440 -0.0037  -0.0023 10 510 779  0.0070  -0.0054 
First we have to compute the mean and standard deviation using the formulas given in Algorithm 2 
456045610
ax    ; 460046010
bx  ;2
axs=22483986 10 45644958.49u  ; 
234805.5
bxs 
Correlation among the variables as 0.486
abxxR and hence the correlation matrix will be  
1.000 0.4860.486 1.000Cªº «»¬¼
                                                                                                                       
(5) Now we will compute the Eigen values from the above correlation matrix using characteristic equation 
1 0.48600.486 1CIOOO  22(1 ) 0.486 0O   73  Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
The Eigen values are 1.486 and 0.514. 
3.Results and Discussion 
Now, the correlation matrix in (5) is both real and symmetric and is a candidate for the theorems to be applied. Figure1 below shows the average execution time of PCA algorithm over a sample of 5 different datasets. The execution time has been recorded in milliseconds. 
4. Conclusion 
PCA is the simplest of the true Eigenvector-based multivariate analyses. It can be used to reveal internal structure of data in a way that best explains the variance in data. PCA is sensitive to outliers in the data that produce large number of errors. So, before applying PCA it is expected to remove outliers. As a limitation the result of PCA depend on the scaling of variables. The applicability of PCA constrained by certain assumption made in derivation. Our work explores the underly ing principles of PCA and exploits the inherent mathematical theory for efficient computation. The fi gure below conclusively shows that computation time has been reduced to achieve the same results. 
02004006008001000TillEigenValue
TillEigenVector
Fig.1. execution times of Eigen values and Eigen vectors and the time saved. 
References 
[1] T. M. Conte and W. Hwu, Benchmark Characterization,"IEEE Comp uter, vol. 24, no. 1, pp. 48-56, Jan. 1991. [2] Raj Jain, The Art of Computer Systems Performance Analysis, Techniques for Experimental Design, Measurement, Simulation, and Modeling. [3] Kirby and Sirovich, 1990. Application of Karhunen-Loeve Procedure for the Characterization of Human Faces. IEEE [4] Taranpreet Singh, Face Recognition Based on PCA Algorithm. [5] S Ekhe, Y Chincholkar, Improved Face Recognition using PCA & LDA [6] R Gottumukkal, V K Asari, An Improved Face Recognition Technique Based on Modular PCA Approach. 74   Jyotirmoy Sarkar et al.  /  AASRI Procedia   8  ( 2014 )  68 – 74 
[7] R. H. Saavedra and A. J. Smith, “Analysis of Benchmark Characteristics and Benchmark Performance Prediction,” ACM TOCS, vol. 14, no. 4, pp. 344–384, Nov. 1996. [8] L. Eeckhout, H. Vandierendonck, and K. De Bosschere, “Quantifying the Impact of Input Data Sets on Program Behavior and its Applications,” JILP, vol. 5, Feb. 2003, http://www.jilp.org/vol5 [9] Gilbert Strang, “Introduction to Linear Algebra”, 4
th Edition, SIAM, 2009. 