Classiﬁcation random forest with exact conditioning for spatial prediction ofcategorical variables
Francky Fouedjio
AngloGold Ashanti Australia Ltd., Growth and Exploration, 140 St. Georges Terrace, Perth, WA, 6000, Australia
ARTICLE INFO
Keywords:Categorical variableClassiﬁcationExact conditioningPrincipal component analysisSigned distanceSpatial predictionQuadratic programmingABSTRACT
Machine learning methods are increasingly used for spatially predicting a categorical target variable whenspatially exhaustive predictor variables are available within the study region. Even though these methods exhibitcompetitive spatial prediction performance, they do not exactly honor the categorical target variable's observedvalues at sampling locations by construction. On the other side, competitor geostatistical methods perfectly matchthe categorical target variable's observed values at sampling locations by essence. In many geoscience applica-tions, it is often desirable to perfectly match the observed values of the categorical target variable at samplinglocations, especially when the categorical target variable's measurements can be reasonably considered error-free.This paper addresses the problem of exact conditioning of machine learning methods for the spatial prediction ofcategorical variables. It introduces a classi ﬁcation random forest-based approach in which the categorical target variable is exactly conditioned to the data, thus having the exact conditioning property like competitor geo-statistical methods. The proposed method extends a previous work dedicated to continuous target variables byusing an implicit representation of the categorical target variable. The basic idea consists of transforming theensemble of classiﬁcation tree predictors' (categorical) resulting from the traditional classi ﬁcation random forest into an ensemble of signed distances (continuous) associated with each category of the categorical target variable.Then, an orthogonal representation of the ensemble of signed distances is created through the principalcomponent analysis, thus allowing to reformulate the exact conditioning problem as a system of linear inequalitieson principal component scores. Then, the sampling of new principal component scores ensuring the data's exactconditioning is performed via randomized quadratic programming. The resulting conditional signed distances areturned out into an ensemble of categorical outputs, which perfectly honor the categorical target variable'sobserved values at sampling locations. Then, the majority vote is used to aggregate the ensemble of categoricaloutputs. The effectiveness of the proposed method is illustrated on a simulated dataset for which ground-truth isavailable and showcased on a real-world dataset, including geochemical data. A comparison with geostatisticaland traditional machine learning methods show that the proposed technique can perfectly match the categoricaltarget variable's observed values at sampling locations while maintaining competitive out-of-sample predictiveperformance.
1. IntroductionThe spatial prediction of a categorical target variable when auxiliaryspatial information is available everywhere within the region under studyhas become ubiquitous in geosciences. Typical examples include pre-dicting land use classes, land cover categories, drainage classes, vegetationspecies, landslide types, rock types, soil types, lithofacies, hydrofacies, andgeological units. The mapping of categorical variables plays an essentialrole in a wide variety of geoscience applications. It is used to aid in risk-aware decision-making in many areas, such as environmental studiesand natural resource management. Various methods have been proposedfor spatially predicting categorical target variables when spatiallyexhaustive predictor variables are available within the study region. Thesemethods include geostatistical methods ( Goovaerts, 2001;Hengl et al., 2004,Hengl et al., 2007), generalized linear mixed models-based ap-proaches (Cao et al., 2011,Cao et al., 2014), and classiﬁcation machine learning techniques (Kanevski, 2008;Kanevski et al., 2009;Hengl et al., 2018;Maxwell et al., 2018;Du et al., 2020;Giaccone et al., 2021). Geostatistical methods for spatially predicting a categorical variablein the presence of auxiliary spatial information available everywhere
E-mail addresses:ffouedjio@anglogoldashanti.com ,francky.fouedjio@gmail.com.
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.11.003Received 13 August 2021; Received in revised form 25 October 2021; Accepted 30 November 2021Available online 11 December 20212666-5441/©2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95within the region under study include indicator kriging with externaldrift (IKED), regression-kriging of indicators (RKI), and regression-kriging of memberships (RKfM). Indicator kriging with external drift(IKED) assumes that auxiliary variables are linearly related to the classoccurrence of the categorical target variable ( Goovaerts, 2001). The auxiliary variables are incorporated into the indicator kriging system asdeterministic linear functions. Its implementation is challenging since itis often problematic to simultaneously estimate the parameters ofexternal drift and the covariance function of the stochastic component.Regression-kriging of indicators (RKI) combines multinomial logisticregression of the categorical target variable on predictor variables withkriging of the regression residuals (Hengl et al., 2004,Hengl et al., 2007). Thus, the regression modeling is supplemented with the modeling ofvariograms for regression residuals, which are then interpolated andadded back to the regression estimate. RKI has been adapted toregression-kriging of memberships (RKfM) by substituting crisp indicatorvalues with continuous membership values ( Hengl et al., 2007). Indeed, under the RKI method, the interpolation of residuals might lead to valuesoutside the physical range (<0o r>1). Although easy to implement, these indicator kriging-based methods have some well-known short-comings. The predicted probabilities of the target variable's categoriesare not guaranteed to belong to the [0, 1] interval and sum up to one.Therefore, post-processing methods of the predicted probabilities arerequired (Bogaert, 2004;Allard et al., 2011). Also, under these methods, the outcome values of the conditional cumulative distribution functionmay not be monotonic. A posterior correction of the resulting conditionalprobabilities is often necessary either through a Gaussian transformationor via a logistic regression model (Pardo-Igúzquiza and Dowd, 2005). Another alternative for spatially predicting a categorical responsevariable in the context of spatially exhaustive auxiliary informationavailable within the spatial domain of interest consists of using gener-alized linear mixed models-based approaches. Cao et al. (2011)propose a spatial multinomial logistic mixed model in which spatially correlatedlatent variables are assumed to account for the spatial dependency in thecategorical target variable. The proposed model is represented as amultinomial logistic function of spatial covariances between target andsampling locations. The sought-after class occurrence probability func-tion for a target location is written as a multinomial logistic linearcombination of covariance values between the target and source datalocations, which can be analogous to the dual form of kriging methods.This method was later extended to incorporate heterogeneous auxiliaryinformation for spatial prediction of categorical variables ( Cao et al., 2014). These generalized linear mixed models-based approaches are freeof the aforementioned inherent problems of indicator kriging-basedmethods. However, they are computationally intensive compared to in-dicator kriging-based methods.Machine learning techniques are increasingly used for spatially pre-dicting a categorical response variable when auxiliary information isavailable everywhere within the study region. Indeed, the number ofpredictor variables that help explain the spatial variation in the targetvariable has grown dramatically, making other methods cumbersome toapply. Also, some machine learning methods are well-known forhandling complex non-linear relationships and interactions and requireless data pre-processing. Classiﬁcation machine learning methods haveproven relevant for spatially predicting categorical variables in manyresearch works, includingAlbrecht et al. (2021);Kumar et al. (2020); Hengl et al. (2018);Latifovic et al. (2018);Kuhn et al. (2018);Sahoo and Jha (2017);Othman and Gloaguen (2017);Cracknell and Reading (2015, 2014);Yu et al. (2012). Even though classiﬁcation machine learning
methods (e.g., random forest, support vector machines) exhibitcompetitive spatial prediction performance, they do not exactly honorthe categorical target variable's observed values at sampling locations byconstruction. On the other side, competitor geostatistical techniques(such as regression-kriging of indicators) perfectly match the categoricaltarget variable's observed values at sampling locations by essence. Inmany geoscience applications, it is desirable to perfectly match theobserved values of the categorical target variable at sampling locations,especially when the categorical target variable's measurements can bereasonably considered error-free (hard data).This work addresses the problem of exact conditioning of machinelearning methods for the spatial prediction of categorical variables. It in-troduces a classiﬁcation random forest-based method in which the cate-gorical target variable is exactly conditioned to the data, thus having theexact conditioning property like competitor geostatistical methods. Therandom forest popularity for spatial prediction relies on its ability to ef ﬁ- ciently deal with many predictor variables, handle complex nonlinear re-lationships and interactions, and require less data pre-processing, and be anon-parametric method. The proposed approach extends a previous workdedicated to continuous target variables by using an implicit representationof the categorical target variable (Fouedjio, 2020). The exact conditioning to the data is achieved through a step-by-step approach. First, classi ﬁcation
Fig. 1.Signed distance transform approach - (a) categorical spatial variable with two categories; (b) signed distance function associated with category 0 ; (c) signed distance function associated with category 1.
Table 1Simulated data example - simulation parameters.
Mean Covariance functionType Scale SillX
1(⋅) 10 Gaussian 12 1X
2(⋅) 10 Exponential 7 1X
3(⋅) 10 Cardinal Sine 1 1X
4(⋅) 10 Cubic 20 1ϵ(⋅) 0 Spherical 10 500F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
83tree predictors' ensemble (categorical) resulting from the traditional clas-siﬁcation random forest is transformed into an ensemble of signed distances(continuous) corresponding to each category of the categorical target var-iable. Second, an orthogonal representation of the ensemble of signed dis-tances is created through the principal component analysis. Third, the exactconditioning problem is reformulated as a system of linear inequalities onprincipal component scores, thus allowing the sampling of new principalcomponent scores (via the randomized quadratic programming), ensuringthe exact conditioning to the data. Fourth, the resulting conditional signeddistances are turned out into an ensemble of categorical outputs, whichperfectly honor the categorical target variable's observed values at samplinglocations. Finally, the majority vote is used to aggregate the ensemble ofcategorical outputs. Theﬁnal output also matches the target variable'sobserved values at sampling locations by construction. On the one hand, theproposed method's effectiveness is illustrated on a simulated dataset forwhich the ground truth is available. On the other hand, the proposedtechniqueis exhibitedon a real-world datasetcomprising geochemicaldata.A comparison is also made with geostatistical and classical machinelearning methods (regression-kriging of indicators, random forest, andsupport vector machines).The remainder of the paper is structured as follows. Section 2de- scribes the different ingredients required to apply the proposed method.
Fig. 2.Simulated data example - (a), (b), (c), (d) predictor variables, (e) exhaustive categorical target variable, and (f) sampled categorical target var iable.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
84Fig. 3.Simulated data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machines ; (c) cat- egorical target variable’observed values at training locations. The misclassi ﬁcation rate in the training data is 18.20% and 35.00%, respectively, for the traditional random forest and support vector machines.
Fig. 4.Simulated data example -B¼10 000 unconditionalﬁrst PC scores andT¼1000 conditionalﬁrst PC scores.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
85Section3demonstrates the proposed approach's effectiveness on a syn-thetic dataset as well as a real-world dataset. A comparison with geo-statistical and classical machine learning methods is provided. Section 4 offers concluding remarks.2. MethodologyLet {C(s):s2D} be the categorical target variable deﬁned on aﬁxed continuous spatial domain of interest D⊂R
d, with aﬁnite set of possible categorical outputs (categories) {c
1,…,c K} which are mutually exclusive and collectively exhaustive. There exist ncategorical target variable's observed valuesfCðs
iÞgi¼1;…;n (hard data) corresponding to samplinglocationsfs
i2Dgi¼1;…;n . In addition to the categorical target variable,there is a set of predictor variables {x
1(s),…,xp(s):s2D} exhaustively known in the spatial domainD. We address the problem of predicting thecategorical target variable over the spatial domain Drepresented byN grid locations using the categorical target variable's observed values andpredictor variables' data. In addition, the categorical target variable'spredicted values at sampling locations must be the same as the categor-ical target variable's observed values at sampling locations, i.e., ^Cðs
iÞ¼ Cðs
iÞ;i¼1;…;n. The description of the different ingredients needed toimplement the proposed exact conditioning method is given in this sec-tion. The implementation is carried out in the R platform ( R Core Team, 2020).2.1. Random forest classiﬁerTheﬁrst step of the proposed method consists of training the tradi-tional random forest (RF) classiﬁer on the data. Random forest classiﬁer is an ensemble method where several individual decision trees aretrained on various subsets of the training dataset (bootstrap samples)using different subsets of available predictor variables, followed by anaggregation (Breiman, 2001). The bootstrapping of the training data andthe random selection of subsets of predictor variables ensure that eachdecision tree in the random forest is unique, which reduces the overallvariance of the random forest classiﬁer. For theﬁnal decision, the RF classiﬁer aggregates the decisions of individual trees through a votingscheme such as the majority voting, i.e., for each observation, each de-cision tree votes for one category, and RF chooses the category with thehighest number of votes.Classiﬁcation random forest has some tuning parameters that can beoptimized. There are, among others, the number of trees, number ofpredictor variables randomly selected at each node, proportion of ob-servations to sample in each decision tree, and minimum number ofobservations in a decision tree's terminal node. These hyperparametersare optimized via cross-validation. In practice, there is no need to tunethe number of decision trees; it is usually recommended to set it to a largenumber, allowing the convergence of the prediction error to a stableminimum (Hengl et al., 2018). The implementation of the classiﬁcation random forest is carried out using the R packages ranger(Wright and
Fig. 5.Simulated data example - prediction map for (a) traditional classi ﬁcation random forest, (b) support vector machines, (c) regression-kriging of indicators, and (d) classiﬁcation random forest with exact conditioning.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
86Ziegler, 2017) andtuneRanger(Probst et al., 2018). The training of the classical random forest classi ﬁer results in an ensemble of classiﬁcation tree predictorsf~C
bðsÞ:s2Dgb¼1;…;B , where B is the number of decision trees. As the traditional random forest classi ﬁer is not explicitly designed to match the data perfectly, classi ﬁcation tree predictors and the aggregated classiﬁcation tree predictors do not necessarily match the categorical target variable's observed values atsampling locations. Sincef~C
bðsÞ:s2Dgb¼1;…;B do not match data perfectly, they will be called“unconditional classiﬁcation tree pre- dictors”. The next steps aim to generate conditional classi ﬁcation tree predictors that perfectlyﬁt the categorical target variable's observedvalues at sampling locations.2.2. Signed distance transformThe second step of the proposed method includes transforming theunconditional classiﬁcation tree predictors’ensemble (categorical) intoan ensemble of unconditional signed distance functions (continuous)corresponding to each category of the categorical target variable. Thecategorical target variable {C(s):s2D} withKcategoriesfckgk¼1;…;K can be viewed as a variable that creates distinct boundaries in the study re-gionD. Each categoryc
k(k¼1,…,K) can be codiﬁed by a binary variable indicating their presence or absence: I
k(s)¼1i fC(s)¼c k, andI k(s)¼0i f C(s)6¼c
k,8s2D. Each categoryc kcan be represented by a signed dis-tance functionφ
k(⋅) such thatc k¼{s2D,φ k(s)/C200}. The signed distance transform approach (Grevera, 2007;Davies, 2012) can be used to transform each categoryc
k({Ik(s):s2D}) into a signed distance function φ
k(⋅). Indeed, each categoryc kdeﬁnes ap-dimensional binary image {I
k(s):s2D} where each point (pixel) has either a value of 1 indicatingthe presence of the categoryc
kor a value of 0 indicating the absence ofthe categoryc
k. For every point (pixel) set to 1, a distance transformassigns a value indicating the negatively signed distance from that point(pixel) to the nearest point (pixel) set to 0. Similarly, for every point(pixel) set to 0, a distance transform assigns a value indicating thepositively signed distance from that point (pixel) to the nearest point(pixel) set to 1. An illustration of the signed distance transform method isgiven inFig. 1. Additionally, the signed distance transformation isone-to-one. The bijectivity is obtained using the following rule:CðsÞ¼argmin
c1;…;c Kðφ1ðsÞ;…;φKðsÞÞ;8s2D:(1)Thus, the ensemble of unconditional classi ﬁcation tree predictors f~C
bðsÞ:s2Dgb¼1;…;B is converted to an ensemble of unconditional
Fig. 6.Simulated data example - prediction uncertainty (entropy) map for (a) traditional classi ﬁcation random forest, (b) support vector machines, (c) regression- kriging of indicators, and (d) classiﬁcation random forest with exact conditioning.
Table 2Simulated data example - predictive performance statistics in the testing datasetcontaining 39 500 observations.
Methods Accuracy Rand indexRandom Forest 0.619 0.734Support Vector Machines 0.602 0.729Regression-Kriging of Indicators 0.623 0.746Random Forest with Exact Conditioning 0.639 0.747F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
87signed distance functions/C8~φbkðsÞ:s2D/C9b¼1;…;B for each categoryc k(k ¼1,…,K) via the signed distance transform approach previouslydescribed. The following idea uses principal component analysis tocreate an orthogonal representation of the ensemble of unconditionalsigned distance functions. Then, the exact conditioning problem isreformulated as a linear inequality problem on the principal compo-nent scores. Then, new principal component scores that ensure theexact conditioning to the hard data are generated through the ran-domized quadratic programming. Since the principal componentorthogonalization is bijective, conditional signed distance functions areobtained by reconstruction. The combination rule de ﬁned in Eq.(1)is applied to obtain conditional classiﬁcation tree predictors (categorical) that exactly match the categorical target variable's observed values atsampling locations.2.3. Principal component analysisThis step consists of performing principal component analysis (PCA)on each ensemble of unconditional signed distance functions/C8~φbkðsÞ:s2D/C9b¼1;…;B ðk¼1;…;KÞ. This results in the following decomposition inﬁnite dimensions:~φ
bkðsÞ¼XLl¼1αbl;kψl;kðsÞ;8s2D;b¼1;…;B;k¼1;…;K;(2)wheref
αbl;kgl¼1;…;L are principal component scores (coefﬁcients) and f
ψl;kðsÞ:s2Dgl¼1;…;L are principal components factors (eigen-functions);L¼min(B,N).For each categoryc
k(k¼1,…,K), PCA is applied to a matrixΓ k(B/C2 N) arranged as a set ofBrow vectors, each representing a single un-conditional signed distance function/C8~φ
bkðsÞ:s2D/C9. PCA is paralleliz- able for each matrixΓ
k(k¼1,…,K). In Eq.(2), the ensemble /C8~φ
bkðsÞ:s2D/C9b¼1;…;B can be viewed as a set of images and/C8~φbkðsÞ:s2D/C9 as an image. Thus, the resulting principal components factorsf
ψl;kðsÞ:s2Dgl¼1;…;L are images as well. Hence, Eq.(2)provides a decomposition of the images into a set of eigen-images and a set of co-efﬁcients. It is important to note that in the PCA framework, the eigen-functions are consideredﬁxed, while the coefﬁcients are considered
Fig. 7.Real-world data example - some predictor variables: (a) elevation, (b) Landsat 8 band 6, (c) gravity survey high-pass ﬁltered Bouguer anomaly, (d) potassium counts from gamma ray spectrometry.
Fig. 8.Real-world data example - (a) categorical target variable and (b) training and testing locations.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
88random. The PCA is used here more as an orthogonal decompositionmethod than a dimension reduction technique since all the principalcomponent factors are kept, as shown in Eq. (2). The bijective nature of PCA allows the reconstruction of signed distance functions from co-efﬁcients. In other words, an image can be reconstructed back once allthe principal component factors and scores are used.2.4. Randomized quadratic programmingGiven the PCA decomposition of the ensemble of unconditionalsigned distance functions as described in Sect. 2.3, this step consists ofgenerating new principal component scores such that signed distancefunctions deﬁned in Eq.(2)perfectly match the data. Letφ
kðsÞ¼XLl¼1θl;kψl;kðsÞ;8s2D;k¼1;…;K;(3)wherefθ
l;kgl¼1;…;L are random coefﬁcients andf ψl;kðsÞ:s2Dgl¼1;…;L are principal component factors derived from the PCA decomposition ofunconditional signed distance functions as given in Eq. (2). The categorical target variable's observed values at sampling loca-tions (hard data) inform the sign of the signed distance function associ-ated with a category at sampling locations. Thus, the set of hard data canbe converted into a set of inequality constraints using Eq. (3). Let assume that at the sampling locations
1, the categoryc 2is observed, i.e.,C(s 1)¼ c
2. This means that the signed distance function associated with thecategoryc2should be negative at locations 1(φ2ðs1Þ/C200), and the signed distance functions associated with other categories should be positive atlocations
1(φkðs1Þ/C210;8k6¼2;k¼1;…;K). For eachφkðk¼1;…;KÞ, the conditioning to all data locations is expressed by the followinginequalities:8<:/C21φ
kðs1Þ¼θ 1;kψ1;kðs1Þþθ 2;kψ2;kðs1Þþ⋯þθ L;kψL;1ðs1Þ/C200o r/C210 …/C21φ
kðsnÞ¼θ 1;kψ1;kðsnÞþθ 2;kψ2;kðsnÞþ⋯þθ L;kψL;kðsnÞ/C200o r/C210:(4)In Eq.(4), theninequalities corresponding tonhard data can be summarized as:~Ψ
kθk/C200;k¼1;…;K: (5) New PC scores vectorθ
k¼ðθ 1;k;…;θ L;kÞTthat matches data are generated by solving the following randomized quadratic optimizationproblem (Fouedjio et al., 2021a):min
θk2RL/C0ðθ
k/C0βkÞTΣ/C01kðθk/C0βkÞ/C1subject to~Ψ kθk/C200;k¼1;…;K:(6) whereβ
k/C24Nð μk;ΣkÞ, and the mean μkand the covariance matrixΣ kare computed using unconditional PC scoresf
αbl;kgl¼1;…;L derived from the PCA of unconditional signed distance functions given in Eq. (2). Speciﬁcally,
Fig. 9.Real-world data example - predicted categorical target variable at training locations by (a) classical random forest and (b) support vector machine s; (c) categorical target variable’observed values at training locations.
μk¼"1BX Bb¼1αbl;k#
l¼1;…;L ;Σ k¼1B/C01X Bb¼1ðαbk/C0μkÞðαbk/C0μkÞT;with αbk¼hαbl;ki
l¼1;…;L : (7)F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
89For each sampleβtk/C24Nð μk;ΣkÞðt¼1;…;TÞ, quadratic programming (Goldfarb and Idnani, 1983) is performed toﬁnd a solutionθ
tkthat sat- isﬁes the inequality constraints and minimizes the quadratic objectivefunction in Eq.(6). The covariance matrixΣ
kin Eq.(7)is a diagonal matrix because the PC scores are uncorrelated by construction. Co-efﬁcientsθ
tkcan be also generated via the Gibbs sampling method(Fouedjio et al., 2021b). However, this approach can be time-consumingfor very large datasets since Gibbs samples are highly correlated.Given conditional PC scoresfθtkgt¼1;…;T , conditional signed distance functions are obtained by reconstruction:φ
tkðsÞ¼XLl¼1θtl;kψl;kðsÞ;8s2D: (8)
Fig. 10.Real-world data example - unconditional and conditional ﬁrst two PC scores associated with each category.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
90Fig. 11.Real-world data example - prediction map provided by (a) traditional classi ﬁcation random forest, (b) support vector machines, and (c) regression-kriging of indicators, and (d) classiﬁcation random forest with exact conditioning.
Fig. 12.Real-world data example - prediction uncertainty (entropy) map provided by (a) traditional classi ﬁcation random forest, (b) support vector machines, (c) regression-kriging of indicators, and (d) classi ﬁcation random forest with exact conditioning.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
91Conditional classiﬁcation tree predictors are given by applying thecombination rule deﬁned in Eq.(1): C
tðsÞ¼argmin
c1;…;c K/C0φt1ðsÞ;…;φtKðsÞ/C1;8s2D;t¼1;…;T:(9)
Since all the individual reconstructed signed distance functionsfφ
tkðsÞ:s2Dgt¼1;…;Tðk¼1;…;KÞperfectly match the hard data, condi-tional classiﬁcation tree predictorsfC
tðsÞ:s2Dgt¼1;…;T do also. The ag- gregation of the conditional classiﬁcation tree predictors using the majority vote rule leads to theﬁnal outcomef^CðsÞ:s2Dg. This latter coincides with the categorical target variable observed values at sam-pling locations.The number of unconditional classiﬁcation tree predictorsBshould be large enough to allow good coverage of the solution space whenperforming the exact conditioning. Indeed, the number of categoricaltarget variable’observations deﬁnes the number of inequalities con-straints as shown in Eq.(4). The larger is the number of unconditionalclassiﬁcation tree predictors, the wider is the solution space of Eq. (6). So, too many constraints (hard data) relative to too few unconditional clas-siﬁcation tree predictors will lead to too small uncertainty. It is worthmentioning that the number of conditional classi ﬁcation tree predictorsT does not depend on the number of unconditional classi ﬁcation treepredictorsBunder randomized quadratic programming. That is to say, T can be smaller or greater thanB.To summarize, the proposed classiﬁcation random forest with exact conditioning is performed using the following pseudo algorithm:
3. Empirical examplesThe proposed classiﬁcation random forest with exact conditioning isillustrated using simulated and real-world datasets. A prediction perfor-mance comparison is carried out with a geostatistical method (regres-sion-kriging of indicators) and traditional machine learning techniques(random forest and support vector machines). Hyper-parameters associ-ated with each machine learning method have been optimized throughcross-validation. The proposed classiﬁcation random forest with exact conditioning uses the same ensemble of decision trees generated by theclassical random forest.The predictive performance of each method is assessed on a testingdataset using theﬁrst evaluation statistic, i.e., the accuracy. The accuracycorresponds to the percentage of observations that are correctly classi-ﬁed. It has a value between 0 and 1. The higher is the accuracy, the betteris the model. In addition to the accuracy, the Rand index is calculated.The Rand index measures the similarity between the predicted classi ﬁ- cation and true classiﬁcation on the testing data by considering all pairsof points and counting pairs that are assigned in the same or differentcategory in the predicted and true classiﬁcations. The Rand index has a value between 0 and 1, with 0 indicating that the two classi ﬁcations do not agree on any pair of points and 1 indicating the same classi ﬁcations.
3.1. Simulated data exampleIn this simulated case study, we consider a categorical target variablewith four categories, and four continuous predictor variables de ﬁnedTable 3Real-world data example - predictive performance statistics in the testing datasetcontaining 140 observations.
Methods Accuracy Rand indexRandom Forest 0.386 0.649Support Vector Machines 0.342 0.575Regression-Kriging of Indicators 0.336 0.672Random Forest with Exact Conditioning 0.421 0.710F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
92over the spatial domain [0,100]2. The categorical target variable isgenerated according to the following model:8s2½0;100/C138
2;CðsÞ¼c k ifYðsÞ2½q k;qkþ1½;k¼1;…;4;(10)withYðsÞ¼50sinðX
1ðsÞÞ þ3X 1ðsÞX 2ðsÞþX 3ðsÞ2þ50sinðX 4ðsÞÞ þϵðsÞ; X
1(⋅),X 2(⋅),X 3(⋅), andX 4(⋅) are predictor variables; andϵ(⋅) is a latent (non-observed) variable; the limits q
j's are taken as the 0, 0.25, 0.50, 0.75, and 1 quantiles of the random function Y(⋅), so that the complete system of eventsc
k(k¼1,…, 4) can be deﬁned. The four predictor variables and the latent variable are simulated onthe spatial domain [0,100]
2based onﬁve independent Gaussian isotropic stationary random functions ( Chiles and Delﬁner, 2012) with different speciﬁcation of means and covariance functions as given inTable 1. The simulation is performing using the R package RGeostats package (Renard et al., 2020).Fig. 2presents the simulated data over a 200/C2200 regular grid. The map of the categorical target variable dis-played inFig. 2e is considered as the reference map.To demonstrate the proposed method's ability to exactly match thecategorical target variable's observed values at sampling locations, n¼ 500 stratiﬁed random samples are taken as the training data ( Fig. 2). The set ofn¼500 stratiﬁed random samples amounts to 1.25% of total lo-cations in the reference map and each category contains 125 samples.The rest of data (39 500 samples) is kept aside for the testing. The goal isto reconstruct the reference map of the categorical target variable(Fig. 2e) using the sampled categories (Fig. 2f) with an aid of the observed four spatial auxiliary variables ( Fig. 2a - d) such that the cate- gorical target variable's predicted values coincide with the categoricaltarget variable's observed values at sampling locations.Fig. 3shows the categorical target variable's observed values attraining locations and those predicted by the traditional random forestand support vector machines. There is a signi ﬁcant disagreement be- tween the observed values and the predicted values of categorical targetvariable at training locations. The misclassi ﬁcation rate in the training data is 18.20% and 35.00%, respectively, for the traditional randomforest and support vector machines. For the traditional random forest, thenumber of decisions trees has been set to 10 000, and the hyper-parameters have been optimized through cross-validation. Concerningthe support vector machines, the kernel function and the hyper-parameters have been selected using cross-validation.The traditional classiﬁcation random forest is performed on thetraining data with a large number of decision trees set to B¼10 000. Thus, an ensemble ofB¼10 000 unconditional classiﬁcation tree pre- dictorsf~C
bðsÞ:s2½0;100/C1382gb¼1;…;10 000 is constructed. This latter ensemble is transformed into an ensemble of unconditional signed dis-tance functionsn~φ
bkðsÞ:s2½0;100/C1382o
b¼1;…;10 000 for each categoryc k(k ¼1,…, 4), according to the methodology described in Sect. 2. Principalcomponent analysis is performed on this ensemble, followed by thesampling of new PC scores ensuring the data's exact conditioning. T¼ 1000 new PC scores are generated, thus giving an ensemble of T¼1000 conditional signed distance functionsnφ
tkðsÞ:s2½0;100/C1382o
t¼1;…;1000 (k ¼1,…, 4). This latter are turned into conditional classi ﬁcation tree predictorsfC
tðsÞ:s2½0;100/C1382gt¼1;…;1000 that perfectly match the cate- gorical target variable's observed values at sampling locations. The ma-jority vote scheme is then used to derive the predicted categorical targetvariablef^CðsÞ:s2½0;100/C138
2g. This latter also perfectly matches the cat-egorical target variable's observed values at sampling locations byconstruction.Fig. 4shows PC scores before the conditioning (unconditional PCscoresf
αlkgl¼1;…;10 000 ) and after the conditioning (conditional PC scoresfθ
tkgt¼1;…;1000 ) for each categoryc k(k¼1,…, 4). One can notice that the points cloud of conditional PC scores is less scattered than those fromunconditional PC scores due effectively to the exact conditioningexpressed as a system of linear inequalities.Fig. 5presents prediction maps provided by the traditional classi ﬁ- cation random forest, support vector machines, regression-kriging ofindicators, and the proposed classiﬁcation random forest with exact conditioning. One can notice the prediction maps provided by regression-kriging of indicators (Fig. 5c) and the proposed classiﬁcation random forest (Fig. 5d) are exactly conditioned to the training data ( Fig. 2f), which is not the case for the ones provided by traditional classi ﬁcation random forest (Fig. 5a) and support vector machines (Fig. 5b). The general appearance of prediction maps resulting from the traditionalclassiﬁcation random forest and the proposed classi ﬁcation random forest with exact conditioning looks similar. However, there are somelocal differences due to the exact conditioning nature of the proposedmethod. It is important to highlight that the proposed classi ﬁcation random forest with exact conditioning uses the ensemble of decisiontrees generated by the classical random forest as starting point. Overall,the prediction map of the proposed classiﬁcation random forest exhibits more similar spatial patterns present in the reference map ( Fig. 2e) than the regression-kriging of indicators.The traditional classiﬁcation random forest, support vector machines,regression-kriging of indicators, and proposed classi ﬁcation random forest with exact conditioning provide the probabilities for each possibleoutcome of the categorical target variable at any spatial location. Thus,the prediction uncertainty can be quantiﬁed through information en- tropy (Wellmann and Regenauer-Lieb, 2012 ).Fig. 6presents the pre- diction uncertainty maps associated with each method. Theregression-kriging of indicators and the proposed classi ﬁcation random forest provide zero entropy at sampling location by construction whilethe traditional classiﬁcation random forest and support vector machinesdo not. The traditional classiﬁcation random forest, support vector ma-chines, and proposed classiﬁcation random forest provide lower entropyin local neighborhoods dominated by a single category compared to theregression-kriging of indicators.The predictive performance statistics in the testing dataset (contain-ing 39 500 observations) for the traditional classi ﬁcation random forest, support vector machines, regression-kriging of indicators, and proposedclassiﬁcation random forest with exact conditioning are reported inTable 2. In addition to exactlyﬁtting the categorical target variable'sobserved values at sampling locations, the proposed classi ﬁcation random forest maintains a competitive out-of-sample predictiveperformance.3.2. Real-world data exampleIn this real case study, the categorical target variable is Tl (Thallium)geochemical concentration transformed into ﬁve categories through quantiles and observed at 568 locations over the study region in south-west England (Kirkwood et al., 2016). Predictor variables include elevation, gravity, magnetic, Landsat, radiometric, and their derivatives,totaling 26 predictor variables. Some predictor variables are displayed inFig. 7.Fig. 8a shows the categorical target variables's observations. Theobservations are partitioned into a training set ( /C2475%) and testing set (/C2425%) as shown inFig. 8b. The testing set is built such that all thecategories have roughly the same number of observations.Fig. 9shows the categorical target variable's observed values attraining locations and those predicted by the classical random forest andsupport vector machines. There is a considerable disagreement betweenthe observed values and the predicted values of categorical target vari-able at training locations. The misclassiﬁcation rate in the training data is 33.80% and 52.58%, respectively, for the classical random forest andsupport vector machines. For the classical random forest, the number ofdecisions trees has been set to 10 000, and the hyper-parameters havebeen optimized through cross-validation. Regarding the support vectormachines, the kernel function and the hyper-parameters have beenselected using cross-validation.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
93The learned conventional random forest model consists of anensemble ofB¼10 000 classiﬁcation tree predictors. This latterensemble is transformed into an ensemble of unconditional signed dis-tance functions for each category, following by PCA and randomizedquadratic programming as described in the methodology section (Sect.2). This results to unconditional and conditional PC scores as shown inFig. 10;T¼1000 conditional PC scores are generated. As mentioned inthe simulated data example, the points cloud of conditional PC scores isless spread out than those from unconditional PC scores because of theexact conditioning.Prediction maps provided by the traditional classi ﬁcation random forest, support vector machines, regression-kriging of indicators, andproposed classiﬁcation random forest with exact conditioning aredepicted inFig. 11. The prediction map provided by each method differsnotably. In particular, the prediction map of the proposed classi ﬁcation random forest is different from the one provided by the traditionalclassiﬁcation random forest. This is explained by the exact conditioningnature of the proposed method. It is important to highlight that theproposed classiﬁcation random forest with exact conditioning uses thesame ensemble of decision trees generated by the traditional classi ﬁca- tion random forest. Although the regression-kriging of indicators pro-vides exact conditioning, its prediction map shows a noisier spatialdistribution of categories (Fig. 11c). In contrast, the prediction mapprovided by the proposed classiﬁcation random forest with exactingconditioning depicts more regular and continuous contours ( Fig. 11d) and is consistent with the training data shown in Fig. 9c. Fig. 12presents the prediction uncertainty (entropy) map under thetraditional classiﬁcation random forest, support vector machines,regression-kriging of indicators, and proposed classi ﬁcation random forest with exact conditioning. The prediction uncertainty map resultingfrom the proposed classiﬁcation random forest with exact conditioningdiffers signiﬁcantly from the others. In particular, the prediction uncer-tainty map provided by the traditional and proposed classi ﬁcation random forest differ notably due to the exact conditioning in the pro-posed method and not in the conventional method. Under the proposedclassiﬁcation random forest, local neighborhoods dominated by only onecategory show lower entropy than local neighborhoods dominated byseveral categories of the target variable.Table 3provides the predictive performance of the traditional clas-siﬁcation random forest, support vector machines, regression-kriging ofindicators, and proposed classiﬁcation random forest with exact condi-tioning in the testing dataset (containing 140 observations). The pro-posed classiﬁcation random forest shows better predictive performancethan the three other methods according to the accuracy. The cost of non-using the proposed classiﬁcation random forest in this case is not negli-gible. There is an accuracy improvement of 25% and 9% respectively,compared to the regression-kriging of indicators and traditional classi-ﬁ
cation random forest. Thus, the proposed approach can exactly matchthe categorical target variable's observed values at sampling locationswhile achieving good out of sample predictive performance.4. ConclusionThis paper proposed a classiﬁcation random forest-based method forthe spatial prediction of categorical variables in which the categoricaltarget variable is exactly conditioned to the data. The exact conditioningmeans that the predicted values of the categorical target variable atsampling locations are the same as those observed at sampling locations.This property is well-known in geostatistical methods. The proposedmethod combines classiﬁcation random forest, signed distance functions,principal component analysis, and randomized quadratic programmingto achieve the exact conditioning of the categorical target variable to thedata. The effectiveness of the proposed method has been demonstratedon simulated and real datasets.Typical characteristics of the proposed method are the following. Itcan perfectly match the categorical target variable's observed values atsampling locations while achieving good out-of-sample predictive per-formance compared to competitor geostatistical methods such asregression-kriging of indicators. It is easy to implement since it combineswell-known existing statistical and machine learning methods. It caneasily handle a large number of categories consistently through thesigned distance representation. The proposed method can provide real-istic prediction uncertainties of the categorical target variable. It has theadvantage of not producing noisy spatial prediction maps, as one canobserve for regression-kriging of indicators. The proposed method is freeof the inherent problems of regression-kriging of indicators such as thepredicted probabilities of the target variable's categories that are notguaranteed to belong to the [0, 1] interval and sum up to one. Updatingthe categorical target variable predictive map when few observations areadded can be carried out quickly. Only the last part of the proposedmethod, i.e., the randomized quadratic programming, should beperformed.The proposed method is computationally intensive compared toregression-kriging of indicators and conventional classi ﬁcation random forest. However, it comprises components that can be performed inparallel according to the target variables' categories, including condi-tional principal component scores generation. The proposed method re-quires that the number of unconditional classi ﬁcation tree predictors should be large enough to allow good coverage of the solution spacewhen performing the exact conditioning. Indeed, the number of cate-gorical target variable’observations deﬁnes the number of inequalities constraints. The larger is the number of unconditional classi ﬁcation tree predictors, the wide is the solution space for the exact conditioning. So,too many constraints (hard data) relative to too few unconditional clas-siﬁcation tree predictors will lead to too small uncertainty. Nonetheless,it will always be possible to meet this constraint because the number ofunconditional classiﬁcation tree predictors is a free parameter. Althoughthe proposed method uses the random forest as the base learner, it can beused with other ensemble machine learning methods (e.g., boosting).Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.ReferencesAlbrecht, T., Gonz/C19alez-/C19Alvarez, I., Klump, J., 2021. Using machine learning to mapWestern Australian landscapes for mineral exploration. ISPRS Int. J. Geo-Inf. 10 . Allard, D., D’Or, D., Froidevaux, R., 2011. An ef ﬁcient maximum entropy approach for categorical variable prediction. Eur. J. Soil Sci. 62, 381 –393. Bogaert, P., 2004. Spatial prediction of categorical variables: the bme approach. In:Sanchez-Vila, X., Carrera, J., G /C19omez-Hern/C19andez, J.J. (Eds.), geoENV IV— Geostatistics for Environmental Applications. Springer Netherlands, Dordrecht,pp. 271–282.Breiman, L., 2001. Random forests. Mach. Learn. 45, 5 –32. Cao, G., Kyriakidis, P., Goodchild, M., 2011. A multinomial logistic mixed model forprediction of categorical spatial data. Int. J. Geogr. Inf. Sci. 25, 2071 –2086. Cao, G., Yoo, E.H., Wang, S., 2014. A statistical framework of data fusion for spatialprediction of categorical variables. Stoch. Environ. Res. Risk Assess. 28, 1785 –1799. Chiles, J.P., Delﬁner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley & Sons.Cracknell, M., Reading, A., 2015. Spatial-contextual supervised classi ﬁers explored: a challenging example of lithostratigraphy classi ﬁcation. IEEE J. Select. Topics Appl. Earth Observ. Remote Sens. 8, 1–14. Cracknell, M.J., Reading, A.M., 2014. Geological mapping using remote sensing data: acomparison ofﬁve machine learning algorithms, their response to variations in thespatial distribution of training data and the use of explicit spatial information.Comput. Geosci. 63, 22–33. Davies, E., 2012. Chapter 9 - binary shape analysis. In: Davies, E. (Ed.), Computer andMachine Vision, fourth ed. Academic Press, Boston, pp. 229 –265. Du, P., Bai, X., Tan, K., Xue, Z., Samat, A., Xia, J., Li, E., Su, H., Liu, W., 2020. Advances offour machine learning methods for spatial data handling: a review. J. Geovisual.Spatial Anal. 4.Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.Artiﬁ. Intel. Geosci. 1, 11–23. Fouedjio, F., Scheidt, C., Yang, L., Achtziger-Zupan /C20ci/C20c, P., Caers, J., 2021a. A geostatistical implicit modeling framework for uncertainty quanti ﬁcation of 3DF. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
94geo-domain boundaries: application to lithological domains from a porphyry copperdeposit. Comput. Geosci. 157, 104931 . Fouedjio, F., Scheidt, C., Yang, L., Wang, Y., Caers, J., 2021b. Conditional simulation ofcategorical spatial variables using Gibbs sampling of a truncated multivariate normaldistribution subject to linear inequality constraints. Stoch. Environ. Res. Risk Assess.35, 457–480.Giaccone, E., Oriani, F., Tonini, M., Lambiel, C., Mari /C19ethoz, G., 2021. Using data-driven algorithms for semi-automated geomorphological mapping. Stoch. Environ. Res. RiskAssess. 1–17.Goldfarb, D., Idnani, A., 1983. A numerically stable dual method for solving strictlyconvex quadratic programs. Math. Program. 27, 1 –33. Goovaerts, P., 2001. Geostatistical modelling of uncertainty in soil science. Geoderma103, 3–26.Grevera, G.J., 2007. Distance Transform Algorithms and Their Implementation andEvaluation. Springer New York, New York, NY, pp. 33 –60. Hengl, T., Heuvelink, G.B., Stein, A., 2004. A generic framework for spatial prediction ofsoil variables based on regression-kriging. Geoderma 120, 75 –93. Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr €aler, B., 2018. Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables.PeerJ 6, e5518.Hengl, T., Toomanian, N., Reuter, H.I., Malakouti, M.J., 2007. Methods to interpolate soilcategorical variables from proﬁle observations: lessons from Iran. Geoderma 140, 417–427. Pedometrics 2005. Kanevski, M., 2008. Advanced Mapping of Environmental Data: Geostatistics, MachineLearning and Bayesian Maximum Entropy. John Wiley &Sons. Kanevski, M., Pozdnoukhov, A., Timonin, V., 2009. Machine Learning for SpatialEnvironmental Data: Theory, Applications, and Software. EPFL press . Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016. Stream sediment geochemistry as atool for enhancing geological understanding: an overview of new data from southwest England. J. Geochem. Explor. 163, 28 –40. Kuhn, S., Cracknell, M.J., Reading, A.M., 2018. Lithologic mapping using random forestsapplied to geophysical and remote-sensing data: a demonstration study from theEastern Goldﬁelds of Australia. Geophysics 83, B183 –B193.Kumar, C., Chatterjee, S., Oommen, T., Guha, A., 2020. Automated lithological mappingby integrating spectral enhancement techniques and machine learning algorithmsusing aviris-ng hyperspectral data in gold-bearing granite-greenstone rocks in Hutti,India. Int. J. Appl. Earth Obs. Geoinf. 86, 102006 . Latifovic, R., Pouliot, D., Campbell, J., 2018. Assessment of convolution neural networksfor surﬁcial geology mapping in the South Rae geological region, Northwestterritories, Canada. Rem. Sens. 10 . Maxwell, A.E., Warner, T.A., Fang, F., 2018. Implementation of machine-learningclassiﬁcation in remote sensing: an applied review. Int. J. Rem. Sens. 39, 2784 –2817. Othman, A.A., Gloaguen, R., 2017. Integration of spectral, spatial and morphometric datainto lithological mapping: a comparison of different machine learning algorithms inthe Kurdistan region, NE Iraq. J. Asian Earth Sci. 146, 90 –102. Pardo-Igúzquiza, E., Dowd, P.A., 2005. Multiple indicator cokriging with application tooptimal sampling for environmental monitoring. Comput. Geosci. 31, 1 –13
. Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and tuning strategies forrandom forest. Wiley Interdiscipl. Rev.: Data Min. Knowl. Discov. https://doi.org/ 10.1002/widm.1301.R Core Team, 2020. R: A Language and Environment for Statistical Computing. RFoundation for Statistical Computing, Vienna, Austria. URL. https://www.R-project.o rg/.Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. RGeostats:geostatistical package. URL:http://cg.ensmp.fr/rgeostats. r package version 12.0.1 . Sahoo, S., Jha, M.K., 2017. Pattern recognition in lithology classi ﬁcation: modeling using neural networks, self-organizing maps and genetic algorithms. Hydrogeol. J. 25,311–330.Wellmann, J.F., Regenauer-Lieb, K., 2012. Uncertainties Have a Meaning: InformationEntropy as a Quality Measure for 3-D Geological Models. Tectonophysics, vols.526–529. Modelling in Geosciences, pp. 207 –216. Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for highdimensional data in Cþþand R. J. Stat. Software 77, 1–17. Yu, L., Porwal, A., Holden, E.J., Dentith, M.C., 2012. Towards automatic lithologicalclassiﬁcation from remote sensing data using support vector machines. Comput.Geosci. 45, 229–239.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 82 –95
95