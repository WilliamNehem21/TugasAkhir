Comparison of CNN-based deep learning architectures for ricediseases classiﬁcation
Md Taimur Ahada,⁎,Y a nL ib, Bo Songc, Touhid Bhuiyand
aDepartment of Computer Science and Engineering, Daffodil International University, Savar, Bangladesh
bSchool of Mathematics, Physics and Computing University of Southern Queensland, Toowoomba, Australia
cSchool of Engineering University of Southern Queensland, Toowoomba, Australia
dDepartment of Computer Science and Engineering, Faculty of Science and Information Technology, Daffodil International University, Bangladesh
abstract article info
Article history:Received 10 June 2022Received in revised form 6 July 2023Accepted 7 July 2023Available online 14 July 2023Although convolutional neural network (CNN) paradigms have expanded to transfer learning and ensemblemodels from original individual CNN architectures, few studies have focused on the performance comparisonof the applicability of these techniques in detecting and localizing rice diseases. Moreover, most CNN-basedrice disease detection studies only considered a small number of diseases in their experiments. Both these short-comings were addressed in this study. In this study, a rice disease classi ﬁcation comparison of six CNN-based deep-learning architectures (DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, andSeresnext101) was conducted using a database of nine of the most epidemic rice diseases in Bangladesh. In ad-dition, we applied a transfer learning approach to DenseNet121, MobileNetV2, Resnet152V, Seresnext101, and anensemble model called DEX (Densenet121, Ef ﬁcientNetB7, and Xception) to compare the six individual CNN net- works, transfer learning, and ensemble techniques. The results suggest that the ensemble framework providesthe best accuracy of 98%, and transfer learning can increase the accuracy by 17% from the results obtained bySeresnext101 in detecting and localizing rice leaf diseases. The high accuracy in detecting and categorisationrice leaf diseases using CNN suggests that the deep CNN model is promising in the plant disease detection domainand can signiﬁcantly impact the detection of diseases in real-time agricultural systems. This research is signi ﬁcant for farmers in rice-growing countries, as like many other plant diseases, rice diseases require timely and earlyidentiﬁcation of infected diseases and this research develops a rice leaf detection system based on CNN that is ex-pected to help farmers to make fast decisions to protect their agricultural yields and quality.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Deep learningConvolutional neural networks (CNNs)Transfer learningPlant leaf disease detection
1. IntroductionThe observable success of convolutional neural networks (CNNs) hasshifted the technology in detecting and localizing rice diseases usingtheir leaves. Following the path, recent CNN studies suggest that theuse of CNN has increased in rice leaf disease detection and segmentation(Chen et al., 2021;Patil and Kumar, 2022). Usually, a diseased rice leaf is covered with spots, colours, and diseased shapes ( Mohanty et al., 2016). Thus, a diseased leaf which has a different colour texture and dimensionthan a healthy rice leaf provides an opportunity to perform image anal-ysis using a CNN network and to collect information on inconsistencyamong the pixels of the entire leaf ( Mitkal et al., 2016;Xu et al., 2020). Each of the pixels of a leaf is expected to be similar concerningany characteristic or available property, such as colour, intensity, or tex-ture. However, in a case where a small group of pixels differs fromothers, it provides information about inconsistency in an object or thepresence of other objects.Despite this fact, a handful of research efforts were devoted ( Chen et al., 2018;Akhter et al., 2019;Islam et al., 2018;Sarker et al. 2016) in detecting rice disease using CNN, however, there are still gaps inthe CNN-based rice leaf disease detection research. Firstly, as rice dis-eases are different from country to country, a comprehensive study onmost of the epidemic diseases in a given country should be conducted.The number of types of rice diseases should be increased instead ofusing a few key classes (Acharya et al., 2020). Secondly, a study should inform whether original CNN architectures, transfer learning, or ensem-ble techniques can provide better accuracy for rice leaf diseasedetection. Consequently, this study aims to examine the success oforiginal state-of-the-art CNN architectures, such as DenseNet121(Huang et al., 2017), Inceptionv3 (Adegun and Viriri, 2021), MobileNetV2 (Nagasubramanian et al., 2020), resNext101 (Adegun and Viriri, 2021),Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
⁎Corresponding author at: Department of Computer Science and Engineering, DaffodilInternational University, Savar, BangladeshE-mail addresses:MdTaimur.Ahad@usq.edu.au,taimurahad.cse@diu.edu.bd (M.T. Ahad).
https://doi.org/10.1016/j.aiia.2023.07.0012589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/Resnet152V (Sankupellay and Konovalov, 2018) and Seresnext101 (Chen et al., 2018;He et al., 2015) in classifying the rice plant diseasedetection. The ultimate goal of this research is to reach the modelthat provides the highest accuracy rate in classifying rice leaf diseasedetection. Furthermore, this study also aims to conduct a transferlearning of DenseNet121, MobileNetV2, Resnet152V ( Xie et al., 2017), and Seresnext101 (Chen et al., 2018) networks, and ensemble learning of Densenet121, EfﬁcientNetB7, and Xception (Chollet, 2017)n e t w o r k s .Another motivation to conduct this research is that rice disease de-tection is still preliminary among farmers in most developing countrieswith technical laggards. A commonly used method for rice disease de-tection is still manual visual inspection - simply using naked-eye obser-vation by trained experts (Mohanty et al., 2016). In doing so, a large team of experts, as well as continuous monitoring, are required. This iscostly and time-consuming for poor farmers when a farm is large. Onthe other hand, in some countries, farmers do not have proper mecha-nisms or even have no idea that they can consult with experts. Manualplant disease identiﬁcation is a more laborious task, subject to humanerrors, and can be done only in limited areas. Therefore, automatic de-tection techniques have become increasingly in demand.The main contributions of this paper are as follows: ﬁrstly, this re- search provides a comparison of six original CNN architectures, namelyDenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, andSeresnext101, using a dataset of the nine most epidemic rice diseases inBangladesh. Secondly, we also applied a transfer learning approach onDenseNet121, MobileNetV2, Resnet152V, and Seresnext101 to concludeif transfer learning is capable of increasing accuracy, and thirdly, anensembled model called DEX based on Densenet121, Ef ﬁcientNetB7 and Xception networks were applied to draw a comparison among orig-inal, transfer learning, and ensemble techniques.2. Literature reviewThis section reviews the related research work aiming at a deep-learning network model that can provide better accuracy for rice leafdisease detection. It includes an overview of the basic CNN networksand their various architectural structures. Several image processingtechnologies that were applied to rice leaf disease detection are alsodiscussed.2.1. Types of rice diseasesAs a cereal grain, rice is the most widely consumed staple food forover half of the population in the world (Nguyen-Quoc and Hoang,2020). Like many other developing countries, rice is the major sourceof income for rural farmers, especially in Bangladesh. Therefore, whenrice production is hindered due to various rice diseases, it impacts thenational economy.Rice diseases are commonly involved in an abnormal physiologicalprocess that distorts the rice plant's normal structure, growth, and nu-trition. Each country, however, has different rice diseases that impactpeople differently (Acharya et al., 2020). For example, rice yellow mot- tle disease is not known to occur in other parts of the world, exceptTanzania (Huang et al., 2017). Blast rice disease causes 11–15% yield loss annually (Adegun and Viriri, 2021). Sarker et al. reported that Sheath blight, a rice disease caused by Rhizoctonia solani, affects thecrop in almost every season in Bangladesh. The disease reduces thequality of the rice as well as the quantity of the crop, which in turn af-fects the economy of a country like Bangladesh where agriculture isthe main sector (Nagasubramanian et al., 2020). However, among all the rice diseases, the following are the most adverse rice diseases inBangladesh (Sankupellay and Konovalov, 2018): 1. Bacterial Leaf Blight: Xanthomonas OryzaePvoryzae is responsible for bacterial leaf blight (BL), occurring mostly during the wet season,especially when water overﬂows through riceﬁelds. In some areas in
Asia, it can reduce crop yield by up to 50%, even up to 80% ( Chen et al., 2018).2. Blight:Bacterial panicle blight, also known as blight, is caused bygram-negative bacterial pathogens Burkholderia glumaeand B. gladioli. It is a prevalent disease in many rice-growing regions glob-ally. The symptoms of bacterial panicle blight include several charac-teristic features. Panicle discolouration is a common symptom,where affected panicles may turn dark brown or black. Anothersymptom is grain rot, where the infected grains may exhibit decayor rotting. Additionally, sterileﬂorets, which fail to produce viableseeds, are observed in affected panicles ( Sainath et al., 2015;Zhu and Gong, 2018).3. Brown Spot:Brown leaf spots are another serious rice disease world-wide, caused byBipolaris Oryzae(Breda de Haan) Shoemaker. Thesymptoms are leaf spots throughout the growing season, mostly onthe leaf blade, small spots of dark brown to reddish-brown, circularto oval in shape, while older spots have a light, reddish-brown orgrey centre surrounded by a dark to reddish-brown margin ( Zhu and Gong, 2018).4. Hispa:Rice Hispa, scientiﬁcally known asDicladispa Armigera,i sa major pest that affects rice plants. It is commonly found during thetillering stage of rice growth, with higher populations observed atthis stage. Rice Hispa feeds on the epidermis of the upper leaves,causing scraping damage. In the context of Sylhet, a region inBangladesh, it is mentioned that Rice Hispa forms part of a largeand contiguous population that extends into neighbouring Assam.This suggests that the pest is prevalent and poses a signi ﬁcant prob- lem in both areas (He et al., 2015).5. Leaf Scald:Leaf scald, caused byMicrodochium Oryzae, is a fungal dis- ease. This causes the scalded appearance of leaves. Zonate lesions ofalternating light tan and dark brown starting from leaf tips or edges,oblong lesions with light brown halos in mature leaves. Individual le-sions are 15 cm long and 0.51 cm wide or may almost cover an entireleaf. The continuous enlargement and coalescing of lesions result inthe blighting of a large part of the leaf blade.6. Leaf Smut:Leaf smut, caused by the fungus Entyloma Oryzae, is atype of smut disease that affects rice plants. However, it does not pro-duce smut balls on spikelets. Instead, leaf smut causes dark brown toblack lesions on the leaves of the rice plant. These lesions are com-posed of masses of fungal spores. False smut causes chalkiness ofgrains which leads to a reduction in grain weight. It also reducesseed germination. Velvety smut balls on spikelets, Spore balls are ini-tially orange and turn greenish black when mature, they are someidentiﬁable symptoms of leaf smut. The second stage of infection oc-curs when the spikelet nearly reaches maturity.7. Leaf Blast:Blast disease is a rice disease caused by Rhizoctonia solani.It affects the crop in almost every season in Bangladesh, causing11–15% yield loss annually reported that Sheath blight. The diseasereduces the rice quality as well as its quantity, which in turn affectsthe economy of a country like Bangladesh where agriculture is themain sector (Chen et al., 2018).8. Shath Blight:Sheath blight is caused by the fungal pathogen Rhizoc-tonia solani. It is a widespread disease that affects rice plants in bothtemperate and tropical rice-growing regions. The typical symptomsof sheath blight manifest as oval to irregular lesions on the ricesheath (the protective covering of the stem) and leaf blades. Theselesions typically have a greyish inner colour and a dark brown mar-gin. The distinct colouration helps in identifying and distinguishingthe disease from other rice pathogens.9. Tungro:Tungro is a viral disease that affects rice plants. It is distrib-uted in South and Southeast Asia, including Bangladesh. The diseaseis characterized by several characteristic features, such as stunted
growth of the plant, twisted leaves, reduced tillering (formation ofside shoots) and delayedﬂowering. The main vector responsible forthe transmission of rice tungro virus is the green planthopperM.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
23(Nephotettix virus). In Bangladesh, susceptible rice varieties are par-ticularly susceptible to Tungro disease. Studies have shown thattungro incidences were common in susceptible cultivars, with re-corded incidence rates ranging from 85% to 81% in the south tonorthwest of the country. This indicates a signi ﬁcant impact of the disease on rice cultivation in these regions.2.1. Convolutional neural networksIn CNNs, the term“convolutional”refers to the mathematical opera-tion of convolution, which combines two functions to produce a thirdfunction. In the context of CNNs, convolutional layers apply ﬁlters or ker- nels to the input data, resulting in the generation of feature maps. Thesefeature maps capture different aspects or patterns in the input data.CNNs consist of multiple layers arranged sequentially. The basicstructure typically includes an input layer, followed by severalconvolutional layers, pooling layers, a fully connected layer, and ﬁnally an output layer. The convolutional layers are responsible for extractingfeatures from the input data through the application of ﬁlters. The pooling layers help reduce the spatial dimensions of the feature maps,reducing computational complexity. The fully connected layer connectsthe extracted features to the output layer, allowing the network to makepredictions or perform classiﬁcation based on the learned features.The input and output layers are considered the visible layers of theCNN, while the intermediate layers, such as convolutional and poolinglayers, are referred to as hidden layers. These hidden layers play a cru-cial role in learning and extracting hierarchical representations fromthe input data. Overall, the combination of convolutional layers, poolinglayers, and fully connected layers in CNNs allows the network to effec-tively learn and recognize patterns in complex data, making them par-ticularly well-suited for tasks such as image recognition and computervision (Xie et al., 2017).2.1.1. State-of-the-art CNN architectures. In this study, several CNN archi- tectures are divided into three broad categories, original, transfer learn-ing, and ensemble technique. The architecture of a basic CNN is given inFig. 1.2.1.2. Original CNN networks.An original CNN architecture in this re-search refers to a CNN network and algorithm that is available inKeras or Github. In this research, a CNN algorithm is kept original asﬁrst proposed by its authors and programmers, with no change of pro-cessing units, parameters and hyper-parameter optimization strategies,design patterns and connectivity of layers. Often a well-known CNNnetwork was developed and evolved by different researchers and pro-grammers through various challenges For example, the AlexNet archi-tecture was the winner of ILSVRC 2012 and was proposed byKrizhevsky et al., (Chollet, 2017) ResNet was proposed by He et al.(Chollet, 2017) from Microsoft and won 2015 ILSVRC. DenseNet as anextension of ResNet wasﬁrst proposed in 2016 by Huang et al. fromFacebook (Chollet, 2017;Patil and Kumar, 2022). Several original CNN architectures with the versions are discussed in the next sections:2.1.2.1. DenseNet-121.DenseNet-121 architecture iteratively concat-enates the feature maps from one layer to another layer along the net-work, which is useful for classiﬁcation tasks. The DenseNet-121 modelwas claimed better than MobileNetV2, ResNet50 and NASNet architec-tures (Shujaat et al., 2021).2.1.2.2. Inception V3.Inception V3 developed by Google is the thirdrelease in the Deep Learning Evolutionary Architectures series. The In-ception V3 architecture, which has the Softmax function in the lastlayer, consists of 42 layers in total and the input layer takes imageswith 299 × 299 pixels.2.1.2.3. MobileNetV2.Overall, MobileNetV2's design choices make itwell-suited for deployment on resource-constrained mobile deviceswhile maintaining competitive performance for various computer vi-sion tasks such as image classiﬁcation, object detection, and semantic
segmentation. One key feature of MobileNetV2 is the use of an invertedresidual structure with residual connections between the bottlenecklayers. Inverted residual blocks aim to reduce computational complexitywhile maintaining accuracy. The residual connections help in gradientﬂow and facilitate the training of deeper networks. The intermediate ex-pansion layer in MobileNetV2 incorporates lightweight depth-wise con-volutions (referred to as depth-wise separable convolutions) tointroduce non-linearity andﬁlter features efﬁciently. Depth-wise con- volutions separate the spatial and channel-wise convolutions, reducingthe computational cost while retaining the expressive power of the net-work.The architecture of MobileNetV2 typically starts with an initial fullyconvolutional layer with 32ﬁlters. This layer processes the input data
Fig. 1.The architecture of a basic CNN.M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
24and extracts initial features. It is followed by 19 residual bottlenecklayers, which are the primary building blocks of the network. These bot-tleneck layers further reﬁne and transform the features, utilizing theinverted residual structure and depth-wise convolutions ( Shujaat et al., 2021).2.1.2.4. ResNeXt.ResNeXt, short for Aggregated Residual TransformNetwork, is a CNN architecture that builds upon the concepts of Resid-ual Networks (ResNets) and Inception Networks. It introduces theidea of a split, transform, and merge block and emphasizes the conceptof cardinality to improve performance. In ResNeXt, a split, transform,and merge block is used, where multiple transformations are appliedwithin the block. These transformations help in learning diverse repre-sentations and capturing different levels of abstraction. The cardinalityparameter is introduced to deﬁne the number of transformation pathswithin the block. Increasing cardinality has been shown to enhancethe model's performance. ResNeXt has demonstrated impressive per-formance in various computer vision tasks, including image classi ﬁca- tion, object detection, and image segmentation. By combining theconcepts of ResNets, Inception Networks, and cardinality-based trans-formations, ResNeXt achieves improved accuracy while maintainingcomputational efﬁciency. It's worth noting that the details of ResNeXt'sarchitecture, such as the number of layers, cardinality, and block con ﬁg- urations, may vary depending on speciﬁc implementations and varia- tions. Different versions of ResNeXt have been proposed to optimizethe trade-off between performance and computational cost for differenttasks and datasets (Patil and Kumar, 2022). 2.1.2.5. ResNet.ResNet, short for Residual Network, introduced theconcept of residual layers and skip connections to address the issue ofvanishing gradients in deep neural networks. The main innovation ofResNet is the residual layer, which allows the network to learn residualmappings rather than attempting to learn the full mapping from theinput to the desired output directly. By using residual layers, the net-work can more easily capture and propagate gradients through the net-work, even in very deep architectures. To facilitate the ﬂow of gradients and address the vanishing gradient problem, ResNet incorporates skipconnections, also known as shortcut connections or identity mappings.These connections enable the gradients to ﬂow directly from the end layers to earlier layers, bypassing the intermediate layers. This allowsthe gradients to propagate more effectively, preventing them fromdiminishing or vanishing as they backpropagate through the network.The skip connections in ResNet also enable the network to besigniﬁcantly deeper than previous architectures. ResNet models havebeen successfully built with depths of 50, 101, or even 152 layers. Theability to train and effectively optimize such deep networks has been amajor advancement in theﬁeld of deep learning. By using residuallayers and skip connections, ResNet mitigates the vanishing gradientproblem and facilitates the training of very deep neural networks. Thishas led to improved performance and accuracy in various computer vi-sion tasks, including image classiﬁcation, object detection, and imagesegmentation (Alegbejo et al., 2006;Ou, 1980).2.1.3. Transfer learning.One approach, a transfer learning technique, isbased on the knowledge gained from a training dataset and is used fortraining a different but relevant task or ﬁeld (Weiss et al., 2016). In this deep learning process, theﬁrst few layers are trained to deﬁne the characteristics of the task. The last few layers of the trained networkcan be removed and retrained with new layers for the target task. It re-fers to the situation whereby what has been learned in one setting isadopted to improve the optimization in another setting. With limitedcomputational requirements, ResNeXt-101 achieved state-of-the-artpower and time speed (Albawi et al., 2017). Training a deep learning model with a small dataset is often insuf ﬁ
- cient for its model's performance. Transfer learning is a process of pre-initialize a model using the weights obtained by training a differentmodel on a larger, different, dataset. In the work conducted by Karimiet al. (2021), it was reported that although transfer learning reducedthe training time on the target task, accuracy improvement dependson data quality. Large improvements are observed only when the seg-mentation task is more challenging and the target training data issmaller (Hossain et al., 2017).2.1.4. Ensemble technique.Ensemble learning is one of the deep learningtechnologies that combine multiple primary learners through a fusionstrategy to improve overall generalization performance ( He et al., 2015). Ensemble learning has attracted a lot of attention because of itseasy-to-understand structure and promising classi ﬁcation performance by combining more than one CNN model. Ensemble learning is a tech-nique that incorporates multiple models for ﬁnal decision-making The ultimate goal of an ensemble is that by combining multiple models,the errors of a single model can be corrected (compensated for) byother models, making the overall score (prediction and classi ﬁcation) of the ensemble better than any individual participating model(Kawasaki et al., 2015).3. Literature reviewThe literature review suggests that four approaches can be used forthe automatic diagnosis of rice diseases.Theﬁrst approach to automatic rice disease detection is throughconventional means, such as pattern recognition techniques ( Chen et al., 2018;Akhter et al., 2019). The study by Phadikar Sil (Islam et al., 2018) proposed an approach to rice disease identi ﬁcation in which the diseased rice images were classiﬁed using a self-organizing map(SOM) (via a neural network) that extracted the train images character-istics of the infected parts of a leaf were obtained from the rice diseases,while four different types of images were used for testing purposes. Sat-isfactory classiﬁcation results have been reported.Sarker et al. (2016) presented a technique that uses only one feature, namely red, green,and blue (RGB) values, to detect and classify rice diseases. Usingimage processing, a disease was identiﬁed based on percentages of RGB values of an affected region. After extracting the RGB percentagesfrom the affected region and grouping them into different classes,they were fed into a simple classiﬁer called Naive Bayes, which classiﬁed diseases into different categories. Three rice diseases were successfullydetected and identiﬁed using this technique: rice, brown spot, riceblight and rice blast. This technique was ef ﬁcient and faster because only one feature (RGB values) extracted from the affected area wasused, requiring minimal computation time to identify and classify thediseases. Instead of processing the whole leaf, this technique was suc-cessful in detecting the diseases using only small parts.The second method is to use a Support Vector Machine (SVM). Forexample,Albawi et al. (2017);Hossain et al. (2017)used this method. Alfred et al. (2021)proposed an automated approach to classify riceplant diseases, brown spot diseases and leaf smut diseases based ontheir morphological changes. A total of 1000 spot images taken with aNikon COOLPIX P4 digital camera of a rice ﬁeld were used. Results were reported with 79.5% and 68.1% accuracy on the Bayesian andSVM classiﬁers, respectively. An SVM technique was also used by(Singh and Misra, 2017) for multiclass classiﬁcation to identify three types of rice diseases (bacterial leaf blight, brown spot and leaf smut).The images of infected rice plants were taken with a digital camerafrom a paddyﬁeld and achieved an accuracy of 93.33% on the trainingdata set and 73.33% on the test data set.The third approach is the digital image processing techniques ofMcNeely-White et al. (2020);Atila et al. (2021);Chambon et al. (2021). Zhou et al. investigated a technique for assessing the extent ofhop disease in rice crops, using a fuzzy C-means algorithm to classify re-gions into one of four classes: no disease, light disease, moderate dis-ease, and severe disease. Their study achieved an accuracy of 87% indistinguishing cases in which a planthopper did or did not occur,while the accuracy in distinguishing four groups was 63.5%. Chambon et al. (2021)was to identify and classify six types of mineral de ﬁcienciesM.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
25in rice. The study used features such as texture and colour for a devel-oped speciﬁc multi-layer neural network. Both networks consist of ahidden layer with a different number (40 for texture and 70 forcolourr)of neurons in the hidden layer, in which 88.56% of the pixels were cor-rectly classiﬁed. Similarly, the same authors proposed another similarwork that successfully identiﬁed two types of diseases (blast andbrown spot) affecting rice plants (Chambon et al., 2021). The fourth approach is texture analysis and feature extraction usingcomputer vision for enhancing the accuracy and rapidity of diagnosingthe results.Phadikar and Sil (2008)developed an approach of textureanalysis to identify four rice diseases (bacterial leaf blight, blast,brown spot and tungro virus) using fractal Fourier. In their proposedstudy, the image of a rice leaf was converted to CIELab colour space,and the system was able to achieve an accuracy of 92.5% ( Phadikar and Sil, 2008). The features extracted from diseased and unaffectedleaf images, the grey level co-occurrence matrix (GLCM) and the colourmoment of the leaf lesion region were implemented by Lu et al. (2017) to create a 2-dimensional\\D feature vector and related features. Re-dundant features were eliminated with a genetic algorithm-based fea-ture selection method to generate 14-D feature vectors to minimizecomplexity. The technique has shown a promising result. However, toimprove its detection accuracy, there is a need for more optimizationprocedures to take place. The key features of rice diseases, the brownspot and blast diseases, were described utilizing the colour of textureof rice leaf photos byPhadikar and Sil (2008). However, the efﬁciency of rice disease identiﬁcation needs to be improved.Phadikar et al. (2012), the entropy-based bipolar threshold tech-nique was employed for the segmentation of the image after improvingits brightness and contrast. The author sought to integrate image pro-cessing and soft computing technique for the detection of rice plantsattacked by several types of diseases. The idea behind the techniquewas robust when utilized effectively. However, the average accuracyof identiﬁcation on the four datasets was 82% which indicated thatmore enhancement was still required. Image processing and machinelearning methods were utilized to non-destructively screen seedlingswith rickets (Islam et al., 2018). Moreover, genetic algorithms were em-ployed to develop SVM classiﬁers to optimize feature selection andmodel parameters for differentiating healthy seedlings and infectedones. The overall accuracy achieved in their study was 87.9%. Since a dis-ease may have several different symptoms at the same time, this ap-proach should be tested if other diseases are also present. It suggeststhat this approach has its limitations. Therefore, deep learning-basedmodels became popular to detect diseases in various plants.Singh and Singh (2010)study performed rice plant disease detectionwith a deep CNN. Using a VGGNet architecture, researchers at Chen et al. (2021)performed the classiﬁcation of rice plant diseases. Chen et al.also proposed a CNN model, namely MobileNet Beta, by extending apre-trained MobileNetV2 model to detect plant diseases ( Chen et al., 2020). Too et al. reported that the DenseNet architecture achieved ahigh test accuracy of 99.75% (Praveen Kumar and Domnic, 2019). Geetharamani and Pandian trained a 9-layer CNN architecture usingthe PlantVillage dataset and achieved a classi ﬁcation accuracy of 96.46% using the test dataset (Too et al., 2019).Mohanty et al. (2016), on the other hand, used AlexNet and GoogLeNet to classify plant dis-eases and achieved a classiﬁcation accuracy of 99.35%Geetharamani and Pandian (2019)
. Using the PlantVillage dataset, the Ferentinossmodel of the VGG architecture delivered the highest accuracy at99.53% (Arnal Barbedo, 2013).Zhou et al. (2013)reported an automatic identiﬁcation and diagno- sis of rice diseases using CNN as a deep learning method. Using a datasetof 500 natural images of diseased and healthy rice leaves and stems cap-tured from the rice experimentalﬁeld, a CNN network was trained toidentify 10 common rice diseases. Under the 10-fold cross-validationstrategy, the proposed CNN-based model achieved an accuracy of95.48%.Sanyal and Patel (2008)suggested a faster R-CNN approach, whichseemed to be ideal for the detection of rice diseases due to its goodspeed and high accuracy. Shrivastava et al. (2019) also applied a CNN al-gorithm for rice plant disease classiﬁcation using a transfer learning ofdeep convolution neural network. Using an AlexNet CNN model, themodel was able to classify rice diseases with a classi ﬁcation accuracy of 91.37%.Asfarian et al. (2014)developed a CNN approach for detecting dis-eases and pests (ﬁve classes of diseases, three classes of pests, and oneclass of healthy plants and others) from rice plant images. A total num-ber of 1426 images were collected that were captured using four differ-ent types of cameras and the system achieved a mean validationaccuracy of 94.33%.Akhter et al. (2019)also suggested a new stacked CNN architecturethat used two-stage training to substantially reduce the model sizewhile retaining a high classiﬁcation accuracy. Several CNN architectures,such as MobileNet, NasNet Mobile, and SqueezeNet, were used. Experi-mental results showed that the proposed architecture achieved the de-sired accuracy of 93.3% with a signiﬁcantly reduced model size, for example, 99% smaller than that of VGG16.3.1.1. Knowledge gap in rice LEAF disease detection using CNNDespite the fact, Phadikar et al. (2012) observed that computer-aided rice disease detection and classiﬁcation have received special at- tention,Asfarian et al. (2014)criticized for low accuracy rates usingthe rice disease detection models. Our literature review in this studyalso suggest that the classiﬁcation accuracies by most of the existingmethods are between 50% and 95% ( Asfarian et al., 2014). Moreover, those achieving higher accuracies were usually tested with fewer dis-eases. The performance would deteriorate if more diseases were in-cluded. (Acharya et al., 2020) and (Huang et al., 2017) discussed the gap between the current capabilities of image-based methods for auto-matic rice disease identiﬁcation and the real-world implementationneeds.4. MethodologyThe experiments in this study were conducted based on GoogleCoLab using the Keras library. TensorFlow which is one of the best Py-thon deep learning libraries available for working with machine learn-ing methods on Python was used. In this study, the original, transferlearning and ensemble models were trained using google collab Teslagraphics processing unit (GPU). TPU is available through the GoogleCollaboratory framework by Google. Initially, the colab framework pro-vides up to 12GB random access memory (RAM) and about 360GB GPUin the cloud for research purposes.4.1. DatasetsData collection was the exceedingly cardinal quest for our research.We have put a vast effort to gather a great number of datasets. Since thisresearch aimed to detect rice diseases, that mainly occurred inBangladesh, most rice epidemic diseases found in the country were con-sidered. Therefore, the data collected from rice leaf images included acombination of the Rice Leaf Disease Dataset from the University of Cal-ifornia Irvine (UCI) Machine Learning Repository, a dataset from pub-licly available respiratory and a dataset collected from Bangladesh RiceResearch Institute (BRRI). An example of rice leaves with various dis-eases is given inFig. 2.Theﬁ
nal combined dataset contains nine (9) classes of ricediseases, with each class having one hundred (100) images foreach type of disease. Images in the dataset are coloured images ofvarious sizes and have a white background. The original imageswere divided into training and test sets with a ratio of 70:30 (seeTable 1).M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
264.2. Process of experimentsThe processes of the experiments are described in Fig. 3. (1) Image Acquisition:In this step, we downloaded the images fromthe targeted sites to provide as input. Images in the dataset werechecked manually to identify if they had a white background. Inthe case where images (mainly from the BRRI) had colouredbackgrounds, images were placed on a white background. If dis-ease symptoms such as spots, diseased colour, and diseasedshape were not visible in an image, the image was removedfrom the dataset.(2) Image Augmentation:Image augmentation is used in this step.Image augmentation is the procedure by which an existingdataset is expanded by transforming the original dataset to cre-ate more new data, and in such a way that new data are alsolabel-preserving (Sankupellay and Konovalov, 2018, Meeras SalmanAl-Shemarry et al., 2019). The goal is to increase the var-iance of the dataset while ensuring that new data are meaningfuland do not merely add unnecessary volume to the dataset(Sankupellay and Konovalov, 2018). When used in a machine-learning context, it can improve model generalization, maketrained models more robust to unseen data, and increase modelaccuracy (Sankupellay and Konovalov, 2018).With these aims, we conducted data augmentation in the trainingdata. However, position augmentation such as scaling, cropping, ﬂip- ping, rotation, and colour augmentation such as brightness, contrast,and saturation was deployed. Random rotation from−15 degrees to 15 degrees, rotations of multiple of 90 degrees at random, random dis-tortion, shear transformation, verticalﬂip, horizontalﬂip, skewing and intensity transformation were also used as part of the data augmenta-tion process. In this way, 10 augmented images from every originalimage have been created. Random choice of a subset of the transforma-tions helps augment an original image in a heterogeneous way.In this study, each pixel value of images in the original and aug-mented images wasﬁrst normalized dividing by 255. The images werethen resized to a default size accepted by each model. In our experi-ment, input image resolutions were necessarily resized for all modelsof EfﬁcientNet architecture due to our hardware limitations. Throughtrial and error, it was seen that the maximum allowed input size thatour hardware resources were sufﬁcient for the training of the EfﬁcientNet model which has the highest number of parameters of132 × 132. Therefore, the input size for all models of Ef ﬁcientNet archi- tecture was set as 132 × 132 to evaluate all models under the same con-ditions. Table II summarizes the default image resolutions and thenumber of parameters deﬁned for deep learning models.(3) Training:A CNN learner model is created at this stage. By usingDenseNet, EfﬁcientNetB3, MobileNet, VGG16 and ResNet10 ar-chitectures, a model was trained based on the given datasetand then tested its classiﬁcation accuracy.All the models were trained for 175 epochs (iterations) with EarlyStopping callbacks (patience = 10 iterations) Patience is the numberof epochs with no improvement after which training will be stopped.An Adam optimizer, a combination of Stochastic Gradient Descent(SGD) with momentum and RMSProp (Root Mean Squared Propaga-tion, or RMSProp, is an extension of gradient descent and the AdaGradversion of gradient descent that uses a decaying average of partial gra-dients in the adaptation of the step size for each parameter.) wereused for faster convergence with the parameters like learning rate wasset atαα= 0.0001,β1β1=0 . 9 ,β2
β2=0 . 9 9 9a n dϵ= 1×1 0−7ϵ=1×1 0−7. The same optimizer was used for all threemodels and then the models were saved as .h5 ﬁles. The time taken for model training is−31 s (s)/epoch (Iterations) for DenseNet201and 17 s/epoch for each of the models ResNet50V2 and Inceptionv3.In this researchstandard deviationwas used as a model performancemetric since the dataset used in this experiment does not have anymajor imbalance.Categorical cross-entropywas used as a loss function for all CNN architectures since this work deals with multi-class classi ﬁ- cation. All intermediate layers of the CNN architectures used in thiswork havereluas the activation function while the activation functionused in the last layer wassoftmax. The hyperparameters used are as fol-lows: the dropout rate was 0.3, the learning rate was 0.0001, the batchsize was 64, and the number of epochs was 275. An adaptive momentestimation (Adam) optimizer was used for updating the model weights.All the images were resized to the default image size for each priorarchitecture.(4) Classiﬁcation:In this step, neural networks (DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, resnext101,and Xception) were used in the automatic detection of leaf dis-eases. The neural network was chosen as a classi ﬁcation tool due to its well-known technique as a successful classi ﬁer for many real applications. After the training model, the evaluationmodel was built for rice disease detection based on the highestprobability of occurrence, the images of rice leaves were classi-ﬁed into different disease classes using a softmaxoutput layer.
Fig. 2.Images examples of the rice diseases used.Table 1Images used in the train, test and validation sets.Total images Training images Validation imagesOriginal Dataset 900 630 270Augmented Dataset 42,876 34,992 7884M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
275. Experimental resultsThe results from the experiments are presented in three sectionsbased on the architectures of original individual networks, transferlearning and ensemble techniques. The results obtained are expectedto answer the following questions:1. Which original CNN network provides better accuracy in detectingrice leaf disease?2. Does transfer learning improve accuracy?3. Does the ensemble technique improve the accuracy?Several performance measures for machine learning classi ﬁcation models are used to assess how well those CNN base algorithms performin a given context. The following performance metrics are considered:5.1. AccuracyAccuracy is one metric for evaluating classi ﬁcation models. Infor- mally, accuracy is the fraction of predictions our model got right. For-mally, Accuracy is the ratio of correctly labelled images to the totalnumber of samples (Kawasaki et al., 2015). The formula for accuracy is given below (1):Accuracy¼
TPþTNTPþFPþTNþFNð1Þ5.2. PrecisionPrecision is deﬁned as the probability given a positive label, and howmany of them are positive (Ferentinos, 2018). Precision tells us howmany of the correctly predicted cases turned out to be positive. Preci-sion is a useful metric in cases where FP is a higher concern than FN.The formula of precision is given below ( 2):Precision¼TPTPþFP ð2Þ5.3. RecallRecall or Sensitivity is the accuracy of positively predicted instancesdescribing how many were labelled correctly ( Kawasaki et al., 2015). Recall tells us how many of the actual positive cases we were able topredict correctly with our model. The recall is a useful metric in caseswhere FN trumps FP. The formula of recall is given below ( 3):F1/C0score¼
2
1Recallþ1Precisionð3ÞF1-score, as an additional measure for classi ﬁcation accuracy, con- siders both precision and recall. F1-score is a harmonic mean of Preci-sion and Recall, and so it gives a combined idea about these twometrics. It is maximum when Precision is equal to Recall.5.4. SpeciﬁcitySpeciﬁcity refers to the ability of a diagnostic test to correctly iden-tify a rice leaf that is healthy or free from disease. It measures the per-centage of true negative results. A highly speci ﬁc test has a low false positive rate. However, in case of a highly speci ﬁc test can be interpreted with conﬁdence as a strong indication that the rice leaf is a diseased one.The equation is given below (4):
Fig. 3.Process of experiments.M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
28Specificity¼TNTNþFP ð4Þ5.5. Training loss and validation lossTraining loss is a measure of how well a model ﬁts the training data. It quantiﬁes the discrepancy between the predicted output of the modeland the actual target values in the training set. The goal during trainingis to minimize this loss, which indicates that the model is learning to ac-curately represent the relationship between the input data and the cor-responding output targets.Validation loss, on the other hand, assesses how well the model gen-eralizes to new, unseen data. It measures the discrepancy between themodel's predictions and the true target values in a validation set or aportion of the training data that is held out for evaluation. The validationloss helps determine if the model has learned meaningful patterns or ifit is overﬁtting.Overﬁtting occurs when a model becomes too complex or too closelyﬁts the training data. In such cases, the model may start capturing noiseor irrelevant patterns from the training set, making it less effective atgeneralizing to new data. Overﬁtting is often characterized by a lowtraining loss but a high validation loss, indicating that the model is notperforming well on unseen data.To combat overﬁtting, techniques such as regularization, dropout,and early stopping can be employed. Regularization methods help pre-vent the model from excessivelyﬁtting the training data by introducingpenalties or constraints on the model's parameters. Dropout randomlydeactivates a portion of the neurons during training, reducing themodel's reliance on speciﬁc features or patterns. Early stopping stopsthe training process when the validation loss starts to increase, prevent-ing the model from further overﬁtting.The aim is to strike a balance where the model minimizes both thetraining loss and the validation loss, indicating that it is learning mean-ingful patterns without overﬁtting the data. This ensures that the modelgeneralizes well and performs accurately on unseen data.5.6. Confusion matrixA confusion matrix is a table that summarizes the results of a classi-ﬁcation model by comparing the predicted labels with the true labels ofa dataset. It provides a comprehensive view of the model's performanceby displaying the counts of true positive (TP), false positive (FP), truenegative (TN), and false negative (FN) predictions. Confusion matricesare valuable tools in evaluating and comparing different models,selecting appropriate thresholds, and understanding the trade-offs be-tween various performance measures. They offer a clear and concisesummary of the model's predictive performance and are widely usedin machine learning and classiﬁcation tasks.5.7. SupportSupport refers to the number of actual occurrences or instances of aparticular class within a dataset. It represents the frequency or preva-lence of a speciﬁc class. Imbalanced support occurs when there is a sig-niﬁcant disparity in the number of instances between different classesin the training data. For example, if one class has a much larger numberof instances compared to another class, the dataset is considered imbal-anced. Imbalanced support can pose challenges in training classi ﬁers and evaluating their performance. Classiﬁers tend to be biased towards the majority class due to the larger number of instances, resulting inlower accuracy or performance metrics for the minority class. This im-balance can indicate potential structural weaknesses in the reportedscores of the classiﬁer, as the overall performance may not accuratelyreﬂect its ability to correctly classify all classes. To address imbalancedsupport, various techniques can be employed. Strati ﬁed sampling is one approach that ensures each class is represented proportionally inthe training and evaluation datasets. This helps provide a more balancedrepresentation of classes during model training and evaluation.Rebalancing techniques, such as oversampling the minority class orundersampling the majority class, can also be used to mitigate theeffects of imbalanced support during training.5.8. Which original CNN network provides better accuracy in detecting riceLEAF disease?In this section, the performances of the six original individual CNNnetworks (DenseNet121, Inceptionv3, MobileNetV2, resNext101,Resnet152V, and Seresnext101) are presented. The classi ﬁcation perfor- mance of the models isﬁrst presented. The overall measures for thosemodels are then discussed. Gathering in addition to the descriptors, pos-sible causes, and areas of opportunity for improvement of results.Table 2displays the accuracy of the DenseNet121, Inceptionv3,MobileNetV2, resNext101, Resnet152V, Seresnext101. TheDenseNet121 and Inceptionv3 models achieved the highest accuracyat 97% and Seresnext101 gave the lowest accuracy value of 79%.The Precision, Recall, F1-score and Speci ﬁcity obtained by the DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V,and Seresnext101 models for each of the classes are shown in Table 3. Considering the precision values for each on the test dataset,DenseNet121, Inceptionv3, and MobileNetV2, architectures providethe best performance. The above table suggests that the DenseNet121,Inceptionv3, and MobileNetV2 models classi ﬁed Blight, Leaf Blast and Tungro diseases with 99% accuracy. The Seresnext101 performed lowprecision having the lowest identiﬁcation of the Bacterial blight leafwith only 56% accuracy. All models identiﬁed Hispa with an average ac- curacy by the six models. Seresnext-101 requires large amounts of im-ages for training to learn accurate representations compared withDenseNet121, Inceptionv3, MobileNetV2, resNext101, and Resnet152V.If the network receives less testing data, it provides lower classi ﬁcation accuracy. Moreover, the architecture and hyperparameters of theSEResNeXt-101 model could impact its performance. If the model archi-tecture is not suitable for the speciﬁc image classiﬁcation task, or if hyperparameters such as learning rate, batch size, or regularization set-tings are not properly tuned, it could result in lower accuracy. Lastly,since the SEResNeXt-101 model is too complex and has too many pa-rameters relative to the size of the training dataset, leading to over ﬁtting and reducing performance on new images (See Table 4).5.9. Does transfer learning improve accuracy?In this section, the performance of four transfer learning CNN archi-tectures is presented.Table 6shows the accuracies obtained in the testsets by DenseNet121, Seresnext101, EfﬁcientNet and, Xception models. The test accuracies shown inTable 5were calculated as the ratio of thenumber of correctly classiﬁed samples to the number of all samples. TheDenseNet1121 model achieved the highest accuracy of 97%. However,the accuracy improvement from the original network to transfer learn-ing by the SeresNext101 network is mentionable. The network
Table 2Classiﬁcation accuracy of the individual CNN networks indetecting rice diseases.Architecture AccuracyDenseNet121 97%Inceptionv3 97%MobileNetV2 94%resNext101 96%Resnet152V 93%Seresnext101 79%M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
29improved by a 17% accuracy increase after applying the transfer learn-ing approach.Table 6shows the Precision, Recall, F1-score, and Speci ﬁcity results from the CNN networks with transfer learning. In general, high Preci-sion, high Recall, and high Speciﬁcity represent a better model. The ex-perimental results demonstrate that SeresNext-101 had a low precisionin detecting Bacterial leaf Blight with 56% accuracy. However, aftertransfer learning, the model reached 98% accuracy.The associated TP, FN, FP, and TN are shown in Table 8.F o rr i c ed i s e a s e detection and classiﬁcation, we applied the seresNext101 Model with atransfer learning approach as the model received the lowest accuracy inearlier experiments (Without transfer learning). In addition to theSeresNext101 model, we also selected DenseNet121, Ef ﬁcientNet and Xception models for rice leaf disease detection and classi ﬁcation. As these are deep convolutional networks and we were interested to see ifthe models are useful for small-scale datasets. The confusion matrix ofDenseNet121, Seresnext101, EfﬁcientNet and Xception is shown inFig. 4.5.10. Does the ensemble technique improve the accuracy?In this research, the ensemble stack is developed on three differentoriginal CNN models, Densenet121, EfﬁcientNetB7, and XceptionNet. To accelerate the training process, we adopted a transfer learningstrategy. In addition to this, the output from these models was sent toa post-processing block containing a fully connected layer followed bya dropout layer and aﬁnal logit layer for classifying the image. For betterconvergence of our models, we used a learning rate decaying strategywhich divided the learning rate by 10 only when the loss stops decreas-ing for three continuous epochs and an early-stopping strategy thathalts the training process after the learning rate decayed 5 times(Kawasaki et al., 2015).Table 3Precision, Recall, f1 and Speciﬁcity result of CNN networks with transfer learning.Bacterial leaf blight Blight Brown Spot Hispa Leaf blast Leaf scaled Leaf smut Sheath Blight TungroDensenet121Precision 96% 99% 97% 97% 99% 96% 96% 94% 99%Recall 100% 100% 92% 99% 93% 97% 97% 95% 99%F1-score 98% 100% 95% 98% 96% 96% 97% 95% 99%Support (N) 872 864 871 870 869 854 869 870 869Speciﬁcity 99.97% 100.00% 99.07% 99.87% 99.11% 99.67% 99.67% 99.52% 99.90%Inceptionv3Precision 96% 99% 97% 97% 99% 96% 96% 94% 99%Recall 99% 100% 92% 97% 90% 99% 100% 96% 97%F1-score 98% 100% 95% 94% 95% 97% 98% 98% 97%Support (N) 868 869 864 871 867 869 867 867 866Speciﬁcity 100.00% 99.99% 98.48% 99.81% 98.78% 99.74% 99.09% 99.93% 99.97%Mobilenetv2Precision 96% 99% 97% 97% 99% 96% 96% 94% 99%Recall 99% 100% 92% 97% 90% 99% 100% 96% 97%F1-score 98% 100% 95% 94% 95% 97% 98% 98% 97%Support (N) 868 869 864 871 867 869 867 867 866Speciﬁcity 99.93% 99.99% 99.07% 99.97% 98.81% 99.94% 99.94% 99.57% 99.69%Resnet152v2Precision 94% 100% 100% 84% 86% 89% 96% 97% 98%Recall 99% 100% 89% 87% 82% 96% 99% 95% 94%F1-score 96% 100% 94% 85% 84% 93% 97% 96% 96%Support (N) 872 869 864 866 868 870 870 863 866Speciﬁcity 99.91% 100.00% 98.78% 98.44% 97.97% 99.51% 99.85% 99.41% 99.27%Resnext101Precision 96% 100% 93% 97% 99% 96% 96% 94% 99%Recall 100% 100% 92% 99% 93% 97% 97% 95% 99%F1-score 98% 100% 95% 98% 96% 96% 97% 95% 99%Support (N) 872 864 871 870 869 854 869 870 869Speciﬁcity 99.24% 99.72% 96.39% 95.15% 98.34% 97.72% 99.66% 99.25% 99.40%Seresnext101Precision 56% 87% 79% 84% 72% 96% 80% 95% 81%Recall 93% 86% 40% 71% 65% 73% 89% 95% 99%F1-score 70% 87% 53% 77% 69% 83% 84% 95% 89%Support (N) 867 860 865 872 868 870 867 867 872Speciﬁcity 99.26% 98.63% 94.23% 97.06% 96.51% 97.32% 98.84% 99.45% 99.93%
Table 4Table 4shows the values of TP, TN, FP, and FN by the different CNN architectures.Bacterialleaf blightBlight BrownSpotHispa LeafblastLeafscaledLeafsmutSheathBlightTungroDensenet121TP 870 864 804 861 805 830 845 836 862FN 39 6 24 30 7 38 31 50 8FP 2 0 67 9 64 24 24 34 7TN 7130 7171 7146 7141 7165 7149 7141 7121 7164Inceptionv3TP 867 863 756 857 778 852 805 860 863FN 36 0 58 73 19 7 78 22 14FP 0 1 111 14 89 19 66 5 2TN 7212 7251 7190 7171 7229 7237 7166 7228 7236Mobilenetv2TP 863 868 794 849 781 861 863 836 844FN 37 9 82 3 54 29 8 27FP 5 1 67 2 86 4 4 31 22TN 7125 761 7160 7097 7160 7111 7134 7155 7137resnet152v2TP 865 869 772 750 716 834 859 819 811FN 60 4 2 148 118 99 34 26 16FP 7 92 116 152 36 11 44 55TN 7383 7442 7449 7301 7329 7346 7411 7426 7433Resnext101TP 809 844 559 453 733 678 844 810 821FN 78 72 70 74 418 59 255 89 105FP 62 23 303 412 131 189 27 61 49TN 8079 8089 8096 8089 7746 8102 7902 8068 8053Seresnext101TP 808 742 345 616 566 635 769 820 866FN 647 101 90 113 218 24 195 46 200FP 59 118 520 256 302 235 98 47 6TN 7928 8481 8487 8457 8356 8548 8380 8529 8370M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
30All the models have been trained for 60 epochs with Early Stoppingcallbacks (patience = 10 epochs). Adam optimizer, a combination ofSGD with momentum and RMSProp, was used for faster convergencewith the parameters as learning rate αα= 0.0001,β1β1=0 . 9 , β2β2 = 0.999 andϵ=1×1 0−7ϵ=1×1 0−7. The same optimizer is used for all three models and then the models are saved as .h5 ﬁles. The time taken for model training is−31 s/epoch for DenseNet201 and 17 s/epoch for each of the models ResNet50V2 and Inceptionv3.In Figure the gradual change in the loss function (both training as wellas validation) through the epochs has been depicted for all three modelsof DenseNet201, ResNet50V2 and Inceptionv3. With a 97.62% (seeTable 7) accuracy, the ensemble model outperformed the original CNNarchitecture (Densenet121, EfﬁcientNetB7 and XceptionNet).The precision on ensembling suggests that the model received 99%on Bacterial blight, which was 98% with transfer learning and 56% onthe original CNN model (seeTable 8). Even though the F1-score hadthe lowest accuracy (53% in the case of Brown Spot using Seresnext101)the ensemble model had 95% in that case). However, the Precision, Re-call, f1 and Speciﬁcity result of CNN networks with the ensemble isshown inTable 8.Fig. 5shows the confusion matrix of the ensemble model. Fig. 6 shows the training accuracy and validation accuracy of the ensemblemodel of Densenet121, EfﬁcientNetB7 and XceptionNet, where thex-axis represents the number of epochs and the y-axis represents theaccuracy and loss percentages.Fig. 6indicates that the training and validation data are split appropriately with no over- ﬁtting. Fig. 7shows the training loss and validation loss over epochs by theensemble technique. A loss function is used in CNN to optimize anarchitecture. The loss is calculated on training and validation and its in-terpretation is based on how well the model is doing in these two sets. Itis the sum of errors made for each example in training or validation sets.Loss value implies how poorly or well a model behaves after each itera-tion of optimization.Fig. 7suggests that the training loss was around 3%while the validation loss was 5% in 175 epochs.Fig. 8presents the accuracy of six different CNN-based models(DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152Vand Seresnext101), transfer learning and ensemble model on nine clas-ses of rice:6. DiscussionsIn this research, we performed an in-depth investigation of the per-formances of original individual CNN, transfer learning, and ensemblemodels. We compared the results of six different CNN-based modelsof DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152Vand SeresNext101 by applying them to the nine classes of rice diseases(seeFig. 8for accuracy). The dataset used includes 14,118 rice leafimages. After image expansion through rotation, we obtained 34,992images for training and 7884 images for testing. Among the original in-dividual networks, Densenet121 provides the best classi ﬁcation results in identifying rice leaf diseases.Bari et al., 2021;Nayak and Singh, 2021 also support theﬁndings that Denesenet121 delivers relatively high ac-
curacy. This is because, in DenseNet, each layer obtains a “collective knowledge”from all preceding layers as layers receive inputs from allpreceding layers and pass them on to the next layers.Our investigation suggests that transfer learning of deep learningmodels provides slightly improved accuracy than the original individualnetworks for small datasets (Number of imageless than 2000). In thiscase, only after careful training including transfer learning, the accuracywas higher than the original CNN architecture. The transfer learningstrategies in this research were based on using the pre-trained modelfor training and extracting features. Surprisingly we found thatseresNext101 has improved by 17% of accuracy after a transfer learningprocess. This is consistent with the results from the study conducted byOloko-Oba and Viriri (2021) that SE-ResNeXt-101 normally would in-volve more parameters and was computationally expensive but hasshown good results on the ImageNet classi ﬁcation tasks. Performing transfer learning from images trained on Imagenet (general imagessuch as cats, dogs, etc.) or MURA (X-ray images on different parts ofthe body but not the chest) improved results compared to scenarioswhen transfer learning was not used at all.Table 5Accuracy results of CNN networks with transfer learning.Architecture AccuracyDenseNet121 97%Seresnext101 96%EfﬁcientNet 95%Xception 92%
Table 6Precision, Recall, f1 and Speciﬁcity result of CNN networks (Based on the number of images).Bacterial leaf blight Blight Brown Spot Hispa Leaf blast Leaf scaled Leaf smut Sheath Blight TungroDensenet121Precision 97% 96% 97% 98% 95% 99% 96% 94% 94%Recall 98% 100% 93% 98% 97% 90% 97% 99% 97%F1-score 98% 98% 95% 98% 96% 94% 96% 96% 95%Support (N) 715 804 884 871 912 1154 842 908 718Speciﬁcity 99.80% 99.92% 99.17% 99.72% 98.34% 99.64% 99.67% 99.85% 99.76%Serenext101Precision 98% 96% 95% 97% 95% 94% 91% 92% 98%Recall 98% 100% 92% 96% 97% 92% 91% 97% 95%F1-score 98% 98% 94% 96% 96% 94% 94% 93% 97%Support (N) 713 804 888 877 908 1152 841 910 716Speciﬁcity 99.71% 99.73% 99.07% 99.49% 98.85% 99.64% 99.59% 99.45% 99.52%EfﬁcientNetPrecision 98% 97% 97% 96% 97% 98% 92% 93% 90%Recall 98% 98% 92% 97% 95% 90% 95% 96% 98%F1-score 98% 98% 94% 97% 98% 94% 93% 94% 94%Support (N) 717 811 883 869 908 1153 845 910 712Speciﬁcity 99.84% 99.78% 99.07% 99.70% 98.36% 99.45% 99.34% 99.45% 99.84%XceptionPrecision 95% 98% 94% 90% 95% 91% 89% 87% 94%Recall 97% 98% 89% 97% 86% 88% 95% 92% 93%F1-score 96% 98% 92% 94% 90% 90% 92% 90% 93%Support (N) 716 815 889 871 910 1147 833 915 712Speciﬁcity 99.73% 99.78% 98.84% 99.70% 98.21% 99.50% 98.38% 99.02% 99.42%M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
31Not surprisingly, from our investigation, we found that the ensembleof deep learning models improved its accuracy over a single CNN archi-tecture. Ourﬁndings also support the study by (Acharya et al., 2020).7. Contributions of this researchThis research offers several key contributions. Firstly, this researchexperimented using nine types of rice diseases. Secondly, in this re-search, a comparison of six original CNN architectures (DenseNet121,Inceptionv3, MobileNetV2, resNext101, Resnet152V, and Seresnext101)was conducted. Thirdly, we applied a transfer learning approach onDenseNet121, MobileNetV2, Resnet152V, Seresnext101, and an ensem-ble model called DEX (Densenet121, EfﬁcientNetB7, and Xception) to draw a comparison among the original CNN networks, transfer learning,
Fig. 4.(A): CM after TL of DenseNet121. (B): CM after TL of Ef ﬁcientNetB7.(C): CM after TL of Xception. (D): CM after TL of Seresnext101.
Table 7Accuracy results of the Ensemble model.Architecture AccuracyEnsemble model (DEX) 97.62%M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
32Table 8Precision, Recall, f1 and Speciﬁcity result of ensembled CNN networks.Ensemble DEX model (Densenet121, Ef ﬁcientNetB7 & XceptionNet)Bacterial blight Blight Brown Spot Hispa Leaf blast Leaf scaled Leaf smut Sheath Blight TungroPrecision 99% 99% 99% 98% 97% 99% 94% 96% 96%Recall 99% 100% 93% 99% 98% 94% 99% 99% 99%F1-score 99% 99% 95% 99% 97% 97% 96% 96% 98%Speciﬁcity 99.84% 100% 99.12% 99.70% 98.36% 99.45% 99.34% 99.45% 99.84%Support 711 803 886 878 912 1146 844 913 715
Fig. 5.CM of ensembled three CNN.
Fig. 6.Training and validation errors over the iteration of ensembled three CNN.M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
33and ensemble technique. The results suggest that the ensemble frame-work provides the best accuracy of 98%, and the transfer learning in-creases a 17% accuracy from the results by Seresnext101 in detectingand localizing rice leaf diseases.8. Conclusion and future researchThere are some limitations in the current stage of the research,which need to address in future work. The use of free-of-charge re-sources (Google Colab) limits the experiments of this study. As GoogleColab offers the server for a limited time, the hyperparameter tuning,training the base model training other than Imagenet (this researchused Imagenet as the base database), and the application of Adadelta,FTRL, NAdam, Adadelta, and many more optimizers were not performedin this study. Another limitation is that the research used secondarydata that are available publicly, not primary data directly collectedfromﬁelds.In the future, we want to create a user interface for the detection andlocalization of rice leaf diseases for farmers. This interface would notonly detect but also provide a guide on how the diseases can be con-trolled. As mobile phones are seen as a preferred technological deviceamong developing country users, we aim to develop a mobile phone-based rice leaf disease detection application tool.The experimentation and the observations presented here are veryimportant when models are being constructed with small datasets.In this research, the accuracy of the ensemble DEX model from
Fig. 7.Training and validation accuracy over the epochs of ensembled three CNN.
Fig. 8.Accuracy comparison among individual CNN, transfer learning and ensemble.M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
34Densenet121, EfﬁcientNetB7 & XceptionNet was found to produce thehighest accuracy in classifying rice diseases from rice leaves. The successof the proposed architecture was compared with the transfer learningand six state-of-the-art individual CNN architectures. Experimentalstudies were conducted in both original and augmented versions ofthe image dataset. Considering both the average accuracy and the aver-age precision metric on both the original and augmented datasets, theDEX model was found to be superior to other CNN architectures.FundingThe work was not supported by any external funding.CRediT authorship contribution statementMd Taimur Ahad:Writing–original draft, Conceptualization, Meth-odology.Yan Li:Data curation, Writing–original draft, Supervision.Bo Song:Visualization, Investigation.Touhid Bhuiyan:Writing–review & editing.Data availabilityThe data used to support theﬁndings of this study are available fromthe corresponding author upon request.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Acharya, A., et al., 2020, November. Plant disease detection for paddy crop using ensemble of CNNs. 2020 IEEE International Conference for Innovation in Technology (INOCON).IEEE, pp. 1–6.Adegun, A., Viriri, S., 2021.Deep learning techniques for skin lesion analysis and mela-noma cancer detection: a survey of the state-of-the-art. Artif. Intell. Rev. 54 (2),811–841.Akhter, M.S., et al., 2019.Plant virus diseases and their management in Bangladesh. CropProt. 118, 57–65.Albawi, S., Mohammed, T.A., Al-Zawi, S., 2017. August). Understanding of a convolutional neural network.2017 international conference on engineering and technology (ICET) . Ieee, pp. 1–6.Alegbejo, M.D., et al., 2006.Rice yellow mottle virus disease, a new disease of rice inZamfara, Nigeria. Intern. Rice Res. Notes 31 (39), 39.Alfred, R., et al., 2021.Towards paddy rice smart farming: a review on big data, machinelearning, and rice production tasks. IEEE Access 9, 50358 –50380. Al-Shemarry, Meeras Salman, Li, Yan, Abdulla, Shahab, Wen, Peng, 2019. An ef ﬁcient tex- ture descriptor for the detection of license plates from vehicle images in dif ﬁcult con- ditions. IEEE Transactions on Intelligent Transportation Systems. Vol 99, pp. 1 –12. https://doi.org/10.1109/TITS.2019.2897990 . Arnal Barbedo, J.G., 2013.Digital image processing techniques for detecting, quantifyingand classifying plant diseases. SpringerPlus 2 (1), 1 –12. Asfarian, A., et al., 2014.A computer vision for rice disease identi ﬁcation to support inte- grated pest management. Crop Prot. 61, 103 –104. Atila, Ü., et al., 2021.Plant leaf disease classiﬁcation using EfﬁcientNet deep learning model. Ecol. Inform. 61, 101182.Bari, B.S., et al., 2021.A real-time approach of diagnosing rice leaf disease using deeplearning-based faster R-CNN framework. PeerJ Comp. Sci. 7, e432.Chambon, S., et al., 2021.When High-Performing Models Behave Poorly in Practice: Peri-odic Sampling Can Help.Chen, C.F., et al., 2018.Big-little net: an efﬁcient multi-scale feature representation for vi- sual and speech recognition. arXiv preprint arXiv:1807.03848.Chen, J., et al., 2020.Identifying plant diseases using deep transfer learning and enhancedlightweight network. Multimed. Tools Appl. 79 (41), 31497 –31515. Chen, J., et al., 2021.Identiﬁcation of rice plant diseases using lightweight attention net-works. Expert Syst. Appl. 169, 114514.Chollet, F., 2017.Xception: deep learning with depthwise separable convolutions. Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 1251–1258.Ferentinos, K.P., 2018.Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.Geetharamani, G., Pandian, A., 2019. Identiﬁcation of plant leaf diseases using a nine-layer deep convolutional neural network. Comput. Electr. Eng. 76, 323 –338. He, K., et al., 2015.Delving deep into rectiﬁers: surpassing human-level performance on imagenet classiﬁcation. Proceedings of the IEEE International Conference on Com-puter Vision, pp. 1026–1034.Hossain, M., et al., 2017.Occurrence of blast disease in rice in Bangladesh. Am. J. Agricult.Sci. 4 (4), 74–80.Huang, G., et al., 2017.Densely connected convolutional networks. Proceedings of theIEEE Conference on Computer Vision and Pattern Recognition, pp. 4700 –4708. Islam, T., et al., 2018, April.A faster technique on rice disease detectionusing image pro-cessing of affected area in agro-ﬁeld. 2018 Second International Conference on Inven- tive Communication and Computational Technologies (ICICCT). IEEE, pp. 62 –66. Karimi, D., Warﬁeld, S.K., Gholipour, A., 2021.Transfer learning in medical image segmen- tation: New insights from analysis of the dynamics of model parameters and learnedrepresentations. Artiﬁcial intelligence in medicine 116, 102078.Kawasaki, Y., et al., 2015, December. Basic study of automated diagnosis of viral plant dis- eases using convolutional neural networks. International Symposium on Visual Com-puting. Springer, Cham, pp. 638 –645. Kumar, J.P., Domnic, S., 2019. Image based leaf segmentation and counting in rosetteplants. Inform. Proc. Agricult. 6 (2), 233 –246. Lu, Y., et al., 2017.Identiﬁcation of rice diseases using deep convolutional neural net-works. Neurocomputing 267, 378 –384. McNeely-White, D., et al., 2020. Inception and ResNet features are (almost) equivalent.Cogn. Syst. Res. 59, 312–318.Mitkal, P., et al., 2016.Leaf disease detection and prevention using image processing usingMATLAB. Intern. J. Recent Trends Eng. Res. (IJRTER) 2 pp.2455 –1457. Mohanty, S.P., et al., 2016.Using deep learning for image-based plant disease detection.Front. Plant Sci. 7, 1419.Nagasubramanian, K., et al., 2020. Usefulness of interpretability methods to explain deeplearning based plant stress phenotyping. arXiv preprint arXiv:2007.05729. Nayak, J.K., Singh, P., 2021.Fundamentals of Research Methodology Problems and Pros-pects. SSDN Publishers & Distributors.Nguyen-Quoc, H., Hoang, V.T., 2020. Rice seed image classiﬁcation based on HOG descrip- tor with missing values imputation. TELKOMNIKA (Telecommunication ComputingElectronics and Control) 18 (4), 1897 –1903. Ou, S.H., 1980.Pathogen variability and host resistance in rice blast disease. Annu. Rev.Phytopathol. 18 (1), 167–187.Patil, R.R., Kumar, S., 2022.Rice-fusion: a multimodality data fusion framework for Ricedisease diagnosis. IEEE Access 10, 5207 –5222. Phadikar, S., Sil, J., 2008, December. Rice disease identiﬁcation using pattern recognition techniques. 2008 11th International Conference on Computer and Information Tech-nology. IEEE, pp. 420–423.
Phadikar, S., Sil, J., Das, A.K., 2012. Classiﬁcation of rice leaf diseases based on morpholog- ical changes. International Journal of Information and Electronics Engineering 2 (3),460–463.Sainath, T.N., Kingsbury, B., Saon, G., Soltau, H., Mohamed, A.R., Dahl, G., Ramabhadran, B.,2015.Deep convolutional neural networks for large-scale speech tasks. Neural Netw.64, 39–48.Sankupellay, M., Konovalov, D., 2018, November. Bird call recognition using deep convolutional neural network, ResNet-50. Proceedings of ACOUSTICS. Vol. 7, No. 9,pp. 1–8.Sanyal, P., Patel, S.C., 2008.Pattern recognition method to detect two diseases in riceplants. Imag. Sci. J. 56 (6), 319 –325. Sarker, M.M., et al., 2016.Status of rice false smut disease in Natore district of Bangladesh.Bangl. Rice J. 20 (2), 31–37.Shrivastava, P., Soon, T.K., Idris, M.Y.I.B., Mekhilef, S., 2019. Overview of model-based on- line state-of-charge estimation using Kalman ﬁlter family for lithium-ion batteries. Renewable and Sustainable Energy Reviews 113, 109233.Shujaat, M., et al., 2021.Cr-prom: a convolutional neural network-based model for theprediction of rice promoters. IEEE Access 9, 81485 –81491. Singh, K.K., Singh, A., 2010.A study of image segmentation algorithms for different typesof images. Intern. J. Comp. Sci. Iss. (IJCSI) 7 (5), 414.Singh, V., Misra, A.K., 2017.Detection of plant leaf diseases using image segmentation andsoft computing techniques. Inform. Proc. Agricult. 4 (1), 41 –49. Too, E.C., et al., 2019.A comparative study ofﬁne-tuning deep learning models for plant disease identiﬁcation. Comput. Electron. Agric. 161, 272 –279. Weiss, K., Khoshgoftaar, T.M., Wang, D., 2016. A survey of transfer learning. Journal of Big data 3 (1), 1–40.Xie, S., et al., 2017.Aggregated residual transformations for deep neural networks. Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 1492–1500.Xu, Zhe, Guo, Xi, Zhu, Anfan, He, Xiaolin, Zhao, Xiaomin, Han, Yi, Subedi, Roshan, 2020.Using deep convolutional neural networks for image-based diagnosis of nutrient de-ﬁciencies in rice. Comput. Intell. Neurosci. 2020.Zhou, Z., et al., 2013.Rice plant-hopper infestation detection and classi ﬁcation algorithms based on fractal dimension values and fuzzy C-means. Math. Comput. Model. 58(3–4), 701–709.Zhu, X., Gong, S., 2018.Knowledge distillation by on-the- ﬂy native ensemble. Adv. Neural Inf. Proces. Syst. 31.M.T. Ahad, Y. Li, B. Song et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 22 –35
35