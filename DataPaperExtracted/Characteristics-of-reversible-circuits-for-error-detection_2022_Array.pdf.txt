Array 14 (2022) 100165
Available online 12 April 2022
2590-0056/© 2022 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/locate/array
Characteristics of reversible circuits for error detection
Lukas Burgholzera,∗, Robert Willeb,c, Richard Kuenga
aInstitute for Integrated Circuits, Johannes Kepler University Linz, Austria
bChair for Design Automation, Technical University of Munich, Germany1
cSoftware Competence Center Hagenberg GmbH (SCCH), Austria
A R T I C L E I N F O
Keywords:
Emerging technologies
Reversible logic
Error detection
Simulation
Quantum computingA B S T R A C T
In this work, we consider error detection via simulation for reversible circuit architectures. We rigorously
prove that reversibility augments the performance of this simple error detection protocol to a considerable
degree. A single randomly generated input is guaranteed to unveil a single reversible error with a probability
that only depends on the size of the error, not the size of the circuit itself. Empirical studies confirm that this
behavior typically extends to multiple errors as well. In conclusion, reversible circuits offer characteristics that
reduce masking effects – a desirable feature that is in stark contrast to irreversible circuit architectures.
1. Introduction
The detection of errors is a fundamental problem in electrical engi-
neering and computer science. Given a circuit 𝐶1with𝑛inputs and 𝑚
outputs (the Golden Specification ), the task is to decide whether a given
circuit realization 𝐶2(theDesign Under Verification ) describes the same
functionality on the logical level.
Many approaches exist that address this important and challenging
problem. In this work, we focus on error detection protocols that
only require simulation runs of the two circuits—as opposed to formal
verification techniques which explicitly utilize structural knowledge
about both circuits [ 1–6]. This is a severe restriction, but simulations
alone are – in principle – sufficient to solve this task. If the two circuits
are equivalent, they have the same input–output behavior. Conversely,
suppose that they are functionally distinct. Then, there exists at least
one input string for which the two circuits produce distinct outputs. In
formulas:
∃⃗ 𝑥∈{0,1}𝑛such that 𝐶1(⃗ 𝑥)≠𝐶2(⃗ 𝑥). (1)
Such an input successfully detects the discrepancy between 𝐶1and𝐶2
and serves as a counterexample for the equivalence of the circuits.
The problem, however, is how to find counterexamples (1). If we
only allow simulations of both circuits, i.e., we consider them as black
boxes, we do not have actionable advice on how to choose promising
input strings and we may as well generate inputs uniformly at random:
⃗ 𝑥∼Unif(
{0,1}𝑛), i.e., we flip an unbiased coin for each input value
(⃗ 𝑥= (𝑥𝑛,…,𝑥1), where𝑥𝑛,…,𝑥1∼𝑥and Pr[𝑥= 0]= Pr[𝑥= 1] =
1∕2). Subsequently, we simulate both circuits with this input and check
∗Corresponding author.
E-mail addresses: lukas.burgholzer@jku.at (L. Burgholzer), robert.wille@tum.de (R. Wille), richard.kueng@jku.at (R. Kueng).
1https://www.cda.cit.tum.de/research/quantum/ .whether they produce the same output: 𝐶1(⃗ 𝑥)?=𝐶2(⃗ 𝑥). If the outputs
are distinct, we have found a counterexample. The circuits cannot be
equivalent. But if the outputs are the same, the test is inconclusive. In
this case, we must repeat it with new (randomly generated) inputs until
we either find a counterexample (non-equivalence) or have exhausted
all2𝑛possible inputs (equivalence). The latter, unfortunately, can be a
very real possibility. The two circuits 𝐶1and𝐶2may differ on a single
input only and it is extremely unlikely to quickly find this input by
(random) chance.
To make matters worse, classical circuits can mask even ‘‘small’’
errors very effectively. For 𝑛= 8, this is illustrated in Fig. 1. A cascade
of logical AND gates, realizing the functionality 𝑦=𝑥𝑛⋅…⋅𝑥1(ideal
circuit𝐶1), is affected by a single bit-flip error (erroneous implemen-
tation𝐶2) in the second layer. It is easy to check that only 4 out of all
28= 256 input strings can detect this discrepancy.
Masking is a serious issue for error detection using simulation
techniques. No malicious intent is required to fool randomly generated
inputs. The circuit may do it all by itself. Needless to say, this issue
has been well-known for decades. Error detection based on random
inputs (alone) often pales in comparison to other more sophisticated
techniques. Today’s state of the art is governed by constrained-based
stimuli generation techniques [ 7–11], fuzzing [ 12], etc. But on the
positive side, error detection using randomly-chosen inputs is based on
minimal assumptions, namely the possibility to simulate two circuits
as black boxes. Moreover, it is intuitive and individual simulation runs
are easy and fast to execute.
https://doi.org/10.1016/j.array.2022.100165
Received 31 July 2021; Received in revised form 15 February 2022; Accepted 1 April 2022Array 14 (2022) 100165
2L. Burgholzer et al.
Fig. 1. Error detection in classical circuits is hard: Suppose that a cascade of logical AND
gates, realizing the Boolean function 𝑦=𝑥8⋅…⋅𝑥1, is affected by a single bit-flip error
(red) in the second layer. Only 4out of the 28= 256 possible input strings can detect
this error.
Fig. 2. Illustration of main rigorous contributions: Simulations with uniformly random
inputs completely expose any single reversible error in a given reversible circuit.
The two scenarios are exactly equivalent (‘‘no masking’’). In the lower scenario, the
probability of correct distinction is governed by the size 𝑘of the error, not the total
number of lines.
2. Summary of results: Error detection in reversible circuits
We have seen that, in general, simulation with (uniformly) ran-
dom inputs is not a viable strategy for detecting errors in classical
circuits. Already a single ‘‘small’’ error can be exceedingly difficult
to detect (masking). Perhaps surprisingly, this dark picture lightens
up considerably if we consider reversible implementations of logical
functionalities. As the name suggests, reversible circuits are circuits
whose action can be undone by running the circuit backwards. More
formally,𝑛-bit reversible circuits implement permutations on the set of
all2𝑛bit strings. This, in particular, implies that the number of input
and output bits must be the same ( 𝑛=𝑚). Despite these restrictions,
reversible circuits are universal, i.e., anylogical function on 𝑛bits
can be implemented by a reversible circuit [13] and efficient mapping
techniques are readily available [14–16] (this implementation may
require strictly more than 𝑛bits, though). Negation ( NOT), exclusive or
(CNOT ) and the Toffoli gate ( CCNOT ) are examples of simple reversible
functionalities. Viewed as a logic gate, CCNOT is also universal. Every
reversible circuit can be constructed from Toffoli gates alone [13].
To summarize, reversible circuits bear strong similarities with clas-
sical (irreversible) circuits, but there are some notable additional char-
acteristics. Chief among them is reversibility itself which implies that
information cannot easily escape. Here, we show that this has profound
implications for error detection with random inputs. More precisely,
(i) reversible circuits can never mask single reversible errors (rigor-
ous result, see Proposition 1)
(ii) the probability of detecting a single reversible error only de-
pends on its size, i.e., on the number of bits it affects, not the
total number of bits (unsurprising rigorous result, see Lemma 2)
(iii) multiple reversible errors are typically even easier to detect
(empirical studies, see Fig. 3 and discussions in Section 4)The first two insights are mathematical statements that address single
errors only. They readily follow from reversibility and fundamental
properties of uniformly random input strings. We refer to Section 3
for details and Fig. 2 for illustrative caricatures. When combined, they
imply the following confidence bound for detecting single errors with
random inputs.
Theorem 1. Suppose that a general reversible circuit is affected by a single
reversible error of size 𝑘and fix𝛿∈ (0,1)(confidence). Then, at most
⌈log(1∕𝛿)2𝑘−1⌉randomly selected inputs suffice to witness this error with
probability (at least) 1 −𝛿.
For𝑘= 1– a single bit-flip error ( NOT)anywhere within the circuit
– this statement can be further improved (see Theorem 2) and actually
becomes deterministic: already a single (random) input is guaranteed
to detect this error with certainty. We emphasize that this statement is
true irrespective of the number of lines and the circuit’s size. It is simply
impossible to hide a single bit-flip inside a reversible circuit. Such a
behavior is strikingly different from irreversible circuit architectures.
There it can routinely happen that order 2𝑛random inputs are necessary
to detect even a single bit-flip error, see e.g. Fig. 1.
The multiple-error case is much more intricate, because error lo-
cations and circuit structure start to matter. This leads to drastically
different behaviors of best case (independent errors) and worst case
(severe masking) behavior. To better understand the typical behavior
of multiple errors, we resort to numerical simulations. These indicate
a (close-to) best-case behavior: the probability of failing to detect a
total of𝑙reversible errors is exponentially suppressed in 𝑙, see Fig. 3.
Additional simulation results and details are provided in Section 4.
Note that a similar line of thought has recently been presented for
the domain of quantum computing (which bears many similarities to
reversible circuits). More precisely, a verification scheme heavily based
on simulation has been proposed in [17] and refined in [18]. A similar
theoretical result has been presented in [19].
3. Rigorous theory for single errors
3.1. Reversible circuits and error model
We will work in the reversible circuit model for 𝑛input bits (and 𝑛
output bits). A high-level of mathematical abstraction already suffices
to deduce powerful consequences. An 𝑛-bit reversible circuit imple-
ments a permutation 𝑅∶{0,1}𝑛→{0,1}𝑛of all 2𝑛bit strings. Reversing
the circuit, that is running it backwards, produces the unique per-
mutation𝑅𝑇∶{0,1}𝑛→{0,1}𝑛that undoes the original circuit:
𝑅𝑇◦𝑅=𝑅◦𝑅𝑇= id, where id(⃗ 𝑥) =⃗ 𝑥for all⃗ 𝑥∈{0,1}𝑛is the
identity permutation (‘‘do nothing’’). This defining feature suffices to
deduce three elementary properties that will form the basis of our proof
strategy.
Lemma 1 (Characteristics of Reversible Circuits ).Consider reversible
circuits𝑅1,𝑅2,𝑅3∶{0,1}𝑛→{0,1}𝑛and an𝑛-bit string⃗ 𝑥∈{0,1}𝑛. Then,
(i) output equivalence is unaffected by composition:
𝑅1(⃗ 𝑥) =𝑅2(⃗ 𝑥)⇔(𝑅3◦𝑅1)(⃗ 𝑥) = (𝑅3◦𝑅2)(⃗ 𝑥)
(ii) invariance of the uniform distribution:
⃗ 𝑥∼Unif({0,1}𝑛)implies𝑅1(⃗ 𝑥) ∼Unif({0,1}𝑛)
(iii) non-trivial action: suppose 𝑅1≠id. Then, there are at least two bit
strings such that 𝑅1(⃗ 𝑥)≠⃗ 𝑥.
Proof. All proofs utilize the fact that reversible circuits act like
permutations on the set of all 2𝑛bit strings.
(i) Permutations are invertible transformations. As such, they pre-
serve equivalence: 𝑦=𝑦′if and only if 𝑅(𝑦) =𝑅(𝑦′)for any
reversible circuit 𝑅. The claim follows from setting 𝑦=𝑅1(⃗ 𝑥),
𝑦′=𝑅2(⃗ 𝑥)and𝑅=𝑅3.Array 14 (2022) 100165
3L. Burgholzer et al.
Fig. 3. Typical accumulation effects for multiple errors (log–log plot): number𝑙of randomly injected reversible errors ( 𝑥-axis) vs. average number of random inputs required to detect
erroneous behavior ( 𝑦-axis) in a generic 𝑛= 20-bit reversible circuit with 4000 gates. Different colors denote worst-case errors of increasing size 𝑘. Solid lines track the theoretical
best-case behavior (independent errors, see Eq. (5) below). For small 𝑙, the plot highlights an excellent agreement between typical (diamonds) and best-case (solid lines) behavior.
Fig. 4. (Single) error model and compatible circuit decomposition: An ideal reversible
circuit (blue) is corrupted by a single reversible error (red). The error location begets
a decomposition of ideal and corrupted circuit into matching constituents: 𝑅=𝑅2◦𝑅1
(ideal) and ̃𝑅=𝑅2◦𝐸◦𝑅1(corrupted).
(ii) The uniform distribution over 𝑛-bit strings assigns the same
weight to each of the 2𝑛bit strings. Permuting the bit strings can-
not affect the weights and, by extension, the uniform distribution
itself.
(iii) The number of invariant bit strings ( ⃗ 𝑥∈{0,1}𝑛∶𝑅1(⃗ 𝑥) =⃗ 𝑥) is
equal to the number of fix points of the underlying permutation.
A non-trivial permutation of 2𝑛elements can have at most 2𝑛− 2
fix points (transposition). □
Different reversible circuits of compatible bit-size 𝑛can be combined
to yield another (larger) circuit: (𝑅2◦𝑅1)(⃗ 𝑥) =𝑅2(𝑅1(⃗ 𝑥))for input⃗ 𝑥∈
{0,1}𝑛(‘‘composition’’). The reverse direction is also possible (‘‘decom-
position’’) and, arguably, more interesting. Circuit diagrams provide a
well-established tool that does precisely that. They decompose a possi-
bly complicated circuit into a structured sequence of simpler building
blocks. We use circuit decomposition on a rather high level to reason
about single reversible errors in reversible circuits. Suppose that an 𝑛-bit
reversible circuit 𝑅is affected by a reversible error 𝐸that produces a
functionally different circuit ̃𝑅. Then, the location of this error within
the circuit suggests a compatible decomposition into three parts:
(i)𝑅1∶{0,1}𝑛→{0,1}𝑛describes the original functionality up to
the location where the error occurs (‘‘past’’),
(ii)𝐸∶{0,1}𝑛→{0,1}𝑛captures the error as an additional circuit
layer on all 𝑛bits (‘‘present’’),(iii)𝑅2∶{0,1}𝑛→{0,1}𝑛describes the original functionality from
the error location onwards (‘‘future’’).
In summary,
̃𝑅=𝑅2◦𝐸◦𝑅1,while𝑅=𝑅2◦𝑅1, (2)
and we refer to Fig. 4 for a visual illustration.
3.2. No masking for random inputs
We now have all building blocks in place to present and derive
the main conceptual result of this work. It addresses the probability
of detecting single reversible errors in arbitrary reversible circuits (2)
based on a single random input ⃗ 𝑥∼ Unif( {0,1}𝑛).
Proposition 1 (No Masking ).Fix𝑅=𝑅2◦𝑅1(ideal circuit) and ̃𝑅=
𝑅2◦𝐸◦𝑅1(single, reversible error). Then, the probability of detecting this
discrepancy with a random input ⃗ 𝑥∼ Unif( {0,1}𝑛)only depends on the
error𝐸, not the actual circuit. More precisely,
Pr[̃𝑅(⃗ 𝑥)≠𝑅(⃗ 𝑥)]= Pr[𝐸(⃗ 𝑥)≠⃗ 𝑥],
where the probability is taken with respect to the uniform distribution over
all2𝑛possible input strings.
Proof. This statement is an immediate consequence of two elementary
characteristics of reversible circuit architectures. Apply Lemma 1(i) to
remove the effect of 𝑅2,
Pr[̃𝑅(⃗ 𝑥) =𝑅(⃗ 𝑥)]=[𝑅2◦𝐸◦𝑅1(⃗ 𝑥) =𝑅2◦𝑅1(⃗ 𝑥)]
= Pr[𝐸(𝑅1(⃗ 𝑥))=𝑅1(⃗ 𝑥)],
and note that, according to Lemma 1 (ii), ⃗ 𝑥∼ Unif( {0,1}𝑛)implies
𝑅1(⃗ 𝑥) ∼ Unif( {0,1}𝑛).□
Although simple to prove, Proposition 1 pinpoints remarkable dif-
ferences between reversible and irreversible circuits. As illustrated in
Fig. 2, the former cannot hide errors from randomly sampled inputs
(‘‘no masking’’).
We emphasize that a uniformly random selection of input strings
is crucial to arrive at such a powerful conclusion. Reversibility alone
is enough to ignore the final portion of the circuit 𝑅2(after the errorArray 14 (2022) 100165
4L. Burgholzer et al.
has occurred). Reversible circuits always map (non-)equal bit strings
to (non-)equal bit strings. In contrast, the first portion of the circuit 𝑅1
(before the error has occurred) can affect concrete inputs ⃗ 𝑥∈{0,1}𝑛.
But if⃗ 𝑥is sampled randomly, then 𝑅1(⃗ 𝑥)will be a different, but still
random, bit string. The uniform distribution is special in the sense
that it is invariant under reversible transformations. The circuit 𝑅1
may affect every concrete input, but it does not affect the underlying
distribution.
3.3. Only error size matters
We have seen that uniformly random inputs can uncover single
reversible errors in a general reversible circuit. According to Proposi-
tion 1, the probability of witnessing a discrepancy only depends on the
error, not the underlying circuit structure.
We say that an error 𝐸∶{0,1}𝑛→{0,1}𝑛hassize𝑘if it only
affects𝑘bits in a nontrivial fashion. The remaining 𝑛−𝑘bits are not
touched at all. We refer to Fig. 2 for a visual illustration of this summary
parameter. Intuitively, we would expect that ‘‘large’’ errors are easier
to detect than ‘‘small’’ ones and that the number of lines 𝑛plays an
active role. However, the following simple statement shows that the
probability of detecting an error in the worst case is exponentially
suppressed with respect to the error size 𝑘, but is independent of the
actual number of bits 𝑛.
Lemma 2 (Only Error Size Matters ).Suppose that 𝐸∶{0,1}𝑛→{0,1}𝑛
is a reversible error that only affects 𝑘bits in a non-trivial fashion and
⃗ 𝑥∼ Unif( {0,1}𝑛)is sampled from the uniform distribution. Then,
Pr[𝐸(⃗ 𝑥)≠⃗ 𝑥]≥2−(𝑘−1).
Proof. Suppose, without loss of generality, that the error 𝐸only affects
the least-significant 𝑘bits, i.e.,𝐸(⃗ 𝑥) =𝐸(𝑥𝑛,…,𝑥1) = (𝑥𝑛,…,𝑥𝑘+1,𝑦𝑘,
…,𝑦1), where (𝑦𝑘,…,𝑦1) =̃𝐸(𝑥𝑘,…,𝑥1). Since𝐸is reversible, its
restriction ̃𝐸∶{0,1}𝑘→{0,1}𝑘to the𝑘relevant bits must also be
reversible. Moreover, ̃𝐸≠id, because𝐸is non-trivial. Lemma 1 (iii)
then implies that there must be at least 2 bit strings of size 𝑘that
are affected by ̃𝐸. Finally, we use the fact that ⃗ 𝑥= (𝑥𝑛,…,𝑥1) ∼
Unif( {0,1}𝑛)implies that the least-significant 𝑘bits are also distributed
uniformly: (𝑥𝑘,…,𝑥1) ∼ Unif( {0,1}𝑘). Therefore,
Pr[𝐸(⃗ 𝑥)≠⃗ 𝑥]= Pr[̃𝐸(𝑥𝑘,…,𝑥1)≠(𝑥𝑘,…,𝑥1)]≥2
2𝑘.□
This probability bound is actually sharp. Worst-case reversible er-
rors of size 𝑘permute exactly 2 out of the 2𝑘possible𝑘-bit inputs on
which they act. Concrete examples of such a behavior are NOT (𝑘= 1),
CNOT (𝑘= 2),CCNOT (𝑘= 3) and, more generally, a (𝑘− 1)-fold
controlled NOT gate on𝑘bits (general 𝑘). The numerical simulations
shown in Fig. 3 are based on injecting such worst-case errors at random
circuit locations.
3.4. General confidence bound for detecting single reversible errors
We now have all necessary ingredients to establish a rigorous per-
formance guarantee for reversible error detection with (uniformly)
random inputs. The following statement bounds the number of uni-
formly random inputs that may be required to detect a single reversible
error of size 𝑘.
Theorem 2. Fix𝑅=𝑅2◦𝑅1(ideal circuit), ̃𝑅=𝑅2◦𝐸◦𝑅1(sin-
gle, reversible error) and 𝐸has size𝑘. Suppose that ⃗ 𝑥1,…,⃗ 𝑥𝑁are𝑁
(independent) uniformly random inputs. Then,
Pr[⋀
1≤𝑖≤𝑁{̃𝑅(⃗ 𝑥𝑖) =𝑅(⃗ 𝑥𝑖)}]
≤exp(−𝑁∕2𝑘−1)
In words, the probability of failing to detect a single error is exponentially
suppressed in the number 𝑁of random test inputs.
Fig. 5. Partial simplification for multiple errors: Simulation with uniformly random inputs
exposes multiple errors only partially. Everything before the first error ( 𝑅1) and after
the last error ( 𝑅3) can be safely ignored, but the part in between ( 𝑅2) does matter.
Different circuit structures can lead to strikingly different error detection probabilities.
Theorem 1 above is a streamlined consequence of this observation:
setting𝑁=⌈log(1∕𝛿)2𝑘−1⌉provides a concrete number of repetitions
that ensures that we detect the discrepancy with probability (at least)
1 −𝛿.
Proof of Theorem 2. For𝑁= 1(one random input), the claim readily
follows from combining Proposition 1 and Lemma 2 (more precisely,
their contrapositions):
Pr[̃𝑅(⃗ 𝑥1) =𝑅(⃗ 𝑥1)]= Pr[𝐸(⃗ 𝑥1) =⃗ 𝑥1]≤1 − 2−(𝑘−1).
This bound readily extends to the general 𝑁-case by using the as-
sumption that the individual input strings ⃗ 𝑥1,…,⃗ 𝑥𝑁are all sampled
independently. Joint probabilities of independent events factorize and
we conclude
Pr[⋀
1≤𝑖≤𝑁{̃𝑅(⃗ 𝑥𝑖) =𝑅(⃗ 𝑥𝑖)}]
=𝑁∏
𝑖=1Pr[̃𝑅(⃗ 𝑥𝑖) =𝑅(⃗ 𝑥𝑖)]
≤(1 − 2−(𝑘−1))𝑁. (3)
Apply 1+𝑥<exp(𝑥)for all𝑥∈R(convexity of the exponential function)
with𝑥= −2−(𝑘−1)to complete the argument. □
The bound provided in Theorem 2 is simple, but not sharp (the in-
equality 1+𝑥≤exp(𝑥)is never tight). As such, it always under-estimates
the actual confidence level. This discrepancy is most pronounced for
small error sizes 𝑘. The extreme case is a single NOT error (𝑘= 1). For
𝑘= 1, the bound in Eq. (3) becomes (exactly) zero. By contraposition,
every possible input bit string is guaranteed to detect a single bit-flip error
that is hidden anywhere within the circuit.
4. Empirical analysis for multiple errors
In the previous section, we have established strong theoretical
support for detecting single reversible errors. At its heart has been the
decomposition ̃𝑅=𝑅2◦𝐸◦𝑅1illustrated in Fig. 4. Reversibility and
uniformly random inputs have subsequently allowed us to discuss away
the circuit portions 𝑅2and𝑅1completely. In turn, we were able to
focus exclusively on the error itself.
For more than one error, this is in general not an option anymore.
While we can safely ignore circuit contributions before the first and
after the last error, the circuit in between cannot be ignored, see Fig. 5.
The relation between errors and intermediate circuit parts governs how
likely it is to witness the overall error.
In this section, we analyze error accumulation effects in generic
reversible circuits. To obtain guiding intuition, we will first isolate
and discuss the two extreme cases. Independent errors (best case, seeArray 14 (2022) 100165
5L. Burgholzer et al.
Fig. 6. Best-case scenario for two errors: One of the errors, say 𝐸2, commutes with the
relevant circuit part 𝑅2. Reordering allows us to treat the two errors as a single effective
error̃𝐸=𝐸1◦𝐸2. In addition, 𝐸1and𝐸2affect disjoint bit collections (independence)
and̃𝐸factorizes nicely into two disjoint components: Pr[̃𝑅(⃗ 𝑥)≠𝑅(⃗ 𝑥)]= Pr[̃𝐸(⃗ 𝑥)≠⃗ 𝑥]≥
1 − (1 − 2−(𝑘−1))2(quadratic improvement).
Section 4.1) and maximal masking (worst case, see Section 4.2) turn out
to behave in a radically different fashion. Subsequent numerical studies
demonstrate that typical error accumulation effects closely follow the
best-case trajectory: Multiple errors are typically much easier to detect
than a single error.
4.1. Best-case behavior: Commuting and independent errors
Let us first discuss 𝑙= 2reversible errors of size 𝑘. An extension to
multiple errors ( 𝑙≥3) and different sizes will be straightforward. Fig. 6
provides valuable guidance for potential best-case behavior. Suppose
that one of the errors, say 𝐸2, can be pulled through the central circuit
part𝑅2without affecting it: 𝐸2◦𝑅2=𝑅2◦𝐸2. If circuit and error
commute in such a fashion, we can group both errors into a single layer
and have effectively reduced the problem to the single-error case which
we already understand:
̃𝑅=𝑅3◦𝐸2◦𝑅2◦𝐸1◦𝑅1= (𝑅3◦𝑅2)◦(𝐸2◦𝐸1)◦𝑅1.
The only remaining question is: what is the probability of failing to
detect the cumulative error 𝐸2◦𝐸1with a single random input? This
failure probability is smallest if the two errors are independent in
the sense that they act on disjoint sets of 𝑘bits each. A uniformly
random input ⃗ 𝑥∈ Unif( {0,1}𝑛)then ensures that the failure probability
factorizes:
Pr[(𝐸2◦𝐸1)(⃗ 𝑥) =⃗ 𝑥]=2∏
𝑖=1Pr[𝐸𝑖(⃗ 𝑥) =⃗ 𝑥]≤(1 − 2−(𝑘−1))2.
This argument readily extends to multiple errors ( 𝑙≥3). Taking the
complement ensures
Pr[̃𝑅(⃗ 𝑥)≠𝑅(⃗ 𝑥)]=1 − Pr[𝐸𝑙◦⋯◦𝐸1(⃗ 𝑥) =⃗ 𝑥]
≥1 −(1 − 2−(𝑘−1))𝑙, (4)
provided that all 𝑙errors commute with the circuit (first equality) and
act on different subsets of 𝑘bits each (second inequality). Rel. (4)
highlights that the probability of (best case) error detection increases
substantially with the number of errors 𝑙. Intuitively, this makes sense:
more errors should be easier to detect. This insight has implications
for the number 𝑁of random inputs that are required to detect 𝑙best-
case errors of size 𝑘each. To pinpoint them, it is instructive to view
a single simulation run as a biased coin toss: we detect a discrepancy
with probability 𝑝= Pr[̃𝑅2(⃗ 𝑥)≠𝑅2(⃗ 𝑥)](‘‘heads’’) and fail to detect it
with probability 1 −𝑝= Pr[̃𝑅2(⃗ 𝑥) =𝑅2(⃗ 𝑥)](‘‘tails’’). When attempting
to detect a discrepancy, we input new randomly generated inputs until
we find a mismatch. This is equivalent to tossing the biased coin
until ‘‘heads’’ appears. The expected number of required coin tosses to
achieve this goal is 1∕𝑝(geometric distribution). Together with Rel. (4),
this analogy allows us to conclude that we expect to require
𝑁(↓)
expect≤1
1 −(1 − 2−(𝑘−1))𝑙(best case) (5)
random inputs to detect 𝑙commuting and independent errors of size 𝑘
each. This bound is sharp. It holds with equality if each of the 𝑙errors
is a worst-case error of size 𝑘, e.g. a (𝑘− 1)-fold controlled NOT gate.
Fig. 7. Worst-case scenario for two errors: Two bit-flip errors ( 𝑘= 1) affect one control
line of a (𝑛−1)-fold controlled NOT-gate. These errors do not commute with the relevant
circuit part 𝑅2. Quite the opposite: two errors with size 𝑘= 1produce an effective error
̃𝐸of size𝑘= (𝑛− 1). To make matters even worse, such a (𝑛− 2)-fold controlled NOT
error is extremely difficult to detect: Pr[̃𝑅(⃗ 𝑥)≠𝑅(⃗ 𝑥)]= Pr[̃𝐸(⃗ 𝑥)≠⃗ 𝑥]= 4∕2𝑛(masking).
We conclude this section with a simplified interpretation of Rel (5).
For small 𝑙(in comparison to 2(𝑘−1)), the claim is comparable to
𝑁(↓)
expect≈ 2𝑘−1∕𝑙, which can also be observed in Fig. 3: the slopes of
the solid lines match this estimate rather well whenever the number
of errors𝑙is small compared to 2(𝑘−1). Under best-case assumptions,
detecting𝑙size𝑘-errors is𝑙-times easier than detecting a single error
of the same size.
4.2. Worst-case: anti-commuting errors and masking
We expect that worst case error accumulation should occur when
errors and relevant circuit portion do not commute at all (‘‘anti-
commutation’’). If this is the case, the probability of detecting errors
can become exponentially small in the total number of bits. We illus-
trate this by means of an example that is illustrated in Fig. 7: 𝐸1and
𝐸2are bit-flip errors ( 𝑘= 1) that affect the first bit while 𝑅2∶{0,1}𝑛→
{0,1}𝑛is a(𝑛− 1)-fold controlled NOT-gate. It is easy to check that
𝐸2◦𝑅2◦𝐸1=̃𝐸◦𝑅2,
wherẽ𝐸is a(𝑛−2)-fold controlled NOT gate that acts on all bits, except
the very first one ( 𝑘=𝑛− 1). This is a single worst-case error of almost
maximal size. Proposition 1 and Lemma 2 assert
Pr[̃𝑅(⃗ 𝑥)≠𝑅(⃗ 𝑥)]= Pr[̃𝐸(⃗ 𝑥)≠⃗ 𝑥]=4
2𝑛.
This success probability is exponentially small in the total number of
bits and we expect to require a total of
𝑁(↑)
expect≥2(𝑛−2)(worst case) (6)
random inputs in order to detect the discrepancy. Even worse error
accumulation effects can occur for more errors ( 𝑙≥3) and/or larger
error sizes ( 𝑘≥2). But already Rel. (6) is almost as bad as it can be.
It is only a factor of two away from 2𝑛−1—the absolute worst case for
distinguishing anypair of reversible circuits, see Lemma 1 (iii).
4.3. Empirical studies
The multiple-error case is intricate by comparison, because the
interplay between error (locations) and underlying circuit geometry
starts to matter. We have seen that this leads to strikingly different best-
(commuting errors, Sub. 4.1) and worst-case (anticommuting errors,
Sub. 4.2) behavior. Concrete problem instances fall into the wide range
between these extreme cases. In this section, we employ numerics to
delineate typical behavior.
We study the effect of size- 𝑘reversible errors in reversible circuits
with𝑛lines. For a given number of lines 𝑛, we construct random
reversible circuits with 𝑔≈(𝑛2)arbitrary multi-controlled NOT
gates. When injecting errors of size 𝑘, we always consider (𝑘− 1)-
fold controlled NOT gates which represent the worst case behavior,Array 14 (2022) 100165
6L. Burgholzer et al.
Fig. 8. Confirmation of theoretical results : Scatter-plot of required simulations ( 𝑦-axis) for detecting a single reversible error of size 𝑘in a circuit with 𝑛= 20 (left plot) and 𝑛= 40
(right plot) lines. Different colors denote varying values of 𝑘∈ {1,2,3,4,5}. This experimentally confirms that the distribution of simulations does not depend on the number of
lines and that the number of required simulations grows exponentially with the size of the error.
Fig. 9. Comparison of worst-case and average-case errors : performed simulations ( 𝑥-axis) vs. cumulative distribution function (cdf) for detecting 𝑙= 1,2,4,6reversible errors of size
𝑘= 5(𝑦-axis). The red curve corresponds to injecting worst-case errors, while the blue curve delineates the cdf for detecting randomly generated errors of the same size. This goes
to show, that average-case errors require far less simulations than worst-case ones.
as discussed in Section 3.3. Without loss, we assume that these errors
are geometrically local, i.e., they only affect neighboring lines. All
experiments were repeated 10 000 times with different random seeds
in order to ensure adequate statistical uniformity.
First and foremost, we confirm interesting aspects of the theory
developed in Section 3. To this end, we considered the injection of a
single size- 𝑘-error and count the required number of simulations for
detecting this error. The results are depicted in Fig. 8. In contrast to
classical intuition, the probability of detecting a single reversible error
of size𝑘is (1) completely independent of the circuit under consid-
eration, and (2) diminishes exponentially in the error size 𝑘, i.e., thesmaller the error, the greater its impact. This is in excellent agreement
with Theorem 2. On average, the required simulations exactly follow
the predicted 2𝑘−1trajectory with no apparent variation. Additionally,
the distributions of results is the same when simulating the circuits
𝑅=𝑅2◦𝑅1and̃𝑅=𝑅2◦𝐸◦𝑅1as compared to only simulating the
error𝐸itself.
The next set of numerical experiments pilots us in more interesting
territory. Namely, the multiple-error case. We have already teased the
results in the introduction and summarized them in Fig. 3. The aver-
aged number of inputs highlights an excellent agreement between the
observed behavior and the best-case scenario discussed in Section 4.1.Array 14 (2022) 100165
7L. Burgholzer et al.
The deviation from this optimum for higher numbers of errors can be
explained by accumulation affects of errors not acting independently
(see Section 4.2).
Last but not least, we emphasize that – up to this point – theoretical
and empirical results have been contingent on a worst-case assumption:
each injected size- 𝑘error is a (𝑘− 1)-fold controlled NOT-gate. In a
final series of evaluations, we analyzed the success probability after
conducting a certain number of simulations when choosing errors at
random . More precisely, each size- 𝑘error is a randomly selected gate
sequence with the additional constraint that none of the 𝑘relevant lines
remain unaffected (such a scenario would produce an error of size (at
most) (𝑘−1)). We expect that this error model captures typical behavior
in a more accurate fashion. The results are shown in Fig. 9 and highlight
a considerable discrepancy between random (blue) and worst-case (red)
errors. This is not at all surprising. Random errors of size 𝑘tend to
factorize into several independent contributions and the probabilities of
detecting them with random inputs factorizes accordingly, see Sub. 4.1.
Such factorizations lead to an increased error detection probability
within (very) few simulation runs.
5. Conclusion
In this work, we have shown the impact of the reversible circuit
paradigm on the probability of detecting errors in circuits. Our rigor-
ous analysis shows, that, as opposed to classical/irreversible circuits,
reversible circuits can never mask single errors and, that the probability
of detecting a single reversible error only depends on the error’s size
and not at all on the surrounding circuit. Empirical evaluations have
shown that, in case of multiple errors, the detection probability is very
close to the theoretical best case. Finally, we have observed that, in
case the assumption of worst-case errors is dropped, the probability of
detecting these errors is increased even more.
CRediT authorship contribution statement
Lukas Burgholzer: Investigation, Software, Validation, Writing –
original draft, Visualization. Robert Wille: Conceptualization, Funding
acquisition, Writing – review & editing, Supervision. Richard Kueng:
Methodology, Formal analysis, Writing – original draft, Supervision,
Project administration.
Declaration of competing interest
No author associated with this paper has disclosed any potential or
pertinent conflicts which may be perceived to have impending conflict
with this work.Acknowledgments
The authors want to thank J. Küng for inspiring discussions through-
out the early stages of this project and W. Schreiner for further valuable
feedback.
This work received funding from the European Research Council
(ERC) under the European Union’s Horizon 2020 research and in-
novation program (grant agreement No. 101001318 ), was part of the
Munich Quantum Valley, which is supported by the Bavarian state
government with funds from the Hightech Agenda Bayern Plus, and has
been supported by the BMK, BMDW, and the State of Upper Austria in
the frame of the COMET program (managed by the FFG).
References
[1] Disch S, Scholl C. Combinational equivalence checking using incremental
SAT solving, output ordering, and resets. In: Asia and South Pacific Design
Automation Conference. 2007, p. 938–43.
[2] Marques-Silva Ja, Glass T. Combinational equivalence checking using satisfi-
ability and recursive learning. In: Design, Automation and Test in Europe.
1999.
[3] Molitor P, Mohnke J. Equivalence checking of digital circuits: fundamentals,
principles, methods. Springer; 2010.
[4] Jha S, Lu Y, Minea M, Clarke EM. Equivalence checking using abstract BDDs.
In: Int’l Conference on Comp. Design. 1997.
[5] Clarke EM, Grumberg O, Kroening D, Peled DA, Veith H. Model checking. MIT
Press; 2018.
[6] Biere A, Cimatti A, Clarke EM, Zhu Y. Symbolic model checking without BDDs.
In: Tools and algorithms for the construction and analysis of systems. 1999, p.
193–207.
[7] Yuan J, Pixley C, Aziz A. Constraint-based verification. Springer; 2006.
[8] Biere A, Kunz W. SAT and ATPG: boolean engines for formal hardware
verification. In: Int’l Conference on CAD. 2002, p. 782–5.
[9] Wille R, Große D, Haedicke F, Drechsler R. SMT-based stimuli generation in the
systemc verification library. In: Forum on specification and design languages.
2009.
[10] Kitchen N, Kuehlmann A. Stimulus generation for constrained random simulation.
In: Int’l Conference on CAD. 2007, p. 258–65.
[11] Gent K, Hsiao MS. Fast multi-level test generation at the RTL. In: IEEE annual
symposium on VLSI. 2016, p. 553–8.
[12] Laeufer K, Koenig J, Kim D, Bachrach J, Sen K. RFUZZ: coverage-directed fuzz
testing of RTL on FPGAs. In: Int’l Conference on CAD. 2018.
[13] Toffoli T. Reversible computing. In: Automata, languages and programming, vol.
85. Springer; 1980, p. 632–44.
[14] Zulehner A, Wille R. Make it reversible: efficient embedding of non-reversible
functions. In: Design, automation and test in Europe. 2017.
[15] Maslov D, Dueck GW. Reversible cascades with minimal garbage. IEEE
Transactions on CAD of Integrated Circuits and Systems 2004;23(11):1497–509.
[16] Zilic Z, Radecka K, Kazamiphur A. Reversible circuit technology mapping from
non-reversible specifications. In: Design, Automation and Test in Europe. 2007.
[17] Burgholzer L, Wille R. The power of simulation for equivalence checking in
quantum computing. In: Design Automation Conference. 2020.
[18] Burgholzer L, Kueng R, Wille R. Random stimuli generation for the verification
of quantum circuits. In: Asia and South Pacific Design Automation Conference.
2021.
[19] Linden N, Wolf Rd. Lightweight detection of a small number of large errors in
a quantum circuit, arXiv:2009.08840, 2020.