Unilateral Alignment: An interpretable machine learning method forgeophysical logs calibration
Wenting Zhanga, Jichen Wangb, Kun Lic, Haining Liue, Yu Kanga,c,d, Yuping Wud, Wenjun Lv
a,c,*
aDepartment of Automation, University of Science and Technology of China, Hefei, 230027, China
bCollege of Control Science and Engineering, China University of Petroleum, Qingdao, 266580, China
cInstitute of Advanced Technology, University of Science and Technology of China, Hefei, 230031, China
dInstitute of Artiﬁcial Intelligence, Hefei Comprehensive National Science Center, Hefei, 230088, China
eShengli Geophysical Research Institute, SINOPEC Group, Dongying, 257022, China
ARTICLE INFO
Keywords:Interpretable machine learningGeophysical logs calibrationData distribution discrepancyABSTRACT
Most of the existing machine learning studies in logs interpretation do not consider the data distributiondiscrepancy issue, so the trained model cannot well generalize to the unseen data without calibrating the logs. Inthis paper, we formulated the geophysical logs calibration problem and give its statistical explanation, and thenexhibited an interpretable machine learning method, i.e., Unilateral Alignment, which could align the logs fromone well to another without losing the physical meanings. The involved UA method is an unsupervised featuredomain adaptation method, so it does not rely on any labels from cores. The experiments in 3 wells and 6 tasksshowed the effectiveness and interpretability from multiple views.
1. IntroductionArtiﬁcial intelligence (AI) is increasingly becoming an importantscientiﬁc and technological tool in solving geosciences issues such asearthquakes detection, spatial prediction, stratigraphic correlation,seismic processing and interpretation, etc. ( Magrini et al., 2020;Foued- jio, 2020;Xu et al., 2022;Zhou et al., 2020;Birnie et al., 2021). It has reported that the rapidly evolvingﬁeld of machine learning (ML), a popular way to realize AI, will play a key role in geosciences ( Bergen et al., 2019). However, due to the black-box issue existing in many MLmodels, we cannot know the way how an ML model works, so cannotassess its reliability and have enough conﬁdence to apply it in produc- tion. We believe that the interpretability of ML will play an extremelyimportant role in geosciences in which there are too many risk-awareapplications, e.g., exploration.The geophysical logs interpretation means the prediction of boreholegeology information (e.g., lithofacies, porosity) according the logs, whichis a fundamental and worthy work to understand the undersurface earth.The recent years have witnessed the development of the applications ofmachine learning in logs interpretation. For example, a semi-supervisedmodel named Laplacian support vector machine is proposed to solve theproblems of scarce labels (Li et al., 2020). The smoothnesses implying in the feature space and depth are utilized to propagate the labels formlabelled samples to unlabelled ones, therefore increasing the classi ﬁca- tion accuracy. Although the Laplacian works in semi-supervised learning,the setting of Laplacian is still empirical so that the introduction ofunlabelled data might result in a worse performance compared with justusing labelled data. Hence, an ensemble mechanism is used to combinesome candidate Laplacians and calculate an optimal one to guarantee thesafety of using semi-supervision (Li et al., 2021). More similar work could be found in the literatures (Dunham et al., 2020;Imamverdiyev and Sukhostat, 2019;Zhu et al., 2018).Although there has been a great deal of researches in this area, mostof them work under the assumption of independent and identically dis-tribution (iid), i.e., the data from training wells should have similardistribution with those data from testing wells. However, such anassumption barely hold due to the differences in logging equipments,borehole conditions, etc. As illustrated in Fig. 1c, there are a signiﬁcant distribution discrepancy between the training dataset and testing dataset,so iid does not hold anymore. As shown in Fig. 1a andb, the non-iid problem causes an accuracy decrease in interpreting the logs fromanother wells by the trained classiﬁer. Both supervised and semi-
* Corresponding author. Department of Automation, University of Science and Technology of China, Hefei, 230027, China.E-mail address:wlv@ustc.edu.cn(W. Lv).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2022.02.006Received 27 December 2021; Received in revised form 27 February 2022; Accepted 27 February 2022Available online 14 March 20222666-5441/©2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 192–201supervised classiﬁcation methods cannot handle such a situation, sosome pre-processing work should be done in advance of interpretation. Inaddition, data mining and processing techniques can be coupled withmachine learning algorithms to solve the problem of data imbalance(Long et al., 2016;Zhong et al., 2020) Filtering techniques help us to remove some of the random non-stratigraphic responses ( Lv et al., 2018; Ruckebusch, 1983), but do not have a signiﬁcant effect on the distribu- tion alignment. For such a geophysical logs calibration issue, we canresort to the Domain Adaptation (DA) methods which are very popular inmachine learning.As a transfer learning method, DA tries toﬁnd the relevance between two datasets and then adopts the knowledge from one domain to another.DA could works in an unsupervised manner which means that we onlyneed the samples from two datasets. DA generally falls into three cate-gories: model-, sample-, and feature-based methods ( Yang et al., 2020). The sample-based DA re-weights the samples overlapped in the featurespace during the training phase to yield a more robust model in bothdomain, but it could only handle the small distribution discrepancy issue(Chang et al., 2021b;Chang et al., 2022). The model-based DA re-adjust the source-domain classiﬁer toﬁt the target domain (Liu et al., 2020). For a neural network model, usually the last few layer are allowed to adjust,so the transferability is not strong enough. The feature-based DA (fDA)tries to map the samples to a feature space with higher dimension suchthat two domains align (Li et al., 2018). As we know that a deep neural network possesses multiple layers for feature extraction, so the fDAusually has more feasibility. Most fDA methods map both domains, so thephysical meanings of the original features would loss ( Chang et al., 2021a). Instead, we could just align one domain to another in the originalfeature space, therefore maintaining the physical meanings ( Chen et al., 2018;Wu et al., 2022). Such a fDA method could be named UnilateralAlignment (UA) which might be an excellent tool for geophysical logscalibration to our opinions.
Fig. 1.Illustration of data distribution discrepancy. (a) Logs, cores (i.e, labels), and predictions on Well A using the model trained on Well B; (b) Logs, cores, pre- dictions on Well B using the model trained on Well A; (c) t-SNE ( van der Maaten and Hinton, 2008 ) visualization of data distribution for Well A and Well B. Gray, yellow, and green indicate mudstone, sandstone, and dolomite, respectively. Dots and boxes indicate the data from Well A and Well B, respectively.
Fig. 2.Illustration of the UA framework.
Table 1Dataset description.
WellsbLithologya
Mu Si Co SumA 955 512 113 1580B 514 396 108 1018C 735 220 100 1055
aMu, mudstone; Si, siltstone; Co, conglomeratic sandstone.
bLogging curves of each well: AC, acoustic log; CAL, caliper log; COND, con-ductivity log; GR, gamma ray log; R25, 2.5 m bottom gradient resistivity; and SP,spontaneous potential log resistivity.
Table 2Macro-average F1-score (%) on 6 tasks.
Tasks MethodsS.O.
aT.O.bUAB→A 82.7 98.5 95.9C→A 80.0 99.3 92.7A→B 82.9 99.9 97.2C→B 79.7 99.5 93.5A→C 92.3 99.4 96.1B→C 73.3 98.4 93.7
aWeighted ELM classiﬁer trained with source-domain examples.
bWeighted ELM classiﬁer trained with target-domain examples.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
193In this paper, we would exhibit an interpretable machine learningmethod, i.e., UA, which could align the logs from one well to anotherwithout losing the physical meanings. In addition, the involved UAmethod is an unsupervised fDA method, so it does not rely on any labelsfrom cores. The contributions are threefold. First, we formulate thegeophysical logs calibration problem and give its statistical explanation.Second, we present the derivation of UA and illustrate its application inlogs calibration. Third, we conduct massive experiments in 3 wells and 6tasks to show the effectiveness and interpretability from multiple views.The above three contributions correspond to Sec. 2, 3, and 4, respectively.2. Formulation of geophysical logs calibration problemIn the logs interpretation problem (e.g., lithofacies identi ﬁcation), the well logging feature vector (also named instance or sample) of dlogs at a certain depth is denoted byx
i¼½x i1;xi2;⋯;x id/C1382Rd, including acoustic log (AC), Gamma ray log (GR), caliper log (CAL), etc. The lithofaciescorresponding to the instancex
iis denoted byy
i¼½0;…;0;1zﬄﬄﬄﬄﬄ}|ﬄﬄﬄﬄﬄ{k
;0;…;0|ﬄﬄﬄ{zﬄﬄﬄ}
k0/C0k/C138;
Fig. 3.F1-score of each class in six tasks.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
194Fig. 4.Visualization of Well A logs in the task A→B before and after calibration.
Fig. 5.Visualization of Well B logs and the corresponding predicted lithologies in the task A →B before and after calibration.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
195whenx ibelongs to thek-th (1/C20k/C20k 0) lithofacies type of the totalk 0
types.nexamples (x i,yi) along the depth constitute a labelled dataset D¼ fðx
i;yiÞgni¼1from the well.In most cases, the source-domain instances and the target-domainones have different probability distributions. With a logs calibration,the accuracy of lithofacies identiﬁcation might decrease when predictingthe target-domain instances by the model trained on source-domain ex-amples. In this case, the expert-interpreted dataset is denoted as thesource-domain datasetD
s¼f ðxsi;ysiÞgns
i¼1, and the dataset of an unin- terpreted well is called target-domain dataset D
t¼fxtjgnt
j¼1, wheren s,nt
represent the number of the instances in D s,Dt, respectively. Besides, these instances of both domains belong to the same feature space X, which means thatx
s;xt2X. The label spaceYofD sis also identical with D
t's, i.e.,ys;yt2Y.Since the datasets of two domains are sampled from respective jointdistributions, the discrepancies of probability distributions between twodomains can be seen as the discrepancies of two joint distributions i.e.,P
s(xs,ys)6¼P t(xt,yt)(Vapnik, 1999). The joint distributions ofD sandD t
are rewritten asP s(xs,ys)¼P s(xs)Ps(ys|xs),P t(xt,yt)¼P t(xt)Pt(yt|xt),respectively, wherePs(⋅),P t(⋅) are marginal probability distributions, andP
s(⋅|⋅),P t(⋅|⋅) are conditional ones. In general, the conditional probabilitydistributions are shared across domains ( Stojanov et al., 2021). Under the assumption ofP
s(ys|xs)¼P t(yt|xt), the problem turns to tackle theunequality of marginal probability distributions, i.e., P
s(xs)6¼P t(xt). In order to improve the performance in the case of probabilitydiscrepancy, we propose a calibration method to learn a unilateral cali-brating functionfcompellingP
s(f(xs))/C25P t(f(xt)) and consequently P
s(f(xs),ys)/C25P t(f(xt),yt). In this way, the classiﬁer trained onD swould have a good performance in the interpretation task on a target-domainwell.3. Unilateral Alignment for geophysical logs calibrationWeﬁrst give some preliminaries in Sec.3.1 and 3.2, and then yields the Unilateral Alignment (UA) algorithm in Sec. 3.3.3.1. Extreme Learning Machine (ELM)Extreme Learning Machine (ELM) can be used to address classi ﬁca- tion and regression problems, which is proposed by G.-B. Huang ( Huang et al., 2004,2011). Its model training is efﬁcient compared with some traditional learning algorithms, meantime completing regression andmulti-class classiﬁcation tasks effectively. For example, considering aclassiﬁcation task with training datasetfX;Yg¼f ðx
i;yiÞgni¼1, ELM cor- responding to the training dataset with Lhidden layer nodes is described asX
Li¼1βigðw>ixjþb iÞ¼o j;j¼1;…;n;(1)whereg(⋅) is the nonlinear piecewise continuous activation function (e.g.,Sigmoid function),w
i¼½w i1;wi2;⋯;w id/C1382Rdandb iare the randomly- generated weight vector and bias of the i-th hidden layer node,β
iis the output weight of thei-th hidden layer node.o
jis the prediction of thej-th instance.To minimize the predicted error, the error function can be de ﬁned as X
nj¼1koj/C0yjk¼0; (2)wherey
jdenotes thej-th label among thendistances. Combining (1) andTable 3The mean values of each log on the task A→B.
Domain MeanAC CAL COND GR R25 SPOriginal Source 429.50 21.81 43.50 81.41 1.80 90.69Aligned Source 429.07 21.18 43.45 81.32 1.79 90.62Original Target 417.17 22.25 44.31 74.22 1.68 87.84Aligned Target 429.07 21.18 43.45 81.32 1.79 90.62
Table 4The mean values of each log on the task A→C.
Domain MeanAC CAL COND GR R25 SPOriginal Source 429.50 21.81 43.50 81.41 1.80 90.69Aligned Source 429.50 21.81 43.50 81.41 1.80 90.69Original Target 438.92 22.03 44.96 74.37 1.83 96.41Aligned Target 437.15 21.98 43.66 79.69 1.82 95.44
Fig. 6.Frequency distribution histogram of SP log in the task A →B.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
196(2) yieldsHβ¼Y, whereH¼½hðx 1Þ;hðx 2Þ;⋯;hðx nÞ/C138 2Rn/C2Lrepresents the mapped instance matrix, and β2R
L/C2k 0denotes the output weight matrix,hðx
iÞ¼½gðx iw1þb1Þ;gðx iw2þb2Þ;…;gðx iwLþbLÞ/C138 2R1/C2Lis thei- th mapped instance. Hence, the loss function ismin
βkHβ/C0Yk2; (3)whereβcould be calculated by the Moore-Penrose (MP) generalizedinverse, that is,β
*¼HyY; (4)whereH
yis the MP generalized inverse matrix of H. To avoid the over- ﬁtting issue, (3) could be augmented asminβ12kβk 2þC2kHβ/C0Yk 2; (5)whereCis the penalty factor to balance the two terms. Furthermore, wehaveβ
*¼8>>>><>>>>:H>/C18HH
>þIC/C19/C01
Y;n<L/C18H
>HþIC/C19/C01
H>Y;n/C21L (6)3.2. Maximum mean discrepancy (MMD)Maximum Mean Discrepancy (MMD) ( Gretton et al., 2006)i s
Fig. 7.Frequency distribution histogram of SP log in the task A →C.
Fig. 8.Frequency distribution histogram of AC log in the task A →B.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
197generally used to measure the similarity between two probability dis-tributions of the source-domain and the target-domain instances. Thedistance is measured by mapping the instances into a Reproducing KernelHilbert Space (RKHS). Given two distributions PandQ, the formula of the distance based on the maximum mean discrepancy criterion betweenthem isMMD
2ðF;P;QÞ¼sup
f2FkEP½fðxÞ/C138 /C0E Q½fðyÞ/C138k2(7)whereHdenotes an RKHS space,Fdenotes a class of functions. The continuous functionf(⋅) is the mapping function, sup(⋅) is an upper bound function, andE
P(x),E Q(x) are expectations. The empirical estimation ofMMD in an RKHS isMMD2ðX;YÞ¼j j1nX ni¼1φðx iÞ/C01mX mj¼1φðx jÞjj2H(8)whereφ(⋅) is the kernel-induced feature mapping function.3.3. Unilateral Alignment (UA)For the case thatP
s(xs)6¼P t(xt), UA of geophysical logs calibrationaims to adjust the logs of one well to align it with another. To achieve thisgoal, we propose the UA calibration method by combining ELM andMMD.We introduce the random mapping used in ELM to our method, i.e.,the instancex
iis mapped byhðx iÞ2RL, whereLis the dimension of mapped instance. Additionally, we deﬁneH
s2Rns/C2LandH t2Rnt/C2Las
Fig. 9.Frequency distribution histogram of COND log in the task A →B.
Fig. 10.Frequency distribution histogram of GR log in the task A →B.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
198the random mapping matrices of all the instances in datasets D s,Dt, respectively. In this case, MMD could be expressed byMMD
2ðDs;DtÞ¼j j1n
sXns
i¼1hðxsiÞ/C01n
tXnt
j¼1hðxtjÞjj2(9)A transform matrixβoperating to each mapped instance is used toreduce the distribution discrepancy, so we havemin
βjj1n
sXns
i¼1hðxsiÞβ/C01n
tXnt
j¼1hðxtjÞβjj2¼min
βTrðβ>H>ΩHβÞ(10)whereTr(⋅) is the matrix trace, andΩ2R
n/C2ndenotes the marginal PMMD matrix, that is,~Ω
ij¼8>>>>>>>><>>>>>>>>:1n
2s;i;j/C20n s
1n
2t;i;j>n s
/C01n
snt;others(11)Nevertheless, the random mapping function leads to the interaction oflogs, thereby the mapped features lose the physical meanings. In thiscase, the regularization term of source domain maintenance ( Chen et al., 2018) is introduced in the process of calculating β, which could maintain the physical meanings of aligned features. Hence, the process of logscalibration is interpretable through our method. Speci ﬁcally, by intro- ducing the source domain maintenance term, the equation (10)can be written asmin
βλ2Trðβ >H>ΩHβÞþγ2kX
s/C0H sβk2(12)where the second term denotes the regularization term of source domainmaintenance.X
s2Rns/C2Lis the source-domain instance matrix. Thepenalty factorsλandγare used to control the contributions of each term.To prevent the overﬁtting issue, (12) is augmented with a complexitymeasure term as follows:min
β12kβk 2þλ2Trðβ >H>ΩHβÞþγ2kX
s/C0H sβk2:(13)Eventually, we obtain the solution of (13), that is,β*¼8>>>><>>>>:H>s/C18In
γþHsH>sþλγΩHH >/C19/C01
Xs;n<L/C18
IL
γþH>sHsþλγH>ΩH/C19/C01
H>sXs;n/C21L(14)whereβ* is the optimal transform matrix, andI
n2Rn/C2n,IL2RL/C2Lare both identity matrices. Speciﬁcally, the UA calibrating functionfcould be deﬁned asfðx
sÞ¼H sβ¼~X s/C25X sandfðxtÞ¼H tβ¼~X t, where~X s2 R
ns/C2d;~Xt2Rnt/C2dare the calibrated instance matrices corresponding toH
s,Ht, respectively. In this case, this method could compel the proba-bility distributionsP
sð~xsÞ/C25P tð~xtÞ, and consequently achieveP sð~xs;ysÞ/C25 P
tð~xt;ytÞ. Subsequently, we could use the source-domain calibrateddatasetð~X
s;YsÞto train an ELM and test on target-domain calibrateddataset~X
twith high accuracy, whereY s2Rns/C2k0is the label matrix of source-domain instances. The UA framework is illustrated in Fig. 2.4. Experimental analysisIn this section, we conduct a large number of experiments using well-logging data collected from multiple wells in Jiyang depression, BohaiBay Basin to verify the validity and interpretability of our method. First,we describe the dataset and the experimental settings. Second, wecompare the performances of UA, S.O., and T.O., and present the accu-racy of each lithology via the bar graphs. Finally, we show the visualizedresults and discuss the interpretability of our method. Here, S.O., meansthe model trained on source-domain dataset only, which could be seen asthe lower bound of UA. If UA has a lower accuracy than S.O., thennegative transfer happens. T.O. means the model trained on target-domain dataset only, which could be seen as the upper bound of UA.4.1. Experimental settingsThe dataset consists of three wells, which is shown in Table 1. Wells A, B, and C are from the same area and have the same types of logs andlithologies, that is, they have the same input and output space. To verifythe calibration effect, a three-class ELM is trained to classify three typesof lithologies and evaluated by average F1-Score which considers both
Fig. 11.Frequency distribution histogram of R25 log in the task A →B.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
199the accuracy and recall ratios of the classiﬁcation model. F1-Score could be expressed byaverageF1/C0Score¼
F1þF 2þ⋯þF c
c (15)where c is the number of categories and the F
iis F1-Score ofi-th category which can be expressed byF
i¼2/C2Pi/C2R i
PiþR i(16)where theP
irepresents thei-th Precision andR iis the Recall, which can be computed byP
i¼TPTPþFPR
i¼TPTPþFN (17)where the TP, FP and FN represent true positive(an instance are positiveclasses and are judged to be positive classes), false positive(a false class isjudged to be a positive class) and false negative(a false class is judged tobe a negative class), respectively.Due to the wide range of logs, we use min-max normalization toconvert all logging data to those within 0 and 1. The number of hiddenneurons are set 800, the weights and bias are generated randomly by theGaussian distribution with variance equalling 3. The trade-off parameterγis searched from 10
/C03to 104, andλfrom 102to 106. S.O. and T.O. are realized by weighted ELM which could eliminate the in ﬂuence of un- balanced classes. All experiments are implemented by Python 3.8 on adesktop computer with 2.90-GHz CPU and 16.00-GB RAM.4.2. Comparative studyThe performances of these methods are evaluated by different com-binations of wells on the experimental dataset. For example, we set WellA as the source domain for training and Well B as the target domain fortesting (i.e., task A→B). Therefore, there are a total of six combinationsto be compared, i.e., A→B, A→C, B→A, B→C, C→A, and C→B. Table 2lists the macro-average F1-scores of these methods on 6 tasks. Wehave the following observations. (i) In all six tasks, the high F1-score wasachieved by using T.O. method, while the F1-score was signi ﬁcantly decreased by using S.O. method. This indicates that the data distributionvaries from well to well. (ii) The F1-scores of UA performing on all tasksare higher than S.O., which shows the effectiveness of UA in the case ofdata discrepancy. (iii) The F1-scores of UA is very close to those of theT.O. baselines. Especially, it can reach 97.2% on the task A→B. (iv) In the six tasks, the F1-scores of UA are improved by about 10% comparedwith S.O. baseline method, and even improved by 20% on the task B →C. (v) Although the F1-score of S.O. baseline is relatively high, UA still havean improvement on task A→C. The F1-scores for each lithology areshown inFig. 3. It is easy toﬁnd that S.O. usually overﬁts to the mudstone so that its F1-score might be high on some tasks. UA eliminates the dis-tribution discrepancies in the six tasks, so the classi ﬁcation after logs calibration could perform well on each lithology.4.3. Deeper discussionsTaking experiment A→B as an example,Fig. 4andFig. 5show the changes of logs of Well A and Well B. It is obvious that each log in sourcedomain has a slight change as shown in Fig. 4. Most of the macroscopic characteristics are preserved. Some microscopic characteristics areweakened due to low-passﬁltering function of the regularization term ofsource domain maintenance. As shown in Fig. 5, some logs has a great change and the predictions are signiﬁcantly improved by UA Observing areas a and b inFig. 5, we think that UA smoothen R25 by integratingCAL, so the calibrated R25 can be seen as that under the situation withoutborehole expansion. Such a smoothness in R25 avoids many mis-classiﬁcations. In areas of c and d, although the calibrated logs changegreatly, it keeps the basic variation trend of the original logs.The following analysis are based on the statistic information of logs,i.e., the differences of means and distribution histograms between twodomains. Taking A→B and A→C as an examples, it can be seen fromTable 3andTable 4that the mean of each log in the aligned (calibrated)target domain and source domain are almost the same as those in theoriginal source domain. This indicates that the means are alignedunilaterally. Subsequently,Fig. 6presents the distribution histograms ofWell A SP and Well B SP in two domains. It can be seen that the SP logdistribution of target domain is obviously aligned to the one of sourcedomain, while the original SP log distribution of Well A and Well B isquite different. This could be the reason why the F1-score in the task A → B could be increased greatly as shown in Table 2. According toFig. 7,i t can be seen that there is a small difference in SP log distribution betweenWell A and Well C, so S.O. method used to predict Well C can achieve ahigh F1-score of 92.3%. In this case, UA should align to Well A in thehigh-dimensional space while preserving the original information of WellC as much as possible. The frequency distribution histograms of otherlogs are shown inFigs. 8–11which all support the superiority of UA5. Conclusions and further workIn this paper, we formulated the geophysical logs calibration problemand give its statistical explanation, and exhibited an interpretable ma-chine learning method, i.e., UA, which could align the logs from one wellto another without losing the physical meanings. Massive experiments in3 wells and 6 tasks showed the effectiveness and interpretability frommultiple views. The involved UA method is an unsupervised featuredomain adaptation method, so it does not rely on any labels from cores.Actually, the source-domain labels could be used to pre-train a model andoutputs some pseudo-labels for the target domain, such that we canobtain a conditional MMD instead of the marginal MMD used in thepaper. With the conditional MMD, the conditional probability distribu-tion of both domain could be aligned, which might have a better cali-bration effect than the current work.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgementsThis work was supported in part by the National Natural ScienceFoundation of China under Grant 61903353, and in part by the SINOPECProgrammes for Science and Technology Development under GrantPE19008-8.References
Bergen, K.J., Johnson, P.A., de Hoop, M.V., et al., 2019. Machine learning for data-drivendiscovery in solid earth geoscience. Science 363 (6433) . Birnie, C., Ravasi, M., Liu, S., Alkhalifah, T., 2021. The potential of self-supervisednetworks for random noise suppression in seismic data. Artif. Intell. Geosci. 2, 47 –59. Chang, J., Li, J., Kang, Y., Lv, W., Xu, T., Li, Z., Xing Zheng, W., Han, H., Liu, H., 2021a.Unsupervised domain adaptation using maximum mean discrepancy optimization forlithology identiﬁcation. Geophysics 86 (2), ID19–ID30. Chang, J., Kang, Y., Zheng, W.X., Cao, Y., Li, Z., Lv, W., Wang, X.-M., 2021b. Activedomain adaptation with application to intelligent logging lithology identi ﬁcation. IEEE Trans. Cybern.Chang, J., Kang, Y., Li, Z., Zheng, W.X., Lv, W., Feng, D.-Y., 2022. Cross-domain lithologyidentiﬁcation using active learning and source reweighting. IEEE Geoscience andRemote Sensing Letters 19, 1–5. Chen, Y., Song, S., Li, S., Yang, L., Wu, C., 2018. Domain space transfer extreme learningmachine for domain adaptation. IEEE Trans. Cybern. 49 (5), 1909 –1922.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
200Dunham, M.W., Malcolm, A., Kim Welford, J., 2020. Improved well-log classi ﬁcation using semisupervised label propagation and self-training, with comparisons topopular supervised algorithms. Geophysics 85 (1), O1 –O15. Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.Artif. Intell. Geosci. 1, 11–23. Gretton, A., Borgwardt, K., Rasch, M., Sch €olkopf, B., Smola, A., 2006. A kernel method for the two-sample-problem. Adv. Neural Inf. Process. Syst. 19, 513 –520. Huang, G.-B., Zhu, Q.-Y., Siew, C.-K., 2004. Extreme learning machine: a new learningscheme of feedforward neural networks. In: Proceedings of the IEEE InternationalJoint Conference on Neural Networks, vol. 2. Ieee, pp. 985 –990. Huang, G.-B., Zhou, H., Ding, X., Zhang, R., 2011. Extreme learning machine forregression and multiclass classiﬁcation. IEEE Trans. Syst. Man Cybern. B (Cybernetics) 42 (2), 513–529. Imamverdiyev, Y., Sukhostat, L., 2019. Lithological facies classi ﬁcation using deep convolutional neural network. J. Petrol. Sci. Eng. 174, 216 –228. Li, S., Song, S., Huang, G., Wu, C., 2018. Cross-domain extreme learning machines fordomain adaptation. IEEE Trans. Syst. Man Cybern.: Systems 49 (6), 1194 –1207. Li, Z., Kang, Y., Feng, D., Wang, X.-M., Lv, W., Chang, J., Zheng, W.X., 2020. Semi-supervised learning for lithology identi ﬁcation using Laplacian support vector machine. J. Petrol. Sci. Eng. 195, 107510 . Li, Z., Kang, Y., Lv, W., Zheng, W.X., Wang, X.-M., 2021. Interpretable semisupervisedclassiﬁcation method under multiple smoothness assumptions with application tolithology identiﬁcation. Geosci. Rem. Sens. Lett. IEEE 18 (3), 386 –390. Liu, H., Wu, Y., Cao, Y., Lv, W., Han, H., Li, Z., Chang, J., 2020. Well logging basedlithology identiﬁcation model establishment under data drift: a transfer learningmethod. Sensors 20 (13), 3643 . Long, W., Chai, D., Aminzadeh, F., 2016. Pseudo density log generation using arti ﬁcial neural network. In: SPE Western Regional Meeting. OnePetro . Lv, W., Kang, Y., Zhao, Y., 2018. Self-tuning asynchronous ﬁlter for linear Gaussian system and applications. IEEE/CAA J. Automat. Sin. 5 (6), 1054 –1061.Magrini, F., Jozinovi/C19c, D., Cammarano, F., Michelini, A., Boschi, L., 2020. Localearthquakes detection: a benchmark dataset of 3-component seismograms built on aglobal scale. Artif. Intell. Geosci. 1, 1–10. Ruckebusch, G., 1983. A kalmanﬁltering approach to natural gamma ray spectroscopy in well logging. IEEE Trans. Automat. Control 28 (3), 372 –380. Stojanov, P., Li, Z., Gong, M., Ca, R., Carbonell, J., Zhang, K., 2021. Domain adaptationwith invariant representation learning: what transformations to learn? Adv. NeuralInf. Process. Syst. 34.van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9(Nov), 2579
–2605.Vapnik, V., 1999. The Nature of Statistical Learning Theory. Springer science &business media.Wu, Y., Yang, Y., Lv, W., Chang, J., Li, Z., Feng, D., Xu, T., Li, J., 2022. Robust unilateralalignment for subsurface lithofacies classi ﬁcation. IEEE Trans. Geosci. Rem. Sens. 60, 1–13.Xu, T., Chang, J., Kang, Y., Lv, W., Li, J., Han, H., Liu, H., 2022. Intelligent cross-wellsandstone prediction based on convolutional neural network. IEEE Geoscience andRemote Sensing Letters 19, 1–5. Yang, Q., Zhang, Y., Dai, W., Pan, S.J., 2020. Transfer Learning. Cambridge UniversityPress.Zhong, R., Johnson, R.L., Chen, Z., 2020. Using machine learning methods to identify coalpay zones from drilling and logging-while-drilling (lwd) data. SPE J. 25, 1241 –1258, 03.Zhou, R., Cai, Y., Zong, J., Yao, X., Yu, F., Hu, G., 2020. Automatic fault instancesegmentation based on mask propagation neural network. Artif. Intell. Geosci. 1,31–35.Zhu, L., Li, H., Yang, Z., Li, C., Ao, Y., 2018. Intelligent logging lithological interpretationwith convolution neural networks. Petrophysics 59, 799 –810, 06.W. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 192 –201
201