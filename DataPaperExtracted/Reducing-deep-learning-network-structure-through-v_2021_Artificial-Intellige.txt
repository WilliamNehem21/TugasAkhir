Reducing deep learning network structure through variable reductionmethods in crop modeling
Babak Saravia,A .P o u y a nN e j a d h a s h e m ia,b,⁎, Prakash Jhab,c,B oT a n gd
aDepartment of Biosystems and Agricultural Engineering, Michigan State University, East Lansing, MI 48824, USA
bDepartment of Plant, Soil and Microbial Sciences Michigan State University, East Lansing, MI 48824, USA
cFeed the Future Innovation Lab for Collaborative Research on Sustainable Intensi ﬁcation, Kansas State University, Manhattan, KS 66506, USA
dDepartment of Electrical and Computer Engineering, Mississippi State University, MS 39762, USA
abstract article info
Article history:Received 23 March 2021Received in revised form 30 September 2021Accepted 30 September 2021Available online 2 October 2021Crop models are widely used to predict plant growth, water input requirements, and yield. However, existingmodels are very complex and require hundreds of variables to perform accurately. Due to these shortcomings,large-scale applications of crop models are limited. In order to address these limitations, reliable crop modelswere developed using a deep neural network (DNN) –a new approach for predicting crop yields. In addition, the number of required input variables was reduced using three common variable selection techniques: namelyBayesian variable selection, Spearman's rank correlation, and Principal Component Analysis Feature Extraction.The reduced-variable DNN models were capable of estimating future crop yields for 10,000,000 different weatherand irrigation scenarios while maintaining comparable accuracy levels to the original model that used all inputvariables. To establish clear superiority of the methodology, the results were also compared with a very recentfeature selection algorithm called min-redundancy max-relevance (mRMR). The results of this study showedthat the Bayesian variable selection was the best method for achieving the aforementioned goals. Speci ﬁcally, theﬁnal Bayesian-based DNN model with a structure of 10 neurons in 5 layers performed very similarly(78.6% accuracy) to the original DNN crop model with 400 neurons in 10 layers, even though the size of the neu-ral network was reduced by 80-fold. This effort can help promote sustainable agricultural intensi ﬁcations through the large-scale application of crop models.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Deep learningArtiﬁcial intelligentVariable reductionCrop modelingYield predictionIrrigation
1. IntroductionA major challenge in the twenty-ﬁrst century is meeting the needs ofthe fast-growing human population that burgeons demand on food,water, and energy (triple nexuses) (Slavin, 2016). The world's popula- tion has grown exponentially in the past 100 years and is expected toreach 10 billion by 2055 (Kitzes et al., 2008). The Food and Agriculture Organization of the United Nations predicts that this growth will de-mand 50% more food, equating to a 70% increase in food production(FAO, 2017). Given that most of the world's agricultural land is alreadyin production (Bruinsma, 2003), management efﬁciency must improve to match the demand. As a result, new techniques are emerging, whichtake both resource and climate limitations into account ( Gebbers and Adamchuk, 2010).Increases in crop productivity have been largely attributed (50 –60%) to breeding and the development of hybrid cultivars, followed byimproved management practices ( Connor et al., 2011;Sacks and Kucharik, 2011). Technological advancements have helped ﬁne-tune management with increased adaptation of yield simulation ( Ali and Deo, 2020;Ali et al., 2018a),ﬁeld monitoring (Rao and Sridhar, 2018), and other data-driven practices (Pathak et al., 2018). Most recently, pre- cision in resource management has been continuously ﬁne-tuned through crop models and use of satellite navigation systems ( Abbasi et al., 2014;Basso et al., 2001;Lobell and Burke, 2010). Digitized agriculture, or smart farming, has made a signi ﬁcant contribution to improving productivity, ensuring food security, andprotecting the environment (Tyagi, 2016). Wide-scale employment of smart agriculture is necessary for meeting the coming challengesof food security and water deﬁciency; applying inputs in the rightamount, time, and place are the basis of ef ﬁcient crop management systems (Gebbers and Adamchuk, 2010). External inﬂuences can make it difﬁcult to operate at optimal efﬁciency, since many factors such as climate, pests, and disease can adversely in ﬂuence crop man- agement plans. Smart agriculture can help modify management sys-tems to maximize crop yield while minimizing input requirementsgiven current conditions.Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
⁎Corresponding author at: Department of Biosystems and Agricultural Engineering,Michigan State University, East Lansing, MI 48824, USA.E-mail address:pouyan@msu.edu(A.P. Nejadhashemi).
https://doi.org/10.1016/j.aiia.2021.09.0012589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/1.1. Crop modelingThe complexity of agricultural systems demands exploratory re-search to discover and validate system factors' interactions and in ﬂu- ences on each other. At the systems level, biotic and abiotic factorsinteract nonlinearly and are very difﬁcult to study in isolation. Mean- while, crop simulation models can help researchers go around resourceconstraints by mimicking the physiological process in connection withrelevant physical conditions (soil and weather). Crop models help syn-thesize complex systems through a reductionist approach, i.e., reducingthe number of inputs to only include the components that signi ﬁcantly inﬂuence crop growth and development ( De Wit and De Vries, 1983). System behavior and processes can be hypothesized through properanalysis of crop models. Historically, crop models have been used foryield gap analysis, understanding weather impacts, studying crop phe-nology and physiology, and developing management strategies ( Jha et al., 2018). Most of the dynamic crop models produced to date havebeen developed from equations representing growth processes andthe inﬂuence of abiotic factors like soil and weather ( Hoogenboom et al., 2015).Deterministic crop models can be categorized into three maingroups: statistical models (Lobell and Asseng, 2017;Schlenker et al., 2006), mechanistic models (Arnold et al., 2012), and functional models (Arnold et al., 2012;Ballesteros et al., 2016). However, these determin- istic models could not capture spatial and temporal variability in inputs(soil, climate, and other factors). Therefore, input uncertainty could leadto biases in model outputs that must be validated. Biophysical models(De Wit, 1965) and their continuous evolution (Bouman et al., 1996) have led to signiﬁcant advancements in crop biomass/yield estimationwith less risk of bias and uncertainty. Basic crop growth simulationmodels include Elementary CROp growth Simulator-ELCROS ( Bouman et al., 1996), BAsic CROp growth Simulator-BACROS ( De Vries, 1973; De, 1978;Goudriaan, 1977;van Keulen, 1975), Simple and Universal Crop growth Simulator-SUCROS (Spitters et al., 1989), WOrld FOod Studies-WOFOST (Van Diepen et al., 1989), Modules for Annual CRop Simulation-MACROS (Penning de Vries, 1989), rice crop model-ORYZA (Kropff, 1994), and PAPRAN (Seligman and Van Keulen, 1981;Van Keulen, 1982). These basic models were the precursors of the moderncrop simulation models: Agricultural Production Systems sIMulator-APSIM (McCown et al., 1996), Decision Support System forAgrotechnology Transfer-DSSAT (Jones et al., 2003a, 2003b), Cropping Systems simulation model-CropSyst ( Stöckle et al., 2003), InfoCrop (Aggarwal et al., 2006
), and other dynamic models that assess biophys-ical impacts on crops.Crop models play a signiﬁcant role in interpretingﬁeld experiment results, assisting in a timely decision-making process for input manage-ment (Jha et al., 2018) and for climate change's impact on crop yield(Lobell and Asseng, 2017). Biophysical crop models estimate the pro-cesses and factor inﬂuence through parameterized equations ( Wallach et al., 2018). Parameter estimation can be performed through sensitivityanalysis and model calibration (Sehgal et al., 2017) as long as uncer- tainty is accounted for (Ahuja and Ma, 2011;He et al., 2009). For in- stance, many environmental variables, like precipitation, temperature,solar radiation, irrigation, and fertilizer applications, can directly impactcrop growth and yield. Not only the quantity, but also the timing of en-vironmental variables play a signiﬁcant role in determining overall out-comes. The large number of variables and nonlinear system responseslimit the techniques and algorithms that can be used to model biophys-ical systems.1.2. Artiﬁcial intelligence application in crop modelingA systematic approach to analyzing the complex and unpredictablebehavior of agriculture can be used to meet smart farming requirements(Chi et al., 2016;Hashem et al., 2015). Heterogeneous data collection, processing, and analysis produce a vast network requiring real-timedata synthesis and reclassiﬁcation (Kempenaar et al., 2016). The appli- cation of new artiﬁcial intelligence (AI) techniques in agricultural ﬁelds is desirable, given their ability to analyze and use big data. Additionally,these techniques can be used for model development without extensiveknowledge of the speciﬁc area of application (Angermueller et al., 2016; Latha and Mohana, 2016;Menger et al., 2018). One of the most popular AI techniques is Deep Learning (DL). DL is a large structure (multiplelayers) of an Artiﬁcial Neural Network (ANN), which has been aroundsince the seventies. Using DL techniques for modeling has only recentlybecome possible through the advancements made in computer hard-ware technologies, such as high-performance computer clusters(HPCC), multicore computer processing units (CPUs), and powerfulgraphic processing units (GPUs). However, most agriculture-relatedDL experiments conducted thus far have employed Convolutional Neu-ral Network (CNN) architectures, which vary in learning rates and ef ﬁ- ciency (Amara et al., 2017;Prasad et al., 2020). In principle, DL is similar to ANN, with higher performance capabil-ities and more than three layers. Both of these techniques are widelyused (Chen et al., 2014;LeCun et al., 2015;Schmidhuber, 2015). How- ever, a comparative study among these techniques showed the superi-ority of the DL-based model to ANN to estimate biochemical oxygendemand and total phosphorus loads at watershed scales ( Song et al., 2016). In another study byKhaki and Wang (2019), a deep neural net- work (DNN) model was designed for predicting crop yield, and the re-sults were compared against a shallow neural networks (SNN) model.Overall, the predictive model based on DNN outperformed the SNNmodel.DL allows multiple levels of abstraction by hierarchical data process-ing. In agriculture, DL is a new but promising approach with immensepotential, uncovering different areas and dimensions through image vi-sualization and analysis. The most explored areas of DL application inagriculture are classiﬁcation of land use and land cover, crop type, rec-ognition or identiﬁcation of plant type in weed management, countingofﬁnal produce (fruits/vegetables), and recognizing diseases in plants.Algorithms based on DL have even more potential to predict futurefarm parameters, such as soil moisture ( Song et al., 2016) and weather (Sehgal et al., 2017). Variations in available data help to generate furthertraining in DL models, which offers the ability to differentiate betweencharacteristics and achieving greater accuracy in classi ﬁcation (Kamilaris and Prenafeta-Boldú, 2018). However, differentiation based on data is difﬁcult to identify when assessing crop type ( Dyrmann et al., 2017;Ienco et al., 2017;Kussul et al., 2017;Rebetez et al., 2016), crop stages (Chen et al., 2017;Minh et al., 2017;Namin et al., 2018; Yalcin, 2017), and crop conditions (Amara et al., 2017;Prasanna et al., 2016;Rahnemoonfar and Sheppard, 2017;Sladojevic et al., 2016). Com- plexities in layered structures which change gradually with phenologycreate a complex network of data, increasing the size of the dataset(Einheuser et al., 2012). As a result, the computational capability to de-velop and train the model is compromised by the number of variablesand training performance is reduced.In order to address the problems associated with the high number ofinput variables necessary to model environmental and agricultural sys-tems, many techniques have been used such as the Bayesian variable se-lection (O'Hara and Sillanpää, 2009;Woznicki et al., 2015), the Spearman's rank correlation (Einheuser et al., 2012;Einheuser et al., 2013;M a r e te ta l . ,2 0 1 0;Waite et al., 2010), and the Principle Compo- nent Analysis Feature Extraction methods ( Khalid et al., 2014; Pearson, 1901). Other popular machine-learning-based variable reduc-tion techniques have been applied to model portions of the biophysicalsystem: the ant colony optimization ( Dorigo and Di Caro, 1999)w a s used for selecting the best intrinsic mode functions in forecastingmonthly solar radiation (Prasad et al., 2019), the bio-inspired Bat algo-
rithm was used to improve features estimated for forecasting monthlyrainfall (Ali et al., 2018b), the Simulated Annealing algorithm ( FAO, 2017) was used in the development of a drought model ( Ali et al., 2019), the singular value decomposition algorithm ( Bretherton et al.,B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
1971992) was used to develop a model for forecasting weekly solar radia-tion (Prasad et al., 2020), and the non-dominated sorting genetic algo-rithm (Deb et al., 2002) was used in the development of a long-termprecipitation model (Ali et al., 2020).In summary, crop models are widely used to predict plant growth,water input requirements, and yield. However, the existing modelsare very complex and require hundreds of variables to perform accu-rately. Due to these shortcomings, the large-scale applications of cropmodels are limited. Machine learning techniques, such as DL, can beused to address some of these limitations; however, their applicationsare currently limited to qualitative assessments, such as computer vi-sion and speech recognition (Liu et al., 2017). Here, we tried to address the existing problems with crop models by not only developing a DLmodel for predicting yield and water requirements, but also by reducingthe number of input variables from hundreds to only a few. This way,the DL crop models can be used for real-time and large-scale applica-tions, which are not currently possible. In this study, we evaluated thereliability of the DL crop model using about 10 million tested scenarios,which to the best of our knowledge has not been attempted before. Thiseffort will mainstream crop model applications for predicting yield andwater use at the regional and national scale. Crop models can help guidepolicymakers toward achieving sustainable water and food security inthe 21st century.The paper is organized as follows: In section 2, ﬁrst, the input vari- ables (e.g., precipitation) and output variables (e.g., crop yield) for thedevelopment of DL crop models were described. Next, the architecturefor the DL model, along with training and testing procedures, were ex-plained. Due to the high number of the input variables, three differenttechniques were tested for variable reduction ef ﬁciency without compromising the model accuracy. Finally, the best DL models devel-oped by the variable reduction techniques were compared with anewer feature selection method. Under section 3, a DL crop modelwas developed using all input variables (800). This model was used asa reference to examine the performance of variable reduction tech-niques. Systematically, the number of variables was reduced until themodel accuracy was compromised. The smallest structure for crop DLmodels with comparable accuracy to the reference model were thenidentiﬁed and the performance of the variable reduction method wasevaluated against other commonly used methods. Finally, under section4, the results of the analysis were synthesized to identify the best ap-proach for the development of accurate and reliable DL crop models.2. Materials and methods2.1. Overview of methodologyFig. 1presents an overview of this study. First, 100 weather scenarioswere combined with 100,000 random irrigation applications to create10,000,000 scenarios in which crop production could be examinedusing a crop model. Next, a DL model was trained and tested based onthese 10 million scenarios. Eight hundred input variables were intro-duced within the DL model, including four varying environmental vari-ables during a 200-day crop growing season. Three commonly usedvariable reduction techniques were then used to develop additionaldeep learning models based on the reduced number of input parame-ters and smaller structures. These models were tested and comparedwith the original deep learning model to identify the best new modelswith the lowest number of input variables. The impact of variable reduc-tion methods on the performance of DNN models with various architec-tures were discussed at length. In each round of the experiment, thenumber of input variables was reduced (400, 200, 100, 50, 40, 30, 20,and 10 variables), and the model's DNN structure was downsized (com-binations of 600, 400, 200, 100, 50, 40, 30, 20, 10, 8, 6, and 4 neurons by5 0 ,4 0 ,3 0 ,2 0 ,1 0 ,9 ,8 ,7 ,6 ,5 ,4 ,3 ,2 ,a n d1l a y e r s ) .T h ep r o c e s sw a si t e r -ated until a minimum number of architectures with comparable accu-racy to the original model was identiﬁed. Each computed model wasidentiﬁed by three numbers in the form of Inputs-Neurons-Layers. Forexample, model 50–40-6 indicates a DNN with 50 input variables, 40neurons per layer, and 6 layers. Finally, the best DNN model was com-pared against a new DNN model developed using a recent feature selec-tion method.2.2. Model input variablesIn this study, several environmental input variables were consid-ered, including precipitation and irrigation, maximum temperature,minimum temperature, and solar radiation. 800 variables were intro-duced for a 200-day growing season. Irrigation was added in combina-tion with precipitation to generate the total water applied in mm perday. To generate the model's training data, 100 different climate scenar-ios were considered. These climate scenarios were combined with100,000 random irrigation scenarios and applied to a crop model,which generated 10,000,000 records of maize yields. Data related to cli-mate variabilities were produced by weather generators as describedbelow.2.2.1. Generating weather dataProviding daily precipitation, temperature, and solar radiation in ad-
dition to irrigation amounts, is essential for assuring the accuracy ofcrop model outputs. One-yearﬁeld weather data was collected andused to generate different weather realization scenarios. The monthlytemperature, solar radiation, and precipitation were collected from thenearest weather station to the study site ( PRISM, 2011). A weather sto- chastic disaggregation tool was used to generate daily weather informa-tion from the environmental variables' monthly historical records(Hansen and Ines, 2005). This tool generated daily weather informationby disaggregating average monthly data from historical records. To ac-curately simulate weather in the study area, 30 years of data were
Fig. 1.Overview of the procedures for generating reduced deep neural network structuresfor crop yield estimations.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
198used in the weather generator. The weather generator uses stochasticapproaches to generate daily information for the locations of interest.In regards to precipitation, three conditions above the average, the aver-age, and below the average levels of precipitation are considered. Pre-cipitation occurrence is modeled by the Markov chain and the amountis sampled from hyperexponential distribution or probability densityfunction of the random variablex(precipitation occurrence) given byEq.(1).fxðÞ ¼α
exp−xβ
1/C16/C17β
1þ1−αðÞexp−xβ
2/C16/C17β
2ð1Þwhere:αis the mixing probability of hyperexponential distribution, andβ
1,β2are means of theithcomponent of a hyperexponential rainfallintensity distribution.In addition, maximum and minimum temperatures are sampledfrom a Gaussian distribution function conditioned on the occurrenceof precipitation. To generate solar radiation, the weather generatorused thelogitfunction transformations of daily clearness rescaled be-tween upper and lower limits ofﬁeld solar radiation Eq.(2): logit pðÞ ¼ln p=1−pðÞðÞ ð 2Þwhere,pis a probability of daily clearness.To consider statistical behavior of daily sequences, the amount wasshifted to match monthly average values of solar radiation and temper-atures from historical records.2.2.2. Data preparationAfter removing duplicate data from the 10,000,000 generated re-cords, 8,970,685 unique maize production data remained ( Table 1). These data were used for further developing the DL model. Despitethe fact that two computers were used in the study, a 24 core ofIntel® Xeon® CPU E5-2680 v3 @ 2.50GHz with Quadro M6000 GPUand an Intel® Core™i7-4770 CPU @ 3.40GHz with GeForce GTX 1080GPU, the GPU memory did not allow for all available records to beused in training the DNN. To reduce the number of records and maintaindiversity, the data were categorized by maize yield into 12 categories,each 1000 kg apart, then sampled to create 10 similar population distri-bution datasets.Table 1shows the number of data records in eachdataset.2.3. Study areaIrrigation Research Park in Gainesville, Florida, was selected as thestudy area for analyzing the effect of irrigation on maize yield. The Irri-gation Research Park is located at the experimental station of the Uni-versity of Florida (29° 37′8″N, 82° 22′22″W). The humid subtropical climate with abundant rainfall after the growing season makes this loca-tion suitable for quantitative studies of water management ( Lascodyand Melbourne, 2002). Less than 30% of the annual rainfall occursfrom February through May, which is the main growing season(Fig. 2). The 30-year minimum and maximum temperatures of the loca-tion were 14.3 °C and 26.7 °C, respectively. On average, the highest tem-perature ranged from 32.7 °C in July to a low of 19 °C in January.Millhopperﬁne sand was the major soil type found in this region,known to drain moderately well.2.4. Crop modelingThe crop model selected for this study is the Decision Support Sys-tem for Agrotechnology Transfer (DSSAT). This model is designed to dy-namically model over 40 different crops and has been widely used in thepast 30 years by researchers and academic institutes worldwide(Hoogenboom et al., 2015;Jones et al., 2003a, 2003b;Nurudeen, 2011). According to the DSSAT website ( Hoogenboom et al., 2019), the model has been used by more than 14,000 researchers, educators,consultants, extension agents, growers, and policy/decision-makers inover 150 countries. DSSAT's software application package includessoil, weather, crop management tools, and experimental data. DSSATsimulates and models crop growth, development, and yield as a func-tion of the soil, weather, and plant dynamics.To obtain data for the purposes of DNN model training, a maize irri-gation experiment was setup in DSSAT and calibrated based on the re-sults from the experimental study site at the Irrigation Research Park.
The growing season comprises 200 days ( Hoogenboom et al., 2015). The DSSAT inputﬁles were setup using maize cultivar (McCurdy84aa) with planting and harvesting dates of February 16 and May 7, re-spectively. Ten irrigation applications were selected to generate randomscenarios within the growing season to further train the DNN model ir-rigation applications. Irrigations occurred within the growing season,with amounts ranging from 10 mm to 250 mm of water per day.2.5. Deep learning architecture for crop modelingExtensive knowledge of climate, geology, and agricultural manage-ment practices is needed to accurately operate typical crop models. Inaddition, the application of these types of models on a large-scale is lim-ited by model complexity. To address these limitations, this study wasaimed to show the potential for applying DL techniques to crop models.The DL architecture that was used in this study was a DNN with a MultiLayers Perceptron (MLP) architecture ( Fig. 3). MLP is a feed-forward DNN and was selected for this study, since it has been shown to success-fully generate solutions for classiﬁcation problems (Deng and Yu, 2014). In this large-scale analysis, we were interested in estimating the pro-duction class rather than the actual yield due to the high level of uncer-tainty for individualﬁelds. To make the production estimation morereliable, the maize production level was grouped into 12 classes (rangesfrom 0 to 12,000 kg/ha), with each class of input representing a range of1000 kg/ha yield. For example, class 0 represents 0 to 1000 kg/ha, and
Table 1Nonduplicated maize yield datasets for different climatological and irrigation scheduling that were used for deep learning crop model development .Yield class Data set 1 Data set 2 Data set 3 Data set 4 Data set 5 Data set 6 Data set 7 Data set 8 Data set 9 Data set 100–1000 46,872 46,919 47,131 46,985 46,664 46,718 46,680 46,861 46,707 44,9361000–2000 99,136 99,251 99,783 100,049 99,829 99,882 99,719 99,821 100,204 96,5682000–3000 102,770 102,178 102,101 102,235 102,288 102,187 102,320 102,286 101,961 99,0323000–4000 167,257 168,053 167,320 167,595 167,256 167,508 167,786 166,691 167,349 161,9424000–5000 114,608 114,090 114,052 113,762 114,278 114,607 113,753 114,485 114,531 110,3205000–6000 85,792 86,493 86,807 86,388 86,571 86,460 86,144 86,361 86,183 83,3326000–7000 99,090 99,033 99,108 99,439 99,650 98,712 99,751 99,675 99,294 96,5107000–8000 62,626 62,559 61,742 62,081 61,887 62,443 62,292 62,502 62,201 60,0658000–9000 61,518 60,911 61,190 61,082 60,970 60,798 61,037 60,995 61,254 59,2769000–10,000 24,199 24,258 24,296 24,222 24,551 24,302 24,178 24,181 24,305 23,42110,000–11,000 30,280 30,394 30,753 30,377 30,232 30,574 30,539 30,385 30,303 29,62211,000–12,000 5852 5861 5717 5785 5824 5809 5801 5757 5708 5661Total 900,000 900,000 900,000 900,000 900,000 900,000 900,000 900,000 900,000 870,685B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
199class 1 represent 1000 to 2000 kg/ha. Different combinations of hiddenlayers with neurons (computation nodes) within each layer were usedin the DNN to form the crop models. Activation functions of the sametype were assigned for all neurons. The TanH (tangent hyperbolic) acti-vation function was used for all hidden layers and the SoftMax (normal-ization constraint on the total output probability function) activationfunction was used for the output layer ( Costa, 1996). Once the DNN structure was created, training is necessary and was performed asdetailed in the next section.2.6. Training and validation of the modelsAll models were trained using theﬁrst dataset of 900,000 records and 10% of the dataset (90,000 records) were used for validationthroughout the training process. The backpropagation ( L i ue ta l . , 2017) method and the gradient descent (Baldi, 1995) algorithm were also used to train the network by minimizing a de ﬁned cost (Baldi, 1995). In this study, the negative log-likelihood equation Eq. (3)was used as a cost function (Friedman, 2002). The learning rate wasﬁxed at 0.01, and the mini-batch size was 1000 records in all training runs.After reaching the lowest validation error, training was continued for100 epochs to ensure that it was not trapped in the local minimum.Cost¼
1n
bzþXnk¼0log PðY¼ykjcosnπxLþb
nsinnπxL/C16/C17 ð3Þwhere,n
bzis the mini-batch size,nis the number of output classes (12in this case), andPis a function of likelihood probability calculated bySoftmax (normalization constraint on the total output probability func-tion) (Costa, 1996).To compute the required time for each epoch in the DNN structurewithllayers andnneurons per layer, it was assumed the total calcula-tion time of each layer in parallel for the feed-forward and feed-backward process on the GPU isT
lsecond. To feed the next layer, theneuron's output between GPU memory and machine main memorywas transferred inT
nsecond. Considering the training process, Eq. (4) calculated the time consumption for one epoch.T
epoch¼l/C2Tþn/C2l/C2T n ð4ÞEq.(4)demonstrated that training time had a direct relationshipwith the number of layers and the number of neurons per layer. Byreducing the number of layers and the number of neurons per layer,the training process became faster.2.7. Variable reductionTo improve training performance and reduce the computationalpower required for developing the DL models. Among the numerousvariable reduction techniques, some of the most commonly used tech-niques in environmental and agricultural studies were applied(Woznicki et al., 2015). The following three variables reductionmethods were evaluated for preprocessing the data.(1) Bayesian Variable Selection (O'Hara and Sillanpää, 2009)In theory, a Bayesian model explains a response variable (output)with a (large) number of explanatory variables (inputs). The Bayes-ian Variable Selection method selects a small subset of variables thatcan be inferred and used to explain a large fraction of the variationpresent in the response. In many cases, the variable selection isdone by specifying the variables; the variable selection task is to es-timate the marginal posterior probability of whether the variableshould be included in the model or not ( O'Hara and Sillanpää, 2009). Several Bayesian Variable Selection software tools are currentlyavailable, such as BayesFactor, BayesVarSel, and BMS ( Forte et al., 2018); however, none of them is suitable to work with a largedataset. As a result, we identiﬁed the Bayesian Generalized LinearRegression (BGLR) software (Pérez and de los Campos, 2014) that can work with big data. The BGLR is an R-based statistical packagebased on the Gibbs sampler technique with scalar updates to reducethe number of input variables (Casella and George, 1992). (2) Spearman Variable Selection (Zwillinger and Kokoska, 1999)Spearman Rank Correlation method calculates the ranking cor-relation between each variable and the output (Eq. (5)). In this method, it was assumed that the variables with higher cor-relations would have a greater impact on outputs and should beconsideredﬁrst in the model.rs¼SSuvﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃSS
uu/C2SS vvp ð5Þwhere,r
spresents the Spearman Rank Correlation coef ﬁcient,SS uvis the covariance of the input and the output variables, respectively,SS
uuandSS vvare standard deviations of input and output variables.
Fig. 2.The 30-year (1981–2010) average weather conditions at Gainesville, Florida.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
200A python library fromscipy.org(scipy.stats.spearmanr) was used tocalculate the Spearman Rank Correlation coef ﬁcient (Zwillinger and Kokoska, 1999) between inputs and outputs for variable selectionpurposes.(3) Principal Component Analysis Feature Extraction ( Khalid et al., 2014)Principal Component Analysis (PCA) feature extraction is an orthog-onal transformation for converting correlated variables to a smallerset of uncorrelated variables. PCA feature extraction method uses ei-genvalues ofX
TXto calculate linear transformations between thesetwo sets. This method is called feature/variable extraction ( Khalid et al., 2014).In this study, a python library (sklearn.decomposition.PCA) wasused to perform this analysis. This library used the LAPACK imple-mentation of the full Singular Value Decomposition (SVD) or a ran-domized truncated SVD, a method introduced by Halko et al. (2011).2.8. Feature (variable) selection base on max-relevance and min-redundancyMin-redundancy and max-relevance (mRMR) ( Menger et al., 2018) is a novel and popular method that was initially introduced by Peng et al. (2005)and then improved byBugata and Drotar (2020).T h e mRMR algorithm selects a set of explanatory variables with the highestrelevancy and the lowest redundancy level to describe the output vari-able. Consequently, the most dependent variables are identi ﬁed in a large set of variables, which ultimately results in better classi ﬁcation. Through reduced input variable redundancy, a smaller model withequal or better performance can be obtained.Research byBugata and Drotar (2020)showed that the max- redundancy is not always equivalent to max-dependency as wasassumed byPeng et al. (2005). Therefore,Bugata and Drotar (2020) suggested that by adding an objective to the algorithm to maximize de-pendency, the overall performance of the mRMR algorithm can be im-proved. In fact, applying the revised mRMR algorithm resulted in
Fig. 3.Schematic representation of the deep neural network with multi-layer perceptron architecture that was used for crop yield estimations.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
201better feature selection under a real-world scenario ( Bugata and Drotar, 2020).3. Results of discussion3.1. Deep neural network structure analysis3.1.1. Evaluation of the original deep learning model's accuracy with 800input variablesThe original DL model was trained and tested using 800 inputs. Theinputs consisted of four environmental variables varying over a 200-daycrop growing season and all models were trained on the ﬁrst 900,000 recordsets.Fig. 4and Table S1 (Supplementary Materials) present theaccuracy of the original DL models with different structures. The predic-tion accuracy results are shown in the “Training Set”columns and the average accuracy rates from the nine test sets are shown in the “Test Sets”columns. The results showed that in smaller DNN structures (50to 100 neurons per layer), more layers resulted in lower model accu-racy. This behavior was previously observed by Schmidhuber (2015), who noticed decreasing accuracy from exploding/vanishing gradients.The accuracy rates in the DNNs with small structures were signi ﬁcantly lower (30% to 70%) than those with larger structures (400 to 1000 neu-rons per layer) (75% to 80%), which can be seen in Fig. 4. This behavior shows that a DL model based on a large dataset with 800 inputs vari-ables needs a large structure to perform reasonably well.In this study, the accuracy reduced at a lower rate in DNNs withmore than 400 neurons per layer compared to smaller DNNs with an in-creasing number of layers. However, larger structures require morecomputational time and more powerful hardware to perform at an ac-ceptable level. It should be noted that accuracy rates in training setsand test sets were identical, indicating that the models were notoverﬁtted.3.1.2. The number of input variables for deep learning models was reducedto 400Table S2 andFig. 5show the accuracy training and test sets of theDNN model predictions with 400 input variables. Evidently, the resultwith the Bayesian and Spearman Rank Correlation methods in the train-ing set and the average of the nine test sets were almost identical. Thesimilarity indicated that the models were not over ﬁtted. Furthermore, the PCA feature extraction method displayed greater accuracy withthe training set but poor performance with the test sets. As an unsuper-vised feature extraction method, PCA identi ﬁes and extracts the least correlated features from the input dataset ( Calesella et al., 2021). Mean- while, the developed DL models based on this extraction method wereoverﬁtted during the training process.Additionally,Fig. 5shows that the 400 neuron DNN with manylayers had a lower accuracy with the Bayesian Variable Selectionmethod. Similar results were observed with the Spearman Rank Corre-lation, but the Bayesian Variable Selection method showed a decrease inaccuracy when the number of layers increased, while the SpearmanRank Correlation method did not. This indicated that the SpearmanRank Correlation method was less affected by the vanishing/explodinggradients issue. The PCA feature extraction method showed almost nosensitivity to an increasing number of layers. Although it reached a sig-niﬁcant prediction accuracy (more than 97%) with the training set, poorperformance on test sets showed the model was highly over ﬁtted. Bayesian Variable Selection and Spearman Rank Correlation methodshad the same prediction accuracy on both training and test sets ofDNNs with 600 neurons per layer. For the PCA feature extractionmethod, the prediction accuracy on training sets were increased on600 neurons per layer, demonstrating how the method became lessreliable due to overﬁtting with larger DNN structures.3.1.3. The numbers of input variables for deep learning models werereduced to 200,100 and 50 numberTable S3 and Fig. S1 (Supplementary Materials) presented the DNNmodels' accuracy with 200 input variables. Table S4 and Fig. S2 pre-
sented the DNN models' accuracy with 100 input variables. Almostidentical behaviors to 400 input variables were observed with 200 and100 variables. One hundred input variables (Fig. S2) indicated a ﬂat ac- curacy amount for all DNN structures because the number of neuronswas twice that of the number of inputs. As a result, the DNN was moreﬂexible and compensated for vanishing/exploding gradient issues. Theresults from the Bayesian Variable Selection and the Spearman RankCorrelation methods showed comparable accuracy levels for the DNNmodels with similar architectures (79%). Meanwhile, the DNN modelsusing the PCA feature extraction method had the highest accuracy forthe training set (over 98%) and signiﬁcantly lower accuracy for the test sets (less than 39%). Thisﬁnding indicated the model wasoverﬁtted.Fig. S1 shows a decrease in accuracy in relation to an increase in thenumber of layers in the DNN with 200 neurons per layer. However, for400 and 600 neurons per layer, the accuracy rate remained almost con-stant for all three variable reduction methods. It can be observed inFig. S2 that 100 input variables' accuracy rates were constant for allthe different DNN structures except for the PCA feature extractionmethod, which had a comparable accuracy rate to the 400 inputsmodels. This illustrates that improvements in accuracy were establishedby increasing the number of layers in the training set without improvingtest set performance.To study the effect of additional input variable reductions, some newmodels with only 50 input variables were computed. Table S5 and
Fig. 4.Deep neural network original models' accuracy with 800 input variables under different model structures.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
202Fig. S3 show the accuracy validation result for these models on the train-ing sets and test sets with 50 input variables, which turned out to beidentical to the results of 100 input variables. This result shows thatthe three variable reduction methods successfully reduced input vari-ables and can be expected to produce models with small DNN structuresat acceptable prediction accuracy levels. In the next step, the DNN struc-ture was reduced in order toﬁnd out if these methods would work withsmaller DNN structures.3.1.4. Evaluating the performance of deep learning models with limitedlayersTo understand the impact of the number of hidden layers and vari-able reductions on the DNN model's accuracy, the number of layerswas limited to less than 10 (1 to 9) for three different sets of neurons(i.e., 200, 400, and 600) and 200 input variables (Table S6).Fig. S4 indicates that with 200 inputs and the proper number of neu-rons per layer, the accuracy with 1 hidden layer is almost the same asthe large DNN structures with several layers. This shows the robustnessof all three chosen variable reduction methods on the system. In otherwords, a shallow DNN (with less than 10 layers) with a higher numberof neurons per layer (200, 400, and 600) performed as well as a deepDNN (with more than 10 layers) with several hidden layers (10, 20,30, 50 hidden layers). However, similar to previous results, the modelsdeveloped based on the PCA feature extraction method had excellentaccuracy on the training sets and poor performance on the test sets.3.1.5. Evaluate the impact of the number of neurons and layers on deeplearning model accuracyIn this stage, the performance of several models was computed witha small number of neurons per layer. Table S7 and Fig. S5 show the ac-curacy result for the DNN models with 50 input variables. The number
Fig. 5.Deep neural network models' accuracy with 400 input variables (a) Bayesian (b) Spearman (c) PCA.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
203of neurons per layer in these models was 4, 6, 8, and 10, while the num-ber of layers was 1 through 9. In general, the overall accuracy of the DNNmodels with a low number of neurons (e.g., ten neurons in Fig. S5) waslower than the accuracy of the models with more neurons (e.g., 200neurons in Fig. S4). Meanwhile, the DL models' accuracy ﬂuctuated when the number of layers increased for the same number of neurons.However, when the data was rearranged (Fig. S6), the ﬂuctuation pat- terns became more apparent in which the accuracy increased as thenumber of neurons increased for the same number of layers.Meanwhile, DL models based on the Bayesian and Spearman vari-able reductions methods are more consistent in performance betweentesting and training sets, while the DL models developed using thePCA variable reduction method are less robust. For example, in sectionC of Fig. S5 the test set shows a considerable drop in accuracy comparedto the training set.Finally, regardless of the variable reduction methods and the num-ber of layers, the accuracy of the DL model is higher as the number ofneurons increases (Fig. S5).3.2. Identifying the smallest DNN model with good accuracyLarge DNN structures require signiﬁcant time for training their net- work (Sun et al., 2019). Therefore, the goal of this section was to identifythe minimum number of input variables that can be used to develop aDL model with similar accuracy as the original model, with 800 inputvariables. Here we considered four models with 10, 20, 30, and 40 var-iables.Table 2shows models by 10 to 40 inputs and 1 to 9 layers predic-tion accuracy results. The highest accuracy for all three variablereduction methods was obtained using the DNN with 10 neurons perlayer, as highlighted inTable 2. Among these three methods, only theDL model based on the Bayesian variable reduction method was suc-cessful in producing reliable data comparable to the original model,with 800 input variables.The model with 30 inputs, 10 neurons, and 5 layers (50 computa-tional units) created by the Bayesian Variable Selection method hadthe same accuracy as a model with 800 inputs, 400 neurons, and 10layers (Table S1 with 4000 computational units). This suggests thatthe DNN structure developed using the Bayesian Variable Selectionmethod, which was 80 times smaller, with 1/27 the number of inputvariables, could have the same accuracy as the larger model. However,it took three weeks to calculate the posterior probability vector using900,000 simulated crop yield datasets. This hurdle makes the BayesianVariable Selection method less useful for fast applications.The Spearman variable selection method, with more than 200 inputvariables, was acceptable with larger DNN structures since they had thesame accuracy as the Bayesian Variable Selection model with 200 neu-rons per layer. However, the Spearman produced model reduced theprediction accuracy when the number of neurons per layer was re-duced. This method was found to work well with larger input variablesand DNN structures compared to the Bayesian Variable Selectionmethod. In addition, the processing time for variable selection wasmuch faster (~10 min) than the Bayesian Variable Selection method.The PCA model consistently had the best results on the training setand poor performance on test sets with larger DNN structures showedinferior performance on training sets and test sets with small DNNstructures. The accuracy of the smallest model's DNN structure foreach method of variable reduction compared to the original 800 inputvariable models can be seen inTable 3.One of the main beneﬁts of variable reduction is minimizing thestructure of DNN and, as a result, reducing prediction run time. To
Table 2Deep neural network models' architectures and accuracy for the training and test sets.Inputs Neurons Layers Bayesian Spearman PCATraining Set Test Sets Training Set Test Sets Training Set Test Sets10 10 1 53.73% 53.72% 65.82% 65.88% 27.54% 22.41%20 10 1 71.86% 71.83% 69.15% 69.11% 32.31% 18.34%30 10 1 77.84% 77.85% 54.28% 54.32% 35.63% 21.18%40 10 1 78.52% 78.52% 71.02% 70.94% 38.64% 16.81%10 10 2 68.89% 68.86% 60.08% 60.12% 29.61% 23.20%20 10 2 76.88% 76.87% 56.80% 56.82% 35.45% 18.92%30 10 2 78.53% 78.52% 61.18% 61.18% 36.88% 20.64%40 10 2 78.55% 78.54% 69.46% 69.40% 39.54% 17.21%10 10 3 68.95% 68.91% 59.19% 59.28% 31.24% 23.83%20 10 3 76.81% 76.80% 54.05% 54.02% 35.75% 19.60%30 10 3 78.57% 78.56% 54.24% 54.16% 37.54% 21.04%40 10 3 78.57% 78.56% 52.14% 52.08% 40.70% 17.16%10 10 4 69.20% 69.18% 61.28% 61.33% 32.44% 24.79%20 10 4 77.03% 77.01% 56.49% 56.55% 36.28% 19.95%30 10 4 78.58% 78.57% 54.11% 54.15% 38.96% 20.67%40 10 4 78.57% 78.57% 56.78% 56.74% 40.35% 17.77%10 10 5 69.93% 69.91% 48.66% 48.65% 33.21% 25.41%20 10 5 76.94% 76.92% 51.80% 51.76% 37.44% 19.67%30 10 5 78.59% 78.58% 45.41% 45.43% 39.61% 20.99%40 10 5 78.57% 78.57% 50.89% 50.82% 41.34% 17.07%10 10 6 67.81% 67.84% 50.39% 50.41% 33.44% 24.93%20 10 6 77.11% 77.10% 55.79% 55.76% 37.42% 19.32%30 10 6 78.57% 78.56% 41.60% 41.69% 39.83% 21.10%40 10 6 78.56% 78.55% 48.09% 48.08% 41.11% 18.24%10 10 7 67.00% 66.95% 42.95% 42.96% 33.77% 26.10%20 10 7 77.02% 76.98% 47.74% 47.75% 37.56% 19.85%30 10 7 78.57% 78.57% 50.02% 49.96% 39.64% 20.68%40 10 7 78.57% 78.56% 48.63% 48.60% 41.91% 16.88%10 10 8 67.02% 67.03% 45.74% 45.80% 33.98% 26.14%20 10 8 77.03% 77.01% 49.00% 48.96% 38.26% 20.14%30 10 8 78.58% 78.57% 43.69% 43.70% 40.12% 21.12%40 10 8 78.54% 78.54% 51.99% 52.00% 41.38% 17.57%10 10 9 67.27% 67.24% 45.49% 45.61% 34.05% 25.69%20 10 9 76.96% 76.94% 39.34% 39.34% 38.38% 19.46%30 10 9 78.58% 78.57% 37.95% 37.97% 40.09% 21.54%40 10 9 77.70% 77.70% 55.71% 55.70% 42.21% 16.98%B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
204measure this beneﬁt, the original model (800 inputs, 400 neurons, 10layers) and the smallest DNN model (30 inputs, 10 neurons, and 5layers) that show the same level of accuracy have been run on 90,000records on the same CPU platform to measure their execution time. Asa result, the original model with 800 inputs took 166 s to run and thesmall model based on the Bayesian Variable Selection method took0.86 s, which is 193 times faster. A decrease in the overall runtime ofthe DNN crop model can help with the application of these types ofmodels at the large scale that is necessary for policymakers to makean informed decision at the national and international levels.3.3. Comparing by mRMRIn this section, the mRMR method was used to identify a set of vari-ables from the 800 inputs to develop the DNN models comparable withthe most accurate model developed by the Bayesian variable selectionmethod.Fig. 6shows the comparison between these two methods.The DL models computed by mRMR are usually around 10% less accu-rate than the similar models developed based on the Bayesian variableselection method. The lower accuracy established a clear superiority ofthe Bayesian variable selection methodology to the mRMR.Table 3The accuracy of the smallest deep neural network structure for the three variable reduc-tion methods compare to the original model with 800 inputs.DNN structure Maximum accuracyModels Inputs Neurons Layers Training Set Test Sets800 inputs 800 400 10 78.58% 78.54%Bayesian 30 10 5 78.59% 78.58%Spearman 40 10 1 71.02% 70.94%PCA 40 10 9 42.21% 26.14%
Fig. 6.Comparison between min-redundancy max-relevance and Bayesian model accuracy.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
2054. ConclusionA cropping system modeled by DL with many input variables pro-duces a large DNN structure, which is very computationally intensive.Additionally, the training process required for large DNNs can be verytime-consuming. This shortcoming has limited their usage for large-scale applications. However, the utilization of an ef ﬁcient deep learning crop model can be a game-changer. For example, a regional-scale irriga-tion scheduling system can be developed using a hybrid system com-prised of DL-based crop models, a weather forecasting system, and anoptimization algorithm.In this study, we examined the possibility of developing a simplerdeep learning model with comparable accuracy through the applicationof different variable reduction methods (i.e., Bayesian Variable Selec-tion, Spearman Rank Correlation, and PCA variable extraction method).The Bayesian Variable Selection method was identi ﬁed as the most robust method of the three evaluated in this study. However, calculatingthe posterior probability for each variable is very time-consuming. TheSpearman Rank Correlation was ranked the second best with similar ac-curacy to the Bayesian Variable Selection method. The performance ofthese models were also examined against the recently improvedmRMR technique. In general, the models based on the mRMR techniqueare less accurate than the ones based on the Bayesian and the SpearmanRank Correlation techniques. Finally, the DNN models that were devel-oped based on the PCA feature extraction method had the highest accu-racy during the training tests, but the lowest levels during testing.Unfortunately, almost all DNN models developed by this techniquewere overﬁtted.Even though the Bayesian Variable Selection method was selected asthe best for this study; however, future studies are necessary for analyz-ing the robustness of this method and reduce uncertainty by utilizingensembles of crop simulation models under different irrigationschemes, climatological conditions, and crop management strategies.Ultimately, the results of this work can then be compared to determinethe best selection method for a wide variety of crops/regions.Declaration of Competing InterestThe authors declare that they have no con ﬂict of interest.AcknowledgmentsThe authors would like to thank Ms. Anna Raschke for her reviewand constructive feedback. This work was supported by the USDA Na-tional Institute of Food and Agriculture, Hatch project 1019654.Appendix A. Supplementary dataSupplementary data to this article can be found online at https://doi. org/10.1016/j.aiia.2021.09.001.References
Abbasi, A.Z., Islam, N., Shaikh, Z.A., 2014. A review of wireless sensors and networks ’ap- plications in agriculture. Comput. Stand. Inter. 36 (2), 263 –270. Aggarwal, P.K., Kalra, N., Chander, S., Pathak, H., 2006. Infocrop: a dynamic simulation model for the assessment of crop yields, losses due to pests, and environmental im-pact of agro-ecosystems in tropical environments. i. Model description. Agric. Syst.89 (1), 1–25.Ahuja, L., Ma, L., 2011.A synthesis of current parameterization approaches and needs forfurther improvements. Methods of Introducing System Models into AgriculturalResearch. 2, pp. 427–440.Ali, M., Deo, R.C., 2020.Modeling wheat yield with data-intelligent algorithms: arti ﬁcial neural network versus genetic programming and minimax probability machineregression. Handbook of Probabilistic Models. Butterworth-Heinemann, pp. 37 –87. Ali, M., Deo, R.C., Downs, N.J., Maraseni, T., 2018a. Cotton yield prediction with Markov Chain Monte Carlo-based simulation model integrated with genetic programing algo-rithm: a new hybrid copula-driven approach. Agric. For. Meteorol. 263, 428 –448.Ali, M., Deo, R.C., Downs, N.J., Maraseni, T., 2018b. Multi-stage hybridized online sequen- tial extreme learning machine integrated with Markov Chain Monte Carlo copula-Batalgorithm for rainfall forecasting. Atmos. Res. 213, 450 –464. Ali, M., Deo, R.C., Maraseni, T., Downs, N.J., 2019. Improving SPI-derived drought forecasts incorporating synoptic-scale climate indices in multi-phase multivariate empiricalmode decomposition model hybridized with simulated annealing and kernel ridgeregression algorithms. J. Hydrol. 576, 164 –184. Ali, M., Deo, R.C., Xiang, Y., Li, Y., Yaseen, Z.M., 2020. Forecasting long-term precipitation for water resource management: a new multi-step data-intelligent modellingapproach. Hydrol. Sci. J. 65 (16), 2693 –2708. Amara, J., Bouaziz, B., Algergawy, A., 2017. A deep learning-based approach for banana leaf diseases classiﬁcation. Datenbanksysteme für Business, Technologie und Web(BTW 2017)-Workshopband, pp. 79 –88. Angermueller, C., Pärnamaa, T., Parts, L., Stegle, O., 2016. Deep learning for computational biology. Mol. Syst. Biol. 12 (7), 878.Arnold, J.G., Moriasi, D.N., Gassman, P.W., Abbaspour, K.C., White, M.J., Srinivasan, R.,Santhi, C., Harmel, R., Van Griensven, A., Van Liew, M.W., Kannan, N., 2012. SWAT: model use, calibration, and validation. T. ASABE 55 (4), 1491 –1508. Baldi, P., 1995.Gradient descent learning algorithm overview: a general dynamical sys-tems perspective. IEEE Trans. Neural Netw. 6 (1), 182 –195. Ballesteros, R., Ortega, J.F., Moreno, M.Á., 2016. FORETo: new software for reference evapotranspiration forecasting. J. Arid Environ. 124, 128 –141. Basso, B., Ritchie, J., Pierce, F., Braga, R., Jones, J., 2001. Spatial validation of crop models for precision agriculture. Agric. Syst. 68 (2), 97 –112. Bouman, B., Van Keulen, H., Van Laar, H., Rabbinge, R., 1996. The‘school of de wit’crop growth simulation models: a pedigree and historical overview. Agric. Syst. 52(2–3), 171–198.Bretherton, C.S., Smith, C., Wallace, J.M., 1992. An intercomparison of methods for ﬁnding coupled patterns in climate data. J. Clim. 5 (6), 541e560.World agriculture: towards 2015/2030: an FAO perspective. In: Bruinsma, J. (Ed.),Earthscan.http://www.fao.org/3/y4252e/y4252e00.htm (Accessed 12 January 2021). Bugata, P., Drotar, P., 2020.On some aspects of minimum redundancy maximum rele-vance feature selection. Sci. China Inf. Sci. 63 (1), 1 –15. Calesella, F., Testolin, A., De Grazia, M.D.F., Zorzi, M., 2021. A comparison of feature extrac- tion methods for prediction of neuropsychological scores from functional connectiv-ity data of stroke patients. Brain Inform. 8 (1), 1 –13. Casella, G., George, E.I., 1992.Explaining the GIBBS sampler. Am. Stat. 46 (3), 167 –174. Chen, Y., Lin, Z., Zhao, X., Wang, G., Gu, Y., 2014. Deep learning-based classiﬁcation of hyperspectral data. IEEE J. Sel. Top. Appl. Earth Obs. Remote Sens. 7 (6), 2094 –2107. Chen, S.W., Shivakumar, S.S., Dcunha, S., Das, J., Okon, E., Qu, C., Taylor, C.J., Kumar, V.,2017.Counting apples and oranges with deep learning: a data-driven approach.IEEE Robot Autom. Lett. 2 (2), 781 –788. Chi, M., Plaza, A., Benediktsson, J.A., Sun, Z., Shen, J., Zhu, Y., 2016. Big data for remote sensing: challenges and opportunities. Proc. IEEE 104 (11), 2207 –2219. Connor, D.J., Loomis, R.S., Cassman, K.G., 2011. Crop Ecology: Productivity and Manage- ment in Agricultural Systems. Cambridge University Press.Costa, M., 1996.Probabilistic interpretation of feedforward network outputs, with rela-tionships to statistical prediction of ordinal quantities. Int. J. Neural Syst. 7 (5),627–637.De, W., 1978.Simulation of assimilation, respiration and transpiration of crops. Simula-tion Monographs. PUDOC, Wageningen.De Vries, F.P., 1973.Substrate Utilization and Respiration in Relation to Growth and Main-tenance in Higher Plants. Ph.D. dissertationWageningen University.De Wit, C., 1965.Photosynthesis of leaf canopies. Agricultural Research Report no 663.Center for Agricultural Publication and Documentation, Wagenin-gen, TheNetherlands.De Wit, C., De Vries, F.P., 1983. Crop growth models without hormones. Neth. J. Agri. Sci.31, 313–323.Deb, K., Pratap, A., Agarwal, S., Meyarivan, T.A., 2002. A fast and elitist multiobjective genetic algorithm: NSGA-II. IEEE Trans. Evol. Comput. 6 (2), 182 –197. Deng, L., Yu, D., 2014.Deep learning: methods and applications. Found. Trend SignalProcess. 7 (3–4), 197–387.Dorigo, M., Di Caro, G., 1999.Ant colony optimization: a new meta-heuristic. Proceedingsof the 1999 Congress on Evolutionary Computation-CEC99 (Cat. No. 99TH8406). Vol.2. IEEE, pp. 1470–1477.Dyrmann, M., Jørgensen, R.N., Midtiby, H.S., 2017. RoboWeedSupport - detection of weed locations in leaf occluded cereal crops using a fully convolutional neural network.Adv. Anim. Biosci. 8 (2), 842 –847. Einheuser, M.D., Nejadhashemi, A.P., Sowa, S.P., Wang, L., Hamaamin, Y.A., Woznicki, S.A.,2012.Modeling the effects of conservation practices on stream health. Sci. TotalEnviron. 435, 380–391.Einheuser, M.D., Nejadhashemi, A.P., Wang, L., Sowa, S.P., Woznicki, S.A., 2013. Linking biological integrity and watershed models to assess the impacts of historical landuse and climate changes on stream health. Environ. Manag. 51 (6), 1147 –1163. FAO, Food and Agriculture Organization of the United Nations, 2017. The State of Food and Agriculture: Leveraging Food Systems for Inclusive Rural Transformation. Foodand Agriculture Organization of the United Nations.Forte, A., Garcia-Donato, G., Steel, M., 2018.
Methods and tools for bayesian variable selec- tion and model averaging in normal linear regression. Int. Stat. Rev. 86 (2), 237 –258. Friedman, J.H., 2002.Stochastic gradient boosting. Comput. Stat. Data Anal. 38 (4),367–378.Gebbers, R., Adamchuk, V.I., 2010. Precision agriculture and food security. Science 327 (5967), 828–831 12.Goudriaan, J., 1977.Crop Micrometeorology: A Simulation Study. DoctoraldissertationPudoc. Wageningen University.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
206Halko, N., Martinsson, P.G., Tropp, J.A., 2011. Finding structure with randomness: proba- bilistic algorithms for constructing approximate matrix decompositions. SIAM Rev.53 (2), 217–288.Hansen, J.W., Ines, A.V., 2005. Stochastic disaggregation of monthly rainfall data for cropsimulation studies. Agric. For. Meteorol. 131 (3 –4), 233–246. Hashem, I.A.T., Yaqoob, I., Anuar, N.B., Mokhtar, S., Gani, A., Khan, S.U., 2015. The rise of “big data”on cloud computing: review and open research issues. Inf. Syst. 47, 98 –115. He, J., Dukes, M., Jones, J., Graham, W., Judge, J., 2009. Applying glue for estimating ceres- maize genetic and soil parameters for sweet corn production. T. ASABE 52 (6),1907–1921.Hoogenboom, G., Jones, J., Wilkens, P., Porter, C., Boote, K., Hunt, L., Singh, U., Lizaso, J.,White, J., Uryasev, O., 2015.Decision Support System for Agrotechnology Transfer(Dssat) Version 4.6. Dssat Foundation, Prosser, Washington.Hoogenboom, G., Porter, C.H., Boote, K.J., Shelia, V., Wilkens, P.W., Singh, U., White, J.W.,Asseng, S., Lizaso, J.I., Moreno, L.P., Pavan, W., 2019. The DSSAT crop modeling ecosys- tem. Adv. Crop. Model. Sustain. Agri. 173 –216. Ienco, D., Gaetano, R., Dupaquier, C., Maurel, P., 2017. Land cover classiﬁcation via multitemporal spatial data by deep recurrent neural networks. IEEE Geosci. RemoteSens. Lett. 14 (10), 1685 –1689. Jha, P.K., Kumar, S.N., Ines, A.V., 2018. Responses of soybean to water stress and supple- mental irrigation in upper indo-gangetic plain: ﬁeld experiment and modeling approach. Field Crops Res. 219, 76 –86. Jones, J.W., Hoogenboom, G., Porter, C.H., Boote, K.J., Batchelor, W.D., Hunt, L., Wilkens,P.W., Singh, U., Gijsman, A.J., Ritchie, J.T., 2003a. The DSSAT cropping system model. Eur. J. Agron. 18 (3–4), 235–265. Jones, J.W., Hoogenboom, G., Porter, C.H., Boote, K.J., Batchelor, W.D., Hunt, L.A., Wilkens,P.W., Singh, U., Gijsman, A.J., Ritchie, J.T., 2003b. The DSSAT cropping system model. Eur. J. Agron. 18 (3–4), 235–265. Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: a survey. Comput. Electron. Agric. 147, 70 –90.Kempenaar, C., Lokhorst, C., Bleumer, E., Veerkamp, R., Been, T., van Evert, F., Boogaardt,M., Ge, L., Wolfert, J., Verdouw, C., 2016. Big Data Analysis for Smart Farming: Results of Two Project in Theme Food Security. Tech. Rep.Wageningen University & Research.Khaki, S., Wang, L., 2019.Crop yield prediction using deep neural networks. Front. PlantSci. 10, 621.Khalid, S., Khalil, T., Nasreen, S., 2014. A survey of feature selection and feature extraction techniques in machine learning. Proceedings of 2014 Science and Information Con-ference. 2014. SAI, pp. 372 –378. Kitzes, J., Wackernagel, M., Loh, J., Peller, A., Gold ﬁnger, S., Cheng, D., Tea, K., 2008. Shrink and share: humanity ’s present and future ecological footprint. Philos. Trans. R. Soc.Lond. Ser. B Biol. Sci. 363, 467 –475. Kropff, M.J., 1994.Oryza1-an ecophysiological model for irrigated rice production. SARPResearch Proceedings. DLO-Research Institute for Agrobiology and Soil Fertility.Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., 2017. Deep learning classi
ﬁcation of land cover and crop types using remote sensing data. IEEE Geosci. Remote Sens.Lett. 14 (5), 778–782.Lascody, R., Melbourne, N., 2002. The Onset of the Wet and Dry Seasons in East CentralFlorida, a Subtropical Wet-dry Climate. National Weather Service Weather ForecastOfﬁce Melbourne, FL.Latha, C.P., Mohana, P., 2016.A review on deep learning algorithms for speech and facialemotion recognition. Aptikom 1 (3), 88 –104. L e C u n ,Y . ,B e n g i o ,Y . ,H i n t o n ,G . ,2 0 1 5 . Deep learning. Nature 521 (7553), 436 –444. Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., Alsaadi, F.E., 2017. A survey of deep neural net- work architectures and their applications. Neurocomputing 234, 11 –26. Lobell, D.B., Asseng, S., 2017. Comparing estimates of climate change impacts fromprocess-based and statistical crop models. Environ. Res. Lett. 12 (1), 015001.Lobell, D.B., Burke, M.B., 2010. On the use of statistical models to predict crop yieldresponses to climate change. Agric. For. Meteorol. 150 (11), 1443 –1452. Maret, T., Konrad, C., Tranmer, A., 2010. Inﬂuence of environmental factors on biotic responses to nutrient enrichment in agricultural streams. J. Am. Water Resour.Assoc. 46 (3), 498–513.McCown, R.L., Hammer, G.L., Hargreaves, J.N.G., Holzworth, D.P., Freebairn, D.M., 1996.APSIM: a novel software system for model development, model testing and simula-tion in agricultural systems research. Agric. Syst. 50 (3), 255 –271. Menger, V., Scheepers, F., Spruit, M., 2018. Comparing deep learning and classical machine learning approaches for predicting inpatient violence incidents from clinical text.Appl. Sci. 8 (6), 981.Minh, D.H.T., Ienco, D., Gaetano, R., Lalande, N., Ndikumana, E., Osman, F., Maurel, P., 2017.Deep Recurrent Neural Networks for Mapping Winter Vegetation Quality CoverageVia Multi-temporal Sar Sentinel-1 arXiv preprint arXiv,1708.03694.Namin, S.T., Esmaeilzadeh, M., Naja ﬁ, M., Brown, T.B., Borevitz, J.O., 2018. Deep phenotyp- ing: deep learning for temporal phenotype/genotype classi ﬁcation. Plant Meth. 14 (1), 66.Nurudeen, A.R., 2011.Decision Support System for Agro-technology Transfer (DSSAT)Model Simulation of Maize Growth and Yield Response to NPK Fertilizer Applicationon a Benchmark Soil of Sudan Savanna Agro-ecological Zone of Ghana (Doctoral Dis-sertation). Kwame Nkrumah University of Science and Technology Kumasi.O'Hara, R.B., Sillanpää, M.J., 2009. A review of Bayesian variable selection methods: what,how and which. Bayesian Anal. 4 (1), 85 –117. Pathak, R., Barzin, R., Bora, G.C., 2018. Data-driven precision agricultural applications usingﬁeld sensors and Unmanned Aerial Vehicle. IJPAA 1 (1).Pearson, K., 1901.On lines and planes of closestﬁt to systems of points in space. Lond. Edinb. Dublin. Philos. Mag. J. Sci. 2 (11), 559 –572. Peng, H., Long, F., Ding, C., 2005. Feature selection based on mutual information criteria ofmaxdependency, max-relevance, and min-redundancy. IEEE Trans. Pattern Anal.Mach. Intell. 27 (8), 1226 –1238. Penning de Vries, F.W.T., 1989. Simulation of ecophysiological processes of growth in sev-eral annual crops. Int. Rice Res. Inst. 29.Pérez, P., de los Campos, G., 2014. BGLR: a statistical package for whole genome regres- sion and prediction. Genetics 198 (2), 483 –495. Prasad, R., Ali, M., Kwan, P., Khan, H., 2019. Designing a multi-stage multivariate empirical mode decomposition coupled with ant colony optimization and random forest modelto forecast monthly solar radiation. Appl. Energy 236, 778 –792. Prasad, R., Ali, M., Xiang, Y., Khan, H., 2020. A double decomposition-based modelling approach to forecast weekly solar radiation. Renew. Energy 152, 9 –22. Prasanna, Mohanty, S., Hughes, D., Salathe, M., 2016. Using Deep Learning for Image- based Plant Disease Detection (arXiv e-prints).PRISM, 2011. Prism Climate Data. http://prism.oregonstate.edu/ (Accessed 12 January 2021).Rahnemoonfar, M., Sheppard, C., 2017. Deep count: fruit counting based on deep simu- lated learning. Sensors 17 (4), 905.Rao, R.N., Sridhar, B., 2018.IoT based smart crop-ﬁeld monitoring and automation irriga- tion system. 2018 2nd International Conference on Inventive Systems and Control(ICISC). IEEE, pp. 478 –483.Rebetez, J., Satizábal, H.F., Mota, M., Noll, D., Büchi, L., Wendling, M., Cannelle, B., Pérez-Uribe, A., Burgos, S., 2016.Augmenting a Convolutional Neural Network With LocalHistograms-A Case Study in Crop Classi ﬁcation From High-resolution UAV Imagery. InESANN.Sacks, W.J., Kucharik, C.J., 2011. Crop management and phenology trends in the us cornbelt: impacts on yields, evapotranspiration and energy balance. Agric. For. Meteorol.151 (7), 882–894.Schlenker, W., Hanemann, W.M., Fisher, A.C., 2006. The impact of global warming on us agriculture: an econometric analysis of optimal growing conditions. Rev. Econ. Stat.88 (1), 113–125.Schmidhuber, J., 2015.Deep learning in neural networks: an overview. Neural Netw. 61,85–117.Sehgal, G., Gupta, B., Paneri, K., Singh, K., Sharma, G., Shroff, G., 2017. Crop planning using stochastic visual optimization. 2017 IEEE Visualization in Data Science (VDS). IEEE,pp. 47–51.Seligman, N., Van Keulen, H., 1981. Papran: a simulation model of annual pasture produc- tion limited by rainfall and nitrogen, simulation of nitrogen behavior of soil-plant sys-tems: papers of a workshop. Models for the Behavior of Nitrogen in Soil and Uptakeby Plant, Comparison Between Different Approaches, Wageningen, the Netherlands,January 28–February 1, 1980. Centre for Agricultural Publishing and Documentation,Wageningen, Netherlands, p. 1981.Sladojevic, S., Arsenovic, M., Anderla, A., Culibrk, D., Stefanovic, D., 2016. Deep neural net- works based recognition of plant diseases by leaf image classi ﬁcation. Comput. Intell. Neurosci. 1–11 3289801.Slavin, P., 2016.Climate and famines: a historical reassessment. Wiley Interdiscip. Rev. 7(3), 433–447.Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., Yang, J., 2016. Modeling spatio-temporal distri- bution of soil moisture by deep learning-based cellular automata model. J. Arid Land.8( 5 ) ,7 3 4–748.Spitters, C., Van Keulen, H., Van Kraalingen, D., 1989. A simple and universal crop growth simulator: Sucros87. Simulation and Systems Management in Crop Protection. Pudoc,pp. 147–181.Stöckle, C.O., Donatelli, M., Nelson, R., 2003. Cropsyst, a cropping systems simulation model. Eur. J. Agron. 18 (3 –4), 289–307. Sun, P., Wen, Y., Han, R., Feng, W., Yan, S., 2019. Gradientﬂow: Optimizing Network Per- formance for Large-Scale Distributed DNN Training. IEEE Trans. Big Data.Tyagi, A.C., 2016.Towards a second green revolution. Irrig. Drain. 65 (4), 388
–389. Van Diepen, C.V., Wolf, J., Van Keulen, H., Rappoldt, C., 1989. Wofost: a simulation model of crop production. Soil Use Manag. 5 (1), 16 –24. van Keulen, H., 1975.Simulation of Water Use and Herbage Growth in Arid Regions.Pudoc, Wageningen University.Van Keulen, H., 1982.Crop production under semi-arid conditions, as determined by ni-trogen and moisture availability. Simulation of Plant Growth and Crop Production.Pudoc, pp. 234–249.Waite, I., Brown, L., Kennen, J., May, J., Cuffney, T., Orlando, J., Jones, K., 2010. Comparison of watershed disturbance predictive models for stream benthic macroinvertebratesfor three distinct ecoregions in western US. Ecol. Indic. 10 (6), 1125 –1136. Wallach, D., Makowski, D., Jones, J.W., Brun, F., 2018. Working with Dynamic Crop Models: Methods, Tools and Examples for Agriculture and Environment. AcademicPress.Woznicki, S.A., Nejadhashemi, A.P., Ross, D.M., Zhang, Z., Wang, L., Esfahanian, A.H., 2015.Ecohydrological model parameter selection for stream health evaluation. Sci. TotalEnviron. 511, 341–353.Yalcin, H., 2017.Plant phenology recognition using deep learning: deep-pheno. 6th Inter-national Conference on Agro-geoinformatics. IEEE, pp. 1 –5. Zwillinger, D., Kokoska, S., 1999. CRC Standard Probability and Statistics Tables and For-mulae. Crc Press.B. Saravi, A.P. Nejadhashemi, P. Jha et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 196 –207
207