Few-shot learning for biotic stress classiﬁcation of coffee leaves
Lucas M. Tassisb, Renato A. Krohlinga,b,⁎
aLABCIN - Nature Inspired Computing Lab, Federal University of Espirito Santo, Av. Fernando Ferrari, 514, CEP, 29075-910 Vitória, Esp ırito Santo, ES, Brazil
bPPGI - Graduate Program in Computer Science, Federal University of Espirito Santo, Av. Fernando Ferrari, 514, CEP 29075-910, Vitória, Espírito Sant o, ES, Brazil
abstract article info
Article history:Received 17 August 2021Received in revised form 31 December 2021Accepted 1 April 2022Available online 9 April 2022In the last few years, deep neural networks have achieved promising results in several ﬁelds. However, one of the main limitations of these methods is the need for large-scale datasets to properly generalize. Few-shot learningmethods emerged as an attempt to solve this shortcoming. Among the few-shot learning methods, there is a classof methods known as embedding learning or metric learning. These methods tackle the classi ﬁcation problem by learning to compare, needing fewer training data. One of the main problems in plant diseases and pests recogni-tion is the lack of large public datasets available. Due to this dif ﬁculty, theﬁeld emerges as an intriguing applica- tion to evaluate the few-shot learning methods. The ﬁeld is also relevant due to the social and economic importance of agriculture in several countries. In this work, datasets consisting of biotic stresses in coffee leavesare used as a case study to evaluate the performance of few-shot learning in classi ﬁcation and severity estimation tasks. We achieved competitive results compared with the ones reported in the literature in the classi ﬁcation task, with accuracy values close to 96%. Furthermore, we achieved superior results in the severity estimationtask, obtaining 6.74% greater accuracy than the baseline.© 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Plant diseases and pests classiﬁcation Image classiﬁcationFew-shot learningMeta-learningConvolutional neural networks
1. IntroductionPlants are often exposed to environmental stresses that can cause avast impact on growth and production. Plant stresses can be biotic andabiotic. Biotic stresses are caused by pathogens such as fungi, bacteria,viruses. They can also be caused by pests. On the other hand, abioticstresses are caused by heat, drought, cold, salinity, among others(Suzuki et al., 2014).Coffee is one of the main crops in Brazil, and particularly, in the stateof Espírito Santo. One of the main challenges faced by farmers is thepests and diseases that affect the coffee crops. They can limit the yieldfor both small and big farmers, causing losses that may impair thecrop harvest. Each stress requires precise management, and the use ofthe wrong technique may cause damage to the environment and theworker's health. They can also decrease the quality of the ﬁnal product (Ventura et al., 2017).Among the biotic stresses that affect coffee leaves, four of the mainones are leaf miner, rust, brown leaf spot, and cercospora leaf spot.Fig. 1illustrates symptoms caused by the stresses.In addition to classifying the stress that affects the leaves, it is alsoimportant to estimate its severity. The severity is measured by thepercentage of leaf area that is injured. Precise severity estimation can as-sist in predicting yield loss, monitoring and forecasting epidemics, andcorrect management decisions (Bock et al., 2010). Classifying plant biotic stresses and estimating their severity is acomplex task due to several factors: lack of precise delimitation of thelesion area; variation of characteristics between lesions of the sametype (such as color, shape, and size); the presence of multiples lesionson the same leaf; and the fact that different stresses may have similarsymptoms (Barbedo, 2016). In the last few years, deep learning tech-niques have been presenting promising results in image classi ﬁcation and recognition (Goodfellow et al., 2016). These techniques have even been used successfully in the classiﬁcation of plant diseases (Boulent et al., 2019), emerging as a useful tool to assist farmers and agricultureprofessionals.However, traditional machine learning and deep learning tech-niques present some limitations, among them is the need for large-scale datasets to the models generalize well ( Wang et al., 2020). So, new methods were proposed aiming to create models that generalizedwell with fewer data. This led to the proposal of meta-learning methods (Hospedales et al., 2020), and in particular, thefew-shot learning methods.The limited number of datasets is one of the main challenges thatinﬂuence the use of deep learning techniques for diseases and pestsrecognition in plants (Barbedo, 2018). This leads to weak models since the datasets do not have enough examples for deep neuralArtiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
⁎Corresponding author at: LABCIN - Nature Inspired Computing Lab, Federal Universityof Espirito Santo, Av. Fernando Ferrari, 514, CEP, 29075-910 Vitória, Esp ırito Santo, ES, Brazil.E-mail address:rkrohling@inf.ufes.br(R.A. Krohling).
https://doi.org/10.1016/j.aiia.2022.04.0012589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/networks to properly generalize. Thereby, the investigation of few-shot learning methods for classifying stresses on plants becomes rele-vant.As pointed out, there are several works in the literature that applieddeep neural networks for plant disease recognition. Most of them makeuse of traditional convolutional neural networks in different crops,obtaining accuracy values over 90% ( Mohanty et al., 2016;Rahman et al., 2018;Elhassouny and Smarandache, 2019;Geetharamani and Pandian, 2019;Barbedo, 2019).Some works also explored the use of deep neural networks speci ﬁ- cally in coffee crops.Manso et al. (2019)developed a method combining traditional segmentation algorithms with neural networks to classifyspots in images of coffee leaves. The algorithm also estimated the sever-ity of the lesions using the segmented spot area. Esgario et al. (2020) used deep learning to classify and estimate the severity of lesions in cof-fee leaves and symptoms images. In their work, they also evaluated theuse of a multi-task network for both classi ﬁcation and severity estima- tion, comparing with a network trained for a single task. The workused the same datasets used in this work, serving as a baseline for the re-sults obtained using the few-shot learning method. Esgario et al. (2022) developed a similar method, but instead of using traditional segmenta-tion methods, they used deep neural networks in the segmentationtask. In addition to the algorithm, an app for smartphones was devel-oped to assist farmers in the identiﬁcation of diseases and pests.Tassis et al. (2021)proposed a method combining instance and semantic seg-mentation to identify pests and diseases from in- ﬁeld coffee trees im- ages. Their approach consisted of a three-stage method that combinedinstance segmentation, semantic segmentation, and classi ﬁcation to classify and estimate the severity of the lesions affecting the coffee trees.Recently, some works started exploring the use of few-shot learningin plant disease recognition.Wang and Wang (2019)explored the sia- mese network architecture to classiﬁy diseases of three different typesof leaves. They obtained accuracy over 90% while using only 20 samplesper class in the training set.Argüeso et al. (2020)evaluated siamese net- works with triplet loss in the PlantVillage dataset also reaching accuracyvalues over 90%. Theﬁrst step was training a convolutional neural net-work (CNN) for general plant features extraction. Next, they evaluatedtheﬁne-tuning for new unseen classes, obtaining similar performancewhile training on a dataset reduced by almost 90%. Jadon (2020)pro- posed the SSM (Stacked Siamese Matching) network, combining twofew-shot learning methods: Siamese Networks and Matching Net-works. The method obtained over 92% accuracy in two plant datasets,outperforming a classical CNN architecture. Next, Aﬁﬁet al. (2021)eval- uated the performance of Triplet Network and deep Adversarial MetricLearning (DAML) while shifting training domains. They achieved 99%accuracy when the domain shift was small, and 81% when the shiftwas large. This work also indicates that a model trained on a largedataset can beﬁne-tuned on small datasets by the use of few-shot learn-ing methods.In this work, we explore the use of few-shot learning and featureextraction for plant stress recognition and severity estimation. Toexperiment, we will use a dataset of biotic stresses in coffee leaves.The remainder of this paper is organized as follows. Section 2 pre-sents some fundamental concepts on deep learning and few-shotlearning. Moreover, we also describe the methods Triplet Networkand Prototypical Network used in this work. Section 3 presents theexperimental setup and results. Finally, in Section 4, we drawsome conclusions.2. Method
2.1. PreliminariesIn this section, we present some fundamental concepts on deeplearning, describing the convolutional neural networks.Convolutional neural networks (CNN) ( LeCun et al., 1989) were pro- posed to process data with grid-like topology such as images and hashad success in practical applications in computer vision ( Goodfellow et al., 2016). A standard CNN is composed of three main types of layers:convolutional,poolingandfully connected layers.Fig. 2illustrates an ex- ample of a generic CNN architecture.2.1.1. Convolutional layerThe network is namedconvolutionalafter the mathematical opera- tion ofconvolutionapplied in this layer (Goodfellow et al., 2016). The layer is responsible for extracting features from the input data. Thesefeatures can be edges, points, or even full objects. The convolutionallayer consists of a set of learnableﬁlters, also known askernels. These kernels slide (i.e.convolve) across the input data producing a feature mapas an output (also known asactivation maps)(Li et al., 2021). At the end of a convolution layer, usually, we apply an activation function in the resulting feature map. Activation functions are important becausethey introduce non-linearity into the output, remove all negative values,and highlight important features. A usual activation function used in theliterature is the rectiﬁed linear unit, or simply, ReLU (Goodfellow et al., 2016).2.1.2. Pooling layerIn the pooling layer, the feature maps from the convolution layer arespatially reduced by a pooling (also known as downsampling) opera-tion. The pooling operation statistically summarizes regions from theactivation maps, reducing their size while maintaining their main fea-tures (Goodfellow et al., 2016). Two common ways of performing thepooling operation are maximum and average polling. In the maximumpooling, we summarize the set of values by its higher value. On theother hand, in the average pooling, we summarize the set of values bycalculating the average value.
Fig. 1.Example of biotic stresses that affect the coffee leaves.L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
562.1.3. Fully connected layerThe fully connected layer consists of a typical feedforward net-work, also known as multilayer perceptrons (MLPs) ( Goodfellow et al., 2016). They usually act as the classiﬁer in the CNNs, using the features extracted by the previous layers as input. In the con-text of CNNs, the inputs fed to a fully connected network are thefeature maps from the convolution and pooling layers. Since thefeature maps are tensors and the fully connected network receivesav e c t o ra si n p u t ,aﬂattenoperator is applied to the maps,returning avector.A CNN architecture can also be used without a fully connected layer,returning theﬂattened vector as output. This vector is sometimes re-ferred to as anembedding. This kind of architecture that produces em-beddings is extensively used in the few-shot learning context, aspresented in the next section.In the following, we present some fundamental concepts on few-shot learning. First, Section 2.2 presents some basic concepts and de ﬁni- tions in few-shot learning. Next, Section 2.3 presents the embeddinglearning methods, describing the ones used in this work.2.2. Few-shot learningOne of the main shortcomings in machine and deep learning is theneed of large-scale data to properly generalize ( Wang et al., 2020). To address this issue, the meta-learning paradigm was proposed(Hospedales et al., 2020), and, in particular, thefew-shot learning methods(Wang et al., 2020).Few-shot learning (FSL) aims to solve tasks using a limited amountof training data. This class of algorithms was inspired by the fact thathumans only need a small number of examples to learn and general-ize some tasks. For instance, given a few photos of a stranger, we caneasily recognize this person on another set of pictures ( Wang et al., 2020).MostFSLmethods proposed are for supervised learning problemssuch as image classiﬁcation and object recognition. Although not ex-plored in this work, one of the main uses of few-shot learning is learningto generalize to new classes unseen during training ( Wang et al., 2020; Hospedales et al., 2020;Chen et al., 2020).Unlike in most machine and deep learning conventional algorithms,in the few-shot learning context (and more particularly, in the few-shotimage classiﬁcation context), we usually train the models on a set oftaskscreated from the training data. Some authors also refer to this asepisodes(Ravi and Larochelle, 2017).Each task has asupport setand aquery set.T h eq u e r ys e tc o n s i s t so f the data that we want to make some inference about ( i.e.data that we want to classify in a classiﬁcation problem). The support set consistsof the labeled data that we use to assist in the inference. We usually de-note a task as an-wayk-shot problem, where
nis the number of classes in the support set; andkis the number of examples per class in the sup-port set.Fig. 3illustrates an example of a few-shot learning task.There are a few approaches in the literature to categorize the FSL methods. One of them is organizing the methods in initializationbased methods, distance metric learning based methods, and hallucina-tion based methods (Chen et al., 2020).2.2.1. Initialization based methodsAddress the few-shot learning problem by learning toﬁne tune.T h e r e are two main approaches to this. One of them attempts to learn goodmodel initialization (i.e.parameters of the network) and the other fo-cuses on learning the optimizer. Both approaches diminish the needfor data or the number of gradient steps to train the network.
Fig. 2.Illustration of a convolutional neural network consisting of convolutional, pooling and fully connected layers.
Fig. 3.Example of a 3-way 3-shot task. In this example, n= 3 because we have three different classes (cercospora leaf spot, leaf miner, and brown leaf spot) in the support set; and k=3 because we have three examples per class in the support set. The query set contains the image that we want to classify assisted by the ones in the support s et.L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
572.2.2. Distance metric learning based methodsAddress the few-shot learning problem by learning to compare.T h e s e methods attempt to decrease the need for data by training a model tolearn the similarity between images, instead of training the model to sim-ply classify the input image. Intuitively, if a model can distinguish two dif-ferent images, it can learn to classify unseen classes with few data. Thesemethods are also known as embedding learning methods.2.2.3. Hallucination based methodsAddress the few-shot learning problem by learning to augment. These methods tackle the lack of data directly, attempting to learn agenerator using the data available. This generator can then create newinstances for data augmentation.In our work, we focus on distance metric learning-based, also knownas embedding learning, methods. This class of methods was chosen dueto the promising results in image classiﬁcation tasks. In the next section, we provide more insight and deﬁnitions about this class of methods.2.3. Embedding learningEmbedding learning is a class of few-shot learning methods that ad-dress the classiﬁcation task as a comparison problem by learning to com- pare. But how do the models learn to compare? The embedding learningmethods produce a representation ( i.e.an embedding) of the input data with lower dimensionality than the original input using an embeddingfunction. Next, these embeddings are compared by a similarity function,in a way that embeddings from the same class are close together, andembeddings from different classes are separated ( Wang et al., 2020). Fig. 4illustrates a generic embedding learning model.Speciﬁcally, in the image classiﬁcation task, the embedding functionis usually a CNN without the fully connected layer, since the ﬂattened vector is a lower-dimensional representation of the input data. Wemay also refer to the embedding function as feature extractor. The sim-ilarity function is usually a distance function like the Euclidean or cosinedistance, depending on the model used.There are many embedding learning models proposed in the litera-ture. Among them, perhaps the most known are: Siamese Network(Koch et al., 2015), Triplet Network (Schroff et al., 2015), Matching Net- works (Vinyals et al., 2016), Prototypical Network (Snell et al., 2017), and Relation Network (Sung et al., 2018). In our work, we only used the Triplet Network and Prototypical Network. The Prototypical Networkwas selected because, despite being a relatively straightforward method,it still achieves competitive performance when compared to moremodern methods. The Triplet Network was selected because it is one oftheﬁrst few-shot learning methods proposed, serving as a baseline forthe few-shot learning results. Both models are described in the following.2.3.1. Triplet networkThe Triplet Network (TripletNet) (Schroff et al., 2015) consists of a few-shot learning model that uses three images as input (thus thenametriplet):anchor,positive,a n dnegative. The anchor consists of the image that we want to classify. Next, the positive consists of an imagethat is from the same class as the anchor. Lastly, the negative consistsof an image from another class (that is not the anchor's class). The ob-jective is to produce a representation of the inputs, so that the anchoris similar to the positive and different from the negative.Since we are working with images, the TripletNet uses a CNN modelas its feature extractor (embedding function). The similarity functionused is the squared Euclidean distance. The model aims to generalizein a way that the embeddings of the anchor and the positive are close
to each other in the feature space (are similar), and the embeddingsfrom the anchor and negative are far away from each other (are differ-ent). To that end, the TripletNet uses a loss function named triplet loss, described by Eq.1(Schroff et al., 2015):Lx
a;xp;xn/C0/C1¼max/C16/C12/C12/C12/C12fx
aðÞ−fx p/C0/C1/C12/C12/C12/C12 22−/C12/C12/C12/C12fx aðÞ−fx nðÞ/C12/C12/C12/C1222þα0/C17ð1Þwhere,f(x
a) is the embedded anchor,f(x p) is the embedded positive, f(x
n) is the embedded negative, andαis the margin enforced between positive and negative pairs.The idea behind the triplet loss can be visualized in Fig. 5.Fig. 6illus- trates the TripletNet model. Algorithm 1 presents pseudo-code for theloss computation in a single batch.
Fig. 4.Generic embedding learning model illustration. The inputs images are used as input for an embedding function. The embeddings produced are compared b y a similarity function, resulting in the output.
Fig. 5.Illustration of the learning process using the triplet loss. The loss aim to minimizethe distance between anchor and positive, whilst maximizing the distance between theanchor and negative. Based on the ﬁgure bySchroff et al. (2015).L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
58At the end of the training stage, the TripletNet with triplet loss pro-duces a feature extractor used to cluster instances from the same class.In order to use it as a classiﬁer, we usually combine this model with a k- nearest neighbors classiﬁer (KNN) (Cunningham and Delany, 2020), in a way that the embeddings from the training set act as the training sam-ples to the KNN.2.3.2. Prototypical networksThe Prototypical Network (ProtoNet) ( Snell et al., 2017) is one of the most effective approaches among the few-shot learning methods. Themain idea is that instead of comparing the embeddings from the sup-port set directly with the query set embedding, the ProtoNet computesaprototypefor each class in the support set, and compares the queryembedding with them.Formally, for each classnin the support set, we compute the proto-typec
n¼1k∑ki¼1fxiðÞ, wherekis the number of shots, andf(x i) is theembedding for each examplexifrom classn. After computing the prototypes, the embedded query sample f(x) is compared with the prototypes using a distance function. Unlike the TripletNet, theProtoNet can produce an output without the addition of anotherclassiﬁer. Given the distance functiond, the ProtoNet can produce a distribution over classes for a query xbased on a softmax over distances to the prototypes (Snell et al., 2017):py¼njxðÞ ¼
exp−dfxðÞ,c n ðÞðÞ∑
n0exp−dfxðÞ,c n0 ðÞðÞ ð2Þwherexis the query input data,yis the corresponding label,dis the dis- tance function (in our work we use the Euclidean distance), f(x)i st h e embedded query input, andc
nis the prototype for classn. Fig. 7illustrates the Prototypical Network model. Algorithm 2presents the loss computation for a single task using a single queryexample.
Fig. 6.Illustration of the Triplet Network model. The three input images are used as input to the feature extractor. Next, the embeddings are used as input in t he triplet loss computation, returning the model's output.L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
593. Experiments and resultsIn this section, we present and diwescuss the experimental results ob-tained by the few-shot learning methods Triplet Network ( Schroff et al., 2015) and Prototypical Network (Snell et al., 2017)i nt h ec o n t e x to fp l a n t stresses classiﬁcation and severity estimation. Besides using the TripletNet,and ProtoNet we also evaluated several backbones (feature extractors) ineach method to assess the contrast among different feature extractors.First, Section 3.1 presents the datasets used in the experiments. Next,Section 3.2 presents the experimental setup used. Section 3.3 presentsand discusses the results obtained by the methods used in the experiments.3.1. DatasetThe datasets used in this work were developed by Krohling et al. (2019). The data consists of images of arabica coffee leaves affected bycommon biotic stresses. The images were captured using smartphones.The datasets are divided in two sets: Leaf DatasetandSymptoms Dataset. The data used is available for download in GitHub.
13.1.1. Leaf datasetConsists of a set of 1685 images containing the entire leaf area over awhite area background. Each image also contains labels indicating thepredominant stress and its severity. There are four biotic stress classes:leaf miner, rust, brown leaf spot, and cercospora leaf spot, in addition tothe healthy class, totalingﬁve classes. The stress severity label is dividedintoﬁve classes, according to the percentage of injured area: healthy(<0.1%), very low (0.1 %−5%), low (5.1 %−10%), high (10.1 %− 15%) and very high (>15%).Fig. 8shows examples of images in thisdataset.Table 1presents some details about the number of instancesper class.3.1.2. Symptoms datasetConsists of a set of 2722 images from isolated symptoms. Most of thesymptoms were cropped from the original leaf images, totaling a num-ber of 2147 symptoms images. In addition to that, Esgario et al. (2020) also used 575 cropped images made available by Barbedo (2019),t o t a l - ing the 2722 images. Similar to the Leaf Dataset, there are four bioticstress classes: leaf miner, rust, brown leaf spot, and cercospora leafspot, in addition to the healthy class. Fig. 9shows examples of images in this dataset.Table 2presents some details about the number of in-stances per class.
Fig. 7.Illustration of the Prototypical Network model. The examples in the query and support set are used as input by a feature extractor. Using the embedded s upport samples, the pro- totype for each class in the support set are computed, resulting in c
1,c2,a n dc 3. Next, the distances between the embedded query input and prototypes are computed. Lastly, the softmax over the distances results in the classi ﬁcation output.
1www.github.com/esgario/lara2018L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
603.2. Experimental setupThe experiments performed were divided in three: Experiment I, Ex-periment II, and Experiment III. Experiment I consists of the biotic stressclassiﬁcation in the Leaf Dataset. Experiment II consists of the bioticstress classiﬁcation in the Symptoms Dataset. Finally, Experiment IIIconsists of the severity estimation in the Leaf Dataset.In each experiment, we evaluated the following few-shot learningmodels: TripletNet and ProtoNet. In particular, the ProtoNet was evalu-ated in the 5-way 1-shot and 5-way 5-shot settings. The TripletNet alsoused ak-Nearest Neighbors algorithm
2as classiﬁer, using the features extracted by the network as input. For each few-shot learning modelwe evaluatedﬁve different backbones (feature extractors) for featureextraction: ResNet50 (He et al., 2016), MobileNetv2 (Sandler et al., 2018), VGG16 (Simonyan and Zisserman, 2014), DenseNet121 (Huang et al., 2017), and EfﬁcientNet-B4 (Tan and Le, 2019).Table 3contains the number of parameters in each model.We used a 70%/15%/15% split of the dataset for training, validation,and testing, respectively. The images were resized to 224x224 and themodels were pre-trained in ImageNet. Random augmentations (resize,ﬂip, crop, color jitter) were applied online during the training phase.The models trained on 100 epochs, using an SGD optimizer ( Ruder, 2016) with learning rate equals to 0.01 (decaying by 0.5 every 20epochs), momentum equals 0.9, and weight decay equals 0.0005. Foreach image in the dataset, we randomly generated a support set withimages from the training set.In order to evaluate the models' performance we computed the fol-lowing metrics: accuracy (ACC), precision (PR), recall (RE), and F
1-score (F
1). Since the metrics precision and recall are only de ﬁned for binary classiﬁcation, we computed the macro average over the ﬁve classes. We also computed the average training time per epoch.The experiments were carried out in the Google Colab3environment with 16GB RAM and a Tesla T4 GPU. All the code was written in Python,with the help of several open-source libraries: PyTorch ( Paszke et al., 2019), Learn2Learn (Arnold et al., 2020), NumPy (Harris et al., 2020), Seaborn (Waskom, 2021), Matplotlib (Hunter, 2007), and Scikit-learn (Pedregosa et al., 2011). The source code used in these experiments isavailable in GitHub.
4
3.3. Results and discussionIn this section, we present the results obtained by the few-shotlearning methods in each of the experiments previously described.3.3.1. Experiment IResultsIn theﬁrst experiment, we present the results obtained in the bioticstress classiﬁcation task in the Leaf Dataset. First, we show the resultsobtained by the TripletNet for each backbone. Table 4presents the per- formance metrics obtained by the TripletNet, and Table 5contains the average training time per epoch.Next, we present the results obtained by the ProtoNet in the 5-way1-shot and 5-way 5-shot settings.Table 6presents the performance metrics obtained by both ProtoNet settings, and Table 7contains the av- erage training time per epoch.To ease the visualization of the accuracy per class obtained by thefew-shot learning methods, we also provide the confusion matricesfor the TripletNet, 5-way 1-shot ProtoNet, and 5-way 5-shot ProtoNetinFig. 10. We only show the confusion matrices obtained by the back-bones with the highest accuracy for each method.To visualize the models' output embeddings we also provide thescatter plots for TripletNet, 5-way 1-shot ProtoNet, and 5-way5-shot ProtoNet inFig. 11. These plots were obtained by reducingthe test fold output embeddings into a two-dimensional spaceusing the t-Distributed Stochastic Neighbor Embedding (t-SNE)(Van der Maaten and Hinton, 2008). The t-SNE parameters used to produce the visualization were t he default values in the scikit- learn library.
5
DiscussionAnalyzing the TripletNet results inTable 4, we observe that all back- bones achieved a high accuracy, ranging from 93.25% to 95.24%. The bestoverall backbones were the MobileNetv2 and VGG16, with an accuracyof 95.24%. However, the MobileNetv2 had a better recall and F
1score than the VGG16. Looking at the training time results in Table 5,w e
Fig. 8.Examples of leaf dataset images.
Table 1Leaf dataset details.Biotic stress #Images Severity #ImagesHealthy 272 Healthy 272Leaf Miner 387 Very Low 924Rust 531 Low 332Brown Leaf Spot 348 High 101Cercospora Leaf Spot 147 Very High 56Total 1685 Total 1685
2We tested values ofk= 5,10,15 and since the results were similar, we only show theresults fork=53colab.research.google.com
4www.github.com/lucastassis/pg-coffee
5https://scikit-learn.org/stable/modules/generated/sklearn.manifold.TSNE.htmlL.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
61also observe that the MobileNetv2 had the shortest training time, withan average training time per epoch of 40.03 sper epoch, compared to 67.83sobtained by the VGG16.In the results obtained by the ProtoNet, presented in Table 6,w e observe that this method had a slightly higher average accuracy whencompared to the TripletNet. The best backbone in the 5-way 1-shot set-ting was the ResNet50 (although the VGG16 and DenseNet121 obtainedthe same accuracy, the remaining metrics were worse than the onespresented by ResNet50), with an accuracy of 95.63%. As for the 5-way5-shot setting, the MobileNetv2 obtained the best results in everymetric, with an accuracy of 96.03%. Regarding training times, theMobileNetv2 obtained the shortest, with 14.29 sand 28.24sper epoch in the 5-way 1-shot and 5-way 5-shot settings, respectively.Analyzing the confusion matrices in Fig. 10,w eo b s e r v et h a ta l l the models obtained 100% accuracy in detecting leaves with brownleaf spot. The results obtained for the classes healthy, leaf miner,and brown leaf spot were also high. The lowest accuracy was ob-tained in the class cercospora leaf spot, with an accuracy value of67% in all few-shot learning methods. This result coincides with thework byEsgario et al. (2020), in which the lowest accuracy was ob-tained in this same class. This result may be justi ﬁed due to the sim- ilarity of this lesion with the others or due to the imbalanced dataset.One difference from the work byEsgario et al. (2020),w a st h a tt h e y obtained 100% accuracy in healthy leaves, whereas we obtained inthe class brown leaf spot. Investigating this, it seems that since theinput image resizing causes some distortions in the biotic stresses,and the few-shot learning methods are comparison-based methods,some rust lesions might have become too small and indistinguish-able from a healthy leaf.The scatter plots presented inFig. 11regarding the t-SNE embedding visualization shows that the backbones were able to generalize well,with some mistakes concerning the cercospora leaf spot class, that hada higher intersection with the other classes' clusters.The best result reported byEsgario et al. (2020)in the biotic stress classiﬁcation task using the Leaf Dataset was 95.63%, using a ResNet50as classiﬁer. This result is similar to ours, and the 5-way 5-shot ProtoNetwith MobileNetv2 backbone even slightly outperformed it, with an ac-curacy of 96.03%.3.3.2. Experiment IIResultsIn the second experiment, we present the results obtained in the bi-otic stress classiﬁcation task in the Symptoms Dataset. First, we showthe results obtained by the TripletNet for each backbone. Table 8pre- sents the performance metrics obtained by the TripletNet, and Table 9 contains the average training time per epoch.Next, we present the results obtained by the ProtoNet in the 5-way1-shot and 5-way 5-shot settings.Table 10presents the performance metrics obtained by both ProtoNet settings, and Table 11contains the average training time per epoch.To ease the visualization of the accuracy per class obtained by thefew-shot learning methods, we also provide the confusion matricesfor the TripletNet, 5-way 1-shot ProtoNet, and 5-way 5-shot ProtoNetinFig. 12. We only show the confusion matrices obtained by the back-bones with the highest accuracy for each method.To visualize the models' output embeddings we also provide thescatter plots for TripletNet, 5-way 1-shot ProtoNet, and 5-way 5-shotProtoNet inFig. 13.
Fig. 9.Examples of symptoms dataset images.
Table 2Symptoms dataset details.Biotic stress #ImagesHealthy 256Leaf Miner 593Rust 991Brown Leaf Spot 504Cercospora Leaf Spot 378Total 2722
Table 3Number of parameters (M) in each model.Backbone Parameters (M)ResNet50 25.56MobileNetv2 3.4VGG16 138.36DenseNet121 7.97EfﬁcientNet-B4 19L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
62DiscussionObserving the TripletNet results inTable 8, we may note that again the methods obtained a high accuracy, ranging from 94.63% to 96.42%.In particular, the best result was obtained by the MobileNetv2 back-bone, with an accuracy of 96.42%, precision of 96.24, recall of 96.54%,andF
1-score of 96.38%. The best average time per epoch was also fromMobileNetv2. Other backbones such as the ResNet50, DenseNet121,and EfﬁcientNet-B4 also obtained close results to the one by MobileNetv2.The VGG16 had the worst scores out of all the backbones.Analyzing the ProtoNet results inTable 10we observe that all the backbones had a good performance with a best accuracy of 96.72% inthe 5-way 1-shot setting and 96.42% in the 5-way 5-shot. Although itis expected that a higher number of shots will translate into higher ac-curacy, we obtained a higher average accuracy in the 1-shot setting.We understand that this is due to the fact that since the SymptomsDataset contains only a small number of classes, the feature extractordoes not need many support examples per class to generalize well.It also indicates that a single sample from each class contains all therepresentative features from the class. Considering only the results ob-tained in the 5-way 1-shot setting, we observe that the Ef ﬁcientNet- B4 outscored every backbone in every metric, with an accuracy of96.72%, precision of 95.73%, recall of 96.91% and F1-Score of 96.72%.However, looking the results inTable 11, we observe that this backbone had the largest training time, with an average of 37.68 sper epoch. In comparison, the ResNet50 and MobileNetv2 obtained accuracies of96.12% and 96.42% in 21.28sand 16.28sper epoch, respectively. These accuracies are competitive with the ones obtained by Ef ﬁcientNet-B4, but the models were almost twice as fast to train. In particular, theMobileNetv2 had a great trade-off in terms of accuracy and averagetime per epoch. The DenseNet121 and VGG16 obtained lower resultsof accuracy and also had a higher training time than the ResNet50 andMobileNetv2. Considering the results obtained in the 5-way 5-shot set-ting, we observe that in this setting the ResNet50 had the best perfor-mance. The backbone obtained an accuracy of 96.42%, precision of96.24%, recall of 96.27%, and F1-Score of 96.25%. The second and thirdbest accuracies were obtained by the Ef ﬁcientNet-B4 (96.11%) and MobileNetv2 (95.82%), respectively. Similar to the results presentedfor the 1-shot setting, the MobileNetv2 and ResNet50 had the besttrade-off between accuracy and training time.Analyzing the confusion matrices inFig. 12, we observe that all the models obtained 100% accuracy in classifying healthy leaves. The lowestaccuracy was obtained in the class cercospora leaf spot, with accuracyvalues ranging from 90% to 94%. The lower accuracy on the cercosporaleaf spot class coincides with the previous experiment's results. Oneinteresting thing to note is that the results obtained in the biotic stressclassiﬁcation task were better in the Symptoms Dataset than the onesobtained in the Leaf Dataset. This is in agreement with those providedbyEsgario et al. (2020)andBarbedo (2019). Since the images in the Symptoms Datasets focus only on the lesion itself, the network has aneasier task of detecting the region of interest. For example, in the LeafDataset, the networks have to deal with background and non-injuredareas in the leaf, which makes the classiﬁcation task harder. The scatter plots presented inFig. 11regarding the t-SNE embedding visualization shows that the backbones were able to generalize well.Note that in this experiment the clusters have less intersection thanthe ones presented in Experiment I, corroborating with the better clas-siﬁcation scores.The best result reported byEsgario et al. (2020)in the biotic stress classiﬁcation task using the Symptoms Dataset was 97.07%, using aResNet50 as a classiﬁer. This result is similar to ours (96.72%). This dif-ference is about 0.35% and it may be due training variation.3.3.3. Experiment IIIResultsIn the third experiment, we present the results obtained in theseverity estimation task in the Leaf Dataset. First, we show the resultsobtained by the TripletNet for each backbone. Table 12presents the per- formance metrics obtained by the TripletNet. Since this experiment uses
the Leaf Dataset, the average training time per epoch results are similarto Experiment I. Thus, the average training time per epoch is the same asinTable 5andTable 7for TripletNet and ProtoNet, respectively.Next, we present the results obtained by the ProtoNet in the 5-way1-shot and 5-way 5-shot settings.Table 13contains the performance metrics obtained by both ProtoNet settings.To ease the visualization of the accuracy per class obtained by thefew-shot learning methods, we also provide the confusion matricesfor the TripletNet, 5-way 1-shot ProtoNet, and 5-way 5-shot ProtoNetinFig. 14. We only show the confusion matrices obtained by the back-bones with the highest accuracy for each method.Table 4Performance metrics obtained by Triplet Networks in Experiment I.TripletNet resultsBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 94.05 92.21 88.96 90.17MobileNetv2 95.24 94.62 91.45 92.71VGG16 95.24 95.12 90.69 92.25DenseNet121 93.25 92.23 87.14 88.64EfﬁcientNet-B4 93.65 91.73 87.49 88.85
Table 5Training time per epoch for Triplet Networks in Experiment I.TripletNet Training Time per Epoch (s)Backbone Time (s)ResNet50 52.97MobileNetv2 40.03VGG16 67.83DenseNet121 70.07EfﬁcientNet-B4 101.87Table 6Performance metrics obtained by Prototypical Networks in Experiment I.ProtoNet Results5-way 1-shotBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 95.63 95.72 91.94 93.36MobileNetv2 94.44 92.32 91.36 91.80VGG16 95.63 95.07 91.64 93.02DenseNet121 95.63 94.74 91.94 93.04EfﬁcientNet-B4 94.44 92.53 89.54 90.635-way 5-shotBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 94.84 93.38 91.25 92.14MobileNetv2 96.03 96.12 92.21 93.70VGG16 94.44 92.95 90.24 91.31DenseNet121 95.63 95.72 91.87 93.32EfﬁcientNet-B4 95.24 94.00 92.05 92.89
Table 7Training time per epoch for protypical networks in Experiment I.ProtoNet training time per Epoch (s)Backbone 5-way 1-shot 5-way 5-shotResNet50 20.75 35.43MobileNetv2 14.29 28.24VGG16 34.65 62.24DenseNet121 28.23 59.71EfﬁcientNet-B4 41.52 87.28L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
63To visualize the models' output embeddings we also provide thescatter plots for TripletNet, 5-way 1-shot ProtoNet, and 5-way 5-shotProtoNet inFig. 15.DiscussionAnalyzing the TripletNet results presented in Table 12, we observe that the average accuracy in the severity estimation task was lowerthan the ones in the previous experiments, with accuracy rangingfrom 86.11% to 91.27%. The best results were obtained by theMobileNetv2, with an accuracy of 91.27%, precision of 88.25%, recall of86.57%, andF
1-score of 87.32%. The MobileNetv2 also had a lower train-ing time per epoch. The lower scores in precision, recall, and F
1-score also indicate that there was a problem in the generalization of someclasses. This is expected due to the imbalanced dataset. Also, sincethere is a hierarchy-like nature between the classes, many of the mis-takes are, for example, classifying the severity as Highinstead ofVery High, which may not be a big mistake. This is observable in the confusionmatrices shown. Regarding the results obtained by the ProtoNet inTable 13, we may observe that the 5-way 1-shot setting obtained ahigher accuracy than the 5-way 5-shot, with scores of 88.49% and85.87%, respectively. The 5-way 1-shot setting also obtained higherprecision, recall andF
1-score. This is an interesting result since asmentioned previously, the 5-way 5-shot setting is expected to performbetter than the 5-way 1-shot setting. However, we believe that becauseof the hierarchy-like nature of the severity classes, when the ProtoNetcomputes each class prototype (particularly, in the 5-way 5-shot, bycomputing an average ofﬁve embeddings for each class), there maybe some distortion in the resulting prototype embedding. Since the dif-ference between classes is not as apparent as it may be in other cases(such as in the biotic stress classiﬁcation), this average may confusethe network. Considering the 5-way 1-shot setting, we observe thatthe MobileNetv2 surpassed every model, with an accuracy of 93.25%,precision of 94.11%, recall of 92.40% and F
1-score of 93.03%. For the 5-way 5-shot setting, the best performance was obtained by theDenseNet121, with an accuracy of 91.27%, precision of 87.82%, recall of85.98%, andF
1-score of 86.62%.
Fig. 10.Confusion matrices obtained by the few-shot learning methods in Experiment I.
Fig. 11.Visualization obtained by t-SNE for the few-shot learning methods in Experiment I.Table 8Performance metrics obtained by Triplet Networks in Experiment II.TripletNet ResultsBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 95.82 95.53 95.98 95.73MobileNetv2 96.42 96.24 96.54 96.38VGG16 94.63 94.53 94.41 94.46DenseNet121 95.52 95.28 95.23 95.25EfﬁcientNet-B4 95.22 95.18 95.10 95.14
Table 9Training time per epoch for Triplet Networks in Experiment II.TripletNet Training Time per Epoch (s)Backbone Time (s)ResNet50 67.03MobileNetv2 50.48VGG16 84.97DenseNet121 88.88EfﬁcientNet-B4 125.82L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
64Table 10Performance metrics obtained by Prototypical Networks in Experiment II.ProtoNet Results5-way 1-shotBackbone ACC (%) PR (%) RE(%) F
1(%)ResNet50 96.12 95.91 96.14 96.02MobileNetv2 96.42 96.33 96.27 96.29VGG16 94.33 94.35 93.91 94.09DenseNet121 95.52 95.51 95.31 95.41EfﬁcientNet-B4 96.72 96.66 96.75 96.705-way 5-shotBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 96.42 96.24 96.27 96.25MobileNetv2 95.82 95.77 95.63 95.68VGG16 94.93 94.70 94.76 94.71DenseNet121 94.93 95.10 94.68 94.84EfﬁcientNet-B4 96.12 96.09 96.11 96.10
Table 11Training time per epoch for Triplet Networks in Experiment II.ProtoNet Training Time per Epoch (s)Backbone 5-way 1-shot 5-way 5-shotResNet50 21.28 48.76MobileNetv2 16.28 40.93VGG16 35.32 78.29DenseNet121 33.34 76.29EfﬁcientNet-B4 37.68 83.27
Fig. 12.Confusion matrices obtained by the few-shot learning methods in Experiment II.
Fig. 13.Visualization obtained by t-SNE for the few-shot learning methods in Experiment II.Table 12Performance metrics obtained by Triplet Networks in Experiment III.TripletNet ResultsBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 86.11 83.74 80.12 81.70MobileNetv2 91.27 88.25 86.57 87.35VGG16 87.30 83.77 81.10 82.37DenseNet121 89.68 88.84 86.30 87.34EfﬁcientNet-B4 86.11 83.25 75.97 78.17
Table 13Performance metrics obtained by Prototypical Networks in Experiment III.ProtoNet Results5-way 1-shotBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 88.89 88.62 84.82 86.31MobileNetv2 93.25 94.11 92.40 93.03VGG16 84.52 74.01 74.47 74.03DenseNet121 85.32 77.18 78.00 77.09EfﬁcientNet-B4 90.48 84.34 82.24 82.825-way 5-shotBackbone ACC (%) PR (%) RE (%) F
1(%)ResNet50 83.73 74.00 73.52 73.43MobileNetv2 86.51 70.04 70.06 69.64VGG16 81.35 76.36 70.63 72.63DenseNet121 91.27 87.82 85.98 86.62EfﬁcientNet-B4 86.51 82.35 81.28 81.70L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
65Analyzing the confusion matrices inFig. 14,w eo b s e r v et h a tt h el o w - est accuracy was obtained in the class Very High, with accuracy valuesranging from 73% to 91%. The lower accuracy on the Very High classmay be due to the dataset imbalance. As mentioned before, observingthe confusion matrices we note that the mistakes made by the networksare mainly close to the diagonal. This means that the mistakes are not asworrisome as they may seem to be, since most of them may be becauseof the hierarchy-like nature of the classes.The scatter plots presented inFig. 15regarding the t-SNE embedding visualization show that the methods had more dif ﬁculties generalizing the classes. Note that in this experiment the clusters have a higher inter-section, particularly between the classes Low, Very Low, High, and VeryHigh, than the ones presented in Experiment I and II.The best result reported byEsgario et al. (2020)in the severity esti- mation task using the Leaf Dataset was 86.51%, using a ResNet50 as aclassiﬁer. In this particular task, the few-shot learning methodsachieved a better result, with an accuracy of 93.25%.3.3.4. Overall considerationsOverall the ProtoNet (in both settings) obtained higher metricsscores than the TripletNet. The 5-way 5-shot was expected to achievehigher accuracy than the 5-way 1-shot methods, however, the 5-way1-shot had a better training time/accuracy trade-off in most cases.Comparing the backbones, the MobileNetv2 presented the best trade-off between training time and accuracy, outperforming or presentinga similar performance in most cases. This is interesting because,due to its size (seeTable 3), the MobileNetv2 can be embedded onsmartphones. The results in Experiment I and II were similar to theones byEsgario et al. (2020), however, the few-shot learning methodshad a much better performance in Experiment III. This result showsthat few-shot learning methods are promising and might help improveaccuracy in the severity estimation task.4. ConclusionIn this work, we evaluated the use of few-shot learning methods ap-plied to plant biotic stress recognition by using Prototypical Networksand Triplet Networks for coffee leaves biotic stress classi ﬁcation and se- verity estimation. We evaluated the Triplet Networks and PrototypicalNetworks (both settings) on three experiments, obtaining promisingresults. In theﬁrst experiment, consisting of the biotic stress classi ﬁca- tion in the Leaf Dataset, we achieved an accuracy of 96.03%. In the sec-ond experiment, consisting of the biotic stress classi ﬁcation in the Symptoms Dataset, we achieved an accuracy of 96.72%. Both resultswere similar to the ones reported in the literature. However, in thethird experiment, we achieved an accuracy of 93.25% in the severity es-timation task. This result was signiﬁcantly better than the result re- ported in the literature and shows promise in the use of few-shotlearning methods for plant biotic stress recognition. The work also pre-sented some limitations, among them, there was a lack of performanceevaluation of the methods in unseen classes during training. We also didnot evaluate any kind of domain shift to assess the adaptability and ro-bustness of the methods to new classes. However, despite the limita-tions, the achieved results showed great promise in the few-shotlearning methods as an alternative for plant biotic stress recognition. Al-though the work focused on plant biotic stress recognition, the method-ology can be expanded to otherﬁelds. Future works include theperformance evaluation of few-shot learning methods in unseen
Fig. 14.Confusion matrices obtained by the few-shot learning methods in Experiment III.
Fig. 15.Visualization obtained by t-SNE for the few-shot learning methods in Experiment III.L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
66classes, evaluation of domain shift to new classes, use of other few-shotlearning methods, and decreasing the size of the dataset to assess theamount of data needed for proper generalization.Credit author statementL. M. Tassis- Methodologies, Software, Experiment Simulations,Writing Original Draft, Writing-Reviewing and Editing. R. A. Krohling-Conceptualization, Methodologies, Review, Editing and Supervision.Declaration of Competing InterestAll authors disclose that there is no conﬂict of interest including any ﬁnancial, personal or other relationships with other people or organiza-tions.AcknowledgementsR.A. Krohling thanks the Brazilian research agency ConselhoNacional de Desenvolvimento Cientíﬁco e Tecnólogico (CNPq) forﬁnan- cial support under grant no. 304688/2021-5.References
Aﬁﬁ, A., Alhumam, A., Abdelwahab, A., 2021. Convolutional neural network for automatic identiﬁcation of plant diseases with limited data. Plants 10 (1), 28.Argüeso, D., Picon, A., Irusta, U., Medela, A., San-Emeterio, M.G., Bereciartua, A., Alvarez-Gila, A., 2020.Few-shot learning approach for plant disease classi ﬁcation using im- ages taken in theﬁeld. Comput. Electron. Agric. 175, 105542.Arnold, S.M., Mahajan, P., Datta, D., Bunner, I., Zarkias, K.S., 2020. learn2learn: a library formeta-learning research. arXiv e-prints. arXiv:2008.12284. Barbedo, J.G.A., 2016.A review on the main challenges in automatic plant disease identi-ﬁcation based on visible range images. Biosyst. Eng. 144, 52 –60. Barbedo, J.G., 2018.Factors inﬂuencing the use of deep learning for plant disease recogni-tion. Biosyst. Eng. 172, 84–91. Barbedo, J.G.A., 2019.Plant disease identiﬁcation from individual lesions and spots using deep learning. Biosyst. Eng. 180, 96 –107. Bock, C., Poole, G., Parker, P., Gottwald, T., 2010. Plant disease severity estimated visually, by digital photography and image analysis, and by hyperspectral imaging. Crit. Rev.Plant Sci. 29 (2), 59–107.Boulent, J., Foucher, S., Théau, J., St-Charles, P.-L., 2019. Convolutional neural networks for the automatic identiﬁcation of plant diseases. Front. Plant Sci. 10, 941.Chen, W.-Y., Liu, Y.-C., Kira, Z., Wang, Y.-C.F., Huang, J.-B., 2020. A closer look at few-shotclassiﬁcation. arXiv e-prints.arXiv:1904.04232. Cunningham, P., Delany, S.J., 2020. k-nearest neighbour classi ﬁers. arXiv e-prints. arXiv:2004.04523.Elhassouny, A., Smarandache, F., 2019. Smart mobile application to recognize tomato leaf diseases using convolutional neural networks. 2019 International Conference of Com-puter Science and Renewable Energies (ICCSRE), pp. 1 –4. Esgario, J.G., de Castro, P.B., Tassis, L.M., Krohling, R.A., 2022. An app to assist farmers in the identiﬁcation of diseases and pests of coffee leaves using deep learning. Informa-tion Processing in Agriculture 9 (1), 38 –47. Esgario, J.G., Krohling, R.A., Ventura, J.A., 2020. Deep learning for classiﬁcation and severity estimation of coffee leaf biotic stress. Comput. Electron. Agric. 169, 105162.Geetharamani, G., Pandian, J.A., 2019. Identiﬁcation of plant leaf diseases using a nine- layer deep convolutional neural network. Comput. Electr. Eng. 76, 323 –338. Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep learning. MIT Press Available at http:// www.deeplearningbook.org. Harris, C.R., Millman, K.J., van der Walt, S.J., Gommers, R., Virtanen, P., Cournapeau, D.,Wieser, E., Taylor, J., Berg, S., Smith, N.J., Kern, R., Picus, M., Hoyer, S., van Kerkwijk,M.H., Brett, M., Haldane, A., del Río, J.F., Wiebe, M., Peterson, P., Gérard-Marchant,P., Sheppard, K., Reddy, T., Weckesser, W., Abbasi, H., Gohlke, C., Oliphant, T.E.,2020.Array programming with NumPy. Nature 585 (7825), 357 –362. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778.Hospedales, T., Antoniou, A., Micaelli, P., Storkey, A., 2020. Meta-learning in neural net-works: a survey. arXiv e-prints. arXiv:2004.05439. Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected convolutional networks. Proceedings of the IEEE Conference on Computer Visionand Pattern Recognition, pp. 4700 –4708. Hunter, J.D., 2007.Matplotlib: A 2d graphics environment. Comp. Sci. & Eng. 9 (3), 90 –95. Jadon, S., 2020.SSM-net for plants disease identiﬁcation in low data regime. IEEE/ITU In- ternational Conference on Artiﬁcial Intelligence for Good (AI4G), pp. 158 –163. Koch, G., Zemel, R., Salakhutdinov, R., 2015. Siamese Neural Networks for One-Shot Image Recognition. ICML Deep Learning Workshop, p. 2.Krohling, R., Esgario, J.G., Ventura, J.A., 2019. BRACOL - a Brazilian arabica coffee leaf im- ages dataset to identiﬁcation and quantiﬁcation of coffee diseases and pests. Mendeley Data.LeCun, Y., Boser, B., Denker, J.S., Henderson, D., Howard, R.E., Hubbard, W., Jackel, L.D.,1989.Backpropagation applied to handwritten zip code recognition. Neural Comput.1( 4 ) ,5 4 1–551.Li, F.-F., Krishna, R., Xu, D., 2021. Cs231n: convolutional neural networks for visual recog-nition lecture notes. Stanford University https://cs231n.github.io/.Manso, G.L., Knidel, H., Krohling, R.A., Ventura, J.A., 2019. A smartphone application to de- tection and classiﬁcation of coffee leaf miner and coffee leaf rust. arXiv e-prints arXiv:1904.00742.Mohanty, S., Hughes, D., Salathe, M., 2016. Using deep learning for image-based plant dis- ease detection. Front. Plant Sci. vol. 04, 7.Paszke, A., Gross, S., Massa, F., Lerer, A., Bradbury, J., Chanan, G., Killeen, T., Lin, Z.,Gimelshein, N., Antiga, L., Desmaison, A., Kopf, A., Yang, E., DeVito, Z., Raison, M.,Tejani, A., Chilamkurthy, S., Steiner, B., Fang, L., Bai, J., Chintala, S., 2019. Pytorch: an imperative style, high-performance deep learning library. Adv. Neural Inf. Proces.Syst. 32, 8024–8035.Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825 –2830. Rahman, C.R., Arko, P.S., Ali, M.E., Khan, M.A.I., Wasif, A., Jani, M.R., Kabir, M.S., 2018. Iden- tiﬁcation and recognition of rice diseases and pests using deep convolutional neuralnetworks. arXiv e-prints arXiv:1812.01043.Ravi, S., Larochelle, H., 2017.Optimization as a model for few-shot learning. InternationalConference on Learning Representations (ICLR).Ruder, S., 2016. An overview of gradient descent optimization algorithms. arXiv e-prints.arXiv:1609.04747.Sandler, M., Howard, A., Zhu, M., Zhmoginov, A., Chen, L.-C., 2018. Mobilenetv2: Inverted residuals and linear bottlenecks. Proceedings of the IEEE Conference on Computer Vi-sion and Pattern Recognition, pp. 4510 –4520. Schroff, F., Kalenichenko, D., Philbin, J., 2015. FaceNet: a uniﬁed embedding for face recog- nition and clustering. arXiv e-prints arXiv:1503.03832.Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv e-prints arXiv:1409.1556.Snell, J., Swersky, K., Zemel, R., 2017. Prototypical networks for few-shot learning. Ad- vances in Neural Information Processing Systems, pp. 4077 –4087. Sung, F., Yang, Y., Zhang, L., Xiang, T., Torr, P.H., Hospedales, T.M., 2018. Learning to com-pare: Relation network for few-shot learning. Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 1199 –1208. Suzuki, N., Rivero, R.M., Shulaev, V., Blumwald, E., Mittler, R., 2014. Abiotic and biotic stress combinations. New Phytol. 203 (1), 32 –43. Tan, M., Le, Q., 2019.Efﬁcientnet: Rethinking model scaling for convolutional neural net-works. International Conference on Machine Learning, pp. 6105 –6114. Tassis, L.M., Tozzi de Souza, J.E., Krohling, R.A., 2021. A deep learning approach combining instance and semantic segmentation to identify diseases and pests of coffee leavesfrom in-ﬁeld images. Comput. Electron. Agric. 186, 106191.Van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9 (86), 2579–2605.Ventura, J.A., Costa, H., de Santana, E.N., Martins, M.V.V., 2017. Manejo das doenças do cafeeiro conilon. Café Conilon 435 –479. Vinyals, O., Blundell, C., Lillicrap, T., Wierstra, D., et al., 2016. Matching networks for one shot learning. Advances in Neural Information Processing Systems, pp. 3630 –3638. Wang, B., Wang, D., 2019.Plant leaves classiﬁcation: a few-shot learning method based on siamese network. IEEE Access 7, 151754 –151763. Wang, Y., Yao, Q., Kwok, J.T., Ni, L.M., 2020. Generalizing from a few examples: a survey on few-shot learning. ACM Comput. Surv. 53 (3).Waskom, M.L., 2021.seaborn: statistical data visualization. J. Open Source Softw. 6 (60),3021.L.M. Tassis and R.A. Krohling Artiﬁcial Intelligence in Agriculture 6 (2022) 55 –67
67