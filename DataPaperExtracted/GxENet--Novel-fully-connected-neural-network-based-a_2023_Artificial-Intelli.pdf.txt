GxENet: Novel fully connected neural network based approachesto incorporate GxE for predicting wheat yield
Sheikh Jubaira,⁎, Olivier Tremblay-Savarda,M i k eD o m a r a t z k ib
aDepartment of Computer Science, University of Manitoba, 66 Chancellors Cir, Winnipeg, MB, R3T 2N2, Canada
bDepartment of Computer Science, University of Western Ontario, 1151 Richmond St, London, ON, N6A 3K7, Canada
abstract article info
Article history:Received 27 November 2022Received in revised form 30 April 2023Accepted 15 May 2023Available online 19 May 2023The expression of quantitative traits of a line of a crop depends on its genetics, the environment where it is sownand the interaction between the genetic information and the environment known as GxE. Thus to maximize foodproduction, new varieties are developed by selecting superior lines of seeds suitable for a speci ﬁc environment. Genomic selection is a computational technique for developing a new variety that uses whole genome molecularmarkers to identify top lines of a crop. A large number of statistical and machine learning models are employedfor single environment trials, where it is assumed that the environment does not have any effect on the quanti-tative traits. However, it is essential to consider both genomic and environmental data to develop a new variety,as these strong assumptions may lead to failing to select top lines for an environment. Here we devised threenovel deep learning frameworks incorporating GxE within the deep learning model and predicted line-speci ﬁc yield for an environment. In the process, we also developed a new technique for identifying environment-speciﬁc markers that can be useful in many applications of environment-speci ﬁc genomic selection. The result demonstrates that our best framework obtains 1.75 to 1.95 times better correlation coef ﬁcients than other deep learning models that incorporate environmental data depending on the test scenario. Furthermore, the fea-ture importance analysis shows that environmental information, followed by genomic information, is the drivingfactor in predicting environment-speci ﬁc yield for a line. We also demonstrate a way to extend our framework for new data types, such as text or soil data. The extended model also shows the potential to be useful in genomicselection.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Genomic predictionMulti-environment trialDeep learningGxEEnviromics
1. IntroductionFood crop production faces impending challenges in feeding theglobal population, including increasing population, reduction in agricul-tural inputs, and global climate change. These challenges have led toglobal problems in food security. Around 85 million more people werefacing a severe food crisis in 2021 compared to what was reported in2016, which resulted in around 193 million people facing severe hungeracross 36 countries (FAO, 2022). This crisis has been worsened recentlyboth by human made and natural phenomena such as domestic foodprice inﬂation, war and pandemic (FAO, 2022;World Bank Group, 2022;UN World Food Programme, 2022;Kakaei et al., 2022). To address this problem, we need to produce more food with the limited resourcesavailable by creating improved varieties of crops that can perform wellin different environments.A genotype of a crop organism is its complete set of genetic makeupthat inﬂuence its traits. However, a genotype often refers to genes thathave different alleles. Thus to avoid confusion, we will refer genotypesas lines in the rest of the article. To create a new variety from lineswith improved traits, we need to consider a line's genetics and its inter-action with the environment where it is sown ( Washburn et al., 2021; Lin et al., 2020). This phenomenon is known as genotype by environ-ment interaction (GxE), which refers to the fact that even if a varietyproduces the desired values of quantitative traits in one environment,it may not provide us with the same outcome in another environment(Lenz et al., 2017). Thus, any tools that are developed to aid in cropbreeding need to replicate the impact of GxE within the model by incor-porating genetic and environmental information. In this work, we aimto build such computational tools that estimate a trait even before sow-ing the crop.The advancement of the next-generation sequencing technology,such as genotyping by sequencing (GBS) and restriction-site associatedDNA sequencing (RAD-seq), enables us to capture the genetic diversityamong different lines of the same species ( Nguyen et al., 2019;EspositoArtiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
⁎Corresponding author.E-mail address:jubairs@myumanitoba.ca(S. Jubair).
https://doi.org/10.1016/j.aiia.2023.05.0012589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/et al., 2020). These sequencing technologies usually provide us withmany molecular markers, typically covering the whole genome of thespecies. These markers can be employed for studying genetic diversity,identiﬁcation of quantitative trait loci for a speci ﬁc trait (Boudhrioua et al., 2020;Zhang et al., 2019;Zegeye et al., 2018;Tang et al., 2018) and for estimating genomic breeding values for different traits ( Tong and Nikoloski, 2021;Jubair et al., 2021;Khaki and Wang, 2019;Ma et al., 2018;Rachmatia et al., 2017;Crossa et al., 2016). The environment of a crop is the combination of the weather, soilandﬁeld management information of where it is sown ( Jubair and Domaratzki, 2023;Sharma et al., 2022;Washburn et al., 2021;Khaki and Wang, 2019). While the most frequently obtained weather andsoil information includes precipitation, temperature, pressure, radia-tion, wind speed, humidity, day length, soil electrical conductivity, cal-cium carbonate, saturated hydraulic conductivity, gypsum content andpH, the most frequently usedﬁeld management variables include man-agement practices such as sowing pattern, number of pre-irrigations,and the amount of fertilizer and insecticide applied on the ﬁeld (Washburn et al., 2021;Lin et al., 2020;Khaki and Wang, 2019; Montesinos-López et al., 2019;Shook et al., 2021;Khaki et al., 2020; Guo et al., 2020;Sandhu et al., 2021). However, based on the previousresearch, this is a minimal list of the weather, soil and ﬁeld management variables. The potential list of variables can be arbitrarily large. The ef-fect of environmental variables on crops differs from growing cycle togrowing cycle, even for the same trait and line ( Sonkar et al., 2019; Tadesse et al., 2019). For example, researchers developed differentwheat varieties suitable for different traits and weather conditionssuch as heat-stressed (Ly et al., 2018), high rainfall (Tadesse et al., 2010) and favourable environments where crops are provided with op-timum water and heat (Juliana et al., 2017). Genomic selection (GS) is a computational technique of selectingtop lines to create new varieties of a crop. GS takes genotyped dataand, increasingly, environmental information as input and predictsquantitative traits as outputs (Khaki and Wang, 2019;Meuwissen
et al., 2001). There have been many genomic selection applicationsthat use machine learning with genomic data only ( Zhang et al., 2019; Jubair et al., 2021;Ma et al., 2018;Gianola et al., 2011;Pérez- Rodríguez et al., 2012;González-Camacho et al., 2012, 2016 ; Rachmatia et al., 2017;Jubair and Domaratzki, 2019). This type of geno- mic selection is known as a single-environment trial as it is assumedthat the environmental effect on plants remains constant; hence singleenvironment trials are not able to capture the environmental effect ongenotypes of a line. Then, some genomic selection models take environ-mental information only as the input and predict average quantitativetrait of lines for that speciﬁc environment (Lin et al., 2020;Shook et al., 2021;Khaki et al., 2020). As no genetic information is given asthe input to the models, they also do not capture the effect of the envi-ronment on genotypes. These models are known as multi-environmentmodels as they are able to predict average quantitative traits for differ-ent environments. On the other hand, other more recent multi-environment models consider both environmental and genomic dataand the interaction between them to predict line speci ﬁc phenotype (Washburn et al., 2021;Khaki and Wang, 2019;Montesinos-López et al., 2019;Jarquín et al., 2014). However, the process of buildingmulti-environment models incorporating environmental and genomicdata is not well understood. In this work, we aim to build novel machinelearning approaches for a wheat dataset that combines genomic and en-vironmental information that is capable of making predictions in novelenvironments where previous crop performance data is not available.As GS for multi-environment trials requires different types of data,such as weather and genomic data, incorporating these pieces of datatogether to capture GxE is a signiﬁcant challenge. Deep learning (DL)methods that employ neural networks are known for their ability tohandle heterogeneous data and have been successfully applied insome recent papers for GS with multi-environment trials ( Washburn et al., 2021;Lin et al., 2020;Khaki and Wang, 2019;Montesinos-Lópezet al., 2019). The main building blocks of deep learning models are arti-ﬁcial neural networks, such as fully connected neural networks (linearlayers), recurrent neural networks and convolutional neural networks.Each deep learning model has at least one input layer, more than twohidden layers of neural networks and an output layer. The input andoutput of the neural networks are the neurons, where the input layerneurons are the input features such as genetic marker data or environ-ment variables. The input to the hidden neural networks are thefeatures from the previous layer and produce a learned feature repre-sentation as the output by applying some functions, based on the typeof employed neural network. Finally, the output layer takes the outputof the last hidden layer as the input and employs the neural networkfunctions to make theﬁnal prediction. In this work, all our proposedframeworks employ fully connected neural networks where each neu-ron in a hidden layer is the linear function of all neurons of the previouslayer. Thus each neuron of the current layer represents summarized in-formation of all previous neurons.In this work, we proposed three deep learning frameworks thatcombine genotyped data and environmental information, such asweather andﬁeld management data, to replicate GxE and predictwheat yield in multi-environment trials. These frameworks differ onhow GxE is incorporated within the deep learning model or whetherﬁeld management information is integrated. Overall, we have the fol-lowing contributions:1. We proposed a novel concept of global and local marker sets for fea-ture selection where the global marker sets are the markers impor-tant for yield prediction irrespective of any environment. On theother hand, local markers are environment-speci ﬁc important markers for a certain trait.2. We devised two deep learning frameworks where we carefullymodelled the interaction between weather variables and genotypeddata and predicted line-speciﬁc yield value of wheat.3. We extended one of the frameworks (third framework) to integrateunstructured text aboutﬁeld notes.4. We employed DeepLift (Shrikumar et al., 2017), a method to identify which features contribute more towards prediction in a deep learn-ing model, to understand how environmental data, genetic informa-tion and text data contribute to predicting yield in our models.2. Materials and methods2.1. Dataset2.1.1. Genotyped and phenotyped dataGenotypic data for Spring Wheat(Triticum aestivum)is collected from the CIMMYT dataverse used in the Feed the Future InnovationLab (Poland et al., 2021). The phenotypic data and the environmentalinformation are also obtained from the CIMMYT dataverse for four dif-ferent nurseries: International Bread Wheat Screening Nursery(IBWSN), High Rainfall Wheat Yield Trial (HRWYT), Elite SelectionWheat Yield Trial (ESWYT) and Wheat Yield Collaboration Yield Trial(WYCYT). Here, the meaning of trial and nursery is the same, and wewill use trial to indicate both of them in the later part of this work.Each trial is located in many places. Trials are also categorized into dif-ferent mega-environments based on weather conditions such as theamount of rainfall, soil acidity, the necessity of irrigation, and altitudesof the locations. Thus locations in a trial have similar weather condi-tions. Typically lines are sown in multiple cycles, and in a cycle, thesame lines are sown in numerous locations of the same trial. The cyclesare usually numbered, such as 45th IBWNSN and 1st WYCYT, where theﬁrst part indicates the cycle number and the second part is the trial. Wecollected the data of 1st WYCYT to 6th WYCYT, 11th HRWYT to 27thHRWYT, 29th ESWYT to 36th ESWYT and 36th IBWSN to 52nd IBWSN.Although the locations of the CIMMYT wheat breeding programhave eight mega-environments, our collected data mostly falls in twoS. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
61mega-environments: mega-environment 1 and mega-environment 2.Locations in mega-environment 1 have favourable conditions forwheat breeding where rainfall is usually low and irrigation is optimal.On the other hand, locations in mega-environment 2 are high rainfallareas where precipitation occurs during the growing cycle, and irriga-tion is not needed.Table 1shows the nursery information along withtheir mega-environment information.Locations in mega-environment 1 have favourable conditions forwheat breeding where rainfall is usually low and irrigation is optimal.On the other hand, locations in mega-environment 2 are high rainfallareas where precipitation occurs during the growing cycle, and irriga-tion is not needed.Table 1shows the nursery information along withtheir mega-environment information.The lines were sown over multiple years in different locations whichwe are going to refer as a site-year (combination of locations and year).As the environment of a speciﬁc location is not constant and changeseach year, each site-year is considered a different environment.Table 2shows the number of unique locations and lines, number of cy-cles and total lines for each nursery type. From the table, we observethat IBWSN trials have the highest number of unique lines that aresown in 171 unique locations which creates 72,776 line-site-year com-binations. The quantity of line-site-year combinations of IBWSN trialsare followed by ESWYT, HRWYT and WYCYT respectively.We performed the Anderson-Darling test to check whether the dis-tributions of yield in any of the trials follow a normal distribution. Ourresult shows that none of the four trials are normally distributed asthe statistics of the Anderson-Darling test range from 224 to 11, whichis much higher than the critical values of all signi ﬁcance levels.Table 3 shows the detailed result of the test along with their signi ﬁcance levels.2.1.2. Weather dataThe weather data for each site-year is collected from the CIMMYTdataverse from 1990 to 2018 containing all locations of InternationalWheat Improvement Network (IWIN). The weather data contains 769locations along with nine weather variables of each location such asthe hourly average amount of precipitation, maximum relative humid-ity, minimum relative humidity, shortwave radiation ( MJ=m
2=d), max- imum and minimum temperature (C), maximum vapour pressuredeﬁcit (kPa), 2 m wind speed (m/s) and 10 m wind speed (m/s).Although the sowing date of crops are recorded in entirety, there aremany missing values for when the crops are harvested; hence, we con-sider nine months of environmental data as the input to the machinelearning model. The nine months period starts at least two monthsbefore the sowing date to capture the environmental effect on the soilbefore sowing (as the sowing date is available), and the followingseven months are considered as the growing season. A monthlyaverage of all the weather variables for each of the nine monthsare calculated, which provides us with 9/C29¼81 weather variables for each environment.2.1.3. Field notesField notes are obtained from the CIMMYT dataverse for each site-year. These notes are mostly unstructured text and contain a widevariety of information. Data also differs from trial cycle to trial cycle.This data contains information such as how much and which fertilizeris applied, disease development information, number of irrigations be-fore sowing, moisture available before sowing, major weed species,soil aluminum toxicity and many more. Though our aim was to collectinformation before sowing, we are unable to verify that all the informa-tion in this data is taken before sowing.2.2. Train-test SplitFrom the genotyped data, we createdﬁve different training, test and validation partitions where each set has 70% –15%–15% training, valida- tion and test split. While dividing the data, we ensure that lines that areselected for the test set in a partition are not observed in the training set.Thus the training data contains the information of the environmentswhere this 15% test will be sown as other lines are already sown inthose environments. As the model already observed the environmentof test lines in the training data, in later part of this work, we will referthis test case as environment observed scenario or test scenario one.For each previously created partitions, we also randomly samplesome locations with 85% probability that the location will be in thetraining set and 15% probability that the location is in the test set. Inthis scenario, if the location is in the test set, we did not include any ofthe site-year data for that location in the training set or validation set.Although most of the lines in the test set were sown in other site-years, the training set also does not contain the 15% lines separated fortesting in the previous step. Thus there may be some lines in thetest set that are not present in the training data. As the model did notobserve the test locations in training data, we will refer this test caseas environment unobserved scenario or test scenario two. Finally, thetraining-test partitioning strategy creates two test scenarios for eachpartition: i) no lines in the training set are also in the test set, but thetest set and training set may contain different lines grown in the sameenvironment, and ii) the training data and the test data do not containany locations in common, but the training set may contain informationon how some lines performed in other site-years.2.3. Weather data clusteringIn this work, we group site-years into clusters to identify statisticallyrelated locations and use these groups to ﬁnd group-speciﬁc important markers. To obtain the grouping, we calculated the yearly average ofeach weather variable for each location from 1990 until 2019 of allIWIN locations. We then applied hierarchical agglomerative clusteringwith the number of clustersc¼25. The output of the clustering method
Table 1Environments of each nursery. ME refers to Mega-Environment. CIMMYT has6 M-environments for Spring Wheat.IBWSN HRWYT ESWYT WYCYTRainfall Lowrainfall>500 mm LowrainfallMixedMega-Environment ME1 ME2 ME1 MixedIrrigation Optimal No Optimal No information Overall Condition Favourable Rainfall during cropping cycleFavourable MixedTable 2Nursery-type speciﬁc information of genotypes and locations.IBWSN HRWYT ESWYT WYCYTUnique Locations 171 122 216 77Unique Lines 2619 305 369 109Number of Cycles 17 14 8 6Number of Line-Site-Year Combinations 72,776 6,984 25,712 3,012
Table 3Anderson-Darling test of yield distribution. Since the statistics are larger than the criticalvalues of all signiﬁcance levels, the hypothesis that the data comes from a normal distribu-tion is rejected.Critical ValuesTrials Test Statistics 15% 10% 5% 2.5% 1%IBWSN 224.479 0.576 0.656 0.787 0.918 1.092HRWYT 193.598 0.576 0.656 0.787 0.917 1.091ESWYT 28.912 0.576 0.656 0.787 0.918 1.092WYCYT 11.149 0.576 0.656 0.787 0.917 1.091S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
62is the cluster assignment for each site-year. Finally, one of the clustercategories is assigned to a location in which the site-year combinationof that location is the most frequent. Though we clustered all IWIN loca-tions, there are 320 unique IWIN locations (1483 site-year) in four trialsfor which we identiﬁed the cluster category. The primary purpose ofclustering in our work is to use the cluster to ﬁnd environment- speciﬁc important markers. Thus we did not focus on ﬁnding the appro- priate number of clusters. We use the cluster category information of alocation to identify cluster-speciﬁc important markers of wheat foryield.2.4. Feature selectionEach marker in our dataset is represented by three values: 1, 0, −1. We applied the Hardy-Weinberg equilibrium (HW) ( Acquaah, 2009)t o each marker in the training set to obtain the genotype frequency. Eachgenotype is then replaced by one of the three quantities obtainedfrom applying HW. As we haveﬁve training sets, each marker of a linewill haveﬁve frequency values. An average of these ﬁve frequency values is used as the genotype frequency. Fig. 1shows an example of how the average frequency is calculated.Research shows that some markers contribute towards yield irre-spective of the environment (Lenz et al., 2017). Also, some speciﬁc markers contribute more in a speciﬁc condition. For instance, Lenz et al. (Lenz et al., 2017) demonstrated that selecting the top 250 previ-ously known important markers of black spruce results in the samecorrelation coefﬁcient score obtained by randomly selecting 4993markers. They also observed that selecting fewer than 500 markersrandomly decreases the correlation score between the predictedtraits and the true traits. When the important markers are notknown, previous research shows t hat feature selection methods were able to identify biologically and statistically signi ﬁcant markers for yield prediction (Jubair et al., 2021). Thus by applying feature selection, we aim to identify important markers irrespective of anyenvironment (global marker set) as well as markers that play anessential role in a speciﬁc condition (local marker set).To identify the global marker set, theﬁrst step is to calculate the av- erage yield of each line over all environments. After ﬁnding the average yield of lines, mutual information (MI) regression ( Ross, 2014)f e a t u r e selection is applied to each marker. Again, as we have ﬁve different training sets, each marker will haveﬁve different MI scores. We calcu- lated the average MI score for all the markers across ﬁve folds and then selected the top 2000 markers with the highest MI scores. Fig. 2 shows how MI is obtained for a speciﬁct r a i n i n gs e t .To obtain the local marker set, weﬁrst exclude all markers that are in the global marker set. Then an average phenotypic value is calculated
for all lines in each environmental cluster obtained previously in section2.3. For each cluster, the average MI (averaged over all training sets) foreach marker is measured and the top 100 markers were chosen for eachcluster. As the global feature sets are excluded from these markers, theexpectation is that the identiﬁed markers are more related to the envi-ronmental effect. This marker selection process selected another 2052unique markers. After combining global and local marker sets, wehave 4052 markers for our machine learning model. Fig. 3shows the procedure for obtaining the local marker set.2.5. Deep learning framework 1 (F1)We now describe ourﬁrst DL framework. This framework was de-signed to test whether genomic and environmental information canbe treated equally as data in the model, and whether genomic markerscan be scaled with environmental data to capture a marker-by-environment effect. In this framework, the ﬁrst step is to represent markers of each line with their genotype frequency, as described in sec-tion 2.4. After obtaining the genotype frequency, global and localmarker sets are obtained by applying the procedure described previ-ously, again in section 2.4. As different environmental variables havedifferent ranges of values and the genotype frequency obtained by ap-plying HW ranges between 0 and 1, environmental variables are nor-malized by applying Min-Max scaler ( Buitinck et al., 2013)t ob r i n g them in the same range of genotype frequency. The input to the deeplearning model is lines represented by 4052 markers and the corre-sponding normalized site-year environmental data. The output of thismodel is the predicted yield.Fig. 4shows the overall workﬂow of deep learning framework 1. We applied three different deep learningmodels in this framework. The major difference between the threemodels is their depth and when the environmental information is inte-grated. Details of the deep learning models are given below.2.5.1. Deep learning model 1 (F1M1)In this model, the assumption is that all the markers and weathervariables may interact with each other at the same time. The input tothe model is the concatenated vector of marker data and weather vari-ables totalling 4133 input neurons. It then contains 15 blocks of linearand ReLU layers which are followed by an output regression layer. Theoutput of the odd blocks are connected by a residual connection fromthe previous odd block to the current odd block. Fig. 5shows the archi- tecture of this model. This model is similar to the model of Khaki and
Fig. 1.Markers represented by the average genotype frequency. Here, the example is shown with three training sets. We used ﬁve training sets to calculate the genotypic frequency average of each marker.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
63Wang (Khaki and Wang, 2019) as both models employed linear layersand consider that all environmental variables and markers to interactwith each other at the same time. We will refer to this variant of theF1 framework as the F1M1 framework.2.5.2. Deep learning model 2 (F1M2)To imitate the interaction between all environment variables and amarker, theﬁrst block of this model is 4052 parallel linear layers,where the input of each linear layer is all 81 environmental variablesalong with one marker. We chose 54 neurons as the hidden neuronsfor the linear layers by experimenting with various numbers of outputneurons as they minimize the validation loss for some initial epochsquicker. A ReLU is applied on the stacked output, followed by a blockof linear and ReLU layers. A linear layer is applied as the regressionlayer at the end.Fig. 6shows the architecture of the deep learningmodel in the F1M2 framework. We will refer to this variant of the F1framework as the F1M2 framework.2.5.3. Deep learning model 3 (F1M3)The intuition of the deep learning model in the F1M3 framework isthat markers interact with each other even before they interact withthe environment. It is the result or summary of the marker interactionthat interacts with the environment. To imitate this, we constructed adeep learning model withﬁfteen blocks of neural networks. Eachblock contains a linear layer followed by a ReLU activation function.Theﬁrst linear layer of theﬁrst block is the input layer, which takes allthe selected markers as the input. The output of this layer is a 750-dimensional vector. The subsequent 11 blocks of neural networks takethe input from the previous block and again produce an output of a750 dimensional vector. A residual connection between the odd blocksof theﬁrst 12 blocks of neural networks is employed to make surethat none of the blocks suffer from the vanishing gradients problem.After theﬁrst 12 blocks of neural networks, 750 parallel linear layerswere employed where the input of each linear layer was all 81 environ-ment variables along with one of the 750 neurons from the previousblock. The outputs of each of these linear layers are 54-dimensionalvectors that are stacked together. After the parallel linear layers, anotherthree blocks of neural networks are applied with a residual connectionbetween the output of parallel neural networks and the output ofblock fourteen. After these three neural network blocks, a linear layeris applied to perform regression.Fig. 7shows the neural network archi- tecture. We will refer to this variant of the F1 framework as the F1M3framework.2.6. Deep learning framework 2 (F2)Previous research shows that deep learning models can successfullypredict traits for trials where the input is the genomic information(Zhang et al., 2019;Jubair et al., 2021;Ma et al., 2018;Gianola et al., 2011;Pérez-Rodríguez et al., 2012;González-Camacho et al., 2012, 2016;Rachmatia et al., 2017;Jubair and Domaratzki, 2019)o rw e a t h e r information (Lin et al., 2020;Shook et al., 2021;Khaki et al., 2020). In the former scenario, the predicted traits are the average of all locationsfor a speciﬁc line. In the later scenario, the predicted traits are the aver-age over all genotypes grown in a particular environment. To estimatetraits in multi-environment, statistical models such as BLUP andGBLUP try to capture the average effect of genetic information andthen add variance due to environmental changes. Some of the statisticalmethods employ dimensionality reduction techniques ( Rogers and Holland, 2021) and kernel tricks (Jarquín et al., 2014) to capture GxE. However, the memory requirements of these models increase rapidly
with the increase of lines and environments in the training set.
Fig. 2.Top global marker selection procedure. An average of phenotypes across all site-year is calculated for each line. Then, mutual information is applie d.
Fig. 3.Top local marker selection procedure (markers were not shown). Tables in the ﬁgure shows how average phenotype is calculated. 100 markers are selected from each individual cluster. There are 25 clusters in total. As some markers are common among the clusters, this leads to 2052 unique local markers.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
64In our proposed architecture, weﬁrst learn representations of geno- typed data and environmental variables separately. These two modelsare optimized to predict the average yield over environments and geno-types, respectively. We then concatenate these two representations andpredict the environment-speciﬁc yield for a speciﬁc line assuming that markers and environments work as two groups and one group has aneffect on another group for environment speci ﬁc yield prediction of each line.Fig. 8shows the proposed deep learning framework 2. This deeplearning framework is based on one deep regression model and twodeep representation learning models: i) optimized for an averageyield of a line over all environments (line-speci ﬁc average yield) and ii) optimized for predicting average yield over all lines for an environ-ment (environment-speciﬁc average yield). Theﬁrst step for predicting line-speciﬁc average yield is to identify the global marker set by apply-ing the procedure described in section 2.4. After obtaining the globalmarker set, a neural network model is trained with this marker set asfeatures to predict the line-speciﬁc average yield. The last layer beforethe regression layer of this neural network model produces a 256-dimensional representation vector for each line which is one of the in-puts to the deep regression model.The input to the second representation learning model optimized forpredicting environment-speciﬁc average yield is nine months of envi-ronmental variables for each site-year. This model produces a represen-tation of a 54-dimensional vector for each input for the environmentalvariables. After training and testing these two models, for each site-year, the representation vectors of these two models are concatenated,which serve as the input to the deep regression model that predictsyield for each site-year. This framework will be referred to as F2. Now,we describe the architecture of each of the models of F2 individually.2.6.1. Representation learning model optimized for predicting line-speci ﬁc average yieldFig. 9shows the representation learning model that predicts the av-erage yield over environments. The input to this model is a line repre-sented by marker frequency. Each block of neural networks contains a
Fig. 4.Deep learning framework 1 work ﬂow. In this workﬂow, we employed three different deep learning models.
Fig. 5.Architecture of the deep learning model in the F1M1 framework. This model concatenates the marker and weather variables and passed to a linear neural n etwork block that con- tains a linear and ReLU layer. The architecture of this model is similar to the model of ( Khaki and Wang, 2019).S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
65linear layer, a leaky ReLU activation function and a dropout layer. Thehidden layer of the liner layer contains 2000 neurons in the ﬁrstﬁve blocks of the model. The last three blocks of neural networks have666, 444 and 296 hidden nodes. All leaky ReLU layers have the sameslope of 0.1 for the negative values. Theﬁrstﬁve dropout layers have the probability of 0.5 to drop a neuron. The next two dropout layershave the probability of 0.4, and the last dropout layer has the probabilityof 0.2 to drop a neuron. The last layer is the regression layer that predictsaverage yield across environments.2.6.2. Representation learning model optimized for predictingenvironment-speciﬁc average yieldFig. 10shows the representation learning model that predicts aver-age yield over lines. The input to this model is the environmental vari-ables of nine months normalized by a min-max scaler. There are fourblocks of neural networks where each block contains a linear layer, aReLU activation function and a dropout layer. Each block produces anoutput of 54 hidden neurons with a dropout probability of 0.25. Thelast layer is the regression layer that predicts the average yield for anenvironment.2.6.3. Yield prediction modelFig. 11shows the yield prediction model. This is a shallow modelthat contains three blocks of neural networks. Each block has a linearlayer and a ReLU activation function. The last layer is the regressionlayer that predicts yield for a speciﬁc line in a speciﬁc environment.2.7. Deep learning framework 3 (F3)None of the previous two frameworks contain any informationabout theﬁeld management and soil information. In this framework,we integrated unstructured text data which may provide informationabout management andﬁeld conditions.Fig. 12shows framework 3. This framework is an extension of the F2.The major difference between the two frameworks is that an agri-bertmodel (Rezayi et al., 2022) is employed to obtain a representation ofsoil and environment-related text. A 768-dimensional representationis obtained for all texts and then an average representation is calculatedfor each site-year. The length of all individual notes are <256 tokens. Asthe maximum length of texts of the agribert model is 512 tokens, it canobtain the representation of the full note. This 768-dimensional vectoris also concatenated with the output of two representation learningmodels and provided as the input to the shallow model.2.8. General settings of deep learning modelsAll the models are optimized using the Adam optimizer with a learn-ing rate of 1e
/C05. Mean square error is applied as the loss function. Train-ing is stopped when there is no improvement in PCC for the validationset in at least 30 consecutive epochs.3. Results3.1. Environment data clusterTo understand how similar the locations are in the same cluster, weapplied TSNE on the yearly average of weather variables to reduce the
Fig. 6.Architecture of deep learning model in the F1M2 framework.
Fig. 7.Architecture of the deep learning model in the F1M3 framework. Output of each oddnumber of ReLU layer is connected through a residual connection.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
66dimension to a three-dimensional space of each site-year. We then labeleach site-year by its nursery and cluster assignment to visualize howseparable the site-years are in the three-dimensional space. Fig. 13 shows how different trials and clusters are mapped in a three-dimensional space. From theﬁgure on the left (site-year weather databy nursery), we observe that the trials are not well separable fromeach other in a three-dimensional space though they are createdbased on similar environments. On the other hand, from ﬁgure on the right (site-year weather data by cluster group), we observe that thegroupings created by the clustering algorithm have better separabilitythan the nursery-based grouping. As it is very dif ﬁcult to understand theﬁgure with this large number of clusters, we also generated interac-tiveﬁgures for both.
1
3.2. Effect of adding environmental variablesWe experimented with three deep learning models in deep learningframework 1 (F1M1, F1M2 and F1M3), where these models differ pri-marily on how the environment and genotype interactions are cap-tured within the model. In the F1M1, the assumption was that allgenomic and environmental factors interact with each other at thesame time. On the other hand, in the F1M2, the assumption is thateach marker separately interacts with the environment ﬁrst, and the resulting outcome then affects yield. Finally, in the F1M3, the re-lationship between markers isﬁrst taken into account, and then theenvironment interacts with the combined marker relationship to es-timate yield.During training, as we haveﬁve folds, we trained and tested theF1M1 and F1M3 for all folds. However, after training the ﬁrst fold, we identiﬁed that the F1M2 signiﬁcantly overﬁts as we obtained alow Pearson Correlation Coefﬁcient (PCC) on test and validationdata (≈0:11) and a high PCC (≈0:8) on training data. Furthermore,as 4052 parallel fully connected neural networks are employed justafter the input layer, the trainable parameters and training timeare also very high for this F1M2 model (around 1.5 h per epoch onNVIDIA GTX 1080). Thus, we did not train the F1M2 for the rest ofthe folds.Table 4shows the F1M1 and F1M3 comparison on the test sets. Fromthe table, we observe that models in F1 that consider genetic interactionﬁrst and then capture GxE (F1M3) perform better on the following twotest case scenarios: i) when environments are observed, but lines arenot (test scenario one) and ii) when lines may be observed, but loca-tions are not (test scenario two). Models in F1M3 are 1.62 to 2.05times better than F1M1 counterparts on PCC score for differentfolds of test scenario one. Models in F1M3 also outperform modelsin F1M1 in the second test scenario, with PCC scores 1.35 to 1.82times higher than F1M1 on different folds. However, the standarddeviation of the F1M1 is lower than the F1M3, which indicates thatmodels in F1M1 have less variations (more stable performance)across folds. This result is consistent with other research where itis observed that deep learning models that allow interactionsbetween datatypes perform better than the models that do notconsider these interactions (Sharma et al., 2022;Kick et al., 2023; Jarquin et al., 2021).3.3. Effect of global markers vs global + local markers in F1M3To understand the effect of the global and local marker sets, in thissection, we present the results obtained when our F1M3 framework istrained with the global marker set only. Thus the input to this F1M3framework is 2000 markers instead of the 4052 markers. The rest ofthe architecture is the same as the previous F1M3. Fig. 14shows the per- formance of the two models in the F1M3 on two test scenarios. From theﬁgure, we observe that although models in F1M3 trained only on theglobal marker set have higher PCC values in four folds out of ﬁve on
Fig. 8.Deep learning framework 2 (F2).
1Seehttps://htmlpreview.github.io/?https://github.com/sheikhjubair/GxENet/blob/main/ﬁgures/cluster_label.htmlandhttps://htmlpreview.github.io/?https://github.com/ sheikhjubair/GxENet/blob/main/ﬁgures/trial_cluster.html.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
67test scenario one, we observe the opposite outcome for test scenariotwo. The average PCC value of test scenario one trained with the globalmarker set is 0.729, which is 1.7% higher than the model trained on theglobal + local marker set. However, the average PCC value of test sce-nario two trained with global marker set is 0.381, which is 2.2% lowerthan the models trained on the global + local marker set. We also con-ducted at-test on the PCC scores for both test scenarios. The p-value of test scenario one is 0.543 and test scenario two is 0.524 which indicatesthat the PCC score of the two models has identical variance and there isnot a statistically signiﬁcant difference between the two ways of train-ing the model. Overall, the results showed that the effect of theenvironment-speciﬁc markers on the models is minimal for predictingyield. However, local markers improve PCC when the model does notobserve the locations.3.4. Performance of the F1M3 vs the F2Our F2 framework is the combination of three different deep learn-ing models. Theﬁrst representation learning model is a deep learningmodel that predicts line-speciﬁc average yield.Table 5shows the PCC scores acrossﬁve-fold for both test scenarios. The average PCC for testscenario one, where the model knows environments is 0.606. However,when the environments were not observed, the average PCC goes downto 0.167. As the input to this model is genotyped data and the output isaverage yield, this model supports the observation of other recent re-search which is models that do not incorporate environmental informa-tion are not suitable for predicting top lines for a new environment
Fig. 9.Representation learning model for predicting line-speci ﬁc average yield.
Fig. 10.Representation learning model for predicting environment-speci ﬁc average yield.
Fig. 11.Yield prediction model for predicting line speci ﬁc yield for each site-year.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
68(Washburn et al., 2021;Lin et al., 2020;Khaki and Wang, 2019; Montesinos-López et al., 2019).The objective of the second representation learning model is to cap-ture the environmental effect on yield by estimating the average yieldover genotypes for an environment. Table 6shows PCC scores across ﬁve folds for both test scenarios. From the table, we observe thatalthough the average PCC in the observed environment is higher thanin the second test scenario, the performance in the second test scenariois also satisfactory as PCC score 0.518 indicates that there exists somelinear relationship between the target and predicted average yield.The yield prediction model of the F2 framework estimates theenvironment-speciﬁc yield of a speciﬁc line by combining the
Fig. 12.Deep learning framework 3 (F3).
Fig. 13.Site-year weather data by nursery (left) and cluster group (right). Each point in the ﬁgure indicates a site-year. Different colors either indicate a nursery (left) or a cluster group (right).S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
69representation learnt from the previous two models. Fig. 15shows the PCC scores forﬁve folds on both test scenarios. The average PCC scoreof the yield prediction model of the F2 framework in test scenario oneis 0.734, while the average PCC score of F1M3 was 0.712, indicating2.2% improvement over F1M3 framework. Although the average PCCscore of the yield prediction model of the F2 framework for test scenarioone is higher, we observe that the models in F1M3 have higher PCCscores in two folds out ofﬁve, indicating no clear advantage of usingone framework over another. However, in test scenario two, yield pre-diction models of the F2 have higher PCC scores in all ﬁve folds, with an average PCC of 0.454, which is 5.18% improvement over the F1M3.The result demonstrates that F2 shows improvement when comparedto all the variants of the F1.In addition, we conducted at-test on the results of the F1M3 and F2frameworks. Thep-value obtained from the t-test of test scenario one is0.398 and test scenario two is 0.164, which indicates that the PCC scoresof the two models have identical variance and there is no statisticallysigniﬁcant difference between the two results of the two frameworks.However, a 5.18% improvement in the average PCC score of the F2framework over the F1M3 framework on test scenario two is a notableimprovement. Moreover, the F2 framework needs signi ﬁcantly less training time, as described in section 3.7.In all the reported results above, we calculated a global PCC scorewhere we considered all cycles of all trials in the test set of a speci ﬁc fold. As PCC measures the linear relationship between actual yield andpredicted yield, global PCC score and cycle-speci ﬁc PCC score of a trial may vary. Thus, we calculated cycle-speci ﬁc PCC scores to understand how our models perform for different cycles of a trial. As there are lotsof cycles and trials combinations, to present the result, we divided thePCC score into three ranges: PCC≤0 indicates the model did not learnanything, 0 < PCC≤0:4 indicates the performance of the model is ran-dom andﬁnally, PCC>0:4 is our desirable range which indicates thereexist some linear relationship between predicted yield and true yield.Table 7shows the result for our two test scenarios. From the table, weobserve that yield prediction models of the F2 framework have a highernumber of cycles with PCC scores >0:4 compared to the models of F1M3. On average, there are 30.2 cycles in the ﬁr s tt e s ts c e n a r i oa c r o s s ﬁve-folds, and the F2 framework has 29.2 cycles with PCC >0:4. In test scenario two, the average number of cycles in the F2 frameworkwith PCC>0:4 is 5.4 while the average number of cycles in each fold is6.6. We also observe that eight unique cycles have PCC < 0 :4i nF 2 , with 51st IBWSN cycle having the most frequent appearance. Whileexperimenting with the F1M3 framework, we identi ﬁed that 11 unique cycles have low PCC scores (< 0:4), and again 51st IBWSN cycle is themost frequent acrossﬁ
ve-fold. Five cycles with low PCC in the F2 frame-work are also present in F1M3. There is no speci ﬁct y p eo ft r i a l si nt h e cycles that have low performance as the low-performance cyclesoccur in all trials except WYCYT.Table 4Comparison of PCC overﬁve folds between F1M1 and F1M3 for test scenario one and test scenario two.Folds F1M3(test scenario one) F1M1(test scenario one) F1M3/F1M1 F1M3(test scenario two) F1M1(test scenario two) F1M3/F1M11 0.697 0.375 1.85 0.359 0.257 1.392 0.759 0.372 2.04 0.382 0.257 1.483 0.690 0.354 1.94 0.470 0.258 1.824 0.740 0.360 2.05 0.353 0.260 1.355 0.677 0.417 1.62 0.452 0.258 1.75Average 0.712 0.375 1.89 0.403 0.258 1.56Std 0.031 0.022 0.041 0.001
Fig. 14.Comparison of PCC scores of F1M3 models trained on global markers and global + environment speci ﬁc markers and evaluated on the test scenario one (left) and test scenario two (right).Table 5PCC scores acrossﬁve folds of the representation learning model of the F2 framework forpredicting line speciﬁc average yield.Folds Test Scenario One Test Scenario Two1 0.549 0.0302 0.691 0.0703 0.594 0.1604 0.578 0.2985 0.601 0.281Average 0.606 0.167S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
703.5. Feature importanceToﬁnd the feature importance of environmental variables, we em-ployed DeepLift (Shrikumar et al., 2017) on the representation learning model optimized for predicting environment-speci ﬁc average yield(one of the components of F2). DeepLift measures how the informationis propagated along the network and assigns importance scores (re-ferred to as the attribution scores) to input variables. We used theCaptum library (Kokhlikyan et al., 2020) for DeepLift implementation and employed the default parameters to the DeepLift model. A positiveattribution score for an environmental variable means it is positivelyinﬂuencing the prediction, while a negative attribution score meansthe opposite.Fig. 16shows the importance of each feature where each bar is anenvironmental variable. The variables are in the following order foreach month: (1) precipitation, (2) maximum relative humidity, (3) min-imum relative humidity, (4) shortwave radiation, (5) maximum tem-perature, (6) minimum temperature, (7) maximum vapour pressuredeﬁcit, (8) wind speed 2 m, (9) wind speed 10 m. The ﬁgure shows that maximum temperature has a positive effect in the ﬁrst two months. It is worth mentioning that theﬁrst two months in our dataset areTable 6PCC scores acrossﬁve fold of the representation learning model of the F2 framework thatpredicts environment speciﬁc average yield.Folds Test Scenario One Test Scenario Two1 0.786 0.5032 0.844 0.5643 0.797 0.4804 0.855 0.5365 0.837 0.507Average 0.823 0.518
Fig. 15.Comparison of PCC scores between F2 and F1M3 on test scenario one (left) and test scenario two (right).
Table 7Number of cycles in each ranges of PCC for both test scenarios.Folds PCC Range Test Scenario One Test Scenario TwoNumber ofCycles (F1M3)Number ofCycles (F2)Number ofCycles (F1M3)Number ofCycles (F2)1 PCC≤00 1 0 00 < PCC≤0 . 4 20 32PCC>0.4 27 28 3 4Number of line and site-year combinations 12,543 22,9962 PCC≤01 1 0 00 < PCC≤0 . 4 10 20PCC>0.4 31 32 5 7Number of line and site-year combinations 14,915 15,8843 PCC≤01 0 1 10 < PCC≤0 . 4 01 21PCC>0.4 28 28 7 8Number of line and site-year combinations 14,094 15,8844 PCC≤00 0 0 00 < PCC≤0 . 4 31 21PCC>0.4 30 32 2 3Number of line and site-year combinations 12,792 24,3125 PCC≤01 0 0 00 < PCC≤0 . 4 11 11PCC>0.4 25 26 5 5Number of line and site-year combinations 12,684 20,400Average PCC≤0 0.6 0.4 0.2 0.20 < PCC≤0.4 1.4 0.6 2 1PCC>0.4 28.2 29.2 4.4 5.4S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
71before sowing crops. In the third month, maximum vapour pressurecontributes more than any other environmental variables for predictingaverage yield. We observe this trend for maximum vapour pressure forthe rest of the months of the growing cycle except the ﬁfth month, where shortwave radiation contributes more than the vapour pressure.The effect of shortwave radiation increases signi ﬁcantly in the fourth andﬁfth months, and then goes down gradually. In the third month,the importance of precipitation and maximum vapour pressure in-creases as we enter the months when seeds are sown. The maximumtemperature has a continuous positive effect from the ﬁfth month to the end of the growing cycle. Both wind speed variables have little tono impact from the third month to the end.Previous research has shown that precipitation plays an importantrole, especially in the early month of the crop-growing season(Washburn et al., 2021;Måløy et al., 2021). However, in our work, we observe less impact of rainfall. The main reason is that about 90% of ob-servations in our dataset (101,777 out of 111,836 observations combin-ing training, test and validation set) are from IBWSN and ESWYTnurseries where rainfall is low and the land receives optimal irrigation.On the other hand, only 6.2% observations (6961 observations) are fromHRWYT nurseries, where the crop gets rainfall during the croppingcycle. Since the environmental condition of the WYCYT nursery ismixed, we can not verify the amount of rainfall those nurseries received.However, WYCYT nurseries are only contributing to about 3.8% of thedata.To understand the effect of genomic information and environmentalvariables for predicting environment-speci ﬁc yield of each line, we em- ployed DeepLift in theﬁnal model of the F2 framework that combinesthe output of two representation learned models. Fig. 17shows the attribution scores of representation learned features for both testscenarios. From theﬁgure, we observe that although the numberof learned features of the environmental representation is lessthan the marker representation, they contribute more towardsenvironment-speciﬁc yield estimation for a line. However, manymarker representation features have a small positive effect on theoutcome. This observation aligns with other research that showsthat yield is a complex trait and lots of small effect genes contributeto yield (Rogers et al., 2021).
Fig. 16.Feature importance of weather variables obtained by employing DeepLift on environment speci ﬁc average yield model of F2. The left ﬁgure is the test scenario one and the right ﬁgure is the test scenario two. Each bar in the ﬁgure represents an environmental variable. The variables are in the following order for each month: (1) precipitation, (2) maximum relative humidity, (3) minimum relative humidity, (4) shortwave radiation, (5) maximum temperature, (6) minimum temperature, (7) maximum vapour pressure d eﬁcit, (8) wind speed 2 m, (9) wind speed 10 m.
Fig. 17.Feature importance of representation learning features obtained by employing DeepLift on the ﬁnal deep learning model of F2 that combines marker representation and environ- ment variable representation to predict environment speci ﬁc yield for a line.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
72In addition, we calculated the sum of the absolute attribution scoresof all neurons for marker and weather feature representations of testscenario one. The results are 0.261 and 0.318 for marker and weatherrepresentations, respectively. However, when the sum of positive attribu-tion scores is calculated, the scores drop to 0.156 and 0.299 for markerand weather representations, respectively. Moreover, the overall sum ofattribution scores of all neurons for each feature representation is positive(0.05 and 0.28 for marker and weather representations, respectively).Furthermore, 57.09% neurons from marker representation have a positiveimpact, while 88.88% neurons from environmental representation have apositive impact for predicting yield. Overall, these observations and theFig. 17demonstrate the importance of adding environmental featuresfor the genomic prediction task.3.6. Performance of the F3 architectureSince the F3 model is the extension of the F2 and we could not verifythat all the text information for a speciﬁc environment can be obtained or predicted before sowing crops, we did not make any comparisonof the F3 with models other than F2; rather, we presented it todemonstrate how other information such as text data or soil data canbe incorporated in the F2 architecture. However, as these text data aremostlyﬁeld management data collected before and during the growingseason, this architecture may play a vital role in selecting superior linesfor the next growing cycle if there are many similarities in ﬁeld manage- ment among the growing seasons of a speci ﬁcl o c a t i o n . Fig. 18shows the PCC scores of the F3 architecture. The average PCCof F3 is 0.741 for test scenario one and 0.467 for test scenario two. The F3model performs slightly better than the F2 in four folds in test scenarioone and two folds in test scenario two. We also conducted a t-test on the PCC scores for both test scenarios. The p-value of test scenario one is 0.684 and test scenario two is 0.615 which indicates that the PCCscore of the two models has identical variance and there is not astatistically signiﬁcant difference between the two results of the twoframeworks.We also employed DeepLift on the F3 architecture to ﬁnd out the text feature importance.Fig. 19shows the attribution score of eachinput neuron to theﬁnal deep learning model of the F3 architecture.Theﬁgure shows that environment representation has the most sig-niﬁcant impact on yield prediction. While 94.25% of the marker
Fig. 18.Comparison of PCC scores between F3 and F2 frameworks.
Fig. 19.Feature importance of representation learning features obtained by employing DeepLift on the yield prediction model of the F3 that combines marker r epresentation, environment variable representation andﬁeld notes representation to predict environment speci ﬁc yield for a line.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
73representation neurons and 98.14% of the environmental neuronshave a positive impact, only 26.11% of the text representationneurons have a positive impact on environment-speci ﬁc yield esti- mation. Moreover, the sum of all attribution scores for each representa-tion type shows that the environment and genotype have a positivesum (0.237 and 0.119, respectively) and the text representation has anegative sum (−0.144). However, when only the positive attributionscores are calculated for each of the representations, the text representa-tion's sum of attribution scores is higher than the sum of marker repre-sentation's atrribution scores(0.141 and 0.122, respectively).These observations show that the text data we had access to have alimited impact on our prediction task. However, since these text dataare very short text and heterogeneous, more detailed and informativetext data potentially could improve the model performance or havemore positive inﬂuence in the prediction. In addition, the text repre-sentation is learnt from a BERT-based model known as agriBERT,which is primarily trained on agricultural journal papers. Since ourtexts areﬁeld notes, a BERT model trained onﬁe l dn o t e sw o u l db e more suitable. These demonstrate the challenges in incorporatinghighly heterogenous text into deep learning models, especiallywhere missing data may be present and differences in data collectioncan be expected. We expect that future work may improve the abilityto use management data in prediction. Moreover, since we can notverify that all the unstructured texts were before sowing season,this F3 model is a demonstration of how this textual informationcan be integrated with genomic and environmental data when anappropriate dataset is available.3.7. GPU utilization and training timeFor training these models, two GPUs are used: i) NVIDIA RTX A6000and ii) NVIDIA GeForce GTX 1080. Training time depends on which GPUwe are using. Overall, training time is much less in NVIDIA RTX A6000GPU as this is faster than the NVIDIA GTX 1080. Overall, the trainingtimes of models in F1M2 are higher than any other models as they aremuch larger in terms of parameters. Deep learning models in F2 andF3 frameworks are faster to train than all other models. To comparethe training time of each model, we run them on NVIDIA RTX A6000GPU.Table 8shows the time required to train different models for oneepoch.Since the line speciﬁc average yield and environment speciﬁca v e r - age yield models of the F2 and F3 frameworks are independent ofeach other, they can be trained in parallel, which reduces the overalltraining time of the F2 and F3 frameworks. Moreover, each epoch ofthese two models requires signiﬁcantly less time compared to othermodels. Furthermore, each epoch of the yield prediction models of theF2 and F3 frameworks needs approximately 3.05 s and 5.51 s, which isless than the simple F1M1 framework. Overall, the simplicity in the ar-chitecture of the models in the F2 and F3 frameworks helps them trainfaster than the other frameworks.4. ConclusionIn this work, we proposed three novel deep learning frameworkswhere the neural network models vary mostly on how environmentalinformation is incorporated into the model. These models are curatedto incorporate GxE. Among three models, we identi ﬁed that the frame- work which employs two representation learning models optimized forpredicting line speciﬁc average yield and environment speciﬁc average yield and then combines these two representations to estimateenvironment-speciﬁc yield for a speciﬁc line, is slightly better than the others. This framework shows 1.95 to 1.75 times better performance,depending on the test scenario, than some existing deep learningmodels. Later, we extend the F2 framework by integrating text datafromﬁeld notes.In this dataset, we do not have any information on the soil. As someresearch shows that soil plays a vital role in yield ( Washburn et al., 2021), adding soil information to the model may help estimate yieldmore accurately. We showed that our F2 framework could easily be ex-tended by adding new information as we extended the F2 frameworkby addingﬁeld notes. While devising these frameworks, we assumed
that the weather of the growing season can be predicted ahead oftime. Thus our models focus on only estimating the traits of the cropsby using a representation of the weather during the growing season. Fu-ture work should incorporate weather prediction for the growing sea-son from historical weather. While obtaining the representation of theweather variable, the input to the model was the monthly average ofweather variables. Future work should also try to determine the effectof incorporating weekly or daily weather variables as the input to themodel.Finally, our F2 framework performs well in two test scenarios wheretheﬁrst test scenario is more straightforward to predict than the secondone. In theﬁrst test scenario, we predicted yield in a scenario where en-vironments are observed, but lines are not observed in any of the envi-ronments during the training of the model. In the second scenario,locations in test sets are not observed but the model may observelines during training. All the models have better performance in theﬁrst test scenario compared to the second one. The result is understand-able as the attribution score obtained by employing DeepLift shows thatweather variables play a signiﬁcant role in estimating yield.Data and code availabilityAll data used in this work can be downloaded from the CIMMYTwebsite (Poland et al., 2021;CIMMYT, 2022). The codes can be downloaded from GitHub:https://github.com/sheikhjubair/GxENet .FundingThis research did not receive any speciﬁc grant from funding agen- cies in the public, commercial, or not-for-pro ﬁts e c t o r s .CRediT authorship contribution statementSheikh Jubair:Conceptualization, Methodology, Software, Valida-tion, Formal analysis, Investigation, Writing –original draft, Writing– review & editing.Olivier Tremblay-Savard:Writing–review & editing.Mike Domaratzki:Writing–review & editing, Supervision, Conceptualization.Declaration of Competing InterestThe authors declare that the research was conducted in the absenceof any commercial orﬁnancial relationships that could be construed as apotential conﬂict of interest.Table 8Time required to train different models on NVIDIA RTX A6000 GPU.Model Average Time for One Epoch (seconds)Approximate Numberof Epochs for TrainingF1M1 7.8 170F1M2 3262.36 40F1M3 280.26 100Line Speciﬁc Average Yield(F2 and F3) 0.4 500Environment Speciﬁc AverageYield (F2 and F3) 0.08 6000Yield Prediction Model (F2) 3.05 350Yield Prediction Model (F3) 5.51 300S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
74References
Acquaah, G., 2009.Principles of Plant Genetics and Breeding. John Wiley & Sons.Boudhrioua, C., Bastien, M., Torkamaneh, D., Belzile, F., 2020. Genome-wide associationmapping of sclerotinia sclerotiorum resistance in soybean using whole-genomeresequencing data. BMC Plant Biol. 20, 195. https://doi.org/10.1186/s12870-020- 02401-8.Buitinck, L., Louppe, G., Blondel, M., Pedregosa, F., Mueller, A., Grisel, O., Niculae, V.,Prettenhofer, P., Gramfort, A., Grobler, J., Layton, R., VanderPlas, J., Joly, A., Holt, B.,Varoquaux, G., 2013.API design for machine learning software: experiences fromthe scikit-learn project. ECML PKDD Workshop: Languages for Data Mining and Ma-chine Learning, pp. 108 –122.CIMMYT, 2022. CIMMYT research data and software repository network. https://data. cimmyt.org/accessed 2022-08-04.Crossa, J., Jarquín, D., Franco, J., Pérez-Rodríguez, P., Burgueño, J., Saint-Pierre, C., Vikram,P., Sansaloni, C., Petroli, C., Akdemir, D., Sneller, C., Reynolds, M., Tattaris, M., Payne, T.,Guzman, C., Peña, R.J., Wenzl, P., Singh, S., 2016. Genomic prediction of gene bankwheat landraces. G3 Genes|Genomes|Genetics 6, 1819 –1834.https://doi.org/10. 1534/g3.116.029637.Esposito, S., Carputo, D., Cardi, T., Tripodi, P., 2020. Applications and trends of machinelearning in genomics and phenomics for next-generation breeding. Plants 9.https://doi.org/10.3390/plants9010034 . F A O ,2 0 2 2 .2 0 2 2G l o b a lR e p o r to nF o o dC r i s e s . https://www.fao.org/documents/card/en/ c/cb9997en/accessed 2022-08-04.Gianola, D., Okut, H., Weigel, K.A., Rosa, G.J., 2011. Predicting complex quantitative traitswith bayesian neural networks: a case study with Jersey cows and wheat. BMCGenet. 12, 87. URL:https://doi.org/10.1186/1471-2156-12-87 . González-Camacho, J., de Los Campos, G., Pérez, P., Gianola, D., Cairns, J., Mahuku, G., Babu,R., Crossa, J., 2012. Genome-enabled prediction of genetic values using radial basisfunction neural networks. Theor. Appl. Genet. 125, 759 –771. URL:https://doi.org/ 10.1007/s00122-012-1868-9. González-Camacho, J.M., Crossa, J., Pérez-Rodríguez, P., Ornella, L., Gianola, D., 2016.Genome-enabled prediction using probabilistic neural network classi ﬁers. BMC Genomics 17, 208. URL:https://doi.org/10.1186/s12864-016-2553-1 . Guo, J., Khan, J., Pradhan, S., Shahi, D., Khan, N., Avci, M., Mcbreen, J., Harrison, S., Brown-Guedira, G., Murphy, J.P., Johnson, J., Mergoum, M., Esten Mason, R., Ibrahim, A.M.H.,Sutton, R., Griffey, C., Babar, M.A., 2020. Multi-trait genomic prediction of yield-related traits in us soft wheat under variable water regimes. Genes 11. https://doi. org/10.3390/genes11111270. Jarquín, D., Crossa, J., Lacaze, X., Du Cheyron, P., Daucourt, J., Lorgeou, J., Piraux, F.,Guerreiro, L., Pérez, P., Calus, M., Burgueño, J., de los Campos, G., 2014. A reactionnorm model for genomic selection using high-dimensional genomic and environ-mental data. Theor. Appl. Genet. 127, 595 –607. URL:https://doi.org/10.1007/ s00122-013-2243-1.Jarquin, D., de Leon, N., Romay, C., Bohn, M., Buckler, E.S., Ciampitti, I., Edwards, J., Ertl, D.,Flint-Garcia, S., Gore, M.A., Graham, C., Hirsch, C.N., Holland, J.B., Hooker, D., Kaeppler,S.M., Knoll, J., Lee, E.C., Lawrence-Dill, C.J., Lynch, J.P., Moose, S.P., Murray, S.C., Nelson,R., Rocheford, T., Schnable, J.C., Schnable, P.S., Smith, M., Springer, N., Thomison, P.,Tuinstra, M., Wisser, R.J., Xu, W., Yu, J., Lorenz, A., 2021. Utility of climatic informationvia combining ability models to improve genomic prediction for yield within the ge-nomes toﬁelds maize project. Front. Genet. 11. https://doi.org/10.3389/fgene.2020. 592769.Jubair, S., Domaratzki, M., 2019. Ensemble supervised learning for genomic selection.2019 IEEE International Conference on Bioinformatics and Biomedicine (BIBM),pp. 1993–2000https://doi.org/10.1109/BIBM47256.2019.8982998 . Jubair, S., Domaratzki, M., 2023. Crop genomic selection with deep learning and environ-mental data: a survey. Front. Artif. Intell. 5. https://doi.org/10.3389/frai.2022. 1040295.Jubair, S., Tucker, J.R., Henderson, N., Hiebert, C.W., Badea, A., Domaratzki, M., Fernando,W.G.D., 2021. Gptransformer: a transformer-based deep learning method forpredicting fusarium related traits in barley. Front. Plant Sci. 12. https://doi.org/10. 3389/fpls.2021.761402.
Juliana, P., Singh, R.P., Singh, P.K., Crossa, J., Huerta-Espino, J., Lan, C., Bhavani, S., Rutkoski,J.E., Poland, J.A., Bergstrom, G.C., et al., 2017. Genomic and pedigree-based predictionfor leaf, stem, and stripe rust resistance in wheat. Theor. Appl. Genet. 130,1415–1430. URL:https://doi.org/10.1007/s00122-017-2897-1 . Kakaei, H., Nourmoradi, H., Bakhtiyari, S., Jalilian, M., Mirzaei, A., 2022. Effect of covid-19 on food security, hunger, and food crisis. COVID-19 and the Sustainable DevelopmentGoals. Elsevier, pp. 3–29.Khaki, S., Wang, L., 2019. Crop yield prediction using deep neural networks. Front. PlantSci. 10.https://doi.org/10.3389/fpls.2019.00621 . Khaki, S., Wang, L., Archontoulis, S.V., 2020. A cnn-rnn framework for crop yield predic-tion. Front. Plant Sci. 10.https://doi.org/10.3389/fpls.2019.01750 . Kick, D.R., Wallace, J.G., Schnable, J.C., Kolkman, J.M., Alaca, B., Beissinger, T.M., Edwards, J.,Ertl, D., Flint-Garcia, S., Gage, J.L., Hirsch, C.N., Knoll, J.E., de Leon, N., Lima, D.C.,Moreta, D.E., Singh, M.P., Thompson, A., Weldekidan, T., Washburn, J.D., 2023. Yieldprediction through integration of genetic, environment, and management datathrough deep learning. G3 Genes|Genomes|Genetics 13. https://doi.org/10.1093/ g3journal/jkad006.U R L :https://academic.oup.com/g3journal/article-pdf/13/4/jkad006/49809187/jkad006.pdf . Kokhlikyan, N., Miglani, V., Martin, M., Wang, E., Alsallakh, B., Reynolds, J., Melnikov, A.,Kliushkina, N., Araya, C., Yan, S., Reblitz-Richardson, O., 2020. Captum: a uniﬁed and generic model interpretability library for pytorch arXiv:2009.07896.Lenz, P., Beaulieu, J., Mansﬁeld, S.D., Clément, S., Desponts, M., Bousquet, J., 2017. Factorsaffecting the accuracy of genomic selection for growth and wood quality traits in anadvanced-breeding population of black spruce (picea mariana). BMC Genomics 18,335. URL:https://doi.org/10.1186/s12864-017-3715-5 . Lin, T., Zhong, R., Wang, Y., Xu, J., Jiang, H., Xu, J., Ying, Y., Rodriguez, L., Ting, K.C., Li, H.,2020. Deepcropnet: a deep spatial-temporal learning framework for county-levelcorn yield estimation. Environ. Res. Lett. 15, 034016. URL: https://doi.org/10.1088/ 1748-9326/ab66cb.Ly, D., Huet, S., Gauffreteau, A., Rincent, R., Touzy, G., Mini, A., Jannink, J.L., Cormier, F.,Paux, E., Lafarge, S., Le Gouis, J., Charmet, G., 2018. Whole-genome prediction of reac-tion norms to environmental stress in bread wheat (triticum aestivum l.) by genomicrandom regression. Field Crop Res. 216, 32 –41. URL:https://www.sciencedirect.com/ science/article/pii/S0378429016308401 .https://doi.org/10.1016/j.fcr.2017.08.020 . Ma, W., Qiu, Z., Song, J., Li, J., Cheng, Q., Zhai, J., Ma, C., 2018. A deep convolutional neuralnetwork approach for predicting phenotypes from genotypes. Planta 248,1307–1318. URL:https://doi.org/10.1007/s00425-018-2976-9 . Måløy, H., Windju, S., Bergersen, S., Alsheikh, M., Downing, K.L., 2021. Multimodal per-formers for genomic selection and crop yield prediction. Smart Agricult. Technol. 1,100017. URL:https://www.sciencedirect.com/scienc e/article/pii/S2772375521000174 . https://doi.org/10.1016/j.atech.2021.100017 . Meuwissen, T.H.E., Hayes, B.J., Goddard, M.E., 2001. Prediction of Total genetic value usinggenome-wide dense marker maps. Genetics 157, 1819 –1829. URL:https://doi.org/ 10.1093/genetics/157.4.1819.https://academic.oup.com/genetics/article-pdf/157/4/ 1819/42032331/genetics1819.pdf . Montesinos-López, O.A., Montesinos-López, A., Tuberosa, R., Maccaferri, M., Sciara, G.,Ammar, K., Crossa, J., 2019. Multi-trait, multi-environment genomic prediction ofdurum wheat with genomic best linear unbiased predictor and deep learningmethods. Front. Plant Sci. 10.https://doi.org/10.3389/fpls.2019.01311 . Nguyen, K.L., Grondin, A., Courtois, B., Gantet, P., 2019. Next-generation sequencing accel-erates crop gene discovery. Trends Plant Sci. 24, 263 –274.https://doi.org/10.1016/j. tplants.2018.11.008.Pérez-Rodríguez, P., Gianola, D., González-Camacho, J.M., Crossa, J., Manès, Y.,Dreisigacker, S., 2012. Comparison between linear and non-parametric regressionmodels for genome-enabled prediction in wheat. G3 Genes|Genomes|Genetics 2,1595–1605. URL:https://doi.org/10.1534/g3.112.003665 .https://academic.oup.com/ g3journal/article-pdf/2/12/1595/40570733/g3journal1595.pdf . Poland, J., Dreisigacker, S., Shrestha, S., Wu, S., Singh, R., Mondal, S., Juliana, P., Crossa, J.,Basnet, B.R., Crespo, L., et al., 2021. Genotypic Data from Cimmyt Bread Wheat Breed-ing Lines Used in the Feed the Future Innovation Lab for Applied Wheat Genomics.URL:https://hdl.handle.net/11529/10695 . Rachmatia, H., Kusuma, W.A., Hasibuan, L.S., 2017. Prediction of maize phenotype basedon whole-genome single nucleotide polymorphisms using deep belief networks.J. Phys. Conf. Ser. 835, 012003. URL: https://doi.org/10.1088/1742-6596/835/1/ 012003.Rezayi, S., Liu, Z., Wu, Z., Dhakal, C., Ge, B., Zhen, C., Liu, T., Li, S., 2022. Agribert:knowledge-infused agricultural language models for matching food and nutrition.In: Raedt, L.D. (Ed.), Proceedings of the Thirty-First International Joint Conferenceon Artiﬁcial Intelligence, IJCAI-22, International Joint Conferences on Arti ﬁcial Intelli- gence Organization , pp. 5150 –5156. URL:10.24963/ijcai.2022/715. Rogers, A.R., Holland, J.B., 2021. Environment-speci ﬁc genomic prediction ability in maize using environmental covariates depends on environmental similarity to trainingdata. G3 Genes|Genomes|Genetics 12. https://doi.org/10.1093/g3journal/jkab440 . https://academic.oup.com/g3journal/article-pdf/12/2/jkab440/45932746/jkab440.pdf.Rogers, A.R., Dunne, J.C., Romay, C., Bohn, M., Buckler, E.S., Ciampitti, I.A., Edwards, J., Ertl,D., Flint-Garcia, S., Gore, M.A., Graham, C., Hirsch, C.N., Hood, E., Hooker, D.C., Knoll, J.,Lee, E.C., Lorenz, A., Lynch, J.P., McKay, J., Moose, S.P., Murray, S.C., Nelson, R.,Rocheford, T., Schnable, J.C., Schnable, P.S., Sekhon, R., Singh, M., Smith, M.,Springer, N., Thelen, K., Thomison, P., Thompson, A., Tuinstra, M., Wallace, J., Wisser,R.J., Xu, W., Gilmour, A.R., Kaeppler, S.M., De Leon, N., Holland, J.B., 2021. The impor-tance of dominance and genotype-by-environment interactions on grain yield varia-tion in a large-scale public cooperative maize experiment. G3 Genes|Genomes|Genetics 11.https://academic.oup.com/g3journal/article-pdf/11/2/jkaa050/37042012/jkaa050.pdf.Ross, B.C., 2014. Mutual information between discrete and continuous data sets. PLoS One9, 1–5. URL:https://doi.org/10.1371/journal.pone.0087357 . Sandhu, K., Patil, S.S., Pumphrey, M., Carter, A., 2021. Multitrait machine- and deep-learning models for genomic selection using spectral information in a wheat breedingprogram. Plant Genome 14, e20119. https://doi.org/10.1002/tpg2.20119 . Sharma, S., Partap, A., de Luis Balaguer, M.A., Malvar, S., Chandra, R., 2022. Deepg2p: fus- ing multi-modal data to improve crop production arXiv:2211.05986.Shook, J., Gangopadhyay, T., Wu, L., Ganapathysubramanian, B., Sarkar, S., Singh, A.K.,2021. Crop yield prediction integrating genotype and weather variables using deeplearning. PLoS One 16, 1 –19. URL:https://doi.org/10.1371/journal.pone.0252402 . Shrikumar, A., Greenside, P., Kundaje, A., 2017. Learning important features through prop-agating activation differences. In: Precup, D., Teh, Y.W. (Eds.), Proceedings of the 34thInternational Conference on Machine Learning, PMLR, pp. 3145 –3153. URL:https:// proceedings.mlr.press/v70/shrikumar17a.html . Sonkar, G., Mall, R., Banerjee, T., Singh, N., Kumar, T.L., Chand, R., 2019. Vulnerability of in-dian wheat against rising temperature and aerosols. Environ. Pollut. 254, 112946.https://doi.org/10.1016/j.envpol.2019.07.114 . Tadesse, W., Manes, Y., Singh, R.P., Payne, T., Braun, H.J., 2010. Adaptation and perfor-mance of cimmyt spring wheat genotypes targeted to high rainfall areas of theworld. Crop Sci. 50, 2240 –2248.https://doi.org/10.2135/cropsci2010.02.0102 . Tadesse, W., Sanchez-Garcia, M., Assefa, S.G., Amri, A., Bishaw, Z., Ogbonnaya, F.C., Baum,M., 2019.Genetic gains in wheat breeding and its role in feeding the world. CropBreed. Genet. Genom. 1, e190005.S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60–76
75Tang, X., Liu, G., Zhou, J., Ren, Q., You, Q., Tian, L., Xin, X., Zhong, Z., Liu, B., Zheng, X., et al.,2018. A large-scale whole-genome sequencing analysis reveals highly speci ﬁc ge- nome editing by both cas9 and cpf1 (cas12a) nucleases in rice. Genome Biol. 19,84. URL:https://doi.org/10.1186/s13059-018-1458-5 . Tong, H., Nikoloski, Z., 2021. Machine learning approaches for crop improvement:leveraging phenotypic and genotypic big data. J. Plant Physiol. 257, 153354. https:// doi.org/10.1016/j.jplph.2020.153354 . UN World Food Programme, 2022. Update: Global Food Crisis 2022. https://www.wfp. org/publications/update-global-food-crisis-2022 . accessed 2022-08-04. Washburn, J.D., Cimen, E., Ramstein, G., Reeves, T., O ’Briant, P., McLean, G., Cooper, M., Hammer, G., Buckler, E.S., 2021. Predicting phenotypes from genetic, environment,management, and historical data using cnns. Theor. Appl. Genet. 134, 3997 –4011. URL:https://doi.org/10.1007/s00122-021-03943-7 .World Bank Group, 2022. Food security update. https://www.worldbank.org/en/topic/ agriculture/brief/food-security-update . accessed 2022-08-04. Zegeye, W.A., Zhang, Y., Cao, L., Cheng, S., 2018. Whole genome resequencing from bulkedpopulations as a rapid qtl and gene identi ﬁcation method in rice. Int. J. Mol. Sci. 19. URL:https://www.mdpi.com/1422-0067/19/12/4000 . Zhang, H., Wang, X., Pan, Q., Li, P., Liu, Y., Lu, X., Zhong, W., Li, M., Han, L., Li, J., Wang, P., Li,D., Liu, Y., Li, Q., Yang, F., Zhang, Y.M., Wang, G., Li, L., 2019. Qtg-seq accelerates qtl ﬁne mapping through qtl partitioning and whole-genome sequencing of bulked segre-gant samples. Mol. Plant 12, 426 –437.https://doi.org/10.1016/j.molp.2018.12.018 .S. Jubair, O. Tremblay-Savard and M. Domaratzki Artiﬁcial Intelligence in Agriculture 8 (2023) 60 –76
76