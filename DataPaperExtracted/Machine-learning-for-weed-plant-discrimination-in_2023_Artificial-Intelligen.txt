Machine learning for weed–plant discrimination in agriculture 5.0:An in-depth review
Filbert H. Juwonoa,⁎, W.K. Wongb, Seema Vermac, Neha Shekhawatc, Basil Andy Leaseb, Catur Aprionod,⁎
aDepartment of Electrical and Electronic Engineering, Xi'an Jiaotong-Liverpool University, Suzhou 215123, China
bDpartment of Electrical and Computer Engineering, Curtin University Malaysia, Miri 98009, Malaysia
cSchool of Physical Sciences, Banasthali Vidyapith, Rajasthan 304022, India
dDepartment of Electrical Engineering, Universitas Indonesia, Depok 16424, Indonesia
abstract article info
Article history:Received 12 March 2023Received in revised form 25 July 2023Accepted 11 September 2023Available online 19 September 2023Agriculture 5.0 is an emerging concept where sensors, big data, Internet-of-Things (IoT), robots, and Arti ﬁcial In- telligence (AI) are used for agricultural purposes. Different from Agriculture 4.0, robots and AI become the focusof the implementation in Agriculture 5.0. One of the applications of Agriculture 5.0 is weed management whererobots are used to discriminate weeds from the crops or plants so that proper action can be performed to removethe weeds. This paper discusses an in-depth review of Machine Learning (ML) techniques used for discriminatingweeds from crops or plants. We speci ﬁcally present a detailed explanation of ﬁve steps required in using ML algorithms to distinguish between weeds and plants.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Agriculture 5.0Machine learningUnmanned aerial vehicleWeed-plant discrimination
Contents1 . I n t r o d u c t i o n ............................................................... 1 41 . 1 . A u t o m a t e d w e e d c o n t r o l : A t e c h n o l o g i c a l p r o g r e s s .......................................... 1 41 . 2 . C o n t r i b u t i o n s a n d s t r u c t u r e.................................................... 1 52 . D a t a a c q u i s i t i o n ............................................................. 1 53 . I m a g e p r e - p r o c e s s i n g : B a c k g r o u n d r e m o v a l................................................ 1 53 . 1 . E x c e s s G r e e n I n d e x ( E x G I ) ..................................................... 1 63 . 2 . R a t i o V e g e t a t i o n I n d e x ( R V I ) a n d V e g e t a t i o n I n d e x N u m b e r ( V I N ) ................................... 1 63 . 3 . N o r m a l i z e d D i f f e r e n c e V e g e t a t i o n I n d e x ( N D V I )........................................... 1 63 . 3 . 1 . O t s u t h r e s h o l d...................................................... 1 73.3.2. Modiﬁe d O t s u t h r e s h o l d .................................................. 1 73 . 4 . T r a n s f o r m e d V e g e t a t i o n I n d e x ( T V I ) , c o r r e c t e d T V I ( C T V I ) , a n d T h i a m ' s T V I ( T T V I ) ............................ 1 73 . 5 . P e r p e n d i c u l a r V e g e t a t i o n I n d e x ( P V I ) ................................................ 1 74 . F e a t u r e - b a s e d r e c o g n i t i o n........................................................ 1 84 . 1 . B i o l o g i c a l m o r p h o l o g y f e a t u r e s ................................................... 1 84 . 2 . S p e c t r a l f e a t u r e s ......................................................... 1 84 . 3 . T e x t u r e f e a t u r e s .......................................................... 1 95. Classiﬁc a t i o n a l g o r i t h m s......................................................... 2 05 . 1 . M L - b a s e d a l g o r i t h m s....................................................... 2 05 . 2 . E n s e m b l e l e a r n i n g ......................................................... 2 05 . 3 . D L - b a s e d a l g o r i t h m s ........................................................ 2 16 . P u b l i c l y a v a i l a b l e d a t a s e t s ........................................................ 2 27 . C h a l l e n g e s a n d o p p o r t u n i t i e s ....................................................... 2 3Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
⁎Corresponding authors.E-mail addresses:Filbert.Juwono@xjtlu.edu.cn(F.H. Juwono),weikitt.w@curtin.edu.my(W.K. Wong),seemaverma3@yahoo.com(S. Verma),shekhawatneha37@gmail.com (N. Shekhawat),basil.lease@curtin.edu.my(B.A. Lease),catur@eng.ui.ac.id(C. Apriono).
https://doi.org/10.1016/j.aiia.2023.09.0022589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/8 . C o n c l u s i o n............................................................... 2 3D e c l a r a t i o n o f C o m p e t i n g I n t e r e s t....................................................... 2 3A c k n o w l e d g e m e n t s .............................................................. 2 3R e f e r e n c e s .................................................................. 2 3
1. IntroductionRecently, Information and Communication Technology (ICT) hasbeen immersed in agricultural practises. Agriculture 4.0, which is alsoknown as data-driven agriculture or smart farming or digital farming,uses telematics, data management, and precision agriculture to increasethe quality of the crops (Saiz-Rubio and Rovira-Más, 2020). The Na- tional Research Council (1997) deﬁnes precision agriculture asthe ap- plication of modern information technologies to provide, process andanalyze multisource data of high spatial and temporal resolution for deci-sion making and operations in the management of crop production .I ti s clear that precision agriculture focuses on applying inputs(e.g., fertilizers, pesticides) when necessary in the speci ﬁc quantities. This will optimize the resources and make the plants receive whatthey really need. The researchers in this area focus on creating systemsthat can make operational and strategical decisions using the data sup-plied. Traditionally, farmers go around the farms to check the conditionsof the crops and make decisions based on the acquired experience. Thisconventional method is no more feasible as the size of ﬁelds has grown immensely and traditional methods demand more working resources.Agriculture 5.0 has evolved as ICT has advanced. Different from Ag-riculture 4.0, Agriculture 5.0 employs precision agriculture and un-manned and autonomous equipment ( Saiz-Rubio and Rovira-Más, 2020). In other words, robots and Artiﬁcial Intelligence (AI) play impor- tant roles in thisﬁeld. Technologically, computer vision, Machine Learn-ing (ML), and Unmanned Aerial Vehicles (UAVs), or commonly knownas drones, have been utilized to build Agriculture 5.0 system.Moreover, computer vision has recently played a vital role in en-abling the digital world to interact with the physical world. In general,the technology exploits cameras coupled with computer, rather thanthe human eyes, to identify, track, and estimate targets for image pro-cessing. It plays a critical role in technology by powering object detec-tion, classiﬁcation, and tracking of objects in all domains. Recently,smart agriculture tasks (Neupane and Baysal-Gurel, 2021) that include plant ailment detection, weed detection, crop yield prediction, identi ﬁ- cation of plant species, are achieved through computer visiontechnology.UAVs are recently taking up considerably space in research domainso as to make the industry less labor-intensive. Applications of com-puter vision to drone technology makes it be able to interpret and inter-act with surroundings, including buildings, trees, and diverse terrain.The use of data from UAVs for farming applications has been gainingan enormous popularity in both research and industry. Compared tohuman-based monitoring, UAV is considered to be cheaper and moreconvenient with shorter inspection time needed. The optimal controlof the UAV in the presence of obstacles is commendable, which is a crit-ical factor while working with two crop rows.The initial step in Agriculture 5.0 is data collection, followed by au-tonomous decision making and its implementation. The data collectedfrom physical sources such as sensors and cameras contains valuable in-formation, which is directly relevant to the process of decision making.The connection between the data and the decision stage involves ﬁlter- ing routines and AI algorithms for getting only the right data and help-ing the farmer make correct decisions. The last step of physicalexecution of the decision, also referred as actuation, is carried out by ad-vanced equipment coupled with computer control units.The exponential growth of robotic applications in smart farmingshows its efﬁciency in handling workforce and cost of production. Theadvanced level of precision agriculture deals with applying the aboveprinciples using unmanned operated equipment and systems that in-volve autonomous decisions. The use of autonomous and semi-autonomous robots in theﬁeld of agriculture has immensely increasedin last decades. They possess arms that can detect weeds and canspray chemicals on the affected areas speci ﬁcally, thus reducing the overall cost.1.1. Automated weed control: A technological progressIn general, weeds hinder the movement of irrigation water, disruptthe process of applying pesticides, and serve as a breeding ground forpathogens. Thus, weed detection and control play a vital role in overallcrop productivity and farming expenses. Effective weed control plays acrucial role in agriculture by minimizing crop yields, dramatically esca-lating production costs, negatively impacting crops, and signi ﬁ
cantly compromising product quality. Note that weeds compete with produc-tive crops for essential resources such as water, nutrients, and light,thereby diminishing overall agricultural output.Traditional weed management methods include chemical meansi.e., uniform application of herbicides throughout the ﬁeld using me- chanical means or physical elimination. The former method leads tothe overuse of chemicals as the weed spatial density varies across theﬁeld which ends up in environmental apprehension and formation ofherbicide-resistant weeds. The latter results in labor intensive farmingmethods in turn causing a steep increase in production cost. Theabove issues can be solved by applying concept of Site-Speci ﬁc Weed Management (SSWM) (Lati et al., 2021). SSWM involves detection of patches of weeds across theﬁeld and carrying out spot spraying or me-chanical removal ways which can be broadly classi ﬁed into two catego- ries: 1) prescription maps-based methods ( De Castro et al., 2018) where the areas of weed emergence are detected to focus the application ofweedicide, and 2) real time monitoring method ( Xu et al., 2018) that simultaneously detects and controls weeds by spraying weedicide onthe spot.Existing techniques for image processing provide extremely promis-ing outcomes under optimal imaging conditions, even though they canbe quite challenging under real time conditions. The overlap of plantleaves and weeds at different stages of growth makes them synony-mous to each other. In addition, the leaves are often occluded, discol-ored or damaged by undesired materials which alter themorphological as well as spectral characters of the leaves. As the light-ing conditions change during different times in a day, the shadowsformed by the plant canopy and the solar inclination can directly impactthe colour of vegetation. Furthermore, different plant growth stages alsoresult in variations of the morphological, textural, and spectral attri-butes to leaves.This paper focuses on the discrimination between crop and weedusing ML approaches. Note that traditional image processing techniqueslike Hough transform can also be used to discriminate crop and weed(Bah et al., 2017). However, these approaches are not of our interest. In-stead, we commonly use image processing techniques to extract thefeatures like color, shape, etc. to be fed to the ML algorithms. The ef ﬁ- ciency of these methods mostly depends on the manually designed fea-tures and have high dependence on image acquisition methods, pre-processing methods, and the standard of features extracted. Earlierstudies in this category primarily used color co-occurrence matrix-based texture analysis for images ( Chang et al., 2012). WithF.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
14advancement in technology, optical sensor coupled with image process-ing algorithms has been implemented for variable rate weedicide appli-cations. The major challenge in classiﬁcation of weeds from crops is caused by their similar spectral signature.Image processing-based weed control is usually performed throughseveral steps. After the images are required, they may not be suitable forfurther processing. For example, it is required to discriminate the vege-tation from the soil. The steps will be further discussed in this paper.Note that this paper can be seen as a complementary to a few relatedworks, such as (Wang et al., 2019;Hasan et al., 2021).1.2. Contributions and structureAs previously discussed, discriminating weeds from plants is neces-sary in weed management, regardless the method that is chosen,i.e., using chemical or non-chemical. In this paper, we focus on the dis-cussion of weed control applications, their recent advances, and alsochallenges. The steps to do weed-crop discrimination can be simply ex-plained as follows. Initially, the images are captured using robots orUAVs equipped with cameras. The images must be free from any back-ground interference (e.g., soil). Trained experts can be employed tolabel and distinguish between the weeds and the crops in the images.If any challenges arise during image acquisition, publicly availabledatasets with annotations can be utilized as an alternative. Afterward,the classiﬁcation task begins by selecting the features, which can be per-formed either manually or automatically. ML algorithms use the se-lected features for learning the patterns, while Deep Learning (DL),which is a subset of ML, can be utilized to perform both automatic fea-ture selection and classiﬁcation. This concept is depicted inFig. 1. The review of the state-of-the-art technology for weed-crop dis-crimination will be discussed in the following structure. The paper startsby examining the motivation behind weed-crop recognition task inSection 1. Moreover, various data acquisition methods and preprocess-ing strategies aimed at distinguishing between vegetation and the back-ground (background removal) are presented in Section 2andSection 3, respectively. Subsequently, feature-based weed-crop recognition tech-niques are presented inSection 4. Next, ML and DL approaches for clas-siﬁcation tasks are elaborated inSection 5. The publicly available datasets are discussed brieﬂyi nSection 6. The challenges and opportu- nities can be found inSection 7. Finally, the paper concludes the discus-sion inSection 8.2. Data acquisitionMobile robots (Xu et al., 2019)o rU A V s(Tripicchio et al., 2015)a r e preferred to be used to capture images of the plantation. The robotsand UAVs should be equipped with either multispectral orhyperspectral cameras. A multispectral camera can capture severalbands, usually R (red), G (green), B (blue), and NIR (Near-Infrared). InSankaran et al. (2013), the authors used two types of cameras, eachcamera had three different bands. Theﬁrst camera had G, B, and NIR bands while the second one had R, G, and NIR bands.On the other hand, a hyperspectral sensor may provide hundreds orthousands of bands. A MicroHyperspec sensor which captures imagesusing 325 bands in the visible band and NIR band was used in Lu et al. (2019). In the paper, it is also said that hyperspectral images were inlimited supply. In terms of the vegetation properties estimation, the au-thors concluded that hyperspectral images produced a comparable per-formance compared with the multispectral images.Hyperspectral sensors can be classiﬁed into point-scan, area-scan, and line-scan categories based on the scanning method they use to ob-tain 3D-hyperspectral cubes (x,
y,λ)(Patrício and Rieder, 2018;Qin et al., 2013). Point-scanning cameras are designed to capture a singularpoint along two-dimensional area. Spectrum of a single pixel is obtainedusing this method. An area-scanning camera captures a 2-D grayscaleimagex,yðÞof a single band. Finally, a line-scanning camera, whichcan be considered as the extension of the point-scanning type, gener-ates a 2-D imagey,λðÞ. The difference can be seen inFig. 2. The quality of captured images is inﬂuenced by various environmen- tal factors, including lighting conditions (such as day time, night time,and shadows) and humidity levels (wet vs dry crops) ( Jakubczyk et al., 2023). InManea and Calin (2015), it has been clearly proved that illumination conditions plays an important role for determiningthe quality of the images. High light intensity leads to a loss of informa-tion, while low light intensity introduces dark current noise. Further-more, the quality of images captured using line-scan method isaffected by the attitude and position changes of the UAV ( Xue et al., 2021).3. Image pre-processing: Background removalImage enhancement is required to improve image quality so that it issuitable for feature extraction and classi ﬁcation tasks. The manually taken images sometimes contain disturbances, such as shadows, stones,water, and soil (Gée et al., 2008;Lease et al., 2020). The images are typ- ically multispectral images which may contain the following four chan-nels: Red - R (630–690 nm), Green - G (510–580 nm), Blue - B (450–510 nm), Mid-Infrared - MIR (300 –500 nm), and Near-Infrared - NIR (700–1100 nm). Vegetation Indices (VIs) are usually used to sepa-rate the vegetation from the disturbances. Note that each componenthas different reﬂectance to a speciﬁc band. For example, soil may havelow reﬂectance to the red spectrum. Using this principle, we can differ-entiate vegetation from the disturbances.According to the bands, VIs can be categorized into VIs without infra-red channels (RGB) and with infrared channels (RGB + NIR/MIR). Ac-cording to the method of obtaining the mathematical formula, VIs canbe categorized intoﬁrst and second generation (Bannari et al., 1995). First generation VIs are obtained using empirical method without
Fig. 1.Concept of weed detection.F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
15considering atmospheric effects, soil brightness, and soil color. Mean-while, second generation VIs are obtained through mathematical andphysical reasoning, logical experiment, and simulation. Furthermore,according to the method, VIs can be grouped into ( Mróz and Sobieraj, 2004):1. Slope-based: This method uses the ratio of NIR and R. This methodincludes Ratio Vegetation Index (RVI), Vegetation Index Number(VIN), and Normalized Difference Vegetation Index (NDVI). Theymay have some variants that have been proposed by various re-searchers.2. Distance-based: This method uses soil line as the reference and thenmeasures the perpendicular distance of each pixel point to this soilline. This method includes Perpendicular Vegetation Index 1 and 2(PVI
1and PVI 2, respectively), Difference Vegetation Index (DVI),and Soil Adjusted Vegetation Index (SAVI) and its variants.3. Orthogonal transformation: This group uses orthogonal transforma-tion to create new uncorrelated bands.4. Red Edge Inﬂection Point (REIP): This method observes the red edge,which is a fast shift in reﬂectance found in green plant spectrum atthe transition between visible and near-infrared wavelengths, usingGaussian, polynomial, and Lagrangian models.We notice from the above discussion, there are many well-knownVIs in practice. Thorough review papers of VIs can be found in Xue and Su (2017),Bannari et al. (1995),a n dMróz and Sobieraj (2004). Here, we revisit some VIs as shown in Fig. 3.3.1. Excess Green Index (ExGI)TheExGIis deﬁned byGée et al. (2008)andWoebbecke et al. (1995) ExGI¼2g/C0r/C0b, ð1Þwhereg¼
Gn
RnþG nþB n, ð2Þr¼
Rn
RnþG nþB n, ð3Þb¼Bn
RnþG nþB n, ð4ÞR
n,Gn, andB nare the normalized RGB coordinates ranging between 0and 1. The normalized coordinates are given byG
n¼GG
m, ð5ÞR
n¼RR
m, ð6ÞB
n¼BB
m, ð7ÞwhereR,G,a n dBare the true coordinates andR
m,Gm,a n dB mare the maximum true coordinates. Note that R
m¼G m¼B m¼255 for 24-bit color images.Fig. 4shows the images before and after ExGIpreprocessing.3.2. Ratio Vegetation Index (RVI) and Vegetation Index Number (VIN)RVI and VIN are the twoﬁrst VIs (Bannari et al., 1995). They use the reﬂectance of the red,Rand near-infrared,NIR. The RVI is mathemati- cally given byRVI¼
RNIR, ð8ÞwhileVINis expressed byVIN¼
1RVI¼ NIRR: ð9Þ3.3. Normalized Difference Vegetation Index (NDVI)NDVI is the most popular VI used in the literature ( Bosilj et al., 2018). NDVI measures the spectral reﬂectance difference between near-infrared and red spectrum. Mathematically, NDVI is expressed byNVDI¼
NIR/C0RNIRþR, ð10Þor using theVINtermNVDI¼
VIN/C01VINþ1, ð11Þ
Fig. 2.Hyperpectral sensor scanning methods.
Fig. 3.VIs discussed in this paper.F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
16whereNIRis the near-infrared spectrum reﬂectance andRis the red spectrum reﬂectance. It is worth mentioning that NIR can be replacedby mid-infrared (MIR) (Bannari et al., 1995). This process indirectly converts the image into grey scale with most, but not all, backgroundpixels removed. Subsequently, the image is converted into black andwhite as a binary mask using Otsu threshold ( Otsu, 1979), that will be described brieﬂy below. The binary mask is then applied to the gray-scale image to remove the background.3.3.1. Otsu thresholdLet us assume that a grayscale image has Lpixel levels. We would like to separate the image into two classes, background (class C
0)a n d object (classC
1). ClassC 0is a group of pixels with level 1,⋯,T½/C138and C
1is a group of pixels with levelTþ1,⋯,L½/C138,w h e r eTis the threshold. The probabilities for class occurrence are given byω
0¼ωTðÞ ¼Pr C 0ðÞ ¼∑Ti¼1pi, ð12Þandω
1¼1/C0ωTðÞ ¼Pr C 1ðÞ ¼∑Li¼Tþ1pi:ð13ÞThe class mean levels are given byμ
0¼∑Ti¼1ipi=ω0¼μTðÞ=ωTðÞ, ð14Þandμ
1¼∑Li¼Tþ1ipi=ω1¼μT/C0μTðÞðÞ=1/C0ωTðÞðÞ,ð15ÞwhereμTðÞ ¼∑
Ti¼1piandμT¼μLðÞ ¼∑Li¼1ipi¼ω 0μ0þω 1μ1. The optimal threshold,T
∗is the threshold which satisﬁes T
/C3¼arg max
1≤T<Lσ2BTðÞ; ð16Þwhereσ
2BTðÞis the between-class variance given byσ
2BTðÞ¼ω 0ω1μ1−μ0 ðÞ2; ð17Þorσ2BTðÞ ¼μTωTðÞ /C0μTðÞ½/C1382
ωTðÞ1/C0ωTðÞ½/C138: ð18Þ3.3.2. Modiﬁed Otsu thresholdOtsu threshold is not the only means to separate the background andthe objects. For example, a modiﬁed Otsu threshold was proposed inSomasundaram and Genish (2012). Instead of using the class meanvalues, the standard deviation values are used in Eq. (17). The modiﬁed between-class variance is then given byσ
2BTðÞ ¼ω 0ω1SB/C0S O ðÞ2, ð19ÞwhereS
BandS Oare standard deviation values for the background pixelsand the object pixels, respectively.3.4. Transformed Vegetation Index (TVI), corrected TVI (CTVI), and Thiam'sTVI (TTVI)TVI, CTVI, and TTVI are three VIs that are derived from NDVI ( Mróz and Sobieraj, 2004). The TVI is given byTVI¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃNDVIþ0:5p, ð20ÞforNDVI≥−0:5 . To solve the problem ofNDVI <−0:5 , CTVI was proposed. CTVI is given byCTVI¼
NDVIþ0:5∣NDVIþ0:5∣ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ∣NDVIþ0:5∣p,ð21Þwhereﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃ∣NDVIþ0:5∣pis the TTVI.3.5. Perpendicular Vegetation Index (PVI)The original PVI takes into account the difference between bare soilreﬂectance and vegetation reﬂectance. In particular, it measures the “greenness”level of the vegetation which includes the bare soil andthen subtract the bare soil reﬂectance to obtain the vegetation index.The formula uses two bands, i.e. red channel and NIR channel. The for-mula is given byBannari et al. (1995)
(a) Original image
 (b) ExGI image
Fig. 4.Before and after ExGI Preprocessing.F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
17PVI¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃR s/C0R v ðÞ2þNIR s/C0NIR v ðÞ2q,ð22Þwhere subscriptssandvrefer to soil and vegetation, respectively.An improved PVI formula is given by Bannari et al. (1995)PVI¼
NIR/C0aR/C0bﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃa
2þ1p, ð23Þwhereais the slope of the bare soil line and bis the y-axis intercept of the bare soil line.4. Feature-based recognitionAs previously stated inSection 1.2, ML algorithms require features for their operation. In particular, features extracted from the image,such as color, texture, and shape, are fed to the ML algorithms. Conven-tionally, these methods require manual design of features (engineeredfeatures) and heavily depend on image acquisition methods, prepro-cessing techniques, and the accuracy of feature extraction. Forengineered features, researchers have used biological morphology,spectral, or texture features of theﬁeld images for the classiﬁcation of weed and crops. Note that traditional ML methods typically require asmaller sample size and a shorter training time when compared totheir DL counterparts due to less computational requirements and com-plexity. However, the type of features used, the selection, and tuning ofthose features play a critical role in determining the ability of the classi-ﬁer to discriminate between crops and weeds.4.1. Biological morphology featuresThe physical characteristics of plants are referred to as morphologyfeatures. It is obvious that each plant has unique physical traits, suchas leaf shape and size. For example, the leaves of Pluchea indicahave the form ofabovatus, i.e., round shape like eggs (Susetyarini et al., 2020). Therefore, shape abstraction in regard to plant geometry andstructure would give sufﬁcient information on the plant's identity. Theapproach is mainly based on shape features from leaf edge patternswhich includes leaf curvature or lobe. Often, multiple features whichcontribute to the recognition of the plant are used, such as area, dimen-sionless ratios, length, perimeter, moments and width of either the leafor the plant are used. In general, there are many morphological featuresthat can be used: geometry, shape, size, and color of plant organs(e.g., root, stem, leaf andﬂower) (Speck and Speck, 2021).Søgaard (2005)used active shape models to classify weed species.The database of the shape model consisted of 19 major weed speciesin Danishﬁelds. The database holds the information of leaf and plantfeatures. The features included the shape and structure de ﬁnition of the plants. The database was formed from seedling growth to two trueleaves. Color images were taken from individual seedings. A plastic dif-fuser was used to prevent image overexposure. The images were thencategorized using template matching, where each species was discrim-inated against based on template deformation against the optimal ﬁt. Fig. 5shows the model applied on weed images.Ahmed et al. (2012)used the feature vector of shape, color, and mo-ment. An ML model was validated with 224 images consisting of chilli,pigweed, marsh herb, lamb's quarters, cogon grass, and bur cucumber.The authors noted that the method was dependent on a large numberof images to ensure a more robust model. They also noted that plantholes and image noise affected the model.The biological morphology approach to various machine visionmethods mentioned previously achieved high recognition rates underoptimal conditions. This shows the theoretical feasibility of using suchmethods in environments where plants are in controlled, damage-free, and unoccluded environment.4.2. Spectral featuresPrevious studies by other researchers have employed the re ﬂection of light from plants to identify their types. This feature operates basedon the color or spectral reﬂectance exhibited by the leaves. Remarkably,this technique can effectively identify the plant type even when there ispartial occlusion of the plant. The unique cellular characteristics of a leaflead to distinct responses when exposed to speci ﬁ
c wavelengths of elec- tromagnetic waves. For example, there is a signi ﬁcant difference in NIR reﬂectance between dicotyledonous and monocotyledonous plants(Kim and Reid, 2006). Moreover, other than NIR, some authors haveused visible waveband spectrum and water absorption waveband spec-trum as features (Kim and Reid, 2006;Hatﬁeld and Pinter, 1993). Woebbecke et al., 1995proposed the use of color features in stan-dard color slide images to discriminate weeds from the soil. Lightreﬂected from the leaves of the plants were captured and used for clas-siﬁcation based on the reﬂectance wavelength in red, NIR, green, andblue. Their system classiﬁed leaves from soybeans, giant foxtail, ivyleaf, morning-glory, and velvetleaf. Reﬂectances from the other edges of the leaves were used to compute mean, variance, and skewnesswhich were used to determine the type of plant. They experimentedwith the classiﬁer by either including or excluding leaf orientation in
Fig. 5.Example of active shape models by Søgaard (2005).F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
18the classiﬁer. It was found that by excluding leaf orientation features,the classiﬁer managed to obtain a higher classiﬁcation rate than when including leaf orientation. The researchers also concluded that color im-ages could distinguish between plants and the background but were notsufﬁcient to determine the plant species. It was noted that this tech-nique was affected by leaf orientation as the re ﬂected wavelength changes with the orientation of the leaf.Another example of using spectral feature for classifying carrots,cabbage, and weeds can be found in Hemming and Rath (2001).T h e classiﬁer was based on weighted fuzzy classiﬁcation. It was shown that color features could discriminate plants from the soil, and the clas-siﬁer showed an improvement in terms of classi ﬁcation accuracy when color features were used in addition to morphology features. Theirmethod resulted in good classiﬁcation rates depending on the growthand density of the weeds. They noted that this technique is susceptibleto occluded leaves.Spectral features have also been used in Feyaerts and Van Gool (2001). In particular, blue-NIR spectral was observed to discriminatesugar beet plants from weeds. It cannot be denied that crop and weedexhibit comparable spectral signatures. Therefore, Object-Based ImageAnalysis (OBIA) techniques can improve classi ﬁcation results (De Castro et al., 2018). OBIA works by segmenting images into clusters ofneighboring pixels with similar spectral values, referred to as “objects.”4.3. Texture featuresTexture features are deﬁned as a collection of metrics used to quan-tify the apparent texture of an image. The texture provides informationon the spatial arrangements of pixels in a region or the entire image. Themetrics represent properties such as coarseness, smoothness, andregularity (Bakhshipour et al., 2017).A texture may be considered as clusters of similarities in an image.There are four types of texture, namely statistical features, structuralfeatures, model-based features, and transformation-based features. Sta-tistical features involve calculating the local features of each point in theimage, obtaining a collection of statistical data from the distribution oflocal features, and evaluating the spatial distribution of grey values. Sta-tistical moments are computed through the image or the region's inten-sity histogram, and the extracted information are sets of statistics thatprovide properties such as skewness,ﬂatness, and contrast. The Grey Level Co-occurrence Matrix (GLCM) is a common approach forobtaining statistical characteristics (Haralick et al., 1973). The structural texture is primarily described as a collection of well-deﬁned texture components. The structural texture is de ﬁned by the characteristics and placement rules of texture components. Structuraltextures are rarely utilized in agricultural applications since they canonly represent highly regular textures. The texture image is representedas a linear combination of a probability model or a collection of basisfunctions for model-based textures, and the model coef ﬁcients are uti- lized as texture features (Rallabandi and Sett, 2005). Model-based texture characteristics, such as structural texture, arerarely utilized in the literature for plant recognition. The converted fea-tures are retrieved by converting the image into a new space where thecoordinate system has a near interpretation of texture characteristics.For transformation-based features, curve and wavelet transformationare two commonly utilized transformation techniques (
López- Granados, 2011). These transformations are typically carried out beforethe feature extraction process. The Fourier transform method is rarelyutilized because of the lack of spatial location. Several wavelet families,including Haar and Gabor, have been utilized to detect speci ﬁc novel characteristics that are difﬁcult to describe in the spatial domain.Gaborﬁlters improve spatial positioning approaches and have been uti-lized in a wide range of weed and crop recognition applications.The authors inChaki et al. (2015)used the Gaborﬁlter to convolve a grey image of plant leaves with empirically established parameters tocreate a collection of complex signals, resulting in real and imaginaryportions. GLCM was calculated based on the virtual signal followingthe Gaborﬁlter, followed by a collection of GLCM-based features. In an-other study,Kumar and Prema (2016)used rotation-invariant wavelet features for weed identiﬁcation. For acquiring these characteristics,the radon transform approach was used. It substantially lowered thenumber of characteristics and identiﬁed images in a variety of directionsrapidly. The impact of rotation on the input and output images might beshown through Radon transform. The wavelet function was then usedto split the Radon transform output into distinct sub-bands, and the tex-tural characteristics, such as energy and uniformity of each sub-band,were calculated. Curvelet transform decomposes the image at multiplesizes and angles, making feature extraction easier.The authors inBakhshipour et al. (2017)executed a single-level Haar discrete wavelet decomposition on a grey image to get four sub-images representing approximation images, vertical details, horizontaldetails, and diagonal details. The GLCM texture was then extractedfrom the four sub-images. Local Binary Pattern (LBP) is a single channeland rotational invariant texture encoderﬁrst proposed byHe and Wang (1990)describing the spatial structure of the local image texture. Theeffectiveness of LBP was later proven and described by Ojala et al. (2002). LBP is often used as a visual texture descriptor for many classi-ﬁcation problems. Some of the common uses of LBP are in applicationsof face detection (Jin et al., 2004), facial recognition (Liu et al., 2016), vi- sual inspection (Tajeripour et al., 2008), motion analysis (Zhao and Pietikainen, 2006), and texture analysis (Mäenpää and Pietikäinen, 2005). Literature has shown that LBP is a powerful texture visual de-scriptor in many applications; thus its use can be further explored insmart agriculture applications, such as weed classi ﬁcation tasks. The authors inAhmed et al. (2011)proposed an automated weed classiﬁcation with LBP-based template matching. The study evaluatedthe feasibility of using LBP-based texture patterns to classify weed im-ages into broadleaf and grass categories. This is to note that the classi ﬁ- cation is of the entire image rather than pixel-level classi ﬁcation (i.e., classify the whole image rather than pixel-level labelling). The im-ages used were colored images comprised of 100 broadleafs and 100grass weeds samples. LBP features were extracted from the imagesand were trained with two classiﬁers. The authors concluded that therotation invariant LBP had strong discriminative abilities, given thecorrect parameter setting was chosen for LBP.In the follow-up work, the authors in Ahmed et al. (2014)proposed using local pattern-based texture descriptors, which includes LBP forthe classiﬁcation of broadleaf and grass weeds. Again, the classi ﬁcation was of the entire image rather than pixel-level classi ﬁcation. The re- searchers experimented with three local texture operators with varyingparameter conﬁgurations. From theirﬁndings, rotation invariant local patterns could provide stable performance in the presence of orienta-tion variations, illumination variations, image noise, and lower memoryand computational cost compared with wavelet methods. The desirablecharacteristics saved considerable computational power as no prepro-cessing was needed for illumination normalisation and data augmenta-tion in terms of orientation. It showed that that LBP was able to achieverobust performance in uncontrolled natural environment.The authors inLe et al. (2019)studied on combining multiple LBPfeatures with a classiﬁer to classify canola, corn maize, and radish. Theimages used were in a lab controlled environment, where only individ-ual plant species were captured at a given time without including otherspecies. Similar to the previous, the classi ﬁcation was of the entire image rather than pixel-level classiﬁcation. The authors suggested thatin plant classiﬁcation, the accuracy metric itself was not a suf ﬁcient in- dicator to determine its acceptable performance for plant classi ﬁcation. They suggested that the Precision, Recall, and F1-score metrics weresuitable to validate the performance of the model for plant classi ﬁcation. From theirﬁndings, it was noticed that when visualising the texture fea-ture distribution, corn leaves' texture features were distinct comparedto leaves of canola and radish. Their results showed that combiningLBP features could achieve accuracy greater than 91% for corn, canola,F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
19and radish plants and background. The authors concluded that combin-ing multiple robust LBP features greatly improved the accuracy and F1scores in the validation sets.The authors inSamarajeewa et al. (2018)examined the identiﬁca- tion ofﬂower patches in the aerial images using two techniques, namelyLBP classiﬁcation and CIELAB color space threshold classi ﬁcation. In the paper, LBP set pixel intensity thresholds based on the surrounding envi-ronment, allowing high-intensity values to be displayed, effectively dis-tinguishing the plants from the background. The CIELAB thresholdalgorithm determined a threshold value for each RGB channel basedon the histogram. Then, in both approaches, erosion was used to elimi-nate any remaining noise.Existing research work has shown that there are only few researchpapers discussing LBP applications in weed classi ﬁcation tasks. Most of the papers utilize LBP to classify entire images or image patches ratherthan pixel-level classiﬁcation (Ahmed et al., 2014, 2011;Le et al., 2019;Samarajeewa et al., 2018). Despite the advantages and desirableproperties of LBP described in the literature for entire image weed clas-siﬁcation, LBP has yet to be applied in pixel-level weed classi ﬁcation. The main challenge of pixel-level classiﬁcation is the computational cost required when computing multiple LBP feature sets over eachimage window that contain plant pixels. Each image window representsa plant pixel label (i.e., crop pixel or weed pixel). The high computa-tional cost hinders the advancement of LBP for pixel-level classi ﬁcation. With the current advancement of Application Speci ﬁc Integrated Circuit (ASIC)-based real-time chip-based LBP extractor, the implementation ofa real-time pixel-level weed classiﬁcation becomes easier.Table 1 shows a summary of the three features.5. Classiﬁcation algorithms5.1. ML-based algorithmsML-based classiﬁers are coupled with various features to increasethe accuracy of the system for precision agriculture. These traditionalmethods, once again, require to be designed on hand crafted featuresand are highly depended on the image quality, pre-processing, andlearning algorithms. Advantages of ML-based techniques are advocatedon grounds of small sample sizes required to train the system and lowrequirements on graphics processing units, thus, making agriculturalmachinery and equipment inexpensive. Traditional ML methods couldgive efﬁcient performance in research, given the soil conditions and illu-mination remain the same throughout. These conditions make the real-time applications difﬁcult to be implemented.In this subsection, we elaborate several ML classi ﬁcation algorithms that are commonly used for crop/weed discrimination task, such as k- Nearest Neighbor (KNN) (Ahmad et al., 2011), Artiﬁcial Neural Net- works (ANNs) (Jeon et al., 2011), decision trees (Goel et al., 2003), and Support Vector Machine (SVM) ( Shahbudin et al., 2017;Le et al., 2019
). KNN belongs to a non-parametric algorithm, i.e., it does nothave the knowledge of the data distribution (lazy learner). It works bycalculating the distance (similarity) of a new data sample with its k neighbours. Following that, the new data sample will be classi ﬁed tothe class with the shortest distance. Note that kis usually an odd num- ber to avoid a tie.ANN is an ML algorithm that is designed to mimic the principle ofhuman brain. An ANN consists of layers of linked arti ﬁcial neurons. The fundamental ANN structure is known as a perceptron. A perceptronworks by receiving input signals, weighting and summing them. Subse-quently, it passes the sum to a nonlinear activation function to producethe output. ANN is trained to adjust the weights that minimize the dif-ference between the target output and the resulted output. Thebackpropagation algorithm is usually used to optimize the weights.SVM is considered as a linear classiﬁer. Suppose that we have two classes, SVM creates a hyperplane that optimally separates the two clas-ses. For multiclass classiﬁcation problem, we can extend the concept byusing Error-Correcting Output Coding (ECOC) with either One vs. All(OVA), One vs. One (OVO), Dense Random (DR), or Sparse Random(SR) conﬁguration (Wong et al., 2021). If the data samples are not dis- tributed linearly, a kernel approach, such as the Radial Basis Function(RBF), can be employed to increase the dimension.A decision tree is supervised ML algorithm with tree-like structurethat can be used for both classiﬁcation and regression. A decision treeconsists of a root node, splitting nodes, branch/sub-tree nodes, decisionnodes, and terminal nodes. Several algorithms have been used for build-ing the decision tree. In general, entropy measure is used to ﬁnd the appropriate feature to split the data at each node of the tree.5.2. Ensemble learningEnsemble learning methods employ social learning behaviours likethose observed in human social interactions, such as working in agroup, receiving peer feedback, and voting before deciding. Workingin a group allows each member to have various perspectives and repre-sentations of the data, which aids in getting a more reliable prediction(Kuncheva, 2014). As a result, ensemble learning approaches areintended to increase the prediction performance of a particular statisti-cal learner or model by combining basic base learners. An interestingcharacteristic of ensemble learning is that it encourages diversityamong simple base learners (Bishop, 2006). Individual learners create independent errors; therefore, diversity would minimize the error gen-erated by the learners. There are several techniques for building an en-semble model. However, it has been discovered that utilizing variouslearning sets increases variety within the ensemble ( Torres Sospedra et al., 2011).While the notion of combining classiﬁers is not new, research into various combination techniques, features, classi ﬁers, and innovative ap- plications is still ongoing. The authors in Fumera and Roli (2005)offered a theoretical and experimental study of linear combiners for multi-classiﬁer models. Their study focused on basic and weighted averagingat the categorization decision level. The research revealed that the out-put of linear combinations was affected by the accuracy and correlationof the individual classiﬁers. Weighted averaging outperformed basic av-eraging, according to the data. Weight optimization for classi ﬁers was still an unresolved issue. The authors suggested that future studiesmight involve the use of metaheuristic optimization techniques.
Table 1Summary of feature types.Morphology Spectral TexturalCharacteristics Physical characteristics Re ﬂection of light Spatial arrangements of pixels Examples Leaf shape, stem color NIR, infrared LBP, GLCMPros Rotation and scale invariant Rich information Discriminative power Useful in segmentation tasks Discriminative power Robust to noiseVisible to human Cons Noise sensitivity High dimesionality High computational Limited contextual information Limited in spatial information No spatial information More complexF.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
20The authors inKittler et al. (1998)presented a theoretical structure for classiﬁer combinations that is universal. While the authors focusedon ensembles with unique feature sets, in their comparable work(Kittler, 1998), both unique and common characteristics were investi-gated. It was created as a mathematical framework using the sum andproduct rules. The rules serve as the foundation for others, such as max-imum, minimum, median, and majority voting. Moreover, the authorsran extensive tests on the handwritten digits dataset. The sum ruleoutperformed other rules despite their most stringent assumptions. Itdemonstrates that the prediction error have a considerably less impacton the sum rule; hence, the theoretical paradigm established is consis-tent with their previous experimental results.In the study ofTorres-Sospedra and Nebot (2014) ,t h ea u t h o r s utilized an ensemble of NN classiﬁers combined with noisy statisticalcharacteristics of HSL (hue, saturation, brightness) color images inorange groves to classify weeds. Their implementation includedboosting techniques and was tested on 130 images of orange treeswith weeds, resulting in a good classiﬁcation accuracy. The authors discovered that the ensemble is appropriate for weed detectionwith noisy characteristics. The authors in Ahmad et al. (2018)uti- lized AdaBoost with Nave Bayes, a boosted ensemble technique com-bined with statistical visual characteristics, to categorize wide andgrass weed images in another study. Their technique correctly iden-tiﬁed 250 images with a classiﬁcation accuracy ranging from 94.72%to 98.40%. The authors discovered that utilising ensemble techniquesimproved overall accuracy by 4.7% compared to using simply anindividual classiﬁer in difﬁcult classiﬁcation tasks such as weed classiﬁcation.Combining different perspectives, forecasts, or models to minimizeuncertainty is not a novel concept. Around 200 years ago, Laplace of ﬁ- cially stated that he had demonstrated that the correct combination oftwo probabilistic techniques would produce better results than com-pared to only one (Laplace, 1820). Numerous methods have been devel-oped with the ensemble technique in combining various models,including aggregation, combination, committee, and fusion ( Drucker et al., 1994;Lam and Suen, 1995;Kittler et al., 1998;Polikar et al., 2008).5.3. DL-based algorithmsDL algorithms use a large number of images to train and validate forrecognition and classiﬁcation. With further progress in computationalpower as well as the availability of data, DL algorithms have gained pop-ularity due to their capability to extract multidimensional andmultiscale spatially essential information of crops and weeds usingConvolutional Neural Networks (CNNs) ( Dyrmann et al., 2016). The basis of CNN is a stack of convolutional layers which basically performﬁltering process (Albawi et al., 2017). Each convolutional layer acceptsinput data, transform as required and transfers them to the next layer.The convolutional operationﬁnally helps in simplifying the data for bet-ter processing and feature extraction.In speciﬁc tasks like image classiﬁcation and object detection and recognition, DL algorithms possess many advantages over conventionallearning approaches. The disadvantages of the conventional human in-
tervened feature extraction methods are usually overcome by its capa-bility of enhanced data expression by automated feature extraction.Recently, a lot of research articles on DL-based weed detection and clas-siﬁcation have been published.As implicitly mentioned in the previous section, there are two typesof weed-crop discrimiation approaches that are investigated: pixel-level (or pixel-wise) and image-wise classi ﬁcations. This subsection mainly focuses on pixel-wise classiﬁcation. In the pixel-wise classiﬁca- tion, each pixel in an image is segmented and marked as weed or croppixel. Pixel-wise approaches offer the advantage of being able to esti-mate the yield of each individual pixel. In applications that speci ﬁcally require this level of information, using a pixel-wise implementationwould be advantageous.The authors inDi Cicco et al. (2017)used a pixel-wise classiﬁcation CNN model SegNet. The SegNet model uses an encoder-decoder net-work structure and the fully connected layer is a softmax layer whichenables pixel classiﬁcation. The author used a synthetic dataset gener-ated by procedural generative model with real world plant textures ina virtual environment to train the classi ﬁcation model. The method was able to reduce efforts on manual pixel annotation of real world im-ages by a human. The synthetic dataset was used to train SegNet forcrop-weed pixel classiﬁcation and was able to achieve per-class averageaccuracy of 91.3%.The authors inMcCool et al. (2017)proposed the use of leveraging Deep CNN (DCNN) to learn lightweight models for pixel level crop-weed classiﬁcation problems. The authors proposed three stages totrain the classiﬁcation model. Theﬁrst stage used a pre-trained DCNNmodel which provides cutting edge classi ﬁcation performance but at a high computational cost. In mobile automated ﬁeld robotics application, this model was not suitable with low frame rate of 0.12 fps. Thus, thesecond stage used the pre-trained DCNN model as a teacher model totrain a lightweight DCNN student model. This method was known asmodel compression which results in lower accuracy, but much lowercomplexity. The third stage involved multiple lightweight model com-bined as a mixture of models which resulted in higher classi ﬁcation per- formance. The classiﬁcation rate of the combination of lightweightmodels was 88.9%. The research found that while cutting edge DCNNmodel was able to achieve 93.9% classiﬁcation accuracy, it was only able to process at 0.12 fps. The mixture of combined lightweightDCNN was able to compute at between 1.07 and 1.83 fps which is fasterwith less parameters.The authors inHaug and Ostermann (2015)proposed a plant classi- ﬁcation model without the use of segmentation. Often, in the agricul-tureﬁeld, crop and weeds grow too close together and overlapping isa common occurrence. Thus, the authors proposed the use of a RandomForest classiﬁer with statistical features to classify pixels rather thansegments. The classiﬁcation of pixels eliminated the challenges whichwere associated with segmentation methods such as overlapping andcomplex background. The classiﬁer was used to estimate the spatialpixel location based on features of overlapping neighborhood pixels.The model was further spatially smoothed with Markov Random Field.A sample of the prediction from this model is shown in Fig. 6.T h emodel was able to achieve an average classi ﬁcation accuracy of 85.9% in the CWFID benchmark dataset on binary classi ﬁcation. One of the most prominent DL approach that is speci ﬁc for segmen- tation is U-Net. U-Net has commonly been applied for medical imagessegmentation as demonstrated inArun et al. (2020). The authors in Subeesh et al. (2022)applied various pre-trained models for distin-guishing bell peppers and weeds. The overall accuracy of the investi-gated models ranged from 94.5% to 97.7% with InceptionV3outperformed the other models with 97.7% accuracy, 98.5% precision,and 97.8% recall at 30-epoch and 16-batch size. Furthermore, thefalse-positive for this Inception3 model was 1.4% and the false-negative was 0.9%. The approaches detected the entire vegetation objectinto either weed or crop class and they are not based on segmentation.InFarooq et al. (2019), remote sensing vegetation patches wereevaluated and classiﬁed as weed or crops. Because of high spectral sim-ilarity between weeds and crops, patch-based classi ﬁcation approaches were used in the paper. Furthermore, the methods of CNN and Histo-gram of Oriented Gradients (HoG) were assessed and compared. InBrilhador et al. (2019), the authors investigated the impact of individualdata augmentation transformations on the pixel-level classi ﬁcation of crops and weeds when using a DL model. The research focused on im-plementation of the image augmentation approach to increase trainingimage sets similar to the Generative Adversarial Networks (GAN) ap-proaches in generating synthetic images. In Wang et al. (2020),a n encoder-decoder DL network was investigated for pixel-wise semanticcrop and weed segmentation. To optimize the network's input, differentinput representations, including different color space transformationsF.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
21and color indices, were compared. In Sa et al. (2018), the authors pro- posed a method for dense semantic weed categorization using multi-spectral Micro Aerial Vehicle (MAV) images. In particular, the authorsemployed SegNet, a recently designed encoder-decoder cascadedCNN, to infer dense semantic classes in sugar beet and weed datasets.The authors inJiang et al. (2020)proposed a CNN-based Graph Convolutional Network (GCN). Based on semi-supervised learning, theGCN graph exploited labelled and unlabeled features to enrich themodel, and testing samples obtained label information from labelledweed data via propagating through the network. GCN-ResNet-101 ob-tained 97.80%, 99.37%, 98.93%, and 96.51% recognition accuracies onfour separate weed datasets, outperforming state-of-the-art networks(AlexNet, VGG16 and ResNet-101). In Dyrmann et al. (2016), the au- thors used a CNN to classify seedlings of 22 plant species. Plants wereaccurately classiﬁed 86.2% of the time. Indos Santos Ferreira et al. (2017), a DL method was proposed for soybean weed detection. In par-ticular, CNN was used on SLIC superpixels-segmented images and UAVphotos formed a picture database. This experiment obtained 97%weed detection accuracy using CNN.InBah et al. (2018), UAV images were collected and an attempt wasmade to classify into crop/weed pixels. Instead of manual annotation ofpixels which is time consuming, the authors proposed an unsupervisedapproach. The procedure contains multiple steps. First, crop rows weredetected to identify inter-row weeds. Inter-row weeds formed thetraining dataset in the second phase which was to distinguish fromthe weed. CNNs on this dataset were used to build a model which wasable to detect the crop and the weeds in the images. The results ob-tained were comparable to those of traditional supervised trainingdata labeling, with differences in accuracy of 1.5% in the spinach ﬁeld and 6% in the beanﬁeld when compared with supervised approach(hand annotation).InChavan and Nandedkar (2018), AlexNet and VGGNET were com-b i n e di nA g r o A V N E Tf o rc r o pw e e dc l a s s iﬁcation. Incremental learning used a system to learn new weeds and crops more successfully.Performance was compared to AlexNet, VGGNET, and other approaches.The proposed approach was evaluated using seedling dataset. The im-plementation yielded 98.21% (7 species detection) and 93.64% (12 spe-cies detection). InHu et al. (2020), the authors proposed a graph-basedDL architecture that recognizes weeds from RGB photographs of rangelands, called Graph Weeds Net (GWN). GWN collects regional patternsand creates multi-scale graph representations for weed classi ﬁcation. GWN also suggests key regions for robotic in- ﬁeld actions. The architec- ture achieved state-of-the-art performance with top-1 accuracy(98.1%).Another category of DL algorithms is Recurrent Neural Networks(RNNs) (Hochreiter and Schmidhuber, 1997 ;Verma and Dubey, 2021). The feedback loop available in RNNs allows them to perform asan expert forecasting machine rather than recognition by CNNs. Thenetworks try to make an inﬂuence of previous inputs in current inputsand outputs, which helps in increasing the overall performance. Theyprovide a pixel-level classiﬁcation for all the multispectral images avail-able. The generator in the network works in creating photo-realistic im-ages, aiming for extra training data.Table. 2summarizes some signiﬁcant research exploration involvingDL. The literature review has revealed that the majority ofimplementations have utilized CNN as the foundational approach for ei-ther segmentation-(or pixel-wise) or annotation (or image-wise)-based classiﬁcation.6. Publicly available datasetsAcquiring crop/weed images using UAV is not an easy task. Fortu-nately, there exist various public domain datasets that enable re-searchers to explore implementation of ML algorithm. We also dividethe datasets into two types: segmentation and annotation. Due to the
morphology of the plants, pixel-wise classi ﬁcation is more complex as they are mostly in green spectrum. Most of these datasets provide alogical mask to indicate if the referred pixel is weed or crop category.
Fig. 6.Example pixel based classiﬁcation without segmentation by Haug and Ostermann (2015).
Table 2DL implementation on weed crop recognition.Authors Approaches Application Achievement (Accuracy) Segmentation/AnnotationSubeesh et al., 2022 Pre-trained DL models Bell pepper vs. Weed 94.5%- -97.7% Annotation Arun et al., 2020 U-net Carrot vs weed (CWFID) 95.34% Segmenttion Farooq et al., 2019 CNN Remote sensing vegetation patches 94.72% Segmentation Brilhador et al., 2019 CNN/data augmentation Carrot vs weed (CWFID) 83.44% (average dice similarity)SegmentationWang et al., 2020 Coder/encoder Sugar beet vs weed (sugar beet dataset) 96.12% Segmentation Sa et al., 2018 Coder/encoder CNN Sugar beet vs weed (sugar beet dataset) ∼0.8 for F1/ 0.78 for AUC Segmentation Jiang et al., 2020 GCN Multiple public dataset 96.51%- -99.37% Segmentation Dyrmann et al., 2016 CNN 22 Species of seedling 86.2% Annotation dos Santos Ferreira et al.,2017 CNN Weed vs soybean 97% SegmentationBah et al., 2018 Unsupervised method/CNN Spinach/bean vs weed 94%/88.73% Segmentation Chavan and Nandedkar, 2018 Hybrid ResNet/AlexNet(Pre-trainedCNN)Dataset of seedlings from variousspecies. 98.21% (7 species) Segmentation93.64% (12 species) Hu et al., 2020 Graph deep learning DeepWeeds dataset 98.21% SegmentationF.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
22For pixel-based implementation, dataset normally consist of severalHigh Deﬁnition (HD) images in the order of 20 –100 as the images can be divided into smaller sections for evaluations/training. For the annota-tion type, the object plants are segmented using green index. Subse-quently, the entire image can be categorized as either weed or crop.This category normally carries more images as most implementationapplies some forms of DL approach such as CNN. The summary of thedatasets is shown inTable 3.7. Challenges and opportunitiesThe problem of identifying background from vegetation (crop orweed) is much more straightforward. However, this still presents achallenge. Agricultural scenes often have cluttered and textured back-grounds, which can interfere with the accurate segmentation of weedand crop regions. Shadows, soil variations, and other artifacts can ob-scure plant boundaries. Environmental conditions, such as soil type, hu-midity, and temperature, can affect plant appearances. Models trainedin one environment may not generalize well to different conditions.The key challenge in crop-weed classi ﬁcation for annotation- based techniques is the construction of the classi ﬁcation model and the optimization of the model parameters. The classi ﬁer model, like any other image classiﬁcation issue, is created for speciﬁc applica- tions, and its parameters must beﬁne-tuned and optimized. Classi-ﬁer optimization necessitates the use of several algorithms in orderto attain a high classiﬁcation rate while minimizing false positivesand data overﬁtting.Pixel-wise segmentation for weed –crop discrimination is also a challenging task. The most apparent is the high expense of resourcesand computation. Each pixel must be identi ﬁed to either a crop or a weed pixel, which is a difﬁcult task. This also has an impact on the anno-tation task for each pixel, which may result in incorrect annotation. It isalso worth noting that the majority of agriculture segmentation takesuse of UAV datasets. It is evident that weed pixels would be substan-tially smaller than crop pixels. This is known as a class imbalance situa-tion in machine learning. Models may become biassed in favour of thedominant class (crops), resulting in poor weed segmentation. Finally,we see that some weeds, in terms of color, texture, and shape, stronglyresemble certain crops. Therefore, distinguishing between these visu-ally similar classes becomes challenging, especially when the plantsare in early growth stages.In general, a few key challenges are listed out in the following:•The vegetation images obtained for the training of the ML model areoften obtained at different growth stages of the vegetation with vary-ing vegetation orientation and size.•In a natural growing environment, crop and weed tend to overlapeach other, thus applying image segmentation methods to separatec r o p sa n dw e e d si sd i fﬁcult.•When using too many features, clustering or forming of the relation-ship between observations becomes more dif ﬁcult and complex for the model or, in other words, the “curse of dimensionality”issue is the challenge.One essential to encouraging development is the use of edge devicesrather than ofﬂine or cloud-based alternatives. High quality images canbe difﬁcult to create and store. The majority of the ideas presented hereare computationally demanding and unsuitable for edge devices. Moreresearch work using techniques with reduced computation complexityor low energy consumption, such as FPGA-based approaches, may be in-cluded. Because the majority of these systems are linked to the Internet
of Things (IoT), edge devices can further optimize data transmission andreduce the number of data packets delivered.8. ConclusionVarious techniques employing ML have been investigated in thispaper. We present an overview of crop and weed recognition or dis-criminating algorithms. The biggest problem is how the technology isused, followed by the ways used owing to crop and weed morphology.As a result, determining which techniques are preferable on a “apple-to- apple”basis is extremely challenging. In this regard, stakeholders mayconsider implementing ways that are most comparable in order to ben-eﬁt from proven outcomes. We expect that this paper may give a com-plete overview of crop-weed detection in Agriculture 5.0 point of view.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgementsThis research was supported partly by ASEAN-India CollaborativeR&D scheme under ASEAN-India S&T Development Fund (AISTDF), FileNumber: CRD/2020/000248 and partly by Universitas Indonesia's Inter-national Indexed Publication (PUTI) Q2 Grant, year 2023, number: NKB-803/UN2.RST/HKP.05.00/2023.References
Ahmad, I., Siddiqi, M.H., Fatima, I., Lee, S., Lee, Y.-K., 2011. Weed classiﬁcation based on haar wavelet transform via k-nearest neighbor ( k−nn) for real-time automatic sprayer control system. Proceedings of the 5th International Conference on Ubiqui-tous Information Management and Communication, ICUIMC ’11, New York, NY, USA. Ahmad, J., Muhammad, K., Ahmad, I., Ahmad, W., Smith, M.L., Smith, L.N., Jain, D.K., Wang,H., Mehmood, I., 2018.Visual features based boosted classi ﬁcation of weeds for real- time selective herbicide sprayer systems. Comput. Ind. 98, 23 –33.Table 3Details on publicly available dataset.Name Author No of samplesApplication Segmentation/AnnotationCWFID Haug and Ostermann, 2015 60 Carrot-weed Segmentation Cowpea dataset LR et al., 2021 150 Southern pea-weed Segmentation Carrot-Weed dataset Lameski et al., 2017 39 Carrot-weed Segmentation Crop-weed discrimination dataset Bosilj et al., 2020 20 Onion-weed Segmentation Paddy crops-weeds digital image dataset Kamath and Balachnadra,2021 300 Paddy-weed SegmentationRice seedling and weed imagesegmentation Ma et al., 2019 224 Rice seedling-weed SegmentationFood crops and weed images Sudars et al., 2020 1118 6 food crops-8 weed species Annotation Weed species classiﬁcationSkovsen et al., 2019 261 Vegetation biomass Segmentation Plant seedling classiﬁcationGiselsson et al., 2017 961 12 species at several growth stages Annotation DeepWeeds Olsen et al., 2019 17509 8 species of weeds native to Australia in situ with neighboringﬂora AnnotationF.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
23Ahmed, F., Bari, A.H., Shihavuddin, A., Al-Mamun, H.A., Kwan, P., 2011. A study on local binary pattern for automated weed classi ﬁcation using template matching and sup- port vector machine. 2011 IEEE 12th International Symposium on Computational In-telligence and Informatics (CINTI), pp. 329 –334. Ahmed, F., Al-Mamun, H.A., Bari, A.H., Hossain, E., Kwan, P., 2012. Classiﬁcation of crops and weeds from digital images: a support vector machine approach. Crop Prot. 40,98–104.Ahmed, F., Kabir, M.H., Bhuyan, S., Bari, H., Hossain, E., 2014. A u t o m a t e dw e e dc l a s s iﬁca- tion with local pattern-based texture descriptors. Int. Arab J. Inform. Technol. 11 (1),87–94.Albawi, S., Mohammed, T.A., Al-Zawi, S., 2017. Understanding of a convolutional neural network. 2017 International Conference on Engineering and Technology (ICET),pp. 1–6.Arun, R.A., Umamaheswari, S., Jain, A.V., 2020. Reduced u-net architecture for classifying crop and weed using pixel-wise segmentation. 2020 IEEE International Conferencefor Innovation in Technology (INOCON), pp. 1 –6. Bah, M.D., Haﬁane, A., Canals, R., 2017.Weeds detection in uav imagery using slic and the hough transform. 2017 Seventh International Conference on Image Processing The-ory, Tools and Applications (IPTA), pp. 1 –6. Bah, M., Haﬁane, A., Canals, R., 2018.Deep learning with unsupervised data labeling for weed detection in line crops in uav images. Remote Sensing 10.Bakhshipour, A., Jafari, A., Nassiri, S.M., Zare, D., 2017. Weed segmentation using texture features extracted from wavelet sub-images. Biosyst. Eng. 157, 1 –12. Bannari, A., Morin, D., Bonn, F., Huete, A.R., 1995. A review of vegetation indices. Remote Sens. Rev. 13 (1–2), 95–120.Bishop, C.M., 2006.Pattern Recognition and Machine Learning.Bosilj, P., Duckett, T., Cielniak, G., 2018. Connected attribute morphology for uni ﬁed veg- etation segmentation and classiﬁcation in precision agriculture. Comput. Indus. 98, 226–240.Bosilj, P., Aptoula, E., Duckett, T., Cielniak, G., 2020. Transfer learning between crop types for semantic segmentation of crops versus weeds in precision agriculture. J. FieldRobot. 37 (1), 7–19.Brilhador, A., Gutoski, M., Hattori, L.T., de Souza Inácio, A., Lazzaretti, A.E., Lopes, H.S.,2019.Classiﬁcation of weeds and crops at the pixel-level using convolutional neuralnetworks and data augmentation. 2019 IEEE Latin American Conference on Compu-tational Intelligence (LA-CCI), pp. 1 –6. Chaki, J., Parekh, R., Bhattacharya, S., 2015. Plant leaf recognition using texture and shape features with neural classiﬁers. Pattern Recogn. Lett. 58, 61 –68. Chang, Y.K., Zaman, Q.U., Schumann, A.W., Percival, D.C., Esau, T.J., Ayalew, G., 2012.Development of color co-occurrence matrix based machine vision algorithms forwild blueberryﬁelds. Smart Agric. Technol 28 (3), 315 –
323. Chavan, T.R., Nandedkar, A.V., 2018. Agroavnet for crops and weeds classi ﬁcation: a step forward in automatic farming. Comput. Electron. Agric. 154, 361 –372. De Castro, A.I., Torres-Sánchez, J., Peña, J.M., Jiménez-Brenes, F.M., Csillik, O., López-Granados, F., 2018.An automatic random forest-obia algorithm for early weed map-ping between and within crop rows using uav imagery. Remote Sens. (Basel) 10 (2).Di Cicco, M., Potena, C., Grisetti, G., Pretto, A., 2017. Automatic model based dataset gen- eration for fast and accurate crop and weeds detection. 2017 IEEE/RSJ InternationalConference on Intelligent Robots and Systems (IROS), pp. 5188 –5195. dos Santos Ferreira, A., Matte Freitas, D., Gonçalves da Silva, G., Pistori, H., TheophiloFolhes, M., 2017.Weed detection in soybean crops using convnets. Comput. Elect.Agric. 143, 314–324.Drucker, H., Cortes, C., Jackel, L.D., LeCun, Y., Vapnik, V., 1994. Boosting and other ensem- ble methods. Neural Comput. 6 (6), 1289 –1301. Dyrmann, M., Karstoft, H., Midtiby, H.S., 2016. Plant species classiﬁcation using deep convolutional neural network. Biosyst. Eng. 151, 72 –80. Farooq, A., Hu, J., Jia, X., 2019. Analysis of spectral bands and spatial resolutions for weedclassiﬁcation via deep convolutional neural network. IEEE Geosci. Remote Sens. Lett.16 (2), 183–187.Feyaerts, F., Van Gool, L., 2001. Multi-spectral vision system for weed detection. PatternRecogn. Lett. 22 (6), 667–674. Fumera, G., Roli, F., 2005.A theoretical and experimental analysis of linear combiners formultiple classiﬁer systems. IEEE Trans. Pattern Anal. Mach. Intell. 27 (6), 942 –956. Gée, C., Bossu, J., Jones, G., Truchetet, F., 2008. Crop/weed discrimination in perspective agronomic images. Comput. Elect. Agric. 60 (1), 49 –59. Giselsson, T., Jørgensen, R., Jensen, P., Dyrmann, M., Midtiby, H., 2017. A Public Image Database for Benchmark of Plant Seedling Classi ﬁcation Algorithms. Goel, P., Prasher, S., Patel, R., Landry, J., Bonnell, R., Viau, A., 2003. Classiﬁcation of hyperspectral data by decision trees and arti ﬁcial neural networks to identify weed stress and nitrogen status of corn. Comput. Elect. Agric. 39 (2), 67 –93. Haralick, R.M., Shanmugam, K., Dinstein, I.H., 1973. Textural features for image classiﬁca- tion. IEEE Trans. Syst. Man Cybern. 6, 610 –621. Hasan, A.S.M.M., Sohel, F., Diepeveen, D., Laga, H., Jones, M.G., 2021. A survey of deep learning techniques for weed detection from images. Comput. Elect. Agric. 184,106067.Hatﬁeld, P., Pinter, P., 1993.Remote sensing for crop protection. Crop Prot. 12 (6),403–413.Haug, S., Ostermann, J., 2015.Ac r o p / w e e dﬁeld image dataset for the evaluation of com- puter vision based precision agriculture tasks. Computer Vision - ECCV 2014 Work-shops.He, D.-C., Wang, L., 1990.
Texture unit, texture spectrum, and texture analysis. IEEE Trans.Geosci. Remote Sens. 28 (4), 509 –512. Hemming, J., Rath, T., 2001. Pa—precision agriculture: computer-vision-based weed identiﬁcation underﬁeld conditions using controlled lighting. J. Agric. Eng. Res. 78(3), 233–243.Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8), 1735–1780.Hu, K., Coleman, G., Zeng, S., Wang, Z., Walsh, M., 2020. Graph weeds net: a graph-based deep learning method for weed recognition. Comput. Elect. Agric. 174, 105520.Jakubczyk, K., Siemiatkowska, B., Wieckowski, R., Rapcewicz, J., 2023. Hyperspectral imag- ing for mobile robot navigation. Sensors 23 (1).Jeon, H.Y., Tian, L.F., Zhu, H., 2011. Robust crop and weed segmentation under uncon- trolled outdoor illumination. Sensors 11 (6), 6270 –6283. Jiang, H., Zhang, C., Qiao, Y., Zhang, Z., Zhang, W., Song, C., 2020. Cnn feature based graph convolutional network for weed and crop recognition in smart farming. Comput.Elect. Agric. 174, 105450.Jin, H., Liu, Q., Lu, H., Tong, X., 2004. Face detection using improved lbp under bayesian framework. Third International Conference on Image and Graphics (ICIG ’04), pp. 306–309.Kamath, R., Balachnadra, M., 2021. Paddy Crop and Weeds Digital Image Dataset. Kim, Y., Reid, J.F., 2006.Spectral sensing for plant stress assessment - a review. Agric.Biosyst. Eng. 7 (1), 27–41.Kittler, J., 1998.Combining classiﬁers: a theoretical framework. Pattern. Anal. Applic. 1 (1), 18–27.Kittler, J., Hatef, M., Duin, R.P., Matas, J., 1998. On combining classiﬁers. IEEE Trans. Pattern Anal. Mach. Intell. 20 (3), 226 –239. Kumar, D.A., Prema, P., 2016. A novel wrapping curvelet transformation based angulartexture pattern (wctatp) extraction method for weed identi ﬁcation. ICTACT J. Image Video Process. 6 (3).Kuncheva, L.I., 2014.Combining Pattern Classiﬁers: Methods and Algorithms. John Wiley & Sons.Lam, L., Suen, C.Y., 1995.Optimal combinations of pattern classi ﬁers. Pattern Recogn. Lett. 16 (9), 945–954.Lameski, P., Zdravevski, E., Trajkovik, V., Kulakov, A., 2017. Weed Detection Dataset with RGB Images Taken Under Variable Light Conditions. pp. 112 –119. Laplace, P.S., 1820.Théorie analytique des probabilités. Courcier.Lati, R.N., Rasmussen, J., Andujar, D., Dorado, J., Berge, T.W., Wellhausen, C., P ﬂanz, M., Nordmeyer, H., Schirrmann, M., Eizenberg, H., Neve, P., Jørgensen, R.N., Christensen,S., 2021.Site-speciﬁc weed management—constraints and opportunities for the weed research community: insights from a workshop. Weed Res. 61 (3), 147–153. Le, V.N.T., Apopei, B., Alameh, K., 2019. Effective plant discrimination based on the combi- nation of local binary pattern operators and multiclass support vector machinemethods. Inform. Process. Agric. 6 (1), 116 –131. Lease, B.A., Wong, W.K., Gopal, L., Chiong, C.W.R., 2020. Pixel-level weed classiﬁcation using evolutionary selection of local binary pattern in a stochastic optimised ensem-ble. SN Comput. Sci. 1 (337), 1 –13. Liu, L., Fieguth, P., Zhao, G., Pietikäinen, M., Hu, D., 2016. Extended local binary patterns for face recognition. Inform. Sci. 358, 56 –72. López-Granados, F., 2011.Weed detection for site-speciﬁc weed management: mapping and real-time approaches. Weed Res. 51 (1), 1 –11. LR, V.N., Krishnan, A., Krishnan, K.V., P, H, Haritha, Z.A.S.A., 2021. Southern pea/weedﬁeld image dataset for semantic segmentation and crop/weed classi ﬁcation using an encoder-decoder network. Proceedings of the International Conference on Systems,Energy & Environment (ICSEE) 2021.Lu, B., He, Y., Dao, P.D., 2019. Comparing the performance of multispectral and hyperspectral images for estimating vegetation properties. IEEE J. Select. TopicsAppl. Earth Observ. Remote Sensing 12 (6), 1784 –1797. Ma, X., Deng, X., Qi, L., Jiang, Y., Li, H., Wang, Y., Xing, X., 2019. Fully convolutional network for rice seedling and weed image segmentation at the seedling stage in paddy ﬁelds. PloS One 14 (4), 1–13.Mäenpää, T., Pietikäinen, M., 2005. Texture analysis with local binary patterns. Handbook of Pattern Recognition and Computer Vision, pp. 197 –216. Manea, D., Calin, M.A., 2015.Hyperspectral imaging in different light conditions. ImagingSci. J. 63 (4), 214–219.McCool, C., Perez, T., Upcroft, B., 2017. Mixtures of lightweight deep convolutional neural networks: applied to agricultural robotics. IEEE Robot. Automat. Lett. 2 (3),1344–1351.Mróz, M., Sobieraj, A., 2004.Comparison of several vegetation indices calculated on thebasis of a seasonal spot xs time series, and their suitability for land cover and agricul-tural crop identiﬁcation. Technical Report. University of Warmia and Mazury in Olsz-tyn.Neupane, K., Baysal-Gurel, F., 2021. Automatic identiﬁcation and monitoring of plant dis- eases using unmanned aerial vehicles: a review. Remote Sens. (Basel) 13 (19).Ojala, T., Pietikäinen, M., Mäenpää, T., 2002. Multiresolution gray-scale and rotation in- variant texture classiﬁcation with local binary patterns. IEEE Trans. Pattern Anal.Mach. Intel. 7, 971–987.Olsen, A., Konovalov, D.A., Philippa, B., Ridd, P., Wood, J.C., Johns, J., Banks, W., Girgenti, B.,Kenny, O., Whinney, J., Calvert, B., Azghadi, M.R., White, R.D., 2019. Deepweeds: a multiclass weed species image dataset for deep learning. Sci. Rep. 9, 2058.Otsu, N., 1979.A threshold selection method from gray-level histograms. IEEE Trans. Syst.Man Cybern. 9 (1), 62–66.Patrício, D.I., Rieder, R., 2018.Computer vision and artiﬁcial intelligence in precision agri- culture for grain crops: a systematic review. Comput. Elect. Agric. 153, 69 –
81. Polikar, R., Topalis, A., Parikh, D., Green, D., Frymiare, J., Kounios, J., Clark, C.M., 2008. An ensemble based data fusion approach for early diagnosis of Alzheimer ’sd i s e a s e . Inform. Fusion 9 (1), 83–95.Qin, J., Chao, K., Kim, M.S., Lu, R., Burks, T.F., 2013. Hyperspectral and multispectral imag- ing for evaluating food safety and quality. J. Food Eng. 118 (2), 157 –171. Rallabandi, V.S., Sett, S., 2005. Unsupervised texture classiﬁcation and segmentation. WEC. 5, pp. 299–302.F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
24Sa, I., Chen, Z., Popović, M., Khanna, R., Liebisch, F., Nieto, J., Siegwart, R., 2018. Weednet: dense semantic weed classiﬁcation using multispectral images and mav for smartfarming. IEEE Robot. Automat. Lett. 3 (1), 588 –595. Saiz-Rubio, V., Rovira-Más, F., 2020. From smart farming towards agriculture 5.0: a review on crop data management. Agronomy 10 (2).Samarajeewa, T., Suduwella, C., Jayasuriya, N., Kumarasinghe, P., Gunawardana, K., DeZoysa, K., Keppitiyagama, C., 2018. Identiﬁcation of lantana camara distribution using convolutional neural networks. 2018 18th International Conference on Ad-vances in ICT for Emerging Regions (ICTer), pp. 221 –228. Sankaran, S., Khot, L.R., Maja, J.M., Ehsani, R., 2013. Comparison of two multiband cameras for use on small uavs in agriculture. 2013 5th Workshop on Hyperspectral Image andSignal Processing: Evolution in Remote Sensing (WHISPERS), pp. 1 –4. Shahbudin, S., Zamri, M., Kassim, M., Abdullah, S.A.C., Suliman, S.I., 2017. Weed classiﬁca- tion using one class support vector machine. 2017 International Conference onElectrical, Electronics and System Engineering (ICEESE), pp. 7 –10. Skovsen, S., Dyrmann, M., Krogh Mortensen, A., Laursen, M., Gislum, R., Eriksen, J.,Farkhani, S., Karstoft, H., Jørgensen, R., 2019. The Grassclover Image Dataset for Semantic and Hierarchical Species Understanding in Agriculture. pp. 2676 –2684. Søgaard, H.T., 2005.Weed classiﬁcation by active shape models. Biosyst. Eng. 91 (3), 271–281.Somasundaram, K., Genish, T., 2012. Modiﬁed otsu thresholding technique. Commun. Comput. Inform. Sci. 283, 445 –448. Speck, O., Speck, T., 2021.Functional morphology of plants –a key to biomimetic applica- tions. New Phytol. 231 (3), 950 –956. Subeesh, A., Singh, B., Chandel, N.S., Rajwade, Y.A., Rao, K.V.R., Kumar, S.P., Jat, D., 2022.Deep convolutional neural network models for weed detection in polyhouse grownbell peppers. Artif. Intel. Agric. 6, 47 –54. Sudars, K., Jasko, J., Namatevs, I., Ozola, L., Badaukis, N., 2020. Dataset of annotated food crops and weed images for robotic computer vision control. Data Brief 31, 105833.Susetyarini, E., Wahyono, P., Latifa, R., Nurrohman, E., 2020. The identiﬁcation of morpho- logical and anatomical structures of pluchea indica. J. Phys. Conf. Ser. 1539 (1),012001.Tajeripour, F., Kabir, E., Sheikhi, A., 2008. Fabric defect detection using modi ﬁed local binary patterns. EURASIP J. Adv. Signal Process. 2008, 60.Torres Sospedra, J., et al., 2011. Ensembles of artiﬁcial neural networks: Analysis and development of design methods. PhD thesisUniversitat Jaume I.Torres-Sospedra, J., Nebot, P., 2014. Two-stage procedure based on smoothed ensembles of neural networks applied to weed detection in orange groves. Biosyst. Eng. 123,40–55.Tripicchio, P., Satler, M., Dabisias, G., Ruffaldi, E., Avizzano, C.A., 2015. Towards smart farming and sustainable agriculture with drones. 2015 International Conference onIntelligent Environments, pp. 140 –143. Verma, T., Dubey, S., 2021.Prediction of diseased rice plant using video processing andLSTM-simple recurrent neural network with comparative study. Multimed. Tools
Appl. 80, 29267–29298.Wang, A., Zhang, W., Wei, X., 2019. A review on weed detection using ground-based ma- chine vision and image processing techniques. Comput. Elect. Agric. 158, 226 –240. Wang, A., Xu, Y., Wei, X., Cui, B., 2020. Semantic segmentation of crop and weed using an encoder-decoder network and image enhancement method under uncontrolled out-door illumination. IEEE Access 8, 81724 –81734. Woebbecke, D.M., Meyer, G.E., Bargen, K.V., Mortensen, D.A., 1995. Color indices for weed identiﬁcation under various soil, residue, and lighting conditions. Trans. ASAE 38 (1),259–269.Wong, W.K., Juwono, F.H., Apriono, C., 2021. Vision-based malware detection: a transfer learning approach using optimal ecoc-svm con ﬁguration. IEEE Access 9, 159262–159270.Xu, Y., Gao, Z., Khot, L., Meng, X., Zhang, Q., 2018. A real-time weed mapping and precision herbicide spraying system for row crops. Sensors 18 (12).Xu, X.-k, Li, X.-m, Zhang, R.-h, 2019. Remote conﬁgurable image acquisition lifting robot for smart agriculture. 2019 IEEE 4th Advanced Information Technology, Electronicand Automation Control Conference (IAEAC). vol. 1, pp. 1545 –1548. Xue, J., Su, B., 2017.Signiﬁcant remote sensing vegetation indices: a review of develop-ments and applications. J. Sensors 2017.Xue, Q., Yang, B., Wang, F., Tian, Z., Bai, H., Li, Q., Cao, D., 2021. Compact, uav-mounted hyperspectral imaging system with automatic geometric distortion recti ﬁcation. Opt. Express 29 (4), 6092 –6112. Zhao, G., Pietikainen, M., 2006. Local binary pattern descriptors for dynamic texture rec-ognition. 18th International Conference on Pattern Recognition (ICPR ’06). vol. 2, pp. 211–214.F.H. Juwono, W.K. Wong, S. Verma et al. Artiﬁcial Intelligence in Agriculture 10 (2023) 13 –25
25