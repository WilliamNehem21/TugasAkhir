Deep belief networks and cortical algorithms: A comparative studyfor supervised classiﬁcation
Yara Rizk, Nadine Hajj, Nicholas Mitri, Mariette Awad⇑
Department of Electrical and Computer Engineering, American University of Beirut, Beirut, Lebanon
article info
Article history:Received 23 March 2017Revised 17 January 2018Accepted 17 January 2018Available online 3 March 2018Keywords:Deep learningDeep belief networksCortical algorithmsabstract
The failure of shallow neural network architectures in replicating human intelligence led the machinelearning community to focus on deep learning, to computationally match human intelligence. The wideavailability of increasing computing power coupled with the development of more efﬁcient training algo-rithms have allowed the implementation of deep learning principles in a manner and span that had notbeen previously possible. This has led to the inception of deep architectures that capitalize on recentadvances in artiﬁcial intelligence and insights from cognitive neuroscience to provide better learningsolutions. In this paper, we discuss two such algorithms that represent different approaches to deeplearning with varied levels of maturity. The more mature but less biologically inspired Deep BeliefNetwork (DBN) and the more biologically grounded Cortical Algorithms (CA) are ﬁrst introduced to givereaders a bird’s eye view of the higher-level concepts that make up these algorithms, as well as some oftheir technical underpinnings and applications. Their theoretical computational complexity is thenderived before comparing their empirical performance on some publicly available classiﬁcation datasets.Multiple network architectures were compared and showed that CA outperformed DBN on most datasets,with the best network architecture consisting of six hidden layers./C2112018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Contents1. Introduction . . ........................................................................................................ 8 22. Artificial neural networks history . . . . . .................................................................................... 8 2 2.1. Concepts from neuroscience. . . . . . . . . . . . . . .......................................................................... 8 2 2.2. Shallow beginnings . . . . . .......................................................................................... 8 3 2.3. Shallow networks’ limitations . . . . . . . . . . . . .......................................................................... 8 3 2.4. Deep architectures . . . . . .......................................................................................... 8 3 3. Deep belief networks . . . . . . . . . . . . . . . .................................................................................... 8 4 3.1. Overview . . . . . . . . . . . . . .......................................................................................... 8 4 3.2. Network structure . . . . . . .......................................................................................... 8 43.2.1. Restricted Boltzmann machines. . . . . . . . . . . . . . . . . . ............................................................ 8 4 3.2.2. Deep belief networks . . . . . . . ............................................................................... 8 4 3.3. Training algorithm. . . . . . .......................................................................................... 8 53.3.1. Restricted Boltzmann machines. . . . . . . . . . . . . . . . . . ............................................................ 8 5 3.3.2. Deep belief networks . . . . . . . ............................................................................... 8 5
https://doi.org/10.1016/j.aci.2018.01.0042210-8327//C2112018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author.E-mail addresses:yar01@aub.edu.lb(Y. Rizk),njh05@aub.edu.lb(N. Hajj),ngm04@aub.edu.lb(N. Mitri),mariette.awad@aub.edu.lb(M. Awad). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 15 (2019) 81–93
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
4. Cortical algorithms. . . . . . . . . . . .......................................................................................... 8 5 4.1. Overview . . . . . . ................................................................................................. 8 54.2. Network structure................................................................................................ 8 54.3. Mathematical model . . . . . . . . . . . . . . . . . ............................................................................. 8 6 4.4. Training algorithm. . . . . . . . . . . . . . . . . . . ............................................................................. 8 6 4.4.1. Random initialization . . . . .................................................................................. 8 6 4.4.2. Unsupervised feed-forward learning . . . . . . . . .................................................................. 8 6 4.4.3. Supervised feedback learning . . . . . . . . . . . . . .................................................................. 8 7 5. Theoretical computational complexity . . . . . . . . . . . . . . ....................................................................... 8 7 5.1. Number of non-zero weights . . . . . . . . . . ............................................................................. 8 8 5.1.1. DBN . . . . . . . . . . . . . . . . . . .................................................................................. 8 8 5.1.2. CA..................................................................................................... 8 85.2. Number of operations per neuron . . . . . . ............................................................................. 8 8 5.2.1. Summation . . . . . . . . . . . . .................................................................................. 8 8 5.2.2. Activation function . . . . . . .................................................................................. 8 8 5.3. Pruning. . . . . . . . ................................................................................................. 8 95.4. Overall computational complexity . . . . . . ............................................................................. 8 9 6. Empirical comparison . . . . . . . . .......................................................................................... 8 9 6.1. Experimental setup . . . . . . . . . . . . . . . . . . ............................................................................. 8 9 6.2. Classification results . . . . . . . . . . . . . . . . . ............................................................................. 9 1 6.3. Network connectivity . . . . . . . . . . . . . . . . ............................................................................. 9 1 6.4. Effect of batch size . . . . . . . . . . . . . . . . . . ............................................................................. 9 1 6.5. Statistical analysis................................................................................................ 9 17. Conclusion ........................................................................................................... 9 2Acknowledgment . . . . . . . . . . . . .......................................................................................... 9 2 References ........................................................................................................... 9 2
1. IntroductionIn an endeavor to replicate human level intelligence, artiﬁcialintelligence (AI) research has fused insights from the ﬁelds of com-puter science, cognitive neuroscience, computational science, and alitany of others to produce algorithms that perform with increasingefﬁcacy on what is arguably the core element of intelligence:learning.Notable among the many learning algorithms in AI are artiﬁcialneural networks (ANN) and their many variants. ANN are collec-tions of interconnected artiﬁcial neurons that incrementally learnfrom their environment and attempt to mimic some of the basicinformation processing processes in the brain. Their function isdeﬁned by the processing performed at the neuron level, the con-nection strengths between neurons (synaptic weights), and net-work structure (organization and linkage of neurons) [1]. It is the latter that resides at the core of the discussion presented herein.Throughout their evolution, discussed in more details in thenext section, shallow ANN still suffer from multiple issues in thecontext of complex applications requiring a higher level of abstrac-tion. However, with the rapid increase in processing power, theopportunity to successfully implement the computationallydemanding designs of deeper architectures has recently emerged.The development of efﬁcient training algorithm such as Hintonet al.’s greedy algorithm[2]has also helped ANN’s resurgence.Furthermore, ﬁndings in computational neuroscience have led toincreased interest in deep, biologically inspired architectures[3–5]which adhere more faithfully to neuro-scientiﬁc theories ofthe human brain’s topology.In this paper, we limit the scope of our comparative study totwo - nowadays popular - algorithms: Hinton et al.’s Deep BeliefNetworks (DBN)[2], and Cortical Algorithms (CA)[6]. While many other deep architectures have been developed, including longshort-term memory for sequential data processing and convolu-tional neural networks for image processing, this comparativestudy compares feedforward architectures. Speciﬁcally, DBN, oneof the more efﬁcient deep architecture training algorithms is com-pared to CA, a feedforward architecture with more biologicallyfaithful properties. Deep neural networks (DNN), speciﬁcallyDBN, is presented as the state of the art of ANN in their traditionalforms with network topologies built from layers of neuron modelsbut with more advanced learning mechanics and deeper architec-ture, without modeling the detailed biological phenomena consti-tuting human intelligence. Maintaining a high-level abstraction ofthe biological modeling, results in simpler mathematical modelsfor DBN compared to CA. On the other hand, CA represents the shifttowards incorporating more biologically inspired structures thanDBN, like cortical columns and inhibiting and strengthening learn-ing rules, as outlined by Edelman and Mountcastle’s work [7]. The structure of the paper is such that Section 2summarizes the history of ANN while Sections3 and 4review the fundamental con- cepts and learning schemes of DBN and CA, respectively. Section 5 derives both algorithms’ theoretical computational complexity.Finally, Section6presents an empirical comparison on classiﬁca-tion tasks before concluding with closing remarks in Section 7.2. Artiﬁcial neural networks historyBefore delving into the deeper network structures presented inthis paper, we will go over the evolution of neural networks fromtheir shallow beginnings to the complex structures that haverecently become popular.2.1. Concepts from neuroscienceDespite the advances in neuroscience and technology that haveallowed for a detailed description of the structure of the brain, thelearning process in the brain is yet to be completely understood.Biologically, the brain mainly consists of the cerebrum, the cerebel-lum, and the brain stem[8].The cerebral cortex, biologically deﬁned as the outer layer of tis-sue in the cerebrum and believed to be responsible for higher orderfunctioning, is an association of an estimated 25 billion neuronsinterconnected through thousands of kilometers of axons propa-gating and spreading about 10
14synapses simultaneously[9], arranged in six layers and divided into regions, each performinga speciﬁc task[10].82 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93Though it is not very clear how certain areas in the brainbecome specialized, it is known that multiple factors affect thefunctional specialization of the brain areas such as structure, con-nectivity, physiology, development and evolution [11]. Neurons, considered the basic element in the brain, have different shapesand sizes but are all variations of the same underlying scheme,i.e. they start the same general-purpose function but become spe-cialized with training[12]. While dendrites are the site of receptionof synaptic inputs, axons convey electrical signals over long dis-tances. Inputs to neurons cause a slow potential change in the stateof the neuron; its characteristics are determined by the membranecapacitance and resistance allowing temporal summation [13]. Studies showed that the organization of the cortex can beregarded as an association of columnar units [14,15], each column being a group of nodes sharing the same properties. Learning in thehuman brain is mainly performed using plastic connections,repeated exposures and ﬁring and inhibition of neurons. In a sim-pliﬁed manner, information ﬂowing in the cortex causes connec-tions in the brain to become active, over time, with repeatedexposures these connections are strengthened creating a represen-tation of the information processed in the brain. Moreover, inhibi-tion of neurons - physically deﬁned as prohibiting neurons fromﬁring - partly account for the forgetting process [16].2.2. Shallow beginningsAt a nodal level, ANN started with the simpliﬁed McCulloch-Pitts neural model (1943)[17], which was composed of a basicsummation unit with a deterministic binary activation function.Successors added complexity with every iteration. At the level ofactivation functions, linear, sigmoid, and Gaussian functions cameinto use. Outputs were no longer restricted to real values andextended to the complex domain. Deterministic models gave wayto stochastic neurons and spiking neurons which simulated ionicexchanges. All these additions were made to achieve more sophis-ticated learning models.At the network level, topologies started out with single layeredarchitectures such as Rosenblatt’s perceptron (1957) [18], Widrow and Hoff’s ADALINE network (1960) [19]and Aizerman’s kernel perceptron (1964)[20]. These architectures suffered from poorperformance and could not learn the XOR problem, a simple butnon-linear binary classiﬁcation problem. This led to the introduc-tion of more complex networks starting with the multilayer per-ceptron (Rumelhart, 1986)[21], self-recurrent Hopﬁeld networks(1986)[22], self-organizing maps (SOM or Kohonen networks,1986)[23], adaptive resonance theory (ART) networks (1980s)[24]and various others which are considered shallow architecturesdue to the small number of hidden layers.Successive iterations incrementally improved on their prede-cessors’ shortcomings and promised higher levels of intelligence,a claim that was made partially feasible due to the hardware’simproved computational capabilities [25]and due to the develop- ment of faster and more efﬁcient training and learning algorithms.Learning mechanics, whether supervised (back propagation) orunsupervised (feed forward algorithms), matured in parallel andallowed for better performance in a varied set of speciﬁc tasks.Nonetheless, the compound effect of the innovation targeting allaspects of these shallow networks was not enough to capture truehuman intelligence while large computational needs throttled theprogress of deeper networks.2.3. Shallow networks’ limitationsSupervised learning presents many challenges includingthe curse of dimensionality[26]where the increase in the numberof features and training samples makes learning morecomputationally demanding. Furthermore, non-linear data is moredifﬁcult to divide into classes due to the inherent feature overlap.Unable to position themselves as strong AI models - general intel-ligent acts as deﬁned by Kurzweil - which can faithfully emulatehuman intelligence, ANN lagged Support Vector Machines (SVM)[27]in the 1990–2000s.2.4. Deep architecturesThe early 2000s saw a resurgence in ANN research due toincreased processing power and the introduction of more efﬁ-cient training algorithms which made training deep architec-
tures feasible. Hinton et al.’s greedy training algorithm [2] simpliﬁed the training procedure of Boltzmann machines whiledeep stacking networks broke down training to the constitut-ing blocks of the deep network to reduce the computationalburden. Furthermore, Schmidhuber’s long short-term memoryarchitecture[28]allowed the training of deeper recurrent neu-ral networks. While these architectures do not borrow biologi-cal properties from the brain beyond the neuron, deeparchitectures with neural network topologies that adhere morefaithfully to neuro-scientiﬁc theories of the human brain’stopology are gaining traction in the connectionist communitydue in part to the momentum achieved in computationalneuroscience.One of the major and most relevant contributions in that ﬁeldwas made by Edelman and Mountcastle [7]. Their ﬁndings lead to a shift from positioning simpliﬁed neuron models as fundamen-tal functional units of an architecture to elevating that role to cor-tical columns, collections of cells characterized by common feed-forward connections and strong inhibitory inter connections. Thisprovided a biologically feasible mechanism for learning and form-ing invariant representations of sensory patterns that earlier ANNdid not.Additionally, two supplementary discoveries were believed tobe key in emulating human intelligence. The ﬁrst was the sus-pected existence of a common computational algorithm in theneocortex[12]. This algorithm is pervasive throughout theseregions irrespective of the underlying mental faculty. Whetherthe task is visual, auditory, olfactory, or other, the brain seemsto deal with sensory information in very similar ways. The sec-ond was the hierarchical structure of the human neocortex[12]. The brain’s regions are hierarchically connected so thatthe bidirectional ﬂow of information merges into more complexrepresentations with every layer, further abstracting the sensorystimuli.The combination of these two ﬁndings forms potentialgrounds for building a framework that replicates human intelli-gence; a hierarchy of biologically inspired functional units thatimplement a common algorithm. These novel insights from neu-roscience have been reﬂected in the machine learning (ML) andAI ﬁelds and have been implemented to varying layers in severalalgorithms.While CA restructured the neurons and their connectionsas well as the learning algorithm [6]based on Edelman and Mountcastle’s ﬁnding[7], other algorithms modeled otherbiological theories of the brain’s workings. Symbolic architec-tures such as Adaptive Character of Thought (ACT-R) [29] modeled working memory coupled with centralized controlthat refers to long term memory when needed. Emergentistarchitectures such as Hierarchical Temporal Memory (HTM)[30]are based on globalist memory models and use rein-forcement or competitive learning schemes to generate theirmodels. Integrating both classes of architectures to formhybrid architectures also exist and include Learning IntelligentDistribution Agent (LIDA)[31].Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 833. Deep belief networks3.1. OverviewDNN are deeper extensions of shallow ANN architectures thatare composed of a simpliﬁed mathematical model of the biologicalneuron but do not aim to faithfully model the human brain as doCA or some other ML approaches. DNN are based on the Neocogni-tron, a biologically inspired image processing model [32], that attempt to realize strong AI models through hierarchical abstrac-tion of knowledge. Information representation is learned as datapropagates through the network, shallower layers learn low-levelstatistical features while deeper layers build on these features tolearn more abstract and complex representations. Lacking clearskills for logical inferences, DNN need more morphing to be ableto integrate abstract knowledge in a human manner. Recurrentand convolutional neural networks, ﬁrst introduced in the 1980s,can be considered predecessors of DNN and were trained usingback-propagation which has been available since 1974.ANN were ﬁrst trained using back-propagation, an algorithmthat updates the network weights by propagating the output errorbackwards through the network[33]. However, the propagated error vanishes to zero as the network depth increases, preventingearly-layer weights from updating and signiﬁcantly reducing theperformance of the network[34–37]. Thus, other training algo- rithms for DNN were investigated. In 1992, Schmidhuber proposedto train recurrent neural networks by pre-training layers in anunsupervised fashion then ﬁne-tuning the network weights usingback-propagation[28]. Momentum further picked up in 2006when Hinton et al. proposed a greedy training algorithm for DBNspeciﬁcally. In what follows, we restrict our discussion of DNN toDBN, a popular and widely used deep architecture, trained usingHinton et al.’s algorithm.While DBN is a type of deep ANN, back-propagation fails to pro-duce a suitable model that performs well on training and testingdata due to DBN’s architectural characteristics [2]. This has been attributed to the ‘‘explaining away” phenomenon. Explainingaway, also known as Berkson’s paradox or selection bias, rendersthe commonly held assumption of layer independence invalidand consequently adds complexity to the inference process. Thehidden nodes become anti-correlated because their extremelylow probabilities make the chances of both ﬁring simultaneouslyimpossible.To remedy this issue, Hinton et al. proposed a training algo-rithm based on the observation that DBN can be broken down tosequentially stacked restricted Boltzmann machines (RBM), atwo-layer network inter-layer neuron connections only. This novelapproach rekindled the interest in these deep architectures andsaw DBN applied to many problems from image processing[38,39], natural language processing[40–42], automatic speech recognition[43–46]and feature extraction and reduction [47– 49], to name a few.3.2. Network structure3.2.1. Restricted Boltzmann machinesRBM, ﬁrst known as Harmonium by [50], are two-layer net- works where only inter-layer neuron connections are allowed.They are a special case of Boltzmann machines (BM) which allowboth inter and intra-layer connections. RBM’s neurons form twodisjoint sets (as indicated inFig. 1by the black boxes), satisfyingthe deﬁnition of bipartite graphs. Thus, training RBM is less com-plex and faster. The neuron connections in RBM may be directedor undirected; in the latter case, the network forms an auto-associative memory which is characterized by bi-directional infor-mation ﬂow due to feedback connections [2].3.2.2. Deep belief networksDBN are stacked directed RBMs, except for the ﬁrst RBM whichcontains undirected connections, as shown in Fig. 1. This network architecture signiﬁcantly reduces the training complexity andmakes deep learning feasible. Focusing on two layers of the net-work, the weighted edges connecting the various neurons areannotated using the variable notation n
‘n;mwhich implies that
Fig. 1.DBN architecture[51].Table 1DBN nomenclature.
n‘n;m Weight of the edge connecting the nth neuron in the‘th layer to the mth neuron in the‘thþ1 layer;‘is suppressed when there are only 2 layers in the networkn
rn Vector of connection weights leaving the nth neuron in the‘th layer n
‘ Matrix of weights connecting the ‘th layer to the‘thþ1 layer
l Learning rate
j Number of Gibbs sampling steps performed during CDNHidden-layer neuron cardinalityMInput-layer neuron cardinalityLNumber of hidden layerstSampling stepQð:j:ÞConditional probability distributionh
‘ Binary conﬁguration of the ‘th layer pðh
‘ÞPrior probability ofh‘the current weight values x
0 Input layer data pointx
ðtÞm Binary conﬁguration ofmth input-layer neuron at sampling step t XSet of training pointsH
n Binary conﬁguration variable of neuron nin the hidden layer at sampling stepth
ðtÞn Binary conﬁguration value of neuron nin the hidden layer at sampling steptb
m mth input-layer neuron bias
cn nth hidden-layer neuron bias84 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93neuronnin layer‘is connected to neuronmin layer‘þ1. An exhaustive list of the entire nomenclature adopted in this sectionis included inTable 1.3.3. Training algorithm3.3.1. Restricted Boltzmann machinesHinton et al. proposed the contrastive divergence (CD) algo-rithm to train RBM in both supervised and unsupervised scenarios.CD estimates the log-likelihood gradient using a
jGibbs sampling steps which is typically set to 1. The optimal weight vector isobtained by maximizing the objective function in (1)through gra- dient descent[52]. CD’s pseudo-code is summarized in Table 2. Gibbs sampling is a randomized MCMC algorithm that allowsthe sampling of approximate samples from a multivariate proba-bility distribution[53]. The generated samples are correlated andform a Markov chain. Eq.(2)describes the energy function thatrepresents the joint probability distribution, derived from Gibbsdistribution and calculated using(3).n
n;m,bmandcnare real valued weights;h
nandx mcan take values in the setf0;1g[54].
max nPx2XPðxÞð 1ÞEðx;hÞ¼/C0X
Nn¼1XMm¼1nn;mhnxm/C0XMm¼1bmxm/C0XNn¼1cnhn ð2Þpðx;hÞ¼
1P
xP
he/C0Eðx;hÞe/C0Eðx;hÞð3Þ
3.3.2. Deep belief networksA simple and efﬁcient layer-wise training algorithm was pro-posed for DBN by Hinton et al. in 2006 [2]. It trains the layers sequentially and greedily by tying the weights of unlearned layers,using CD to learn the weights of a single layer and iterating until alllayers are trained. Tying the weights not only allows us to useRBM’s training algorithm but also eliminates the ‘‘explainingaway” phenomenon. Then, the network weights are ﬁne-tunedusing a two-pass ‘‘up-down” algorithm.In general, deep networks are pre-trained using unsupervisedlearning before using labeled data to improve the model withsupervised learning. This scheme almost always outperforms net-works learned without pre-training [56]since this phase acts as a regularizer[57,58]and aid[59]for the supervised optimizationproblem.The energy contained in the directed model can be calculatedusing(4)where the maximum energy is upper bounded by (5) and achieves equality when the network weights are tied. Atequality, the derivative is equal to (6)and is used to solve the now simpler maximization problem.
Eðx0;h0Þ¼/C0 ðlogpðh0Þþlogpðx0jh0ÞÞ ð4Þlogpðx0ÞPX
8h0Qðh0jx0Þðlogpðh0Þþlogpðx0jh0ÞÞ/C0X
8h0Qðh0jx0ÞlogQðh0jx0Þð5Þ
@logpðx0Þ@n
n;m¼X
8h0Qðh0jx0Þlogpðh0Þð6Þ
After iteratively learning the weights of the network, the up-down algorithm[2]ﬁne-tunes the network weights. This algorithmis a supervised variant of the wake-sleep algorithm that uses CD tomodify the network weights. The wake-sleep algorithm [60]is an unsupervised algorithm used to train neural networks in twophases: the ‘‘wake” phase is applied on the feed-forward path tocompute the weights and the ‘‘sleep” phase is applied on the feed-back path. The up-down algorithm, described in Table 3, is applied to network to reduce under-ﬁtting which is commonly observed ingreedily-trained networks.Speciﬁcally, in the ﬁrst phase (up-pass) of the algorithm, theweights on the directed connections, termed generative weightsor parameters, are modiﬁed by calculating the wake-phase proba-bilities, sampling the states, and updating the weights using CD. Onthe other hand, the second phase (down-pass) stochastically acti-vates earlier layers through the top-down links, termed inferenceweights or parameters. The sleep-phase probabilities are calcu-lated, the states are sampled and the output is estimated.4. Cortical algorithms4.1. OverviewCA are a deep artiﬁcial neural network model, which borrowsseveral concepts and aspects from the human brain. The maininspiration is drawn from the ﬁndings of Edelman and Mountcastle[15,7], which state that the brain is composed of cortical columnsarranged in six layers. He also uses the concept of strengtheningand inhibiting to build a computational training algorithm capableof extracting meaningful information from the sensory input andcreating invariant representations of patterns. Further descriptionof the CA model and its biologically plausible aspects can be foundin[61,51,62].4.2. Network structureA typical CA network consists of an association of columnsgrouped in layers or levels. A column is a collection of neuronsassociated with the same stimulus, as shown in Fig. 2. Hence, CA can be considered as a three-level hierarchy structure. The neu-rons, like in other neural network architectures, use an activation
Table 3Up-Down training algorithm workﬂow [2].
1. Bottom-up phasea. Calculate wake-phase probabilitiesb. Sample network statesc. Using wake-phase probabilities, calculate CD statisticsd. Perform Gibbs sampling for
jiterations e. Using (1.d), calculate sleep-phase CD statistics2. Down-pass phasea. Compute sleep-phase probabilities through top-down passb. Sample network statesc. Estimate network output3. Re-compute generative weights4. Re-compute network’s directed sub-graph weights5. Re-compute inference weightsTable 2Contrastive divergence workﬂow used in training RBM [55].
1. Set the weights to zero: n‘¼0;‘¼1...L 2.
8x2Xa. Propagate training instance through the networkb. For
jsampling stepsi. Loop overNhidden-layer neurons and sample h
ðtÞn/C24pðh njxðtÞÞ ii. Loop overMinput-layer neurons and sample x
ðtÞm/C24pðx mjhðtÞÞ c. Loop over input and hidden-layer neurons and computei.
Dnn;m¼Dnn;mþpðH n¼1jxð0ÞÞxð0Þm/C0pðH n¼1jxðjÞÞxðjÞm
ii.Dbm¼Dbmþxð0Þm/C0xðjÞm
iii.Dcn¼DcnþpðH n¼1jxð0ÞÞ/C0pðH n¼1jxðjÞÞY. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 85functionfð:Þto compute their output from their input. This activa-tion function is common for all neurons in the architecture.Columns in a layer connect to those in the subsequent level:these connections are referred to as vertical connections and existonly between consecutive levels. A synaptic input terminating at acolumn’s input is shared within neurons constituting said column,however, the learned weights of the connections are different lead-ing to distinct neuronal outputs. The latter are summed to form thecolumn’s output being forwarded to the next layer. Such conﬁgura-tion permits the column to act as a basic computational structurein CA as opposed to neurons in DBN and other neural networkarchitectures.Furthermore, lateral connections or intra-level connections, are‘‘communication means” employed to deliver inhibiting signalsthat modify a column’s weights based on the activity of other col-umns during training. Contrarily to their vertical counterpart, theseconnections do not transmit data and are hence are not explicitlyshown inFig. 2. Data can only ﬂow in a bottom-up direction, fromlevel to level, through vertical connections.4.3. Mathematical modelThe complete mathematical description based on the model of[6,63]can be found in[62,61], and is summarized below basedon the adopted nomenclature inTable 4. A column ofNneurons receives connections from the output ofcolumns in the previous layer and hence is represented by a 2Dmatrix concatenating vectors of weights of incoming connectionssynapsing at each of its neurons as shown in Eq. (7). Epoch number is denoted bye, layer number byl, the receiving column index byc, the receiving neuronnand the sending column bys. Deﬁning!l;eas the output vector of levellfor epocheand tl;ec
the output of columnc, within the same level; for the same train-ing epoch, we can write(8). The output of a neuron,z
l;ec;ndeﬁned by (9)is the result of the nonlinear activation function fð:Þin(10)in response to the weighted sum of the input connections while theoutput of the column is the sum of the outputs of the column’sneurons.Tis a constant (across layers) tolerance parameter empir-ically selected and the nonlinear activation function emulates thebrain’s observed nonlinear activity.
Nl;ec¼Nl;ec;1...Nl;ec;n ...Nl;ec;Nhi
¼n
l;ec;1;1...nl;ec;n;1...nl;ec;N;1
...............n
l;ec;1;s...nl;ec;n;s...nl;ec;N;s
...............n
l;ec;1;C
l/C01...nl;ec;n;C
l/C01...nl;ec;N;C
l/C012666666666437777777775ð7Þ
!l;e¼tl;e1...tl;ec ...tl;eC
lhi ð8Þ
tl;ec¼XNn¼1zl;ec;n
zl;ec;n¼fXCl/C01
c¼1nl;ec;n;stl/C01;ec ! ð9Þfð
sÞ¼11þe
s/C1ð/ðsÞ/C0TÞ
/ðsÞ¼/C02;ifs¼1
s;otherwise/C26 ð10Þ
4.4. Training algorithm4.4.1. Random initializationThe network is assumed to be initially fully connected with ran-dom weak weights (with absolute values less than 0.1). This is acommon ‘‘blank slate” approach to ensure that the network isnot initially biased to any speciﬁc pattern. As learning proceeds,these weights are incrementally updated so that the connectivityof the network is modiﬁed. All weights that fall to zero representdisabled connections. Therefore, an initially fully connected net-work is not necessarily preserved.4.4.2. Unsupervised feed-forward learningThe ﬁrst stage in training a cortical network aims at creatinginput-speciﬁc representations via random ﬁring and repeatedexposure. An input propagating through the levels of the networkcauses certain columns to ﬁre (i.e. generate a threshold crossingresponse) based on initially random weights. This activation isthen reinforced by strengthening the active columns’ weightsand inhibiting neighboring ones. Repeated exposure or batchlearning trains columns to identify (via activation) particularaspects or patterns of the training data, extracting discriminatoryfeatures with increasing complexity through levels (lower levelsrecognize basic elements, higher levels in the hierarchy learnhigher order concepts and reasoning). The strengthening processincreases a column’s weights rendering it more receptive tostimulation, while inhibition weakens the weights diminishingactivity of a certain column. Connections formed or weakened
Table 4CA nomenclature.
cDestination column indexnDestination neuron indexlDestination level indexsOrigin column indexeTraining epochNNumber of nodes in a columnC
l Number of columns in level l TTolerancen
l;ec;n;s Weight of connection between neuron n, columnc, levell, and column sin previous level, during epoch e N
l;ec;n Vector of connection weights entering neuron n, of columnc, levell
Nl;ec Matrix of weights entering column c, levell!
l;e Output vector of levell
tl;ec Output of columncz
l;ec;n Output of neuronn, columncof levell
Fig. 2.Illustration of a cortical network.86 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93are plastic, i.e. can be altered during the feedback learning phaseand vice versa. Strengthening and inhibition rules are shown inEqs.(11)–(13).
nl;eþ1c;n;s¼tl;ec/C1ðnl;ec;n;s/C0XðNl;ecÞÞ ð11Þn
l;eþ1c;n;s¼tl;ec/C1nl;ec;n;sþal;ec;n;sþq/C111þe
nl;ec;n;s/C0T
XðNl;ecÞ0BB@1CCAð12Þ
XðNl;ecÞ¼XNn¼1XCl/C01
c¼1al;ec;n;snl;ec;n;s
al;ec;n;s¼1i fnl;ec;n;s>/C15
0 otherwise( ð13Þ
4.4.3. Supervised feedback learningThe feed-forward learning phase relies only on aspects of thegiven data to train columns that identify signiﬁcant features withno error propagation or label information. The supervised feedfor-ward stage aims at correcting misclassiﬁcations occurring whenthe network is exposed to variations of the same patterns (moreaccurately training class) which may result due to the absence oflabel information in the previous phase.Following the unsupervised phase, a ‘‘unique representation”based on the average ﬁring scheme observed for a particular pat-tern is stored, the feedforward learning ﬁne-tunes the network’sweights to achieve this scheme (within certain bounds to avoidoverﬁtting) for all instances of a known class. A misclassiﬁcationhence triggers an error signal at the top-most or output layer(where the ﬁnal output is produced) correcting misﬁring columns(through inhibition) and strengthening weakened columns ﬁringfor the original pattern forcing the column ﬁring for the originalpattern (strengthening and inhibition are the only weight updaterules adopted in opposition to gradient descent employed in thebackpropagation learning of traditional artiﬁcial neural networks).After multiple exposures, the top level attains a stable state alsoknown as a ‘‘stable activation” in which columns are able to cor-rectly generate the desired ﬁring scheme. The term ‘‘ﬁring scheme”refers to the ﬁring pattern of neurons for a given input.After stabilizing the top layer, the error signal is propagatedback to the previous level which in turn executes a series ofinhibition and strengthening to achieve the desired ﬁringscheme. The same process in repeated for each of the layersuntil a convergence condition (expressed as a discrepancybetween the desired and actual activation) is met. The trainingexits when all layers are stable and the network can recognizeall variations of the same pattern in the training data (within acertain tolerance).A pseudo-code showing the implementation of the trainingalgorithm is shown inTable 5. An illustration of the feedback train-ing process is shown inFig. 3. The top row shows the ﬁring scheme(blue squares represent active columns) obtained after the feedfor-ward propagation of two variations of the same pattern as well asthe desired ﬁring scheme (average activation obtained based on allvariations of this pattern in the training set). The middle and bot-tom rows show a succession of training epochs (for pattern 1 and 2respectively) in which the error signal is generated at the top leveland stable activations are formed from top to bottom. One can seethat at convergence both instances are represented with the sameﬁring scheme in the network.5. Theoretical computational complexityIn this section, we derive the theoretical computational com-plexity of each algorithm to assess the required resources whendeploying such a network for real world problems. We investigate
Table 5CA generic training workﬂow.
1. Random initializationforl¼1:6forc¼1:C
l
Nlc¼randnðC l/C01;NÞ 2. Feedforward phasea. Processing of training datafor all training instancesforl¼1:6compute!
l;e
forl¼1:6forc¼1:C
l
iftl;ec>/C15
forn¼1:Napply(12) elseforn¼1:Napply(11) b. Store average representations of patternsfor all classesforl¼1:6forc¼1:C
l
tl;avgc¼tlc
3. Feedback phasewhileMSE>hfor all training instancesforl¼1:6while
!l;e/C0!l;e/C12/C12/C12/C12/C12/C12>hforc¼1:C
l
iftl;avgc/C0tlc/C12/C12/C12/C12/C12/C12>hforn¼1:Napply(11) elseforn¼1:Napply(12)Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 87two aspects of the computational complexity: memory and com-putations. While the required memory storage depends on thenumber of non-zero weights in the network, the number of com-putations depends on the non-zero weights and the adopted acti-vation function. Comparing CA to DBN, the more computationallydemanding network is data speciﬁc since each problem wouldresult in a different number of non-zero weights. We empiricallycompare the network sizes in the next section.5.1. Number of non-zero weights5.1.1. DBNDBN is formed ofRlayers withM
rneurons in layerr. During training, the network starts out fully connected leading to a totalnumber of weights equal toN
w¼PRr¼1Mr/C1M r/C01. Assuming not all weights are non-zero when training is complete, with
c2½0;1/C138is the fraction of non-zero weights or ﬁring rate, the number ofnon-zero weights is equal toN
NZW¼PRr¼1c/C1M r/C1M r/C01. Empirical results in Section6reveal that
cis usually greater than 90%. Know-ing that the weights are double precision ﬂoating point numberswhich require 8 bytes of storage, an upper bound on the memoryrequirements during training areP
Rr¼18/C1M r/C1M r/C01. During testing, the number of bytes is equal toP
Rr¼18/C1c/C1M r/C1M r/C01.5.1.2. CACA is formed ofR¼6 levels; the ﬁrst level containsL
1¼Icol- umns with M neurons per column. We assume, without loss ofgenerality, the number of columns is cut in half in each subsequentlevel, as used in[6,63,64]i.e. levelrcontainsL
r¼I2r/C01. Each level has a weight matrix with dimensionsL
r/C2M. Therefore, the total num- ber of weights is equal toN
w¼PRr¼1Lr/C1M/C1L r/C01. However, some of these weights could be equal to zero. Therefore, we denote by
c2½0;1/C138the ﬁring rate of the network, the fraction of neurons thatare non-zero, which leads to the number of ﬁring neurons equal toN
FC¼cPRr¼1Lr. To simplify the computations, a uniform distribu-tion of ﬁring columns across levels will be assumed. Therefore,the number of ﬁring columns per level is N
FC=L¼NFC
R. Finally, the total number of non-zero weights can be approximated byN
NZW¼PRr¼1M/C1L r/C1NFC=L. Empirical results in Section6reveal that
cdoes not usually exceed 50%. Knowing that the weights are dou-ble precision ﬂoating point numbers which require 8 bytes of stor-age, an upper bound on the memory requirements during trainingareP
Rr¼18/C1L r/C1M/C1L r/C01. During testing, the number of bytes is equaltoP
Rr¼18/C1c/C1M/C1L r/C1PRr¼1Lr
R.5.2. Number of operations per neuronSince a neuron is composed of a summation and an activationfunction, the number of operations performed to obtain the outputof each neuron can be divided into the operations to compute thesum and the activation function computational complexity.5.2.1. SummationThe number of ﬂoating point operations required to computethe summation depends on the number of input connections of aneuron. Assuming there aremconnections,mmultiplications andmadditions (including the bias term) are required, which isof the order ofOðm2Þ. For an input layer neuron,mis equal to the number of features in the input vector. For a hidden layer neu-ron,mis at most equal to the number of neurons in the previouslayer.5.2.2. Activation functionDepending on the activation function, the computation of eachneuron output will require a certain number of ﬂoating point andcomparison operations. Some of popular activation functions andtheir computational complexity are summarized in Table 6. The hard limit activation function, described by (14), requires one comparison. The linear function in (15)requires one multipli- cation and one addition whereas the piece-wise linear function in(16)requires two comparisons and at most one multiplication andaddition operation.
/ðx iÞ¼1i fxiP00 otherwise/C26 ð14Þ/ðx
iÞ¼axiþb ð15Þ/ðx
iÞ¼bifxiPb
axiþbif/C0b6x i<b/C0bifx
i</C0b8><>: ð16Þ
Table 6Computational complexity of some activation functions
Activation function Equation Computations (per neuron)Hard limit (14) Oð1Þ Linear (15) Oð2Þ Piecewise linear (16) Oð4Þ Gaussian (17) Oðm
3Þ Sigmoid, Tangent (18) Oð165Þ Sigmoid, Logarithm (19) Oð83Þ Softmax (20)Oð83M
RÞ
Fig. 3.An example of the feedback process.88 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93Computing the Gaussian activation function, described by(17), requiresmmultiplications andmðm/C01Þadditions to compute the magnitude term, where mrepresents the dimen- sionality of the vectorx
iand is equivalent to the number ofinput connections of a neuron. Dividing by the standard devi-ation requires 3 additional multiplication operations. In addi-tion, the calculation of an exponential, estimated using theTaylor series expansion with approximately 10 terms, requiresapproximately 81 operations. In total, m
2ðm/C01Þþ81 opera- tions are required.
/ðx iÞ¼exp/C0kx i/C0lik2
2r2 ! ð17Þ
The tangent sigmoid function in (18)has two exponential terms, each requiring approximately 81 operations, in additionto two additions and one multiplication operations whichleads to a total of approximately 165 operations. On the otherhand, the logarithmic sigmoid in (19)has one exponential term and one addition and multiplication for a total of 83operations.
/ðx iÞ¼tanhðx iÞ¼1/C0e/C02xi
1þe/C02xið18Þ/ðx
iÞ¼11þe
/C0xið19Þ
Each exponential term in the softmax activation function,described by(20), requires approximately 81 operations. Thenumerator has one exponential term while the denominator hasM
Rterms. In total, 81ðM Rþ1Þþ1 operations are required.
/ðx iÞ¼exi
P
j2M Rexjð20Þ
5.3. PruningThe term synaptic pruning refers to the procedure by whichsynaptic connections are terminated during the early ages ofmammals’ lives[65]. Starting with approximately 86/C68 billion neurons at birth, the human brain quintuples its size until adoles-cence after which the volume of synaptic connection decreasesagain[66]. This process of pruning is mainly thought of as thebyproduct of learning. While the total number of neuronsremains roughly unaltered, the distribution and number of con-nections are tailored by learning [67,68]: the structure of the brain moves from a stage where its primary function falls underperception–action due to disconnected cortical hubs [69,70]to distributed networks spanning the brain performing a variety ofcomplex cognitive tasks[71].The training of a cortical network bears several resemblances tothe synaptic pruning procedure: starting with a fully connectedstructure, the network goes through an unsupervised phase whereconnections are pruned through inhibition leaving only ‘‘signiﬁ-cant” ones. In more accurate terms, the strengthening of ﬁring col-umns ensures the establishment of selective connections whileinhibition prunes irrelevant connections. This process plays a cru-cial role not only in extracting meaningful characteristics of theinput stimuli but also in avoiding overﬁtting due to the especiallycomplex structure of a cortical network. This is further demon-strated in the number of non-zero weights in different networkarchitectures as shown in our theoretical and empirical analysis:the number of ‘‘alive” connections decreases with the increase ofparameters, hence a minimal effect on performance as shown inour experiments. This property of CA’s structure and training isnot shared with DBN.5.4. Overall computational complexityIn summary, the overall computational cost of DBN and CAdepends on the architecture of the network: the number of layersand the number of neurons per layer which affect the number ofconnections. The activation function can be ﬁxed for both DBNand CA. Sigmoid is a common activation function used in bothalgorithms. After training, some of these connections will have aweight of zero. Based on the previous sections, we notice that fora ﬁxed network architecture, CA will have less non-zero weightsdue to the pruning algorithm. However, the depth of the best net-works for each algorithm vary based on the data. six-layer archi-tectures are commonly used in the literature for CA while DBNdepth’s varied from 3 to 5 hidden layers.6. Empirical comparisonIn this section, we report on the experimental results of DBNand CA on multiple databases from the UCI ML repository [72]. We compare the classiﬁcation accuracy, network complexity andcomputational complexity of both algorithms.6.1. Experimental setupCA and DBN were empirically compared on classiﬁcation prob-lems using datasets from the UCI machine learning repository,described inTable 7. The datasets contained real-valued features.A 4-fold cross validation was adopted, i.e. each database is ran-domly divided into 4 sets where 3 sets (or 75% of the data samples)are used in training and 1 set is used in testing. The results areaveraged over four runs where each set is used for testing onceand the remaining for training. The CA library is a set of Matlabfunctions obtained from[73,74]; it was run on a Windows 7machine with Intel Core i5. The DBN library is a set of Matlab func-tions modiﬁed from[75]; it was run on an Intel Core i7 processormachine. Multiple network architectures, summarized in Table 8, were tested on the datasets. The number of neurons for the hiddenlayers are displayed only. The input layer’s neurons are equal to thenumber of features and the output layer’s neurons are equal to thenumber of classes. The chosen architectures can be grouped into
Table 7UCI dataset characteristics.
Dataset Number of instancesNumber offeaturesNumber ofclassesPlanning relax 182 12 2Breast cancer diagnosticWisconsin 569 32 2Tic Tac Toe 958 9 2Spambase 4601 57 2Wilt 4889 5 2White wine 4898 11 2MNIST 70,000 784 10Skin segmentation 245,057 3 2
Table 8Nomenclature adopted for network architectures.
Network name Network architecture (hidden layers)N1 [5, 5]N2 [50, 50]N3 [50, 50, 50]N4 [500, 500, 500]N5 [500, 500, 2000]N6 [1000, 1000, 2000]N7 [2000, 1000, 500, 250, 125, 62]Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 89one of three sets based on the pattern of hidden layer neurons: net-works with an equal number of neurons in all hidden layers, net-works with an increasing (doubling) number of neurons as thelayer depth increases and networks with a decreasing (halving)number of neurons as the layer depth increases. Furthermore, thenumber of hidden layers is increased from 2 to 6 hidden layers. Forthe results reported inTable 9, DBN’s unsupervised training andﬁne-tuning algorithms were each run for 50 epochs and the batchsize was set to 10. The second column indicates the number of col-umns per layer for CA (there were 20 neurons per column) and thenumber of neurons per column for DBN. The connectivity of a net-work is computed by taking the ratio of weights greater than 5 %of the average value of weights to the total number of weights in thenetwork. The threshold is not set to a ﬁxed value since the weightsrange of weights varies based on the input data, i.e. for some data-sets all the weights might be less than this set threshold eventhough this threshold might be very small. Furthermore, thethreshold is not set to zero since some weights will not exactlyzero but signiﬁcantly smaller than the other weights in the net-work and their contribution is insigniﬁcant. The classiﬁcation
Table 9Classiﬁcation results.
DBN CADataset Net. size Acc. (%) Connec. (%) Acc. (%) Connec. (%)Planning N1 69.4 91.1 71.4 86.4relax N2 68.8 49.6 65.2 72.5 N3 69.4 55.0 70.8 68.7N470.6 9.1 75.6 55.1 N5 68.8 3.8 80.8 43.4N6 68.1 3.2 82.5 30.9N7 66.3 6.0 86.1 29.1 Breast N1 62.1 99.1 97.0 95.8cancer N2 88.4 98.7 97.1 90.7 diagnostic N3 88.4 98.5 96.8 86.4Wisconsin N4 87.5 97.3 97.5 77.1 N5 87.5 97.3 98.2 68.3N6 87.0 97.2 98.2 56.5N7 67.9 96.9 99.0 35.6 Tic N1 65.1 100.0 67.0 86.3tac N2 65.1 94.9 79.3 83.5toe N3 65.1 97.0 80.1 79.8 N4 65.1 99.4 89.4 68.1N5 65.1 99.4 90.5 56.4N6 65.1 98.9 92.7 45.9N7 65.1 99.1 93.1 34.7 Spambase N1 89.9 98.1 90.5 89.7 N2 90.5 98.7 91.8 87.6N3 91.4 98.9 92.8 79.9N492.2 96.7 93.7 66.4 N5 91.3 96.1 94.3 58.1N6 91.8 95.3 95.6 46.8N7 88.5 93.5 97.5 36.3 Wilt N1 94.6 100.0 94.6 88.4 N2 94.6 99.9 95.5 80.7N3 94.6 99.9 96.5 77.5N4 94.6 99.9 97.8 66.9N5 94.6 99.4 98.2 55.3 N6 94.6 99.2 98.0 45.5N7 94.6 97.8 98.2 35.8 White N1 96.4 94.1 95.1 81.7 wine N2 96.4 99.7 96.0 80.3 N3 96.4 98.6 97.5 73.2N4 96.4 70.1 98.1 67.9N5 96.1 73.0 98.9 47.3N6 96.3 60.0 99.5 36.8N7 96.4 62.8 99.7 26.1 MNIST N1 38.8 88.1 96.2 89.8 N2 84.1 91.8 97.1 85.2N3 84.2 98.8 98.5 79.1N4 89.0 47.2 98.5 58.7N5 88.3 54.9 99.2 43.5N689.4 53.9 99.8 37.6 N7 86.5 40.8 99.8 28.5 Skin N1 79.6 80.6 94.5 83.2segmentation N2 79.6 94.4 94.8 79.5 N3 79.6 95.8 94.8 75.3N4 79.6 97.2 95.7 64.3N5 79.6 97.1 97.8 53.8N6 79.6 97.1 97.8 50.1N7 79.6 97.3 99.9 33.5The bold values refer to the best accuracy obtained per dataset.90 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93accuracy simply reports the percentage of correctly classiﬁedinstances in the test set. Running time is not compared since eachalgorithm is written and run in a different environment.6.2. Classiﬁcation resultsFirst, we compare the performance of DBN when varying thenetwork architecture. For some databases, such as White wine,the performance does not signiﬁcantly vary as the network archi-tecture varies, achieving approximately 96.4% accuracy. On theother hand, performance varied signiﬁcantly across architecturesfor MNIST, ranging from 38.8% to 89.47%. Hence, DBN is either ableto achieve good accuracy or bad accuracy on a speciﬁc database.This is mainly due to the lack of enough training points to allowDBN to learn a good model of the data. However, for other datasetssuch as Breast cancer diagnostic Wisconsin, the classiﬁcation accu-racy varied signiﬁcantly as the network architecture varied. Next,considering CA’s performance on the various datasets when vary-ing the network size, we notice that the six-layered networkalways outperforms other network sizes on all datasets, althoughit occasionally marginally outperforms some networks. For exam-ple, CA achieved 99.9% accuracy for N7 on skin segmentation com-pared to 94.5% for N1 (2 layers). Comparing CA to DBN, we noticethat CA outperforms DBN on almost all datasets. In addition, a six-layered architecture seemed to always perform better than otherarchitectures, reducing the burden of searching for the optimalnetwork architecture by training multiple networks.6.3. Network connectivityDBN exhibit high connectivity for most datasets on various net-work sizes. Therefore, Hinton et al.’s greedy training algorithmdoes not eliminate a large number of connections resulting in analmost fully connected graph. This is evident in the experimentalresults reported inTable 9which reports the percentage of non-zero weights in a DBN network to be around 90%. Unlike DBN,CA generally results in a sparsely connected network, evident bythe low percentage of non-zero weights obtained after training;CA networks had less than 50% of their connections in tact com-pared to almost 90% for DBN. Furthermore, we notice that CA’sconnectivity tends to decrease as the network size increases whichimplies that if certain data does not require a large network, CA’straining algorithm will reduce the number of connections to pro-duce a sparsely connected network. For example, connectivity per-centage decreases from 98.7% to 40.8% for MNIST when increasingthe network architecture from a two-hidden layer network to asix-layer architecture.6.4. Effect of batch sizeThe DBN code randomly reorders and divides the data into minibatches. While training, each mini batch is loaded into memoryand used to update the weights of the network connections.Increasing the size of a batch meant loading a larger chunk of datainto memory. This renders training slower if the chunk was toolarge to ﬁt into memory. However, decreasing the batch size meantmore memory transfers per epoch and therefore, increasing train-ing time. On the other hand, CA does not randomly reorder anddivide the data into mini batches. Therefore, the network isexposed to the data in the order it was presented. Theoretically,reordering or dividing the data has no effect on the performanceof the algorithm as all patterns are employed and must achieve astable activation.6.5. Statistical analysisNext, we perform the pairwiset-test[76]and Friedman test [77]to gain further insight into the differences between CA andDBN. The N7 architecture is trained on the various databases.Table 10summarizes the p-values of both tests and show that
Table 10Statistical signiﬁcance tests.
Pairwiset-test Friedman testDatabase p-value Statistically signiﬁcant p-value Statistically signiﬁcant Nemenyi critical distancePlanning relax 2.88E /C04 Y 3.64E /C002 Y 0.1 Breast cancer diagnostic Wisconsin 0 Y 9.26E /C0126 Y 0.1 Tic tac toe 5.28E /C010 Y 1.92E /C009 Y 0.1 Spambase 0 Y 0 Y 0.0Wilt 4.65E /C079 Y 4.74E /C075 Y 0.0 White wine 0 Y 0 Y 0.0MNIST 9.11E /C084 Y 6.09E /C061 Y 0.0 Skin segmentation 0 Y 0 Y 0.0
Fig. 4.CA vs. DBN ranking based on Nemenyi post hoc test.Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 91there is a statistical signiﬁcance between CA and DBN on all data-bases; the p-value was less than 5%. Furthermore, the Nemenyipost hoc[78]was performed once a signiﬁcant difference wasobserved, to rank the algorithms. The rankings revealed that CAoutperformed DBN on most databases except for Skin Segmenta-tion, as shown inFig. 4. The critical distance is summarized inTable 10.7. ConclusionIn this work, we compared two DNN architectures on super-vised classiﬁcation problems. While DBN can be easily seen as anestablished technique developed from within a traditional AI per-spective, CA are more biologically inspired and can be classiﬁedas theories in the making, solidly rooted in principles inheritedfrom neuroscience research. A theoretical computational complex-ity analysis for both algorithms was presented before empiricallycomparing CA and DBN. Experiments were run on eight publiclyavailable classiﬁcation databases. Multiple CA and DBN networkarchitectures were compared based on their classiﬁcation accuracyand resulting network connectivity. CA achieved the best perfor-mance on most databases using a six-layer architecture withdecreasing number of hidden neurons for deeper layers. Resultsshowed that deeper CA networks had lower connectivity than shal-lower CA networks. Furthermore, DBN did not prune as many con-nections as CA’s training algorithm. On the tested databases, CAgenerally had a higher classiﬁcation accuracy than DBN. In thespan of this work, we attempted to provide the reader with enoughbackground and technical details for each of the algorithms whileunderstanding that the breadth of the topic necessitates the inclu-sion of more involved insights. We therefore urge the interestedreader to use this paper as a foundation for further exploration ofthe rapidly expanding sub-ﬁeld of deep learning.AcknowledgmentThis work has been partly supported by the National Center forScientiﬁc Research in Lebanon and the University Research Boardat the American University of Beirut.References
[1]S. Samarasinghe, Neural Networks for Applied Sciences and Engineering: FromFundamentals to Complex Pattern Recognition, CRC Press, 2006
. [2]
G. Hinton, S. Osindero, Y.-W. Teh, A fast learning algorithm for deep belief nets,Neural Comput. 18 (7) (2006) 1527–1554
. [3]
E.M. Izhikevich, Which model to use for cortical spiking neurons?, IEEE TransNeural Netw. 15 (5) (2004) 1063–1070
. [4]
H. De Garis, C. Shuo, B. Goertzel, L. Ruiting, A world survey of artiﬁcial brainprojects, part i: large-scale brain simulations, Neurocomputing 74 (1) (2010)3–29
.[5]
B. Goertzel, R. Lian, I. Arel, H. De Garis, S. Chen, A world survey of artiﬁcial brainprojects, part ii: biologically inspired cognitive architectures, Neurocomputing74 (1) (2010) 30–49
.[6]
A.G. Hashmi, M.H. Lipasti, Cortical columns: building blocks for intelligentsystems, in: IEEE Symposium on Computational Intelligence for MultimediaSignal and Vision Processing, IEEE, 2009, pp. 21–28
. [7]
G.M. Edelman, V.B. Mountcastle, in: The Mindful Brain: Cortical Organizationand the Group-Selective Theory of Higher Brain Function, Masachusetts Inst ofTechnology Pr, 1978
.[8]
R.J. Baron, The Cerebral Computer: An Introduction to the ComputationalStructure of the Human Brain, Psychology Press, 2013
. [9] J. Nolte, The Human Brain: An Introduction to its Functional Anatomy.[10]
J.M. DeSesso, Functional anatomy of the brain, in: Metabolic Encephalopathy,Springer, 2009, pp. 1–14
. [11]
N. Geschwind, Specializations of the human brain, Scientiﬁc American 241 (3)(1979) 180–201
.[12]
R.C. O’Reilly, Y. Munakata, Computational Explorations in CognitiveNeuroscience: Understanding the Mind by Simulating the Brain, MIT Press,2000
.[13]
M. Catani, D.K. Jones, R. Donato, et al., Occipito-temporal connections in thehuman brain, Brain 126 (9) (2003) 2093–2107
.[14]J. Szentagothai, The ferrier lecture, 1977: the neuron network of the cerebralcortex: a functional interpretation, Proc. R. Soc. Lond. Ser. B. Biol. Sci. 201(1144) (1978) 219–248
.[15]
V.B. Mountcastle, The columnar organization of the neocortex, Brain 120 (4)(1997) 701–722
.[16]
A.S. Benjamin, J.S. de Belle, B. Etnyre, T.A. Polk, The role of inhibition inlearning, Human Learn.: Biol., Brain, Neurosci.: Biol., Brain, Neurosci. 139(2008) 7
.[17]
W.S. McCulloch, W. Pitts, A logical calculus of the ideas immanent in nervousactivity, Bull. Math. Biophys. 5 (4) (1943) 115–133
. [18]
F. Rosenblatt, The perceptron: a probabilistic model for information storageand organization in the brain, Psychol. Rev. 65 (6) (1958) 386
. [19] B. Widrow, et al., Adaptive adaline neuron using chemical memistors, 1960.[20]
A. Aizerman, E.M. Braverman, L. Rozoner, Theoretical foundations of thepotential function method in pattern recognition learning, Autom. Rem.Control 25 (1964) 821–837
. [21]
J.L. McClelland, D.E. Rumelhart, P.R. Group, et al., Parallel distributedprocessing, Explorations in the microstructure of cognition 2 (1986) 184
. [22]
J.J. Hopﬁeld, Neural networks and physical systems with emergent collectivecomputational abilities, Proc. Nat. Acad. Sci. 79 (8) (1982) 2554–2558
. [23]
T. Kohonen, Self-organized formation of topologically correct feature maps,Biol. Cybernet. 43 (1) (1982) 59–69
. [24]
S. Grossberg, Competitive learning: from interactive activation to adaptiveresonance, Cognit. Sci. 11 (1) (1987) 23–63
. [25]
J. Misra, I. Saha, Artiﬁcial neural networks in hardware: a survey of twodecades of progress, Neurocomputing 74 (1) (2010) 239–255
. [26] L. Arnold, S. Rebecchi, S. Chevallier, H. Paugam-Moisy, An introduction to deeplearning, in: ESANN, 2011.[27]
V. Vapnik, The Nature of Statistical Learning Theory, Springer Science &Business Media, 2000
.[28]
J. Schmidhuber, Learning complex, extended sequences using the principle ofhistory compression, Neural Comput. 4 (2) (1992) 234–242
. [29]
J.R. Anderson, Act: a simple theory of complex cognition, Am. Psychol. 51 (4)(1996) 355
.[30]
J. Hawkins, S. Blakeslee, On Intelligence, MacMillan, 2007 . [31] S. Franklin, F. Patterson Jr., The lida architecture: adding new modes oflearning to an intelligent, autonomous, software agent, pat 703 (2006) 764–1004.[32]
K. Fukushima, Neocognitron: a hierarchical neural network capable of visualpattern recognition, Neural Netw. 1 (2) (1988) 119–130
. [33]
P.J. Werbos, Backpropagation through time: what it does and how to do it,Proc. IEEE 78 (10) (1990) 1550–1560
. [34] S. Hochreiter, Untersuchungen zu dynamischen neuronalen netzen, Master’sthesis, Institut fur Informatik, Technische Universitat, Munchen.[35] S. Hochreiter, Y. Bengio, P. Frasconi, J. Schmidhuber, Gradient ﬂow in recurrentnets: the difﬁculty of learning long-term dependencies, 2001.[36]
G.E. Hinton, To recognize shapes, ﬁrst learn to generate images, Prog. BrainRes. 165 (2007) 535–547
. [37]
Y. Bengio, J. Louradour, R. Collobert, J. Weston, Curriculum learning, in:Proceedings of the 26th Annual International Conference on Machine Learning,ACM, 2009, pp. 41–48
.[38] V. Nair, G.E. Hinton, 3d object recognition with deep belief nets, in: Advancesin Neural Information Processing Systems, 2009, pp. 1339–1347.[39]
Y. LeCun, K. Kavukcuoglu, C. Farabet, Convolutional networks and applicationsin vision, in: Proceedings of the IEEE International Symposium on Circuits andSystems, IEEE, 2010, pp. 253–256
. [40]
R. Collobert, J. Weston, A uniﬁed architecture for natural language processing:Deep neural networks with multitask learning, in: Proceedings of the 25thInternatuional Conference on Machine Learning, ACM, 2008, pp. 160–167
. [41] S. Zhou, Q. Chen, X. Wang, Active deep networks for semi-supervisedsentiment classiﬁcation, in: Proceedings of the 23rd InternationalConference on Computational Linguistics: Posters, Association forComputational Linguistics, 2010, pp. 1515–1523.[42] X. Glorot, A. Bordes, Y. Bengio, Domain adaptation for large-scale sentimentclassiﬁcation: a deep learning approach, in: Proceedings of the 28thInternational Conference on Machine Learning, 2011, pp. 513–520.[43]
G.E. Dahl, D. Yu, L. Deng, A. Acero, Context-dependent pre-trained deep neuralnetworks for large-vocabulary speech recognition, IEEE Trans. Audio, Speech,Lang. Process. 20 (1) (2012) 30–42
. [44]
T.N. Sainath, B. Kingsbury, B. Ramabhadran, P. Fousek, P. Novak, A.-R.Mohamed, Making deep belief networks effective for large vocabularycontinuous speech recognition, in: IEEE Workshop on Automatic SpeechRecognition and Understanding, IEEE, 2011, pp. 30–35
. [45] A.-R. Mohamed, D. Yu, L. Deng, Investigation of full-sequence training of deepbelief networks for speech recognition, in: INTERSPEECH, 2010, pp. 2846–2849.[46]
A.-r. Mohamed, T.N. Sainath, G. Dahl, B. Ramabhadran, G.E. Hinton, M. Picheny,et al., Deep belief networks using discriminative features for phonerecognition, in: International Conference on Acoustics, Speech and SignalProcessing, IEEE, 2011, pp. 5060–5063
. [47]
A.-R. Mohamed, G.E. Dahl, G. Hinton, Acoustic modeling using deep beliefnetworks, IEEE Trans. Audio, Speech, Lang. Process. 20 (1) (2012) 14–22
. [48] P. Hamel, D. Eck, Learning features from music audio with deep beliefnetworks, in: ISMIR, Utrecht, The Netherlands, 2010, pp. 339–344.[49]
G.E. Hinton, R.R. Salakhutdinov, Reducing the dimensionality of data withneural networks, Science 313 (5786) (2006) 504–507
.92 Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93[50]P. Smolensky, Information processing in dynamical systems: Foundations ofharmony theory, Department of Computer Science, University of Colorado,Boulder, 1986
.[51] R. Khanna, M. Awad, Efﬁcient Learning Machines: Theories, Concepts, andApplications for Engineers and System Designers, Apress, 2015.[52]
G. Hinton, A practical guide to training restricted boltzmann machines,Momentum 9 (1) (2010) 926
. [53]
S. Geman, D. Geman, Stochastic relaxation, gibbs distributions, and thebayesian restoration of images, IEEE Trans. Pattern Anal. Mach. Intell. (6)(1984) 721–741
.[54]
B. Aleksandrovsky, J. Whitson, G. Andes, G. Lynch, R. Granger, Novel speechprocessing mechanism derived from auditory neocortical circuit analysis,Proceedings of the 4th International Conference on Spoken Language, vol. 1,IEEE, 1996, pp. 558–561
. [55]
A. Fischer, C. Igel, An introduction to restricted boltzmann machines, in:Progress in Pattern Recognition, Image Analysis, Computer Vision, andApplications, Springer, 2012, pp. 14–36
. [56]
D. Erhan, Y. Bengio, A. Courville, P.-A. Manzagol, P. Vincent, S. Bengio, Whydoes unsupervised pre-training help deep learning?, J Mach. Learn. Res. 11(2010) 625–660
.[57]
Y. Bengio, Learning deep architectures for AI, Found. Trends/C210Mach. Learn. 2 (1) (2009) 1–127
.[58] D. Erhan, P.-A. Manzagol, Y. Bengio, S. Bengio, P. Vincent, The difﬁculty oftraining deep architectures and the effect of unsupervised pre-training, in:Internartional Conference on Artiﬁcial Intelligence and Statistics, 2009, pp.153–160.[59]
Y. Bengio, P. Lamblin, D. Popovici, H. Larochelle, et al., Greedy layer-wisetraining of deep networks, Adv. Neural Inf. Process. Syst. 19 (2007) 153
. [60]
G.E. Hinton, How neural networks learn from experience, Scient. Am. 267 (3)(1992) 145–151
.[61]
N. Hajj, Y. Rizk, M. Awad, A mapreduce cortical algorithms implementation forunsupervised learning of big data, Proc. Comp. Sci. 53 (2015) 327–334
. [62]
N. Hajj, M. Awad, Weighted entropy cortical algorithms for isolated arabicspeech recognition, in: International Joint Conference on Neural Networks,IEEE, 2013, pp. 1–7
.[63] A. Hashmi, M.H. Lipasti, Discovering cortical algorithms, in: IJCCI (ICFC-ICNC),2010, pp. 196–204.[64]A. Hashmi, A. Nere, J.J. Thomas, M. Lipasti, A case for neuromorphic isas, ACMSIGARCH Computer Architecture News, vol. 39, ACM, 2011, pp. 145–158
. [65]
G. Chechik, I. Meilijson, E. Ruppin, Synaptic pruning in development: acomputational account, Neural Comput. 10 (7) (1998) 1759–1777
. [66]
F.I. Craik, E. Bialystok, Cognition through the lifespan: mechanisms of change,Trends Cog. Sci. 10 (3) (2006) 131–138
. [67]
L. Steinberg, Cognitive and affective development in adolescence, Trends Cog.Sci. 9 (2) (2005) 69–74
.[68]
E. D’Angelo, A. Antonietti, S. Casali, C. Casellato, J.A. Garrido, N.R. Luque, L.Mapelli, S. Masoli, A. Pedrocchi, F. Prestori, et al., Modeling the cerebellarmicrocircuit: new strategies for a long-standing issue, Front. Cell. Neurosci. 10(2016) 176
.[69]
G.M. Shepherd, The Synaptic Organization of the Brain, Oxford UniversityPress, 2003
.[70]
P. Fransson, U. Åden, M. Blennow, H. Lagercrantz, The functional architectureof the infant brain as revealed by resting-state FMRI, Cereb. Cort. 21 (1) (2011)145–154
.[71]
N. Gogtay, J.N. Giedd, L. Lusk, K.M. Hayashi, D. Greenstein, A.C. Vaituzis, T.F.Nugent, D.H. Herman, L.S. Clasen, A.W. Toga, et al., Dynamic mapping ofhuman cortical development during childhood through early adulthood, Proc.Nat. Acad. Sci. USA 101 (21) (2004) 8174–8179
. [72] M. Lichman, UCI Machine Learning Repository, 2013 < http://archive.ics.uci. edu/ml>.[73] J. Mutch, U. Knoblich, T. Poggio, CNS: A GPU-Based Framework for SimulatingCortically-Organized Networks, Massachusetts Institute of Technology,Cambridge, MA, Tech. Rep. MIT-CSAIL-TR-2010-013/CBCL-286.[74] J. Mutch, Cns: Cortical Network Simulator, 2017 < http://cbcl.mit.edu/jmutch/ cns/>.[75] R. Salakhutdinov, G. Hinton, Deep Belief Networks, 2015 < http://www. cs.toronto.edu/hinton/MatlabForSciencePaper.html >. [76] C. Nadeau, Y. Bengio, Inference for the generalization error, in: Advances inNeural Information Processing Systems, 2000, pp. 307–313.[77]
M. Friedman, The use of ranks to avoid the assumption of normality implicit inthe analysis of variance, J. Am. Statist. Assoc. 32 (200) (1937) 675–701
. [78] P. Nemenyi, Distribution-Free Multiple Comparisons, Ph.D. thesis, PrincetonUniversity, NJ, 1963.Y. Rizk et al. / Applied Computing and Informatics 15 (2019) 81–93 93