Transfer Learning for Multi-Crop Leaf Disease Image Classiﬁcation usingConvolutional Neural Network VGG
Ananda S. Paymode ⁎, Vandana B. Malode
MGM's Jawaharlal Nehru Engineering College, Aurangabad 431001, Maharashtra, India
abstract article info
Article history:Received 9 October 2021Received in revised form 8 December 2021Accepted 30 December 2021Available online 7 January 2022In recent times, the use of artiﬁcial intelligence (AI) in agriculture has become the most important. The technol-ogy adoption in agriculture if creatively approached. Controlling on the diseased leaves during the growing stagesof crops is a crucial step. The disease detection, classi ﬁcation, and analysis of diseased leaves at an early stage, as well as possible solutions, are always helpful in agricultural progress. The disease detection and classi ﬁcation of different crops, especially tomatoes and grapes, is a major emphasis of our proposed research. The important ob-jective is to forecast the sort of illness that would affect grapes and tomato leaves at an early stage. TheConvolutional Neural Network (CNN) methods are used for detecting Multi-Crops Leaf Disease (MCLD). The fea-tures extraction of images using a deep learning-based model classi ﬁed the sick and healthy leaves. The CNN based Visual Geometry Group (VGG) model is used for improved performance measures. The crops leaves imagesdataset is considered for training and testing the model. The performance measure parameters, i.e., accuracy, sen-sitivity, speciﬁcity precision, recall and F1-score were calculated and monitored. The main objective of researchwith the proposed model is to make on-going improvements in the performance. The designed model classi ﬁes disease-affected leaves with greater accuracy. In the experiment proposed research has achieved an accuracy of98.40% of grapes and 95.71% of tomatoes. The proposed research directly supports increasing food production inagriculture.© 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Convolutional Neural Network (CNN)Artiﬁcial Intelligence (AI)Visual Geometry Group (VGG)Multi-Crops Leaf Disease (MCLD)
1. IntroductionTo contribute to the development of nations, knowledge of agricul-ture sectors is crucial. Agriculture is a one-of-a-kind source of wealththat develops farmers. For a strong country, the development of farmingis a necessity and a need in the global market. The world ’s population is growing at an exponential rate, necessitating massive food productionin the next 50 years. Information about different types of crops and dis-eases occurring at each level and its analysis at an early stage play a keyand dynamic role in the agriculture sector. A farmer's main problem isthe occurrence of various diseases on their crops. The disease classi ﬁca- tion and analysis of illnesses is a crucial concern for agriculture'soptimum food yield. Food safety is a huge issue due to a lack of infra-structure and technology, so crop disease classi ﬁcation and identi- ﬁcation are important to be considered in the coming days. This isnecessary for yield estimation, food security, and disease management.Detection and recognition of crops illnesses is an important study topicbecause it could be capable of monitoring huge ﬁelds of crops and de- tecting disease symptoms as soon as they occur on plant leaves. As aresult,ﬁnding a quick, efﬁcient, least inexpensive, and effectiveapproach to determine crops diseases instances is quite important(C. J.Chen et al., 2021).Artiﬁcial intelligence (AI) provides considerable assistance to agri-culture, which enhances a nation's gross domestic product (GDP)mostly through this sector. Climate change, labour scarcity, rainy seasonuncertainty, natural disasters, and various diseases on plant leaves areall major issues in agriculture. The plant leaves recognition and detec-tion studies with edge intelligence applied to agriculture. There is anew advancement with different deep learning models that overcomesthe challenge. The YOLOv3 neural network model is based on deeplearning and is built on an embedded system and the NVIDIA JetsonTX2. The system is implemented on a drone, and photographs of plantsare taken, pest positions are identiﬁed, and pesticides are applied asneeded; this is a novel approach based on deep learning (Al Hiary et al., 2011).Hyper spectral and multispectral knowledge acquisition techniquesand applications have exhibited their utility in improving agriculturalproduction and practises by providing farmers and agricultural manage-ment with crucial data on the elements impacting crop condition andgrowth. This technology has been widely employed in a variety of agri-cultural applications, including sustainable agriculture ( Ang, 2021).Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
⁎Corresponding author.E-mail addresses:anandpaymode@gmail.com(A.S. Paymode), vandanamalode@jnec.ac.in(V.B. Malode).
https://doi.org/10.1016/j.aiia.2021.12.0022589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/Weed detection in vegetable plantations is more dif ﬁcult than in crop plantations due to uneven plant spacing. Deep Learning technology isa novel method that blends with image processing. This approach con-centrates solely on recognising plants, avoiding the handling of numer-ous plant species. Furthermore, by reducing the amount of trainingimage collection and even the complexity of weed detection, this tech-nique can improve plant diagnosis accuracy and performance ( Jin et al., 2021).The massive crop loss occurred because of the failure to predict dis-ease at an early stage, which always results in lower crop production. Asa result, identifying and analysing crop diseases is a critical step in en-suring crop quality (Wu, 2020). As high computing speed and powerhave recently improved, the availability of massive datasets improvesthe system's efﬁciency.In this section, there are various techniques for detecting and classi-fying crop leaf disease. We present the related survey as a system thatemploys a variety of classiﬁer techniques. There are two types of combi-nations: serial and hybrid, with the combination of serial and parallelachieving the signiﬁcant performance parameter within 600 images(Massi et al., 2020). The hybrid combination has a recognition rate of91.11%, which is higher than the serial, parallel, and deep learning ap-proaches. For identifying and analyzing leaf illness, a deep learningconvolutional neural network (CNN) model was used to classify healthyand sick images. The model train contained 25 different plants, 58 clas-ses’sets, including healthy and diseased plants, and had 87,848 images.Using several (Ferentinos, 2018) model architectures, the best perfor-mance success rate was 97.53%. The Multi-Context Fusion Network(MCFN), a deep learning-based method, is built and prepared for cropdisease detection. The MCFN aids in the extraction of visual informationfrom 50,000 crop photos. The MCFN produced 77 common crops in-fected using a deep fusion model, with a 97.50% identi ﬁcation accuracy (Jin et al., 2021).The identiﬁcation of weeds in crops using the CovNet algorithm isalso a potent and cutting-edge approach. In recent research, boundingboxes were drawn across cropped images and the model was trained.Colour-based segmentations are applied to images and colour informa-tion, and visual categorization is calculated for weed images. The colourindex was examined with a genetic algorithm and Bayesian categoriza-tion (Jin et al., 2021). The deep residual network and the deep densenetwork are combined in the hybrid deep learning model. The hybriddeep learning model reduces training parameters while increasing ac-curacy by up to 95.00 % (Zhou et al., 2021). Deep transfer learning is an amazing performance methodology for identifying plant diseases. For pre-trained datasets, Inceptionand ImageNet modules were utilized ( Chen et al., 2020). The perfor- mance of pepper, vegetable, potato, and tomato leaf images in theplantvillage database was studied and enhanced using support vec-tor machine (SVM) and multi-layer perceptron. After training themodel the system achieves a higher performance accuracy of 94.35%(Kurmi et al., 2020).To detect and recognize corn dietary sickness, a Deep ConvolutionalNeural Network was deployed., The recognition of corn leaf diseased ac-curacy was 88.46 %, and the usage of hardware, such as a raspberry pi3with an Intel Movidius Neural Compute Stick and a system GPU thatpre-trained the CNN Model, resulted in superior metric accuracy perfor-mance (Sun et al., 2020).With the rapid growth of artiﬁcial intelligence and deep learn-ing technology, computer vision (CV) made a breakthrough. TheCV-based approaches are commonly utilized for diagnosing grapeleaf diseases. The principle component analysis (PCA) and backpropagation methods aid in the diagnosis of grape diseases suchas downy mildew and powdery mildew, with a research accuracyof 94.29 % (Xie et al., 2020), using VGGNet. The weights are initial-
ized using ImageNet pre-trained datasets, and over through thereal - world dataset, such approaches had a validation accuracy of91.83 %.2. Material & methods2.1. DatasetsTo support our research in the area of collection of images availablefrom Pennsylvania state university named plantvillage dataset. Thedataset plant-village included 152 crop solutions, 38 crop classes, and19 crop categories, for 54,303 crop leaves images. In the datasets, highquality JPEG image format with 5471width and 3648 height pixels areavailable. In the pre-processing, de-nosing, segmentation and afterimages are 256 X 256 pixels (Gandhi et al., 2018). The plantvillage is a well-known dataset for crop disease, with a large number of publicdatasets available. A plantvillage dataset images were captured in thelab, thus they are used as training datasets. Our model tested on realﬁeld captured images, As a result, we must concentrate on developingour ownﬁeld database. The test images were captured with a separateMegapixel camera and stored in a database. The datasets prepared intheﬁeld are available and be used in the proposed research. The agro-deep mobile application was used to capture some on- ﬁeld crops images. Theﬁeld photographs were taken with the redmi Note 5 Pro MIUIGlobal 11.0.5.0(PRIMEXM), Android Version PKQ1.180904.001, and acamera frame 4:3 high picture quality on 16 MP+5MP with f/2.2 aper-ture pixel, in a variety of natural environments. The disease-affected andhealthy photos are the most common image categories collected for re-search purposes. Healthy spot contaminated, mosaic virus, yellow leafcurl virus, septoria leaf spot bacterial spot, early blight, late blight, leafmould, septoria leaf spot, and spider mites are examples of tomatoimagery.2.2. Proposed researchThe schematic inFig 1depicts a potential view for multi-crop leafdisease classiﬁcation and analysis. Initially, plant leaf disease imagesare collected and classiﬁed into several categories. Pictureﬁltering, grey transformation, picture sharpening, and scaling are some of theimage-processing techniques. By using data augmentation methods,new sample photos are created from available photos to enhance andprepare the dataset. Augmentation procedures like turning, translation,and randomized transformation are employed to enhance the size ofthe dataset. The photos are then used as input to the suggested approachfor training the model in the following stage. The newly trained architec-tural model is used to anticipate previously unseen images. Eventually,theﬁndings of plant disease detection and identi ﬁcation are achieved. Finally, complete details of these steps are depicted in later parts ( Table 1).2.3. Sample images categoryThe sample images of crops shown in Fig. 2depict the category of ﬁeld’s tomato leaf images of various disease and healthy classes. The im-ages are one-of-a-kind for each type of disease symptom, pattern, spot,and colour mark. Speciﬁc tomato plant leaf diseases such as bacterialwilt, leaf mold, and grey spots are identiﬁed and detected as disease im- pacted recognition traits (Paymode et al., 2020). Fig. 3depictsﬁeld images of grape vine leaves obtained in the Nashikdistrict of Maharashtra, India. A grape category, Healthy 423, Black Rot1180, Black Measles 1383 and Leaf Blight 1076 images were recorded,recognized, and captured. The datasets for grape plant leaves were gen-erated by adjusting the brightness and hue of images from the A to Dcategory (SeeFigs. 4-5).The second crop of tomatoes sampled Early blight 1000, Mosaic virus373, Bacterial spot 2127, Late blight1909, Leaf mould 952, Septoria leafspot 1771, 1404 spot, spider mites 1676 and Yellow leaf curl 3209. Thedeep learning based methods are state-of-the-art in computer vision,which is used in image recognition and classi ﬁcation. In general, dataset col- lecting, data pre-processing, image segmentation, feature extraction, andclassiﬁcation are the four stages of Artiﬁcial Intelligence (AI) in agricultureA.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
24approaches for crop leaf disease detection and classi ﬁcation utilising Convolutional Neural Network (CNN). A Google Colaboratory platformwas used to pre-process the image, extraction of features, and classify it.2.4. Image augmentationThe large number of datasets improves the learning algorithms' perfor-mance and prevents overﬁtting. Obtaining a real-time dataset for use asinput to a training model is a complex and time-consuming operation. Asa result, data augmentation broadens the range of training data availableto deep learning models. Deep learning-based augmentation approachesinclude imageﬂipping, cropping, rotation, colour transformation, PCAcolour augmentation, noise rejection, Generative Adversarial Networks(GANs), and Neural Style Transfer (NST) ( Arun Pandian et al., 2019). The Faster DR-IACNN approach for detecting grape leaf diseases is based ondeep learning. The automatic extraction of spots on leaves has a high de-tection speed and accuracy. There are 4449 original photographs and62,286 photos developed using data augmentation techniques.
Fig 1.Proposed research systemﬂow diagram.
Table 1A study of deep learning techniques with classi ﬁcation and recognition rate (See Fig. 12).Approach Classi ﬁcation Model Recognition rate (%)Hybrid Combination (Massi et al., 2020) Three SVM SVM 91.11 Deep Learning (Ferentinos, 2018) CNN VGG 97.53 Multi-Context Fusion Network (MCFN) ( Wu, 2020) CNN AlexNet &VGG16 97.50 Deep Transfer Learning (DTL) ( Chen et al., 2020) CNN VGG 91.83 Machine Learning (Kurmi et al., 2020) SVM MLP 94.35 Deep Learning (Sun et al., 2020) DCNN DCNN 88.46 Deep Learning (Xie et al., 2020) Faster DR-IACNN Inception-v1 ResNet-v2 81.11A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
25The images are converted into a vector ofﬁxed features through fea- ture extraction in segmentation. The color, texture, and shape are thesystem-adopted features. A means, conﬁdence intervals, and sleekness have been employed as colored methods, with HSV and RGB colorspaces being retrieved. The gray-level co-occurrence matrix is preferredwhen extracting texture features from a colour image. This approach isused to identify plant diseases.2.5. Transfer learningThe model's optimization and training is a tough and time-consuming operation. A powerfulgraphical processing unit (GPU)is required for the training, as well as millions of training examples.However, transfer learning, which is employed in deep learning,solves all of the problems. The pre-trained Convolutional NeuralNetwork (CNN) used in transfer learning is optimized for one taskand transfers knowledge to different modes ( Nevavuori et al., 2019). The multi-crop image dataset model comprises a size of224 X 224. The residual network (ResNet) needed to be tweaked.In all ResNet models, theﬁnal layer before the softmax is a 7 X 7average-pooling layer. A smaller image can ﬁt into the network when the pooling size decreases. The basic picture preparation isnecessary for the transfer learning considerations with the multi-cropped image dataset.3. Results & discussion3.1. Convolutional neural networkThe convolutional layers, pooling layers, fully-connected layers, anddense layers constitute the architecture of the Convolutional NeuralNetwork (CNN) (SeeFig. 6). The layers' description is shown below.3.1.1. Convolutional layerConvolutional layers' fundamental function is to extract unique fea-tures from images. The implementation of convolutional layers on anormal basis facilitates the extraction of input features ( Chen et al., 2020), The features extraction (Hi) among several layers in CNN is com-puted using the formula below.H
i¼φH i−1Wiþb i ðÞ ð 1ÞWhere, Hi - Feature map, Wi–Weight, bi is offset andφ–Rectiﬁed Lin- ear Unit (RELU)3.1.2. Pooling layersThe pooling layers are a crucial component of a Convolutional NeuralNetwork (CNN). It shrinks the size of convolved features in dimensionwhile simultaneously minimizing the computational resources neces-sary for image processing. Pooling arise categorized into two types:max pooling and average pooling. Max pooling returns the maximumvalue of images, whereas an average pooling returns the average valueof the image section.3.1.3. Drop-out layersThe dropout layers improve the capability of a trained model. It pro-vides regularization and prevents the model from over- ﬁtting by de- creasing the correlation between the neurons. The drop out process isused in all the activation functions but it is scaled by factor ( Liu, 2020).3.1.4. Flatten layersIt collapses the spatial dimensions of the mapped pooled featureswhile retaining the channel dimensions. The ﬂattened layer adds extra dimensions and after it is transformed into a vector. The vectored feed
Fig 2.Sample tomato leaf images (A: Mosaic Virus, B: Healthy, C: Target Spot, D: Late Blight, E: Bacterial Spot, F: Septoria Spot, G: Spider Mite, H: Leaf Mold , I: Early Blight, J: Yellow Leaf.
Fig 3.Sample grapes plant leaf images. (A: Grape Black Rot, B: Grape Esca (Black Measles), C: Healthy, D Grape Leaf blight (Isariopsis Leaf Spot).A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
26to fully connected layers also known as the dense layer or fully con-nected layers.3.1.5. Fully-connected layersFully connected layers are needed for extracted images classi ﬁcation features because of their special purpose. The softmax function predictsearlier extracted image attributes from preceding layers. Softmax is amulticlass classiﬁcation activation function in the output layers. Theneural network layer uses a multilayer perceptron model (MLP) as aclassiﬁer for two-class classiﬁcation. The model with nonlinearity,which is introduced in the full vectors using recti ﬁed linear unit (RELU) activation. The versatility of class separation is greater whenemploying a support vector machine (SVM). The essentials of SVM areas described in the following:Minimize
1=2∑nj¼1W21þC∑Nj¼1ξjð2ÞWhere C is the tuning measure, subject to the constraint y
j(W∙X+b )≥ 1–ζ,j=1 ,2 ,3…N. The softmax parameterγ= 1 and C = 1 are used throughout training and test sets of the classi ﬁcation algorithm.
Fig 4.Multi-crops image augmentation (a) (A: Original B: Rotate, C: Color, D: Image Point, E: Hstack, F: Size G: Gaussian Noise, H: Shape).
Fig 5.A B: H Stack, C: Original, D: Augmentation, E: Batch H stack, F: Adaptive Gaussians Noise.A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
27The ConvNet architecture design's main component is its depth. Bydeﬁning additional design parameters and growing the network depthcontinuously, by adding more convolutional layers that are doable byusing extremely small (3 x 3) convolution ﬁlters in all layers. As a result, they've developed substantially more accurate ConvNet architecturesthat not only reach state-of-the-art accuracy on ﬁxed dataset classiﬁca- tion and localisation tasks, but are also applicable to other image recog-nition datasets, where they perform admirably even when utilised aspart of relatively simple pipelines(Simonyan and Zisserman, 2015).Our ConvNets are fed aﬁxed-size 224 x 224 RGB picture during training.The only pre-processing we perform is removing each pixel from themean RGB value determined on the training set. We apply ﬁlters with a very small receptiveﬁeld 3 x 3 to send the image through a stack ofconvolutional layers. We also use 1 x 1 convolution ﬁlters in one of the conﬁgurations, which are a linear change of the input channels (followedby non-linearity). The convolution stride is set to 1 pixel, and the spatialpadding of the convolutional layer input is set to 1 pixel for 3 conv. layersso that the spatial resolution is kept after convolutional. Five max-pooling layers, which follow part of the convolutional layers, do spatialpooling (not all the convolutional layers are followed by max pooling)Max-pooling is done with stride 2 over a 2 x 2 pixel window.3.2. VGG16The Convolutional Neural Network based VGG16 pre-trainedmodels are used to improve the performance and classify the cropimages as healthy and disease images. For quality detection andanalysis of crop leaf images, the initial model transfers informationfrom pre-trained VGG16 models. The Convolutional Neural Network(CNN) model retained new images of the ﬁeld and learned to per- form a model for disease detection and classi ﬁcation (Alencastre- Miranda et al., 2021).The VGG model improved with large kernel-sized ﬁlters, with 11 and 5 convolutional layers with a 3 x 3-kernel ﬁlter size. The input image size isﬁxed at 224 x 224. Following image pre-processing, imageswere passed through a convolutional layer with a ﬁlter size of (3 x 3).For linear transformation of the input channel, the ﬁlter size is set to (1 x 1). The stride size isﬁxed to 1 and max pooling is performed with 2 x 2 sizesand stride set to 2. In the next steps, fully connected layers have thesame conﬁguration with 4096 channels in each layer. The ﬁnal layer is the softmax activation layers, followed by the RELU activation functions(SeeFig. 7).3.3. Performance measureThe F1 score, accuracy matrix, and Receiver operating characteristic(ROC), as well as the area under the curve (AUC), are being used to eval-uate segmentation performance (AUC). The performance of the classi-ﬁer is measured using evaluation metrics.3.3.1. Accuracy metricsThe model performance for all classes is accurately measured. Theaccuracy is calculated by adding the total number of correct predictionsto the total number of predictions. The performance parameter calcula-tion of precision and recall and F1-Score are measured. The accuracy isexpressed in terms as follows.AC¼
TPþTNðÞTPþFPþFNþTNðÞ ð3ÞWhere, TP is True Positive, TN True Negative, FN False Negative andFP False Positive Samples. The classiﬁer performance measure using evaluation metrics are gives as;
Fig 6.Proposed convolutional neural network CNN architecture.
Fig 7.Proposed convolutional neural network (CNN) VGG16 architecture.A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
28TPR¼SensitivityðÞ ¼TPTPþFNðÞ,TNR¼SpecificityðÞ TNTNþFPðÞð4ÞFPR¼
FPFPþTNðÞ ð5ÞWhere, TPR is True Positive Rate, TNR True Negative Rate, and FPRFalse Positive Rate.Precision¼
TPTPþFPðÞ,Recall¼ TPTNþFNðÞð6ÞG−Mean¼∏
mK¼1Recall K/C18/C191mð7ÞHere m represents the number of categories and G denotes the TNRand FPR accuracy ratio.Mean average precision (mAP), which consists of Precision, Recall,and Mean, is the algorithm assessment standard employed. Image pro-cessing and detection rely heavily on the mAP. From the entire results,the accuracy has classiﬁed correctly. From the completeﬁndings, the re- call is correctly classiﬁed.The F1 score is another important metric for evaluating the algo-rithm. It's precision and recall fundamental that's presented as follows:F1Score¼
2/C2Precision/C2RecallPrecisionþRecallðÞ ð8Þ3.3.2. Receiver operating characteristicThe receiver operating characteristic (ROC) curve is used to understanddeterministic indications of categorization sorting as well as computationalmodeling challenges. The curve is a graph that shows the ratio of false pos-itives to true positives underdifferent standard limits (SeeFig. 8). A prototype also with largest true negative rate values was used to cor-rectly categorize defectives, and the model with the highest true positiverate values was used to correctly classify healthily. To boost productivityby reducing processing time for training and testing, the MCC (MatthewsCorrelation Coefﬁcient) is employed for the total computation. MCC is acriterion for categorizing complex data into distinct categories. MCC is a su-perior method to accuracy which only has signi ﬁcant importance if the true positives, true negatives, false negatives, and false positives outcomesare all positive. The MCC ranges from 1 (poorest judgment) to 1 (perfectpredictions), with an MCC of 0 suggesting a random guess.The model is tuned by the number of epochs, hidden layers, hiddennodes, activation functions, dropout, learning rates, and batch size. Themodel performance is affected by hyper parameter tuning. The term"hyper parameter tuning" refers to the process of repeatedly adjusting hid-den layers, epochs, activation function, or learning rate. The model isﬁne- tuned to achieve the best accuracy while minimising the average loss.The experimental analysis was carried out on Google research prod-ucts on Google Colaboratory. The Colaboratory platform supports pythonprogramming, and nearly all of the Python libraries are uploaded andinstalled for research purposes. The Python 3 Google Compute Enginebackend (GPU) with RAM of 12.72 GB and disc space of 68.40 GB is avail-able while experimenting. The dataset is uploaded with the drivemounted, and the model is trained on the Google platform with highconﬁgurations. A Python convert to image function is used for convertingall the images to an array and fetching images from the directory.The processed images come from a directory, and all label images aretransformed using the label binarized sklearn python package. The totalnumber of images is divided into train and test using train-test-split py-thon functions. The model parameters were set as shown in Table 2,a n d the model was trained to calculate all trainable and non-trainable pa-rameters. The Adam optimization algorithm is used to train the deeplearning convolutional neural network model. The algorithm optimizedthe sparse gradient noise issue.The input network uses 224 X 224 images, and the batch size is 30for grapes and 25 for tomatoes, respectively, and the same test is per-formed for different epochs with batch size and learning rate. In everypolling layer with a 2 x 2-pool size and the RELU function utilized inthe network, the model performs a max-pooling operation. The outputof the last layer is a oftmax-activation multi-crop-developed prediction.During the network's training phase, hyper parameters such as learningrate and epoch size were adjusted. The average accuracy achieved was98.40% for grapes and 95.71% for tomatoes, respectively. The learningrate is tested at different values to optimize targeted performance mea-sured. The validation process is based on a total number of images fromthe multi-crop dataset. With the setting of different epochs and batchsize, the accuracy improved and grew.The crops-leaf images datasets are used to train the model and iden-tiﬁcation of class and category of disease with transfer learning tech-niques including VGG16. The original datasets are divided intotraining data 80%, validation data 10% and testing data 10%.
Fig 8.Receiver operating characteristics FP versus TP
Table 2Parameter setting for trained the modelHyperparameter Value SettingCrops Grapes & TomatoesConvolutional Layers 13Max Pooling Layer 5Dropout Rate 015/0.25/0.50Activation Function Relu, SoftmaxEpochs 20/25/30/40/45Learning Rate 0.00001/0.0001Image Size 224 x 224 x 3
Table 3Experimental results of the grape model for setting different parameters.No. of epochs Learning rate Dropout rate No. of images Training loss Training accuracy Validation loss Validation accuracy40 0.00001 0.25 450 0.0897 0.9840 0.0486 0.988930 0.0001 0.50 450 0.1136 0.9585 0.0686 0.986745 0.00001 0.25 400 0.0995 0.9796 0.0529 0.985845 0.0001 0.25 750 0.0875 0.9696 0.0521 0.985330 0.0001 0.50 450 0.1326 0.957 0.0686 0.984340 0.001 0.30 600 0.1139 0.9606 0.0529 0.9831A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
293.4. Training and validation accuracyTraining and validation accuracy is measured by setting differ-ent values while training the model. The experiments were carriedout at Google Colaboratory on the available RAM of 12.50 GB. Whileperforming the experiment, different values are set for the follow-ing: the number of epochs, learni ng rate, dropout rate, and the number of images noted as training loss, training accuracy, valida-tion loss, and validation accuracy. A model's performance is mea-sured and veriﬁed on the grape and tomato crops' leaves. Table 3 andTable 4show the details of the results of experiments carriedon grapes and tomatoes, respectively.3.5. Figures and graphsA model's performance is measured and veri ﬁed with training, test- ing, and validation methods for grapes and tomatoes leaves. Fig 9and Fig. 10show the training and validation accuracy and loss of the grapeleaves and tomatoes, respectively.The confusion matrix has been used to measure the performance pa-rameter for grapes and tomatoes leaves, as shown in Fig. 11. Experiment with the facts collected. The suggested approach is tested using ourgrapes and tomatoes image datasets, which were taken in a real- ﬁeld with various backdrop and light intensities, similar to the tests done inSection 4.4.Table 4Experimental results of the tomatoes model for setting different parameters.No. of epochs Learning rate Dropout rate No. of images Training loss Training accuracy Validation loss Validation accuracy25 0.0001 0.25 200 0.1643 0.9571 0.2627 0.943235 0.00001 0.20 180 0.2203 0.9281 0.3143 0.901330 0.00001 0.15 200 0.2624 0.9097 0.3345 0.892630 0.00001 0.25 200 0.3042 0.8983 0.3736 0.884930 0.00001 0.25 180 0.4871 0.8354 0.4149 0.867130 0.00001 0.50 200 0.5226 0.8255 0.4508 0.8538
Fig 9.Training and validation. (a) Accuracy and (b) loss of VGG16 grapes.
Fig 10.Training and validation. (a) Accuracy and (b) Loss of VGG16 tomatoes.A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
30To assure the diversity of sample images and avoid the over ﬁtting problem, data augmentation techniques such as random rotation, ﬂip- ping, and scale transform, as well as associated pre-processing activities,are used to extend the training samples. The processes are described inmore detail below.1. Image resize: The total images scaled into size of 224 x 224 pixels, forthe modelﬁt and minimum 200 images taken from each healthy andunhealthy category are augmented with data augmentation methods.2. Image pre-processing: Image pre-processing is used to darken thedifferent lengths of the image data, going to bring them into ratio
Fig 11.Confusion matrix. (a) Tomatoes and (b) grapes.
95.71%95.00%91.83%86.10%81.11%80.30%70%75%80%85%90%95%100%Accuracy in Percentage
Tomatoes Methods
Fig 13.Comparison between different model vs accuracy in percentage (%) with proposed VGG16 tomatoes.98.40%97.53%97.50%91.83%88.46%81.11%
0%20%40%60%80%100%120%
ProposedVGG16Deep LearningVGGMul/g415-ContextFusion NetworkAlexNet &VGG16Deep TransferLearning VGGDeep LearningDCNNDLIncep/g415on-v1ResNet-v2Accuracy in Percentage
Grapes Methods 
Fig 12.Comparison between different model vs accuracy in percentage(%) with proposed VGG16 grapes.A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
31and retaining the initial images' knowledge formation whileattempting to prevent image deformation.3. The dataset partition and training. In this section a selection of ran-dom sample images for proposed experiments and calculated withcarried out the result as per Section 4.4.4. Validation and testing. The testing is done on the images that wereused to evaluate the model, and new images from outside modelingare used to check the model effectiveness. The output results arecompared to the real categories, the effectiveness of the controlthat goes with them is computed.The residual block collection and DesnseNet used in task of tomatoleaf disease identiﬁcation with RDN restructured model. After inputimage normalizing and adding the convolutional layer residual modulesdense layer classify the tomato disease images with 95% accuracy dis-ease dataset (Zhou et al., 2021). The public data set of the AI ChallengerCompetition in 2018 used the Inception-ResNet-v2 model using theRELU activation function, with an accuracy of 86.1%. ( Ai et al., 2020), under complex background conditions, the accuracy of VGG Net is91.83 %. One more approach to INC-VGGN rice disease detection withan average accuracy of 80.38% for both "Phaeo- sphaeria Spot" and"Maize Eyespot" diseases (J.Chen et al., 2020)( S e eFig. 13).4. ConclusionIn this paper, there are two types of crop disease leaves were collectedand prepared as a dataset with available data. The techniques of data aug-mentation, dataset pre-processing, training, and testing are applied to theconvolutional neural network-based VGG16 model. The proposed modelis built and tested to improve the performance measured and compared.The evaluation metrics parameters are higher and increased as comparedto other available datasets and methods. Therefore, our proposed re-search work increased accuracy for grapes by 98.40% and for tomatoesby 95.71%. Always improving the performance of on-ﬁeld crops, leaf im- ages and diseases classiﬁcation and analysis is a critical step, but with ourmodel achieved the highest performance, which supported agriculturaldevelopment. The major focus of research is to provide advancement inthe agriculture sector and an increase in food production. The collectionand preparation of genuine datasets and applying to the deep learningmodels with multiple crops leaves images is a future target. In the future,the use of Inception V3 and ResNet-based CNN models for much deeperanalysis of crop images is anticipated. Our work encourages and stimu-lates farmers, which ultimately raises farm income and helps to buildup powerful countries.AcknowledgementsFarmers from Nashik and Aurangabad, Maharashtra [India],contributed to the collection of realﬁe l dc r o pi m a g e sf o rr e s e a r c h purposes. We would like to thank you Dr. Panjabrao DeshmukhKrushi Vidyapeet (Dr. PDKV), Akola , Maharashtra [India], for their encouragement and assistance.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.Credit author statementWe the undersigned declare that this manuscript is original, has notbeen published before and is not currently being considered for publica-tion elsewhere.We would like to draw the attention of the Editor to the followingpublications of one or more of us that refer to aspects of the manuscriptpresently being submitted. Where relevant copies of such publicationsare attachedWe wish to draw the attention of the Editor to the following factswhich may be considered as potential con ﬂicts of interest and to signif- icantﬁnancial contributions to this work. [OR]We wish to conﬁrm that there are no known conﬂicts of interest as- sociated with this publication and there has been no signi ﬁcantﬁnancial support for this work that could have inﬂuenced its outcome. We conﬁ
rm that the manuscript has been read and approved byall named authors and that there are no other persons who satis ﬁed the criteria for authorship but are not listed. We further con ﬁrm that the order of authors listed in the manuscript has been approved byall of us.We conﬁrm that we have given due consideration to the protectionof intellectual property associated with this work and that there are noimpediments to publication, including the timing of publication, withrespect to intellectual property. In so doing we con ﬁrm that we have followed the regulations of our institutions concerning intellectualproperty.We further conﬁrm that any aspect of the work covered in this man-uscript that has involved either experimental human patients has beenconducted with the ethical approval of all relevant bodies and that suchapprovals are acknowledged within the manuscript.We understand that the Corresponding Author is the sole contactfor the Editorial process (including Editorial Manager and directcommunications with the ofﬁce). He/she is responsible for commu-nicating with the other authors about progress, submissions of revi-sions andﬁnal approval of proofs. We conﬁrm that we have provided a current, correct email address which is accessible by the Corre-sponding Author and which has been con ﬁgured to accept email from biomaterials@elsevier.com.References
Ai, Y., Sun, C., Tie, J., Cai, X., 2020. Research on recognition model of crop diseases and insectpests based on deep learning in harsh environments. IEEE Access 8, 171686 –171693. https://doi.org/10.1109/access.2020.3025325 . Alencastre-Miranda, M., Johnson, R.M., Krebs, H.I., 2021. Convolutional neural networksand transfer learning for quality inspection of different sugarcane varieties. IEEETrans. Indust. Inform. 17 (2), 787 –794.https://doi.org/10.1109/TII.2020.2992229 . Ang, K.L.M., Seng, J.K.P., 2021. Big data and machine learning with hyperspectral informa-tion in agriculture. IEEE Access 9, 36699 –36718.https://doi.org/10.1109/ACCESS. 2021.3051196.Arun Pandian, J., Geetharamani, G., Annette, B., 2019. Data augmentation on plant leaf dis-ease image dataset using image manipulation and deep learning techniques. Pro-ceedings of the 2019 IEEE 9th International Conference on Advanced Computing,IACC 2019, pp. 199–204https://doi.org/10.1109/IACC48062.2019.8971580 . Chen, J., Chen, J., Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020. Using deep transfer learningfor image-based plant disease identi ﬁcation. Comput. Electr. Agricult. 173 (November 2019) 105393.https://doi.org/10.1016/j.compag.2020.105393 . Chen, C.J., Huang, Y.Y., Li, Y.S., Chen, Y.C., Chang, C.Y., Huang, Y.M., 2021. Identi ﬁcation of fruit tree pests with deep learning on embedded drone to achieve accurate pesticides p r a y i n g .I E E EA c c e s s9 ,2 1 9 8 6 –21997.https://doi.org/10.1109/ACCESS.2021. 3056082.Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Elect. Agric. 145 (September 2017), 311 –318.https://doi.org/10.1016/j. compag.2018.01.009.Gandhi, R., Nimbalkar, S., Yelamanchili, N., Ponkshe, S., 2018. Plant disease detection usingCNNs and GANs as an augmentative approach. 2018 IEEE International Conference onInnovative Research and Development, ICIRD 2018, no. May: 1 –5https://doi.org/10. 1109/ICIRD.2018.8376321. Hiary, H., Al, S.B., Ahmad, M., Reyalat, M. Braik, ALRahamneh, Z., 2011. Fast and accuratedetection and classiﬁcation of plant diseases. Int. J. Comput. Appl. 17 (1), 31 –38. https://doi.org/10.5120/2183-2754 . Jin, X., Che, J., Chen, Y., 2021. Weed identi ﬁcation using deep learning and image process- ing in vegetable plantation. IEEE Access 9, 10940 –10950.https://doi.org/10.1109/AC- CESS.2021.3050296.Kurmi, Y., Gangwar, S., Agrawal, D., Kumar, S., Srivastava, H.S., 2020. Leaf image analysis-based crop diseases classiﬁcation. Signal Image Video Process. https://doi.org/10. 1007/s11760-020-01780-7. Liu, S.Y., 2020. Artiﬁcial intelligence (AI) in agriculture. IT Profes. 22 (3), 14 –15.https:// doi.org/10.1109/MITP.2020.2986121 . Massi, I.E., Mostafa, Y.E.-s., Yassa, E., Mammass, D., 2020. Combination of multiple classi-ﬁers for automatic recognition of diseases and damages on plant leaves. Signal ImageVideo Process.https://doi.org/10.1007/s11760-020-01797-y .A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
32Nevavuori, P., Narra, N., Lipping, T., 2019. Crop yield prediction with deep convolutionalneural networks. Comput. Elect. Agric. 163 (June) 104859. https://doi.org/10.1016/j. compag.2019.104859.Paymode, A.S., Malode, V.B., Shinde, U.B., 2020. Artiﬁcial intelligence in agriculture for leaf disease detection and prediction: a review 13 (4), 3565 –3573. Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scaleimage recognition. 3rd International Conference on Learning Representations, ICLR2015 - Conference Track Proceedings .
http://www.robots.ox.ac.uk/. Sun, J., Yang, Y., He, X., Xiaohong, W., 2020. Northern maize leaf blight detection undercomplexﬁeld environment based on deep learning. IEEE Access 8, 33679 –33688. https://doi.org/10.1109/ACCESS.2020.2973658 .Wu, W., Yang, T.L., Li, R., Chen, C., Liu, T., Zhou, K., Sun, C.M., Li, C.Y., Zhu, X.K., Guo, W.S.,2020. Detection and enumeration of wheat grains based on a deep learning methodunder various scenarios and scales. J. Integr. Agric. 19 (8). https://doi.org/10.1016/ S2095-3119(19)62803-0.Xie, X., Ma, Y., Liu, B., He, J., Li, S., Wang, H., 2020. A deep-learning-based real-time detec-tor for grape leaf diseases using improved convolutional neural networks. Front. PlantSci. 11 (June) 1–14.https://doi.org/10.3389/fpls.2020.00751 . Zhou, C., Zhou, S., Xing, J., Song, J., 2021. Tomato leaf disease identi ﬁcation by restructured deep residual dense network. IEEE Access 9, 28822 –28831.https://doi.org/10.1109/ ACCESS.2021.3058947.A.S. Paymode and V.B. Malode Artiﬁcial Intelligence in Agriculture 6 (2022) 23 –33
33