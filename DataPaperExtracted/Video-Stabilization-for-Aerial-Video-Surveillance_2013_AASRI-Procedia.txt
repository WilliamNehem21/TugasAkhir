 AASRI Procedia   4  ( 2013 )  72 – 77 
2212-6716 © 2013 The Authors. Published by Elsevier B.V.
Selection and/or peer review under responsibility of American Applied Science Research Institute
doi: 10.1016/j.aasri.2013.10.012 ScienceDirect
Abst
Aeria
move
(UAV
and m
remo
perfo
 
© 20
Sele
 
Keyw
1. In
T
unw
unde
 
* 
E
 Vid
tract 
al video stabili z
ement of mobi
V). Our syste m
matching to es t
ove video nois
ormance. 
013. Publishe d
ection and/or p
words:  Aerial Vi d
ntroduction 
The quality of 
wanted vibrate d
esired motio n
 
Corresponding a
E-mail address : w2013 AA S
deo Sta b
Ahle m
a
Univ e
zation system a
le sensor. In t h
m is based on k e
timate paramet e
e. A number o
d by Elsevier 
peer review u n
deo stabilization, k
output video 
d motion, bo o
n between fr a
author. Tel.: +21 6
alha.ahlem, ali. wSRI Confer e
bilizatio n
m Walhaa, 
a REGIM- Lab. R e
ersity of Sfax, Na t
aims to remov e
his article we p
eypoints tracki n
ers of affine tr a
of real aerials 
B.V. 
nder responsi b
kalman filtering, 
in mobile sur v
om or pan. Vi d
ames has bee n
6-21749702; fax: 
wali, adel.alimi@ ience on Int e
n for Ae
Ali Walia
esearch Laborat o
tional Engineeri n
 Sfax 3038, T
 
e undesired mo t
present a new v
ng. We use Sca
ansformation m
videos survei l
bility of Ame r
Motion estimati o
veillance syst
deo stabilizat i
n remove d. T
 +216-7467754 5
ieee.org elligent Sys t
rial Vid e
and Adel M
ory on Intelligent 
ng Scool of Sfax (
Tunisia  
tion in aerial v i
video stabilizat
le Invariant Fe a
model. Then, K a
llances demon
rican Applied 
on,Scal Invariant 
ems suffers f r
ion is used to 
Therefore it h
5. tems and C o
eo Surv e
M. Alimia *
Machines, 
(ENIS), BP1173
ideo. This mot i
ion system for 
ature Transfor m
alman filter wi t
strate that thi s
Science Res e
feature transfor m
rom different u
create a new v
has become eontrol 
eillance 
* 
ion is the resul t
Unmanned A e
m (SIFT) key po
th median filte r
s method can a
earch Institute 
m; 
undesired jitt e
video sequen c
essential in mt of undesired 
erial Vehicles 
oint detection, 
r is applied to 
achieve good 
er like track, 
ce where the 
many mobile 
Available online at www.sciencedirect.com
© 2013 The Authors. Published by Elsevier B.V.
Selection and/or peer review under responsibility of American Applied Science Research InstituteOpen access under CC BY -NC-ND  license.
Open access under CC BY -NC-ND  license.73  Ahlem Walha et al.  /  AASRI Procedia   4  ( 2013 )  72 – 77 
surveillance systems such as unmanned aerial vehicle (UAV) sy stems. Also it is the first step in many aerial applications such as background estimation and object tracking [1]. Techniques of video stabilization can be divided into four groups: optical approach, mechanical approach, electronic approach and digital approach. In this paper we  focused on digital approach. This technique is an image pre-processing. All digital stabilization systems handle three essential aspects. The first one estimates the global motion, the second one concerns the motion smoothing and the last one is related to the motion compensation.  Among these components, the step of glob al motion estimation is the most vital but also the most difficult one [9]. In case of a fixed camera, a strong winds or small vibration from heavy traffic can caused the global motion.  In this case background is almost fixed over the long term [15]. Consequently, motion can be calculated by local point tracking [1], or by searching a region that contain few motions [7]. In the case of mobile platform, on which the research is conducted, the global motions include two elements: the intentional and the unwanted motion. Several efforts have been put in case of mobile camera.  Block matching techniques improves motion estimation by using different adaptive fi lters [9]. These method present good results if the video does not contain moving object [8].  In this paper we present a new system to stab ilize aerial video surveillance by extracting and matching SIFT point for consecutive frames. In the following, Sec.2 cites some related works on video stabilization; Sec. 3 describes our proposed system. Results achieved by our system are presented in Sec.4. Finally Sec.5 presents summarized conclusions. 2. Related work The goal of feature based algorithms is to estimate  interframe motion by extraction features from video images [15]. Some techniques [6] that combine features extraction with other robust filters have good performances. Motion filtering is also a significant step in  video stabilization process. In this step undesired movement is recognized by evaluation of estimate d motion. Different methods have been introduced to correct translational and rotational jitters. Kalman f iltering [10] and extended Kalman filtering, Frame Position Smoothing [13], Gaussian filtering [17] a nd Motion Vector Integration [14] are among these techniques.  A video stabilization algorithm using SIFT [12] has been introduced in [14]. Junlan et al [14] uses Iterative Least Squares method to reduce estimati on error then uses Adaptive Motion Vector Integration to filter intentional camera motion. Another system [19]  employs SIFT point in order to calculate interframe motion. They recognize intentional movement by Kalman filtering and reduce error variance by using particle filter. But, in this system original SIFT algorithm’ s parameters doesn’t adapt to video stabilization system. And both Kalman filtering and particle filtering calcu lated for each frame implies intensive computation.  3. Approach Our input is a real video captured from UAV. First of all, SIFT point are extracted and matched for two consecutive frame.  Next, inter frames motion is estimated using affine transform model. Finally, both Kalman filtering and median filtering are used in the s tep of frame compensation. The details are explained as follows. 3.1. SIFT Point Extraction and matching SIFT is presented by David Lowe as a local feature description [12]. SIFT point are invariant to image translation, rotation and scale [2]. Therefore, it can identify and track keypoints over multiple frames of video. 74   Ahlem Walha et al.  /  AASRI Procedia   4  ( 2013 )  72 – 77 
It can also afford robust matching in our case where aerial video challenges are considered such as noise, viewpoint changing and inconstant illumination. 
 
In our approach, we estimate global motion vector by  extracting SIFT points from two successive frame. Next we calculate local motion vector by matching they  two sets of invariant features. In other words, local motion vector between frame 
n-1 and frame n, can be estimated by extracting both the first and the second keypoints K
point1 (xpoint1, ypoint1, 1) and K point2 (xpoint2, ypoint2, 1) from these two frames. In this step, we can show how the keypoint has probably moved from two successive frames. Then we use RANSAC (Random Sample Consensus) to select optimal matching. But, by this  method, we obtain a whole number of local motion vectors. They sets of vector does not contain helpful indication for real movement of the camera because they include matches related to moving objects in the frame. Deal with this problem we assume that, comparing to other motions, the velocity of moving objects in the scene is very large. For this reason we use a fixed threshold to eliminate moving object. As a result, we can generate the transformation matrix. 3.2. Motion Estimation Motion can be described either by a 2-D model or by a 3-D model [2]. The various transformations occurring in the 2D plane are Translation, Euclidean or rotation, Similarity and Affine. Thus, in our method we adopt a four parameter 2-D affine estimation mode l to describe geometric transformation between two consecutive frames.  Given a point localized as P
n(xn, yn ,1) in  frame n, and located as  P n+1 (xn+1,yn+1,1) in frame
n+1, the transformation model  from P n to P n+1 can be described as:  
1.111nnnn yxAyx
                                                                                              (1) 
A is an affine matrix precisely described by   rotation,  Sc scaling, and xTrand yTrtranslations of the camera in a scene with.      
     (2)   In this matrix has only four free parameters compared  to the complete affine transformation matrix which originally has six: one scale, one angle, and two translations. To resolve this issue, we use linear Least Squares Method on a set of iterations. In fact, it can provide robust parameter estimation. 3.3. Motion  compensation In this final step, we need to correct the current frame to obtain stable image. But parameters calculated in equation (1) contain two types of motion: motion of the sensors and normal movement of the UAV. To compensate the current frame we should separate these two types of motion.      100cossinsincos.yxTrScScTrScScA75  Ahlem Walha et al.  /  AASRI Procedia   4  ( 2013 )  72 – 77 
 
Fig.  1 Sample images from VIRAT Aerial Video Dataset 
Kalman filter is a basic procedure to put the new frame according to the estimated motion. So we use this filter to estimate the motion of the sensor. Then we use median filter in order to refine the obtained result. 4. Experimental  and
  Results We test our system on a variety of scenes from VIRAT Ae rial Video dataset [16] containing low resolution sequences of 720 x 480 pixels captured in 30 fps. This aerial dataset is characterized by zooming, varying
 
viewpoints and scale. The results of the proposed system are illustrated in Fig. 2 
 
 Fig.  2.  Original and stabilized video. First row: frame 1,40, 80 and 211 of original sequence  is shown here. Second row: sta bilized sequences.  
Peak Signal-to-Noise Ratio (PSNR) is used to ev aluate the quality of our stabilized video. PSNR computed between   two consecutives frames is defined as:  
nMSEInPSNRMAX10log10                                             (3) 
The PSNR gives a relation between the desired outp ut and the obtained video. In this equation, nMSE measure the Mean-Square-Error between successive frames, I
MAX is the maximum pixel value of an image. Frame dimensions are represented by N and M. The PSNR value for each frame of the original video and our stabilized video are shown in Fig 3. Higher PNSR be tween two stabilized frames represent good quality of stabilized video.   
76   Ahlem Walha et al.  /  AASRI Procedia   4  ( 2013 )  72 – 77 
MyNxnN
yxIyxIMNnMSE
1121,,1                                    (4)  
 
Fig.  3. PSNR of two original video and two stabilized video.   
The measurement of Interframe Transformation Fidelity (ITF) is determined by  
11
11frameNkframe
kPSNRNITF        (5) 
  TIF is the average of the PSNR between two consecutives frames.  In general this average is used for each value, to obtain an approximate estimation of the quality of the stabilized video. Similar to PSNR, upper ITF values indicate super quality video stabilization. ITF valu es for three video sequences tested are shown in Table1. This evaluation illustrate that, the ITF of our stabilized videos is superior to the ITF of the original videos. The ITF of our stabilized videos enhances, which is acceptable. 
Table. 1 ITF of original and stabilized videos Videos Original ITF  Stabilized ITF  
Video1 23,74 23,98 
Video2 18,04 21,87 Video3 25,65 27,54 
5. Conclusions and future works  A new system for aerial video stabilization has been introduced  in this article. The main idea of this system is to filtered undesired motion by detecting and matching SIFT point in order to predict the interframe motion. To evaluate our system we used real video captured by a camera installed on UAV.  The experimental results prove the efficiency and accuracy of our stabilization system. Our future work will concentrate on performing motion estimation by integrating optical flow in the process of local motion detection. 010203040
0 1 02 03 04 15 16 27 38 39 3Video 2 Original
Video 1 Original
Video2 Stabilized
Video 1 stabilized77  Ahlem Walha et al.  /  AASRI Procedia   4  ( 2013 )  72 – 77 
References 
[1] Hong S, Hong T and Wu Y, Multi-resolution unmanned aerial vehicle video stabilization, Aerospace and 
Electronics Conference (NAECON), pages 126-131, 2010. [2] Manish O and  Prabik. K. B, Improving video st abilization in the presence of motion blur, Computer Vision, Pattern Recognition, Image Processing and Graphics, pages 78-81, 2011. [3] Rawat P and  Singhai J, Review of Motion Estima tion and Video Stabilization techniques For hand held mobile video, Signal & Image Processing: An Intern ational Journal (SIPIJ) Vol.2, No.2, June 2011. [4] Morimoto C and Chellappa R,  Fast electronic digitl image stabilization , Conference on Pattern Recognition, vol. 3, pages .284-288,   1996. [5] Walha A, Wali A and Alimi  AM, Support Vector Ma chine Approach for Detecting Events in Video Streams, ” Advanced Machine Learning Technologies and Applications” 143-151, 2012 [6] Lu W, Hongying Z, Shiyi G, Ying M and Sijie L, The adaptive compensation algorithm for small UAV image stabilization, Geoscience and Remote Sens ing Symposium (IGARSS), pages 4391-4394, 2012. [7] Batur AU and Flinchbaugh B, Video Stabilisati on with Optimized Motion Estimation Resolution, International Conference on Image Processing, pages .465-468, 2006 [8] Jesse S J, Zhigang Z, Guangyou X, Digital Video Sequence Stabilization Based on 2.5D Motion Estimation and Inertial Motion Filtering, Real-Time Imaging, Volume 7, Issue 4, Pages 357–365, 2001. [9] Tico M and Vehvilainen M, Constraint Motion Filtering for Video Stabilisationsing, Proc. of International Conference on Image Processing, pages.569-572, 2005. [10] Erturk S. Image sequence stabilisation based on kalman filtering of frame positions. Electronics Letters, 37(20), 200. [12] Lowe D. Distinctive image features from scale-i nvariant keypoints. International Journal of Computer Vision, Vol. 60(2), pages 91–110, 2004. [13] Erturk S, Image sequence stabilisation: motion vector integration (MVI) versus frame position smoothing (FPS), Image and Signal Processing and Analysis, pages 266-271,  2001. [14] Junlan Y, Schonfeld D and Mohamed M, Robust Video Stabilization Based on Particle Filter Tracking of Projected Camera Motion, Circuits and Systems for Video Technology, IEEE Transactions on , vol.19, no.7, pages 945-954, 2009. [15] Wali A and  Alimi AM, Incremental learning approa ch for events detection from large video dataset, Advanced Video and Signal Based Surveillance (AVSS), pages 555 – 560,  2010. [16] Sangmin O, Anthony H, Amitha P, Naresh C, Chia-C hih C, Jong TL, Saurajit M, J. K. A, Hyungtae L, Larry D, Eran S, Xioyang W, Qiang J, Kishore R, Mubarak S, Carl V, Hamed P, Deva R, Jenny Y, Antonio T, Bi S, Anesco F, Amit RC and  Mita D. A large-scale be nchmark dataset for event recognition in surveillance video. Computer Vision and Pattern Recognition (CVPR), pages  527 – 528,  2011. [17] Hong S and Atkins E, Moving Sensor Video Image Processing Enhanced with Elimination of Ego Motion by Global Registration and SIFT,  Tools with  Artificial Intelligence, ICTAI '08, poges37-40, 2008. 