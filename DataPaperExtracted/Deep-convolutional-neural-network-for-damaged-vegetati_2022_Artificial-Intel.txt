Deep convolutional neural network for damaged vegetationsegmentation from RGB images based on virtual NIR-channel estimation
Artzai Picona,⁎, Arantza Bereciartua-Pereza, Itziar Eguskizab, Javier Romero-Rodriguezc, Carlos Javier Jimenez-Ruiz
c,T i l lE g g e r sd, Christian Klukasd, Ramon Navarra-Mestred
aTECNALIA, Basque Research and Technology Alliance (BRTA), Parque Tecnológico de Bizkaia, C/ Geldo. Edi ﬁcio 700, E-48160 Derio - Bizkaia, Spain
bUniversity of the Basque Country, Plaza Torres Quevedo, 48013 Bilbao, Spain
cBASF Espanola S.L. Carretera A376, 41710 Utrera Sevilla, Spain
dBASF SE, Speyererstrasse 2, 67117 Limburgerhof, Germany
abstract article info
Article history:Received 30 June 2022Received in revised form 12 September 2022Accepted 12 September 2022Available online 24 September 2022
Keywords:Vegetation indices estimationVegetation coverage mapNear infrared estimationConvolutional neural networkDeep learningPerforming accurate and automated semantic segmentation of vegetation is a ﬁrst algorithmic step towards more complex models that can extract accurate biological information on crop health, weed presence and phenologicalstate, among others. Traditionally, models based on normalized difference vegetation index (NDVI), near infraredchannel (NIR) or RGB have been a good indicator of vegetation presence. However, these methods are not suit-able for accurately segmenting vegetation showing damage, which precludes their use for downstream pheno-typing algorithms. In this paper, we propose a comprehensive method for robust vegetation segmentation inRGB images that can cope with damaged vegetation. The method consists of a ﬁrst regression convolutional neu- ral network to estimate a virtual NIR channel from an RGB image. Second, we compute two newly proposed veg-etation indices from this estimated virtual NIR: the infrared-dark channel subtraction (IDCS) and infrared-darkchannel ratio (IDCR) indices. Finally, both the RGB image and the estimated indices are fed into a semantic seg-mentation deep convolutional neural network to train a model to segment vegetation regardless of damage orcondition. The model was tested on 84 plots containing thirteen vegetation species showing different degreesof damage and acquired over 28 days. The results show that the best segmentation is obtained when the inputimage is augmented with the proposed virtual NIR channel (F1=0 :94) and with the proposed IDCR and IDCS veg- etation indices (F1=0:95) derived from the estimated NIR channel, while the use of only the image or RGB indi-ces lead to inferior performance (RGB(F1=0 :90) NIR(F1=0:82) or NDVI(F1=0:89) channel). The proposed method provides an end-to-end land cover map segmentation method directly from simple RGB images andhas been successfully validated in real ﬁeld conditions. © 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
1. IntroductionVegetation coverage map estimation is of great importance as a ﬁrst stage of more complex algorithms aiming to automatically assess cropstatus, measure the effect of nutrients, evaluate the stress situation in acrop or quantify the effect of existing agricultural practices ( Bendig et al., 2015; Picon et al., 2022a). Generating an accurate vegetation segmentationmap serves other algorithms and models to perform subsequent and pre-cise assessments over this segmented segmentation maps such as damageestimation (Picon et al., 2019a), weeds analysis (Picon et al., 2022a)o r presence of plagues (Bereciartua-Pérez et al., 2022)a m o n go t h e r s . Traditionally, vegetation index calculations ( Bannari et al., 1995) have been used for vegetation coverage estimation. Leaves presentArtiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
⁎Corresponding author.E-mail address:artzai.picon@tecnalia.com(A. Picon).
particularly low reﬂectance on the visible light (450–750 nm) range ex- cept for the fairly small window of the visible spectrum which is thegreen color, the signature reﬂectance of chlorophyll, around 550 nm.The rest of the visible wavelengths have minor representation. This en-couraged researchers to deﬁne vegetation indexes toﬁnd indicators by combining more spectral wavelengths than just the ones on the visiblerange. In their 2014 research reviewLi et al. (2014)claim that the leaf mesophyll -that we can imagine as the leaf ﬂeshy tissue- reﬂects low light in the visible spectrum, but has a major contribution to near-infrared (700–1200 nm). Moreover, they say that NIR radiation can pen-etrate the vegetal canopy from the upper leaves to the lower ones,which makes the actual structure determinant to the ﬁnal NIR reﬂec- tance. The canopy structure is composed of several factors such as leafthickness, overlapping, height and growth habit among others. That isthe main reason why NIR is considered to be best suited for estimatingvegetal biomass. In fact, many Vegetation Indices (VIs) involve the
https://doi.org/10.1016/j.aiia.2022.09.0042589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/combination of NIR reﬂectance with other light spectra and channels. Awidely-used channel for vegetation-soil discrimination is NormalizedDifference Vegetation Index -NVDI- (Rouse et al., 1974), which is a com- bination of NIR and the red -R- channel of the usual RGB color codi ﬁca- tion for visual cameras. These vegetation indexes not only serve to inferplant coverage map or biomass. Biomass-related Vegetation Indicessuch as green biomass reported byGitelson et al. (2003),c a np r o v i d ei n - formation about leaf cover, leaf area index, chlorophyll per ground areaand intercepted fraction of radiation with combinations of NIR and RGBchannels. Those combinations included the widely known NVDI but alsoSimple Ratio (SR), Red Edge (λ
RE), Photochemical reﬂectance index (PRI), Structural Independent Pigment Index (SIPI) and others. They no-ticed that by measuring such indices of water level, pigment, biomass,etc. they could infer if plants suffered disease, had a certain risk of ﬁre or salt-stress among other situations.Several works have tried to correlate the NDVI and other VegetationIndices (VIs) based on light channels with vegetation coverage estima-tion (Price, 1992; Huete et al., 1997; Wu et al., 2007; Zhengwei et al.,2009; Roth and Streit, 2018; Devia et al., 2019; Ren and Zhou, 2019 ). Most of them are based on acquiring experimental data on soil andleaf reﬂectance of different light channels and correlate those re ﬂec- tance with combinations of such channels (which are called VegetationIndices). They also try to correlate those Vegetation Indices with the ac-tual biomass measurements (fresh weight, dry weight) by means of lin-ear and non-linear regressive analysis. The aforementioned researchworks correlate the different VIs with the actual biomass weighting bymeans of pixel-wise linear or non-linear regression analysis.An additional problem is the necessity of the NIR image channel inaddition to the standard RGB channels to obtain these vegetation indi-ces. This presents two important drawbacks: First, high price and lowaccessibility of these cameras which are often accompanied by lack ofavailability of speciﬁc knowledge for the end-user. And second, the im-possibility of addressing speciﬁc use cases where standard, low-cost de-vices or light acquisition devices are required. This is of special relevanceon Unmanned Aerial Vehicles -UAVs- such as drones or cell-phonebased applications (Johannes et al., 2017) among others.However, these indices lack the capability of generating complexmodels (Hemming and Rath, 2001) as they are based on single pixel in-formation. Image processing based methods integrate spatial informa-tion over RGB images or vegetation index channels that can cope withcomplicated tasks such as specie identiﬁcation (Hemming and Rath, 2001), disease classiﬁcation (Johannes et al., 2017; Huddar et al.,2012), insect counting (Bereciartua-Pérez et al., 2022) among others. However, it is with the advent of deep learning techniques whenimage processing based models have become capable to perform com-plex image identiﬁcation tasks with equal or higher performance thanhumans (Picon et al., 2019a) in scenarios such as radiology (Yala et al., 2019), board games (Granter et al., 2017) among others. In precision ag- riculture, deep neural networks have being successful for pest classi ﬁca- tion (Picon et al., 2019a; Picon et al., 2019b; Argüeso et al., 2020 ), crop and weed segmentation (Milioto et al., 2018; Sa et al., 2018; Piconet al., 2022a), insect counting (Bereciartua-Pérez et al., 2022).The com- bination of vegetation indices with image processing and analysis algo-rithms have been successfully used for more complex applications suchas forest dynamics analysis (Sader and Winne, 1992),irrigated rice map- ping (Nguyen et al., 2012), environment quality analysis (Fung and Siu, 2000) or crop identiﬁcation (Jakubauskas et al., 2002). Although these RGB based models have demonstrated capable of performing vegetation
segmentation (Hassanein et al., 2018; Netto et al., 2018 ), they have not been tested to segment damaged vegetation.2. Related worksOne of the research lines to overcome the need for speci ﬁc multi- spectral acquisition devices is the generation of algorithms capable forvirtually estimating the near infrared channel from a RGB image. Inthis sense, several authors have worked on this ﬁeld for the last few years. Most of the authors only use pixel information and infer the NIRchannel based on the values of the Red, Green and Blue channels ofthat pixel (Rabatel et al., 2011a; Rabatel et al., 2011b ). In this sense, Arai et al. (2016)found a linear correlation between NIR and Greencolor channel, allowing them to estimate NIR re ﬂectance using a con- ventional RGB camera through a regression analysis. Furthermore theycalculated the NDVI index with images from a visible camera mountedon a drone using this method where other authors have analysed thehyperspectral endmembers to develop a pixel based method to esti-mate NIR images from RGB data (de Lima et al., 2019). These methods, although simple and fast, do not exploit spatial information containedon the pixel neighborhood. These intensity level relationships containinformation on visual shapes and textures ( Picón et al., 2009; Picon et al., 2011) that allows a more accurate estimation of the NIR channeland better performance of image processing methods.Some more modern research integrates the spatial information dis-tribution from the RGB pixels to infer the NIR channel. For example,Khan et al. (2018); Lima et al., 2019propose a method to estimate sev-eral vegetation indices by means of a neural network. However, theyuse a neural network that do not estimate the vegetation indexes atpixel level resolution, only the average vegetation index for the wholeinput tile with an average coefﬁcient of determination ofR2¼0:92. Re- cent methods have taken advantage of convolutional neural networksfor NIR channel estimation. For example, Aslahishahri et al. (2021) andde Lima et al. (2022)employed pix2pix (Isola et al., 2017)t oa c c u - rately estimate NIR channel from RGB images from UNet based genera-tors (Ronneberger et al., 2015). These pix2pix methods have beensuccessfully extended on the medical domain ( Picon et al., 2021; Picon et al., 2022b) by employing more efﬁcient loss functions and generatorarchitectures based on fully convolutional DenseNet ( Jégou et al., 2017) which is more parameter efﬁcient than traditional UNet.In this work, we propose and validate an end-to-end method for ro-bust damaged vegetation segmentation over RGB images without theneed for an infrared capable camera. This method can estimate a virtualNIR channel from an RGB image and use it to feed a vegetation segmen-tation neural network with an extended image obtaining better resultsthan RGB based segmentation models regardless its damage condition.This method provides the following contributions:•The deﬁnition of two new vegetation indices: Infrared-Dark-Channel-Substractive index (IDCS) and the Infrared-Dark-Channel Ratio index(IDCR) which are sensitive to vegetation coverage map estimation onsituations of plant damage presence.•A convolutional semantic regression network to estimate near infra-red channel from RGB image (RGB2NIR) that can optionally incorpo-rate an adversarial loss.•A vegetation biomass coverage estimation semantic segmentationnetwork that takes as input a multichannel image composed by theR, G, B channels and the (estimated) NIR, and vegetation indexes.•A end to end method that takes a RGB image, estimates the requiredindices and segments the vegetation coverage map of the image.3. MaterialsIn order to develop and validate the proposed models, thirteen veg-etal species were selected: three crops: GLXMA ( Glycine max), HELAN (Helianthus annuus), ZEAMX (Zea mays), seven broad leaf species: ABUTH (Abutilon theophrasti), AMARE (Amaranthus retroﬂexus), CHEAL (Chenopodium album), DATST (Datura stramonium), POLCO (Fallopia convolvulus), POROL (Portulaca oleracea), SOLNI (Solanum nigrum)a n d three grass species: DIGSA (Digitaria sanguinalis), ECHCG (Echinochloa crus-galli), SETVE (Setaria verticillata). 84 plots were planted combiningthe presence of the different species. 24 plots contained GLXMA, 24HELAN and 24 ZEAMX, whereas the weeds were randomly distributedA. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
200among the main plots. Each cropﬁeld (plot) followed a different weedcontrol treatment to generate damage on the different species.Each plot image was acquired with a Micasense RedEdge MX camerafrom a 2 meters height. This camera hasﬁve different sensors: blue(450 nm), green(560 nm), red(650 nm), red-edge(730 nm) and NIR(840nm) and delivered 1280x920px images. As each image band wastaken by a different monochrome sensor, acquired images were co-registered by applying the afﬁne transformation that minimized theMattes mutual information between channels ( Klein et al., 2009; Shamonin et al., 2014) following the same method we performed inPicon et al. (2022b). Images were taken at different days after cropseeding (DAS = 14, 16, 32, 35, 38, 42, 44, 49). About 5%of the images were not correctly co-registered due to the short acquisition distanceand were removed from the data set leading to 504 RGB-NIR registeredimages from the 84 plots. From these images, 358 images were ran-domly chosen, and the vegetation coverage was manually segmentedusing CVAT annotation tool.Fig. 1shows some of the acquired plots.To avoid bias, the distribution of images across the training, valida-tion and test datasets was selected by plots. This means that each cropﬁeld (plot) was assigned an identiﬁcation number and all images be-longing to the same cropﬁeld (plot) were assigned to the same subsetof data (train, validation, test). This ensures that images from thesame plot taken on consecutive days are assigned to the same set,avoiding contamination of the training, validation and test sets. Eightypercent of the cropﬁelds (plots) were randomly chosen for training,while the remaining 20%were distributed into validation and testsets, and all images were incorporated into the set determined bytheir cropﬁeld (plot) number resulting into 24 plots for training, 2 forvalidation and 3 for testing.4. Proposed methodIn our approach, we propose the use of a semantic regression neuralnetwork to estimate a virtual NIR channel from an RGB image. This vir-tual channel is then used to enrich the original RGB image to generate amulti-channel image that is used to train a multispectral vegetation seg-mentation convolutional neural network. This multichannel image in-cludes not only the original red, green and blue channels and theestimated virtual NIR channel but also different multi-spectral indicesderived from this four channels.The intuition behind this is based on the fact that NIR channel is agood estimator for vegetation which is relatively robust to vegetationdamage as it can be appreciated inFigs. 8 and 9where the damaged parts of the plant present similar NIR response. Complementing this vir-tual NIR information into the original RGB image might help on vegeta-tion segmentation.Proposed method is depicted inFig. 2. An RGB image passes through the RGB2NIR network, which is responsible for estimating the virtualnear-infrared channel of the image. This virtual channel is used, to-gether with the original RGB image to generate the additional imagechannels containing the vegetation indices (NDVI, IDCS and IDCR).These generated channels are aggregated in a multichannel image.This enhanced and more informative image feeds a semantic segmenta-tion neural network responsible for estimating the vegetation coveragemap of the image.Below, we present and detail the different modules for the end-to-end method for robust vegetation segmentation over RGB images.4.1. RGB2NIR: estimation of near infrared channel from RGB imagesTheﬁrst module of the proposed method (rgb2nir module on Fig. 2) is responsible for estimating NIR information from RGB images. To thisend, we employ a fully convolutional DenseNet architecture Jégou et al. (2017)similar to the one we used inPicon et al. (2021). This net- work combines the descriptive power from traditional segmentationnetworks based on fully convolutional versions such as SegNet(Badrinarayanan et al., 2017) where the accuracy for the border detec-tion is provided by the skip connections on the U-Net segmentation net-work (
Ronneberger et al., 2015).Concretely, the proposed fully convolutional DenseNet network(Jégou et al., 2017) was set to an input size of 224x224 pixels. Architec-ture follows original paper implementation where the number of initialconvolutionﬁlter was set to 48. The encoder was composed by 5 transi-tion down blocks (TD) with 4 convolution layers each with a growingrate of 16. For the decoder part, we use 5 transition up (TU) layerseach of them is linked with their corresponding encoder block. This al-lows recovering the input image high level details as these skip connec-tions transfer the low level features and spatial information from thesource domain into the detailed reconstruction of the target domain.
Fig. 1.Examples of the generated images: a) RGB Image, b) red-edge image, c) near-infrared image, d) Ground-truth of plant coverage.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
201The last layer of this network has been substituted by a sigmoid ac-tivation function and the loss function has been replaced by a mean ab-solute error loss in order to learn a pixel-wise regression transformationthat translates the image from the source to the target domain. The lastlayer consists of a 224x224x1 that performs NIR reconstruction. Thenetwork is trained by minimizing a mean absolute error loss functionwhich can be enhanced by an adversarial perceptual loss function fol-lowing a pix2pix architecture (Isola et al., 2017). The addition of the per- ceptual adversarial loss functions ensures that the generated image isvisually plausible and the estimated image cannot be distinguishedfrom a real image by a speciﬁcally trained network. Similar approachhas been followed byAslahishahri et al. (2021); de Lima et al., 2022 to include a perceptual adversarial loss. However, they use UNet architec-ture as generator which increases the number of trainable parameters.Proposed fully convolutional DenseNet network has 2,3 M parameterswhereas its UNet counterpart has 24 M instead.4.2. Vegetation indices for vegetation estimationExisting vegetation indices for vegetation segmentation fails underthe presence of plant damage or direct illumination. This is caused bythe fact that the presence of damage on the plant normally increasesthe reﬂectance on particular visible wavelengths that makes indicessuch as NDVI or negative CIE-a (Johannes et al., 2017) channels to fail on appropriately segmenting non –healthy vegetation. Additionally,changes on illumination intensity reduce the robustness for other bio-markers such as NIR channel due to intensity scale changes betweenthe dark and the illuminated areas of the image. In order to overcomethis issue, we propose two new indices that are vaguely inspired onthe Dark Channel Prior (He et al., 2010; Galdran et al., 2015) method. This method is used to estimate haze on the images for image restora-tion purposes. It estimates the haze level by considering the minimumvalue of the R, G and B channels taking advantage of the whitish halocreated by fog on a image. Dark channel is calculated as the minimumof the red, green and blue channels on the spatial neighborhood ofeach pixel. However, for our approach, we will just consider the mini-mum value for the R, G and B channels for each pixel without consider-ing their neighborhood (Section4.3). DCðR,G,BÞ¼minðR,G,BÞ:ð1ÞIn this work, we adapt this formulation to propose two new vegeta-tion indices: the Infrared-Dark-Channel Substractive (IDCS) (Eq. (3))index, that reﬂects the relative intensity of the NIR channel bysubstracting the dark channel and the Infrared-Dark-Channel Ratioindex (IDCR) (Eq. (2)), that reﬂects the intensity ratio of the NIR channelagainst the dark channel. this formulation can be extended for multi-spectral or hyperspectral images just getting the minimum for all thespectral range or for speciﬁc spectral ranges.IDCS¼NIR/C0minðR,G,B,NIRÞ:ð
2ÞIDCR¼NIR=ðminðR,G,B,NIRÞþ∊Þ:ð3ÞIf we analyseFig. 3, the proposed indices correlate the presence ofvegetation on a plot image and are more robust to the existence of dam-age on the vegetation.Fig. 3shows an example of a RGB image of a plotand its corresponding NIR channels and NDVI values together with theproposed IDCS and IDCR indices. It can be appreciated that the proposedIDCS and IDCR vegetation indices can separate better between vegeta-tion and non-vegetation pixels even for the unhealthy part of the plants.In order to get quantitative metrics for the vegetation-soil separation ca-pabilities of the proposed indices,proposed indices were comparedagainst other indices and color channels: r, g and b channels, CIE-L,CIE-a and CIE-b color channels from CIELab Zhang and Wandell (1997), NDVIRouse et al. (1974)and NIR channel. On one hand, we cal-culate the intersection of the probability density function ( Lee et al., 2005) from the indices values between the vegetation and the non-vegetation classes. This metric measures the existing overlap betweenthe distribution of the intensity values for the two classes (soil and veg-etation), showing a intersection of 0 :0 a perfect separability among theclasses whereas a value of 1 :0 indicates a full overlap among classes. Onthe other hand, we also measure the Area Under the Curve Metric(Fawcett, 2006) of a hypothetical Naïve Bayes classiﬁer applied over the vegetation indices for the different classes. This curve measuresthe true positives rate against the false positives rate. An ideal classi ﬁer will present a ROC value of 1.0 whereas a random classi ﬁer will arise to a ROC value of 0.5.Table 1shows the average results and standard devia-tion obtained with each vegetation index. It can be appreciated that NIR,NDVI, and CIE-a channels are appropiate vegetation indices for vegeta-tion estimation with AuC values of 0.916, 0.989 and 0.937 respectively.
Fig. 2.Coverage estimation diagram Infrared-Dark-Channel Ratio index model diagram: An RGB image goes through the RGB2NIR transformation network and a vi rtual NIR channel is calculated. NDVI, Infrared-Dark-Channel Substractive (IDCS), Infrared-Dark-Channel Ratio index (IDCR) channels are estimated by their corresp onding formula from the R,G,B and esti- mated NIR channels. All these channels are used to generate a multi-channel image that is fed into a vegetation segmentation convolutional neural net work (see Section4.3).A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
202However, best results are obtained by the proposed IDCS and IDCR esti-mated channels that obtain an AuC value of 0.998 and 0.997 with an his-togram intersection value of 0.166 and 0.198. This can be explained asthis vegetation index is more robust to the damage of the plants andto non–homogeneous image illumination.Figs. 4 and 5depict the prob- ability density distribution of the intensity values of the pixels contain-ing vegetation against the pixels that contains other elements for agiven image.4.3. Vegetation segmentation convolutional neural networkA second network with similar structure as the one de ﬁned in Section4.1is used for plant coverage estimation. In this case, theinput layer of the network size is MxNxK, where M and N representsthe height and the width of the input image and K the number of chan-nels used. In our case the number of input channels is K = 7. Thesechannels are composed by the R, G, B channels of the original imageand all the estimated channels by the previous methods (NIR, NDVI,IDCS and IDCR). Theﬁnal layer of this network is composed by two out-put channels of size M and N (MxNx2) resembling the original size ofthe image. one of the output channels maps the estimation for theplant coverage segmentation meanwhile the other contains the otherclasses. A softmax activation layer that ensures the mutually exclusive-ness of the two classes. This network is minimized over a categoricalcross-entropy loss function.5. Results5.1. NIR channel estimation from RGB imageThe two models for the estimation of the near infrared channel fromRGB images deﬁned in Section4.1were trained over the training set ofdescribed in Section3for 100 epochs on the mae loss approach and 40epochs for the pix2pix approach. Training was performed over 224x224pixel tiles that were extracted randomly from the full-size images. Assize of the image is 1280x920 pixels, the tiling process ensure an equiv-alent number of approximately 12096 tiles for training considering notile overlap. In order to generate more variability on the input dataset,several augmentation techniques were performed on the images suchas shifting, rotating and scaling. To simulate light changing conditions,RGB and NIR intensity channels are multiplied by the random constantfactor simulating changes on light intensity coherent with dichromaticreﬂection model (Tominaga, 2020). These tiles are fed into the neuralnetwork as described in Section4.1. Adam optimiser was employedfor training and the learning rate was set to 10
/C05. A reduction of the learning rate is performed when the loss function value on the valida-tion set raises or stagnates.Table 2shows inference results for the proposed models. BothPearson’sc o e fﬁcient and the mean absolute error are shown as perfor-mance metrics. We have compared the proposed DenseNet basedmodel withAslahishahri et al. (2021)based UNet model. We can appre-ciate that metrics are slightly better when using the proposed DenseNetarchitecture rather than the UNet architecture. This might be caused bythe better parameter efﬁciency of the DenseNet model. Loss evolutionand regression grapsh are depicted for the winning model in Figs. 7 and 6respectively. It is noteworthy to remark that, in the pix2pix re-lated model, only mean average error loss is depicted as the adversarialloss is based on competition among the discriminator and adversarialloss part in the generator. Obtained regression results ( Fig. 6)s h o w the correlation graph between the real NIR values and the estimatedvalues.The use of fully convolutional DenseNet provides better reconstruc-tion performance than using UNet architecture. It can be also appreci-ated that the use of a pix2pix (Isola et al., 2017) based adversarial loss contributes not only to generate more plausible images but also to re-duce the error between the predictions and the real NIR images thatpresent a lower error rate (5%) and better correlation coef ﬁcient (r = 0.96). his endorses the pix2pix based design approach ( Aslahishahri et al., 2021; de Lima et al., 2022).Figs. 8 and 9show examples of the es- timation of image tiles for both conﬁgurations. It can be appreciated thatthe NIR estimation is accurate, even for damaged part of the plants,which consist of whitish necrosis spots along the image.5.2. Vegetation coverage map estimationA semantic segmentation network as deﬁned in Section4.3is used to estimate the plant coverage map. For that, the network was trained overthe training set of described in Section 3.T r a i n i n gw a sp e r f o r m e do v e r 224x224 pixel tiles that were extracted randomly from the full-size im-ages during 30 epochs. The tiles are fed into the near infrared estimation
Table 1AuC and histogram intersection values obtained by the different im-age vegetation indices or channels for each image of the entiredataset.Image Channel AuC (average) /C6std Intersection/C6stdr0 :872/C60:085 0:463/C60:132 g0 :666/C60:108 0:551/C60:230 b0 :736/C60:110 0:601/C60:117 CIE-L 0 :678/C60:113 0:586/C60:172 CIE-a 0 :937/C60:049 0:257/C60:173 CIE-b 0 :740/C60:089 0:298/C60:213 NIR 0 :916/C60:071 0:551/C60:189 NDVI 0 :989
/C60:008 0:261/C60:081 IDCS 0 :998/C60:002 0:166/C60:126 IDCR 0 :997/C60:004 0:198/C60:120
Fig. 3.Plot image: a) RGB Image, b) NIR image, c) NDVI, d) Proposed IDCS, e) Proposed IDCR. Bottom close-up of an unhealthy leaf.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
203Fig. 4.Top) Field images representing the target vegeta
tion index, bottom) Probability dis
tribution of the pixels containi
ng vegetation and not vegetati
on for their respective vegetation index for their corresponding image.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
204Fig. 5.Top) Images representing the target vegetation i
ndex, bottom) Probability distribution of the pixels containing vegetation and not vegetation for
their respective vegetation index.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
205neural network to get the estimated NIR channel. Additional vegetationindices (NDVI, IDCS and IDCR) are calculated from the R, G, B and the es-timated NIR channels. Training is performed over the training set of thedataset described in3. After training, the validation subset of the datasetwas used to calculate the optimal thresholds values that maximized thebalanced accuracy (BAC). These thresholds were applied over the test-ing set. In order to measure the effect of using estimated NIR channelsinstead the real ones, speciﬁc experiments using real NIR channelhave been performed.We performed an ablation study showing different channel combi-nations for the multi-spectral input image that feeds the vegetation seg-mentation neural network. Results of the vegetation coverage mapestimation over the testing set are depicted on Table 3. Performance of the different algorithms is analysed based on twocommon metrics for semantic segmentation: 1) The balanced accuracy(BAC), which represents the average value between true positive rateand true negative rate (Eq. (4)), and 2) the F-Score (Eq. (7)) that returnsthe geometric average among the precision (Eq. (6)) and recall (Eq. (5))Table 2Performance of the two NIR estimation models given by their Pearson coef ﬁcient p and the mean absolute error (MAE).Algorithm name Network Loss r MAERGB2NIR (ours) Fully Convolutional DenseNet mean average error (MAE) 0.93 0.05RGB2NIR pix2pix (ours) Fully Convolutional DenseNet MAE with adversarial loss (pix2pix) 0.96 0.04RGB2NIR UNet Aslahishahri et al. (2021) mean average error (MAE) 0.91 0.06 RGB2NIR pix2pix UNet Aslahishahri et al. (2021) MAE with adversarial loss (pix2pix) 0.94 0.05
Fig. 6.Regression graphs between real and predicted NIR values for the proposed model. Left) Mean Average Error loss, Right) Mean Average Error + Adversaria ll o s s .
Fig. 7.Evolution of Mean Average Error (MAE) and validation Mean Average Error for the proposed model. Left) Mean Average Error trained model, Right) Mean Av erage Error (MAE) + Adversarial loss trained model. In adversarial model only MAE loss is plotted.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
206of the model. The precision is calculated as the number of true vegeta-tion pixels divided by the number of all predicted vegetation pixelswhereas the recall is the number of true positive vegetation pixels di-vided by the number of all true vegetation pixels. For unbalanceddatasets in semantic segmentation, F-Score is normally preferred toBAC as it ignores the effect of the true negative class (non vegetationpixels). TP, TN, FN and FP refers to true positives, true negatives, falsenegatives and false positives respectively.BAC¼ ðððTP=ðTPþFNÞþðTN=ðTNþFPÞÞÞ=2:ð4ÞRecall¼TP=ðTPþFNÞð 5ÞPrecision¼TP=ðTPþFPÞð 6ÞF
1¼2∗ðPrecision∗RecallÞ=ðPrecisionþRecallÞð7Þ
Fig. 8.Examples of NIR channel prediction with the RGB2NIR algorithm. NIR reconstruction can be appreciated for the healthy and unhealthy (white necrosis s pots) of the plant.
Fig. 9.Examples of NIR channel prediction with the RGB2NIR pix2pix algorithm. NIR reconstruction can be appreciated for the healthy and unhealthy (white ne crosis spots) of the plant.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
207Table 3Results for the different algorithm combination for vegetation coverage map estimation.Image channels # Channels AuC BAC Sensitivity Speci ﬁcity NPV PPV F1NIR (Real) 1 0.981 0.937 0.916 0.925 0.979 0.769 0.836NIR 1 0.977 0.924 0.926 0.922 0.981 0.741 0.823NDVI 1 0.991 0.958 0.967 0.950 0.992 0.823 0.889IDCS 1 0.996 0.974 0.982 0.966 0.995 0.874 0.925IDCR 1 0.992 0.964 0.972 0.956 0.993 0.841 0.902RGB 3 0.992 0.962 0.967 0.958 0.992 0.848 0.904RGB + NIR (Real) 4 0.995 0.976 0.978 0.979 0.997 0.903 0.939RGB + NIR 4 0.997 0.978 0.982 0.974 0.996 0.901 0.940RGB + NDVI 4 0.997 0.977 0.980 0.973 0.995 0.900 0.939RGB + IDCS 4 0.995 0.967 0.964 0.969 0.992 0.878 0.919RGB + IDCR 4 0.9980.981 0.986 0.974 0.9970.903 0.943RGB + NIR + NDVI + IDCS + IDCR 7 0.998 0.980 0.980 0.996 0.914 0.913 0.946RGB + NIR + IDCS + IDCR 6 0.998 0.984 0.988 0.980 0.997 0.919 0.952
Fig. 10.Thisﬁgure shows examples of coverage map estimation for the different combinations. a) Original image, b) RGB, c) NIR, d) NDVI, e) IDCS, f) IDCR, g) GroundTr uth.
Fig. 11.Thisﬁgure shows examples of coverage map estimation for the different combinations. a) Original image, b) RGB + NIR, c) RGB + NDVI, d) RGB + IDCS, e) RGB + IDCR,f) GroundTruth.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
208As we can appreciate inTable 3, the use of the RGB image alone pro-duces a F-score of 0:904. When using just one channel/index to generatevegetation map, we can appreciate that the use of already existing veg-etation indices such as NDVI and NIR offer a reduced performance (F-score of 0:823 and 0:889 respectively). However, using just one of theproposed indices for vegetation estimation raises better results IDCS(F-Score=0:925), IDCR (F-Score=0:902). If we analyse the effect ofusing real NIR channels or estimated ones, we can appreciate that it isalmost equivalent to use estimated or real NIR channels. Fig. 10shows examples of segmentedﬁelds under the different combinations.Fig. 11shows segmentation results for the combination of RGB withone of the vegetation indices. Results on Table 2show that the combina- tion of RGB channels with the proposed indices deal to better resultsthan the RGB baseline. The combination of RGB with the proposedIDCR index obtains the best F-Score with a value of 0 :943. However, it is with the combination with several of the proposed vegetation indiceswhen better results are achieved. The best results (see Fig. 12)a r eo b - tained with the combination of RGB + NIR + IDCS + IDCR achievingaB A C = 0:984 and a F-score=0:952.6. ConclusionsIn this work, we have presented an end-to-end method for robustvegetation segmentation over RGB images which is able to appropri-ately segment vegetation even when vegetation presents damaged con-ditions without the need of an infrared capable camera.We have proposed a convolutional semantic regression network toestimate a virtual near infrared channel from an RGB image (RGB2NIR)that can optionally incorporate an adversarial loss. With this adversarialloss, the proposed network can accurately estimate the NIR channel(p_value = 0.96, RMS=4%). This demonstrates that the adversarialloss contributes to generate more efﬁcient estimation for the NIR chan-nel than just employing a convolutional semantic regression network.We have introduced two novel vegetation indices such as Infrared-Dark-Channel-Substractive index (IDCS) and the Infrared-Dark-Channel Ratio index (IDCR). We have proven that these indices havegood separability properties to differentiate vegetation regardless itsdamage. These vegetation indices can be used independently for vege-tation segmentation purposes independently.We have generated a vegetation segmentation network to segmentdamaged vegetation. When feeding the shown model using only RGBimage achieves a F1-score of 0 :90. This segmentation performance in-creases when the RGB image is extended with the proposed virtualNIR channel (F1=0:94) or the with proposed vegetation indices (F1=0:95) that are derived from the estimated NIR channel.The proposed NIR estimation method could be adapted in the futureto be applied not only for vegetation coverage estimation but to otheragricultural use cases where NIR information might be relevant. Thiswill reduce the need for for without the need for expensive IR camerasthat could extend the application range for these methods.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Arai, K., Gondoh, K., Shigetomi, O., Miura, Y., 2016. Method for nir reﬂectance estimation with visible camera data based on regression for ndvi estimation and its applicationfor insect damage detection of rice paddy ﬁelds. Int. J. Adv. Res. Artif. Intell. 5, 17 –22. Argüeso, D., Picon, A., Irusta, U., Medela, A., San-Emeterio, M.G., Bereciartua, A., Alvarez-Gila, A., 2020.Few-shot learning approach for plant disease classi ﬁcation using im- ages taken in theﬁeld. Comput. Electron. Agric. 175, 105542.Aslahishahri, M., Stanley, K.G., Duddu, H., Shirtliffe, S., Vail, S., Bett, K., Pozniak, C.,Stavness, I., 2021. From rgb to nir: predicting of near infrared re ﬂectance from visible
Fig. 12.Thisﬁgure shows examples of coverage map estimation for the different combinations. a) Original image, b) RGB + NDVI + NIR + IDCS + IDCR, c) RGB + NIR + IDCS + IDC R, d) GroundTruth.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
209spectrum aerial images of crops. In: Proceedings of the IEEE/CVF International Con-ference on Computer Vision, pp. 1312 –1322. Badrinarayanan, V., Kendall, A., Cipolla, R., 2017. Segnet: a deep convolutional encoder- decoder architecture for image segmentation. IEEE Trans. Pattern Anal. Mach. Intell.39, 2481–2495.Bannari, A., Morin, D., Bonn, F., Huete, A., 1995. A review of vegetation indices. Remote Sens. Rev. 13, 95–120.Bendig, J., Yu, K., Aasen, H., Bolten, A., Bennertz, S., Broscheit, J., Gnyp, M.L., Bareth, G.,2015.Combining uav-based plant height from crop surface models, visible, andnear infrared vegetation indices for biomass monitoring in barley. Int. J. Appl. EarthObs. Geoinf. 39, 79–87.Bereciartua-Pérez, A., Gómez, L., Picón, A., Navarra-Mestre, R., Klukas, C., Eggers, T., 2022.Insect counting through deep learning-based density maps estimation. Comput. Elec-tron. Agric. 197, 106933.Devia, C.A., Rojas, J.P., Petro, E., Martinez, C., Mondragon, I.F., Patino, D., Rebolledo, M.C.,Colorado, J., 2019.High-throughput biomass estimation in rice crops using uav mul-tispectral imagery. J. Intell. Robot. Syst. 96, 573 –589. Fawcett, T., 2006.An introduction to roc analysis. Pattern Recogn. Lett. 27, 861 –874. Fung, T., Siu, W., 2000.Environmental quality and its changes, an analysis using ndvi. Int.J. Remote Sens. 21, 1011–1024.Galdran, A., Pardo, D., Picón, A., Alvarez-Gila, A., 2015. Automatic red-channel underwater image restoration. J. Vis. Commun. Image Represent. 26, 132 –145. Gitelson, A.A., Viña, A., Arkebauer, T.J., Rundquist, D.C., Keydan, G., Leavitt, B., 2003. Re- mote estimation of leaf area index and green leaf biomass in maize canopies.Geophys. Res. Lett. 30.Granter, S.R., Beck, A.H., Papke Jr, D.J., 2017. Alphago, deep learning, and the future of the human microscopist. Arch. Pathol. Lab. Med. 141, 619 –621. Hassanein, M., Lari, Z., El-Sheimy, N., 2018. A new vegetation segmentation approach for croppedﬁelds based on threshold detection from hue histograms. Sensors 18, 1253.He, K., Sun, J., Tang, X., 2010.Single image haze removal using dark channel prior. IEEETrans. Pattern Anal. Mach. Intell. 33, 2341 –2353. Hemming, J., Rath, T., 2001.Pa—precision agriculture: computer-vision-based weed iden-tiﬁcation underﬁeld conditions using controlled lighting. J. Agric. Eng. Res. 78,233–243.Huddar, S.R., Gowri, S., Keerthana, K., Vasanthi, S., Rupanagudi, S.R., 2012. Novel algorithm for segmentation and automatic identi ﬁcation of pests on plants using image pro- cessing. 2012 Third International Conference on Computing, Communication andNetworking Technologies (ICCCNT ’12). IEEE, pp. 1–5. Huete, A., Liu, H., Batchily, K., Van Leeuwen, W., 1997. A comparison of vegetation indices over a global set of tm images for eos-modis. Remote Sens. Environ. 59, 440 –451. Isola, P., Zhu, J.-Y., Zhou, T., Efros, A.A., 2017. Image-to-image translation with conditionaladversarial networks. In: Proceedings of the IEEE conference on computer vision andpattern recognition, pp. 1125 –1134. Jakubauskas, M.E., Legates, D.R., Kastens, J.H., 2002. Crop identiﬁcation using harmonic analysis of time-series avhrr ndvi data. Comput. Electron. Agric. 37, 127 –139.
Jégou, S., Drozdzal, M., Vazquez, D., Romero, A., Bengio, Y., 2017. The one hundred layerstiramisu: fully convolutional densenets for semantic segmentation. In: Proceedings ofthe IEEE Conference on Computer Vision and Pattern Recognition Workshops, pp.11–19.Johannes, A., Picon, A., Alvarez-Gila, A., Echazarra, J., Rodriguez-Vaamonde, S., Navajas,A.D., Ortiz-Barredo, A., 2017.Automatic plant disease diagnosis using mobile capturedevices, applied on a wheat use case. Comput. Electron. Agric. 138, 200 –209. Khan, Z., Rahimi-Eichi, V., Haefele, S., Garnett, T., Miklavcic, S.J., 2018. Estimation of vege- tation indices for high-throughput phenotyping of wheat using aerial imaging. PlantMethods 14, 20.Klein, S., Staring, M., Murphy, K., Viergever, M.A., Pluim, J.P., 2009. Elastix: a toolbox for intensity-based medical image registration. IEEE Trans. Med. Imaging 29, 196 –205. Lee, S., Xin, J., Westland, S., 2005. Evaluation of image similarity by histogram intersectionvol. 30. Color Research & Application: Endorsed by Inter-Society ColorCouncil, The Colour Group (Great Britain), Canadian Society for Color, Color ScienceAssociation of Japan, Dutch Society for the Study of Color, The Swedish Colour CentreFoundation, Colour Society of Australia, Centre Français de la Couleur pp. 265 –274. Li, L., Zhang, Q., Huang, D., 2014. A review of imaging techniques for plant phenotyping.Sensors 14, 20078–20111.de Lima, D.C., Saqui, D., Ataky, S., Jorge, L.A.d.C., Ferreira, E.J., Saito, J.H., 2019. Estimating agriculture nir images from aerial rgb data. International Conference on Computa-tional Science. Springer, pp. 562 –574. de Lima, D.C., Saqui, D., Mpinda, S.A.T., Saito, J.H., 2022. Pix2pix network to estimate agri- cultural near infrared images from rgb data. Can. J. Remote Sens. 48, 299 –315. Lima, D.C.d., Saqui, D., Ataky, S., Jorge, L.A.d.C., Ferreira, E.J., Saito, J.H., 2019. Estimating ag- riculture nir images from aerial rgb data. International Conference on ComputationalScience. Springer, pp. 562 –574. Milioto, A., Lottes, P., Stachniss, C., 2018. Real-time semantic segmentation of crop and weed for precision agriculture robots leveraging background knowledge in cnns.2018 IEEE International Conference on Robotics and Automation (ICRA). IEEE,pp. 2229–2235.Netto, A.F.A., Martins, R.N., de Souza, G.S.A., de Moura Araújo, G., de Almeida, S.L.H.,Capelini, V.A., 2018.Segmentation of rgb images using different vegetation indicesand thresholding methods. Nativa 6, 389 –394. Nguyen, T.T.H., De Bie, C., Ali, A., Smaling, E., Chu, T.H., 2012. Mapping the irrigated rice cropping patterns of the mekong delta, vietnam, through hyper-temporal spot ndviimage analysis. Int. J. Remote Sens. 33, 415 –434. Picon, A., Alvarez-Gila, A., Seitz, M., Ortiz-Barredo, A., Echazarra, J., Johannes, A., 2019a.Deep convolutional neural networks for mobile capture device-based crop diseaseclassiﬁcation in the wild. Comput. Electron. Agric. 161, 280 –290. Picon, A., Ghita, O., Rodriguez-Vaamonde, S., Iriondo, P.M., Whelan, P.F., 2011.Biologically-inspired data decorrelation for hyper-spectral imaging. EURASIP J. Adv.Signal Process. 2011, 1–10.Picón, A., Ghita, O., Whelan, P.F., Iriondo, P.M., 2009. Fuzzy spectral and spatial feature in- tegration for classiﬁcation of nonferrous materials in hyperspectral data. IEEE Trans.Industr. Inf. 5, 483–494.Picon, A., Medela, A., Sánchez-Peralta, L.F., Cicchi, R., Bilbao, R., Al ﬁeri, D., Elola, A., Glover, B., Saratxaga, C.L., 2021.Autoﬂuorescence image reconstruction and virtual staining for in-vivo optical biopsying. IEEE Access 9, 32081 –32093. Picon, A., San-Emeterio, M.G., Bereciartua-Perez, A., Klukas, C., Eggers, T., Navarra-Mestre,R., 2022a.Deep learning-based segmentation of multiple species of weeds and corncrop using synthetic and real image datasets. Comput. Electron. Agric. 194, 106719.Picon, A., Seitz, M., Alvarez-Gila, A., Mohnke, P., Ortiz-Barredo, A., Echazarra, J., 2019b.Crop conditional convolutional neural networks for massive multi-crop plant diseaseclassiﬁcation over cell phone acquired images taken on real ﬁeld conditions. Comput. Electron. Agric. 167, 105093.Picon, A., Terradillos, E., Sánchez-Peralta, L.F., Mattana, S., Cicchi, R., Blover, B.J., Arbide, N.,Velasco, J., Etzezarraga, M.C., Pavone, F.S., et al., 2022b. Novel pixelwise co-registered hematoxylin-eosin and multiphoton microscopy image dataset for human colon le-sion diagnosis. J. Pathol. Inform. 13, 100012.Price, J.C., 1992.Estimating vegetation amount from visible and near infrared re ﬂectances. Remote Sens. Environ. 41, 29 –34. Rabatel, G., Gorretta, N., Labbé, S., 2011a. Getting ndvi spectral bands from a single stan- dard rgb digital camera: a methodological approach. Conference of the Spanish Asso-ciation for Artiﬁcial Intelligence. Springer, pp. 333 –342. Rabatel, G., Gorretta, N., Labbé, S., 2011b. Getting ndvi spectral bands from a single stan- dard rgb digital camera: a methodological approach. Conference of the Spanish Asso-ciation for Artiﬁcial Intelligence. Springer, pp. 333 –342. Ren, H., Zhou, G., 2019.Estimating green biomass ratio with remote sensing in arid grass-lands. Ecol. Ind. 98, 568–574.Ronneberger, O., Fischer, P., Brox, T., 2015. U-net: convolutional networks for biomedical image segmentation. International Conference on Medical image computing andcomputer-assisted intervention. Springer, pp. 234 –241. Roth, L., Streit, B., 2018.Predicting cover crop biomass by lightweight uas-based rgb andnir photography: an applied photogrammetric approach. Precision Agric. 19, 93 –114. Rouse, J., Haas, R., Schell, J., Deering, D., 1974. Monitoring vegetation systems in the great plains with ertsvol. 351. NASA special publication pp. 309.Sa, I., Popović, M., Khanna, R., Chen, Z., Lottes, P., Liebisch, F., Nieto, J., Stachniss, C., Walter,A., Siegwart, R., 2018.Weedmap: a large-scale semantic weed mapping frameworkusing aerial multispectral imaging and deep neural network for precision farming.Remote Sens. 10, 1423.Sader, S., Winne, J., 1992.Rgb-ndvi colour composites for visualizing forest change dy-namics. Int. J. Remote Sens. 13, 3055 –3067. Shamonin, D.P., Bron, E.E., Lelieveldt, B.P., Smits, M., Klein, S., Staring, M., 2014. Fast paral- lel image registration on cpu and gpu for diagnostic classi ﬁcation of alzheimer’sd i s - ease. Front. Neuroinform. 7, 50.Tominaga, S., 2020. Dichromatic re ﬂection model. Computer Vision: A Reference Guide, pp. 1–3.Wu, J., Wang, D., Bauer, M.E., 2007. Assessing broadband vegetation indices and quickbird data in estimating leaf area index of corn and potato canopies. Field Crops Res. 102,33–42.Yala, A., Lehman, C., Schuster, T., Portnoi, T., Barzilay, R., 2019. A deep learning mammography-based model for improved breast cancer risk prediction. Radiology292, 60–66.Zhang, X., Wandell, B.A., 1997. A spatial extension of cielab for digital color-image repro-duction. J. Soc. Inform. Display 5, 61 –63. Zhengwei, Y., Hu, Z., Liping, D., Yu, G., 2009. A comparison of vegetation indices for corn and soybean vegetation condition monitoring. Geoscience and remote sensing sym-posium. IGARSS.A. Picon, A. Bereciartua-Perez, I. Eguskiza et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 199 –210
210