Data-driven approaches for time series prediction of daily production in theSulige tight gasﬁeld, China
Qi Zhanga, Ziwei Chenb, Yuan Zenga, Hang Gaoa, Qiansheng Weia, Tiaoyu Luob, Zhiguo Wang
b,*
aThe Third Gas Production Plant of PetroChina Changqing Oil ﬁeld Branch, Wushenqi, Inner Mongolia, 017000, PR China
bSchool of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, PR China
ARTICLE INFO
Keywords:Prediction of production time seriesLong short-term memory neural networkRandom forestSupport vector machineABSTRACT
The Sulige tight gasﬁeld is presently the largest gasﬁeld in China. Owing to the ultralow permeability and strong heterogeneity of the reservoirs in Sulige, the number of production wells has exceeded 3,000, keeping the stablegas supply in the decade. Thus, the daily production prediction of gas wells is signi ﬁcant for monitoring pro- duction and for implementing and evaluating stimulation measures. Therefore, on the basis of the three data-driven time series approaches, the daily production of 1692 wells over 10 years was mining for the daily pro-duction prediction of wells in Sulige. The jointed deep long short-term memory and fully connected neuralnetwork (DLSTM-FNN) model was proposed by introducing the recurrent neural network's sequential expressionability and was compared with random forest (RF) and support vector regression (SVR). After the daily productionpredictions of thousands of wells in Sulige, the proposed DLSTM-FNN model signi ﬁcantly improved the time series prediction accuracy and efﬁciency in the short training samples and had strong availability and practica- bility in the Sulige tight gasﬁeld.
1. IntroductionThe Sulige gasﬁeld is located northwest of the Ordos Basin, China.Since 2011, it has become China's largest gas ﬁeld (H. Yang et al., 2012). Owing to sedimentation, diagenesis, and accumulation processes ( T. Yang et al., 2012;Zou et al., 2012;Jiang et al., 2007), the Sulige gasﬁeld reservoirs show ultralow permeability and strong heterogeneity, result-ing in the low production of wells. Hence, in the last decade, the numberof wells in the Sulige gasﬁeld had to be increased rapidly for thecontinuous expansion of productivity. Subsequently,liquid-accumulation, intermittent, and low production wells are gradu-ally increasing, making the production management of the entire gasﬁeld more complicated. Therefore, it is crucial to explore the productionrules of various gas wells in different stages, especially predicting pro-duction dynamics and monitoring production anomalies for the wholelife cycle management of gas wells, which helps in maintaining long-termstable production and supply of natural gas.The commonly used production prediction methods of gas wells canbe divided into numerical simulation based on the physical model andproduction decline curve analysis based on the statistical law.Reservoir numerical simulation utilizes computer models to estimatetheﬂuids dynamics in porous media. The numerical simulation requires alarge amount of static geological and dynamic production data to achieveaccurate geological modeling and high-quality history matching. There-fore, the dynamic simulation takes a lot of time, and there are deviationsbetween the predicted results and the actual gas ﬁeld production results. In reservoir engineering, many decline curve analysis models havebeen proposed to predict oil and gas production. As early as 1945, Arpsproposed a classical exponential decline model, hyperbolic declinemodel, and harmonic decline model ( Arps, 1945). Chen et al. proposed a generalized decline model, linear decline model, and pan-exponentialdecline model in China (Chen and Tang, 2016;Chen and Zhou, 2015; Chen and Fu, 2019). However, it is still challenging to select the pa-rameters for tight sandstone gas in the abovementioned decline models,thus limiting its application in production dynamic prediction.Recently, on the basis of the available production data, various ma-chine learning methods, especially neural networks, have been success-fully used to select the featured parameters that affect well productionand then predict oil and gas well production under a supervised learningframework (Cao et al., 2016;Hou, 2019;Liu et al., 2019,2020;Gu et al.,
* Corresponding author.E-mail address:emailwzg@gmail.com(Z. Wang).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2022.02.005Received 15 December 2021; Received in revised form 25 February 2022; Accepted 26 February 2022Available online 3 March 20222666-5441/©2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 165–1702019).However, in the abovementioned oil and gas well production pre-diction methods, whether physical model-driven or data-driven algo-rithms, they focused more on medium-to-long-term productionprediction in the unit of month or year and were less involved in timeseries prediction of a single well's daily production and abnormalmonitoring in the whole life cycle of a tight gas ﬁeld (Dong and Yang, 2009;Sagheer and Kotb, 2019;Jiang et al., 2021). Therefore, considering the current and past statuses of daily production time series, we proposeda deep network prediction method based on long short-term memory(LSTM) neural network and further compared it with traditionaldata-driven machine learning approaches such as random forest (RF) andsupport vector regression (SVR).2. Data-driven approaches2.1. Random forest and support vector regressionA decision tree (DT) is a supervised learning algorithm that selects thebest decision scheme by constructing the decision process and calculatingthe corresponding mathematical expectation under the condition ofknown probabilities of all events. It can be expressed as a tree structure,in which the internal nodes represent different levels of decision schemesand the leaf nodes (endpoints) represent the ﬁnal decision results. However, they are prone to overﬁtting. Therefore, ensemble learningmethods are often used to replace a single DT, that is, RF ( Breiman, 2001). Assuming that the prediction results of each DT are good, there isoverﬁtting for different datasets. The RF notes that by constructing manyDTs and averaging their prediction results, the robustness of the DTs canbe improved while maintaining their prediction ability. The RF algorithmﬁrst conducts random sampling with replacement of the dataset to obtainseveral new datasets to create as many different DTs as possible. For thesenewly created datasets, each DT is constructed, and the weighted averageis conducted according to the different importance of features to obtainthe prediction results of the RF.The model function of SVR is a linear function. The SVR constructs aninterval band on both sides of the linear function. The loss is not calcu-lated for the data samples falling into the interval band but for thesamples located outside the interval band, and it is calculated accordingto a speciﬁc loss function. The SVR also introduces the kernel functionmethod from the support vector machine to expand the algorithm tohigher dimensional data space. There are many kinds of kernel functions.In this paper, the Gaussian kernel was used to map data samplesx
1andx 2into high-dimensional space, which is deﬁned ask
rbfðx1;x2Þ¼exp/C0/C0γjjx 1/C0x 2jj2/C1; (1)whereγis the parameter controlling the width of the Gaussian kernel.2.2. Recurrent neural networkFor time series such as the daily production of gas wells, the recurrentneural network (RNN) (Hopﬁeld, 1982) was introduced to capture the correlation between the current and past moments. The RNN model hasthe following characteristics: the output at one moment will be a part ofthe input at the next moment; for the multilayer stacked RNN, the pa-rameters of different layers can be shared, thus reducing the number oftraining parameters; and the length of the input data can be variable.Fig. 1shows the basic structure of RNN. Let the input data X¼½x
0;x1; …;x
N/C01/C138. A represents a structural neuron unit, x trepresents the input of thet-th network,o
trepresents the output of thet-th network, andh t
represents the hidden state of the output of the t-th network. The input–output relationship can be expressed as/C26h
t¼σðWxhxtþW hhht/C01þb hÞo
t¼W hyht/C01þb o; (2)whereσis the activation function,W xhis the weight matrix applied tox t
to compute the hidden stateh t,W hhis the weight matrix applied to thecurrent hidden stateh
tto calculate the next hidden stateh t/C01,W hyis the weight matrix applied to the hidden state h
tto calculate the outputo t, andb
handb oare offset items belonging to the hidden state and output,respectively. According to equation(2),h
tfrom any layer of the network can affect the subsequent output, which has strong practical signi ﬁcance for processing time series.However, the RNN networks are challenging to achieve long-termtransmission of the input. When the back-propagation mechanism isadopted to update the weights in the RNN, the gradient of the latter layerwill be multiplied by that of the former network layer as the number ofnetwork layers increases gradually. Hence, the RNN is prone to gradientexplosion/disappearance (Bengio et al, 1994). To solve the problem of long-term dependencies, many network structures have been proposed,such as LSTM neural networks and gated recurrent units.2.3. LSTM neural networkLSTM is one of the most widely used RNN structures ( Hochreiter and Schmidhuber, 1997). Compared with the traditional RNN structure, theLSTM adds memory and gating units on the original basis, which caneffectively avoid the disappearance/explosion of the gradient and canextract the long-term correlation that may exist in the data. Fig. 2depicts a schematic of an LSTM unit. The LSTM unit introduces three types ofgating units, which are the input, forget, and output gates. The generalequation of the input and output of the gating unit can be expressed asy¼
σðWxþbÞ; (3)whereWandbare the weight matrix and bias of the gating unit,respectively, and
σis the activation function. In the LSTM, the sigmoidfunction is generally used, and its expression is
σðxÞ¼11þe
/C0x(4)At timet, the input data of the LSTM are denoted by x
t, the output data are denoted byh
t, the memory unit isc t,/C10represents the
Fig. 1.The architecture of a recurrent neural network.
Fig. 2.A block of the long short-term memory.Q. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 165 –170
166multiplication of the corresponding bit elements, and the input gate iis used to determine how much input data x
tare allocated to the memory unitc
t. The forward propagation process isi
t¼σ/C0W
xixtþW hfht/C01þb i/C1 (5) Considering some features in the time series may not be meaningfulfor the target task. Thus, the forgetting gate is introduced to reduce theinﬂuence of these features at subsequent time tþn. The forgetting gatef mainly selects the discarded sample features by changing the memoryunitc
tas follows:f
t¼σ/C0W
xfxtþW hiht/C01þb f/C1; (6)c
t¼ft/C10c t/C01þit/C10tanhðW xcxtþW hcht/C01þb cÞ(7) The output gateodetermines the output datah
tof the network through the memory unitc
tand the input datax t. At this time, the output h
tof the network takes into account both the current input x tand the content learned by the previous network (that is, the memory unit c
t/C01), and its expression iso
t¼σðW xoxtþW hoht/C01þb oÞ; (8)h
t¼o t/C10tanhðc tÞ (9) In summary, for a samples
iof the training set, its data and label are x i
andy i, respectively. In this paper, the architecture of a deep networkbased on the deep LSTM and a fully connected network, that is, DLSTM-FNN, isﬁnally proposed, as shown inFig. 3. The forward propagation expression is8>>>><>>>>:f
1¼RNNðx iÞf
2¼Reluðf 1Þf
3¼Dropoutðf 2Þ:f
4¼Linearðf 3Þf
5¼Poolðf 4Þ(10)In equation(10), RNN represents the recurrent neural network. In thisstudy, we selected the LSTM unit, where the depth of hidden layers was30, which means the deep LSTM (DLSTM). Relurepresents the linear rectiﬁcation activation function, which is used to enhance the nonlinearexpression capability of the network. Dropoutrepresents the drop layer, and the inactivation rate was set to 0.5, which aims to alleviate theoverﬁtting problem of the model.LinearandPoolrepresent the full connection layer (FNN) and the average pooling layer, respectively, tomake the outputf
5have the same shape as the labely i. The root mean square error was used as the loss function, and adaptive moment esti-mation (Adam) was used as the weight optimization algorithm.3. Time series predictionThe reservoir indicators of gas wells generally include static geolog-ical and dynamic production data. Static geological data refer to well-head position coordinates, depth, resistivity, sonic interval transit time,rock density, compensated neutrons, shale content, gas layer thickness,total porosity, permeability, and hydrocarbon saturation. Dynamic pro-duction data mainly refer to the daily gas production, gas pressure, casingpressure, and production time of a gas well under different productiondates. To facilitate the comparison of different methods, this study ig-nores the remaining dynamic production characteristics and only con-siders the daily gas production time series. Next, the RF, SVR, andDLSTM-FNN models were applied to dynamically predict the daily gasproduction time series for 1692 tight gas wells in the Sulige gas ﬁeld.3.1. Sample set constructionIn the daily production time series of 1692 tight gas wells in Sulige,the earliest production date started on November 5, 2006, and thelongest production time was 5405 days as of August 28, 2021. Let thedaily gas production of gas wellvibedata i. Owing to the different pro- duction times and durations of each gas well, the length of data
iin each gas well was not completely consistent. To enable large quantities of gaswell data to be inputted into the DLSTM-FNN for training and predictionsimultaneously, alldata
imust have the same length. One method is to ﬁll in with zero values, that is, to make the value of data
iat all nonmining times be zero. Another method is zero removal, that is, delete the zerovalues existing indata
iand splice it. After splicing, the timing sequencelength of the dataset is the minimum length of data
iafter zero removal. However, both methods have advantages and disadvantages. The zero-ﬁlling method retains the consistency of the daily gas production ofeach gas well in the production time, but it will result in a considerablenumber of zeros in the dataset, which reduces the convergence speed ofthe neural network. The zero removal method reduces the dif ﬁculty of neural network training because it retains effective production data, butit leads to inconsistencies in the corresponding production times ofdifferent gas wells. Additionally, because the effective production data ofindividual gas wells are relatively small, the time sequence length of thecombined dataset will be very low, limiting the expressive ability of theneural network.Therefore, if the zero-ﬁlling method is adopted to construct dataset D for all gas wells, its dimensionD
shapeisð1692;5405Þ. In this study, it was unnecessary to consider all gas wells. Supposing the number of gas wellsto be considered isn, for the zero-ﬁlling method,D
shape is equal toðn; 5405Þ; for the zero removal method,D
shapeis equal toðn;l minÞ; andl minis the minimum length after zero removal of data
i. Let the dimension of the time series Dbeðn;lÞ. First,Dwas divided
Fig. 3.The schematic of the DLSTM-FNN architecture.Q. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 165 –170
167into the training setDtrainand the test setDtest. That is, for a time series of lengthl, assign theﬁrstl
traininformation ofDtoDtrainand the lastl test
information toDtest. Next, construct input dataxand input labelyfor D
trainandDtest, respectively. At timet, we hope to use the sequencexfor some time beforetto predict the sequenceyfor some time aftert. Assuming that the sequence length of xisl
x, the sequence length ofyisl y, and the sample formed by a pair ofxandyat timetiss
t, the samples tþ1
can be generated at timetþ1, as shown inFig. 4. FollowingFig. 4, for the training setD
train, the sample datasetStraincan be constructed as½s 0;s1;…; s
mtrain/C138, and the number of samplesmtrainisltrain/C0lx/C0lyþ1. Similarly, for the test setD
test,Stestcan be constructed as½s 0;s1;…;s mtest/C138, and the number of test setsm
testisltest/C0lx/C0lyþ1. IfStrainandStestare divided into training set datax
train, training set labelytrain, test set dataxtest, and test set labely
test, their dimensions areðmtrain;lx;nÞ,ðmtrain;ly;nÞ,ðmtest;lx;nÞ, andðm
test;ly;nÞ, respectively.3.2. Result analysisUsing the information of 1692 gas wells in Sulige as the dataset, theRF, SVR, and DLSTM-FNN models were constructed. First, we reducedthe dimensions ofxandytoðm;l
xÞandðm;l yÞ, respectively. Then, model training was performed on the basis of x
trainandytrain. The length of the training sample was only 10% of the total sample data, which belongedto the short training sample set. On the basis of the data-driven ap-proaches, we used the trained model to predict x
trainandxtestto obtainytrainpredandytestpred, and then, we compared them with the observed valuesy
trainandytestto analyze regression accuracy. As a deep learning model,the DLSTM-FNN needs to balance prediction accuracy and computationaltime. By constant optimizations of the network hyperparameters, ﬁnally, the shortest sequence lengthl
xwas set to 1, the label sequence length l y
was 1, the learning rate was 0.0001, the weight attenuation was 0.0005,and the number of iterations was 150. Under optimal hyperparameters,the average predicting speed of the DLSTM-FNN model was 67.59 s perwell, which can fulﬁll the real-time requirement of the actual productionabnormality monitoring in Sulige. Here the root mean square error(RMSE) was used as an accuracy measurement, which can be given asfollowsRMSE¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1nXni¼1/C16y
i/C0yipred/C172s; (11)wherey
iis the current observed value andyipredis its predicted value. For the 1692 gas wells in the Sulige gasﬁeld,Table 1compares the RMSE of the three gas well production dynamic prediction models: the RF, SVR,and DLSTM-FNN on the training set and testing set.Although the RF had the smallest error in the short training sampleset, which was 0.0884904, it had the largest error in the test set, whichwas 0.1878669. The DLSTM-FNN had the smallest error in the testing set,which was 0.1378165 and 0.03 less than the SVR prediction error.Additionally, its prediction was the best.To further analyze the life cycle of the gas well, typical gas wells S1and S2 that have produced for 3922 and 1145 days, respectively, weretaken as examples.Fig. 5shows the gas well production prediction resultsof the three data-driven models, and Table 2shows their corresponding RMSE. For gas wells S1 and S2, DLSTM-FNN had the smallest RMSE onthe testing set, which was 0.0722189 and 0.1025749, respectively. BycomparingFig. 5a, c, and 5e, the DLSTM-FNN showed more accuratelypredicted production trends and mutation locations in the measuredcontinuous production period (1000–1500 days), the intermittent pro- duction period (1500–2000 days), and the economic inefﬁciency period (>2000 days) of gas well S1. During the 1145-day production cycle, thedaily production of gas well S2 was zero due to multiple shutdowns, andthe daily productionﬂuctuated sharply because of multiple productionstimulation measures. In the prediction results of gas well S2 ( Fig. 5b, d, and 5f), the DLSTM-FNN model better predicted the locations of zerodaily production abnormalities and severe ﬂuctuations (Fig. 5f). There- fore, compared with the traditional RF and SVR, the data-driven DLSTM-FNN model had the advantages of short training sample sets, quick speedof large-scale predictions, and more accurate results. Furthermore, theprediction of DLSTM-FNN is expected to be used for monitoring pro-duction anomalies, for providing a speci ﬁc time window, and for a quantitated production evaluation for the stimulation measures.4. ConclusionThis paper studied the daily production prediction of the Sulige tightgas wells. Three data-driven approaches of time series prediction,including the RF, SVR, and LSTM, were applied. Considering the poten-tial information of current and past statuses in the time series, we pro-posed an optimal set of the DLSTM-FNN hyperparameters suitable for thedaily production prediction of tight gas wells.The availability and practicability of the three data-driven models in
Fig. 4.The sample composition of the daily production time series.Table 1The RMSE of the production prediction models for 1692 gas wells in Sulige.
Model Training Set Testing SetRF 0.0884904 0.1878669SVR 0.1409583 0.1636799DLSTM-FNN 0.1663612 0.1378165Q. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 165 –170
168dynamic daily production prediction were veri ﬁed using 1692 time series of wells in the Sulige gasﬁeld. Among them, the DLSTM-FNN deeplearning model had a good ability in time series modeling. For the shorttraining sample set and the rapid prediction of thousands of wells, thequantitated prediction performances revealed that the DLSTM-FNNmodel was more accurate and reliable and was more in line with theactual dynamic trend of the daily production of gas wells. Furthermore,the proposed DLSTM-FNN model can be applied to the time series pre-diction of other tight gasﬁelds based on transfer learning.FundingThis work was supported by the National Key R&D Program of China (2020YFA0713404).Declaration of competing interestThe authors declare that they do not have any commercial or asso-ciative interest that represents a conﬂict of interest in connection with the work submitted.References
Arps, J.J., 1945. Analysis of decline curves. Transact. Am. Inst. Min. Metal. Petrol. Eng.160 (1), 228–247.Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with gradientdescent is difﬁcult. IEEE Trans. Neural Network. 5 (2), 157 –166. Breiman, L., 2001. Random forests. Mach. Learn. 45, 5 –32. Cao, Q., Banerjee, R., Gupta, S., et al., 2016. Data driven production forecasting usingmachine learning. In: SPE Argentina Exploration and Production of UnconventionalResources Symposium. SPE, Buenos Aires .
Fig. 5.Data-driven methods to predict daily productions of gas wells S1 and S2. (a): The prediction result of RF on S1. (b): The prediction result of RF on S2. (c ): The prediction result of SVR on S1. (d): The prediction result of SVR on S2. (e): The prediction result of DLSTM-FNN on S1. (f): The prediction result of DLST M-FNN on S2.
Table 2The RMSE of the production prediction models for wells S1 and S2.
Model S1 training set S1 testing set S2 training set S2 testing setRF 0.1090542 0.1285114 0.1269109 0.1587212SVR 0.2011788 0.0957835 0.1945521 0.1073256DLSTM-FNN 0.2042352 0.0722189 0.19246629 0.1025749Q. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 165 –170
169Chen, Y., Fu, L., 2019. Establishment, comparison and application of power functiondecline model. Petrol. Geol. Recov. Ef ﬁc. 26 (6), 87–91. Chen, Y., Tang, W., 2016. Establishment and application of generalized decline model.Acta Pet. Sin. 37 (11), 1410–1413. Chen, Y., Zhou, C., 2015. Establishment, comparison and application of the linear declinetype. Acta Pet. Sin. 36 (8), 983–987. Dong, W., Yang, S., 2009. Factors affecting production decline performance andproduction forecasting. Nat. Gas Geosci. 20 (3), 411 –415. Gu, J., Zhou, M., Li, Z., et al., 2019. Oil well production forecast with long- short termmemory network model based on data mining. Special Oil Gas Reservoirs 26 (2),81–85, 135.Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8),1735–1780.Hopﬁeld, J.J., 1982. Neural networks and physical systems with emergent collectivecomputational abilities. In: Proceedings of the National Academy of Sciences of theUnited States of America, 79. National Academy of Sciences of the United States ofAmerica, pp. 2554–2558, 8. Hou, C., 2019. New well oil production forecast method based on long-term and short-term memory neural network. Petrol. Geol. Recov. Ef ﬁc. 26 (3), 105–110.Jiang, F., Pang, X., Jiang, Z., et al., 2007. Physical simulation experiment of gas chargingin tight sandstone. Geol. Rev. 53 (6), 844 –849. Jiang, Y., Chen, X., Bao, H., 2021. A new model for rapid prediction of horizontal wellproduction decline in shale gas staged fracturing: case study of Fuling shale gas ﬁeld. Nat. Gas Geosci. 32 (6), 845–850. Liu, W., Liu, W.D., Gu, J., Shen, X., 2019. Predictive model for water absorption insublayers using a machine learning method. J. Petrol. Sci. Eng. 182, 106367 . Liu, W., Liu, W., Gu, J., et al., 2020. Oil production prediction based on a machinelearning method. Oil Drill. Product. Technol. 42 (1), 70 –75. Sagheer, A., Kotb, M., 2019. Time series forecasting of petroleum production using deepLSTM recurrent networks. Neurocomputing 323, 203 –213. Yang, H., Fu, J., Liu, X., Meng, P., 2012. Accumulation conditions and exploration anddevelopment of tight gas in the upper paleozoic of the Ordos basin. Petrol. Explor.Dev. 39 (3), 295–303.Yang, T., Zhang, G., Liang, K., 2012. The exploration of global tight sandstone gas andforecast of the development tendency in China. Strat. Study CAE 14 (16), 64 –68. Zou, C., Yang, Z., Tao, S., et al., 2012. Nano-hydrocarbon and the accumulation incoexisting source and reservoir. Petrol. Explor. Dev. 39 (1), 13 –26.Q. Zhang et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 165 –170
170