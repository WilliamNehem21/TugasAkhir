ORIGINAL ARTICLE
An inverse problem approach to patternrecognition in industry
Ali Sever *
University Of North Carolina at Charlotte, Charlotte, NC 28223, USAReceived 31 January 2013; revised 1 January 2014; accepted 27 February 2014
Available online 12 March 2014
KEYWORDSArtiﬁcial neuralnetworks;Inverse problems;Machine learning;Pattern recognition;RegularizationAbstract Many works have shown strong connections between learning andregularization techniques for ill-posed inverse problems. A careful analysis showsthat a rigorous connection between learning and regularization for inverse prob-lem is not straightforward. In this study, pattern recognition will be viewed as anill-posed inverse problem and applications of methods from the theory of inverseproblems to pattern recognition are studied. A new learning algorithm derivedfrom a well-known regularization model is generated and applied to the taskof reconstruction of an inhomogeneous object as pattern recognition. Particu-larly, it is demonstrated that pattern recognition can be reformulated in termsof inverse problems deﬁned by a Riesz-type kernel. This reformulation can beemployed to design a learning algorithm based on a numerical solution of a sys-tem of linear equations. Finally, numerical experiments have been carried outwith synthetic experimental data considering a reasonable level of noise. Goodrecoveries have been achieved with this methodology, and the results of thesesimulations are compatible with the existing methods. The comparison resultsshow that the Regularization-based learning algorithm (RBA) obtains a promis-ing performance on the majority of the test problems. In prospects, this methodcan be used for the creation of automated systems for diagnostics, testing, and
*Tel.: +1 704 687 8580, +1 704 463 3118; fax: +1 704 687 8580.E-mail address:ali.sever@uncc.eduURL:http://www.uncc.eduPeer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2015) 11, 1–12
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
2210-8327ª2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.http://dx.doi.org/10.1016/j.aci.2014.02.004control in various ﬁelds of scientiﬁc and applied research, as well as in industry.
ª2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.
1. Introduction
No patterns can be derived solely from empirical data ( Yee and Haykin, 1993). Some hypotheses about patterns have to be chosen and, from among patternssatisfying these hypotheses, a pattern with a good ﬁt to the data must besought.
Neurocomputing brought a new terminology to data analysis: searching forparameters of their input/output functions is called learning, and samples of datatraining sets and a capability to satisfactorily process new data that have not beenused for learning is called generalization.
The capability of generalization depends upon the choice of a hypothesis set ofinput/output functions, in which one searches for a pattern (a functional relation-ship) that matches the empirical data. So a restriction of the hypothesis set to onlyphysically meaningful functions can improve generalization.
Inverse problems frequently arise in experimental situations when one is inter-ested in the description of the internal structure of a system and is given indirect,noisy data. Estimating the response of a system given a complete speciﬁcation ofthe internal structure, on the other hand, is the forward problem.
The modeling problem arises when one is given noisy data, observed over irreg-ular intervals of space and time, and is asked to develop a reasonable model to ﬁtthose observed data (Vapnik, 1998).
With the advent of high-speed computers and artiﬁcial intelligence techniques,this modeling problem underwent a metamorphosis and emerged as a machinelearning problem (Bauer et al., 2007; Gdawiec and Domanska, 2011). Tikhonov and Lanweber regularized that learning algorithms have recently received anincreasing interest due to both theoretical and computational motivations ( Abru- kov et al., 2006; Kurkova, 2012; Tiknonov and Arsenin, 1977 ). Fractal, optimiza- tion, and a two-dimensional functional relational model have been used as afeature in several pattern recognition methods (Chang et al., 2010; Lo Gerfoet al., 2008; Noureddine, in press). Considerable attention is currently being de-voted to new possibilities of using artiﬁcial neural networks (ANN) in view of theirincreasing importance for solving the problem of automated reconstruction of theinner structure of an object. Accompanying algorithms that effectively quantifyuncertainties, deal with ill-posedness, and fully take the nonlinear model intoaccount are needed Therefore, it is necessary to both look for possible ways toimprove the classical learning algorithms already existent in the literature, andto identify new methods which can compete with the traditional ones in speed,robustness, and quality of results.2 A. SeverInverse problems are often formulated by assuming that the underlying phe-nomenon is a dynamic system characterized by mathematical equations, althoughno such assumption is always essential. Often the goal is to build an algorithmicmodel of the underlying phenomena. In some contexts a model is only a meansto an end. The ultimate goal in such cases is to test the validity of a hypothesis.In these cases, the model is used as a classiﬁer (e.g., neural nets and decision trees),and it matters little whether the model is parametric or non-parametric; the clas-siﬁcation accuracy becomes more important. From this point of view the entireﬁeld of Machine Learning can be treated as an exercise in solving inverse problems(Bauer et al., 2007; Prato et al., 2007). By their very nature, inverse problems aredifﬁcult to solve. Sometimes they are ill-posed. A well-posedmathematical prob- lem must satisfy the following requirements: existence, uniqueness and stability.The existence problem is really a non-issue in many realistic situations becausethe physical reality must be a solution. However, due to noisy and/or insufﬁcientmeasurement data, an accurate solution may not exist. More often, the majordifﬁculty is to ﬁnd a unique solution; this especially when solving a parameteridentiﬁcation problem. Different combinations of parameter values (includingboundaries and boundary conditions) may lead to similar observations. Oneuseful strategy to handle the non-uniqueness issue is to utilize a prioriinformation as additional constraints. These constraints generally involve the imposition ofrequirements such as smoothness on the unknown solution or its derivatives, orpositivity, or maximum entropy or some other very general mathematical prop-erty. A more aggressive approach would be the use of regularization. Given anobserved data set, genetic algorithms and genetic programming can be used tosearch a hypothesis space.
In this paper, starting from a reformulation of the pattern recognition as an in-verse problem, we introduce an alternative learning algorithm derived by a well-known regularization method. We use a Riesz-type kernel to solve classiﬁcationtasks by transforming the geometry of input space by embedding them into higherdimensional, inner product spaces, and introducing a regularization method whichadds to the derived integral equation a new term, called stabilizer, which penalizesundesired input/output functions. We split the problem into a simpler, ill-posedproblem (an integral equation with a Riesz-type kernel) and a well-posed problem.In this way, we isolate and better control the propagation of errors due to theill-posedness (Noureddine, in press). Then we show that this reformulation canbe employed to design a learning algorithm based upon a numerical solution ofa system of linear equations.
The rest of the paper is organized as follows: The next Section describes ourmodel and justiﬁes its use. In Section3, we formulate the proposed regularizedlearning algorithm. Section IV presents main simulation results. We compareour Regularization-based Algorithm (RBA) with the Support Vector Machine(SVM) and Semanteme-based Support Vector Machine (SSVM) in Section 5. Finally, we conclude the paper with a summary of the work in Section VI.Inverse problem approach to pattern recognition 32. Generalization model as Regularization
Let us formulate the generalized problem as regularization in the following way:
Find a functionr 12L 1ðXÞ;X2Rn, given the functionB(x k)=w(x k),xk2O k, O
k2Rn. Therefore, we have the following integral equation of the ﬁrst kind
Ar1ðxÞ¼BðxÞ;x2X k ð2:1Þ
whereAr 1(x)=/C242 Xk(x,y)r 1(y)dyandk(x,y) = (1/2p)2|x/C0y|/C02andAis consid-
ered as an operator fromL 1(O) intoL 1(Ok). This integral equation is the Fred-holm integral equation of the ﬁrst kind with a Riesz-type kernel.
First we need to show that Eq.(2.1)represents a severely ill-posed problem.Then we have to prove that a solutionr
1(a) to the Eq.(2.2)exists and is unique.
Theorem 1.Let us assume thatOandO kare nonintersecting domains inR3. Then the integral Eq.(2.1)with the Riesz-type kernel represents an ill-posed problem.
Proof.We should notice that there are no singularities in the Riesz-type kernelwith the domains deﬁned above. We then claim that whether r
12L2(O) is contin- uous or not, (Ar
1)(x) is continuous in the usual sense. In fact,
Ar1ðx1Þ/C0Ar 1ðx2Þjj ¼Z
Xjx1/C0yj/C02r1ðyÞdy/C0Z
Xjx2/C0yj/C02r1ðyÞdy/C12/C12/C12/C12/C12/C12/C12/C126jr
1ðyÞjjjx 1/C0yj/C02/C0jx 2/C0yj/C02jdy6kr
1k2:Z
Xjjx1/C0yjr/C0jx 2/C0yjaj2:dy
Since the integrand ||x 1/C0y|r/C0|x 2/C0y|a| is uniformly continuous, we have if
|x1/C0x2|<d,
jjx1/C0yjr/C0jx 2/C0yjaj2<efor8y2X:
Therefore,
jAr 1ðx1Þ/C0Ar 1ðx2Þj6kr 1k2:e:jXj;
|X| stands for a certain measure ofXfor any givene>0 .
Now it is clear that if we take anyB(x)eL 2(Xk) which is continuous, then thereis nor
1eL2(X) such thatAr 1=B. So the existence requirement of the well-posedness is violated. Therefore the Eq.(2.1)is ill-posed.
For the integral Eq.(2.1), with a Riesz-type kernel and non-intersecting do-mainsXandX
k, there is uniqueness inL 2(X)Prato and Zanni, 2008. Djatlov’s work shows a logarithmic type of stability estimate (Kress, 1989).
We use the Tikhonov regularization method (Kress, 1989; Sever, 1999) to solve the ill-posed problem in Eq.(2.1). In this method, instead of Eq.(2.1), we solve the following regularized equation:4 A. SeverðA/C3AþaIÞr 1ðxÞ¼A/C3BðxÞð2:2Þ
whereIis the identity operator, andais a regularization parameter. Now we will
show the solutionr 1(a) and its convergence to the solution B whenaﬁ0, provided r
1exists and the uniqueness for the original Eq.(2.1). Now we need to prove theexistence of the solutionr
1(a) and its convergence to the solutionfwhenaﬁ0, providedr
1exists and is unique.
Theorem 2.A solutionr 1(a) to the Eq.(2.2)exists and is unique. AlsoR aydeﬁned asr
1(a) is a regularizer to the Eq.(2.1)onX=X M, provided that the equation Ar
1= 0 has only zero solution.
Proof.First we assume that A is compact, so isA*A, butA*Ais also self-adjoint. ThusA
*Ahas a complete eigenfunction system denoted by { e k} with the corre- sponding eigenvalues {k
k}.
Then, we haver 1(a)=P
kr1k(a)e k,A*y=P
kBAkekusing equation, we get
ðkkþaÞr 1kðaÞ¼B Ak
which impliesr 1k(a)=B Ak/(kk+a) is uniquely determined. Uniqueness can be
obtained without using the expression of the solution. In fact, since A*Ais posi- tive, we have
ððA/C3AþaÞx;xÞ¼ðAx;AxÞþaðx;xÞ>0 for8x–0:
Therefore it cannot happen that there is some r*1„0 such that
ðA/C3AþaÞr/C31¼0
which means
kernelðA/C3AþaIÞ¼f0g:
Now we show thatr 1(a) is actually a regularizer. To this end, we may assume nowAr
1=Bwherer 1eXM. Noticing thatA*A+aIis also self-adjoint, we have
ððA/C3AþaÞÞr 1ðaÞ;e kÞ¼ðA/C3B;e kÞðA/C3Ar1;ekÞ
or
ðr1ðaÞ;ððA/C3AþaÞe kÞ¼ðr 1;A/C3AekÞ
ððkkþaÞðr 1ðaÞ;e kÞ¼k kðr1;ekÞ
withr 1(a)=P
kr1k(a)e k,r1=P
krkek,therefore, we get
r1kðaÞ¼ðk k=kkþaÞr 1
and
r1kðaÞ/C0r 1k¼ðk k=kkþaÞr 1k/C0r 1k¼/C0 ða=k kþaÞr 1k:Inverse problem approach to pattern recognition 5Therefore
jjr1kðaÞ/C0r 1kjj2¼X
kjr1kðaÞ/C0r 1kj2¼X
kða2=ðk kþaÞ2Þr21kðkk>0Þ <a
X
k6KKðk kþaÞ/C02jr1kj2þX
k>Kjr1kj2
Here we assume thatk kPk k+1P...
For alle> 0 sincex2L 2,P
k|r1k|2converges. We ﬁrst chooseKsuch thatP
k|r1k|2<e2/2.
Then, for the ﬁxedK, we may chosedsuch that
aX
k6KðkkþaÞ/C02jr1kj2<e2=2:
Consequently,
jjr1kðaÞ/C0r 1kjj6e
It remains to prove thatR a,i.e.,r 1is continuous. By observing that
r1kðaÞ/C0r 1kða0Þ¼ð ða 0/C0aÞk kÞ=ðk kþaÞðk kþa 0Þr1k
And using a similar argument to what we had above, we get the continuity of r 1k.
Spectral representation of self-adjoint operators in Hilbert space gives the gen-eral case. AssumptionAr
1= 0 if and only ifr 1= 0 guarantees that the limit of r
1(a) is unique.
3. Regularized learning algorithm
In this section we formulate a regularized learning algorithm based upon theTikhonov regularization algorithm. For computational reasons, letOandO
kbe the domain inR
2and we will regard the integral operator
Ar1ðxÞ¼Z
Xr1ðyÞ=jx/C0yj2dy
as deﬁned fromL 2(O) intoL 2(Ok). By using the deﬁnition of an adjoint operator inL
2, we haveA*:L2(Ok)ﬁL 2(O) deﬁned by
A/C3BðyÞ¼Z
XkBðxÞ=jx/C0yj2dx y2Xð2:3Þ
andA*Ar1(y) becomes
A/C3Ar1ðyÞ¼Z
XkZ
Xr1ðy0Þjx/C0yj2jx/C0y0j2dy0dxð2:4Þ
wherex2O k, andy,y02O. By discretizing Eq.(2.4), we have6 A. SeverA/C3Ar1ðyiÞ¼Xnk;j¼1
wjwkr1ðy0jÞ=jx k/C0yij2jxk/C0y0jj2yi;y0j2Xﬃ
Xnj¼1
HI;jr1ðy0jÞ ð2:5Þ
where
HI;jﬃXnk;j¼1
wjwk=jxk/C0yij2jxk/C0y0jj2
andw j, andw kare the weight functions. By discretizing(2.3), we have
A/C3BðyiÞﬃXnk¼1
wkBðx kÞ=jx k/C0yij2ð2:6Þ
From(2.3)–(2.6)we have the discretized matrix equation in the form of
ðaIþHÞr 1ðyÞ¼A/C3Bð2:7Þ
for some regularization parameters,a> 0. Now the problem is reduced to solving
systems of linear equations.
4. Simulation results
In this section we want to investigate the effectiveness of the regularized learningalgorithm introduced in Section3.
Let us denote ‘real object’ byr t, and ‘computed object’ byr c. To test the meth- od numerically, it is necessary to generate ‘B(x)’ in the integral Eq.(2.1).W ed o this by specifyingrand evaluating the integral numerically. Once we have thenumerical values ofB(x), we use these as our data and recover the pattern insidethe required region. The steps of test calculation are:
(1) specifyr t,(2) calculate the integral(2.1),(3) use(2.7)to ﬁndr
c,(4) comparer
twithr c.
Our test calculation used
(i) smooth surface (Fig. 1)(ii)r
t=r 0+x 1r1+x 2r2(two objects)
wherex i’s are characteristic functions of unknown objects, and r 0= ½ and r
I=1(Fig. 3). In the case of smooth surface (i), the numerical calculations haveshown that whenr
tis a smooth polynomial, the reconstruction is a very goodapproximation ofr
c(Fig. 2).Inverse problem approach to pattern recognition 7In case (ii), we used two different regularization parameters for our reconstruc-tion: simply,a=1 0
/C07anda=1 0/C010. We provided the cross sections ofr tandr c. The proposed model was able to distinguish the objects. We observed that thecomputedr
cwas always smoothed. The location of the objects was well producedbyr
c, and also the shape of ther cwas a fair indication of the objects (Figs. 4 and 5 andTable 1).
Equation(2.7)involves main parameters that must be adjusted for greatest efﬁ-ciency: the regularization parameter and the number of grid points. Looking at thereconstructions, the numerical experiments described above have shown that thereconstructed surface is smooth and close to the true surface. The reconstructionwas usually a fair representation of the shape of the r.
Summarizing, the simplicity and the reconstruction accuracy make theproposed regularized learning model well suited for the considered application.
Figure 1Smooth surfacer t=x+y.
Figure 2Reconstruction of surface inFig. 1.8 A. Sever5. Results and discussion
In this section, misclassiﬁcation rate (Li and Wang, 2009) is used to evaluate the efﬁciency of our algorithm. Misclassiﬁcation rate refers to the ratio of the numberof misclassiﬁed exemplars to the total number of exemplars in the dataset. Theratio is computed using the Formulation(5.3). Correspondingly, the classiﬁcationaccuracy is determined by the Formulation(5.2).
Figure 3Test domain with two objects.
Figure 4Cross-section from the reconstruction of r t=r 0+x 1r1+x 2r2wherex i’s are characteristic functions of unknown objects, and r
0= ½ andr I= 1, anda=1 0/C07.Inverse problem approach to pattern recognition 9cerror¼X
i¼1;/C01number i=nð5:1Þ
cerror¼1/C0cðerrorÞð5:2Þ
where number irefers to the number of misclassiﬁed exemplars in the positive and
negative classes, andnis the total number ofX. A smallerc errorvalue indicates higher classiﬁcation accuracy and better classiﬁcation efﬁciency. Conversely, a big-gerc
errorvalue indicates worse classiﬁcation efﬁciency.
Experiments are performed on the purely syntactic datasets from the UCI ma-chine learning repository (UCI, 1998). We compare our Regularization-basedAlgorithm (RBA) with the Support Vector Machine (SVM) and Semanteme-basedSupport Vector Machine (SSVM) in theTable 2. In each test, all patterns withmissing attribute values are initially removed. Continuous Dataset (CDS), Discon-tinuous Dataset I (DDS-I), and Discontinuous Dataset II (DDS-II) have theirﬁxed real and computed pattern sets. The 9% outliers existing in DDS-II’s trainingpatterns are identiﬁed, and the dataset is classiﬁed as unbalanced. The continuousTable 1Experimental datasets.Cross-sectional points r
0= 1/2 andr 1=1/C01/C00.5 0 0.5 1 r
s– True value 0.5 1 0.5 1 0.5a=1 0
/C070.507 0.982 0.53 0.982 0.507a=1 0
/C0100.582 0.973 0.532 0.973 0.591 Absolute error fora=1 0
/C07( % ) 1262 1Absolute error fora=1 0
/C010(%) 16 3 6 3 18
Figure 5Cross-section from the reconstruction of r t=r 0+x 1r1+x 2r2wherex i’s are characteristic functions of unknown objects, and r
0= ½ andr I= 1, anda=1 0/C010.10 A. Severattributes in datasets are preprocessed using the Formulation (5.3) Li and Wang, 2009.
xij¼ðx ij/C0min
ifxijgÞÞðmax
ifxijg/C0min
ifxijgÞ ð5:3Þ
A number of classiﬁcation algorithms depend on the similarity or dissimilarity of
exemplars, such as Euclidean distance and inner production, among others. How-ever, majority of these algorithms only process continuous-attributed data, notdiscontinuous data. Discontinuous data (surface in our examples) are extreme,having disordered and unbalanced distribution.
For parameter selection and optimization, regularized learning algorithm facesthe problem of selecting parametera. In many algorithms, the standardization ofthe selection method forais rarely performed (Han and Zhao, 2009). The heuristic method or some optimization algorithm is used to select a. In this study, the heuristic method is used in each experiment to standardize datasets. Table 1shows the relationship between classiﬁcation accuracy and the value ain the RBA algorithm.
6. Concluding remarks and future work
In this section, we brieﬂy discuss several results obtained and issues related to theproposed RBA learning and recognition technique. Some of these issues may beviewed as merits while others as limitations leading to open research problemsfor the future.
The implementation of the proposed algorithm shows that the method isreasonably accurate for the reconstruction of two objects, using artiﬁcially gener-ated data whose distributions are known. We have seen, both theoretically andexperimentally, that pattern classiﬁcation can be viewed as an ill-posed, inverseproblem to which a method of regularization may be applied. As shown in Table 2, our proposed regularized learning algorithm has already shown promising perfor-mance in comparison with the state-of-the-art approaches, such as Support VectorMachines (SVMs), on benchmark datasets and real-life test problems.
The information obtained from a preliminary analysis is by no meansexhaustive of the method discussed here and suggests several areas of additionalinvestigation. We recognize the clear connection between regularization theoryfor inverse problems, and pattern recognition as learning, and this allow us toTable 2Misclassiﬁcation ratio of different algorithms.Dataset SVM (%) SSVM (%) RBA (%)CDS 2.9 3.1 5.32DDS-1 16.3 9.1 4.95DDS-2 15.5 9.42 9.45Average 11.5 7.2 6.57Inverse problem approach to pattern recognition 11introduce a new learning algorithm. On one front, improvements have to be doneboth on the algorithm (different regularizer properties must be investigated) andthe applications (non-homogeneous 3-D object recognition). More detailed workis needed to improve the effectiveness of the numerics in general. In addition, theanswer to exactly how sensitive the method is to moderate amounts of noise is anopen question.
References
Abrukov, V.S., et al., 2006. Artiﬁcial Neural Networks and Optical Diagnostics. In: Sixth InternationalConference on Intelligence Systems Design and Applications.
Bauer, F., Preverzev, S., Rosasco, L., 2007. On regularization algorithms in learning theory. J. Complexity 35,52–72
.
Chang, Y.F., Lee, J.C., Mohd Rijal, O., Syed Abu Bakar, S.A.R., 2010. Efﬁcient online handwritten Chinesecharacter recognition system using a two-dimensional functional relationship model. Int. J. Appl. Math.Comput. Sci. 20 (4), 727–738.http://dx.doi.org/10.2478/v10006-010-0055-x .
Gdawiec, K., Domanska, D., 2011. Partitioned iterated function systems with division and a fractal dependencegraph in recognition of 2D shapes. Int. J. Appl. Math. Comput. Sci. 21 (4), 757–767. http://dx.doi.org/ 10.2478/v10006-011-0060-8, ISSN (Print) 1641-876X.
Han, Xian-Pei, Zhao, Jun, 2009. The creation of semantic metadata based on Wikipedia. J. Chin. Inform.Process. 23 (2), 108–114
.
Kress, R., 1989. Linear integral equation. In: Applied Math. Sciences. Springer-Verlag, Berlin, New York .
Kurkova, V., 2012. Complexity estimates based on integral transforms induced by computational units. NueralNetw. 33, 160–167
.
Li, Zhi-Hua, Wang, Shi-Tong, 2009. Clustering with outliers-based anomalous intrusion detection[J]. Syst. Eng.Electron. 31 (5), 1227–1230
.
Lo Gerfo, L., Rosasco, L., Odone, F., De Vito, E., Verri, A., 2008. Spectral algorithms for supervised learning.Neural Comput. 20 (7), 1873–1897
.
Noureddine, S., in press. An optimization approach for the satisﬁability problem, Appl. Comput. Inform. http:// dx.doi.org/10.1016/j.aci.2011.11.002.
Prato, M., Zanni, L., 2008. Inverse problems in machine learning: an application to brain activity interpretation.J. Phys. 135
.
Prato, M., Zanni, L., Zanghirati, G., 2007. On recent machine learning algorithms for brain activityinterpretation. In: 23rd Annual Review of Progress in Applied Computational Electromagnetics, March 19–23, 2007, Verona, Italy, pp. 1939–1946.
Sever, A., 1999. On uniqueness in the inverse conductivity problem. Math. Methods Appl. Sci. 22 (12), 953–966 .
Tiknonov, A.N., Arsenin, V., 1977. Solutions of Ill-posed Problems Transl from Russian. John Wiley Sons, NewYork, Toronto
.
UCI repository of machine learning database [EB/OL], 1998. Available from: < http://www.ics.UCI.edu/ ~mlearn/MLRepository.html>.
Vapnik, V.N., 1998. Statistical Learning Theory. John Wiley and Sons, New York .
Yee, P., Haykin, S., 1993. Pattern classiﬁcation as an ill-posed, inverse problem: a regularization approach. In:ICASSP’93 Proceedings of the 1993 IEEE International Conference on Acoustics, Speech, and SignalProcessing: Plenary, Special, Audio, Underwater Acoustics, VLSI, Neural Networks, vol. I.12 A. Sever