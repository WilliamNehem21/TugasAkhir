Using deep learning techniques and genetic-based feature extraction forpresentation attack mitigation
John Jenkinsa, Kaushik Roya, Joseph Sheltonb,*
aDepartment of Computer Science North Carolina A &T SU, Greensboro, NC, USA
bDepartment of Engineering and Computer Science Virginia State University, Petersburg, VA, USA
ARTICLE INFO
Keywords:Spooﬁng mitigationBiometric recognitionDeep learningABSTRACT—
Biometric authentication systems are becoming more prevalent for commercial use with computers and smartdevices. Biometric systems also have several vulnerable points that can be exploited by a hacker to gain unau-thorized access to a system. Replay attacks focus on capturing feature extractors (FEs) during transmission,decrypting, and replaying for illegal access. The Genetic and Evolutionary Feature Extraction (GEFE) technique,developed at North Carolina A&T State University, recently showed promising results in mitigating replay attacksin combination with a feature selection algorithm. Biometric-based presentation attacks, the focus of this work, isanother biometric system vulnerability primarily focused on presenting a biometric sample of quality to illegallygain access to secured data. Recently, deep learning techniques to mitigate presentation attacks have shownpromising results. However, the accuracy of deep learning-based biometric presentation attack detection (PAD)methods are limited by the quality of the samples provided. In absence of large sets of original biometric sampledata, data augmentation has been shown to be successful in generating synthetic biometric image data andimproving the performance of deep learning techniques applied. The novelty of this paper lies in the followingtwo aspects: First, a data augmentation technique with Generative Adversarial Networks (GANs) is used togenerate comparative synthetic (spooﬁng) dataset. With the proliferation of deep fakes in media, this technique should provide insight on the GAN technique often used. Once properly trained, the synthetic images are used tocreate spooﬁng datasets. Second, the GEFE technique is used in combination with the GANs to generate improvedanti-spooﬁng feature extractors optimized to mitigate presentation attacks. The combination of GEFE and GANs isused to identify those discriminative biometric features used to mitigate synthetic presentation attacks. The GEFEþGAN technique outperforms the LBP and GEFE techniques alone in overall identi ﬁcation and veriﬁcation results on spooﬁng datasets.
1. IntroductionBiometric authentication systems are becoming more prevalent forcommercial use with computers and smart devices [ 1]. Biometric authentication has inherent advantages over other authenticationmethods such as token-based and knowledge based methods [ 2–5]. Token-based authentication systems use some form of token, such as adriver’s license, or an ID card [5,6]. This form of authentication could beconsidered the most vulnerable to compromise, due to the fact that to-kens can be lost or stolen. Knowledge-based authentication systems havea slight advantage from a security standpoint, in that authentication isbased on what a user knows, i.e. password or pin. A password cannot beeasily stolen, but if a user writes it down, or is seen entering it, then it canbe taken and used maliciously [6,7]. A biometric-based authenticationsystem has an advantage over the two previously mentioned systems inthat biometric modalities are difﬁcult to replicate and are unique to in-dividuals [6–8]. The focus of this research is on evolving unique featureextractors for cyber security.Biometric systems have several vulnerable points that can beexploited by a hacker to gain unauthorized access to a system. Onevulnerable point, and the focus of this work, is the acceptance of pre-sentation attacks. Biometric-based presentation attacks are primarilyfocused on gaining access to a counterfeit biometric sample and imple-menting the same biometric to illegally gain access to secured data.Although biometric authentication strengthens security protocolsthrough unique feature extraction, presenting a biometric sample of
* Corresponding author.E-mail addresses:jmjenki1@aggies.ncat.edu(J. Jenkins),kroy@ncat.edu(K. Roy),jshelton@vsu.edu,jshelton@vsu.edu(J. Shelton).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100029Received 12 June 2019; Received in revised form 21 April 2020; Accepted 26 April 2020Available online 30 April 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 7 (2020) 100029quality to illegally gain access to a biometric system is feasible. It isknown that any 2D face biometric system with no anti-spoo ﬁng measures can be easily spoofed by presentation attacks [ 22]. Local Binary Patterns (LBP) is one of the more popular texturaltechniques for facial recognition [26,27]. The LBP technique has also been used for spooﬁng detection. In Chingovska et al. [23], anti-spooﬁng efforts went towards creating a face-spoo ﬁng database. The database followed guidelines for developing anti-spoo ﬁng algorithms. Despite its simplicity, LBP has proven to be very powerful in texture classi ﬁcation. In Gragnaniello et al. [17], LBP is used in combination with a livenessdetection algorithm on multimodal biometrics. Recently, deep learningtechniques to mitigate presentation attacks have shown promising results[18–20]. The popularity of Convolutional Neural Networks (CNN) aroseafter“AlexNet”[25] won the Large Scale Visual Recognition Challenge(LSVRC) of 2012. AlexNet recorded a rank-5 error rate of 15.3% whensorting over a million images into 1000 different classes. In Menotti et al.[20], the focus in building an anti-spooﬁng system with convolutional networks lead to in outstanding classiﬁcation results. Results strongly indicate that deep learning techniques can be used for robust spoo ﬁng detection [24].This proposes a few questions. How does training deep learning basedbiometric system using deep learning generated images effect biometricrecognition? What are the distinct (discriminative) biometric featuresused to mitigate presentation attacks? In this work, we contribute a morecomplex and novel data augmentation technique to generate a synthetic(spooﬁng) datasets. Using the deep learning technique of GenerativeAdversarial Networks (GANs) [28], the generated images will be ofquality to spoof the popular texture-based feature extraction method,LBP. Referred to by some as the most interesting idea in machine learningin the last 10 years, GANs have the ability to generate realistic photos ofobjects, scenes, and people that do not exist, yet are undetectable byhumans [35]. GANs are simple yet powerful. The generated spoo ﬁng datasets will then be used to improve Genetic and Evolutionary FeatureExtraction (GEFE) feature extraction accuracies. The GEFE feature ex-tractors will optimize the LBP feature extractors ’dimensions and displacement. The proposed GEFE technique will use a ﬁtness function optimized for mitigation synthetic (generated) images. Through thesesteps, a novel feature extraction technique to mitigate the effects ofpresentation attacks is contributed. With the proliferation of deepfakes[36,37] in media, this technqiue should provide insight on the GANtechnique often used. In combination with GEFE, the generated featureextractors will also provide insight on which biometric features on thesamples has the higher discriminative capabilities with its optimize di-
mensions and displacement. The remainder of this work follows with aliterature review of popular techniques for mitigatigating presentationattacks. This is followed by our methodology, experiments, and results.Lastly, we have our discussion and future direction.2. Literature reviewPresentation attacks are the presentation of impersonated humancharacteristics to the biometric capture subsystem. This is done to spoofbiometric recognition to illegally gain access to the biometric system.Print photo attacks, 3-D masks, and image regeneration have all beenused to successfully gain access to biometric systems [ 17]. A 2D face system with no anti-spooﬁng measures can be easily spoofed by pre-sentation attacks. Multimodal systems are suggested in scenarios withhigh probabilities of spoof attacks. Unfortunately, a biometric systemwith several modes may not need all modalities to be spoofed tocompromise the entire multimodal system [ 22]. Multimodal biometrics has shown to improve the performance of biometric systems, though theyare often expensive, and they can also increase the number of vulnera-bilities that can be exploited by an intruder [ 22]. The next section of this chapter will be an overview of some texture-based presentation attackmitigation. LBP is texture-based and one of the more notable techniquesfor biometric classiﬁcation. In Section II ⋅B, an overview of recent deeplearning based mitigation techniques is given. With promising resultsfrom deep learning techniques in image classi ﬁcation, similarﬁndings in spooﬁng detection and mitigation are favorable. The scope of this reviewexplores mitigation techniques for facial biometrics, excluding tech-niques designed generally for image classiﬁcation.2.1. Texture-based mitigation techniquesIdentify applicable funding agency here. If none, delete this text box.The LBP feature extraction technique forms texture patterns from thepixel intensity values of biometric images [ 26,27]. This method uses the texture patterns to create Feature Vectors (FVs) associated with the im-ages. The LBP method can be applied to any uniquely textured data, suchas facial and iris recognition. For biometric recognition, the ﬁrst step is to split an image into even sized regions. A histogram is associated witheach region, where the frequencies of texture patterns are stored. A FV isformed from the concatenated histograms resulting from each region.Texture patterns are measured by comparing pixel intensity values toone another within a region. More speciﬁcally, each pixel that is inclu- sively within a region will be compared with its nearest neighboringpixels. A pixel on the border of a region cannot be considered as a centerpixel since it does not have pixels within its region surrounding itentirely. Each texture pattern is represented as a binary string. Equations(1) and (2) show how a binary string can be extracted from a region on animage and converted into a decimal value denoting a histogram bin. Theterm LBP(Ni,c), denotes the decimal value of a texture pattern for aneighborhood of pixels. The term c represents the pixel intensity value ofthe center pixel, N represents the set of neighboring pixel intensity valuesfor c, and i represents the ith neighboring pixel of c. Equation (2)signiﬁes the difference being taken between each neighboring pixel and the centerpixel.LBPðNi;cÞ¼Xi/C01i¼0sðN i;cÞ2i(1)sðN
i;cÞ¼/C260;if Ni/C0c/C2001;if N
i/C0c>0 (2)Some of the earlier techniques for biometric spoo ﬁng detection used textural methods classiﬁcation. In Chingovska et al. [23], anti-spooﬁng efforts went towards creating a face-spoo ﬁng database. The database followed guidelines for developing anti-spoo ﬁng algorithms. The data- base should provide attacks capable of penetrating simple face recogni-tion systems. Each database provided an evaluation of the scores that abaseline face recognition system would be vulnerable for spoof attacks[23]. LBP based face spooﬁng counter-measure, variants of LBP weregauged for its efﬁciency against a variety of attacks. The LBP (3x3)technique resulted in a 14.84% Half Total Error Rate (HTER) on thetraining set and a 15.17% HTER on the test set. Results also showed thatthe traditional LBP technique had the best performance/complexitytradeoff [23]. Despite its simplicity, LBP has proven to be very powerfulin texture classiﬁcation.In Gragnaniello et al. [17], biometric spooﬁng detection explored using the LBP technique in combination with face liveness detectionmethods on multimodal biometrics. The multimodal system includessamples from theﬁngerprint, the iris, and face biometrics. For theexperiment, protocol required a cropped small region (64x64 pixel to80x80 pixel). The novel Histogram of Invariant Gradients (HIG), avariant of Scale-Invariant Feature Transform (SIFT) and Histogram ofOriented Gradients (HOG) are all textural methods tested with the intentof preserving robustness. A linear Support Vector Machine (SVM) clas-siﬁer was used to avoid feature selection. K-means clustering withEuclidean distance was also used for the joint quantization of features.Although some techniques reduce the average error by as much as 75%,the overall analysis could not clearly distinguish a descriptor performinguniformly better than others [17].J. Jenkins et al. Array 7 (2020) 100029
2Presentation Attack Detection (PAD) algorithms have also been pro-posed exploring micro-texture variation using Binarized Statistical ImageFeatures (BSIF) and micro-frequency variations using 2D Cepstrum [ 24]. The 2D Ceptrum feature extraction is widely used in the domain ofspeech and image processing. The BSIF method denotes each pixel as abinary string obtained by computing its response to a ﬁlter. Theﬁlters are trained utilizing the statistical properties of the natural images. The BSIFand 2D Ceptrum feature vectors are concatenated to form a single vectorbefore obtaining a decision using linear SVM classi ﬁer. Experimental results revealed that, the PAD algorithm ’s best scheme was an Average Classiﬁcation Error Rate (ACER) of 10.21% on face and an ACER of 0%on the iris modality [24].2.2. Deep learning based mitigation techniquesThe popularity of Convolutional Neural Networks (CNN) arose after“AlexNet”[25] won the Large Scale Visual Recognition Challenge(LSVRC) of 2012. AlexNet recorded a rank-5 error rate of 15.3% whensorting over a million images into 1000 different classes. Deep learningtechniques have shown promising results in mitigating presentation at-tacks [18–20].In Bharati et al. [18], a novel algorithm for facial image retouchingdetection using deep learning techniques. Digitally altered, “photo- shopped”, images are common practice in the online/social mediacommunities. To detect digital retouching in facial images, the Super-vised Restricted Boltzmann Machine (SRBM) based algorithm is pro-posed [18]. The detection algorithm uses four local facial patchesextracted from a full facial image; the left and right periocular, mouth,and nose regions. The size of each extracted facial patch is 64 /C264. Each patch is trained on a three layer SDBM. The size of learned representationfor each SDBM is 256. Once the features are trained and concatenated, aSVM classiﬁer is trained for two-class classiﬁcation. The proposed algo- rithm showed signiﬁcant advances in retouching detection. Experimentsyield a high of 55.7% classiﬁcation accuracy for existing makeupdetection algorithm, with the proposed algorithm achieving nearly 87%classiﬁcation accuracy on the same dataset. Additionally, experimentsshowed that the improvements in classiﬁcation accuracy were attributed to the supervised DBM and SVM classiﬁcation [18]. In Yang et al. [19], spooﬁng mitigation relied on the deep Convolu-tional Neural Network (CNN) to learn features of high discriminativeability in a supervised manner. Combined with some data pre-processing,the face anti-spooﬁng performance improves drastically [ 19]. First with data preparation, spatial and temporal augmentation is performed on theface, periocular and iris modalities [ 19]. After face localization, the
temporal augmentation extracts spatiotemporal texture features frommulti-frames in the video dataset. Temporal augmented data setsgenerally contain more information about the images. The spatialaugmentation approach will employ a bottleneck approach to extractmore information from the background region [ 19]. A canonical CNN structure is used for feature learning. The proposed network uses ﬁve convolutional layers, followed by three fully-connected layers.Response-normalization layers are used for the outputs of the ﬁrst and second convolutional layers. The max-pooling layers are plugged toprocess the outputs of theﬁrst, second andﬁfth convolutional layers. The ReLU (Rectiﬁed Linear Unit) non-linearity is applied to the output of allconvolutional and fully connected layers. To avoid over- ﬁtting, two dropout layers and a soft max function follows the ﬁrst two fully con- nected layers and the output layer, respectively. The LibSVM (Library forSupport Vector Machines) toolkit was used as the classi ﬁer for face anti-spooﬁng [19]. Results displayed a better performance as the spatialscale increased. The proposed method achieved HTERs (Half Total ErrorRates) lower than 5% on two datasets. These results prove the power ofCNN in anti-spooﬁng efforts. These results also indicate the positive ef-fect of background region on face anti-spoo ﬁng task. The anti-spooﬁng system achieves nearly perfect performance on the max scale [ 19]. Such results show the promise in deep learning techniques indiscriminating features.In Menotti et al. [20], the focus in building an anti-spooﬁng system with convolutional networks resided on a combination of two ap-proaches. The proposed architecture, spoofnet, entailed detectingspooﬁng in different biometric modalities. Theﬁrst approach focused on learning the appropriate CNN architecture for each biometric modality.The second approach consisted of learning ﬁlter weights via back-propagation. Architecture optimization (AO) is based upon the CNNarchitectures with stacked feedforward convolutional operations bymeans of hyperparameter optimization. Filter optimization (FO) consistsof learningﬁlter weights via the well-known back-propagation algorithm[20]. The AO is used to adapt the architecture to the biometric samples,as the FO is used to mold important discriminative features for real andfake biometric classiﬁcation. The AO and FO techniques areﬁrst evalu- ated separately, then in combination. The results from this researchstrongly indicate that convolutional networks can be used for robustspooﬁng detection. The AO/FO approach resulted in accuracies of98.93% and 99.38% on separate datasets with multiple biometric mo-dalities [20]. The outstanding classiﬁcation results emphasized the interplay between the architecture andﬁlter optimization approaches for the spooﬁng problem [20].With this review we identiﬁed a few similarities in the mitigationtechniques. The textured based methods are often used in combination inwith deep learning techniques. Deep learning based techniques showedsigniﬁcant improvement from baseline in classi ﬁcation accuracies for spooﬁng attacks. We also found similarities in the preprocessing of bio-metric samples before the use of the techniques. Although we can iden-tify the periocular region as a strength in biometric recognition, gapsexist inﬁnding the distinct features in the periocular region that mitigatespooﬁng attacks.3. MethodologyIn this section, we provide an overview of our proposed methodologyincluding a novel deep learning approach to generate a synthetic(spooﬁng) dataset. We will then address the feature extraction tech-niques for biometric recognition. This research proposes to contributeempiricalﬁndings in improving biometric recognition against presenta-tion attacks using deep learning techniques.3.1. Generating spooﬁng datasetsIn our experiment, we will useGenerative Adversarial Networks (GANs)to generate a synthetic (spooﬁng) dataset. GANs to generate a synthetic(spooﬁng) dataset. GANs are a novel and promising approach to variousproblems that involve generating photorealistic images [ 28]. The basic approach involves two competing neural networks. Fig. 1shows an
Fig. 1.Illustration of competing neural networks, discriminator D and gener-ator G, in Generative Adversarial Networks (GAN) technique.J. Jenkins et al. Array 7 (2020) 100029
3illustration of the GAN approach.One is a discriminator, D, that attempts to determine whether apresented image is authentic or synthetic. The other is a generator, G,that tries to generate images that can successfully deceive the discrimi-nator. The generator begins with a random noise (Z) input and continuesgenerating samples with information from the discriminator. The twonetworks are trained in tandem, competing against the other, and eachhas information about the other’s successes or failures. As the generatorimproves, the images become increasingly realistic to deceive thediscriminator. In unison, the discriminator continually improves itsdiscriminative capability, requiring the generated (spoo ﬁng) images to become more detailed in order to remain undetected. Training is oftenunstable with simple GANs, generating senseless noise for output [ 29, 30].Advancements in GANs stability introduced a class of CNNs calledDeep Convolutional Generative Adversarial Networks (DCGANs) [ 30,36, 37]. The proposed architecture is proven to be more stable in trainingGANs for image generation [30]. The architectural guidelines for stableDCGANs replace all pooling layers, in a typical CNN, with strided con-volutions (discriminator) and fractionally-strided convolutions (gener-ator). Batch normalization is used in both the generator and thediscriminator to normalize the inputs to nonlinearities. All fully con-nected hidden layers are also removed. The use of ReLU and LeakyReLUactivations are also used in all layer of the generator and discriminator,excluding the output.Fig. 2shows a graphical model of the DCGAN ar-chitecture [30] (seeFig. 3).3.2. Genetic and evolutionary feature extraction (GEFE)GEFE, a technique proposed in Shelton et al. [ 13], is a hybrid of a
Genetic and Evolutionary Computations (GEC) to optimize the LBPfeature extractors. A GEC is a general problem solving technique based onsimulated evolution. Aﬁtness function is used to compute the wellness ofa solution and the best solutions procreate to create better solutions. TheGEFE feature extractors (FE) optimize the texture-based feature extrac-tors’dimensions and displacement on the image sample [ 13–16]. The ith candidate feature extractor, fe
i, consists of uniform patches. The candidate is a six-tuple,<X
i,Yi,Wi,Hi,Mi,fi>, whereX i¼{x i,0,xi,1,…, x
i,n-1}represents the x-coordinates of the center of the npossible patches andY
i¼{y i,0,yi,1,…,y i,n-1}represents the y-coordinates of center of the n possible patches. The widths and heights of the npatches are represented byW
i¼{w i,0,wi,1,…,w i,n-1}andH i¼{h i,0,hi,1,…,h i,n-1}. Because the patches are uniform,W
k¼{w k,0,wk,1,…,w k,n-1}is equivalent tow k,0¼ w
k,1,…,w k,n-2,wk,n-1andH k¼{h k,0,hk,1,…,h k,n-1}is equivalent toh k,0¼ h
k,1,…,h k,n-2,hk,n-1, meaning that the widths and heights of every patch
Fig. 2.DCGANs architecture uses a series of four fractionally-strided (5x5)convolutions. Theﬁrst layer of the GAN takes a 100 dimensional uniform noisedistribution as the input. For the discriminator, the last convolution layer isﬂattened and then fed into a single sigmoid output [ 30].
Fig. 3.Image samples from the same subject on iPhone5 inside and outsidedatasets were focused and cropped around the periocular regions. Resulting animage size of 64x64 pixel.
Fig. 4.GEFEþGAN technique presented in psuedocode; The ﬁtness doubles the error value if the closest subject is a DCGAN (spoof) sample.J. Jenkins et al. Array 7 (2020) 100029
4are the same. Uniform sized patches are used because uniform sizedpatches outperformed non-uniform sized patches [ 13].M
i¼{m i,0,mi,1,…, m
i,n-1}represents the masking values for each patch and f irepresents the ﬁtness offe
i. The masking value determines whether a patch is activatedor deactivated, and by extension, whether certain pixels will be processedor not. If a patch is deactivated, by setting m
i¼0, then the sub-histogram will not be considered in the distance measure, and the number of fea-tures to be used is reduced. Otherwise, the patch is activated, with m
i¼1 [13–16].Theﬁtness,ﬁ, is determined by applying fei towards a dataset ofsubject’s biometric images. A subject has a number of images that vary,and these images are separated into a probe set and a gallery set (G). Thefe
iis applied on these images to create FVs, and the FVs in the probe setare compared to all of the FVs in the gallery set using the Manhattandistance measure. The two FVs that have the least Manhattan distanceare considered to be matches. If a probe FV is incorrectly matched with agallery FV, thenfe
iis said to cause an error. Thefe ialso considers how much surface area of the image it covers. The resulting fe
iis the number of errors (
ε) added to the percent of patches not masked out (the sum-mation of all masking values over the total number of patches, (n)). Boththe error and percentage of patches must be reduced so the ﬁtness is being minimized.Traditionally, GEFE is applied on a simulation of a biometric identi-ﬁcation system. To simulate this, K biometric images are collected from Ndifferent individuals. One image per individual will represent a biometricsample being identiﬁed and the K-1 images of all N individuals representsthe database of known individuals. A GEFE FE is applied on every imageand feature vectors (FVs) are created for each. Each FV that is to beidentiﬁed is compared to the database of FVs and the most similar matchis considered the identity of the unknown FV. If there is a mismatch, theﬁtness of the FE is penalized. With this scheme, there are N*(N*(K-1)) comparisons made.Theﬁtness is based on the number of correct matches between in-stances, so the bestﬁtness occurs when discriminating features areextracted from each instance. The more discriminating the features, themore likely that the resulting FVs from the same individuals will have abetter similarity score than FVs from different individuals. The GEFE
many
scheme, proposed in Jenkins et al. [ 15], compares every instance in a dataset into every other instance. The instances involved do not changefrom the dataset used by the traditional GEFE method, but the number ofcomparisons is now(N*K)*((N*K)-1). The number of comparisons is now greater, thus allowing for a more complex environment to evolveFEs in. Though the number of instances has not changed, the increase ofinstance comparisons results in FEs that have a superior identi ﬁcation performance.The proposed methodology for this work is to use the DCGANarchitecture as a data augmentation technique to generate a spoo ﬁng dataset. The generated spooﬁng dataset will be tested for its quality andused in the GEFE technique to generate improved feature extractors thatcan mitigate presentation attacks. Similar to the combination of BSIF and2D Ceptrum [24], the GEFE technique has been used previously tooptimize LBP FEs to mitigate replay attacks with high biometric recog-nition accuracy. The proposed GEFE technique will be optimized tomitigate synthetic images. The GEFE FEs also will provide insight onwhich biometric features have the higher discriminative capabilities asGEFE optimizes the dimensions and displacement of FEs on the biometricsample.4. ExperimentThis research applies the DCGAN and GEFE techniques to periocularimages from the BIPLab MICHE-I dataset [ 32]. The periocular images from the BIPLab MICHE-I datasets are taken from the Apple iPhone5 andSamsung GalaxyS4 smartphones. From this dataset, we form two subsets:the iPhone5 dataset and the Galaxy dataset. For the iPhone5 datasets,images were taken from the FaceTime HD (front) camera of 1.2 mega-pixels (MP). For the GalaxyS4 datasets, images were taken from theCMOS (front) camera of 2.0 MP. For the BIPLab dataset, 25 subjects wereused with 8 periocular samples per subject. Image samples consisted of 4indoors using artiﬁcial light and 4 taken outdoors using natural light. Allimages from the BIPLab dataset were, taken 10 cm away from the device.Comparisons among image data were done using the approach describedin the Methodology section where there are Kimages collected fromN different individuals.Originally, images from iPhone5&GalaxyS4 datasets had a resolu- tion of 480x640 pixel or greater. For ease of image generation, classi ﬁ- cation, and recognition, all images were resized and cropped to 64x64pixel; similar to protocols in previous research [ 17,18]. The images were zoomed in to ensure coverage of the periocular region. In cropping theimage, superﬂuous noise, such as the shirt textures, hair textures, andother background noises are removed. Cropping also allows ﬂexibility when images are not taken exactly as directed. The 64x64 pixel size alsoworks best for this experiment as the DCGAN technique generates four64x64 pixel sized images as the output. All four samples for each subjectare used for input comparison (discriminator) in the DCGAN technique.This DCGAN is trained for 2500 epochs. The DCGAN techniques will usea learning rate of 0.0005. This process generates four unique syntheticsamples for each subject. These generate samples are placed in the galleryset of the original datasets to test the technique ’s accuracy. First we test the quality of the images generated by the DCGANtechnique. The spooﬁng samples will be added to the gallery set of asimple LBP-3x3 identiﬁcation system. An LBP 3x3 system means thatimages are split into three rows and three columns of even sized regionswhere features are extracted from each region and concatenated to formthe feature vector for an image. The identi ﬁcation and veriﬁcation ac- curacies will be recorded and compared, that with the original datasetand the spooﬁng datasets as well. These results will show us the quality ofDCGAN generated samples and the power of Generative AdversarialNetworks. Next, we will use the spooﬁng datasets for training with the GEFE technique to generate improved feature extractors. During thisprocess, the GEFE technique will not only be looking for distinct featuresfor periocular recognition, but also features to mitigate presentation at-tacks. GEFE uses the Estimation of Distribution Algorithm (EDA) [ 31]a s the GEC with 250 function evaluations with a population size of 20candidates FEs.All subject samples are used for input in DCGAN technique togenerate corresponding spooﬁng datasets. This DCGAN technique trainson a GPU for 2500 epochs. The DCGAN techniques will use a learningrate of 0.0005. This DCGAN technique generates equal unique syntheticsamples for each subject. These generate samples are then placed in thegallery set of the original datasets to test the technique ’s accuracy; doubling the original dataset in size.
Fig. 5.Image samples from original Apple iPhone-inside (real) dataset and theDCGAN generated (counterpart) datasets for spoo ﬁng.J. Jenkins et al. Array 7 (2020) 100029
5For the GEFE experiments, we will use a 60/40 training/testing setsplit. GEFE
many is applied on the original datasets for a baseline accuracy;referred as GEFE in our experiments. The GEFE technique is then appliedon the spooﬁng datasets with the DCGAN generated samples added to thegallery set; referred to as GEFEþGAN in our experiments. GEFEþGAN is optimized for DCGAN generated presentation attacks. The originalGEFE (GEFE
many) technique applies an equal value of error for all non-matching subjects. In the GEFEþGAN2 technique, the error value isdouble for matching a subject with a DCGAN generated sample duringthe evolutionary process. By doubling the error value for matching sub-jects with a DCGAN generated samples, the evolved feature extractors areless likely to match a presentation attack over real subjects. This is pro-posed to optimize the anti-spooﬁngﬁtness of the generated feature ex- tractors. This is also shown in the pseudocode below.The identiﬁcation performance will be presented using CumulativeMatch Characteristic (CMC) curves. As each probe sample is comparedagainst all gallery samples, the resulting scores are sorted and ranked.This determines the True Positive Identiﬁcation Rate (TPIR); the rank at which a true match occurs. CMC curves plot the TPIR against ranks. Theveriﬁcation performance will be presented using Receiver OperatorCharacteristic (ROC) curves. As each probe sample is compared againstall gallery samples, the True Accept Rate (TAR) and False Accept Rate(FAR) of subjects are calculated at multiple thresholds. TAR is the mea-sure of likelihood that the biometric security system will correctly ver-iﬁes a true claim of identity. The FAR is the measure of likelihood asystem incorrectly accepts an access attempt by an unauthorized user.The TAR and FAR are stated as the ratio of the number of true and falseacceptances divided by the number of identi ﬁcation attempts.5. Results and discussionThis research applied the LBP feature extraction technique on data-sets with samples generated from the DCGAN (see Fig. 4). For the LBP identiﬁcation system, theﬁrst sample of each subject is placed into aprobe set. All other samples are placed in a gallery set. The systemcompares each subject’s probe sample to the gallery set, containing theother samples of the current subject as well as the samples of othersubjects in the system. For calculating the identi ﬁcationﬁtness for the LBP histograms, the Manhattan distance metric was used.Figs. 6–13, below, shows the Cumulative Match Characteristic (CMC)curves and the Receiver Operator Characteristic (ROC) curves on theiPhone5 datasets (seeFig. 5). The CMC curve plots the identiﬁcation accuracies of each method, while the ROC curve plots the veri ﬁcation accuracies. The CMC curve plots the rank at which a true match occursThe ROC curve plots the True Accept Rate (TAR) and False Accept Rate(FAR) of subjects calculated at multiple thresholds.Fig. 6shows the CMC results of GEFE generated feature extractors onthe iPhone5-inside dataset. GEFEþGAN is theﬁrst technique to reach 100% at Rank 4, followed by GEFE at Rank 5. LBP reaches 100% at Rank14.Fig. 7shows the ROC results (Loged Scaled) of the GEFE generatedfeature extractors on the iPhone5-inside dataset. LBP has a TAR of 0.8 ata FAR of 0.04. GEFE has a TAR of 0.811 at a FAR of 0.0402. GEFE þGAN has a TAR of 0.757 at a FAR of 0.04.
Fig. 8shows the CMC results of GEFE generated feature extractors onthe iPhone5-inside dataset with DCGAN samples added. GEFE þGAN is theﬁrst technique to reach 100% at Rank 6, followed by GEFE at Rank 7.LBP reaches 100% at Rank 14.Fig. 9shows the ROC results (Log Scaled)
of the GEFE generated feature extractors on the iPhone5-inside datasetwith DCGAN samples added. LBP has a TAR of 0.787 at a FAR of 0.0452.GEFE has a TAR of 0.811 at a FAR of 0.0402. GEFE þGAN has a TAR of 0.757 at a FAR of 0.0461. Soon after, GEFE has a TAR of 0.892 at a FAR of
Fig. 6.CMC results for LBP and GEFE techniques on the iPhone5-inside dataset.
Fig. 7.ROC results for LBP and GEFE techniques on the iPhone5-inside dataset.
Fig. 8.CMC results for LBP and GEFE techniques on the iPhone5-inside datasetwith DCGAN samples added.
Fig. 9.ROC results for LBP and GEFE techniques on the iPhone5-inside datasetwith DCGAN samples added.
Fig. 10.CMC results for LBP and GEFE techniques on the iPhone5-outside dataset.J. Jenkins et al. Array 7 (2020) 100029
60.138 while GEFEþGAN has a TAR of 0.865 at a FAR of 0.143.Fig. 10shows the CMC results of GEFE generated feature extractorson the iPhone5-outside dataset. GEFE is the ﬁrst technique to reach 100% at Rank 4, followed by GEFEþGAN at Rank 5. LBP doesn’t reaches 100% until Rank 22.Fig. 11shows the ROC results (Log Scaled) of the GEFEgenerated feature extractors on the iPhone5-outside dataset. LBP has aTAR of 0.947 at a FAR of 0.0194. GEFE has a TAR of 0.824 at a FAR of0.0225. GEFEþGAN has a TAR of 0.797 at a FAR of 0.0231.Fig. 12shows the CMC results of GEFE generated feature extractorson the iPhone5-outside dataset with DCGAN samples added. At Rank 5,GEFE reach 96% as GEFEþGAN reaches 92%. GEFE is theﬁrst technique to reach 100% at Rank 8, followed by GEFE þGAN at Rank 11. LBP doesn’t reaches 100% until Rank 25.Fig. 14shows the ROC results (Log Scaled) of the GEFE generated feature extractors on the iPhone5-outsidedataset with DCGAN samples added. LBP has a TAR of 0.950 at a FAR of0.0392. GEFE has a TAR of 0.824 at a FAR of 0.0450. GEFE þGAN has a TAR of 0.851 at a FAR of 0.0493.Figs. 14–20, below, shows the Cumulative Match Characteristic(CMC) curves and the Receiver Operator Characteristic (ROC) curves onthe GalaxyS4 datasets. The CMC curve plots the identi ﬁcation accuracies of each method, while the ROC curve plots the veri ﬁcation accuracies. The CMC curve plots the rank at which a true match occurs The ROCcurve plots the True Accept Rate (TAR) and False Accept Rate (FAR) ofsubjects calculated at multiple thresholds.Fig. 14shows the CMC results of the GEFE generated feature ex-tractors on the GalaxyS4-inside dataset. GEFE þGAN is theﬁrsttechnique to reach 100% at Rank 8. All other techniques are 96% at Rank8. The GEFE technique reaches 100% at Rank 14, followed by LBP atRank 15.Fig. 15shows the ROC results (Log Scaled) of the GEFEgenerated feature extractors on the GalaxyS4-inside dataset. LBP has aTAR of 0.7733 at a FAR of 0.08611. GEFE has a TAR of 0.8378 at FAR of0.0895. GEFEþGAN has a TAR of 0.8108 at a FAR of 0.0957. The ROCcurves begin to alter as GEFEþGAN technique surpasses the GEFEtechniques. GEFEþGAN technique has a TAR of 0.8649 at a FAR of0.128. The GEFE technique has a TAR of 0.8514 at a FAR of 0.1402.Fig. 16shows the CMC results of the GEFE generated feature ex-tractors on the GalaxyS4-inside dataset with DCGAN (synthetic) samplesadded. GEFEþGAN is theﬁrst technique to reach 100% at Rank 17. Thisis followed by the GEFE techniques reaching 100% at Rank 20 and LBP atRank 25. The GEFE technique is theﬁrst reach 92% at Rank 5, followed by GEFEþGAN at Rank 11.Fig. 17shows the ROC (Log Scaled) of the GEFE generated feature extractors on the GalaxyS4-inside dataset withDCGAN (synthetic) samples added. LBP has a TAR of 0.82667 at a FAR of0.1333. GEFE has a TAR of 0.8514 at a FAR of 0.1235. GEFEþGAN has a TAR of 0.7838 at a FAR of 0.130.Fig. 18shows the CMC results of the GEFE generated feature ex-tractors on the GalaxyS4-outside dataset. GEFE þGAN is theﬁrst tech- nique to reach 100% at Rank 2, followed by GEFE at Rank 3. The LBPtechnique reaches 100% at Rank 19. Fig. 19shows the ROC results (Loged Scaled) of the GEFE generated feature extractors on the GalaxyS4-outside dataset. LBP has a TAR of 0.8133 at a FAR of 0.03944. GEFE has aTAR of 0.8514 at a FAR of 0.0376. GEFEþGAN has a TAR of 0.8649 at a FAR of 0.0355.Fig. 20shows the CMC results of the GEFE generated feature ex-tractors on the GalaxyS4-outside dataset with DCGAN (synthetic) sam-ples added. Both GEFE and GEFEþGAN reach 100% at Rank 5. LBP reach 96% at Rank 13, but doesn’t reach 100% until Rank 23.Fig. 21 shows the ROC (Log Scaled) of the GEFE generated feature extractors onthe GalaxyS4-outside dataset with DCGAN (synthetic) samples added.LBP has a TAR of 0.8 at a FAR of 0.0373. GEFE has a TAR of 0.8243 at aFAR of 0.0355. GEFEþGAN has a TAR of 0.7973 at a FAR of 0.0337 (seeFig. 22).The GEFE (GEFE
many) and GEFEþGAN techniques are applied to a total of eight datasets (real and spooﬁng). Both GEFE techniques used an EDA as the GEC, with 250 function evaluations, and a population size of20 candidates FEs. The identiﬁcation performances are presented usingCumulative Match Characteristic (CMC) curves. CMC curves plot theTrue Positive Identiﬁcation Rate (TPIR), the rank at which a true matchoccurs.Table 1shows a summary of the identiﬁcation accuracies for GEFE experiments on iPhone5 datasets. For the iPhone5-inside datasets,GEFE reaches 100% at Rank 5. For the iPhone5-inside spoo ﬁng datasets, 100% is reached at Rank 7. Also for the iPhone5-inside dataset, GEFE þ GAN reaches 100% at Rank 4, and Rank 6 on spoo ﬁng dataset. For the iPhone5-outside datasets, GEFE reaches 100% at Rank 4, and Rank 8 onthe spooﬁng dataset. Also for the iPhone5-outside dataset, GEFE þGAN reaches a 100% at Rank 5. GEFEþGAN reaches 100% on iPhone5- outside spooﬁng dataset at Rank 11.Table 2shows a summary of the identiﬁcation accuracies for GEFE
Fig. 11.ROC results for LBP and GEFE techniques on the iPhone5-outside dataset.
Fig. 12.CMC results for LBP and GEFE techniques on the iPhone5-outsidedataset with DCGAN samples added.
Fig. 13.ROC results for LBP and GEFE techniques on the iPhone5-outsidedataset with DCGAN samples added.
Fig. 14.CMC results for LBP and GEFE techniques on the GalaxyS4-inside dataset.J. Jenkins et al. Array 7 (2020) 100029
7experiments on GalaxyS4 datasets. For the GalaxyS4-inside datasets,GEFE reaches 100% at Rank 14 (96% at Rank 8). For the iPhone5-insidespooﬁng datasets, 100% is reached at Rank 20 (92% at Rank 5). Also forthe GalaxyS4-inside dataset, GEFEþGAN reaches 100% at Rank 8, and Rank 17 on spooﬁng dataset. For the GalaxyS4-outside datasets, GEFEreaches 100% at Rank 3, and Rank 5 on the spoo ﬁng dataset. Also for the GalaxyS4-outside dataset, GEFEþGAN reaches a 100% at Rank 2. GEFEþGAN reaches 100% on GalaxyS4-outside spoo ﬁng dataset at Rank 5. The GEFEþGAN technique showed promising results with higher identiﬁcation accuracies than the GEFE technique on six out of eight
Fig. 15.ROC results for LBP and GEFE techniques on the GalaxyS4-inside dataset.
Fig. 16.CMC results for LBP and GEFE techniques on the GalaxyS4-insidedataset with DCGAN samples added.
Fig. 17.ROC results for LBP and GEFE techniques on the GalaxyS4-insidedataset with DCGAN samples added.
Fig. 18.CMC results for LBP and GEFE techniques on the GalaxyS4-outside dataset.
Fig. 19.ROC results for LBP and GEFE techniques on the GalaxyS4-outside dataset.
Fig. 20.CMC results for LBP and GEFE techniques on the GalaxyS4-outsidedataset with DCGAN samples added.
Fig. 21.ROC results for LBP and GEFE techniques on the GalaxyS4-outside dataset.
Fig. 22.GEFE (left) and GEFEþGAN (right) generated feature extractors (FEs) illustrated on biometric samples.J. Jenkins et al. Array 7 (2020) 100029
8datasets. Veriﬁcation results are also comparable on all datasets. Theresults shows that the GEFEþGAN technique optimizes the anti-spooﬁng ﬁtness of the generated feature extractors. (see Fig. 23). For the GEFE FEs, the experimental results show that the majority ofthe eyes and eyelids are chosen for extraction. The FEs tends to favor theupper and lower lid folds. The suprabrow and upper eyelid are coveredby the FEs. The FEs also covers the most of the nasal area. For the GEFE þ GAN FEs, one could extrapolate from the displacement that the canthi(eye corners) are keys to identiﬁcation and presentation attack mitiga-tion. The generated FEs favor the lateral and medial canthus over usingthe entire eye area. The malar fold is the de ﬁned groove extending from the lateral canthus. The nasojungal crease is also a de ﬁned groove in the lower lid fold, between the medial canthus and the nose [ 34]. The nasofacial sulcus is covered by both GEFE FEs, but more signi ﬁcantly on the outside dataset; instead of the entire nose. It seems the natural light inthe iPhone5-outside datasets exposes the periocular features better fordiscriminative capabilities. These areas could all be outlining featuresused to mitigate presentation attacks with synthetic images.6. Conclusion and future reasearchIn this paper, we apply a data augmentation technique with GANs togenerate comparative synthetic (spooﬁng) dataset. We trained the syn- thetic images using GAN and created spoo ﬁng datasets. The GEFE tech- nique is then used in combination with the GANs to generate improvedanti-spooﬁng feature extractors optimized in an attempt to mitigatepresentation attacks. The combination of GEFE and GANs is used toidentify those discriminative biometric features used to mitigate syn-thetic presentation attacks. The GEFEþGAN technique outperforms the LBP and GEFE techniques alone in overall identi ﬁcation and veriﬁcation results on spooﬁng datasets.To improve upon the GEFEþGAN technique, we would continue byapplying experiments on images taken from the UBI periocular recog-nition datasets[38]. The UBI dataset uses images taken from a profes-sional camera, the Canon EOS 5D (12.8 MP). The UBI dataset also has 15periocular samples per subject. With an increase of samples, the DCGANgenerated samples should be more accurate. With higher quality syn-thetic samples, the GEFE experiments should increase the quality ofdiscriminative feature extraction.CRediT authorship contribution statementJohn Jenkins:Writing - original draft, Writing - review&editing, Software, Investigation.Kaushik Roy:Supervision, Writing - original draft, Conceptualization, Funding acquisition. Joseph Shelton:Writing - original draft, Writing - review&editing, Conceptualization.AcknowledgmentThis research is based upon work supported by the Army ResearchOfﬁce (Contract No. W911NF-15-1-0524) and National Science Foun-dation (Award Number:1900187). Any opinions, ﬁndings, and conclu- sions or recommendations expressed in this material are those of theauthor(s) and do not necessarily reﬂect the views of Army Research Of-ﬁce and National Science Foundation.References
[1] EyeLock debutsﬁrst laptop with embedded EyeLock ID Iris authenticationtechnology - eyeLock. Retrieved April 1, 2018.[2]Ratha N, Connell J, Bolle R. Enhancing security and privacy in biometrics-basedauthentication systems. IBM Syst. J. IBM Systems Journal 2001:614 –34. [3]Hassan A, Bhram A, Saleh A. Applying a new functional model to improve thesecurity of biometric systems. In: 3rd international conference on communicationsoftware and networks; 2011 . [4]Jain A k, Ross A, Prabhakar S. An introduction to biometric recognition. IEEETransactions on Circuits and for Video Technology 2004:4 –20. [5]Hao Zhuo, Yu Nenghai. A security enhanced remote password authenticationscheme using smart card. In: Second international symposium on data, privacy, andE-commerce; 2010. p. 56–60. [6]Lamport L. Password authentication with insecure communication. Commun ACM1981;24(11):770–2.[7] Robert W Frischholz, Dieckmann Ulrich. BioID: a multimodal biometricidentiﬁcation system,". Computer Feb. 2000;33(2):64 –8.https://doi.org/10.1109/ 2.820041.[8]O’Gorman L.“Comparing passwords, tokens, and biometrics for userauthentication,”. Proc IEEE Dec. 2003;91(12):2019 –40. [13] Shelton, J., Dozier, G. V., Bryant, K. S., Small, L., Adams, J., Popplewell, K.,... & Ricanek, K. (2011, April). Comparison of genetic-based feature extraction methodsfor facial recognition. In MAICS (pp. 216 –220). [14]Alford A, Steed C, Jeffrey M, Sweet D, Shelton J, Small L, Kelly JC. Genetic & Evolutionary Biometrics: hybrid feature selection &weighting for a multi-modal biometric system. IEEE; 2012. p. 1 –8. [15] Jenkins, J., Shelton, J.,&Roy, K. (2016, October). A comparison of genetic basedextraction methods for periocular recognition. In Information Communication andManagement (ICICM), International Conference on (pp. 309 –313). IEEE. [16]Shelton J, Jenkins J, Roy K. Extending disposable feature templates for mitigatingreplay attacks. Int J Inf Priv Secur Integr 2017;3(2):96 –116. [17]Gragnaniello D, Poggi G, Sansone C, Verdoliva L. An investigation of localdescriptors for biometric spooﬁng detection. IEEE Trans Inf Forensics Secur 2015; 10(4):849–63.[18]Bharati A, Singh R, Vatsa M, Bowyer KW. Detecting facial retouching usingsupervised deep learning. IEEE Trans Inf Forensics Secur 2016;11(9):1903 –13.
Fig. 23.GEFE (red) and GEFEþGAN (blue) generated feature extractors (FEs) transposed a detailed illustration of the periocular region. The illustration de-tails the surface anatomy [34].
Table 1Summary of Identiﬁcation Accuracies for. GEFE Experiments on iPhone5datasets.
Dataset Technique CMC Rank at 100%Real SpoofiPhone5-inside LBP 14 14 GEFE 5 7GEFEþGAN 4 6 iPhone5-outside LBP 22 25 GEFE 4 8GEFEþGAN 5 11
Table 2Summary of Identiﬁcation Accuracies for. GEFE Experiments on GalaxyS4datasets.
Dataset Technique CMC Rank at 100%Real SpoofGalaxyS4-inside LBP 15 25 GEFE 14 20GEFEþGAN 8 17 GalaxyS4-outside LBP 19 23 GEFE 3 5GEFEþGAN 2 5J. Jenkins et al. Array 7 (2020) 100029
9[19]Yang J, Lei Z, Li S. Learn convolutional neural network for face anti-spoo ﬁng. 2014. arXiv preprint arXiv:1408.5601 . [20]Menotti D, Chiachia G, Pinto A, Schwartz WR, Pedrini H, Falc ~ao AX, Rocha A. Deep representations for iris, face, and ﬁngerprint spooﬁng detection. IEEE Trans Inf Forensics Secur 2015;10(4):864 –79. [22]Rodrigues RN, Kamat N, Govindaraju V. Evaluation of biometric spoo ﬁng in a multimodal system. 2010, September. In: Biometrics: theory applications andsystems (BTAS); 2010. Fourth IEEE International Conference on (pp. 1 –5). IEEE. [23]Chingovska I, Anjos A, Marcel S. On the effectiveness of local binary patterns in faceanti-spooﬁng. 2012, September. In: Biometrics special interest group (BIOSIG);2012. BIOSIG-Proceedings of the International Conference of the (pp. 1 –7). IEEE. [24]Raghavendra R, Busch C. Presentation attack detection algorithm for face and irisbiometrics. 2014, September. In: Signal processing conference (EUSIPCO); 2014.Proceedings of the 22nd European (pp. 1387 –1391). IEEE. [25]Raghavendra R, Venkatesh S, Raja KB, Busch C. Transferable deep convolutionalneural network features forﬁngervein presentation attack detection. 2017, April. In: Biometrics and forensics (IWBF); 2017. 5th International Workshop on (pp.1–5). IEEE.[26]Ojala T, Pietikainen M, Maenpaa T. Multiresolution gray-scale and rotationinvariant texture classiﬁcation with local binary patterns. IEEE Trans Pattern AnalMach Intell 2002;24(7):971–87. [27]Ahonen T, Hadid A, Pietikainen M. Face description with local binary patterns:application to face recognition. IEEE Trans Pattern Anal Mach Intell 2006;28(12):2037–41.[28]Goodfellow I, Pouget-Abadie J, Mirza M, Xu B, Warde-Farley D, Ozair S, Bengio Y.Generative adversarial nets. Adv Neural Inf Process Syst 2014:2672 –80. [29]Ledig C, Theis L, Husz/C19ar F, Caballero J, Cunningham A, Acosta A, Shi W. Photo-realistic single image super-resolution using a generative adversarial network.2016. arXiv preprint arXiv:1609.04802 . [30]Radford A, Metz L, Chintala S. Unsupervised representation learning with deepconvolutional generative adversarial networks. 2015. arXiv preprint arXiv:1511.06434.[31]Larranaga P. A review on estimation of distribution algorithms. In: Estimation ofdistribution algorithms. Boston: Springer; 2002. p. 57 –100. [32]De Marsico M, Nappi M, Riccio D, Wechsler H. “Mobile Iris Challenge Evaluation (MICHE), biometric iris dataset protocols, ”. Pattern Recogn Lett 2015:17 –
23. [34]Bowman PH, Fosko SW, Hartstein ME. Periocular reconstruction. Semin Cutan MedSurg 2003;22(4):263–72. [35]Miller Arthur I. 10 ian goodfellow ’s generative adversarial networks: AI learns to imagine,". The Artist in the Machine: The World of AI-Powered Creativity, MITP2019:87–98.[36]Chesney B, Citron D. Deep fakes: a looming challenge for privacy, democracy, andnational security. Calif Law Rev 2019;107:1753 . [37]Korshunov P, Marcel S. Deepfakes: a new threat to face recognition? assessment anddetection. 2018. arXiv preprint arXiv:1812.08685 . [38]Padole C, Proença H. Periocular recognition: analysis of performance degradationfactors, in proceedings of theﬁfth IAPR/IEEE international conference on biometrics–ICB 2012. 2012. New Delhi, India, March 30 .J. Jenkins et al. Array 7 (2020) 100029
10