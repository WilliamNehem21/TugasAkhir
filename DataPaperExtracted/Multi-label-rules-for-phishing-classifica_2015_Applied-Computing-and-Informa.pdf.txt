ORIGINAL ARTICLE
Multi-label rules for phishingclassiﬁcation
Neda Abdelhamid*
Computing and Informatics Department, De Montfort University, Leicester, UKReceived 3 January 2014; revised 13 May 2014; accepted 3 July 2014
Available online 15 July 2014
KEYWORDSAssociative rule;Associative classiﬁca-tion;Data mining;Website phishing;On-line securityAbstract Generating multi-label rules in associative classiﬁcation (AC) fromsingle label data sets is considered a challenging task making the number of exist-ing algorithms for this task rare. Current AC algorithms produce only the largestfrequency class connected with a rule in the training data set and discard all otherclasses even though these classes have data representation with the rule’s body.In this paper, we deal with the above problem by proposing an AC algorithmcalled Enhanced Multi-label Classiﬁers based Associative Classiﬁcation(eMCAC). This algorithm discovers rules associated with a set of classes fromsingle label data that other current AC algorithms are unable to induce. Further-more, eMCAC minimises the number of extracted rules using a classiﬁer buildingmethod. The proposed algorithm has been tested on a real world applicationdata set related to website phishing and the results reveal that eMCAC’s accu-racy is highly competitive if contrasted with other known AC and classic classi-ﬁcation algorithms in data mining. Lastly, the experimental results show that ouralgorithm is able to derive new rules from the phishing data sets that end-userscan exploit in decision making.
ª2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.
*Tel.: +44 (0)116 2 50 60 70.E-mail address:P09050665@myemail.dmu.ac.uk. Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2015) 11, 29–46
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2014.07.002
2210-8327ª2014 Production and hosting by Elsevier B.V. on behalf of King Saud University.1. Introduction
Generally and according to (Tsoumakas and Katakis, 2007), there are two types of classiﬁcation problems, these are termed as single label and multi-label. In a singlelabel classiﬁcation, each training case in the input data is associated with only oneclass. In cases where the input data set contains just two class labels, the problem iscalled binary classiﬁcation. However, if more than two classes are available, theproblem is named multi-class classiﬁcation. The majority of the research worksconducted in classiﬁcation data mining are concerned with single label classiﬁca-tion, i.e. (Li et al., 2008; Chien and chen, 2010; Wang et al., 2011). However, domain applications like medical diagnoses, website phishing detection, text cate-gorisation (TC) and bioinformatics may necessitate the production of multiplelabel rules. This is since there is a class overlapping among the training cases inthese applications data. Meaning a set of attribute values in the rule’s body maylink with more than one class in the data set and thus producing all these classesin the rule’s consequent (right hand side). It is advantageous to generate the otherclasses besides the largest frequency class with the rule’s body since they containvaluable information having a sufﬁcient representation in the data set.
In the last few years, a learning strategy which applies the association rule inclassiﬁcation data called associative classiﬁcation (AC) emerged ( Thabtah et al., 2010; Thabtah et al., 2011; Wang et al., 2011). Most AC algorithms like MAC(Abdelhamid et al., 2012), CMAR (Li et al., 2001) and others usually apply anassociation rule technique to discover the rules, and then ﬁlter out the rules toinclude only those which their consequent is the class attribute. Experimentalresearch works (Jabbar et al., 2013; Jabez, 2011) indicated that AC algorithms fre-quently build more accurate classiﬁers than classic classiﬁcation approaches suchas the probabilistic approach (Witten and Frank, 2002), decision tree (Quinlan, 1993) and rule induction (Cohen, 1995). The algorithm proposed in this articleis part of the AC family.
Limited research attempts in AC have been conducted to produce rules withmore than one class, i.e. Lazy AC (CLAC) (Veloso et al., 2011) and Multi-label Multi-class AC (MMAC) (Thabtah et al., 2004). The rest of the existing AC algo-rithms is unable to deal with discovering multi-label rules from single label datasets, and normally derive only the largest frequency class connected with the attri-bute value(s) and ignore all other classes even if these classes have large frequen-cies with the attribute value(s). For instance, this condition occurs if an attributevalue such as <A> is associated with two class labels (cl
1,c l2) in different places (examples) within the training data set with frequencies equal to 44 and 45 respec-tively. A typical AC algorithm like CBA will only take on class ‘‘cl
2’’ simply because it has a larger frequency than cl
1with <A> and ignores class cl 1even if this class is statistically signiﬁcant with <A>. This surely makes the selectionof (<A>, cl
2) questionable. In the proposed algorithm we pick the two classlabels and construct a multi-label rule rather than removing class cl
1. This enables30 N. Abdelhamidthe decision maker to obtain knowledge missed by current AC algorithms. Theprimary motivation of this paper is to deal with the problem of generatingmulti-label rules via AC from single label data sets. In other words, we intendto discover all class labels associated with the attribute values bringing up noveland useful knowledge normally missed by current algorithms.
In this paper, a new multi-label rule based AC called Enhanced Multi-labelClassiﬁers based Associative Classiﬁcation (eMCAC) is proposed. This algorithmextracts from data sets not only rules with the most obvious class but rules that areassociated with a ranked set of class labels. When an attribute value in a trainingdata set is connected to more than one class in different locations with certain fre-quencies, the proposed algorithm extracts and sorts all of them in the rule conse-quent according to their frequencies. Thus, later in the prediction step, there canbe more alternatives (classes) when the rule is used in predicting the class for a testcase.
The proposed algorithm generates rules from the complete training data set andwithout performing recursive learning as the MMAC algorithm, which requireslearning from parts of the training data set. This means MMAC ends up with sev-eral single label classiﬁers that are then merged in a separate step to make the ﬁnalclassiﬁer. Another main distinction between eMCAC and MMAC is that our algo-rithm’s way of computing the conﬁdence and the support for a multi-label rule isbased on the average conﬁdence and support values of all (Items, Classes) con-tained within the rule, whereas, MMAC assigns the top ranked class conﬁdenceand support to the multi-label rule.
The proposed algorithm employs a rule pruning method that considers a rulepart of the classiﬁer if its body is contained within the training example. This isdone without considering the class similarity of both the evaluated rule and thetraining case, thus ensuring a high rule coverage with respect to the training casesand consequently a smaller number of extracted rules. Section 4demonstrates the applicability of the proposed algorithm on real world application data related tophishing that was collected from phishy and legitimate websites ( www. phishtank.com)(www.millersmiles.co.uk).
The AC problem, its related basic concepts and relevant literature are given inSection2. Section3surveys common approaches in the literature and the pro-posed algorithm is presented in Section4. Experimentations and result analysisare demonstrated in Section5, and lastly conclusions are given in Section6.
2. The problem
The general description of the problem besides main deﬁnitions has been summa-rised by (Abdelhamid et al., 2012).
LetTdenote the domain of the training cases andCbe a list of classes. Each caseteTmay be given a classc1,c2,...,c
kforc ieC, and is represented as a pair(t,(c
k)) wherec kis a class fromCassociated with the casetin the training data.Multi-label rules for phishing classiﬁcation 31LetHdenote the set of classiﬁers forTﬁCwhere each caseteTis given a class and the goal is to ﬁnd a classiﬁerheHthat maximises the probability thath(t)=c for each test case. In our algorithm, and in case of multi-label data we assume thatit is transformed to a single label data format after applying the copy transforma-tion method (Section 3.1). The proposed algorithm deals with the single label dataformat only. The multi-label data set is displayed inTable 1.Table 2denotes the data obtained after applying the copy transformation method in Table 1and before the mining process starts.
The main deﬁnitions related to the AC problem that the proposed algorithmutilises are given below:
For the training data setTwithmattributesA 1,A2,...,A mandClis a set of classes,
Deﬁnition 1:An attribute value set (AttValSet) can be described as a set of dis-joint attribute values contained in a training case, denoted < ( A
i1, a
i1),...,(A ik,aik)>.Deﬁnition 2:A ruleris of the form <AttValSet,cl>, whereceCis the class. Deﬁnition 3:The actual occurrence (ActOccr)o frinTis the number of cases inTthat matchr’santecedent.Deﬁnition 4:The support count (SuppCount)o fris the number of cases inT that matchesr’santecedent, and belong to a classcl
i. Deﬁnition 5: A rulerpasses the user minimum support threshold (MinSupp)i f forr, theSuppCount(r)/|T|PMinSupp, where |T| is the number ofcases inT.Deﬁnition 6: A rulerpasses the user minimum conﬁdence threshold (MinConf) ifSuppCount(r)/ActOccr(r)PMinConf.Deﬁnition 7: A single label rule is represented as:Antecedentﬁc, where ante-cedent is anAttributeValueSetand the consequence is a class.Deﬁnition 8: A multi-label rule is represented as:Antecedentﬁc
i1/C218ci2...c i3, where antecedent is anAttributeValueSetand the consequence isa set of class labels.
Table 1Sample of a multi-label data set.TID Attribute
1 Attribute 2 Attribute 3 Class label 1y z b c l
1,c l2
2 xabc l l,c l3
3 yadc l 1
4 xadc l 3
5 yabc l 3,c l1
6 yadc l 1,c l2,c l3
7 xkbc l 3
8 xkdc l 3,c l1
9 xkbc l 2,c l332 N. Abdelhamid3. Literature review
3.1. Data transformation (optional)
Several data transformation methods exist in the literature to convert multi-labeldata into one or more single label data. To demonstrate these data transformationmethods we use the data set ofTable 1which consists of nine training cases thatbelong to the following class set {cl
1,c l2,c l3}. We summarise some of the commonmethods from (Tsoumakas and Katakis, 2007) and use our own example to fur-ther simplify them.
The ﬁrst data transformation method simply removes any multi-label case fromthe training data set. Therefore, fromTable 1, cases located in TID (1,2,5,6,8,9)are discarded. Another data transformation method selects one class of each caseeither arbitrarily or subjectively by the domain expert. So from Table 1a single associated class for each of the multi-label cases may be picked. Another morerealistic method transforms every multi-label case into a single label one by replac-ing the multi-label case (x
i,Yi) with |Y i| cases. After that a number of methodscould be applied such as copy-weight which associates a weight of (1/|Y
i|) to each of the transformed cases.Table 2shows the copy transformation method afterapplying it againstTable 1, which our algorithm employs when the input dataare multi-label .
Finally, a common data transformation method used in image classiﬁcationthat derives a single label binary classiﬁer for every class in the class set is calledBinary Relevance (BR) (Boutell et al., 2003). It transforms the original multi-labeldata set into |L| data sets which contain all the cases. This method gives a positiveindicator for a class if it is associated with a case in the training data set and aTable 2The transformed multi-label data of Table 1 after processing.Attribute
1 Attribute 2 Attribute 3 Class label yzbc l
1
yzbc l 2
xabc l l
xabc l 3
yadc l 1
xadc l 3
yabc l 3
yabc l 1
yadc l 1
yadc l 2
yadc l 3
xkbc l 3
xkdc l 3
xkdc l 1
xkbc l 2
xkbc l 3Multi-label rules for phishing classiﬁcation 33negative indicator otherwise. For classifying test data, BR outputs the union of allclass labels that are predicted by the |L| classiﬁers.
3.2. Related works
The majority of existing AC mining algorithms use rules learnt from the trainingdata set for constructing a single label classiﬁer which in turn is utilised for predict-ing test data. Thus, there are limited numbers of research articles related to multi-label rules in AC. Hereunder, we shed the light on two approaches and othertechniques related to traditional multi-label classiﬁcation in data mining. It isworth to note that the traditional classiﬁcation algorithms surveyed in this sectionare related to multi-label data sets and they assume each training example to beassociated with more than one class. This is unlike the proposed algorithm thatassumes each training example to be linked with a single class but producesmulti-label rules.
Veloso et al. (2011)proposed a multiple label AC algorithm that adopts the lazyclassiﬁcation approach in which it delays the reasoning process until test data aregiven. Unlike binary classiﬁcation which does not consider the correlation amongclasses, the lazy algorithm takes into account class relationships. Furthermore, itdeals with the small disjuncts (rules that cover limited number of training data),this may reduce classiﬁcation accuracy. This lazy approach has been comparedwith BoosTexter (Schapire and Singer, 2000) on three medium size data sets from‘‘http://portal.acm.org/dl.cfm’’ with respect to error rate. The results producedshow that this method is competitive to BoosTexter.
Another AC algorithm called MMAC was proposed to ﬁnd multiple label rulesfrom single label data sets. It has been reported that MMAC was able to generatehigher quality classiﬁers than CBA and decision trees on a number of UCI datasets in regard to classiﬁcation accuracy. One obvious limitation of the MMACis that the classiﬁer produced is extracted from parts of the training data setand the requirement of the recursive phase to ﬁnd the multi-label rules.
Wang et al. (20110proposed an enhanced Emerging Pattern (EP) algorithmcalled ADA that constructs rules from both the input training data set as wellas the classiﬁed resources such as text documents. ADA classiﬁer gets amendedon the ﬂy after the classiﬁed resources reach a certain amount. The authors haveupdated the classiﬁer by reﬁning the newly discovered knowledge using the exist-ing rules. Moreover, ADA uses the maximum entropy approach to classify testcases where multiple rules that are applicable to the test case contribute to the pre-diction decision. Overall, ADA can be considered a semi-incremental algorithmsince few training examples or users set of frequent patterns (keywords) are onlynecessary to build the classiﬁer instead of the complete training examples. Then,the classiﬁed examples as well as the rules are employed to update the classiﬁerby adding or removing rules. Limited experiments on four data sets from theUCI data repository (Merz and Murphy, 1996) have been conducted using34 N. AbdelhamidADA, CBA (Liu et al., 1998), CMAR (Li et al., 2001) and C4.5 (Quinlan, 1993). The results showed similarity on the classiﬁcation accuracy performance of the ACalgorithms and superiority over the decision tree approach (C4.5).
In image classiﬁcation, pictures may belong to multi-labels, i.e. different objectswithin a view. This problem is called class overlapping where a scene may containmultiple labels. A scene classiﬁcation method called cross training was developedin (Boutell et al., 2003). This method trains on each available label in an image inturn during the training step in order to consider all available labels. The resultsproduced reveal that the developed scene classiﬁcation algorithm performs wellwith respect to classiﬁcation accuracy.
Tsoumakas and Katakis (2007)surveyed common learning approaches relatedto multi-label classiﬁcation in several domains. The authors have ﬁrstly presentedthe multi-label data transformation methods used in the literature, and then sur-veyed the different multiple label learning approaches including the adaptationand the transformation methods. Experimentation using three different datatransformation methods and various learning algorithms with respect to differentevaluation measures, has been conducted. The accuracy results of the comparedalgorithms revealed that the ‘‘PT3’’ transformation method (considers each differ-ent set of labels that exist in the multi-label data collection as a single label) whenused with a learning algorithm generates the highest results in each of the consid-ered data collections.
A kernel based machine learning algorithm from (Quaresma and Rodrigues,2003) was applied to the problem of categorising legal documents written inPortuguese for the general attorney ofﬁce. This problem is considered multi-labelsince a legal document may belong to more than one category. The authors haveutilised vector representation based on bag-of-word for document’s representa-tion without the usage of semantic and syntactic information. Moreover, eachdocument was processed by applying lemmatisation and stopword elimination.In experimentation, only the top ﬁve categories in regard to frequency docu-ments were used and a number of machine learning algorithms includingSVM, C4.5 and Naı¨ve Bayes have been contrasted with respect to predictionaccuracy and Information Retrieval (IR) measures (Precision, Recall and F1).The results indicated that SVM and C4.5 are more likely to be applicable tothe classiﬁcation of the attorney general documents since they scored the highestprediction rate. The Naı¨ve Bayes scored a low prediction rate and therefore itwas excluded from further experiments. The authors have run SVM and C4.5after reducing the document collection by removing words that had a frequencyin less than a speciﬁc number of documents, and they used words that appearedat least in 55 documents. The results on reduced document collection showed aslight superiority of C4.5 over SVM with respect to IR measures, i.e. precision,recall, and F1.Multi-label rules for phishing classiﬁcation 354. The eMCAC algorithm
Our algorithm consists of three main steps: Rule discovery, classiﬁer building andclass assignment. In the ﬁrst step, eMCAC iterates over the training data set inwhich rules are found and extracted. In step (2), redundant rules are discardedwhich means that rules do not have training data coverage. The outcome of thesecond step is the classiﬁer which contains rules. The last step involves testingthe classiﬁer on the test data set to measure its predictive rate. Details on eMCACsteps are given in the subsequent sections. The proposed algorithm assumes thatthe input attributes in the training data set are categorical (having distinct values),and for each of these attributes, all possible values are mapped to a set of positiveintegers. For continuous attributes any discretisation method can be employedbefore the training phase.
4.1. Pre-processing data
This step is optional and only required when the input data are multi-label. In thiscase, we copy each training example with each of its connected class labels. So, ifthere is a training example linked with two classes, this example is repeated witheach class. One of the reasons behind our selection of this method is that we wouldlike to treat each class inside the multi-label example equally. This is since classlabels are not sorted in the ﬁrst place within the multi-label training data set foreach case and therefore we do not have prior knowledge on the best class for eachtraining example. Later, when the classiﬁer is constructed we will be able to sortclasses within each rule generated based on their frequency with the rule’s attributevalues in the training data set after the transformation. Another reason for select-ing this data format method is that we do not want to lose any knowledge thatmight be useful to the decision maker by ignoring class labels particularly whenother data transformation methods are chosen including ‘‘largest frequency class’’,‘‘class random selection’’, ignore multi-label case’’, etc.
4.2. Data representation
In the last few years, some scholars (Abdelhamid et al., 2012; Thabtah et al., 2005) have developed AC algorithms which employ vertical data layout. A training dataset in the vertical data layout consists of a group of attribute values, where eachattribute value is followed by its locations (Tids) in the training data set. Table 3 shows the vertical layout of attribute values ‘‘y’’, ’’z’’, and ‘‘b’’ from Table 1.A number of research studies (Thabtah, 2007; Abdelhamid et al., 2012) revealed that the vertical data format is more effective for representing data than the horizontalformat since it makes the process of identifying frequent attribute values efﬁcientspeciﬁcally the task involving the support counting. This is simply because verticalalgorithms use simple TIDs intersection among attribute values to accomplish the36 N. Abdelhamidtask of support counting. There are few AC algorithms that employ the verticaldata layout for mining classiﬁcation rules such as MAC and MMAC.
In the proposed algorithm, the vertical data format is used to represent thetraining data set before frequent attribute discovery and rule generation processesbegin.
4.3. Rule discovery methodology
In this section, we describe the process of ﬁnding and generating rules.4.3.1. Frequent attribute values discovery
The eMCAC algorithm uses a rule discovery method that utilises fast intersectionamong attribute values TIDs to discover the rules. The TID of an attribute valueholds the locations (row Ids) that contain the attribute values and its associatedclass labels in the training data set. The learning method of the proposed algo-rithm discovers the frequent attribute value of size 1 (F1) after scanning the train-ing data set once. In particular, for each attribute value linked with a class, itssupport is computed from its TIDs list in which the size of the subset of the TIDslist that is associated with the largest frequency class of an attribute value dividedby the size of the training data set denotes the attribute value support. In caseswhere an attribute value is connected with more than one class it will end up withmore than one support. This ensures the production of the multiple label rulessince the algorithm allows an attribute value to be associated with multiple labelsas long as they are frequent.
When F1 is generated, the algorithm simply intersects the TIDs of the disjointattribute values in F1 to discover the candidate attribute values of size 2, and afterﬁnding F2, the possible remaining frequent attribute values of size 3 are obtainedfrom intersecting the TIDs of the disjoint attribute values of F2, and so forth.Since this frequent ruleitems discovery approach iterates over the training dataset once, it is highly effective according to several experimental studies in the lit-erature of data mining community especially with regard to processing time andmemory usage. More details on the advantage of vertical algorithms over tradi-tional ones are given in (Thabtah, 2007).Table 3Sample of vertical data layout for 3 attribute values.‘‘y’’ ‘‘z’’ ‘‘b’’1113 2 5 5 6 79Multi-label rules for phishing classiﬁcation 374.3.2. Rule production
When frequent attribute values are identiﬁed, eMCAC generates any one as a rulewhen it passes theMinConfthreshold. This is accomplished in a straightforwardmanner since all necessary information for calculating the rules conﬁdence valuesare stored in the attribute value TIDs. Any frequent attribute value that holds aconﬁdence value smaller than theMinConfgets discarded.
For any attribute value connected with many classes and which becomes fre-quent, eMCAC generates a multi-label rule for it when it passes the MinConf threshold. For example, the attribute value <a> ofTable 2is linked with two class labels, e.g. (cl
1,c l3) 4 times each in the training data set. Assume that theMinSuppandMinConfare set to 4/15 and 40% respectively. This means(<a>, cl
1), and (<a>, cl 3) have higher support and conﬁdence than theMin- SuppandMinConfthresholds and therefore two rules can be produced in thiscase:aﬁcl
3, andaﬁcl 1. For this example, a typical AC algorithm such asCBA only derives the rule that has higher coverage in the training data, meaningany of the above single label rules can be produced. On the other hand, the pro-posed algorithm does not discard any useful knowledge and for the above exampleit produces a multi-label ruleR:aﬁcl
1/C218cl3, where normally class labels areranked based on their count with the attribute values. In the above example theclass rank within the rule is random sincecl
1and cl 3have the same count whenlinked with <a> in the training data set.
4.4. Classiﬁer construction
Once the complete set of rules is derived a rule sorting procedure is invoked toensure that rules with high conﬁdence and support values are given higher priorityto be selected during building the classiﬁer. The rule sorting procedure utilisedconsiders different criteria to favour among rules. The criteria order is: rule’s con-ﬁdence, support, length and class frequency. This ordering of rules has been usedsince it reduces rule random selection in the prediction step when no rules arefound to be applicable to the test case which positively affects the classiﬁcationaccuracy of the classiﬁer.
After rules are sorted from which a subset gets chosen to comprise the classiﬁer.Precisely, and for each training case, eMCAC iterates over the complete set ofrules discovered (top-down fashion) and marks the ﬁrst rule that corresponds tothe training case to be part of the classiﬁer. A rule gets inputted into the classiﬁerif it covers at least a single training case. The rule coverage does not necessitate thesimilarity between the rule’s class and that of the training case. This results often inmore training coverage for each rule since all training data belonging to the rulebody are removed during evaluation. This surely reduces overﬁtting and usuallyends up with less number of rules. The same process is repeated until all training38 N. Abdelhamidcases are removed (covered) or all rules have been tested. Finally, the algorithmderives the classiﬁer.
Any remaining unmarked rules are discarded by the proposed algorithm sincesome higher ranked rules have covered their training cases during building theclassiﬁer and therefore these unmarked rules become redundant. In cases whenthere are unclassiﬁed cases remaining in the training data set, a default rule forthe largest count class linked with the unclassiﬁed cases is formed.
4.5. Class assignment of test data
The proposed algorithm ﬁres the ﬁrst sorted rule in the classiﬁer applicable to thetest case and assigns its class to the test case. The rules attribute values must becontained in the test case in order to be chosen for classifying the test case class.When there is no rule fully applicable to the test case then we take on the ﬁrst rulethat partly matches the test case attribute value. Unlike the majority of currentprediction procedures in AC mining that takes on the default class when no rulesare applicable to the test case our prediction procedure minimises the utilisation ofthe default rule in class assignment process of test cases which normally improvesupon the resulting classiﬁer performance. This is since default rule has been cre-ated with high error from the remaining unclassiﬁed training data cases while
Table 4Phishing data selected features.Feature DescriptionIP address If IP address exists in URL ﬁPhishy elseﬁLegit Long URL If URL length < 54 ﬁLegit URL lengthP54 and675ﬁSuspicious elseﬁPhishy URL’s having @ symbol If URL has ‘@’ ﬁPhishy else Legit Adding preﬁx or suﬃx If domain part has ‘–’ ﬁPhishy elseﬁLegit Misuse of HTTPs protocol Use of https & trusted issuer & age P2 yearsﬁLegit using https & issuer is not trustedﬁSuspicious elseﬁPhishy Request URL Request URL < 22% ﬁLegit request URLP22% and < 61%ﬁSuspicious elseﬁPhishy URL of Anchor URL anchor % < 31% ﬁLegit URL anchor%Pand667%ﬁSuspicious elseﬁPhishy Server form handler SFH If ‘about:blank’ or empty ﬁPhishy SHD redirects to diﬀerentdomainﬁSuspicious elseﬁLegit Abnormal URL No hostname in URL ﬁPhishy elseﬁLegit Using pop-up window Right click disabled ﬁPhishy rightClick showing alertﬁSuspicious elseﬁLegit Redirect page Redirect page #s61ﬁlegit redirect page #s > 1 and < 4ﬁSuspicious elseﬁphishy DNS record No DNS record ﬁPhishy elseﬁLegit Hiding the links Change of status bar onMouseOver ﬁPhishy no ChangeﬁSuspicious
elseﬁLegit Website traﬃc webtraﬃc < 150,000 ﬁLegit webTraﬃc > 150,000ﬁSuspicious elseﬁPhishy Age of domain AgeP6 monthsﬁLegit elseﬁPhishyMulti-label rules for phishing classiﬁcation 39building the classiﬁer and reducing its usage in the prediction step is a deﬁniteadvantage.
5. Experimental results
In this section, we conduct experimentations on a number classiﬁcation data setrelated to website phishing to evaluate the eMCAC algorithm performance. Themain evaluation measures used in the experiments are prediction rate (Accuracy),(label-weight, any label) (deﬁned later in this section), and classiﬁer size (numberof rules). A number of algorithms have been contrasted with the proposed algo-rithm with respect to the above mentioned evaluation measures. A number of fea-tures (16) related to the process of identifying the type of websites have beencollected from different phishing sources, e.g. Phishtank ( www.phishtank.com), yahoo directory and Millersmiles archive (www.millersmiles.co.uk). The different features contributing to the classiﬁcation of the type of the websites have beenadopted from (Mohammad et al., 2012). The reason for choosing these featuresis since the authors of (Mohammad et al., 2012) have applied frequency analysison over 5000 collected websites (training examples) containing over 27 featuresas a feature selection metric and found out that 16 features have signiﬁcant impacton the process of phishing detection. We have collected 1350 websites, details onthe data set features are displayed inTable 4. One distinguishing differencebetween the data we have collected and other scholars is that we have included‘‘Suspicious’’ (Phishy or Legitimate) class label which converted the phishingproblem from binary classiﬁcation to a multi-label classiﬁcation. This made theproblem harder and more realistic since we have considered the overlappingamong the class labels.
5.1. Settings
In all experiments, tenfold cross validation testing method has been employed forfair evaluation of the classiﬁers derived by the algorithms considered and to reduceoverﬁtting. Furthermore, six dissimilar classiﬁcation algorithms which utilise avariety of rule learning methodologies have been considered for contrasting pur-poses with the eMCAC. These algorithms are MMAC ( Thabtah et al., 2004), CBA (Liu et al., 1998), PART (Frank and Witten, 1998), MCAR (Thabtah et al., 2005), RIPPER (Cohen, 1995) and C4.5 (Quinlan, 1993).
The experiments were conducted on an I3 machine with 2.3 Ghz. The experi-ments of C4.5, PART and RIPPER were carried out in Weka software ( Witten and Frank, 2002). For AC algorithms, we have selected CBA, and MCAR for sin-gle label classiﬁer comparison and MMAC for multi-label classiﬁer. CBA andMMAC source code has been obtained from their prospective authors and theproposed algorithm and MCAR were implemented in Java.40 N. AbdelhamidFinally, we have set theMinSuppandMinConfthresholds for the AC algo-rithms (CBA, MCAR, MMAC, eMCAC) to 2% and 50% respectively for allexperiments. The main reason for giving theMinSupp2% is that previous researchworks, e.g. (Thabtah et al., 2005), have suggested thatMinSuppvalues ranging between 2% and 5% may balance between the number of rules generated andthe predictive accuracy of the classiﬁer. On the other hand and for the MinConf parameter, it has been set to 40% since it has minor effect on the performanceof the classiﬁers.
5.2. Data collection
Phishing features can be extracted in a number of ways one of which is manualextraction where users derive features and judge their legitimacy. In this method,users have to spend a lot of time studying the up-to-date phishing collection tech-niques which is an infeasible approach for the majority of the users. The secondmethod employed in extracting phishing features is the automatic extraction. Thisis accomplished by examining the webpage and extracting a set of patterns relatedto phishing and legitimate type web pages. This involves examining the webpageproperties and all its features. Webpage properties are typically derived fromHTML tags, URL address and Javascript source code.
To conduct our experiments a set of phishing websites were collected fromPhishtank archive (www.phishtank.com), which is a free community site whereusers can submit, verify, track and share phishing data. In addition, we utilisedthe Millersmiles archive (www.millersmiles.co.uk), which is considered a primesource of information about spoof emails and phishing scams. The legitimate web-sites were collected from the yahoo directory and starting point directory using aPHP script. The PHP script was plugged with a browser and it collected 548 legit-imate websites out of 1353 websites. There is 702 phishing URLs, and 103 suspi-cious URLs. Sample of the phishing data (10 examples) for all features is shown inTable 4. Some of the collected features hold categorical values termed as‘‘Legitimate’’, ’’Suspicious’’ and ‘‘Phishy’’, these values have been replaced withnumerical values 1, 0 and/C01 instead of ‘‘Legitimate’’, ‘‘Suspicious’’ and ‘‘Phishy’’respectively.
5.3. Phishing results analysis
Fig. 1summarises the prediction accuracy produced by the considered algorithmsfor the phishing problem data. It is obvious fromFig. 1that the eMCAC algorithmoutperformed the other AC algorithms and the traditional one in predicting the typeof the websites. In particular, the eMCAC algorithm outperformed RIPPER, C4.5,PART, CBA, and MCAR by 1.86%, 1.24%, 4.46%, 2.56%, 0.8% respectively.Overall, the prediction accuracy obtained from all algorithms is considered accept-able and that reﬂects goodness of our features in predicting the website class. ItMulti-label rules for phishing classiﬁcation 41should be noted that eMCAC accuracy was calculated using label-weight (deﬁnedbelow) and only the largest frequency class is utilised for computing the accuracyfor the remaining algorithms since they are single label ones.
One main reason for achieving higher predictive accuracy by the eMCAC algo-rithm is its ability not only to extract one class per rule but also all possible classlabels in the form of a disjunctive multiple label rule. This extra useful knowledgeis usually missed by the majority of existing AC algorithms and can contributepositively in predictive power as well as serve the need for the end-user. Thiscan be clearly obvious in real world applications such as website phishing.Fig. 2lists the number of rules generated by all algorithms against the phishingdata set. The ﬁgure stresses the point that the AC algorithm especially MCAR stillgenerates alarge number of rules if contrasted to decision trees, rule induction orhybrid classiﬁcation.
For the multi-label classiﬁers, we contrasted the proposed algorithm withMMAC multiple label AC algorithm.Fig. 3illustrates the two measures valuesderived by the eMCAC and the MMAC algorithms named ‘‘Label-weight’’ and‘‘Any-label’’ (Thabtah, 2007). Hereunder are the equations for calculating thetwo evaluation measures:
Figure 1The classiﬁcation accuracy (%) for the contrasted algorithms derived from the phishing data.
Figure 2Average number of rules generated by the contrasted algorithms derived from the phishing dataproblem.42 N. AbdelhamidX
d2DX
i:hiðdÞ¼cðdÞfgfiðdÞ=XkðdÞj¼1
fjðdÞ ! ð1Þ d2D:9iwithh
iðdÞ¼cðdÞ/C8/C9/C12/C12/C12/C12=mð2Þ
whereDdesignates a test data set withmdata casesd 1,d2,...,d m, andCrepre-
sents the set of class labels. Casedhas classc(d) in the test data set. A (possiblymulti-label) classiﬁer is a multifunctionh:Dﬁ2
C, where fordeD,h(d)=Æh1(d), h
2(d),...,hk(d)(d)æ. The number of times ofh1(d),h2(d),...,hk(d)(d) in the training data set are given asf
1(d),f2(d),...,fk(d)(d), respectively.
To further clarify how the label-weight evaluation measure works, consider forcase a rule R:X/C217Yﬁl
1/C218l3where attributes value (X,Y) is associated 30 and 20 times with class labelsl
1andl 3in the training data respectively. This is why classl 1
precedes classl 3in R. The label-weight technique assigns the predicted class weightto the test case if the predicted class matches the actual class of the test case. Onthe other hand, ‘‘Any-label’’ evaluation measure considers 100% correct classiﬁca-tion when any of the multi-label rule’s class matches the test case class. So for therule R if the test case class is eitherl
3orl1the test case will be given as ‘‘1’’. Thisexplains its higher rate withinFig. 3. In the same ﬁgure eMCAC algorithm outper-formed the eMMAC algorithm in both label-weight and any-label evaluation mea-sures for the phishing data.
The increase of the predictive accuracy for both evaluation measures for theeMCAC algorithm is due to its ability to reduce the default class usage during pre-diction step in which if no rules are applicable to the test case the prediction pro-cedure of the eMCAC algorithm takes on the highest ranked partly matching ruleand assigns it to the test case. Further, the rule discovery method of the proposedalgorithm extracts the multi-label rules early without the need to perform recursivelearning which necessitates learning from independent sets of the training datasimilar to rule induction and greedy approaches. Instead, our algorithm learnsthe multi-label rules from the whole training data set once by discovering single
Figure 3Accuracy % computed using Label weight and any label measures for eMCAC and MMACalgorithms.Multi-label rules for phishing classiﬁcation 43label rules that surviveMinSuppandMinConfearly and merge only those thatshare the antecedent (body) to generate the multi-label rules. This could causerules to overlap in their training cases but the eMCAC removes the overlappingduring constructing the classiﬁer by storing only rules that have training datacoverage.
To signify the importance of the additional knowledge produced by the pro-posed algorithmFig. 4displays the number of multi-label rules with respect totheir consequent part (class labels on the right hand side). The proposed algorithmwas able to extract multiple label rules from the phishing data set solving animportant problem in classiﬁcation data mining regarding rules overlapping classlabels. In particular, Figure 10 shows that the eMCAC algorithm generated 24multiple label rules that represent ‘‘Legitimate OR Phishy’’ website class. Thismulti-label class is in fact websites that are suspicious and mainly classiﬁed by cur-rent classiﬁcation algorithms as ‘‘Phishy’’ since they do not account the class over-lapping problem. In other words, the eMCAC algorithm was able to extract rulesthat current AC algorithms and traditional classiﬁcation algorithms ignore bring-ing up interesting useful information for the end-user. The fact that the eMCACalgorithm ﬁnds this additional knowledge is an indicator of the ability of the algo-rithm to discover new data insights most current AC algorithms are unable todetect.
6. Conclusions
In this paper, a new multi-label rule-based classiﬁcation algorithm based on ACmining called eMCAC has been proposed. The originality of the proposed algo-rithm is its ability to generate rules with multiple class labels from single data setsand without recursive learning in current AC methods like MMAC. Experimentalresults against crucial applications named website phishing have been conductedto evaluate the performance of the proposed algorithm in classifying websites.The measures of evaluation are label-weight, any-label, accuracy and number of
Figure 4Number of class labels per rule derived by the eMCAC algorithm from the phishing data.44 N. Abdelhamidrules and the contrasted algorithms are CBA, MCAR, MMAC, PART, C4.5 andRIPPER. The results of the experiments showed that the proposed algorithm out-performed the considered algorithms on the real world phishing data with respectto accuracy. Further, the label-weight and any-label results of the proposed algo-rithm are better than those of the MMAC algorithm for the same data. TheeMCAC algorithm was able to produce multi-label rules from the phishing datawhere each training example is associated with one class. We have identiﬁed asmaller effective feature set for detecting the type of the website after applyingChi-square feature selection method. The results of all considered algorithms otherthan eMCAC have been consistent in detecting the phishing website. In the nearfuture, we intend to apply the eMCAC algorithm on unstructured data related totext categorisation.
References
Abdelhamid, N., Ayesh, A., Thabtah, F., Ahmadi, S., Hadi, W., 2012. MAC: A multiclass associativeclassiﬁcation algorithm. J. Inf. Knowl. Manage. 11 (2), 1250011-1–1250011-10
.
Boutell, M., Shen, X., Luo, J., Brown, C., 2003. Multi-label semantic scene classiﬁcation. Technical report 813,Department of Computer Science, University of Rochester, Rochester, NY 14627 & Electronic ImagingProducts R & D, Eastern Kodak Company.
Chien, Y., Chen, Y., 2010. Mining associative classiﬁcation rules with stock trading data – A GA-based method.Knowl. Based Syst. 23 (2010), 605–614
.
Cohen, W., 1995. Fast effective rule induction. Proceedings of the 12th International Conference on MachineLearning, CA, USA, pp. 115–123.
Frank, E., Witten, I., 1998. Generating accurate rule sets without global optimisation. In: Shavlik, J., (ed.),Machine Learning. Proceedings of the Fifteenth International Conference, Madison, Wisconsin. MorganKaufmann.
Jabbar, M.A., Deekshatulu, B.L., Chandra, P., 2013. Knowledge discovery using associative classiﬁcation forheart disease prediction. Adv. Intel. Syst. Comput. 182 (2013), 29–39
.
Jabez, C., 2011. A statistical approach for associative classiﬁcation. Eur. J. Sci. Res. 58 (2), 140–147 .
Li, X., Qin, D., Yu, C., 2008. ACCF: Associative Classiﬁcation Based on Closed Frequent Itemsets. Proceedingsof the Fifth International Conference on Fuzzy Systems and Knowledge Discovery – FSKD, pp. 380–384.
Li, W., Han, J., Pei, J., 2001. CMAR: Accurate and efﬁcient classiﬁcation based on multiple-class associationrule. Proceedings of the IEEE International Conference on Data Mining – ICDM, pp. 369–376.
Liu, B., Hsu, W., Ma, Y., 1998. Integrating classiﬁcation and association rule mining. Proceedings of the KDD,New York, NY, pp. 80–86.
Merz, C., Murphy, P., 1996. UCI Repository of Machine Learning Databases. University of California,Department of Information and Computer Science, Irvine, CA
.
Mohammad, R.M., Thabtah, F., McCluskey, L., 2012. An Assessment of Features Related to Phishing Websitesusing an Automated Technique. ICITST, London
.
Quaresma, P., Rodrigues, I., 2003. PGR: Portuguese Attorney General’s Ofﬁce decisions on the web. In:Bartenstein, Geske, Hannebauer, Yoshie, (Eds.), Web Knowledge Management and Decision Support,LNCS/LNAI 2543, Springer-Verlag, pp. 51–61.
Quinlan, J., 1993. C4.5: Programs for machine learning, Morgan Kaufmann, San Mateo, CA.
Schapire, R.E., Singer, Y., 2000. BoosTexter: a boosting-based system for text categorization. Mach. Learning 39(2/3), 135–168
.
Thabtah, F., 2007. Review on Associative Classiﬁcation Mining, Journal of Knowledge Engineering Review.Cambridge Press 22:1, 37–65
.
Thabtah, F., Cowling, P., Peng, Y., 2005. MCAR: Multi-class classiﬁcation based on association rule approach.Proceeding of the 3rd IEEE International Conference on Computer Systems and Applications. Cairo, Egypt,pp. 1–7.Multi-label rules for phishing classiﬁcation 45Thabtah, F., Cowling, P., Peng, Y., 2004. MMAC: A new multi-class, multi-label associative classiﬁcationapproach. Proceedings of the Fourth IEEE International Conference on Data Mining (ICDM ‘04). Brighton,UK, pp. 217–224.
Thabtah, F., Hadi, W., Abdelhamid, N., Issa, A., 2011. Prediction phase in associative classiﬁcation mining. Int.J. Softw. Eng. Knowl. 21 (06), 855–876
.
Thabtah, F., Mahmood, Q., McCluskey, L., Abdel-Jaber, H., 2010. A new classiﬁcation based on associationalgorithm. J. Inf. Knowl. Manage. 9 (01), 55–64
.
Tsoumakas, G., Katakis, I., 2007. Multi-label classiﬁcation: an overview. In: David Taniar (Ed.), InternationalJournal of Data Warehousing and Mining, Idea Group Publishing, 3(3), pp. 1–13.
Veloso, A., Meira, W., Zaki, M., Goncalves, M., Mossri, H., 2011. Calibrated lazy associative classiﬁcation. Inf.Sci. Int. J. 13 (181), 2656–2670
.
Wang, X., Yue, K., Niu, W., Shi, Z., 2011. An approach for adaptive associative classiﬁcation. Expert Syst. Appl.Int. J. 38 (9), 11873–11883
.
Witten, I., Frank, E., 2002. Data Mining: Practical Machine Learning Tools and Techniques with JavaImplementations. Morgan Kaufmann, San Francisco
.46 N. Abdelhamid