Rice disease identiﬁcation method based on improved CNN-BiGRU
Yang Lua,⁎,X i a o x i a oW ua, Pengfei Liua,H a n gL ib,c, Wanting Liua
aColloge of Information and Electrical Engineering, Heilongjiang Bayi Agricultural University, Daqing, Heilongjiang 163319, China
bArtiﬁcial Intelligence Energy Research Institute, Northeast Petroleum University, Daqing, Heilongjiang 163318, China
cSanya Offshore Oil and Gas Research Institute, Northeast Petroleum University, Sanya, Hainan 572025, China
abstract article info
Article history:Received 4 February 2023Received in revised form 8 August 2023Accepted 20 August 2023Available online 25 August 2023In theﬁeld of precision agriculture, diagnosing rice diseases from images remains challenging due to high errorrates, multiple inﬂuencing factors, and unstable conditions. While machine learning and convolutional neuralnetworks have shown promising results in identifying rice diseases, they were limited in their ability to explainthe relationships among disease features. In this study, we proposed an improved rice disease classi ﬁcation method that combines a convolutional neural network (CNN) with a bidirectional gated recurrent unit(BiGRU). Speciﬁcally, we introduced a residual mechanism into the Inception module, expanded the module'sdepth, and integrated an improved Convolutional Block Attention Module (CBAM). We trained and tested theimproved CNN and BiGRU, concatenated the outputs of the CNN and BiGRU modules, and passed them to theclassiﬁcation layer for recognition. Our experiments demonstrate that this approach achieves an accuracy of98.21% in identifying four types of rice diseases, providing a reliable method for rice disease recognition research.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Deep learningCNN-BiGRURice diseaseFeature relationship
1. IntroductionRice is a major crop in Asia (Khush, 1997). With the intensiﬁcation of rice cultivation, its yield and economic bene ﬁts have increased continu- ously. However, this has also led to the frequent occurrence of rice dis-eases, which pose a threat to food security ( He et al., 2010). In China, blast disease, bacterial leaf blight, sheath blight, and rice stripe diseaseare the major types of diseases that are prone to occur ( Zhu et al., 2004;Sun et al., 1998;Peng et al., 2015). These diseases can occur throughout the entire growth cycle of rice, and 10% –15% of rice yield losses are caused by rice diseases (Peng et al., 2009). Timely and effec- tive monitoring of rice diseases is particularly important for stabilizingyield and saving costs. Traditional crop disease monitoring methods in-volve destructive sampling, laboratory analysis, and specialized instru-ments, which are destructive and lagging. Computer vision is ascientiﬁc method for obtaining information about the investigatedcrop without contact. Rice diseases mainly manifest on the stems andleaves of rice, and mobile devices such as cameras can record and obtaininformation about plant phenotypes and their changes, providing anobjective means for the quantiﬁcation of rice diseases compared to vi-sual methods. In theﬁeld of image recognition research, machine learn-ing methods such as random forests, support vector machines, and KNNhave had a profound impact. (Rumpf et al., 2010) used support vectormachines and hyperspectral technology to identify early sugar beet dis-eases, with an accuracy rate of 97% for distinguishing healthy sugar beetfrom diseased leaves.Wang et al. (2019)extracted color, shape, and tex- ture features of fungal diseases such as powdery mildew, rust, and leafspots on wheat leaves and used support vector machines to classifyand identify them. (Majumdar et al., 2015) used fuzzy c-means cluster- ing algorithm to extract 25 disease features of wheat leaves and classi-ﬁed them using an artiﬁcial neural network to determine whetherwheat was infected. Traditional machine learning relies on manuallygenerated speciﬁc features, which results in low recognition ef ﬁciency. Remote sensing methods have achieved certain research results incrop stress monitoring, but lack spectral speci ﬁcity of diseases, which may encounter uncertainty issues in monitoring. With the continuousdevelopment of big data and deep learning research, plant phenotyperesearch based on deep learning methods provides a new means forcrop disease recognition and has achieved signi ﬁcant results in crop monitoring and management.(Sethy et al., 2020) evaluated the performance of 13 rice diseaseidentiﬁcation models. Statistical analysis showed that the ResNet50and SVM classiﬁcation models had higher recognition accuracy thanthe other models. (Waheed et al., 2020) proposed an optimized dense convolutional neural network called DenseNet, which achieved an accu-racy of 98.06% and had fewer parameters and computation time thansimilar CNNs. (Da Costa et al., 2020) studied external defects in toma-toes and compared them with transfer learning training, showing thatthe ResNet18 model was the best. In peanut variety identi ﬁcation,Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
⁎Corresponding author.E-mail address:luyanga@sina.com(Y. Lu).
https://doi.org/10.1016/j.aiia.2023.08.0052589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/(Yang et al., 2021) improved the VGG16 andﬁne-tuned it to achieve an accuracy of 96.7%, which was 8.9% higher than the original VGG16.(Ferentinos, 2018) used multiple deep learning models to detect plantleaf diseases and achieved the highest accuracy of 99.53% on the testset using the VGG network. (Barbedo, 2018) used transfer learning on GoogleNet to detect 56 diseases in 12 types of plants. ( Atole and Park, 2018) used AlexNet to differentiate between three categories of riceplants - healthy, diseased, and snail-infected - based on 227 images.(Too et al., 2019) evaluated the performance of deep convolutional neu-ral networks in plant disease classiﬁcation usingﬁne-tuning, and the DenseNet network achieved a testing accuracy of 99.75% on the plantvillage dataset. (Jiang et al., 2020) used convolutional neural networks(CNNs) to extract image features of rice leaf diseases. They then appliedthe SVM method to classify and predict speci ﬁc diseases, obtaining an accuracy of 96% using 10-fold cross-validation to determine the optimalSVM parameters. (Chen et al., 2020) used transfer learning to identifydifferent diseases in rice and corn, and classi ﬁed plant disease images using the pre-trained VGGNet network. ( Rahman et al., 2020)u s e d the architectures of VGG16 and InceptionV3 and ﬁne-tuned them to de- tect and recognize rice pests and diseases. The experimental resultsshowed that VGG16 and InceptionV3 had recognition accuracies of97.12% and 96.37%, respectively. (Woo et al., 2018) proposed a spatial and channel-wise feedforward convolutional neural network attentionmodule. It is a lightweight model that can be integrated into mostCNNs. In the lightweight CNN model proposed by ( Bao et al., 2021), which integrates CBAM, the recognition accuracy of wheat spikes intheﬁeld reached 94.1%.These work have demonstrated the reliability of computer visiontechniques in diagnosing crop diseases. As researchers tackle old issuesand discover new challenges, they are exploring ways to combine differ-ent methods to address complex problems. However, current CNNsused in visual applications have limitations, as they only emphasizelocal features and overlook the relationships between these extractedfeatures. For instance, the distribution patterns and internal weight dif-ferences of features are crucial in capturing long-term feature depen-dencies, which is necessary for identifying rice disease symptoms. Thisinformation cannot be fully captured by local features alone, thus re-quiring special attention. Recently, RNNs such as LSTM ( Graves and Graves, 2012) and GRU (Chung et al., 2014) have gained attention for their excellent performance in handling sequence features in languageand signal processing tasks. The expression of disease features can beviewed as numerical matrix information, which can be ﬂattened into nonlinear sequence data for effective processing by RNNs. This allowsresearch to focus on information that CNNs may overlook. Speci ﬁc pro- cessing methods need to be developed for downstream tasks in com-puter vision, and for modeling rice diseases, wider networks are betterable to detectﬁner-grained disease features. This study improves themodel by considering disease features as semantic information describ-ing the type of disease and transforming it into sequence data, which isthen trained and summarized using RNNs. This method combines CNNsand RNNs to diagnose rice diseases, attempting to learn the relation-ships between features and address current research limitations.This paper makes the following contributions:(1) We propose an improved CNN-BiGRU model for diagnosing ricediseases using images.(2) To accurately capture the features of rice diseases, we integrate animproved Convolutional Block Attention Module (CBAM), which
increases the model's focus on regions of interest in the image,thereby reducing the negative impact of noise on feature capture. (3) The high-level features extracted by the CNN are ﬂattened and treated as sequential data, and BiGRU is introduced to further in-vestigate the dependency between the preceding and followinginformation of rice disease characteristics.(4) The model achieves a recognition accuracy of 98.21% on four ricedisease samples, making it a reliable automated detection method.2. Methods2.1. The structure of improved CNN-BiGRUThe proposed method in this paper combines CNN and RNN for diag-nosing rice diseases. In order to enable the model to learn contextualfeatures between upper and lower layers of features, we used BiGRUto learn relevant information from both forward and backward direc-tions. To reduce the computational cost incurred by ﬂattening the entire image, we employed an improved CNN module as an image feature en-coder. The CNN module works by aggregating information and shrink-ing data normalization. The output of the improved CNN is thentransformed from matrix to vector form, and fed into the RNN for train-ing. The output data is then transmitted back for further re ﬁnement. The output from both the CNN and RNN modules were concatenated to formtheﬁnal decision data, which is then passed to the classi ﬁcation layer for classiﬁcation. The overall architecture is illustrated in Fig. 1. The speciﬁc method will be described in detail below.2.2. The structure of improved CNNTo improve the performance of Convolutional Neural Networks(CNNs), increasing the network's width and depth is a direct method,but this may lead to higher computational costs and network degrada-tion. Fixed convolutional kernel sizes can also result in the loss ofdisease feature information. The Inception module includes a wide net-work layer and several different sizes of convolutional kernels. By usingmulti-scale convolutional calculations, it extracts multi-scale featuresthat effectively describe the image. To accurately extract rice disease in-formation, we established a CNN model based on the InceptionV1(Szegedy et al., 2015) module, with an increased depth of the module.Two 3 × 3 convolutional kernels are equivalent to a 5 × 5, with a largerreceptiveﬁeld for detecting larger disease points in rice diseases whilereducing computational complexity. We replaced a branch and addeda 3 × 3 convolutional kernel to extract small point information, captur-ing local disease information and avoiding the loss of small target infor-mation. A 1 × 1 convolutional kernel can be used for upsampling anddownsampling, and canﬂexibly adjust the output feature size. As thenetwork depth increases, we adopt a residual mechanism ( He et al., 2016) to address network degradation issues by using skip connectionsto combine shallow and deep layer information. The residual structureH(x) = F(x) + x allows for small weight updates when F(x) is closeto zero, and concatenating processed data during optimization can ef-fectively improve efﬁciency. We improve the Conv shortcut by usingconvolutional operations to calculate H(x) = F(x) + ρ(x), where ρ(x) denotes the convolutional operation. As can been seen in part 2ofFig. 2, we combine the smallﬁeld of view information from the outputof the 3 × 3 convolutional kernel calculation with the large ﬁeld of view information from the output of the 3 × 3 convolutional kernel in part 1.This approach not only alleviates gradient problems, but also allows forricher feature combinations. Finally, we merge the outputs of eachbranch to obtain multi-scale features that effectively describe thedisease image.The training time for the Inception module is lengthy. To improve itsperformance, we incorporated four enhanced CNN modules in our ex-periment as detailed inSection 3.4. These modules produced varyingnumbers of feature maps. Speciﬁcally, in the enhanced CNN, module 1produced 128 feature maps, module 2 produced 256 feature maps,module 3 produced 512 feature maps, and module 4 produced 1024feature maps. For more information, please refer to
Table 1.2.3. Improved CBAMIn order to capture rice stress features more accurately, theconvolutional block attention module was introduced and improved,Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
101which used the feature MapF∈R c/C2h/C2w, as input to infer the 1-dimensional channel attention Map M
CF∈R C/C21/C21 and the 2-dimensional spatial attention Map M
SF∈R 1/C2h/C2w . The feature mapf received by the channel attention module, after maximum pooling andaverage pooling, obtains two different feature descriptions, F
CAvg andF
CMax, and then transferred to the multilayer perceptron ( mlp) layer. The original channel attention module is shown in Fig. 3(a). The di- mension reduction operation of the initial MLPlayer will lose some fea- ture information. In order to retain the features that are meaningful fordiagnosis, this paper replaces it with three one-dimensional convolutionﬁlters to generate features.F
CAvgandF CMaxsums the elements and outputs the eigenvector to generate the channel note ﬁgure M
CF⊂R c/C2h/C2w, the improved channel attention module is shown inFig. 3(b). The channel note diagram can be calculated according toEq.(1).M
CF¼ρðC1C1C1Avgpool FðÞðÞðÞðÞ þC1C1C1Maxpool FðÞðÞðÞðÞ Þ ð1Þwhereρdenotes the sigmoid function and C1 represents a one-dimensional convolutional layer.Spatial attention is a complement to the Channel Attention module,in order to calculate spatial attention, max pooling and average poolingoperations are applied along the channel attention axis, connectingthem together to form a valid feature descriptor. Finally, the spatial at-tention is generated by convolutional M
SF⊂R h/C2wlayers, which can be calculated by Eq.(2).M
SF¼ρf7/C27ðÞmaxpool,avgpool½/C138ðÞ ð 2Þwhereρdenotes the sigmoid function,f7/C27ðÞrepresents the convolutional kernel size 7/C27. The integration mode of improvedCBAM in convolutional neural network is shown in Fig. 3(c).2.4. Bidirectional gated recurrent neural networkThe internal structure of GRU is shown in Fig. 4. GRU obtains the gate state by the previous hidden state ht−1and the current inputx
t, the reset gate determines how the new inputinformation is combined with the previous memory, which is expressedas Eq.(3):r
t¼σω r•ht−1 ;xt ½/C138ðÞ ð 3ÞThe update gate determines the amount of previous memory savedto the current time step, which is expressed as Eq. (4): u
t¼σω u•h t−1ðÞ ;xt/C2/C3/C0/C1 ð4Þwhereσdenotes the Sigmoid function, through which data can betransformed into values in the range of 0 –1 to act as gating signals to control the amount of informationﬂowing through the valve in all di-mensions.ω
randω uare the weights of the reset gate and the updategate, respectively. After obtaining the gated signal, the current input x
t
and reset data are stitched together, and then activated by the tanhfunction to obtain the candidate hidden state of the moment h
0tat time t, which is expressed as an Eq.(5):h
0t¼tanhω•r t⊗h t−1 ;xt ½/C138ðÞ ð 5ÞThe size of theω•½r
t⊗h t−1/C138determines the degree of combinationof the current inputx
tand historical information, the larger it is, themore information needs to be retained, the higher the degree of integra-tion, conversely, the less information is retained.The current hidden stateh
tis expressed as Eq.(6): h
t¼1−u t ðÞh t−1þu t⊗h0tð6Þ
Fig. 1.Structure of Inproved CNN-BIGRU.
Fig. 2.The internal structure of improved CNN module.Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
102The size of theu tis positively correlated with the information to beremembered in interval [0,1], (1-u
t)ht−1indicating selective forgetting the hidden state at the previous moment, u
t⊗h0tindicating selective memory of the hidden state,h
tindicating forget certain types of theprevious step information inh
t−1, and add the current new memoryinformation inh
0t, thus forming theﬁnal memory within the unit.Hidden states use reset gates to control how much informationhas been forgotten in the past, and update gates control how muchnew information needs to be received from historical states in thecurrent state. This design addresses the gradient decay problem inrecurrent neural networks and better captures dependency betweenfeatures in sequence data. The output information of the neural unitsof the last layer of the entire RNN network model is the most abun-dant, and we stitch it together as a secondary feature with the outputfeature of CNN.Table 1Structural parameters of improved CNN.Layer Kernel size Output shapeInput (None,224,224,3)Conv 7 × 7,s = 2 (None,112,112,64)Improved Block1 S = 2 (None,56,56,128)Improved Block2 S = 2 (None,28,28,256)MaxPool 3 × 3,s = 2 (None,14,14,512)Improved Block3 S = 2 (None,7,71,024)MaxPool 3 × 3,s = 1 (None,14,14,512)Improved Block4 S = 2 (None,7,71,024)Conv 1 × 1,s = 1 (None,7,71,024)Global average pool (None,1024)softmax (None,4)
(a).Origin channel attention module.
(b).Improved channel attention module.
(c).CBAM is integrated with convolutional module.
Fig. 3.(a) is the original channel attention module, (b) is the improved channel attention module, and (c) is the integration of improved CBAM in convolution al neural network.
Fig. 4.The internal structure of the GRU unit.Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
103BiGRU consists of two sets of GRU that input coercive features fromforward and backward, respectively, providing contextual global fea-tures for the neural network, with a structure shown in Fig. 5, where GRU1 represents the backward, GRU2 represents forward input.2.5. Evaluation indicatorsUsing these indicators, they areAccuracy,Precison,Recall,a n dF1 value for evaluation, and the formulas are equations such as Eqs. (7)– (10), respectively as shown.Accuracy¼
TNþTPTPþTNþFPþFN ð7ÞPrecison¼
TPTPþFP ð8ÞRecall¼
TPTPþFN ð9ÞF1¼
2/C3Recall/C3PrecisonPrecisonþRecall ð10ÞwhereTPdenotes the number of true positive samples, TNdenotes the number of true negative samples,FPdenotes the number of false posi-tive samples, andFNdenotes the number of false negative samples.3. Experiment3.1. Experimental environmentThis experiment was conducted on the Windows operating system(64-bit), GPU is NVIDIA RTX3050, CPU is Intel (R) Core (TM) i511400(H) with 16GB of memory and all code is python, deep learning plat-form Tensorﬂow2.4.0.3.2. Experimental dataFour plant diseases, namely, Rice blast, Sheath blight, Brownspot,and Leaf blight were collected. Experimental datasets were collectedby laboratory personnel from Heilongjiang Bayi Agricultural Universitypaddyﬁelds (45°46 46°55 N, 124°19,125°12E) during the planting pe-riods of June–August 2020 and 2021. The diversity of data sources canresist overﬁtting and strengthen the generalization ability of themodel, this research combined public data with self-made data ( Yang et al., 2023), proved that data from different sources is feasible andimproved the generalization ability of model. The diversity of dataobtained in theﬁeld can be enlarged by different devices,the imageswere captured using iPhone12, Redmi K30Pro, and Huawei P30Prodevices, with resolutions of 2532 × 1170, 2400 × 1080, and2340 × 1080, respectively. The data was captured in well-lit environ-ments, and the devices were placed approximately 20 –30 cm away from the samples to ensure image clarity. After sorting and screening,a total of 2414 images were included in the dataset. Fig. 6.d i s p l a y s some example images.3.3. Data preprocessing and data enhancementTo ensure standardization, we converted the collected rice images,which had varying resolutions and sizes, into 224 × 224 pixels forease of training. These images were in RGB color format and had signif-icant noise, making it difﬁcult to extract image features. To overcomethis challenge, we converted the images into the HSV color space,which describes colors based on hue, saturation, and value. The HSVcolor space is better suited for feature extraction, and thus we usedthis format for the images.To prevent over ﬁtting during model training, we enhanced and expanded the dataset. This involved rotating someimages, adjusting their width and height, randomly scaling their ampli-tude, andﬂipping some images horizontally.Weﬁrst divided the dataset into training, validation, and testing sets in a ratio of 6:2:2 and then con-duct augmentation.Table 2shows the dataset size before and after ex-pansion andFig. 7shows the distribution of the dataset.3.4. Experimental parameters and resultsThe model training is divided into two stages. In the ﬁrst stage, toﬁnd the optimal weight and scale of the improved CNN, wesuperimposed the proposed improved CNN module to carry out thetest, and adopted the 10 cross-validation method to divide the datainto ten parts, and gradually carried out, recording the results of eachtest, as shown inTable 3. The accuracy of the model increases with thedepth, and the model degrades after it is stacked to the ﬁ
fth module. If the residual structure is used to improve, the increase in depth willcause a large investment in computing resources, and the trainingspeed of the wide network is slow due to it contains differentbranches.Therefore, according to the current data, the CNN modulecan be improved by adding four modules to obtain the optimalperformance.The second stage is toﬁnd the best parameters and the best datasource acquisition location of BiGRU on the basis of the ﬁrst stage. Trans- ferring the original data to BiGRU will increase a lot of calculation costs.As mentioned above, improving the output of CNN module as a carrierof sensitive information removes some irrelevant information, andgradually discards low-sensitive information in the process of hiddenlayer processing, but it does not mean that these information is uselessfor decision-making. We introduced BiGRIU to conduct separate inde-pendent tests, and gradually processed the output of the improvedCNN modules 1 to 4 into one-dimensional vector form, and transferredit to BiGRIU with different parameter settings for experiments. Wespliced the output data and recorded the experimental results. The spe-ciﬁc results are shown inTable 4.It can be seen in table that the model achieves the highest accuracywhen the network has 2 layers and 80 nodes. Compared to the im-proved CNN alone, the accuracy has increased by 4.09%, which con ﬁrms our hypothesis. Models that can successfully handle sequence data canbe applied in theﬁeld of image recognition and can achieve good re-sults. Compared to increasing the scale by stacking models, the cost ofBiGRU is relatively small and can still achieve signi ﬁcant improvements. From the data, it is evident that as the number of nodes or layers in-creases, the model's accuracy improves, but after reaching a certainscale, it becomes challenging to improve performance. Next, we willconcatenate and conduct experiments on the data of the improvedCNN with improved CBAM and BiGRU. We propose an improved CNN-BiGRU model for rice disease image diagnosis, and the model parame-ters are set as shown inTable 5.The proposed improved CNN-BiGRU model was trained, as shown inFig. 8, which displays the accuracy and loss curves of the proposedmethod. The model was well-trained, with fast convergence and lowloss rate. Our proposed approach is to improve CNN by combining itwith BiGRU. The improved CNN module output was used for BiGRUcomputation, effectively capturing feature regularity information. Theoutput features were concatenated with the CNN module output to
Fig. 5.The structure of the BiGRU.Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
104overcome the limitations of CNN, and were transmitted to the classi ﬁca- tion layer forﬁnal diagnosis. The accuracy of the proposed modelreached 97.42% after feature concatenation. We also compared ourresults with those of classical models that have achieved good resultsin rice disease diagnosis-related work, as shown in Fig. 9, which displays the performance of deep learning models in the rice datasetclassiﬁcation.Different models were trained in the rice disease dataset, and theexperimental results were compared. The 0, 1, 2, and 3 in the ﬁgure represent the disease categories. The experimental results showthat the proposed method has more dense internal data points, sothe similarity is higher and the best classi ﬁcation effect is achieved. Experiments show that in the experiment of rice disease data, the ac-curacy of the improved CNN-BiGRU fusion model proposed in thispaper is higher than that of other classical deep learning models,which veriﬁes the effectiveness of the improved method proposedin this study.3.5. Ablation experiment3.5.1. Ablation experiment of improved CBAMWe conducted ablation experiments on different CNN modules inte-grated with the improved CBAM attention mechanism to enhance themodel's focus on disease spots. In the initial CBAM, the MLP layer
Fig. 6.Disease images data.
Table 2The amount of rice disease data before and after data enhancement.Disease type Oringin ExpansionBlast 783 1500Blight 553 1500Brown spot 632 1500Shealth blight 446 1500Total 2414 6000
Fig. 7.Distribution of training set, validation set and test set.Table 3Initial network scores.Number of modules Single CNN accuracy With Improved CBAM accuracy1 60.02% 60.56%2 84.25% 85.31%3 89.61% 90.36%4 92.12% 95.26%5 85.31% 89.21%6 84.56% 91.23%
Table 4Initial network scores.Layers Nodes First moudle Second moudle Third moudle Fourth moudle1 layer 20 52.21% 55.12% 62.14% 66.11%40 62.14% 71.21% 85.22% 88.51%80 66.31% 75.21% 89.32% 91.67%2 layers 20 58.32% 60.11% 70.01% 79.62%40 63.22% 65.54% 73.12% 84.52%80 75.22% 88.16% 96.21% 95.22%3 layers 20 71.06% 80.64% 91.64% 92.01%40 70.21% 73.06% 92.61% 94.35%80 69.42% 74.32% 93.11% 95.31%
Table 5Model parameters.Parameters Valuelearning rate 0.001Optimizer AdamEpoch 18Batch size 20Loss function categorical crossentropyY. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
105extracts information through dimensionality reduction anddownsampling, which may lead to the loss of some information, partic-ularly for features with scattered spot layouts. We compared thisimprovement against the original method to study which aspects of at-tention mechanism bring performance improvements to the model.Table 6provides the experimental data.The data in theTable 6suggests that the attention mechanism's im-pact on the shallow layers is subtle. Due to the processing of shallow fea-tures is not deep enough. As processing continues, higher-level featuresare gradually extracted, and the model strengthens its focus on this part.
Fig. 8.Accuracy and loss rate curves of improved CNN-BiGRU.
Fig. 9.Deep learning model performance in rice dataset.
Table 6The ablation experimental data of CBAM in Improved CNN.First module Second module Third module AccuracyImproved CNN 92.12%Improved CNN with✓ 92.23% CBAM✓✓ 92.79% ✓✓ ✓ 93.11% Improved CNN with✓ 92.26% improved CBAM✓✓ 93.21% ✓✓ ✓ 93.96%Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
106In addition, improvements have weakened the impact of dimensionalityreduction on the data, and this has been achieved through the use ofone-dimensional convolution. This approach has effectively enhancedthe impact of CBAM on CNN, resulting in improved performance.3.5.2. Ablation experiment of BiGRUAs CNN lacks the ability to quantify the inter-relationship betweenfeatures during the process of extracting local features, we introducedBiGRU to analyze this information. And also analyzed the impact ofBiGRU on the experiment and conducted ablation experiments. Weinvestigated the support provided by the recurrent neural network inanalyzing the features extracted by CNN, while maintaining consistencyin node numbers.Table 7presents the experimental data.From the data in theTable 7, it can be seen that in the case where theperformance of the model has room for improvement, the recurrentneural network can effectively improve the performance of the modelby analyzing the interdependence between features. Research onother deep learning models has proven the reliability of this method.GRU and LSTM have structural differences and perform differently indifferent tasks. Obviously, BiGRU is worth adopting in this task.4. Discussion4.1. Confusion matrix analysisThe performance of the improved CNN-BiGRU model in the test setis analyzed. The confusion matrix is shown in Fig. 10.Table 7Analysis of Recurrent neural network combined with improved CNN.GRU BiGRU LSTM BiLSTM AccuracyImproved CNN✓ 95.11%"2% ✓ 97.42%"4.31%✓94.96%"2.84%✓96.56%"3.45% VGG16✓ 92.56%"2.33% ✓ 95.31%"5.08%✓91.26%"1.03%✓94.23%"4% AlexNet✓ 93.36%"2.13% ✓ 96.23%"5%✓92.96%"1.73%✓95.32%"4.09% ResNet18✓ 94.63%"1.32% ✓ 96.79%"3.48%✓93.89%"0.58%✓96.23%"2.52%
Fig. 10.The confusion matrix of the proposed method.Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
107300 samples each of rice diseases: Brownspot, Rice blast, Blight andSheal blight participated in test. Our improved CNN showed reducedmisclassiﬁcation rates for each disease 25, 23, 22, and 18 respectively,which further decreased to 13, 20, 16, and 14 after integrating theCBAM module. Notably, the CBAM module signi ﬁcantly improved rec- ognition of sesame spot disease. By combining the BiGRU module to ex-tract advanced features, misclassiﬁcation rates were further reduced to7, 8, 5, and 9 respectively. The results demonstrate the effectiveness ofusing RNNs to process features of rice diseases, thereby enhancing theperformance of the CNNs.4.2. Parameter analysisThe number of nodes in this paper is obtained through human ex-ploration and experience, and there is still a certain degree of subjectiv-ity. We will analyze the number of nodes in the model. We set αandβ to represent the magniﬁcation factors of CNN and BiGRU respectively,and discuss the inﬂuence of the number of nodes on the model perfor-mance through scaling. In addition, we also adjust the size of the inputdata to explore the impact of this move on the model.According toTable 8, the model's accuracy changes as the number ofnodes changes. Increasing the number of nodes improves accuracy butcomes at the cost of increased computation and training time. Effectivemethods must balance cost and performance. The model achieves thehighest accuracy of 98.21% whenαis maintained at 1 andβis main- tained at 1.5.Different input sizes yield features of varying granularity. Generally,larger inputs are believed to enhance model performance, but theycome with inevitable computation cost increases. This cost can be sig-niﬁcant, especially with large amounts of data. Therefore, it is necessaryto explore an input size suitable for a particular task, such as the rice dis-ease diagnosis model.Table 9shows that size of inputs have signiﬁcant impacts on model performance, but larger sizes inevitably come withhuge costs. It is necessary to consider an input size that is suitable forthe current task while being practical and cost-effective. In this paper,the effect of 224 × 224 is relatively ideal compared to others.5. ConclusionThis paper proposed an improved CNN for extracting features of ricediseases by combining Inception and ResNet theories. The network ad-dresses the high misidentiﬁcation rate by incorporating informationabout the rice disease images and their surrounding context. Theimproved CNN also includes CBAM module for more precise featureextraction. Additionally, BiGRU was introduced to recognize therelationships between stress image features, deepening the model'sunderstanding of the images and the structural characteristics of ricediseases. The effectiveness of the proposed model is demonstratedthrough experiments, which show higher accuracy and lower costcompared to other models. However, the current dataset still has limita-tions, with only four types of rice disease samples. To address this,future work will focus on collecting more comprehensive real- ﬁeld samples and improving the model structure to increase its generaliza-tion ability in complex scenarios.Conﬂicts of Interest StatementThe authors declare that there is no conﬂict of interests.CRediT authorship contribution statementYang Lu:Writing–original draft, Investigation, Software.Xiaoxiao Wu:Conceptualization, Supervision, Funding acquisition. Pengfei Liu: Methodology, Writing–review & editing, Project administration.Hang Li:Formal analysis, Visualization.Wanting Liu:Validation.
AcknowledgmentsThis work was supported in part by the National Natural ScienceFoundation of China under Grants U21A2019, 61873058 and61933007, the Hainan Province Science and Technology Special Fundunder Grant ZDYF2022-SHFZ105, Heilongjiang Natural Science Founda-tion of China under Grant LH2020F042 and the Scienti ﬁc Research Starting Foundation for Post Doctor from Heilongjiang under GrantLBH-Q17134.References
Atole, R.R., Park, D., 2018. A multiclass deep convolutional neural network classi ﬁer for detection of common rice plant anomalies. Int. J. Adv. Comput. Sci. Appl. 9 (1).https://doi.org/10.14569/IJACSA.2018.090109 . Bao, W., Yang, X., Liang, D., Hu, G., Yang, X., 2021. Lightweight convolutional neural net-work model forﬁeld wheat ear disease identiﬁcation. Comput. Electron. Agric. 189, 106367.https://doi.org/10.1016/j.compag.2021.106367 . Barbedo, J., 2018. Impact of dataset size and variety on the effectiveness of deep learningand transfer learning for plant disease classi ﬁcation. Comput. Electron. Agric. 153, 46–53.https://doi.org/10.1016/j.compag.2018.08.013 . Chen, J., Chen, J., Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020. Using deep transfer learningfor image-based plant disease identi ﬁcation. Comput. Electron. Agric. 173, 105393. https://doi.org/10.1016/j.compag.2020.105393 . Chung, J., Gulcehre, C., Cho, K., Bengio, Y., 2014. Empirical evaluation of gated recurrentneural networks on sequence modeling. arXiv preprint arXiv:1412.3555. https:// arxiv.org/pdf/1412.3555.Da Costa, A.Z., Figueroa, H.E., Fracarolli, J.A., 2020. Computer vision based detection of ex-ternal defects on tomatoes using deep learning. Biosyst. Eng. 190, 131 –144.https:// doi.org/10.1016/j.biosystemseng.2019.12.003 . Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.https://doi.org/10.1016/j.compag.2018.01.009 . Graves, A., Graves, A., 2012. Long short-term memory. Supervised Sequence Labellingwith Recurrent Neural Networks. 37 –45.https://doi.org/10.1162/neco.1997.9.8.1735 . He, W., Huang, D., Liu, C., Cen, Z., Zhang, Y., Ma, Z., Li, R., 2010. Genetic analysis of resis-tance to bacterial leaf streak in common wild rice. J. Plant Pathol. 40 (180 –185). https://doi.org/10.13926/j.cnki.apps.2010.02.012 . He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. Con-ference on Computer Vision and Pattern Recognition , pp. 770 –778.https://arxiv.org/ abs/1512.03385.Jiang, F., Lu, Y., Chen, Y., Cai, D., Li, G., 2020. Image recognition of four rice leaf diseasesbased on deep learning and support vector machine. Comput. Electron. Agric. 179,105824.https://doi.org/10.1016/j.compag.2020.105824 . Khush, G.S., 1997. Origin, dispersal, cultivation and variation of rice. Plant Mol. Biol. 35,25–34.https://doi.org/10.1023/A:1005810616885 . Majumdar, D., Kole, D.K., Chakraborty, A., Majumder, D.D., 2015. An integrated digitalimage analysis system for detection, recognition and diagnosis of disease in wheatleaves. Proceedings of the Third International Symposium on Women in Computingand Informatics, pp. 400 –405.https://doi.org/10.1145/2791405.2791474 . Peng, S., Tang, Q., Zou, Y., 2009. Current status and challenges of rice production in China.Plant Prod. Sci. 12 (1), 3 –8.https://doi.org/10.1626/pps.12.3 . Peng, W., Li, K., Zeng, Y., 2015. Research progress on microbial control of rice diseases.J. Jiangxi Agric. Univ. 37 (4), 625 –631.https://doi.org/10.13836/j.jjau.2015096 . Rahman, C.R., Arko, P.S., Ali, M.E., Khan, M.A.I., Apon, S.H., Nowrin, F., Wasif, A., 2020. Iden-tiﬁcation and recognition of rice diseases and pests using convolutional neuralTable 8Analysis of the number of model nodes.αβ AccuracyImproved CNN 0.75 93.15% ↓2.11% 1 95.26%1.25 95.29% "0.03% 1.5 95.36% "0.1% Improved CNN-BiGRU 1 0.75 96.12% ↓2.09%1 97.26%1.5 98.21%"0.95%2 98.20%"0.94%
Table 9Experimental data with different input sizes.Inputsize Accuracy Time150 × 150 96.95% 2h10min224 × 224 98.21% 3h45min299 × 299 98.23% 7h3minY. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
108networks. Biosyst. Eng. 194, 112 –120.https://doi.org/10.1016/j.biosystemseng.2020. 03.020.Rumpf, T., Mahlein, A.-K., Steiner, U., Oerke, E.-C., Dehne, H.-W., Plümer, L., 2010. Early de-tection and classiﬁcation of plant diseases with support vector machines based onhyperspectral reﬂectance. Comput. Electron. Agric. 74 (1), 91 –99.https://doi.org/10. 1016/j.compag.2010.06.009. Sethy, P.K., Barpanda, N.K., Rath, A.K., Behera, S.K., 2020. Deep feature based rice leaf dis-ease identiﬁcation using support vector machine. Comput. Electron. Agric. 175,105527.https://doi.org/10.1016/j.compag.2020.105527 . Sun, G., Du, X., Rongxiang, T., Sun, S., 1998. Strategies for controlling rice blast and pros-pects for research in the 21st century. J. Plant Pathol., 289 –292.https://doi.org/10. 13926/j.cnki.apps.1998.04.001 . Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,Rabinovich, A., 2015. Going deeper with convolutions. Conference on Computer Vi-sion and Pattern Recognition , pp. 1 –9.https://arxiv.org/abs/1409.4842 . Too, E.C., Yujian, L., Njuki, S., Yingchun, L., 2019. A comparative study of ﬁne-tuning deep learning models for plant disease identi ﬁcation. Comput. Electron. Agric. 161, 272–279.https://doi.org/10.1016/j.compag.2018.03.032 . Waheed, A., Goyal, M., Gupta, D., Khanna, A., Hassanien, A.E., Pandey, H.M., 2020. An op-timized dense convolutional neural network model for disease recognition andclassiﬁcation in corn leaf. Comput. Electron. Agric. 175, 105456. https://doi.org/10. 1016/j.compag.2020.105456. Wang, H., Chen, D., Li, C., Tian, N., Zhang, J., Xu, J.-R., Wang, C., 2019. Stage-speci ﬁcf u n c - tional relationships between tub1 and tub2 beta-tubulins in the wheat scab fungusfusarium graminearum. Fungal Genet. Biol. 132, 103251. https://doi.org/10.1016/j. fgb.2019.103251.Woo, S., Park, J., Lee, J.-Y., Kweon, I.S., 2018. Cbam: convolutional block attention module.Proceedings of the European Conference on Computer Vision (ECCV) , pp. 3 –19. https://arxiv.org/abs/1807.06521 . Yang, H., Ni, J., Gao, J., Han, Z., Luan, T., 2021. A novel method for peanut variety identi ﬁ- cation and classiﬁcation by improved vgg16. Sci. Rep. 11 (1), 15756. https://www. nature.com/articles/s41598-021-95240-y . Y a n g ,L . ,Y u ,X . ,Z h a n g ,S . ,L o n g ,H . ,Z h a n g ,H . ,X u ,S . ,L i a o ,Y . ,2 0 2 3 .G o o g l e n e tb a s e do nresidual network and attention mechanism identiﬁcation of rice leaf diseases. Comput. Electron. Agric. 204, 107543. https://doi.org/10.1016/j.compag.2022. 107543.Zhu, Y., Leung, Hei, Chen, H., Wang, Y., Tang, K., Zhao, X., Zhou, J., Tu, J., Li, Y., He, X., Zhou,J., Sun, Y., Twng, W., 2004.Sustainable control of rice diseases by using disease resis-tance gene diversity. China Agric. Sci. 37 (6), 832 –839.Y. Lu, X. Wu, P. Liu et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 100 –109
109