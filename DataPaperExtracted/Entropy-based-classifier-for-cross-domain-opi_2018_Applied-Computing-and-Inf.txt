Original Article
Entropy based classiﬁer for cross-domain opinion mining
Jyoti S. Deshmukha, Amiya Kumar Tripathyb,⇑
aDepartment of Computer Engineering, PAHER University, Udaipur, India
bDepartment of Computer Engineering, Don Bosco Institute of Technology, Mumbai, India
article info
Article history:Received 30 August 2016Revised 11 February 2017Accepted 20 March 2017Available online 22 March 2017Keywords:Data miningOpinion miningKnowledge discoveryExpert systemsInformation systemsMachine learningabstract
In recent years, the growth of social network has increased the interest of people in analyzing reviewsand opinions for products before they buy them. Consequently, this has given rise to the domain adap-tation as a prominent area of research in sentiment analysis. A classiﬁer trained from one domain oftengives poor results on data from another domain. Expression of sentiment is different in every domain. Thelabeling cost of each domain separately is very high as well as time consuming. Therefore, this study hasproposed an approach that extracts and classiﬁes opinion words from one domain called source domainand predicts opinion words of another domain called target domain using a semi-supervised approach,which combines modiﬁed maximum entropy and bipartite graph clustering. A comparison of opinionclassiﬁcation on reviews on four different product domains is presented. The results demonstrate thatthe proposed method performs relatively well in comparison to the other methods. Comparison ofSentiWordNet of domain-speciﬁc and domain-independent words reveals that on an average 72.6%and 88.4% words, respectively, are correctly classiﬁed./C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionOpinionated text has created a new area of research in textanalysis. Traditionally, fact and information-centric view of textwas expanded to enable sentiment-aware applications. Nowadays,increased use of the Internet and online activities like ticket book-ing, online transactions, e-commerce, social media communica-tions, blogging, etc. has led to the need for the extraction,transformation and analysis of huge amount of information. There-fore, new approaches need to applied to analyze and summarizethe information[14].Organizations take the review of product given by users seri-ously, as it adversely affects the sales of the product. Consequently,organizations take the effort to respond to the reviews, as well asmonitor the effectiveness of its advertising campaigns. In thisregard, sentiment analysis, a popular method, is used to extractand analyze sentiments[5,4].Opinion mining is constantly growing due to the availability ofviews, opinions and experiences about a product/service online, aspeople are shedding their inhibition to express their opinionsonline. However, automatic detection and analysis of opinionsabout products, brands, political issues, etc. is a daunting task.Opinion mining involves three chief elements: feature andfeature-of relations, opinion expressions and the related opinionattributes (e.g., polarity), and feature-opinion relations. An opinionlexicon is a list of opinion expressions or a set of adjectives, whichare used to indicate opinion/sentiment polarity like positive, nega-tive and neutral. This lexicon arises from synonyms in the Word-Net, while antonyms are used to expand lexicon in the form ofgraphs. Such a dictionary-based approach has been used to par-tially disambiguate the results of parts of speech tagger. Further,fuzzy logic is used to determine opinion boundaries and to adoptsyntactic parsing to learn and infer propagation rules betweenopinions and features[24,13].Medhat et al.[18]conducted a survey on sentiment algorithmsand its applications and found that sentiment classiﬁcation andfeature selection are more prominent areas in recent research.They also reported that Support vector machine and Naïve Bayesalgorithms are the generally used algorithms to classify senti-ments, and English is the language used in many resources likeWordNet. Opinions and reviews given on social networking sitesare used to generate datasets for the experiments.The WordNet is a generalized lexicon and cannot be used forsentiment analysis; therefore, a need arose for the development
http://dx.doi.org/10.1016/j.aci.2017.03.0012210-8327//C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author at: School of Science, Edith Cowan University, Perth,Australia.E-mail addresses:jyoja2007@gmail.com(J.S. Deshmukh),amiya@dbit.in(A.K. Tripathy).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 14 (2018) 55–64
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
of sentiment lexicon. SentiWordNet evolved out of WordNet wascreated as a lexical resource for opinion mining. It assigns to eachsynset of WordNet three sentiment scores: positive, negative andneutral[19,11].Manufacturers, as well as consumers, require opinion miningtools to collect opinions about a certain product. The opinion anal-ysis tools can be used by manufacturers to decide a marketingstrategy for estimating production rate. On the other hand, con-sumers can use these tools to make decision on buying a new pro-duct or take a trip to vacation locations, or select hotel, etc.Labeled opinions are used to analyze the classiﬁer. Practically,labeled opinions for every domain is not possible, as it delimitedby time and cost, while domain adaptation or transfer learningcould be used to circumvent this limitation. In this paper, we pro-pose the approach of domain adaptable lexicon which predicts thepolarity of lexicon of one domain using a set of labeled lexicon ofanother domain using a modiﬁed entropy algorithm. This algo-rithm uses enhanced entropy with modiﬁed increment quantityinstead of traditional entropy algorithm. Dataset of different typesof products containing textual reviews has been used for evalua-tion. Multiple experiments were carried out to analyze the algo-rithm using accuracy and F-measure. We designed the approachin two phases: (i) preprocessing of dataset and (ii) applying classi-ﬁer and clustering on dataset.The rest of the paper is structured as follows. In Section 2,w e describe the related work on domain adaptation approaches. InSection3, we introduce our new improved entropy based semi-supervised approach. In Section4, we evaluate our approach usingcross-domain sentiment classiﬁcation tasks, and compare it withother baseline methods. Finally, in Section 5we draw conclusions on the proposed approach and set directions for future work.2. Related workThe text documents containing opinions or sentiments wereclassiﬁed based on their polarity, i.e. whether a document is writ-ten with a positive approach or a negative approach. Althoughmachine learning approach uses a word’s polarity as a feature,the polarity of some words cannot be determined without domainknowledge. Hence, the reusability of learned result of a domain isessential. Transfer learning, also known as domain adaptation, canbe used to address this challenge. Transfer learning utilizes theresults learned in a source domain to solve a similar problem inanother target domain[22]. Approaches used to classify singleand cross-domain polarity opinions are usually a bag of words,n-grams or lexical resource-based classiﬁers.The main aim of domain adaptation is to transfer knowledgeacross domains or tasks. Tagging the opinion word and buildinga classiﬁer is time consuming and expensive, as opinions aredomain dependent. Normally, users express their opinions speciﬁcto a particular domain. An opinion classiﬁer trained in one domainmay not work well when directly applied to another domain due tomismatch between domain-speciﬁc words. Thus, domain adapta-tion algorithms are extremely desirable to reduce domain depen-dency and labeling costs. Sentiment classiﬁcation problem areconsidered as a feature expansion problem, in which related fea-tures are appended to reduce mismatch of features between thetwo domains. To overcome this problem, sentiment-sensitive the-saurus, which contains different words and their orientation in dif-ferent domains, has been created. Bollegala et al. [7]used labeled, as well as unlabeled data, for evaluation. The results suggested thatmethod performs signiﬁcantly well compared to baseline.To overcome domain adaptation issue, various adaptationmethods have been proposed in the past, e.g., ensemble of classi-ﬁers. Combination of various feature sets and classiﬁcation tech-niques yielded in the ensemble framework was proposed by Xiaet al.[26]. They used two types of feature sets, namely, Parts-of-speech information and Word-relations and Naïve Bayes, Maxi-mum Entropy and Support Vector Machines classiﬁers. For betteraccuracy, ensemble approaches like ﬁxed combination, weightedcombination and Meta-classiﬁer combination, were applied. Liet al.[29]proposed active learning in which source and target clas-siﬁers were trained separately. Using Query By Committee (QBC)selection strategy, informative samples were selected, and classiﬁ-cation decision were made by combining classiﬁers. Label propaga-tion was used to train both classiﬁers. The result demonstratedthat signiﬁcantly outperformed the baseline methods.Most often, opinions are given in the natural language. Onemajor issue with natural language is the ambiguity of words. Fer-sini et al.[10]applied Bayesian ensemble model in which uncer-tainty and reliability was taken care. Greedy approach was usedfor classiﬁer selection, while gold standard datasets were usedfor experimental analysis. However, classiﬁcation performance isfrequently affected by the polarity shift problem. Polarity shifters
are words and phrases that can change sentiment orientation oftexts. Xia et al.[28]addressed this issue using three-stage modelswhich include detection of polarity shift, removal of polarity shiftsand sentiment classiﬁcation. Onan et al. [2]proposed the weight based ensemble classiﬁer, in which weighted voting scheme wasused to assign weight to classiﬁer. As a base learner Bayesian logis-tic regression, Naïve Bayes, linear discriminant analysis, logisticregression and Support vector machine are used. A different typeof experimental analysis shows better result than conventionalensemble learning. Da Silva et al. [8]used classiﬁer ensembles formed by different classiﬁer which is applicable to ﬁnd productson the web. Augustyniak et al.[16]demonstrated Twitter dataset to have good accuracy only for positive and negative queries. Theyfound that Bag of Words with ensemble classiﬁer performs betterthan supervised approach.Identiﬁcation of feature and weighting is an important step inopinion mining. Khan et al.[12]proposed a new approach thatidentiﬁed features and assigned term label using SentiWordNet.In this method, point wise mutual information and chi squareapproaches were used to select features to SentiWordNet thatwere weighted. Support vector machine was used as classiﬁer.Experimental evaluation on benchmark dataset shows effective-ness of approach.Social networking sites contains text data in long format as wellas short messages with symbols, emoticons etc. Opinion detectionin long reviews is easy than short reviews, as short reviews containfewer features, and more symbols, idioms etc. hence difﬁcult toextract opinion. Lochter et al.[15]proposed ensemble approach to tackle this issue. This approach used text normalization methodsto improve the quality of features. The features thus ﬁltered andenhanced served as the input for machine learning algorithms. Pro-posed framework was evaluated using real and non-coded datasetsand concluded that this approach was superior to other methodswith a 99.9% conﬁdence level. However, this approach was sug-gested to be expensive for ofﬂine processes due to higher cost ofcomputing power. Hence, parallelization of this process has beenstated as future work by the authors.Sparseness is another issue in short text data. Word co-occurrence and context information approaches are generally usedfor solving sparseness issue. These approaches are less efﬁcient. Toaddress this problem, Chutao et al.[32]considered probability dis- tribution of terms as the weight of terms.Similar to ensemble classiﬁers, graph-based methodology arealso used for domain adaptation. Dhillon et al. [25]proposed the graph-based domain adaptation method. Similarity graphs wereconstructed between features from all domains, if these featureswere similar then it demonstrated the presence of edge between56 J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64them. All labeled features were used in metric-learning algorithms.Graph was constructed using data-dependent metric and theweight was calculated for each edge. Experimental results demon-strated the reduction of classiﬁcation error.Pan and Yang[22]and Wang and Shi[31]focused on bridging the gap between domain-speciﬁc and domain-independent lexi-cons, as these approaches do not work well when applied to twoextremely different domains. Singh and Husain [30]presented dif- ferent datasets used in sentiment analysis as well as classiﬁcationand clustering methods. This review reveals that same method isnot applicable for all domains. From the analysis of the literature,it can be summarized that Naïve Bayes works well for or text clas-siﬁcation, clustering for consumer services, and SVM for biologicalreview and analysis.Training and testing data from same feature space and samedistribution has been reported to give good results for machinelearning algorithms. Estimating the effect of distribution changesthrough statistical models is reported to be very expensive, as ithas to be rebuilt from scratch. In many real world applications, itis expensive or impractical to recollect the needed training dataand rebuild the models. In such cases, domain adaptation or trans-fer learning between task domains would be desirable.To overcome the problem of feature distribution variance acrossdomains, Xiao and Guo[21]proposed a feature space independentsemi-supervised kernel matching method, based on a Hilbert-Schmidt Independence Criterion. Two kernel matrices were cre-ated over the instances in the source domain and the instancesin the target domain. Each labeled instance in the target domainwas deﬁnitely mapped into a source instance with the same classlabel through prediction function. Evaluation of the proposedmethod performed on Amazon product reviews and Reuters’ mul-tilingual newswire stories showed reduction in human annotationefforts.The Lexicon based approach works with the polarities of theopinion-oriented words and relies on a lexicon. A collection ofknown terms that contribute to the sentiment of a text is calledsentiment lexicon. Many open source lexicons are available whichserve as a database for extracting the polarity values of opinionwords. But these generic polarity lexicons reﬂect the most genericsentiment of opinion words. An opinion word need not express thesame sentiment everywhere, i.e., opinion words could be context-dependent or domain-speciﬁc. The word ‘‘ small”i n‘ ‘room is too small” indicates a negative opinion, whereas in ‘‘ small screen size” as seen in the mobile domain indicates a positive opinion.The variation of opinion found for the same word in differentdomains restricts the usage of generic lexicons as it generalizesthe polarity of a word. Therefore, lexicons with updated polarityvalues that can give polarity of a same word in different domainsusing same lexicon database will have to be built. The proposedwork attempts in building such an enhanced polarity lexicon usingthe maximum entropy algorithm with modiﬁcation being made inincrement quantity which helps in reﬁning the classiﬁcation gran-ularity from document to word level. The knowledge gained fromone domain is used to predict and classify the polarity of opinionwords from another domain, resulting in an improved lexiconusing semi-supervised approach. The common words from alldomains having same polarity orientation are treated as domain-independent words and remaining as domain-speciﬁc words.3. Proposed framework3.1. Generic processesMost of the existing research regarding opinion mining isdomain dependent, which limits the scope of the application aswell as the generalization of the information. Hence, generalizeddomain adaptable algorithms are needed for the automatic identi-ﬁcation and classiﬁcation of opinion lexicons.Domain adaptability is a major issue in sentiment analysis oropinion mining, which has been addressed in the proposed frame-work. There are many resources and training corpora available inEnglish with proven results. A proposed model will be trained froma training dataset, which will be used for sentiment classiﬁcation.SentiWordNet resource will be used for this research as it is a pub-licly available for opinion lexicons with polarity.An opinion lexicon is one or more words with positive or nega-tive orientation. Lexicons are used when no training data are avail-able because the training data contain prior knowledge about thesentiment of a feature. It is a vital component of unsupervised sen-timent classiﬁcation methods. The construction of a large sized lex-icon is an expensive and time-consuming task. Hence, buildingautomated methods that inﬂuence existing resources to expandexisting lexicons are needed.Domain adaptation of sentiment models from a domain withsufﬁcient labeled data to a new domain with less labeled data isa challenge that requires new and efﬁcient algorithms to solve it.The proposed system has constructed a domain adaptable lexiconwhich can adapt seamlessly from one domain to another. Theexpected outcome would be a set of lexicons with polarities for dif-ferent domains with development of robust model.Labeled set of documents from source and labeled or unlabeleddocuments from target domain is taken as input (
Fig. 1). Prepro- cessing is done to eliminate unnecessary words called as stopwords. The irrelevant data would be eliminated by this process.Most of the English sentences include words like ‘‘a, an, of, the, I,it, you, etc.” Such words do not carry any particular meaning. Infor-mation extraction from natural language can be done effectivelyand clearly by avoiding those words which occur frequently. To
Fig. 1.Workﬂow of proposed system.J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64 57remove stop words from sentences, a text ﬁle that consists of list ofEnglish stop words is used.After the removal stop word, parts of speech like noun, adjec-tive, adverb, verb, etc. are extracted using the parser. Parsing is avital step as it gives opinion words as an output. Sentence parsinginvolves assigning different parts of speech tags to a given text.This process is known as Part-Of-Speech (POS) tagging. For infor-mation extraction, POS tagging is important because each categoryplays a speciﬁc role within a sentence. Nouns give names toobjects, or entities from reviews. An adjective describes opinion.Also, some verbs and adverbs can play an important role as anadjective.Examples:.the/DT battery/NN life/NN on/IN the/DT iphone/JJ 4S/CD is/VBZamazing/JJthis/DT phone/NN is/VBZ very/RB slow/JJIn pre-processing step, text review is ﬁrst divided into sen-tences. Stanford parser is used to generate the POS tagging of eachword present in the sentence[9], as it is essential to ﬁnd generallanguage patterns.Adjectives and adverbs are good indicators of opinion, hence areextracted from each review. Some verbs are also considered asopinion, e.g., like, love, recommend, etc. Two consecutive words,i.e., adverb-verb, adverb-adjective also extracted from processedtagged reviews as a verb alone does not indicate opinion. Nounsare not considered in framework.All tagged words after POS tagging phase tagged words are clas-siﬁed using an algorithm explained in Section 3.2.3.2. AlgorithmClassiﬁcation of opinions can be done using a modiﬁed maxi-mum entropy algorithm. The increment quantity is modiﬁedaccording to the importance of the measure of words as speciﬁedin Eq.(3). The maximum entropy classiﬁer is closely related tothe Naïve Bayes classiﬁer, except that it uses a search-based opti-mization to ﬁnd weights for the features that maximize the likeli-hood of the training data. It can handle mixture of boolean, integer,and real-valued features[17]. It is also used when the conditionalindependence of the features cannot be assumed, i.e., in problemslike text classiﬁcation where features are words and are not inde-pendent[1].The main aim of the study is to construct a stochastic modelthat accurately represents the behavior of the random process.Letdbe document in a dataset; w,the word present in document;andc,the class.1. For each wordwand classc2C, a joint featurepðw;cÞ¼f(w,c) =Nis deﬁned, whereNis the number of times thatwoccurs in a document in classc.(Ncould also be boolean, registering pres-ence vs. absence.)2. Empirical distribution is used to build the statistical model ofthe random process, which distributes text to speciﬁc class.
fiðd;cÞ¼Z10if c¼c i&dcontainsw kotherwiseð1Þ
Above indicator function called as feature. Via iterative optimiza-tion, assign a weight to each joint feature so as to maximize thelog-likelihood of the training data.
3. The probability of classcgiven a documentdand weightskisPðcjd;kÞ¼def expPikifiðc;dÞP
c02CexpPikifiðc0;dÞð2Þ
wherek¼k iþd iðkis calculated by an iterative scaling algorithm Þ d
iis increment quantity
As the granularity of classiﬁcation is reﬁned from document-level to word level, the increment quantity ( d
i) is modiﬁed. The modiﬁed quantity (dm
i) is deﬁned as
dm i¼1MlogXkiidfi !; ð3Þ
whereM¼maxPkifiðd;cÞ.
Eq.(3)calculates inverse document frequency of each word,which is a popular measure of words’ importance. It is deﬁned asthe logarithmic ratio of the number of documents in a collectionto the number of documents containing the given word. This sug-gests that uncommon words have higher idf
iand common function words have loweridf
i, whereidf iis inverse document frequency.This is useful to measure the words ability to discriminate betweendocuments.Mis the sum of all features in training instance. Fea-ture value is taken astﬁdf
i.Algorithm works on word level. POS tagged words are extractedfrom preprocessing steps are used. Each word acts as feature. Fea-ture valuefiðd;cÞis calculated astﬁdf
i. of each word. As per Eq.(3) inverse document frequency of each POS tagged word is calculated.Algorithm is executed for total number of features provided ininput dataset. According to this weight words are classiﬁed intotwo categories. Classiﬁed words are having POS tag, polarity tag,and weight value. From this list, common and uncommon wordsare picked and used for bipartite graph clustering explained below.Probability distribution of classcis calculated based on term frequency. Classiﬁcation process predicts the polarity of the targetdomains lexicon from source domain. The clustering algorithm isapplied on classiﬁed word lists and documents until it reaches con-vergence. All extracted words from source domain are tagged, andweight is calculated for each word using mutual information avail-able for words. Target words are extracted and compared with thesource. If they match then they will be categorized as domain-independent, otherwise domain-speciﬁc. Domain-independentwords are from both source and target domains; whereas,domain-speciﬁc are from target domain only. A graph is con-structed between domain dependent and domain-independentwords. Co-occurrence relationship between these words repre-sents edge. Occurrence of domain-speciﬁc word along withdomain-independent word means that both a related to each otherand assign edge. Using domain-independent words weight isassigned to domain-speciﬁc words and classiﬁed accordingly. Eachﬁle form target domain is assigned score on which basis it is clas-siﬁed as positive or negative. Each word has weight assigned to it.Summation of weights of words in each sentence gives score tosentence. Then addition of all sentence score is nothing but scoreof ﬁle. On the basis of this, ﬁle is classiﬁed.Clustering helps in reducing mismatch between domain-speciﬁc words of source and target domains. Two sets of lexiconsare extracted as an output with polarity which is compared withSentiWordNet (Fig. 2).4. Result and discussions4.1. Experiment1The dataset from John et al.[6]was used for experiments. It contains a collection of product reviews from Amazon.com. This dataset contains three types of ﬁles positive, negative and unla-beled in XML format. Each line in form of: feature:<count> .... fea58 J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64ture:<count>#label#:<label>, e.g., old_boy:1 i_am:1 the: 1 boy_-had:1 so_i:1 #label#: negative. These ﬁles were extracted usingXML ﬁle splitter and reviews were converted into text ﬁle. Thedataset contains 1000 positive ﬁles and 1000 negative ﬁles for eachdomain. The reviews are about four product domains: Books (B),DVDs (D), Electronics (E) and Kitchen appliances (K) and are writ-ten in English language. For experiment, labeled dataset of 1000positive and 1000 negative ﬁles was used. An instance in eachdomain is recorded inTable 1. Except book domain other domainshad more number of positive instances.From this dataset, 12 cross-domain sentiment classiﬁcationtasks were constructed:B!D;B!E;B!K;D!B;D!E; D!K;E!B;E!D;E!K;K!B;K!D;K!E, where the word before an arrow corresponds to the source domain and theword after an arrow corresponds to the target domain.FromTable 2, it is evident that Book and DVD, if considered as asource domain, achieve a good compatibility with electronics andkitchen domain, which is considered as target domain. Besides,electronic and kitchen are compatible domains.Baseline methods use in this study are Feature Ensemble plusSample selection (SS-FE)[27], Spectral feature alignment (SFA)[23], and Supervised word clustering (SWC) [20]. SFA achieved was between 72.5% and 86.75%, SS-FE was between 72.94% and84.87% and SWC was between 72.11% and 85.33%, whereas accu-racy of proposed algorithm was between 70% and 88.35%.Only the DVD, electronics, and kitchen were considered as thesource domain, while book, kitchen and DVD as a target domain,producing comparatively less accurate results than the baselinemethod (Fig. 3). There are two key points in proposed framework:ﬁrst it classiﬁes the words and documents, and then clusters them.After classiﬁcation step, opinionated words are extracted withweight value as well as polarity. These weights are very importantfactor as it increases the importance of discriminative terms. Pro-posed approach also identiﬁes domain-independent and speciﬁcfeatures which are used for clustering. All classiﬁed words clus-tered again leads to acceptable results. One of the reasons forlow accuracy for some domain is imbalance of class labels andthe presence of word disambiguation.Accuracy is used as an evaluation measure. Accuracy is the pro-portion of correctly classiﬁed examples to the total number ofexamples; on the other hand, error rate refers to incorrectly classi-ﬁed examples to correctly classiﬁed examples. F-measure or preci-sion and recall can be used as evaluation measures.F-measure is only deﬁned in terms of true positive (TP), false
positive (FP) and false negative (FN), while true negative (TN) isnot considered. Accuracy and F-measure is compared for proposedapproach which shows that, in general, F-measure is similar toaccuracy. But only single class is considered in F-measure as posi-tive class (Fig. 4). On the other hand, when calculating accuracyequal weight is given to both the classes.Classiﬁed words are used to ﬁnd domain-independent anddomain-speciﬁc words from the respective domains. Domain-independent and domain-speciﬁc words are compared to the Sen-tiWordNet[3], in order to ﬁnd out how many words match withthem (Tables 3–6). From the tables, it has been observed that onan average 72.6% domain-speciﬁc words are correctly classiﬁedfor different domains; while88.4% words from domain-independent word list are correctly classiﬁed. Domain-independent words typically occur in every domain; hence, match-ing percentage is more than the matching percentage of domain-
Fig. 2.Flow of proposed algorithm.
Table 1Negative and positive instances for multi-domain dataset.
Domain Name Negative Instances Positive InstancesBook 73,500 72,794DVD 66,126 76,759Electronics 43,806 44,321Kitchen appliances 36,106 36,733Table 2Comparative analysis of accuracy of proposed method and baseline methods.
Source!Target Accuracy (%)Proposed MethodAccuracy(%) SS-FEAccuracy(%) SFAAccuracy(%) SWCB!D82.45 79.10 82.55 81.66B!E78 74.24 72 77.04B!K78.65 78.07 78 82.26D!B74.35 80.38 77 79.95D!E79.78 77.07 77 76.98D!K84.21 77.82 81 82.13E!B82.15 72.86 75.5 72.11E!D87.8 74.60 77 73.81E!K81.44 84.87 87.1 85.33K!B81.05 72.94 74 75.78K!D70 75.70 77 76.88K!E88.35 82.93 84.6 84.78
Fig. 3.Accuracy analysis.J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64 59speciﬁc words. As SentiWordNe thas generalized opinion lexicons,percentage of matching domain-speciﬁc words is relatively less.4.2. Experiment 2To evaluate accuracy of proposed algorithm for unlabeled targetdataset, we have performed 12 cross-domain tasks with labeledsource dataset and unlabeled target dataset. Fig. 5illustrates the accuracy analysis. Accuracy achieved by proposed method forunlabeled target lies between 65.65% and 98.0%; whereas, labeledtarget achieves accuracy between 70.0% and 88.35%. Performanceof classiﬁer, therefore, increases signiﬁcantly. It shows that elec-tronics, kitchen and DVD domains are compatible with each otherdue to their more similar features. In general, the features of thesethree domains are more or less similar. Also kitchen as source andelectronics as target and vice versa gives better accuracy as bothdomains share more common features. Kitchen appliances domainalso shares electronics appliances; hence, major information ofboth domains is similar. But kitchen as source and DVD as targetdoes not give good results for both labeled and unlabeled dataset,because kitchen and DVD are not similar to each other, as that ofkitchen and electronics. Usually, if two domains are more similarthen a number of features transferred from source to target arealso more because source data is used as training dataset and tar-
Fig. 4.Analysis of accuracy vs F-measure.
Table 3Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering book (B) as a source domain.
Domains Domain-speciﬁc words Domain-independent words No. of words matching SentiWordNetDomain-speciﬁc words Domain-independent wordsB?D 11,503 9744 8934 8972B?E 5250 4796 4077 4395B?K 4200 4325 3262 3979
Table 4Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering DVD(D) as a source domain.
Domains Domain-speciﬁc words Domain-independent words No. of words matching SentiWordNetDomain-speciﬁc words Domain-independent wordsD?B 11,130 9744 8644 9009D?E 5238 4781 4068 4418D?K 4205 4320 3266 3980
Table 5Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering Electronics (E) as a source domain.
Domains Domain-speciﬁc words Domain-independent words No. of words matching SentiWordNetDomain-speciﬁc words Domain-independent wordsE?B 16,105 4769 12,508 4430E?D 16,466 4781 12,789 4462E?K 4802 3723 3729 3454
Table 6Comparison of domain-speciﬁc and domain-independent words against SentiWordNet considering Kitchen (K) as a source domain.
Domains Domain-speciﬁc words Domain-independent words No. of words matching SentiWordNetDomain-speciﬁc words Domain-independent wordsK?B 16,549 4325 12,853 3980K?D 16,927 4320 13,147 3987K?E 6296 3723 4890 344960 J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64get features are derived from it. The impact of unlabeled targetdata is more than that of labeled target data. It states that unla-beled target data can be used for the accuracy gain, as well asreduce the annotation cost signiﬁcantly.4.3. Experiment 3Generally, Naïve Bayes and SVM algorithms are used for textclassiﬁcation. Experiment 3 was conducted to evaluate proposedframework. using Rapid Miner 5.3.015 software for Naïve Bayesand SVM algorithm,. The software contains text mining plug-inwhich converts non-structured textual data into structured formatfor further analysis. This study adopted implemented linear SVM asmost of the text classiﬁcation problems are linearly separable. Fur-ther, as text classiﬁcation contains large number of features, linearkernel was found to be better suited for this purpose. Results wereobtained from Blitzer dataset for four different domains of uni-grams and for applying word frequency in document and in entirecorpus.Naïve Bayes classiﬁer is a probabilistic classiﬁer based on prob-ability models that incorporate strong independence assumptionsamong the features. Independence assumption of features is a sub-tle issue with Naïve Bayes. If certain feature and class label valuedo not occur together then the frequency-based probability esti-mate will become zero. When all the probabilities are multiplied,the answer will be zero and this affects the posterior probabilityestimate. Hence, it provides poor accuracy compared to the SVMfor some domains. The result shows that the proposed algorithmwas better than both SVM and Naïve Bayes.Figs. 6and7provides accuracy and F-measure analysis for eachof 12 cross-domain sentiment classiﬁcation tasks on Amazon pro-duct reviews. For this study, 1000 positive and 1000 negativereviews were taken as the source and target domains. Comparisonof proposed approach with baseline approaches shows thatdomain adaptation from book as source domain to the DVD as tar-get domain rather than kitchen as source to DVD as target domainis more feasible. It also shows that relatedness between source andtarget domain reviews are more important factors for the effective-ness of domain adaptation.Electronics as a source domain is more compatible with everydomain. This suggests that more number of features are relevantin both source and target domains. The proposed approach pro-vides the highest accuracy for electronics as compared to baselineas well as SVM and Naïve Bayes. Compared to accuracy, F-measureresults are improved but these are only related with positive doc-uments. Results show that the proposed approach have betteraccuracy and F-measure. Further, Naïve Bayes provides betterresults than SVM. Some of the major drawback of Naïve Bayes isassumption of independent attributes and difﬁculty in interpreta-tions of SVM results. In proposed framework, maximum entropythat is used provides a natural mechanism of multiclass classiﬁca-tion. The results are better as increment quantity focuses on termfrequency and inverse document frequency. It concentrates on fea-tures and its presence which is not focused in Naïve Bayes or SVM.Random Trees area collection of individual decision trees, inwhich each tree is generated from different samples and subsetsof the training data. Classiﬁcation of dataset based on randomsub selection of training samples result in many decision trees,hence this method is called Random trees. Each tree can be votedto make ﬁnal decision.An experiment was carried using Rapid Miner 5.3.015 software.Results are recorded inFig. 8, which shows the comparisonbetween the proposed approach and the Random tree. The pro-posed approach gives better accuracy than the Random tree. Therandom trees classiﬁer takes the input feature vector, classiﬁes itwith every tree in the forest, and produces the class label thatreceived the majority of ‘‘votes” as output. Using bootstrapapproach, training sets are generated. Vectors are randomlyselected, hence some vectors will occur more than once or somewill be absent. All variables are not used to ﬁnd the best split.In contrast, the proposed approach works at word level whereeach word acts as a feature. Later, importance of word is analyzedusing term frequency and inverse document frequency of eachword consequently producing better accuracy. Using Random treehigher accuracy achieved in Electronics as source and kitchen astarget domain wherein the proposed approach it is reverse way.
Fig. 6.Accuracy analysis for 12 cross-domain classiﬁcation tasks.
Fig. 5.Accuracy analysis of unlabeled and labeled target.J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64 61Therefore, it is clear that electronics and kitchen are more compat-ible domains as they are having similar features.4.4. Experiment 4For testing the model, Amazon’s balanced 6cats dataset col-lected by Mark Drezde and processed by Richard Johansson in2012 has been used for this study. The review collection is dividedinto six topic subdirectories: Books, Camera, DVD, Health, Musicand Software. The document in each topic directory is divided intopositive and negative subdirectories. From this dataset, 30 classiﬁ-cation tasks were constructed. For 1000 positive and 1000 negativeﬁles of each domain, the accuracy was achieved between 70% and97% (Fig. 9).Highest accuracy achieved in software as source and camera astarget domain. B!C;C!B;D!M;H!S;M!B;S!C clas-
Fig. 7.F-measure analysis for 12 cross-domain classiﬁcation tasks.
Fig. 8.Accuracy comparison with Random Tree.
Fig. 9.Accuracy analysis for Amazon balanced 6 cats dataset.62 J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64siﬁcation tasks are giving high accuracy. From the results, it wasfound that book, camera and music domains are having more com-mon features.Tables 7–9present domain-speciﬁc, domain-independent andmatching words with SentiWordNet. Domain-independent wordsare from both source and target domains. Domain-speciﬁc are onlyfrom target domains. With SentiWordNet matching percent isaverage 55.7%.4.5. DiscussionFor our experiments, two different datasets were used. Eachdataset consisted of labeled positive and negative text review doc-uments. All the results from above sections reveal that the pro-posed approach gives better accuracy than baseline methods.Word is important entity as it indicates sentiment or opinion ofobject. The proposed framework is based on modiﬁed entropy clas-siﬁer. Opinionated words are extracted based on the term fre-quency and inverse document frequency. Increment quantity ismodiﬁed as granularity reﬁned from document to word levelwhich shows drastic difference between traditional maximumentropy and modiﬁed entropy. Bipartite graph clustering is appliedon classiﬁed data which has enhanced the results.As compared to baseline methods, moderate accuracy wasachieved by the proposed method. Relatedness between sourceand target domain is important factor in domain adaptation. Alsofor unlabeled target dataset better accuracy was achieved. It meansthat it can signiﬁcantly reduce the annotation cost also. F-measureis also taken as evaluation measure which shows better results ofproposed framework over base line methods. But it does not con-sider the true negative features. Some domains are not compatibleto each other or relatedness between these domains is less, hencethe results are lower.5. ConclusionsOpinion mining is a popular research area; yet, researchers havemainly focussed on domain adaptation. This work addressed themajor issue of domain adaptation. In this work, semi-supervisedapproach was used which holds maximum entropy classiﬁer withmodiﬁed increment value and bipartite clustering. Labeled as wellas unlabeled set of lexicons from different domains were collectedfrom Amazon are used for experimental analysis of the proposedapproach. Pre-processing step was used to remove noise fromdataset. Each word from dataset is tagged for parts of speech usingthe Stanford parser. This tagged data is used by classiﬁer which isbased on featurestﬁdfiand idf i,. value which is useful to measurethe words’ ability to discriminate between documents. Domain-speciﬁc and domain-independent lexicons are used for clustering.Classiﬁed lexicons are compared with SentiWordNet 3.0 to ﬁndmatching percentage as SentiWordNet is publicly available lexiconresource.This work was able to produce relatively good results for someof the domain, and it was able to handle only two classes with anacceptable accuracy. Domain-speciﬁc and domain-independentwords compared to SentiWordNet 3.0 shows average matchingpercent 68.25%. The experimental results of proposed approachhave shown a signiﬁcant increase in accuracy for different domainsover baseline approach as the proposed framework emphasizes ongranularity of the word. This is the major change in classiﬁer incomparison to traditional approach. Importance of each word thathas more impact on results of classiﬁer was classiﬁed. Testing ofapproach carried on Amazon cat6 dataset, which shows a signiﬁ-cant improvement in accuracy ranging from 3 to 6 points com-pared to dataset from Blitzer. In comparison to SVM and NavieBayes, we have proposed an algorithm that could provide betteraccuracy. It shows that relatedness between domains is a majorfactor for effectiveness of domain adaptation.In the proposed system, bipartite graph clustering was used toreduce the mismatch between domain speciﬁc words of sourcedomain and target domain. Domain-independent words were usedto cluster domain-speciﬁc words from source and target domains.To train classiﬁer for target domain, clustering was used as itreduced the gap between domain-speciﬁc words of differentdomains. Future studies can be taken up to determine the co-clustering of words and documents from different domains. Theproposed system focuses on only words, in future non-word fea-tures like the age of document, the recommendation counts of doc-ument can be considered. At present, framework considers onlyunigrams and reviews are in English language. Also in future thiswork can be extended for other languages as well as n-grams.References
[1] Abinash Tripathy, Ankit Agrawal, Santanu Kumar Rath, Classiﬁcation ofsentiment reviews using n-gram machine learning approach, in: ExpertSystems with Applications, vol. 57, 2016, pp. 117–126.[2] Aytug Onan, Serdar Korukoglu, Hasan Bulut, A multiobjective weighted votingensemble classiﬁer based on differential evolution algorithm for textsentiment classiﬁcation, in: Elsevier Expert Systems With Applications, vol.62, 2016, pp. 1–16.[3] Baccianella Stefano, Esuli Andrea, Sebastiani Fabrizio, SentiWordNet 3.0: AnEnhance Lexical Resource for Sentiment Analysis and Opinion Mining, in:Proceedings of the 7th Language Resources and Evaluation Conference (LREC2010), Valletta, Malta, May 17–23, 2010, pp. 2200–2204.[4] A. Bermingham, M. Conway, L. McInerney, N. O’Hare, A. Smeaton, CombiningSocial network analysis and sentiment analysis to explore the potential foronline radicalisation, in: Proc. of Int’l Conf. on Advances in Social NetworkAnalysis and Mining, Athens, Greece, July 20–22, 2009, pp. 231–236.Table 7Domain-independent, Domain-speciﬁc and SWN matched words for Book as sourcedomain.
Domains Domain-speciﬁcwordsDomain-independentwordsSWN matchedwordsB?C 3942 4459 4533B?D 12,044 9896 10,358B?H 4356 4468 4444B?M 10,304 7438 7910B?S 4712 5222 5356
Table 8Domain-Independent, Domain-Speciﬁc and SWN matched words for DVD as sourcedomain.
Domains Domain-speciﬁcwordsDomain-independentwordsSWN matchedwordsD?B 10,519 9896 10,259D?C 3770 4631 4670D?H 4306 4518 4481D?M 9305 8437 8883D?S 4741 5193 5342
Table 9Domain-independent, Domain-speciﬁc and SWN matched words for Camera assource domain.
Domains Domain-speciﬁcwordsDomain-independentwordsSWN matchedwordsC?B 15,956 4459 5049C?D 17,309 4631 5264C?H 5186 3638 3652C?M 13,845 3897 4410C?S 6029 3905 4142J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64 63[5]Bing Liu, Sentiment Analysis & Opinion Mining, Kindle Edition., Morgan &Claypool Publishers, 2012
. [6] John Blitzer, Mark Dredze, Fernando Pereira, Biographies, Bollywood, boom-boxes and blenders: domain adaptation for sentiment classiﬁcation, in:Association of Proceedings of the 45th Annual Meeting of the ComputationalLinguistics (ACL), Prague, Czech Republic, June 2007, pp. 440–447.[7] D. Bollegala, D. Weir, J. Carroll, Cross-domain sentiment classiﬁcation using asentiment sensitive thesaurus, in: Knowledge and Data Engineering, IEEETransactions, vol. 25(8), 2013, pp. 1719–1731.[8] N.F. Da Silva, E.R. Hruschka, E.R. Hruschka, Tweet sentiment analysis withclassiﬁer ensembles, in: Elsevier Decision Support Systems, vol. 66, 2014, pp.170–179.[9] M.C. De Marneffe, B. MacCartney, C.D. Manning, Generating typed dependencyparses from phrase structure parses, in: Proceedings of LREC, vol. 6, 2006, pp.449–454.[10]
E. Fersini, E. Messina, F.A. Pozzi, Sentiment analysis: Bayesian EnsembleLearning, Elsevier Decision Support Syst. 68 (2014) 26–38
. [11] A. Esuli, F. Sebastiani, Senti-WordNet: A Publicly Available Lexical Resource forOpinion Mining, in: Proc. of the 05th Conf. on Language Resources andEvaluation, Genova Italy, May 22–28, 2006, pp. 417–422. Available at < http:// sentiwordnet.isti.cnr.it/>. [12]
Farhan Hassan Khan, Usman Qamar, Saba Bashir, S WIMS: semi-supervisedsubjective feature weighting and intelligent model selection for sentimentanalysis, Knowledge-Based Syst. 100 (2016) 97–111
. [13]
Haiqing Zhang, Aicha Sekhari, Yacine Ouzrout, Abdelaziz Bouras, Jointlyidentifying opinion mining elements and fuzzy measurement of opinionintensity to analyze product features, Eng. Appl. Artiﬁcial Intelligence 47(2016) 122–139
.[14]
Kumar Ravi, Vadlamani Ravi, A survey on opinion mining and sentimentanalysis: tasks, approaches and applications, Knowledge Based Syst. 89 (2015)14–46
.[15]
J.V. Lochter, R.F. Zanetti, D. Reller, T.A. Almeida, ShortText opinion detectionusing ensemble of classiﬁers and semantic indexing, Expert Syst. Appl. 62(2016) 243–249
.[16] Lukasz Augustyniak, Tomasz Kajdanowicz, Piotr Szyma ´nski, Włodzimierz Tuligłowicz, Przemyslaw Kazienko, Reda Alhajj, Boleslaw Szymanski, Simpleris better? lexicon-based ensemble sentiment classiﬁcation beats supervisedmethods, in: International Workshop on Curbing Collusive Cyber-gossips inSocial Networks (C3-2014), August 17, 2014 Proc. IEEE/ACM Int. Conf.Advances in Social Network Analysis and Mining, ASONAM, Beijing, China,2014.[17] McCallum Andrew, Freitag Dayne, Pereira Fernando, Maximum entropymarkov models for information extraction and segmentation, in: 17thInternational Conf. on Machine Learning, Stanford University, June 29-July 2,2000.[18]W. Medhat, A. Hassan, H. Korashy, Sentiment analysis algorithms andapplications: a survey, Ain Shams Eng. J. 5 (4) (2014) 1093–1113
. [19]
G.A. Miller, WordNet: a lexical database for English, Commun. ACM 38 (11)(1995) 39–41
.[20] Min Xiao, Feipeng Zhao, Yuhong Guo, Learning latent word representations fordomain adaptation using supervised word clustering, in: Proceedings of the2013 Conference on Empirical Methods in Natural Language Processing, pages152–162, Seattle, Washington, USA, 18–21 October, 2013.[21]
Min Xiao, Yuhong Guo, Feature space independent semi-supervised domainadaptation via kernel matching, IEEE Trans. Pattern Anal. Mach. Intelligence 37(1) (2015) 52–66
.[22]
S. Pan, Q. Yang, A survey on transfer learning, IEEE Trans. Knowledge Eng. 22(10) (2009) 1345–1359
. [23] Pan SinnoJialin, Xiaochuan Ni, Jian-Tao Sun, Qiang Yang, Zheng Chen, Cross-domain sentiment classiﬁcation via spectral feature alignment, in:Proceedings of the 19th International World Wide Web Conference, ACM,Raleigh, USA, April 26–30, 2010.[24]
B. Pang, L. Lee, Opinion mining and sentiment analysis, Found. Trends Inform.Retrieval 2 (1–2) (2008) 1–135
. [25] Paramveer S. Dhillon, Partha Talukdar, Koby Crammer, Metric Learning forGraph-Based Domain Adaptation, University of Pennsylvania Department ofComputer and Information Science Technical Report No. MS-CIS-12-17,January 2012.[26] Rui Xia, Chengqing Zong, Shoushan Li, Ensemble of feature sets andclassiﬁcation algorithms for sentiment classiﬁcation, in: InformationSciences, vol. 181(6), 15 March 2011, pp. 1138–1152.[27]
Rui Xia, Chengqing Zong, Xuelei Hu, E. Cambria, Feature ensemble plus sampleselection: domain adaptation for sentiment classiﬁcation, IEEE Intelligent Syst.28 (3) (2013) 10–18
.[28]
Rui Xia, Feng Xu, Jianfei Yu, Yong Qi, Erik Cambriac, Polarity shift detection,elimination and ensemble: a three-stage model for document-level sentimentanalysis, Inform. Process. Manage. 52 (2016) 36–45
. [29] Shoushan Li, Yunxia Xue, Zhongqing Wang, Guodong Zhou, Active learning forcross-domain sentiment classiﬁcation, in: Proceedings of IJCAI’13 the Twenty-Third International Joint Conference on Artiﬁcial Intelligence, 2013, pp. 2127–2133.[30]
P.K. Singh, M.S. Husain, Methodological study of opinion mining andsentiment analysis techniques, Int. J. Soft Comput. 5 (1) (2014) 11
. [31] M. Wang, H. Shi, Research on sentiment analysis technology and polaritycomputation of sentiment words, in: Proc. of Int’l Conf. on Progress inInformatics and Computing, Shanghai, vol. 1, December 10–12, 2010, pp. 331–334.[32] Zheng Chutao, Liu Cheng, Wong Hau-San, Iterative term weighting for shorttext data, in: Proceedings of the 2015 IEEE International Conference onSystems, Man, and Cybernetics, Hong Kong, 9–12 October 2015.64 J.S. Deshmukh, A.K. Tripathy / Applied Computing and Informatics 14 (2018) 55–64