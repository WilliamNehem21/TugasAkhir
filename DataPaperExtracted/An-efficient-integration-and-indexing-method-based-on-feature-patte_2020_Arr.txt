An efﬁcient integration and indexing method based on feature patterns andsemantic analysis for big data
Madhu Mahesh Nashipudimatha,b,*, Subhash K. Shindec, Jayshree Jaina
aDepartment of Computer Engineering, Paci ﬁc Academic Higher Education and Research University, Udaipur, India
bDepartment of Computer Engineering, Pillai College of Engineering, New Panvel, Navi Mumbai, India
cDepartment of Computer Engineering, Lokmanya Tilak College of Engineering, Navi Mumbai, India
ARTICLE INFO
Keywords:Big dataIntegrationFeature patternsIndexingSemantic analysisABSTRACT
Big Data has received much attention in the multi-domain industry. In the digital and computing world, infor-mation is generated and collected at a rate that quickly exceeds the boundaries. The traditional data integrationsystem interconnects the limited number of resources and is built with relatively stable and generally complex andtime-consuming design activities. However, the rapid growth of these large data sets creates dif ﬁculties in learning heterogeneous data structures for integration and indexing. It also creates dif ﬁculty in information retrieval for the various data analysis requirements. In this paper, a probabilistic feature Patterns (PFP) approachusing feature transformation and selection method is proposed for ef ﬁcient data integration and utilizing the features latent semantic analysis (F-LSA) method for indexing the unsupervised multiple heterogeneous integratedcluster data sources. The PFP approach takes the advantage of the features transformation and selection mech-anism to map and cluster the data for the integration, and an analysis of the data features context relation usingLSA to provide the appropriate index for fast and accurate data extraction. A huge volume of BibText dataset fromdifferent publication sources are processed to evaluated to understand the effectiveness of the proposal. Theanalytical study and the outcome results show the improvisation in integration and indexing of the work.
1. IntroductionDue to the emergence of digital data and communication networks, ahuge collection of repositories has become available to the public. Manytools are available to extract information [ 1] from the various sources, but it is essential to extract data effectively and accurately. Some re-searchers came up with models and discussions where integration andindexing play important role for mining the information or querying.Integrating and indexing needs an accurate learning model to associatethe data [2] and have an effective result. This task turns more challengingto develop efﬁcient data integration and indexing for relevant informa-tion when the data is in unstructured form of distribution.Data integration is a computationally efﬁcient and accurate method in theﬁeld of data mining for the data categorization. The purpose ofdata integration is to serve trusted data from a variety of sources. It is aneed to handle a large data set in terms of volume, dimensions, andcomplexity of data features. The core objective of data integration in real-time data from various domain sources is to generate valuable andmeaningful information. The conventional data integration techniquesprocess large volume of data utilizing the process of "ETL (extract,transform and Load)".The source of information from many heterogeneous data sourcegenerates several challenges for integrating with the conventional dataintegration techniques. The heterogeneity can exist at the schema level,where different data sources often describe the same domain usingdifferent representations. It can also exist at the instance level, wheredifferent sources can represent the same real-world entity in differentways. For example, the bibliographical data of academic articles repre-sents diverse information but contextually they are related to a subjectcategory. Many approaches for analyzing the heterogeneity data forintegration are suggested in past through schema mapping [ 3], record linkage through object reference and entity matching. But the challengesare still lying for integrating multiple source and unstructured data ob-jects, due to its probability of information description and diversity in thefeatures.The indexing refers to the organization of data according to a speci ﬁc schema or plan. The indexing provides a list of tags or names to access theintegrated and structured data in fast and accurate manner. Indexing is
* Corresponding author. Department of Computer Engineering, Paci ﬁc Academic Higher Education and Research University, Udaipur, India. E-mail address:madhumn@mes.ac.in(M.M. Nashipudimath).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100033Received 2 June 2019; Received in revised form 22 April 2020; Accepted 4 June 2020Available online 10 June 20202590-0056/©2020 Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 7 (2020) 100033applied for big data to perform retrieval tasks from voluminous, complexdatasets with scalable, distributed storage in cloud computing. It isimpractical to perform manual exploration on such records. An ef ﬁcient, high-throughput indexing technique would optimize the performance ofdata query operations. Therefore, efﬁcient indexing techniques are required to effectively access big data, but the diverse in data featurescreate many challenges to index a data object accurately.The big data growing rate in real-time makes indexing and querying ishighly challenging task. The schemes of Big Data indexing [ 4]i st o fragment the datasets according to criteria that will be used frequently inquery [5] The fragments are indexed with each containing value satis-fying some query predicates. This is aimed at storing the data in a moreorganized manner, thereby easing information retrieval. Hence a rightindexing technique is essential to effectively access big data such as,semantic indexing-based approach [ 2] bitmap indexes [4] etc. Indexing play an important role in supporting queries on high-dimensional bigdata efﬁciently where queries with value and dimension sub-settingconditions are commonly used by researchers to ﬁnd useful informa- tion from big data.The rightful necessity of an efﬁcient integration and indexing methodfor improvising the big data query processing motivates for a better ideafor information retrieval in big data. The study of the feature extractionand selection suggested an effective mean for integrating unsuperviseddata to a meaningful group and also support in semantic indexing-basedapproaches.Few works are proposed for the integration [ 7] and indexing [4]t o simplify the data accessing. It’s difﬁcult to conclude about integration and indexing with these discussed articles due to its inappropriate inte-gration and indexing and heterogeneity of information in the article. Inthis regard, the integration of bibliographical data is considered as one ofthe most important tasks in the area of digital library [ 1]. A huge volume of bibliographical data of the research articles form abig data collection from the multiple publication sources which can beused as source for integration and indexing process.The goal of this article is provide integration approach based on PFPand indexing based on F-LSA, as a solution to the researchers and sci-entists for utilizing the bibliographical data. It will help for to ef ﬁcient search of literature and identify latest research trends through mining thelinking of articles.The structure of this paper is as follows: Background study is dis-cussed in Section2. Section3represents the proposed integration andindexing method, Section4is a discussion of the experimental evaluationand results. Conclusion of the proposed work is summarized in Section 5.2. Background studyData integration and uncertainty management are central topics intheﬁeld of data quality. Data integration [ 3] is one of the basic activities used to improve the quality of data which is distributed among inde-pendent data sources. The traditional data integration systems are sys-tems interconnecting a limited number of resources, which are relativelystable in time. These have been typically built with complex andtime-consuming design activities.As discussed in Ref. [6] of 2018, data integration system needs tohandle uncertainty levels on the semantic mappings between the datasources and the mediated schema. It is essential for effective constructionof indexing based on the keywords for data accessing queries. It ispossible by tagging features for schema elements. It helps to discovermappings through understanding the meaning of tagging features. But as
per work done in 2019, it has many challenges for understanding thedependable features [8] and its associations for accurate integration[10].The number of variables or features is often increasing to describe theinformation in many domains for the different form of data, such asmultimedia data, web data, research articles [ 1] etc. Sometimes, these high-dimensional data consists of many multiple values based on itsobservations for a feature. In fact, all the features are often not importantand distinctive because they are mostly found correlated or redundant toeach other [11]. It might be very noisy for the selection [ 12] in some scenario. The function of these high dimensions may lead to low ef ﬁ- ciency or poor performance in conventional learning models [ 13]. Therefore, it is challenging to learn the high-dimensional data featureswhich will help to improve the accuracy and comprehensibility of results.It also has challenges to eliminate the unrelated and repeating featuresthat involve in large volumes of data, which is a need to select from asubset of the data feature.Integration is a computationally efﬁcient and accurate method in theﬁeld of data mining for the data categorization. It coordinates to handle alarge data set in terms of volume, dimensions, and complexity of datafeatures. Various approaches are proposed for multi-domain [ 6] and uncertain data [8] for efﬁcient and accurate integration. An uncertaindata objects clustering based on the probability of similarity distributionis presented in recent paper [9] of 2019. It suggests that, the difﬁculties of integrating the uncertain data object is due to probability distributionwhich arises in many situations. The problem related distribution basedfeature similarity can be worked out effectively by means of the featureselection [11] and the feature extraction [14] methods. It will reduce the dimension of featuresﬁnding a meaningful feature set group andselection.Feature selection method is classiﬁed as a regulation method of feature selection for the mostly termed as "supervised" or "unsupervised"feature selection. Supervised feature selection methods uses the rela-tionship between characteristics and feature information [ 13] that leads to selection of the signiﬁcant and relevant features [11]. But studying large volume of data in unsupervised feature selection leads to dif ﬁculty for analysis. It happens due to deﬁcient of feature information to referfeature selection. Hence this article focuses on the issue of unsupervisedfeature selection.2.1. Importance of feature selectionThe process of identifying the suitable and effective features is animportant task. It is one of the most widely used mechanism in variousmulti-domain analysis [6,12] and information mining [1]. Some re- searchers had speciﬁcally worked on possible ways of data collection too.The article [10] published in 2018 ranks different classiﬁcation methods separately for hard data sets and for easy data sets. Random forest isreported as winning method. Classiﬁers ranking is analyzed usingdifferent datasets with varying instances and attributes but data size re-mains as future effort. And hence it offers new insights and motivation todevelop and upgrade classiﬁcation and feature selection approach usingbig data.The method of selecting a variety of features for a machine learningapplication have been studied and proposed by T. Nguyen et al. [ 15]. These algorithms can be categorized as "supervised", "semi-supervised"and "unsupervised algorithms" based on the method of using the learningfeature information [13]. The supervised methods are looked in the waythat they are able to choose the distinctive features [ 16] as latest art of 2019,because the information is encoded in the differential features.Using the available feature correlation, one can understand scanty-basedmethods based on form of semantic clustering [ 17] mentioned during 2013 and model-based clustering [18] mentioned in 2019. This study suggests that data model with small number of features for a large vol-ume of unlabeled data will generate result. It can be easily con ﬁgured to construct a common cluster data set. But it may face the problem ofhighly unrelated data integration due to the low feature sample problem,which is challenging for the supervised learning and data management.Selvakumar et al. [21] had worked on the implementation of threedifferent classiﬁcation techniques. These experiments of classi ﬁcation were carried out with a multi-dimensional healthcare dataset. Thismultidimensional diabetes disease dataset contained 100 observationswith 7 attributes. k-Nearest Neighbor gives higher accuracy as comparedM.M. Nashipudimath et al. Array 7 (2020) 100033
2to accuracy of Binary Logistic Regression and Multilayer Perceptron.Working with high dimension, and huge sized dataset on speci ﬁc suitable platform remains as future scope.As there is no enough information about the data features and smallfeatures, supervised algorithms may unintentionally remove many spe-ciﬁc features or fail with select features that are not relevant. Thus thesemi-supervised feature selection can be developed to take advantage ofthe labelled and labelled data. But semi-supervised feature selectionsystem has to guide to search for distinctive features in absence of labels.Hence it is seen as a much more difﬁcult problem [13]. It evaluates the characteristic of interest for their ability to retain some element prop-erties. In many real-world applications, the lack of unlabeled data andrapid accumulation of high-dimensional resources creates the challengesfor the labels identiﬁcation. Hence its demands for a very promising andautonomic system for unsupervised feature selection technologies tomeet the integration problem.2.2. Unsupervised feature selection for integrationIn unsupervised feature selection based integration, informationidentiﬁes a subset of the features of holding a unique group as no label isavailable. These groups are in accordance with the speci ﬁed standard for the more challenging grouping or clustering criterion [ 15]. The difﬁ- culties in the unsupervised feature selection are mostly being solvedusing the probability model methods [ 9,15]. Here the features are sorted into the group of labels utilizing the "feature selection based grouping"considering the latent features variable. Venkatesh, B, and J. Anuradhal
[13] proposed a visual feature and user separate class features for agenerative model in 2019. The purpose is to the group the high dimen-sional relevant data. Similarly W. Fanet al [ 14] proposed a probability model in 2013 for the global integration capabilities and unsupervisedfeature selection.Y. Guan et al. [22] suggested a framework of reasoning of the vari-ations of the "unsupervised non-Gaussian" method for feature selection.Unlike previous research, this work is based on the unsupervised learningmodel for data transformation and feature selection, the purpose toenhance the data integration task for better results. It will maintain se-mantic similarities along with selection of best features for distinguishingknowledge of the original data information in heterogeneous datasets.The feature Patterns is applied to each input key-value pair to generate anarbitrary number of intermediate key-value pairs. The reducer is appliedto all values associated with the same intermediate key to generateoutput key-value pairs.2.3. TechniquesThe biblometric study [7] is carried out in 2018 to analyze the inte-grated care literature and tests the usefulness of indexing the literaturewithin PubMed. This idea boosted to experiment with Bibtex data on bigdata platform. Few Literatures [5,9] guide to know and analyze multipletechniques for indexing with pros and limitations.In this paper [19], the modiﬁed Feature vector selection (FVS) method, easy tuning version of Support vector machine(SVM) selects asmall number of data points for implementation. The FVR model is alsosolved analytically, as in least-squared SVM. Authors had worked withtwenty six imbalanced dataset and comparison is done with several SVMbased methods. Result show effectiveness of suggested method. Henceproposed method is also compared with SVM in integration performanceanalysis section.Yousefpouer et al. [20]. had worked in 2017 on algorithm, whereTDM is a term-document matrix that is weighted using the termfrequency-inverse document frequency (TF-IDF) scheme. The most rele-vant features were selected from the frequency-based integration toproduce theﬁnal feature subset. The experimental results for differentdatasets show that the proposed methods can effectively improve clas-siﬁcation performance. Hence performance analysis of proposed methodis carried with respect to Term frequency.Recently in 2019, Ritik M, and Abhaya K ⋅S proposed text analysis to convert understood text data into meaningful data for analysis and pro-vide sentiment analysis. Semantic based Naïve Bayesian classi ﬁcation algorithm [25,35] is used for textﬁltering and performance accuracy iscalculated. This article inspired to implement Naive Bayes and furtheradd a component of probabilistic feature Patterns (PFP) approach andperform experiment on big data. Purpose of PFP component and exper-imenting with big data is to analyze the performance and its contributionin indexing and recommendation as this task is yet to be experimentedwith.The big data analytics on IoT based healthcare system is developedusing the Random Forest Classiﬁer (RFC) and MapReduce process [33]a s recent work of 2019. The optimal attributes are chosen by usingImproved Dragonﬂy Algorithm (IDA) from the database for the betterclassiﬁcation.This method gives better results for limited number and features ofattribute. In addition to this, the limitation of the proposed algorithm iscomputationally slow because of the big database. It is clear from therecent literature review, that the Naive Bayes has wide scope to experi-ment with big data but is not sufﬁciently experimented over all with following combinations, such as.1 Multidimensionality and multi features2 Unstructured and complex data3 Bibtext and huge volume informationSo in this article the above identiﬁed gaps have be experimented over Bibtext data. The performances of PFP with Naive Bayes, SVM and TFapproaches and presented after analysis and measure. Further perfor-mance of proposed F-LSA for indexing is justi ﬁed by comparison analysis of PFP with F-LSA and TM with LSA. It is also justi ﬁed for different datasets like CiteseerX, Bibtext and Cora at concluding point.3. Proposed integration and indexing method3.1. Problem descriptionTraditional learning systems are mostly studied over supervised ma-chine learning systems. In these systems, data objects are associated witha label and the learning systems supervised the set of data to learn thefeature characteristics. It will be used for the classi ﬁcation as shown in Fig. 1.This learning is well adapted for single concept, but the complexityarises when the object has multiple features. So, the improvisation oftraditional supervised learning to adapt the multi-features data objects isbeing discussed in literature [21,23]. But most of the proposed solutionsare based on the features dependence learning or associating the features
Fig. 1.Traditional supervised learning.M.M. Nashipudimath et al. Array 7 (2020) 100033
3through counting their co-occurrences [ 24]. However, this might not be applicable for the domains where such kind of feature information is notavailable. In some cases, the fundamental dependency and correlations offeature are identiﬁed using association rules algorithm, but its fails tofacilitate the multiple changing datasets and its features in differentdomains.Target is to construct a classiﬁer based on new feature learning sys-tem which can perform on different source of multiple features datasetsas shown inFig. 2. System provides an accurate classiﬁcation for the integration and a semantic association based indexing support for thefaster accessing.The mechanism of the proposed approach works in three phases.■Semantic data relation learning (SDRL) procedure to construct thesemantic related classiﬁcation patterns.■The process of data acquisition from different sources.■Third phase consists of two sub-tasks:✓Data cleaning is a basic step to be performed. Feature trans-formation and selection method utilizing pattern knowledge fol-lows the cleaning stage. A probabilistic feature Patterns (PFP)approach for the efﬁcient integration is carried out through se-mantic classiﬁcation over a Naïve Bayes classiﬁer.✓Feature based latent semantic analysis [ 26] to construct similarity index. User access module is built to present a visualization of therelated BibText data which is used later for analysis mechanism.In this article, data features are utilized to propose a probabilisticfeature Patterns (PFP) approach for the ef ﬁcient integration through semantically classiﬁcation over a Naïve Bayes classiﬁer [25]. This approach implements a feature transformation and feature selectionmethod to accurately map the data through computing the principalcomponent analysis for multi-value data. It is used for multi-value datatransformation and selection for the effective integration of datasets. Anindexing [27] by features based latent semantic analysis (F-LSA) methodis implemented on the integrated data. In F-LSA, the data semantic as-sociation relation with the categorized data is learned and patterns areidentiﬁed. These data are used as the key sets of mapping data features. Itmeans a representation to have a similarity index to facilitate theenhancement and precise searches for big data. The detail in discussed innext section.3.2. Proposed system modules3.2.1. Semantic data relation learning (SDRL)The supervised classiﬁer needs a precise learning mechanism toperform an accurate classiﬁcation. The mechanism of SDRL provides thelearned knowledge for the design of a classi ﬁer which further classiﬁes the data for the integration. One such "One-To-k (OT-k) Label learningmethod" is proposed in Refs. [23]. It describes the approach of classifyingdata to a single label class through multidimensional features of data. Itsuggests that an object having two or more feature in similar can beconsidered for a single label class. This approach is best suitable for dataclassiﬁcation of multidimensional features. This methodology is inheri-ted for SDRL process.The learning of SDRL implements two methods. The ﬁ
rst method extracts the required feature information from the data to the relevantand speciﬁed article class which is termed as, "One-Feature Class (OFC)"as shown inTable 1. The second methodﬁnds the most k relevant fea- tures which supports for classifying through constructing a featurepattern.To illustrate the SDRL, let’s assume a set of training data sets T, thatconsists of various article documents for each speci ﬁc OFC represented as, T¼{d1,. . ., dn\}. Assume a vector V consists of all OFC. For each OFCin V, the data sets in T are processed to construct a relevant feature vectorclass as R. Next by utilizing the features in R of each OFC, most appealingfeatures are found through computing feature density (Density) amongthe features using Eq.(1).Density¼
Pni¼1ðfn2DÞjDj (1)The value of Density ranges between 0 and 1. In this case, "0
00is
IntegraƟon
Publisher2Data AcquisiƟon from ArƟcle PublicaƟons Data IntegraƟon and Indexing SemanƟc Data RelaƟons Learning (SDRL)
Publisher1
Data Cleaning Data ExtracƟon
SemanƟc ClassiﬁcaƟon
Integrated and IndexedDatabase Indexing through F-LSAArƟcle Class Patterns ArƟcle related SemanƟc Data
PaƩernsGeneraƟon
--
UserSearch Query Retrieved Relevant Results Result VisualizaƟonPublisher3
Fig. 2.System architecture.Table 1One-feature class table.
One-Feature Class (OFC) Relevant FeaturesAerospace { f1, f2, f3, f4, f4, f5,…} Biomedical { f1, f2, f3, f4, f4, f5,…} Health Informatics { f1, f2, f3, f4, f4, f5,…} Cloud Computing { f1, f2, f3, f4, f4, f5,…} Data Mining { f1, f2, f3, f4, f4, f5,…}M.M. Nashipudimath et al. Array 7 (2020) 100033
4considered as least relevant and "100as most relevant for OFC. The mechanism OFC table construction through the method of SDRL is beingpresented in Algorithm-1.
Algorithm-1Generation of OFC relevant features
Input:T→Training Datasets.V→Set of OFCDensity_Th→Feature density threshold.fori¼0, each class inVloop{C¼V[i];//–get the training data sets of a classD[]¼T [C];forj¼0, each data record inDloop{ F[]¼extractFeatures (D[j]);jþþ;}s¼0;//–construction of relevant featuresfork¼0, each features inFloop{ f¼F[k];//–Compute feature densityDensity¼getDensity(f);if(Density/C21Density_Th)thenR [s]¼f;sþþ;end if}iþþ;}
Most of the existing feature selection approaches with multiple fea-tures datasets suffer from information loss [ 4,6]. So it is essential to minimize this information loss in the SDRL. This is surmounted by dataextraction through a binary association of the feature, followed bycomputation [11] of Information Gain(IG). It is used toﬁnd the other features which can be supportive for the data classi ﬁcation along with the R as presented inFig. 3.If the frequency of term in a set of features >1, the binary value of feature is 1 else it will be 0. The measure of IG is the ratio of the sum-mation of feature frequency over the number of data records processed.Based on theseﬁnal k-features generated, needed classiﬁcation pattern combination is constructed for the article classi ﬁcation. It will facilitate the integration process.3.2.2. Data acquisitionData acquisition is a process to gathering data from distributed in-formation sources. Filtering and cleaning is done and followed by storingthem in scalable, big data-capable data storage. Different architecturesfor data acquisition have been proposed to address the different char-acteristics of big data. Most of data acquisition scenarios assume high-volume, high-velocity, high-variety, but low-value data. This makesadaptable and time-efﬁcient gathering,ﬁltering, and cleaning algorithms as a need to ensure that only the high-value fragments of the data areactually processed for the analysis.Most of the time, demand is to obtain data from the sources which aredifﬁcult to retrieve. In such cases, structured or unstructured data ﬁles delimited with comma or spaces are downloaded. It is easy to process andanalyze if theﬁles are limited but in case of high number, diverse andunstructured data makes it quite complex to process and analyze.In this paper, dataﬁles of different technical article publishers areacquired from Information Extraction and Synthesis Laboratory (IESL). Itaims to support the mining actionable knowledge from unstructured text.The gathered data containing a more than 4000 number of BibText ﬁles which have more than 6 million article records downloaded from theinternet. The data records are extracted through processing each indi-vidualﬁle and imply the data cleaning method to remove the data con-taining noise andﬁlling up the missing data.3.2.3. Data integrationFeatures are extracted to perform the feature transformation and se-lection for integration. The selected features are semantically associatedto classify using the pattern generated through SDRL. Each data recordsconsists of few key dataﬁelds such as author, title, keywords and abstractknown as metadata of the article. The terms are extracted from thekeywords and abstractﬁeld to construct a set of terms which will use forthe semantic classiﬁcation to classify the article class for the integration.Semantic classiﬁcation performs relevance association computationto relate the latent semantic relevancy [ 28]. Since, the article is technical information, the semantic-based classiﬁcation [29] will compute the similarity between the set of terms information constructed to identifyeach article.A“Concept of Similarity mechanism" [ 9] is derived to perform the association of data record. The concept of such terms and article meta-data is considered toﬁnd its relevance. It associates with other elementsof the article class.Let’s consider a data record,ĐR which consists of set of terms belongto an article class as,ĐR¼{t1, t2,. . . ., tn }. Toﬁnd the relation asso- ciation, the relation between the terms and the features of the article classis calculated. It gives frequency of term, calculation as f(x) for the de-pendency of this term to other terms using the equation-2.fðxÞ¼
Term t
Z;Term2Articlep (2)where, T is the term related to the terms of article class Article
p, and Z is the total number of extracted terms from the data records. On completionof freq(x) identiﬁcation, the similarity relevancy probability as P(t|a), iscomputed of each term present in each record as against each article classas A, using the equation-3.similarity/C0r:Article
p/C1¼Prob/C2/C0term\Article
p/C1/C14/C12/C12Article
p/C12/C12/C3(3) Let’s assume data record, d has a phrase like “Big data and Data mining”. Using equation(2), the probability "Semantic Term Similarity"is computed in related to article class, n as shown in equation-4,similarity/C0r:Article
p/C1
n¼Prob/C2/C0termðBig;Data;MiningÞ\Article p/C14/C12/C12Article
p/C12/C12/C1/C3(4) Few techniques [21] are discussed for classiﬁcation. But The func- tionality of a Naive Bayes Classiﬁers [25] is completely independent from the class attribute values variations, which generally being termed asCondition-Independence. This approach is constructed based on theassumption for the simpliﬁcation and generalization of class informationin relation to the Naïve Bayes probability assumptions. The constructedclass information symbolizes the characteristic of the class attributerelation which supports the classiﬁer in accurate classiﬁcation. It was experienced that "Bayes approach" is useful in an assured circumstancesand it is extremely reliant on the hypothesis of the target classi ﬁcation information for the proﬁcient outcomes. Because of high dependency, alittle divergence in the supposition hypothesis constructs a set of inac-curacy in recognition. In the proposed semantic classi ﬁcation method,
Fig. 3.k-Feature Generation.M.M. Nashipudimath et al. Array 7 (2020) 100033
5Naive Bayes is used to compute the probability of relevance in compareto the designed OTF article class.The probability of relevance of terms t with article class (AC) isdeﬁned as, pr(t(1→n)\ac(1→k)). Based on this pr(ac1-k) obtained foreach AC, the highest probable relevance AC is selected as, pr(C) toclassify the record. The methodology of the modi ﬁed Naïve Bayes to compute the Bayes probability similarity as βin relevance to AC terms for efﬁciently classifying the data sets is presented in Algorithm-2.
Algorithm-2Semantic Classiﬁcation Method
Input:T→Set of technical terms of a data record d. AC_ Set[ x]→Set of article class.AC_Terms[ x]→Set of terms fromAC_Set[ x] β¼0;//–probability similarityC¼null;//–Initial record ClassFork¼0; k < size(AC_Set [k]);each value inAC_Set[k] AC_Terms[ x]¼AC_Set[k];//–Compute the Bayes probability similarity
α¼prob(AC_Terms [x]\T);if
α>βthenC¼AC
k;β¼
α;End IfEnd ford, classiﬁed as,C.
This classiﬁed class C of the data record is used for integration. Thismethod is evaluated extensively for analysis over a BibText data collec-tion of different publishers and stored in a structure manner for theindexing.3.2.4. Data indexingThe indexing of structured data using feature latent semantic analysis(LSA) has been widely used in manyﬁelds of natural language processing [29] as per work done in 2018, where co-occurrence features can beacquired by the transfer relations between the records. Theco-occurrence information of the terms can be captured when SingularValue Decomposition (SVD) is decomposed as proposed by Refs. [ 30]i n the year 2019.The example shown inTable 2andTable 3are for data record and its feature document matrix respectively. The term weight represents wordfrequency of the term and the matrix is transform to a singular valuereducing to a two-dimensional space.A sample dataset consists of the titles of 9 technical articles. Termsoccurring in more than one title are extracted. There are two classes ofrecords as, "Computer Interaction (I)" and "Data Mining (M)". This datasetcan be described by means of a term by document matrix where each cellentry indicates the frequency with which a term occurs in a record.The value of similarity between the terms will be 1 in Table 3for occurrence of terms information.The similarity of 0 is between some disintegrated terms, that meansthere is no or little relations. By the changes of the similarity value, onecan think "user" and "interface", "interface" and "human", "user" and"human" co-occur. In LSI [31], the "user" and "human" projected to thesame dimension space. Comparison of the feature matrix between thesimilarity matrix transformed to SVD is shown in Table 4. The degree of similarity in features reﬂects the correlation between the terms. The weight value not only reﬂects the correlation in the fea- tures but also embodies the co-occurrence information between thefeatures in SVD space. The similarity degree in the documents mainlydepends on the number of the co-occurrence of features. In generatinglatent semantic space, the latent relations will be excavated because ofthe transitivity between the features.Tables 4 and 5represent the similar data records indexed to theirrelevance group. This will be useful for accessing the data record muchfaster by reducing the time complexity.3.3. Data access and visualization based on PFP and F-LSAUser data access model is designed to evaluate the proposed inte-gration and indexing mechanism. It consists of a user interface to submita query for mining and visualization of Bibliographic data of the tech-nical article. The search query retrieves the most relevance articles asresults and its information as shown in Fig. 4andFig. 5. The aim ofFig. 4is to show the search result by proposed method.Fig. 5shows theﬁrst page search result along with total number of pagesto follow. It also shows total record count of matching through searchmethod supported by total time taken for search operation. User can clickon page centric view link provided in front of each search result.A visualization analysis of data records in relation to the author ispresented by an article centric view as shown in Fig. 5. This is detail of page link from previous page (as shown in Fig. 4). It provides details of particular search result, i.e. author names, related article title and key-words. It also gives details and reference of related article with respect tokeywords.4. Experiment evaluationThe experimental work was carried out over Hadoop Framework forstorage the big data. Processing and analysis was performed using Javatechnologies. The data access visualization is evaluated through a webinterface provided by Apache Tomcat Web server. The SDRL mechanismis implemented to generate the required knowledge pattern for integra-tion. Later, the obtained data sources are cleaned and exported toHadoop for storage.The Java program performs the integration through semantic classi-
ﬁcation of data record with the help of knowledge pattern. Aftercompletion of integration, the indexing method F-LSA is executed to rankthe integrated data group. To evaluate the improvisation in the indexing,a Data Access Visualization interface is built, where multiple queries aresubmitted to measure the accuracy of the outcome.4.1. Data sourceData are acquired from the dataﬁles of diverse technical article publishers from Information Extraction and Synthesis Laboratory (IESL)[32]. It consists of more than 4000 number of BibText ﬁles which have more than 6 million article records.The parameters used to retrieve data for integration is keywords ofarticle. These keywords are further tokenized as terms of term document.Table 2The data records titles.
RecordidArticleClassArticle TitleS11 I Recognition of Head Gestures Using Hidden MarkovModelsS12 I Graphical User Interfaces and IntegrationS13 I Direct Manipulation for Comprehensible, Predictable andControllable User InterfacesS14 I A Brief Direct History of Human Computer InteractionTechnologyS15 I Integration of Speech and Gesture for Multimodel Human-Computer InteractionS21 M An Overview of Document Mining Technology andSoftwareS22 M Mining Sequential Patterns: Generalizations AndPerformance ImprovementsS2d3 M Web Based Parallel/Distributed Medical Data MiningUsing Software AgentsS24 M Evaluation of Sampling for Data Mining of AssociationRulesS25 M Mining Association Rules with Item patterns ConstraintsM.M. Nashipudimath et al. Array 7 (2020) 100033
6The obtained dataﬁles are transformed to speciﬁed tagged form for pre- processing and imply the data cleaning method to remove the datacontaining noise andﬁlling up the missing data.Authors assume that each article has keywords after abstract whichwill be extracted as features along with features from abstract. Title ofarticle is preprocessed to extract keywords from it, if list of keywords arenot given after abstract in article. Number of terms and documents areinitialized based on selected dataset toﬁnd/extract features using pro- posed method toﬁnd article. The system conﬁguration used to perform experiment is I5 machine with 8 GB of RAM.4.2. Evaluation measuresIt is a subjective work similar to article [ 33] of 2019 and difﬁcult to evaluate the quality of integration. The high degree of similarity betweenthe cluster and within the cluster is achieved by low af ﬁnity integration objective function in design.This can be seen as an internal standard for the quality of the inte-grated cluster. It is observed in the literature [ 22] also, however, it is not necessary to translate the good effect in the application for a good scoreon the internal reference.Two criteria for evaluation of the results like Purity and NormalizedMutual Information (NMI) are used as discussed in Ref. [ 34]. These methods are used for the cluster that is allocated. The Purposeis toﬁnd/evaluate whether the object of integrated result is same asoriginal class information. This is checked for each object to check itsmatching with original class.Each data clusters is represented as, C¼{C1,…,CJ } for the partition of the dataset constructed through the clustering algorithm, and parti-tioned, P¼{P1, PI } the partition inferred by the unique classi ﬁcation. J and I are correspondingly the numbers of clusters denoted as |C| and thenumber of cluster classes denoted as |P|. The N denotes the total numberof data objects in datasets.Purity Metric:The evaluation of a measure of purity is a simple andtransparent. Each individual cluster is allocated to the class determinedby dividing the number of including the number of objects in the preci-sion of the speciﬁed object and this exact allocation, and most frequentlyin the cluster data sets to calculate the purity [ 34]. The number of clusters is easy to achieve a high purity as shown in equation (5). Therefore, trade-off quality cannot be used for the purity of the number of clusters.Table 3Transformed feature matrix based on article terms.
Tid Terms S11 S12 S13 S14 S15 S21 S22 S23 S24 S25 Totaldoc1 Interfaces 1 1 0 0 0 0 0 0 0 0 2 doc 2 User 1 1 0 0 0 0 0 0 0 0 2 doc 3 Direct 1 0 0 1 0 0 0 0 0 0 2 doc t4 Interaction 0 0 0 1 1 0 0 0 0 0 2 doc 5 Computer 0 0 0 1 1 0 0 0 0 0 2 doc t6 Human 0 0 0 1 1 0 0 0 0 0 2 doc t7 Integration 0 1 0 0 1 0 0 0 0 0 2 doc 8 Mining 0 0 0 0 0 1 1 1 1 1 5 doc 9 Patterns 0 0 0 0 0 0 1 0 0 1 2 doc 10 Data 0 0 0 0 0 0 0 1 1 0 2 doc 11 Software 1 0 0 0 0 0 0 1 0 0 2 doc 12 Association 0 0 0 0 0 0 0 0 1 1 2 Total 4304412333
Table 4Feature similarity matrix truncated to two-dimension.
doc1doc2doc3doc4doc5doc6doc7doc8doc9doc10doc11doc12doc1 0.587 0.581 0.574 0.581 0.598 0.598 0.571 /C00.547/C00.489 0.225 0.347 /C00.403 doc 2 0.581 0.594 0.588 0.912 1.521 0.487 0.487 /C01.214 0.984 0.287 /C00.189 0.142 doc 3 0.574 0.588 0.997 2.104 3.124 1.124 1.057 /C00.657 0.199 0.471 /C00.147 0.440 doc t4 0.581 0.912 2.104 2.011 3.014 1.351 1.192 0.214 /C00.324/C00.214/C00.147 0.247 doc 5 0.598 1.521 3.124 3.014 0.547 0.547 0.981 0.124 /C00.314 0.145 0.112 0.263 doc t6 0.598 0.487 1.124 1.351 0.547 2.124 0.214 0.941 0.814 /C00.214/C00.614/C00.547 doc t7 0.571 0.487 1.057 1.192 0.981 0.214 0.987 /C01.021 0.140/C01.021 0.149 0.127 doc 8/C00.547/C01.214/C00.657 0.214 0.124 0.941 /C01.021 3.147 1.158 0.981 1.024 0.924 doc 9/C00.489 0.984 0.199 /C00.324/C00.314 0.814 0.140 1.158 1.201 1.095 0.547 0.458 doc 10 0.225 0.287 0.471 /C00.214 0.145/C00.214/C01.021 0.981 1.095 2.021 3.001 2.014 doc 11 0.347/C00.189/C00.147/C00.147 0.112/C00.614 0.149 1.024 0.547 3.001 1.260 0.902 doc 12/C00.403 0.142 0.440 0.247 0.263 /C00.547 0.127 0.924 0.458 2.014 0.902 1.225
Table 5Similarity matrix between the data records.
S11 S12 S13 S14 S15 S21S2d2S23S24S25S110.527 1.243 1.024 1.254 0.547 /C00.065/C00.127/C00.147/C00.124/C00.012 S121.243 3.247 2.254 3.147 2.047 0.241 0.250 /C00.021 0.451 0.114 S131.024 2.254 2.414 2.995 1.324 /C00.132/C00.214/C00.357/C00.121/C00.221 S141.254 3.147 2.995 2.265 3.248 /C00.214 0.144 0.140 /C00.124/C00.148 S150.547 2.047 1.324 3.248 1.554 0.012 /C00.124/C00.147 0.521 /C00.612 d1/C00.065 0.241 /C00.132/C00.214 0.012 3.624 1.547 0.784 0.665 0.687 d2/C00.127 0.250 /C00.214 0.144 /C00.124 1.547 0.625 1.241 1.741 1.514 d3/C00.147/C00.021/C00.357 0.140 /C00.147 0.784 1.241 1.847 2.054 2.154 d4/C00.124 0.451 /C00.121/C00.124 0.521 0.665 1.741 2.054 2.147 1.884 d5/C00.012 0.114 /C00.221/C00.148/C00.612 0.687 1.514 2.154 1.884 2.149M.M. Nashipudimath et al. Array 7 (2020) 100033
7PurityðC;PÞ¼1NX
jmax/C12/C12C j\P i/C12/C12 (5)NMI Metric:The measure of NMI metric offers a number of inde-pendent information in the cluster. The measure takes a maximum value[15], when this integration to partition completely consistent with theoriginal partition. The average mutual information of NMI is computedbetween a pair of clusters and the individual class as shown inequation-6.NMIðC;PÞ¼
PIi¼1PJj¼1/C12/C12C
j\P i/C12/C12log NjCj\Pijj
Cj\PijﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃP
Jj¼1/C12/C12C
j/C12/C12logjPijNPii¼1jPijlogjPijNq (6)Many previous studies have only been used to analyze the puritymetric and evaluate the clustering algorithm performance. However,when a larger number of clusters are available for the integration, it iseasy to achieve purity measure. Particularly if each object has its owndata clusters purity measured as 1. In addition, many of the partitionshave the same purity as they are associated with each other. For example,
Fig. 4.Data access results.
Fig. 5.Data access visualization.
Fig. 6.Integrated data purity analysis.M.M. Nashipudimath et al. Array 7 (2020) 100033
8a number of object data in each cluster is different to objects that make upthe cluster. So, measure NMI metrics is measured to have the overalleffectiveness and how obtained clusters results are equivalent to theoriginal classes.In order to evaluate the usefulness of the proposed indexing method,the indexed results "precision (IPR), recall (IRR) and accuracy (IAR)" aremeasured. The result as the based on the standard equations as shownbelowIP
R¼jNo:of True resultjjTotal No:of Associated ResultjIR
R¼jNo:of True resultjjTotal No:of RelatedþAssociated ResultjIA
R¼jTotal No:of Associated ResultjjTotal No:of Search ResultjThe outcome of these measures is discussed in the following section.It is based on the quantitative results to determine the indexed resultsmeasurement towards ranking.4.3. ResultsIn this section, the experiment result analysis of the proposed PFPbased integration is discussed. F-LSA based indexing method is analyzedthrough comparison of semantically associating approaches based onTerm Frequency (TF), Naive Bayes (NBP), and Support Vector Machines(SVM) to measure the performance improvisation.An experiment result analysis is provided for the integration andindexing through comparing the outcomes in their normal and with theproposed method. In the Normal analysis form, data integration is madesimply based on it terms related to the class. In case of proposed inte-gration the integrated data clusters is analyzed against the class articlesthrough measuring it purity and NMI.4.3.1. Integration performance analysisThe analysis of the integration performance in terms of purity mea-sure and NMI measure is presented in Figs. 6and7respectively. The proposed PFP shows an average of 8% higher purity compared to theexisting semantic based associating approaches over the records.In comparison with NB based association, PFP shows a nearby puritybut TF and SVM shows linear loss of purity with increasing the number ofrecords. The enhancement in the purity is due to the accuracy in learningof the feature association between the data records to its article classthrough the probabilistic feature Patterns and semantic similarity.In case of NMI analysis the mutual information is measured, which isshared between the data record and the learned article class terms inproposed PFP and the existing semantic based associating approaches forthe integration. Result shows the decreasing in NMI variation in case ofTF, Support Vector Machine and Naïve Bayes due it has high number ofindependent terms with increase number of data records. But the pro-posed PFP probability of relevance of terms with article class increaseswith the number of mutual information and supports in achieving betterNMI in comparison. The SDRL approach makes to relate the anonymityrecords more precise in order to achieve better purity and NMI incomparison.4.3.2. Indexing performance analysisThe indexing of data helps to retrieve information much faster andaccurate. To have an analysis of the indexing approach "Precision","Recall" and "Accuracy" of the indexed data is measured through an useraccessing model. Evaluation of proposed F-LSA based indexing methodwith TF and LSI based indexing for the integrated data according theirarticle classes is carried out. The outcome of the indexing analysis resultsare shown inFigs. 8–10respectively.Figs. 8and9show the performance of indexing accuracy in terms ofprecision and recall in comparison. The relevance of result retrievedagainst each query are measured to compute the precision and recall.Figures show that more number of data records in the article classes leadsto reduction the precision and increase of recall. The reduction of theprecision is due to the increase in the number of irrelevant associatedresults in the retrieve results. This might be the cause of the purityreduction in the integrated data and lowering in similarity index amongthe data record which results in poor indexing. But an enhancement of anaverage of 14% precision with TF based indexing and 3% with LSI isbeing achieved with utilizing the proposed F-LSA based index approach.Fig. 10shows the accuracy of the retrieved query results. The accuracy ofretrieved results completely depends on the semantic similarity measurebetween the query and indexed results. The irrelevancy in indexing andlower in semantic similarity index among the records lowers down theaccuracy of the retrieved results. The computed result shows a reductionin accuracy with increasing number of data records, but an average of15% improvisation in accuracy is achieved in compared to TF basedindexing and 6% in compare to LSI over the integrated data.4.3.3. Integration and indexing performance analysisThe analysis efﬁciency and ability of the proposed PFP based inte-gration through semantically classiﬁcation over a Naïve Bayes classiﬁer with F-LSA based indexing (PFP with F-LSA) over the heterogeneoussources is carried out. Evaluation is carried out with the three digitalbibliography datasets: "CiteSeerX", "BibText" and "Cora" with the Normalform of integration with LSA based indexing (Normal with LSA). Ef ﬁ- ciency is tested by submitting few queries to the designed used data ac-cess interface such as, "Software engineering", "Arti
ﬁcial Intelligence" and "Data Security" on the three datasets. Over 100 top retrieved results areanalyzed for the query toﬁnd the number of true result, number ofassociated results and total number of related results from the number ofsearch results.Figs. 11 and 12show the precision and recall performance for theproposed PFP with F-LSA and Normal with LSA. The obtained result ofthe proposed PFP with F-LSA shows an average of 13.2% of improvisa-tion in the precision and 10.5% of low recall in compare to the Normalwith LSA. The improvisation achieved due to the retrieval of morenumber of related results, which suggest the improvisation of the inte-gration. The total number of true results suggests the accuracy of theindexing. In case of accuracy measure, an average of 11.8% of improvi-sation is observed due to high number of associated result retrieval asshown inFig. 13.The experiment outcome of the precision, recall and accuracy againstthe various bibliographic datasets shows an enhancement compared toTF with LSA. The improvisation of the result is due to ef ﬁcient integration and indexing of data records semantically. The methods of F-LSA facil-ities to retrieve data, to identify the most similarity data records indexed
Fig. 7.Integrated data NMI analysis.M.M. Nashipudimath et al. Array 7 (2020) 100033
9to their relevance group formed through integration.Purpose of indexing the records is for faster access and quick retrieval.Accuracy depends on indexing as it depends on feature representation.Good/unique features results in better accuracy. Good features revealdata clearly and any method depends how quality features are extractedfor indexing.5. ConclusionIntegration and indexing are two principal functions needed toorganize and retrieve data faster in today ’s big data environment. Hence Implementation is in two folds. Initial line of work is a PFP approach forthe efﬁcient integration through semantic classi ﬁcation over a Naïve Bayes classiﬁer. Further an indexing by F-LSA method is implemented onthe integrated data.This paper reported a systematic performance evaluation of an ef ﬁ- cient integration of Bibtext data sets. The proposed PFP approach provedas right choice compared to SVM and TF especially for following threepossibilities such as multidimensionality and multi features, unstruc-tured and complex data, Bibtext and huge volume information.The selection of features and semantic analyzing is able to enhancethe integration in big data. A process of semantic data relation learning(SDRL) method is discussed to learn k-features related to collection ofdata. These features are analyzed to predict a one-feature class of a data.Further effective grouping is done for Integration. The process ofindexing utilizes the latent semantic analysis (LSA) method to under-stand the degree of similarity between features. The correlation betweenthe terms of data records ranks appropriately for indexing.
Fig. 8.Precision result analysis.
Fig. 9.Recall result analysis.
Fig. 10.Accuracy result analysis.
Fig. 11.Precision performance analysis.
Fig. 12.Recall performance analysis.
Fig. 13.Accuracy performance analysis.M.M. Nashipudimath et al. Array 7 (2020) 100033
10The integration method associates the features semantically to clas-sify the data record using learned patterns. Further a feature LSA amongthe integrated class data is implemented to construct the indexing of theintegrated data structure. The user access model is presented to evaluatethe effectiveness of the integration and indexing based on bibliographicaldata which includes various technical article published by well knownpublishers in different domains. Further Performance analysis of twodifferent combinations i,e“PFP with F-LSA”and“TF with F-LSA”is presented. The effectiveness and efﬁciency of PFP with F-LSA is proved for different data set like Citeseer and Cora apart from Bibtext. There isimprovisation in terms of precision and accuracy during indexed dataretrieval. Dynamic construction of index during query processing can becarried out in future to minimize the complexity of index column storage.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.CRediT authorship contribution statementMadhu Mahesh Nashipudimath:Conceptualization, Methodology, Investigation, Writing - original draft, Software. Subhash K. Shinde: Data curation, Software, Visualization, Supervision. Jayshree Jain: Validation, Writing - review&editing, Supervision.AcknowledgmentMadhu Mahesh Nashipudimath would like to thank Principal andManagement of Pillai College of Engineering, New Panvel, Navi Mumbai,India for their continue support and cooperation during this researchwork. Special gratitude to Dr. Satishkumar Varma for his constructivecriticism and suggestions.This research work received no speciﬁc grant from any funding agency in the public, commercial, or not-for pro ﬁt sectors.References
[1]Salloum SA, Al-Emran M, Monem AA, Shaalan K. Using text mining techniques forextracting information from research articles. Intell Nat Lang Process Trend Appl.2018;740:373–97.[2]Zhanga P, Essaidc A, Merkb CZ, Cavalluccia D. Case-based reasoning for knowledgecapitalization in inventive design using latent semantic analysis. In: Elsevierinternational conf. on know. based and intelligent information and eng. systems;2017. p. 6–8.[3]Jain Divya, Singh Vijendra. Feature selection and classi ﬁcation systems for chronic disease prediction: a review. Egypt Inf J 2018;19(3):179 –89. [4]Nashipudimath MM, Shinde SK. Indexing in big data. In: Computing,communication and signal processing. Springer; 2019. p. 133 –42. [5]Gani A, Siddiqa A, Shamshirband1 S, Hanum F. A survey on indexing techniques forbig data: taxonomy and performance evaluation. Springer-Verlag Knowledge Info.System; 2015.[6]Lei Cong, Zhu Xiaofeng. Unsupervised feature selection via local structure learningand sparse learning. Multimed Tool Appl 2018;77(22):29605 –22. [7]Lewis Suzanne, Damarell Raechel A, Tieman Jennifer J, Camilla Trenerry. Findingthe integrated care evidence base in PubMed and beyond: a bibliometric study ofthe challenges. Int J Int Care 2018;18(3) . [8]Roh Y, Heo G, Whang SE. A survey on data collection for machine learning: a bigdata integration perspective. IEEE Trans Knowl Data Eng 2019 . [9]Zhou Peng, Chen Jiangyong, Fan Mingyu, Du Liang, Shen Yi-Dong, Li Xuejun.Unsupervised feature selection for balanced clustering. Knowl Based Syst 2019 .[10] Luan, Cuiju, and Guozhu Dong. "Experimental identi ﬁcation of hard data sets for classiﬁcation and feature selection methods with insights on method selection."Data Knowl Eng 118 : 41-512018.[11]Pratiwi Asriyanti Indah. On the feature selection and classi ﬁcation based on information gain for document sentiment analysis. Appl Comput Intell Soft Comput2018.[12]Hancer Emrah, Xue Bing, Zhang Mengjie. Differential evolution for ﬁlter feature selection based on information theory and feature ranking. Knowl Base Syst 2018;140:103–11.[13]Venkatesh B, Anuradha J. A hybrid feature selection approach for handling a high-dimensional data. In: Innovations in computer science and engineering. Singapore:Springer; 2019. p. 365–73. [14]Fan W, Bouguila N, Ziou D. Unsupervised hybrid feature extraction selection forhigh-dimensional non-Gaussian data clustering with variation inference. IEEE TransKnowl Data Eng Jul. 2013;25(No. 7):1670 –85. [15] Thi Nguyen T-H, Huynh V-N. A k-means-like algorithm for clustering categoricaldata using an information theoretic-based dissimilarity measure. SpringerInternational Publishing Switzerland; 2016. p. 115 –30.https://doi.org/10.1007/ 978-3-319-30024-5.[16]Adnan K, Akbar R. An analytical study of information extraction from unstructuredand multidimensional big data. J Big Data 2019;6(1):91 .[17]Yang J, Li X. MapReduce based method for big data semantic clustering. In: IEEEinternational conference in systems, man, and cybernetics (SMC); 2013. p. 2814 –9. [18]Celeux Gilles, Maugis-Rabusseau Cathy, Sedki Mohammed. Variable selection inmodel-based clustering and discriminant analysis with a regularization approach.Adv Data Anal Classif 2019;13(1):259 –78. [19]Liu, Zio E. Integration of feature vector selection and support vector machine forclassiﬁcation of imbalanced data. Appl Soft Comput J 2018 . [20]Yousefpour Alireza, Ibrahim Roliana, Hamed Haza Nuzly Abdel. Ordinal-based andfrequency-based integration of feature selection methods for sentiment analysis.Expert Syst Appl. 2017;75:80 –93. [21]Selvakumar S, Senthamarai Kannan K, GothaiNachiyar S. Prediction of diabetesdiagnosis using classiﬁcation based data mining techniques. Int J Stat Syst 2017;12(2).[22]Guan Y, Jordan MI, Dy JG. A uniﬁed probabilistic model for global and local unsupervised feature selection. In: Proc. 28th Int. Conf. Mach. Learn.; 2011.p. 1073–80.[23]Kumar Reddy LK, Phani Kumar S. A OT-k label learning classi ﬁcation based on association rules for multi-label datasets. J Theor Appl Inf Technol 2017;95(19) . [24]Zhang Qingchen, Yang Laurence T, Castiglione Arcangelo, Chen Zhikui, Peng Li.Secure weighted possibilistic c-means algorithm on cloud for clustering big data. InfSci 2019;479:515–25.[25]Mallik Ritik, Abhaya Kumar Sahoo. A novel approach to spam ﬁltering using semantic based naive bayesian classi ﬁer in text analytics. In: Emerging technologies in data mining and information security. Singapore: Springer; 2019. p. 301 –9. [26] Adamu1 FB, Habbal1 A, Hassan1 S, Cottrell RL, White B, Abdullahi1 I. A survey onindexing techniques for big data: taxonomy and performance evaluation. In: 4thinternational conference on internet applications, protocol and services; 2016.https://doi.org/10.13140/RG.2.1.1844.0721 . [27]Liu Xiaojun, Wang Mengmeng, Fu Hanliang. Visualized analysis of knowledgedevelopment in green building based on bibliographic data mining. J Supercomput2018:1–17.[28]Anandarajan Murugan, Hill Chelsey, Nolan Thomas. Semantic space representationand latent semantic analysis. In: Practical text analytics. Cham: Springer; 2019.p. 77–91.[29]Merchant Kaiz, Pande Yash. Nlp based latent semantic analysis for legal textsummarization. In: 2018 international conference on advances in computing,communications and informatics (ICACCI). IEEE; 2018. p. 1803 –7. [30]Li Zhen, Li Weiguang, Zhao Xuezhi. Feature frequency extraction based on singularvalue decomposition and its application on rotor faults diagnosis. J Vib Contr 2019;25(6):1246–62.[31]Song W, Park SC. Genetic algorithm for text clustering based on latent semanticindexing. Elsevier Comput Math Appl 2009;57:1901 –7
. [32] Bibtext data is available on https://sites.google.com/a/iesl.cs.umass.edu/home/data/bibtex.[33]Lakshmanaprabu SK, Shankar K, Ilayaraja M, Nasir Abdul Wahid, Vijayakumar V,Chilamkurti Naveen. Random forest for big data classi ﬁcation in the internet of things using optimal features. International journal of machine learning andcybernetics 2019;10(10):2609 –18. [34]Lakshmanaprabu SK, Shankar K, Ilayaraja M, Nasir Abdul Wahid, Vijayakumar V,Chilamkurti Naveen. Random forest for big data classi ﬁcation in the internet of things using optimal features. International journal of machine learning andcybernetics 2019;10(10):2609 –18. [35]Bettoumi Safa, Jlassi Chiraz, Arous Najet. Collaborative multi-view K-meansclustering. Soft Computing 2019;23(3):937 –45.M.M. Nashipudimath et al. Array 7 (2020) 100033
11