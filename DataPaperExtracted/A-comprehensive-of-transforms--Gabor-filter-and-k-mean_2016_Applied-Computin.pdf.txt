ORIGINAL ARTICLE
A comprehensive of transforms, Gabor ﬁlter andk-means clustering for text detection in imagesand video
V.N. Manjunath Aradhyaa,*, M.S. Pavithrab
aDept. of MCA, SJCE, Mysore, India
bDept. of MCA, DSCE, Bangalore, IndiaReceived 8 December 2013; revised 3 August 2014; accepted 6 August 2014
Available online 23 August 2014
KEYWORDSWavelet transform;Multilingual text;Wavelet decomposition;Gabor ﬁlter;k-means clustering;Linked list approach;Wavelet entropyAbstract The present paper presents one of the efﬁcient approaches toward multilingual textdetection for video indexing. In this paper, we propose a method for detecting textlocated in varyingand complex background in images/video. The present approach comprises four stages: In the ﬁrststage, combination of wavelet transform and Gabor ﬁlter is applied. By applying single level 2Dwavelet decomposition with Gabor Filter, the intrinsic features comprising sharpen edges and tex-ture features of an input image are obtained. In the second stage, the resultant Gabor image is clas-siﬁed usingk-means clustering algorithm. In the third stage, morphological operations areperformed on clustered pixels. Then a concept of linked list approach is used to build a true textlinesequence of connected components. In the ﬁnal stage, wavelet entropy of an input image is mea-sured by signifying the complexity of unsteady signals corresponding to the position of textlinesequence of connected components in leading to determine the true text region of an input image.The performance of the approach is exhibited by presenting promising experimental results for 101video images, standard ICDAR 2003 Scene Trial Test dataset, ICDAR 2013 dataset and on ourown collected South Indian Language dataset.
ª2014 King Saud University. Production and hosting by Elsevier B.V. This is an open access articleunder the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/ ).
1. Introduction
With the current multimedia technology, the captured imagesand understanding these images through its contents havegained lots of attention from the computer vision community.Contents of an images and video help in clear understandingthe information present within. A text is one of the imagesand video content which carries semantic information, andmay help to provide the scene description of an image. Hence,the detection and extraction of either scene or graphics text has*Corresponding author.E-mail addresses:aradhya.mysore@gmail.com(V.N. Manjunath Aradhya),ngspavithra@gmail.com(M.S. Pavithra).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2016) 12, 109–116
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2014.08.0012210-8327ª2014 King Saud University. Production and hosting by Elsevier B.V.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/3.0/ ).been widely used in content based image indexing and retrie-val. The detection of text present in video is being used in videosummary and in video sequence retrieval. Text detection is apre-processing task for text recognition. Nowadays, digitalimages also carry useful information. Uploading these imagesto social networking sites is getting increased day by day. Sodetecting texts of digital images also plays an important andchallenging role in image retrieval system. Born-digital imagesare generated by computer software and are saved as digitalimages. Born-digital images have complex foreground/back-ground, low resolution and have severe soften edges [1].S o detecting text from such born-digital images is difﬁcult. Textsin natural scene images also have to be robustly detectedbefore being recognized and retrieved. Scene images containtexts such as the advertising boards, name plates, addressboards of houses, and landmarks of streets, are captured nat-urally when the scene images are taken by the camera. There-fore scene text is embedded in the background as a part of thescene. Scene images are complex because the backgrounds arecomplex containing the text in different sizes, styles and align-ments and the resolution of the image is low. Hence detectionof text from natural scene images remains still challengingtask. Text detection and recognition in images and videoframes aim at integrating advanced optical character recogni-tion (OCR) and text based searching technologies [2]. In this regard, the existing text detection methods use text featuressuch as gradient[3], edge[4]and texture[5]information. Withsuch efforts, detection of text in images and video remains achallenging task due to variations of text background, fontand orientations.
Recent text detection in natural scene images, born-digitalimages and video text detection has also been surveyed. Gon-zlez and Bergasa[6]described a method to read text in naturalimages, using geometric and gradient properties. Zeng et al. [7] presented a framework for detecting text from webpage andemail images, based on maximum gradient difference values.Hence the recent literature study implies that detection of textfrom either natural scene or digital images and text detectionin video are still in the pace of research. Our proposed algo-rithm implementsk-means clustering algorithm in detectingtrue text regions. In this regard, earlier research works of textdetection in video and images based on k-means clustering algorithm are surveyed herewith, Phan et al. [8]developed a text detection approach with the Laplacian operator. Thenk-means is used to classify all the pixels into clusters. Wuet al.[5]described a text localization method based on texturesegmentation by computing texture features. k-means algo- rithm is applied for classiﬁcation. Shivakumara et al. [9] describe a method based on the Laplacian in the frequencydomain. In this, the input image is ﬁltered with Fourier–Lapla-cian. Then,k-means clustering is used to identify candidatetext regions.
The above stated studies revealed that the text detectionapproaches are either region or texture based methods.Though the concept ofk-means clustering algorithm and aconnected component analysis have used, the detection accu-racy of the text region can still be improved without missingtext data and reducing most of the falsely detected blocks ofan image. Detection of texts of south Indian language is stilla challenging task. Words of such south Indian languagesare framed with modiﬁers and compound bases. To detectsuch a texts in an images/video, we propose a method basedon Transforms, Gabor ﬁlter andk-means clustering. By sus-
taining the development of our system[10], which describes the text detection method in color and regular images. In theﬁrst stage of the system[10], wavelet transform and Gabor ﬁl-ter are applied to extract sharpened edges and textural featuresof a given input image. In the second stage of the methodwavelet entropy is calculated to get an energy value of aresulted Gabor image in order to ﬁnd the high frequency tex-ture elements of a processed one to determine the true textregion of an image. As a progress of our work [11], in the pre- sent paper, we propose a multilingual text detection systemwith the wavelet transform, Gabor ﬁlter and k-means cluster- ing. The proposed method concentrates mainly on detection ofEnglish and south Indian language texts in images/video. Thesystem yields better results for various background complexi-ties and texts between other dominant non-text objects. Exper-iments are carried out on 101 video images dataset of [8], our own collected multilingual language dataset, ICDAR 2003scene trial test dataset, ICDAR 2013 dataset and on videoframes of Kannada. Comparative studies are reported indetail. The rest of this paper is organized as follows. Section 2 describes our proposed method. Experimental results are pre-sented and performance evaluation on considered datasets isdiscussed in Section3. Finally, conclusions are drawn.
2. Proposed methodology
The proposed method is a robust multilingual text detectionapproach based on the sequential adoption of Wavelet trans-form, Gabor ﬁlter,k-means clustering and a measure of wave-let entropy. First, by applying single-level discrete 2-D wavelettransform, a single-level 2-D wavelet decomposition is per-formed. As a result approximated and detailed co-efﬁcientsare obtained. Then detail co-efﬁcients are merged and aver-aged to extract efﬁcient texture feature information. Gabor ﬁl-ter is further applied in order to obtain edge information of animage. The resulted Gabor output image is grouped into threeclusters by applying thek-means algorithm to classify thebackground, foreground and the true text pixels of an image.In the next stage, morphological operations are performed toobtain connected components, then a concept of linked listapproach is in turn used to build a true text line sequence ofconnected components. In the ﬁnal stage, wavelet entropy ismeasured in an each connected component sequence in orderto determine the true text region of an input image. A completetext detection procedure of the proposed work is shown inFig. 1and explained in the following sub-sections.
2.1. Discrete wavelet transform for texture feature extraction
The Wavelet Transform is a method convolution of the wave-let function with the signal. The ability of the discrete wavelettransform to decompose a signal at different independentscales and to do it in a very ﬂexible way [12]. The discrete wavelet transform (DWT) is an implementation of the wavelettransform using a discrete set of the wavelet scales and trans-lations. In our research work wavelets are used as analyticaltools for signal processing. A study has performed about thedevelopment of the discrete wavelet transform (DWT), as aseries expansion of signals in terms of wavelets and scalingfunctions which are associated with low pass and high pass110 V.N. Manjunath Aradhya, M.S. Pavithraﬁlters respectively. With DWT a signal is decomposed into acoarse approximation and detail information. One of thebeneﬁcial application of the DWT is that, it performs textureclassiﬁcation in order to distinguish micro and macro textures.The 2D Discrete Wavelet Transform works as follows: Therows of the 2D signal are passed through a HPF and a LPF,and down-sampled by two. This divides the image into twosub images. Second, all columns of each sub image are ﬁlteredusing the HPF and LPF and down sampled by two. We exper-imented on Daubechies family’s ﬁrst order Daubechies waveletfunction i.e., db1. The advantage of compact supportnessproperty of the Daubechies wavelets is ﬁnite number of ﬁlterparameters and fast implementations. Accordingly single level2-D wavelet decomposition is performed with respect to thedb1 Daubechies wavelet function to extract texture featureand edge information of an input image. The obtained resultis shown inFig. 2(b) for an input imageFig. 2(a).
2.2. Gabor ﬁlter
Gabor ﬁlter mainly provides better spatial localization. Themain intension of employing Gabor ﬁlter is for texturesegmentation. In[13], Gabor ﬁlters are utilized via multireso-lution structure, consisting of ﬁlters tuned to several differentfrequencies and orientation. The main purpose of usingGabor ﬁlter after employing wavelet transform are thefollowing:
/C15The multiresolution structure relates the Gabor features towavelets by utilizing several frequencies and orientations./C15Has supporting properties for feature extraction./C15To perform localized analysis and to extract a localphenomena.
The vital part in the process of employing Gabor ﬁlter is ﬁl-ter parameter selection and ﬁlter construction. The Gabor ﬁl-ter is basically a Gaussian modulated by a complex sinusoid.The ﬁlter is represented as follows:Gðx;y;theta;fÞ¼expð½/C01=2ðx0=SxÞ2þðy0=SyÞ2/C138Þ/C3cosð2/C3pi/C3f/C3x
0Þð1Þ
wherex0¼x/C3cosðthetaÞþy/C3sinðthetaÞandy0¼ y/C3cosðthetaÞ/C0x/C3sinðthetaÞ. WhereGðx;y;theta;fÞbe the function deﬁning a Gabor ﬁlter, ‘theta’ is the orientation andits value set topi/2, ‘f’ is the spatial frequency its value set to1/sqrt(2), ‘SxandSy’ are variances alongxandyaxes respec- tively. Gabor ﬁlter representation is optimal and shows betterperformance for classifying text region. The resultant image ofdetail co-efﬁcients obtained by performing single-level 2Ddecomposition is averaged and is shown in Fig. 2(c). Then, the Gabor ﬁlter function is applied on to the obtained averageimage of detail co-efﬁcients, in order to extract local phenom-ena. The Gabor ﬁlter resulted image is shown in Fig. 2(d). The implementation of Gabor ﬁlter proves better performance inclassifying text region.
2.3. Implementation of k-means clustering algorithm
Clustering in[14]is a group of methods for ﬁnding anddescribing cohesive groups in data, such as compact clustersof entities in the feature space. To ﬁnd and describe a cohesivegroups in our data, we appliedk-means clustering. The mainobjective of applyingk-means is to minimize the total intra-cluster variance as well as to build a classiﬁcation over empir-ical data. The procedure ofk-means clustering begins withinitiatingktentative centroids. Further, tasks such as collect-ing clusters around centroids and updating centroids with clus-ter means, until its convergence are repeatedly performed. k- means mainly depends on the initialization of the centroids.In this regard, we initializedkvalue as 2 and observed thedetected true text region. When we initialize kvalue as 3, the obtained true text region was more accurate than whenk¼2. So by initializingkvalue as 3, the background, fore-ground and the true text pixels of an image are well classiﬁedwhich is shown inFig. 2(e). A cluster with true text pixels isshown inFig. 2(f).
Figure 1Work ﬂow diagram of the proposed text detection system.A comprehensive of transforms, Gabor ﬁlter and k-means clustering 1112.4. Morphological operations for detection of true text pixels ofan image
To the obtainedk-means clustering resultant image, weapplied morphological dilation operation with a rectanglestructuring element of size 5·3 to get connected componentsof true text pixels. The obtained resultant images are shown inFig. 2(g), (h) and (i). A concept of linked list approach is thenused to build a true text line sequence in order to get asequence of connected components as in [15]to detect a sequence of true text regions of an input image. A linked listis a concept of linear data structure which accesses the ele-ments sequentially. In a list rowwise elements are arrangedfrom starting to the end of the list. In order to access each ele-ment in the list for data processing, a function has to be cre-ated which traverses the list. In the proposed system, alinked list concept has selected as the choice of data structureand the point to be noticed that an element of the list refers toa connected component obtained by applying morphologicaloperations. A group of nearby elements of a list i.e., nearbyconnected components together represent a sequence. To buildthis sequences of connected components, center point of theright most part of a connected component is calculated usingEq.(3). By ﬁndingR
midpoint, a horizontal line with respectto the zero degree up to the line reaches to the nearestconnected component is obtained. ‘mid’ is center point of the
connected component and ‘R mid’ is the center point of the rightmost part of the connected component. With this approach,the proposed system is able to maintain elements by positionand allows for data manipulation of an each connected com-ponent. The result obtained for this stage is shown in Fig. 2(j).
mid¼ymaxþymin
2ð2Þ R
mid¼ðx max;midÞð3Þ
2.5. Calculation of wavelet entropy
The wavelet entropy carries information about the degree oforder, disorder associated with a multi-frequency signalresponse. From the obtained sequence of connected compo-nents, we calculated the wavelet entropy using Eq. (4)of the corresponding region of a sequence of connected componentsin an input image, in order to obtain the true text region as wellto eliminate falsy blocks of an image. Then we extracted anenergy information from an input image of the regions speci-ﬁed. Average energy of all the regions speciﬁed in the inputimage is ﬁxed as a thresholdt. If the speciﬁed sequence of atext region is/C65o ft, as a result, a region is considered as atext region else it is considered as a non-text region.
Figure 2(a) Input image of 101 video images dataset. (b) Resulted image of single level 2-D wavelet decomposition. (c) Resultantaverage image of single level 2-D wavelet decomposition of detail co-efﬁcients. (d) Resulted Gabor image with feature extraction. (e)Resulted image ofk-means clustering by settingK= 3. (f) An image of third cluster consisting of true text pixels. (g), (h) and (i) Resultant images of morphological operations. (j) A Resultant image obtained for linked list approach. (k) True text region detected image.112 V.N. Manjunath Aradhya, M.S. PavithraFig. 2(k) shows the text region obtained from the calculationof wavelet entropy.
EsðÞ¼X
ilogs2i/C0/C1 ð4Þ
wheresis the signal ands iðÞis the coefﬁcients ofsin an orthonormal basis.
3. Experimental results and discussion
The proposed method was experimented on intel CORE Duo2.0 GHz machine with MATLAB R2008a. In our experiment,we used four challenging datasets. The ﬁrst dataset contains101 video images of[8], which are extracted from news pro-grammes, sports videos and movie clips. The dataset includesboth graphic text and scene text of different languages, e.g.English, Chinese and Korean. The second dataset is the SouthIndian Language dataset, in which an image comprising eithera text of Kannada, Tamil, Telugu and Malayalam languages.The main purpose of considering south Indian languages textswas because, words were framed on the basis of compoundbases, modiﬁers and extra modiﬁers. Detecting such a text isa challenging task. The third dataset is the mostly citedICDAR 2003 Scene Trial Test dataset comprising 251camera-based images and Fourth dataset is the ICDAR 2013Born-Digital Images (Web and Email) and Scene Images data-set. The performance of the proposed system is evaluated atthe block level. The blocks are determined as the categoriesdescribed in[8].
3.1. Experiment on 101 video images dataset
The experiment was conducted on 101 video images providedby[8]. The dataset comprises video images with horizontal textlines with a different font color and varying background. Thesuccessful text detection images obtained for a dataset of 101video images are shown inFig. 3(a) and (b).For each image in the dataset we manually count the Actual
Text Blocks (ATB). We compared the proposed method withthe existing text detection methods such as Edge-based [4], Laplacian[8]and Transforms and Gabor based[10]methods. In order to evaluate the performance of the proposed methodon 101 video images dataset, we use the performance measuresdeﬁned in[8].Table 1shows the comparative study of the pro-posed and existing methods. From the table, it is clear that theobtained TDBs are more i.e., the system detects more numberof true text blocks, FDBs are sustained as in Transforms andGabor based method[10]indicating that there exists few falsealarms. MDBs are considerably reduced, which shows that themiss detection of text blocks is very few in number. We com-pared the proposed method with existing text detection meth-ods such as Edge-based[4], Laplacian[8]and Transforms andGabor based[10]methods. FromTable 2, it is clear that theproposed method has higher DR and lesser MDR comparedto existing methods and FPR is sustained as of Transformsand Gabor based method[10]. The main goal of the proposedsystem is to achieve highest DR by detecting true text blocks ofan image, in this regard we reached DR = 98.9%. By ourexperiment, it is proved that the proposed method exhibitshigher detection rate and considerably lesser miss detectionrate than the existing methods. For this dataset, Average Pro-cessing Time is 2.9 s. The best execution time is 2.6 s of imagesize 320·240 and the worst execution time is 3.5 s for720·576 image size.
Figure 3(a) and (b) Results of true text region detected images 101 video images dataset. (c) and (d) Results of true text detection ofICDAR 2003 Scene Trial Test dataset. (e) and (f) Results of true text detection of ICDAR 2013 Born-Digital Images dataset. (g) and (h)Results of true text detection of ICDAR 2013 Text in Scene Images dataset.Table 1Results obtained for the dataset of 101 video images.
Method ATB TDB FDB MDBEdge-based[4]491 393 86 79Laplacian[8]491 458 39 55Transforms and Gabor based[10]491 481 78 53Proposed491 486 78 15A comprehensive of transforms, Gabor ﬁlter and k-means clustering 1133.2. Experiment on our own collected south Indian languagesdataset
Our own collected multilingual dataset is challenging and textspresent in varying complex background with different fontcolor and size. In some images, texts are present in betweendifferent dominant non-text objects of an image. This particu-lar kind of images leads in resulting false positives. The mainapplicability of the proposed system is that it detects text ofSouth Indian Languages which may further help to developsouth Indian languages OCR. To exhibit the performance ofthe proposed method with South Indian languages dataset(based on the formation of a word of considered four southIndian languages are described), we have conducted experi-ment on our own collected South Indian language datasetcomprising 114 images with Kannada, Tamil, Telugu andMalayalam texts. Kannada images are collected by capturingscene text as well as front cover of the Kannada text booksand novels. Remaining languages text images are collectedfrom magazines and few are images of posters. In this datasettexts present in an image are with different font size, color andvarying background. Since Kannada text is framed with mod-iﬁers and compound bases, a text is detected with missing ofcurving portion as well as modiﬁers and its curves. With allthese reasons, performance measures are considerably lesscompared to English text images. Successful Text detectionresults of south Indian languages are shown in Fig. 4(Table 3).
From the experimental results inTable 4, it is noticed that the texts of south Indian languages present in an image arecompletely detected along with its modiﬁers and compoundbases as a whole word. Our previous method i.e. Transformsand Gabor based[10]detects text region with an existence offalse alarms and few missing text data. For the same dataset,the present method detects the region of text with its modiﬁersand compound bases in an input image. Though the DetectionRate (DR) reduced to low and Miss Detection Rate (MDR) isincreased compared to the method[10], but one of the notice-
able measure i.e. False Positive Rate (FPR) gets reduced.From this, it is proven that the proposed system executestoward ﬁnding most of the text elements of an image. AverageProcessing Time (APT) is also measured, the proposed systemyields result much faster in time compared to [10]. The best execution time is 2.6 s for image size 160·45 and the worst execution time is 3.0 s for image size 1024·500.
3.3. Experiment on ICDAR test datasets
Experiment is conducted on ICDAR 2003 Text Locating data-set and Scene Images dataset and ICDAR 2013 Born-DigitalImages (Web and Email) and Scene Images dataset. ICDAR2003 Scene Trial Test dataset contains 251 camera-basedimages. During the experiment, for each image in the datasetnumber of Actual Text Blocks(ATB), Truly detected block(TDB), Falsely Detected Block (FDB) and Text block withMissing Data (MDB) are manually counted. To evaluate theproposed method on ICDAR 2003 Scene Trial Test dataset,we compared the proposed method with three existing meth-ods such as Gradient[3], Laplacian[9]and Bayesian Classiﬁ-cation and Boundary Growing[16]methods that areexperimented on ICDAR 2003 Scene Trial Test dataset. Inaddition to the existing categories, Average Processing Time(APT): Processing time per frame required for detecting textin the images deﬁned in[16]is considered. The performancemeasures deﬁned in[16]Recall (R) = TDB/ATB, PrecisionTable 2Performance results obtained for 101 video imagesdataset.
Method DR FPR MDREdge-based[4]80.0 18.0 20.1 Laplacian[8]93.37.912.0 Transforms and Gabor based[10]97.9 13.9 11.0 Proposed 98.9 13.73.0
Figure 4(a–d) Successful text detection results of south Indian languages dataset of Kannada, Tamil, Telugu and Malayalam textsrespectively.Table 3Results obtained for our own collected south Indianlanguage dataset.
Method ATB TDB FDB MDBTransforms and Gabor based[10]314 309 116 19Proposed314 291 85 28
Table 4Performance results obtained on our own collectedsouth Indian language dataset.
Method DR FPR MDR APT (s) Transforms and Gabor based[10]
98.4027.296.147.9Proposed92.6722.609.622.6114 V.N. Manjunath Aradhya, M.S. Pavithra(P) = TDB/(TDB + FDB),F-measure (F)=2·P·R/ (P+R), Misdetection Rate (MDR) = MDB/TDB are con-sidered to evaluate our experiment. The successful text detec-tion images obtained for ICDAR 2003 Scene Trial Testdataset are shown inFig. 3(c) and (d). By observing the imageswith the detected true text blocks, we noticed that the pro-posed method estimates are most representative. Table 5shows the experimental results of the proposed method with existingmethods. The proposed method achieves the highest recall,F-measure and lowest MDR. Thus for the dataset ICDAR2003 Scene Trial Test, the proposed method outperforms allthe existing methods in most of the performance measures.Average Processing Time for the dataset is 3.9 s. The best exe-cution time is 3.3 s for image size 640·480 and the worst exe-cution time is 4.1 s for image size 1600·1200.
To evaluate the performance of the proposed method forICDAR 2013 Born-Digital Images (Web and Email) datasetand Reading Text in Scene Images dataset, the computationis performed based on detection of words as per the protocolof ICDAR 2013 competition. To evaluate the detection of text
based on word wise, linked list approach is not employed inthe procedure of the proposed system. Performance evaluationmetric is followed as in[17]. ICDAR 2013 Born-Digital Images(Web and Email) dataset comprises 141 images. Experimentresults are compared with the methods submitted to the chal-lenge: ICDAR 2013 Robust Reading Competition of born-digital images[17]. Experimental results are detailed inTable 6. The result proves that the proposed system is competitive andcomparable to other existing methods. For reference purpose,we also provided the text detection results in terms of text lineby employing the linked list approach. Successful text detec-tion images of ICDAR 2013 Born-Digital images dataset areshown inFig. 3(e) and (f). The average processing time for thisdataset is 2.6 s, the best execution time is 2.1 s for an image size680·106 and the worst execution time is 2.9 s for an imagesize 573·174.
We also conducted an experiment on ICDAR 2013 Read-ing Text in Scene Images dataset. Experimental results areTable 5Experimental result on ICDAR 2003 Scene Trial Test dataset.
Method Recall Precision F-Measure MDR APT (s) Gradient[3] 0.520.830.64 0.081.0 Laplacian–Fourier[9] 0.86 0.76 0.81 0.13 6.8 Bayesian Classiﬁcation and Boundary Growing [16]0.87 0.72 0.78 0.14 7.9 Proposed 0.92 0.730.82 0.063.9
Table 6Experimental result on ICDAR 2013 Born-DigitalImages (%).
Method Recall Precision F-score USTB-TexStar82.38 93.83 87.74TH-TextLoc 75.85 86.82 80.96I2R-NUS-FUS 71.42 84.17 77.27Proposedbased on word level 75.45 80.11 77.71Proposedbased on textline level 89.00 90.00 89.00Table 7Experimental result on ICDAR 2013 Reading Text inScene Images dataset (%)
Method Recall Precision F-Score USTB-TexStar 66.45 88.47 75.89 Text Spotter 64.84 87.51 74.49CASIA-NLPR 68.24 78.89 73.18Proposedbased on word level71.0961.99 66.22 Proposedbased on textline level 98.90 80.20 88.50
Figure 5Text detection results of Kannada news video.A comprehensive of transforms, Gabor ﬁlter and k-means clustering 115compared with the few of the competitive methods submittedto the challenge: ICDAR 2013 Reading Text in scene images[17]. Successful text detection images results of ICDAR 2013Text in Scene Images dataset are shown in Fig. 3(g) and (h). Experimental results are shown inTable 7. For reference purpose, inTable 7we also provided the text detection resultsin terms of text line by employing the linked list approach. Inthis case, the average processing time for the dataset is 3.3 s,the best execution time is 2.6 s for an image size 533 ·263 and the worst execution time is 7.0 s for an image size3888·2582.
3.4. Experiment on video
We also conducted an experiment on our own collected videosequences consisting of English and Kannada texts. The sys-tem successfully detects text present in the video. The systemdetects the true text region in a video with the same bit rateof the input video. This approach plays an important role inthe recent progress of text detection technique for video. Theresultant text detection in video is shown as video frames inFig. 5.
4. Conclusion
An efﬁcient method for text detection in images and video isintroduced. The combination of wavelet transforms, Gabor ﬁl-ter extracts texture features and sharpen edges of an inputimage. Grouping the data of an extracted information is wellperformed usingk-means clustering by consideringkvalue as 3, resultant of this is classiﬁcation of background, fore-ground and the true text pixels. By employing the morpholog-ical operations and the concept of linked list approach,connected components of true text line sequences are obtained.Finally, wavelet entropy of an input image is measured corre-sponding to the positions of true text line sequence of con-nected components. The most beneﬁcial feature of usingGabor ﬁlter with the combination ofk-means clustering isthat, the proposed system could able to extract most of the textinformation, sharpen edge information and able to classifybackground, foreground and true text pixels of an input imageefﬁciently. On the highly cited ICDAR 2003 Scene Trial Testdataset the method surpasses the existing methods in termsof most of the considered performance measures. Performanceresults on ICDAR 2013 Robust Reading Competition datasetsof born-digital images and scene images prove competitivecompared with the methods submitted to the Text Localizationof ICDAR 2013 Robust Reading Competitions. The proposedsystem is better suitable for horizontal texts, texts framed withmodiﬁers and compound bases such as texts of south Indianlanguages present in images/video. Thus, the proposed systemcan extensively be used to detect a true text region accuratelyin attaining high detection rate with a very few missing datain numbers. In this regard, further we propose a better textdetection system to represent discontinuities along with theedges or curves in the images especially for South Indian lan-guage scenario in order to detect a close boundary around thetexts.References
[1] J. Zhang, R. Cheng, K. Wang, H. Zhao, Research on the textdetection and extraction from complex images, in: TheProceedings of Fourth International Conference on EmergingIntelligent data and Web Technologies, 2013, pp. 708–713.
[2]D. Chen, J. Odobez, J. Thiran, A localization/veriﬁcationscheme for ﬁnding text in images and video frames based oncontrast independent features and machine learning methods,Signal Process.: Image Commun. 19 (2004) 205–217
.
[3] E.K. Wong, M. Chen, A new robust algorithm for video textextraction, pattern recognition, in: The Proceedings of FirstAsian Conference on Pattern Re cognition (ACPR), vol. 36,2003, pp. 1397–1406.
[4] C. Liu, C. Wang, R. Dai, Text detection in images based onunsupervised classiﬁcation of edge-based features, in: ICDAR,2005, pp. 610–614.
[5] V. Wu, R. Manmatha, E.M. Riseman, Finding text in images,in: The Proceedings of the ACM International Conference onDigital Libraries, 1997, pp. 23–26.
[6]A. Gonzlez, L.M. Bergasa, Text reading algorithm for naturalimages, Image Vis. Comput. 31 (3) (2013) 255–274
.
[7]C. Zeng, W. Jia, X. He, Text detection in born-digital imagesusing multiple layer images, in: ICASSP, IEEE, 2013, pp. 1947–1951
.
[8] T. Phan, P. Shivakumara, C. Tan, A Laplacian method forvideo text detection, in: The Proceedings of 10th InternationalConference on Document Analysis and Recognition, 2009, pp.66–70.
[9]P. Shivakumara, T. Phan, C. Tan, A Laplacian approach tomulti-oriented text detection in video, IEEE Trans. PatternAnal. Mach. Intell. 33 (2011) 412–419
.
[10]V.N. Manjunath Aradhya, M.S. Pavithra, C. Naveena, A robustmultilingual text detection approach based on transforms andwavelet entropy, The Proceedings of 2nd InternationalConference on Computer, Communication, Control andInformation Technology (C3IT-2012), vol. 4, ProcediaTechnology (Elsevier), 2012, pp. 232–237
.
[11]V.N. Manjunath Aradhya, M.S. Pavithra, An application of K-means clustering for improving video text detection, TheProceedings of the International Symposium on IntelligentInformatics (ISI’12), vol. 182, Springer, Berlin, Heidelberg,2013, pp. 41–47
.
[12]C.S. Burrus, R.A. Gopinath, H. Guo, Introduction to Waveletsand Wavelet Transforms, A Primer, Rice University Houston,Prentice-Hall, Inc., Texas, New Jersey, 1998
.
[13] J. Ilonen, J.K. Kamarainen, H. Kalviainen, Efﬁcientcomputation of Gabor features, in: Research Report,Lappeenranta University of Technology, Lappeenranta,Finland.
[14]B. Mirkin, Core Concepts in Data Analysis: Summarization,Correlation and Visualization, Springer-Verlag, London, 2012
.
[15] C. Naveena, V.N. Manjunath Aradhya, A linked list approachfor handwritten textline segmentation, J. Intell. Syst. (2012) (inpress).
[16]P. Shivakumara, R.P. Sreedhar, T.Q. Phan, S. Lu, C.L. Tan,Multioriented video scene text detection through Bayesianclassiﬁcation and boundary growing, IEEE Trans. CircuitsSyst. Video Technol. 22 (8) (2012) 1227–1235
.
[17] D. Karatzas, F. Shafait, S. Uchida, M. Iwamura, L. Gomez, S.Robles, J. Mas, D. Fernandez, J. Almazan, L.P. de las Heras,ICDAR 2013 robust reading competition, in: The Proceedingsof 12th International Conference of Document Analysis andRecognition, IEEE CPS, 2013, pp. 1115–1124.116 V.N. Manjunath Aradhya, M.S. Pavithra