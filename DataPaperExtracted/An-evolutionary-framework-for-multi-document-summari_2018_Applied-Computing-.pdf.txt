An evolutionary framework for multi document summarization usingCuckoo search approach: MDSCSA
Rasmita Rautraya,⇑, Rakesh Chandra Balabantarayb
aDepartment of Computer Science and Engineering, Siksha ‘O’ Anusandhan University, Bhubaneswar 751030, Odisha, India
bDepartment of Computer Science, IIIT, Bhubaneswar, Odisha, India
article info
Article history:Received 21 February 2017Revised 1 May 2017Accepted 11 May 2017Available online 13 May 2017Keywords:Text summarizerMulti-document summarizationExtractive summaryCuckoo searchabstract
In today’s scenario the rate of growth of information is expanding exponentially in the World Wide Web.As a result, extracting valid and useful information from a huge data has become a challenging issue.Recently text summarization is recognized as one of the solution to extract relevant information fromlarge documents. Based on number of documents considered for summarization, the summarization taskis categorized as single document or multi-document summarization. Rather than single document,multi-document summarization is more challenging for the researchers to ﬁnd accurate summary frommultiple documents. Hence in this study, a novel Cuckoo search based multi-document summarizer(MDSCSA) is proposed to address the problem of multi-document summarization. The proposedMDSCSA is also compared with two other nature inspired based summarization techniques such asParticle Swarm Optimization based summarization (PSOS) and Cat Swarm Optimization based summa-rization (CSOS). With respect to the benchmark dataset Document Understanding Conference (DUC)datasets, the performance of all algorithms are compared in terms of ROUGE score, inter sentence simi-larity and readability metric to validate non-redundancy, cohesiveness and readability of the summaryrespectively. The experimental analysis clearly reveals that the proposed approach outperforms the othersummarizers included in this study./C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionNow a day’s the rate of information growth is expanding expo-nentially in the World Wide Web, which creates information over-load problem. One solution to this problem is shortening ofinformation, called text summarization (TS). Text summarizationis the process of creating shorter version of original text withoutlosing main contents[1]called summary. The summary providesa quick guide to create interest on information, helps in makingdecision on document whether it is readable or not as well as itis served as a time saver for users[2]. The way in which summary is generated either is an extraction or an abstraction method [3,4].Extraction based summaries are generated by selecting the impor-tant portions of the original text. Whereas, abstraction based sum-maries requires linguistic analysis to construct new sentences fromthe original text[5,6]. Based on dimension, extraction based sum-maries can be categorized into two ways i.e., generic or querydependent[7]. Generic summary reﬂects the major content ofthe documents without any additional information. But, Query-dependent summary focuses on the information expressed in thegiven queries[8,9].Number of documents considered for generating summary, canclassify the summarization problem as single document or multi-document summarization[10,11]. When a document is condensedinto a shorter version, it is called single document summarization,whereas condensing a set of documents into a summary is calledmulti-document summarization. Therefore, summarization ofmultiple documents can be considered as an extension of summa-rization of single document[12]. In multi-document summariza-tion, search space is larger compared to single documentsummarization, which makes it more challenging for extractingimportant sentences. In that context, multi-document summariza-tion can be considered as an optimization problem with the objec-tive of producing optimal summary containing informative
http://dx.doi.org/10.1016/j.aci.2017.05.0032210-8327//C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author.E-mail addresses:rashmitaroutray@soauniversity.ac.in (R. Rautray),rakesh@iiit- bh.ac.in(R.C. Balabantaray).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 14 (2018) 134–144
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
sentences of the original documents. Nature inspired optimizationbased approaches are the suitable choices to address this optimiza-tion problem. In literature several meta heuristic techniques suchas particle swarm optimization (PSO), differential evolution (DE),harmony search (HS), Cuckoo search (CS) and genetic algorithm(GA) are applied in single or multi-document summarization.Being inspired by the application of Cuckoo search in other opti-mization problems[13–34], in this study a novel Cuckoo searchalgorithm based summarizer is presented for multi-documentsummarization. Though single document using Cuckoo searchalgorithm is present in literature[35]but, multi-document sum- marizer using Cuckoo search is new to this area. Further the modelis also compared with Particle Swarm Optimization based summa-rizer and Cat Swarm Optimization based summarizer. The perfor-mance of such models are analyzed over DUC datasets withrespect to few summary evaluation metrics such as ROUGE score,inter sentence similarity and readability metric. These evaluationmetrics are considered to validate the non-redundancy, cohesive-ness and readability of the generated summary.The structure of paper is organized as follows. Section 2brieﬂy describes the related works on text summarization problem usingglobal optimization techniques. Section 3introduces the proposed extractive summarization model. Section 4presents Cuckoo search based summarizer for solving summarization problem. Next, Sec-tion5details the numeric calculation for objective function, Sec-tion6elaborates on experiments and result analysis and ﬁnallySection7addresses the conclusions.2. Related worksIn this section, a theoretical study of evolutionary algorithmsbased text summarization and various applications of Cuckoosearch algorithm is discussed.In multi-document summarization, compression of multipledocuments, speed of sentence extraction, redundancy betweensentences and sentence selection are the critical issues in the for-mation of useful summaries. In the past, such issues are resolvedby statistical tools. But, due to signiﬁcantly poor performance ofstatistical tools in text extraction, from 2000 onwards a numberof global optimization techniques such as particle swarm opti-mization (PSO)[2,11,36–38], differential evolution (DE)[1,7,11,12,36,37,39–44], and genetic algorithm (GA)[10,45–51] are proposed by several researchers for improving the performanceof sentence selection in document summarization. Initially, theoptimization algorithm GA was ﬁrst used in test summarizationproblem[45]to retrieve relevant document based on query andrelevant judgments. Thereafter in[46], the author has evaluates the efﬁciency of GA with ﬁtness functions for relevance feedbackin information retrieval problem for maintaining the documentorder. Later on GA based programming technique is used for fuzzyretrieval system to extract information based on query by applyingoff-line adaptive process[48]and in[49], the author has used GA for text summarization based on sentence score. Each sentencescore is obtained through the comparison of each sentence withall other sentences as well as with the document title by cosinemeasure. The informative features weights are calculated usingGA to inﬂuence the words relevancy. Word relevancy deﬁnes rele-vancy and rank of the sentences having highest score with respectto a threshold, are selected as summary sentences. A single docu-ment generic summary has been extracted based on different sen-tence features using GA by comparing with some other techniquesand were evaluated using ROUGE score [10]. Kogilavani et al.[50] Presents a feature based multi-document generic summarizationusing GA & clustering to enhance the summary quality by maxi-mizing length, coverage and informativeness while minimizingthe redundancy. Whereas, genetic algorithm based document sum-marization has been proposed to generate optimal summary bycombining article sentences and query sentence to achieve satis-ﬁed length, high coverage, high informativeness and low redun-dancy in summary[51,52]. However the GA is providing betterresult for text summarization. But GA suffers from issues of more
parameter tunning[39]. To obtain better summary with lessparameter tunning, the authors of[1,7,40,41]have used DE for text summarization problem. Aliguliyev [1]presents a generic docu- ment summarizer based on sentence clustering using DE. Whereasin[42], a single document summarizer focuses on sentence featureas key ingredient instead of clustering to extract summary. A sum-marizer for single document based on clustering has been pre-sented and made comparison of discrete DE and conventional DEfor summarization and showed comparison result by the authorsof[36]. Alguliev et al.[43]have used DE algorithm to enhance sen-tence feature based summary by maximizing content coverage,readability and cohesion to improve text readability and informa-tiveness of summary. As the problem of summarization is consid-ered as discrete optimization problem in [43], to solve such problem the author has used adaptive DE to maximize informa-tiveness of summary while reducing the redundancy of summary.In contrast, the summarization problem is considered as p-medianproblem and Quadratic Boolean programming problem by theauthors of[7,40], for that a new variation of DE with self adaptivemutation and crossover parameters and binary DE is used. Whereas in[43], adaptive crossover parameter is used for optimizing thesummary result. The models discussed in [7,12,39]not only express sentence-to-sentence relationship, but also expresssummary-to-document and summary-to-subtopics relationships.In all the above cases, DE based summarizer is showing signiﬁ-cantly better result than GA based summarizer both for singleand multi-document summarization.Rautray and Balabantaray[37]presents a generic summarizerfor single document using particle swarm optimization algorithm,by considering content coverage and redundancy feature as keyaspects of summary. For solving such problem, the objective func-tion is designed by taking weighted average of content coverageand redundancy features. Another PSO based single documentsummarizer is also proposed in[11], which has used the same objective function as described in[37], but by taking features of text as an input arguments instead of sentence weights as inputarguments to the model. Binwahlan et al. [2]have presented a PSO based extractive summarizer where expression of ROUGE isused as ﬁtness functions for extraction of summary sentences.The summary based on PSO is also presented by Asgari et al. [38] considering summary features such as content coverage, readabil-ity and length. A multi-document summarization system usingPSO has been presented in[36]based on the concept of clusteringof sentences by calculating inter sentence similarity between sen-tences and sentence to document set to achieve content coverageand diversity of summary. In contrast, similarity metric also usedby Alguliev et al.[44]to achieve content coverage, diversity andlength of summary for multiple document sets. Rautray et al.[53]presents cat swarm optimization (CSO) algorithm based multidocument summarizer, which takes content coverage, readabilityand cohesion as key aspects of summary. The summary is evalu-ated over DUC dataset and compared with two other optimizationalgorithms such as particle swarm optimization and harmonysearch algorithm, in which CSO shows competitively better resultthan other two algorithms.Cobos et al.[15]have implemented Cuckoo search algorithm forweb document clustering or web clustering engine. Cuckoo searchuses Balanced Bayesian Information Criteria for ﬁtness functionand compared against existing clustering algorithms for webdocument, Sufﬁx Tree Clustering, Lingo and Bisecting K-meanR. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144 135algorithm. The CS algorithm shows a signiﬁcant improvementresult than other algorithms. A new biodiesel engine have devel-oped by Wong et al.[16]to achieve fewer emissions, low fuel costand wide operating range of engine using Cuckoo search algorithm.The CS algorithm is compared with PSO algorithm and the resultshows that CS is similar to PSO bur with less user defendparameters.For minimization of power loss and maximization of voltagemagnitude, reconﬁguration network methodologies using CS algo-rithm have proposed by Nguyen and Truong [17]. The radial topol- ogy of network is maintained by CS algorithm, which is comparedwith PSO and other compared methods in literature and the resultof CS is more noticeable. A combinatorial optimization approachusing Cuckoo search algorithm[18]have introduced to minimize possible number of test cases by considering the combination ofinputs for detecting defects. Here Cuckoo search algorithm is usedto create optimized combinatorial test set. Along with these engi-neering applications, many other recent applications of CuckooSearch algorithm are listed inTable 1.Though various optimization algorithms were proposed in past,but application of Cuckoo search algorithm for developing summa-rizer is very few in the area of text summarization. Mirshojaei andMasoomi[35]has already addressed summarization problemusing Cuckoo search algorithm. But it is applied only for single doc-ument summarization. Here, summarization result of Cuckoosearch algorithm is compared with the summarization result ofparticle swarm optimization algorithm, bacterial foraging opti-mization algorithm and word summarizer in terms of F-score.Among all cases, the F-score of Cuckoo search algorithm is showingcomparatively better than the other results.To the best of the authors’ knowledge, no study is available inthe open literature with the application of Cuckoo search algorithmfor multi-document summarization problem.3. Multi-document summarizationMulti-document summarization is an automatic process to cre-ate a concise and comprehensive document, called summary frommultiple documents. The entire procedure of multi-documentsummarization is divided into three steps such as preprocessing,input representation and summary representation. The overviewof summarization system is shown in Fig. 1. Input to the summa-rization system is multiple documents such as D 1,D2,...,D N. The documents are initially preprocessed, and the result is gonethrough input representation and summary representation toextract ﬁnal summary. The detail of summarization process is dis-cussed in the following subsections.3.1. PreprocessingPreprocessing goes through four sub processes./C15Sentence segmentation:From the set of input text documents,each individual documentDis segmented separately asD={S
1, S
2,...,S n}, whereS jdenotesjth sentence in the document foreasy extraction of summary sentence, and n is the number ofsentences in document./C15Tokenization:Terms of each sentence are tokenized as T={t
1, t
2,...,t m}, wheret kfor k = 1,2,..., m. represents all the distinctterms occurring inDandmis the number of terms./C15Stop word removal:Most commonly used words in Englishlanguage such as ‘a’, ‘an’, and ‘the’ which has less important sig-niﬁcance with respect to the document are removed./C15Stemming:It is a process of chopping off the ends of words to acommon base form.3.2. Input representationIn this section the preprocessed data presented in word form isused to calculate weight (sum of term frequencies) for each sen-tence known as sentence informative score. The sentence informa-tive score, represented as weight of sentence is further entered asinput to the optimization algorithm for implementation. Thedetails of input representation is discussed in Fig. 3.3.3. Summary representationThe objective of summary representation is generating sum-mary of document sets containing useful information. Throughthe optimal sentence selection process, the important sentencesrepresenting summary is selected by comparing the sentenceinformative score obtained through optimization algorithm withrespect to a pre speciﬁed threshold value (see Fig. 4).4. Cuckoo search based multi-document summarizerCuckoo search (CS) is one of latest meta heuristic algorithm,inspired by the species of bird called the Cuckoo. Cuckoos are fas-cinating birds because of their aggressive reproduction strategyand beautiful sounds, they can make [54–56]. The mature Cuckoos lay their eggs in the nests of other host birds or species [57]. The nest containing each egg represents a solution, and each Cuckoocan lay only one egg that represents new and potentially bettersolution. The standard Cuckoo search algorithm can be describedby three idealized rules: 1) One egg is laid by each Cuckoo in a ran-dom nest represents a solution sets; 2) The best eggs contained inthe nests will carry over to the next generation; 3) The number ofavailable nests is ﬁxed, and a host bird can discovered an alien eggwith a probabilityðP
aÞ. If this condition satisﬁes, either the egg canbe discarded or abandon the nest by the host, and built a new nestelsewhere.For implementation point of view, CS algorithm can use thesimplest form where each nest has only a single egg. In this casethere is no distinction between egg, nest or Cuckoo, as each nestcorresponds to one egg which also represents one Cuckoo. Thealgorithm can be extended to more complicated cases in whicheach nest has multiple eggs representing a set of solutions.Table 1Applications of Cuckoo search algorithm in recent years.
Author(s) ApplicationDash et al.[13] Thermal system Udayraj et al.[14] Heat transfer problems Cobos et al.[15] Clustering Ljouad et al.[19] Object tracker & Kalman ﬁlter Araghi et al.[20] Trafﬁc signal controller Wong et al.[16] Engine optimization Nguyen et al.[21] Hydrothermal scheduling Dash et al.[22] Thermal system Nguyen et al.[17] Network conﬁguration Abd-Elaziz et al.[23] Power system Zineddine[24] Computer security Nguyen et al.[25] Hydrothermal scheduling Dos Santos et al.[26] Energy conservation Wang et al.[27] Solar radiation Elkeran[28] Sheet nesting problem Bhargava et al.[29] Phase equilibrium problem Fateen et al.[30] Phase stability calculation Ding et al.[31] Fuzzy system Ahmed et al.[18] Software engineering Panda et al.[32] Multilevel thresholding Bhandari et al.[33] Satellite image segmentation Kumar et al.[34] FIR differentiator design136 R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144When generating new solutionsxtþ1i, a balanced combination of a local random walk and the global explorative random walk isused. This can be controlled by a switching parameter P
a. The local random walk can be written as:
xtþ1i¼xtiþa/C2S/C10HðP a/C0eÞ/C10xtj/C0xtk/C16/C17ð1Þ
wherextjandxtkare two different solutions selected randomly byrandom permutation,HðuÞis a Heaviside function,
eis a random number drawn from a uniform distribution and s is the step size.On the other hand, the global random walk is carried out by usingLévy ﬂights. A Lévy ﬂight contains successive random steps[56,58,59], and is characterized by a sequence of rapid jumps, canbe represented by the following equation:xtþ1i¼xtiþa/C10L/C19evyðkÞð 2Þ
where ais step size, which should be proportional to scale ofoptimization problem (i.e.
a>0),/C10is entry wise move during multiplication and LévyðkÞis random numbers drawn from Lévydistribution.
D1 D2 D3 DN
Sentence Segmentation Tokenization Stop word removal Stemming
Sentence Informative score calculation Cuckoo Search implementation
Optimal sentence selection
Summary Multiple Documents
PreprocessingInput RepresentationSummary Representation
Fig. 1.Overview of summarization system.
Fig. 2.Flowchart of document preprocessing.R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144 137In-addition, the steps involved in MDSCSA is discussed below.Step 1: Collect a set of multiple documents M, where M = {D
1, D
2,...,D N}. EachD irepresents individual document of set M.Length of each D
iis represented in terms of number of sentences,which vary from document to document.Step 2: Preprocess each text document D
iusing the sentence seg- mentation, tokenization, stop word removal and stemming stepsas shown inFig. 2.Step 3: Calculate the Informative score IS
jk(i.e. the sentence weight derived from the sum of term frequencies) for each sentence S
jof the preprocessed document D
iusing Eq.(3).
ISjk¼tf jk/C2logðn=n kÞð 3Þ
where IS jkrepresents informative score for each sentence S jwith respect to term t
k.t fjkis the term frequency (i.e. number of timesthe term t
koccurred in sentence S j,nkdenotes the number of sen- tences in which t
kappears. The termlog(n/n k) is referred as inverse sentence frequency used in vector space model for sentenceretrieval.
Step 4: Calculate inter sentence similarity for the preprocessed doc-ument D
iusing Eq.(4).
simðs i;sjÞ¼Pmk¼1ISikISjkﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃP
mk¼1IS2ik/C1Pmk¼1IS2jkq;i;j¼1;...;nð4Þ
Step 5: Select least similar sentences for each D ibased on a thresh- old similarity value.Step 6: Merge the selected all least similar sentences of each D
ias a single document D
input.Step 7: Initialize CS parameters such as population size, rate ofalien eggs, step factor(Sf)and levy exponent(k). Step 8: Use sentence IS score as nest information of each Cuckoo within the speciﬁed search space. Each nest corresponds to a poten-tial solution to the given optimization problem.Step 9: Compute the ﬁtness function f
ifor each of these nests as per the given problem using Eq.(3).Step 10: The new population of nests is obtained using Lévy ﬂightas speciﬁed in Eq.(2).Step 11: Calculate the ﬁtness f
jcorresponding to the new nests andcompare with the ﬁtness f
iof the previous nests.Step 12: If f
jis better than f i.Replace the previous nest solution by new nest solution.Step 13: In the new population, select a fraction P
aof worst per- forming nests. Replace these nests by randomly generated oneswithin the speciﬁed search space & build new ones.Step14: Compute the ﬁtness function for the new nests obtained.Step15. Based on the ﬁtness values, record the best performingnests in the current population set. Which are then compared withthe best nest obtained until current generation, and replace currentbest by previous best nest.Step 16: If the termination criterion is not met, go to Step 9.Step 17: Select sentences chronologically from the document basedon their threshold.5. Summary evaluation criteriaThe objective of the TS problem is to maximize informativenesswhile reducing redundancy and preserving readability of the
Fig. 3.Flowchart of input representation.138 R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144generated summary. Therefore in this paper, authors have tried tobuild summaries from document sets with multiple objectivessuch as content coverage as well as non-redundancy, cohesionand readability, which are explained in following objective func-tionf(S) and that formalize as three sub-functions such as f
cov(S), f
coh(S) andf read(S) to optimize summary.
fðSÞ¼fcovðSÞþfcohðSÞþfreadðSÞð5Þ
The objective function (i.e., Eq. (1)) balances the content coverage, cohesion and readability of the summary. The ﬁrst termevaluates content coverage of the summary. A summary contains aset of relevant sentences, which covers the main content of docu-ment set. The main content of document is reﬂected by the highestweighted sentence or center of the each document. Therefore thecontent coverage of summary is represented as:
fcovðSÞ¼Simðs i;OÞi¼1;2...nð6Þ
whereO= the center of the main content collection of sentences i.e.,O={O
1,O2,...,O n} of document sets andO iis weighted average of sentences of each document. Similarity between S
iandO(speciﬁed in Eq.(4)) is evaluated to measure importance of the sentences.Higher similarity values correspond to high content coverage.
The cohesion between the sentences in the summary is connec-tion of ideas both at the sentence level and at the paragraph level.This helps in understanding the complete text in a better way. Theideas of summary select a subset of s/C26Dor sentence to sentence relationship that chosen fromD. This can be represented as:
fcohðSÞ¼1/C0Simðs i;sjÞi–j¼1;2;...;nð7Þ
The higher value off coh(S) speciﬁes high connection betweensentences and vice versa.The summary readability select a subset of s/C26Dthat maximizes the inter sentence relationship of schosen fromD.As fread(S)measures similarity (speciﬁed in Eq.(4)) betweenS iand S j,
the higher value off read(S) speciﬁes higher readability of the sum-mary, which is deﬁned as:
freadðSÞ¼Simðs i;sjÞi–j¼1;2;...;nð8Þ
6. Experiment and result analysisThis section conduct experiments to test proposed summarizationsystem empirically. The MDSCSA is compared with CSOS and PSOSmulti-document summarizer with respect to two years of DUC data-sets. All the summarizer models are implemented in MATLAB Version2014a) in a system with Window 7 operating system. After obtainingthe simulation result, the analysis of summary result has been carriedout using ROUGE tool in terms of ROUGE score.6.1. DatasetThe open bench mark datasets from DUC (Document Under-standing Conference) are used for the evaluation of text extractionresult.Table 2provides a short description of DUC data sets. By thestep of data preprocessing, less signiﬁcant words or stop wordsfrom the original documents are removed by comparing with theavailable stop word list in net and the terms are stemmed usingthe most common stemmer in English called Porter’s stemmer.6.2. Controlling parametersControlling parameters of any optimization algorithm are appli-cation oriented. Thus, there is no ﬁxed value is assign to these
Fig. 4.Flowchart of summary representation.R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144 139parameters. Therefore derivation of parameters is obtainedthrough number of simulations. For this text extraction problem,the controlling parameters of MDSCSA, CSOS and PSOS modelsare present inTable 3.6.3. Evaluation metricFor summary evaluation, ROUGE-1.5.5 package developed by[60]is used in this study. It is used as the evaluation metric for textsummarization. ROUGE includes different methods such asROUGE-L, ROUGE-N, ROUGE-S, ROUGE-W and ROUGE-SU to mea-sure the n-gram match between systems generated summariesand human summaries. Here ROUGE-N metric compares N-gramsof two summaries, and counts the number of matches:
ROUGE/C0N¼P
S2SummrefP
N/C0gram2S Count matchðN/C0gramÞP
S2SummrefP
N/C0gram2S CountðN/C0gramÞð9Þ
where N stands for the length of the N-gram, count match (N-gram)is the highest number of N-grams co-occurring in candidate sum-mary and reference-summaries. Count (N-gram) is the number ofN-grams in the reference summaries.
Furthermore, sensitivity, positive predictive value (PPV) andsummary accuracy (Summary
acc) are used for summary evaluation.The sensitivity, PPV and Summary
accof summary are evaluated based on the outcomes of candidate summary (Candidate
sum), ref- erence summary (Reference
sum), true sentences (True sen) and least signiﬁcant sentences (LS
sen). The summary which is generated byour proposed summarizer is called candidate summary. Whereas,the summary is refer for an evaluation, called reference summary.In both the summary, the common sentences are referred as truesentences. But the sentences, neither in Candidate
sum nor in Reference
sumis called LS sen. Sensitivity, PPV and Summary accare calculated using the following equations.
Sensiti vity¼True sen jjTrue
sen jjþReferencesum jjð10ÞPPV¼
True sen jjTrue
sen jjþCandidate sum jj ð11ÞS
acc¼True sen jj þLS senjjTrue
sen jj þLS senjj þReferencesum jj þCandidate sum jjð12Þ6.4. Performance analysisThis section analyses the performance of various models on thebasis of three summary evaluation criteria as discussed inSection4.6.4.1. Observation 1 (based on ROUGE-N)The summary performance has been evaluated by usingROUGE-N with twoNvalues such as ROUGE-1 and ROUGE-2 met-rics. These matrices are highly correlated with the human judg-ments. ROUGE-1 measures the overlap of unigrams between thesystem summary and the manual summaries created by humanwhile ROUGE-2 compares the overlap of bigrams [43]. The ROUGE-N evaluation is done based on content coverage, cohesive-ness and text readability of summary. A model providing higherROUGE metric indicates higher similarity of the generated sum-mary with respect to the original document sets. Though theROUGE-N value is represented in terms of three different metricssuch as precision, recall and F-measure value, F-measure isassumed to have more signiﬁcance for selection of a summary. Inthis study the model selection is done based on the bestF-measure of the ROUGE-N values. Table 4shows the statistical analysis in term of worst, mean and best of F-measure ofROUGE-1 and ROUGE-2 evaluation metrics observed for the PSOS,CSOS and MDSCSA algorithm on DUC 2006 and DUC 2007 docu-ment set respectively. The evaluation metrics are observed forthe system generated summaries (summary generated by PSOS,CSOS and MDSCSA) with human generated summaries present inDUC. From the comparison of F measure it is observed that the bestF measure value with respect to ROUGE-1 for all the three opti-mization algorithms are falling within the range 0.41–0.44 andwith respect to ROUGE-2 it is within the range 0.07–0.13 forDUC 2006 dataset. Similarly for DUC 2007 dataset, the best F mea-sure value with respect to ROUGE-1 is falling within the range0.40–0.43 and with respect to ROUGE-2 it is within the range0.08–0.10. Though the values are data dependent it is clearlyobserved that, Cuckoo search is providing better F measure values(best of statistical analysis) for both the ROUGE scores on both thedatasets. Further the precision, recall and F measure of both theROUGE scores with respect to two datasets is speciﬁed in Table 5. Analyzing the three matrices of ROUGE-N score and documentclassiﬁcation metrics, it is clearly observed that MDSCSA is provid-ing better result compared to PSOS and CSOS with respect toROUGE-1 and ROUGE-2 for both the datasets. The F measure valueof ROUGE-N score is dependent on both recall and precision value.Similarly summary accuracy is dependent on both sensitivity andPPV score. So instead of evaluating the summarizers with respectto precision, recall, sensitivity and PPV score separately, the modelvalidation is done based on the F measure value and summaryaccuracy value (seeTable 6).6.4.2. Observation 2 (based on cohesion)Cohesion is an essential element for the reader to be clear andto achieve its ﬁnal purpose. It refers to the degree to which sen-tences (or even different parts of one sentence) are connected so
Table 3Parameters used for PSO, CSO and CS based summarizer.
PSOS CSOS MDSCSAPopulation size 50 docs Population size 50 docs Population size 50 docsC1 [0,2] SMP 3 Rate of alien eggs ( P
a) 0.75 C2 [0,2] CDC 0.2 Step size ( S
f) 0.5 Vmin,Vmax [0,1] SRD 0.2 Levy exponent ( k) 0.8 W 0.45 Mixture ratio (MR) 0.5 w, C 0.5, 4Table 2Dataset description.
Data set parameters Size (DUC2006) Size (DUC2007)Number of clusters 50 45Number of documents in each clusters 25 25Average no. of sent. per doc 30.12 37.5Maximum no. of sent. per doc 79 125Minimum no. of sent. per doc 5 9Data source AQUAINT AQUAINTSummary length (in words) 250 250140 R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144that the ﬂow of ideas is easy to follow. Cohesiveness in summarydoes not mean just ‘‘grammatically correctness” of sentences butcohesive summary refers to the connection of ideas both at thesentence level and at the paragraph level. Therefore cohesion ofconsecutive sentences helps in understanding the complete textbetter[40]. The common metric used to compute cohesiveness ofsummary is cosine similarity by considering average similarity ofthe sentences.Fig. 5shows cohesion score of different methodsTable 4Performance comparisons of PSOS, CSOS and MDSCSA summarizer based on ROUGE-N (F measure) metric for DUC2006 and DUC2007 data.
Dataset Evaluation metric Optimization algorithm Worst Mean BestDUC 2006 Rouge-1 PSOS 0.39087 0.4009 0.41127 CSOS 0.4003 0.4070 0.4229MDSCSA 0.40422 0.4115 0.4311 Rouge-2 PSOS 0.05848 0.0651 0.0784 CSOS 0.0714 0.0831 0.09033MDSCSA 0.07677 0.0864 0.13986 DUC 2007 Rouge-1 PSOS 0.3916 0.3991 0.40967 CSOS 0.3908 0.4098 0.4207MDSCSA 0.4000 0.4116 0.4243 Rouge-2 PSOS 0.0743 0.0758 0.0762 CSOS 0.0809 0.0881 0.08903MDSCSA 0.0817 0.0892 0.1034
Table 5Precision, recall and F measure of ROUGE-N score for both the dataset.
Dataset Evaluation metric Optimization algorithm Recall Precision F measureDUC 2006 Rouge-1 PSOS 0.44151 0.38491 0.41127 CSOS 0.43098 0.41520 0.4229MDSCSA 0.43655 0.4258 0.4311 Rouge-2 PSOS 0.08255 0.07469 0.0784 CSOS 0.0995 0.08271 0.09033MDSCSA 0.12346 0.16129 0.13986 DUC 2007 Rouge-1 PSOS 0.44679 0.37825 0.40967 CSOS 0.46158 0.38662 0.4207MDSCSA 0.4583 0.3951 0.4243 Rouge-2 PSOS 0.0841 0.0697 0.0762 CSOS 0.0924 0.0859 0.08903MDSCSA 0.1093 0.09824 0.1034
Table 6Performance comparison of PSO, CSO and CS summarizer based on sensitivity, PPV and summary accuracy for both the dataset.
Dataset Optimization algorithm Evaluation metricsSensitivity PPV Summary
acc
DUC 2006 PSOS 0.5 0.4 0.9734 CSOS 0.56 0.5294 0.9800MDSCSA 0.6 0.5708 0.99 DUC 2007 PSOS 0.5 0.3529 0.9808 CSOS 0.5833 0.5 0.9904MDSCSA 0.62 0.54 0.9951
Fig. 5.Cohesion score comparison on DUC2006 and DUC2007 dataset.R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144 141on DUC datasets. From the analysis, it is observed that MDSCSA hasshown comparatively better cohesion value than the PSOS andCSOS for both the datasets.6.4.3. Observation 3 (based on readability)This experiment involves readability of summary, which means‘‘how easily materials can be read and understood? This dependson several factors including the average length of sentences, thenumber of new words contained, and the grammatical complexityof the language used in a passage” [61]. Readability can be calcu- lated by the formula discussed inTable 7. Readability is estimated in terms of the number of years of education one needs to have tocomprehend that text[62]. The higher value of readability metricsupports easy reading and understanding of generated summarywhereas lower value creates difﬁculty in reading and understand-ing of the summary. The readability score of three different sum-marizers for DUC 2006 dataset and DUC 2007 dataset is showninFigs. 6 and 7respectively. From the analysis, it is clearlyobserved that, for DUC 2006 dataset MDSCSA is providing betterreadability score with respect to FKGL, FOG, SMOG and ARI metricscompared to PSOS and CSOS and for CL metric all the three summa-rizers are producing almost same result. For DUC 2007 datasetMDSCSA is providing better readability score with respect to allthe metrics compared to both PSOS and CSOS summarizer.7. ConclusionThis paper focuses on a Cuckoo search based multi-documentsummarizer to create a generic extractive summary. The summa-rizer is also compared with particle swarm optimization basedsummarizer and cat swarm optimization based summarizer. Theperformance of all discussed summarizers are evaluated in termsof ROUGE score, inter sentence similarity and readability metricto validate non-redundancy, cohesiveness and readability of thesummary respectively on a benchmark dataset called as DocumentUnderstanding Conference datasets in three experiments. Observa-tion 1 and 2 discusses non-redundancy and cohesiveness of sum-mary, where in most of the cases Cuckoo search based model isshowing better ROUGE score. Similarly in readability test discussedin observation 3, MDSCSA is also showing better readable score ofthe summary inFigs. 6and7compared to PSOS & CSOS basedmodel. From the above observations, it can be concluded that theperformance of MDSCSA is signiﬁcantly better than the CSOS andPSOS algorithm in summary generation.Table 7Readability metric formulas.
Readability metric Formula Equation no.Flesch Kincaid Grade Level (FKGL) 0.39 /C2(words/sentences) + 11.8/C2(syllables/words)/C015.59 (13) Gunning fog score (FOG) 0.4 (Average Sentence Length + Percentage of Hard Words) (14)SMOG Index (SMOG) 1.0430 /C2sqrt(30/C2complex words/sentences) + 3.1291 (15) Coleman Liau (CL) 5.89 /C2(characters/words)/C00.3/C2(sentences/words)/C015.8 (16) Automated readability index (ARI) 4.71 /C2(characters/words) + 0.5/C2(words/sentences)/C021.43 (17)
024681012
FKGL FOG SMOG CL ARIReadability score
Readability metricsResult on DUC 2006MDSCSA
CSOS
PSOS
Fig. 6.Readability score of different methods on DUC2006 dataset.
024681012
FKGL FOG SMOG CL ARIReadability score
Readability metricsResult on DUC 2007MDSCSA
CSOS
PSOS
Fig. 7.Readability score of different methods on DUC2007 dataset.142 R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144Controlling of evolutionary algorithm parameters are purelydata dependent in the experiment of any application. As Cuckoosearch algorithm is an evolutionary approach, thus the limitationof this approach is its controlling parameters. Therefore more sys-tematic approach of parameter setting will be explored in ourfuture work. The performance of this approach can also be exam-ined using other competent nature inspired algorithms.References
[1]R.M. Aliguliyev, A new sentence similarity measure and sentence basedextractive technique for automatic text summarization, Expert Syst. Appl. 36(4) (2009) 7764–7772
.[2] M.S. Binwahlan, N. Salim, L. Suanmali, Swarm based text summarization, in:Computer Science and Information Technology-Spring Conference, 2009,IACSITSC’09, International Association of, IEEE, 2009, April, pp. 145–150.[3] K. Jezˇek, J. Steinberger, Automatic text summarization (the state of the art2007 and new challenges), in: Proceedings of Znalosti, 2008, February, pp. 1–12.[4]
E. Lloret, M. Palomar, Text summarisation in progress: a literature review,Artif. Intell. Rev. 37 (1) (2012) 1–41
. [5]
M. Mendoza, S. Bonilla, C. Noguera, C. Cobos, E. León, Extractive single-document summarization based on genetic operators and guided local search,Expert Syst. Appl. 41 (9) (2014) 4158–4169
. [6]
H. Oliveira, R. Ferreira, R. Lima, R.D. Lins, F. Freitas, M. Riss, S.K. Simske,Assessing shallow sentence scoring techniques and combinations for singleand multi-document summarization, Expert Syst. Appl. 65 (2016) 68–86
. [7]
R.M. Alguliev, R.M. Aliguliyev, M.S. Hajirahimova, GenDocSum+ MCLR: Genericdocument summarization based on maximum coverage and less redundancy,Expert Syst. Appl. 39 (16) (2012) 12460–12473
. [8] I. Mani, M.T. Maybury, Advances in Automatic Text Summarization, MIT Press,1999.[9]
X. Wan, J. Xiao, Exploiting neighborhood knowledge for single documentsummarization and keyphrase extraction, ACM Trans. Inf. Syst. (TOIS) 28 (2)(2010) 8
.[10]
M.A. Fattah, F. Ren, GA, MR, FFNN, PNN and GMM based models for automatictext summarization, Comput. Speech Lang. 23 (1) (2009) 126–144
. [11]
R. Rautray, R.C. Balabantaray, A. Bhardwaj, Document summarization usingsentence features, Int. J. Inf. Retrieval Res. (IJIRR) 5 (1) (2015) 36–47
. [12]
R.M. Alguliev, R.M. Aliguliyev, N.R. Isazade, CDDS: constraint-driven documentsummarization models, Expert Syst. Appl. 40 (2) (2013) 458–465
. [13]
P. Dash, L.C. Saikia, N. Sinha, Comparison of performances of several Cuckoosearch algorithm based 2DOF controllers in AGC of multi-area thermal system,Int. J. Electr. Power Energy Syst. 55 (2014) 429–436
. [14]
Udayraj, K. Mulani, P. Talukdar, A. Das, R. Alagirusamy, Performance analysisand feasibility study of ant colony optimization, particle swarm optimizationand cuckoo search algorithms for inverse heat transfer problems, Int. J. HeatMass Transf. 89 (2015) 359–378
. [15]
C. Cobos, H. Muñoz-Collazos, R. Urbano-Muñoz, M. Mendoza, E. León, E.Herrera-Viedma, Clustering of web search results based on the cuckoo searchalgorithm and Balanced Bayesian Information Criterion, Inf. Sci. 281 (2014)248–264
.[16]
P.K. Wong, K.I. Wong, C.M. Vong, C.S. Cheung, Modeling and optimization ofbiodiesel engine performance using kernel-based extreme learning machineand cuckoo search, Renewable Energy 74 (2015) 640–647
. [17]
T.T. Nguyen, A.V. Truong, Distribution network reconﬁguration for power lossminimization and voltage proﬁle improvement using cuckoo search algorithm,Int. J. Electr. Power Energy Syst. 68 (2015) 233–242
. [18]
B.S. Ahmed, T.S. Abdulsamad, M.Y. Potrus, Achievement of minimizedcombinatorial test suite for conﬁguration-aware software functional testingusing the Cuckoo search algorithm, Inf. Softw. Technol. (2015)
. [19]
T. Ljouad, A. Amine, M. Rziza, A hybrid mobile object tracker based on themodiﬁed Cuckoo Search algorithm and the Kalman Filter, Pattern Recogn. 47(11) (2014) 3597–3613
. [20]
S. Araghi, A. Khosravi, D. Creighton, Intelligent cuckoo search optimized trafﬁcsignal controllers for multi-intersection network, Expert Syst. Appl. 42 (9)(2015) 4422–4431
.[21]
T.T. Nguyen, D.N. Vo, Modiﬁed cuckoo search algorithm for short-termhydrothermal scheduling, Int. J. Electr. Power Energy Syst. 65 (2015) 271–281
. [22]
P. Dash, L.C. Saikia, N. Sinha, Comparison of performances of several FACTSdevices using Cuckoo search algorithm optimized 2DOF controllers in multi-area AGC, Int. J. Electr. Power Energy Syst. 65 (2015) 316–324
. [23] A.Y. Abd-Elaziz, E.S. Ali, Cuckoo search algorithm based load frequencycontroller design for nonlinear interconnected power system, Int. J. ElectricPower Energy Syst., 73 C (2015) 632–643.[24]
M. Zineddine, Vulnerabilities and mitigation techniques toning in the cloud: Acost and vulnerabilities coverage optimization approach using Cuckoo searchalgorithm with Lévy ﬂights, Comput. Security 48 (2015) 1–18
. [25]
T.T. Nguyen, D.N. Vo, A.V. Truong, Cuckoo search algorithm for short-termhydrothermal scheduling, Appl. Energy 132 (2014) 276–287
. [26]
L. Dos Santos Coelho, C.E. Klein, S.L. Sabat, V.C. Mariani, Optimal chiller loadingfor energy conservation using a new differential cuckoo search approach,Energy 75 (2014) 237–243
.[27]J. Wang, H. Jiang, Y. Wu, Y. Dong, Forecasting solar radiation using anoptimized hybrid model by Cuckoo Search algorithm, Energy 81 (2015) 627–644
.[28]
A. Elkeran, A new approach for sheet nesting problem using guided cuckoosearch and pairwise clustering, Eur. J. Oper. Res. 231 (3) (2013) 757–769
. [29]
V. Bhargava, S.E.K. Fateen, A. Bonilla-Petriciolet, Cuckoo search: a new nature-inspired optimization method for phase equilibrium calculations, Fluid PhaseEquilib. 337 (2013) 191–200
. [30]
S.E.K. Fateen, A. Bonilla-Petriciolet, A note on effective phase stabilitycalculations using a Gradient-Based Cuckoo Search algorithm, Fluid PhaseEquilib. 375 (2014) 360–366
. [31]
X. Ding, Z. Xu, N.J. Cheung, X. Liu, Parameter estimation of Takagi-Sugeno fuzzysystem using heterogeneous cuckoo search algorithm, Neurocomputing 151(2015) 1332–1342
.[32]
R. Panda, S. Agrawal, S. Bhuyan, Edge magnitude based multilevel thresholdingusing Cuckoo search technique, Expert Syst. Appl. 40 (18) (2013) 7617–7628
. [33]
A.K. Bhandari, V.K. Singh, A. Kumar, G.K. Singh, Cuckoo search algorithm andwind driven optimization based study of satellite image segmentation formultilevel thresholding using Kapur’s entropy, Expert Syst. Appl. 41 (7) (2014)3538–3560
.[34]
M. Kumar, T.K. Rawat, Optimal design of FIR fractional order differentiatorusing cuckoo search algorithm, Expert Syst. Appl. 42 (7) (2015) 3433–3449
. [35] S.H. Mirshojaei, B. Masoomi, Text summarization using cuckoo searchoptimization algorithm, J. Comput. Robotics 8(2) (2015) 19–24.[36]
R.M. Alguliev, R.M. Aliguliyev, C.A. Mehdiyev, Sentence selection for genericdocument summarization using an adaptive differential evolution algorithm,Swarm Evolutionary Comput. 1 (4) (2011) 213–222
. [37] R. Rautray, R.C. Balabantaray, Comparative study of DE and PSO overdocument summarization, in: Intelligent Computing, Communication andDevices, Springer India, 2015, pp. 371–377.[38] H. Asgari, B. Masoumi, O.S. Sheijani, Automatic text summarization based onmulti-agent particle swarm optimization, in: Intelligent systems (ICIS), 2014Iranian conference on, IEEE, 2014, February, pp. 1–5.[39]
R.M. Alguliev, R.M. Aliguliyev, N.R. Isazade, DESAMC+ DocSum: Differentialevolution with self-adaptive mutation and crossover parameters for multi-document summarization, Knowl.-Based Syst. 36 (2012) 21–38
. [40] A. Abuobieda, N. Salim, M.S. Binwahlan, A.H. Osman, Differential evolutioncluster-based text summarization methods, in: Computing, Electrical andElectronics Engineering (ICCEEE), 2013 International Conference on, IEEE,2013, August, pp. 244–248.[41] S. Karwa, N. Chatterjee, Discrete differential evolution for text summarization,in: Information Technology (ICIT), 2014 International Conference on, IEEE,2014, December, pp. 129–133.[42]
K. Nandhini, S.R. Balasundaram, Extracting easy to understand summary usingdifferential evolution algorithm, Swarm Evolutionary Comput. 16 (2014) 19–27
.[43]
R.M. Alguliev, R.M. Aliguliyev, N.R. Isazade, Multiple documentssummarization based on evolutionary optimization algorithm, Expert Syst.Appl. 40 (5) (2013) 1675–1689
. [44]
R.M. Alguliev, R.M. Aliguliyev, M.S. Hajirahimova, C.A. Mehdiyev, MCMR:Maximum coverage and minimum redundant text summarization model,Expert Syst. Appl. 38 (12) (2011) 14514–14522
. [45]
M.D. Gordon, Probabilistic and genetic algorithms for document retrieval,Commun. ACM 31 (10) (1988) 1208–1218
. [46]
C. López-Pujalte, V.P. Guerrero-Bote, F. de Moya-Anegón, Order-based ﬁtnessfunctions for genetic algorithms applied to relevance feedback, J. Am. Soc.Inform. Sci. Technol. 54 (2) (2003) 152–160
. [47]
O.C. García, F. de Moya Anegón, C. Zarco, A GA-P algorithm to automaticallyformulate extended Boolean queries for a fuzzy information retrieval system,Mathware Soft Comput. 7 (2) (2000) 309–322
. [48] R.M. Alguliev, R.M. Aliguliyev, Effective summarization method of textdocuments, in: Web Intelligence, 2005, Proceedings, The 2005 IEEE/WIC/ACM International Conference on, IEEE, 2005, September, pp. 264–271.[49] A. Kogilavani, P. Balasubramanie, Clustering based optimal summarygeneration using genetic algorithm, in: Communication and ComputationalIntelligence (INCOCCI), 2010 International Conference on, IEEE, 2010,December, pp. 324–329.[50] Y.X. He, D.X. Liu, D.H. Ji, H. Yang, C. Teng, Msbga: A multi-documentsummarization system based on genetic algorithm, in: Machine Learningand Cybernetics, 2006 International Conference on, IEEE, 2006, August, pp.2659–2664.[51] X. Zhao, J. Tang, Query-focused summarization based on genetic algorithm, in:2010 International Conference on Measuring Technology and MechatronicsAutomation, IEEE, 2010, March, pp. 968–971.[52] A.E. Eiben, J.E. Smith, Introduction to Evolutionary Computing Eiben, SpringerPublisher, 2015.[53]
R. Rautray, R.C. Balabantaray, Cat swarm optimization based evolutionaryframework for multi document summarization, Phys. A: Stat. Mech. Appl. 477(2017) 174–186
.[54] X.S. Yang, S. Deb, Cuckoo search via Levy ﬂights, in: Proceedings of WorldCongress on Nature and Biologically Inspired Computing (NaBIC 2009), IEEEPublications, USA, 2009, pp. 210–214.[55]
X.S. Yang, S. Deb, Engineering optimisation by cuckoo search, Int. J. Math.Modelling Numer. Optimisation 1 (4) (2010) 330–343
. [56]
X.S. Yang, S. Deb, Cuckoo search: recent advances and applications, NeuralComput. Appl. 24 (1) (2014) 169–174
.R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144 143[57]A.B. Mohamad, A.M. Zain, N.E. Nazira Bazin, Cuckoo search algorithm foroptimization problems—a literature review and its applications, Appl. ArtiﬁcialIntelligence 28 (5) (2014) 419–448
. [58] W.C.E. Lim, G. Kanagaraj, S.G. Ponnambalam, Cuckoo search algorithm foroptimization of sequence in pcb holes drilling process, in: Emerging Trends inScience, Engineering and Technology, Springer India, 2012, pp. 207–216.[59]
G. Kanagaraj, S.G. Ponnambalam, N. Jawahar, Reliability-based total cost ofownership approach for supplier selection using cuckoo-inspired hybridalgorithm, Int. J. Adv. Manuf. Technol. (2014) 1–16
.[60] C.Y. Lin, E. Hovy, Automatic evaluation of summaries using n-gram co-occurrence statistics, in: Proceedings of the 2003 Conference of the NorthAmerican Chapter of the Association for Computational Linguistics on HumanLanguage Technology-Volume 1, Association for Computational Linguistics,2003, May, pp. 71–78.[61]M. Zamanian, P. Heydari, Readability of texts: state of the art, Theory PracticeLanguage Stud. 2 (1) (2012) 43
. [62] J. Kondru, Using part of speech structure of text in the prediction of itsreadability, Doctoral dissertation, University of Texas at Arlington, 2006.144 R. Rautray, R.C. Balabantaray / Applied Computing and Informatics 14 (2018) 134–144