Leguminous seeds detection based on convolutional neural networks:Comparison of Faster R-CNN and YOLOv4 on a small custom dataset
Noran S. Ouf ⁎
Faculty of Engineering, Cairo University, Giza, Egypt
abstract article info
Article history:Received 13 February 2022Received in revised form 21 March 2023Accepted 29 March 2023Available online 12 April 2023This paper help with leguminous seeds detection and smart farming. There are hundreds of kinds of seeds and itcan be very difﬁcult to distinguish between them. Botanists and those who study plants, however, can identifythe type of seed at a glance. As far as we know, this is the ﬁrst work to consider leguminous seeds images with different backgrounds and different sizes and crowding. Machine learning is used to automatically classify andlocate 11 different seed types. We chose Leguminous seeds from 11 types to be the objects of this study. Thosetypes are of different colors, sizes, and shapes to add variety and complexity to our research. The images datasetof the leguminous seeds was manually collected, annotated, and then split randomly into three sub-datasetstrain, validation, and test (predictions), with a ratio of 80%, 10%, and 10% respectively. The images consideredthe variability between different leguminous seed types. The images were captured on ﬁve different back- grounds: white A4 paper, black pad, dark blue pad, dark green pad, and green pad. Different heights and shootingangles were considered. The crowdedness of the seeds also varied randomly between 1 and 50 seeds per image.Different combinations and arrangements between the 11 types were considered. Two different image-capturingdevices were used: a SAMSUNG smartphone camera and a Canon digital camera. A total of 828 images wereobtained, including 9801 seed objects (labels). The dataset contained images of different backgrounds, heights,angles, crowdedness, arrangements, and combinations. The TensorFlow framework was used to construct theFaster Region-based Convolutional Neural Network (R-CNN) model and CSPDarknet53 is used as the backbonefor YOLOv4 based on DenseNet designed to connect layers in convolutional neural. Using the transfer learningmethod, we optimized the seed detection models. The currently dominant object detection methods, Faster R-CNN, and YOLOv4 performances were compared experimentally. The mAP (mean average precision) of the FasterR-CNN and YOLOv4 models were 84.56% and 98.52% respectively. YOLOv4 had a signi ﬁcant advantage in detec- tion speed over Faster R-CNN which makes it suitable for real-time identi ﬁcation as well where high accuracy and low false positives are needed. The results showed that YOLOv4 had better accuracy, and detection ability, as wellas faster detection speed beating Faster R-CNN by a large margin. The model can be effectively applied under avariety of backgrounds, image sizes, seed sizes, shooting angles, and shooting heights, as well as different levelsof seed crowding. It constitutes an effective and ef ﬁcient method for detecting different leguminous seeds in complex scenarios. This study provides a reference for further seed testing and enumeration applications.© 2023 The Author. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Machine learningObject detectionLeguminous seedsDeep learningConvolutional neural networksFaster R-CNNYOLOv4
1. IntroductionAgriculture is vital for human survival and remains a major driver ofseveral economies around the world. Increasing demand for food andcash crops, due to a growing global population, the challenges posedby climate change, and pandemics impacted communities and farmersworldwide. In a crisis like the one we are facing right now; seeds facea sharp rise in demand. Efforts to produce enough nutritious and afford-able food are affected by the current health crisis, there is a pressingneed to increase farm outputs while incurring minimal costs andhuman interactions. To address these challenges, big agricultural dataand new deep learning technologies are needed to be applied in theagriculturalﬁeld to help better understand, monitor, measure, and ana-lyze various physical aspects and phenomena for both short-scale cropmanagement as well as for larger-scale agricultural ecosystems' obser-vation, to enhance the management and decision making by situationand context (Ouf, 2018).In this research we focus on short-scale crop monitoring, speci ﬁcally leguminous seeds detection using deep learning algorithms. The basicprinciple of machine learning (ML) is to construct algorithms that canreceive input data and use statistical techniques to predict an outputArtiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
⁎Corresponding author.E-mail address:noransouf@gmail.com(N.S. Ouf).
https://doi.org/10.1016/j.aiia.2023.03.0022589-7217/© 2023 The Author. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the C C BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/while updating outputs as new data becomes available. In addition toinvestigating the recognition pattern and arti ﬁcial intelligence, ML expands to the investigation of the construction of algorithms that canlearn from and make predictions on data ( Sarker, 2021). Deep learning (DL) is a subset of ML inspired by the structure of thehuman brain (Elshawi et al., 2021). The central difference is that in tra-ditional machine learning, all of the data analysis and theories develop-ment like decision trees, logistic regressions, naive Bayes, and supportvector machines was done essentially by the programmer who lookedcarefully at a particular problem and then designed features thatwould be useful features for handling this problem. These kinds ofsystems would end up with millions of hand-designed features. Itturns out that the machine was learning almost nothing but only run-ning a learning numerical optimization algorithm to do numeric optimi-zation by putting a parameter weight in front of each feature andadjusting those numbers to optimize performance. However, deeplearning is part of thisﬁeld which is called representation learning.The idea of representation learning is to feed to computers raw signalsfrom the world, whether it is visual signals or language signals, and
Fig. 1.Theﬁve different Backgrounds used in the dataset.
Fig. 2.The different crowdedness represented in the dataset.Table 1Related studies.Reference Attributes Classes Crop Dataset ML algorithm Accuracy measure(Lawal, 2021, 123 CE) Detection of tomatoesin natural daylightTwo (Ripe and unripetomatoes)Tomato 125 images YOLO-Tomato models (A, B, C) (ModiﬁedYOLOv3)YOLO Tomato-A with AP 98.3%YOLO-Tomato-B with AP 99.2%,YOLO-Tomato-C with AP 99.4% (Roy et al., n.d.) Real-timeﬁne-grainobject detectionFour (Different diseasesin tomato plants)Tomato 1200 images Modi ﬁed YOLOv4 mAP of 96.29(Roy and Bhaduri,2022)Real-time growthstage detectionFour (Growth phases ofmango)Mango 420 images DenseNet-Fused YOLOv4mAP of 96.2(Kundu et al., 2021) Seeds Classiﬁcationand Quality TestingFour (Maize excellent,maize bad, pearl millet,clustered)Seeds of pearlmillet and maize3954 images YOLOv5 mAP of 98.3(Mathew and Mahesh,2022)Leaf-based diseasedetection in bellpepper plantTwo (Healthy part,Bacterial spot)Bell pepper plant 4000 images YOLOv5 mAP of 90.7(Li et al., 2022) Detection of powderymildew on strawberryleavesTwo (Infected leaves,powdery mildew)Strawberry 1023 images DAC-YOLOv4 (Modiﬁed YOLOv4)mAP of 72.7
Fig. 3.The different arrangements and combinations of seed types in the dataset.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
31then the computer can automatically, by itself, come up with good inter-mediate representations that will allow it to do tasks by inventing itsown features in the same way that in the past the human being wasinventing the features. Generally, manually designed features tend tobe over-speciﬁed, incomplete, take a long time to design and validate,and only get to a certain level of performance. However, the learnedfeatures are easy to adapt, fast to train, and they can keep on learningso that they get to a better level of performance than has been achievedpreviously. Deep learning ends up providing this sort of very ﬂexible, al- most universal learning framework which is just great for representingall kinds of information.
Fig. 4.Object detector architecture.
Fig. 5.Inception module.
Fig. 6.Module A.
 Fig. 7.Module B.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
32DL has erupted over the last decade and enormously succeeded andexpanded with continuous new improvements that are profoundly dif-ferent from the vast majority of what happened in machine learning inthe 80s, 90s, and 00s. Convolutional Neural Networks (CNN) have had arenaissance (Zhao et al., 2019), starting from approximately 2010, theﬁeld has been progressing quite so quickly in its ability to be sort ofrolling out better methods month on month due to technologicaladvances that have since happened that make this all possible. DL is em-ployed with good performance in a variety of computational tasks suchas image classiﬁcation, object detection, and computer vision.Object detection is the process of identiﬁcation/classiﬁcation and lo- calization of an object or multi objects in an image based on previouslydeﬁned classes or types. Among many factors and efforts that lead to thefast evolution of object detection techniques, notable contributionsshould be attributed to the vast amounts of data that favor deeplearning models, the development of deep convolution neural net-works, the advanced algorithms that provided better ways of learningintermediate representations, end-to-end joint system learning, andtransferring information between domains and between contexts,GPUs computing power that allows parallel vector processing, andfree cloud services that support a free GPU.
Fig. 8.Module C.
Table 2Inceptionv2 parameters.Type Patch size/stride Input sizeConv 3 × 3/2 299 × 299 × 3Conv 3 × 3/1 149 × 149 × 32Conv padded 3 × 3/1 147 × 147 × 32Pool 3 × 3/2 147 × 147 × 64Conv 3 × 3/1 73 × 73 × 64Conv 3 × 3/2 71 × 71 × 80Conv 3 × 3/1 35 × 35 × 1923xInception Module A 35 × 35 × 2885xInception Module B 17 × 17 × 7682xInception Module C 8 × 8 × 1280Pool 8 × 8 8 × 8 × 2048Linear Logits 1 × 1 × 2048Softmax Classiﬁer 1 × 1 × 1000
Fig. 9.The Faster R-CNN architecture.Table 3Faster R-CNN conﬁguration.Model Faster R-CNNNumber of classes 11Min Input dimension 600Max Input dimension 1024Image resize 416*416Feature extractor Inceptionv2First stage nms IoU threshold 0.7First stage max proposals 300Score converter SOFTMAXBatch size 12Initial learning rate 0.0002Momentum optimizer value 0.9Number of steps 35,000Augmentation Random horizontal ﬂipN.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
33There are two types of object detection models, the one-stage (densedetection) model and the two-stage (sparse prediction) model ( Jiao et al., 2023). A two-stage detector uses a preliminary stage whereregions of importance are detected and then classi ﬁed to see if an object has been detected in these areas. Two-stage detectors include theRegion-based Convolutional Neural Network (R-CNN) algorithms thathave truly been a game-changer for object detection tasks since 2013when Girshick (Girshick et al., 2013) presented R-CNN that made major progress in theﬁeld of object detection in terms of accuracy. Hefollowed it by Fast Region-based Convolutional Neural Network (FastR-CNN) (Girshick, 2015), and Faster Region-based Convolutional NeuralNetwork (Faster R-CNN) (Ren et al., 2017), which will be used in this research. On the contrary, a one-stage detector is capable of detectingobjects without the need for a preliminary step. The advantage of aone-stage detector is the speed it can make predictions quickly allowingreal-time use. One-stage detectors include Single Shot MultiBox Detec-tor (SSD) (Liu et al., 2016), and You Only Look Once (YOLO)(Bochkovskiy et al., 2020;Redmon et al., 2015;Redmon and Farhadi, 2018) algorithms. These algorithms combined the ﬁeld of object detection with deep learning, achieving signi ﬁcant improvements in accuracy, loss, and runtime.The plant/Seed detectionﬁeld has signiﬁcant room for improvement in terms of seed types, scene recognition, accuracy, loss, runtime, andease of use. This study aimed to use deep learning-based models todetect 11 different types of leguminous seeds using the free availableCOLAB platform.2. Related workVarious studies addressed crop/seed classi ﬁcation and detection by using deep learning. Recent studies have been conducted to performclassiﬁcation, disease detection, and observation of growth stages of
Fig. 10.Mosaic augmentation.
Table 4YOLOv4 conﬁguration.Parameter ValueClasses 11Batch size 64Subdivisions 16Max. Batches 22,000Steps 17,600, 19,800Filters 48Input width and height 416*416
Table 5Faster R-CNN and YOLOv4.Faster R-CNN YOLOv4Framework TensorFlow DarknetPhases RPN + Fast R-CNNdetectorConcurrent bounding-boxregression and classiﬁcation Neural network type Fully convolutional Fully convolutionalBackbone featureextractorInceptionv2 CSPDarknet53Location detection Anchor-based Anchor-basedTable 6Computer conﬁguration.Parameter Google colabCPU Intel(R) Xeon(R) CPU @ 2.30GHzCPU cores 2RAM 12GBDisk space 25GBGPU Nvidia Tesla T4GPU memory 16GBGPU memory clock 1.59GHzPerformance 8.1 TFLOPSSupport Mixed Precision YesRAM 12GBDisk space 358GB
Fig. 11.Detection Boxes Recall/AR@100 (small).N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
34different seeds and crops by using deep learning techniques (SeeTable 1).(Lawal, 2021, 123 CE) modiﬁed YOLOv3 model, the called YOLO-Tomato models used to detect two classes of ripe and unripe tomatoes,they constructed a small dataset of 125 images of tomatoes. The datasetwas grouped into Raw, 0.5 ratio, and 0.25 ratio for training and testing.They kept all things the same as the YOLOv3 model with an increase inthe concatenated features and an increase features in the FPN of theYOLO-tomato model. YOLO-Tomato-A was activated with Leaky Recti-ﬁed Linear Unit (ReLU)31 having FDL × 3. The six layers of YOLOv3were pruned as YOLO-Tomato-B was activated with Mish28 havingFDL × 1, and YOLO-Tomato-C was activated with Mish28 havingFDL × 2 and SPP26. The compared results of AP showed that YOLO-Tomato-A, B, and C outperformed the YOLOv3 model but not YOLOv4for both raw data and 0.5 ratio. However, the AP of YOLO-Tomato-Cwas slightly increased compared to YOLOv4 for 0.25 ratio but thedetection speed also increased. The performance of the models washigh because of the very small datasets which also require furtherinvestigation.(Roy et al., n.d.) proposed a real-time object detection model thatwas developed based on the YOLOv4 algorithm. They included CSP1-nblock in the backbone, CSP2-n module in the neck, and DenseNet inthe backbone to optimize feature extraction, transfer, and reuse. Theproposed detection model was used to detect four different diseasesin tomato plants. 300 images from each of the four different tomatoplant diseases are collected from the publicly available Kaggle Datasetto construct a dataset consisting of 1200 images and augmented to ob-tain the custom dataset of 12,000 images. The model outperforms theexisting state-of-the-art detection models in detection accuracy andspeed with mean average precision (mAP) value of 96.29% comparedto 92.84 for YOLOv4.(Roy and Bhaduri, 2022) proposed a real-time object detectionframework Dense-YOLOv4 based on an improved version of theYOLOv4 algorithm. The proposed model included DenseNet in the back-
Fig. 12.Detection Boxes Recall/AR@100 (medium).
Fig. 13.Detection Boxes Recall/AR@100 (large).
Fig. 14.Detection Boxes Recall/AR@100.
Fig. 15.Detection Boxes Recall/AR@10.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
35bone, SPP block, and modiﬁed path aggregation network PANet in theYOLOv4 framework. The model was used to detect different growthstages of mango. A total of 420 original images consisting of 105 imagesfrom each of the four growth phases have been considered to constructthe original dataset and augmentation has been applied expanding theoriginal dataset tenfold. The mean average precision (mAP) of the pro-posed model has reached up to 96.20% at a detection rate of 44.2 FPS.The proposed Dense-YOLOv4 has outperformed the state-of-the-artYOLOv4 with a 4.73% increase in mAP.(Kundu et al., 2021) used the YOLOv5 model for the classiﬁcation and quality testing of seeds. The dataset consisted of 3954 images ofseeds of pearl millet, healthy and diseased maize, and clustered wasconstructed. The model achieved the precision and recall of 99% inclassifying the seeds into two classes pearl millet and maize with thequality of healthy or diseased.(Mathew and Mahesh, 2022) work focused on the identiﬁcation of diseases in bell pepper plant in largeﬁelds. YOLOv5 was used for detect-ing bacterial spot disease in the bell pepper plant from the symptomsseen on the leaves. The dataset consisted of a total of 4000 images,2000 healthy and 2000 with bacterial spots. The YOLOv5 achieved90.7%.(Li et al., 2022) used a dataset of 6371 images of Strawberry pow-dery mildew (PM) and infected leaves (IL). The original YOLOv4 back-bone and neck were replaced by their proposed backbone and neckwith depth-wise convolution and hybrid attention mechanism. Theycombined the proposed backbone and neck, forming four new networkstructures, the best one was named DAC-YOLOv4. DAC-YOLOv4 usedthe depth-wise convolution and CSPNet structure concept. Comparedwith YOLOv4, the mean average precision (mAP) of DAC-YOLOv4reaches 72.7%, while the size is greatly compressed.3. Materials and methodsWe chose Leguminous seeds from 11 types to be the objects of thisstudy. Those types are of different colors, sizes, and shapes to add vari-ety and complexity to our research. Since the objects of the study are not
Fig. 16.Detection Boxes Recall/AR@1.
 Fig. 18.Detection Boxes Precision/mAP@.50IOU.
Fig. 17.Detection Boxes Precision/mAP@.0.75.
 Fig. 19.Detection Boxes Precision/mAP.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
36on publicly available datasets designed and dedicated for machinelearning and computer vision research as ImageNet ( http://www. image-net.org/), PASCAL VOC (http://cvlab.postech.ac.kr/∼Mooyeol/ pascal_voc_2012/), COCO (http://cocodataset.org/), the images dataset of the leguminous seeds was manually collected, annotated, and thensplit randomly into three sub-datasets train, validation, and test (pre-dictions), with a ratio 80%, 10%, 10% respectively.3.1. Image collectionImages for our dataset were manually collected. The images consid-ered the variability between different leguminous seed types. The 11types of leguminous seeds selected to be the research objects of thisstudy areGlycinemax,Lens culinaris-dark,Lens culinaris-yellow,Lupinus albus,Medicago sativa,Phaseolus vulgaris-pink,Phaseolus vulgaris-red, Phaseolus vulgaris-white,Trifolium alexandrinum, Trigonella foenun graecum, andVicia faba.The images were captured onﬁve different backgrounds: white A4paper, black pad, dark blue pad, dark green pad, and green pad. Differentheights and shooting angles were considered. The crowdedness of theseeds also varied randomly between 1 and 50 seeds per image. Differentcombinations and arrangements between the 11 types were consid-ered. Two different image-capturing devices were used: a SAMSUNGsmartphone camera and a Canon digital camera. A total of 828 imageswere obtained, including 9801 seed objects (labels). The datasetcontained images of different backgrounds, heights, angles, crowded-ness, arrangements, and combinations as shown in F i g s .1 ,2 ,a n d3.3.2. Object annotationObject detection or object recognition is the task of identi ﬁcation and localization of the object in an image. The collected images were la-beled by an object annotation tool. Among the commonly used tools likeLabelImg, Imglab, LabelMe, Labelbox, and RectLabel, LabelImg (Tzutalin,
Fig. 20.Detection Boxes Precision/mAP (small).
Fig. 21.Detection Boxes Precision/mAP (medium).
Fig. 22.Detection Boxes Precision/mAP (large).
Fig. 23.Yolov4 mAP@0.5.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
37https://github.com/tzutalin/labelImg) tool was selected. The image an-notations generated were saved in both a .txt ﬁle format, the input for YOLO, and a .xmlﬁle format used with the PASCAL VOC dataset thatcan be easily converted to TFrecords.3.3. Data standardizationWe used the Darknet deep learning framework for the YOLOv4model. Now ready, the images and annotations data were input intothe model. For the Faster R-CNN model, we used TensorFlow deeplearning framework, which needed the .xml annotations data to be con-verted into the TFRecord data type. Then the dataset was randomly splitinto train, validation, and test sets with ratios of 80%, 10%, and 10%,respectively.3.4. Pre-processingAll images in our dataset were pre-processed before they were inputinto the models. The most important pre-processing was the inputimage resolution that wouldﬁt our models to avoid running out ofmemory, low speed, and low accuracy. Images were resized to416 × 416 pixels.3.5. Network modelsAll object detectors Consist of a backbone, neck, and detection head.First, the input image is fed to the backbone which compresses featuresdown through a convolutional neural network. Unlike image classi ﬁca- tion, object detection backbones are not the end of the network. Predic-tions can't be made off only of them; Localization needs to be along withclassiﬁcation. Localization is the task of locating an object in the imageby drawing multiple bounding boxes, so the feature layers of theconvolutional backbone need to be mixed and held up in light of one an-other. The combination of backbone feature layers happens in the neckthen the detection occurs in the head.It is also useful to split object detectors into two categories, as showninFig. 4(Bochkovskiy et al., 2020): one-stage detectors and two-stage de-tectors. While two-stage detectors decouple the task of object localizationand classiﬁcation for each bounding box, one-stage detectors make thepredictions for object localization and classi ﬁcation simultaneously.
Fig. 24.Yolov4 loss.
Fig. 25.Predictions for images with different crowdedness.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
383.5.1. Faster R-CNNTwo models were used and compared for object detection. TheFaster R-CNN Model was developed from R-CNN and Fast R-CNN. Likeall the R-CNN family, Faster R-CNN is a region-based well-establishedtwo-stage object detector, which means the detection happens in twostages. The Faster R-CNN architecture consists of a backbone and twomain networks or, in other words, three networks. First is the backbonethat functions as a feature extractor by running a convolutional neuralnetwork on the original map to extract basic features and generate afeature map. In this study, Inceptionv2 pre-trained on the MS COCOdataset was chosen as the backbone. The two main networks, the ﬁrst network is a simple regional proposal network (RPN) that proposes aset of regions of interest not using a selective search algorithm likeits predecessors. The second network is an evaluation network or adetection network that processes both the feature map and the regionsof interest generated by the previous networks by a classi ﬁcation layer and a bounding box regression layer, generating the class andbounding box.There are multiple different feature extractors available to choosefrom, including VGG16, Inception, ResNet, and MobileNet. Fig. 5shows the Inception module of GoogLeNet used to build the Inceptionv1model. The model's convolutional layers used several ﬁlter kernel sizes 1 × 1, 3 × 3, and 5 × 5 (blue blocks) along with max-pooling(pink block). Additional 1 × 1 convolutions were added before the3×3a n d5×5c o n v o l u t i o n st or e d u c ec o m p u t a t i o n s .T h e ﬁlter concat- enation block (in green) concatenates the output depth of the convolu-tions and max pooling. Inceptionv1 consisted of nine linearly stackedmodules, a deep model of 27 layers. Inceptionv2 used three modi ﬁed module types different from the one used in Inceptionv1. Module A,shown inFig. 6,r e p l a c e do rf a c t o r i z e dt h e5×5c o m p u t a t i o n a l l ye x p e n -sive convolutions with two 3 × 3 convolutions. Module B, shown inFig. 7, replaced the nxn convolutions with a 1xn convolution followedby nx1 convolution. Eventually, Module C is shown in Fig. 8, where the moduleﬁlters are expanded to prevent loss of information due to di-mension reduction. Our choice of Inceptionv2 as the backbone networkwas based on its architecture which helps reduce the problem ofvanishing gradient and increasing loss faced by deeper models thatcan cause overﬁtting, especially with our small dataset. The parametersof Inceptionv2 42 layers are shown in Table 2based on the three mod- ules described above.The RPN is a simple convolutional network with anchors of ﬁxed dimensions and ratios that generates a set of bounding boxes called re-gions of interest (ROI) at each anchor location. The detection networkreceives the ROI and looks for objects within those regions —eventually, the detector returns proposals ofﬁnal bounding boxes with a conﬁ- dence score. The architecture of our Faster R-CNN model is shownbelow inFig. 9, and the Faster R-CNN conﬁguration is shown inTable 3.3.5.2. YOLOv4The original YOLO (You Only Look Once) used a Darknet ﬂexible framework that was written in low-level languages and has produceda series of the best real-time object detectors in computer vision. TheYOLO series moves ever forward with the publication of YOLOv4 inthe past couple of months. YOLOv4 outperforms other object detectionmodels by a signiﬁcant margin in inference speed. The Original YOLOwas theﬁrst object detection network to combine the problem ofdrawing bounding boxes and identifying class labels in one end-to-end differentiable network. YOLOv2 made a number of iterative im-provements on top of YOLO, including BatchNorm, higher resolution,
and anchor boxes. YOLOv3 built upon previous models by adding an ab-jectness score to bounding box prediction, added connections to thebackbone network layers, and made predictions at three separate levelsof granularity to improve performance on smaller objects.CSPDarknet53 is used as the backbone for YOLOv4 based onDenseNet designed to connect layers in convolutional neural networkswith the following motivations to alleviate the vanishing gradient prob-lem, bolster feature propagation, encourage the network to reuse fea-tures, and reduce the number of network parameters.The neck then is to mix and combines the features formed in theConvNet backbone to prepare for the detection step. The componentsof the neck typicallyﬂow up and down among layers and connectonly a few layers at the end of the convolutional network. Of all the op-tions as FPN, PAN, NAS-FPN, BiFPN, ASFF, and SFAM, YOLOv4 choosesPANet for the feature aggregation of the network. Additionally,YOLOv4 adds an SPP block after CSPDarknet53 to increase the receptiveﬁeld and separate out the most important features from the backbone.Finally, the detection head used by YOLOv4 is the same as YOLOv3with anchor-based detection steps and three levels of detectiongranularity.Bag of Freebies is so termed because they improve the network'sperformance without adding to inference time in production. MostlyBag of Freebies has to do with data augmentation. YOLOv4 uses dataaugmentation to expand the size of its training set and expose themodel to semantic situations that it would not have otherwise seen.Most of the augmentation was already known in the ﬁeld of computer vision except the mosaic data augmentation, which combines fourimages together in one image, resizes along with the BB and put themon one grid, then takes a random crop from the center as shown inFig. 10. The technique help with teaching the model to ﬁnd smaller ob- jects and pay less attention to surrounding scenes that are not immedi-ately around the object. Our custom dataset is a good candidate formosaic, which helps a lot with real-world objects, small objects, andmobile objects (the position of objects changes in an image).Mosaic was optimized by equally sampling all parts of the image (in-cluding objects not just at the center). Since larger upfront objects are
Fig. 26.Predictions for images with different background.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
39Fig. 27.Predictions for images with different combinations and multiple types.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
40more commonly labeled (detected) than smaller further away objectsby a detector, we placed smaller objects at the edge up and down inan image so the mosaic random rectangular crop from the center willcapture them (zoom in on smaller objects).YOLOv4 conﬁguration is shown inTable 4. The maximum batches are set to be the number of classes * 2000. The training steps are set tobe 80% of maximum batches and 90% of maximum batches, and the ﬁl- ters in the three convolutional layers before the YOLO layer are set to be(number of classes+5) *3 (SeeTable 5).3.6. The assessment method3.6.1. Loss functionThe performance of a model is assessed by a cost function or a lossfunction. The smaller the loss function is, the better the model ﬁts. Sim- ilar to fast R-CNN. Faster R-CNN is optimized for a multi-task loss func-tion (Wu et al., 2020). The loss function combines the losses ofclassiﬁcation and bounding box regression as follows:L¼L
clsþLbox ð1Þ
Fig. 28.Predictions for images with all types.
Fig. 29.YOLOv4 predictions.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
41Lpifg;t ifgðÞ ¼1N
clsX
iLclspi;p/C3i/C0/C1þ λN
boxX
ip/C3i:Lsmooth1ti−t/C3i/C0/C1ð2Þwhereirepresents the index of an anchor in a batch, p
iis the predicted probability of anchoribeing an object,p
i∗is the Ground truth label (binary) of whether anchoriis an object; when the anchor is apositive sample,p
i∗= 1, and when it is a negative sample p i∗=0 .I t can be seen that the regression loss term is only activated if theanchor is positive;t
iis the predicted four parameterized coordinatesof the positive sample anchor;t
i∗is the ground truth coordinates ofthe positive sample anchor;λis a balancing parameter used to weighclassiﬁcation lossL
clsand bounding box regression loss L boxso that both terms are roughly equally weighted, and the default value of λ set to be∼10;N
clsandN boxare normalization terms used to normalizeclassiﬁcation loss itemL
clsand regression loss itemL box, respectively. WhereL
clsis the log loss function over two classes, as we can easilytranslate a multi-class classiﬁcation into a binary classiﬁcation by predicting a sample being a target object versus not. L
1smooth is the smoothL
1loss. The classiﬁcation loss functionL clsis a Boolean classiﬁer (object or not), and the formula is as follows:L
clspi,p∗i/C0/C1¼/C0p∗ilogpi/C01/C0p∗i/C0/C1log 1/C0p
i ðÞ ð3ÞThe bounding box regression loss function L
boxis used to calculate the difference between the two transformations, and the formula is asfollows:L
boxti,t∗i/C0/C1¼Rt
i/C0t∗i/C0/C1 ð4ÞwhereRfunction is deﬁned asSmoothL1xðÞ ¼0:5x∗,if xjj<1xjj /C00:5otherwise/C26 ð5Þ3.6.2. Precision, recall, and mAPThe mAP is a popular metric used to assess the performance of an ob-ject detection model. It involves two concepts: precision and recall. Foran object, precision, also known as the positive predicted value, is theratio of correctly predicted positive observations to the total predictedpositive observations, which means high precision relates to the lowfalse-positive ratePrecision¼
TPTPþFP ð6ÞWhile recall, also known as true-positive rate or sensitivity, is theratio of correctly predicted positive observations to all observations inan actual classRecall¼
TPTPþFN ð7ÞA trade-off between precision and recall performance can be ad-justed by the model'sﬁnal layer softmax threshold. Increasing thethreshold would decrease the number of FP which will lead to higherprecision and lower recall. Similarly, to increase recall we need to de-crease the number of FN which will reduce precision. Commonly in ob-ject detection tasks, precision needs to be high (predicted positives to beTP). Precision and recall are widely used along with other metrics suchas accuracy, which is simply a ratio of correctly predicted observation tothe total observationsAccuracy¼
TPþTNTPþFPþFNþTN ð8Þand F1-score also known as true negative rate (TNR) ( Boracchi et al., 2017) is the weighted average of Precision and Recall not as simple as
Fig. 30.YOLOv4 predictions with multiple types.
Table 7Model performance evaluation.Model mAP@0.5 Time/msYOLOv4 98.52% 47.2Faster R-CNN 84.56% 53.1N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
42accuracy but more useful than accuracy. Accuracy is a great measure butonly when you have symmetric datasets where values of false positivesand false negatives have a similar cost. However, the F1 score takes bothfalse positives and false negatives into account. If the cost of falsepositives and false negatives are very different, it's better to look at bothPrecision and Recall.F1Score¼
2∗Recall∗PrecisionðÞRecallþPrecision ð9ÞIntersection over Union (IoU) deﬁnes the calculation of AP for objectdetection. The IoU is given by the ratio between the area of intersectionand the area of the union of the predicted bounding box and groundtruth bounding box as shown in the equation.IoU¼
Area of OverlapArea of Union ð10ÞThe IoU would be used to determine if a predicted bounding box(BB) is TP, FP, or FN. The TN is not evaluated as each image is assumedto have an object in it. Traditionally, IoU is set to 0.5. when the objectdetection model run on an image, a predicted bounding box would bedeﬁned to be a TP if the IoU is >0.5, FP if either IoU < 0.5 or the boundingbox is duplicated, and FN If the object detection model missed the targeteither because there is no detection at all or the predicted BB hasan IoU > 0.5 but has the wrong classiﬁcation, the predicted BB would be FN.Precision and recall were calculated for a given class across the testset. Each BB would have its conﬁdence level, usually given by its softmaxlayer, and would be used to rank the output.3.6.3. Interpolated precisionBefore we plot the PR curve, we needﬁrst need to know the interpo- lated precision. The interpolated precision, p
interp, is calculated at each recall level,r, by taking the maximum precision measured for that r. The formula is given as such:p
interp rðÞ ¼max
r:r≥rprðÞ ð 11Þwherep~rðÞis the measured precision at recall ~r. Their intention of interpolating the PR curve was to reduce the im-pact of“wiggles”caused by small variations in the ranking of detections,then we can plot the PR curve. For each example, the corresponding pre-cision, recall, and interpolated precision are calculated by the formulasdeﬁned above. The AP is then calculated by taking the area under thePR curve. This is done by segmenting the recalls evenly into 11 parts:{0,0.1,0. 2,…,0.9,1}. We get the following:AP¼
111∑
r∈0, 0:1, 0:2, 0::,0:9, 1fgpinterp rðÞ ð12Þ3.6.4. COCO metricsTheCOCOdataset is typically used to train and validate object detec-tion models. It contains a broad range of 80 object classes that will help amodel generalize. Transfer learning then can be used to expose pre-trained models on the COCO dataset to new training data and new ob-ject detection tasks. COCO provided six new methods of calculating ARand mAP at different IoU thresholds and object sides. The metrics aremAP at IoU = 0.5 which is IoU of BBs needs to be above 0.50, mAP atIoU = 0.75 which IoU of BBs needs to be above 0.75, mAP at IoU =0.50: 0.05: 0.95, mAP for small objects that have an area below 32
2px, mAP for medium objects that have an area between 32
2and 962px, mAP for large objects that have an area above 96
2px. The most impor- tant metric mAP is evaluated by calculating AP at starting IoU = 0.5 toan IoU = 0.95, with incremental steps of 0.05. The results are thenaveraged.Other metrics involve AR at different object sizes and number ofdetections. The metrics are AR for small objects that have an areabelow 322px, AR for medium objects that have an area between 322
and 962px, AR for large objects that have an area above 962px, AR with the number of detections below 100, AR with the number of detec-tions below 10, AR with only one detection.This would allow better differentiation of models as some datasetshave more small objects than others. In this case, precision and recallcan be calculated. The average precision (AP) and average recall (AR)curves can be retrieved through multiple calculations and trials foreach class, and the area under the curve is the AP value. The mAP for ob-ject detection is the average of the AP calculated for all the classes asshown in the following formula:mAP¼
1Q
Rjj∑q∈Q RAP qðÞ ð13Þwhere Q is the number of queries.3.7. Computer conﬁgurationColaboratory or Colab is an AI browser-based platform that allowswriting and executing notebooks written in python code. It allows usto run our code on a free GPU. The speci ﬁcations of the machine are shown inTable 6.4. Model training and results4.1. Model training4.1.1. The faster R-CNNWe monitored the training and veriﬁcation process, and the param- eters were adjusted and tuned accordingly. For the feature extraction,we used Inceptionv2, as mentioned earlier, which was pre-trained onthe COCO dataset and tuned for our custom dataset; this process oftransfer learning greatly reduced the training time. The model beganto converge quickly at 5000 iterations. The number of iterations had agreat inﬂuence on the training effect of the model. As shown in theFigures below, the performance of the model was signi ﬁcantly improv- ing with iterations meaning the model is learning and there is nounderﬁtting. Reaching 30,000 iterations, the curves start to reach apoint of stability. A choice of 30,000 iterations steps was perfect inorder for the model to generalize and don't over ﬁt with our data. Figs. 11, 12, and 13Show the Detection Boxes Recall/AR@100 (small),Detection Boxes Recall/AR@100 (medium), and Detection BoxesRecall/AR@100 (large) that represent the average recall for smallobjects, average recall for medium objects, and average recall for largeobjects with 100 detections.Figs. 14, 15, and 16. Show the Detection Boxes Recall/AR@100,Detection Boxes Recall/AR@10, and Detection Boxes Recall/AR@1 thatrepresent average recall with 100 detections, average recall with ten de-tections, and average recall with one detection.The above 6 metrics were based on recall or mAR (mean average re-call). Detection Boxes Recall/AR@(1,10,100) are mean average recallssliced by the number of detections in the image. AR@1 means that itwill compute the mean average recall across all images with at mostone detection (i.e., 0 or 1), across all classes, and all IoU thresholds. ForAR@10, it would do the same, but across all images with at most 10detections (i.e. 0≤n≤10). AR@100 is for at most 100 detections. The ﬁg- ures converged and stabilized quickly. AR@10 and AR@100 show thehighest value and reached above 0.6 since most of the images containten or more detections (objects).Detection Boxes Recall/AR@100 (small, medium, large) are mean av-erage recalls sliced by the size of the detected bounding box for at mostN.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
43100 detections. This means that it only takes images with at most 100detections (most of the images). Sizes of the small is (0, 32*32), medium(32*32, 96*96), large (96*96, 1e5*1e5).Detection Boxes Precision/mAP, as mentioned earlier, is computingthe precision over all images, classes, and IoU thresholds and then tak-ing the average (SeeFig. 19).Detection Boxes Precision/mAP@.50IoU speci ﬁes the IoU, so it doesn't go over all IoU thresholds as general mAP. It computes onlythe average precision at the 0.5 IoU threshold. The idea of this metricis to give you a rough sense of precision, not super strict about theposition of bounding boxes (only require at least IoU = 0.5 to countas positive). The mAP@0.5 reached above 80%, according to Fig. 18. Also, Detection BoxesPrecision/mAP@.75IoUcomputed at IoU = 0.75 instead of IoU = 0.5. The idea of this metric is to give a rough sense ofprecision, being too strict about the position of bounding boxes (requir-ing at least IoU = 0.75 to count as positive). Fig. 17shows that it reached above 60%.Detection Boxes Precision/mAP (small, medium, large) are essen-tially the same as mAP above but sliced by the size of the boundingboxes. The small one is only computing mAP for bounding boxes thatare small (area < 32*32 pixels). Medium is for bounding boxes with32*32 < area < 96*96. Large is for area > 96*96 (in reality the imple-mentation for large is 96*96 < area < 1e5*1e5). These metrics allowyou to get a sense if your model is performing better/worse in speci ﬁc sizes of bounding boxes. As shown, our Faster R-CNN model performsthe best with medium-sized objects (most of our 11 categories) (SeeFigs 20, 21 and 22).4.1.2. YOLOv4Figs. 23 and 24Show the loss andmAP@0.5, respectively. After 2200 iterations, the average loss drops below 2.0, and mAP stabilized prettyquickly, eventually reaching almost 99%.4.2. PredictionsThe Predictions clearly show that our Faster R-CNN model wasaccurate with the different crowdedness, backgrounds, combinations,and multiple types, as shown inFigs. 25, 26, and 27. However, it strug- gles with the detection of small-sized Medicago sativa,Trifolium alexandrinum, and Trigonella foenun graecum when there are manypredictions in an image or they are placed next to relatively larger sizeseeds, as shown inFig. 28.YOLOv4 predictions with all types of seeds, even the small-size typeswas incredibly accurate. The error rate was approximately zero, asshown inFigs. 29 and 30.5. DiscussionIn previous plant/seed detection studies, it has been observed thatthe datasets were very small and they focused on detecting a few cate-gories which contributed to the high mAP achieved. In this study, twomodels were trained and tested for leguminous seed recognition.Their performances were compared in identifying 11 different catego-ries of leguminous seeds, some are very small and have similar colorsagainst different backgrounds and within the same image. As showninTable 7. the YOLOv4 was the most successful one with a 16.5% in-crease in mAP than Faster R-CNN, and an inference speed of 47.2 mscompared to 53.1 ms for Faster R-CNN.Small object detection has always been a research hotspot in theﬁeld of object detection. In agricultural production, many seeds arevery small and mostly have similar colors, making seed detection a chal-lenge. This research used object detection models for the accurate iden-tiﬁcation and positioning of small seeds. Mosaic data augmentation inYOLOv4 was utilized in the training stage, which mixed four imagesinto one image. The utilization of Mosaic data augmentation has provento help with the detection of the smallest leguminous seed types in ourdataset and also with the detection of very close seeds with similarcolors.6. ConclusionIn this paper, the leguminous seeds dataset was collected and withthe use of transfer learning two deep learning-based models weretrained. YOLOv4 proved to improve the accuracy and the runtimewith less computation load. The model can be applied to the detectionof seed images in a variety of complex backgrounds at multiple scales,and in multi-angle environments. With an error rate of <2% and a run-
ning time of <2 s, the YOLOv4 model constitutes an effective tool for thedetection of leguminous seeds.Future work will be focused on building larger datasets and keeptesting new algorithms to further optimize the model and improvethe mAP and speed of detection. Moreover, current work can be ex-tended to different seed detection, seed disease detection, real-timeseed detection and enumeration, and various automated agriculturaldetection processes.CRediT authorship contribution statementNoran S. Ouf:Conceptualization, Methodology, Software, Valida-tion, Formal analysis, Investigation, Resources, Data curation, Writing–original draft, Writing–review & editing, Visualization.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Bochkovskiy, A., Wang, C.-Y., Liao, H.-Y.M., 2020. YOLOv4: Optimal Speed and Accuracy of Object Detection.Engineering applications of neural networks. In: Boracchi, G., Iliadis, L., Jayne, C., Likas, A.(Eds.), Communications in Computer and Information Science. 744. https://doi.org/ 10.1007/978-3-319-65172-9. Elshawi, R., Wahab, A., Barnawi, A., Sakr, S., 2021. DLBench: a comprehensive experimen-tal evaluation of deep learning frameworks. Clust. Comput. 24, 2017 –2038.https:// doi.org/10.1007/S10586-021-03240-4 . Girshick, R., 2015.Fast R-CNN. IEEE International Conference on Computer Vision. IEEEComputer Society, USA, pp. 1440 –1448. Girshick, R., Donahue, J., Darrell, T., Malik, J., 2013. Rich feature hierarchies for accurate ob- ject detection and semantic segmentation. Proc. IEEE Comput. Soc. Conf. Comput. Vis.Pattern Recognit. 580–587.Jiao, L., Zhang, F., Liu, F., Member, S., Yang, S., Member, S., 2023. A Survey of Deep Learning-based Object Detection. pp. 1 –30. Kundu, N., Rani, G., Dhaka, V.S., 2021. Seeds classi ﬁcation and quality testing using deep learning and YOLO v5. ACM Int. Conf. Proc. Ser. 153 –160.https://doi.org/10.1145/ 3484824.3484913.Lawal, M.O., 2021. 123AD. Tomato detection based on modi ﬁed YOLOv3 framework. Sci. Report. 11, 1447.https://doi.org/10.1038/s41598-021-81216-5 . Li, Y., Wang, J., Wu, H., Yu, Y., Sun, H., Zhang, H., 2022. Detection of powdery mildew onstrawberry leaves based on DAC-YOLOv4 model. Comput. Electron. Agric. 202,107418.https://doi.org/10.1016/J.COMPAG.2022.107418 . Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.Y., Berg, A.C., 2016. SSD: singleshot multibox detector. Lect. Notes Comput. Sci. (including Subser. Lect. Notes Artif.Intell. Lect. Notes Bioinformatics) 9905 LNCS. pp. 21 –37.https://doi.org/10.1007/ 978-3-319-46448-0_2.Mathew, M.P., Mahesh, T.Y., 2022. Leaf-based disease detection in bell pepper plant usingYOLO v5. Sign. Image Video Process. 16, 841 –847.https://doi.org/10.1007/S11760- 021-02024-Y/FIGURES/12.S. Ouf, N., 2018. A Review on the Relevant Applications of Machine Learning in Agricul-ture. IJIREEICE 6, 1–17.https://doi.org/10.17148/IJIREEICE.2018.681Redmon, J., Farhadi, A., 2018.YOLOv3: An Incremental Improvement.Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2015. You only look once: uniﬁed, real- time object detection. Proc. IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit.2016-December, pp. 779–788.Ren, S., He, K., Girshick, R., Sun, J., 2017. Faster R-CNN: towards real-time object detectionwith region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. 39, 1137 –1149. https://doi.org/10.1109/TPAMI.2016.2577031 .N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
44Roy, A.M., Bhaduri, J., 2022. Real-time growth stage detection model for high degree of oc-cultation using DenseNet-fused YOLOv4. Comput. Electron. Agric. 193, 106694.https://doi.org/10.1016/J.COMPAG.2022.106694 . Roy, A.M., Bose, R., Bhaduri, J., n.d. A fast accurate ﬁne-grain object detection model based on YOLOv4 deep neural network. https://doi.org/10.1007/s00521-021- 06651-x.Sarker, I.H., 2021. Machine learning: algorithms, real-world applications and research di-rections. SN Comput. Sci., 1–21.https://doi.org/10.1007/S42979-021-00592-X .Wu, W., Le Yang, T., Li, R., Chen, C., Liu, T., Zhou, K., Sun, C. Ming, Li, C. Yan, Zhu, X. Kai, Guo,W. Shan, 2020. Detection and enumeration of wheat grains based on a deep learningmethod under various scenarios and scales. J. Integr. Agric. 19, 1998 –2008.https:// doi.org/10.1016/S2095-3119(19)62803-0 . Zhao, Z.Q., Zheng, P., Xu, S.T., Wu, X., 2019. Object detection with deep learning: a review.IEEE Trans. Neural Networks Learn. Syst. 30, 3212 –3232.https://doi.org/10.1109/ TNNLS.2018.2876865.N.S. Ouf Artiﬁcial Intelligence in Agriculture 8 (2023) 30 –45
45