Deep learning regularization techniques to genomics data
Harouna Soumarea,b,*, Alia Benkahlab,1, Nabil Gmatic,1
aThe Laboratory of Mathematical Modelling and Numeric in Engineering Sciences, National Engineering School of Tunis, University of Tunis El Manar, R ue B/C19echir Salem Belkhiria Campus Universitaire, B.P. 37, 1002, Tunis Belv /C19ed/C18ere, Tunisia
bLaboratory of BioInformatics, BioMathematics, and BioStatistics, Institut Pasteur de Tunis, 13 Place Pasteur, B.P. 74 1002, Tunis, Belv /C19ed/C18ere, Tunisia
cCollege of Sciences&Basic and Applied Scientiﬁc Research Center, Imam Abdulrahman Bin Faisal University, P.O. Box 1982, 31441, Dammam, Saudi Arabia
ARTICLE INFO
Keywords:Deep learningOverﬁttingRegularization techniquesDropoutGenomicsABSTRACT
Deep Learning algorithms have achieved a great success in many domains where large scale datasets are used.However, training these algorithms on high dimensional data requires the adjustment of many parameters.Avoiding overﬁtting problem is difﬁcult. Regularization techniques such as L
1andL2are used to prevent the parameters of training model from being large. Another commonly used regularization method called Dropoutrandomly removes some hidden units during the training phase. In this work, we describe some architectures ofDeep Learning algorithms, we explain optimization process for training them and attempt to establish a theo-retical relationship betweenL
2-regularization and Dropout. We experimentally compare the effect of these techniques on the learning model using genomics datasets.
1. IntroductionIn the last decade, Deep Learning (
DL) algorithms have achieved tremendous success in many domains where large scale datasets are usedsuch as Bioinformatics [2,13,50,62,88,94], Natural Language Processing [5,15,28,47,71], Computer Vision and Speech Recognition [ 1,4,29,34, 37,56,65].In this work, we review a class of
DLalgorithms called Feedforward Neural Network (
FNN)[54,68,91], in which information moves in onedirection, from input to output through sequential operations called“layers”. These models are the generalization of logistic regressionmodels, both (
FNNand logistic regression) are widely used in Bioinfor-matics and Biomedical science to perform classi ﬁcation and diagnosis tasks [8,20,21,24,27,44,48,70,73]. In most cases, we look for a nonlinear mappingy¼fðxÞbetween a variableyand a vector of variablesx. The form offdepends on the complexity of the studied problem.Logistic regression deﬁnes a low complexity model using a simple nonlinear mapping from inputs to outputs. Whereas
FNNdeﬁnes a more complex mapping between inputs and their corresponding outputs, thusresulting models have high complexity and ﬂexibility and better pre- diction capacity. However, increasing the complexity of predictivemodels increases also the risk of overﬁtting problem, which repercussionis that the training modelﬁts well the training dataset but looses itsprediction capacity on unseen datasets.Preventing overﬁtting problem is one major challenge in trainingthese algorithms. However, there are many techniques that deal with theproblem of overﬁtting called“regularization techniques”. The most used regularization techniques in Machine Learning (ML) community areL1
andL2regularization's [53]. The idea is to prevent the weights of themodel from being large by adding a supplementary term to the lossfunction. The effect of this penalization is to make it so the learning al-gorithm prefers to learn small weights. This method makes models lesscomplex and avoid the risk of overﬁtting. Another commonly used reg- ularization technique so-called“Dropout”, developed by Hinton et al. [33] consists to randomly remove some neurons (in hidden layers) dur-ing the training phase. This forces the hidden units to extract useful in-formation's from the input data and reduce co-adaptation betweenhidden units, thus making the model less sensitive to the speci ﬁc weights of neurons. The Dropout technique allows to train an exponential numberof (thinned) Networks in a reasonable time [ 33]. During the test phase, taking the mean prediction of the different (thinned) Networks isequivalent to test on a single Network with all the hidden neurons [ 6] (without dropping out any unit). To compensate the fact that the weightsare learned under Dropout, the outcome weights of neurons of each
* Corresponding author. The Laboratory of Mathematical Modelling and Numeric in Engineering Sciences, National Engineering School of Tunis, Unive rsity of Tunis El Manar, Rue B/C19echir Salem Belkhiria Campus Universitaire, B.P. 37, 1002, Tunis Belv /C19ed/C18ere, Tunisia. E-mail addresses:soumare.harouna@enit.utm.tn(H. Soumare),Alia.Benkahla@pasteur.tn(A. Benkahla),nmgmati@iau.edu.sa(N. Gmati).
1These autors contributed equally, the order of their names is alphabetical.
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100068Received 28 November 2020; Received in revised form 26 March 2021; Accepted 10 May 2021Available online 24 May 20212590-0056/©2021 Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 11 (2021) 100068hidden layer are multiplied by the Dropout rate of that layer, which is again in terms of computation time. However, the quality of thisapproximation remains little known.Many theoretical Dropout analyses have been explored [ 6,23,31,49, 55,58,75,79,81]. Baldi et al. [6] showed how the technique acts asadaptative stochastic Gradient Descent. Wager et al. [ 79] analyzed Dropout as an adaptive regularizer for Generalized Linear Models(GLMs). Ma et al. [46] attempted to explicitly quantify the gap betweenDropout's training and inference phases and showed that the gap can beused to regularize the standard Dropout training loss function.This paper explains the mathematics behind training
DLalgorithms and attempts to further establish the theoretical relationship that existsbetween Dropout and other regularizations, mainly L
2norm. We compare experimentally the effects of regularization techniques ontraining models using two different genomic classi ﬁcation datasets. The human
DNAis a long chain of 3 billion base pairs, the function of alarge part of it, is unknown. Some fragments of
DNAcalled genes code for proteins that play important roles in chemical processes essential to life.Some changes in the genes cause a dysfunction in the production of thecorresponding proteins, which could cause genetic diseases. The mostcommon genetic changes are called Single Nucleotide Polymorphisms(SNP
S) and are caused by a change of a base pair by another one at agiven position in the genome. It has been shown that some SNP
Sare involved in several human diseases and can be used to predict humanreponse to certain drugs [27].In our experiments, we started by using Logistic Regression on cancerdatasets, obtained from the Expression Project for Oncology (
EXPO)[60]. Then we trained
FNNon one 1000 Genomes Project dataset for individualancestry prediction according to their genetic pro ﬁle [59]). All in- dividuals are represented in both datasets by their SNP
Sproﬁle [14]. This work is organized as follows:Section 2describes the
FNNarchi- tectures and the mathematics behind them; in Section 3Gradient descent algorithm is presented; Section4describes the traditional regularizationtechniques and Dropout, Section5describes the materials and methodsand in Section6, we present the experimental results, where differentregularization techniques are used.2. Deep Learning: Feedforward Neural Network(
FNN)In this work, we discuss Feedforward Neural Network (
FNN)[42,54, 57,69,76,91] or Multi-Layer Perceptron (
MLP). In such Networks, the in- formation moves only from the input to the output (see Fig. 2), without any loop. This type of model is mostly used for supervised
MLtasks such as regression or classiﬁcation tasks, where the target function is known. Thebasic supervised learning algorithm is linear regression [ 12,51,82], in this task the algorithm learns to map an input data x2R
dto some real valuey, by a linear transformationf:R
d→Rx→z¼x/C1wþb:Wherewandbare respectively the weight vector and the bias term. Thesymbol“/C1”is the dot product between two vectors. Another simple su-pervised learning algorithm called logistic regression is used for theclassiﬁcation problems where the target function takes discrete values.Given an input datax, the logistic regression [18,19,39,74,86] applies a non linear function to its corresponding linear regression output z,t o produce classes membership probabilities. For example, in a binaryclassiﬁcation task, givenxand it corresponding classC
1, logistic regression algorithm outputs the conditional probability PðC
1jxÞofx givenC
1. This probability is given by sigmoid function σðzÞ¼11þe/C0z. In the case where there are more than two classes, the conditional probabilityPðC
ijxÞis given by the softmax functionsoftmaxðzÞi¼eziPnck¼1ezk. Wherez¼ ðz
1z2…zncÞandz i¼x/C1w iþb i.wiandb iare respectively the weight vector and bias term of theith classC
i.ncis the number of classes and theclass probabilities sum up to 1, i.e.Pnc
i¼1softmaxðzÞi¼1.Fig. 1describes the simplest possible Neural Network (
NN), which contains a single neuron corresponding exactly to an input-output mapping. A neuron withsigmoid output function is equivalent to logistic regression.
FNNis a nonlinear function, which is also composed of several simpler func-tions(neurons, where the output of a neuron can be used as an input ofanother. Each of these functions provides a new representation of theinput data. It is composed of an input layer, one or more hidden layer(s)and an output layer.2.1. Supervised Neural NetworkLet's consider anLhidden layer Feedforward Neural Network, inwhichninput training samplesX¼x
1;x2;…;x nare labeled, i.e., given an inputx
i, the corresponding output by the model is known and denoted y i
oryðx iÞ. Whereyis a vector containing labels. A standard Neural Networkcan be described as follows:a
ðlÞj¼φ/C16zlj/C17; (1)z
lj¼X
iwlijaðl/C01Þiþblj¼aðl/C01Þ/C1wljþblj;(2)wherez
lj,bljandalj(a0j¼x j, for ad-dimensional inputx¼ðx 1x2…x dÞT) are thejth hidden input, bias and activation function of the lth layer, respectively.w
lijis the weight connection from theith unit of theðl/C01Þth layer to thejth unit of thelth layer.w
ljandaðl/C01Þare, respectively, the incoming weight vector to thejth neuron of layerland the output vector of(l-1)th layer,φis any activation function.
FNNs can be seen as a generalization of simple regression models. In fact, keeping only theinput layer with one linear output neuron in a Feedforward Networkdeﬁnes a linear regression, and with a sigmoid or softmax function at theoutput layer represents a logistic regression. Learning of a supervisedNetwork [26,63,87,90] consists toﬁnd the parametersw
jandb jso that outputa
Lfrom the model approximates the desired output vector yðxÞ, for all training inputsx. To achieve this goal, we deﬁne a mean squared error loss functionC¼
1nX
x2XCx; (3)C
x¼12kyðxÞ/C0aLðxÞk22¼12Pnc
k¼1ðyk/C0aLkÞ2. Wherey kandaLkarekth output activation and desired output respectively, for a given input x. There is another choice of loss function known as crossentropy. Todeﬁne this function, let's consider a binary classi ﬁcation problem with sigmoid output functiona
LðxÞ¼ σðzÞ, for each training input samplex, we havePðy¼0jxÞ¼1/C0Pðy¼1jxÞ¼1/C0a
LðxÞand more generallyPðy¼y
ijxiÞ¼aLðxiÞyi/C01/C0aLðxiÞ1/C0y i/C1:
Fig. 1.Logistic regression“neuron”.H. Soumare et al. Array 11 (2021) 100068
2Supposing that the couplesðx i;yiÞ,i2f0;…;1gare independent, the likelihood function is givenP yjXÞ¼/C0Y
ni¼1Pðy¼y ijX! (4)¼/C0Y
ni¼1aLðxiÞyi/C01/C0aLðxiÞ1/C0y i/C1: (5)Training the
NNconsists to maximize the likelihood function which isequivalent to minimize the crossentropy loss function de ﬁned byC¼/C0
1nXni¼1yilogaLðxiÞþð1/C0y iÞlogð1/C0aLðxiÞÞ:(6)Consider now, a multi-class classiﬁcation problem, where the labels are mutually exclusive. In this case, (6) takes the formC¼/C0
1nXni¼1Xnc
k¼1ykðxiÞlogaLkðxiÞ: (7)a
LkðxiÞis the softmax function satisfying 0/C20aLkðxiÞ/C201 andPnc
k¼1aLkðxiÞ¼ 1. In the rest of this work,Cdenotes the loss function deﬁned by(3).2.2. Unsupervised Neural Network(Autoencoder)So far, we have described
FNNin the supervised learning case. Here,we suppose that input samplesX¼fx
1;x2;…;x ngare unlabeled, where x
i2Rd. Autoencoder is one the most used unsupervised learning algo-rithms [41,52,77,83,93]. An Autoencoder is a
NNdesigned to learn an identity function in a way that the original input can be reconstruct froma compressed version. Such a network will allow the discovery of a moreefﬁcient and compressed representation of the input data. It consists oftwo parts, an encoder and a decoder. Encoder maps input samples to ahidden representation and decoder tries to reconstruct inputs from anencoder, so it contains at least one hidden layer. In an AutoencoderNetwork, the targetyðxÞof each input samplexis the input it self, i.e. yðxÞ¼x,8x2Rd. At the end the output has the size of the input.The main objective of an Autoencoder is to automatically capture themost relevant features from input data. It is also used as a nonlineardimensionality reduction technique [ 32,66,80] to transform a highd dimensional data to a lower dimensional data. Mathematically it isdeﬁned by the following application:o:R
d→Rd
xi→φ0
W01∘φW1ðxiÞ;8x i2Rd
WhereφW1andφ0
W01are the encoding and decoding functions parame-trized byW
12Rd/C2handW012Rh/C2drespectively and deﬁned as follow:φ
W1:Rd→Rh
xi→a hðxiÞ;φ0
W01:Rh→Rd
ahðxiÞ→oðx iÞ:Wherea
handoare, respectively, the hidden and output layers outputvectors. The parametersðW
1;W01Þare learned by minimizing the reconstruction error between the input and the output of NetworkL¼
12nXni¼1kxi/C0φ0
W01∘φW1ðxiÞk22:After Autoencoder training, the decoding layers are removed and theencoding layers are retained and the learned matrix W
1is then used as parameters of theﬁrst layer(s) of the supervised Network (see Fig. 3). Alternatively, the couple (W
1;W01) can be learned jointly(seeFig. 4) with the classiﬁcation Network [22,61,92]by minimizingC
T, the following loss functionC
T¼Cþγ2nXni¼1kxi/C0^x ik22:Where^x
i¼φ0
W01∘φW1ðxiÞ,^Xis a matrix whose rows are formed by ^x i’s andγis a tuning parameter.3. Gradient descentOnce the loss function is deﬁned, gradient descent strategy is typicallyused to minimize it. Gradient descent is a ﬁrst-order optimization strat- egy for nonlinear minimization problems [ 17]. The loss functionCis minimized iteratively by using Gradient descent method [ 3] given byw
lij→wlij/C0α
nX
x2X∂Cx
∂wlij: (8)Where
αis the learning rate. For the sake of simpli ﬁcation, we assume that there are no bias termsb
ljor simply consider it as an additionalcomponent ofw
lj. At each iteration, we have to compute partial de-rivatives ofC
xfor each training inputx, and then average them to update weightsw
lij. Unfortunately, this method can be very expensive andlearning occurs slowly when the number of training inputs is large. Thisproblem of learning slowness can be avoided by the Stochastic Gradient
Fig. 2.Classiﬁcation network.
Fig. 3.Autencoder
Fig. 4.Classiﬁcation network&autoencoder.H. Soumare et al. Array 11 (2021) 100068
3Descent (SGD) method.3.1. Stochastic gradient descentThe idea of stochastic gradient descent [ 9,10,40,89] is to estimate at each iteration partial derivatives for only a small randomly chosensampleX
m¼fx 1;x2;…;x mgcalled mini-batch and train with it.w
lij→wlij/C0α
mX
x2X m∂Cx
∂wlij: (9)We then take another randomly chosen mini-batch and the weight pa-rameters are updated on it, until the training inputs are exhausted, whichis called an epoch of training. At this point, we start again with a newepoch. To compute the partial derivatives,
∂Cx
∂wlijat each layer, we apply the chain rule:
∂Cx
∂wlij¼δljðxÞ∂zlj
∂wlij¼δljðxÞal/C01iðxÞ:Whereδ
lj¼∂Cx
∂zljrepresent the error function ofjneuron in thelth layer, for an inputx. For the sake of simplicity, we just write δ
ljandal/C01iinstead of δ
ljðxÞandal/C01iðxÞ. This expression tells us how a little change in theweighted input to thejth neuron in layerlchanges the overall behavior of the loss function. The backpropagation algorithm is used to compute δ
lj
for each layer.3.2. BackpropagationBackpropagation [30,84,85] is a widely used algorithm in minimizingFeedforward Neural Network loss functions. It uses the chain rule tocompute iteratively the error of each neuron in a Network, from theoutput to the input layer.Errors at the output layer:Let's begin by computingδ
Lj;i2f1;…;cg, errors of neurons in the last layerL. By using the chain rule, we haveδ
Lj¼∂Cx
∂aLj∂aLj
∂zLj
¼/C16y j/C0aLj/C17φ0/C16z
Lj/C17 (10)Because the loss function depends on z
Lj, throughaLjonly. Errors at any hidden layer:errorδ
ljof any hidden neuronjat any layerl. The weighted inputz
ljof a hidden layerlis linked to the loss function through all weighted inputsðz
lþ1kÞkto the next layer.δ
lj¼X
k∂Cx
∂zlþ1k∂zlþ1k
∂zlj
¼X
kδlþ1k∂zlþ1k
∂zlj:Using the chain rule, we have
∂zlþ1k
∂zlj¼∂zlþ1k
∂alj∂alþ1k
∂zlj
¼wlþ1jkφ0/C16z
lj/C17:δ
lj¼φ0/C16z
lj/C17X
kwlþ1jkδlþ1k : (11)The above expression tells us that error functions at any hidden layer aregiven by the weighted sum of the error functions at the next layer. Whichmeans that errors are computed backwards, hence the name back-propagation. By writing partial derivatives,∂Cx
∂wlijwith respect toδlj, the gradient descent updating rule is rewrittenw
lij→wlij/C0α
mX
x2X mXnh
j¼1δljðxÞal/C01iðxÞ:(12)Wheren
his the number of neurons in thelth layer. Typically, in DL al- gorithms, the SGD algorithm is combined with backpropagation, wherewe have to compute the gradient of a loss function, to be minimized for alarge set of data. The implementation of this algorithm is done in a fewsteps:1. Provide a set of training examples2. For each example x:givea
1ðxÞ, and perform the following steps:●Do a Feedforward: Forl¼2;3;…;Lcomputez
lðxÞ¼Wlal/C01ðxÞþ b
lwithalðxÞ¼φðzlðxÞÞ.●Output error functionδ
L:ComputeδLðxÞ¼ rC x/C12φ0ðzLðxÞÞ. ●Backpropagate the error: Forl¼L/C01;L/C02;…;2 compute δ
lðxÞ¼ ð ðWlþ1ÞTδlþ1ðxÞÞ /C12φ0ðzlðxÞÞ 3. Gradient descent:Forl¼L;L/C01;…;2 update the weights according to the formulaW
l→Wl/C0α
mP
x2X mδlðxÞðal/C01ðxÞÞT. We can also show with small computations that the update formula for the vector b
l
containing the bias terms in any llayer is written:bl→bl/C0
α
mP
x2X mδlðxÞTo implement stochastic gradient descent in practice, an external loopgenerating mini training example runs, and an external loop runningthrough several training epochs are required. However, these wereomitted for simplicity.4. Regularization techniquesOne of the most serious problems in training
MLmodels, particularly for
NN, is overﬁtting. This problem occurs when a training model is toocomplex.4.1. L
1and L2regularization techniquesA widely used technique to reduce a model complexity is to add aregularization term [26] to the loss function C. The new model lossfunctionC
λis deﬁned as follows:C
λ¼CþλΩðWÞThese, update the general cost function by adding another term known asthe regularization term, whereΩisL
1orL2norm andwis the NNweight parameters.4.1.1. L
2regularizationTheL
2regularization term, commonly known as weight decay. The idea of this technique also known as ridge regressionorTikhonov regularization [78], is to add aL
2term to the function to be minimized, inthis caseΩðWÞ¼
12kWk22. This added term inL2norm imposes the weights to live in a sphere of radius inversely proportional to the regu-larization parameter [ [26], p. 249]λ. In this context, the updating rule, using gradient descent strategy becomesw
lij→/C161/C0αλn/C17w lij/C0α
nXni¼1∂Cxi
∂wlij: (13)this means that, after each iteration, the weights are multiplied by afactor slightly smaller 1. It tends to force the model to prefer smallweights.H. Soumare et al. Array 11 (2021) 100068
44.1.2. L1regularizationL
1regularization modiﬁes the loss function by adding aL1term, i.e. ΩðWÞ¼P
w2Wjwj. The idea behind this technique is to regularize theloss function by removing the irrelevant features from the trainingmodel. In this situation, the updating rule is writtenw
lij→wlij/C0αλnsgn/C16w lij/C17/C0α
nXni¼1∂Cxi
∂wlij:(14)Wheresgnðw
lijÞis the sign ofwlij. Both types of regularization try topenalize the big weights when it's necessary by shrinking them after eachupdating step, but the way of shrinkage is different [ 26]. WhenL
2reg- ularization is used, the weights are shrunk by an amount proportional tow
lij, whereas inL1regularization, the weights are shrunk by constantquantity toward to zero. As shown in Fig. 5(graph on the left), in a two dimensional space, theL
1norm deﬁnes a parameter space bounded by aparallelogram at the origin. In this case, the loss function is likely to hitthe vertices of the parallelogram rather than its edges. L
1regularization removes some of the parameters, thus L
1technique can be used as a feature selection technique. On the other hand, the L
2regularization deﬁnes a circle whose radius size is inversely proportional to the regu-larization parameter (seeFig. 6).4.2. Dropout techniqueIn training a
NN, Dropout technique regularizes learning by droppingout some hidden units with certain probability. This is equivalent tomodifying [72] the
NNby setting some hidden activation functions tozero. Using Dropout, we can formally deﬁne the
NNas follows:~a
lj¼δljalj; (15)At each neuronj, in a hidden layerl, the output activationa
ljis multiplied by a sampled variableδ
lj, to produce thinned output activations ~alj. These thinned functions are then used as inputs to the next layer and the sameprocess is applied at each layer. This application is equivalent to samplinga sub Neural Networks from a larger network. Where δ
ljis a Bernoulli random variable (δ
lj↪Bernoulli(pl)) of parameterpl, i.e. a neuron in the lth layer is kept with a probability of p
land removed with a probability
1/C0p
l.Srivastava et al. [72] suggested that, applying Dropout to a
NNwithn units can be seen as sampling 2
nsub Networks with weight sharing. In thetest phase, as it is not always practical to take the mean of 2
nmodels, an approximate averaging method is used. The idea is to approximate theexponentially many Networks by a single
NNwithout Dropout. To correct the fact that training outgoing weights of a layer are obtained undercondition that neurons were retained with a probability p, the weights are simply multiplied byp. This approximation has been proved for lo-gistic and linear regression models [ 72,79]. But, for Deep Neural Net- works
DNNs, there is an unknown gap between the expected output ofexponential sub Networks and the output of a single deterministic model.Ma et al. [46] showed that under some assumptions on input data, thegap is controlled and it can be used to regularize the single
NN.In this work, without any assumption of input data, we quantifyexplicitly the gap and then show how it related to L2regularization.4.2.1. Dropout application to linear networksTo see more clearly the relationship between L
2-regularization, we start by studying the problem in a very simple case, where all activationfunctions in the model are linear. Consider a
NN, where all units are linear (i.e.a
l¼al/C01Wl, wherealandWlare the output vectors and weight matrix of layerl2f1;…;Lgrespectively). The Dropout
NNloss function is
1nX
x2X/C13/C13/C13yðxÞ/C0a
L/C01ðxÞ~WL/C13/C13/C1322þ1/C0pL/C01
pL/C01/C13/C13/C13Σ
L/C01~WL/C13/C13/C1322:(16)yðxÞis the output vector given an input vector x.Σ
L/C01¼/C18
1ndiagðaL/C01ðXÞðaL/C01ðXÞÞTðXÞÞ/C1912
,~WL¼pL/C01WL. Given a matrixA,w e denote bydiagðAÞ, a diagonal matrix with the same size and diagonalelements asA.At each layerl,w ed eﬁne a matrixa
lðXÞwhose columns correspond to the values taken by the vector of the activation function a
lacross input data:a
lðXÞ¼ ðaliðxjÞÞ,1/C20i/C20m,1/C20j/C20n, wherealiðxjÞis theith output neuron in(l)th layer of thejth input andmis the number of neurons in the layer.Proof. Training a standard nn without dropping neurons is done byminimizing the following loss function:
1nX
x2X/C13/C13yðxÞ/C0aL/C01ðxÞWL/C13/C1322(17)Dropout modiﬁes the training process and the loss function in (17) becomes
1nX
x2XEδL/C01/C13/C13yðxÞ/C0/C0δL/C01ðxÞ/C12aL/C01ðxÞ/C1WL/C13/C1322:(18)Whereδ
L/C01is a random vector of the layer L/C01 withδL/C01i↪ Bernoulliðp
L/C01Þand/C12denotes the Hadamard product. Using the formulaEðX
2Þ¼ðEðXÞÞ2þVarðXÞfor a random variable X, we show that (18)is equal to
1nX
x2X/C13/C13yðxÞ/C0E
δL/C01/C0/C0δL/C01ðxÞ/C12aL/C01ðxÞ/C1WL/C1/C13/C1322þ1nX
x2XVar/C0/C0δL/C01ðxÞ/C12aL/C01ðxÞ/C1WL¼
1nX
x2X/C13/C13yðxÞ/C0pL/C01aL/C01ðxÞWL/C13/C1322þ1nX
x2XWLVar/C0δL/C01ðxÞ/C12aL/C01ðxÞ/C1ðWLÞT:
Fig. 5.Two dimensional graphical interpretation of L1andL2regularizations.H. Soumare et al. Array 11 (2021) 100068
5AsVarðδL/C01ðxÞ/C12aL/C01ðxÞÞ ¼/C181/C0pL/C01
pL/C01/C19a
L/C01ðxÞaL/C01ðxÞT, we obtain the desired result. Under the assumption that input layers follow aGaussian distribution with standard deviation
σ, Dropout is equivalent in expectation toL
2-regularization. The regularization parameter λis a function of
1/C0pL/C01
pL/C01σ2which increases (resp. decreases) with the variance ofinput layers
σ2(resp. withpL/C01). Thus, Dropout regularization consists ofdetecting the inputs with more variance and shrink their weights.4.2.2. Dropout application to non linear networksHere, we try to generalize the relationship between Dropout andL
2-regularization to Networks with nonlinear units. Consider a NNwith a non linear activation function, i.e., a
l¼φðal/C01WLÞ. Dropout training expected loss function is given by
WhereΣ
L/C01x¼/C16aL/C01ðxÞðaL/C01ðxÞÞT/C1712
and~WL¼pL/C01WL. Proof. We know that a non linear Dropout Network training loss isdeﬁned as
1nX
x2XEδL/C01/C13/C13yðxÞ/C0φ/C0/C0δL/C01ðxÞ/C12aL/C01ðxÞ/C1WL/C1/C13/C1322:(20)Using triangle inequality, (20) is bounded by
Now, by applying a second order Taylor expansion of φaroundE
δL/C01/C0/C0aL/C01/C12δL/C01ðxÞWLÞ/C1¼pL/C01aL/C01ðxÞWLand by posingZ¼ðaL/C01ðxÞ/C12 δ
L/C01ðxÞÞWL/C0pL/C01aL/C01ðxÞWL,w eh a v eφððaL/C01ðxÞ/C12δL/C01ðxÞÞWLÞ¼ φðp
L/C01aL/C01ðxÞWLÞþφ0ðpL/C01aL/C01ðxÞWLÞZþ12φ00ðpL/C01aL/C01ðxÞWLÞZZT. ThenE
δL/C01ðxÞ/C0φððaL/C01ðxÞ/C12δL/C01ðxÞÞWLÞ/C1/C0φðpL/C01aL/C01ðxÞWLÞ¼12φ00
ðpL/C01aL/C01ðxÞWLÞVarðZZTÞ. BecauseZis centered i.e.,EδL/C01ðxÞðZÞ¼0. Thus, an upper bound of(20)is given by
1nX
x2X/C13/C13/C13yðxÞ/C0φ/C16a
L/C01ðxÞ~WL/C17/C13/C13/C1322þ12n/C181/C0pL/C01
pL/C01/C19X
x2Xkφ00/C16a
L/C01ðxÞ~WL/C17Σ
x~WLk22:Here again, Dropout can be seen as regularizer, where the regularizerrepresents the gap betweenE
δL/C01/C0φððaL/C01/C12δL/C01ÞWLÞ/C1the expected output of exponential thinned Networks produced by applying Dropoutandφðp
L/C01aL/C01WLÞ, the output of a single deterministic Network, inwhich the weights are scaled bypL/C01to compensate the fact that they are learned under conditions in which 1 /C0p
L/C01of hidden units where dropped out. In this case, Dropout training model can be seen as anL
2-regularization where, the regularizer λdepends on: Dropout rate; the variance of each input and output layer.4.2.3. Dropout with others regularization techniquesDropout is known to improve training model performance when it iscombined with other regularization techniques. Batch normalization,introduced by Ref. [35], is a regularization technique used to speed upthe training and improve performance of Deep
NNs. In the training of a
DNN, the distribution of each layer's inputs change, as the parameters of alllayers that come before it, variate. This can slow down by requiring smalllearning rates and careful parameter initialization. Given a batch ofsample used to update parameters, batch normalization normalizes theinputs of each layer by recentering and rescaling (subtracting the meanand dividing by the batch standard deviation). Thus, batch normalizationprevents layers inputs to have large standard deviations [ 35]. show experimentally that batch normalization with large learning rate, speedup signiﬁcantly training as it can eliminate the need for Dropout. In fact,as discussed in Section ??, Dropout look for layer's inputs with morevaritions and shrink their weights, this function of shrinking is thenlargely reduced by batch normalization application. Combining dropout
with batch normalization [25,35,43] can improve
DNNs prediction accu- racy. Dropout regularization is known to give a signi ﬁcant improvement when it is combined with others regularization methods such asmax-norm [72] and weight normalization [67]. Rather than constraining whole weight matrix of each layer as in L
2regularization, these constrain each column of the weight matrix to prevent separately any hiddenneuron from having very large weights. Max-norm regularization con-sists to constrain the incoming weight vector of each hidden neuron tolive in a ball of radiusc, wherecis a hyper-parameter. Weight normal-
ization constrains incoming weight vectors to have unit norm.5. Materials and methodsAll models in this work are constructed using Keras and Tensor ﬂow open source libraries [38]. Logistic regression is built using a Feedfor-ward classiﬁcation network without hidden layers, this model can beextended later to a more complex classiﬁcation Network, depending on
Fig. 6.Dropout.
1nX
x2X/C13/C13/C13/C13yðxÞ/C0φ/C18/C16~W
L/C17T
aL/C01ðxÞ/C19/C13/C13/C13/C1322þ12n/C181/C0pp/C19X
x2Xkφ00/C16a
L/C01ðxÞ~WL/C17Σ
L/C01x~WLk22: (19)
1nX
x2X/C13/C13yðxÞ/C0φ/C0pL/C01aL/C01ðxÞWL/C1/C13/C1322þ1nX
x2X/C13/C13E
δL/C01ðxÞ/C0φ/C0/C0aL/C01ðxÞ/C12δL/C01ðxÞ/C1WL/C1/C1/C0φ/C0pL/C01aL/C01ðxÞWL/C1/C13/C1322:
Table 1Different datasets used in this study.
Dataset #of samples Class 1 Class 2 # of SNP S
Breast-Kidney 604 344 260 10937Colon-Kidney 546 286 260 10937Breast-Colon 630 344 286 10937Colon-Prostate 286 69 355 10937H. Soumare et al. Array 11 (2021) 100068
6the problem complexity. Stochastic gradient descent is adopted in allexperiments as an optimization strategy. Two types of datasets areincluded in our experiments, Expression Project for Oncology (expO)cancer datasets and 1000 Genomes Project ethnicity datasets respectivelyused for training logistic regression and FFN models.Individuals in selected datasets are humans that are represented bythe list of their SNP
S. Each SNPis represented by its genotype (i.e. geneticinformation) at a speciﬁc locus. In a diploid organism at each locus, thereare two copies of alleles, one comes from the father and other from themother. Consequently, a genotype takes one of three values for a diploidorganism: 0 (homozygous reference), 1 (heterozygous) and 2 (homozy-gous alternate). The homozygous reference refers to the base that isfound in the reference genome, an homozygous alternate refers to anybase, other than the reference, that is found at that locus and genotype issaid heterozygous at a given position, when the two alleles are different.The input of a model is a matrix X of size n/C2d, wherenis the number of individuals included in the study and dcorrespond to the number of features (SNP
S). The outputytakes discrete value(s) between 0 and 1.5.1. Expression Project for Oncology(expO) cancer datasetsThe different cancer samples included in this study (see Table 1), are downloaded from Ref. [11]. The original datasets can be obtained fromthe Expression Project for Oncology (expO) that was deposited at GeneExpression Omnibus (GEO) repository [ 7], with accession number GSE2109. The objective of expO is to obtain and perform gene expressionanalysis on cancer tissue samples and assemble the patient's long termclinical results.5.2. 1000 Genomes Project datasetThe 1000 Genomes Project [16] took advantage of developments inNext-generation sequencing (NGS), which allows to sequence
DNAand RNA
much more quickly and cheaply. It's the ﬁrst project to sequence the genomes of a large number of people in populations from different re-gions and countries. In this study,n¼3450 is the number of individuals sampled worldwide from 26 populations and d¼315345 is the number of SNP
S. The desired output of the model is a vector Y2Rc, whose components correspond to the 26 classes of populations (i.e. c¼26). The model consists of an input layer, an output layer and two hidden layers ofequal size. Given the input matrixX, the model output is a vectora
32Rc. A reluaction function is used in the two hidden layers followed by asotmax layer to perform ancestry prediction.6. ExperimentsIn this section, we present the effects of regularization techniques ontraining models for different datasets (see Table 1).6.1. Cancer dataset classiﬁcation using logistic regressionUn-regularized logistic regression results are reported in Table 2, despite its simplicity, logistic regression model gives good classi ﬁcation accuracy on these cancer datasets. To improve prediction capacities ofthe present model, a penalty term is added and obtained results arepresented inTable 3andTable 4forL
1andL2regularization added term, respectively. We can observe from theses table that penalization with theappropriate regularization parameter improves the classi ﬁcation accu- racy. For example, whenL
1regularization is used with regularizer λ¼ 10
/C03, the classiﬁcation accuracy on Breast-Kidney dataset goes from96:53 to99:01. Similarly, whenL
2penalization is applied(λ¼10/C03), the prediction accuracy on Breast-Colon dataset increases from 94.44 inunregularized case to99:44.6.2. Ancestry prediction using a multilayer perceptron (MLP)In this subsection,
FNNis used on 1000 Genome Project ethnicitydataset to predict individuals ancestries. As in the preceding subsection,we start with a simple logistic regression model, which gives a low pre-diction accuracy of54.64%. To achieve better prediction results, a
MLP
with equal hidden units is constructed and the obtained results byvarying the model complexity are reported in Table 5. As expected, the model prediction accuracy starts by increasing with its complexity untilsome level (two hidden layers with 100 units for each one), then it startsto drop. Because beyond this stage, the training model is considered toocomplex and it overﬁts.6.2.1. Classiﬁcation with autoencoderWe start by using a classiﬁcation Network with one hidden layer of 50units with reconstruction path. This gives an accuracy of 84 :85%. When another hidden layer of 50 neurons is added between hidden the repre-sentationa
hand the output layeraL, as described inFig. 4(where MPL¼½50/C138). This last gives an accuracy of 85 :36%. Training the classiﬁ- cation Network with a reconstruction path is dif ﬁcult due to the high dimensionality of the input data.Table 2Unregularized logistic reg.
Dataset Accuracy (in %)Breast-Kidney 96.53Colon-Kidney 97.82Breast-Colon 94.13Colon-Prostate 97.46
Table 3Logistic reg. withL
1norm.
Dataset Regularization L1Accuracy(in %)Breast-Kidney λ¼10
/C02 97.36 λ¼10
/C03 99.01 Colon-Kidney λ¼10
/C02 95.82 λ¼10
/C03 97.45 Breast-Colon λ¼10
/C02 94.44 10
/C03 93.17 Colon-Prostate λ¼10
/C02 98.59 λ¼10
/C03 98.03
Table 4Logistic reg. withL
2norm.
Dataset Regularization L2Accuracy(in %)Breast-Kidney λ¼10
/C02 98.18 λ¼10
/C03 98.02 Colon-Kidney λ¼10
/C02 98.18 λ¼10
/C03 98.91 Breast-Colon λ¼10
/C02 92.86 λ¼10
/C03 99.44 Colon-Prostate λ¼10
/C02 97.18 λ¼10
/C03 96.62
Table 5
MLPaccuracy vs its size.
# of units by hidden layer Accuracy(in %)[50] 81.33[50-50] 81.68[100] 90.68[100/C0100] 92.70[100-100-100] 90.49[500-500-500] 90.46H. Soumare et al. Array 11 (2021) 100068
76.2.2. Classiﬁcation with regularizationWhen Dropout is used alone, the choice of its rate pis very important as inTable 6, we have to be more careful about it. Combining Dropoutwith batch normalization improves the performance training model andmakes the choice ofpless important.L
1andL2regularization, give good prediction accuracy and outperform Dropout as observed in Table 7. However, when Dropout is combined with batch normalization, max-norm and unit-norm outperforms the traditional regularization tech-niques. We have obtained our best prediction accuracy 94.43%, when Dropout is combined with unit norm constraint.7. DiscussionRegularized Logistic Regression has achieved good results comparedto previous machine learning approaches tested on the same cancersamples [70,73]. For instance, Stiglic et al. [73] combined different feature selection methods such as Vector Machines Recursive FeatureElimination (SVM-RFE) and ReliefF followed by SVMork-nearest neighbors. To the best of our knowledge, the best results on these data-sets were reported in Ref. [70], where the authors used Stacked SparseAutoencoders (SSAE) to select most relevant features followed by aclassiﬁcation Neural Network to categorize the samples. Despite itssimplicity, the proposed approach outperforms SSAE on datasets such asBreast-Kidney and Breast-Colon (see Table 8). In Stacked Sparse Autoencoders [36,45] many Autoencoder layers are stacked together toform an unsupervised learning algorithm, where the encoder layercomputed by an Autoencoder will be used as the input to anotherAutoencoder layer. In practice, a logistic regression model may be betterthan a
NNfor relatively small data sets and simple classi ﬁcation tasks, where the classes are more or less linearly separable. Indeed, the latterare more difﬁcult to train, require more training samples and are moreprone to overﬁtting than logistic regression.Logistic regression application to anscentry prediction dataset lead topoor prediction accuracy, which is due to the large dimension of inputfeatures and high nonlinear correlation between them, and to the geneticsimilarity between some ethnic of groups of populations.Training a
NNwith an Autoencoder reconstruction path improved theresults. However, training an Autoencoder in conjunction with the clas-siﬁcation Network makes the high dimensional optimization problemmore difﬁcult to solve than simply training the classi ﬁcation Network, yielding in a higher classiﬁcation error. To improve the results, regula-rization techniques are used. One can notice that traditional regulariza-tion technique's application has more improved the prediction accuracyof the model compared to Dropout. This could be attributed to the factthat Dropout is less effective thanL1andL2regularization techniques [58] when the training model is not complex (as for our model).Combining Dropout with techniques such as Batch normation, Unit normconstraint or Max norm enabled the training model to achieve its bestaccuracy. The obtained results are compared to the results in Table 9 obtained by Ref. [64] on the same dataset. In Ref. [64], the authors proposed auxiliary
NNs to predict the parameters of theﬁrst hidden layer of the classiﬁcation
NNs and different features embedding techniquessuch as Random projection(RP), Per class histogram, and SNPtoVec havealso been proposed. Achieving such prediction accuracy obtained withSNP data, these regularization techniques will allow us to face morecomplicated problems in many domains such as preventive medicine.8. ConclusionIn this work, we have explained stochastic gradient descent optimi-zation technique with back-propagation in training
DLalgorithms. To prevent overﬁtting problem, regularization techniques are studied and,theoretical relationship between Dropout and L
2regularization is established. Experimental results have shown that Dropout, when it iscombined with techniques such as batch normalization, max-norm orunit-norm gives better performance than L
1andL2regularization techniques.For future work, we expect to further study these regularizationtechniques in
DNNand use them to analyze gene expression pro ﬁle data with the aim of predicting rare diseases.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsThis project was partly funded by H3ABioNet, which is supported bythe National Institutes of Health Common Fund under grant numberU41HG006941.AppendixIn this section, we report results obtained by Singh et al. [ 70](Table 8) and those obtained by Romero et al. [ 64](Table 9).Table 6Prediction accuracy for Dropout, batch normalization and dropout combination with batch normalization.
# of uni. by hid. layer Drop.(p ¼0.2) Drop.(p¼0.5) Bat.norm Drop.(p ¼0.2)þBat. norm Drop.(p ¼0.5)þBat. norm(accuracy in%) (accuracy in%) (accuracy in%) (accuracy in%) (accuracy in%)[50] 90.32 87.54 90.58 91.48 92.61[50-50] 89.94 40.96 91.01 92.58 92.93[100] 88.81 92.26 90.75 91.65 93.01[100/C0100] 90.32 64.12 89.19 92.46 93.00
Table 7Prediction accuracy forL
1,L2, Dropout regularization techniques and Dropout combination with others regularization techniques.
# of uni. by hid. layer L1(λ¼10/C04) reg.L2(λ¼10/C03) reg.Drop.(p¼0.5)þBat. norm Drop.(p¼0.5)þMa.norm Drop.(p¼0.2)þUn.norm(accuracy in%) (accuracy in%) (accuracy in%) (accuracy in%) (accuracy in%)[50] 92.13 92.61 92.61 92.35 93.25[50-50] 91.77 90.75 92.93 83.68 90.32[100] 92.70 92.70 93.01 93.83 94.43 [100/C0100] 92.29 93.39 93.00 90.17 91.94[100-100-100] 90.87 91.01 92.42 89.68 92.19H. Soumare et al. Array 11 (2021) 100068
8References
[1] Akhtar N, Mian A. Threat of adversarial attacks on deep learning in computervision: a survey. IEEE Access 2018;6:14410 –30.https://doi.org/10.1109/ ACCESS.2018.2807385.[2] Almagro Armenteros JJ, S ønderby CK, Sønderby SK, Nielsen H, Winther O. Deeploc: prediction of protein subcellular localization using deep learning. Bioinformatics2017;33:3387–95.https://doi.org/10.1093/bioinformatics/btx431 . [3] Amari S. Backpropagation and stochastic gradient descent method.Neurocomputing 1993;5:185–96.https://doi.org/10.1016/0925-2312(93)90006- O.[4] Amodei D, Ananthanarayanan S, Anubhai R, Bai J, Battenberg E, Case C, Casper J,Catanzaro B, Cheng Q, Chen G, et al. Deep speech 2: end-to-end speech recognitionin English and Mandarin. In: Inter. conf. on ML; 2016. p. 173 –82.https://arxiv .org/abs/1512.02595.[5] Bacchi S, Oakden-Rayner L, Zerner T, Kleinig T, Patel S, Jannes J. Deep learningnatural language processing successfully predicts the cerebrovascular cause oftransient ischemic attack-like presentations. Stroke 2019;50:758 –60.https:// doi.org/10.1161/STROKEAHA.118.024124 . [6] Baldi P, Sadowski PJ. Understanding dropout. In: NIPS; 2013. p. 2814 –22.https:// doi.org/10.1016/j.artint.2014.02.004 . [7]Barrett T, Edgar R. [19] gene expression omnibus: microarray data storage,submission, retrieval, and analysis. Meths. in enzy. 2006;411:352 –69. doi: S0076687906110198.[8] Bilen M, Is¸ik AH, Yi/C21git T. A hybrid artiﬁcial neural network-genetic algorithm approach for classiﬁcation of microarray data. In: 2015 23nd SPCA conf. (SIU),IEEE; 2015. p. 339–42.https://doi.org/10.1109/SIU.2015.7129828 . [9]Bottou L. Stochastic gradient learning in neural networks. Proceedings of Neuro-Nımes 1991;91:12.[10]Bottou L. Stochastic gradient descent tricks. In: Neural networks: tricks of the trade.Springer; 2012. p. 421–36. [11] Cancer D. Openml; 2015. https://www.openml.org/search?type ¼data. [Accessed 7 September 2020].[12]Chatterjee S, Hadi AS. Sensitivity analysis in linear regression, vol. 327. John Wiley&Sons; 2009.[13] Chen Y, Li Y, Narayan R, Subramanian A, Xie X. Gene expression inference withdeep learning. Bioinformatics 2016;32:1832 –9.https://doi.org/10.1093/ bioinformatics/btw074. [14] Collins FS, Brooks LD, Chakravarti A. A dna polymorphism discovery resource forresearch on human genetic variation. Geno. research 1998;8:1229 –
31.https:// doi.org/10.1101/gr.8.12.1229 . [15] Collobert R, Weston J. A uni ﬁed architecture for natural language processing: deep neural networks with multitask learning. In: Proceed. of the 25th inter. conf. on ML;2008. p. 160–7.https://doi.org/10.1145/1390156.1390177 . [16] Consortium GP, et al. A map of human genome variation from population-scalesequencing. Nature 2010;467:1061. https://doi.org/10.1038/nature09534 . [17] Curry HB. The method of steepest descent for non-linear minimization problems.Quart. of App. Maths. 1944;2:258 –61.https://www.ams.org/journals/qam/1944-0 2-03/S0033-569X-1944-10667-3/S0033-569X-1944-10667-3.pdf . [18]Denoeux T. Logistic regression, neural networks and dempster –shafer theory: a new perspective. Knowl Base Syst 2019;176:54 –67.[19]Dreiseitl S, Ohno-Machado L. Logistic regression and arti ﬁcial neural network classiﬁcation models: a methodology review. J Biomed Inf 2002;35:352 –9. [20] Fakoor R, Ladhak F, Nazi Z, Huber M. Using deep learning to enhance cancerdiagnosis and classiﬁcation. In: Proceed. of the inter. conf. on ML. New York, USA:ACM; 2013.https://doi.org/10.1109/ICSCAN.2018.8541142 . [21] Fort G, Lambert-Lacroix S. Classi ﬁcation using partial least squares with penalized logistic regression. Bioinformatics 2005;21:1104 –11.https://doi.org/10.1093/ bioinformatics/bti114.[22]Fu X, Wei Y, Xu F, Wang T, Lu Y, Li J, Huang JZ. Semi-supervised aspect-levelsentiment classiﬁcation model based on variational autoencoder. Knowl Base Syst2019;171:81–92.[23]Gal Y, Ghahramani Z. Dropout as a bayesian approximation: representing modeluncertainty in deep learning. In: International conference on machine learning.PMLR; 2016. p. 1050–9. [24] Ganesan N, Venkatesh K, Rama MA, Palani AM. Application of neural networks indiagnosing cancer disease using demographic data. Int J Chem Appl 2010;1:76 –85. https://doi.org/10.5120/476-783 . [25] Garbin C, Zhu X, Marques O. Dropout vs. batch normalization: an empirical study oftheir impact to deep learning. Multimed Tool Appl 2020:1 –39.https://doi.org/ 10.1007/s11042-019-08453-9 . [26] Goodfellow I, Bengio Y, Courville A, Bengio Y. Deep learning, vol. 1. MIT pressCambridge; 2016.https://doi.org/10.1007/s10710-017-9314-z . [27] Group, I.S.M.W., et al.. A map of human genome sequence variation containing 1.42million single nucleotide polymorphisms. Nature 2001;409:928. https://doi.org/ 10.1038/35057149.[28] Guo J, He H, He T, Lausen L, Li M, Lin H, Shi X, Wang C, Xie J, Zha S, et al. Gluoncvand gluonnlp: deep learning in computer vision and natural language processing.J Mach Learn Res 2020;21:1–7. 1907.04433. [29] Hannun A, Case C, Casper J, Catanzaro B, Diamos G, Elsen E, Prenger R, Satheesh S,Sengupta S, Coates A, et al. Deep speech: scaling up end-to-end speech recognition.arXiv preprint arXiv, 1412.5567; 2014. https://arxiv.org/abs/1412.5567 . [30] Hecht-Nielsen R. Theory of the backpropagation neural network. In: N. netw. forpercep. Elsevier; 1992. p. 65–93.https://doi.org/10.1016/B978-0-12-741252- 8.50010-8.[31]Helmbold DP, Long PM. Surprising properties of dropout in deep networks. In:Conference on learning theory. PMLR; 2017. p. 1123 –46. [32] Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neuralnetworks. science 2006;313:504 –7.https://doi.org/10.1126/science.1127647 . [33] Hinton GE, Srivastava N, Krizhevsky A, Sutskever I, Salakhutdinov RR. Improvingneural networks by preventing co-adaptation of feature detectors. 2012. arXivpreprint arXiv, 1207.0580.http://arxiv.org/abs/1207.0580 . [34]Huang K, Hussain A, Wang QF, Zhang R. Deep learning: fundamentals, theory andapplications, vol. 2. Springer; 2019 . [35] Ioffe S, Szegedy C. Batch normalization: accelerating deep network training byreducing internal covariate shift. arXiv preprint arXiv, 1502.03167; 2015.[36]Katuwal R, Suganthan PN. Stacked autoencoder based deep random vectorfunctional link neural network for classi ﬁcation. Appl Soft Comput 2019;85: 105854.[37] Kendall A, Gal Y. What uncertainties do we need in bayesian deep learning forcomputer vision?. In: NIPS; 2017. p. 5574 –84.https://arxiv.org/pdf/1703.04977. pdf.[38] Keras T. Keras. 2015.https://www.tensorﬂow.org/guide/keras/overview . [Accessed 7 September 2020].[39]Kleinbaum DG, Dietz K, Gail M, Klein M, Klein M. Logistic regression. Springer;2002.[40] Kone/C20cnỳJ, Richt/C19arik P. Semi-stochastic gradient descent methods. 2013. arXivpreprint arXiv, 1312.1666.[41]Le L, Patterson A, White M. Supervised autoencoders: improving generalizationperformance with unsupervised regularizers. Adv Neural Inf Process Syst 2018;31:107–17.[42]Li F, Zurada JM, Liu Y, Wu W. Input layer regularization of multilayer feedforwardneural networks. IEEE Access 2017;5:10979 –85. [43] Li X, Chen S, Hu X, Yang J. Understanding the disharmony between dropout andbatch normalization by variance shift. In: Proceed. Of the IEEE conf. On CVPR;2019. p. 2682–90.https://doi.org/10.1109/CVPR.2019.00279 . [44] Liao JG, Chin KV. Logistic regression for disease classi ﬁcation using microarray data: model selection in a large p and small n case. Bioinformatics 2007;23:1945–51.https://doi.org/10.1093/bioinformatics/btm287 . [45]Liu G, Bao H, Han B. A stacked autoencoder-based deep neural network forachieving gearbox fault diagnosis. Mathematical Problems in Engineering 2018;2018.[46] Ma X, Gao Y, Hu Z, Yu Y, Deng Z, Hovy E. Dropout with expectation-linearregularization. 2016. arXiv preprint arXiv, 1609.08017. https://arxiv.org/abs/1
609.08017.[47] Manning CD, Surdeanu M, Bauer J, Finkel JR, Bethard S, McClosky D. The stanfordcorenlp natural language processing toolkit. In: Proceed. of 52nd ann. meet. of ACL:system demonstrations; 2014. p. 55 –60.https://doi.org/10.3115/v1/p14-5010 . [48]Maurya S, Singh V, Dixit S, Verma NK, Salour A, Liu J. Fusion of low-level featureswith stacked autoencoder for condition based monitoring of machines. In: 2018IEEE international conference on prognostics and health management (ICPHM),IEEE; 2018. p. 1–8.[49] Mianjy P, Arora R. On dropout and nuclear norm regularization. In: Internationalconference on machine learning. PMLR; 2019. p. 4575 –84.https://arxiv.org/abs/1 905.11887.[50] Min S, Lee B, Yoon S. Deep learning in bioinformatics. Brief. in bioinfo. 2017;18:851–69.https://doi.org/10.1093/bib/bbw068 .Table 8Logistic reg.vs SSAE.
Dataset Stacked Sparse Autoencoders Logistic regressionBreast-Kidney 98.4 99.01Colon-Kidney 99.5 98.91Breast-Colon 97.3 99.44Colon-Prostate 99.7 98.59
Table 9Reported results in Ref. [64].
Model&Embedding Mean Misclassif. Error. (%) # of freeparam.Basic 8 :31/C61:83 31.5 M Raw end2end 8 :88/C61:41 21.27K Random Projection 9 :03/C61:20 10.1K SNP2Vec 7 :60/C61:28 10.1K Per class histograms 7 :88/C61:40 7.9K Basic with reconstruction 7 :76/C61:38 63 M Raw end2end with reconstruction 8 :28/C61:92 227.3K Random Projection withreconstruction 8:03/C61:0:3 20.2KSNP2Vec with reconstruction 7 :88/C60:72 20.2K Per class histograms withreconstruction 7:44/C60:45 15.8KH. Soumare et al. Array 11 (2021) 100068
9[51]Montgomery DC, Peck EA, Vining GG. Introduction to linear regression analysis.John Wiley&Sons; 2021. [52] Ng A, et al. Sparse autoencoder. CS294A Lect. notes 72. 2011. p. 1 –19.http ://ailab.chonbuk.ac.kr/seminar_board/pds1_ ﬁles/sparseAutoencoder.pdf. [53] Owen AB. A robust hybrid of lasso and ridge regression. Contemp Math 2007;443:59–72.https://doi.org/10.1090/conm/443/08555 . [54]Ozanich E, Gerstoft P, Niu H. A feedforward neural network for direction-of-arrivalestimation. J Acoust Soc Am 2020;147:2035 –48. [55] Pal A, Lane C, Vidal R, Haeffele BD. On the regularization properties of structureddropout. In: Proceedings of the IEEE/CVF conference on computer vision andpattern recognition; 2020. p. 7671 –9.https://arxiv.org/abs/1910.14186 . [56] Patel P, Thakkar A. The upsurge of deep learning for computer vision applications.Int J Electr Comput Eng 2020;10:538. https://doi.org/10.11591/ ijece.v10i1.pp538-548.[57]Pei J, Wang W, Osman MK, Gan X. Multiparameter optimization for the nonlinearperformance improvement of centrifugal pumps using a multilayer neural network.J Mech Sci Technol 2019;33:2681 –91. [58]Phaisangittisagul E. An analysis of the regularization between l2 and dropout insingle hidden layer neural network. In: 2016 7th international conference onintelligent systems, modelling and simulation (ISMS). IEEE; 2016. p. 174 –9. [59] Project, G., . 1000 Genome project datasets.[60] project OE. Expression project for oncology. 2005. https://www.ncbi.nlm.nih .gov/geo/query/acc.cgi?acc¼GSE2109. [Accessed 7 September 2020]. [61]Qi GJ, Zhang L, Lin F, Wang X. Learning generalized transformation equivariantrepresentations via autoencoding transformations. IEEE Transactions on PatternAnalysis and Machine Intelligence; 2020 . [62] Ravì D, Wong C, Deligianni F, Berthelot M, Andreu-Perez J, Lo B, Yang G. Deeplearning for health informatics. IEEE JBHI 2016;21:4 –21.https://doi.org/10.1109/ JBHI.2016.2636665.[63]Reed R, MarksII RJ. Neural smithing: supervised learning in feedforward arti ﬁcial neural networks. Mit Press; 1999 . [64] Romero A, Carrier PL, Erraqabi A, Sylvain T, Auvolat A, Dejoie E, Legault MA,Dub/C19e MP, Hussin JG, Bengio Y. Diet networks: thin parameters for fat genomics.2016. arXiv preprint arXiv, 1611.09340.[65] Rong D, Xie L, Ying Y. Computer vision detection of foreign objects in walnuts usingdeep learning. Comput Electron Agric 2019;162:1001 –10.https://doi.org/ 10.1016/j.compag.2019.05.019 . [66] Sakurada M, Yairi T. Anomaly detection using autoencoders with nonlineardimensionality reduction. In: Proceed. Of the MLSDA 2014 2nd workshop MLS dataanalysis; 2014. p. 4–11.https://doi.org/10.1145/2689746.2689747 . [67] Salimans T, Kingma DP. Weight normalization: a simple reparameterization toaccelerate training of deep neural networks. In: NIPS; 2016. p. 901 –9.https://arxiv.
org/pdf/1602.07868.pdf. [68] Schmidhuber J. Deep learning in neural networks: an overview. Neur. networ.2015;61:85–117.https://doi.org/10.1016/j.neunet.2014.09.003 . [69]Sharkawy AN. Principle of neural network and its main types. J. Adv. Appl.Comput. Math. 2020;7:8–19. [70]Singh V, Baranwal N, Sevakula RK, Verma NK, Cui Y. Layerwise feature selection instacked sparse auto-encoder for tumor type prediction. In: 2016 IEEE internationalconference on Bioinformatics and biomedicine (BIBM). IEEE; 2016. p. 1542 –8. [71] Sit MA, Koylu C, Demir I. Identifying disaster-related tweets and their semantic,spatial and temporal context using deep learning, natural language processing andspatial analysis: a case study of hurricane irma. International Journal of DigitalEarth 2019.https://doi.org/10.1080/17538947.2018.1563219 . [72] Srivastava N, Hinton G, Krizhevsky A, Sutskever I, Salakhutdinov R. Dropout: asimple way to prevent neural networks from over ﬁtting. JMLR 2014;15:1929–58. https://dl.acm.org/doi/abs/10.5555/2627435.2670313 .[73] Stiglic G, Kokol P. Stability of ranked gene lists in large microarray analysis studies.BioMed Res Int 2010;2010.https://doi.org/10.1155/2010/616358 . [74]Sutradhar R, Barbera L. Comparing an arti ﬁcial neural network to logistic regression for predicting ed visit risk among patients with cancer: a population-based cohort study. J Pain Symptom Manag 2020;60:1 –9. [75] Suzuki T. Generalization bound of globally optimal non-convex neural networktraining: transportation map estimation by in ﬁnite dimensional Langevin dynamics. 2020. arXiv preprint arXiv, 2007.05824.[76]Svozil D, Kvasnicka V, Pospichal J. Introduction to multi-layer feed-forward neuralnetworks. Chemometr Intell Lab Syst 1997;39:43 –62. [77]Tian Y, Lu C, Zhang X, Tan KC, Jin Y. Solving large-scale multiobjectiveoptimization problems with sparse optimal solutions via unsupervised neuralnetworks. IEEE transactions on cybernetics 2020 . [78] Tikhonov AN. On the stability of inverse problems. In: Dokl. Akad. Nauk SSSR;1943. p. 195–8.https://doi.org/10.1007/978-3-642-81472-3_5 . [79] Wager S, Wang S, Liang SP. Dropout training as adaptive regularization. In: NIPS;2013. p. 351–9.https://arxiv.org/pdf/1307.1493.pdf . [80] Wang Y, Yao H, Zhao S. Auto-encoder based dimensionality reduction.Neurocomputing 2016;184:232 –42.https://doi.org/10.1016/ j.neucom.2015.08.104.[81] Wei C, Kakade S, Ma T. The implicit and explicit regularization effects of dropout.In: International conference on machine learning. PMLR; 2020. p. 10181 –92.htt ps://arxiv.org/abs/2002.12915 . [82]Weisberg S. Applied linear regression, ume 528. John Wiley &Sons; 2005. [83]Wen T, Zhang Z. Deep convolution neural network and autoencoders-basedunsupervised feature learning of eeg signals. IEEE Access 2018;6:25399 –410.[84] Werbos PJ. Generalization of backpropagation with application to a recurrent gasmarket model. N. networ. 1988;1:339 –56.https://doi.org/10.1016/0893-6080(88) 90007-X.[85] Werbos PJ. Backpropagation through time: what it does and how to do it. Proc ofthe IEEE 1990;78:1550–60.https://doi.org/10.1109/5.58337 . [86]Wright RE. Logistic regression. 1995 . [87]Xia Y, Qin T, Chen W, Bian J, Yu N, Liu TY. Dual supervised learning. In:International conference on machine learning. PMLR; 2017. p. 3789 –98. [88] Xie F, Zhang J, Wang J, Reuben A, Xu W, Yi X, Varn FS, Ye Y, Cheng J, Yu M, et al.Multifactorial deep learning reveals pan-cancer genomic tumor clusters withdistinct immunogenomic landscape and response to immunotherapy. Clin Canc Res2020a;26:2908–20.https://doi.org/10.1158/1078-0432.CCR-19-1744 . [89]Xie Y, Wu X, Ward R. Linear convergence of adaptive stochastic gradient descent.In: International conference on arti ﬁcial intelligence and statistics. PMLR; 2020b. p. 1475–85.[90]Xin J, Embrechts MJ. Supervised learning with spiking neural networks. In:IJCNN’01. International joint conference on neural networks. Proceedings (cat.Noh01CH37222), IEEE; 2001. p. 1772 –7. [91]Yang J, Ma J. Feed-forward neural network training using sparse representation.Expert Syst Appl 2019;116:255 –64. [92]Yang X, Deng C, Zheng F, Yan J, Liu W. Deep spectral clustering using dualautoencoder network. In: Proceedings of the IEEE/CVF conference on computervision and pattern recognition; 2019. p. 4066 –75. [93]Zheng S, Zhao J. A new unsupervised data mining method based on the stackedautoencoder for chemical process fault diagnosis. Comput Chem Eng 2020;135:106755.[94] Zingaretti LM, Gezan SA, Ferr ~ao LFV, Osorio LF, Monfort A, Mu ~noz PR, Whitaker VM, P/C19erez-Enciso M. Exploring deep learning for complex trait genomicprediction in polyploid outcrossing species. Front Plant Sci 2020;11:25. https:// doi.org/10.3389/fpls.2020.00025 .H. Soumare et al. Array 11 (2021) 100068
10