A deep learning method for monitoring spatial distributionof cage-free hens
Xiao Yang, Ramesh Bist, Sachin Subedi, Lilong Chai ⁎
Department of Poultry Science, University of Georgia, Athens, GA 30602, USA
abstract article info
Article history:Received 16 January 2023Received in revised form 28 March 2023Accepted 29 March 2023Available online 3 April 2023The spatial distribution of laying hens in cage-free houses is an indicator of ﬂock's health and welfare. While larger space allows chickens to perform more natural behaviors such as dustbathing, foraging, and perching incage-free houses, an inherent challenge is evaluating chickens' locomotion and spatial distribution (e.g., real-time birds' number on perches or in nesting boxes). Manual inspection of hen's spatial distribution requirescloser observation, which is labor intensive, time consuming, subject to human errors, and stress causing onbirds. Therefore, an automated monitoring system is required to track the spatial distribution of hens for earlydetection of animal welfare and health concerns. In this study, a non –intrusive machine vision method was de- veloped to monitor hens' spatial distribution automatically. An improved You Only Look Once version 5(YOLOv5) method was developed and trained to test hens' distribution in research cage-free facilities (e.g., 200hens per house). The spatial distribution of hens the system monitored includes perch zone, feeding zone, drink-ing zone, and nesting zone. The dataset contains a whole growth period of chickens from day 1 to day 252. About3000 images were extracted randomly from recorded videos for model training, validation, and testing. About2400 images were used for training and 600 images for testing, respectively. Results show that the accuracy ofthe new model were 87 –94% for tracking distribution in different zones for different ages of hens/pullets.Birds' age affected the performance of the model as younger birds had smaller body size and were hard to be de-tected due to blackness or occultation by equipment. The performance of the model was 0.891 and 0.942 for babychicks (≤10 days old) and older birds (> 10 days) in detecting perching behaviors; 0.874 and 0.932 in detectingfeeding/drinking behaviors. Miss detection happened when the ﬂock density was high (>18 birds/m
2) and chicken body was occluded by other facilities (e.g., nest boxes, feeders, and perches). Further studies such aschicken behavior identiﬁcation works in commercial housing system should be combined with the model toreach an automatic detection system.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Keywords:Cage-free systemPrecision farmingSpatial distributionDeep learning
1. IntroductionPoultry distribution and activities are key information in assessinganimal's welfare andﬂock production (Li et al., 2017;Guo et al., 2020a, 2020b, 2022). In cage-free laying hen houses, chickens havemore space to move and perform natural behaviors as compared to con-ventional cage houses (Ben Sassi et al., 2016;Wang et al., 2017;Chai et al., 2018, 2019;Li et al., 2020;Bist and Chai, 2022;Castro et al., 2023). While larger space allows chickens to perform more natural be-haviors such as dustbathing, foraging, and perching in cage-free houses,an inherent challenge is evaluating chickens' health, welfare, and spe-ciﬁc behaviors such as locomotion and spatial distribution (e.g., real-time birds' number on perches or in nesting boxes) ( Chai et al., 2019;Oliveira et al., 2019;Bist et al., 2023). Au automated monitoring systemis required to track the spatial distribution of hens for early detection ofanimal welfare and health concerns (Subedi et al., 2023a, 2023b). In the past years, computer vision has gained fast –paced advances from human detection to animal monitoring ( Aydin et al., 2010;Porto et al., 2015;Lao et al., 2012, 2016;Subedi et al., 2023a). The computer vi- sion system provides a non –intrusive method in livestock monitoring(i.e., swine, cattle, and sheep) (Hitelman et al., 2022;Li et al., 2014; Nasirahmadi et al., 2017). For poultry housing, computer vision or deeplearning models (e.g., convolutional neural network - CNN) have beenapplied to track individual bird for analyzing behaviors ( Fang et al., 2020;Pereira et al., 2013;Subedi et al., 2023a). Some studies have fo- cused on chickens'ﬂoor distribution (i.e., feeding, drinking and walkingzones) (Aydin et al., 2010;Guo et al., 2020a, 2021;Yang et al., 2022). The CNN image processing algorithms showed high accuracy in monitor-ingﬂoor distribution (two dimensions).Guo et al. (2022)comparedArtiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
⁎Corresponding author.E-mail address:lchai@uga.edu(L. Chai).
https://doi.org/10.1016/j.aiia.2023.03.0032589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/different CNN models (i.e., ResNet-152, ResNeXt-101 and ECA-DenseNet-264) in monitoring broilers' behaviors. The models showed88–97% of accuracies. The YOLO (you only look once) is a one-stageCNN algorithm that has been applied to monitor poultry behaviors(Guo et al., 2022;Yang et al., 2022;Jocher et al., 2021).Ding et al. (2019)developed detector to monitor heat stress conditions of broilersby using YOLOv3 (Ding et al., 2019).Ye et al. (2020)proposed a CNN al- gorithm (YOLO + multilayer residual module (MRW) to detect whitefeather broilers stunning states.Subedi et al. (2023a)developed You Only Look Once version 5 (YOLOv5)-pecking models to track hens'pecking behaviors and improved accuracy of the model to 85 –90%. However, existing models have limitations (i.e., low speed detectionand one–time total number of detected chickens is restricted in track-ing spatial distribution of chickens (i.e., ﬂoor distribution + vertical distribution). Vertical distribution patterns of chickens are critical infor-mation for understanding hens' performance and behaviors in cage-freehousing system, an emerging egg production system in the US andEU countries (Chai et al., 2019). The objectives of this study were to:(1) develop a deep learning method for monitoring the spatial distribu-tion of cage-free hens/pullets; (2) quantify the real-time birds' numberin different zones automatically; and (3) optimize the performance ofthe model by incorporating camera angles, chicken ages and ﬂock den- sities.2. Material and methods2.1. Experimental design and data collectionAbout 800 day-old chicks (Hy-Line W-36) were raised in fourresearch chamber rooms (each was measured as 7.3 m long × 6.1 mwide × 3 m high) at the Poultry Science Center at the University ofGeorgia (UGA). Cameras (Swann Communications, Santa Fe Spring,CA) were installed with two different angles (i.e., vertically and horizon-tally) to record the spatial distribution of birds ( Fig. 1). The recorded videos were transferred to massive hard devices for analyzing videoquality and converting to JPG format in the Department of PoultryScience at UGA.Feeders, nipple drinkers, nest boxes (Bestnestbox company, Hudson,Ohio, USA) and a A-frame hen perch were installed in each of researchchamber rooms by referring the dimension suggested by commercialfarms (Chai et al., 2019). The research room was divided virtually intoperching, nesting, drinking, feeding zones for the deep learning algo-rithm to identify the distribution of chickens ( Fig. 2). Husbandry and management were following Hy-line W-36 management guides. Ani-mal management was approved by the Institutional Animal care andUse Committee (IACUC) at the University of Georgia.2.2. Methods for chicken detectionIn this study, an improved YOLOv5 model was developed forchicken detection. The architecture consisted of three parts,i.e., backbone, neck, and head (Fig. 3). The improved YOLOv5 modelis based on CNN network that can take in an input image and captureits spatial characters (learnable weights) to train the network to de-tect object (Liu et al., 2022). In backbone, four different models areused to extract basic features. In neck, feature pyramids and atten- tion mechanism module were utilized to recognize same targetunder separate size and scales. Besides, three attention mechanismmodules were added to enhance small targets detection. In modelhead, three individual feature maps were used to detect target(i.e., hens in different zones). According to Fig. 3,t h e r ea r ef o u r main modules in backbone for extracting features from given pic-tures, including FOCUS, Conv + Bottle Neck + Hard Switch (CBH),cross stage partial (CSP) and spatial pyramid pooling (SPP). Aftereach module, the pixels of pics changed from 608 × 608 pixels to76 × 76 pixels, 38 × 38 pixels, and 19 × 19 pixels ( Zhang et al.,
2022). With these decreased feature maps, the neck network appliesCSP and CBH to generate feature pyramids to aggregate on the fea-tures and submit it to head. However, during the pass progress, thecontextual information will decrease. To obtain more accurate infor-mation and minimize the information loss, an attention mechanismwas introduced to this improved YOLOv5 method (with red back-ground inFig. 3). The attention mechanism is combined withC3Ghost and Ghost modules to enhance the dominated channel
Fig. 1.Experimental setup for collecting laying hens' spatial distribution dataset.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
21attention(Woo et al., 2018). The aim of C3Ghost module is to reduceheavy computational burden as the Ghost applied into YOLOv5 necknetwork.The Ghost module focuses on generating more feature maps byusing fewer parameters. In this study, the Ghost was adopted to processhen's feature maps(Han et al., 2020). The original hen's feature map isblurry after YOLOv5 neck network. However, with the Ghost module,the channel number of hen's feature maps improved, and an enhancedhen's feature pyramid developed. The structure of it is shown in Fig. 4.I n the neck, the three dimension of input feature map is a × b × c, after theneck, the output feature map is a1 × b1 × c1, and the size of kernel isn × n, where a and a1 are heights of feature map, b and b1 are widthsof feature map. Comparing to the convolutional layer, the Ghostmodule processes the ordinary convolution in two steps. During the
Fig. 2.The deﬁnition of different zones. A. nesting zone; B. feeding zone; C. perching zone (3 m long, 1.8 m wide, 6 different heigh for birds to perch from 0.3 m to 2.4 m) ; and D. drinking zone.
Fig. 3.The network structure of improved YOLOv5 for tracking spatial distribution of chickens in cage-free houses.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
22convolutional layer, the basic number of ﬂoating-point operation (FLOPs) is a1 × b1 × c1 × c × n × n, which is usually over 105 whenthe channel number c is 256 and multiplies the ﬁlter number c1. The overload FLOPs lead the critical information of inputted imagines ( Ren et al., 2022). In the two steps of Ghost module, a cheap transformationprocedure was utilized in generating intrinsic feature maps and needsfewerﬁlters. Therefore, the new structure can obtain a notable perfor-mance.FLOPs¼a1/C2b1/C2c1/C2c/C2n/C2nð1ÞWhere a1, b1, and c1 are the output feature map's height, width, andchannels after the convolution operation. C represents the number ofchannels in the feature map input. N represents the size of theconvolutional kernel employed during the convolution operation.2.3. Methods for tracking chickens in different zonesA whole vision dataset (horizontal and vertical) and an interfacewere used to recognize spatial distribution of birds in different zones(Fig. 5). The graphical user interface (GUI) was developed with Pythonbinding for the Qt5 application framework (PyQt5) ( Fig. 6), which en- hances the process of selecting target zones (nesting, perching, feedingand drinking) (Xie et al., 2022). The zones of each bird in the picturewere designedﬁrstly, then the whole area in the image was used asthe reference area to estimate the number of birds in the selectedzones, the equation is showed below.~N
i¼ni
n,1≤i≤x ð2Þ(SeeFig. 7.)Where Ñ
i(birds/m2) is the average number of the target zones afternormalization,n
iis the number of the target zones in the ith picture,n (m
2) is the reference value of whole spatial area, and xis the number of zones recognized in the picture.2.4. Dataset and settingA laying hens' dataset was constructed to evaluate the performanceof the improved YOLOv5 on the detection of birds. The dataset containswhole growth period of bird from 1 week to 36 weeks (baby chicks tohens) and consisting of 3000 pics that were extracted randomly fromrecorded videos. All the pics were labeled by LabelImg Windos_v1.6.1version. Birds under 2 weeks of age were labeled as baby chicks, andbirds 2 weeks or older were labeled as pullets/hens. 2400 pics and 600pics were used during training section and testing section separately.The training was run under in Python Qt5 application frameworkfor 300 epochs with a learning rate 0.01 and a batch size of 16. The
Fig. 4.A demonstration of two steps Ghost module that used in this study for processing poultry images.
Fig. 5.The selected zones in the picture. The blue bullet A to C represents the different high perching zone varying from 7.9 ft. to 0 ft.; the two blue bullet D re present the feeding zone; the blue bullet E represents drinking zone; the blue bullet F represents the nesting zone.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
23conﬁdence threshold is set to 0.25, which means that objects with asimilarity of 0.25 or above can be considered interesting and markswill be assigned.2.5. Model evaluation and statistical analysisTo compare the performance of improved YOLOv5 with othermethods, the precision, recall and F1 score were used as evaluationparameters. The equations of them are showed below:Precision100%ðÞ ¼
TPTPþFPðÞ/C2100ð3ÞRecall100%ðÞ¼TPTPþFNðÞ/C2100ð4ÞF1score100%ðÞ ¼
2∗Precision∗RecallðÞPrecisionþRecallðÞ/C2100ð5ÞThe true positive (TP) is the test result correctly predicts the pres-ence of a characteristic, false positive (FP) is the test result wronglyindicates an attribute is present and false negative (FN) is the testresult that falsely predicts a particular condition is absent.To assess model performance under different situations (i.e., ages,various high of perching zone,ﬂock density and the distributionalzones), a one-way ANOVA and Turkey HSD were conducted by JMPsoftware. The signiﬁcant difference was set at 0.05.3. Results and discussions3.1. Model performance in detecting chickensTo evaluate the model performance and explore optimal setting pa-rameters, the YOLOv5–x method and several training parameters wereapplied. These parameters include image size (e.g., 640 and 320) anddatasets (e.g., individual type dataset and mixed type dataset). The sum-mary outcomes are shown inTable 1. Five experiments were conductedto explore the best model performance among whole chicken group bysetting different parameters (image size, dataset, and attention mecha-nism). image size represents the inputted image, and the epochs repre-sents the training times. Individual type dataset has two categories(i.e., baby chicks (< 1 week old) and pullet/hens that are older than1 week), mixed type dataset is all age of chickens is considered as onetype. We separate baby chicks from pullets/hens because 1 week oryoung chicks had smaller body size and have more challenges to be de-tected than older birds.
Fig. 6.The GUI developing by PyQt5 (b is pullets/hens >10 days; s –baby chicks≤10 days).
Fig. 7.ROC curve comparison results of different detector based on deep learning.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
24During the experiments, the loss function values toward to be stablewhen the epoch approached to 200, so the epoch was 200. From theTable 1, we discovered a) improved YOLOv5 method had better detec-tion comparing to original YOLOv5 on both individual and mixeddatasets, b) mixed dataset has better performance comparing to indi-vidual dataset when trained with improved YOLOv5 and originalYOLOv5 methods, c) increase the imagine size improved the overallmodel precision. These conﬁrmed our setting parameters. The ReceiverOperating Characteristic (ROC) curve shows the sensitivity and speci ﬁty of different detector (Fig. 7).4. Chicken distribution identiﬁcation in perching zonesFig. 8shows the birds detected by improved YOLOv5 model. In theperching zone (Fig. 8A), the model monitored perched chickens from0 to 2.4 m and summed up them to three different levels (the numberof hens in three levels were 7, 61, and 17 from bottom to top of theperch frame), respectively. For baby chicks' perching ( Fig. 8B), there were 8 hardwood perching boards. The number of detected chicks ineach perching board was 2, 3, 1, 5, 4, 1, 6 and 4, respectively, from farto close in theFig. 8B.To test the performance of improved YOLOv5 model in monitoringbirds in perching zones, about 200 images were randomly selected(chickens' age ranged from week 1 to week 20) to test the model(Table 2). The performance of the model was 0.891 and 0.942 for babychicks (≤10 days old) and older birds (> 10 days), respectively. Themiss detection rates of hens and baby chicks were 0.054 and 0.102, re-spectively. Errors or miss detections were caused by high densitychickens (pilling or crowding) and interreferences of perch frame andfeeders. In general, the mew modelﬁtted well in the perching zone (Rture= 0.891 and 0.942).A back propagation (BP) neural network algorithm was used toidentify the chicken distribution in drinking and feeding zones, themissed detection also comes from chicken crowding behaviors andTable 1The adjustment methods and results.Model Dataset Image Size Epochs Precision Accuracy F1 scoreYOLOv5-h1 Baby chicksPullets/hens640 200 63.0% (s) 76.0% (b)23.5% (s)80.1% (b)34.2% (s)77.8% (b) YOLOv5-h2 Baby chicksPullets/hens640 200 62.5% (s) 83.6% (b)33.8% (s)84.6% (b)43.9% (s)84.1% (b) YOLOv5-h3 Baby chicksPullets/hens320 200 65.8% (s) 71.4% (b)22.5% (s)83.6% (b)33.5% (s)77.0% (b) YOLOv5-h4 Mixed type 320 200 91.7% (all) 80.2% (all) 85.6% (all)YOLOv5-h5 Mixed type 320 200 90.2% (all) 91.6% (all) 90.9% (all)Note:s–baby chicks≤10 days;b–Pullets/hens >10 days. YOLOv5-h1 means the experiment parameters are image size 640, dataset is individual type; YOLOv5-h2 means the experiment parameters are image size 640, dataset is individual type with attention mechanism; YOLOv5-h3 means the experiment parameters are image size 320, da taset is individual type; YOLOv5- h4 means the experiment parameters are image size 320, dataset is mixed type; YOLOv5-h5 means the experiment parameters are image size 320, dataset is mixed type with attention mechanism.
Fig. 8.Chickens' distribution in perching zones detected by improved YOLOv5. (A) adult hens (133 days of old) detected; (B) baby chicks detected (8 days of ol d). The letterbin blue means older birds (> 10 days old) and the letter sin blue represents baby chicks ( ≤10 days old).
Table 2Tested performance of improved YOLOv5 on perch zone.Zone Target Chickens True Detection Miss Detection False R
true Rmiss Rfalse
Overlap Occlusion othersPerch (s) 1987 1872 46 41 20 8 0.942 0.054 0.004Perch (b) 866 772 30 27 31 6 0.891 0.102 0.007Note: R
true,Rmissand R falserates were evaluated by true detection number/target chickens' number, miss detection number/target chickens' number and false detection number/ target chickens' number respectively; smeans baby chicks;bmeans hens.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
25collusion problems(Yang et al., 2022). Otherﬂaws in our method are original from the horizonal vison factor (when baby chicks are toosmall under horizontal scale, the miss detection happens) and designedperch zone (when the perch zone is designed narrower than the real sit-uation, there will be less perch chicks included into the perch zone, sothe chicks were missed). The false detection rates were 0.004 and0.007 respectively. This were also the common drawback of other visionbased algorithms (Abdanan Mehdizadeh et al., 2015).4.1. Chicken distribution in feeding and drinking zonesFig. 9demonstrates the distribution of detected birds in feedingzones monitored by improved YOLOv5 model. For each pic, the numberof chickens in targeted areas were analyzed. From Fig. 9A, we can iden- tify the distribution of baby chicks (i.e., 10 days old) in feeding zone in100% accuracy. ForFig. 9B and C, the model detected larger chickens(i.e., 122 day old) in 100% accuracy as well. From Fig. 10A and B, the dis- tribution of 122 days old of hens in drinking areas collected from twodifferent angles. The detection efﬁciency was 100% in theFig. 10Ba s there was no osculation. ForFig. 10A, the feeder (low right corner)could block some chickens during the study.To investigate a larger number of chickens, we used about 200 im-ages to test the model performance in feeding and drinking zones(Table 3). The overall accuracy for baby chicks and older chickenswere 0.874 and 0.932, respectively. Comparing to accuracy of perchzones, detecting baby chicks of feeding and drinking zones was more
Fig. 9.Chickens' distribution in the feeding zone at different ages (A –chickens were 10 days old; B and C –chickens were 122 days old ( bmeans birds were > 10 days; smeans birds were≤10 day).
Fig. 10.The distribution of chickens (i.e., 122 days old) in the drinking zone at different angles: chickens in A (side view in 45 degree) and B (top view) ( bmeans birds were > 10 days; smeans birds were≤10 day).X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
26challenging due to smaller size of body and interferences of equipment.When a higher number of birds were assembled at the same feeder ordrinker, birds standing in front of the feeder or drinker had higher pos-sibility to be recognized than those close at sides of feeder and drinker.In addition, lighting affected image quality and model's performance.Apart from these, birds' density has been reported affect deep learningmodel's performance (Maselyne et al., 2016).4.2. Chicken distribution in nesting zoneIn this study, most hens started to lay their ﬁrst eggs at around 18 weeks of age. The nesting behaviors of hens was analyzed with ournewly developed model because it's important identify if there areﬂoor eggs or not (Gonzalez-Mora et al., 2022). Monitoring hens' distri- bution in nesting zones helps to minimize losses from laying eggs ontheﬂoor.Fig. 11shows the distribution of detected hens in nestingzones with our improved YOLOv5 model. Fig. 11A is the original image of nesting area.Fig. 11B demonstrates the detected hens innesting zone. The model performed with over 90% accuracy in detectinghens in nesting zones.To evaluate model's performance in nesting zone for hens' detectionsystemically, over 200 images were randomly selected from videodataset to improve targeted hens' dataset ( Table 4). The tested accuracy rate in nesting zone reached 0.906, which is slightly less than that inothers zones (i.e., perching, feeding, and drinking zones), becausethere was an equipment hanging above the nesting zone.Comparing to other computer vision-based tools for monitoringnesting zone, our model has a higher precision. For instance, a 0.825 ofaccuracy was reported in a research focusing on monitoring eggs andbirds in nesting zone (Lubich et al., 2019;Hui et al., 2021). In previous studies, the best YOLO model reached an accuracy of 0.885. In thisstudy, our improved YOLOv5 model performed better than most ofexisting YOLO models and other CNN models. However, the modelmissed some detections under high density of hens' ﬂock due to occlu- sion of hanging line or equipment on nesting boxes and zones. Furtherstudies are need to enhance model's performance, especially under
Fig. 11.Chicken distribution in nesting zone. (A) original image of nesting area and (B) detected nesting area.
Table 4Tested performance of improved YOLOv5 on nesting zone.Zone Target Chickens True Detection Miss Detection False detection R
true Rmiss Rfalse
Overlap Occlusion othersNest(b) 873 791 8 13 7 54 0.906 0.061 0.008Note: R
true,Rmissand R falserates were evaluated by true detection number/target chickens' number, miss detection number/target chickens' number and false detection number/ target chickens' number respectively; bmeans hens. There were no baby chicks ( s) because only hens would lay eggs.Table 3Tested performance of improved YOLOv5 on feeding and drinking zones.Birds Target Chickens True Detection Miss Detection False detection R
true Rmiss Rfalse
Overlap Occlusion otherss1027 898 20 19 19 71 0.874 0.056 0.069b768 716 17 15 15 6 0.932 0.061 0.008Note: R
true,Rmissand R falserates were evaluated by true detection number/target chickens' number, miss detection number/target chickens' number and false detection number/ target chickens' number respectively; smeans baby chicks≤10 days;bmeans older birds >10 days.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
27environment of high density of laying hens because commercial housetend to have thousands hens on theﬂoor (Huang et al., 2022;Subedi et al., 2023a).5. ConclusionsIn this study, an improved deep learning model was developedbased on YOLOv5 structure to monitor cage –free hens' spatial and ﬂoor distributions, including the real-time number of birds in perchingzone, feeding zone, drinking zone, and nesting zone. The accuracies ofthe new model were 87–94% for all ages of chickens across zones.Birds' age affected the performance of the model as younger birds hadsmaller body size and were hard to be detected due to blackness or oc-cultation by equipment. The performance of the model was 0.891 and0.942 for baby chicks (≤10 days old) and older birds (> 10 days) in de-tecting perching behaviors; 0.874 and 0.932 in detecting feeding/drink-ing behaviors. The different zones in the chicken house (perch zone,feeding zone, drinking zone, and nesting zone) are related to speci ﬁcb e - haviors of the chickens. For example, some chickens are expected toperch during the night, while during the day they move around thehouse and visit the feeding and drinking zones. Nesting behavior occurswhen hens are about to lay eggs. The current ﬁndings provide refer- ences for automatically monitoring cage –free laying hens' spatial dis- tribution in all age level (from baby chicks to hens). More futurechicken behavior identiﬁcation works could be combined with themodel to reach an automatic detection system.Authors credit statementLilong Chai (L.C.) contributed the research method and resources;Xiao Yang (X.Y.) conducted the experiment and collected the data;X.Y., Ramesh Bist (R.B.) and Sachin Subedi (S.S.) contributed to thedata collection; X.Y. and L.C wrote the main manuscript. All authors re-viewed the manuscript.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgmentsThe study was sponsored by the Egg Industry Center; Georgia Re-search Alliance (Venture Fund); UGA CAES Dean's Of ﬁce Research Fund; UGA COVID Recovery Research Fund; UGA Provost Of ﬁce Rural Eng- agement Fund; and USDA-NIFA; Hatch projects: Future Challenges inAnimal Production Systems: Seeking Solutions through Focused Facili-tation (GEO00895; Accession Number: 1021519) & Enhancing PoultryProduction Systems through Emerging Technologies and HusbandryPractices (GEO00894; Accession Number: 1021518).References
Abdanan Mehdizadeh, S., Neves, D.P., Tscharke, M., Nääs, I.A., Banhazi, T.M., 2015. Imageanalysis method to evaluate beak and head motion of broiler chickens during feeding.Comput. Electron. Agric. 114, 88 –95.https://doi.org/10.1016/j.compag.2015.03.017 . Aydin, A., Cangar, O., Ozcan, S.E., Bahr, C., Berckmans, D., 2010. Application of a fully auto-matic analysis tool to assess the activity of broiler chickens with different gait scores.Comput. Electron. Agric. 73, 194 –199.https://doi.org/10.1016/j.compag.2010.05.004 . Ben Sassi, N., Averós, X., Estevez, I., 2016. Technology and poultry welfare. Animals 6, 62.https://doi.org/10.3390/ani6100062 . Bist, R.B., Chai, L., 2022.Advanced Strategies for Mitigating Particulate Matter Generationsin Poultry Houses. Applied Sciences 12 (22), 11323.Bist, R.B., Subedi, S., Chai, L., Yang, X., 2023. Ammonia emissions, impacts, and mitigation strategies for poultry production: A critical review. Journal of Environmental Man-agement 328, 116919.Castro, F.L.S., Chai, L., Arango, J., Owens, C.M., Smith, P.A., Reichelt, S., DuBois, C., Menconi,A., 2023.Poultry industry paradigms: connecting the dots. Journal of Applied PoultryResearch 32 (1), 100310.Chai, L., Xin, H., Wang, Y., Oliveira, J., Wang, K., Zhao, Y., 2019. Mitigating particulate mat- ter generation in a commercial cage-free hen house. Transactions of the ASABE 62(4), 877–886.Chai, L., Xin, H., Zhao, Y., Wang, T., Soupir, M., Liu, K., 2018. Mitigating ammonia and PM generation of cage-free henhouse litter with solid additive and liquid spray. Transac-tions of the ASABE 61 (1), 287 –294. Ding, A., Zhang, X., Zou, X., Qian, Y., Yao, H., Zhang, S., Wei, Y., 2019. A novel method forthe group characteristics analysis of yellow feather broilers under the heat stressbased on object detection and transfer learning. INMATEH Agric. Eng. 58, 49 –58. https://doi.org/10.35633/INMATEH-59-06 . Fang, C., Huang, J., Cuan, K., Zhuang, X., Zhang, T., 2020. Comparative study on poultry tar-get tracking algorithms based on a deep regression network. Biosyst. Eng. 190,176–183.https://doi.org/10.1016/j.biosystemseng.2019.12.002 . Gonzalez-Mora, A.F., Rousseau, A.N., Larios, A.D., Godbout, S., Fournel, S., 2022. Assessingenvironmental control strategies in cage-free aviary housing systems: egg productionanalysis and random Forest modeling. Comput. Electron. Agric. 196, 106854. https:// doi.org/10.1016/j.compag.2022.106854 . Guo, Y., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., Chai, L., 2021. A machine vision- based method optimized for restoring broiler chicken images occluded by feedingand drinking equipment. Animals 11 (1), 123.Guo, Y., Aggrey, S.E., Wang, P., Oladeinde, A., Chai, L., 2022. Monitoring Behaviors of Broiler Chickens at Different Ages with Deep Learning. Animals 12 (23), 3390.Guo, Y., Chai, L., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., 2020a. A machine vision- based method for monitoring broiler chicken ﬂoor distribution. Sensors 20 (11), 3179.Guo, Y., He, D., Chai, L., 2020b. A machine vision-based method for monitoring scene-interactive behaviors of dairy calf. Animals 10 (2), 190.Han, K., Wang, Y., Tian, Q., Guo, J., Xu, Chunjing, Xu, Chang, 2020. GhostNet: More Featuresfrom Cheap Operations.https://doi.org/10.1109/CVPR42600.2020.00165 . Hitelman, A., Edan, Y., Godo, A., Berenstein, R., Lepar, J., Halachmi, I., 2022. Biometric iden-tiﬁcation of sheep via a machine-vision system. Comput. Electron. Agric. 194, 106713.https://doi.org/10.1016/j.compag.2022.106713 . Huang, Y., Yang, X., Guo, J., Cheng, J., Qu, H., Ma, J., Li, L., 2022. A high-precision method for100-day-old classiﬁcation of chickens in edge computing scenarios based on feder-ated computing. Animals 12, 3450. https://doi.org/10.3390/ani12243450 . Hui, Z., Jian, Z., Yuran, C., Su, J., Di, W. and Hao, D., 2021, August. Intelligent bird ’sn e s th a z - ard detection of transmission line based on RetinaNet model. In Journal of Physics: Conference Series(Vol. 2005, No. 1, p. 012235). IOP Publishing.Jocher, G., Stoken, A., Borovec, J., Chaurasia, A., Changyu, L., Laughing, A., Hogan, A., Hajek,J., Diaconu, L., Marc, Y., 2021.Ultralytics/yolov5: v5. 0-YOLOv5-P6 1280 models AWSsupervise. ly and YouTube integrations. Zenodo 11.Lao, F., Brown-Brandl, T., Stinn, J.P., Liu, K., Teng, G., Xin, H., 2016.Automatic recognition of lactating sow behaviors through depth image processing. Computers and Electronicsin Agriculture 125, 56 –62.Lao, F., Teng, G., Li, J., Yu, L., Li, Z., 2012. Behavior recognition method for individual layinghen based on computer vision. Trans. Chin. Soc. Agric. Eng. 28, 157 –163.https://doi. org/10.3969/j.issn.1002-6819.2012.24.022 . Li, Z., Luo, C., Teng, G., Liu, T., 2014. Estimation of pig weight by machine vision: A review.In: Li, D., Chen, Y. (Eds.), Computer and Computing Technologies in Agriculture VII,IFIP Advances in Information and Communication Technology. Springer, Berlin, Hei-delberg, pp. 42–49.https://doi.org/10.1007/978-3-642-54341-8_5 . Li, N., Ren, Z., Li, D., Zeng, L., 2020. Review: automated techniques for monitoring the be-haviour and welfare of broilers and laying hens: towards the goal of precision live-stock farming. Animal 14, 617 –625.https://doi.org/10.1017/S1751731119002155 . Li, L., Zhao, Y., Oliveira, J., Verhoijsen, W., Liu, K., Xin, H., 2017. A UHF RFID system for studying individual feeding and nesting behaviors of group-housed laying hens.Transactions of the ASABE 60 (4), 1337 –1347. Liu, T., Luo, R., Xu, L., Feng, D., Cao, L., Liu, S., Guo, J., 2022. Spatial channel attention fordeep convolutional neural networks. Mathematics 10, 1750. https://doi.org/10. 3390/math10101750.Lubich, J., Thomas, K., Engels, D.W., 2019. Identiﬁcation and Classiﬁcation of Poultry Eggs: A Case Study Utilizing Computer Vision and Machine Learning. 2 p. 25.Maselyne, J., Saeys, W., Briene, P., Mertens, K., Vangeyte, J., De Ketelaere, B., Hessel, E.F.,Sonck, B., Van Nuffel, A., 2016. Methods to construct feeding visits from RFID registra-tions of growing-ﬁnishing pigs at the feed trough. Comput. Electron. Agric. 128, 9 –19. https://doi.org/10.1016/j.compag.2016.08.010 . Nasirahmadi, A., Edwards, S.A., Sturm, B., 2017. Implementation of machine vision for de-tecting behaviour of cattle and pigs. Livest. Sci. 202, 25 –38.https://doi.org/10.1016/j. livsci.2017.05.014.Oliveira, J.L., Xin, H., Chai, L., Millman, S.T., 2019. Effects of litterﬂoor access and inclusion of experienced hens in aviary housing on ﬂoor eggs, litter condition, air quality, and hen welfare. Poultry Science 98 (4), 1664 –1677. Pereira, D.F., Miyamoto, B.C.B., Maia, G.D.N., Tatiana Sales, G., Magalhães, M.M., Gates, R.S.,2013. Machine vision to identify broiler breeder behavior. Comput. Electron. Agric.99, 194–199.https://doi.org/10.1016/j.compag.2013.09.012 . Porto, S.M.C., Arcidiacono, C., Anguzza, U., Cascone, G., 2015. The automatic detection ofdairy cow feeding and standing behaviours in free-stall barns by a computervision-based system. Biosyst. Eng. 133, 46 –55.https://doi.org/10.1016/j. biosystemseng.2015.02.012. Ren, J., Wang, Z., Zhang, Y., Liao, L., 2022. YOLOv5-R: lightweight real-time detectionbased on improved YOLOv5. J. Electron. Imaging 31, 033033. https://doi.org/10. 1117/1.JEI.31.3.033033.Subedi, S., Bist, R., Yang, X., Chai, L., 2023a. Tracking pecking behaviors and damages ofcage-free laying hens with machine vision technologies. Comput. Electron. Agric.204, 107545.https://doi.org/10.1016/j.compag.2022.107545 .X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
28Subedi, S., Bist, R., Yang, X., Chai, L., 2023b. Tracking ﬂoor eggs with machine vision in cage-free hen houses. Poult. Sci. 102637. https://doi.org/10.1016/j.psj.2023.102637 . Wang, K., Zhao, X., He, Y., 2017. Review on noninvasive monitoring technology of poultrybehavior and physiological information. Trans. Chin. Soc. Agric. Eng. 33, 197 –209. Woo, S., Park, J., Lee, J.-Y., Kweon, I.S., 2018. CBAM: convolutional block attention module.Presented at the Proceedings of the European Conference on Computer Vision(ECCV), pp. 3–19.https://doi.org/10.48550/arXiv.1807.06521 . Xie, J., Jin, Huilong, Wen, T., Du, R., 2022. Gesture recognition controls image style transferbased on improved YOLOV5s algorithm. In: Jin, H., Liu, C., Pathan, A.S.K., Fadlullah,Z.M., Choudhury, S. (Eds.), Cognitive Radio Oriented Wireless Networks and WirelessInternet. Springer International Publishing Ag, Cham, pp. 203 –212.https://doi.org/10. 1007/978-3-030-98002-3_15.Yang, X., Chai, L., Bist, R.B., Subedi, S., Wu, Z., 2022. A deep learning model for detectingcage-free hens on the litterﬂoor. Animals 12, 1983.https://doi.org/10.3390/ ani12151983.Ye, A., Pang, B., Jin, Y., & Cui, J. (2020, December). A YOLO-based neural network with VAEfor intelligent garbage detection and classi ﬁcation. In2020 3rd International Confer- ence on Algorithms, Computing and Arti ﬁcial Intelligence(pp. 1-7). Zhang, Y., Li, M., Ma, X., Wu, X., Wang, Y., 2022. High-precision wheat head detection model based on one-stage network and GAN model. Front. Plant Sci. 13.X. Yang, R. Bist, S. Subedi et al. Artiﬁcial Intelligence in Agriculture 8 (2023) 20 –29
29