Image compression based on 2D Discrete Fourier Transform and matrixminimization algorithm
Mohammed H. Rasheeda, Omar M. Saliha, Mohammed M. Siddeqa,*, Marcos A. Rodriguesb
aComputer Engineering Dept., Technical College/Kirkuk, Northern Technical University, Iraq
bGeometric Modeling and Pattern Recognition Research Group, Shef ﬁeld Hallam University, Shefﬁeld, UK
ARTICLE INFO
Keywords:DFTMatrix minimization algorithmSequential search algorithmABSTRACT
In the present era of the internet and multimedia, image compression techniques are essential to improve imageand video performance in terms of storage space, network bandwidth usage, and secure transmission. A number ofimage compression methods are available with largely differing compression ratios and coding complexity. In thispaper we propose a new method for compressing high-resolution images based on the Discrete Fourier Transform(DFT) and Matrix Minimization (MM) algorithm. The method consists of transforming an image by DFT yieldingthe real and imaginary components. A quantization process is applied to both components independently aimingat increasing the number of high frequency coef ﬁcients. The real component matrix is separated into Low Fre- quency Coefﬁcients (LFC) and High Frequency Coef ﬁcients (HFC). Finally, the MM algorithm followed by arithmetic coding is applied to the LFC and HFC matrices. The decompression algorithm decodes the data inreverse order. A sequential search algorithm is used to decode the data from the MM matrix. Thereafter, alldecoded LFC and HFC values are combined into one matrix followed by the inverse DFT. Results demonstrate thatthe proposed method yields high compression ratios over 98% for structured light images with good imagereconstruction. Moreover, it is shown that the proposed method compares favorably with the JPEG techniquebased on compression ratios and image quality.
1. IntroductionThe exchange of uncompressed digital images requires considerableamounts of storage space and network bandwidth. Demands for ef ﬁcient image compression result from the widespread use of the Internet anddata sharing enabled by recent advances in digital imaging and multi-media services. Users are creating and sharing images with increased sizeand quantity and expect quality image reconstruction. It is clear thatsharing multimedia-based platforms such as Facebook and Instagramlead to widespread exchange of digital images over the Internet [ 1]. This has led to efforts to improve andﬁne-tune present compression algo-rithms along with new algorithms proposed by the research communityto reduce image size whilst maintaining the best level of quality. For anydigital image, it can be assumed that the image in question may haveredundant data and can be neglected to a certain extent. The amount ofredundancy is notﬁxed, but it is an assumed quantity and its amountdepends on many factors including the requirements of the application tobe used, the observer (viewer) or user of the image and the purpose of itsuse [2,3]. Basically, if the purpose of an image is to be seen by humansthen we can assume that the image can have a variable high level ofredundant data. Redundant data in digital images come from the fact thatpixels in digital images are highly correlated to a level where reducingthis correlation cannot be noticed by the human eye (Human VisualSystem) [4,5]. Consequently, most of these redundant, highly correlatedpixels can be removed while maintaining an acceptable level of humanvisual quality of the image. Therefore, in digital images the Low Fre-quency Components (LFC) are more important as they contribute more todeﬁne the image contents than High Frequency Components (HFC).Based on this, the intension is to preserve the low frequency values andshorten the high frequency values by a certain amount, in order tomaintain the best quality with the lowest possible size [ 6,7]. Image frequencies can be determined through a number of trans-formations such as the Discrete Cosine Transform (DCT), DiscreteWavelet Transform (DWT) and Discrete Fourier Transform (DFT) [ 8]. In this study we will use DFT as aﬁrst step in the process to serialize a digitalimage for compression. Since its discovery, the DFT has been used in the
* Corresponding author.E-mail addresses:mhrjabary@gmail.com(M.H. Rasheed),omar.alsabaawi@gmail.com(O.M. Salih),mamadmmx76@gmail.com(M.M. Siddeq),M.Rodrigues@shu. ac.uk(M.A. Rodrigues).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100024Received 21 November 2019; Received in revised form 11 February 2020; Accepted 6 March 2020Available online 8 March 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 6 (2020) 100024ﬁeld of image processing and compression. The DFT is used to convert animage from the spatial domain into frequency domain, in other words itallows us to separate high frequency from low frequency coef ﬁcients and neglect or alter speciﬁc frequencies leading to an image with less infor-mation but still with a convenient level of quality [ 8–10]. We propose a new algorithm to compress digital images based on theDFT in conjunction with the Matrix Minimization method as proposed inRef. [10,11]. The main purpose of matrix minimization is to reduce HighFrequency Components (HFC) to 1/3 of its original size by convertingeach three items of data into one, a process that also increases redundantcoefﬁcients [11,12]. The main problem with Matrix Minimization is thatit has a large probability data called Limited-Data [ 13,14,16]. Such probabilities are combined within the compressed ﬁle as indices used later in decompression.Our previous research [13,14] used the DCT combined with MatrixMinimization algorithm yielding over 98% compression ratios forstructured light images and 95% for conventional images. The mainjustiﬁcation to use DFT in the proposed method is to demonstrate that theMatrix Minimization algorithm is very effective in connection with adiscrete transform and, additionally, to investigate the DFT for imagecompression.The contribution of this research is to reduce the relatively largeprobability table to two values only, minimum and maximum, ratherthan keeping the entire lookup table (referred to as Limited-Data in ourprevious research[10,11,12and13]). The main reason is to increasecompression ratios by reducing the size of the compressed ﬁle header.
Fig. 1.The proposed compression method.
Fig. 2.DFT applied to a 4/C24 matrix of data.
Fig. 3.Quantization and rounding off the real and imaginary components.
Fig. 4.Each block (4/C24) is divided to real and imaginary matrices (afterapplying DFT). The real matrix contains DC value at ﬁrst location, these DC values are saved in a new matrix. The rest of high-frequency coef ﬁcients are saved in a different matrix as shown in contents of the LFC-Matrix, HFC
Real
and HFC Imag.M.H. Rasheed et al. Array 6 (2020) 100024
2The proposed compression algorithm is evaluated and analyzed throughmeasures of compression ratios, RMSE (Root Mean Square Error) andPSNR (Peak Signal-to-Noise Ratio). It is demonstrated that the proposedmethod compares well with the popular JPEG technique.2. The proposed compression algorithmThe proposed compression method is illustrated in Fig. 1. Initially, an original image is subdivided into non-overlapping blocks of size M x Npixels starting at the top left corner of the image. The Discrete Fouriertransform (DFT) is applied to each M x N block independently to repre-sent the image in the frequency domain yielding the real and imaginarycomponents. The Matrix Minimization algorithm is applied to eachcomponent and zeros are removed. The resulting vectors are subjected toArithmetic coding and represent the compressed data.To illustrate the process for each M x N (M¼N¼4) block in the original image, we represent a 4/C24 block inFig. 2below: A uniform quantization is then applied to both parts, which involvesdividing each element by a factor called quantization factor Q followedby rounding the outcomes which results in an increase of high frequencycoefﬁcients probability thus reducing the number of bits needed torepresent such coefﬁcients. The result of this operation is that thecompression ratio increases.Fig. 3illustrate the quantization and rounding off steps. For more information, the uniform quantization (Qrand Qi) are selected heuristically.Up to this point, two matrices (Qr and Qi) have been generated perblock representing the real and the imaginary parts respectively.Regarding the real part, all low coefﬁcient values (i.e. the DC values) are detached and saved into a new matrix called Low Frequency Coef ﬁcients (LFC-Matrix) and its substituted with a zero value in the quantized ma-trix. It is important to note that DC values are only found in the real partswhich highly contribute to the main details and characteristics of theimage. The generated LFC-Matrix size consists of all the DC values of theentire image can be considered small compared to all other High Fre-quency Coefﬁcients (HFC-Matrix) and can be represented with few bytes.Fig. 4illustrates the content of the generated three matrices.Since the size of the LFC-Matrix is small compared to HFC-Matrices, itis very obvious that HFC matrices for both real and imaginary parts needto be reduced to get a reasonable compression. Therefore, the algorithmcalled Matrix-Minimization suggested by Siddeq and Rodrigues [ 10]i s applied. The algorithm is used to reduce the size of HFC matrices bycontracting every three coefﬁcients to a single equivalent value, whichcan be traced back to their original values in the decompression phase.The contraction is performed on each three consecutive coef ﬁcients using Random-Weight-Values. Each value is multiplied by a different randomnumber (Ki) and then their summation is found, the value generated isconsidered a contracted value of the input values. Fig. 5illustrates the Matrix Minimization applied to M x N matrix [ 11,12]. It is important to note that in the decompression phase a search al-gorithm is required toﬁnd the three original values that are used to ﬁnd the contracted value, therefore, the minimum and maximum values ofthe m x n block are stored. The idea behind this is to limit the range ofvalues required to recover the original three values that made the con-tracted value hence increase the speed of the search algorithm atdecompression stage.Because in previous work the range of the search space are limited inthe array for easy searching and this was encoded in the header ﬁle to be used at decompression stage. However, it is possible that complex imagesmay generate large arrays which, in turn, will impair compression (makeit more computationally demanding). For this reason, we suggestedanother method in this paper using DFT and reduced limited search area(i.e. search area contains just two values [MIN, MAX]). Such boundingmakes searching for the sought values easier and faster. Any further
detailed information about Matrix Minimization can be found in
Fig. 5.The Matrix Minimization method for an m x n matrix [ 10–12].
Fig. 6.Separating zeros and nonzero from HFC matrix and coding zero and non-zero values into Zero and Value matrices.
Fig. 7.Decompression steps.M.H. Rasheed et al. Array 6 (2020) 100024
3references [11,12,16]. These three references show with examples howthe Matrix Minimization works with keys and how the limited search isused for decoding.After the Matrix-Minimization algorithm has been applied, the pro-duced HFC-Matrix for both real and imaginary parts are examined and itis possible to see a high probability in the number of zero values than anyother values in the matrix. Therefore, separating zero from non-zerovalues will remove redundant data and hence increase the ef ﬁciency of the arithmetic coding compression [9,10,13,14]. The implementation of the method is by isolating all zero values fromthe matrix while preserving all non-zero values in a new array calledValue Matrix. The total number of zeros removed between each non-zero value in theHFC-Matrixis counted during the process. A newarray calledZero Matrixis then created in which we append a zero valuewhenever we have a non-zero value at the same index in the originalHFC-Matrix followed by an integer that represents the total number ofzeros between any two non-zero values. Fig. 6demonstrates the process of separating zeros and non-zero values [ 14–16]. The zero values in the Zero-Matrix reﬂect the actual non-zero values in sequences in the original matrix. Likewise, the integer values re ﬂect the total number of zeros that come thereafter. Finally, the two matricesare ready for compression by a coding method which in our case isarithmetic coding [6,7]. It is important to note that the proposed methoddescribed above is also applied to the LFC-Matrix which contains the low
Fig. 8.(a), (b) and (c) Lena, Lion and Apple images status are compressed by our proposed method using different quantization values.M.H. Rasheed et al. Array 6 (2020) 100024
4frequency coefﬁcients values of the real part. Up to this point, theValue-Matrix and Zero-Matrix in our case are considered headers andused in the decompression process to regenerate the original HFC andLFC matrices.3. The decompression algorithmThe decompression algorithm is a counter compression operationwhich performs all functions of the compression but in reverse order. Thesteps to decompression start by decoding the LFC-Matrix, Value-Matrixand Zero-Matrix using arithmetic decoding followed by reconstructing auniﬁed array based on Value and Zero matrices and reconstruct the HFC-Matrix for both parts. Siddeq and Rodrigues proposed a novel algorithmcalled Sequential Search Algorithm [ 10–13], which is based on three pointers working sequentially to regenerate the three values thatconstitute the contracted values with assistance of the MIN and MAXvalues which were preserved during the compression process. The MINand MAX values are considered to be the limited space search values usedto restore the actual HFC for both parts (real and imaginary) [ 14–18]. Finally, an inverse quantization and DFT is applied to each part toreconstruct the compressed digital image. Fig. 7illustrates the decom- pression steps.4. Experimental resultsExperimental results shown here demonstrate the ef ﬁcacy of the
Fig. 9.(a), (b) and (c) Boeing 777, Girl and Baghdad colour images are compressed by our proposed method using different quantization values .M.H. Rasheed et al. Array 6 (2020) 100024
5proposed compression technique. Our proposed method was imple-mented in MATLAB R2014a running on an Intel Core i7-3740QMmicroprocessor (8-CPUs). For clarity, we divide the results into two parts:/C15The method applied to general 2D images of different sizes and assesstheir visual quality with RMSE [ 1,3]. Also, we applied Peak Signal-to-Noise Ratio (PSNR) for measuring image quality. Thismeasurement widely used in digital image processing [ 23].Tables 1 and 2show theﬁrst part of results by applying the proposed com-pression/decompression method to six selected images whose detailsare shown inFigs. 8 and 9./C15We apply the proposed compression technique to structured lightimages (i.e. a type of image used for reconstruct 3D surfaces - seeSection5).5. Results for structured light images and 3D surfacesA 3D surface mesh reconstruction method was developed by Rodri-gues [8,19] with a team within the GMPR group at Shef ﬁeld Hallam University. The working principle of the 3D mesh scanner is that thescene is illuminated with a stripe pattern whose 2D image is thencaptured by a camera. The relationship between the light source and thecamera determines the 3D position of the surface along the stripe pattern.The scanner converts a surface to a 3D mesh in a few milliseconds by
Fig. 10.(a) The 3D Scanner developed by the GMPR group, (b) a 2D picture captured by the camera, (c) 2D image converted into a 3D surface patch.
Fig. 11.Original 2D images with different dimensions used by our proposed compression method.
Table 1Results for grey images.
Image ImageSize(MB)Quantization AfterCompression(KB)(Bit/Pixel)bppRMSE PSNRLena 1.0 10 260 0.253 1.2 47.325 138.2 0.134 2.4 44.345 88.1 0.086 3.9 42.2Lion 1.37 25 201 0.143 2.5 44.160 108.4 0.077 5.0 41.1100 71.4 0.05 8.1 39.0Apples 1.37 10 228 0.162 1.2 47.330 91.7 0.065 2.6 43.960 47.8 0.034 4.6 41.5
Table 2Results for colour images.
Image Image Size (MB) Quantization for each layer in R,G,B After Compression (KB) (Bit/Pixel) bpp RMSE PSNRBoeing 777 6.15 10 437.4 0.069 2.1 44.9 25 182.8 0.029 3.9 42.2 Girl 4.29 10 641.1 0.145 3.9 42.2 25 315.6 0.071 5.5 40.7 Baghdad 8.58 25 426.3 0.097 4.4 41.6 35 309.8 0.07 5.6 40.6Table 3Compressed 2D structured light images.
Image ImageSize(MB)Quantization AfterCompression(KB)(bit/Pixel)bppRMSE PSNRCorner 1.25 60 35.4 0.027 4.7 41.4100 17.8 0.013 15.5 36.2Face1 1.37 100 34.0 0.024 8.4 38.8160 18.1 0.012 11.5 37.5Face2 1.37 50 46.2 0.032 6.7 39.8150 20.1 0.014 9.9 38.1M.H. Rasheed et al. Array 6 (2020) 100024
6Fig. 12.(a) and (b): shows the 2D decompressed for Corner ’s image, that used in 3D application to reconstruct 3D mesh surface. The 3D mesh (3D vertices and triangles) is successfully reconstructed without signi ﬁcant distortion at high compression ratios up to 98.6%.M.H. Rasheed et al. Array 6 (2020) 100024
7Fig. 13.(a) and (b): shows decompressed for Face1 2D image, that used in the 3D application to reconstruct 3D mesh surface. The 3D mesh is successfully recon-structed without signiﬁcant distortion at high compression ratios of 98.6%.M.H. Rasheed et al. Array 6 (2020) 100024
8Fig. 14.(a) and (b): shows decompressed for Face2 2D image, that used in 3D application to reconstruct 3D mesh surface. The 3D mesh was successfully reconstruc ted without signiﬁcant distortion at high compression ratios of 98.5%.M.H. Rasheed et al. Array 6 (2020) 100024
9using a single 2D image [19,20] as shown inFig. 10. The signiﬁcance of using such 2D images is that, if the compressionmethod is lossy and results in a noisy image, the 3D algorithms willreconstruct the surface with very noticeable artefacts, that is, the 3Dsurface becomes defective and degraded with problem areas easilynoticeable. If, on the other hand, the 2D compression/decompression isof good quality, then the 3D surface is reconstructed well and there are novisible differences between the original reconstruction and the recon-struction with the decompressed images.Fig. 10(left) depicts the GMPR scanner together with an imagecaptured by the camera (middle) which is then converted into a 3Dsurface and visualized (right). Note that only the portions of the imagethat contain patterns (stripes) can be converted into 3D; other parts of theimage are ignored by the 3D reconstruction algorithms [ 21,22]. The original images used in this research are shown in Fig. 11(Corner, Face1 and Face2). The three images shown in Fig. 11were compressed by themethod described in this paper whose compressed sizes with RMSE andPSNR are shown inTable 3. After decompression, the images were sub-jected to 3D reconstruction using the GMPR method and compared with3D reconstruction of the original images. The reconstructed 3D surfacesare shown inFigs. 12–14.6. Discussion and comparative analysisOur literature survey did not show results for image compressionusing the DFT alone. The reason is that by applying a DFT, it yields twosets of coefﬁcients, real and imaginary. If one wishes to keep those forfaithful image reconstruction, then it is not possible to achieve highcompression ratios. We applied the DFT as described in this paperresulting in images with good visual quality and low compressioncomplexity. A comparative analysis between compression ratios for DFTalone and DFT followed by the Matrix Minimization algorithm showTable 4Comparative analysis of using DFT alone and our proposed method (DFT and Matrix Minimization) based on image quality and compressed size.
Image Size (MB) Quantization Factor Compressed size DFT alone RMSE PSNR Compressed Size DFT þMM RMSE PSNRKB bpp KB bppConventional imagesLena 1.0 45 721 0.7 2.1 44.9 88 0.085 3.9 42.2Lion 1.37 100 808 0.57 3.59 42.5 71 0.05 8.1 39.0Apples 1.37 60 576 0.41 2.3 44.5 47 0.033 4.6 41.5Boeing 6.15 25 2200 0.35 1.8 45.5 182 0.028 3.9 42.2Girl 4.29 25 2350 0.54 3.59 42.5 315 0.071 5.5 40.7Bagdad 8.58 35 3500 0.4 2.3 44.5 309 0.035 5.6 40.6Structured light imagesCorner 1.25 100 615 0.48 2.9 43.5 17 0.013 15.5 36.2Face1 1.37 160 624 0.44 4.8 41.3 18 0.012 11.5 37.5Face2 1.37 150 508 0.36 4.1 42.0 20 0.014 9.9 38.1
Table 5Comparative analysis of compression using JPEG and our approach based on image quality and compression size.
Image Size (MB) Compressed Size by JPEG RMSE PSNR Compressed Size by DFT þMM RMSE PSNRKB bpp KB bppConventional imagesLena 1.0 64 0.062 1.9 45.3 88 0.085 3.9 42.2Lion 1.37 56 0.039 8.8 38.6 71 0.05 8.1 39.0Apples 1.37 48 0.034 3.2 43.0 47 0.033 4.6 41.5Boeing 6.15 210 0.033 8.7 38.7 182 0.028 3.9 42.2Girl 4.29 347 0.078 9.8 38.2 315 0.071 5.5 40.7Bagdad 8.58 279 0.031 3.5 42.6 309 0.035 5.6 40.6Structured light imagesCorner 1.25 26 0.02 14.3 36.5 17 0.013 15.5 36.2Face1 1.37 23 0.016 16.5 35.9 18 0.012 11.5 37.5Face2 1.37 27 0.019 13.1 36.9 20 0.014 9.9 38.1
Fig. 15.Compressed and decompressed greyscale images by JPEG, the quality of the decompressed images varies compared with our approach according to RMSEand PSNR.M.H. Rasheed et al. Array 6 (2020) 100024
10Fig. 16.Compressed and Decompressed colour images by JPEG, the decompressed images (Boeing and Girl) have lower quality compared with our approach ac-cording to RMSE and PSNR. However, our approach couldn ’t reach to JPEG level of compression for Bagdad ’s image on the right.
Fig. 17.3D reconstruction from JPEG compressed images. In (a) reconstruction was possible but with signi ﬁcant artefacts. In (b) 3D reconstruction was not possible as images were too deteriorated.
Table 6Comparative analysis between pervious work [ 12] (Matrix Minimization algorithm) and our approach based on time execution.
Image Size (MB) Previous work (Matrix Minimization algorithm) The proposed algorithmCompressed size (KB) Bits/Pixel (bpp) Decompression time (seconds) Compressed size (KB) Bits/pixel (bpp) Decompression time (seconds)Lena 1.0 120 0.117 102 88 0.085 25Lion 1.37 98 0.069 240 71 0.05 40Apples 1.37 92 0.065 90 47 0.033 15Boeing 6.15 240 0.038 420 182 0.028 150Girl 4.29 399 0.090 330 315 0.071 114Bagdad 8.58 673 0.076 720 309 0.035 198Corner 1.25 56 0.043 84 17 0.013 22Face1 1.37 46 0.032 144 18 0.012 59Face2 1.37 38 0.027 174 20 0.014 66M.H. Rasheed et al. Array 6 (2020) 100024
11enormous differences as shown inTable 4. The results demonstrate that our proposed method of using a DFT inconjunction with the Matrix Minimization algorithm has the ability tocompress digital images up to 98% compression ratios. It is shown thatthe DFT alone cannot compress images with similar ratios and quality.Although it can be seen fromTable 4that our proposed method (DFTþ Matrix Minimization algorithm) increases the overall RMSE and, whilesome image details are lost, reconstructed images are still high quality.Additionally, the proposed method is compared with JPEG technique[23–25] which is a popular technique used in image and videocompression. Also, the JPEG is used in many areas of digital imageprocessing [26]. The main reason for comparing our method with JPEG isbecause JPEG is based on DCT and Huffman coding. Table 5shows the analytical comparison between the two methods.In aboveTable 5it shown that our proposed method is better thanJPEG technique to compress structured light images, while for conven-tional images it can be stated that both methods are roughly equivalent asimage quality varies in both methods. The following Figs. 15–17show comparisons between our approach the and JPEG technique for the im-ages shown inTables 4 and 5Concerning the compression of structured light images for 3D meshreconstruction, the comparison of our method with JPEG shows enor-mous potential for our approach as depicted in Figs. 11–14. Trying to compress the same images using JPEG and then using the decompressedimage to generate the 3D mesh clearly shows the problems and limita-tions of JPEG. This is illustrated in Fig. 17, which shows the JPEG technique on two structured light images for 3D mesh reconstruction.Comparative analysis focused on our previous work on the MatrixMinimization algorithm based on two discrete transforms DWT and DCT,as suggested by Siddeq and Rodrigues [ 9–11] performing compression and encryption at the same time. However, complexity of compressionand decompression algorithms is cited as a disadvantage of previouswork.Table 6shows the decompression time for the Matrix Minimizationalgorithm [12] (previous work) compared with our proposed approach.The advantages of the proposed over previous work are summarized asfollows:/C15The complexity of the decompression steps is reduced in the proposedapproach. This is evident from execution times quoted in Table 6as the current approach runs faster than previous work on the samehardware./C15The headerﬁle information of current approach is smaller than pre-vious work leading to increased compression ratios.It is important to stress the signiﬁcant novelties of the proposed approach which are the reduced number of steps at decompression stageand smaller header information resulting in faster reconstruction fromdata compressed at higher compression ratios. Table 7shows that our proposed image compression method has higher compression ratios andbetter image quality (i.e. for both types conventional and structured lightimages) as measured by RMSE and PSNR.7. ConclusionThis research has demonstrated a novel approach to compress imagesin greyscale, colour and structured light images used in 3D reconstruc-tion. The method is based on the DFT and the Matrix-Minimization al-gorithm. The most important aspects of the method and their role inproviding high quality image with high compression ratios are high-lighted as follows./C15After dividing an image into non-overlapping blocks (4
/C24), a DFT is applied to each block followed by quantizing each part (real andimaginary) independently. Meanwhile, the DC value (Low FrequencyCoefﬁcients) from each block are stored in a new matrix, while therest of the values in the block are the High Frequency Coef ﬁcients. /C15The Matrix-Minimization algorithm is applied to reduce the high-frequency matrix to 1/3 of its original size, leading to increasedcompression ratios./C15The relatively large probability table of previous method was reducedto two values, minimum and maximum leading to higher compressionratios and faster reconstruction.Results demonstrate that our approach yields better image quality athigher compression ratios while being capable of accurate 3D recon-struction of structured light images at very high compression ratios.Overall, the algorithm yields a best performance on colour images andstructured light images used in 3D reconstruction than on standard greyimages.On the other hand, the compression steps introduced by the MM al-gorithm, especially at decompression stage, make the compression al-gorithm more complex than, for instance, standard JPEG. In general, itcan be stated that decompression is slower than compression due to thesearch space to recover the original Low and High Frequency coef ﬁcients. In addition, arithmetic coding and decoding is applied to three sets ofdata (DC values, in addition to real and imaginary frequency coef ﬁcients) adding signiﬁcantly more computation steps leading to increasedexecution time.Conﬂicts of interestThe authors declare that there are no conﬂicts of interest regarding the publication of this paper.CRediT authorship contribution statementMohammed H. Rasheed:Conceptualization, Methodology.Omar M. Salih:Data curation, Writing - original draft.Mohammed M. Siddeq: Visualization, Software.Marcos A. Rodrigues:Supervision, Writing - review&editing.AcknowledgmentsWe grateful acknowledge the Computing, Communication andTable 7Comparative analysis between pervious work [ 12] (Matrix Minimization) and our approach based on image quality and compression sizes.
Image Size (MB) Previous work (Matrix Minimization algorithm) The proposed algorithmCompressed Size (KB) Bits/Pixel (bpp) RMSE PSNR Compressed Size (KB) Bits/Pixel (bpp) RMSE PSNRLena 1.0 120 0.117 6.8 39.8 88 0.085 3.9 42.2Lion 1.37 98 0.069 10.1 38.0 71 0.050 8.1 39.0Apples 1.37 92 0.065 7.1 39.6 47 0.033 4.6 41.5Boeing 6.15 240 0.038 10.2 38.0 182 0.028 3.9 42.2Girl 4.29 399 0.090 8.4 38.8 315 0.071 5.5 40.7Bagdad 8.58 673 0.076 5.9 40.4 309 0.035 5.6 40.6Corner 1.25 56 0.043 16.0 36.0 17 0.013 15.5 36.2Face1 1.37 46 0.032 14.4 36.5 18 0.012 11.5 37.5Face2 1.37 38 0.027 11.2 37.6 20 0.014 9.9 38.1M.H. Rasheed et al. Array 6 (2020) 100024
12Cultural Research Institute (C3RI) and the Research and Innovation Of-ﬁce at Shefﬁeld Hallam University for their support.References
[1]Richardson IEG. Video codec design. John Wiley &Sons; 2002. [2]Sayood K. Introduction to data compression. 2nd ed. Academic Press, MorganKaufman Publishers; 2001. [3]Rao KR, Yip P.Discrete cosine transform: algorithms, advantages, applications. San Diego, CA: Academic Press; 1990 . [4]Gonzalez Rafael C, Woods RichardE. Digital image processing. Addison Wesleypublishing company; 2001. [5]Yuan Shuyun, Hu Jianbo. Research on image compression technology based onHuffman coding. J Vis Commun Image Represent February 2019;59:33 –8. [6]Li Peiya, Lo Kwok-Tung. Joint image encryption and compression schemes based on16/C216 DCT. J Vis Commun Image Represent January 2019;58:12 –24. [7] M. Rodrigues, A. Robinson and A. Osman. Ef ﬁcient 3D data compression through parameterization of free-form surface patches, In: Signal process and multimediaapplications (SIGMAP), proceedings of the 2010 international conference on. IEEE,130-135.[8]Siddeq MM, Al-Khafaji G. Applied minimize-matrix-size algorithm on thetransformed images by DCT and DWT used for image compression. Int J ComputAppl 2013;70:15.[9]Siddeq MM, Rodrigues MA. A new 2D image compression technique for 3D surfacereconstruction. In: 18th international conference on circuits, systems,communications and computers. Greece: Santorin Island; 2014. p. 379 –86. [10] Siddeq MM, Rodrigues MA. A novel image compression algorithm for highresolution 3D reconstruction. 3D Research 2014;5(2). https://doi.org/10.1007/ s13319-014-0007-6. Springer.[11] M.M. Siddeq and Rodrigues Marcos. Applied sequential-search algorithm forcompression-encryption of high-resolution structured light 3D data. In: Blashki,Katherine and Xiao, Yingcai, (eds.)MCCSIS : multi conference on computer scienceand information systems 2015. IADIS Press, 195-202.[12] Siddeq MM, Rodrigues Marcos. A novel 2D image compression algorithm based ontwo levels DWT and DCT transforms with enhanced minimize-matrix-size algorithmfor high resolution structured light 3D surface reconstruction. 3D Research 2015;6(3):26.https://doi.org/10.1007/s13319-015-0055-6 .[13] Siddeq Mohammed, Rodrigues Marcos. A novel high frequency encoding algorithmfor image compression. EURASIP J Appl Signal Process 2017;26. https://doi.org/ 10.1186/s13634-017-0461-4. [14]Siddeq Mohammed, Rodrigues Marcos. DCT and DST based image compression for3D reconstruction. 3D Research 2017;8(5):1 –19. [15] Shefﬁeld Hallam University, Mohammed M Siddeq and Marcos A Rodrigues. Imagedata compression and decompression using minimize size matrix algorithm. WO2016/135510 A1. Patent 2016.[16] M.M. Siddeq and Rodrigues Marcos. Novel 3D compression methods for geometry,connectivity and texture. 3D Research, 7 (13). 2016[17]Siddeq MM, Rodrigues Marcos. 3D point cloud data and triangle Face compressionby a novel geometry minimization algorithm and comparison with other 3Dformats. In: Proceedings of the international conference on computational methods.vol. 3. California USA: University of California; 2016. p. 379 –94. [18] Siddeq MM, Rodrigues AM. A novel hexa data encoding method for 2D imagecrypto-compression. Multimed Tool Appl 2019. https://doi.org/10.1007/s11042- 019-08405-3. Springer.[19]Rodrigues M, Kormann M, Schuhler C, Tomek P. Robot trajectory planning usingOLP and structured light 3D machine vision. Heidelberg: Springer; 2013. p. 244 –53. Lecture notes in Computer Science Part II. LCNS, 8034 (8034) .
[20]Rodrigues M, Kormann M, Schuhler C, Tomek P. Structured light techniques for 3Dsurface reconstruction in robotic tasks. In: Kacprzyk J, editor. Advances inintelligent systems and computing. Heidelberg: Springer; 2013. p. 805 –14. [21]Rodrigues M, Kormann M, Schuhler C, Tomek P. An intelligent real time 3D visionsystem for robotic welding tasks. Mechatronics and its applications. IEEE Xplore;2013. p. 1–6.[22] Wang, Zhou; Bovik, A.C.; Sheikh, H.R.;Simoncelli, E.P. Image quality assessment: 2004 from error visibility to structural similarity". IEEE Trans Image Process. 13(4):600–612.[23]Adler A, Boublil D, Zibulevsky M. Block-based compressed sensing of images viadeep learning. In: 2017 IEEE 19th international workshop on multimedia signalprocessing. Luton: MMSP; 2017. p. 1 –6. [24]BAl-Ani MuzhirShaban, Hammouri Talal Ali. Video compression algorithm basedon frame difference approaches. Int J Soft Comput November 2011;2(No.4) . [25]Adler A. Covariance-assisted matching pursuit. IEEE Signal Process Lett Jan. 2016;23(1):149–53.[26]Dar Y, Elad M, Bruckstein AM. Optimized pre-compensating compression. IEEETrans Image Process Oct. 2018;27(10):4798 –809.M.H. Rasheed et al. Array 6 (2020) 100024
13