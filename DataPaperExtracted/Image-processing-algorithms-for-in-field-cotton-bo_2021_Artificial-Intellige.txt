Image processing algorithms for in-ﬁeld cotton boll detection in naturallighting conditions
Naseeb Singha,⁎,V . K .T e w a r ia,P . K .B i s w a sb,C . M .P a r e e ka,L . K .D h r u wa
aDepartment of Agricultural and Food Engineering, IIT Kharagpur, Kharagpur 721 302, India
bDepartment of Electronics and Electrical Communication Engineering, IIT Kharagpur, Kharagpur 721 302, India
abstract article info
Article history:Received 19 April 2021Received in revised form 5 July 2021Accepted 8 July 2021Available online 10 July 2021
Keywords:Cotton recognitionImage segmentationColor modelsColor thresholdingIn developing countries, the cotton harvesting operation is currently being performed manually. Due to the mo-notonous nature of this task and the involvement of a considerable amount of labor, this operation becomes verytedious and costly. The harvesting robots can be a good alternative for the selective picking of cotton bolls fromtheﬁeld. In this study, an attempt has been made to develop the image processing algorithms for in- ﬁeld cotton boll detection in natural lighting conditions for the cotton harvesting robot. Four image processing algorithmsnamely color difference, band ratio, YCbCr method, and chromatic aberration were proposed for the real-timesegmentation of cotton bolls under natural outdoor light conditions. The performance of developed image pro-cessing algorithms was evaluated and the experimental results revealed that the chromatic aberration methodoutperforms as compared to other developed algorithms. The chromatic aberration method showed the highestidentiﬁcation rate of 91.05% with false positive and false negative rates of 6.99% and 4.88% respectively, among allthe proposed algorithms. The highest sensitivity and speci ﬁcity were found to be 81.31% and 97.53%, respectively, using the chromatic aberration method. Overall, the chromatic aberration approach demonstrated a very prom-ising performance for in-ﬁeld cotton bolls detection under natural lighting conditions which con ﬁrms its appli- cability for the robotic cotton harvesters.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionCotton, a major source of naturalﬁber, is an important cash crop in India (Konduru et al., 2013), harvesting of which is a monotonous taskand currently being performed manually. The mechanization level ofcotton harvesting in India is nil (Mehta et al., 2019) and at the same time, labor cost is increasing as the percentage of agricultural workersto total workers decreased from 59.1% in 1991 to 54.6% in 2011 andprojected to be 40.6% in 2020 (Mehta et al., 2019). As manual picking is tedious and costly, human workers can be replaced by harvesting ro-bots which can reduce the harvesting cost ( Areﬁet al., 2011). Therefore, harvesting robots can be a good alternative for the selective picking ofcotton bolls.In the past, numerous researchers have studied robotic fruit harvest-ing; for instance, tomato harvesting (Kondo et al., 1996, 2010;Lee et al., 1999;Monta et al., 1998;Yabo et al., 2016;Zhao et al., 2016b), straw- berry harvesting (Han et al., 2012;Hayashi et al., 2010;Xiong et al., 2019;Yamamoto et al., 2010), cucumber harvesting (Henten et al., 2009, 2006, 2003, 2002), apple harvesting (Baeten et al., 2008; Bulanon and Kataoka, 2010;De-An et al., 2011;Li et al., 2016b;Nguyen et al., 2013;Silwal et al., 2017), cherry harvesting (Kondo et al., 1996;Tanigaki et al., 2008), sweet pepper harvesting (Bac et al., 2016;Hemming et al., 2014), citrus harvesting (Cai et al., 2009;Edan et al., 1990;Harrell et al., 1990;Mehta and Burks, 2014;Wang et al., 2019), kiwifruit harvesting (Mu et al., 2020;Scarfe et al., 2009; Williams et al., 2019), etc.Theﬁrst major task of a harvesting robot is to recognize the fruitusing machine vision (Willigenburg et al., 2004), in which digital im- ages of harvesting scenes captured using cameras and fruits were de-tected using image processing algorithms. In the past, imageprocessing was used for weed recognition ( Dammer, 2016;
Lee et al., 1999;Tang et al., 2016;Zhang et al., 2016;Zheng et al., 2017), plant dis- eases identiﬁcation (Camargo and Smith, 2009;Lu et al., 2017;Singh, 2019;Singh and Misra, 2017;Tewari et al., 2020), leaf area determina- tion (Chaudhary et al., 2012;Nyakwende et al., 1997;Vázquez- Arellano et al., 2018), etc.In agriculture, image processing was used by numerous re-searchers for fruit recognition.Bulanon et al. (2002a)in their study concluded that out of the RGB model, rg-chromaticity method, and Lu-minance and Red Color Difference method, rg-chromaticity method isthe most suitable for recognition of Fujiapples. In another study, Bulanon et al. (2002b)detect theFujiapples with a success rate of over 88% by using the difference between luminance and red color,Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
⁎Corresponding author.E-mail address:naseeb501@gmail.com(N. Singh).
https://doi.org/10.1016/j.aiia.2021.07.0022589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/whileJi et al. (2012)used color features as well as shape features todetect apples and achieved a recognition rate of 93.3%. For the detec-tion of strawberries,Xiong et al. (2019)used the RGB color model and achieved a precision and recall value of 0.72. Bachche and Oka (2013) tested various color space models and successfully detected sweetpeppers with a recognition rate of 84% by using the HSV color spacemodel.Zhao et al. (2016a)detected 93% of the targeted tomatoes intheir study. To pick oranges, a vision system for a harvesting robotwas developed byHannan et al. (2010)in which the R/(R + G + B) feature was used for the identiﬁcation of oranges.Areﬁet al. (2011)used a combination of RGB, HIS, and YIQ color spaces to recog-nize the ripen tomato with an accuracy of 96.36%. Using RGB colormodel,Putra and Soni (2018)developed several vegetation indicesto estimate photosynthetic pigments and Nitrogen critical level of Ro-busta Coffee plant and achieved coefﬁcient of determination (R
2)a s high as 0.8536 using their proposed indices.For a cotton-picking robot, theﬁrst task will be the recognition ofcotton bolls in theﬁeld. The correct identiﬁcation of cotton bolls in natural illumination conditions is very crucial for robotic cotton har-vesting. In-ﬁeld cotton detection is a challenging task due to varioussizes and irregular shapes of cotton bolls. Illumination conditionsalso make cotton segmentation more challenging as colors of cottonbolls can be bright during high illumination and dull during low illu-mination (during the evening or cloudy conditions), which makes cot-ton boll detection more difﬁcult. In agriculture, segmentation of theregion of interest under varying illumination is always a challengingtask because images can contain shadowed and lighted parts whichcan produce poor threshold results.Putra and Soni (2020)enhanced the measurement accuracy of chlorophyll and nitrogen contentunder uncertain natural light conditions using a consumer-gradeRGB digital camera.Putra and Soni (2017)satisfactorily assessed bio- physical properties of vegetation like plant phenology under differentillumination. In recent times, various image processing techniqueswere also successfully used for the detection of cotton bolls. Jin- shuai et al. (2011)achieved an accuracy of 90.44% for cotton boll de-tection by using YCbCr color space andﬁsher discrimination analysis. Li et al. (2016a)used the region-based semantic image segmentationmethod for in-ﬁeld cotton detection and compare their proposedmethod with other well-established methods. Their proposed methodyields the highest performances of 77.3%, 99.3%, and 97.0% for sensi-tivity, speciﬁcity, and accuracy, respectively. An unsupervised domainadaption method called NCADA for in-ﬁeld identiﬁcation of cotton bolls was proposed byLi et al. (2020)and reported efﬁciency of 86.4%.Wang et al. (2008)detect 85% of cotton bolls correctly byusing the color subtraction method and freeman chain coding to re-move noises. Using ultra-ﬁne spatial resolution UAV images,Yeom et al. (2018)detect the open cotton bolls with an accuracy of 88%.To increase the harvesting accuracy of cotton harvesting robots,there is a necessity to develop new methods and indices which canidentify the in-ﬁeld cotton bolls in different illumination conditionswith minimum image processing time as well as with minimum pos-sible false positive and false negative errors.Hence, this study aimed to develop cotton segmentation algorithmsfor natural illumination conditions such that with single threshold valuein an algorithm, cotton boll can be detected in morning, afternoon orevening time with minimum errors and evaluate them based onimage processing time and segmentation accuracy. For this, four newmethods of cotton segmentation were introduced in addition to otherexisting methods (Jin-shuai et al., 2011;Li et al., 2020;Li et al., 2016a; Wang et al., 2008) which can be used for cotton harvesting robots torecognize cotton bolls. After analyzing four color spaces, RGB andYCbCr color spaces were used for cotton segmentation in natural lightconditions, optimum thresholds were selected and morphologicalopening and closing operations were performed to remove noises andholes in the segmented binary image. The performance of proposedalgorithms was evaluated in terms of hits rate, false positive, false neg-ative, processing time, sensitivity, speciﬁcity, and accuracy.2. Methodology2.1. Data collectionIn this study, a total 135 number of RGB color images (640 × 480pixels) were captured under varying illumination conditions ( Table 1). A 0.922-megapixel digital camera (Logitech Webcam C270) that useda Complementary Metal Oxide Semiconductor (CMOS) as an image sen-sor having a focal length of 4.6 mm and diagonal ﬁeld of view of 55°, and a laptop (8 GB RAM, Intel Core i5 CPU, and Windows 10 operating sys-tem) were used for the cotton image acquisition process. We used dig-ital camera with a conﬁguration ofﬁxed focus, automatic exposure time,and custom white balance mode. Image acquisition and image process-ing were performed in Matlab® ver. R2017 (Mathworks Inc., Natick,MA. USA) (Beale et al., 2017).2.2. Color model analysisThe cotton bolls can be detected easily based on their color features.The commonly popular color space models for feature segmentationfrom color images includes the RGB ( Lurstwut and Pornpanomchai, 2017;Tewari et al., 2020), the HSV (Yang et al., 2015), the L*a*b (Hu
Table 1Number of images captured with varying distance and illumination.Time, h Avg. Lux, lx Distance, mm No. of images8:00–11:00 48,003 500 –700 15700–900 15900–1500 15 11:00–15:00 67,040 500 –700 15700–900 15900–1500 15 15:00–17:00 51,200 500 –700 15700–900 15900–1500 15NomenclatureI
i,j Pixel intensity at (i, j) pixel locationI
R(i,j) Red channel pixel intensity at (i, j) pixel locationI
G(i,j) Green channel pixel intensity at (i, j) pixel locationI
B(i,j) Blue channel pixel intensity at (i, j) pixel locationH (i, j) Hue at (i, j) pixel locationS (i, j) Saturation at (i, j) pixel locationV (i, j) Value at (i, j) pixel locationr
RB Ratio of red and blue channelr
BG Ratio of blue and green channelr
RG Ratio of red and green channelT Threshold valueCA Chromatic aberrationTP True Positive, cotton pixel segmented as cotton pixelTN True Negative, background pixel segmented as back-ground pixelFP False Positive, background pixel segmented as cottonpixelFN False Negative, cotton pixel segmented as backgroundpixelTDC True detected cotton, cotton boll segmented as cottonbollFDC False detected cotton, background segmented as cottonbollMC Missed cotton, Cotton boll segmented as backgroundN. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
143et al., 2015;Zhao et al., 2016a), and the YCbCr (Jin-shuai et al., 2011) color models. The selection of a proper color space model is very crucialfor the successful detection of the particular object from the given colorimages. In past, various researchers have implemented different colorspace models for various agricultural applications ( Bachche and Oka, 2013;Hamuda et al., 2017;Tewari et al., 2020). In these studies, image processing was not carried out under natural illumination condi-tions and hence, challenges for image segmentation under natural illu-mination were not addressed. Therefore, in this study, four-colorspace models, i.e., RGB, normalized RGB (sRGB), HSV, and YCbCr wereanalyzed to determine the suitable color space model for in- ﬁeld cotton bolls segmentation under natural illumination conditions. Initially, thecaptured RGB images were converted into different color spaces usingthe Eqs.(1)–(7). Then the color channels of cotton bolls, stems, andleaves were extracted for each color space model and plotted as aboxplot for analysis as shown inFig. 1.F r o mFig. 1, it can be observed that the cotton bolls color components are comparatively more distin-guishable from that of the stems and leaves in the case of RGB andYCbCr color spaces as compared to the sRGB and HSV color spacemodel. Therefore, in this study, the RGB and the YCbCr color spaceswere selected for developing the image processing algorithms for in-ﬁeld cotton detection.The Eqs.(1)–(5)are used to transform the RGB color space into HSVcolor space (Camargo and Smith, 2009). Given that (I
i,j) exists in RGB color space, then:mxi,jðÞ¼max I R i,jðÞ,IG i,jðÞ,IB i,jðÞ/C0/C1 ð1Þmn
i,jðÞ¼min I R i,jðÞ,IG i,jðÞ,IB i,jðÞ/C0/C1 ð2ÞHi,jðÞ ¼
60/C2I Gi,jðÞ−IBi,jðÞ/C0/C1mx−mnI
Ri,jðÞ>maxI Gi,jðÞ,IBi,jðÞ/C0/C1
180/C2I Bi,jðÞ−IRi,jðÞ/C0/C1mx−mnI
Gi,jðÞ>maxI Ri,jðÞ,IBi,jðÞ/C0/C1
300/C2I Ri,jðÞ−IGi,jðÞ/C0/C1mx−mnI
Bi,jðÞ>maxI Ri,jðÞ,IGi,jðÞ/C0/C10BBBBBBB@1CCCCCCCAð3ÞSi,jðÞ ¼
mx−mnmxhi ð4ÞVi,jðÞ ¼mx½/C138 ð 5ÞThe normalized color r, g, and b in RGB color space are de ﬁned using Eq.(6)(Yang et al., 2015), where R, G, and B are the color components ofthe cottonﬁeld image.r¼
RRþGþB,g¼ GRþGþB,b¼ BRþGþB,rþgþb¼1ð6ÞThe conversion between RGB color space and YCbCr color space isdescribed by Eq.(7)(Shaik et al., 2015).
Fig. 1.Color channels of cotton bolls, stems, and leaves in RGB, sRGB, HSV, and YCbCr color space models.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
144YCbCr264375¼0128128264375þ0:299 0:587 0:114−0:169−0:331 0:5000:500−0:419−0:081264375RGB264375ð7Þwhere, Yϵ[16,235], Cbϵ[16,240], Crϵ[16,240].2.3. Cotton bolls segmentation algorithmsIn this study, two-color space models, i.e., RGB and YCbCr were se-lected and further utilized for developing in- ﬁeld cotton detection algo- rithms. In past, many researchers used RGB ( Bulanon et al., 2002b;Ji et al., 2016;Xiang et al., 2011;Xu and Ying, 2004;Zhao et al., 2005) and YCbCr (Moallem et al., 2017;Sabzi et al., 2020, 2017) color space models to extract the region of interest using color thresholding seg-mentation in different agricultural applications. A total of four imageprocessing algorithms were developed, out of which, three algorithmsbased on the color difference method, the color component ratiomethod, and the chromatic aberration method, were developed by uti-lizing the RGB color model, and one algorithm was developed by utiliz-ing the YCbCr color model. The workingﬂ
owcharts of all the proposed cotton bolls detection algorithms were shown in Fig. 9. The details of each cotton boll detection algorithm are given in the following sections.2.3.1. Color difference methodTo develop the cotton bolls detection algorithm based on the colordifference method, initially, the distribution of R vs. G, R vs. B, and Bvs. G for color data of cotton bolls, leaves, and stems were plotted, asshown inFig. 2. From thisﬁgure, it can be observed that the distributionof B vs. G values for cotton bolls are isolated from the B vs. G values ofleaves and stem whereas, in R vs. G and R vs. B color value distribution,cotton bolls values are overlapped with leaves values which may causethe false detection of leaves pixels as cotton bolls pixels. Thus, the B andG values can be utilized for cotton bolls segmentation from the back-ground. Therefore, a new parameter was developed by subtracting thegreen component from the blue component, i.e., B-G, and the thresholdfunction for cotton bolls segmentation using (B-G) was de ﬁned by Eq.(8).fx,yðÞ ¼1, 1, 1ðÞ,if B−GðÞ≥T0, 0, 0ðÞ,if B−GðÞ<T/C26 ð8ÞWhere, T is the threshold value for (B-G) and R, G, B is the color com-ponent value of the image pixel (x, y).2.3.2. Color component ratio methodIn this method, the ratios of R, G, and B components of cotton, stem,and leaves were studied to segment cotton bolls from the background.Color component ratios for cotton bolls, stems, and leaves were com-puted as given in Eqs.(9) to (11).rRB¼R=B ð9Þr
BG¼B=G ð10Þr
RG¼R=G ð11ÞThese ratios were plotted against pixels' numbers as shown in Fig. 3 and it can be observed that ther
BGvalues for cotton bolls were isolatedfrom ther
BGvalues of stem and leaves. Hence, the r BGvalues were fur- ther utilized for segmenting the cotton bolls from the stem and leaves.The threshold function for cotton bolls segmentation using r
BGvalue was deﬁned by Eq.(12).fx,yðÞ ¼1, 1, 1ðÞ0, 0, 0ðÞ/C26if r
BG≥Tif r
BG<T ð12ÞWhere T is the threshold value for r
BG.2.3.3. Chromatic aberration methodIn this method, initially, the red (R), green (G), and blue (B) compo-nents of the cotton bolls, stem, and leaves for all illumination conditionswere extracted from the original captured RGB image and plotted, asshown inFig. 4. From thisﬁgure, it can be observed that the R, G, andB values of cotton, stem, and leaves are not isolated completely as theG and B components of leaves are overlapped with the cotton colorcomponents. Based on this information, the various combinations of R,G, and B components were tested to isolate the cotton bolls fromstems and leaves. Finally, a chromatic aberration equation (Eq. (13)) was found most suitable for differentiating the cotton bolls from thestem and leaves.CA¼B−
R3−G3 ð13ÞWhere CA is chromatic aberration value; R denotes red component inRGB color space; G denotes green component in RGB color space, andB denotes blue component in RGB color space.The CA values of the values of cotton, stem, and leaves for all illumi-nation conditions, which were calculated using Eq. (13), are shown in Fig. 5. From thisﬁgure, an apparent deviation of CA values of the cottonbolls can be observed from that of the stem and leaves. Hence, the cot-ton bolls could be successfully segmented from the stem and leaves
Fig. 2.Distribution of R vs. G, R vs. B, and B vs. G of cotton bolls, stems, and leaves.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
145based on their CA values. The threshold function for cotton bolls seg-mentation using CA values was deﬁned by Eq.(14).fx,yðÞ ¼1, 1, 1ðÞ0, 0, 0ðÞ/C26if CA≥Tif CA<T ð14ÞWhere, T is the threshold value of chromatic aberration, and f (x, y) isthe value of the pixel (x, y) in theﬁnal binary image obtained after im-plementation of the color thresholding technique. In the binary image,the white color portion (the pixel value is 1) represents the cotton bolls,and the black color portion (the pixel value is 0) represents the back-ground, i.e., stem and leaves.2.3.4. YCbCr color modelTo develop the cotton bolls detection algorithm based on the YCbCrcolor space model, initially, the RGB image was ﬁrst converted into YCbCr color space and Y, Cb, and Cr components of the cotton bolls,stem, and leaves were plotted as shown in Figs. 6–8. FromFig. 6,i tc a n be observed that the distribution of Y vs. Cb values of cotton bolls isisolated from that of stem and leaves. So, cotton bolls can be segmentedfrom the background using appropriate thresholding values of Y and Cbcomponents.2.4. Threshold values selectionThe selection of appropriate threshold values is very crucial forproper segmentation of input images and successful recognition of thecotton bolls using the proposed algorithms. In past, many researchersused various threshold methods for the segmentation of region of inter-est from an image (Bulanon et al., 2002b;Hamuda et al., 2017;Hu et al., 2015;Ireri et al., 2019;Montalvo et al., 2013;Singh, 2019;Tsai and Tseng, 2012;Vitzrabin and Edan, 2016;Yang et al., 2015). In this study, the color thresholding method was applied to segment imagesand recognize cotton bolls.With a purpose to minimize false detection, optimum thresholdvalues were found out using three performance indices namely preci-sion, recall, and F1 score (Rodríguez et al., 2020) which are formally expressed by Eqs.(15)–(17).
Fig. 3.R/B, R/G, and B/G ratios for cotton, stem, and leaves.
Fig. 4.Distribution of RGB values of cotton, leaves, and stem. (Rc –red color component of the cotton; Rl –red color component of the leaves; Rs –red color component of the stem, etc.)N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
146Precision¼TPTPþFP ð15ÞRecall¼
TPTPþFN ð16ÞF1¼
2/C2Precision/C2RecallðÞPrecisionþRecall ð17ÞTP is the number of cotton bolls predicted by the algorithm correctly.FP is the background predicted by the algorithm incorrectly as cottonbolls and FN is the number of cotton bolls predicted by the algorithm in-correctly as background.Precision measures the fraction of correctly classi ﬁed cotton bolls among the total classiﬁed as cotton bolls while recall measures the frac-tion of correctly classiﬁed cotton bolls among the actual number of cot-ton bolls present. To combine properties of precision and recall into asingle measure, the F1 score was used in this study. For perfect precisionand recall values, the F1 value should be 1.In each proposed algorithm, different threshold values were used,and TP, FP, and FN were counted for that threshold which was used tocalculate precision, recall, and F1 score for that particular thresholdvalue.With a primary aim to identify cotton bolls correctly, for an algo-rithm, if 70% pixels of a cotton boll are correctly classi ﬁed as cotton boll pixels, then in the present study, it was assumed that the algorithmclassiﬁed that cotton boll correctly. Precision, recall, and F1 score valueswere drawn against threshold values on abscissas as shown inFigs. 10–12. The point where precision, recall, and F1 score curvesmeet is the value at which TP is higher and FP is lower and was consid-ered as an optimum threshold value. Table 2shows the optimum threshold values for each algorithm used in the present study.To segment the cotton bolls from the background (stem and leaves),an appropriate threshold value of the CA, i.e., T= 37, was chosen from
Fig. 5.Distribution of chromatic aberration values of cotton, leaves, and stem.
Fig. 6.Distribution of Cb and Y components of cotton, stem, and leaves.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
147Fig. 12, as the overlapping of CA values of cotton bolls and background(stem and leaves) was minimum for CA > 37.2.5. Opening and closing morphological operationsAfter applying the thresholding operations to the input color image(Fig. 13(A)), a binary image (Fig. 13(B)) with each white pixel representing cotton boll was obtained. However, this contained noisesin the shape of small clusters of pixels that do not belong to cotton butwere detected as cotton pixels, and some cotton pixels were split be-tween several clusters, instead of being one cluster. The closing mor-phological operation was then applied to connect close clusters, usingdilation followed by erosion implemented with a 5 × 5 pixel squareneighborhood (Fig. 13(C)). To remove noises, a morphological openingoperation by erosion followed by dilation with a neighborhood of 5 × 5pixel square was applied (Fig. 13(D)). Due to randomness in the orien-tation of noises, a square shape was used in this study.2.6. Ground truth images of cotton bollsGround truth images are the ones in which regions of interest (ROI)are segmented using a more accurate method as compare to proposemethod.Hu et al. (2015)compared their proposed automatic segmen-tation algorithm for bananas with the manually segmented bananas re-gion, which acts as ground truth in their study. Tsai and Tseng (2012) compared the accuracy of their proposed color detection method withthe traditional color detection method based on HSL color space.Bachche and Oka (2013)obtained ground truth distance data by manu-ally measuring the distance for the accuracy testing of depth coordinatein their study.
Fig. 7.Distribution of Cr and Y components of cotton, stem, and leaves.
Fig. 8.Distribution of Cb and Cr components of cotton, stem, and leaves.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
148In the present study, for the performance evaluation of proposedsegmentation methods, ground truth images of cotton bolls are needed.Therefore, cotton bolls in RGB images were manually segmented usingthe Adobe Photoshop CC software (Adobe Systems, San Jose, CA), andpixels were counted for each cotton boll.2.7. Performance evaluation of developed algorithmsIn this study, two approaches were used to evaluate the perfor-mance of the proposed algorithms. In the ﬁrst approach, true detected cotton (TDC), false detected cotton (FDC), and missed cotton (MC)were counted. True detected cotton (TDC) indicates an object in the seg-mented image which is recognized as cotton boll when it is a cotton boll.False detected cotton (FDC) indicates an object in the segmented imagewhich is recognized as cotton boll when it is not a cotton boll. Missedcotton (MC) indicates that the algorithm missed a cotton boll to recog-nize it as a cotton boll and treated it as background. Based on this, hitsrate, false-positive rate, and false-negative rate were calculated byusing Eqs.(18)–(20).Hits rate¼
TDCTDCþMC ð18ÞFalse positive¼
FDCTDCþMC ð19ÞFalse negative¼
MCTDCþMC ð20ÞIn the second approach, the performance was evaluated by using thenumber of pixels of true detected cotton and background. In this
Fig. 9.Cotton boll segmentationﬂowchart.
Fig. 10.Precision, recall, and F1 score values against threshold values for the (B-G) method.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
149approach, sensitivity (Se), speciﬁcity (Sp), and accuracy (Ac) termswere used for performance evaluation ( Omid, 2011) and calculated by using Eqs.(21)−(23).Sensitivity¼
TPTPþFN ð21ÞSpecificity¼
TNFPþTN ð22ÞAccuracy¼
TPþTNTPþFNþFPþTN ð23ÞWhere TP is the cotton's pixels predicted by the algorithm correctly.TN is the background's pixels predicated by the algorithm correctly. FPis the background's pixels predicted by the algorithm incorrectly as cot-ton's pixels. And, FN is the cotton's pixels predicted by algorithm incor-rectly as background's pixels.The sensitivity (Eq.(21)) is thus a measure of the accuracy of algo-rithms in true cotton detection, and speci ﬁcity (Eq.(22)) is a measure of the accuracy of algorithms in true background detection. Accuracy(Eq.(23)) was found out by dividing the number of pixels identi ﬁed correctly and total number of pixels. To measure the effectiveness androbustness of the proposed algorithms, mean values ( μ) and standard deviations (σ) were also calculated.3. Results and discussionThe RGB color images of cotton plants, captured in different illumi-nation conditions, were selected for the performance evaluation of pro-posed cotton bolls detection algorithms. The proposed algorithms wereimplemented in Matlab® ver. R2017 (Mathworks Inc., Natick, MA. USA)and cotton bolls segmented results are illustrated in Fig. 14. It can be ob- served that all the cotton bolls having a size above 500 pixels are cor-rectly identiﬁed by all the proposed methods. However, there areover-segmentation (for example: for (B-G) method (3
rdrow, 1stcol- umn, and 3
rdrow, 2ndcolumn), CA method (4throw, 4thcolumn), B/G method (5
throw, 3rdcolumn), YCbCr method (6throw, 3rdcolumn) and under-segmentation (for example: for (B-G) method (3
rdrow, 6th
column), B/G method (5throw, 6thcolumn, and 5throw, 2ndcolumn), YCbCr method (6
throw, 6thcolumn and 6throw, 1stcolumn)) in many
Fig. 11.Precision, recall, and F1 score values against threshold values for the B/G method.
Fig. 12.Precision, recall, and F1 score values against threshold values for the CA method.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
150cases. Some cotton bolls, which were small in size, were missed, whilein some cases background was identiﬁed as cotton incorrectly. All these results were quantiﬁed inTables 3, 4, and 5. The comparison between the correctly detected cotton bollsusing proposed algorithms and manually counted cotton bolls isgiven inTable 3. In this comparison, over-segmentation and under-segmentation were not taken into consideration. From Table 3, it can be observed that the chromatic aberration method correctly identi ﬁed 468 cotton bolls out of 514 cotton bolls followed by the YCbCr method,using which, 461 cotton bolls were identiﬁed correctly. In all three sub- classes i.e., 8:00–11:00 h, 11:00–15:00 h, and 15:00–17:00 h, the chro- matic aberration method outperforms the other proposed methods inthe identiﬁcation of cotton bolls.The performance of proposed algorithms was also compared interms of hits, false positive, and false negative ( Table 4). It can be ob- served that more than 86% of cotton bolls can be identi ﬁed correctly using proposed algorithms with a false-positive rate below 14%. Thehighest hits rate (91.05%) was achieved with the chromatic aberrationmethod followed by the YCbCr method (89.68%). By changing thethreshold value, the hit rate for this method can be increased furtherbut with an increased rate of false-positive. Jin-shuai et al. (2011) achieved an accuracy of 90.44% using YCbCr color space. In thisstudy, an accuracy of 88.52% was achieved using the color differencemethod whereasWang et al. (2008)in their study, achieved an accu- racy of 88.09% using the color subtraction method and freeman chaincoding.
Fig. 13.Morphological opening and closing operations on the binary image. (A) RGBimage. (B) Binary image after thresholding (C) Binary image after the morphologicalclosing operation (D) Binary image after the morphological opening operation
Fig. 14.An example of cotton bolls detection results from the proposed methods for a set of images captured in various light conditions and various complex bac kgrounds. (1strow - raw RGB images; 2
ndrow–ground truth images; 3rdrow–(B-G) method; 4throw–CA method; 5throw–B/G method; 6throw–YCbCr method).N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
151The performance of proposed algorithms in terms of sensitivity,speciﬁcity, and accuracy is given inTable 5. FromTable 5, it can be ob- served that all the proposed algorithms achieved sensitivity, speci ﬁcity, and accuracy of more than 79%, 97%, and 94% respectively. Li et al. (2016a)detected cotton using region-based semantic image segmenta-tion method and achieved 77.3%, 99.3%, and 97.0% sensitivity, speci ﬁc- ity, and accuracy values respectively on forward images whereas, inthe present study, the performance of chromatic aberration was foundto be higher among other proposed methods in the present studywith sensitivity, speciﬁcity and accuracy values of 88.69%, 97.53%, and95.79% respectively.The performance of the proposed algorithms was also evaluatedduring morning, afternoon, and evening time. For this, a total 60 num-ber of images were captured for morning time (with same settings asshown inTable 1), and 12 groups (each havingﬁve randomly selected images) were formed; and the hits rate, false positive, and false negativewere counted for each group of images using the proposed algorithms.The same process was repeated for afternoon and evening time images.The hits rate for proposed methods for the morning, afternoon, and eve-ning time is shown inFig. 15.F r o mt h i sﬁgure, it can be observed that the chromatic aberration method outperforms other proposed methodsfor correctly detecting the cotton bolls. The false-positive rates duringthe morning, afternoon, and evening time is shown in Fig. 16.F r o m thisﬁgure, it can be observed that false-positive rates were higher dur-ing the afternoon as compared to evening and morning time for all pro-posed algorithms. This was because of high illumination during theafternoon and due to the reﬂection of light from stem and leaves,these were considered as cotton bolls by algorithms. The sample casesof false-positive detection due to the reﬂectance of light from the leaves are shown inFig. 17.Fig. 17(A) and (B) show the cotton plant images inRGB and YCbCr color models respectively and Fig. 17(C) and (D) show the segmented images obtained using YCbCr and CA method
Fig. 15.Comparison of hits rate of proposed methods during the morning, afternoon, and evening.
Fig. 16.Comparison of false-positive rate of proposed methods during the morning, afternoon, and evening.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
152respectively. The false-negative rates during the morning, afternoon,and evening time is shown inFig. 18. From thisﬁgure, it can be observed that the false-negative rate was higher during the evening time ascompared to the morning and the afternoon time. It may be becauseof less illumination during the evening, some of the cotton bolls were in-correctly identiﬁed as stem or leaves or soil.The performance of developed cotton boll detection algorithms wasalso compared based on their processing time for each image ( Table 6). The results revealed that the chromatic aberration method took lesserprocessing time as compared to other developed algorithms. This maybe because, in the chromatic aberration method, the input imagedoesn't need to be converted into a double-precision data type and un-like in the YCbCr method, no color space conversion is needed. FromTable 6, it can be observed that the processing time also varies with dif-ferent images for the same algorithm (for example, for the chromaticaberration method, the minimum processing time was 0.327 s and the
Fig. 17.False-positive detection due to re ﬂectance of light from leaves.
Fig. 18.Comparison of false-negative rate of proposed methods during the morning, afternoon, and evening.
Table 2Selected optimum threshold values for proposed methods.Method Optimum thresholdColor difference method, B-G 20Band ratio method, B/G 0.90Chromatic Aberration method 37YCbCr color space method Cb = 150; Y = 145N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
153maximum time was 0.429 s), it was observed that with an increase inthe number of cotton bolls, processing time increases.4. ConclusionsThis study proposed the image processing algorithms for in- ﬁeld cotton boll detection in natural lighting conditions. Initially, four-colorspace models, i.e., RGB, sRGB, HIS, and YCbCr were tested to determinethe suitable color space model for cotton bolls detection. Out of them,the RGB and the YCbCr color spaces were selected and further utilizedfor developing the in-ﬁeld cotton detection algorithms. A total of fourimage processing algorithms were developed for the real-time segmen-tation of cotton bolls under natural outdoor light conditions, out ofwhich, three cotton detection algorithms based on the color differencemethod, the color component ratio method, and the chromaticaberration method, were developed by utilizing the RGB color model,and one algorithm was developed by utilizing the YCbCr color model.The performance of developed image processing algorithms was evalu-ated and the experimental results revealed that the chromatic aberra-tion method outperforms as compared to other developed algorithms.The chromatic aberration method showed the highest identi ﬁcation rate of 91.05% with false positive and false negative rates of 6.99% and4.88% respectively, among all the proposed algorithms. The highest sen-sitivity and speciﬁcity were found to be 81.31% and 97.53%, respectively,using the chromatic aberration method. As hits rate for all proposedmethods was found to be above 86% with a maximum false-positiverate below 14%, therefore, despite several challenges, such as variationsin light conditions, complex background, similarities in some color fea-tures, although all the developed algorithms demonstrated good perfor-mance for cotton boll detection but the chromatic aberration methodoutperform as compared to other developed algorithms. Overall, thechromatic aberration approach demonstrated a propitious performancefor in-ﬁeld cotton bolls detection under natural lighting conditionswhich conﬁrms its applicability for the robotic cotton harvesters.The proposed algorithms have two limitations, i.e., the false positivedetection due to the reﬂection of light and the inability in separating theoverlapped cotton bolls. Therefore, although the experimental resultsindicated that the proposed methods demonstrate a high degree of ro-bustness and accuracy under varying natural outdoor light conditions,further study is required to reduce the false positive due to re ﬂection of light by stem or leaves, false positive due to sky or clouds and splittingthe overlapped cotton bolls.Declaration of Competing InterestThe authors declared that there is no conﬂict of interest.AcknowledgmentTheﬁnancial support received from the Indian Institute of Technol-ogy Kharagpur, Kharagpur, West Bengal, India.References
Areﬁ, A., Motlagh, A.M., Mollazade, K., Teimourlou, R.F., 2011. Recognition and localization of ripen tomato based on machine vision. Aust. J. Crop. Sci. 5, 1144 –1149. Bac, C.W., Roorda, T., Reshef, R., Berman, S., Hemming, J., van Henten, E.J., 2016. Analysis ofa motion planning problem for sweet-pepper harvesting in a dense obstacle environ-m e n t .B i o s y s t .E n g .1 4 6 ,8 5 –97.https://doi.org/10.1016/j.biosystemseng.2015.07.004 . Bachche, S., Oka, K., 2013. Distinction of green sweet peppers by using various color spacemodels and computation of 3 dimensional location coordinates of recognized greensweet peppers based on parallel stereovision system. J. Syst. Des. Dyn. 7, 178 –196. https://doi.org/10.1299/jsdd.7.178 . Baeten, J., Donné, K., Boedrij, S., Beckers, W., Claesen, E., 2008. Autonomous fruit pickingmachine: a robotic apple harvester. Springer Tracts Adv. Robot. 42, 531 –539. https://doi.org/10.1007/978-3-540-75404-6_51 .Table 3Comparison of the number of cotton bolls recognized using proposed algorithms against manually counted cotton bolls.Time, h Avg. Lux, lx Distance, mm No. of cotton bolls counted manually (B-G) Method CA Method B/G Method YCbCr Method8:00–11:00 48,003 500 –700 49 44 45 43 44700–900 55 48 49 48 49900–1500 58 52 53 50 53Sub-Total162 144 147 141 146 11:00–15:00 67,040 500 –700 71 63 66 63 65700–900 63 57 58 56 56900–1500 46 39 41 39 40Sub-Total180 159 165 158 161 15:00–17:00 51,200 500 –700 66 58 60 57 59700–900 47 41 42 39 42900–1500 59 53 54 51 53Sub-Total172 152 156 147 154 Total 514 455 468 446 461
Table 4Performance of cotton segmentation algorithms based on the number of cotton bollrecognized.Method Hits (%) False-positives (%) False-negative (%)Color difference 88.52 13.12 7.52YCbCr 89.68 9.97 6.30Band ratio 86.77 12.47 9.35Chromatic aberration 91.05 6.99 4.88
Table 5Performance of cotton segmentation algorithms based on the number of pixel counts.Algorithm Sensitivity (Se) Speci ﬁcity (Sp) Accuracy (Ac)μσ μ σ μσColor difference 79.49 12.84 97.38 0.48 94.96 1.33YCbCr 80.8 9.70 97.2 0.41 94.78 0.43Band ratio 84.09 15.57 97.17 0.29 95.66 0.73Chromatic aberration88.699.3497.530.3495.790.52
Table 6Comparison of processing time by proposed methods for processing each image.Method Min. time (s) Max. Time (s) Avg. time (s)Color difference 0.479 0.573 0.511Band ratio 0.272 0.379 0.316YCbCr 0.401 0.455 0.427Chromatic aberration 0.327 0.429 0.364N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
154Beale, M.H., Hagan, M.T., Demuth, H.B., 2017. MATLAB: Neural Network Toolbox User ‘S Guide.Bulanon, D.M., Kataoka, T., 2010. Fruit detection system and an end effector for roboticharvesting of Fuji apples. Agric. Eng. Int. CIGR J. 12, 203 –210. Bulanon, D., Kataoka, T., Ota, Y., Hiroma, T., 2002a. A color model for recognition of applesby a robotic harvesting system. J. JAPANESE Soc. Agric. Mach. 64, 123 –133.https:// doi.org/10.11357/jsam1937.64.5_123 . Bulanon, D., Kataoka, T., Ota, Y., Hiroma, T., 2002b. A segmentation algorithm for the au-tomatic recognition of Fuji apples at harvest. Biosyst. Eng. 83, 405 –412.https://doi. org/10.1006/bioe.2002.0132. Cai, J., Zhou, X., Wang, F., 2009. Obstacle identiﬁcation of citrus harvesting robot. Trans. Chinese Soc. Agric. Mach. 40, 171 –175. Camargo, A., Smith, J.S., 2009. An image-processing based algorithm to automaticallyidentify plant disease visual symptoms. Biosyst. Eng. 102, 9 –21.https://doi.org/ 10.1016/j.biosystemseng.2008.09.030 . Chaudhary, P., Godara, S., Cheeran, A.N., Chaudhari, A.K., 2012. Fast and accurate methodfor leaf area measurement. Int. J. Comput. Appl. 49, 22 –25.https://doi.org/10.5120/ 7655-0757.Dammer, K., 2016. Real-time variable-rate herbicide application for weed control in car-rots. Weed Res. 56, 237 –246.https://doi.org/10.1111/wre.12205 . De-An, Z., Jidong, L., Wei, J., Ying, Z., Yu, C., 2011. Design and control of an apple harvestingrobot. Biosyst. Eng. 110, 112 –122.https://doi.org/10.1016/j.biosystemseng.2011. 07.005.Edan, Y., Flash, T., Shmulevich, I., Sarig, Y., Peiper, U.M., 1990. An algorithm deﬁning the motions of a citrus picking robot. J. Agric. Eng. Res. 46, 259 –273. Hamuda, E., Ginley, B.M., Glavin, M., Jones, E., 2017. Automatic crop detection under ﬁeld conditions using the HSV colour space and morphological operations. Comput. Elec-tron. Agric. 133, 97–107.https://doi.org/10.1016/j.compag.2016.11.021 . Han, K.S., Kim, Si Chan, Lee, Y.B., Kim, Sang Chul, Im, D.H., Choi, H.K., Hwang, H., 2012.Strawberry harvesting robot for bench-type cultivation. J. Biosyst. Eng. 37, 65 –74. https://doi.org/10.5307/jbe.2012.37.1.065 . Hannan, M.W., Burks, T.F., Bulanon, D.M., 2010. A machine vision algorithm combining adaptive segmentation and shape analysis for orange fruit detection. Agric. Eng. Int.CIGR J. XI, 1–17.Harrell, R.C., Adsit, P.D., Munilla, R.D., Slaughter, D.C., 1990. Robotic picking of citrus. Robotica 8, 269–278.Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J., Kurita, M.,2010. Evaluation of a strawberry-harvesting robot in a ﬁeld test. Biosyst. Eng. 105, 160–171.https://doi.org/10.1016/j.biosystemseng.2009.09.011 . Hemming, J., Bac, C.W., Van Tuijl, B.A.J., Barth, R., Bontsema, J., 2014. A robot for harvesting
sweet-pepper in greenhouses. Proceed. Int. Conf. Agric. Eng. 6 –10. Henten, V.E.J., Hemming, J., Van Tuijl, B.A.J., Kornet, J.G., Meuleman, J., Bontsema, J., VanOs, E.A., 2002. An autonomous robot for harvesting cucumbers in greenhouses.Auton. Robot. 13, 241 –258.https://doi.org/10.1023/A:1020568125418 . Henten, V.E.J., Van Tuijl, B.A.J., Hemming, J., Kornet, J.G., Bontsema, J., Van Os, E.A., 2003.Field test of an autonomous cucumber picking robot. Biosyst. Eng. 86, 305 –313. https://doi.org/10.1016/j.biosystemseng.2003.08.002 . Henten, V.E.J., Van Tuijl, B.A.J., Hoogakker, G.J., Van Der Weerd, M.J., Hemming, J., Kornet,J.G., Bontsema, J., 2006. An autonomous robot for de-lea ﬁng cucumber plants grown in a high-wire cultivation system. Biosyst. Eng. 94, 317 –323.https://doi.org/10.1016/ j.biosystemseng.2006.03.005. Henten, V.E.J., Van'’t Slot, D.A., Hol, C.W.J., Van Willigenburg, L.G., 2009. Optimal manipu-lator design for a cucumber harvesting robot. Comput. Electron. Agric. 65, 247 –257. https://doi.org/10.1016/j.compag.2008.11.004 . Hu, M.-H., Dong, Q.-L., Liu, B.-L., Pan, L.-Q., Walshaw, J., 2015. Image segmentation of ba-nanas in a crate using a multiple threshold method. J. Food Process Eng., 1 –6https:// doi.org/10.1111/jfpe.12233. Ireri, D., Belal, E., Okinda, C., Makange, N., Ji, C., 2019. A computer vision system for defectdiscrimination and grading in tomatoes using machine learning and image process-ing. Artif. Intell. Agric. 2, 28 –37.https://doi.org/10.1016/j.aiia.2019.06.001 . Ji, W., Zhao, D., Cheng, F., Xu, B., Zhang, Y., Wang, J., 2012. Automatic recognition visionsystem guided for apple harvesting robot. Comput. Electr. Eng. 38, 1186 –1195. https://doi.org/10.1016/j.compeleceng.2011.11.005 . Ji, W., Meng, X., Tao, Y., Xu, B., Zhao, D., 2016. Fast segmentation of colour apple imageunder all-weather natural conditions for vision recognition of picking robots. Int.J. Adv. Robot. Syst. 1 –9.https://doi.org/10.5772/62265 . Jin-shuai, L.I.U., Hui-cheng, L.A.I., Zhen-hong, J.I.A., 2011. Image segmentation of cottonbased on ycbccr color space and ﬁsher discrimination analysis. Acta Agron. Sin. 37, 1274–1279.https://doi.org/10.3724/SP.J.1006.2011.01274 . Kondo, N., Nishitsuji, Y., Ling, P.P., Ting, K.C., 1996. Visual feedback guided robotic cherrytomato harvesting. Trans. Am. Soc. Agric. Eng. 39, 2331 –2338.https://doi.org/ 10.13031/2013.27744.Kondo, N., Yata, K., Iida, M., Shiigi, T., Monta, M., Kurita, M., Omori, H., 2010. Developmentof an end-effector for a tomato cluster harvesting robot. Eng. Agric. Environ. Food 3,20–24.https://doi.org/10.1016/S1881-8366(10)80007-2 . Konduru, S., Yamazaki, F., Paggi, M., 2013. A study of mechanization of cotton harvesting in India and its implications. J. Agric. Sci. Technol. B 3, 789.Lee, W.S., Slaughter, D.C., Giles, D.K., 1999. Robotic weed control system for tomatoes.Precis. Agric. 1, 95–113.https://doi.org/10.1023/A:1009977903204 . Li, Y., Cao, Z., Lu, H., Xiao, Y., Zhu, Y., Cremers, A.B., 2016a. In- ﬁeld cotton detection via region-based semantic image segmentation. Comput. Electron. Agric. 127, 475 –486. https://doi.org/10.1016/j.compag.2016.07.006 . Li, J., Karkee, M., Zhang, Q., Xiao, K., Feng, T., 2016b. Characterizing apple picking patternsfor robotic harvesting. Comput. Electron. Agric. 127, 633
–640.https://doi.org/ 10.1016/j.compag.2016.07.024 .Li, Y., Cao, Z., Lu, H., Xu, W., 2020. Unsupervised domain adaptation for in- ﬁeld cotton boll status identiﬁcation. Comput. Electron. Agric. 178, 1 –7.https://doi.org/10.1016/j. compag.2020.105745.Lu, J., Hu, J., Zhao, G., Mei, F., Zhang, C., 2017. An in- ﬁeld automatic wheat disease diagno- sis system. Comput. Electron. Agric. 142, 369 –379.https://doi.org/10.1016/j. compag.2017.09.012.Lurstwut, B., Pornpanomchai, C., 2017. Image analysis based on color, shape and texturefor rice seed (Oryza sativaL.) germination evaluation. Agric. Nat. Resour. 51,383–389.https://doi.org/10.1016/j.anres.2017.12.002 . Mehta, S.S., Burks, T.F., 2014. Vision-based control of robotic manipulator for citrus har-vesting. Comput. Electron. Agric. 102, 146 –158.https://doi.org/10.1016/j. compag.2014.01.003.Mehta, C.R., Chandel, N.S., Jena, P.C., Jha, A., 2019. Indian agriculture counting on farm mechanization. Agric. Mech. Asia, Africa Lat. Am. 84 –89. Moallem, P., Serajoddin, A., Pourghassem, H., 2017. Computer vision-based apple gradingfor golden delicious apples based on surface features. Inf. Process. Agric. 4, 33 –40. https://doi.org/10.1016/j.inpa.2016.10.003 . Monta, M., Kondo, N., Ting, K.C., 1998. End-effectors for tomato harvesting robot. Artif.Intell. Rev. 12, 11–25.https://doi.org/10.1023/a:1006595416751 . Montalvo, M., Guerrero, J.M., Romeo, J., Emmi, L., Guijarro, M., Pajares, G., 2013. Automaticexpert system for weeds/crops identi ﬁcation in images from maizeﬁelds. Expert Syst. Appl. 40, 75–82.https://doi.org/10.1016/j.eswa.2012.07.034 . Mu, L., Cui, G., Liu, Y., Cui, Y., Fu, L., Gejima, Y., 2020. Design and simulation of an inte-grated end-effector for picking kiwifruit by robot. Inf. Process. Agric. 7, 58 –71. https://doi.org/10.1016/j.inpa.2019.05.004 . Nguyen, T.T., Kayacan, E., De Baedemaeker, J., Saeys, W., 2013. Task and motion planningfor apple harvesting robot. IFAC Proceedings Volumes (IFAC-PapersOnline). IFAChttps://doi.org/10.3182/20130828-2-SF-3019.00063 . Nyakwende, E., Paull, C.J., Atherton, J.G., 1997. Non-destructive determination of leaf areain tomato plants using image processing. J. Hortic. Sci. 72, 255 –262.https://doi.org/ 10.1080/14620316.1997.11515512 . Omid, M., 2011. Design of an expert system for sorting pistachio nuts through decisiontree and fuzzy logic classiﬁer. Expert Syst. Appl. 38, 4339 –4347.https://doi.org/ 10.1016/j.eswa.2010.09.103. Putra, W.B.T., Soni, P., 2017. Evaluating NIR-Red and NIR-Red edge external ﬁlters with digital cameras for assessing vegetation indices under different illumination. InfraredPhys. Technol. 81, 148 –156.https://doi.org/10.1016/j.infrared.2017.01.007 . Putra, W.B.T., Soni, P., 2018. Enhanced broadband greenness in assessing chlorophyll aand b, carotenoid, and nitrogen in Robusta coffee plantations using a digital camera.Precis. Agric. 19, 238 –256.https://doi.org/10.1007/s11119-017-9513-x . Putra, B.T.W., Soni, P., 2020. Improving nitrogen assessment with an RGB camera acrossuncertain natural light from above-canopy measurements. Precis. Agric. 21,147–159.https://doi.org/10.1007/s11119-019-09656-8 . Rodríguez, J.P., Corrales, D.C., Aubertot, J.N., Corrales, J.C., 2020. A computer vision systemfor automatic cherry beans detection on coffee trees. Pattern Recogn. Lett. 136,142–153.https://doi.org/10.1016/j.patrec.2020.05.034 . Sabzi, S., Abbaspour-gilandeh, Y., Javadikia, H., 2017. Machine vision system for the auto-matic segmentation of plants under different lighting conditions. Biosyst. Eng. 161,157–173.https://doi.org/10.1016/j.biosystemseng.2017.06.021 . Sabzi, S., Abbaspour-gilandeh, Y., Ignacio, J., 2020. An automatic visible-range video weeddetection, segmentation and classi ﬁcation prototype in potatoﬁeld. Heliyon 6. https://doi.org/10.1016/j.heliyon.2020.e03685 1–17. Scarfe, A.J., Flemmer, R.C., Bakker, H.H., Flemmer, C.L., 2009. Development of an autono-mous kiwifruit picking robot. ICARA 2009 - Proc. 4th Int. Conf. Auton. Robot. Agents,pp. 380–384https://doi.org/10.1109/ICARA.2000.4804023 . Shaik, K.B., Ganesan, P., Kalist, V., Sathish, B.S., Jenitha, J.M.M., 2015. Comparative study ofskin color detection and segmentation in hsv and ycbcr color space. Procedia Comput.Sci. 57, 41–48.https://doi.org/10.1016/j.procs.2015.07.362 . Silwal, A., Davidson, J.R., Karkee, M., Mo, C., Zhang, Q., Lewis, K., 2017. Design, integration,andﬁeld evaluation of a robotic apple harvester. J. F. Robot. 34, 1140 –1159.https:// doi.org/10.1002/rob.21715. Singh, V., 2019. Sunﬂower leaf diseases detection using image segmentation based onparticle swarm optimization. Artif. Intell. Agric. 3, 62 –68.https://doi.org/10.1016/j. aiia.2019.09.002.Singh, V., Misra, A.K., 2017. Detection of plant leaf diseases using image segmentation andsoft computing techniques. Inf. Process. Agric. 4, 41 –49.https://doi.org/10.1016/j. inpa.2016.10.005.Tang, J.L., Chen, X.Q., Miao, R.H., Wang, D., 2016. Weed detection using image processingunder different illumination for site-speci ﬁc areas spraying. Comput. Electron. Agric. 122, 103–111.https://doi.org/10.1016/j.compag.2015.12.016 . Tanigaki, K., Fujiura, T., Akase, A., Imagawa, J., 2008. Cherry-harvesting robot. Comput.Electron. Agric. 63, 65 –72.https://doi.org/10.1016/j.compag.2008.01.018 . Tewari, V.K., Pareek, C.M., Lal, G., Dhruw, L.K., Singh, N., 2020. Image processing basedreal-time variable-rate chemical spraying system for disease control in paddy crop.Artif. Intell. Agric. 4, 21 –30.https://doi.org/10.1016/j.aiia.2020.01.002 . Tsai, S.-H., Tseng, Y.-H., 2012. A novel color detection method based on HSL color space forrobotic soccer competition. Comput. Math. Appl. 64, 1291
–1300.https://doi.org/ 10.1016/j.camwa.2012.03.073. Vázquez-Arellano, M., Reiser, D., Paraforos, D.S., Garrido-Izard, M., Griepentrog, H.W.,2018. Leaf area estimation of reconstructed maize plants using a time-of- ﬂight cam- era based on different scan directions. Robotics 7 (4). https://doi.org/10.3390/robot- ics704006363, 1-12.Vitzrabin, E., Edan, Y., 2016. Adaptive thresholding with fusion using a RGBD sensor forred sweet-pepper detection. Biosyst. Eng. 146, 45 –56.https://doi.org/10.1016/j. biosystemseng.2015.12.002.N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
155Wang, Y., Zhu, X., Ji, C., 2008. Machine vision based cotton recognition for cotton harvest-ing robot. IFIP Int. Fed. Inf. Process. 259, 1421 –1425.https://doi.org/10.1007/978-0- 387-77253-0_92.Wang, Y., Yang, Y., Yang, C., Zhao, H., Chen, G., Zhang, Z., Fu, S., Zhang, M., Xu, H., 2019.End-effector with a bite mode for harvesting citrus fruit in random stalk orientationenvironment. Comput. Electron. Agric. 157, 454 –470.https://doi.org/10.1016/j. compag.2019.01.015.Williams, H.A.M., Jones, M.H., Nejati, M., Seabright, M.J., Bell, J., Penhall, N.D., Barnett, J.J.,Duke, M.D., Scarfe, A.J., Ahn, H.S., Lim, J.Y., MacDonald, B.A., 2019. Robotic kiwifruitharvesting using machine vision, convolutional neural networks, and robotic arms.Biosyst. Eng. 181, 140 –156.https://doi.org/10.1016/j.biosystemseng.2019.03.007 . Willigenburg, V.L.G., Hol, C.W.J., Van Henten, E.J., 2004. On-line near minimum-time pathplanning and control of an industrial robot for picking fruits. Comput. Electron. Agric.44, 223–237.https://doi.org/10.1016/j.compag.2004.05.004 . Xiang, R., Ying, Y., Jiang, H., 2011. Research on image segmentation methods of tomato innatural conditions. Int. Congr. Image Signal Process. 1268 –1272. Xiong, Y., Peng, C., Grimstad, L., From, P.J., Isler, V., 2019. Development and ﬁeld evalua- tion of a strawberry harvesting robot with a cable-driven gripper. Comput. Electron.Agric. 157, 392–402.https://doi.org/10.1016/j.compag.2019.01.009 . Xu, H., Ying, Y., 2004. Citrus fruit recognition using color image analysis. Intell. Robot.Comput. Vis. XXII Algorithms, Tech. Act. Vis. 5608, 321 –328.https://doi.org/ 10.1117/12.570736.Yabo, G.W., Wang, G., Yabo, W., 2016. Design of end-effector for tomato robotic harvest-ing. IFAC-PapersOnLine 49, 190 –193.https://doi.org/10.1016/j.ifacol.2016.10.035 .Yamamoto, S., Hayashi, S., Saito, S., Ochiai, Y., Yamashita, T., Sugano, S., 2010. Develop-ment of robotic strawberry harvester to approach target fruit from hanging benchside. IFAC Proc. Vol. 3.https://doi.org/10.3182/20101206-3-jp-3009.00016 . Yang, W., Wang, S., Zhao, X., Zhang, J., Feng, J., 2015. Greenness identi ﬁcation based on HSV decision tree. Inf. Process. Agric. 2, 149 –160.https://doi.org/10.1016/j. inpa.2015.07.003.Yeom, J., Jung, J., Chang, A., Maeda, M., Landivar, J., 2018. Automated open cotton boll de-tection for yield estimation using unmanned aircraft vehicle. Remote Sens. 10, 1 –20. https://doi.org/10.3390/rs10121895 . Zhang, Q., Karkee, M., Qin, L., Qin, L., Kaewkorn, C.S., Washington, M., 2016. Design andevaluation of a levelling system for a weeding robot. IFAC-PapersOnLine 49,299–304.https://doi.org/10.1016/j.ifacol.2016.10.055 . Zhao, J., Tow, J., Katupitiya, J., 2005. On-tree fruit recognition using texture properties andcolor data. 2005 IEEE/RSJ Int. Conf. Intell. Robot. Syst. IROS, 263 –268https://doi.org/ 10.1109/IROS.2005.1545592. Zhao, Y., Gong, L., Huang, Y., Liu, C., 2016a. Robust tomato recognition for robotic harvest-ing using feature images fusion. Sensors 173, 1 –12.https://doi.org/10.3390/ s16020173.Zhao, Y., Gong, L., Liu, C., Huang, Y., 2016b. Dual-arm robot design and testing for harvest-ing tomato in greenhouse. IFAC-PapersOnLine 49, 161 –
165.https://doi.org/10.1016/j. ifacol.2016.10.030.Zheng, Y., Zhu, Q., Huang, M., Guo, Y., Qin, J., 2017. Maize and weed classi ﬁcation using color indices with support vector data description in outdoor ﬁelds. Comput. Elec- tron. Agric. 141, 215 –222.https://doi.org/10.1016/j.compag.2017.07.028 .N. Singh, V.K. Tewari, P.K. Biswas et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 142 –156
156