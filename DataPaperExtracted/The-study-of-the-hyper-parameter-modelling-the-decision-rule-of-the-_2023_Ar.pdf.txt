Array 19 (2023) 100310
Available online 20 July 2023
2590-0056/© 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/locate/array
The study of the hyper-parameter modelling the decision rule of the cautious
classifiers based on the 𝐹𝛽measure
Abdelhak Imoussaten
EuroMov Digital Health in Motion, Univ Montpellier, IMT Mines Ales, Ales, France
A R T I C L E I N F O
Keywords:
Cautious classification
Set-valued classification
Belief functions
Supervised machine learningA B S T R A C T
In some sensitive domains where data imperfections are present, standard classification techniques reach their
limits. To avoid misclassifications that have serious consequences, recent works propose cautious classification
algorithms to handle this problem. Despite of the presence of uncertainty and/or imprecision, a point prediction
classifier is forced to bet on a single class. While a cautious classifier proposes the appropriate subset of
candidate classes that can be assigned to the sample in the presence of imperfect information. On the other
hand, cautiousness should not be at the expense of precision and a trade-off has to be made between these
two criteria. Among the existing cautious classifiers, two classifiers propose to manage this trade-off in the
decision step by the mean of a parametrized objective function. The first one is the non-deterministic classifier
(ndc) proposed within the framework of probability theory and the second one is ‘‘evidential classifier based
on imprecise relabelling’’ (eclair) proposed within the framework of belief functions. The theoretical aim of
the mentioned hyper-parameters is to control the size of predictions for both classifiers. This paper proposes to
study this hyper-parameter in order to select the ‘‘best’’ value in a classification task. First the utility for each
candidate subset is studied related to the values of the hyper-parameter and some thresholds are proposed to
control the size of the predictions. Then two illustrations are proposed where a method to choose this hyper-
parameters based on the calibration data is proposed. The first illustration concerns randomly generated data
and the second one concerns the images data of fashion mnist. These illustrations show how to control the
size of the predictions and give a comparison between the performances of the two classifiers for a tuning
based on our proposition and the one based on grid search method.
1. Introduction
In some sensitive applications misclassification can have serious
consequences. This is the case in applications having impacts either on
people’s health or on the environment [ 1], e.g., in medical diagnosis
applications when a classifier is involved to detect early-stage cancer.
In such applications, cautious decisions are necessary when imperfect
data are present. This leads some recent works to focus on cautious
classification. Among the existing cautious classifiers, we focus on
those providing a subset of candidate class labels to a new sample to
classify. We refer to those classifiers as set-valued classifiers. We can
cite among these classifiers the Naive Credal Classifier (ncc) [2,3], the
strong dominance based classifier [ 4], the non-deterministic classifier
(ndc) [ 5], the credal decision trees (CDT) [ 6], the imprecise credal
decision trees (ICDT) [ 7], the classifiers based on generalized criteria
such as Hurwicz (GHC), OWA (GOWAC) [ 8], eclair classifier [ 9,10],
etc. One can find in [ 8–10] other works concerning cautious prediction
including works about conformal prediction [ 11–14] that are discussed
in this paper. But cautiousness should not be at the expense of precision
E-mail address: abdelhak.imoussaten@mines-ales.fr .and a trade-off has to be made between these two criteria. On one
hand, a classifier that predicts always the whole set of the candidate
classes is cautious but its predictions are uninformative. On the other
hand, a classifier that predicts always a single class is precise when the
predictions are good but it is not cautious. Some set-valued classifiers
can control this trade-off as the ndcand eclair classifiers. Indeed, the
utility function implemented in the decision step of both classifiers
has a hyper-parameter 𝛽that is used to control the trade-off between
precision and cautiousness. This hyper parameter is considered as a
user-modifiable parameter for the application of those classifiers and
its theoretical aim is to control the size of the predicted subset of
classes. The choice of 𝛽depends on the level of cautiousness required
for the application in which the classifier is going to be used. This paper
proposes to study this parameter in the case of the two classifiers and
aims to propose suggestions for the choice of the parameter value in the
case of classification task. In the first experiment results, we show, on
simulated data, the impact of the selected hyper-parameter value on the
prediction of the two classifiers when faced to strange samples, i.e., to
https://doi.org/10.1016/j.array.2023.100310
Received 1 April 2023; Received in revised form 5 July 2023; Accepted 17 July 2023Array 19 (2023) 100310
2A. Imoussaten
which the standard classifiers failed to predict the true class labels. In
the second experiment, concerning the images data of fashion mnist, a
comparison between the performances of the two classifiers for a tuning
based on our proposition and the one based on grid search method is
presented. The paper is organized as follows. In the second section,
the reminders about the decision step in the classifiers eclair and ndc
and the measures of set-valued classification performances are given.
The third section presents the studies of the expected utility functions
introduced in the decision step of the two classifiers. Finally, the fourth
section presents the experimental results.
2. Reminders and notations
The set-valued classifiers eclair and ndcare based on the results
of the standard point prediction classifiers to provide respectively the
posterior mass function and the posterior probability function for a
sample𝒙to classify among a set of classes 𝛩= {𝜃1,…,𝜃𝑛}. We
focus in this paper on the decision step of those two classifiers that
involves a utility function that is the 𝐹𝛽score. In this section we give
some reminders about the 𝐹𝛽score and it exploitation in the case of
imprecise predictions by the two classifiers. The evaluation of imprecise
predictions is performed using five measures from the state of the art
that are presented in the end of this section. To simplify notations, we
adopt the following notations for the singleton subsets and subsets of
two elements, in the rest of the paper: 𝜃𝑖∶= {𝜃𝑖},𝜃𝑖𝑗∶= {𝜃𝑖,𝜃𝑗}.
2.1.𝐹𝛽score
The𝐹𝛽score used in the decision step of eclair and ndcto predict
a subset of candidate classes is an adaptation of the 𝐹𝛽score from
information retrieval field and supervised classification methods to
set-valued classification. In the context of binary point prediction for
classification, the 𝐹𝛽score is defined as:
𝐹𝛽(𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑖𝑜𝑛𝑠,𝑡𝑟𝑢𝑡ℎ ) =(1 +𝛽2) recall⋅precision
(𝛽2⋅precision) + recall, (1)
where the quantity precision defined as:
𝑝𝑟𝑒𝑐𝑖𝑠𝑖𝑜𝑛 =𝑛𝑏 𝑜𝑓 𝑡𝑟𝑢𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑡𝑟𝑢𝑒
𝑛𝑏 𝑜𝑓 𝑡𝑟𝑢𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑡𝑟𝑢𝑒 +𝑛𝑏 𝑜𝑓 𝑓𝑎𝑙𝑠𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑡𝑟𝑢𝑒,
and the quantity recall defined as:
𝑟𝑒𝑐𝑎𝑙𝑙 =𝑛𝑏 𝑜𝑓 𝑡𝑟𝑢𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑡𝑟𝑢𝑒
𝑛𝑏 𝑜𝑓 𝑡𝑟𝑢𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑡𝑟𝑢𝑒 +𝑛𝑏 𝑜𝑓 𝑡𝑟𝑢𝑒 𝑝𝑟𝑒𝑑𝑖𝑐𝑡𝑒𝑑 𝑎𝑠 𝑓𝑎𝑙𝑠𝑒,
are two known performance measures in information retrieval and
machine learning. This general expression of 𝐹𝛽score in Eq. (1) is
parametrized by 𝛽that is chosen such that recall is considered 𝛽times
as important as precision .
2.2. The decision step in ndc classifier
The principle of ndcis very simple. Let us consider a sample 𝒙and a
trained point prediction model, denoted 𝛿, that can provide a posterior
probability for the classification of 𝒙. The ndcclassifier consists in a
decision rule 𝑟𝑛𝑑𝑐that is applied to determine the set-valued prediction
for𝒙. Note that, precise predictions are given as singleton subsets. The
predicted subset of classes, using the rule 𝑟𝑛𝑑𝑐, is the one maximizing
the expected utility where the utility associated to each subset of classes
is defined using the 𝐹𝛽measure. More precisely, let us consider a set
of𝑛class labels 𝛩= {𝜃1,…,𝜃𝑛}, the utility of each subset of candidate
classes𝐴⊆𝛩 as the good prediction for 𝒙, having the true class 𝜃𝒙, is
evaluated using the 𝐹𝛽measure as follows:
𝐹𝛽(𝐴,𝜃𝒙) =(1 +𝛽2)⋅|{𝜃𝒙} ∩𝐴|
𝛽2+|𝐴|, (2)
where𝛽is a positive real number and |𝑋|, for𝑋 ⊆ 𝛩 , denotes
the number of elements in 𝑋. The quantity 𝐹𝛽(𝐴,𝜃𝒙)is interpretedas the utility obtained when predicting the subset of class labels 𝐴
when the true class label is 𝜃𝒙. Eq. (2) is analogue to the one in (1)
where the quantities precision andrecall are redefined as precision(𝐴) =
|{𝜃𝒙}∩𝐴|
𝑛𝑏𝑜𝑓𝑐𝑙𝑎𝑠𝑠𝑒𝑠𝑖𝑛𝐴andrecall(𝐴) =|{𝜃𝒙} ∩𝐴|. Note that when the value of 𝛽
is close to 0,𝐹𝛽(𝐴,𝜃𝒙)becomes close to precision(𝐴)thus the size of 𝐴is
disadvantageous, i.e. the larger size of 𝐴the smaller the utility. On the
other hand, when 𝛽is high,𝐹𝛽(𝐴,𝜃𝒙)becomes close to recall(𝐴)and in
this case the size of 𝐴is advantageous. Let us suppose that a posterior
probability distribution 𝑝𝛿(.|𝒙)is provided by a point prediction method
𝛿for the sample 𝒙, then the non-deterministic classifier ndcpredicts
for𝒙the subset of candidate classes that maximize the expected utility
function E(𝐹𝛽(𝐴,.)|𝒙), i.e. :
E(𝐹𝛽(𝐴,.)|𝒙) =𝑛∑
𝑖=1𝐹𝛽(𝐴,𝜃𝑖)⋅𝑝𝛿(𝜃𝑖|𝒙). (3)
Finally, the predicted subset 𝑟𝑛𝑑𝑐(𝒙)for𝒙using the classifier ndcis given
as:
𝑟𝑛𝑑𝑐(𝒙) =𝑎𝑟𝑔 max
𝐴∈2𝛩⧵∅E(𝐹𝛽(𝐴,.)|𝒙). (4)
2.3. The decision step in eclair classifier
The decision step with eclair consists in providing for a sample 𝒙a
subset of classes as prediction, by considering as input the posterior
mass function 𝑚(.|𝒙)and a hyper-parameter 𝛽. The predicted subset
of classes is the one maximizing the expected utility where the utility
associated to each subset of classes is defined using a generalization of
Eq. (2) [9,10]. The main change regarding Eq. (2) is to consider the
general case where the available information about the true class of 𝒙
can be partially known, i.e., a subset 𝐵𝒙of𝛩. It is the case, for example,
when data are coarse [15,16]. The new utility function is then defined
for two subsets 𝐴and𝐵𝒙of𝛩as follows:
𝐹𝛽(𝐴,𝐵𝒙) =(1 +𝛽2)⋅|𝐴∩𝐵𝒙|
𝛽2⋅|𝐵𝒙|+|𝐴|(5)
The quantity 𝐹𝛽(𝐴,𝐵𝒙)is interpreted as the utility obtained when
predicting the subset of class labels 𝐴for the sample 𝒙when its true
class label is partially known and represented by a subset of classes 𝐵𝒙.
In this case, the precision andrecall quantities become:
precision(𝐴) =|𝐴∩𝐵𝒙|
𝑛𝑏𝑜𝑓𝑐𝑙𝑎𝑠𝑠𝑒𝑠𝑖𝑛𝐴,
and
recall(𝐴) =|𝐴∩𝐵𝒙|
𝑛𝑏𝑜𝑓𝑐𝑙𝑎𝑠𝑠𝑒𝑠𝑖𝑛𝐵𝒙.
Let us suppose that a posterior mass function 𝑚(.|𝒙)is known for
the sample𝒙, the eclair classifier predicts for 𝒙the subset of candidate
classes that maximize the expected utility function E(𝐹𝛽(𝐴,.)|𝒙), i.e :
E(𝐹𝛽(𝐴,.)|𝒙) =∑
𝐵⊆𝛩𝐹𝛽(𝐴,𝐵)⋅𝑚(𝐵|𝒙), (6)
where𝐵is the variable representing the true class label of 𝒙. Finally,
the predicted subset 𝑟𝑒𝑐𝑙𝑎𝑖𝑟(𝒙)for𝒙using the classifier eclair is given as:
𝑟𝑒𝑐𝑙𝑎𝑖𝑟(𝒙) =𝑎𝑟𝑔 max
𝐴∈2𝛩⧵∅E(𝐹𝛽(𝐴,.)|𝒙). (7)
2.4. Evaluation measures for the set-valued classifiers
When evaluating a set-valued classifier one ensures that the pre-
dicted subset of classes (1) includes the ‘‘true’’ class and (2) it is as small
as possible depending on the sample data imperfection. Several works
have studied this problem and provide some measures to check the two
conditions (1) and (2) [2,7,17]. Between the least drastic one that is
imprecise accuracy which checks if the prediction contains the true class
label of the sample and the most drastic one that is classical accuracy
which checks if the prediction is equal to the true class label of theArray 19 (2023) 100310
3A. Imoussaten
sample, one can find intermediate measure as Discounted accuracy [18]
that seems to be an interesting measure as it takes into account the size
of the predicted subset. But in order to increase the cautiousness reward
to the degree to which the decision maker prefers to fix it depending
on his application and the quality of the information obtained for the
samples, a family of measure are constructed from Discounted accuracy
measure that are represented by a function 𝑔taking its values in [0,1]
and guaranteeing 𝑔(𝑧)≥𝑧, i.e., the reward with 𝑔is at least the same as
the one given by the discounted accuracy ,𝑔(0) = 0 and𝑔(1) = 1 (see [19]
for more details). Let us consider a samples 𝒙having the true class 𝜃𝒙
and a set-valued classifier 𝛿𝑖𝑐. The five following measures are proposed
to evaluate the performance of 𝛿𝑖𝑐regarding its output for 𝒙:
•theclassical accuracy denoted acc:
𝑎𝑐𝑐(𝛿𝑖𝑐,𝜃𝒙) =1{𝜃𝒙}(𝛿𝑖𝑐(𝒙)).
•theimprecise accuracy denoted impr. acc :
𝑖𝑚𝑝𝑟.𝑎𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙) =1𝛿𝑖𝑐(𝒙)(𝜃𝒙).
•thediscounted accuracy (discAcc) corresponds to the function
𝑔(𝑧) =𝑧[18]:
𝑑𝑖𝑠𝑐𝐴𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙) =1𝛿𝑖𝑐(𝒙)(𝜃𝒙)
|𝛿𝑖𝑐(𝒙)|,
where|𝐴|denotes the size of the subset 𝐴. This measure is also
denoted𝑢50.
•The𝑢65measure that corresponds to the function 𝑔(𝑧) = −0.6⋅
𝑧2+ 1.6⋅𝑧[19]:
𝑢65(𝛿𝑖𝑐,𝑑𝑠𝑡) = −0.6⋅[𝑑𝑖𝑠𝑐𝐴𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙)]2+ 1.6⋅𝑑𝑖𝑠𝑐𝐴𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙).
•The𝑢80measure that corresponds to the function 𝑔(𝑧) = −1.2⋅
𝑧2+ 2.2⋅𝑧[19]:
𝑢80(𝛿𝑖𝑐,𝜃𝒙) = −1.2⋅[𝑑𝑖𝑠𝑐𝐴𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙)]2+ 2.2⋅𝑑𝑖𝑠𝑐𝐴𝑐𝑐 (𝛿𝑖𝑐,𝜃𝒙).
Note that𝑢65is the average measure of 𝑢50and𝑢80and it is the
better suited one to quantify the compromise between precision and
cautiousness.
3. The expected utilities related to 𝜷
3.1. The case of ndc classifier
Let us consider that the posterior probability distribution of a sam-
ple𝒙is known. We denote this distribution by 𝑝(.|𝒙) ∶𝛩→[0,1]. We
consider the parameter 𝛽as a variable and we express the expected
utility function in Section 2.2 for a 𝛽∈[0,+∞[,𝐴⊆𝛩 and𝑝(.|𝒙)as:
𝑢(𝛽,𝐴) =E(𝐹𝛽(𝐴,.)|𝒙) =𝑛∑
𝑖=1𝐹𝛽(𝐴,𝜃𝑖)⋅𝑝(𝜃𝑖|𝒙). (8)
In addition, let us consider the situation where the class 𝜃𝑘is the most
likely class of 𝒙and some times the class 𝜃𝑘is confused with the class
𝜃𝑘′,𝑘≠𝑘′due to data imperfection. The Propositions 3.1 and 3.2 give
some results concerning the predicted subset of classes for 𝒙among the
three options 𝜃𝑘,𝜃𝑘𝑘′and𝛩.
Proposition 3.1. Let suppose that 𝑝(𝜃𝑘|𝒙)> 𝑝(𝜃|𝒙),∀𝜃∈𝛩⧵𝜃𝑘and
𝜃𝑘′=𝑎𝑟𝑔max𝜃∈𝛩⧵𝜃𝑘𝑝(𝜃|𝒙).
If𝑝(𝜃𝑘′|𝒙)>0then it exists 𝛽1≥0such that:
{
𝑢(𝛽,𝜃𝑘𝑘′)≤𝑢(𝛽,𝜃𝑘)if𝛽≤𝛽1
𝑢(𝛽,𝜃𝑘𝑘′)>𝑢(𝛽,𝜃𝑘)if𝛽 >𝛽1.(9)
Elsewhere𝑢(𝛽,𝛩)<𝑢(𝛽,𝜃𝑘𝑘′),∀𝛽≥0.
Proof. We have for all 𝛽≥0,
𝑢(𝛽,𝜃𝑘) =𝑝(𝜃𝑘|𝒙).Table 1
The posterior probability distributions.
𝑥 𝜃1 𝜃2 𝜃3
𝑥1 0.333 0.333 0.333
𝑥2 1 0 0
𝑥3 0.5 0.5 0
𝑥4 0.5 0.4 0.1
and
𝑢(𝛽,𝜃𝑘𝑘′) =1 +𝛽2
2 +𝛽2⋅[𝑝(𝜃𝑘|𝒙) +𝑝(𝜃𝑘′|𝒙)] =1 +𝛽2
2 +𝛽2⋅P(𝜃𝑘𝑘′|𝒙),
where for all subset 𝐴of𝛩,P(𝐴|𝒙) =∑
𝜃∈𝐴𝑝(𝜃|𝒙). On one hand, the
function𝑢(.,𝜃𝑘𝑘′)increases related to 𝛽. Thus𝑢(𝛽,𝜃𝑘𝑘′)≥𝑢(0,𝜃𝑘𝑘′) =
1
2P(𝜃𝑘𝑘′|𝒙), for all𝛽≥0. On the other hand, 𝑝(𝜃𝑘|𝒙)> 𝑝(𝜃𝑘′|𝒙)then
𝑝(𝜃𝑘|𝒙)>1
2P(𝜃𝑘𝑘′|𝒙). So,𝑢(.,𝜃𝑘𝑘′)intersects𝑢(.,𝜃𝑘)at𝛽1≥0such that:
1 +𝛽2
1
2 +𝛽2
1⋅P(𝜃𝑘𝑘′|𝒙) =𝑝(𝜃𝑘|𝒙).
It comes:
𝛽1=√
𝑝(𝜃𝑘|𝒙) −𝑝(𝜃𝑘′|𝒙)
𝑝(𝜃𝑘′|𝒙).□ (10)
Note that the same reasoning for the comparison between the
utilities of 𝜃𝑘and𝜃𝑘𝑘′in Proposition 3.1 can be generalized for the
comparisons between the utilities of 𝜃𝑘and all the subsets 𝐴 ⊂ 𝛩
containing 𝜃𝑘where
𝑢(𝛽,𝐴) =1 +𝛽2
|𝐴|+𝛽2⋅P(𝐴|𝒙),
in this case, 𝛽1becomes:
𝛽1=√
|𝐴|⋅𝑝(𝜃𝑘|𝒙) −P(𝐴|𝒙)
P(𝐴|𝒙) −𝑝(𝜃𝑘|𝒙). (11)
Proposition 3.2. Let suppose that 𝑝(𝜃𝑘|𝒙)>𝑝(𝜃|𝒙),∀𝜃∈𝛩⧵𝜃𝑘.
IfP(𝜃𝑘𝑘′|𝒙) ∈[2
3,1[ then it exists 𝛽2>0such that:
{
𝑢(𝛽,𝛩)≤𝑢(𝛽,𝜃𝑘𝑘′)if𝛽≤𝛽2
𝑢(𝛽,𝛩)>𝑢(𝛽,𝜃𝑘𝑘′)if𝛽 >𝛽2.(12)
Proof. We have for all 𝛽≥0,
𝑢(𝛽,𝛩) =1 +𝛽2
3 +𝛽2,
and
𝑢(𝛽,𝛩) −𝑢(𝛽,𝜃𝑘𝑘′) =(1 +𝛽2)⋅(2 − 3⋅P(𝜃𝑘𝑘′) + (1 − P(𝜃𝑘𝑘′)⋅𝛽2))
(3 +𝛽2)⋅(2 +𝛽2).
IfP(𝜃𝑘𝑘′|𝒙)<2
3, then𝑢(𝛽,𝛩)> 𝑢(𝛽,𝜃𝑘𝑘′),∀𝛽≥0. Else, if P(𝜃𝑘𝑘′|𝒙) = 1 ,
then𝑢(𝛽,𝛩) =1+𝛽2
3+𝛽2<1+𝛽2
2+𝛽2=𝑢(𝛽,𝜃𝑘𝑘′),∀𝛽≥0. Otherwise, let us
consider the following value 𝛽∗≥0such that:
𝛽∗2=3P(𝜃𝑘𝑘′|𝒙) − 2
1 −P(𝜃𝑘𝑘′|𝒙), (13)
the𝛽2=𝛽∗verify the inequalities of Proposition 3.2. □
Example 3.1. Let us consider the case where 𝛩= {𝜃1,𝜃2,𝜃3}. The
posterior probabilities of four samples are given in Table 1 and in Fig. 1.
These distributions express several situations of sharing the probability
masses between the three classes. For the first sample 𝑥1the mass
is uniformly distributed between the classes; for 𝑥2the total mass is
given to the class 𝜃1; for𝑥3the mass is uniformly distributed between
𝜃1and𝜃2; and for𝑥4the mass distribution is as follows 𝑝(𝜃3|𝒙4)<
𝑝(𝜃1|𝒙4)<𝑝(𝜃2|𝒙4). As one can see in Fig. 1, for the samples 𝑥1,𝑥2andArray 19 (2023) 100310
4A. Imoussaten
Fig. 1. The expected utility associated to the four posterior probabilities.
𝑥3,𝛩,𝜃1, and𝜃1,2are respectively the predictions as they maximize
the expected utility regardless the value of 𝛽. While in the case of
𝑥4, the prediction depends on the value of the parameter 𝛽. Indeed,
if𝛽 <𝛽1=√
𝑝(𝜃1|𝒙4)−𝑝(𝜃2|𝒙4)
𝑝(𝜃2|𝒙4)= 0.5, i.e., the value of 𝛽where the curves
of𝑢(.,𝜃1)and𝑢(.,𝜃1,2)intersect, then 𝜃1dominates all the other options.
When𝛽2> 𝛽 > 𝛽1(𝛽2=√
3P(𝜃1,2|𝒙)−2
1−P(𝜃1,2|𝒙)= 2.65), then𝜃1,2dominates all
the other options. When 𝛽≥𝛽2, it is the turn of 𝛩to dominate the other
options.
3.2. The case of eclair classifier
In this subsection, we consider that the posterior mass function of
a sample𝒙is known. We denote this mass function by 𝑚(.|𝒙) ∶ 2𝛩→
[0,1]. In this case, the expected utility function used as the criterion to
choose the subset of classes to associate to 𝒙is the following:
𝑢𝑚(𝛽,𝐴) =E(𝐹𝛽(𝐴,.)|𝒙) =∑
𝐵⊆𝛩𝐹𝛽(𝐴,𝐵)⋅𝑚(𝐵|𝒙), (14)
In this section, we treat only the case of two classes. Consequently,
the multi-class case can be treated using one-against-one prediction
techniques and then infer the final prediction by merging all the
one-against-one predictions.
Proposition 3.3. Let us consider the case where 𝛩= {𝜃1,𝜃2}. If𝑚(𝜃1|𝒙)>
𝑚(𝜃2|𝒙), then it exists 𝛽3≥0such that:
{
𝑢𝑚(𝛽,𝜃12)≤𝑢(𝛽,𝜃1)if𝛽≤𝛽3
𝑢𝑚(𝛽,𝜃12)>𝑢(𝛽,𝜃1)if𝛽 >𝛽3(15)
Elsewhere,𝑢𝑚(𝛽,𝜃12)≥𝑢(𝛽,𝜃1),∀𝛽≥0.
Proof. In one hand, we have,
𝑑𝑢𝑚(𝛽,𝜃1)
𝑑𝛽= −2𝛽
(1 + 2𝛽2)2𝑚(𝜃12|𝒙)consequently 𝑢𝑚(.,𝜃1)decreases ∀𝛽≥0with𝑢𝑚(0,𝜃1) =𝑚(𝜃1|𝒙) +
𝑚(𝜃12|𝒙)and lim𝛽→+∞𝑢𝑚(𝛽,𝜃1) =𝑚(𝜃1|𝒙) +𝑚(𝜃12|𝒙)
2. In the other hand,
we have,
𝑑𝑢𝑚(𝛽,𝜃12)
𝑑𝛽=2𝛽
(2 +𝛽2)2[1 −𝑚(𝜃12|𝒙)]
consequently 𝑢𝑚(.,𝜃12)increases ∀𝛽≥0with𝑢𝑚(0,𝜃12) =1
2+𝑚(𝜃12|𝒙)
2and
lim𝛽→+∞𝑢𝑚(𝛽,𝜃12) = 1 . Obviously, if 𝑢𝑚(0,𝜃1)> 𝑢𝑚(0,𝜃12)then𝑢𝑚(.,𝜃1)
and𝑢𝑚(.,𝜃12)intersect, elsewhere 𝑢𝑚(𝛽,𝜃12)≥𝑢𝑚(𝛽,𝜃1),∀𝛽≥0. The
inequality𝑢𝑚(0,𝜃1)> 𝑢𝑚(0,𝜃12)corresponds to 𝑚(𝜃1|𝒙) +𝑚(𝜃12|𝒙)>
1
2+𝑚(𝜃12|𝒙)
2which is verified when 𝑚(𝜃1|𝒙)> 𝑚(𝜃2|𝒙). Finally,𝛽3is
the solution of 𝑢𝑚(𝛽,𝜃1) =𝑢𝑚(𝛽,𝜃12)which corresponds to the solution
of Eq. (16):
𝑚(𝜃1|𝒙) +1 +𝛽2
1 + 2𝛽2𝑚(𝜃12|𝒙) =1 +𝛽2
2 +𝛽2+1
2 +𝛽2𝑚(𝜃12|𝒙).□ (16)
Remark 3.1. Note that when 𝑚is a Bayesian mass function, i.e.,
𝑚(𝜃12|𝒙) = 0 , Eq. (16) becomes: 𝑚(𝜃1|𝒙) =1+𝛽2
2+𝛽2, which is verified for
the following value of 𝛽3:
𝛽3=𝛽1=√
𝑚(𝜃1|𝒙) −𝑚(𝜃2|𝒙)
𝑚(𝜃2|𝒙).
Example 3.2. To illustrate different situations, we consider six mass
functions (see Table 2 and Fig. 2). Fig. 2 shows that when 𝑚(𝜃1|𝑥) =
𝑚(𝜃2|𝑥), e.g.𝑚1and𝑚4, regardless the mass of 𝜃12, the option 𝜃12obtains
the maximal gains for all 𝛽 > 0. In the other cases, the value of 𝛽3
depends on the mass of 𝜃12, i.e, ignorance. Indeed, the higher the mass
of ignorance, the smaller the value of 𝛽3. This means that if the decision-
maker desire to make precise predictions for examples like those, he
needs to use very small value of 𝛽3lower than the solution of Eq. (16).Array 19 (2023) 100310
5A. Imoussaten
Fig. 2. The utility function for some examples of masses.
Table 2
Mass functions representing several uncertainty situations.
𝜃1𝜃2𝜃1,2 𝜃1𝜃2𝜃1,2
𝑚1 0.5 0.5 0 𝑚4 0.3 0.3 0.4
𝑚2 0.7 0.2 0.1 𝑚5 0.2 0 0.8
𝑚3 0.5 0.2 0.3 𝑚6 0.7 0.3 0
4. Illustration
In this section we present the illustration of the performances of
the classifiers ndcand eclair using generated data in Section 4.1 and
using fashion mnist data in Section 4.2. In the two subsections we
give the comparisons of the classifiers when the hyper-parameter is
tuned based on grid search method or based on the proposition of this
paper regarding the set-valued classification metrics and in Section 4.2
we show how to control the number of the predictions using our
propositions.
4.1. Illustration using simulated data
In this first illustration, we consider a simulated data for three class
labels a,b, and c. For each class label 500training samples of a bivariate
Gaussian distribution are considered, (𝜇𝑎= (0.2,0.65),𝛴𝑎= 0.01𝐼2)
for the class label 𝑎,(𝜇𝑏= (0.5,0.9),𝛴𝑏= 0.01𝐼2)for the class label
𝑏and(𝜇𝑐= (0.8,0.6),𝛴𝑐= 0.01𝐼2)for the class label 𝑐. In addition,
a testing dataset of 50samples for each label are generated using the
same bivariate Gaussian distributions with a Gaussian noise (𝜇=
(0,0),𝛴= 0.001𝐼2). In the end, we have a dataset of 1500 training data
and150test data. First, nine point prediction classifiers are trained and
tested on these data. The point prediction classifiers considered are
the naive Bayes ( nbc), the k-Nearest Neighbour ( knn), the evidential
k-Nearest Neighbour ( eknn), the decision tree ( cart), the random forest(rfc), linear discriminant analysis ( lda), support vector machine ( svm)
and artificial neural networks ( ann), the logistic classifier ( logistic ). The
obtained accuracies are: logistic, ann: 94.67; svm, eknn: 95.33; and
knn, nbc, rfc, lda, cart: 96. These classifiers are introduced here to
detect the samples that are considered as ‘‘strange samples’’ in this
paper, i.e., most point prediction classifiers fail to predict the true class
of those samples. In the opposite case, the samples are considered as
‘‘usual samples’’. This term will also be used, for a given classifier, to
distinguish the samples for which the predicted class obtains a large
probability, i.e., usual, from the others, i.e., strange.
4.1.1. The case of ndc classifier
The idea here for choosing the ndchyper-parameter 𝛽is to avoid
misclassification when the samples are strange and then predict a
subset of classes for those samples. For the samples that are ‘‘usual’’,
the posterior probability of one of the classes is close to 1, thus the
later class obtain the maximum utility regardless the value given to
𝛽(see Section 3.1). Consequently, it is more interesting to fix the
value of𝛽regarding the strange samples in the validation step. Indeed,
the training data of 1500 samples is divided to 1200 samples for
training and 300 (20%) samples for validation. The proposition of this
paper is to consider a fictive probability distribution 𝑝𝑓where the first
component is the mean of the maximal probabilities 𝑝1obtained for
strange samples of the validation data set and the second component is
the mean of the second maximal probabilities 𝑝2, and so on. Thus, 𝑝𝑓=
(𝑝1,𝑝2,…). To determine the strange samples a probability threshold
is considered and it is fixed at 0.99 in this illustration. The value
of𝛽is considered as the threshold behind which if the samples are
considered strange, we should predict the subset of the two first classes
with maximal probabilities. In Proposition 3.1 this theoretical value
corresponds to:
𝛽𝑛𝑑𝑐=√
𝑝1−𝑝2
𝑝2. (17)Array 19 (2023) 100310
6A. Imoussaten
Fig. 3. The predictions obtained with ndc: a large size is given to the point symbols representing predictions that are errors or imprecise.
In Fig. 3, we present the predictions on the data set when 𝛽𝑛𝑑𝑐= 2.677
is determined as in Eq. (17) on the validation data. The samples that
are considered strange for the point prediction classifiers are labelled
by their number in the test dataset. Only the sample number 140is
misclassified in the predictions of 𝑛𝑑𝑐and only three ‘‘usual’’ samples
have imprecise predictions. In general, among the ten strange samples,
eight samples are predicted as subsets of two classes containing the true
class and one as the whole set.
4.1.2. The case of eclair classifier
For the case of eclair classifier, we consider binary classifications
‘‘a against b’’, ‘‘a against c’’ and ‘‘b against c’’. We apply the same
reasoning as in Section 4.1.1, the training data of is divided to 80%
samples for training and 20% samples for validation. Here, also we
consider only strange samples with the same mass threshold, i.e., 0.99.
From Section 3.2, to avoid misclassification for strange samples 𝛽
should be heigh enough to predict 𝜃12when ignorance is heigh. Let
us denote𝑚12the average of 𝑚(𝜃12|𝑥)obtained for each strange sample
in the validation data set. The proposed value of 𝛽is𝛽𝑒𝑐𝑙𝑎𝑖𝑟 that is the
solution of the quadratic Eq. (16) with 𝑚(𝜃12|𝑥) =𝑚12and𝑚(𝜃1|𝑥) =
2⋅(1 −𝑚12)∕3. In Fig. 4, we can see that, for the case ‘‘a against b’’
(𝛽𝑒𝑐𝑙𝑎𝑖𝑟 = 0.519), we have one misclassification and four set-valued
classification for the strange samples. For the case of ‘‘a against c’’
(𝛽𝑒𝑐𝑙𝑎𝑖𝑟 = 0), we have one misclassification and all the other strange
samples are good predictions. While for the case ‘‘b against c’’( 𝛽𝑒𝑐𝑙𝑎𝑖𝑟 =
0.581), we have one misclassification and one set-valued classification
for the strange samples.
4.2. Illustration using fashion mnist data
In this section we propose to select the hyper-parameter 𝛽for the
two classifiers ndcandeclair based on Eq. (10) for the modern version
of mnist dataset [20], i.e. fashion mnist dataset [21]. These data are
more difficult to handle compared to the original ones. Fashion-MNIST,a direct drop-in replacement for the original Y. Lecun’ MNIST dataset
for benchmarking machine learning algorithms [20], is a dataset of
Zalando’s article images [21] consisting of a training set of 60,000
examples and a test set of 10,000 examples. Each example is a 28 ×28
gray-scale image, associated with a label from 10 classes. Fig. 5 shows
several images from this dataset where each class takes three-rows. As
one can see in Figs. 6 and 7 (taken from [22]) from the visualization of
datasets that makes comparison between mnist and fashion mnist [21,
22], fashion mnist dataset seems to be more challenging while for mnist
dataset, classes are clearly separated.
Note that this illustration is presented only for the ndc classifier.
First, we apply a grid search optimization to select the hyper-parameter
𝛽for ndc classifier. The grid search method is based on the objective
function𝑢65as it is better suited to quantify the compromise between
precision and cautiousness. Fig. 8 shows the performances obtained
for the selected grid within the interval [0,3]on the validation data
sets, i.e., 20% of training data, 𝛽= 0.857gives the optimal mean
performance at 0.954.
The fashion mnist data set has 10classes, so it is more difficult
to fix the probability threshold for strange samples compared to the
simulated dated in Section 4.1. Thus, to calculate the different values of
𝛽in the same way as in Section 4.1.1, we considered several threshold
of probabilities and represent the number of predicted subsets for each
value𝛽1calculated as in Eq. (11) on the validation data. Let recall that
the number of data test is 10000 . Moreover, the point prediction classi-
fier used regarding the data nature to learn the posterior probabilities
is the artificial neural networks (sequential model). Fig. 9 gives the
parameter details of the employed architecture. Figures from 10 to 17
give the number of times a subset of 2, 3, 4, 5, 6 and 10 are predicted
related of the values of 𝛽given in the legend. Obviously when changing
the threshold of probabilities for strange examples, the values of 𝛽1
change. In Fig. 10, the value 𝛽1= 0.511corresponds to the 𝛽beyond
which the utility of predicting a subset of size 2 is higher than the
utility of prediction a single class; the value 𝛽1= 1.004correspondsArray 19 (2023) 100310
7A. Imoussaten
Fig. 4. The predictions obtained with eclair : a large size is given to the point symbols representing predictions that are errors or imprecise.
Fig. 5. Fashion-MNIST samples (by Zalando, MIT License).
The classes are: 1=‘T-shirt/top’, 2=‘Trouser’, 3=‘Pullover’, 4=‘Dress’, 5=‘Coat’, 6=‘Sandal’, 7=‘Shirt’, 8=‘Sneaker’, 9=‘Bag’, 10=‘Ankle boot’.
to the𝛽beyond which the utility of predicting a subset of size 3 is
higher than the utility of prediction a subset of size 2; and so on.
One can see that only subsets of size 2, 3, 4 and 5 are predicted, therest of the prediction are single classes. Furthermore, we can observe
a slight increase of the predictions of 2, 3, 4 and 5 classes when the
probability threshold increase. This is due to the mean of probabilitiesArray 19 (2023) 100310
8A. Imoussaten
Fig. 6. mnist data clustering.
Fig. 7. Fashion mnist data clustering.
Fig. 8.𝛽hyper-parameter grid search optimization for ndc.
Fig. 9. The neural networks architecture.
Fig. 10. Threshold fixed at 𝑝= 0.55.
Table 3
𝑢65performances for difference values of 𝛽and𝑝on the test data of fashion mnist.
𝑝= 0.55𝑝= 0.6𝑝= 0.65𝑝= 0.7𝑝= 0.75 Grid search
𝑢65 0.9241 0.9245 0.9249 0.9245 0.9244 0.9244
𝛽1 1.379 1.118 1.224 1.339 0.803 0.857
that are influenced by the values of large probabilities of ‘‘usual’’
samples.
Table 3 shows the best 𝑢65performance measure for each probability
threshold. One can see also, in the last column, the 𝑢65performance
measure obtained using the optimal hyper-parameters tuned with grid
search method. As we can except for 𝑝= 0.55, the results obtained with
our proposition are better than grid search one.
5. Related works
The closest work, in principle, to that of the proposal of this article,
is the one of conformal prediction. Conformal prediction is designed to
perform label predictions successively, each one begin revealed before
the next is predicted [ 11–13], but it is also adapted to classical predic-
tion task as for regression [ 14]. The general principal is the following:Array 19 (2023) 100310
9A. Imoussaten
Fig. 11. Threshold fixed at 𝑝= 0.6.
Fig. 12. Threshold fixed at 𝑝= 0.65.
Fig. 13. Threshold fixed at 𝑝= 0.7.
(1) training data are divided to training part and calibration/validation
part; (2) a classifier/regressor 𝛿is learnt using the training part; (3)
non-conformity (strangeness) score is computed on the calibration part
by comparing the predictions of 𝛿to the true labels; (4) in the same
way as in 3-, each potential class 𝑦of a sample 𝑥in the test data is
associated a non-conformity score 𝛼𝑦related the prediction of 𝛿; (5)
then the𝑝-value is defined for the potential class 𝑦of𝑥as the portion
of the calibration data and 𝑥that have a non-conformity scores greater
than𝛼𝑦; (6) for a given small positive value 𝜖(1% or 5%), the predictive
region output (or the set-valued prediction) is the subset: {𝑦∶𝑝(𝑦)>𝜖}.
The common point between the two classifiers is the fact of building
the predictions for the new samples on the basis of the non-conformity
or the strangeness of some (or all) the samples of the calibration data
Fig. 14. Threshold fixed at 𝑝= 0.75.
Fig. 15. Threshold fixed at 𝑝= 0.8.
Fig. 16. Threshold fixed at 𝑝= 0.85.
compared to what was learned by the classifiers. While the difference
lies in how the non-conformity scores are calculated on the one hand,
and how the predictions are performed on the other hand. Indeed, in
our proposal, non-conformity score is calculated with respect to the
certainty of the prediction, i.e., how closely the sample resembles other
training samples regardless the true class of the calibration samples.
Concerning the set-valued predictions conformal prediction is based
on the concept of confidence level which well established in statistics,
while in our proposal predictions are based on a compromise through
a subjective utility which compares the subsets of classes. Moreover,
Table 4 shows the results obtained by the conformal predictions for
two different confidence levels (0.95% and 0.99%) for the fashion mnist
data. As one can see in the comparison with the result shown in Table 3,
our proposition obtained better 𝑢65scores. But with a high confidence
level, conformal prediction shows a very high score for ‘‘impr. acc’’ atArray 19 (2023) 100310
10A. Imoussaten
Fig. 17. Threshold fixed at 𝑝= 0.9.
Table 4
Conformal predictions for the test data of fashion mnist.
𝑎𝑐𝑐 𝑢50 𝑢65 𝑢80 Impr. acc
𝜖= 0.05 0.6785 0.792 0.83 0.868 0.947
𝜖= 0.01 0.4817 0.6642 0.729 0.7945 0.9888
the expense of a very low accuracy score. The decision rule strategies
have different complexity and it is challenging as the set of alternatives
size is 2𝑛,𝑛=|𝛩|. The ndcclassifier has the lowest optimized computa-
tional complexity, i.e., at worst 𝑂(𝑛)[5]. However, the computational
complexity is challenging for conformal prediction (see [ 14] for more
details) and for eclair classifiers (see [ 10] for more details). Indeed, for
conformal predictions, the non-conformity scores are calculated using
the nearest neighbours of the calibration or test samples in the training
data [ 11]. Regarding eclair classifier, as for any approach representing
imprecision in the data, the computational complexity can be very high.
Indeed, the computational complexity of the reasoning step of the eclair
classifiers becomes very high, i.e., 𝑂(22𝑛), at worst. One can find in [ 10]
some optimizations to overcame the problem. For example, by selecting
the relevant candidate subsets for prediction to a subset of 2𝛩.
6. Conclusion
In this paper we are interested in the set-valued classification. Espe-
cially, we focus on the study of the parameter 𝛽involved in the utility
function used in the decision step of two set-valued classifiers. More
precisely, we studied the predicted subsets depending on this hyper-
parameter. In addition to theoretical propositions, we give practical
method to control the size of the predicted subset in machine learning
applications. While trying to remain very efficient on point prediction
task, set-valued classifiers have the challenge of making machine learn-
ing methods more trustworthy, especially in the presence of imperfect
data. The decision-maker who knows well his data could better control,
using the proposal of this article, the size of the predictions by fixing
the suited value for 𝛽. As a perspective, we intend in our next work
to provide a demonstration that relies on theoretical foundations and
statistical hypotheses to support the choices of the different thresholds
that are experimentally set in our illustrations. We will also try to
handle the merging part of the ‘‘one against all’’ solution suggested
in the eclair part. Indeed, we obtain different values of 𝛽3for each
pair comparisons which make the merging part of the pair comparisons
difficult to handle.CRediT authorship contribution statement
Abdelhak Imoussaten: Study conception and design, Data collec-
tion, Analysis and interpretation of results, Writing – original draft.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Data availability
The information about the data are given in the paper.
References
[1] Jacquin L, Imoussaten A, Trousset F, Perrin D, Montmain J. Control of waste frag-
ment sorting process based on MIR imaging coupled with cautious classification.
Resour Conserv Recy 2021;168:105258.
[2] Zaffalon M. A credal approach to naive classification. In: ISIPTA, vol. 99. 1999,
p. 405–14.
[3] Zaffalon M. Statistical inference of the naive credal classifier. In: ISIPTA, vol. 1.
2001, p. 384–93.
[4] Troffaes MC. Decision making under uncertainty using imprecise probabilities.
Int J Approx Reason 2007;45(1):17–29.
[5] Coz JJd, Díez J, Bahamonde A. Learning nondeterministic classifiers. J Mach
Learn Res 2009;10(Oct):2273–93.
[6] Abellán J, Moral S. Building classification trees using the total uncertainty
criterion. Int J Intell Syst 2003;18(12):1215–25.
[7] Abellan J, Masegosa AR. Imprecise classification with credal decision trees. Int
J Uncertain Fuzziness Knowl-Based Syst 2012;20(05):763–87.
[8] Ma L, Denoeux T. Partial classification in the belief function framework.
Knowl-Based Syst 2021;106742.
[9] Jacquin L, Imoussaten A, Trousset F, Montmain J, Perrin D. Evidential classifica-
tion of incomplete data via imprecise relabelling: Application to plastic sorting.
In: Ben Amor N, Quost B, Theobald M, editors. Scalable uncertainty management.
Cham: Springer International Publishing; 2019, p. 122–35.
[10] Imoussaten A, Jacquin L. Cautious classification based on belief functions theory
and imprecise relabelling. Internat J Approx Reason 2022;142:130–46.
[11] Shafer G, Vovk V. A tutorial on conformal prediction. J Mach Learn Res
2008;9(Mar):371–421.
[12] Vovk V, Gammerman A, Shafer G. Conformal prediction. Algorithmic Learn
Random World 2005;17–51.
[13] Gammerman A, Vovk V, Vapnik V. Learning by transduction. 2013, arXiv
preprint arXiv:1301.7375 .
[14] Papadopoulos H, Proedrou K, Vovk V, Gammerman A. Inductive confidence
machines for regression. In: Machine learning: ECML 2002: 13th European con-
ference on machine learning Helsinki, Finland, August 19–23, 2002 proceedings
13. Springer; 2002, p. 345–56.
[15] Couso I, Sánchez L. Machine learning models, epistemic set-valued data
and generalized loss functions: an encompassing approach. Inform Sci
2016;358:129–50.
[16] Sanchez L, Couso I. A framework for learning fuzzy rule-based models with
epistemic set-valued data and generalized loss functions. Internat J Approx
Reason 2018;92:321–39.
[17] Yang G, Destercke S, Masson M-H. The costs of indeterminacy: How to determine
them? IEEE Trans Cybern 2016;47(12):4316–27.
[18] Tsoumakas G, Vlahavas I. Random k-labelsets: An ensemble method for multil-
abel classification. In: European conference on machine learning. Springer; 2007,
p. 406–17.
[19] Zaffalon M, Corani G, Mauá D. Evaluating credal classifiers by utility-discounted
predictive accuracy. Internat J Approx Reason 2012;53(8):1282–301.
[20] LeCun Y. The MNIST database of handwritten digits. 1998, http://yann.lecun.
com/exdb/mnist/ .
[21] Xiao H, Rasul K, Vollgraf R. Fashion-mnist: a novel image dataset for
benchmarking machine learning algorithms. 2017, arXiv preprint arXiv:1708.
07747 .
[22] Agrawal A, Ali A, Boyd S. Minimum-distortion embedding. 2021, arXiv.