ArtiÔ¨Åcial  Intelligence  in the Life Sciences  1 (2021) 100014 
Contents  lists available  at ScienceDirect  
ArtiÔ¨Åcial  Intelligence  in the Life Sciences  
journal  homepage:  www.elsevier.com/locate/ailsci  
Research  Article  
Maxsmi:  Maximizing  molecular  property  prediction  performance  with  
conÔ¨Ådence  estimation  using  SMILES  augmentation  and deep  learning  
Talia B. Kimber  a , ‚àó , Maxime  Gagnebin  b , Andrea  Volkamer  a , ‚àó 
a In silico Toxicology  and Structural  Bioinformatics,  Institute of Physiology,  Charit√©-Universit√§tsmedizin  Berlin, Charit√©platz  1, 10117, Berlin, Germany 
b Berlin, Germany 
a r t i c l e i n f o 
Keywords:  
Deep learning 
Molecular  property prediction  
Data augmentation  
Open-source  
ConÔ¨Ådence  assessment  
SMILES a b s t r a c t 
Accurate  molecular  property  or activity prediction  is one of the main goals in computer-aided  drug design. Quan- 
titative structure-activity  relationship  (QSAR) modeling  and machine  learning,  more recently  deep learning,  have 
become an integral part of this process.  Such algorithms  require lots of data for training  which, in the case of 
physico-chemical  and bioactivity  data sets, remains  scarce. To address the lack of data, augmentation  techniques  
are increasingly  applied in deep learning.  Here, we exploit that one compound  can be represented  by various 
SMILES strings as means of data augmentation  and we explore several augmentation  techniques.  Convolutional  
and recurrent  neural networks  are trained on four data sets, including  experimental  solubility,  lipophilicity,  and 
bioactivity  measurements.  Moreover,  the uncertainty  of the models is assessed  by applying  augmentation  on the 
test set. Our results show that data augmentation  improves  the accuracy  independently  of the deep learning  
model and of the size of the data. The best strategies  lead to the Maxsmi models, the models that max imize the 
performance  in SMI LES augmentation.  Our Ô¨Åndings  show that the standard  deviation  of the per SMILES pre- 
diction correlates  with the accuracy  of the associated  compound  prediction.  In addition,  our systematic  testing 
of diÔ¨Äerent  augmentation  strategies  provides  an extensive  guideline  to SMILES augmentation.  A prediction  tool 
using the Maxsmi models for novel compounds  on the aforementioned  physico-chemical  and bioactivity  tasks is 
made available  at https://github.com/volkamerlab/maxsmi  . 
1. Introduction  
Drug design is a time-consuming  and costly process  [1,2] with high 
attrition  rates [3] . It can be supported  with in silico methods  by guiding  
the design process,  optimizing  compounds,  and discarding  those with 
undesired  properties  at an early stage of development.  In this context,  
computer-aided  drug design (CADD)  has become  central in the drug 
discovery  pipeline  and is widely adopted  in research  and development  
in both academia  and pharmaceutical  companies.  
Over the last few decades,  there has been a keen interest  in ma- 
chine learning  (ML) and more speciÔ¨Åcally  deep learning  (DL), which 
have been applied  to a variety of areas, covering  computer  vision [4] , 
speech recognition  [5] , as well as the life sciences.  Only to name a few, 
AlphaFold  2 from DeepMind  which predicts  protein  folding [6] , Poten- 
tialNet which focuses  on protein-ligand  binding  aÔ¨Énity  [7] , de novo 
molecular  design suitable  for compound  optimization  [8] , and cytotox-  
icity prediction  as in the work by Webel et al. [9] . Such excitement  in 
DL may be explained  by the main three following  factors [10] . 
‚àó Corresponding  author. 
E-mail addresses:  talia.kimber@gmail.com  (T.B. Kimber),  andrea.volkamer@charite.de  (A. Volkamer).  1. The gain of computational  power through  graphics  processing  
units (GPUs) and tensor processing  units (TPUs).  Platforms  such 
as Google Colaboratory  [11] allow any user to exploit high perfor- 
mance computing  resources  without  any cost and such free and easy 
access is unprecedented.  
2. The ever growing  amount  of available  data. More data are created  
and stored in databases  every day in various  Ô¨Åelds. Many processes  
are automatized  making  data more accessible  and usable either in- 
ternally,  as in pharmaceutical  companies,  or publicly.  For example  
in academic  research,  in competitions,  such as Kaggle [12] , or in 
challenges,  such as D3R ‚Äìt h e Drug Design Data Resource  chal- 
lenge [13] or the Tox21 challenge  [14] . 
3. The advances  in algorithms,  making  models perform  better than ever 
before. Deep learning  algorithms  may surpass  human performance  
if trained  on a data set containing  over 10 million  data points, as 
suggested  by Goodfellow  et al. [10] . 
With the rise of ML/DL research,  many applications  have been ex- 
tended to the Ô¨Åeld of molecular  property  and aÔ¨Énity  prediction,  even 
though insuÔ¨Écient  data remains  a challenge  in the Ô¨Åeld. 
https://doi.org/10.1016/j.ailsci.2021.100014  
Received  4 October  2021; Received  in revised form 13 November  2021; Accepted  15 November  2021 
Available  online 18 November  2021 
2667-3185/¬©2021  The Authors.  Published  by Elsevier B.V. This is an open access article under the CC BY-NC-ND  license 
( http://creativecommons.org/licenses/by-nc-nd/4.0/  ) T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Encoding  molecular  compounds  in both human-  and computer-  
readable  formats  is a necessary  step in CADD. A convenient  encoding  
is SMILES,  or simpliÔ¨Åed  molecular-input  line-entry  system [15] . As the 
name suggests,  SMILES  is a linear notation  of a molecule  based on atom 
and bond enumeration,  as well as branch,  ring closure,  and disconnec-  
tion speciÔ¨Åcation.  Several  advantages  arise from this compact  represen-  
tation. 
1. The printable  characters  make SMILES  easily readable  by computers  
and decipherable  by humans.  
2. Being a single line, SMILES  resemble  words and are therefore  cheap 
to store. 
3. Such a notation  is very popular  and many open-source  databases  
store compounds  in SMILES.  
However,  there is a trade-oÔ¨Äbetween  readability  and speciÔ¨Åcation:  
having a compact  encoding  means losing detailed  information  about the 
molecule  such as 2D or 3D features.  Moreover,  subtle chemistry  rules 
such as aromaticity  do not have a standard  way of being handled  [16] . 
The implementation  of a SMILES  given a compound  can be described  
as follows:  from any starting  atom in the molecule,  enumerate  the atoms 
and bonds following  a path in the molecular  graph. Two aspects  of this 
construction  lead to the non-uniqueness  of SMILES:  1. the atom to start 
the enumeration  from, and 2. the path to follow along the graph. 
Therefore,  one molecule  can have many diÔ¨Äerent  valid SMILES,  sim- 
ply by starting  the enumeration  from a diÔ¨Äerent  atom or by choosing  
a diÔ¨Äerent  path. Nevertheless,  in some settings,  having a bijection  be- 
tween a molecule  and its SMILES  notation  may be sought.  For exam- 
ple, when determining  the overlapping  molecules  from two data sets. 
In this context,  most cheminformatics  tools have their own algorithm  
implemented  allowing  them to always retrieve  the same SMILES  given 
a molecular  graph, such a SMILES  is called canonical  [17] . 
As mentioned  previously,  deep learning  being data greedy and both 
physico-chemical  and bioactivity  databases  being meager,  elaborate  
techniques  have to be integrated  to unleash  the full potential  of deep 
neural networks.  In this context,  data augmentation  in general  [18,19]  , 
and more speciÔ¨Åcally  SMILES  augmentation  [20‚Äì23]  , is a powerful  as- 
sistance  in molecular  prediction.  From a machine  learning  perspective,  
data augmentation  allows the model to see the same object through  
diÔ¨Äerent  angles and has been successfully  applied  in image classiÔ¨Åca-  
tion [4,24] , where images undergo  transformations  such as Ô¨Çipping,  
coloring,  cropping,  rotating,  and translating.  From a computational  per- 
spective,  SMILES  augmentation  is advantageous  because  generating  ran- 
dom SMILES  is fast and memory  eÔ¨Écient,  and even though training  
a model may be more computationally  expensive,  it remains  cheap to 
evaluate.  
The Ô¨Årst occurrence  of SMILES  augmentation  in QSAR modeling  
was developed  by Bjerrum  [20] , where aÔ¨Énity  against  dihydrofolate  
reductase  (DHFR)  is predicted  on a small data set of 756 compounds.  
The model consists  of long short-term  memory  (LSTM)  layers and a 
fully connected  layer for the normalized  log ùêºùê∂ 50 value. Each molecule  
in the data set is augmented  on average  130 times. The model with 
SMILES  augmentation  reaches  a test correlation  coeÔ¨Écient  of 0.68, a 
0.12 increase  with respect to the canonical  model. From then on, sev- 
eral studies have built on the same idea, applying  SMILES  augmenta-  
tion in QSAR modeling  [21] . Moreover,  convolutional  neural networks  
have successfully  been applied  in the context  of SMILES  augmentation,  
outperforming  models using traditional  molecular  descriptors  [22,23]  . 
Such augmentation  techniques  have also emerged  in related Ô¨Åelds, such 
as retrosynthesis  [25,26]  and generative  modeling  [27,28]  . While all 
these studies show the beneÔ¨Åt of augmenting  the data, none of them 
focus, to the best of our knowledge,  on a systematic  analysis  on how 
to augment  the data set best, and most decide a priori on an aug- 
mentation  number.  This study aims at Ô¨Ålling this gap by oÔ¨Äering  a 
systematic  augmentation  approach,  both in the augmentation  strate- 
gies and by how much the data should be augmented.  Moreover,  a 
command-line  interface  is available  for users interested  in the predic- tion of physico-chemical  properties  for novel molecules  and assessing  
the uncertainty  of the prediction.  To this end, all code is made freely 
available  at https://github.com/volkamerlab/maxsmi  . 
2. Methods  
In this section,  we Ô¨Årst describe  diÔ¨Äerent  augmentation  strategies  
which can be used for data augmentation  when dealing  with SMILES.  
Second,  we illustrate  how SMILES  augmentation  can be viewed  as an 
ensemble  learning  technique  when it comes to prediction.  We then ex- 
amine the deep learning  models that are trained  in this study. 
2.1. Augmentation  strategies  
As discussed  in the Introduction,  one compound  can have several 
valid SMILES,  since both the starting  atom and the path along the molec- 
ular graph used to generate  the SMILES  can diÔ¨Äer. Here, the way a user 
can explore  such random  SMILES  is detailed  and Ô¨Åve strategies  to aug- 
ment a single SMILES  to multiple  SMILES  are described:  no augmen-  
tation, augmentation  with duplication,  augmentation  without  duplica-  
tion, augmentation  with reduced  duplication,  and augmentation  with 
estimated  maximum.  For the following  sections,  we assume  that we are 
given a data set ùê∑, containing  ùëÅpairs  of {compound,  label}. Label refers 
to the measured  property,  such as lipophilicity  or solubility.  The imple- 
mentation  of these strategies  is based on the open-source  cheminformat-  
ics software  RDKit [29] . 
2.1.1. No augmentation  
The level zero to augmentation  is having no augmentation  or, in 
other terms, augmentation  of zero. This means that given a data set ùê∑with
 ùëÅcompounds,  the ‚Äúno augmentation  ‚Äùversion  of ùê∑also contains  
ùëÅSMILES.  More speciÔ¨Åcally,  in this setting,  the SMILES  associated  with 
each compound  is the canonical  SMILES.  
2.1.2. Augmentation  with duplication  
Generating  random  SMILES  implies  picking  at random  an initial 
atom and following  a random  path along the molecular  graph. Aug- 
menting  the data set ùê∑by ùëö means that for each of the N compounds  in 
ùê∑, ùëö instances  of random  SMILES  are drawn and the associated  labels 
are matched  for each compound.  In this case, augmenting  ùê∑by ùëö would 
result in the augmented  data set containing  ùëÅ √óùëö data points. In this 
scenario,  all molecules  in ùê∑are multiplied  by the same factor ùëö . Conse- 
quently,  smaller  molecules,  with fewer SMILES  variations,  will contain  
more duplicates  whereas  larger molecules  are more likely to cover a di- 
verse set of random  SMILES.  A disadvantage  of such an augmentation  
strategy  is that SMILES  corresponding  to small molecules  will be over- 
represented  in the data set and could create a bias in model training.  
2.1.3. Augmentation  without duplication  
Removing  duplicated  entries is common  in data wrangling  [30] . In 
the context  of SMILES  augmentation,  this translates  to discarding  du- 
plicates  after having generated  a number  of random  SMILES.  For data 
set ùê∑, the Ô¨Ånal number  of data points after augmentation  varies accord-  
ing to the augmentation  number,  i.e. the number  of times a sample is 
drawn from the valid SMILES  space, and the size of the molecules  in the 
data set. A disadvantage  of such an augmentation  strategy  is that small 
molecules,  which presumably  possess  fewer unique SMILES  representa-  
tives, will be under-represented  in the data set and could create a bias 
in model training.  
2.1.4. Augmentation  with reduced  duplication  
In order to Ô¨Ånd a compromise  between  keeping  or removing  all du- 
plicates,  the notion of augmentation  with reduced  duplication  is intro- 
duced. In this setting,  only a fraction  of the number  of duplicates  is kept. 
Mathematically  speaking,  if the data set ùê∑is augmented  by ùëö , then a 
2 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
function  ùëì( ùëö ) which grows slower than linear is used to control the num- 
ber of replicas  kept for each SMILES.  Sensible  functions  would be the 
squared  root function  ùëì( ùëö ) = ‚àö
ùëö or the natural  logarithm  ùëì( ùëö ) = ln ( ùëö ) , 
the former being used for the experiments  in this study. 
A corner case of the former three augmentation  strategies  by aug- 
mentation  number  ùëö is when ùëö = 1 . In this instance,  a random  SMILES  
will be generated  and the number  of data points would still be ùëÅ, as in 
the ‚Äúno augmentation  ‚Äùcase, with the diÔ¨Äerence  that ‚Äúno augmentation  ‚Äùcontains
 canonical  SMILES  only. 
2.1.5. Augmentation  with estimated  maximum  
The Ô¨Ånal strategy  described  is augmentation  with estimated  max- 
imum, which aims to cover a wide range of the valid SMILES  space 
for a given compound,  or in other words, to generate  a number  of 
unique SMILES  that depends  on the compound.  In our study, the im- 
plementation  of this augmentation  strategy  randomly  samples  SMILES  
corresponding  to a compound,  and the sampling  process  is stopped  
once the same SMILES  string has been generated  a pre-deÔ¨Åned  num- 
ber of times. The experiments  of this study set 10 generations  of 
the same SMILES  as a stopping  criterion.  It is noteworthy  that the 
number  of SMILES  this method  generates  is highly dependent  on the 
size of the compound,  unlike the previous  methods  which always 
generate  a number  of SMILES  bounded  by ùëö . For example,  our im- 
plementation  of this augmentation  strategy  generated  50 659 unique 
SMILES  variations  for the compound  given by the canonical  SMILES  
CC( = O)C1(C)CCC2C3C  = C(C)C4  = CC( = O)CCC4(C)C3CCC21C,  whereas  
only three were generated  for the canonical  SMILES  C = CC = C, namely  
C( = C)C = C, C(C = C) = C, and C = CC = C. 
2.2. SMILES  augmentation  as ensemble  learning  for compound  prediction  
and conÔ¨Ådence  measure  
The application  of data augmentation  strategies  during training  has 
proven to be successful,  as shown in previous  works [4,24] . In QSAR 
modeling  particularly,  SMILES  augmentation  is not only beneÔ¨Åcial  on 
the training  set [22] , but there are also advantages  of augmenting  the 
test set, or more generally,  an unlabeled  data set, as explained  in this 
section.  
Let us assume  that a model ùëÄwith  a set of parameters  Œòwas 
trained  for a certain number  of epochs.  Let us consider  an unlabeled  
data set containing  ùëÅcompounds  for which we want to make pre- 
dictions.  Each compound  ùê∂can be augmented  using random  SMILES:  
ùëÜ 1 ( ùê∂) , ùëÜ 2 ( ùê∂) , ‚Ä¶, ùëÜ ùëò ( ùê∂) , where ùëò depends  on the strategy.  The model ùëÄ Œò
produces  a prediction  for each of those SMILES,  i.e. for ùëñ ‚àà{ 1 , ‚Ä¶, ùëò } 
ÃÇùë¶ ùëñ ( ùê∂) = ùëÄ Œò( ùëÜ ùëñ ( ùê∂)) , 
leading  to a per SMILES  prediction  rather than a per compound  predic- 
tion. Using an aggregation  function  ùê¥ ‚à∂ ‚Ñù ùëò ‚Üí‚Ñù , such as the mean, a 
prediction  for compound  ùê∂can be computed  as 
ÃÇùë¶ ( ùê∂) = ùê¥ (ÃÇùë¶
 1 ( ùê∂) , ‚Ä¶, ÃÇùë¶ ùëò ( ùê∂) ).
 
Such aggregation  can be viewed  as a consensus  among the SMILES  pre- 
diction and interpreted  as ensemble  learning  for a given compound.  
Additionally,  if the standard  deviations  of the predictions  are com- 
puted, they can be interpreted  as a conÔ¨Ådence  in the molecular  property  
or activity  prediction.  If the standard  deviation  is large, then there is a 
high variation  in the per SMILES  prediction,  and the model is uncertain  
in its per compound  prediction.  An illustration  of such a molecular  pre- 
diction is shown in Fig. 1 . Following  the rationale  by Tagasovska  and 
Lopez-Paz  [31] , the aleatoric  and epistemic  uncertainties  are often in- 
tertwined;  the uncertainty  computed  in our work rather falls into the 
aleatoric  category,  a type of uncertainty  linked to the model predictions  
and the randomness  in the input data [31‚Äì33]  . 2.3. Deep learning  models 
Neural networks  are powerful  algorithms  that allow accurate  predic- 
tions on various  tasks. In the case of QSAR/ML/DL  modeling  and more 
speciÔ¨Åcally  the use of SMILES  representation,  two types of models can 
be applied,  convolutional  [ 10 , Chapter  9] and recurrent  [ 10 , Chapter  
10] neural networks.  
In this study, comparing  deep learning  models and how they perform  
with respect to data augmentation  is one of the key focuses.  To this end, 
three types of models are architectured  and trained,  namely  1D and 2D 
convolutional  neural networks  (CONV1D,  CONV2D)  as well as a recur- 
rent neural network  (RNN). The architecture  of the recurrent  network  
consists  of an LSTM layer, followed  by two fully connected  layers of 128 
and 64 units, respectively.  It was inspired  by Bjerrum  [20] , in which an 
LSTM layer is followed  by a single 64 unit fully connected  layer. Using a 
similar approach,  a single 1D convolutional  layer of kernel size 10 and 
stride 1 is applied  in the CONV1D  model. Two fully connected  layers 
follow the convolution.  The CONV2D  adheres  to the same pattern  but 
instead  of using a 1D convolution,  a 2D convolution  operation  is per- 
formed  using one single channel.  Finally,  all three architectured  models 
stay consistent  in the depth of the network  and remain shallow.  
In this study, all deep learning  models are trained  for 250 epochs,  
using mini-batches  of size 16, where the mean squared  error is the con- 
sidered  loss. Optimization  is done with stochastic  gradient  descent  and 
a learning  rate of 0.001. Note that a Ô¨Åxed number  of epochs is used in 
this study, but for sake of completeness,  three sample models with early 
stopping  were also run. The results with and without  early stopping  did 
not change signiÔ¨Åcantly  (data not shown).  Moreover,  some models were 
trained  by adapting  the number  of epochs with respect to the augmenta-  
tion number,  but this only proved to overÔ¨Åt the training  set and yielded  
the same results on the test set as training  with 250 epochs (data not 
shown).  
3. Data and experimental  setup 
This section introduces  the data sets used in this study, namely  their 
provenance  as well as the required  preprocessing.  Furthermore,  a step by 
step instruction  for eÔ¨Écient  SMILES  augmentation  is described.  Finally,  
the evaluation  design and experimental  setup are covered.  
3.1. Provenance  
The data in this research  come from two sources:  MoleculeNet  [34] , 
and the ChEMBL  database  [35] , chosen for two main reasons.  
1. They are freely available  and easily downloadable  or retrievable.  
2. They are often used as benchmarks  for study comparison  [22,36,37]  . 
For tasks in MoleculeNet,  we focus on physico-chemical  predic- 
tion tasks and retrieve  the data from the three following  sets of vary- 
ing sizes, all available  as part of DeepChem  [38] at https://deepchem.  
readthedocs.io/en/latest  . 
1. Measured  water solubility  is referred  to as the ESOL data set [39] . 
The raw data contains  1 128 data points. This data set is further 
processed  to only include  molecules  with at most 25 heavy atoms 
for experimental  setup and is referred  to as ESOL_small.  
2. The FreeSolv  [40] data set consists  of 642 pairs of SMILES  and 
experimental  hydration  free energy of small molecules  in water 
(kcal/mol).  
3. The lipophilicity  data set originates  from ChEMBL  [35] and contains  
4 200 pairs of SMILES  and experimental  values of octanol/water  
distribution  coeÔ¨Écient.  
Bioactivity  data can be found in large quantities  in ChEMBL.  To 
date, over 18 million  activities  are stored in the database,  covering  
3 T.B. Kimber, M. Gagnebin and A. Volkamer  ArtiÔ¨Åcial Intelligence  in the Life Sciences 1 (2021) 100014 
Fig. 1. Compound  prediction  and conÔ¨Ådence  measure  thanks to SMILES augmentation.  Given a compound  represented  by its canonical  SMILES,  a set of 
random SMILES are generated.  The trained machine  learning  model produces  a prediction  for each of the SMILES variations.  Aggregating  these values leads to a per 
compound  prediction  and computing  the standard  deviation  is interpreted  as an uncertainty  in the prediction.  
Table 1 
Data sets for this study. Size of the data sets before and after preprocessing,  as well as the size of the training  and test sets before applying  an augmentation  
strategy,  and the provenance  of the data. 
Dataset Size before preprocessing  Size after preprocessing  Train set 80%, before augmention  Test set 20%, before augmention  Provenance  
ESOL 1 128 1 128 902 226 MoleculeNet  a 
ESOL_small  1 128 1 068 854 214 MoleculeNet  a 
FreeSolv 642 642 513 129 MoleculeNet  a 
Lipophilicity  4 200 4 199 3 359 840 MoleculeNet  a 
AÔ¨Énity (EGFR) 6 026 5 849 4 679 1 170 Kinodata  b 
a https://deepchem.readthedocs.io/en/latest  . 
b https://github.com/openkinome/kinodata  . 
more than 14 000 targets and two million  compounds  [41] . Among  tar- 
gets, kinases  are a well studied  protein  family due to their involve-  
ment, among others, in cancer and inÔ¨Çammatory  diseases  [42] . Kin- 
odata, from the Openkinome  organization  [43] , provides  an already  
curated  data set of human kinase bioactivities,  retrieved  from one of 
the latest versions  to date of ChEMBL  (version  28) and is freely avail- 
able at https://github.com/openkinome/kinodata  . Moreover,  for this 
study, Kinodata  is further Ô¨Åltered  for the epidermal  growth  factor re- 
ceptor (EGFR)  kinase [44] , since it is known to be an important  drug 
target. Its UniProt  identiÔ¨Åer  is given by P00533  [45] . AÔ¨Énity  towards  
the EGFR kinase is quantiÔ¨Åed  using ùëùùêºùê∂ 50 values, the negative  base 10 
logarithm  of ùêºùê∂ 50 [46] . Information  about the data set provenance  and 
size are detailed  in Table 1 . 
3.2. Data preprocessing  and input featurization  
In order to train a deep learning  neural network  on data containing  
molecular  compounds,  the data set undergoes  preprocessing  and com- 
pounds  encoding.  
Once the data sets are retrieved  from their original  source, invalid 
SMILES,  detected  by RDKit [29] , not available  (NA) values and dis- 
connected  compounds,  marked  by a dot in a SMILES,  are removed.  
Molecules  are transformed  to the canonical  SMILES  representation,  us- 
ing RDKit functionalities.  
For model training,  the SMILES  are one-hot  encoded,  based on a 
dictionary  of unique symbols  constructed  from the SMILES  in the data. 
Atoms represented  by two letters, such as Br for bromine  or Cl for chlo- 
rine, as well as @@ for chirality  speciÔ¨Åcation,  are treated as if single symbols.  Finally,  all inputs are padded  up to the length of the longest  
SMILES.  The reader is kindly referred  to the work by Kimber  et al. 
[47] for further details on one-hot  encoding  and padding.  
3.3. Important  steps in SMILES  augmentation  
When processing  SMILES  for augmentation,  some technical  aspects  
are essential.  This section assumes  a training  and test split, but the ra- 
tionale is the same in the presence  of a validation  set. 
Firstly, it is important  that the data are Ô¨Årst split and then aug- 
mented,  rather than augmented  and split. In the latter case, one com- 
pound could have SMILES  appearing  in both training  and testing leading  
to most probably  excellent  performance,  but yet statistically  incorrect.  
Secondly,  storing values such as the length of the longest  SMILES  or 
the dictionary  of characters  should be done not before but after aug- 
menting  the data. Indeed,  augmentation  may lead to the extension  of 
the dictionary  as well as the lengthening  of SMILES.  For example,  the 
canonical  SMILES  CCCC consists  of the letter C solely and contains  four 
characters.  However,  one of its possible  random  variations  is C(C)CC,  
which not only introduces  new characters,  such as the opening  ‚Äú( ‚Äùand 
closing ‚Äú) ‚Äùo f branches  but is composed  of six characters.  Therefore,  
critical values such as length and dictionary  should be retained  after 
augmentation.  
Finally,  these same values should be computed  on the union of the 
training  and the test set for the smooth  training  and evaluation  of the 
model. Indeed,  if the dictionary  of characters  is only built on the basis 
of the SMILES  in the training  data, there might be additional  atoms, or 
characters  in the test set that the model will not recognize  and will not 
4 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
be able to one-hot  encode.  Moreover,  if the length of the longest  SMILES  
is taken from the training  set and not the union of the training  and test 
sets, augmentation  on the test set could produce  a longer SMILES  than 
the longest  one in the training  set, leading  to dimensionality  errors. 
For all the above reasons,  it is important  for machine  learning  engi- 
neers to abide by the steps as described  in this section for statistically  
correct results,  as well as programmatic  error-free  model training  and 
evaluation.  
3.4. Experimental  setup and model evaluation  
In order to draw a conclusion  on the eÔ¨Éciency  of data augmentation,  
three data sets of varying  sizes are considered,  namely  ESOL, FreeSolv,  
and lipophilicity  (see the Provenance  section).  For each of these sets, the 
data are split once into 80% training  and 20% test set, with a Ô¨Åxed ran- 
dom seed for testing to be consistent  with the augmentation  schemes.  
Given all possible  combinations  between  the Ô¨Åve augmentation  strate- 
gies and diÔ¨Äerent  augmentation  numbers,  the three deep learning  mod- 
els, and the various  data sets, including  cross-validation  would have 
added considerable  computational  costs and has therefore  not been im- 
plemented  in this study. 
For model evaluation,  the root mean squared  error (RMSE)  [48] on 
the test set is reported,  so that the lower the RMSE value, the better the 
model. However  additional  information  such as the measure  of goodness  
of Ô¨Åt, also known as the R2 value [49] , on both training  and test sets, 
as well as the time required  for model training  and evaluation  are also 
stored. 
Five augmentations  strategies  are studied:  No augmentation,  which 
considers  the canonical  SMILES  representation.  The augmentations  
with, without,  and with reduced  duplication,  for numerous  augmen-  
tation numbers:  a Ô¨Åner grid from 1 to 20 with a step size of 1, and a 
coarser  grid from 20 to 100 with a step size of 10. Finally,  the estimated  
maximum  strategy  where a SMILES  representation  has to be generated  
10 times for the process  to stop. For this last strategy,  the ESOL_small  
data set (see Table 1 ) is used to keep the augmentation  to a reasonable  
time-scale.  For the same reason,  the same augmentation  strategy  is not 
run on the lipophilicity  data set. 
The augmentation  strategies  are applied  to both the training  set and 
the test set, so that for example,  if the FreeSolv  training  data set is aug- 
mented  20 times without  duplication,  then so would the FreeSolv  test 
set. 
Ensemble  learning  is applied  on each test set and the mean is used as 
aggregation.  However,  a user could easily adapt it to another  function,  
such as the median.  The standard  deviation  is stored for each compound  
in the test set. 
Moreover,  a Random  Forest (RF) model [50] is used as a baseline,  
with all default parameters  from Scikit-learn  [51] . The inputs to the 
model are the Morgan  Ô¨Ångerprints  of radius 2 and length 1024. Aug- 
mentation  strategies  as discussed  above are not applicable  in the context  
of Ô¨Ångerprints.  
Simulations  are run on a GeForce  GTX 1080 Ti, provided  by the cen- 
tral HPC cluster of the Freie Universit√§t  Berlin [52] . 
3.5. Code and documentation  
All code is written  in Python 3 [53] following  PEP8 style 
guide [54] and is freely available  at https://github.com/volkamerlab/  
maxsmi  . Results of this study can be found at the same link. Examples  
and documentation,  generated  via Read the Docs [55] , can be found at 
https://maxsmi.readthedocs.io/en/latest/  . 
Package  management  is done with Anaconda  [56] . RDKit [29] is 
used for cheminformatics,  PyTorch  [57] for deep learning,  and other 
popular  packages  such as Scikit-learn  [51] , NumPy  [58] , and Pan- 
das [59] for general  purposes.  Continuous  integration  is deployed  with 
Github actions [60] ensuring  runs on Linux, Mac, and Windows  operat-  ing systems.  Unit tests are done with Pytest [61] , and code coverage  is 
measured  via Codecov  [62] . 
4. Results  and discussion  
This section gives a thorough  analysis  of the results that are obtained  
using the experimental  setup described  in the previous  section and pro- 
vides the reader with guidelines  on data augmentation  applicable  to new 
data and exempliÔ¨Åed  with aÔ¨Énity  measurements  towards  the EGFR ki- 
nase. An example  of the user prediction  for compounds  through  a simple 
command-line  interface  is described.  
4.1. SMILES  augmentation  improves  model performance  
As mentioned  previously,  deep learning  models are data greedy and 
the Ô¨Åndings  of our study reinforce  this statement  by a systematic  anal- 
ysis of performance  diÔ¨Äerences  when augmenting  the input data. Feed- 
ing a neural network  with diÔ¨Äerent  SMILES  representations  of the same 
compound  leads to better performing  models,  as shown in Figs. 2, A1 , 
and A2 . Improvements  are also visible with respect to the baseline  
model. These observations  are made on all three physico-chemical  data 
sets, namely  ESOL, FreeSolv,  and lipophilicity,  independently  of the data 
set size that ranges between  approximately  600 and 4 000 compounds  
(see Table 1 ). For example,  the ESOL performance  with no augmenta-  
tion has an RMSE value of 0.839 for the CONV1D  model, whereas  the 
performance  of the same model with reduced  augmentation  and ùëö = 70 
achieves  an RMSE as low as 0.569, see Fig. 2 . As the number  of augmen-  
tation increases,  the RMSE values become  smaller,  indicated  by lighter 
shades of purple in Figs. 2, A1 , and A2 . Note that at Ô¨Årst, as the aug- 
mentation  number  increases  in the single digits, there is a clear increase  
in the performance  of the models.  For example,  for the lipophilicity  
data set augmented  with duplication  and the CONV2D  model, the sin- 
gle random  SMILES  model has an RMSE value of 1.309 and reaches  
values below 1 as of an augmentation  number  of 4 (see Fig. A2 ). On the 
ESOL data set, the RNN model without  duplication  starts at an RMSE 
of 1.016 and reaches  values below 0.8 after only an augmentation  of 5 
(see Fig. 2 ). However,  the performance  steadily  reaches  a plateau.  For 
example,  the RMSE of the CONV1D  model trained  on FreeSolv  is slightly  
above 1 as of 20 number  of augmentation  and Ô¨Çuctuates  around this 
value thereafter,  as shown in Fig. 3 . Similar  observations  can be made 
for ESOL and lipophilicity.  Using the same model, the RMSE on ESOL 
reaches  a plateau  around 0.60 at 40 augmentation  steps (see Fig. A4 ) 
and lipophilicity  around 0.60 at 60 (see Fig. A5 ). This result suggests  
the following:  
1. There does not seem to be one optimal  value that particularly  stands 
out. 
2. A trade-oÔ¨Ä between  performance  and computation  time must be 
found. As expected,  the computation  time increases  as the number  
of data points increases,  as shown in Fig. A6 . 
4.1.1. Deep learning  model performance  by architecture  
Not only does augmenting  the data set overall help the learning  for 
all three considered  tasks, but so is the case for all three deep learn- 
ing architectures.  This leads to the observation  that augmentation  im- 
proves performance  independently  of the deep learning  model, suggest-  
ing that for any future QSAR study for molecular  property  prediction  
using SMILES  and deep learning,  SMILES  augmentation  should be the 
method  of choice. However,  in this particular  study and these particular  
deep learning  architectures,  results point to the fact that the CONV1D  
model tends to outperform  the RNN model, which itself seems to out- 
perform  CONV2D.  As shown in Fig. 4 , on the ESOL data using augmen-  
tation with reduced  duplication,  as of an augmentation  number  of 40, 
the RMSE value of the CONV2D  model Ô¨Çuctuates  around 0.7, the RNN 
model around 0.65 and the CONV1D  around 0.6, promoting  the latter 
model to best performing  model. This exhibits  the power of convolutions  
5 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Fig. 2. Test RMSE using data augmentation  on the ESOL data set. The table shows the root mean squared error (RMSE) on the test set for three deep learning  
models and Ô¨Åve SMILES augmentation  strategies,  using various augmentation  numbers,  as well as a baseline  consisting  of a Random  Forest (RF) model with Morgan 
Ô¨Ångerprint  as input. The lighter the purple color, the better the model. The overall best setting is highlighted  in yellow, which for the ESOL data set is augmenting  
the data set 70 times using a reduced  number of duplicates  and training  a 1D convolutional  neural network  (CONV1D).  For interpretation  of the references  to color 
in this Ô¨Ågure legend, the reader is referred  to the web version of this article. 
6 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Fig. 3. Performance  reaches  a plateau  independently  of the augmentation  
strategy.  The performance  of the CONV1D  model trained and evaluated  on 
the FreeSolv  data set reaches a test RMSE value slightly above 1 as of 20 aug- 
mentation  steps and Ô¨Çuctuates  around this value thereafter,  for all augmenta-  
tion strategies:  with, without,  and with reduced  duplication.  For the ESOL and 
lipophilicity  data, see Figs. A4 and A5 . 
Fig. 4. The 1D convolutional  (CONV1D)  model outperforms  the recur- 
rent (RNN) and 2D convolution  (CONV2D)  models.  The Ô¨Ågure shows the 
evolution  of the root mean squared error (RMSE) on the test set with respect 
to the number of augmentation  using reduced  duplication  on the ESOL data. 
CONV1D  outperforms  RNN, which outperforms  CONV2D.  
and their ability to extract relevant  features  in compounds  based on one- 
hot encoded  SMILES  input. This also implies  that although  applying  2D 
convolutions  to SMILES  is programmatically  feasible,  1D convolutions  
are better suited than 2D convolutions,  the latter having shown great 
success  in image classiÔ¨Åcation.  Indeed,  when considering  the one-hot  
encoded  matrix,  SMILES  are more similar to words, in which the posi- 
tion of the atoms is important,  rather than to images.  
4.1.2. There is no best augmentation  strategy  applying  to all data sets 
From an augmentation  strategy  point of view, conclusions  are not 
straightforward.  The three augmentation  strategies,  namely  with, with- 
out, and with reduced  duplication,  all perform  similarly  well, without  
one standing  out. For example,  the test RMSE on the FreeSolv  data set 
trained  using the CONV1D  model reaches  values just above 1 for all 
three strategies,  as shown in Fig. 3 . 
Moreover,  generating  a large portion  of the SMILES  space using the 
strategy  with estimated  maximum  surprisingly  does not lead to the best 
results.  On the ESOL data set, this strategy  reaches  a test RMSE of 0.683 
using RNN, whereas  the same model but using strategies  with, without,  
and with reduced  duplication  already  outperforms  the estimated  max- 
imum as of an augmentation  number  of 19 and onward,  as shown in 
Fig. 5. Generating  a large portion  of the SMILES space does not necessarily  
lead to the best performance.  Even though the RNN model is presented  with 
SMILES variations  that cover a large portion of the SMILES space using the 
augmentations  strategy  with estimated  maximum,  on the ESOL data set, this 
strategy  does not achieve the best results. 
Fig. 5 . Although  less obvious  than in the ESOL case, a similar conclu-  
sion can be made on the FreeSolv  data set and for example  the CONV1D  
model, as shown in Fig. A3 . This suggests  that there might be a point of 
saturation,  where the neural network  stops learning,  even though being 
fed more data. 
4.1.3. Maxsmi  models:  Best performing  model per data set 
From the results of the experiments,  as mentioned  previously,  there 
does not seem to be one augmentation  strategy  that fully stands out, nei- 
ther does a particular  model. However,  from a purely numerical  stand- 
point, there is an optimal  performance  value and this value is high- 
lighted in yellow in Figs. 2, A1 , and A2 . For the ESOL data set, the tuple 
of (model,  augmentation  number,  augmentation  strategy)  that yields 
best performance  is the CONV1D  model, an augmentation  number  of 
70 and keeping  a reduced  number  of duplicates.  For the FreeSolv  data 
set, the same model but generating  70 random  SMILES  keeping  all dupli- 
cates is the best setting.  Finally,  for lipophilicity,  generating  80 random  
SMILES  and removing  duplicates  leads to the best performance.  Given 
these three best models,  we select them for further analysis,  henceforth  
calling them Maxsmi  models and summarized  in Table 2 . 
4.1.4. Performance  comparison  between  canonical  and random  SMILES  
One interesting  observation  from this study is the performance  com- 
parison  between  training  a model with canonical  SMILES  versus train- 
ing a model using one random  SMILES  representation,  in other terms, 
augmentation  of 1. The canonical  model systemically  outperforms  the 
model that uses a random  SMILES.  More speciÔ¨Åcally,  for the ESOL data 
set, the canonical  model reaches  an RMSE value of 0.839 using CONV1D,  
whereas  the random  version  0.964 with the same model. In the Free- 
Solv and lipophilicity  cases, the canonical  model yields an RMSE value 
of 1.963 and 0.994, versus 2.577 and 1.268 for random  SMILES.  A pos- 
sible explanation  for such an outcome  is the simplicity  in the canoni-  
cal SMILES  representation.  The algorithm  in RDKit produces  the more 
readable  SMILES  representation,  one that avoids branches,  as well as 
nested branches.  Table 3 shows some of these diÔ¨Äerences.  For example,  
a random  version  might add brackets,  where the canonical  version  has 
none (see the Ô¨Årst row in Table 3 ), it might add sets of brackets,  where 
the canonical  version  keeps them to a minimum  (see the second row in 
Table 3 ) and the random  version  even allows nested brackets  where the 
canonical  version  avoids them (see the last row in Table 3 ). 
To conclude  with this observation,  if SMILES  augmentation  cannot 
be applied  for future studies for any reason,  practitioners  are highly 
7 T.B. Kimber, M. Gagnebin and A. Volkamer  ArtiÔ¨Åcial Intelligence  in the Life Sciences 1 (2021) 100014 
Table 2 
The b est augmentation  strategies  deÔ¨Åne the Maxsmi  models.  After training  three data sets (ESOL, FreeSolv,  and lipophilic-  
ity) on various deep learning  models (CONV1D,  CONV2D,  and RNN), using diÔ¨Äerent  augmentation  numbers  and strategies,  
the setting that yields the best performance,  or lowest root mean squared error (RMSE) on the test set is selected  and named 
the Maxsmi model. 
Data Model Augmentation  number Augmentation  strategy Test RMSE 
ESOL CONV1D  70 With reduced duplication  0.569 
FreeSolv CONV1D  70 With duplication  1.032 
Lipophilicity  CONV1D  80 Without duplication  0.593 
Table 3 
Models based on the canonical  SMILES outperform  the ones based on a single random  SMILES.  The test prediction  for a 
model trained and evaluated  on the RDKit canonical  SMILES systematically  performs  better than the same model trained and 
evaluated  on a single random SMILES.  ESOL is the prediction  task leading to the values in the table. 
Canonical  SMILES Random SMILES True value Canonical  SMILES prediction  (&error) Random SMILES prediction  (&error) 
CCCCCC C(C)CCCC  ‚àí3 . 84 ‚àí2 . 87 (0.97) ‚àí2 . 77 (1.07) 
CCCC( = O)CC C( = O)(CCC)CC  ‚àí0 . 83 ‚àí1 . 37 (0.54) ‚àí1 . 65 (0.82) 
CCCC( = O)OCC C(OC(CCC)  = O)C ‚àí1 . 36 ‚àí1 . 14 (0.22) ‚àí0 . 55 (0.81) 
recommended  to consider  the canonical  SMILES  representation  rather 
than a random  one. 
4.2. Ensemble  learning  for compound  prediction  and conÔ¨Ådence  measure  
Using the Maxsmi  models established  above, we look into more de- 
tails at the information  gained from ensemble  learning  for molecular  
prediction,  and more speciÔ¨Åcally  at the average  and standard  deviation  
computed  from the per SMILES  prediction.  Feeding  diÔ¨Äerent  SMILES  
representations  to the model and aggregating  the prediction  for each 
SMILES  variation  to obtain a single prediction  per compound  is valu- 
able not only from a practical  point of view where molecular  prediction  
is more informative  than a SMILES  prediction,  but it also allows the 
model to merge information  coming  from diÔ¨Äerent  perspectives  of the 
same compound.  Moreover,  the standard  deviation  associated  with the 
SMILES  predictions  allows to quantify  the uncertainty  of the prediction  
of the model toward a given compound.  The higher the standard  devia- 
tion for a molecule,  the less concurrent  are the predictions  by the model, 
and thus, less conÔ¨Ådent.  
4.2.1. DiÔ¨Äerence  between  canonical  vs. averaged  prediction  
Considering  the Maxsmi  models trained  with their respective  aug- 
mentations,  we analyze  the diÔ¨Äerence  in prediction  on the test set when 
using the canonical  or the averaged  prediction.  More speciÔ¨Åcally,  we 
compare  the prediction  error of the Maxsmi  models when evaluated  on 
the test set twice: once using the canonical  SMILES  for compound  pre- 
diction and a second time averaging  the per SMILES  prediction  using 
the same augmentation  number  and strategy  which was used for train- 
ing. For both evaluations,  the error between  the prediction  and the true 
value is computed.  Fig. 6 shows the histogram  of these errors on the 
ESOL data. As shown in the Ô¨Ågure, more compounds  have an error close 
to zero using the ensemble  learning  evaluation  rather than the canon- 
ical, which incentives  the use of ensemble  learning  for future studies.  
However,  this gain is marginal  and the canonical  prediction  performs  
similarly  well compared  to the averaged  prediction.  In light of the over- 
all gain in the accuracy  of the models,  this indicates  that augmentation  
during training  is the more crucial step. As discussed  in the following  
paragraph,  an advantage  of using augmentation  on the test set is to es- 
timate the conÔ¨Ådence  of the model in its prediction.  
4.2.2. More conÔ¨Ådent  model implies smaller prediction  error 
As mentioned  in the SMILES  augmentation  as ensemble  learning  for 
compound  prediction  and conÔ¨Ådence  measure  section,  computing  the 
standard  deviation  of the per SMILES  prediction  provides  a conÔ¨Ådence  
measure  in the compound  prediction.  In this section,  we analyze  the 
Fig. 6. Lower errors when evaluating  the Maxsmi  model using ensemble  
learning.  There are fewer errors in the evaluation  of the trained Maxsmi models 
when using ensemble  learning  (i.e. the averaged  per SMILES prediction)  vs. the 
canonical  prediction.  
relationship  between  high conÔ¨Ådence  and small prediction  error on the 
test set for the Maxsmi  models.  A way of visually  evaluating  uncertainty  
is to plot the conÔ¨Ådence  curve [32] , which displays  how the error varies 
with the sequential  removal  of compounds  from lowest to highest  conÔ¨Å- 
dence. Fig. 7 shows the conÔ¨Ådence  curve of the Maxsmi  model used on 
the FreeSolv  data. As shown in the Ô¨Ågure, as molecules  with low conÔ¨Å- 
dence are sequentially  removed,  the mean prediction  error decreases.  In 
other words, the error vanishes  as only compounds  with the highest  cer- 
tainty predictions  are kept, demonstrating  a relationship  between  high 
conÔ¨Ådence  and small prediction  error. Fig. A7 shows the conÔ¨Ådence  
curves of the Maxsmi  models for the ESOL and lipophilicity  data. The 
general  trend of the curve is decreasing  in the ESOL case. Once the 10% 
of compounds  with the highest  conÔ¨Ådence  are kept, the error is below 
0.25. However,  in the lipophilicity  case, although  the general  trend is 
also decreasing,  even when keeping  the 10% of compounds  with the 
highest  conÔ¨Ådence,  the error is still above 0.3. 
4.3. Comparison  to other studies 
Given the results of the Maxsmi  models,  their performance  is com- 
pared to other studies,  namely  MoleculeNet  [34] , CNF [22] , and MolP- 
MoFiT [21] , that are trained  and evaluated  on the same data sets as 
Maxsmi,  see Table 4 . 
8 T.B. Kimber, M. Gagnebin and A. Volkamer  ArtiÔ¨Åcial Intelligence  in the Life Sciences 1 (2021) 100014 
Table 4 
The Maxsmi  models reach state-of-the-art  results. Comparison  of four studies on the same data sets (ESOL, FreeSolv,  and 
lipophilicity).  The Maxsmi model outperforms  most of the other models with a lower RMSE on a randomly  split test set. 
Study Test RMSE ( ¬± std if available)  Split Model 
ESOL FreeSolv Lipophilicity  Fold Ratio % (train:valid:test)  Type 
Maxsmi 0.569 1.032 0.593 Single 80 ‚à∂ 0 ‚à∂ 20 Random CNN 
MoleculeNet  [34] 0 . 58 ¬± 0 . 03 1 . 15 ¬± 0 . 12 0 . 655 ¬± 0 . 036 3 80 ‚à∂ 10 ‚à∂ 10 Random GNN 
CNF [22] 0.62 1.11 0.67 5-fold CV NA Random CNN 
MolPMoFiT  [21] NA 1 . 197 ¬± 0 . 127 0.565 ¬± 0.037 10 80 ‚à∂ 10 ‚à∂ 10 Random RNN 
Abbreviations:  RMSE = root mean squared error, std = standard  deviation,  CNN = Convolutional  Neural Network,  GNN = 
Graph Neural Network,  RNN = Recurrent  Neural Network,  NA = not available,  CV = cross-validation.  
Fig. 7. More conÔ¨Ådent  Maxsmi  model on FreeSolv  implies smaller  predic- 
tion error. The general trend of the conÔ¨Ådence  curve is decreasing,  showing  that 
as compounds  with high uncertainty  are removed,  the error becomes  smaller.  
The Ô¨Årst considered  study is MoleculeNet,  where several molecular  
encodings  and models are trained  and evaluated,  but where no augmen-  
tation is used. In MoleculeNet,  the data is randomly  split into training,  
validation,  and test set, using an 80 ‚à∂ 10 ‚à∂ 10 ratio and run three times 
on diÔ¨Äerent  seeds. The best performing  model on the test set for both 
ESOL and FreeSolv  is a message  passing  neural network  with an RMSE 
and standard  deviation  of 0 . 58 ¬± 0 . 03 and 1 . 15 ¬± 0 . 12 , respectively  [ 34 , 
Table S5]. On the lipophilicity  data set, a slightly  diÔ¨Äerent  graph model 
performs  best with 0 . 655 ¬± 0 . 036 . On all three tasks, the Maxsmi  results 
perform  better than MoleculeNet  (see Table 4 ), suggesting  that SMILES  
augmentation  with shallow  neural networks  could perform  at least as 
well as, if not better than, graph neural networks  (GNNs).  
The second study we consider  is the Convolutional  Neural Finger- 
print (CNF) model [22,23]  , in which SMILES  augmentation  is applied,  
generating  unique representations  for each compound,  i.e. augmenta-  
tion without  duplication.  The CNF model is evaluated  using Ô¨Åve-fold  
cross-validation  (CV), however  standard  deviations  are not reported.  
Test RMSE values are 0.62, 1.11 and 0.66 on the ESOL, FreeSolv,  and 
lipophilicity  data set, respectively,  see [ 22 , Table S1]. Similar  to Molecu-  
leNet, the Maxsmi  model slightly  outperforms  these results on all three 
tasks. This suggests  that augmenting  the data set by greater factors,  e.g. closer to 70 as in Maxsmi,  yields better results than 10 times augmen-  
tation as in CNF. 
Finally,  the Molecular  Prediction  Model Fine-Tuning  (MolP- 
MoFiT)  [21] study builds an RNN model based on LSTM layers us- 
ing SMILES  augmentation  with duplication.  The lipophilicity  data is 
augmented  25 times whereas  the FreeSolv  data 50 times. MolPMoFiT  
is trained  and evaluated  using 10 splits of ratio 80 ‚à∂ 10 ‚à∂ 10 for the 
training,  validation,  and test sets. The model reaches  an RMSE value 
(and standard  deviation)  of 1 . 197 ¬± 0 . 127 on the FreeSolv  data and 
0 . 565 ¬± 0 . 037 on the lipophilicity  data, see [ 21 , Figure 3, 4]. While 
Maxsmi  leads on the FreeSolv  prediction  problem,  MolPMoFiT  slightly  
outperforms  Maxsmi  on the lipophilicity  data ( Table 4 ). 
Lastly, study comparison  should be treated with utmost attention,  
since results can not be compared  blindly.  For instance,  if the data pre- 
processing  is done diÔ¨Äerently  in each study, or the splits are not iden- 
tical, or the parameters  of the experiments  are not set to be the same, 
then the results are not fairly comparable.  
4.4. Test case: EGFR aÔ¨Énity data following  the guideline  
Given the results of the Maxsmi  models on the physico-chemical  data 
sets, we now discuss guidelines  to apply SMILES  augmentation  on a new 
data set. The EGFR aÔ¨Énity  data, discussed  in the Provenance  section and 
henceforth  simply referred  to as aÔ¨Énity  data, is used as a test case, but 
the idea can be applied  to diÔ¨Äerent  data sets and broader  use cases. 
Since the aÔ¨Énity  data set contains  5 849 data points after prepro-  
cessing (see Table 1 ), the lipophilicity  and aÔ¨Énity  data are of a similar 
order of magnitude,  although  the latter is somewhat  larger. Therefore,  a 
compromise  between  the size of the data set and the tuple that gives the 
best results for the FreeSolv,  ESOL, and lipophilicity  data (see Table 2 ) 
is found: for the aÔ¨Énity  data, the CONV1D  model is chosen (similarly  
to lipophilicity,  ESOL, and FreeSolv),  the number  of augmentation  is set 
to 70, as for ESOL and FreeSolv,  and the augmentation  strategy  is set to 
augmentation  with reduced  duplication  for a less computational  inten- 
sive training  than augmentation  with duplication.  As comparison,  the 
Maxsmi,  the canonical,  and the baseline  models on aÔ¨Énity  are trained  
and evaluated.  The same experimental  setup for splitting  and evaluation  
as mentioned  in the Data and experimental  setup section is applied.  On 
the test set, the canonical  model reaches  an RMSE value and coeÔ¨Écient  
of correlation  ùëÖ 2 value of 1.031 and 0.494, respectively.  In compari-  
son, the Maxsmi  model shows great improvement  with test RMSE and 
ùëÖ 2 values of 0.777 and 0.712, respectively  (see Table 5 ). Surprisingly,  
Table 5 
The Maxsmi  models strike again! The Maxsmi model developed  for the aÔ¨Énity against the EGFR kinase and the Random  
Forest (RF) baseline  model outperform  the canonical  model. 
Name Model Augmentation  number Augmentation  strategy Test RMSE Test ùëÖ 2 
Maxsmi CONV1D  70 Augmentation  with reduced duplication  0.777 0.712 
Canonical  CONV1D  0 No augmentation  1.031 0.494 
Baseline RF 0 No augmentation  0.758 0.726 
9 T.B. Kimber, M. Gagnebin and A. Volkamer  ArtiÔ¨Åcial Intelligence  in the Life Sciences 1 (2021) 100014 
the RF baseline  model performs  similarly  to the Maxsmi  model, with an 
RMSE of 0.758 and an ùëÖ 2 of 0.726. 
4.5. Maxsmi  models available  for user predictions  
Given the good performance  of the Maxsmi  models for all three 
physico-chemical  tasks and for EGFR aÔ¨Énity,  we retrained  them on all 
points in the data set as a Ô¨Ånal product.  The aim is to oÔ¨Äer a single 
command-line  interface  for prediction.  A user can provide  a SMILES  as 
input, choose a given task and they will receive  an output Ô¨Åle in the 
form of a CSV table with relevant  information,  such as 1. the user input 
SMILES  itself, 2. whether  the compound  was in the training  set, 3. the 
canonical  SMILES,  and its associated  variations  4. the per SMILES  pre- 
dictions,  5. the per compound  prediction,  6. and the standard  deviation.  
A PNG Ô¨Åle of the 2D molecular  graph associated  with the in- 
put SMILES  is also generated.  For example,  for the semaxanib  drug, 
taken from the PKIDB database  [63] , and given by the SMILES  
O = C2C(\1ccccc1N2)  = C/c3c(cc([nH]3)C)C  , lipophilicity  is 
predicted  using the command-line  
$ python  maxsmi/prediction_unlabeled_data.py  
--task  = ‚Äùlipophilicity  ‚Äù--smiles_prediction  = ‚ÄùO = 
C2C(\1ccccc1N2)  = C/c3c(cc([nH]3)C)C  ‚Äù
The command  above was used to generate  the values in Fig. 1 . 
5. Conclusion  
In this study, SMILES  augmentation  applied  to deep learning  molec- 
ular property  and activity  prediction  is investigated.  Five augmentation  
strategies  that can be applied  as SMILES  augmentation  are explored,  
together  with three neural network  architectures,  and the performance  
thoroughly  assessed  on three molecular  data sets: ESOL, FreeSolv,  and 
lipophilicity.  
Our Ô¨Åndings  show that augmentation  improves  the performance  of 
deep learning  models not only independently  of the model, but also 
with respect to the size of the data set. This suggests  that the choice of 
augmentation  strategy  can be viewed  as hyper-parameter  tuning.  
The tuple consisting  of (model,  augmentation  number,  augmenta-  
tion strategy)  that maximizes  the performance  on the test set leads to 
the deÔ¨Ånition  of the Maxsmi  models.  Our Ô¨Åndings  also show that the 
model using canonical  SMILES  outperforms  the one using single ran- 
dom SMILES,  thanks to the simplicity  of the canonical  notation.  
Additionally,  the Maxsmi  models outperform,  or perform  at least as 
well as state-of-the-art  models such as MoleculeNet,  CNF, and MolP- 
MoFiT, on the three physico-chemical  data sets. This suggests  that apply- 
ing simple SMILES  augmentation  techniques  can reach similar or even 
better performance  as sophisticated  models such as graph-based  neural 
networks,  as in the case of MoleculeNet.  Moreover,  we use our Ô¨Ånd- 
ings to guide the application  of SMILES  augmentation  on a new data set 
and provide  a test case with data on aÔ¨Énity  against  the EGFR kinase. Fi- 
nally, we provide  an easy to use framework  for out-of-sample  prediction  
on four tasks: ESOL, FreeSolv,  lipophilicity,  and aÔ¨Énity  against  EGFR, 
which should be helpful to assess properties  of novel compounds.  The 
open-source  code allows to perform  similar studies on diÔ¨Äerent  data sets 
with minor programmatic  adjustments.  As an outlook,  we observe  that strategies  that keep all, or a fraction  
of duplicates,  may help the model to learn inherent  symmetry  in a com- 
pound. Indeed the same random  SMILES  representation  will certainly  
be generated  multiple  times for a symmetric  molecule  even though the 
initial atom and the path along the graph are diÔ¨Äerent.  In this sense, 
SMILES  duplication  is not an artiÔ¨Åcial  construction,  and keeping  repli- 
cas could retain important  information  about the underlying  symmetry  
of a compound.  
6. Abbreviations  
List of abbreviations  used in the paper. 
QSAR Quantitative  Structure-Activity  Relationship  
SMILES SimpliÔ¨Åed  Molecular-Input  Line-Entry  System 
CADD Computer-Aided  Drug Design 
ML Machine Learning 
DL Deep Learning 
LSTM Long Short-Term  Memory 
EGFR Epidermal  Growth Factor Receptor 
NA Not Available  
RMSE Root Mean Squared Error 
CONV1D  1D Convolutional  Neural Network 
CONV2D  2D Convolutional  Neural Network 
RNN Recurrent  Neural Network 
RF Random Forest 
GNN Graph Neural Network 
CNF Convolutional  Neural Fingerprint  
CV Cross-validation  
MolPMoFiT  Molecular  Prediction  Model Fine-Tuning  
Declaration  of Competing  Interest  
The authors  declare  that they have no known competing  Ô¨Ånancial  
interests  or personal  relationships  that could have appeared  to inÔ¨Çuence  
the work reported  in this paper. 
Acknowledgments  
The authors  thank the HPC Service  of ZEDAT,  Freie Universit√§t  
Berlin, for computing  time. Talia B. Kimber  received  funding  from 
the Stiftung  Charit√© in  the context  of the Einstein  BIH Visiting  Fellow 
Project.  The authors  thank Greg Landrum  for beneÔ¨Åcial  discussions,  as 
well as Dominique  Sydow for valuable  advice on documentation.  
Appendix  
Figures 
Figures  in the appendix.  
10 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Fig. A1. Test RMSE using data augmen-  
tation on the FreeSolv  data set. The table 
shows the root mean squared error (RMSE) 
on the test set for three deep learning  mod- 
els and Ô¨Åve SMILES augmentation  strate- 
gies, using various augmentation  numbers,  
as well as a baseline  consisting  of a Ran- 
dom Forest (RF) model with Morgan Ô¨Ån- 
gerprint  as input. The lighter the purple 
color, the better the model. The overall best 
setting is highlighted  in yellow, which for 
the FreeSolv  data set is augmenting  the 
data set 70 times keeping all duplicates  
and training  a 1D convolutional  neural net- 
work (CONV1D).  For interpretation  of the 
references  to color in this Ô¨Ågure legend, the 
reader is referred  to the web version of this 
article. 
11 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Fig. A2. Test RMSE using data augmen-  
tation on the lipophilicity  data set. The 
table shows the root mean squared er- 
ror (RMSE) on the test set for three deep 
learning  models and Ô¨Åve SMILES augmen-  
tation strategies,  using various augmenta-  
tion numbers,  as well as a baseline  consist- 
ing of a Random  Forest (RF) model with 
Morgan Ô¨Ångerprint  as input. The lighter 
the purple color, the better the model. The 
overall best setting is highlighted  in yel- 
low, which for the lipophilicity  data set is 
augmenting  the data set 80 times removing  
duplicates  and training  a 1D convolutional  
neural network  (CONV1D).  For interpreta-  
tion of the references  to color in this Ô¨Åg- 
ure legend, the reader is referred  to the web 
version of this article. 
12 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
Fig. A3. Generating  a large portion  of the SMILES space does not lead 
to the best performance.  Even though the CONV1D  model is presented  with 
SMILES variations  that cover a large portion of the SMILES space using the aug- 
mentations  strategy  with estimated  maximum,  on the FreeSolv  data set, this 
strategy  does not achieve the best results. 
Fig. A4. Performance  reaches  a plateau  independently  of the augmenta-  
tion strategy.  The performance  of the CONV1D  model trained and evaluated  
on the ESOL data set reaches a test RMSE value slightly below 0.6 as of 40 aug- 
mentation  steps and Ô¨Çuctuates  below this value thereafter,  for all augmentation  
strategies:  with, without,  and with reduced  duplication.  
Fig. A5. Performance  reaches  a plateau  independently  of the augmenta-  
tion strategy.  The performance  of the CONV1D  model trained and evaluated  
on the lipophilicity  data set reaches a test RMSE value slightly below 0.6 as of 
60 augmentation  steps and Ô¨Çuctuates  below this value thereafter,  for all aug- 
mentation  strategies:  with, without,  and with reduced  duplication.  
Fig. A6. Trade-oÔ¨Ä between  performance  and computation  time. As ex- 
pected, the training  time of the CONV1D  model on the ESOL data increases  
with the augmentation  number for all augmentation  strategies:  with, without,  
and with reduced  duplication.  Augmenting  the training  set by 100 and keeping 
duplicate  leads to 90 200 data points (see Table 1 ). Training  the model on a GPU 
takes approximately  three hours and reaches a test RMSE of 0.580 (see Figure 2 ). 
However,  augmenting  the data by just 19 leads to a test RMSE of 0.605 in less 
than 30 minutes.  
Fig. A7. ConÔ¨Ådence  curves of the Maxsmi  models on the ESOL and lipophilicity  data. The general trend of the curve in the left plot (the ESOL data) is decreasing,  
showing  a relationship  between  high conÔ¨Ådence  and small mean prediction  error. Although  also generally  decreasing,  the mean prediction  error in the right plot 
(the lipophilicity  data) is still above 0.3 when only keeping the 10% of compounds  with the highest conÔ¨Ådence.  
13 T.B. Kimber, M. Gagnebin and A. Volkamer ArtiÔ¨Åcial Intelligence in the Life Sciences 1 (2021) 100014 
References  
[1] Paul SM, Mytelka DS, Dunwiddie CT, Persinger CC, Munos BH, Lindborg SR, et al. How to improve r&d productivity: the pharmaceutical industry‚Äôs grand challenge. Nat Rev Drug Discovery 2010;9(3):203‚Äì14. doi: 10.1038/nrd3078 . [2] Scannell JW, Blanckley A, Boldon H, Warrington B. Diagnosing the decline in pharmaceutical r&d eÔ¨Éciency. Nat Rev
 Drug Discovery 2012;11(3):191‚Äì200. doi: 10.1038/nrd3681 . [3] Waring MJ, Arrowsmith J, Leach AR, Leeson PD, Mandrell S, Owen RM, et al. An analysis of the attrition of drug candidates from four major pharmaceutical compa- nies. Nat Rev Drug Discovery 2015;14(7):475‚Äì86. doi: 10.1038/nrd4609 . [4] Krizhevsky A, Sutskever I, Hinton GE. 
Imagenet classiÔ¨Åcation with deep convolu- tional neural networks. Commun ACM 2017;60(6):84‚Äì90. doi: 10.1145/3065386 . [5] Graves A, Mohamed A-r, Hinton G. Speech recognition with deep recurrent neural networks. In: 2013 IEEE International Conference on Acoustics, Speech and Signal Processing; 2013. p. 6645‚Äì9. doi: 10.1109/ICASSP.2013.6638947 . [6] Jumper J, Evans R, Pritzel
 A, Green T, Figurnov M, Ronneberger O, et al. Highly accurate protein structure prediction with alphafold. Nature 2021;596:583‚Äì9. doi: 10.1038/s41586-021-03819-2 . [7] Feinberg EN, Sur D, Wu Z, Husic BE, Mai H, Li Y, et al. Potentialnet for molec- ular property prediction. ACS Cent Sci 2018;4(11):1520‚Äì30. doi: 10.1021/acs- centsci.8b00507 .
 [8] Sattarov B, Baskin II, Horvath D, Marcou G, Bjerrum EJ, Varnek A. De novo molecular design by combining deep autoencoder recurrent neural networks with generative topographic mapping. J Chem Inf Model 2019;59(3):1182‚Äì96. doi: 10.1021/acs.jcim.8b00751 . [9] Webel HE, Kimber TB, Radetzki S, Neuenschwander M, Nazar√©M, Volkamer A. Re- vealing 
cytotoxic substructures in molecules using deep learning. J Comput Aided Mol Des 2020;34(7):731‚Äì46. doi: 10.1007/s10822-020-00310-4 . [10] Goodfellow I, Bengio Y, Courville A. Deep learning. MIT Press; 2016 . http://www.deeplearningbook.org [11] Bisong E . Google Colaboratory. Berkeley, CA: Apress; 2019. p. 59‚Äì64 . ISBN 978-1-4842-4470-8 [12] Kaggle. https://www.kaggle.com/ ; 2021. [Online;
 accessed 27-August-2021]. [13] Parks CD, Gaieb Z, Chiu M, Yang H, Shao C, Walters WP, et al. D3R Grand challenge 4: blind prediction of protein‚Äìligand poses, aÔ¨Énity rankings, and relative binding free energies. J Comput Aided Mol Des 2020;34(2):99‚Äì119. doi: 10.1007/s10822-020-00289-y . [14] Huang R, Xia M. Editorial: Tox21 challenge 
to build predictive models of nuclear receptor and stress response pathways as mediated by exposure to environmental toxicants and drugs. Front Environ Sci 2017;5:3. doi: 10.3389/fenvs.2017.00003 . [15] Weininger D. Smiles, a chemical language and information system. 1. introduc- tion to methodology and encoding rules. J Chem Inf Comput Sci 1988;28(1):31‚Äì6.
 doi: 10.1021/ci00057a005 . [16] O‚ÄôBoyle NM. Towards a universal SMILES representation - a standard method to generate canonical SMILES based on the inchi. J Cheminform 2012;4(1). doi: 10.1186/1758-2946-4-22 . [17] Weininger D, Weininger A, Weininger JL. Smiles. 2. algorithm for genera- tion of unique smiles notation. J Chem Inf Comput 
Sci 1989;29(2):97‚Äì101. doi: 10.1021/ci00062a008 . [18] Hemmerich J, Asilar E, Ecker GF. COVER: Conformational oversam- pling as data augmentation for molecules. J Cheminform 2020;12(1). doi: 10.1186/s13321-020-00420-z . [19] Li Y, Rezaei MA, Li C, Li X. Deepatom: A framework for protein-ligand binding aÔ¨Énity prediction. In: 2019 IEEE International Conference on Bioinformatics
 and Biomedicine (BIBM); 2019. p. 303‚Äì10. doi: 10.1109/BIBM47256.2019.8982964 . [20] Bjerrum EJ. Smiles enumeration as data augmentation for neural net- work modeling of molecules. arXiv preprint arXiv:170307076 2017 . https://arxiv.org/abs/1703.07076 [21] Li X, Fourches D. Inductive transfer learning for molecular activity pre- diction: next-gen QSAR models with molpmoÔ¨Åt. J Cheminform 
2020;12(1). doi: 10.1186/s13321-020-00430-x . [22] Kimber TB, Engelke S, Tetko IV, Bruno E, Godin G. Synergy eÔ¨Äect be- tween convolutional neural networks and the multiplicity of smiles for im- provement of molecular prediction. arXiv preprint arXiv:181204439 2018 . https://arxiv.org/abs/1812.04439 [23] Tetko IV, Karpov P, Bruno E, Kimber TB, Godin G. Augmentation
 is what you need!. In: Tetko IV, K ≈Ø rkov√°V, Karpov P, Theis F, editors. ArtiÔ¨Åcial Neural Networks and Machine Learning ‚ÄìICANN 2019: Workshop and Special Sessions. Cham: Springer International Publishing; 2019. p. 831‚Äì5. doi: 10.1007/978-3-030-30493-5_79 . ISBN 978-3-030-30493-5 [24] Shorten C, Khoshgoftaar TM. A survey on image data augmentation 
for deep learn- ing. J Big Data 2019;6(1). doi: 10.1186/s40537-019-0197-0 . [25] Tetko IV, Karpov P, Deursen RV, Godin G. State-of-the-art augmented NLP trans- former models for direct and single-step retrosynthesis. Nat Commun 2020;11(1). doi: 10.1038/s41467-020-19266-y . [26] Sumner D, He J, Thakkar A, Engkvist O, Bjerrum EJ. Levenshtein augmentation improves
 performance of smiles based deep-learning synthesis prediction. ChemRxiv 2020. doi: 10.26434/chemrxiv.12562121.v2 . [27] Ar√∫s-Pous J, Johansson SV, Prykhodko O, Bjerrum EJ, Tyrchan C, Reymond J-L, et al. Randomized SMILES strings improve the quality of molecular generative models. J Cheminform 2019;11(1). doi: 10.1186/s13321-019-0393-0 . [28] van Deursen R, Ertl P, Tetko 
IV, Godin G. GEN: Highly eÔ¨Écient SMILES explorer using autodidactic generative examination networks. J Cheminform 2020;12(1). doi: 10.1186/s13321-020-00425-8 . [29] RDKit: Open-source cheminformatics. http://www.rdkit.org [Online; accessed 01- July-2021]; 2021. [30] Kazil J , Jarmul K . Data wrangling with python: tips and tools to make your life easier. 1st. O‚ÄôReilly Media, Inc.; 2016 . ISBN 1491948817 [31] Tagasovska N, Lopez-Paz D. Single-model uncertainties for deep learning. arXiv preprint arXiv:181100908 2019 . https://arxiv.org/abs/1811.00908 [32] Scalia G, Grambow CA, Pernici B, Li
 Y-P, Green WH. Evaluating scalable uncertainty estimation methods for deep learning-based molecular property prediction. J Chem Inf Model 2020;60(6):2697‚Äì717. doi: 10.1021/acs.jcim.9b00975 . [33] Ayhan M.S., Berens P. Test-time data augmentation for estimation of heteroscedastic aleatoric uncertainty in deep neural networks. 2018. https://openreview.net/pdf?id = rJZz-knjz . [34] Wu Z, Ramsundar B, 
Feinberg EN, Gomes J, Geniesse C, Pappu AS, Leswing K, Pande V. Moleculenet: a benchmark for molecular machine learning. Chem Sci 2018;9:513‚Äì30. doi: 10.1039/C7SC02664A . [35] Gaulton A, Hersey A, Nowotka M, Bento AP, Chambers J, Mendez D, et al. The ChEMBL database in 2017. Nucleic Acids Res 2016;45(D1):D945‚Äì54. doi: 10.1093/nar/gkw1074
 . [36] Mayr A, Klambauer G, Unterthiner T, Steijaert M, Wegner JK, Ceulemans H, Clevert D-A, Hochreiter S. Large-scale comparison of machine learning methods for drug target prediction on ChEMBL. Chem Sci 2018;9:5441‚Äì51. doi: 10.1039/C8SC00148K . [37] Zhang Y, Lee AA. Bayesian semi-supervised learning for uncertainty-calibrated pre- diction of molecular 
properties and active learning. Chem Sci 2019;10:8154‚Äì63. doi: 10.1039/C9SC00616H . [38] Ramsundar B , Eastman P , Walters P , Pande V , Leswing K , Wu Z . Deep learning for the life sciences. O‚ÄôReilly Media; 2019 . ISBN 9781492039839 [39] Delaney JS. Esol: estimating aqueous solubility directly from molecular
 structure. J Chem Inf Comput Sci 2004;44(3):1000‚Äì5. doi: 10.1021/ci034243x . [40] Mobley DL, Guthrie JP. FreeSolv: a database of experimental and calculated hydra- tion free energies, with input Ô¨Åles. J Comput Aided Mol Des 2014;28(7):711‚Äì20. doi: 10.1007/s10822-014-9747-x . [41] ChEMBL. https://www.ebi.ac.uk/chembl/ [Online; accessed 27-August-2021]; 2021. [42] Kooistra AJ, Volkamer A. 
Kinase-centric computational drug development. In: Annual Reports in Medicinal Chemistry. Elsevier; 2017. p. 197‚Äì236. doi: 10.1016/bs.armc.2017.08.001 . [43] OpenKinome. http://openkinome.org/ [Online; accessed 27-August-2021]; 2021. [44] Herbst RS. Review of epidermal growth factor receptor biology. International Journal of Radiation Oncology 
‚àó Biology ‚àó Physics 2004;59(2, Supplement):S21‚Äì6. doi: 10.1016/j.ijrobp.2003.11.041 . [45] Consortium TU. Uniprot: the universal protein knowledgebase in 2021. Nucleic Acids Res 2020;49(D1):D480‚Äì9. doi: 10.1093/nar/gkaa1100 . [46] IC50 Values. OÔ¨Äermanns S, Rosenthal W, editors. Berlin, Heidelberg: Springer Berlin Heidelberg; 2008. doi: 10.1007/978-3-540-38918-7_5943 . ISBN 978-3-540-38918-7 [47] Kimber TB, Chen Y, Volkamer A. Deep 
learning in virtual screening: recent applica- tions and developments. Int J Mol Sci 2021;22(9). doi: 10.3390/ijms22094435 . [48] Mean squared error. Sammut C, Webb GI, editors. Boston, MA: Springer US; 2010. doi: 10.1007/978-0-387-30164-8_528 . ISBN 978-0-387-30164-8 [49] Kv√•lseth TO. Cautionary note about ùëü 
2 . Am Stat 1985;39(4):279‚Äì85. doi: 10.1080/00031305.1985.10479448 . [50] Breiman L. Random forests. Mach Learn 2001;45(1):5‚Äì32. doi: 10.1023/A:1010933404324 . [51] Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blon- del M, a Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D, Brucher M, Perrot 
M, Duchesnay E. Scikit-learn: machine learning in python. Journal of Machine Learning Research 2011;12:2825‚Äì30 . http://jmlr.org/papers/v12/pedregosa11a.html [52] Bennett L., Melchers B., Proppe B. Curta: A general-purpose high-performance computer at Zedat, Freie Universit√§t Berlin. 2020. https://doi.org/10.17169/ refubium-26754 . [53] Van Rossum G , Drake FL . Python 3 reference manual. Scotts Valley,
 CA: CreateSpace; 2009 . ISBN 1441412697 [54] van Rossum G, Warsaw B, Coghlan N. Style guide for Python code. PEP; 2001 . https://www.python.org/dev/peps/pep-0008/ [55] Read the Docs. https://readthedocs.io/en/stable/ [Online; accessed 30-July-2021]; 2021. [56] Anaconda software distribution. 2020. https://anaconda.com/ . [57] Paszke A, Gross S, Massa F, Lerer A, Bradbury J, 
Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. Pytorch: an imperative style, high-performance deep learning library. In: Wallach H, Larochelle H, Beygelzimer A, d‚ÄôAlch√©-Buc F, Fox E,
 Garnett R, editors. Advances in Neural Information Processing Systems 32. Curran Associates, Inc.; 2019. p. 8024‚Äì35 . http://papers.neurips.cc/ paper/9015-pytorch-an-imperative-style-high-performance-deep-learning-library.  pdf [58] Harris CR, Millman KJ, van der Walt SJ, Gommers R, Virtanen P, Courna- peau D, et al. Array programming with numpy. Nature 2020;585(7825):357‚Äì62. doi: 10.1038/s41586-020-2649-2 . [59] The pandas 
development team. pandas-dev/pandas: Pandas. 2020. 10.5281/zen- odo.3509134 [60] GitHub Actions. https://docs.github.com/en/actions [Online; accessed 30-July- 2021]; 2021. [61] Pytest. https://docs.pytest.org/ [Online; accessed 30-July-2021]; 2021. [62] Codecov. https://docs.codecov.com/docs [Online; accessed 30-July-2021]; 2021. [63] Carles F, Bourg S, Meyer C, Bonnet P. PKIDB: a curated, annotated and up- dated database of protein kinase inhibitors
 in clinical trials. Molecules 2018;23(4). doi: 10.3390/molecules23040908 . 14 