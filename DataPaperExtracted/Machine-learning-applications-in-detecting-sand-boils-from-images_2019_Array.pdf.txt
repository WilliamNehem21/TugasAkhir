Machine learning applications in detecting sand boils from images
Aditi Kuchib, Md Tamjidul Hoquea,b,*, Mahdi Abdelguerﬁa,b, Maik C. Flanaginc
aCanizaro/Livingston Gulf States Center for Environmental Informatics, University of New Orleans, New Orleans, LA, 70148, USA
bDepartment of Computer Science, University of New Orleans, New Orleans, LA, 70148, USA
cUS Army Corps of Engineers, New Orleans District, LA, USA
ARTICLE INFO
Keywords:Machine learningStackingObject detectionSand boilsDeep learningSupport vector machineABSTRACT
Levees provide protection for vast amounts of commercial and residential properties. However, these structuresrequire constant maintenance and monitoring, due to the threat of severe weather, sand boils, subsidence of land,seepage, etc. In this research, we focus on detecting sand boils. Sand boils occur when water under pressure wellsup to the surface through a bed of sand. These make levees especially vulnerable. Object detection is a goodapproach to conﬁrm the presence of sand boils from satellite or drone imagery, which can be utilized to assist inthe automated levee monitoring methodology. Since sand boils have distinct features, applying object detectionalgorithms to it can result in accurate detection. To the best of our knowledge, this research work is the ﬁrst approach to detect sand boils from images. In this research, we compare some of the latest deep learning methods,Viola-Jones algorithm, and other non-deep learning methods to determine the best performing one. We also traina Stacking-based machine learning method for the accurate prediction of sand boils. The accuracy of our robustmodel is 95.4%.
1. IntroductionLevees are constructed to provide safety along natural water bodies,to stop theﬂooding of low-lying areas. The presence of sand boils alongthe outside of the levee signiﬁes a possible impending structural failure ofthe construction. This can result in a lot of damage to both property andlife [1]. The analysis and monitoring of sand boils are currently donemanually [2,3].We aim to automate this process by picking the best models out ofseveral developing machine learning models, that can most accuratelydetect sand boils near the levees so that personnel can be more targetedin their monitoring. The dataset for this study has been collectedmanually from various sources since there is no centralized datasetavailable for these relevant images. It has been made sure that it containssatellite images of rough terrain that can pose as a challenge for a ma-chine learning algorithm to identify sand boils from. This resulted in thecreation of a robust predictor capable of identifying potential sand boilswith high accuracy, which was ensured by comparing it with other po-tential machine learning approaches.To the best of our knowledge, this research work is the ﬁrst attempt to detect sand boils from images using effective machine learningapproaches. In this study, we investigate both the renowned and latestrobust object detection algorithms and compare their performances. Wecreate a stacking model that improves on the individual methods. We useViola-Jones’algorithm for Haar cascade based object detection [ 4,5], You Only Look Once (YOLO) deep learning RPN-based object detection[6], Single Shot MultiBox Detector [7] based on convolutional neural nets [8] and deep learning [9], Support Vector Machine (SVM) [10], gradient boosting [11], extreme gradient boosting [12], logistic regres- sion [13], etc. along with stacking [14,15] to predict sand boils from images effectively.2. BackgroundThis section contains a background of sand boils, their signi ﬁcance with respect to levee health, and object detection research ﬁelds.2.1. Signiﬁcance of sand boil detection near leveesLevees are embankments orﬂood banks constructed along naturallyoccurring water bodies in order to stopﬂooding [3]. They provide pro- tection for vast amounts of commercial and residential properties,
* Corresponding author. Canizaro/Livingston Gulf States Center for Environmental Informatics, University of New Orleans, New Orleans, LA, 70148, USA. E-mail addresses:askuchi@uno.edu(A. Kuchi),thoque@uno.edu(M.T. Hoque),mahdi@cs.uno.edu(M. Abdelguerﬁ),maik.c.ﬂanagin@usace.army.mil (M.C. Flanagin).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2019.100012Received 19 June 2019; Received in revised form 26 September 2019; Accepted 19 October 2019Available online 1 November 20192590-0056/©2019 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 3-4 (2019) 100012especially in the New Orleans area. However, levees and ﬂood-walls degrade over time due to the impact of severe weather, developmentof sand boils, subsidence of land, seepage, development of cracks, etc.Further, published data [16] indicates that coastal Louisiana lostapproximately 16 square miles of land between 1985 and 2010. In 2005,there were over 50 failures of the levees and ﬂoodwalls protecting New Orleans, Louisiana, and its surrounding suburbs following the passage ofHurricane Katrina and landfall in Mississippi. These failures causedﬂooding in 80% of the city of New Orleans and all of St. Bernard Parish.Events like Hurricane Katrina in 2005 have shown that levee failures canbe catastrophic, and very costly. If these levees fail due to these condi-tions, there is a potential for signiﬁcant property damage, and loss of life,as experienced in New Orleans in the 2005 hurricane, Katrina. It is ofgreat importance to monitor the physical conditions of levees for ﬂood control [3,17]. The Great Mississippi Flood of 1927 and 1993 are someother examples of the development of sand boils. The Mississippiﬂooding in 2011 also resulted in the development of many sand boils thatlater weakened the levees [18].In this research, we concentrate on detecting sand boils. Sand boilsoccur when water that is under intense pressure wells up through a bed ofsand. Hence, the water looks like it is literally boiling up from the surfacebed of sand [19]. A representation of how a sand boil forms can be seenfromFig. 1. Factors that inﬂuence the formation of sand boils include thepresence of ditches, post holes or seismic shot holes, cracks or ﬁssures from repeated drying and uprooted or decaying roots of trees [ 20]. Since sand boils have some characteristic features such as being cir-cular, and with a discernible center area, it is a good subject for a ma-chine learning model to be able to make accurate predictions on. Thisfault or deﬁciency, shown inFig. 2[21,22], usually appears as bubbling orﬂowing oriﬁce on the land side of a levee. They are typically consid-ered a substantial hazard, especially if they are large and/or have movingsoil [17]. Observations such as the throat width (width of the opening)
Fig. 1.Cross-sectional view of the levee area shows how a sand boil is formed due to intense pressure of water from the waterside of the levee. Water bubbles upthrough a crack or other imperfection on the surface of the silt blanket.
Fig. 2.An image of a sand boil from Vicksburg District [ 22,23].A. Kuchi et al. Array 3-4 (2019) 100012
2and the height and diameter if a cone of sand is formed around it must bemeasured to determine the severity of the sand boil. Levees need to beappropriately maintained and actively monitored for failure. Due to thetremendous length and variations of levee structures, proper operationand maintenance of levees can be challenging, especially when it iscurrently done manually [2,3] using physical surveys which are a drainon time and resources. This thesis aims to speed up this process and assistin the monitoring process of levees and coastal changes by detecting sandboil from images, expected to be collected by drones for monitoring thelevees.2.2. Object detectionObject detection has become a popular approach to ﬁnd certain re- gions within an image. Object detection is an image processing techniquethat detects instances of a target object within a given image. Manypopular algorithms exist to detect objects of particular classes such ashumans (pedestrians) [24], faces [4,5], cars [25], etc. Similarly, we intend to use object detection toﬁnd sand boils in satellite images collected from drones, near levees. Object detection requires input im-ages, positive training samples that consist a class, and negative imagesthat do not have any instance of the positive image within them.Computer vision is the study of algorithms that manipulate image-based data [26]. These algorithms extract features from images andpass them to machine learning models, which then identify particularregions of interest (ROI) in the image. Object detection, however, is acombination of computer vision and machine learning. It recognizes aninstance of the target object and draws bounding boxes around it.The major difference between image detection and recognition is thatthe former is generally concerned with detecting where the image aparticular object resides by drawing bounding boxes around them. Imagedetection can be treated as a simple classi ﬁcation problem where the image can be classiﬁed based on the presence of the object within it.Image detection is an essential preprocessing step for object recognition.Object recognition is used to compare the similarity between two images.For example, face recognition algorithms that group pictures of peopletogether [27].A positive image contains the data about the kind of object beingsearched for, and a negative image sample doesn ’t have any data that resembles a positive image. Object detection algorithms train on a set ofnegative and positive images to learn the parameters of what makes it apositive image. For example, in the case of a sand boil, the circularity ofthe positive image, the central depth of the sand boil, measured as theintensity of pixels, etc. make the recognition of a sand boil feasible. Themachine learning models extract such features and train on them. When atest data set consisting of both positive and negative instances is given tothe model, it is then capable of distinguishing between them and even-tually drawing bounding boxes around the positive instance.Object detection is an interestingﬁeld that has many applications inthe real world. Application of machine learning on object detection canbe challenging depending on the characteristics of the object. Detectionof sand boils within the muddy settings could be very hard to performfrom a given image. Object detection has previously been applied to somesimilar issues [28].3. MethodologyIn this section, we describe all the machine learning methods we useand describe their underlying principles. We have also explained why wechoose to use them.3.1. Dataset creationIn this research, the dataset was collected from different sources.There is no centralized, easy to access data for levees and sand boilsavailable. Hence, most of the collection was done manually. An addi-tional explanation of the subsets of data used for each method isdescribed below.Analysis of sand boils using machine learning requires two types ofdata–negative and positive samples. Positive samples are the imagesthat have an instance of the sand boil present somewhere in it. A negativesample is one in which no positive image is present at all. Both thesetypes of data are essential to training the machine learning methods onwhat constitutes a sand boil and what does not. This data was collectedby scraping different sources.
We have a total of 6300 negative images that were collected fromImageNet [29], Google Images and Open Street Maps [ 30]. The images were resized or sliced to a 150/C2150 size and converted to grayscale.Real positive images are very few and were collected from Google Im-ages. To increase the number of positive samples to be comparable withthe negative images available, we used a function within OpenCV tosynthetically place 50/C250 resized images of sand boils on some negativesamples in order to form a synthetic dataset of positive images as showninFig. 3. These positive images are 6300 in number and are of compa-rable quality to real positives. It was made sure that there were somenegative images and positive images that were very hard to detect, evenfor the human observer.(i) Viola-Jones algorithm–For the Viola-Jones method for sand boildetection, all the collected samples, 6300 positive samples, and6300 negative samples were used.(ii) You Only Look Once (YOLO) object detector –For this method, we used a subset of 956 positive samples to train the network. Furtherdetails about the network and the parameters used for training canbe found in the appendix.(iii) Single Shot MultiBox Detector (SSD) with MobileNetV2 –For SSD, we used a subset of 956 positive samples and 956 negative sam-ples to train the detector.
Fig. 3.Synthetically generated positive images. The positive image is superimposed onto negative images to generate multiple positive images.A. Kuchi et al. Array 3-4 (2019) 100012
3(iv) Non-deep learning methods such as Support Vector Machine(SVM), Logistic Regression (LogReg), K Nearest Neighbors (KNN),etc.–a balanced dataset of 956 positive samples and 956 negativesamples were used.The usage of a subset of images was necessary for the other methodsbecause, to input the images, the exact areas within the samples con-taining the positive region must be hand annotated. It is a manual processthat is intensely time-consuming. Hence, we labeled only 956 images aspositive samples. To do this, a tool called BBoxLabel [ 31] was used. BBoxLabel is an opensource tool that opens a simple GUI where it allowsthe user to load a directory of images and annotate them. These anno-tations are stored in two formats. Both a simple text ﬁle and an XMLﬁle format which both YOLO and SSD use. On the other hand, annotations forthe Viola-Jones method are internally calculated by OpenCV using asimple command. Theseﬁles generated by OpenCV cannot be used byYOLO and SSD because they have one major distinction. The x, y anno-tation from BBoxLabel tool is the center coordinate of the image, whereasthe one calculated by OpenCV is the coordinate of the top-left pixel.3.2. Feature extractionIn this section, we describe the features from both positive andnegative samples that have been extracted and fed into the machinelearning methods. For the Viola-Jones algorithm, the features used arecalled Haar features. These are calculated internally by OpenCV. Theycan be visualized to see which regions of the image contribute to theclassiﬁcation of it being a positive sample. Further discussion and ex-amples can be found in the results section. For the YOLO algorithm andthe SSD method, since the architecture can be considered a part of theconvolutional neural network, the features are extracted internally by theconvolutional net itself. For the non-deep learning methods, we need toextract some features manually as described below.3.2.1. Haralick featuresHaralick features or textural features for image classi ﬁcation [32] can help in identifying the texture characteristics of an object in an image.For aerial photographs or satellite images of sand boils, these features canisolate the textures and regions of interest of the image being searchedfor. In conjunction with the machine learning methods, these featurescan be extremely useful in identifying sand boils from satellite imagery.The basis of these features is a gray-level co-occurrence matrix. Thematrix is generated by counting the number of times a pixel with a value i is adjacent to a pixel with valuejand then dividing the entire matrix bythe total number of such comparisons made. Each feature is thereforeconsidered to be the probability that a pixel with a value iwill be found adjacent to a pixel of valuejin the image. This feature is rotationinvariant. Therefore, it can work well for our case because the structureof a sand boil is predictable, and images taken from any angle (fromdrones or satellites) can be detected successfully. Haralick features yielda list of 13 features.Table 1shows the difference between the Haralickvalues of positive and negative sample images. More information on thefeature is available inAppendix A.3.2.2. Hu MomentsHu moments, also known as image moments [ 33] are important features in computer vision and image recognition ﬁelds. They are useful for describing object segmentation and other properties. It is a weightedaverage (or moment) of the image pixel intensities. For a binary image, itis the sum of pixel intensities (all white) divided by the total number ofpixels in that image. The moments associated with a shape remain con-stant even on rotating the image. Therefore, when drone images whichare taken from multiple angles are used, the feature set doesn ’t change. Hence, these are good features for the detection of sand boils.Hu Moments yield a list of 7 features. Table 2describes the different Hu moments for each of 2 positive and 2 negative samples. Analyzingthese features and feeding them into a machine learning model fortraining will result in the creation of a good positive versus negativeboundary.3.2.3. HistogramsHistograms can be generated directly using OpenCV
’s calcHist func- tion in Python on Cþþ. It calculates the histogram of one or more arrays.We use it to generate 2D histograms to understand the distribution ofpixel intensities in the image. This proves to be a useful feature for thedetection of images based on the assumption that similar images willhave similar histograms. The number of bins can be speci ﬁed in the pa- rameters required. It can range from 32, 64 to 256. The difference be-tween a 32-bin and 256-bin histogram can be seen in Fig. 4. For our 150 /C2150 input images, it is enough to use a 32-bin histogram. This yields aTable 1Each of the images consists of 13 features. These features for positive images and negative images are substantially different from each other. For ex ample, in Haralick feature 7, the values for the positive images are in the range of 4000 –7000, and the features for negative images are signi ﬁcantly higher at the 10000 to 15000 range. Similarly, the other features present the same distinction between positive and negative images when taken in relation to each other.
Image and Type→FeaturesPositive
Positive
 Negative
 Negative
Ha [1] 0.000397 0.000624 0.000154 0.000163Ha[2]239.3129 204.0823 372.8986 312.8296 Ha [3] 0.902915 0.936348 0.950706 0.940855Ha[4]1234.529 1605.22 3783.866 2644.578 Ha [5] 0.157106 0.203365 0.1154 0.123532Ha [6] 179.9395 275.2416 213.5478 275.1554Ha[7]4698.803 6216.799 14762.57 10265.48 Ha [8] 7.78588 7.925233 8.72006 8.576808Ha[9]12.07775 11.81139 13.20276 13.11339 Ha[10]0.000275 0.00027 0.00015 0.000153 Ha[11]4.776676 4.432085 5.108685 5.03963 Ha [12]/C00.23966 /C00.30512 /C00.29649 /C00.27968 Ha [13] 0.978168 0.992403 0.994718 0.992696
Some of the Haralick features indicated using bold values, help discriminate positive versus negative classes.A. Kuchi et al. Array 3-4 (2019) 100012
4Fig. 4.The difference between 256-bin and 32-bin representations of color histograms of images. (a) is the histogram using 32-bin, (b) represents a 256-bin his- togram. 32-bin histogram is enough for a 150 /C2150 image.
Fig. 5.These histograms differentiate between positive samples and negative samples. Figure (a) shows the histogram for a negative image whereas ﬁgure (b) shows the histogram for a positive image.Table 2This table aims to illustrate the difference between the Hu Moment features of the positive samples and negative samples. The binarized images produc e these features. In order to distinguish the positive samples from the negative samples, the conjunction of features is used. For example, in Hu [ 6], the values for the positive samples are negative whereas the negative samples ’values are positive and very high. Similarly, in Hu [ 7], the distinction between positive images and negatives images can be seen very easily.
Image and Type→FeaturesPositive
Positive
 Negative
 Negative
Hu [1] 0.001221164 0.001930522 0.000154 0.001189Hu [2] 6.06E-09 1.04E-08 372.8986 1.78E-10Hu [3] 1.61E-11 1.11E-10 0.950706 3.42E-11Hu[4]1.97E-13 1.16E-10 3783.866 2.99E-11 Hu [5] 1.32E-25 1.32E-20 0.1154 /C04.14E-22 Hu[6]¡1.04E-17¡1.16E-14 213.5478 3.61E-16 Hu[7]¡3.25E-25¡4.14E-22 14762.57 8.59E-22
Some of the Hu Moments indicated using bold values, help discriminate positive versus negative classes.A. Kuchi et al. Array 3-4 (2019) 100012
5list of 32 features.Fig. 4illustrates how these features help in dis-tinguishing positive and negative samples of sand boils. The histogramsare vastly different for each type.Fig. 5shows the distinction between the histogram of a positive imageversus the histogram of a negative image. There is a clear distinctionbetween the positive image and negative image. Hence these features canbe good to distinguish between sand boils and non-sand boils inconjunction with other features.3.2.4. Histogram of oriented gradients (HOG)Histogram of Oriented Gradients (HOG) [ 24] is a very popular feature descriptor for images in the computer vision world and has been knownfor improving the accuracy of detection for images with ﬁxed outlines such as pedestrians. Dalal and Triggs are known for detecting humansusing these features. The algorithm counts the occurrences of gradientorientation in localized portions of an image. Similar to edge detection, itdescribes the local object’s appearance and shapes in an image. Theimage is divided into small connecting regions and for each pixel in eachcell, a histogram of gradient direction is calculated. This can be observedinFig. 6. This results in aﬁnal list of appended histograms which is theﬁnal list of features.Now, the complete list of features includes 7 Hu Moments, 13 Har-alick features, 32 Histogram values, and 648 HOG features –totaling to 700 features for each image.3.3. Viola-Jones object detection algorithmThe Viola-Jones’object detector [4,5] uses the AdaBoost algorithm as the learning algorithm. Using OpenCV, Haar features are calculated andgathered. To implement the algorithm, Open-source Computer Vision(OpenCV) [34], as a Python package is used.The cascade is trained using 25 stages, 4500 positive samples, and3000 negative samples. This results in an XML ﬁle that can be used to make predictions on a test dataset. The test dataset contains a total of8300 images out of which 2000 are negative samples, and the rest arepositive samples of sand boils. The cascade training is stopped at stage25, and the intermediate stages, 10, 15, 20 and 25 are tested for per-formance. The best performance is achieved by cascade stage 15. This isevidenced in the results section.3.4. You Only Look Once (YOLO) algorithmYou Only Look Once, or YOLO [35] is a popular object detection al- gorithm based on the concept of Region Proposal Networks [ 6]. It is a deep learning [36] framework based on TensorFlow [37] and consumes a lot of computational power. YOLO reframes the object detection pipelineof older papers such as R–CNN [38], Faster-RCNN [39] as a single regression problem. It predicts the presence of sand boils from individualimage pixels, the bounding box coordinates and class probabilities ofeach region or cell. Any input image given to this dense neural networkwill output a vector of bounding boxes and class predictions. Each inputimage is divided into S/C2S grid cells as shown inFig. 7. Then, it maps the regions’proposals as shown inFig. 8. Combining these bounding box predictions and region proposals, theﬁnal predictions are made.For the purpose of this research, we used the readily availablecon
ﬁguration called Darknet [40]. The architecture was crafted for usewith the Pascal VOC dataset [41]. Therefore, to use the network with adifferent grid size, a different number of classes (binary in our case) willrequire tuning of these parameters.A TensorFlow implementation of the darknet, called Dark ﬂow [42] was used to run YOLO. Darknet is an open-source neural networkframework written in C language. The architecture of the detector con-sists of 24 convolutional layers followed by two fully connected layers.Alternating 1/C21 convolutional layers reduce the feature space from theprevious layers. These layers are pre-trained on the ImageNet classi ﬁ- cation dataset. For our problem, we chose to train the network fromscratch, without initializing the weights for the network. The averagemoving loss starts at a very high value and slowly reduces. The networkmust be trained until the loss falls below 0.01 in order to get the bestbounding boxes. The training ran for around 2 weeks on a server, atwhich point the loss function was extremely low. At this point, the latestcheckpoint is used to test the detections on a test dataset of 112 images.After the detections are made, the accuracy can be measured bycomparing the ground truthﬁles with the predicted outputs.3.5. Single Shot MultiBox Detector (SSD)The single-shot multibox detector [7,43] is an improvement on the YOLO object detector in the sense that it does not have to traversethrough the image twice as YOLO does. It can map out the object in asingle shot. It is a very fast approach and extremely accurate. SSD is also adeep learning method for object detection.
Fig. 6.Illustration of the histogram of oriented gradients. Observe that the blue lines represent vectors in the direction of light. The length of the arrows represents the value. (a) represents the division of image into individual cells. Figure (b) shows an enlarged portion of the selected area. The direction of the arro w represents the direction of the lighter pixels and the length of the arrow represents the magnitude of the vector by which the pixel intensities differ.A. Kuchi et al. Array 3-4 (2019) 100012
6A single-shot multibox detector can localize the object in a singleforward pass of the network. Its architecture builds on the VGG-16 [ 44] architecture but discards the fully connected layers. VGG-16 performsvery well on high-quality image classiﬁcation. However, in this research, we use the MobileNetV2 [19] architecture in the place of VGG-16.MobileNetV2 is one of the most recent mobile architectures. It favors
Fig. 8.S/C2S grid predicting B bounding boxes and con ﬁdence scores, C class probabilities [ 35].
Fig. 7.Figure showing the division of the image into S /C2S cells. The image of a sand boil in red has its center (blue dot) lying in the central grid cell. This grid cell is responsible for making the prediction [ 35].A. Kuchi et al. Array 3-4 (2019) 100012
7speed over accuracy but still manages to output excellent detections.Features are extracted at multiple scales by the MobileNet network andforward passed to the rest of the network.A PyTorch implementation [45] with some alterations to the numberof classes, etc. of SSD was used to make the detections. After training thenet for 200 epochs, we use the latest generated checkpoint to make thepredictions. Then we produce the bounding boxes for each of the imagesin the test data set.For both YOLO and SSD, a similar method was used to calculate theaccuracy. The ground truthﬁles generated after manually annotating theimages were compared with the predicted outputs to measure theaccuracy in the percentage of the detector.3.6. Stacking3.6.1. Training individual methodsA total of 8 different methods were used, including Support VectorMachine (SVM) [10] which is a discriminative classiﬁer which is deﬁned by a separating hyperplane, Logistic Regression [ 13] which is a technique for analyzing data where there are one or more independent variablesthat determine the dependent variable (outcome), Extremely randomizedTree or ET [46] which constructs randomized decision trees from theoriginal learning sample and uses above-average decision to improve thepredictive accuracy and control over-ﬁtting, Random Decision Forests [47] which is an ensemble learning method for classi ﬁcation, regression, and other tasks, K nearest neighbors [ 48] which stores all available cases and classiﬁes new cases based on a similarity measure, Bagging [ 49] which creates individuals for its ensemble by training each classi ﬁer on a random redistribution of the training set, Gradient Boosting Classi ﬁer (GBC) [11] which builds an additive model in a forward stage-wisefashion, and eXtreme Gradient Boosting (XGB) [ 12] that has more advanced features for model tuning.A feature-ﬁle was created which lists all the 700 features that werediscussed earlier in section3.2, for each image. There are a total of 1912Table 3Table specifying the tested stacking models with details about the base and metaclassiﬁers used in each.
Stacking Model Number Base Classi ﬁers Meta Classi ﬁerSM1 RDF, LogReg, KNN SVMSM2 LogReg, ET, KNN SVMSM3 LogReg, XGBC, KNN SVMSM4 LogReg, ET, XGBC SVMSM5 LogReg, GBC, KNN SVMSM6 LogReg, GBC, ET SVMSM7 LogReg,GBC,ET,KNN GBCSM8 LogReg, GBC, ET, KNN SVMSM9 RDF,LogReg,GBC, KNN GBCSM10 RDF,LogReg,GBC, KNN SVMSM11 LogReg,GBC,SVM,KNN GBCSM12 LogReg, SVM, ET, KNN GBCSM13 LogReg, SVM, ET, KNN SVMSM14 LogReg,GBC,SVM,KNN SVMSM15 LogReg, GBC, ET SVMSM16 LogReg, GBC, ET GBCSM17 RDF, KNN, Bag GBCSM18 RDF, KNN, Bag SVM
SM stands for Stacking Method.
Table 4A comparison of accuracies for all methods run independently.
Method Sensitivity Speci ﬁcity Accuracy Precision F1 Score MCCSVM97.49 92.05 94.7792.46 0.949 0.8967 KNN 61.82 81.9 71.86 77.35 0.6872 0.4463GBC97.28 88.49 92.8889.42 0.9318 0.8610 XGBC 89.43 89.33 89.38 89.34 0.8938 0.7876RDF 92.46 89.74 91.11 90.02 0.9122 0.8224ET95.5 90.48 92.9990.93 0.9316 0.8609 LOGREG 81.27 81.79 81.53 81.7 0.8148 0.6307BAGGING 92.05 87.34 89.69 87.91 0.8993 0.7958
Table 5Performance of various Stacking methods.
Model type Sensitivity Speci ﬁcity Accuracy Precision F1 Score MCCSM 1 0.9749 0.91527 0.94508 0.92004 0.94667 0.89175SM 2 0.98117 0.9205 0.950840.92505 0.95228 0.90334 SM 3 0.97594 0.91318 0.94456 0.91831 0.94625 0.89088SM 4 0.97803 0.9205 0.94927 0.92483 0.95069 0.90003SM 5 0.9728 0.91841 0.94561 0.92262 0.94705 0.89253SM 6 0.97594 0.92364 0.94979 0.92744 0.95107 0.90081SM 7 0.95816 0.94038 0.94927 0.94142 0.94971 0.89868SM 8 0.97699 0.91841 0.9477 0.92292 0.94919 0.89694SM 9 0.95188 0.91736 0.93462 0.92012 0.93573 0.86977SM 10 0.97699 0.91423 0.94561 0.91929 0.94726 0.89297SM 11 0.96757 0.94038 0.953970.94196 0.95459 0.90829 SM 12 0.96444 0.9341 0.94927 0.93604 0.95003 0.89895SM 13 0.97803 0.92992 0.953970.93313 0.95506 0.909 SM 14 0.97699 0.9205 0.94874 0.92475 0.95015 0.89892SM 15 0.97803 0.9205 0.94927 0.92483 0.95069 0.90003SM 16 0.96235 0.93096 0.94665 0.93306 0.94748 0.89375SM 17 0.95816 0.90586 0.93201 0.91054 0.93374 0.8652SM 18 0.9728 0.91736 0.94508 0.9217 0.94656 0.89154Table 6Comparison of all the methods used. The proposed stacking method performs thebest.
Model Name AccuracyViola-Jones Object Detector 87.22%Single Shot MultiBox Detector 88.35%Support Vector Machine (Best performer among eight methods) 94.77%Stacking (LogReg, SVM, ET, KNN) 95.39%A. Kuchi et al. Array 3-4 (2019) 100012
8records split evenly into 956 positive images and 956 negative images ofthe sand boil. On thisﬁle, 10-fold cross-validation is run. Variousmethods including SVM, GBC, XGBC, Logistic regression, random deci-sion forest, etc. are implemented.3.6.2. StackingStacking based machine learning approaches [ 50] have been very successful when applied to some interesting problems [ 28,51–53]. This idea is utilized here to try and develop a better performing sand boilpredictor.Stacking is an ensemble approach that obtains information frommultiple models and aggregates them to form a new and generallyimproved model. We examined 18 different stacking models as describedinTable 3. These models are built and optimized using Scikit-Learn [ 54]. To select the algorithms to be used as base classi ﬁers, we evaluate all these combinations.4. Results and discussionsIn this section, we present the results of our simulations and discussthe important features and factors of the predictions and why just ac-curacy is not a good measure for comparing object detectors. Table 6 shows a summary of all the different methods used and their accuracies.
Fig. 9.Visualization of Haar features generated by OpenCV. These selected Haar features help in determining whether or not the image contains a sand boil. Ea ch of the Haar features (the white and black boxes) are overlaid on the image to check the presence of that feature in each image. If a group of Haar features de ﬁned by the cascade are present in an image, it is categorized as a positive for the sand boil.A. Kuchi et al. Array 3-4 (2019) 100012
94.1. Viola-JonesIn the Viola-Jones’object detection algorithm, we achieve an overallaccuracy of 87.22% (Appendix Bdeﬁnes the metrics to measure the performance). The test data set consisted of 8300 images out of which2000 were negative, and rest were positive. The number of true positivesdetected by the cascade classiﬁer was 5425 out a total of 6300. Thenumber of false positives is 185, true negatives are 1815, and falsenegatives detected were 875. The sensitivity is 86.11%, speci ﬁcity is90.75%, precision 96.70%. The MCC score is impressive at 0.7023. TheF1 score is 0.91099. Overall, the Viola-Jones algorithm performs verywell despite being one of the oldest methods for object detection. Haarfeatures that are generated can be visualized as shown in Fig. 9.Fig. 10 shows some of the detections made by the Viola-Jones object detector.Some of the most important Haar features are the ones found in theimage shown inFig. 8. The integral image is calculated and comparedwith these haar features to determine whether an image is a sand boil ornot.
Fig. 10.True positive detections made by the Viola-Jones detector. These images indicate the bounding boxes drawn by the Viola-Jones cascade. Notice that th e true positives shown here are present on rough terrain which makes it harder for an object detector to ﬁnd positive samples easily.
Fig. 11.Some special false-positive cases detected by the Viola-Jones object detector. These images depict some false positives that were detected. These f all within reasonable doubt of being sand boils. In (a) the false positive that is detected contains a darker center and a roughly circular exterior. This gives th e detector cause to classify it as a sand boil. Similarly, in (b), (c) and (d) the detections made are bounding boxes that contain circular images with darker centers and te xture similar to what a sand boil has.A. Kuchi et al. Array 3-4 (2019) 100012
104.1.1. Discussion about some special false positivesInFig. 11, are some examples of misdetections by the Viola-Jonesclassiﬁer. Some of these detections output multiple bounding boxes onthe images, leading to some false positives. Upon further investigation,though, these false positives look very much like the positive samples.This means that the images within these bounding boxes fall withinreasonable doubt of being a sand boil. The contents feature a darkercircular area inside a lighter circle or so. This can easily be mistaken to bea sand boil. Since our research deals with the detection of sand boils,which are a danger to levee health, some acceptable number of falsepositives are not a problem. It is better to have these being detected thanbeing passed over.4.2. YOLO detectionYOLO yields a set of detections that appear to be correct for a clas-siﬁer–not as a detector. The detected bounding box remains constant forall the images in which it identiﬁes a positive sand boil. The results arenot wrong. But because the problem is that of a detector, i.e., we expectan accurate bounding box to be drawn around the sand boil, the YOLOdetection fails at this task. Given that due to time constraints, the trainingwas allowed to run only for a short duration, the net is unable to drawaccurate bounding boxes around the required images. The net was tunedappropriately. More information can be found in the appendix section ofthe thesis. The images inFig. 12illustrate the detection by YOLO.Perhaps with further tuning and more extended training of the net, wewill achieve better results. For now, we discard the YOLO algorithm fromfurther consideration.4.3. Single Shot MultiBox detectionThe SSD detector yields very promising results with an average pre-cision of 88.35%. Some of the detections made by the SSD algorithm areshown inFig. 13.4.3.1. Discussion on some particularly hard false negativesThe fourthﬁgure inFig. 13(d) does not show any detection. This is afalse negative detection. There is in fact, a sand boil image overlaid onthe terrain in the bottom right. This is, however, extremely hard to ﬁnd even for the human eye. Therefore, the false negatives of this kind can beskipped over. The other three detections were surprisingly accurate,especially because the base image is that of very rough terrain that mightresemble a sand boil’s surface by itself.4.4. StackingThe results from the 8 different methods were extremely good.Table 4describes the results of all the methods that were used inde-pendently. The highest accuracy was that of the support vector machineand extra tree. This is followed by a gradient boosting classi ﬁer and
Fig. 12.Detections made by the YOLO object detector. Figures (a), (b) and (c) show bounding boxes created by the YOLO detector. They are accurate enough to dete ct true positives. But the bounding boxes that are drawn require ﬁne-tuning the net further since they do not correctly predict the exact coordinates of the true position of the sand boil.
Fig. 13.Some difﬁcult detections made by the Single Shot MultiBox Detector. Images (a), (b) and (c) show how the SSD detector makes some very dif ﬁcult predictions despite the underlying terrain. The number ‘1.00’indicates the probability with which the detector thinks it is a positive sand boil. Image (d) shows a False Negative image. SSD was unable to detect the sand boil in this case. This is a reasonable image to miss because of the different textures and similar-looking patt erns in the terrain of the map. Detections showed in (a), (b) and (c) are impressive for an object detector since the images are easy to miss even for humans.A. Kuchi et al. Array 3-4 (2019) 100012
11Fig. 14.Flowchart illustrating the stacking technique. The features of the images are collected and passed on to the selected base classi ﬁers which then make predictions and generate a list of probabilities of each prediction ( Pr
1;Pr2;Pr3). This list of predictions is added to the complete feature ﬁle and fed into the meta classiﬁer. The meta classiﬁer then predicts the outcome.A. Kuchi et al. Array 3-4 (2019) 100012
12random decision forest.Fig. 14illustrates aﬂowchart that shows the stacking methodology indetail.4.4.1. Usage of genetic algorithm for feature selectionThe genetic algorithm is an evolution-based algorithm that iterativelygenerates new populations and selects the best possible set of genes to doso. Over successive generations, the population moves toward the mostoptimized solution. This algorithm can be applied to a variety of opti-mization problems. We chose to use it to select the best possible set offeatures among the 700 that were derived. We arrived at a total of 324features. After running the above-described methods on both sets offeatures, it was determined that the complete set of 700 features per-formed better in comparison to the feature set selected by the geneticalgorithm. The possible reasons for failure might be that the geneticalgorithm-based feature selection uses XGBoost to compute the ﬁtness of the population. The features selected using XGBoost might not always bethe optimal settings for other state-of-the-art algorithms. Another reasonis that the set of HOG features described above work on the basis ofcorresponding pixels. Trying to isolate the features might prove not to beuseful.We tried 18 different types of stacking with different base classi ﬁers. These were chosen based on their differing principles and their accu-racies independently.Table 5shows the comparison of these stackingmodels. The performance of stacking depends on the principle that eachof the base learners helps the meta-learner perform better. In this case,model 13 performs the best. Model 11 and 13 have the same accuracy.But considering the other parameters, especially MCC, model 13 per-forms slightly better. The code and data of our proposed model are freelyavailable herehttp://cs.uno.edu/~tamjid/Software/sandboil/code_data.zip.5. ConclusionsIn this paper, we compared different object detection methods anddetermined the best ones to use for the detection of sand boils. We alsodeveloped a Stacking-based machine learning predictor which focuses onusing the best methods to increase the detection accuracy of the machinelearning model.We also created a database of positive and negative samples of sandboils for use in research. The most appropriate Haar features wereselected using the AdaBoost algorithm, You Only Look Once (YOLO)object detection algorithm was tested. It was ruled out from furtherconsideration because, despite parameter tuning and multiple trials anderrors, the bounding boxes generated were not very useful. Single ShotMultiBox detector was studied and was found to be a good detectionmodel for sand boils with high accuracy of 88.3%. Furthermore, the inputdata was divided into simple and hard detections by the SSD imple-mentation we used. Since this is a single class detection problem, theaverage precision per class that was calculated by SSD pertains only toone class. For the rest of the methods, it is found that SVM performs thebest on 10-fold cross-validation, achieving a high detection accuracy of94.77%. GBC and Extra Tree were also extremely high performing at92.88% and 92.99% respectively. Stacking on all the non-deep learningmethods revealed even better performance of 95.4% accuracy. Hence, forthe detection of sand boils, non-deep learning methods are proven to bethe best. In future studies, a better implementation of YOLO may beincluded. The stacking of deep learning nets and SVM, GBC, etc. mightprove to be useful. To improve individual performance of methods, manyother features can be collected from the images. Also, collecting real-world satellite images of areas near levees and hand annotating the im-ages would yield better results.Table 6shows a comparison of all the methods and their respective accuracies.Declaration of Competing InterestThe authors declare no conﬂict of interest.AcknowledgmentsThe authors gratefully acknowledge the Louisiana Board of Regentsthrough the Board of Regents Support Fund LEQSF (2016 –19)-RD-B-07.Appendix AA list of haralick features and their formulae.Notationpði;jÞ-ði;jÞth entry in a normalized gray-tone spatial dependence matrix, ¼Pði
;jÞ=R. p
xðiÞ- ith entry in the marginal-probability matrix obtained by summing the rows of pði;jÞ;¼PNg
j¼1Pði;jÞ. Ng–Number of distinct gray levels in the quantized image.p
yðjÞ¼PNg
i¼1pði;jÞp
xþyðkÞ¼PNg
i¼1PNg
j¼1δiþj;kpði;jÞwhere k¼2,3,….,2N g.p
x/C0yðkÞ¼PNg
i¼1PNg
j¼1δji/C0jj;kpði;jÞwhere k¼0,1,2,3,….,N g/C01. where the Kronecker delta function δ m;nis deﬁned byδ m;n¼/C261when m¼n0when m6¼n 1) Angular Second Moment:f
1¼P
iP
jfpði;jÞg2
2) Contrast:f 2¼PNg/C01n¼0n2fPNg
i¼1PNg
j¼1pði;jÞgwhere |i-j|¼n3) Correlation:f
3¼P
iP
jðijÞpði;jÞ/C0 μxμy
σxσy
4) Sum of Squares: Variancef 4¼P
iP
jði/C0μÞ2pði;jÞ 5) Inverse Difference Moment:f
5¼P
iP
j11þði/C0jÞ2pði;jÞA. Kuchi et al. Array 3-4 (2019) 100012
136) Sum Average:f 6¼P2Ng
i¼2ipxþyðiÞ7) Sum Variance:f
7¼P2Ng
i¼2ði/C0f 8Þ2pxþyðiÞ8) Sum Entropy:f
8¼/C0P2Ng
i¼2pxþyðiÞlogfp xþyðiÞg 9) Entropy:f
9¼/C0P
iP
jpði;jÞlogðpði;jÞÞ 10) Difference Variance:f
10¼variance of p x/C0y
11) Difference Entropy:f 11¼/C0PNg/C01i¼0px/C0yðiÞlogfp x/C0yðiÞg 12) 13) Information measures of Correlation: f
12¼HXY/C0HXY1maxfHX;HYg f13¼ð1/C0exp½/C02:0ðHXY2/C0HXYÞ/C138Þ2HXY¼/C0P
iP
jpði;jÞlogðpði;jÞÞAppendix B
Formulae for performance metrics of machine learning methods.
Name of Metric De ﬁnitionTrue Positive (TP) Correctly predicted as a sand boilTrue Negative (TN) Correctly predicted as not a sand boilFalse Positive (FP) Incorrectly predicted as a sand boilFalse Negative (FN) Incorrectly predicted as not a sand boilRecall/Sensitivity (Sens.)/True Positive Rate (TPR)
TPTPþFN Speciﬁcity (Spec.)/True Negative Rate (TNR)
TNTNþFP Accuracy (ACC)
TPþTNTPþFPþTNþFN Balanced Accuracy (BACC)
12/C18 TPTPþFNþ TNTNþFP/C19Precision (Prec.)
TPTPþFP F1 score (Harmonic mean of precision and recall)
2TP2TPþFPþFN Mathews Correlation Coefﬁcient (MCC)
ðTP/C2TNÞ/C0ðFP/C2FNÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃðTPþFNÞ/C2 ðTPþFPÞ/C2 ðTNþFPÞ/C2 ðTNþFNÞ p
References
[1]Agency FEM. Evaluation and monitoring of seepage and internal Erosion.Interagency Committee on Dam Safety (ICODS); 2015 . [2] Jay N, Stateler PC. United States Society on dams - monitoring levees. etc, Availablefrom:https://www.ussdams.org/wp-content/uploads/2016/05/Monitoring-Levees.pdf; 2016.[3]Nobrega RaA, James, Gokaraju Balakrishna, Mahrooghy Majid, Dabbiru Lalitha,O’Hara Chuck. Mapping weaknesses in the Mississippi river levee system usingmulti-temporal UAVSAR data. Braz J Cartogr Photogrammetry and Remote Sens2013;65(4):681–94.[4]Viola P, Jones MJ. Robust real-time face detection. Int J Comput Vis 2004;57(2):137–54.[5]Wang Y-Q. An analysis of the viola-jones face detection algorithm. Image ProcessLine 2014;4:128–48.[6]Redmon JaF, Ali. YOLOv3: an Incremental improvement. arXiv; 2018 . [7]Wei Liu DA, Erhan Dumitru, Szegedy Christian, Reed Scott E, Fu Cheng-Yang,Berg Alexander C. SSD : single shot MultiBox detector. CoRR; 2015 . [8]Yann LeCun PH, Leon Bottou, Bengio Yoshua. Object recognition with gradient-based learning. 1999.[9] LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521:436. https:// doi.org/10.1038/nature14539 . Available from:. [10]Vapnik VN. An overview of Statistical learning theory. IEEE Trans Neural Netw1999;10(5).[11]Friedman JH. Greedy function approximation: a gradient boosting machine. AnnStat 2001;29(5):1189–232. [12]Chen T, Guestrin C. XGBoost: a scalable tree boosting system. In: Proceedings of the22Nd ACM SIGKDD International Conference on Knowledge Discovery and DataMining. New York, NY, USA: ACM; 2016. p. 785 –94. Available from: http:// doi.acm.org/10.1145/2939672.2939785 . [13]Szil/C19agyi A, Skolnick J. Efﬁcient prediction of nucleic acid binding function fromlow-resolution protein structures. J Mol Biol 2006;358(3):922 –33. [14] Mishra A, Pokhrel P, Hoque MT. StackDPPred: a stacking based prediction of DNA-binding protein from sequence. Bioinformatics 2018:bty653. https://doi.org/ 10.1093/bioinformatics/bty653 . Available from:.[15]Flot MaM, Avdesh, Kuchi Aditi, Hoque Md, StackSSSPred. A stacking-basedprediction of Supersecondary structure from sequence. In: Kister A, editor. ProteinSupersecondary Structures. Methods in Molecular Biology; 2019. p. 101 –22. 1958. [16] Couvillion, B.R., et al., Land area change in coastal Louisiana from 1932 to 2010, inU.S. Geol Surv Sci Investig Map 3164, scale 1:265,000, 12 p. pamphlet.2011. [17]Schaefer A, Timothy JMOL, Robbins Bryant. Assessing the Implications of sand boilsfor Backward Erosion piping Risk. 2017. p. 124 –36. [18] USACE. Mississippi river and tributaries system 2011 post- ﬂood report. Available from:https://www.mvd.usace.army.mil/Portals/52/docs/regional_
ﬂood_risk_ management/Docs/MRT_PostFloodReport_(Main%20Report).pdf ; 2012. [19]Sandler MaH, Andrew, Zhu Menglong, Zhmoginov Andrey, Chen Liang-Chieh.MobileNetV2: Inverted Residuals and linear Bottlenecks. IEEE/CVF Conference onComputer Vision and Pattern Recognition; 2018. 2018 . [20]Davidson GR, Rigby JR, Pennington Dean, Cizdziel James V. Elemental chemistry ofsand-boil discharge used to trace variable pathways of seepage beneath leveesduring the 2011. Mississippi River ﬂood 2013;28:62–8. [21] Myarklamisscom. Crews repairing sand Boils in tensas parish. Available from: https ://www.myarklamiss.com/news/crews-repairing-sand-boils-in-tensas-parish/172396730; 2011.[22] Mark N. Detection of sand boil locations along the Mississippi river in fulton county,Kentucky. Available from:https://digitalcommons.murraystate.edu/postersatthecapitol/2014/Murray/16/. [23] Ozkan S. Analytical study on ﬂood induced seepage under river levees. Available from:https://digitalcommons.lsu.edu/cgi/viewcontent.cgi?article ¼3341&con text¼gradschool_dissertations; 2003. [24]Dalal NaT. Bill,Histograms of oriented Gradients for human detection . In: International Conference on Computer Vision&Pattern Recognition. CVPR’05); 2005. [25] Wei Y, Tian Q, Guo T. An improved pedestrian detection algorithm integratinghaar-like features and HOG descriptors. Adv Mech Eng 2013;2013:546206. https:// doi.org/10.1155/2013/546206 . [26]Szeliski R. Computer vision: algorithms and applications. Springer Science & Business Media; 2010.[27]Omkar M, Parkhi AV. Andrew Zisserman, deep face recognition. 2015. [28]Corey Maryan MTH, Michael Christopher, Elias Ioup, Abdelguer ﬁMahdi. Machine learning applications in detecting Rip Channels from images. Applied Softcomputing. Elsevier Journal; 2019 .A. Kuchi et al. Array 3-4 (2019) 100012
14[29]Deng J, Dong W, Socher R, Li LJ, Li K, Fei-Fei L. ImageNet: a large-ScaleHierarchical image database. 2009 . [30] OpenStreetMap, c., Planet dump retrieved from. Available from: https://www.ope nstreetmap.org; 2017.https://planet.osm.org. [31] Qiu S. BBox-Label-Tool. Available from: https://github.com/puzzledqs/BBox-Labe l-Tool; 2017.[32] Haralick RM, Shanmugam K, Dinstein IH. Textural features for image classi ﬁcation. IEEE Trans Syst Man Cybern 1973;SMC-3(6):610 –21.https://doi.org/10.1109/ TSMC.1973.4309314. SMC-3.[33]Hu M-K. Visual pattern recognition by moment invariants. IEEE Trans Inf Theory1962;8.[34]Bradski G. The OpenCV library. Dr. Dobb ’s Journal of Software Tools 2000 . [35]Joseph Redmon SD, Girshick Ross, Ali Farhadi. You only look once: Uni ﬁed, real- time object detection. 2015. ArXiv . [36]Zhao Z-QaZ, Peng and Xu Shou-Tao, Wu Xindong. Object detection with deeplearning: a review. IEEE Transactions on Neural Networks and Learning Systems2019.[37] Martin Abadi AA, Paul Barham, Brevdo Eugene, Chen Zhifeng, Craig Citro,Corrado Greg S. Andy Davis,Jeffrey Dean,Matthieu Devin,Sanjay Ghemawat,IanGoodfellow,Andrew Harp,Geoffrey Irving,Michael Isard,Yangqing Jia,RafalJozefowicz, and others,TensorFlow: large-Scale machine Learning on HeterogeneousSystems. Available from:http://tensorﬂow.org/; 2015. [38] Girshick R, Jeff, Darrell T, Malik J. Rich feature hierarchies for accurate objectdetection and semantic segmentation. Comput Vis Pattern Recognit 2014:580 –7. https://doi.org/10.1109/CVPR.2014.81 . [39]Ren SaH, Kaiming, Girshick Ross, Sun Jian, Faster R-CNN. Towards real-time objectdetection with region proposal networks. Proc 28th Int Conf Neural Inf Process Syst2015;1.[40] Redmon J. Darknet: open source neural Networks in C. Available from: http://pj reddie.com/darknet/; 2013 - 2016.[41]Everingham M, SMA, Va~nGool L, Williams CKI, Winn J, Zisserman A. The pascalvisual object classes challenge: a retrospective. Int J Comput Vis 2015;111(1):98–136.[42] Trieu. Darkﬂow. Available from:https://github.com/thtrieu/darkﬂow; 2018. [43] Liu WaA, Dragomir, Erhan Dumitru, Szegedy Christian, Reed Scott, Fu Cheng-Yang,Berg Alexander C. SSD : single shot MultiBox detector. ECCV; 2016. Available from:https://github.com/weiliu89/caffe/tree/ssd . [44]Zisserman KSaA. Very deep convolutional networks for large-Scale imagerecognition. 2014. arXiv. [45] Gao H. PyTorch implementation of SSD. Available from: https://github.com/qfgaoh ao/pytorch-ssd; 2019.[46]Geurts P, Ernst D, Wehenkel L. Extremely randomized trees. Mach Learn 2006;63(1):3–42
.[47]Ho TK.Random decision forests,i ndocument Analysis and recognition . In: Proceedings of the Third International Conference on. Montreal, Que., Canada: IEEE; 1995.p. 278–82. 1995.[48]Altman NS. An introduction to kernel and nearest-neighbor nonparametricregression. Am Stat 1992;46:175 –85. [49]Breiman L. Bagging predictors. Mach Learn 1996;24(2):123 –40. [50]Wolpert DH. Stacked generalization. Neural Netw 1992;5(2):241 –59. [51]Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A,Khosla A, Bernstein M, Berg AC. ImageNet large scale visual recognition challenge.Int J Comput Vis 2015:115. [52] Sumaiya Iqbal MTH. PBRpredict-suite: a suite of models to predict peptiderecognition domain residues from protein sequence. Oxf Bioinform J 2018;34(19):3289–99.https://doi.org/10.1093/bioinformatics/bty352 . [53]Michael Flot AM. Aditi Sharma kuchi, Md tamjidul hoque, StackSSSPred: a stacking- based Prediction of Supersecondary Structure from sequence. Book chapter (chapter 5, pp 101-122). In: Kister A, editor. Protein Supersecondary Structures. Methods inMolecular Biology; 2019. 1958 . [54]Pedregosa FaV, G., Gramfort A, Michel V, et al. Scikit-learn: machine learning inPython. 2011.A. Kuchi et al. Array 3-4 (2019) 100012
15