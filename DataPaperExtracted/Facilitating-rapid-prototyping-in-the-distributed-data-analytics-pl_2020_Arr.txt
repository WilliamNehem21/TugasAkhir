Facilitating rapid prototyping in the distributed data analytics platformOODIDA via active-code replacement
Gregor Ulma,b,*, Simon Smitha,b, Adrian Nilssona,b, Emil Gustavssona,b, Mats Jirstranda,b
aFraunhofer-Chalmers Research Centre for Industrial Mathematics, Chalmers Science Park, 412 88, Gothenburg, Sweden
bFraunhofer Center for Machine Learning, Chalmers Science Park, 412 88, Gothenburg, Sweden
ARTICLE INFO
Keywords:Distributed computingConcurrent computingDistributed data processingHot swappingCode replacementErlangABSTRACT
OODIDA (On-board/Off-board Distributed Data Analytics) is a platform for distributed real-time analytics, tar-getingﬂeets of reference vehicles in the automotive industry. Its users are data analysts. The bulk of the dataanalytics tasks are performed by clients (on-board), while a central cloud server performs supplementary tasks(off-board). OODIDA can be automatically packaged and deployed, which necessitates restarting parts of thesystem, or all of it. As this is potentially disruptive, we added the ability to execute user-de ﬁned Python modules on clients as well as the server. These modules can be replaced without restarting any part of the system; they caneven be replaced between iterations of an ongoing assignment. This feature is referred to as active-codereplacement. It facilitates use cases such as iterative A/B testing of machine learning algorithms or modifyingexperimental algorithms on-the-ﬂy. Various safeguards are in place to ensure that custom code does not have harmful consequences, for instance by limiting the allowed types for return values or prohibiting importing ofcertain modules of the Python standard library. Consistency of results is achieved by majority vote, which pre-vents tainted state. Our evaluation shows that active-code replacement can be done in less than a second in anidealized setting whereas a standard deployment takes many orders of magnitude more time. The main contri-bution of this paper is the description of a relatively straightforward approach to active-code replacement that isvery user-friendly. It enables a data analyst to quickly execute custom code on the cloud server as well as on clientdevices. Sensible safeguards and design decisions ensure that this feature can be used by non-specialists who arenot familiar with the implementation of OODIDA in general or this feature in particular. As a consequence ofadding the active-code replacement feature, OODIDA is now very well-suited for rapid prototyping.
1. IntroductionOODIDA [1] is a modular system for concurrent distributed dataanalytics, with a particular focus on the automotive domain. It processesin-vehicle data at its source instead of transferring all data over thenetwork and processing it on a central server. A data analyst interactingwith this system uses a Python library that assists in creating and vali-dating assignment speciﬁcations which consist of two parts: on-boardtasks carried out by the on-board unit (OBU) in a reference vehicle,and an off-board task that is executed on a central cloud server. Severaldomain-speciﬁc algorithms and methods of descriptive statistics havebeen implemented in OODIDA. However, updating this system istime-consuming and disruptive as it necessitates terminating and rede-ploying software. Instead, we would like to perform an update withoutterminating ongoing tasks. We have therefore extended our system withthe ability to execute custom code, without having to redeploy any partof the installation. This enables users to de ﬁne and execute custom computations both on client devices and the server. This is an example ofa dynamic code update. With this feature, users of our system are able tocarry out their work, which largely consists of either tweaking existingmethods for data analytics or developing new ones, with much fasterturnaround times, allowing them to reap the bene ﬁts of rapid prototyping.In this paper, we describe the active-code replacement feature ofOODIDA. We start off with relevant background information in Sect. 2, which includes a brief overview of our system. In Sect. 3we cover the implementation of this feature, showing how Erlang/OTP and Pythoninteract. We elaborate on the reasoning behind our design consider-ations, including deliberate limitations, and show how it enables rapidprototyping. Afterwards, we show a quantitative as well as a qualitative
* Corresponding author. Fraunhofer-Chalmers Research Centre for Industrial Mathematics, Chalmers Science Park, 412 88, Gothenburg, Sweden.E-mail address:gregor.ulm@fcc.chalmers.se(G. Ulm).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100043Received 28 February 2020; Received in revised form 14 July 2020; Accepted 1 September 2020Available online 24 September 20202590-0056/©2020 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 8 (2020) 100043evaluation of active-code reloading in Sect. 4, before we continue with related work in Sect.5, plans for future work in Sect.6, and end with the conclusion in Sect.7.A condensed version of this paper has been previously published [ 2]. That paper presents a quick summary of the active-code replacementfeature of the OODIDA platform. In contrast, this paper provides bothmore depth, covering various implementation details and extensivetechnical background, as well as increased breadth by giving a morethorough description of relevant parts of our system.2. BackgroundIn this section, we describe the relevant background of active-codereplacement in the OODIDA platform. We start with a brief overviewof OODIDA (Sect.2.1), including a description of assignment speci ﬁca- tions and the user front-end application, before we show how our systemcan be extended with new computational methods (Sect. 2.2). This leads to the motivating use case that describes on-the- ﬂy updating of the sys- tem without taking any part of it down (Sect. 2.3).2.1. OODIDA overviewThis subsection contains a condensed description of OODIDA, whichis comprehensively described elsewhere [ 1]. After a brief overview and an example, we highlight some technical details as well as the status quo anteof our system for prototyping before the addition of the active-codereloading feature.2.1.1. Basic ideaOODIDA is a platform for distributed real-time data analytics in theautomotive domain, targeting aﬂeet of reference vehicles. It connects m analysts tonvehicles. The architecture diagram is shown in Fig. 1. An- alysts use OODIDA for data analytics tasks by creating assignments,which are translated into tasks for connected vehicles. These vehiclescontain an on-board unit (OBU) that isﬁt for general-purpose computing. Yet, OBUs are used merely for data analytics. They do not interfere withcontrolling any part of the vehicle and instead only read CAN bus data.Data analysts use OODIDA for executing various statistical methods andmachine learning algorithms. After updating OODIDA in-house, thesystem can be deployed remotely, which makes new features available toall analysts. A particular focus of this system is on large-scale concur-rency: analysts can issue a multitude of tasks to different subsets of clientsthat are all carried out concurrently. The bottleneck is the availablehardware in the vehicles, but experimental results show that we caneasily carry out dozens of typical analytics tasks concurrently [ 1]. The problem our system solves is that data generation of a connectedvehicle outpaces increases in bandwidth. There is simply too much datato transfer to a central server for processing. Instead, with OODIDA, datais primarily processed on clients and in real time, which leads to costsavings as transmission and storage costs can be greatly reduced.Furthermore, data analysts can get insights a lot faster, which is valuablefor business.2.1.2. Technical detailsOODIDA is a distributed system that runs on three kinds of hardware:data analysts use workstations, the server application runs on an internalprivate cloud, and client applications are executed on OBUs. Our systemcan accommodate multiple users, but in order to simplify the presenta-tion, we mainly focus on a single-user instance. In Fig. 2, the context of our system is shown, indicating that a data analyst uses a front-end f.I n turn,fis connected to a user moduleuthat communicates with the central cloud applicationb(bridge). The workstation of the data analystexecutes bothf
andu. Nodebcommunicates with client nodes con OBUs. Eachcinteracts with an external application a. Data analysts use a front- end applicationfto generate assignment speciﬁcations, which are consumed byuand forwarded tob.O nb, assignments are divided intotasks and forwarded to the chosen subset of clients. On client devices,external applicationsaperform analytics tasks, the results of which aresent tob, where optional off-board tasks are performed. Assignments canbe executed concurrently.Building on this more general view, Figs. 2 and 3present further details of the underlying message-passing infrastructure, which has beenimplemented in Erlang/OTP. We start with the user node u, which is identical touinFig. 2. The user deﬁnes an assignment speciﬁcation with
Fig. 1.OODIDA has a modular architecture. The backend, server, and clientapps as well as the assignment and task handlers have been implemented inErlang/OTP. The user frontend, computation manager and sensor manager havebeen implemented in Python. Modules or functions that carry out tasks can beimplemented in an arbitrary programming language. The sensor manager(bottom) interfaces with the vehicle ’s CAN bus. Active-code replacement makes use of the custom-code storage depicted on the left.
Fig. 2.OODIDA overview: user nodes uconnect to a central cloudb, which connects to clientsc(vectors in boldface). The shaded nodes are implemented inErlang/OTP; the other nodes are external Python applications, i.e. the user front-endsfand external client applications a.G. Ulm et al. Array 8 (2020) 100043
2the help off, which forwards it tou. In turn,uforwards it via the network tob. That node spawns a temporary assignment handler b
0, which divides the assignment into tasks and distributes them to client devices. Ourillustration shows three client nodes x;yandz. Both the client node and its task handler are executed on an OBU, just like the external applicationashown inFig. 3. Each client spawns a temporary task handler perreceived task. For instance, client node xspawns task handlerx
0. Task handlers communicate the task speciﬁcation to the external application a, which performs the requested computational work. Once the resultsare available, they are picked up by the task handler and forwarded to theoriginating assignment handlerb
0. Afterwards, the task handler on theclient terminates. Onceb
0has received the results from all involved taskhandlers, it performs optional off-board computations, sends the resultstob, and terminates. Finally,bsends the assignment results to the userprocessu, which communicates them tof. In which order the various nodes are involved when processing an assignment, including externalapplications on the client and cloud, is shown in Fig. 4. The speciﬁed on-board and off-board computations can be carried out in an arbitraryprogramming language as our system uses a language-independent JSONinterface. However, we focus on a simpli ﬁed version of OODIDA that only uses Python applications to execute both on-board and off-boardtasks.2.1.3. Example assignmentAssignments consist of an on-board task, performed on a centralserver, and an off-board task, performed by each client that is containedin the selected subset of clients. An example of an assignment speci ﬁ- cation is provided inListing 1.1, which shows an example of a relativelybasic assignment and its deﬁnition as an object in Python. The on-boardtask is executed on the chosen subset of clients, and the off-board task onthe central cloud server. The provided example shows an instance ofanomaly detection. The entireﬂeet of vehicles is monitored, with thegoal of detecting whenever a vehicle exceeds a speed threshold value of100. In order to do so, the user speciﬁes the keyword collect, which collects values from the provided list of signals at a certain frequency fora total ofntimes. In the given example, each client collects 36,000samples at a frequency of 10 Hz. In total, this means that we monitor eachvehicle for a total of 60 min. In general, the off-board part of an assign-ment is relatively inexpensive. While it is possible to perform arbitrarycomputations on the server as well, it most commonly collects resultsfrom clients and forwards them to the user the assignment originatedfrom. This happens in our example as well. However, on top, the keyworditerationswith the value‘10’is used which indicates that the cloudserver will issue the on-board assignment sequentially ten times. Thus,the anomaly detection task will run for ten consecutive hours, with asummary report being sent to the user after each hour.Lastly, both the on-board and off-board objects are combined into aSpec object for the assignment speciﬁcation. The user is expected to namesuch an assignment as well as select a subset of clients, which can be doneas random selection ofcclients, a numerical selection based on client IDs,selection based on the vehicle model, or, like in our case, as an assign-ment that is sent out to all clients. In practice the keyword allis only
Fig. 4.Simpliﬁed sequence diagram of OODIDA. The system handles multiple users which can all send multiple assignments that can address arbitrary subsets ofclients, whereas this diagram only shows one user and one client. On top, clients execute tasks concurrently. The system can also perform iterative wo rk on the task and assignment level. In contrast, this diagram shows an assignment consisting of a task that contains one iteration. The worker node on the cloud is de picted as being part of the assignment handler, which captures the most common off-board use case, i.e. aggregating results. Refer to Fig. 2for a depiction of concurrent workﬂows.Fig. 3.This illustration shows the core of OODIDA with permanent nodes(dark) and temporary handlers (light). Clients x,yandzspawn task handlersx
0, y
0, andz0that interact with external applications.G. Ulm et al. Array 8 (2020) 100043
3relevant for some assignments because not all signals are available in allvehicles.Listing 1.1. Example of an assignment speciﬁcation.
2.1.4. PrototypingIn general, an assignment is a tuple of a chosen algorithm and itsparameter values, a subset of signals, and the duration, which is deter-mined by the number of samples and a frequency. This implies that thenumber of potential assignments is very large but has an upper bound asthe number of combinations isﬁnite. Given a reasonably large selectionof algorithms to choose from, OODIDA is quite powerful. Yet, a dataanalyst using this system may want to also deploy novel algorithms. Thepreviously mentioned Listing is a good example: a data analyst inter-acting with OODIDA may want to start an anomaly detection work ﬂow byﬁltering out all vehicles that reach a speed of 100 km/h. Yet, this maynot be fully sufﬁcient to detect dangerous driving. Consequently, addi-tional criteria may need to be met. For instance, one hypothesis couldentail that dangerous driving means driving at a speed of at least 100 km/h for 25% of the time, but a competing hypothesis could be that driving isonly dangerous if it is accompanied by sudden steering angle deviations,or by any steering angle deviation past some threshold. This hints at twoneeds:ﬁrst, concurrently testing competing hypotheses and, second,executing computations that may not be de ﬁnable with the standard methods that are available on our platform.2.1.5. User front-end applicationOODIDA provides a Python front-end application ffor the data ana- lyst for easy creation and validation of assignment speci ﬁcations. Assignment speciﬁcations, an example of which we just detailed, areultimately turned into JSON objects. Because the manual creation of anassignment as a Python dictionary is error-prone, fautomatically veriﬁes the correctness of the provided values. Checks include completeness andcorrectness of the provided dictionary keys, type-checking the corre-sponding dictionary values, and verifying that their range is valid. Forinstance, the value for theﬁeld frequency has to be a positive integer andcannot exceed a certain threshold. Separately verifying assignmentspeciﬁcations is necessitated by the rudimentary type system of Python.In programming languages with a more expressive type system, e.g.Hindley-Milner type inference or Martin-L €of dependent types, some of those checks could be performed by the compiler. If the validation issuccessful, the conﬁguration dictionary is converted into a JSON objectand sent to the user processu, which is also executed on the workstationof the data analyst.2.2. Extending OODIDAAs OODIDA has been designed for rapid prototyping, there is thefrequent need of extending it with new computational methods, both foron-board and off-board processing. In Fig. 4a simpliﬁed representation of the workﬂow of OODIDA is given. In short, to extend the system, theworker nodes have to be updated. These are the applications, mostlyimplemented in Python, that perform on-board and off-board computa-tions. They interact with an arbitrary number of assignment handlers(off-board) and task handlers (on-board). In order to update OODIDAwith new computational methods, the system has to be modi ﬁed. For the user, the only visible change is a new keyword and some associated pa-rameters, if needed. Assuming that we update both the on-board and off-board application, the following steps are required:/C15Update user front-endfto recognize the new off-board and on-boardkeywords./C15Add checks of necessary assignment parameter values to. f /C15Add new keyword and associated methods to cloud applicationworker./C15Add new keyword and associated methods to client applicationworkers./C15Terminate all currently ongoing assignments./C15Shut down OODIDA on the cloud and all clients./C15Redeploy OODIDA./C15Restart OODIDA.Unfortunately, this is a potentially disruptive procedure, not eventaking into account potentially long-winded software development pro-cesses in large organizations. OODIDA has been designed with rapidprototyping in mind, but although it can be very quickly deployed andrestarted, the original version cannot be extended while it is up andrunning. This was possible withfﬂ-erl[3], a precursor that was fully implemented in Erlang, which allows so-called hot-code reloading. Thereare some workarounds to keep OODIDA up-to-date, for instance byautomatically redeploying it once a day. As we are targeting a compar-atively smallﬂeet of reference vehicles, this is a manageable inconve-nience. Yet, users interacting with our system would reap the bene ﬁts of a much faster turnaround time if they were able to add computationalmethods without restarting any of the nodes at all.2.3. Motivating use casesWhile the previous implementation of OODDIA works very well forissuing standard assignments, there are some limitations. The biggest oneis that adding additional algorithms requires updating the worker nodeon clients or the cloud (cf.Fig. 4). This causes all currently ongoing tasksto be terminated and therefore disincentivizes experimentation. Sometasks may have a runtime of hours, after all. Furthermore, there is theproblem that it may not be desirable to permanently add an experimentalalgorithm to the library on the client. This entails that experimentalexecution requires two updates,ﬁrst to push the code update, and af-terwards to restore the state before the update.Theﬁrst exemplary use case we consider consists of temporarilyadding an algorithm to the external client application. If that algorithmproves to be useful, it can be added to all clients via an update of theclient software. Otherwise, no particular steps have to be taken as thecustom piece of code on the client can be easily deleted or replaced bynew custom code. A second, and related use case, consists of temporarilyadding different algorithms to non-overlapping subsets of client devices,e.g. running two variations of an algorithm, with the goal of evaluatingthem. Thus, actionable insights can be generated at a much faster pacethan the procedure outlined in Sect. 2.2would allow. Lastly, there is the issue of extensibility. Python is a mainstreamprogramming language with a rich ecosystem. There are very compre-hensive external libraries available, which are useful for OODIDA, suchas the machine learning libraries Keras [ 18] and scikit-learn [4]. As those are vast projects, it is infeasible to create hooks for an entire library. Yet,it is occasionally useful to call a function of those libraries, in which casea data analyst can deﬁne a custom-code module that loads the externalG. Ulm et al. Array 8 (2020) 100043
4library and calls that function. Consequently, active-code replacementprovides an easy way of quickly accessing the functionality of third-partylibraries.It is also important to keep in mind standard data analytics work-ﬂows: Very commonly, analysts write glue code that uses existing li-braries. For instance, before running a method with user-selectedparameters, data may need to be preprocessed (e.g. consider thesklearn.preprocessingpackage). This kind of task is commonlyexpressed in short scripts. With the feature described in this paper, it ispossible to deploy such code to a client. It is very helpful to be able toexecute custom preprocessing routines on client devices as this enablesnew use cases, based on the assumption that it is not feasible to collectdata from a large number of client devices in realtime, due to its volume.Another very important task for data analytics work ﬂows is algorithmic exploration that goes beyond merely tuning parameters of algorithms.Instead, this may mean modifying the source code of an existing algo-rithm or executing algorithms that were written by the analysts them-selves. In either case, the code that needs to be deployed tends to berelatively short. As these examples show, it is obvious that the bene ﬁto f being able to deploy and execute custom code as opposed to fully rede-ploying the client software installation leads to a much faster turnaroundtime. It also enables an entirely different way of working as the ability toquickly deploy custom code heavily encourages experimentation.3. SolutionIn this section we describe our engineering solution to the problem ofreplacing active code in our system. We start with the assignment spec-iﬁcation the data analyst produces (Sect. 3.1). Afterwards, we focus on the underlying mechanisms for getting a custom piece of code from thedata analyst to the cloud as well as client devices (Sect. 3.2). This is followed by discussing implementation details that make it possible tokeep devices running while replacing a piece of code (Sect. 3.3), followed by our approach to ensuring consistency of results, based on the fact thatnot all clients may be updated at the exact same time (Sect. 3.4). Then we highlight security considerations (Sect. 3.5). Afterwards we show how a complex use case can be implemented with active-code replacement(Sect.3.6) and discuss deliberate limitations of our solution (Sect. 3.7).3.1. Using custom code in an assignmentIn line with the guiding principle that OODIDA should make it as easyas possible for the data analyst to do their job, active-code replacementhas been designed to minimize the need for interventions. The data an-alyst only has to carry out two steps. Theﬁrst is providing a stand-alone Python module with the custom code. It could include imports, which, ofcourse, the user has to ensure to be available on the target OBUs. The onlyrequirement for the structure of the custom code module is that it con-tains a functioncustom_codeas an entry point, which takes exactly oneargument. This is the function that is called on the cloud or client.Additional parameters have to be hard-coded. Before being able to callcustom code, it needs to be deployed. To do so, the user needs to specifythe location of the codeﬁle on their machine and afterwards call thefunctiondeploy_code, which takes as an argument the target, on- boardoroffboard, the location of theﬁle and optionally a speciﬁ- cation of the intended clients, which is ultimately a list of client IDs. InListing 1.2we call a helper function to retrieve the IDs of all vehicles of aparticular type. It is possible to send different modules to non-overlapping subsets of clients via subsequent assignments.The veriﬁcation process of the user front-end application consists oftwo steps. First, the provided module has to be syntactically correct,which is done by loading it in Python. The second check targets theprescribed functioncustom_code
. That function is called with the ex-pected input format, depending on whether it is called on the client or thecloud. We also verify that the returned values are of the expected type. Ifany of these assertions fail, the assignment is discarded. Otherwise, thecustom Python module is sent to the cloud or to clients, depending on theprovided instructions. This step is preceded by producing anotherassignment speciﬁcation that, in either case, contains the entriesuser_idandcustom_code. The value of the latter is an encoding ofthe user-provided Python module. The value of the key modeis either deploy_offboardordeploy_onboard. Once custom code has been deployed, it can be referred to in assignments by setting the value of thekeysonboardoroffboardtocustom. Listing 1.2. Deploying custom on-board code.
3.2. Code forwardingAssuming the provided custom code for the client has passed theveriﬁcation stage, it is turned into a JSON object and ingested by the usermodule for further processing. Within that JSON object, the user-de ﬁned code is represented as an encoded text string. The user module extractsall relevant values from the provided JSON object and forwards it to thecloud processb. In turn,bspawns a new assignment handler b
0for this particular assignment. The next step depends on whether custom code forthe server or client devices has been provided.The process of turning an assignment into tasks for client devices doesnot depend on the provided values and is thus unchanged from thedescription in the paper on OODIDA [ 1] or the brief summary presented earlier in this paper. Nodeb
0breaks the assignment speciﬁcation down into tasks for all clients speciﬁed in the assignment. After this is done,task speciﬁcations are sent to the designated client processes. Each clientprocess spawns a task handler for the current task. Its purpose is tomonitor task completion, besides alleviating the edge process from thatburden and enabling it to process further task speci ﬁcations concur- rently. In our case, the task handler sends the task speci ﬁcation in JSON to an external Python application, which turns the given code into a ﬁle, thus recreating the Python module the data analyst initially provided.The name of the resultingﬁle also contains the ID of the user who pro-vided it. After the task handler is done, it noti ﬁes the assignment handler and terminates. Similarly, once the assignment handler has received re-sponses from all task handlers, it sends a status message to the cloud nodeand terminates. The cloud node sends a status message to inform the userthat the custom code has been successfully deployed. Deploying customcode to the cloud is similar, the main difference being that b
0commu- nicates with the external Python worker application running on thecloud.3.3. Code replacementComputations are performed only after the speci ﬁed amount of data has been gathered. This implies that a custom code module can be safelyreplaced as long as data collection is ongoing. The case where an updatecollides with a function call to custom code is discussed in Sect. 3.4.I fa custom on-board or off-board computation is triggered by the keywordcustom, Python loads the user-provided module using the functionG. Ulm et al. Array 8 (2020) 100043
5reloadfrom the standard library. This happens in a separate processusing themultiprocessinglibrary. The motivation behind this choiceis to enable concurrency in the client application as well as to avoid sometechnical issues with reloading in Python, which would retain de ﬁnitions from a previously used custom module. Instead, our approach creates ablank slate for each reload.The user-speciﬁed module is located at a predeﬁned path, which is known to the reload function. Once loaded, the custom function isapplied to the available data in theﬁnal aggregation step, which is per- formed once and at the end of a task or assignment. When an assignmentusing a module with custom code is active, the external applicationsreload the custom module with each iteration. This may be unexpected,but it leads to greaterﬂexibility. Consider an assignment that runs for anindeﬁnite number of iterations. As the external applications can processtasks concurrently, and code replacement is just another task, the dataanalyst can, for instance, react to intermediate results by deployingcustom code, with modiﬁed algorithmic parameters, that is used in anongoing assignment as soon as it becomes available. As custom code istied to a unique user ID, there is furthermore no interference due tocustom code deployed by other users as every unique user ID is tied to aunique user account, and each user of the system has their own account.One theoretical issue with our approach is that modules may bereloaded repeatedly, which is inefﬁcient. Yet, OODIDA was not designedwith the idea of running arbitrary libraries on the client; instead, it has astrict focus on distributed data analytics. This entails that external li-braries do not pose a problem as bread-and-butter libraries such asscikit-learnand Keras are loaded already when the client applica-tion is started. Thus, these modules are available to a custom module andshould not be imported again by custom code; such imports are reportedby the user-side validator. On top, the user does not have the ability todeploy additional libraries by themselves. Instead, they can only accessthe Python standard library, with some limitations, and a small set ofthird-party libraries. Consequently, code that is imported with eachiteration tends to be small and does not depend on additional externallibraries.3.4. Ensuring consistencyInconsistent updates are a problem in practice, i.e. results sent fromclients may have been produced with different custom code modules inthe same iteration of an assignment. This happens if not all clients receivethe updated custom code before the end of the current iteration. In astreaming context, where clients have the ability to peek into results toget intermediate updates, the same issue could emerge, namely thatclients use different versions of custom code for their computations. Tosolve this problem, each provided module with custom code is taggedwith its md5 hash signature. This signature is reported together with theresults from the clients. The cloud only uses the results tagged with thesignature that achieves a majority and discards all others. Consequently,results are never tainted by using different versions of custom code in thesame iteration. The expectation is that any updated custom code wouldeventually, and quickly, reach a majority. An update may not succeed forvarious reasons. If it is because a client has become unavailable, then saidclient cannot send any results anyway. Should an update not succeed,then the client reports an error. In that case, the update has to berepeated.It is possible that a new custom code version arrives at the same timethe client application wants to load it. This is one example where thestandard approach would be to roll back the update and instead use theprevious custom code version. However, this scenario is less of a concernfor us as we replace computational methods instead of system-levelsoftware. There is deliberately no mechanism for a rollback as the oldversion of the custom code was supposed to be replaced by new customcode, which implies that any results that could be generated by the oldcode instead of the not-yet-available new one are not of any interest tothe user anymore. The only exception where a rollback would be helpfulis in the pathological case where the old and new version of the providedcustom code are identical, and the update with the new custom codefailed. Consequently, we deliberately let this one computation fail andthe client report an error. For the next iteration, the new version of thecustom code can be expected to be available, which means that this issueresolves itself quickly in practice.3.5. Security measuresThe design of our system addresses both internal and external threats,both accidental and deliberate ones. We consequently limit the expres-sibility of the code the user can deploy. In addition, OODIDA is designedto run on a corporate VPN.First, in order to limit the damage the user can do, we enforce that theprovided custom function takes a list of numerical values as input andreturns either a list of numerical values or a numerical value. Verifyingthis property is carried out by the user-front end application before dis-patching a piece of custom code for further processing. There are cor-responding assertions on the on-board and off-board nodes as it istheoretically possible to manually specify custom code that sidesteps thechecks in the front-end application. However, this would require
knowledge of the implementation of those checks, some of which userandomly generated inputs and others static ones. Thus, we consider ithighly unlikely that an antagonistic developer would be able to workaround the checks in the user front-end. The user would have to reliablypredict the input of the test cases and cover it with branching logic, whichis triggered in that particular case, but not otherwise. Guessing ran-domized inputs is arguably not feasible. However, even with perfectknowledge of the implementation and the seed used for generatingrandom data, an antagonistic user would be thwarted. The reason is thatwe programmatically ensure proper behavior of the custom function thatis executed on the client via atry-exceptconstruct for error handling. This step also includes a veriﬁcation that the returned values are indeedas speciﬁed. This would seem like duplicate work as this check is alreadyperformed in the front-end prior to deployment, but it addresses the caseof an omniscient antagonistic user. Thus, this approach closes the pre-viously mentioned loophole an antagonistic developer theoretically hason the client side. A caveat is that this relies on the provided functionterminating. Yet, there is the issue of the halting problem, i.e. anantagonistic developer could write a function that never returns, thuswasting computational resources of the client. Some legitimate compu-tations could take a signiﬁcant amount of time, so it is not possible todistinguish between legitimate and antagonistic code on that metricalone. However, with a generous timeout clause, which is enabled viaPython’smultiprocessinglibrary, a custom function that exceeds aset threshold, regardless of whether the code is antagonistic or not, caneasily be terminated. Therefore, we tackle the problem of antagonisticinput sufﬁciently well from a practical perspective. Also, it has to be keptin mind that our system supports commercial users who can be assumedto be invested in the system being fully operational.Second, there is the issue of cloud security. As has been pointed out,commercial cloud solutions are vulnerable [ 5,6] and therefore require adequate responses [7]. Among others, there are vulnerabilities due tomultitenancy, virtualization, and resource-sharing. As we were aware ofthose issues, we chose to sidestep them by executing OODIDA on acorporate VPN instead. This does not mean that network security is notan issue. However, this approach avoids additional threats that areunique to cloud computing. On a related note, the primary motivationbehind this choice was the need to protect our data. In that regard, notrelying on a third-party commercial cloud computing provider seemedlike an obvious decision.3.6. Complex use casesThe description of active-code replacement so far indicates that theuser can execute arbitrary code on the server and clients, as long as theG. Ulm et al. Array 8 (2020) 100043
6correct inputs and outputs are consumed and produced. What may not beimmediately obvious, however, is that we can now even create ad hoc implementations of the most complex OODIDA use cases, an example ofwhich is federated learning [8]. A key aspect of federated learning,compared to many standard types of assignments on our system, is thatthe results of one iteration are used as the input of the next one. Theoriginal implementation is discussed in the paper on OODIDA [ 1]. With federated learning, clients update machine learning models, which theserver uses as inputs in order to create a new global model. This globalmodel is the starting point for the next iteration of training on clients.Listing 1.3. Example of an assignment speciﬁcation.
Assignments using in-built functions have parameters attached tothem that the user can tweak. For custom code, we added a parameter forcontrolling the workﬂow calledresult_ﬂow, which can take two values:isolated, which is the default, andconnected. With the latter, the results of one iteration are used as input for the next iteration.Listing 1.3shows the exemplary use of it, where code running on theclient retrieves a model. Custom code containing an implementation ofan artiﬁcial neural network was deployed to clients. In addition, customcode for averaging the received updated local models was deployed tothe server. Clients produce these local models at the end of each iterationand send them to the server. The available parameters for custom codeare user-deﬁnable, except that the keywordresult_ﬂowis reserved. For custom code that uses aconnectedworkﬂow, the initial value for the model is set tonull. Theparametersargument can be used in a ﬂexible manner. For instance, in an assignment built on using federatedlearning, an initial global model might be desirable, which could beincluded in this argument.3.7. LimitationsWe consider custom modules a temporary solution. Any function thatis deemed generally useful should be added permanently to worker nodesin an update. We also want to discourage data analysts from heavilyrelying on custom functions. Currently, they can each de ﬁne one custom- code module for the cloud, and one for each client. It would bestraightforward to add the ability to handle multiple custom-code mod-ules per user. Yet, we do not want multiple custom functions to be part oftheir regular workﬂow as the consequences of this would be undesirable.In the worst case, this would mean that each user of the system maintainstheir own custom functions instead of building up a library with relevantalgorithmic methods for all users. There is also a practical aspect of whywe want to discourage users from continually using custom functionsinstead of permanently adding them to the client installation in the formof a library. The client application is designed to be stateless: it processessensor data, discards these data after processing, and sends results to theserver. With this approach, it is straightforward to execute experimentsand also replicate them. In contrast, replication with custom code wouldrequire to separately keep track of the source code ﬁles that were deployed, and in order to do so effectively and reliably, one would needto build an additional component for OODIDA that automates this, inorder to reduce human error.As previously stated, users only have the ability to deploy one sourcecodeﬁle. For security reasons, we chose to not implement the ability todeploy additional librariesad hoc. The standard data science libraries weuse are very well established, which is why they are installed on eachclient. For standard tasks, which consist of the application of existingmethods, this is not a limitation. However, users may need to developnew algorithmic methods. While it is not possible to upload a modi ﬁed scikit-learnlibrary module, there is a very effective workaround forthis situation: It is possible to include library source code in the custom-code module the user wants to deploy. Of course, some modi ﬁcations may be needed to make the user-deﬁned functions callable within the very sameﬁle. In any case, this is an adequate workaround when the goalis to, for instance, test a new algorithm in practice. Granted, normallysuch an algorithm would be part of an external library, but as long as it isbeing worked on, it is simply included in the deployed ﬁle. Lastly, we would like to add that while users cannot add their own libraries to theclient, administrators can. Clients can even be remotely updated, so thelimitations users face is a deliberate design decision instead of an unin-tended limitation of our system.4. EvaluationIn this section we present a qualitative and quantitative assessment ofactive-code replacement. The goal is to both quantify the bene ﬁts of our approach as well as argue for its soundness. We start with the design ofthe experiment (Sect.4.1), which is followed by a description of the datawe used (Sect.4.2) and a note on the environment that was used for theevaluation (Sect.4.3). Afterwards, we show our results and discuss them(Sect.4.4).4.1. Experimental designThe main beneﬁt of active-code replacement is that code for newcomputational methods can be deployed right away and executed almostinstantly, without affecting other ongoing tasks. In contrast, a standardupdate of the cloud or client installation necessitates redeploying andrestarting the respective components of the system. In order to quanti-tatively evaluate the performance difference, we executed OODDIA in anidealized scenario where the user and server were executed on oneworkstation each and one client device on another. We deployed iden-tical code to both the client and the server and took the average of ﬁve runs. The standard deployment procedure assumes that a minimal Linuxinstallation with all necessary third-party applications and libraries isavailable. A complete re-installation would take considerably more time.The total amount of data that needs to be transferred in order to deployOODIDA is around 1 MB. This is zipped data.4.2. Data descriptionCustom code that is to be deployed remotely is turned into a payloadfor an assignment, which is a lightweight JSON format. We took astandard real-world example consisting of 20 lines of Python source code.The source codeﬁle has a size of 0.45 KB. When turned into the payloadof a JSON assignment speciﬁcation, the size of theﬁle to be sent via the network is around 0.52 KB. The amount of custom code used may seemlike a rather small amount of code, but this is beyond typical assignmentsthat consist largely of glue code, tying together various calls to scikit- learnmethods. These are often in the order of 10 lines of code or evenbelow that.4.3. Hardware and software setupWe used three quad-core PC workstations. Workstation 1 contains anIntel Core i7-6700k CPU (4.0 GHz), workstation 2 and 3 an Intel Core i7-G. Ulm et al. Array 8 (2020) 100043
77700k CPU (4.2 GHz). These workstations are equipped with 32 GB RAMeach, of which 23.5 GB were made available to the hosted Linux oper-ating system. They run on Windows 10 Pro (build 1803) and executeOODIDA on Ubuntu Linux 16.04 LTS in VirtualBox 6.0. Workstation 1executed the one instance of the user front-end, workstation 2 the cloudapplication of our system, and workstation 3 one instance of a clientapplication. These workstations were connected via Ethernet.4.4. Results and discussionIn our idealized test setup, where the various workstations that runthe user, cloud and client components of OODIDA are connected viaEthernet, it takes a fraction of a second for a custom on-board or off-board method to be available for the user to call when deployed withactive-code replacement, as shown in Table 1. On the other hand, auto- mated redeployment of the cloud and client installation takes roughly 20and 40 s, respectively. The runtime difference between a standard updateand active-code replacement amounts to three orders of magnitude. Theresults of the standard deployment do not take into account organiza-tional processes that may artiﬁcially lengthen deployment times. Thus, astandard deployment does, in the real world, not just take around 20 and40 s, respectively, but those times plus an additional Δ, which is incomparably larger.Our comparison also highlights that active-code replacement is lessbureaucratic and less intrusive as it does not require interrupting anycurrently ongoing assignments. Indeed, in a realistic industry scenario,an update could take days or even weeks due to software developmentand organizational processes. The big bene ﬁt of active-code replacement is that the user, a data analyst, can issue updates themselves. In contrast,a standard deployment depends on software engineers committing code.Even in a continuous deployment environment in a fast-moving agilesoftware development process, the organizational overhead betweendeﬁning the speciﬁcation of an additional method that is to be added tothe cloud or client installation of OODIDA and the ﬁnal merging can take days, taking into account code-review and best practices like mergingnew commits only after the approval of other team members. Conse-quently, the differenceΔbetween the milliseconds it takes to carry outactive-code replacement and a standard deployment adds many orders ofmagnitude more to the measured time difference.A justiﬁcation for our idealized test setup is in order. Of course, real-world deployment via a wireless or 4G connection would be slower aswell as error-prone. Yet, our evaluation environment reveals the relativeperformance difference of both approaches, eliminating potentially un-reliable data transmission as a source of error. In fact, given that thestandard deployment procedure requires the transmission of a muchlarger amount of data via the network, it would be disadvantaged in anexperiment via wireless or 4G as the potential for packet losses increasesdue to the greater amount of data used. Thus, our experimental resultsunderestimate the true difference in deployment speed. That being said,the largest factor will be due toΔ, whereas the actual time for deploy-ment is negligible in contrast.Despite the beneﬁts of active-code replacement we just mentioned, itis not the case that this approach fully sidesteps the need to update thelibrary of computational methods on the cloud or on clients as OODIDAenforces restrictions on custom code. For instance, some parts of thePython standard library are off limits. Also, the user cannot installexternal libraries. Yet, for typical algorithmic explorations, which usersof our system regularly conduct, active-code replacement is a vitalfeature that increases user productivity far more than the previouscomparison may imply. That being said, due to the limitations of active-code replacement, it is complementary to the standard update procedurerather than a competitive approach.5. Related workThe engineering challenge described in this paper is an extension ofthe OODIDA platform [1]. It was originally an Erlang-only system basedonfﬂ-erl, a framework for federated learning [ 3]. Active-code replacement is a niche topic in applied computing,despite its use in fault-tolerant systems. That it is a niche topic is obviousfrom the fact that there is no uniform terminology. For instance, theofﬁcial Erlang documentation calls itcode replacement.
1Among working Erlang programmers, the termshot-code loadingorhot-code swappingare more common. In contrast, the standard textbooks by Cesarini et al. [ 9, 10] refer to it asupgrading processes. In academia, we encounter furthernames for this concept. In an extensive survey by Seifzadeh et al. [ 19] the chosen term isdynamic software updating. According to their terminology, our approach is calledupdating references of all dependants. Giuffrida and Tanenbaum [11,12] call itlive update, further classiﬁed aschanges to code. Of course, given that the terminology is not settled, we saw little reasonfor not creating a term we considered most suitable for describing theactions that take place.In terms of descriptions of systems that perform active-codereplacement, several approaches have been suggested in the literature.We focus on a selection of them and provide a qualitative comparisonwith active-code replacement in OODIDA. Concretely, we chose MUC byQiang et al. [13], Polus by Chen et al. [14], and Javelus by Gu et al. [15]. Abstractly viewed, these approaches share some similarities withactive-code replacement but they solve different problems. The mostsigniﬁcant difference is that our approach enables temporary updates of aspeciﬁc part of the application, which is due to our use case of distributeddata analytics, as described earlier. We did not ﬁnd a mechanism for distributed active-code replacement that is quite comparable to ours aswe focus on letting the user execute custom code in a restricted envi-ronment. In contrast, the various update mechanism we found in theliterature focus on updating parts of the application that is beingexecuted, often seemingly without any restrictions. It would be far toounsafe to do that in anad hocmanner, which underscores the differencebetween our approach and others.Starting with MUC, the major difference is that this system exclu-sively targets cloud-only updates, while active-code replacement alsoaddresses a potentially very large number of client devices. In MUC, eachupdate leads to forking a new process that is executed concurrently.These processes are synchronized to ensure consistency. In contrast, inOODIDA, assignments are executed concurrently. Yet, as there is nocontinual processing, our problem is simpli ﬁed. We thus replace code at any point but only execute it the next time it is called, which is after abatch of results has arrived. There is no need to concurrently execute anold version of the code as it would taint the state a computation if wecombined results that were arrived at with non-identical code.A signiﬁcant difference between active-code replacement in OODIDAand Polus is that the latter replaces larger units of code instead of isolatedmodules. As mentioned above, this is a much more wide-ranging
Table 1Runtime comparison of active-code replacement of a moderately long Pythonmodule versus regular redeployment in an idealized setting. The provided ﬁgures are the averages ofﬁve runs. As the numbers show, the former has a signi ﬁcant advantage. Yet, this does not factor in that a standard update is more invasive butcan also be more comprehensive. This is expressed by the additional quantity Δ, which captures delays due to industry best-practices but also bureaucracy inorganizations. A standard deployment can easily take days or weeks, dependingon the used software development practices.
Cloud ClientActive-code replacement 20.3 ms 45.4 msStandard redeployment 23.6 s þΔ40.8 sþΔ 1Refer to the section“Compilation and Code Loading ”in the ofﬁcial Erlang documentation:http://erlang.org/doc/reference_manual/code_loading.html(Accessed March 4, 2019).G. Ulm et al. Array 8 (2020) 100043
8procedure than updating custom code for data analytics. In addition,Polus operates in a multi-threading environment instead of the highlyconcurrent message-passing environment of OODIDA. This is a funda-mental difference, which means that the approach of Polus could not beeasily replicated in OODIDA. In essence, we spawn a separate processthat uses custom code. In order to replicate the approach of Polus, wewould need to update the existing process, which would make sand-boxing much more difﬁcult. In our case, a spawned process handler thatcrashes due to illegal custom code, which should not happen to beginwith, cannot take down the entire system. Instead, this process isterminated and the custom code discarded.The goal of Javelus is updating a stand-alone Java application asopposed to a distributed system. This is only indirectly related to our usecase. However, in principle, one could of course assume that this appli-cation is executed on the cloud and assume that mechanisms that enableremote updates are in place. We only mention Javelus because it uses anapproach that is quite similar to ours. Via a “lazy update mechanism”, code replacement only has an effect if a module is indeed used. Whilethere are arguably some issues with this approach in a stand-aloneapplication where such updates are intended to not only be temporary,this is elegant nonetheless. In active-code replacement, we do somethingvery similar. By separating custom-code deployment and custom-codeexecution, we only load a custom module if it is needed. In fact, thissolution was suggested by the fact that our system ﬁrst gathers data and afterwards processes it. Even streams are emulated as micro-batches. Theobservation that code that performs data analytics tasks is only invokedinﬁxed intervals subsequently led to the creation of the feature describedin this paper. The key modiﬁcation we had to make was to make itpossible to swap such a piece of code, either by pointing to different codeor by updating aﬁle that uses the same reference. We chose the latterapproach as it was more straightforward to implement, even thoughthere would be little difference in practice had we chosen the otherapproach.6. Future workIn addition to plans for future work on the OODIDA platform ingeneral, we also have concrete ambitions for active-code replacement.The feature, as described, works as intended. Yet, we did point out somelimitations and workarounds. The guiding idea is that the execution ofcustom code should be safe. One way of achieving this is by placing re-strictions on the provided code, such as only allowing certain returntypes. As of now, we only allow numerical values or lists of numericalvalues. This is sufﬁcient for a very large number of data analytics tasks.Yet, in order to make active-code replacement more useful, we shouldextend the set of allowed return types. Related is the problem that weexclude certain Python libraries from being called. A further investiga-tion is needed in order to determine if a more ﬁne-grained approach would make sense.A major limitation of the current iteration of active-code replacementin OODIDA is that the user is not allowed to deploy additional libraries.However, as this is a potentially very useful feature, we would like toexplore the possibility of creating sandboxed environments on the client,perhaps a solution based on lightweight containers via Docker [ 16]. This also refers back to an earlier note (cf. Sect. 3.7) on the current difﬁculties of reproducing deployments with custom code [ 17]. In fact, the idea of deploying a custom lightweight container to the client, which contains acustom sandboxed environment, seems promising. While this wouldarguably make custom deployments take more time, it would still beincomparably faster than the standard work ﬂow for updating the client installation, as that is a decision the user cannot take by themselves andmay take days or weeks, depending on organizational processes.7. ConclusionOODIDA was originally designed with the goal of enabling rapidprototyping, largely achieved by automated deployment of the clientinstallation. In reality, however, organizational bureaucracy and thedemands of industry best-practices slow down this process a lot. Incontrast, with the help of the active-code replacement feature, we pro-vide a safe and easy-to-use alternative that sidesteps such hurdles. Ofcourse, we had to make some concessions by placing limitations on user-deﬁned custom-code modules. Nonetheless, active-code replacement iseminently useful in practice as it enables data analysts working withOODIDA to pursue a highly interactive work ﬂ
ow, given how quickly custom code can be deployed on the system.Credit author statementGregor Ulm: Conceptualization, Methodology, Software, Validation,Writing - original draft, Visualization, Project administration SimonSmith: Software, Validation, Writing - review&editing Adrian Nilsson: Software, Validation, Writing - review&editing Emil Gustavsson: Writing - review&editing, Supervision, Project administration MatsJirstrand: Writing - review&editing, Supervision, Funding acquisition.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgementsThis research wasﬁnancially supported by the project Onboard/Offboard Distributed Data Analytics (OODIDA) in the funding programFFI: Strategic Vehicle Research and Innovation (DNR 2016 –04260), which is administered by VINNOVA, the Swedish Government Agencyfor Innovation Systems. It took place in the Fraunhofer Cluster ofExcellence“Cognitive Internet Technologies. ”Ramin Yahyapour (Uni- versity of G€ottingen) provided insightful comments during a posterpresentation.References
[1]Ulm Gregor, Gustavsson Emil, Jirstrand Mats. OODIDA: on-board/off-boarddistributed data analytics for connected vehicles. 2019. ”arXiv Preprint arXiv: 1902.00319.[2]Ulm Gregor, Gustavsson Emil, Jirstrand Mats. Active-code replacement in theoodida data analytics platform. In: European conference on parallel processing,vols. 715–19. Springer; 2019a. [3]Ulm Gregor, Gustavsson Emil, Jirstrand Mats. Functional federated learning inErlang (Fﬂ-Erl). In: Josep Silva, editor. Functional and constraint logicprogramming. Cham: Springer International Publishing; 2019b. p. 162 –78. [4]Pedregosa Fabian, Varoquaux Ga €el, Gramfort Alexandre, Michel Vincent, Bertrand Thirion, Grisel Olivier, Blondel Mathieu, et al. Scikit-learn: machinelearning in Python. J Mach Learn Res 2011;12(Oct):2825 –30. [5]Shaikh Farhan Bashir, Haider Sajjad. Security threats in cloud computing. In: 2011international conference for Internet technology and secured transactions, vols.214–19. IEEE; 2011.[6]Islam Tariqul, Manivannan D, Zeadally Sherali. A classi ﬁcation and characterization of security threats in cloud computing. Int. J. Next-Gener. Comput 2016;7(1):268–85.[7]Sabahi Farzad. Cloud computing security threats and responses. In: 2011 ieee 3rdinternational conference on communication software and networks. IEEE; 2011.p. 245–9.[8]McMahan H Brendan, Moore Eider, Ramage Daniel, Hampson Seth, others.Communication-efﬁcient learning of deep networks from decentralized data. arXivPreprint arXiv:1602.05629 2016 . [9]Cesarini Francesco, Thompson Simon. Erlang programming: a concurrent approachto software development. O ’Reilly Media, Inc; 2009. [10]Cesarini Francesco, Steve Vinoski. Designing for scalability with erlang/otp:implement robust, fault-tolerant systems. O ’Reilly Media, Inc; 2016. [11]Giuffrida Cristiano, Tanenbaum Andrew S. Prepare to die: a new paradigm for liveupdate. Amsterdam:”Department of Computer Science, Vrije Universiteit ; 2009. [12]Giuffrida Cristiano, Tanenbaum Andrew S. A taxonomy of live updates. In Proc. Of the 16th Asci Conf. 2010. [13] Qiang Weizhong, Chen Feng, Yang Laurence T, Jin Hai. MUC: updating cloudapplications dynamically via multi-version execution. Future Generat Comput Syst2017;74:254–64.https://doi.org/10.1016/j.future.2015.12.003 .G. Ulm et al. Array 8 (2020) 100043
9[14]Chen Haibo, Yu Jie, Chen Rong, Zang Binyu, Pen-Chung Yew. Polus: a powerful liveupdating system. In: In29th international Conference on software engineering (Icse ’07). IEEE; 2007. p. 271–81.[15]Gu Tianxiao, Cao Chun, Xu Chang, Ma Xiaoxing, Zhang Linghao, Lu Jian. Javelus: alow disruptive approach to dynamic software updates. In: 2012 19th asia-paci ﬁc software engineering conference, vol. 1. IEEE; 2012. p. 527 –36. [16]Merkel Dirk. Docker: lightweight Linux containers for consistent development anddeployment. Linux J 2014;2014(239):2 .[17]Boettiger Carl. An introduction to docker for reproducible research. ACM SIGOPS -Oper Syst Rev 2015;49(1):71 –9. [18]Chollet François, others. Keras: deep learning library for theano and tensor ﬂow. 2015. 8,”URL:Https://Keras. Io/K7. [19]Seifzadeh Habib, Hassan Abolhassani, Mohsen Sadighi Moshkenani. A survey ofdynamic software updating. J Softw: Evol. Process 2013;25(5):535 –68.G. Ulm et al. Array 8 (2020) 100043
10