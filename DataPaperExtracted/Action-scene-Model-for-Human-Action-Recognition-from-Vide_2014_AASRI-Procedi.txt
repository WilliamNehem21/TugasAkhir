 A A S R I  P r o c e d i a    6   (  2 0 1 4  )   1 1 1  –  1 1 7  
2212-6716 © 2014 The Authors. Published by Elsevier B. V. Open access under CC BY-NC-ND license.  
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.05.016 ScienceDirect
2013 2nd AASRI Conference on Computational Intelligence and Bioinformatics 
Action-Scene Model for Human Action Recognition from 
Videos*
Yifei Zhang+, Wen Qu, Daling Wang 
Northeastern University, Shenyang 110819, China 
Abstract 
Human action recognition from realistic videos attracts more attention in many practical applications such as on-line video 
surveillance and content-based video management. Single action recognition always fails to distinguish similar action 
categories due to the complex background settings in realistic videos. In this paper, a novel action-scene model is explored 
to learn contextual relationship between actions and scenes in realistic videos. With little prior knowledge on scene 
categories, a generative probabilistic framework is used for action inference from background directly based on visual 
words. Experimental results on a realistic video dataset validate the effectiveness of the action-scene model for action 
recognition from background settings. Extensive experiments were conducted on different feature extracted methods, and 
the results show the learned model has good robustness when the features are noisy.  
© 2013 Published by Elsevier B.V. 
Selection and/or peer review under responsibility of American Applied Science Research Institute 
Keywords: action recognition, contextual cue, video processing, action-scene model  
* Supported by the Key Program of National Natural Science Foundation of China under Grant No.61033007 and the Fundamental 
Research Funds for the Central Universities of China under Grant No.N100304004. 
+  Corresponding author. Tel.: +86-(0)24-83687776 
E-mail address: zhangyifei@mail.neu.edu.cn 
Available online at www.sciencedirect.com
© 2014 The Authors. Published by Elsevier B. V. Open access under CC BY-NC-ND license.  
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute112   Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
1. Introduction  As more and more video surveillance systems appear in every walk of life, automatic human action recognition from realistic videos has become a major concern for both academic researchers and commercial companies. In action recognition, one fundamental problem is to recognize whether an action is belong to a given action category. Although a great amount of impressive results have been achieved on datasets collected from controlled environments, such as KTH [1], WEIZMEN [2], much less progresses have been made on realistic videos due to complex background settings an d action alternations from realistic videos. However, the intricate background and varied actions equally gi ve us a clue to explore the contextual relationship between actions and scenes for action recognition on realistic videos.  As human actions always occur under particular scenes where a rich source of conte xtual cues will present, the contextual relationships of background settings could be learned as a complement for action recognition. Although recognition based on scene detectors have b ecome commonplace in the related literatures [3,4], a ma
 in problem of detector based methods is how to sel ect general scene and action categories for all videos. Otherwise, training detectors is time-consuming and performances of recognition will be affected by the learned detectors. In this paper, we intend to learn contextual cues using a generative framework with little prior knowledge on scene categories. The contextual relationship between actions and scenes could be used to infer actions from background settings. An action-scene model is proposed to automatically model the relationship between actions, scenes and background features, where the number of scenes only need to be given instead of the categories of scenes. Firstly, a video will be segmented into person regions and background regions. Then the relationship of a given action category and scenes will be modeled using an action-scene model. Finally a factor is computed based on the proposed m odel to measure the importance of learned context cue and the responding probability of action category is given from this model.  The rest of this paper is organized as follows. Section 2 reviews related work. Section 3 describes the process of feature extraction and video representation. Actio n-scene model is proposed in Section 4 and experimental results are shown in section 5. Finally we present concluding remarks and future work in Section 6.  2. Related Work Human action recognition has been an important area of research in the field of computer vision. From single scenario to complicated scenario, various approaches are presented for action recognition to meet different application demands.  Early works were focus on action recognition with single scene [5]. However, this scenario is too ideal to appear in realistic circumstances. As more attention paid to complex scenarios, some recent approaches [3,6,7,8] tried to deal with “wild” videos. Laptev et al automatically annotated movie videos using multi- channel non-linear SVMs [6]. Liu et al proposed a framework to rec ognize actions which combined both motion and static features [7]. These appear-based learning approaches work hard to select general scene types and object categories suitable for all videos. To  handle the complex realistic videos, contextual semantics are used in lots of works. Ikizler-Cinbis et al combined the features of object, scene and actions in a multiple instance learning framework [9]. However, they didn’t learn the relationship between objects, actions and scenes for action identification. Marszalek et al expl oited the context of movie scripts and developed a joint scene-action SVM-based classifier by training several scene detectors [4]. However, due to a lack of training data for all the potential objects and scenes, it is time-consuming to collect annotated data for each category. In addition, a semantic gap would be generated between the learned contextual cues from texts and 113  Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
visual features. In this paper, we propose a novel approach different from the detector-based methods in learning contextual cues that has no use for action and sce ne categories. Inspired by the recent work of action recognition in static images [10]
 and unsupervised learning method in [2], we use a generative model to represent action and scene in videos. The underlying  action-scene dependency is captured on the action-scene model directly from visual features.  3. Visual Feature Extraction and Video Representation  3.1. Body detection  The person body detection in videos is based on Felzenszwalb's object detector [11] and mean shift tracking [12]. We first use the object detector to find candidate person regions from each frame image. Since there ar
e many false alarms and miss-detections, a slidin g window based method is used to discard false alarms and track person area for missing detections. Two detections are viewed as similar if the bounding boxes overlap exceeds 40%. The window size is empirically set as 15 frames and the threshold is half of the windows size. Then, the miss detections are filled with tracking from previous frames using mean shift algorithm. The person detection provides the bond ing boxes of person location at each frame.  3.2. Features extraction xPerson-centric feature extraction. We us
e the spatio-temporal interest point detector proposed by Dollar [13] to capture the spatially and temporally interesting movements. After extracting the person-centric features from a video clip, we describe each interest po int with HOG (Histograms of oriented Gradients) [14] descriptor.  xBackground feature extraction. The
 non-person region is seen as background region. In order to capture the color, shape and local features of the background, we ex tract color histograms for color features, Gist descriptors [15] for shape features and SIFT descripto rs [16] from key frames for static features.  3.3. Bag-of-features video representation Bag-of-feature based video representation has approved to be effective for action recognition in unstrained videos [7]. The bag-of-feature model represents a video clip as a vector over a feature vocabulary or “visual codebook”. The “visual codebook” is constructed by clustering visual features detected from all videos. The center of each cluster is defined to be a codeword or visual word in the codebook. Thus, each detected feature in a video clip can be assigned to a unique cluster having the nearest distance. The ith bin of bag-of-feature is the number 
 of features that be assigned to the ith cluster in the video.  4.Learnin
g of Action-scene Relationship The detector based context learning needs to know the prior knowledge about scene categories which is usually dependent on the dataset. Therefore, the detector-based method is usually inflexible to the general datasets, as well as the unstable performance of detectors is prone to affect the accuracy. In this paper, we aim to identify the action class in a video from scene features with less prior knowledge. Compared with scene categories occur in the dataset, the approximate number of them is easier to get. According to the relation between videos, actions , scenes and visual words, we naturally deduce a 114   Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
generative probabilistic model to model the contextual cues between them. In the action-scene model, each video can be seen as a mixture of action categories, where each action is a probability distribution over scenes and each scene is associated with a distribution over visual words. Each video clip is looked as a distribution of actions, and each action is a distribution of visual word. The action-scene model discovers not only which action happe
ns in a video, but also which scene is associated with the action.  Suppose we have a collection of V video clips, each video v is represented as a set of visual words. Each word belongs to the visual codebook including W unique visual words. Assume we have K action categories, S scene categories. Now we can describe the process generating each video v as:  1) For each clip v, there is a multinomial distribution over K action categories ș
v~ Dirichlet(Į). 2) For e ach visual word vi of the clip v: a) Choose an action category z
vi according to Multinomial distribution (ș v). b)  Choose a scene s
vi from p(s vi|zvi, ȥz), a multinomial probability conditioned on the action z vi and ȥ z. Here, ȥ
z is a multinomial distribution of action over scene s vi with ȥ z~ Dirichlet(ȕ). c) Choose a visual word w
vi from p(w vi|svi,ĳs), a multinomial probability conditioned on the scene s vi and ĳ s. And ĳ
s is the distribution of scene over visual words with ĳ s~ Dirichlet(ȟ).  Here, ș, ĳ an d ȥ are multinomial distribution associated with video clip v, scene s and action z. Under this generative processing, the probability of 
 video corpus on visual word set w, conditioned on Į, ȕ and ȟ is: 

     
VvNiv vviNi zz vi viNi s svi vi
vvvivivvivi
p z Pp z w Pp S w Ps z w P
1111 ) | ( ) | () | ( ) , | () | ( ) , | ( ) , , | , , , , (D T T[ \ \E M M[ E D \ T(1)This action-scene model includes the following parameters: the action distribution of video clip ș, action- scene distri
 bution ȥ, the scene distribution ĳ and the assignments of individual 
words to action z and scene s. In this paper, we use Gibbs sampling [17] to ev
 aluate the posterior distribution on z and scene s. Then ș, ĳ and ȥ can be inferred from the results of z and s.Given z, s, w, Į, ȕ and ȟ, computing the posterior distributions on ș, ĳ and ȥ is straightforward. Using the hypothesis th
 at the Dirichlet is conjugate to the multinomial, we can estimate ș, ĳ and ȥ with:  
¦¦¦   [[\EEMDDTS CCW CCK CCSZszSZszszWSwsWSwswsZVzvZVzvzv,, (2)where 
ZVzvC is number of the words from video v been assigned to an action z,  WSwsC is the time that a word w been assigned to a scene s, and SZszC  is the time that a scene s been assigned to an action z, and ¦ZVzvC ,
¦WSwsC and¦SZszC respectively denote the total number of words in video  v ,the total number of words assigned to scene s, and the total number of scenes assigned to action z. Then for each word, the basic equation for the Gibbs sampler is:  115  Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
sz ws zvvi vi vivivivi w z s m w j s x z P \MT[ED      ) , , , , , , | , ((3) where z
vi=z,s vi=s represent the assignments of the ith word in a video clip to action z and scene s respectively. w
vi=w represents the observation that the ith word is the wth word in the codebook, and z -vi,s-vi represent all action and scene assignment not including the ith word. Since the assignment of visu
al words to actions and scenes will be given quickly from equation 3, the recogn ition processing will be carried out by tracking three count matrixes 
ZVC,WSC andSZC.
In the training stage, the Gibbs sampling is applied over all the visual words in the training set while the count matrixes will be computed and saved. When classifying a new video clip, the Gibbs sampling is run on the visual word in this clip. The assignments of visual words to actions and scenes will be evaluated from equation 3. The count matrices are updated from each assignment. The video clip is classified with a category index with the largest probability in ș
v. Since ș v only draw the index of action category without knowing the actual action class label for the index, an action class la bel is assigned to the index by using ground truth labels in the training dataset. For each action index, it is named with the most popular action class label within videos belongs to this index.  5. Experimental Results We evaluate our approach on the challenging YouTube dataset  provided by Liu et al [7] which consists of 1168 videos collected by Liu et al. The dataset covers 11 action classes: basketball shooting, biking, diving, golf swing, horse riding, soccer juggling, swing, te nnis swing, trampoline jumping, volleyball spiking, and walking with a dog. Each category of action is performed under several different scenes and divided into 25 subsets.The hidden parameter ș,ĳ and ȥ were estimated based on training set. For all models we fixed the number of actions
 at K=11 and S is set to be the actual number of scene categories in dataset. The other parameters are set as Į=50/K,ȕ = 0.3 and ȟ = 0.01 from experimental analysis.  We compare 
our action-scene model with LDA[18]. The average performances with two models are shown in Table 1. We see that the proposed method achieve an improvement over all the action categories. The improvement is especially remarkable for the actions that have strong relation with scenes: basketball (15.13%) and trampoline jump (10.72%). It demonstrates that the scene cues are informative and complementary for identifying the actions in realistic videos.  116   Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
0.00%10.00%20.00%30.00%40.00%50.00%60.00%70.00%80.00%90.00%100.00%
basketballbikingdivinggolfridingsoccerswingtennistrampolinevolleyballwalking
categoryEvarege percisio nLDA Action-Scene
Fig. 1. The Recognition performance of action recognize for Youtube Dataset 
To evaluate the effect of person detector on the reco gnition performance, we compare the results using manually annotated person areas and the areas from person detectors in Section 3.1, and the results in Table 1 show that the performance of LDA can receive a better performance th an the action-scene model when the pers
on areas are annotated precisely, and a worse performance when the person areas are automatically detected. The action-scene model has better robustness than LDA when the features are noisy.  
Table 1. The Recognition performance with different feature extraction methods Recognition Model Annotation Detector 
LDA 78.3% 62.4% 
Action-Scene 73.5% 76.8% 
6. Conclusion In this paper, we explore to recognize human actions from background settings of realistic videos. An action-scene model is proposed to model and learn the relationship between actions and scenes with little prior knowledge on scene categories. A generative le arning method is used to infer actions from scenes according to a certain distribution. Experimental results validate that the addition of the context cues indeed improve the recognition precision. Besides, our pr oposed action-scene model has good robustness when applied in realistic video datasets.  There are a wide variety of potential applications on the research of action recognition. For further work, we intend to combine our method with known methods to learn application specified contextual cues from the massive realistic videos. References [1] C.Schuldt, I.Laptev, and B.Caputo, “Recognizing human actions: a local SVM approach,” in ICPR, 2004.  [2] J.Niebles, H.Wang, and L.Fei-Fei, “Unsupervised learning of human action categories using spatial-Evarege  percision 117  Yifei Zhang et al.  /  AASRI Procedia   6  ( 2014 )  111 – 117 
temporal words,” International Journal of Computer Vision, vol. 79, issue 3, pp. 299-318, 2008.  [3] T.Motwani, R.Mooney, “Improving video activity recognition using object recognition and text mining,” in ECAI, 2012.  [4] M.Marszalek, I.Laptev, C.Schmid, “Actions in context,” in CVPR, 2009. [5] M. Blank, L. Gorelick, E. Shechtman, “Actio ns as space-time shapes,” Phil. Trans. Roy. Soc. London, vol. A247, pp. 529-551, April 1955.  [6] Laptev, M.Marszalek, C.Schmid, B.Rozenfeld, “Learning realistic human actions from movies,” in CVPR, 2008.  [7] J.Liu, J.Luo, M.Shah, “Recognizing realistic actions from videos “in the wild” in CVPR, 2009.  [8] D.Han, L.Bo, C.Sminchisescu, “Selection and context for action recognition,” in ICCV, 2009.  [9] N.Ikizler-Cinbis, S.Sclaroff, “Obj
ect, scene and actions: combining multiple features for human action recognition,” in ECCV, 2010.  [10] B.Yao, L.Fei-Fei, “Modeling mutual context of object and human pose in human-object interaction activities,” in CVPR, 2010.  [11] P. F. Felzenszwalb, R. B. Girshick, D. McAllester, and D. Ramanan, “Object detection with discriminatively trained partbased models, ” IEEE Trans. Pattern Anal. Mach. Intell., 2010.  [12] D. Comaniciu, V. Ramesh, P. Meer, “Real-time trackin g of non-rigid objects using mean shift, ” in CVPR, 2000.  [13] P.Dollar, V.Rabaud, G.Cottrell, S. Belongie. “Behavior recognition via sparse spatio-temporal features,” in 2nd joint IEEE International workshop on visual surveillance and pe rformance evaluation of tracking and surveillance, 2005.  [14] N. Dalal, B. Triggs, “Histograms of oriented gradients for human detection,” in CVPR, 2005. [15] A.Oliv, A.Torralba, “Modeling the shape of the scene: a holistic representation of the spatial envelope,” International Journal of Computer Vision, vol. 42, issue 3, pp. 142-175, 2001.  [16] D.Lowe. “Distinctive image features form scale-invariant keypoi nts,” International Journal of Computer Vision, vol. 60, issue 2, pp. 91-110, 2004.  [17] B.Schölkopf, A.Smola, Learning with Kernels: Support Vector Machines, Regularization, Optimization and Beyond, MIT Press, 2002.  [18] D.Blei, A.Ng, M.Jordan, “Latent dirichlet allocation,” Journal of Machine Learning Research, issue 3, pp. 993-1022, 2003.  