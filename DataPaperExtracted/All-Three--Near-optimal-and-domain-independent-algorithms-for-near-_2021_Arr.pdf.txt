All-Three: Near-optimal and domain-independent algorithms fornear-duplicate detection
Aziz Fellah
School of Computer Science and Information Systems, Northwest Missouri State University, Maryville, MO, 64468, USA
ARTICLE INFO
Keywords:Near-duplicate detectionNear-duplicatesApproximate duplicatesClusteringData mining applications and discoveryData cleaningABSTRACT
In this paper, we propose a general domain-independent approach called Merge-Filter Representative-basedClustering (Merge/C0Filter/C0RC) for detecting near-duplicate records within a single and across multiple datasources. Subsequently, we develop three near-optimal classes of algorithms: constant threshold ( CT) variable threshold (VT) and function threshold (FT), which we collectively callAll/C0Threealgorithms. Merge/C0Filter/C0RC and All/C0Three mold the basis of this work. Merge /C0Filter/C0RC works recursively in the spirit of divide-merge fashion for distilling locally and globally near-duplicates as hierarchical clusters along with their prototyperepresentatives. Each cluster is characterized by one or more representatives which are in turn re ﬁned dynami- cally. Representatives are used for further similarity comparisons to reduce the number of pairwise comparisonsand consequently the search space. In addition, we segregate the results of the comparisons by labels which werefer to as very similar, similar, or not similar. We complement All /C0Three algorithms by a more thorough reexamination of the original well-tuned features of the seminal work of Monge-Elkan's ( ME) algorithm which we circumvented by an afﬁne variant of the Smith-Waterman's ( SW) similarity measure. Using both real-world benchmarks and synthetically generated data sets, we performed several experimentsand extensive analysis to show that All /C0Three algorithms which are rooted in the Merge /C0Filter/C0RC approach signiﬁcantly outperform Monge-Elkan's algorithm in terms of accuracy in detecting near-duplicates. In addition,All/C0Three algorithms are as efﬁcient in terms of computations as Monge-Elkan's algorithm.
1. IntroductionThe problem of identifying whether multiple representations of areal-world entity or object are the same has been originally de ﬁned by NewCombe et al. [1]. Since then, this long-standing problem has beenstudied extensively in computer science and related ﬁelds under various names using a multiplicity of terminology; just to name, record linkage[2,3] in the statistics community, approximate matching [ 4] in infor- mation retrieval, entity resolution [5], object identiﬁcation [6] in ma- chine learning, merge/purge [4,7,8] and near-duplicate detection [9–18] in databases and algorithms. In large and various databases, a major taskin a data cleaning process is identifying sets of records that are seman-tically duplicates of each other, but not syntactically identical. Such typeof duplicate records are also referred to as similar, approximate ornear-duplicates in research literature. In the context of this paper, weadopt the last terminology, that is, near-duplicates. The most commonvariations in representing the same entity ( i.e.,records, objects) with a multitude of representations can primarily arise from typographical er-rors, misspellings, missing data, and differences in abbreviations andschemes, as well as the integration of multiples data sources into a singledata set. In general in data cleaning, near-duplicates may exist within asingle source, whereas in data integration near-duplicates may existwithin or across various data sources. However, both cases have the samecommon goal, detecting near-duplicates and the closeness of similarityamong entities in a large collection accurately and ef ﬁciently. That is, to determine which records/objects in the same or different databases referto the same underlying real-world entity.For example, consider the following references that have beenrecorded at three different colleges,“Jeff David Ullman Stanford Univer- sity, Dept. of Computer Science”;“Jeffrey D. Ullman Computer Science Dept.,Stanford Univ., CA, USA”;“J. D. Ullman, Department of Computer Science,Stanford University, USA”. All the three references refer to the same in-dividual, even though they are quite different if byte-by-byte compari-sons are used. It is often the case that data in different repositories holdinformation regarding identical entities, but might be stored in differentformats and schemes which may result in a possible data inconsistencyand nonconformity. One example would be identifying authors and ci-tations in a bibliography database such as DBLP, CompuScience, and
E-mail address:afellah@nwmissouri.edu.
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100070Received 20 February 2021; Received in revised form 5 May 2021; Accepted 12 May 2021Available online 27 May 20212590-0056/©2021 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 11 (2021) 100070CiteSeer. Several algorithms, in particular domain-dependent, for quan-tifying the degree of similarities have received particular attention in awide range of applications, including web search engines and mining,medical and census data, plagiarism and spams, mailing list deduplica-tion, and image database. A substantial body of research has been con-ducted in a spectrum of domains, see for example [ 19–28].2. Background and related workNaive near-duplicate detection methods which are based oncomparing every pair of records become intractable in the context ofhuge data collections such as social networks. For example, Facebookwith over 2.97 billion active users worldwide, results in more than 2.45/C210
19comparisons. Several methodologies and solutions have beenproposed in research to reduce the total computational cost and improvethe accuracy. In a stand-alone literature review, a variety of blockingmethods which are based on the selection of a key ( blocking key) for generating subsets of potential near duplicates have been investigated inresearch [5,11,29]. The main focus of these methods is that recordshaving the same key value should be added to the same block and labeledas potential near-duplicates for further analysis. Jaro and Winkler [ 3,24, 30] extended the key-based approach by using expressions with multiplekeys which increases the accuracy and reduces the number of falsematches and false misses. Such keys can be adjusted interactively bytrial-and-error analysis to achieve the best results. However, the processofﬁnding a perfect key is often difﬁcult and requires good domain knowledge. Other related blocking criteria based on applications ’char- acteristics (i.e.,medical, census, bibliography data) are also used as aﬁrst-step preprocessing to identify initial blocking. In general, blockingalgorithms are of low time complexity, but with several drawbacks. Forexample, records with minor typographical errors or simple misspellingare clustered in different blocks. It has been observed in Refs. [ 23,24] that 40–70% of the matches are found in theﬁrst blocking pass. By the fourth blocking pass, 0.0001% of the pairs are actually duplicates. Theseresults show that the proportions of pairs that were duplicates in suc-cessive blocking criteria fell at an exponential rate. A well-knownduplicate detection framework known as sorted neighborhood method(SNM) was proposed by Hernandez and Stolfo [ 8,31] and is based on that near-duplicates tend to localize in the neighborhood. The algorithm usesblocking to sort all records based on a sorting key and then slidessequentially a window ofﬁxed size (sliding window) over the sorted re- cords. The window uses the blocking scheme by including all recordswith similar keys (lexically nearby keys) in the same window. All recordswithin each sliding window are considered as potential near-duplicatesand then compared with each other, however, the window size is dif ﬁ- cult to set. Overall, the sliding window is a more robust approach thanother techniques in improving the near-duplicate detection accuracy, butit is likely to fail in grouping similar records outside the window size ifsubstantial typographical errors occurs in the ﬁrst characters of the sorting key. Moreover, there is substantial research that has been pro-posed as an umbrella forSNMwhere blocking has been empiricallyevaluated on different domains and data sets. Some of these extendedvariations ofSNMand blocking approaches outperform the basic SNM and blocking approaches in terms of reducing the complexity cost andimproving the accuracy. For instance, Yan et al. [ 7] proposed an adaptive variant scheme ofSNMfor record linkage by adjusting the size of thesliding window dynamically during the execution time to buildnon-overlapping blocks, but it has been con ﬁrmed that their work did not outperform the originalSNM
. Other complicated near-duplicate identi-ﬁcation techniques such asq-gram, iterative blocking, overlappingblocking, multiple blocking, token-based, learning process, and domainknowledge, with some assumptions on the entities have been investi-gated in the literature, just to name few [ 5,25,32–34]. Furthermore, with rapid advances in big data computing and web era,near-duplicate detection research is expanding in many ways to industryapplications such as managing massive documents, and extractinginsights and multi-dimensional information from business, ﬁnancial (i.e., credit cards), and healthcare data. A comprehensive evaluation and anumbrella of techniques have been investigated. A common factor be-tween these algorithms is they cannot guarantee ﬁnding all near- duplicates and also cannot guarantee the accuracy. Vogal et al. [ 35] provided an annealing standard to evaluate near-duplicate detectionresults. In other words, the accuracy and completeness of duplicatesshould converge incrementally and interatively to a gold or silver stan-dard that deﬁnes which records represent the same real-world entities.Near-duplicate detection algorithms mainly focus on effectivenessand efﬁciency, but not on scalability which has been addressed byNaumann et al. [36], who assign appropriate similarity measures to at-tributes based on their semantics. For instance, names of persons shouldbe compared differently than email addresses even though they belong tothe same string data type. Overall, most of these methods require a goodunderstanding of the application domain and a supervised user interac-tion for reﬁning and adjusting parameters such as distance functions,window size, and thresholds.The most predominant domain-independent algorithm for near-duplicate detection is that of Monge-Elkan ( ME)[4,14]. This seminal work is based on stretching adequately the SNM's sliding window [8] that holds aﬁxed number of record sets and grouping records into clusters.Monge-Elkan's algorithm has a much more improved ef ﬁciency over many duplication methods including the SNMalgorithm, but has the same detection accuracy as theSNMand performs even far better than a large number of other algorithms in terms of time and accuracy. Inaddition, a spectrum of similarity and distance measures, highly depen-dent on the application domain, have been investigated in the literatureand are certainly not a new area of research. They range from fairlysimple schemes to more complex well-tuned edit distances. The mostprominent class of character-based metrics known as edit distances areLevenshtien, Jaro-Winkler, and Smith-Waterman similarity measures [ 3, 30,37]. One extension to the Levenshtein distance is theNeedleman-Wunsch [38], which additionally allows variable sub-stitution's cost for different characters. That is, it provides a mapping foreach pair of symbols (i.e.,characters) from the alphabet to some cost.Other effective similarity measures, token-based and hybrid metrics,have been also investigated in the literature, for example, Jaccard,n-grams, Cosine, Monge-Elkan, and natural language processing tech-niques (i.e., TF-IDF) similarity measures. Similarity metrics range fromfairly simple schemes to more complex well-tuned edit distances, see forexample, [11,13,37,38].3. ContributionsWe propose a new generalized domain-independent framework andsubsequently three classes of algorithms for detecting near-duplicatesamong entities within one or more attributes in large database sets.These algorithms are synthetically complemented by near-duplicategenerator algorithm (NDG). In the rest of the paper, we refer to such aframework as Merge-Filter Representative-based Clustering (Mer-ge/C0Filter/C0RC) and to such a set of algorithms as constant threshold ( CT), variable threshold (VT), and function thresholds (FT), respectively. For convenience, we collectively refer to these three algorithms as All /C0Three algorithms and to a speciﬁc algorithm by its conventional name, eitherCT,VT,o rFT. In addition, we integrate an efﬁcient synthetic near- duplicate generator algorithm (NDG) into All/C0Three algorithms. The NDGalgorithm is capable of scaling up to generate algorithmically tens ofmillions of synthetic records. One of our aim here is to capture andmodify the full power of Monge-Elkan ( ME)[4,14] and Smith-Waterman (SW)[37] algorithms in the context of our work. That is, Mer-ge/C0Filter/C0RC is a domain-independent approach that combines thetoken-based of Monge-Elkan and the character-based internal ofSmith-Waterman similarity function, both of which we modi ﬁed and augmented with a well-tuned afﬁne parameterization. Still, this is notenough to solve the quality and complexity of our approach due toA. Fellah Array 11 (2021) 100070
2anomalies which are associated with the transitivity relation andthreshold choices. Thus, we segregate the results of the comparisons bylabels which we refer to asvery similar,similar,o rnot similar, and furthermore minimize an objective function without the user's inter-vention. Each constructed cluster has one or more representativeswhich are dynamically computed to measure the prototypicality of each recordin the cluster. The record comparison is performed with only represen-tatives rather than with all records in the cluster. Thus, records do nothave to be compared to all other records, but to only cluster represen-tatives which are considered for subsequent comparisons. We introducecluster representatives which retain the most relevant syntactic and se-mantics features of the records in the cluster. The idea behind thisapproach is that cluster representatives reduce the total number of recordcomparisons, without reducing substantially the accuracy of the dupli-cate detection process.Clusters’representatives are dynamically distilled and accuratelyachieved throughout a set of comparison functions, very similar, similar,or not similar, and threshold settings, constant, variable, or function. Thenumber of similarity comparisons is reduced from O(n
2)t oO(nm), where mand andnare the number of representatives and records, respectivelyandm≪n, mis always independent ofn, but dependent on the class of the algorithm. Each of these algorithms has a different impact whenrunning on a set of thresholds, a constant, variable, or function. All al-gorithms are implemented in Cþþand use the same data set to make faircomparisons. We ran an extensive experimental study using several realbenchmarks and algorithmically generated synthetic data. We do notassume any speciﬁc structure in the data nor rely on any informationavailable in the source data. That is, data has not been standardized,preprocessed, nor transformed, and syntactic as well as semantic errorsremain as potential errors in the data. Experimental implementationsshow the Merge/C0Filter/C0RC detection approach greatly reduces thenumber of comparisons and the precision achieves a value of nearly 1.0, aprecision which is close to the optimal outperforming consistently theseminal work of Monge-Elkan.Our proposed set of algorithms do not presume a speci ﬁc application domain, but in contrast they are tuned toward any domain-independentapplications. Monge-Elkan's (ME) algorithm is relatively domain-independent with the purpose of integrating and matching web scienti-ﬁc papers from multiple sources, typically an alphanumeric domain class.The parameters used in ME are mapped to such a class of applicationswith only a restricted possibility of tuning the threshold values to providea better accuracy. In addition, the heuristic method of ME minimizes thenumber of pairwise record comparisons with potential record duplicatesand integrates some key concepts such as the minimum edit-distance ofSW.The rest of the paper is organized as follows. Section 3reviews and summarizes the effectiveness of Monge-Elkan and Smith-Waterman al-gorithms. Section4addresses the metric measures used in detecting near-duplicates. Section5explains how to calculate and choose betweenprecision and recall using F-measure. The choice, adjustment, andthreshold tuning are deﬁned in this section.Section6proposes the merge-ﬁlter cluster representative frameworkwhich addresses the details of the accuracy performance using clusterrepresentatives throughout precision and recall metrics. Section 7pro- vides a fully algorithmic technique and presents three different domain-independent algorithms, constant, variable, and function thresholds fordetecting near-duplicates, all of them under the umbrella ofMerge/C0Filter/C0RC.In Section8, we present a thorough experimental evaluation of ourapproach and compare the effectiveness of All /C0Three algorithms in terms of accuracy and efﬁciency with the seminal work of Monge-Elkan.We used benchmark data as well as synthetic data to meet speci ﬁc con- ditions that are not available in existing real data. Synthetic data has beengenerated using the near-duplicate generator (NDG) algorithm. Section9 concludes the paper with potential and future research directions. Inaddition, we provide an appendix to make our paper self contained.4. Effectiveness of Monge-Elkan and Smith-Waterman algorithmsIn this paper, we focus on the Smith-Waterman ( SW) edit distance [37] that was originally developed for identifying common molecularsubsequences, like DNA or proteins. Two strings xandymay not be entirely similar but contain regions, perhaps in the middle, that exhibithigh similarity. Finding such a pair of regions, one from each of the twostrings, is referred to as alocal alignment. The Smith-Waterman algorithmﬁnds the local alignment between two strings with the maximum possiblescore using a dynamic approach that runs in O(|x||y|) time. The main limitation of SW is it places heavier penalties ( i.e.,higher cost) on mis- matches in the middle of strings rather than at the beginning and the endof strings. This may create a problem when the errors are in the middle ofthe strings. In this regard and in order to eliminate this inconvenience,we primarily consider the Monge-Elkan's methodology [ 14,26], a well-tuned matching methodology normalized in the interval ½0;1/C138, allowing additional parameters, and introduces gaps in the alignment oftwo strings. Much of the power of the Monge-Elkan's algorithm is due toits ability to include sequences of non-matching characters, gaps ( afﬁne gaps), in the alignment of two strings. In our work, we add the gap cost asanother variant of the Smith-Waterman ( SW) algorithm that offers a solution to the above problem and other related duplicate detection is-sues. By adding a cost, we extend the two extra edit operations, starting gapandextending gap.In general and for instance, the gap penalty denoted by cost(gap) ¼s þe/C2l, wheresis the afﬁne cost of starting a gap in an alignment, eis the cost of extending a gap, andlis the length of a gap in the alignment of twostrings. Usually afﬁne gap penalizes gap extension less than gap opening(e<s) thus we decrease the penalty for contiguous mismatched sub-strings by using a single long gap over many short gaps. Since the dif-ferences between near-duplicate records often arise because of manyabbreviations or extra-string insertions and omissions, the af ﬁne-gap model produces a better similarity and more accurate results than mostthe other edit distance metrics. Moreover, the af ﬁne-gap algorithm per- forms well to detect similarities when records have minor syntacticaldifferences, including typographical errors, abbreviations, and trunca-tions. In fact, the Monge-Elkan's algorithm approximates the solution tothe optimal assignment problem in combinatorial optimization. Thisapproximation is a reasonable trade-off between accuracy and
complexity.In sum, Monge-Elkan's complexity is quadratic in number of tokensand one can deﬁne the Monge-Elkan's measure over two text strings thatcontain several tokens as:MongeElkanðx;yÞ¼
1jxjXjxji¼1max sim
j¼1;jyjðx½i/C138;y½i/C138Þwhere |x| and |y| are the number of tokens inxandy, respectively and sim(x,y) is an internal similarity function to measure the similarity be-tween two individual tokens.In the paper, we adopt a modiﬁed version of the Smith-Watermansimilarity edit distance as inter-token similarity measure. Formally, letc(x
i,yi) denote the cost of the edit distance that aligns ith character of stringxtoj
thcharacter of stringy. Then the SW algorithm computes acost matrixMthat represents a maximum-cost string alignment by thefollowing recurrence rule based on Monge-Elkan's algorithm [ 4].Mði;jÞ¼max8>>>><>>>>:Mði/C01;j/C01Þþc/C0x
i;yj/C1Mði/C01;jÞþeif alignði/C01;j/C01Þends in a gap Mði/C01;jÞþsif alignði/C01;j/C01Þends in a match Mði;j/C01Þþeif alignði/C01;j/C01Þends in a gap Mði;j/C01Þþsif alignði/C01;j/C01Þends in a match5. Metric measures and threshold effectsSimilarity refers to a measure of likeness between two objects ( i.e.,A. Fellah Array 11 (2021) 100070
3records, entities); and the dissimilarity between two objects is refereedto as a distance. We deﬁne a record as a set of tokens drawn from a ﬁnite universeU.L e tnbe the number of real-world entities over a plurality oflarge databases that consist of recordsR
n¼r1;r2;…;r n, where a large number ofr
iare potential near-duplicates. We denote by D¼D 1;D2; …;D
dthe set of problem domains.Similarity and distance measures are often normalized in the range½0;1/C138, and½0;∞/C138or½0;some distance/C138, respectively. Formally, we deﬁne a similarity and distance measures as follows:Deﬁnition 5.1. A similarity measure is a non-negative function sim:D
1/C2D 2→[0,1],such that sim(r 1,r2)¼0i fr 1andr 2are least similar and sim(r
1,r2)¼1i fr 1andr 2are identical. (D 1might be equal toD 2). Deﬁnition 5.2. A distance measure is a non-negative function dist:D
1/C2D 2→[0,1],such that dist(r 1,r2)¼0i fr 1andr 2are exactly similar or identical, and dist(r
1,r2)¼1i fr 1andr 2are not similar. The search space for detecting near-duplicates can be reduced underthe assumption the relation“is duplicate of”or“is similar to”, is transitive. However, the theory of transitivity is not always ﬂawless in practice because of the propagation of errors as explained earlier. Duplicate re-cords tend to be sparsely distributed over a large database space and thepropagation of errors is statistically insigni ﬁcant [26]. The complexity and evaluation measures to assess a near-duplicate record algorithm thathave been addressed are theefﬁciencyandaccuracy. Accuracy is considered the most important quality assessment dimension innear-duplicate algorithms, and it is measured in terms of two prominentmeasures,precisionandrecall. The other related measure is F-measurewhich determines the harmonic mean of the precision and recall values.The goal of our evaluation is toﬁnd the best metrics in terms of qualityand effectiveness with respect to different domain-independent data sets.We classify each value pair of comparisons as very similar,similar,o rnot similarsince errors may occur during the process of detectingnear-duplicates. For completeness, as it is illustrated in Fig. 1, we divide the search space into subspaces and denote by true positives all candidatepairs that are correctly declared to be duplicates ( i.e.,expected matches), and false positives all candidate pairs that are incorrectly declared to beduplicates while in fact they may not be duplicates ( i.e.,there should not be a match).Similarly, true negatives are pairs that are correctly recognized as notbeing duplicates (i.e.,expected mismatch), and false negatives are pairsthat are not declared to be duplicates while in fact they are ( i.e.,there should be a match). The metric precision measures the fraction of correctduplicates over the total number of record pairs classi ﬁed as duplicates by the algorithm. The metric recall measures the fraction of recordscorrectly clustered over the total number of duplicates. A high recallmeans no false misses and a high precision means few false matches;thus, there is a trade-off between high recall and high precision. Theother measure we want to introduce is the reduction ratio, which is the relative reduction in the number of pairs to be compared. This means asearch space reduction strategy is needed in order to reduce the numberof record comparisons. As a consequence, we introduce prototype clusterrepresentativeswhich retain the most relevant syntactic and semanticsfeatures of the records in the cluster where comparisons take place withcluster representatives, instead of all records, thus the search space canbe reduced with the improvement of both true and declared subspaces. Inother words, the reduction of false positives and in particular false neg-atives have an impact on the accuracy. A high recall means no falsemisses and indicates high accuracy of the duplicate detection results. Ahigh reduction ratio achieves an even more effective search spacereduction. A high precision means few false matches and has the oppositeeffect than recall. The other metric measure, reduction ratio in the rangeof½0;1/C138,i sd eﬁned as 1 - (declared duplicates/all tuple pairs). The simi-larity threshold line inFig. 1ensures a trade-off among recall, precisionand reduction ratio. If we shift the similarity threshold line (similaritythreshold 1) to the right, consequently more tuple pairs will be rejectedand increase the reduction ratio and precision. This rejection would ul-timately lead to decrease the recall metric. If we shift the similaritythreshold line to the left then the opposite trade-off effect between recall,precision, and reduction ratio will take place. Similarly and in the samefashion, shifting the similarity threshold line (similarity threshold 2)would have an effect on true and false positives.Relations between threshold similarity metrics and accuracy are animportant part of the near-duplication detection process. Differentthreshold setting and parameter tuning ( i.e.,distance functions, window size) have been used to classify whether a pair of records is a duplicate ora non-duplicate. Several experiments have been conducted and cutoffthresholds for a speciﬁc application domain with its own characteristics,often manually conﬁgured, have shown to achieve the“best”precision, recall and F-measure values. The cutoff value of the threshold along withother key tuning parameters can be very time consuming as the searchspace can grow exhaustively and even exponentially, which necessitatessome forms of threshold optimization [ 35,39,40].6. Merge-Filter Representative-based Clustering: A near-optimaldomain-independent approachSimilar in spirit to divide and merge methodology for clustering [ 41], Merge-Filter Representative-based Clustering ( Merge/C0Filter/C0RC) com- bines a top-down divide phase with a bottom-up merge phase in a hier-archical scheme as described in subsequent sections. Merge /C0Filter/C0RC is made of two trees, a top-down and bottom-up tree, annotated withcluster records. Merge/C0Filter/C0RC is mainly geared towards the needs ofdetecting near-duplicates with a provision of forming the best possiblenear-optimal clusters of records by integrating a variant of Smith-Wa-terman's and Monge-Elkan's algorithms into the detection approach.Importantly, we are only interested in unconstrained algorithms, ratherthan adopting any standard clustering algorithm. That is, there is nopreliminary assumption of rationality to choose the number of clusters asinput or other domain speciﬁc parameters. LetCdenote the initial cluster containingnrecords inR
n, andddenote the depth of a node in thetop-down cluster treeTwhose root (C;n)i sa td¼0 and internal nodes are (C
i,nb), whereC iandnbrefer to the name of the cluster and thenumber of records inC
i, respectively as illustrated inFig. 2. The divide phase of Merge/C0Filter/C0RC recursively splits the collection of recordsinto two equal halves at each level of Tand constructs the treeTon the basis of these records. The near-duplicate detection process starts withthe data set of the initial clusterCthat is expected to contain near-duplicate records. We start the construction at the root with nre- cords and end-up at the leaf level with one single record per cluster, ( C
l, 1) (Fig. 2). The number of times the split is done is exactly equal to theheight of the treeT,O(logn), because the size of the clusters decreasesapproximately by half at each level of Twhich is deﬁned as follows: Deﬁnition 6.1. A top-down cluster treeTis a full binary tree such that (i) the nodes ofTare subclusters ofC(ii) every internal node ofThas two subcluster of records, each of sizen/2
d, (iii) every leaf ofTis a subcluster of records of size one; (iv) the root node of Tis the clusterCthat consistsFig. 1.Similarity threshold factors and tradeoff between prominent mea-sure metrics.A. Fellah Array 11 (2021) 100070
4ofnrecords.Letℂ¼C
1;C 2;…;C nbe the set of clusters ofTproduced by the divide phase. Each internal node of Tis a subset of the data set records. The left and right children of a nodeC
iforms a partition of the parent such that∣C
i∣¼n=2dand 1/C20i,i/C20n(Fig. 2). LetTbe a top-down cluster tree then for any two clusters we have eitherC
i⊂C jorC j⊂C ior C
i\C j¼∅,i6¼j,1/C20i,j/C20n.Starting at the leaves ofT, the bottom-up merging phase is appliedrecursively to each node ofTtowards the root enabling the constructionof a new tree of clusters whose root is ^Tas depicted inFig. 3.T o accomplish this, the near-optimal cluster of an interior node ^C
iin the tree^Tis obtained by merging and distilling dynamically the near-optimalclusters of the left and right children of ^C
i. The result of clusteringTis a partition^C¼^C
1;^C 2;…;^C qwhere one or more^C iare the nodes of^T, referred to as near-optimal duplicate tree, i/C20qandq≪n(qis much less thann). The value ofqis not known in advance and is independent of n. At the beginning of the Merge/C0Filter/C0RC bottom-up phase, we initialize the set of leaf clusters of^Twith the set of leaf clusters ofT. Then, we apply a hierarchical agglomerative clustering to the leaf clusters bybringing the leaves up to the root level by level, and near-optimizing theobjective function locally at each iteration when two clusters arecompared and eventually merged. The choice of the objective functionuses the dynamic programming of SW algorithm re ﬁned by different parameters as it will be explained in the next sections. Each constructedcluster^C
ihas one or more cluster representatives that are dynamicallycomputed to measure the prototypicality of each record in the cluster.The record comparison is performed with only representatives ratherthan with all records in the cluster. Consequently, records do not have tobe compared to all others but only cluster representatives are consideredfor subsequent comparisons. That is, if a given record is not similar to arecord(s) in a set of cluster representatives then it will not match theother record members of the cluster. In general, Merge /C0Filter/C0RC takes as parameters two unordered pairs of clusters ( ^C
i,^C j), and their respective cluster representatives ( ^R
i,^Rj) where 1/C20i,j/C20n, then returns whether (^C
i,^C j) arevery similar,similarornot similarbyﬁnding the optimal local alignment using the SW algorithm with the maximumsimilarity score. Let such a set of cluster representatives denoted by ^Ri¼ f^r
i1;^ri2;…;^r ijg,1/C20j/C20lwherelindicates the number of representativesfor aﬁxedi./C12/C12/C12^C
i/C12/C12/C12¼kand/C12/C12/C12^R
i/C12/C12/C12¼l, wherelis much less thank. Every generatedith cluster^C
i, represented as a node in^T, has a set of near- duplicates and cluster representatives referred to as fr
ijgkj¼1andf^r ijglj¼1, respectively. The results of detecting near-duplicates, however, maybecome sensitive to the initial selection of representatives. Initially,Merge/C0Filter/C0RC uses one record as a cluster representative, then rep-resentative(s) might be subsequently updated, either by retaining thesame ones or iteratively re-computing and re-assigning new representa-tives. In addition, Merge/C0Filter/C0RC enforces transitivity between re-cords in a cluster^C
i. Each remaining record is compared to therepresentatives and placed in the cluster of the closest representative.The idea behind this approach is that cluster representatives reduce thetotal number of record comparisons, without reducing substantially theaccuracy of the duplicate detection process. The number of similaritycomparisons to be considered is reduced from O(n
2)t oO(nm), wherem and andnare the number of representatives and records, respectively. m ≪nandmis always independent ofn, but dependent on the class of the algorithm (constant, variable, or function).The cornerstone property of Merge/C0Filter/C0RC is based on the choice of the appropriate objective function gand its parameters. We are interested inﬁnding locally the optimal clustering at each level of ^T, and alsoﬁnding globally the near-optimal clustering at the root created bythe Merge/C0Filter/C0RC merge phase. That is,gshould guarantee toﬁnd the optimal local alignment,OPTðAÞ, and quantiﬁes the similarity in terms of a high-scoring alignmentA. That is, we maximize the objectivefunctiongby assigning a score for each alignment obtained by the SWalgorithm. LetΣ* denote the set ofﬁnite words over an alphabetΣandx ¼x
1…xp,y¼y 1…yqwherex,y2Σ*. The state space of all alignments ofxandyis the mappingg:ð〈x;y〉Þ7!Z, the set of integers. The optimal local alignment score of the subsequences x
1…xpandy 1…yqis obtained by maximizinggamong all alignments. That is,g
0ð〈x;y〉Þ¼max
Agð〈x;y〉Þ
Fig. 2.Construction of a top-down cluster tree T.
Fig. 3.Vizualization of a near-optimal duplicate cluster tree ^T.A. Fellah Array 11 (2021) 100070
5OPTðAÞ ¼argmax
Agð〈x;y〉ÞLet.^C
land^C rbe the left and right children of an internal node ^Cin^T. LetNEAR/C0OPT
DUPTREE ð^C;iÞbe the near-optimal duplicate sub-tree forthe node^Cusingiclusters as stated recurrently in the following theo-rem. The space of near-optical solutions is represented in a data structure.That is, a tree that can be efﬁciently used toﬁnd near-optimal solutions that satisfy the optimal local alignment. Near-optimal and recursivelyconstructed bottom-up subtrees, DUPTREEð^C;iÞ, facilitate the re- optimization (i.e.,tuning) in the object functiongto satisfy the proper- ties and accomplish the near-optimality solution. Thus, each subtree ofthe node^Cusingiclusters,DUPTREEð^C;iÞ, transparently represents the space of near-optimal solutions and how each subtree relates to eachother. We propose the following theorem which formally and compactlydescribes the near-optimal detection solution along with the corre-sponding objective functiong. We explore the clustering methodology of[41] further and formally introduce near-optimal duplication tree of anode^Cusingicluster deﬁned asNEAR/C0OPT
DUPTREE ð^C;iÞ. The objective function values of each alignment obtained by the SW algo-rithm guarantee the optimal local alignment across all subtrees of ^T. Theorem 6.1.NEAR/C0OPT
DUPTREE ð^C;iÞ¼/C26^C ifi¼1 NEAR/C0OPT
DUPTREE/C16^C
l;j/C17[NEAR/C0OPT DUPTREE/C16^C
r;i/C0j/C17ifi>1wherej¼argmin
1/C20j<ig/C16NEAR/C0OPT DUPTREE/C16^C
l;j/C17[NEAR/C0OPT DUPTREE/C16^C
r;i/C0j/C17/C17Proof. We proceed by induction oni. The base case handles all clusters of^T. That is, all initial clusters^C
1;^C 2;…;^C qwith a single represen- tative that is generated in the divide phase. Starting at the leaves, the ﬁrst optimal clusters are originated from the SW algorithm. If two records arein the same cluster then they are considered to be near duplicate orsimilar, and if not they are dissimilar.For the induction case, we can now assume the claim is true for all i, 1<i<n. Let inode(^T) be an internal node of^T, and^T
1and^T 2the left and right subtrees recursively build from the leaf clusters of ^T, respectively. Without loss of generality, deﬁne^C
1land^C 1rto be the clusters whose root is^T
1; and^C 2land^C 2rbe the clusters rooted in the right subtree ^T 2, ordered asð^C
1l;^C 1rÞ;ð^C 2l;^C2rÞ. Denote by (^R 1l;^R1r) and (^R 2l;^R2r), the set of cluster representatives of ( ^C
1l;^C 1r) and (^C 2l;^C 2r), respec- tively. Let^R
1l¼^R 1l[^R 1rand^R 2r¼^R 2l[^R 2rsuch that/C12/C12/C12^R il/C12/C12/C12¼p>1 and/C12/C12/C12^R
ir/C12/C12/C12¼q>1, for i¼1, 2. That is, a cluster has more than onerepresentative which is used for subsequent comparisons. Importantly,representative clustering reduces the total number of record comparisonssubstantially and furthermore representatives are bounded by two sim-ilarity threshold values,θ
uandθ l. Keeping this potential of multiplecluster representatives,^C
l¼^C 1l[^C 1rand^C r¼^C 2l[^C 2r. For convenience, let^R
1l¼f^rl1;1;^rl1;2;…;^rl1;pgand^R 2l¼f^rl2;1;^rl2;2;…;^rl2;pg. Similarly^R
1rand^R 2rare deﬁned. If two records (i.e., representatives)are in the same cluster then they are considered to be near-duplicates orexactly similar, and if not they are dissimilar. For instance, let R
3¼fr 1; r
2;r3gbe the set of records to be compared and the initial good thresholdchosen is in the interval½0:83⋯0:90/C138. Assume we have the following similarities: sim(r
1,r2)¼0.88, sim(r 1,r3)¼0.72, and sim(r 2,r3)¼0.80. Then, (r
1,r2) are classiﬁed as near-duplicates and assigned to the samecluster. However, (r1,r3) and (r 2,r3) would be non-duplicates. Now, consider a new record r
4added toR 3with sim(r 1,r4)¼0.75, sim(r 2,r4)¼ 0.88, and sim(r
3,r4)¼0.96. Then, (r 2,r4) and (r 1,r4) are classiﬁed as near-duplicated and non-duplicated, respectively. However, due to theanomaly in the transitivity relation and the non-appropriate choice of thethreshold, the pair of records, (r
1,r4) and (r 2,r3), would be reclassiﬁed as near-duplicates although there were not. This is an important note thatshould largely foster the choice of appropriate thresholds for larger datasets. With our proposed approach, the results of the comparisons aresegregated by labels referred to as very similar, similar, or not similar;and the quality and accuracy of the classiﬁcation is further improved by ﬁnding near-optimal or sub-optimal thresholds to identify more accu-rately duplicate records by minimizing the objective function without theusers intervention. With this viewpoint and based on the SW algorithm,we start computing the optimal clustering for the leaf nodes and then ﬁnd the near-optimal clustering, relatively to the optimal local alignment, forany internal node. That is, at the end of the SW algorithm during themerge phase,NEAR/C0OPT
DUPTREE ð^TÞgives the almost optimal clus- tering. We consider two upper and lower bound threshold values, θ
land θ
u, which are extensively studied in domain independent large databases.Both bounds which are rooted in the seminal Monge-Elkan's algorithmhave shown effectiveness as they become core standard thresholds inresearch literature [39,40,42] and in almost every approximate duplicatedetection algorithm. The two upper and lower bound threshold valuesare mainly governed by the probability of errors for ﬁnding optimal re- cord alignments. Thus, we set the semantic similarity threshold param-eter for which two records are considered semantically similar to θ
u.I n the same way as semantically similar records, we set the non-similaritythreshold toθ
l.We setθ
uto 0.7 andθ lto 0.5 for declaring two records as near-duplicate and non-duplicate, respectively. θ/C20θ
lproduces loose simi- larity (not similar),θ/C21θ
uproduces high similarity (very similar), and if θ falls between these values it is regarded as a regular similarity (similar).Let^C
land^C rbe the left and right children of an internal node ^Cwhose cluster representative is^R. We denote by^R
land^R rthe left and right cluster representative children of ^R. As a consequence ofTheorem 6.1, we state the following result.Corollary 6.1.NEAR/C0OPT
DUPTREE ð^R;iÞ¼/C26^R ifi¼1 NEAR/C0OPT
DUPTREE/C16^R
l;j/C17[NEAR/C0OPT DUPTREE/C16^R
r;i/C0j/C17ifi>1wherej¼argmin
1/C20j<igðNEAR/C0OPT DUPTREE/C16^R
l;j/C17[NEAR/C0OPT DUPTREE/C16^R
r;i/C0j/C17/C17
7. Competitive algorithmsWe introduce three classes of algorithms from different perspectives,a constant threshold (CT), a variable threshold (VT), and a functionthreshold (FT) algorithm, collectively referred to as All /C0Three algo- rithms. Each of these algorithms has a different impact when running onvarious benchmarks and synthetic data sets.7.1. Constant threshold (CT) algorithmWe use the same notations in line with Section 6. Let^Tbe the bottom- up cluster tree, and^T
1and^T 2the left and right subtrees recursively buildfrom the leaf clusters of^T, respectively. Assume n, the number of records,is a power of 2 for the sake of convenience. The output of the approxi-mate duplicate SW merge phase is a partition of clusters ^C¼^C
1;^C2;…;A. Fellah Array 11 (2021) 100070
6^Cq, where each^C iis a node of^Tand q is unknown in advance. Withoutloss of generality, let assume^C
1land^C 1rbe the clusters whose root^T 1; and^C
2land^C 2rbe the clusters rooted in the right subtree ^T 2of^T, ordered asð^C
1l;^C1rÞ;ð^C 2l;^C2rÞ.The procedure C
ONSTANT THRESHOLD (^C1l;^C2l) in algorithm 1 is based on the idea of the Merge/C0Filter/C0RC approach which compares andsubsequently updates two given clusters, either by merging them into anew cluster and removing one of the original cluster. Moreover, the al-gorithm iteratively recomputes and reassigns new representatives. Eachrecord is compared to the representatives and placed in the cluster of theclosest representative. The number of comparisons is reduced from O( n
2) to O(mn), where reduced m and n are the number of representatives andrecords, respectively (mis much smaller thann).7.2. Variable threshold (VT) algorithmIn procedure V
ARIABLE THRESHOLD (^C1l;^C 2l) in algorithm 1, each cluster has only one representative and only one variable threshold valueis considered, starting atθ
1¼0.5. In line with the approximate duplicateSW algorithm, the variable threshold algorithm (VT) compares clustersfrom the left subtree with clusters from right subtree. That is, we compare^C
1lwith^C 2land^C 2r, then we compare^C 1rwith^C 2land^C 2r. Without loss of generality and for algorithmic simplicity, we assumecluster^C
1has^r 1as a record representative andθ 1as a threshold. Sim- ilarity,^C
2has^r 2as a record representative andθ 2as a threshold. At start, clusters have one record and the threshold value θ
1is set to 0.5.7.3. Function threshold (FT) algorithmIn this third category of algorithms each cluster has more than onerepresentative. Furthermore, two variable threshold values are consid-ered, an upper bound valueθ
u¼0.7, and a calculated threshold value θ c. In line with the approximate duplicate SW algorithm, the functionthreshold algorithm (FT) compares clusters from the left subtree withclusters from right subtree. That is, we compare ^C
1lwith^C 2land^C 2r, then we compare^C
1rwith^C 2land^C 2r. In addition, we compare^C 1r
with every cluster in the second half subtree.The function C
OMPARE ð^C1;^C 2Þshows the very similar case when maxSW>θ
u(lines 4 and 5). The other two cases, similar(θ
l/C20maxSW/C20θ u) and not similar (maxSW<θ l), can also be treated in a same manner. Lines 4 and 5 in the function C
OMPARE (^C1;^C2) should be substituted by lines 4 and 5 in the function C
HECK-SIMILARITY ð^C1;^C2Þ, respectively. In the same way, lines 11 and 12 should be substituted bylines 4 and 5. For the case of no similarity, lines 4 and 5 in the functionC
OMPARE ð^C1;^C 2Þshould be substituted by lines 7 and 8 in the functionC
HECK-SIMILARITY ð^C1;^C2Þ.In the same way lines 11 and 12 should be substituted by lines 7 and8. Let^R
1and^R 2be the set of the cluster representatives of ^C 1and^C 2, respectively such that/C12/C12/C12^R
1/C12/C12/C12¼pand/C12/C12/C12^R
2/C12/C12/C12¼q, wherep,q/C211. For algo- rithmic convenience, let assume that q/C21p.Let^R
1¼f^r 1igpi¼1and^R 2¼ f^r
2jgqj¼1be the cluster representatives of^C 1and^C 2, respectively. The value maxSWrefers to the value returned by the Smith-Watermannalgorithm.
Algorithm 1All/C0Three Algorithms.
1:Initialization:Two cluster leaf records (C l,C l) 2:procedureC
ONSTANT THRESHOLD (^C1l;^C2l)\(⊳\) CT algorithm(continued on next column)Algorithm 1(continued)
3: Compare (^C 1l,^C 2l)\(⊳\) comparison of^C 1lwith^C 2l
4:if(^C 1l,^C 2l) are very similarthen 5:^C
new←Merge (^C 1l,^C 2l)6: Remove^C
2l
7:^R new←^R 1l\(⊳\) cluster rep.←rep. of^C 1l
8:end if9:if^C
1l,^C 2lare similarthen10:^R
new←Merge (^C 1l,^C 2l) 11: Remove^C
2l
12:^R new←^R 1l[^R 2l\(⊳\) cluster rep.←rep. of^C 1l[rep. of^C 2l
13:end if14:if(^C
1l,^C 2l) are not similarthen 15: no operation16:end if17: Goto step 1 and compare ( ^C
1l,^C 2r) 18: Goto step 1 and compare ( ^C
1r,^C 2l) 19: Goto step 1 and compare ( ^C
1r,^C 2r) 20:return(^R
1l;^R2l)\(⊳\) returns cluster representatives21:end procedure1:procedureV
ARIABLE THRESHOLD (^C1,^C 2)\(⊳\) VT algorithm 2: Compare andﬁndSWof^r
1and^r 2\(⊳\) at startθ 1,θ2are 0.5 for^C 1,^C 2
3:if(SW>θ 1or (SW>θ 2)then4:if(θ
1>θ2)then5: Merge (^C
1,^C 2)6: Remove^C
2
7: Set representative←^r 1\(⊳\) update representative to^r 1
8:θ 1←ðθ 1þSWÞ=2\(⊳\) updateθ 1
9:end if10:if(θ
2/C21θ1)then11: Merge (^C
1,^C 2)12: Remove^C
1
13: Set representative←^r 2\(⊳\) update representative to^r 2
14:θ 2←ðθ 2þSWÞ=215:end if16:else17:^C
1and^C 2are not similar18: no operation19:end if20:end procedure1:procedureF
UNCTION THRESHOLD (^C1l;^C2l)\(⊳\) FT algorithm 2: Find maxSWbetween^C
1land every cluster in^T 2
3: Suppose maxSWis between^C 1land^C 2r
4:if(maxSW>θ u)then5: Merge (^C
1l,^C 2r)6: Remove^C
1l
7: Representative←representative of^C 2r
8:end if9:if(θ
c/C20maxSW<θ u)then10: Merge (^C
1l,^C 2r)11: Remove^C
2r
12: Representative←Union of representatives13:end if14:if(maxSW<COMPUTE(θ
c)then 15: no operation16:end if17:end procedure1:functionC
OMPARE (^C1;^C2)\(⊳\) very similar case 2: Compare andﬁndSWof^r
11with eachf^r 2jgqj¼1
3: Find maxSW;m←maxSW4:ifat any time (max SW>θ
u)then 5: (^C
1,^C 2) are very similar6: Stop and return maxSW7:end if8:if(maxSW<θ
u)then9: Compare andﬁndSWof^r
12with eachf^r 2jgqj¼1
10: Find maxSW; start with maxSWequal to m 11:ifat any time (maxSW>θ
u)then(continued on next page)A. Fellah Array 11 (2021) 100070
7Algorithm 1(continued)
12: (^C 1,^C 2) are very similar13: Stop and return maxSW14:end if15:end if16:end function1:functionC
OMPUTE (θc)2:t←0.3 (1/C0θ
u)\(⊳\)θ u: Monge-Elkan's threshold3:x←the number of representative in ^C
2r/C01 4: Letm←minimum(x, maximum number of allowed representatives)5: D←t/(maximum number of allowed representatives)6:θ
c←2/C2t/C0m/C2D7:returnθ
c
8:end function1:functionC
HECK SIMILARITY (^C1;^C2)\(⊳\) Check degree of similarity: \(⊳\) very similar, similar, not similar2:if(maxSW>θ
u)then3: (^C
1,^C 2) are very similar4:end if5:if(maxSW/C20θ
u) and (maxSW/C21θ l)then 6: (^C
1,^C 2) are similar7:end if8:if(maxSW<θ
l)then9: (^C
1,^C 2) are not similar10:end if11:end function1:functionC
OMPARE LEFT-RIGHTSUBTREES (^C1l;ð^C 2l;^C2rÞ) 2: Find maxSWbetween^C
1land every cluster in^T 2\(⊳\) similar to function C OMPARE
3: Suppose maxSWis between^C 1land^C 2r
4:if(maxSW>θ u)then5: Merge (^C
1l,^C 2r)6: Remove^C
1l
7: Representative←representative of^C 2r
8:end if9:if(θ
c/C20maxSW<θ u)then10: Merge (^C
1l,^C 2r)11: Remove^C
2r
12: Representative←Union of representatives13:end if14:if(maxSW<θ
c)then15: do nothing16:end if17:end function
Without loss of generality, let assume ^C 1land^C 1rbe the clusters whose root^T
1; and^C 2land^C 2rbe the clusters rooted in the rightsubtree^T
2of^T, ordered asð^C 1l;^C1rÞ;ð^C 2l;^C2rÞ. As a result ofTheorem 6.1, we state the following results:Corollary 7.1.NEAR/C0OPT
DUPTREE ð^C1l;iÞ¼8<:^C
1l ifi¼1 NEAR/C0OPT
DUPTREE/C16^Cl2;j/C17[NEAR/C0OPT DUPTREE/C16^Cr2;i/C0j/C17ifi>1wherej¼argmin
1/C20j<ig/C16NEAR/C0OPT DUPTREE/C16^Cl2;j/C17[NEAR/C0OPT DUPTREE/C16^Cr2;i/C0j/C17/C17Corollary 7.2.NEAR/C0OPT
DUPTREE ð^C 1r;iÞ¼8<:^C
1r ifi¼1 NEAR/C0OPT
DUPTREE/C16^Cl2;j/C17[NEAR/C0OPT DUPTREE/C16^Cr2;i/C0j/C17ifi>1wherej¼argmin1/C20j<ig/C16NEAR/C0OPT DUPTREE/C16^Cl2;j/C17[NEAR/C0OPT DUPTREE/C16^Cr2;i/C0j/C17/C178. Evaluation metrics and experiments8.1. Evaluation metricsThe main goal of this experimental evaluation is to compare theperformance in terms of accuracy and effectiveness of all four algorithmscovered in this work, ME, CT, VT, and FT algorithms. We adopt the pu-rity, inverse purity, and F-measure metrics for our extensive evaluation.Denote by^C¼^C
1;^C 2;…;^C qthe set of the true actual clusters wherewe refer to each^C
ias a class and letC¼C 1;C 2;…;C kbe the set of near-duplicate clusters to be evaluated by the SW algorithm. Then theprecision,Pð^C
i;C jÞand recallRð^C i;C jÞof^C iwith respect toC jare deﬁned as follows:P/C16^C
i;C j/C17¼/C12/C12/C12^C
i\C j/C12/C12/C12/C12/C12C
j/C12/C12and R/C16^Ci;C j/C17¼/C12/C12/C12^C
i\C j/C12/C12/C12/C12/C12/C12^C
i/C12/C12/C12wherePð^C
i;C jÞ¼RðC j;^C iÞ. Let^nbe the total number of clustered entities, including near-duplicates, purity is formulated by the weightedaverage of the maximum precision values achieved by the clusters on oneof^C
j:fPurityg¼X
kj¼1/C12/C12C
j/C12/C12^nmax
qi¼1P/C16^C i;C j/C17However, inverse purity considers the cluster with maximum recall foreach class^C
i, and it is obtained by taking the weighted average of themaximum recall valuesInversePurity¼X
qi¼1/C12/C12/C12^C
i/C12/C12/C12^nmaxkj¼1R/C18^Ci;C j/C19The values of the purity and inverse purity metrics are within the range of0–1. The higher purity the better is the inverse purity. Purity penalizesclustering noise (i.e., duplicates) in a cluster, which means grouping re-cords incorrectly, but it does not reward grouping records from the same^C
j. If every cluster contains only one record, the maximum purity is 1.However, reversing the role of^CandC, that is inverse purity (C;^C) would penalize splitting records belonging to the same cluster ^C
jinto different clusters. In other words, inverse purity rewards grouping near-duplicates together, but it does not penalize noisy records from different^C
j. In a similar way to precision and recall, there is a tradeoff relation-ship between inverse purity and purity. Inverse purity rewards groupingnear-duplicates and the maximum inverse purity is achieved by puttingall records in one single cluster. For each class ^C
i, the F-measure of that class is:F/C16^C
i;C j/C17¼maxkj¼12/C2P/C16^C i;C j/C17/C2R/C16^C
i;C j/C17P/C16^C
i;C j/C17þR/C16^C
i;C j/C17The F-measure, a combination of purity and inverse purity, is in the rangeof½0;1/C138and a higher F-measure indicates a better clustering. The F-measure, is in the range of½0;1/C138, indicates a better clustering. The F-measure of the clustering [35,36] which computes the weighted averageof maximal F-measure values is deﬁned as:A. Fellah Array 11 (2021) 100070
8F/C0measure¼Xqi¼1F/C16^C i;C j/C17/C12/C12/C12^C
i/C12/C12/C12/C12/C12/C12^C/C12/C12/C128.2. Experiments and data setsAlthough ME and All/C0Three algorithms are provably correct, butverifying the accuracy of an implementation is a challenging task. Withno prepossessing steps, all these algorithms are implemented in C þþ. The experiments were carried out on several publicly available real datasets which cover a spectrum of different data characteristics and sizes. Wehave used four different data sets in our experiments. Three real data setsfrom various sources often used in related research, and the fourth largedata set was generated synthetically (arti ﬁcially) using the near- duplicate generator (NDG) algorithm as described below.NDGgener- ates ten of millions of near-duplicates for each set of real data.
Algorithm 2Near-Duplicate Generator (NDG) Algorithm
1: Remove a random number of characters from the data set record2: Replace a random number of characters with others3: Flip theﬁrst and last attributes (i.e.,ﬂip theﬁrst and last names in the lists) 4: Duplicate a random character. This might be done to more than one character5: Abbreviate randomly - Keep the ﬁrst character but this might be duplicated or removed as above
We ran our experiments on four categories of data set as follows.Voting
1We used the list of registrar voters in British Columbia as ouroriginal small data set with no duplicates. The original total number ofrecords in the voting list is 225 and 975 records. Then we complementeach of the two original voting data sets with a set of near-duplicatesgenerated by theNDGalgorithm. Additionally, we augmented and top-ped the voting data set to 1977 and 3745 references, respectively.Cora
2Cora data set contains bibliographic records and citations inscientiﬁc papers classiﬁed in several classes. The cora citation data set,which consists of a data set of original references and research papers, isoften used in the duplicate detection community. Additionally, weaugmented and topped the cora data set to 21,152 references and 32,005,a substantial larger data set for our experiments.DBLP
3DBLP is the bibliography database for computer science re-cords from the DBLP web site. Each record is a concatenation of authornames(s), title of the publication, some keywords, an abbreviated refer-ence format citation (i.e., journal, book, editor). It consists of 43,935 realobjects, both for relational and XML data. Additionally and for our ex-periments, we augmented the data set to 63,553 references.Synthetic
4The near-duplicate generator algorithm (NDG) is capable of algorithmically generating tens of millions of synthetic records from aset or original records. For instance, the voting, cora and DBLP data setshave been scaled up and topped by tens of thousands of records. Thesynthetic data set of 573,879 has been augmented and topped by NDGto an average of 352.992 records, for an average total of 926.871 recordsover three runs.We generated a random number (0–8) of near-duplicates for each record using theNDGalgorithm for the voting and DBLP data sets, and arandom number (9–20) for the cora data set. Finally, all generated near-duplicates as explained above in theNDGalgorithm are appended to the original data setﬁle. Moreover, we ran theNDGalgorithm three times, for each original real data, to generate three different sets of data (seeTables 1-6). 8.3. Voting lists data sets analysisFirst, we consider a small set of records extracted from the voting list1and list2 of 255 and 975 individuals, respectively. Then we ran threetimes theNDGalgorithm on the voting lists where each run has generateda larger list of near-duplicates. For instance, run
1generated 1058 near-Table 1Detection of near-duplicates in voting list1 data sets.
Voting lists List1 Real Data Set 255 AverageRuns run
1 run 2 run 3 AvgNDGnear-duplicates 1058 947 1002 1002Real data&NDGnear-duplicates 1313 1202 1257 1257ME near-duplicates 1142 1003 1078 1074CT near-duplicates 1054 987 1056 1032VT near-duplicates 1098 988 1003 1029FT near-duplicates 1064 968 998 1010
Table 2Detection of near-duplicates in voting list2 data sets.
Voting lists List2 Real Data Set 975 AverageRuns run
1 run 2 run 3 AvgNDGnear-duplicates 1857 1901 1840 1866Real data&NDGnear-duplicates 2832 2876 2815 2841ME near-duplicates 1633 1818 1702 1717CT near-duplicates 1793 1877 1823 1831VT near-duplicates 1859 1856 1803 1839FT near-duplicates 1855 1902 1857 1871
Table 3Detection of near-duplicates in Cora data sets.
Cora Data Set Cora Real Data Set: 21,152 AverageRuns run
1 run 2 run 3 AvgNDGnear-duplicates 9320 12,754 10,487 10,853Real data&NDGnear-duplicates 30,472 33,906 31,639 32,005ME near-duplicates 11,412 11,674 10,078 11,054CT near-duplicates 9234 12,657 10,456 10,782VT near-duplicates 9341 12,788 10,501 10,876FT near-duplicates 9289 12,768 10,485 10,847
Table 4Detection of near-duplicates in DBLP data sets.
DBLP Data Set DBLP Real Data Set: 43,935 AverageRuns run
1 run 2 run 3 AvgNDGnear-duplicates 21,134 19,345 18,376 19,618Real data&NDGnear-duplicates 65,069 63,280 62,311 63,553ME near-duplicates 22,160 20,018 19,102 20,426CT near-duplicates 21,203 19,177 18,723 19,643VT near-duplicates 21,119 19,256 18,303 19,559FT near-duplicates 21,147 19,257 18,362 19,589
Table 5Detection of near-duplicates in Synthetic data sets.
Synthetic Data Set Synthetic Data Set: 573,879 AverageRuns run
1 run 2 run 3 AvgNDGnear-duplicates 344,516 364,890 349,571 352.992Real data&NDGnear-duplicates 918,395 938,769 923,450 926.871ME near-duplicates 348,602 361,768 352,459 354.276CT near-duplicates 342,823 363,239 348,697 351.586VT near-duplicates 343,945 363,885 348,880 352.236FT near-duplicates 344,323 364,299 349,105 352.575
1http://ww.rootsweb.ancestry.com/canbc.vote898/votea.html .
2http://www.cs.umass.eduand mccallum/code-data.html.
3http://www.informatik.uni-trier.de/ ley/db/.
4Algorithm 2: The near-duplicate generator ( NDG) algorithm.A. Fellah Array 11 (2021) 100070
9duplicates for a total of 1313 records. That is, the original real voting list1has been topped by 1058 near-duplicates ( Table 1). Similarly, the orig- inal real voting list2 has been topped by 1857 for a total of 2832(Table 2).Both tables show the results of theNDGalgorithm on the voting lists over three runs. We carried out the experiments by running the four al-gorithms, ME, CT, VT, and FT on the total number of records. The realdata is topped by duplicates generated byNDG(4th row ofTables 1 and 2). Then we checked whether these algorithms identify the near-duplicates accurately. ME and All/C0Three algorithms performed more or less accurately on small data set sizes (1257 and 2841 voting records)because of the small amount of noise added to the data set. For instance,on small-sized data such as list2, CT and VT algorithms show an accuracyof 98.34% on the average with 31 false positives ((1831/1866 þ1839/ 1866)/2¼98.34%). However, FT algorithm's accuracy is 100.27%where the extra 0.27% represents few false negatives on the average inlist2 (Figs. 1 and 2in Appendix). The ME algorithm accuracy is 92.02%with 149 false positives. Still, the ME accuracy is behind the performanceof All/C0Three Algorithms.8.4. Cora and DBLP data sets analysisOn the other types, medium- and large-sized data, All /C0Three algo- rithms consistently outperformed the ME algorithm in terms of accuracy.On the negative side, the ME algorithm added 2092 false negatives inrun
1, but converged to 201 false negatives on the average due to read-justing the threshold to 0.8 set by the algorithm ( Fig. 3in Appendix). Overall, the ME algorithm added an extra 1.85% of false negatives overthe 32,005 records (Table 3). The Merge/C0Filter/C0RC paradigm com- plemented by the comparison and construction of cluster representativesare very noticeable with a high similarity threshold which helped toimprove the accuracy of All/C0Three algorithms (Table 3,Fig. 3in Ap- pendix). This has been shown in the FT algorithm with only an average of6 false positives (99.95% of accuracy) on the cora data set of 32,005records. Furthermore, the FT algorithm added only 29 false positives(99.85 of accuracy) on the DBLP data set of 63,553 records ( Table 4, Figure 4in Appendix). The performance of All/C0Three algorithms is the most substantial on DBLP with an average of 63,553 records wheremerging andﬁltering clusters’representatives is accurately achievedthroughout the similarity variable and function thresholds. In particular,the VT and FT algorithms falsely detected an average of only 59 and 29false positives, respectively. That is, an accuracy of 99.70% and 99.86%(Figs. 3 and 4in Appendix). However, the ME algorithm is far behindwith an average of 808 (4.12%) false negatives.8.5. Synthetic data set analysisAnother important observation is that the tuning parameters that weinserted in both ME and SW algorithms overall added some extra sig-niﬁcant accuracy to the near-duplicate detection. For instance, and overthe 926,871 synthetic data set (Table 5,Figure 5in Appendix). All/C0Three algorithms detected only 860 (0.092%) false negatives. That is, an ac-curacy of 99.90%. The ME algorithm performed also quite well over926,871 synthetic large-sized data set with 1284 (0.14%) false negative,that is, an accuracy of 99.86%. Overall, ME and CT algorithms hadmissed to identify a very small number of false positives. That is, 0.14%and 0.15% over 926,871 records. On the DBLP data set, All /C0Three al- gorithms, in particular VT and FT algorithms, outperform the MEalgorithm.8.6. Performance evalution: purity, inverse purity and F-measureWith an extensive experimental analysis, the purity, inverse and F-measure achieved a value of nearly 1.0, a precision which outperformsthe seminal work of Monge-Elkan. For instance, in the ME algorithmwhich is based on Jaro-Winkler's metric and using the attribute nameprovided by DBLP, the maximum F-measure is at threshold 0.8 androbust up to 0.9. The precision drops steadily below 0.8 due to many falsepositives. However, with the same algorithm based on Smith-Waterman'smetric, the maximum F-measure is at threshold 0.9 and the SW precisionis within the interval½0:9⋯1:0/C138due to preﬁxes and sufﬁxes which are ignored in some references. For the same algorithm and for differentmetrics (i.e., Jaro-Winkler, Smith-Waterman, Levenshtein), the perfor-mance in terms of F-measure, precision and recall are quite different if weconsider only the attribute afﬁliation in the experiment (Table 6,Figure 6 thruFigure 10in Appendix). As needed, the near optimal threshold wasnot evaluated only once, but reevaluated and recon ﬁgured if necessary as records are added to clusters as illustrated in the set of ﬁgures in appendix.9. ConclusionIn this paper, we have introduced a near-duplication detectionframework by improving Monge-Elkan's algorithm and also including anafﬁne variant of the Smith-Waterman's algorithm to reduce the numberof record comparisons. We have investigated and implemented a familyof algorithms–constant threshold (CT), variable threshold (VT) andfunction threshold (FT) which are collectively referred to as All /C0Three algorithms, all based on the merge-ﬁlter cluster representatives Mer- ge/C0Filter/C0RC approach. Our experiments with real-world and generateddata sets have shown substantial gains in accuracy and potential ef ﬁ- ciency to detect near-duplicates in very large data sets and acrossdifferent domains.As we have predicted in this investigation, the performance of Monge-Elkan's algorithm which is mainly evaluated by the number of near-duplicates within their correct clusters would not be as good and accu-rate as expected. This is due to severalﬂaws in the algorithm, including priority queue and union set structures, record comparison algorithms,restricted clustering methods, and other parameters. The Mer-ge/C0Filter/C0RC approach, complemented with the three classes of algo-rithms achieves a value of nearly 1.0, a precision which is nearly perfectand outperforms the seminal work of Monge-Elkan. In each run of theexperiment, the major evaluation metric is the accuracy measured interms of near-duplicate clusters. The effect of imperfection in terms ofnumber of clusters and accuracy is reﬂected in cluster miss-classiﬁcation with increasing data set sizes in Monge-Elkan's algorithm. All of theseaffect the precision, recall, and F-measure metrics. We believe that thecutoff and optimization values of threshold with other key tuningTable 6ME, CT, VT and FT algorithms: Performance evaluation of purity, inverse purityand F-measure.
Data set near-duplicates Purity Inverse F-measurePerformance PurityVoting List1 ME 0.893 0.839 0.702CT 0.986 0.913 0.956VT 0.980 0.952 0.968FT 0.996 0.998 0.997Voting List2 ME 0.812 0.907 0.821CT 0.965 0.988 0.929VT 0.989 0.995 0.967FT 0.996 0.899 0.998Cora ME 0.925 0.97 0.890CT 0.968 0.988 0.929VT 0.986 0.992 0.943FT 0.978 0.993 0.998DBLP ME 0.815 0.934 0.867CT 0.978 0.988 0.932VT 0.996 0.985 0.985FT 0.999 0.979 0.998Synthetic ME 0.945 0.957 0.921CT 0.937 0.986 0.959VT 0.964 0.983 0.994FT 0.982 0.990 0.991A. Fellah Array 11 (2021) 100070
10parameters throughout semi-supervised machine learning will signi ﬁ- cantly improve the quality of the accuracy and effectiveness of detectingnear-duplicate clusters. This is being investigated as the likely directionand outlook of our current research.AcronymsMerge/C0Filter/C0RC Merge-Filter Representative-based ClusteringCT Constant ThresholdVT Variable ThresholdFT Function ThresholdAll/C0Three Constant Threshold, Variable Threshold, Function ThresholdME Monge-ElkanSW Smith-WatermanSNM Sorted Neighborhood MethodNDGNear-duplicate GeneratorOPTðAÞOptimal Local AlignmentðAÞNEAR/C0OPT
DUPTREE ð^C;iÞnear-optimal duplicate subtree for the node^Cusing i clustersCredit author statementCRediT (Contributor Roles Taxonomy) was introduced with theintention of recognizing individual author contributions, reducingauthorship disputes and facilitating collaboration. The idea came aboutfollowing a 2012 collaborative workshop led by Harvard University andthe Wellcome Trust, with input from researchers, the InternationalCommittee of Medical Journal Editors (ICMJE) and publishers, includingElsevier, represented by Cell Press.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsI acknowledge the collaboration of Dr. Maamir Allaoua who collab-orated with me on several interrelated published papers. He is nowretired from the Dept. of Computer Science, University of Sharjah,Sharjah UAE. I would also like to thank two graduate students, SepidehPashami and Serveh Ghaderi, who spent one entire term carrying out theexperiments and programming parts reported in this paper. I also thankProfessors A. Elmagarmid and V. Verykios for providing me with some ofthe original benchmark data sets.Appendix
Fig. 1.Voting list1 data set: True duplicates vs. ME, CT, VT, and FT Algorithms
Fig. 2.Voting list2 data set: True duplicates vs. ME, CT, VT, and FT AlgorithmsA. Fellah Array 11 (2021) 100070
11Fig. 3.Cora data set: True duplicates vs. ME, CT, VT, and FT Algorithms.
Fig. 4.DBLP data set: True duplicates vs. ME, CT, VT, and FT Algorithms.
Fig. 5.Synthetic data set: Optimal duplicates vs. ME, CT, VT, and FT AlgorithmsA. Fellah Array 11 (2021) 100070
12Fig. 6.Voting list1 data set: Optimal measures vs. ME, CT, VT, and FT measures
Fig. 7.Voting list2 data set: Optimal duplicates vs. ME, CT, VT, and FT measures
Fig. 8.Core data set: Optimal duplicates vs. ME, CT, VT, and FT measuresA. Fellah Array 11 (2021) 100070
13References
[1]Newcombe H, Kennedy J, Axford S, James A. Automatic linkage of vital records.J Sci 1959;130(3881):954–9. [2]Christen P. A survey of indexing techniques for scalable record linkage anddeduplication. IEEE Transactions on Knowledge and Data Engineering (TKDE 2012;24(9):1537–55.[3]Jaro M. Advances in record-linkage methodology as applied to matching. J Am StatAssoc 1989;84(406):414–20. [4]Monge A. Adaptive detection of approximately duplicate database records and thedatabase integration approach to information discovery. Ph.D. thesis, Ph.D. Thesis.San Diego: Dept. of Comp. Sci. and Eng., Univ. of California; 1997 . [5]Whang S, Menestrina D, Koutrika G, Theobald M, Garcia-Molina H. Entityresolution with iterative blocking. In: Proceedings of the ACM internationalconference on management of data. SIGMOD); 2009. p. 219 –32.[6]Weis M, Naumann F, Brosy F. A duplicate detection benchmark for xml (andrelational) data. In: Proceedings of the SIGMOD inter. Workshop on informationquality for information systems. IQIS); 2004. p. 10 –9. [7]Yan S, Lee D, Kan M, Giles L. Adaptive sorted neighborhood methods for ef ﬁcient record linkage. In: Proceedings of the 7th ACM/IEEE-CS joint conf. on Digitallibraries; 2007. p. 185–94. [8]Hernandez M, Stolfo S. The merge/purge problem for large databases. In:Proceedings of the ACM SIGMOD international conference on management of data;1995. p. 127–38.[9]Papenbrock T, Naumann F, Heise A. Progressive duplicate detection. IEEE TransKnowl Data Eng 2018;27(5). 1316 –132. [10]Draisbach U, Naumann F. On choosing thresholds for duplicate detection. In:Proceedings of the 18th international conference on information quality. ICIQ);2013. p. 37–45.[11]Elmagarmid A, Ipeirotis P, Verykios V. Duplicate record detection: a survey. IEEETrans Knowl Data Eng 2007;19(1):1 –16. [12] Chen Q, Zobel J, Verspoor K. Duplicates, redundancies and inconsistencies in theprimary nucleotide databases: a descriptive study, Jounal of Biological databasesand curation. 2017. p. 2–16.https://doi.org/10.1093/database/baw163 .Fig. 9.DBLP data set: Optimal duplicates vs. ME, CT, VT, and FT measures
Fig. 10.Synthetic data set: Optimal duplicates vs. ME, CT, VT, and FT measuresA. Fellah Array 11 (2021) 100070
14[13]Xiao C, Wang W, Lin X, Yu J, Wang G. Ef ﬁcient similarity joins for near-duplicate detection. ACM Trans Database Syst 2011;36(3):15 –41. [14]Monge A, Elkan C. Domain-independent algorithm for detecting approximatelyduplicate database records. In: Proceedings of the ACM SIGMOD workshop onresearch issues on data mining and knowledge discovery. DMKD); 1997. p. 23 –9. [15]Fellah A, Maamir A. A domain independent methodology for near-duplicatedetection. In: Proceedings of the international conference on applied computing,madrid Spain; 2012. p. 139–46. [16]Navarra G. A guided tour to approximate string matching. ACM Comput Surv 2001;33(1):31–88.[17] D. Moreira, al, Image provenance analysis at scale, IEEE Trans Image Process 27(12).[18]Fellah A, Maamir A. Near-optimal domain independent approach for detectingduplicates. In: Proceedings of the 19th international conference on data mining,multimedia and image processing. Paris France; 2017. p. 2633 –42. [19]Bharambe D, Jain S, Jain A. A survey: detection of duplicate record. InternationalJournal of Emerging Technology and Advanced Engineering 2012;2(11):298 –307. [20]Hassanian-esfahani R, Kargar Mj. Sectional minhash for near-duplicate detection.Expert Syst Appl 2018;99(1):203 –12. [21]Herschel M, Naumann F, Szott S, Taubert M. Scalable iterative graph duplicatedetection. IEEE Trans Knowl Data Eng 2012;4(11). 2294 –2108. [22]Naumann F, Herschel M. An introduction to duplicate detection. Synthesis Lectureson Data Management 2010;2(1):1 –87. [23]Winkler W. Overview of record linkage and current research directions. A StatisticalResearch Division, U.S. Census Bureau; 2006. p. 1 –44. [24]Winkler W. Approximate string comparator search strategies for very largeadministrative lists, A Statistical Research Report Series (Statistics 2005-02). U.S.Census Bureau; 2005. p. 1–9. 02. [25]Baxter R, Christen P, Churches T. A comparison of fast blocking methods for recordlinkage. In: Proceedings of the ACM SIGKDD workshop on data Cleaning,Recordlinkage, and object consolidation; 2003. p. 25 –8. [26]Monge A. Matching algorithms within a duplicate detection system. IEEE DataEngineering Bulletin 2000;23(4):14 –20. [27]Yandrapally R, Stocco A, Mesbah A. Near-duplicate detection in web app modelinference. In: Proceedings of the ACM/IEEE 42nd international conference onsoftware engineering; 2020. p. 186 –97.[28]Thyagharajan KK, Kalaiarasi G. A review on near-duplicate detection of imagesusing computer vision techniques. Arch Comput Methods Eng 2021;28:897 –916. [29]Draisbach U, Naumann F, Szott S, Wonneberg O. Adaptive windows for duplicatedetection. In: Proceedings of the IEEE 28th international conference on data
engineering. ICDE); 2012. p. 1073 –83. [30]Jaro M. Probabilistic linkage of large public. Journal of Statistics in Medicine 1995;84(406):414–20.[31]Hernandez M, Stolfo S. Real-world data is dirty: data cleansing and the merge/purge problem. Data Min Knowl Discov 1998;2(1):9 –37. [32]Kopcke H, Rahm E. Frameworks for entity matching: a comparison. Data Knowl Eng2010;69(2):197–210.[33]Papadakis G, Svirsky J, Gal A, Palpanas T. Comparative analysis of approximateblocking techniques for entity resolution. In: Proceedings of the VLDB endowment.PVLDB); 2016. p. 684–95. [34]Papadakis G, Alexiou G, Papastefanatos G, Koutrika G. Schema-agnostic vs schema-based conﬁgurations for blocking methods on homogeneous data. In: Proceedingsof the VLDB endowment. PVLDB); 2015. p. 312 –23. [35]Vogel T, Heise A, Draisbach U, Lange D, Naumann F. Reach for gold: an annealingstandard to evaluate duplicate selection results. ACM Journal of Data andInformation Quality 2014;5:1 –22. [36]Vogel T, Naumann F. Instance-based “one-to-some”assignment of similarity measures to attributes. In: Proceedings of the international conference oncooperative information systems. CoopIS); 2011. 412 –0420. [37]Smith T, Waterman M. Identiﬁcation of common molecular subsequences. J Mol Biol 1981;147. 195–107. [38]Needleman S, Wunsch C. General method applicable to the search for similarities inthe amino acid sequence of two proteins. J Mol Biol 1970;48:443 –53. [39]Deepa K, Rangarajan R, Selvi M. Automatic threshold selection using pso for gabased duplicate record detection. Int J Comput Appl 2013;62(4):181 –7. [40]dos Santos J, Heuser A, Moreira V, Wives L. Automatic threshold estimation for datamatching applications. Inf Sci 2011;181(13):2685 –99. [41]Cheng D, Kannan R, Vempala S, Wang G. A divide-and-merge methodology forclustering. ACM Trans Database Syst 2006;31(4):1499 –525. [42]Li M, Wang H, Li J, Gao H. Efﬁcient duplicate record detection based on similarity estimation. In: Proceedings of the 11th inter. Conf. on Web-page and InformationManagement (WAIM); 2010. p. 595 –607.A. Fellah Array 11 (2021) 100070
15