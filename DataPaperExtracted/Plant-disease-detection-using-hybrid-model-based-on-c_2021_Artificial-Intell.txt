Plant disease detection using hybrid model based on convolutionalautoencoder and convolutional neural network
Punam Bedi, Pushkar Gole ⁎
Department of Computer Science, University of Delhi, Delhi, India
abstract article info
Article history:Received 29 September 2020Received in revised form 7 May 2021Accepted 8 May 2021Available online 11 May 2021
Keywords:Plant disease detectionConvolutional autoencoderConvolutional neural networkDeep learning in agriculturePlants are susceptive to various diseases in their growing phases. Early detection of diseases in plants is one of themost challenging problems in agriculture. If the diseases are not identi ﬁed in the early stages, then they may ad- versely affect the total yield, resulting in a decrease in the farmers' pro ﬁts. To overcome this problem, many re- searchers have presented different state-of-the-art systems based on Deep Learning and Machine Learningapproaches. However, most of these systems either use millions of training parameters or have low classi ﬁcation accuracies. This paper proposes a novel hybrid model based on Convolutional Autoencoder (CAE) network andConvolutional Neural Network (CNN) for automatic plant disease detection. To the best of our knowledge, a hy-brid system based on CAE and CNN to detect plant diseases automatically has not been proposed in any state-of-the-art systems present in the literature. In this work, the proposed hybrid model is applied to detect BacterialSpot disease present in peach plants using their leaf images, however, it can be used for any plant disease detec-tion. The experiments performed in this paper use a publicly available dataset named PlantVillage to get the leafimages of peach plants. The proposed system achieves 99.35% training accuracy and 98.38% testing accuracyusing only 9,914 training parameters. The proposed hybrid model requires lesser number of training parametersas compared to other approaches existing in the literature. This, in turn, signi ﬁcantly decreases the time required to train the model for automatic plant disease detection and the time required to identify the disease in plantsusing the trained model.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionIndia is an agrarian country, and a major part of its economy de-pends on the agricultural sector. The share of agriculture in the IndianGross Domestic Product (GDP) and total exports are 16% and 10%, re-spectively (Himani, 2014). About 75% population of India depends onthe agricultural sector either directly or indirectly ( Himani, 2014). Therefore, disease-free good quality crop production is essential forthe growth of the country's economy.Like human beings, plants are also susceptible to various kinds ofdiseases in their different phenophases. Consequently, the total cropyield and hence the net proﬁt of the farmer are adversely affected. Inorder to address this issue, the early detection of plant diseases is neces-sary. Manual disease detection in plants is done either by farmers or byagricultural scientists. However, this is a very challenging and time-consuming task. To address this problem, many researchers across theglobe presented different state-of-the-art systems for automatic plantdisease detection with the help of various Machine Learning ( Ahmed et al., 2019;Naik and Sivappagari, 2016;P a n i g r a h iK s h y a n a p r a v aPanda et al., 2020;RAO et al., 2020), and Deep Learning techniques(Ashraf and Khan, 2020;Chen et al., 2020;Karlekar and Seal, 2020; Kim et al., 2020). These state-of-the-art systems use a very high numberof training parameters. Consequently, the training time and the predic-tion time of these systems are very high, or they require a machine withhigh computation powers. This research work tries to reduce the num-ber of features used for prediction using the CAE network without a sig-niﬁcant decrease in the classiﬁcation accuracy of plant diseasedetection. This, in turn, decreases the number of training parametersby a signiﬁcant factor resulting in the decrease of training and predic-tion time.Deep Learning techniques are inspired by the architecture of neu-rons present in the human brain (Haykin, 1998). These techniques use Artiﬁcial Neural Networks (ANNs), and its other variants, such asConvolutional Neural Networks (CNNs) and Recurrent Neural Networks(RNNs) to identify the hidden structures in data. There are two promi-nent advantages of Deep Learning techniques over the Machine Learn-ing techniques. First, they automatically extract various features fromraw data, and hence there is no need for an extra feature extractionmodule. Second, Deep Learning techniques reduce the amount of timerequired to process large datasets of high dimensions. Therefore, theDeep Learning techniques are used to build the proposed hybrid model.Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
⁎Corresponding author.E-mail addresses:pbedi@cs.du.ac.in(P. Bedi),pgole@cs.du.ac.in(P. Gole).
https://doi.org/10.1016/j.aiia.2021.05.0022589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/Convolutional Neural Networks (CNNs) and ConvolutionalAutoencoders (CAEs) are two Deep Learning techniques used in manycomputer vision applications due to their effectiveness on image data.Both these techniques use convolution operation to extract various spa-tial and temporal features from image data. CNNs are used to classifyinput images to their respective classes, whereas CAEs are used to re-duce the dimensionality of an image efﬁciently.This paper proposes a novel hybrid model for automatic plant dis-ease detection based on CAE and CNN with fewer training parametersas compared to other state-of-the-art systems present in the literature.Although there are various techniques present in the literature used forautomatic plant disease detection, but a hybrid system of CAE combinedwith a CNN has not been proposed until now in any existing researchwork to the best of our knowledge. Dimensionality reduction usingCAE in the proposed model results in reduction of number of trainingparameters of the model. In order to test the proposed hybrid model,it is applied to detect Bacterial Spot disease in peach plant, which iscaused by a bacterium named Xanthomonas Campestris. This modelcan be used to detect other plant diseases, as well.The rest of the paper is divided into four sections. Section 2discuss several state-of-the-art systems for automatic plant disease detectionpresent in the literature. InSection 3, the material and methods used to design proposed hybrid model is described. The results obtainedby the model to detect Bacterial Spot disease in peach plants are pre-sented inSection 4of the paper. In the end,Section 5concludes the paper.2. Related workMany researchers across the world explored different MachineLearning (Es-saady et al., 2016;Islam et al., 2017;Krithika and Selvarani, 2017;Padol and Yadav, 2016), Deep Learning (Golhani et al., 2018;Ramesh and Vydeki, 2020;Sharma et al., 2019), Image Pro- cessing (Marwaha et al., 2012;Ngugi et al., 2020;Qin et al., 2016;Tewari et al., 2020), Soft Computing (Singh, 2019;Singh and Misra, 2017), and Semantic Web-based (Jearanaiwongkul et al., 2018;Marwaha et al.,2009) techniques to automate plant disease detection. This section dis-cusses some state-of-the-art systems present in the literature used forautomatic plant disease detection.Sanga et al. (Sanga et al., 2020) developed a disease detectiontool for banana plants withﬁve different CNN architectures. Thesearchitectures were VGG-16, ResNet-152, ResNet-50, ResNet-18, andInceptionV3. They found that ResNet-152 outperformed others withan accuracy of 99.2%. They also developed a mobile application so thatfarmers could easily detect diseases in banana plants by uploading leafimages of their banana plants with their smartphones. This mobile ap-plication used the InceptionV3 model for disease prediction with 99%conﬁdence. The number of training parameters used by their bestperforming model, i.e., ResNet-152, was 60 million, as mentioned inthe ResNet paper (He et al., 2016). Another similar work was done inthe paper authored by Chohan et al. ( Chohan et al., 2020). They employed VGG-19 and InceptionV3 CNN architectures for automaticplant disease detection using the PlantVillage dataset. In their researchwork, they also used data augmentation to enlarge the dataset arti ﬁ
- cially. The VGG-19 model outperformed the InceptionV3 model with98% training accuracy and 95% testing accuracy, as claimed by themin their paper. The number of training parameters used by theirbest performing model, i.e., VGG-19, was 143 million, as claimed by(Simonyan and Zisserman, 2015) in their research work.Ferentinos (Ferentinos, 2018) employedﬁve different modern CNN architectures named AlexNet, AlexNetOWTBn, GoogLeNet, Overfeat,and VGG for plant disease detection using the PlantVillage dataset. Inhis paper, he found that VGG outperformed other CNN architectureswith an accuracy of 99.5% using 138 million trainable parameters, asmentioned in the VGGNet paper (Simonyan and Zisserman, 2015). Mohanty et al. (Mohanty et al., 2016) analyzed AlexNet and GoogLeNetCNN architectures' performance for plant disease detection using thePlantVillage dataset. They performed 60 different experiments withthe help of 60 different conﬁgurations. They found that GoogLeNetwith transfer learning performed best with an accuracy of 99.3%. Thenumber of training parameters used by GoogLeNet was around 7 mil-lion, as claimed in their paper (Szegedy et al., 2015).Table 1Summary of various research work.Author and year Dataset used Best model Testing accuracy Number of training parametersSanga et al. (2020)Banana leaf images obtained from banana ﬁeld ResNet-152 99.2% 60 million Chohan et al. (2020)PlantVillage VGG-19 98.3% 143 millionFerentinos (2018)PlantVillage VGGNet 99.5% 138 millionMohanty et al. (2016)PlantVillage GoogLeNet 99.3% 7 millionMohameth et al. (2020)PlantVillage ResNet-50 + SVM 98% 25 millionTiwari et al. (2020)Potato leaf extracted from PlantVillage dataset VGG-19 + Logistic Regression 97.8% 143 millionKhamparia et al. (2020)Tomato, potato, and maize leaf extracted from PlantVillage CAE 86.78% 3.3 million
Fig. 1.The architecture of a typical CNN.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
91Mohameth et al. (Mohameth et al., 2020) used different modern CNN architectures and different classiﬁers for automatic plant disease detection on the PlantVillage dataset. They employed VGG-16, ResNet-50, and GoogLeNet CNN architectures for feature extraction, and forclassiﬁcation, they used k-Nearest Neighbour and Support Vector Ma-chine (SVM) classiﬁers. They observed that SVM with ResNet-50outperformed others with an accuracy of 98%. As mentioned in theResNet paper, the number of training parameters used by ResNet-50was approximately 25 million (He et al., 2016). Similar work was also done by Tiwari et al. (Tiwari et al., 2020). They proposed an automatic disease detection system for potato plants. This system used differentCNN architectures such as VGG-19, VGG-16, and InceptionV3 for featureextraction and different classiﬁers such as Logistic Regression, k-NearestNeighbour classiﬁer, Support Vector Machine (SVM), and Neural Net-work for disease detection. They concluded that VGG-19 with LogisticRegression outperformed others with an accuracy of 97.8%. The numberof training parameters used by VGG-19 was approximately 143 million,as claimed in the VGGNet paper (Simonyan and Zisserman, 2015). Khamparia et al. (Khamparia et al., 2020) proposed a Deep Convolutional Encoder Network system for seasonal crops disease iden-tiﬁcation. They considered 900 leaf images of three crops: potato, to-mato, and maize, distributed in six classes (i.e., ﬁve diseased and one healthy). They achieved 100% training accuracy while the testing accu-racy of their model was 86.78%. Since the training accuracy was muchhigher as compared to the testing accuracy, so there was a chance thatthe trained model overﬁtted on the training data. In their paper, theyhave also mentioned that their system used approximately 3.3 milliontraining parameters, which is much higher than the number of trainingparameters 9,914 used in the proposed work. They have usedAutoencoder and CNN for seasonal crop disease identi ﬁcation. On the other hand, the proposed novel hybrid model is based on CAE andCNN. Moreover, the proposed model also achieves higher testing accu-racy than the testing accuracy of the model proposed by Khampariaet al. (Khamparia et al., 2020). Pardede et al. (Pardede et al., 2018)d e - signed a system for automatic disease detection for corn and potatoplants with CAE and SVM classiﬁers' help. They extracted leaf imagesof potato and corn plants from the PlantVillage dataset. They achieved87.01% and 80.42% accuracy in detecting diseases in potato and cornplants, respectively.As per our knowledge, state-of-the-art systems available in the liter-ature for plant disease detection use a very high number of training pa-rameters ranging from 3.3 million to 143 million ( He et al., 2016; Khamparia et al., 2020;Simonyan and Zisserman, 2015;Szegedy et al., 2015). A summary of these research works has been presented inTable 1.All of the above-discussed research works have a major disadvan-tage that all research works used a very high number of training param-eters. Moreover, training a model with a very high number of trainingparameters requires either a lot of training time or a machine withhigh computation power. This motivated us to work towards reducingthe number of training parameters used for plant disease detectionwithout much decrease in the classiﬁcation accuracy. Hence in this paper, a novel hybrid model is proposed that reduces the dimensional-ity of input leaf image using CAE before classifying it using CNN for plantdisease detection. The dimensionality reduction of plant leaf images be-fore classiﬁcation reduces the number of training parameters by a sig-
niﬁcant factor, which is the majorﬁnding of this research work.3. Material and methodsThis section discusses the material and methods which are used todesign the model.Section 3.1provides a basic understanding of CNNand CAE, which is helpful to understand the proposed work. InSection 3.2, the proposed hybrid model is described in detail. The exper-imental conﬁguration to implement the proposed model is presented inSection 3.3.3.1. Background conceptsThis section describes the basic concepts of Convolutional NeuralNetwork (CNN) and Convolutional Autoencoder (CAE) which are usedto design the proposed hybrid model.3.1.1. Convolutional neural network (CNN)Convolutional Neural Network is a Deep Learning technique thatuses convolution operation instead of simple matrix multiplication. Ascompared to other Deep Learning techniques, CNN deals with imagesmost efﬁciently. It extracts different spatial and temporal features
Fig. 2.Architecture of a N layer Autoencoder.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
92from input images, which play a signiﬁcant role in image classiﬁcation and other computer vision tasks.Fig. 1depicts the architecture of a typ- ical CNN that contains one Input layer, one Output layer, a set ofConvolutional layers (each with an activation function), Pooling layers,and Fully Connected layers (each with an activation function).The Convolutional layer present in the CNN performs the convolu-tion operation. The initial Convolutional layers of a CNN extract the sim-ple lower-level features of an image, and the Convolutional layerspresent at the end of the network extract the complex higher-level fea-tures of an image. The convolution operation is de ﬁned as a binary op- eration (represented by symbol ‘∗’) between two real-valued functions(sayf(x)a n dg(x)). In the continuous domain, it can be mathematicallydeﬁned as in Eq.(1). Similarly, in the discrete domain, the mathematicalformula for convolution operation can be written in Eq. (2)(Goodfellow et al., 2016).fxðÞ∗gxðÞ ¼ZfxðÞ /C1gx−kðÞdk ð1ÞfxðÞ∗gxðÞ ¼∑
∞k¼−∞fxðÞ /C1gx−kðÞ ð 2ÞIn CNN,f(x)a n dg(x) are termed, as input andﬁlter/kernel, respec- tively, and the output of the convolution operation is known as the fea-ture map. The input, kernel/ﬁlter, and feature map are stored asmultidimensional arrays. From the deﬁnition of convolution operation, it can be observed that if the size of the input matrix is m×mand the size of theﬁlter isk×k(wherek≤m), then the size of the output featuremap ism−k+1×m−k+ 1. Thus, it can be concluded that after eachconvolution operation, the size of output feature map is decreased. Inother words, the size of the input image reduces after each convolutionoperation and becomes zero after some convolutions. Hence, it limitsCNN's depth by placing an upper bound on the number of Convolutionallayers present in a CNN. Further, the elements present on the edges andcorners are used less than the elements present in the center of theinput matrix. To tackle these two issues, padding is used in theConvolutional layers present in the CNN.Padding is used to expand the input matrix by appending the layersof zeroes to the input matrix's border. Thus, the input matrix area is in-creased on which the convolution operation has to be performed, whichensures that the size of the input matrix does not decrease after convo-lution operation. It also ensures that the elements present on the edgesand corners are also utilized by adding multiple padding layers. Thereare two types of padding: Valid Padding and Same Padding. In ValidPadding, no layers of zeroes are appended to the input matrix, andhence the dimensionality of the output matrix remains the same as il-lustrated above, i.e.,m−k+1×m−k+ 1. On the other hand, in the Same Padding,pl a y e r sof zeroes are appended to the input matrixsuch that its dimensionality does not change after the convolution oper-ation. The value ofpcan be determined by Eq.(3). Same Padding has been used to design the proposed hybrid model.mþ2p−kþ1¼m⇒p¼k−12 ð3ÞFrom Eqs.(1) and (2), it can be observed that convolution is a linearoperation. Therefore, to extract the non-linear features from images, dif-ferent non-linear activation functions such as Sigmoid, Hyperbolic Tan-gent, Rectiﬁed Linear Unit (ReLu), etc., are used in CNNs after theconvolution operation.After applying non-linear activation functions, pooling operation isperformed. Pooling operation is used to reduce the number of trainingparameters and hence reduce the dimensionality of the feature map itreceives from its preceding layer. It computes a single output valuebased on some statistics from its neighborhood. Some of these statisticsare Max Pooling, Average Pooling, etc. Max Pooling picks the maximumvalue from its neighborhood, and Average Pooling computes the aver-age value in its neighborhood.3.1.2. Convolutional autoencoder (CAE)Autoencoder is a self-supervised learning algorithm that uses a Neu-ral Network for representation learning ( Bisong, 2019). Representation learning is a technique in which a system learns to encode input data.Autoencoders are used to map input data to some lower-dimensionalspace or compressed domain representation. To do this, a bottleneckis introduced in the network, which enforces the system to learn thecompressed domain representation of input data. Fig. 2shows the archi- tecture of aNlayer Autoencoder.An Autoencoder comprises four components: Encoder Network,Bottleneck Layer, Decoder Network, and Reconstruction Loss(Goodfellow et al., 2016). Encoder Network is a Neural Network that en-codes input data to a compressed domain. The Bottleneck layer is thelast layer of the Encoder Network, and its output is known as encodedinput data. Let there areNlayers in an Encoder Network in which thelast layer i.e.,N
thlayer is Bottleneck Layer. The mathematical equationto represent the working of each layer present in the Encoder Networkis shown in Eq.(4).
Fig. 3.Block diagram of the proposed hybrid model.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
93Xeiþ1¼feiWTe
iXeiþb ei/C16/C17∀i¼0, 1, 2,...,Nð4ÞwhereX
eiis the input for theithlayer of Encoder Network,X ei+1is the output of thei
thlayer of the Encoder Network,W eiis the weight vector for thei
thlayer of the Encoder Network,b eiis the bias for theithlayer of the Encoder Network, andf
eiis the activation function for theithlayer of the Encoder Network.Decoder Network is also a Neural Network that takes the output ofthe Bottleneck Layer as input and tries to reconstruct the original data.The number of layers in the Decoder Network is the same as the numberof layers in the Encoder Network but in the reverse order. The last layerof the Decoder Network produces the noisy reconstruction of inputdata. The mathematical equation to represent the working of eachlayer present in the Decoder Network is shown in Eq. (5).Xdiþ1¼fdiWTd
iXdiþb di/C16/C17∀i¼0, 1, 2,⋯,Nð5ÞwhereX
diis the input for theithlayer of Decoder Network,X di+1is the output of thei
thlayer of the Decoder Network,W diis the weight vector for thei
thlayer of the Decoder Network,b diis the bias for theithlayer of the Decoder Network, andf
diis the activation function for theithlayer of the Decoder Network. The difference between the original data X
O
and the reconstructed dataXRis known as Reconstruction Loss. Theautoencoder is trained using Backpropagation Algorithm to minimizethe Reconstruction Loss.Mean Squared Error (MSE)andBinary Cross-
Fig. 4.Flow-diagram to demonstrate the proposed methodology.
Fig. 5.CAE network architecture for the proposed hybrid model.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
94Entropy (BCE) Lossare the two prominently used loss functions to com-pute Reconstruction Loss. The formulae for these two loss functions areshown in Eqs.(6) and (7), whereDis the number of instances in a dataset on which the Autoencoder is applied.MSE X
O,XR/C16/C17¼ 1D∑Dj¼1XOj−XRj/C16/C172
ð6ÞBCE X
O,XR/C16/C17¼− 1D∑Dj¼1XOj/C1logXRjþ1−XOj/C16/C17/C1log 1−XRj/C16/C17ð7ÞThere are different types of autoencoders, such as UndercompleteAutoencoder, Deep Autoencoder, Convolutional Autoencoder, etc. ACNN based autoencoder is known as Convolutional Autoencoder(CAE). It uses Convolutional and Down Sampling (Pooling) layers forencoding the input image to its compressed domain representation.Similarly, Up Sampling and Convolutional layers are used to reconstructthe original image using its compressed domain representation. Sincethis research work deals with plant leaf images, so CAE has been usedto obtain the compressed domain representation before classi ﬁcation,in the proposed hybrid model. The model reduces the size of leaf imagessuch that the prominent features of leaf images are not lost and are fur-ther used in classiﬁcation. Due to compressed domain representationsof leaf images, the number of features is reduced signi ﬁcantly, which eventually reduces the number of training parameters and reduces thetime taken for the training of hybrid system and time taken for classi ﬁ- cation by the hybrid system.3.2. Proposed workIn this research work, a novel hybrid model is designed to detectplant diseases automatically. To the best of our belief, a hybrid systemfor automatic plant disease detection based on CAE and CNN has notbeen proposed in any research work present in the literature. Thismodel uses two deep learning techniques: CAE and CNN. First, theCAE network has been trained to reduce the dimensionality of theinput leaf images. The dimensionality reduction of the leaf images hasbeen done such that the important features of the leaf images are notlost. This has been ensured by applying the upper limit on Reconstruc-tion Loss of CAE. After reducing the dimensionality of leaf images, theoutput of the Encoder Network of CAE, (i.e., compressed domainTable 2Layer wise details of CAE network.LayernumberLayer (type) Input shape Number of ﬁlters/channelsSize of eachﬁlter/channelActivationfunctionPadding OutputshapeNumber of trainingparameters1 Input Layer 256 × 256 × 3 3 –– – 256 × 256 × 3 0 2 Conv #1 256 × 256 × 3 16 3 × 3 ReLu Same Padding256 × 256 × 16 4483 MaxPool #1 256 × 256 × 16 16 2 × 2 –– 128 × 128 × 16 0 4 Conv #2 128 × 128 × 16 8 3 × 3 ReLu Same Padding128 × 128 × 8 1,1605 MaxPool #2 128 × 128 × 8 8 2 × 2 –– 64 × 64 × 8 0 6 Conv #3 64 × 64 × 8 8 3 × 3 ReLu Same Padding64 × 64 × 8 5847 MaxPool #3 64 × 64 × 8 8 2 × 2 –– 32 × 32 × 8 0 8 Bottleneck Layer 32 × 32 × 8 8 3 × 3 ReLu Same Padding32 × 32 × 8 5849 UpSampling Layer #1 32 × 32 × 8 8 2 × 2 –– 64 × 64 × 8 0 10 Conv #4 64 × 64 × 8 8 3 × 3 ReLu Same Padding64 × 64 × 8 58411 UpSampling Layer #2 64 × 64 × 8 8 2 × 2 –– 128 × 128 × 8 0 12 Conv #5 128 × 128 × 8 8 3 × 3 ReLu Same Padding128 × 128 × 8 58413 UpSampling Layer #3 128 × 128 × 8 8 2 × 2 –– 256 × 256 × 8 0 14 Conv #6 (Output Layer) 256 × 256 × 8 3 3 × 3 ReLu Same Padding256 × 256 × 3 219Total number of trainable parameters 4,163
Fig. 6.CNN architecture for the proposed hybrid model.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
95representations of leaf images) is used as input to the CNN. With thehelp of CNN, the input leaf image has been classi ﬁed as either a diseased leaf or a healthy leaf. The block diagram of the proposed hybrid model isshown inFig. 3.F u r t h e r ,t h eﬂow-diagram to demonstrate the proposedmethodology has been shown inFig. 4.The process of designing the proposed hybrid model comprises oftwo steps. Theﬁrst step is creating a CAE network that reduces the di-mensionality of the input leaf images from 256 × 256 to 32 × 32. The ar-chitecture of the CAE network is shown in Fig. 5and its layer-wise details is tabulated inTable 2. The CAE network also contains a DecoderNetwork that is used to reconstruct the original data (in this case, leafimages) from the encoded data. The training of CAE network is donesuch that the Reconstruction Loss is minimized (discussed inSection 3.1.2). This ensures that the CAE network reduces the dimen-sionality of the leaf images without losing its important features. Inthis research work,Normalized Root Mean Squared Error (NRMSE)(Feng et al., 2015) loss function is used to compute the ReconstructionLoss between the original leaf image and the reconstructed leaf images.The formula to computeNRMSEi ss h o w ni nE q .(8).NRMSE X
O,XR/C16/C17¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1N∑Ni¼1XO−XR/C16/C172sP
maximum −P minimumð8ÞwhereX
Ois the original leaf image,XRis the leaf image reconstructed by the CAE network,Nis the total number of leaf images taken into consid-eration,P
maximum is the maximum pixel intensity in the input leaf imagesi.e., 255, andP
minimum is the minimum pixel intensity in the input leafimages, i.e., 0.
Fig. 7.Architecture of the proposed hybrid model.
Table 3Layer wise details of the proposed hybrid model.LayernumberLayer (type) Input shape Number of ﬁlters/channelsSize of eachﬁlter/channelActivationfunctionPadding Output shape Number of training parameters1 Input Layer 256 × 256 × 3 3 –– – 256 × 256 × 3 0 2 Conv #1 256 × 256 × 3 16 3 × 3 ReLu Same Padding256 × 256 × 16 4483 MaxPool #1 256 × 256 × 16 16 2 × 2 –– 128 × 128 × 16 0 4 Conv #2 128 × 128 × 16 8 3 × 3 ReLu Same Padding128 × 128 × 8 1,1605 MaxPool #2 128 × 128 × 8 8 2 × 2 –– 64 × 64 × 8 0 6 Conv #3 64 × 64 × 8 8 3 × 3 ReLu Same Padding64 × 64 × 8 5847 MaxPool #3 64 × 64 × 8 8 2 × 2 –– 32 × 32 × 8 0 8 BottleneckLayer32 × 32 × 8 8 3 × 3 ReLu Same Padding32 × 32 × 8 5849 Conv #7 32 × 32 × 8 6 3 × 3 ReLu Valid padding 30 × 30 × 6 43810 MaxPool #4 30 × 30 × 6 6 2 × 2 –– 15 × 15 × 6 0 11 Conv #8 15 × 15 × 6 16 3 × 3 ReLu Valid padding 13 × 13 × 16 88012 MaxPool # 5 13 × 13 × 16 16 2 × 2 –– 6×6×1 6 0 13 Conv #9 6 × 6 × 16 16 3 × 3 ReLu Valid padding 4 × 4 × 16 2,32014 MaxPool # 6 4 × 4 × 16 16 2 × 2 –– 2×2×1 6 0 15 Flatten Layer 2 × 2 × 16 –– – – 64 0 16 Dense #1 64 –– ReLu–32 2,080 17 Dense #2 32 –– Sigmoid–13 3 Total number of trainable parameters 5,751 Total number of non-trainable parameters 2776P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
96After reducing the dimensionality of input leaf images, the CNN hasbeen applied to classify the input leaf images as either a diseased leaf ora healthy leaf. The output of the Bottleneck Layer of the CAE is taken asthe input for the CNN.Fig. 6depicts the architecture of CNN that is usedto classify leaf images.As already discussed, the proposed hybrid model has been createdby concatenating the layers of the encoder network of CAE and thelayers of the CNN. The architecture of the hybrid model is shown inFig. 7and its layer-wise details are shown in Table 3. The computation of training parameters for each Convolutional layerpresent either in the CAE network or in the proposed hybrid model isdone by Eq.(9)(Goodfellow et al., 2016).N
itraining parameters¼lifilter/C2wifilter/C2ni−1channels /C2nifilters/C16/C17þnifilters ð9ÞwhereN
training parametersi is the number of training parameters in the ith
Convolutional layer present either in the CAE network or in the pro-posed model,l
ﬁlteri andw ﬁlteri are the length and width of eachConvolutionalﬁlters applied in thei
thConvolutional layer, respectively, n
channelsi−1 represent the number of channels present in the output of the i −1
thlayer, andn ﬁltersisymbolize the number of convolutionalﬁlters ap- plied in thei
thlayer of the model.
Fig. 8.(Top row) healthy leaf, (Bottom row) diseased leaf (Bacterial Spot).
Fig. 9.Change in reconstruction loss with respect to epochs.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
97The fully connected layers have also been used at the end of the pro-posed model. The training parameter's calculation for the fully con-nected layers is done using Eq.(10)(Haykin, 1998).N
itraining parameters¼niinput/C2nioutput/C16/C17þnioutput ð10ÞwhereN
training parametersi is the number of training parameters in the ithFully Connected layer, andn
inputiandn outputi represent input and output size ofi
thFully Connected layer, respectively.As already discussed, that the layers of the encoder network of CAE(i.e., layer 1 to layer 8) and layers of CNN (i.e., layer 8 to layer 17) hasbeen used to create the proposed hybrid model. Since the layersimported from the encoder network of CAE are pre-trained therefore,training parameters of these layers are not included while computingthe training parameters of the hybrid model. Hence it can be observedfromTable 3that the hybrid model uses only 5,751 trainable parametersand 2776 non-trainable parameters. Moreover, it can also be observedfromTable 2that the CAE network uses 4,163 training parameters.Thus, the total trainable parameters used in the proposed work can becalculated as the sum of trainable parameters of CAE and hybridmodel. Since CAE uses 4,163 trainable parameters, and the hybridmodel uses 5,751 trainable parameters so the total trainable parametersused in this research work is 9,914.3.3. Dataset descriptionThe proposed hybrid model has been applied to detect Bacterial Spotdisease in peach plants. The leaf images of peach plants have been ex-tracted from the PlantVillage dataset ( Hughes and Salathe, 2015). The dataset contains 4457 leaf images of peach plants, which are evenly dis-tributed in two classes: healthy and diseased (Bacterial Spot). Thehealthy class contains 2160 peach leaf images, and the diseased (Bacte-rial Spot) class comprise of 2297 leaf images of the peach plant. An ex-ample of a healthy and a diseased leaf image of a peach plant is showninFig. 8.
Fig. 10.(Top row) original leaf images, (Bottom row) reconstructed leaf images using the convolutional autoencoder network.
Fig. 11.Change in model loss with respect to epochs of the proposed hybrid model.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
98To train the model, the leaf images of peach plants have been ran-domly divided such that 70% of them form the training dataset, and30% of them form the testing dataset. Thus, the training dataset has3342 leaf images, and the testing dataset has 1115 leaf images.
3.4. Experimental conﬁgurationThe experiments performed in this research work uses the JupyterNotebook, which is an Integrated Development Environment (IDE) forPython programming language. One can also use other programminglanguages such as Matlab, R, etc., to implement the proposed hybridmodel. Thetrain_test_splitfunction ofsklearn(Pedregosa et al., 2011) Application Programming Interface (API) of Python has been used toform the training and testing dataset. To create and train the model,theKerasAPI has been used.To train the CAE network, Adam (Kingma and Ba, 2014) optimizer and NRMSE loss (Feng et al., 2015) have been used with batch size of32 and 200 epochs. Similarly, Adam ( Kingma and Ba, 2014) optimizer and Binary Cross-Entropy (BCE) loss have been used to train the pro-posed hybrid model with batch size of 32 and 100 epochs. In order toavoid retraining of layers imported from the CAE network, i.e., layer 1to layer 8 (described inTable 2), False value has been assigned to thetrainableﬂag of these layers during the training phase of the proposedhybrid model. To prevent model overﬁtting, Early stopping has been used with patience value equals 5 (i.e., if the testing loss does not im-prove overﬁve consecutive epochs, then the training will stop).4. Results and discussionThis section discusses the results of experiments performed in thecurrent research work. First, the results of the CAE network areshown. Next, the results of the proposed hybrid model are shown.TheNRMSEloss has been used to evaluate the performance of theCAE network. It is computed with the help of original leaf images and re-constructed leaf images using Eq.(8). The train and testNRMSEloss be- tween the original and the reconstructed leaf images are 0.0597 and0.0607, respectively. The change in Reconstruction Loss with respectto epochs is shown inFig. 9. Also, some original leaf images with theircorresponding reconstructed leaf images are shown in Fig. 10. The proposed hybrid model has achieved a training accuracy of99.35% and a testing accuracy of 98.38%, with 0.02 training loss and0.05 testing loss. The change of training and testing loss, along withtraining and testing accuracy with respect to epochs, are shown inFigs. 11 and 12, respectively.Precision, Recall, and F1-measure (Zaki and Meira Jr, 2014) have also been computed for the proposed model. The proposed hybrid modelhas achieved 98.0% Precision. Its Recall is 98.72%, and its F1-measure is98.36%.Table 4presents the comparison of the proposed work withthe various research works done in the literature.FromTable 4, it can be observed that the proposed hybrid modelachieved 98.38% testing accuracy, which is more than the testing accu-racies in the research works done by ( Khamparia et al., 2020) with test- ing accuracy 86.78%, (Tiwari et al., 2020) with testing accuracy 97.8%, (Chohan et al., 2020) with testing accuracy 98%, and (Mohameth et al., 2020) with testing accuracy 98%. The testing accuracy of the proposedmodel is slightly lesser than the testing accuracies in the research
Fig. 12.Change in model accuracy with respect to epochs of the proposed hybrid model.
Table 4Comparison of different state-of-the-arts with the proposed work based on their testing accuracies and number of training parameters.Author(s) name and year Proposed approach Testing accuracy Number of training parameters (approximately)Khamparia et al. (2020) Convolutional Encoder Network 86.78% 3.3 million Sanga et al. (2020) ResNet-152 99.2% 60 million Tiwari et al. (2020) VGG-19 + SVM 97.8% 143 million Mohameth et al. (2020) ResNet-50 + SVM 98% 25 million Chohan et al. (2020) VGG-19 98% 143 million Ferentinos (2018) VGGNet 99.5% 138 million Mohanty et al. (2016) GoogLeNet 99.3% 7 million Proposed approach CAE + CNN 98.38% 9,914P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
99works done by (Sanga et al., 2020) with the testing accuracy 99.2%, (Mohanty et al., 2016) with the testing accuracy 99.3%, and(Ferentinos, 2018) with testing accuracy 99.5%. But, in the proposedwork only 9,914 training parameters are used, which is very less ascompared to the number of training parameters in the state-of-the-artsystems discussed inSection 2and tabulated inTable 4.T h e r e f o r e ,t h e proposed hybrid model requires very less training time and very lessprediction time.The proposed model has two prominent use cases. First, it can betrained and used for automatic plant disease detection on low-computational power systems with less training time and predictiontime. Second, the proposed model can also be trained and used onsmartphones. Running a Deep Learning model in mobile applicationsinstead of sending the leaf images of plants to the cloud/server reducesthe latency and provides data privacy to farmers.5. ConclusionDisease detection in plants at the early stages is a hard and challeng-ing task. Many researchers have used different Machine Learning andDeep Learning techniques for automatic plant disease detection. How-ever, most of these techniques either use millions of training parametersor have a low classiﬁcation accuracy. In this paper, a novel hybrid modelwas proposed for automatic plant disease detection that was based ontwo Deep Learning techniques named Convolutional Autoencoder(CAE) network and Convolutional Neural Network (CNN). The pro-p o s e dh y b r i dm o d e lﬁrst obtained compressed domain representationsof leaf images using the encoder network of CAE and then used thecompressed domain representations for classi ﬁcation using CNN. Due to dimensionality reduction using CAE, the number of features, andhence the number of training parameters reduced signi ﬁcantly as com- pared to existing state-of-the-art systems. To test the model, it was ap-plied to detect Bacterial Spot disease in peach plants. The modelachieved 99.35% training accuracy and 98.38% testing accuracy byusing only 9,914 training parameters. Fewer training parameters usedin the proposed hybrid model signiﬁcantly decreased the time requiredto train the model for automatic plant disease detection and the time re-quired to identify the disease in plants using the trained model.Funding sourceThis research did not receive any speciﬁc grant from funding agen- cies in the public, commercial, or not-for-pro ﬁts e c t o r s .Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Ahmed, K., Shahidi, T.R., Irfanul Alam, S.M., Momen, S., 2019. Rice leaf disease detection using machine learning techniques. 2019 International Conference on SustainableTechnologies for Industry 4.0 (STI). IEEE, Dhaka, Bangladesh, pp. 1 –5. Ashraf, T., Khan, Y.N., 2020. Weed density classi ﬁcation in rice crop using computer vision. Comput. Electron. Agric. 175, 105590. https://doi.org/10.1016/j. compag.2020.105590.Bisong, E., 2019. Autoencoders. Building Machine Learning and Deep Learning Models onGoogle Cloud Platform: A Comprehensive Guide for Beginners. Apress, Berkeley, CA,pp. 475–482.https://doi.org/10.1007/978-1-4842-4470-8_37 . Chen, Junde, Chen, Jinxiu, Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020. Using deep transferlearning for image-based plant disease identi ﬁcation. Comput. Electron. Agric. 173, 105393.https://doi.org/10.1016/j.compag.2020.105393 . Chohan, M., Khan, A., Katper, S., Mahar, M., 2020. Plant disease detection using deeplearning. Int. J. Recent Technol. Eng. 9 (1), 909 –914.https://doi.org/10.35940/ijrte. A2139.059120.Es-saady, Y., el Massi, I., el Yassa, M., Mammass, D., Benazoun, A., 2016. Automatic recog-nition of plant leaves diseases based on serial combination of two SVM classi ﬁers.2016 International Conference on Electrical and Information Technologies (ICEIT).IEEE, Tangiers, Morocco, pp. 561 –566.https://doi.org/10.1109/EITech.2016.7519661 . Feng, D., Feng, M., Ozer, E., Fukuda, Y., 2015. A vision-based sensor for noncontact struc-tural displacement measurement. Sensors 15 (7), 16557 –16575.https://doi.org/ 10.3390/s150716557.Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.https://doi.org/10.1016/j.compag.2018. 01.009.Golhani, K., Balasundram, S.K., Vadamalai, G., Pradhan, B., 2018. A review of neural net-works in plant disease detection using hyperspectral data. Inform. Proc. Agric. 5(3), 354–371.https://doi.org/10.1016/j.inpa.2018.05.002 . Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. 1st ed. The MIT Press, Cam- bridge, United Kingdom.Haykin, S., 1998.Neural Networks: A Comprehensive Foundation. 2nd ed. Prentice HallPTR, USA.He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. 2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR). IEEE, LasVegas, NV, USA, pp. 770 –778.https://doi.org/10.1109/CVPR.2016.90 . Himani, 2014. An analysis of agriculture sector in Indian economy. IOSR J. Human. Soc. Sci.19 (1), 47–54.https://doi.org/10.9790/0837-191104754 . Hughes, David P., Salathe, M., 2015. An open access repository of images on plant health to enable the development of mobile disease diagnostics. arXiv 1 –13 preprint arXiv. Islam, M., Dinh, Anh, Wahid, K., Bhowmik, P., 2017. Detection of potato diseases usingimage segmentation and multiclass support vector machine. 2017 IEEE 30th Cana-dian Conference on Electrical and Computer Engineering (CCECE). IEEE, Windsor,ON, Canada, pp. 1–4.https://doi.org/10.1109/CCECE.2017.7946594 . Jearanaiwongkul, W., Anutariya, C., Andres, F., 2018. An ontology-based approach to plantdisease identiﬁcation system. Proceedings of the 10th International Conference onAdvances in Information Technology - IAIT 2018. ACM Press, New York, New York,USA, pp. 1–8.https://doi.org/10.1145/3291280.3291786 . Karlekar, A., Seal, A., 2020. SoyNet: soybean leaf diseases classi ﬁcation. Comput. Electron. Agric. 172, 105342.https://doi.org/10.1016/j.compag.2020.105342 . Khamparia, A., Saini, G., Gupta, D., Khanna, A., Tiwari, S., de Albuquerque, V.H.C., 2020.Seasonal crops disease prediction and classi ﬁcation using deep convolutional encoder network. Circ. Syst. Sign. Proc. 39, 818 –836.https://doi.org/10.1007/s00034-019- 01041-0.Kim, W.-S., Lee, D.-H., Kim, Y.-J., 2020. Machine vision-based automatic disease symptomdetection of onion downy mildew. Comput. Electron. Agric. 168, 105099. https://doi. org/10.1016/j.compag.2019.105099 . Kingma, D., Ba, J., 2014.Adam: a method for stochastic optimization. International Confer-ence on Learning Representations. San Diego, CA, USA, pp. 1 –15. Krithika, N., Selvarani, A.G., 2017. An individual grape leaf disease identi ﬁcation using leaf skeletons and KNN classiﬁcation. 2017 International Conference on Innovations in In-formation, Embedded and Communication Systems (ICIIECS). IEEE, Coimbatore,India, pp. 1–5https://doi.org/10.1109/ICIIECS.2017.8275951 . Marwaha, S., Bedi, P., Yadav, R., Malik, N., 2009. Diseases and pests identiﬁcation in crops - a semantic web approach. Proceedings of the 4th Indian International Conference onArtiﬁcial Intelligence, IICAI 2009. IICAI, Tumkur, Karnataka, India, pp. 1057 –1076. Marwaha, S., Chand, S., Saha, A., 2012. Disease diagnosis in crops using content basedimage retrieval. 12th International Conference on Intelligent Systems Design andApplications (ISDA). IEEE, Kochi, India, pp. 729 –733.https://doi.org/10.1109/ ISDA.2012.6416627.Mohameth, F., Bingcai, C., Sada, K.A., 2020. Plant disease detection with deep learning andfeature extraction using Plant Village. J. Comp. Commun. 8 (6), 10 –22.https://doi.org/ 10.4236/jcc.2020.86002.Mohanty, S.P., Hughes, D.P., Salathé, M., 2016. Using deep learning for image-based plantdisease detection. Front. Plant Sci. 7, 1 –10.https://doi.org/10.3389/fpls.2016.01419 . Naik, M.R., Sivappagari, C.M.R., 2016. Plant leaf and disease detection by using HSV fea- tures and SVM classiﬁe r .I n t .J .E n g .S c i .6( 1 2 ) ,1 –4. Ngugi, L.C., Abelwahab, M., Abo-Zahhad, M., 2020. Recent advances in image processingtechniques for automated leaf pest and disease recognition –a review. Inform. Proc. Agric., 1–25.https://doi.org/10.1016/j.inpa.2020.04.004 . Padol, P.B., Yadav, A.A., 2016. SVM classi ﬁer based grape leaf disease detection, in: 2016 Conference on Advances in Signal Processing (CASP). IEEE, Pune, India, pp. 175 –179 https://doi.org/10.1109/CASP.2016.7746160 . Panda, Panigrahi Kshyanaprava, Himansu, Das, Kumar, Sahoo Abhaya, Chandra,Moharana Suresh, 2020.
Maize leaf disease detection and classi ﬁcation using machine learning algorithms. Progress in Computing. Analytics and Networking. SpringerSingapore, Singapore, pp. 659 –669. Pardede, H.F., Suryawati, E., Sustika, R., Zilvan, V., 2018. Unsupervised convolutionalautoencoder-based feature learning for automatic detection of plant diseases. 2018International Conference on Computer, Control, Informatics and its Applications(IC3INA). IEEE, Tangerang, Indonesia, Indonesia, pp. 158 –162.https://doi.org/ 10.1109/IC3INA.2018.8629518 . Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., Blondel, M.,Prettenhofer, P., Weiss, R., Dubourg, V., Vanderplas, J., Passos, A., Cournapeau, D.,Brucher, M., Perrot, M., Duchesnay, E., 2011. Scikit-learn: machine learning in Python. J. Mach. Learn. Res. 12, 2825 –2830. Qin, F., Liu, D., Sun, B., Ruan, L., Ma, Z., Wang, H., 2016. Identi ﬁcation of alfalfa leaf diseases using image recognition technology. PLoS One 11 (12), 1 –26.https://doi.org/ 10.1371/journal.pone.0168274 . Ramesh, S., Vydeki, D., 2020. Recognition and classi ﬁcation of paddy leaf diseases using optimized deep neural network with Jaya algorithm. Inform. Proc. Agric. 7 (2),249–260.https://doi.org/10.1016/j.inpa.2019.09.002 . Rao, D.R., Krishna, M., Ramakrishna, B., 2020. Smart ailment identiﬁcation system for Paddy crop using machine learning. Int. J. Innov. Eng. Manag. Res. 9 (3), 96 –100.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
100Sanga, S.L., Machuve, D., Jomanga, K., 2020. Mobile-based deep learning models for Ba- nana disease detection. Technol. Appl. Sci. Res. 10 (3), 5674 –5677. Sharma, P., Berwal, Y.P.S., Ghai, W., 2019. Performance analysis of deep learning CNNmodels for disease detection in plants using image segmentation. Inform. Proc.Agric., 1–9.https://doi.org/10.1016/j.inpa.2019.11.001 . Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale image recognition. arXiv 1 –14 preprint arXiv. Singh, V., 2019. Sunﬂower leaf diseases detection using image segmentation based onparticle swarm optimization. Arti ﬁc. Intellig. Agric. 3, 62 –68.https://doi.org/ 10.1016/j.aiia.2019.09.002. Singh, V., Misra, A.K., 2017. Detection of plant leaf diseases using image segmentation andsoft computing techniques. Inform. Proc. Agric. 4, 41 –49.https://doi.org/10.1016/j. inpa.2016.10.005.S z e g e d y ,C . ,L i u ,W e i ,J i a ,Y a n g q i n g ,S e r m a n e t ,P . ,R e e d ,S . ,A n g u e l o v ,D . ,E r h a n ,D . ,Vanhoucke, V., Rabinovich, A., 2015. Going deeper with convolutions. 2015 IEEE Con-ference on Computer Vision and Pattern Recognition (CVPR). IEEE, Boston, MA, USA,pp. 1–9.https://doi.org/10.1109/CVPR.2015.7298594 . Tewari, V.K., Pareek, C.M., Lal, G., Dhruw, L.K., Singh, N., 2020. Image processing basedreal-time variable-rate chemical spraying system for disease control in paddy crop.Artiﬁc. Intellig. Agric. 4, 21 –30.https://doi.org/10.1016/j.aiia.2020.01.002 . Tiwari, D., Ashish, M., Gangwar, N., Sharma, A., Patel, S., Bhardwaj, S., 2020. Potato leaf dis-eases detection using deep learning. 2020 4th International Conference on IntelligentComputing and Control Systems (ICICCS). IEEE, Madurai, India, India, pp. 461 –466. https://doi.org/10.1109/ICICCS48265.2020.9121067 . Zaki, M.J., Meira Jr., Wagner, 2014. Data Mining and Analysis: Fundamental Concepts and Algorithms. 2nd ed. Cambridge University Press, Cambridge, United Kingdom.P. Bedi and P. Gole Artiﬁcial Intelligence in Agriculture 5 (2021) 90 –101
101