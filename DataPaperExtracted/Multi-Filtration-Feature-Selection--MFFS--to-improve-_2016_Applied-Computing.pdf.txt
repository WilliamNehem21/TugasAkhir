ORIGINAL ARTICLE
Multi Filtration Feature Selection (MFFS) toimprove discriminatory ability in clinical data set
S. Sasikalaa,*, S. Appavu alias Balamuruganb, S. Geethac
aAnna University, Tamil Nadu, India
bK.L.N. College of Information Technology, Tamil Nadu, India
cThiagarajar College of Engineering, Tamil Nadu, IndiaReceived 13 June 2013; revised 21 March 2014; accepted 29 March 2014
Available online 5 April 2014
KEYWORDSMedical data mining;Biomedical classiﬁcation;Variance coverage factor;Principal ComponentAnalysis;Multi Filtration FeatureSelectionAbstract Selection of optimal features is an important area of research in medical data miningsystems. In this paper we introduce an efﬁcient four-stage procedure – feature extraction, featuresubset selection, feature ranking and classiﬁcation, called as Multi-Filtration Feature Selection(MFFS), for an investigation on the improvement of detection accuracy and optimal feature subsetselection. The proposed method adjusts a parameter named ‘‘variance coverage’’ and builds themodel with the value at which maximum classiﬁcation accuracy is obtained. This facilitates theselection of a compact set of superior features, remarkably at a very low cost. An extensive exper-imental comparison of the proposed method and other methods using four different classiﬁers(Naı¨ve Bayes (NB), Support Vector Machine (SVM), multi layer perceptron (MLP) and J48 deci-sion tree) and 22 different medical data sets conﬁrm that the proposed MFFS strategy yieldspromising results on feature selection and classiﬁcation accuracy for medical data mining ﬁeld ofresearch.
ª2014 King Saud University. Production and hosting by Elsevier B.V. This is an open access articleunder the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/3.0/ ).
1. Introduction
Data mining application in medicine has proved to be asuccessful strategy in the areas of medical services includingprediction of usefulness of surgical procedures, clinical tests,
medication procedures, and the discovery of associationsamong clinical and diagnosis data[37]. The applicability ofdata mining for healthcare applications is increasingly gainingimportance. The availability of diverse-natured medical datafor diagnosis and prognosis and of pervasive data mining tech-niques to process these data offers medical data mining a dis-tinctive place to truly assist and impact patient care.
Due to proliferation of synergized information from enor-mous patient repositories, there is a paradigm shift in theinsight of patients, clinicians and payers from qualitative anal-ysis of clinical data to demanding a better quantitative visual-ization of information based on all supporting medical data.*Corresponding author. Tel.: +91 9443831534.E-mail addresses:nithilannsasikala@yahoo.co.in(S. Sasikala),app_s@ yahoo.com(S. Appavu alias Balamurugan),sgeetha@tce.edu(S. Geetha). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2016) 12, 117–127
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2014.03.0022210-8327ª2014 King Saud University. Production and hosting by Elsevier B.V.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/3.0/ ).For instance, the physicians can evaluate the diagnostic infor-mation of many patients with identical conditions. In the sameway, they can verify their ﬁndings too, with the conformity ofpeer physicians working on similar cases in other parts of theworld. The patterns that are discovered denote valuableknowledge that helps medical discoveries, for example discov-ering that a certain combination of features may help in better,and more accurate diagnosis of a particular disease. Accuratediagnosis of diseases and subsequently, providing efﬁcienttreatment, form an important part of valuable medical servicesgiven for patients in a health-care system.
The unique characteristics of medical databases that posechallenges for data mining are the privacy-sensitive, heteroge-neous, and voluminous data. These data may have valuableinformation which awaits extraction. The required knowledgeis found to be encapsulated in/as various regularities and pat-terns that may not be apparent in the raw data. Extractingsuch knowledge has proved to be priceless for future medicaldecision making. Feature selection is crucial for analysingvarious dimensional bio-medical data. It is difﬁcult for thebiologists or doctors to examine the whole feature-spaceobtained through clinical laboratories at one time. In machinelearning, all the computational algorithms recommend onlyfew signiﬁcant features for disease diagnosis. Then these rec-ommended signiﬁcant features may help doctors or expertsto understand the biomedical mechanism better with a deeperknowledge about the cause of disease and provide the fastestdiagnosis for recovering the infected patients as early aspossible.
Feature selection methods[12]tend to identify the featuresmost relevant for classiﬁcation and can be broadly categorizedas either subset selection methods or ranking methods. Theformer type returns a subset of the original set of featureswhich are considered to be the most important for classiﬁca-tion. Ranking methods sort the features according to their use-fulness in the classiﬁcation task. Most of the classiﬁers,irrespective of the application domain, uses the rankingstrategy to select the ﬁnal feature subset, in an ad hoc manner.Feature selection, as a pre-processing step to machine learning,is prominent and effective in dimensionality reduction, byremoving irrelevant and redundant data, increasing learningaccuracy, and improving result comprehensibility. Featureselection algorithms generally fall into two broad categories,the ﬁlter model and the wrapper model[37].The ﬁlter model depends on general characteristics of the training data to selectsome features without involving any learning algorithm. Theﬁlter model assesses the relevance of features from data alone,independent of classiﬁers, using measures like distance, infor-mation, dependency (correlation), and consistency. The ﬁltermethod is further classiﬁed into Feature Subset Selection(FSS) and Feature Ranking (FR) methods. The wrappermodel needs one predetermined learning algorithm in featureselection and uses its performance to evaluate and determinewhich features are selected. For each of the generated new sub-set of features, the wrapper model is supposed to learn thehypothesis of a classiﬁer. It has a propensity to ﬁnd featuresbetter suited to the predetermined learning algorithm resultingin superior learning performance, but it also tends to takemore computation time and is economically more expensivethan the ﬁlter model[37]. Whenever dealing with a large num-ber of features, the ﬁlter model is usually chosen due to its highaccuracy[9]. The hybrid model takes the advantages of the twoprevious models, and uses an independent measure to identify
the best subsets for a given cardinality and applies a miningalgorithm to select the best subset among all best subsetsacross different cardinalities. However, the ensemble of a ﬁlterbased model with another ﬁlter based model, once for subsetselection and again for ranking proves to be a promisingapproach, for medical data mining. The ensemble is broughtabout in a fashion so as to reduce the number of featuresand also to enhance the classiﬁcation accuracy.
The objective of this research work is aimed at showing thatthe selection of more signiﬁcant features from the availableraw medical dataset helps the physician to arrive at an accuratediagnosis. The primary focus is on aggressive dimensionalityreduction so as to end up with increase in the prediction accu-racy. The features are subjected to a double ﬁltration process,at the end of which, only the features that increase the accu-racy, and form the subset with the lowest cardinality, withtheir corresponding rank, are obtained. The method employsan efﬁcient strategy of ensemble feature correlation with rank-ing method. The empirical results show that the proposedMulti Filtration Feature Selection (MFFS) embedded classiﬁermodel achieves remarkable dimensionality reduction in the 22medical datasets obtained from the UCI Machine Learningrepository[10]and Kentridge repository[13].
2. Related work
Numerous works have been carried out in the ﬁeld of dimen-sionality reduction for medical diagnosis. The followingsection presents the summary of those works, highlightingthe strengths and weaknesses of each method.
It could be observed that the naive Sequential ForwardFeature Selection (SFFS) (pure wrapper approach) [5]is impractical for feature subset selection from a large numberof samples of high-dimensional features. Hence Gan et al. [4] proposed the Filter-Dominating Hybrid Sequential ForwardFeature Selection (FDHSFFS) algorithm for high dimensionalfeature subset selection. This method proved to be fast butdemanded huge computational complexity. Another variantof the SFFS method called improved F-score and SequentialForward Search (IFSFS) was proposed by Xie and Wang[36]for feature selection to diagnose erythemato-squamousdisease. This method was designed so as to improve theF-score and measured the discrimination between more thantwo sets of real numbers instead of measuring between onlytwo sets of real numbers. The method’s applicability to othermedical data sets was not reported and hence it was a veryspeciﬁc system targeted at the diagnosis of erythemato-squamous disease only.
Another category of feature selection methods used MutualInformation score. Vinh et al.[32]proposed a novel featureselection method based on the normalization of this well-known mutual information measurement and utilized theinformation measurement to estimate the potential of thefeatures. The method could not eclipse the strongly correlatedfeatures impact on the classiﬁcation results. Correlatedfeatures may be accounted for redundancy and hence a singlerepresentative feature from that subset may be selected forfurther processing.
An incremental learning algorithm in which the mostinformative features are learnt at each step, is proposed by118 S. Sasikala et al.Ruckstieb et al.[26]and is called as Sequential Online FeatureSelection (SOFS). Another Scatter Search-based approachcoupled with Decision Trees (SS+DT) is proposed by Linand Chen[17]. The method acquired optimal parameter set-tings and selected the beneﬁcial subset of features that resultedin better classiﬁcation results. In[16]Koprinska empiricallyevaluated feature selection methods for classiﬁcation ofBrain–Computer Interface (BCI) data. A new feature selectionmethod based on rough set theory has been proposed by Pauland Maji[23]. The proposed method identiﬁed discriminativeand signiﬁcant genes from high-dimensional microarray geneexpression data sets.
Correlation Based Filter[3,18]is another strategy for fea-ture selection. Ensemble methods have also been proposed.Raymer et al.[25]proposed a hybrid algorithm that coupleda genetic algorithm with k-nearest-neighbour classiﬁer andapplied it for protein–water binding from X-ray crystallo-graphic protein structure data. MonirulKabi et al. [20]pre- sented a new Hybrid Genetic Algorithm (HGA) for FeatureSelection (FS), called as HGAFS. It employed a new localsearch operation that is devised and embedded in HGA toﬁne-tune the search in feature selection process. The searchprocess is guided in such a way that the less correlated (dis-tinct) features consisting of general and special characteristicsof a given data set are generated in subsequent iterations.
A new approach called Redundancy Demoting (RD) hasbeen proposed by Osl et al.[22]. It takes an arbitrary featureranking as input, and performs improvement in ranking byidentifying redundant features and demoting them to positionsin the ranking in which they are not redundant. Hybridschemes that combine wrapper-based and ﬁlter-basedapproaches are also in the literature[2,11,30]are such schemes where the features are ranked and then selected so as to offersuperior classiﬁcation accuracy. In the ﬁrst stage, the ﬁltermodel is used to rank the features by the relief algorithmand then the highest relevant features are chosen to the classeswith the help of the threshold. In the second stage, they usedshapely values to evaluate the contribution of features to theclassiﬁcation task in the ranked feature subset. Tanwaniet al.[31]gave a study on comprehensive evaluation of a setof diverse machine learning schemes on a number of biomed-ical datasets. Sanchez-Monedero et al.[27]studied and pro- posed the suitability of Extreme Learning Machines (ELM)for resolving bio-informatics and biomedical classiﬁcationproblems.
After reviewing the works on feature selection for medicaldataset[29]it is observed that most of the existing methodssuffer from the following problems: (1) depending on the com-plexity of the search method, the iterations of evaluations aretoo large; (2) they rely on a univariate ranking that does nottake into account interaction between the variables alreadyincluded in the selected subset and the remaining ones. More-over, a method that produces the best accuracy employs morenumber of features and hence more running time is involved inthe construction of the respective classiﬁers. Contrarily, amethod that outputs the fewest number of features producesinferior detection accuracy. A holistic and universal methodthat achieves the best classiﬁcation accuracy with fewestfeatures possible is still an open research problem. This papermakes an attempt to design such a feature selection sequenceand it is called as ‘‘Multi Filtration Feature Selection(MFFS)’’.This paper is organized as follows: Section 3describes the
proposed method with the suitable algorithm. Experimentalresults and discussions are presented in Section 4. The paper is concluded with a mention on the future scope of this work.
3. Proposed system
The proposed method involves four stages. The entire systemﬂow of the proposed model is shown inFig. 1. The individual stages are described in the following text.
3.1. System ﬂow of the proposed method3.1.1. Stage 1 – Relevant Feature Generation Phase (RFGP)
A representative of unsupervised dimensionality reductionmethod is Principal Component Analysis (PCA) [14,39]which aims at identifying a lower-dimensional space maximizing thevariance among data[38]. PCA is a very effective approachof extracting features[6,21].
Let us denote the multi-dimensional dataset in the form of amatrix, A. The dimensions actually represent directions alongwhich the data vary. The feature generation process, whichremoves the irrelevant features and redundant features, mainlyﬁnds an approximate ‘‘basis’’ to the set of directions. Only thecrucial dimensions that serve as the corner stone upon whichother dimensions are dependent are generated from the givendataset. The redundant-duplicate dimensions are ﬁnally elimi-nated with the sense that they can be reconstructed easily fromthe available set of basis dimensions. This is equivalent to ﬁnd-ing the dimensions with maximal variance, since the points arefound to be constant approximately along other dimensions.Variance factor is an important measure that denotes thedegree of data spread in a multi-dimensional dataset. Thusdimensionality reduction is effectively contributed from thisﬁrst step of ﬁltration by choosing appropriate variance factorat which the system yields the minimum number of featureswith maximum accuracy.
Medical Dataset
Feature Ranking Phase (FRP)Stage 1
Stage 2
Stage 3 Feature Re-Ranking Phase (FRRP)
Classifier Evaluation: NB, SVM, J48, MLP
# of Features Running Time AccuracyStage 4 Expert’s DecisionRelevant Feature Generation Phase (RFGP)MFFSEngine
Fig. 1System ﬂow of the proposed MFFS model.MFFS 119The basic theoretical idea behind PCA is ﬁnding the princi-pal components of the dataset that correspond to the compo-nents along which the variation is the most. This is achieved byﬁnding the covariance matrix, i.e., we ﬁnd the principal com-ponents of the data, which correspond to the componentsalong which there is the most variation. This can be done usingthe covariance matrix, AA
Tfor our input matrix A, as follows.
Let the eigen values be represented ask ifor the covariance matrix. Then, the corresponding diagonal matrix is given inEq.(1)as:
L¼k10...00k
2...0...00...k
n266664377775
ð1Þ
The eigenvectorsv iof the matrix should satisfy AATas given in Eq.(2)as
AATvi¼k ivi ð2Þ
On rewriting eigenvectors of the dataset as the rows of amatrix P, the system becomes
AATP¼LP ð3Þ
It is apparent from Eq.(3)that the columns of this matrix Prepresents the principal components of the original matrixand hence conﬁnes to the directions of most variance [39,38].
PCA employs the entire features and it acquires a set ofprojection vectors to extract global feature from given trainingsamples. The approach mainly consists of three primaryprocesses such distinction process, binary session and patterngeneration[29]. All these ﬂavours make PCA[21]more suit- able for applying on medical datasets, which typically havethese characteristics. The variance coverage factor is playinga signiﬁcant role in deciding the important features and hencethis parameter is tuned so as to capture the classiﬁer modelwith the best results.3.1.2. Stage 2 – Feature Ranking Phase (FRP)
The correlation between each feature and the class andbetween two features can be measured and best-ﬁrst searchcan be exploited in searching for a feature subset of maximumoverall correlation to the class and minimum correlationamong selected features. This is realized in the Correlation-based Feature Selection (CFS) method[7]. Correlation basedFeature Selection is an algorithm that couples this evaluationformula with an appropriate correlation measure and a heuris-tic search strategy. CFS quickly identiﬁes and screens irrele-vant, redundant, and noisy features, and identiﬁes relevantfeatures as long as their relevance does not strongly dependon other features. CFS is a fully automatic algorithm– –it doesnot require the user to specify any thresholds or the number offeatures to be selected, although both are simple to incorporateif desired. CFS operates on the original (albeit discretized) fea-ture space, meaning that any knowledge induced by a learningalgorithm, using features selected by CFS, can be interpretedin terms of the original features, not in terms of a transformedspace. Most importantly, CFS is a ﬁlter, and, as such, does notincur the high computational cost associated with repeatedlyinvoking a learning algorithm.The suggestion used by the CFS is on the basis that always
features strongly correlated with the predicted class form thegood feature subset than the features correlated with eachother. The feature subset created by the CFS is computed bythe merit of the feature subset ‘S’ containing ‘ k’ features as in Eq.(4).
The following equation provides the merit of the featuresubset ‘S’.
Merit s¼kxcfﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃkþkðk/C01Þxffp ð4Þ
where Merit sis the evaluating hypothesis of a feature subset ‘S’containing ‘k’ features,
xfcis the average value of feature–classcorrelation, and
xffis the average value of feature–featureinter correlation.
The correlation between two entities ‘i’ and ‘j’,x ijis calcu- lated as in Eq.(5)
xij¼Pði/C0iÞðj/C0 jÞﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ½
Pði/C0iÞ2/C138½Pðj/C0jÞ2/C138q:ð5Þ
where ‘i’ is the record’s value of the independent variable and‘j’ is record’s value of the dependent variable which may beeither feature or class label.
/C22iand /C22jare the means of the valuesof the independent and dependent variables, respectively.3.1.3. Stage 3 – Feature Re-Ranking Phase (FRRP)
In spite of feature extraction and selection, a problem is persis-tent namely the classiﬁer may be biased towards the attributeswith more values. Hence this biased nature has to be elimi-nated for which we employ Symmetrical Uncertainty (SU). Itovercomes the problem of bias towards attributes with morevalues, by dividing information gain by the sum of the entro-pies of feature subsetsS
iandS j.
Symmetry is a desired property for a measure of correla-tions between features. However, information gain is biasedin favour of features with more values. Furthermore, the val-ues have to be normalized to ensure they are comparableand have the same inﬂuence. Therefore, we choose symmetri-cal uncertainty. It compensates for information gain’s biastowards features with more values and normalizes its valuesto the range [0; 1] with value 1 indicating that knowledge ofthe value of either one completely predicts the value of theother and value 0 indicating that X and Y are independent.In addition, it still treats a pair of features symmetrically.Entropy-based measures require nominal features, but theycan be applied to measure correlations between continuousfeatures as well, if the values are discretized properly inadvance. Therefore, we use symmetrical uncertainty in thiswork.
As CFS uses the best-ﬁrst strategy search method to calcu-late the merit of the feature subset, however there is a necessityto ﬁx the stopping criteria. Due to this strictly neededconstrain correlation between features is computed usingSymmetrical Uncertainty (SU) as speciﬁed in Eq. (6).
SU¼2:0/C2HðS jÞþHðS iÞ/C0HðS i;SjÞHðS
jÞþHðS iÞ/C20/C21 ð6Þ
whereH(S j) andH(S i,Sj) are deﬁned as in Eqs.(7) and (8)as:120 S. Sasikala et al.HðS jÞ¼/C0X
fseFS jpðS jÞlog2ðpðS jÞÞ ð7Þ
where a realistic model of a featureS jcan be formed byevaluating the training data, considering the individual’sprobability values ofS
j. A new feature subsetS ican be worked out by partitioning the previously existing feature subset S
j, then the relationship between subsetsS
iandS jis given by:
HðS i;SjÞ¼/C0X
xeXPðS iÞX
yeYPðS i=SjÞlog2PðS i=SjÞð8Þ
The algorithm is better explained by theFig. 2. 3.1.4. Stage 4 – Classiﬁer Evaluation
The proposed system is validated against standard successfulclassiﬁer models[35]. Classiﬁers are constructed with the ﬁnalsubset of features obtained after subjecting the datasets toRFGP, FRP and FRRP steps sequentially. A detailed insightinto various classiﬁers is presented in Section 4.4.
3.2. Algorithm for the proposed MFFS model
Traditional ﬁlter approaches usually select the top ranked fea-tures or eliminate the irrelevant features by using a thresholdcriterion. Since prediction is made after the single ﬁlteringphenomenon, they report feeble accuracy. Alternatively, whenﬁltering is done, more than once, an improved accuracy maybe obtained. Hence the proposed scheme is designed with
Multi Filtration Feature Selection (MFFS) as the central logic.
It consists of the following steps:
4. Test results and discussion
4.1. Test scenario
Empirical study with the synthetic datasets has been executed,to investigate the performance of the proposed algorithm inthe following perspectives:
1. Classiﬁcation accuracy2. Number of features selected3. Average running time
Datasets, test set-up, procedure and objectives for the testsnecessary for the evaluation of these goals are described below.
4.2. Datasets
The proposed approach has been evaluated by experiments on22 biomedical datasets from the UCI machine learning repos-itory[10]and Kentridge repository[13]. The 22 biomedicaldatasets used to test the proposed approach are summarizedinTable 1. The last column inTable 1indicates the ‘‘imbalance1,1 1,2 1,n, 1 m,1 m,2 m,n m
/Input   :Training set with ' ' features and ' ' samples - {(x ,x ,...,x y )......(x ,x ,...,x ,y )}Output : Best Feature Subset,MaxAccuracy,Min Error,respectiveVariance Coverage Factor(δ)nm
/Initialization
Phase I - Relevant Feature Generation Phase:1.δ 0.45,BestFeatureSubset {},S       2.By forward feature selection strategy, popul ate features into S, where S is the feature subset. ←← ← ∅    3. For each FeatureSubset S,       4.ExtractedData = evaluatePCA(S,δ)       5.if (ExtractedData>BestFeatureSubset) then       6.BestFeatureSubset=ExtractedData       7.δ=δ+0.05       8.Repeat step
Phase II - Feature Ranking Phase:s 2 to 7 until δ=0.95; Record the δ value and the respective BestFeatureSubset that gives maximum accuracy.       9.Perform Correlation-based heuristic evaluation  on 
()cfSff
Phase III - Feature Re-Ranking Phase:BestFeatureSubset usingkω                  Merit =k+k k-1ω       10.Arrange the BestFeatureSubset in the decreasing order of the Merit score.       11.Rank[]=Cr
ji i jjieate a Rank for BestFeatureSubset by usign Ranking-based heuristic of symmetrical uncertainty usingH(S )+H(S )-H(S ,S )                SU=2.0×SH(S )+H(S )       12.Return the re-ranked BestFea⎡⎤⎢⎥⎢⎥⎣⎦
Phase IV - Classifier Evaluation Phase:tureSubset        13.Run a 10-fold CrossValidation on th e Original feature set and BestFeaturesubset             using the Naive Bayes,SVM, J48,MLP classifier models.       14.Record the model that yields the maximum accuracy and minimum error.       15.Return BestFeatureSubset,MaxAccuracy ,MinError,respectiveVariance Coverage Factor(δ)
Fig. 2Algorithm of the proposed MFFS model.MFFS 121ratio’’ present in each dataset. It is the ratio between thecardinality of the class with the maximum instance to thecardinality of the class with the minimum instance.
4.3. Test set-up
The tests are carried out in a system with Intel i5, 8 GB RAM,DDR3, 500 GB hard drive on a Windows XP operatingsystem. The proposed algorithm is implemented using Weka[34]. WEKA is acknowledged as a landmark system in the ﬁeldof machine learning and data mining. It has attained wide-spread acceptance among the academia and industry spheres,and has become a widely used tool for data mining research.Another ﬂavour that is highly encouraging is its ‘‘OpenSource’’ nature. The free access given to the source code hasenabled us to develop and customize the modules matchingour work. The stepwise approach is as follows. The input tothe system is given in the Attribute-Relation File Format(ARFF). The proposed algorithm is executed and the featuresin the ranked order are obtained as the output. A result iscreated in Weka using the name speciﬁed in n@relation’’. The attributes speciﬁed undern@attribute’’ and instancesspeciﬁed undern@data’’ are retrieved from the ARFF ﬁleand then they are added to the created table. 10-fold crossvalidation is performed for all classiﬁers[8]. Fifty runs were done for each classiﬁcation algorithm on each dataset withfeatures selected by MFFS method. In each run, a datasetwas split into training and testing set, randomly. The resultsobtained are shown inTables 2–9.
4.4. Classiﬁcation Models4.4.1. Model M1 – Naı¨ve Bayes (NB)
Naı¨ve Bayesian Classiﬁer is a simple probabilistic classiﬁer [35] with an assumption of conditional independence among thefeatures, i.e., the presence (or absence) of a particular featureof a class is unrelated to the presence (or absence) of any otherfeature. It only requires a small amount of training data toestimate the parameters necessary for classiﬁcation. Manyexperiments have demonstrated that NB classiﬁer has workedquite well in various complex real-world situations and outper-forms many other classiﬁers. Kernel estimation has been usedin cases of datasets with numerical attributes. Also superviseddiscretization is done for converting numerical attributes tonominal ones.4.4.2. Model M2 – Support Vector Machine (SVM)
SVM[13,19]ﬁnds the hyper plane with maximum margin inbetween two classes. The Support Vector Machine (SVM) isactually based on learning with kernels some of which formthe support vectors. A great advantage of this technique is thatit can use large input data and feature sets. Thus, it is easy totest the inﬂuence of the number of features on classiﬁcationaccuracy. We implemented SVM classiﬁcation [28]for two types of kernels: polynomial kernel and Gaussian kernel(Radial Basis Function – RBF). The SVM model with com-plexity parameter C as 1.0, epsilon as 1.0E /C012, normalized training data, RBF kernel with gamma as 0.0.1, and toleranceparameters as 0.0010 produced the best results.4.4.3. Model M3 – Decision Tree (DT)
A decision tree[1,33]is a predictive machine-learning modelthat decides the target value (dependent variable) of a newsample based on various attribute values of the available data.Decision tree’s internal node represents different attributes; thebranches between the nodes tell us the possible values thatthese attributes can have in the observed samples, while theend nodes are the target class labels. The J48 decision tree clas-siﬁer[24]operates on the basis of constructing a tree andbranching it based on the attribute with the highest informa-tion gain. The J48 tree with binary split allowed a conﬁdencefactor of 0.25 and reduced error pruning is employed.4.4.4. Model M4 – Multilayer perceptron (MLP)
An MLP[15]can be viewed as a logistic regression, where theinput is ﬁrst transformed using a learnt non-linear transforma-tion. The purpose of this transformation is to project the inputdata into a linearly separable space. This intermediate layer isreferred to as a hidden layer. We have employed a back-propagation network with 0.3 as learning rate and 0.02 asmomentum. The attributes are normalized in the range of(0.1, 0.9). The training was carried out for 500 epochs.
4.5. Test procedure
For realizing dimension reduction via PCA, the orthogonalbase components are obtained out of the datasets through lin-ear transformation. For the evaluation the PCA variance cov-erage parameterdis varied in the range of (0.45, 0.95). Inpreliminary testdvalues outside this range did not lead touseful results for the analysis. So we tested in this range. Theproposed MFFS calculates the correlations of feature-classand feature–feature using CFS and the feature subset spaceis searched. The subset with the highest merit is subjected toanalysis. Then the resulting subset is re-ranked using symmet-rical uncertainty principle. For performance analysis, the mod-els M1-M4, generated according to the earlier discussion areapplied to the optimal feature subset, returned after MFFSsteps. The best accuracy in percentage along with the respec-tive variance coverage and the number of features involvedto achieve it are returned.
4.6. Test objectives
From the goals stated above, the following objectives areestablished:
O1 – enhancing the detection accuracy for the classiﬁermodel, which is measured using the detection accuracy met-ric expressed in percentage.O2 – reducing the number of features so as to achieve thebest accuracy in each classiﬁer model.O3 – reducing the running time for model generation whichis measured in seconds.O4 – determining the inﬂuence of the variance coverage ofthe PCA feature extraction model on the MFFSperformance.
Test objectives O1 and O2 are the obvious test goals withinthe focus of this work. High detection accuracy with the least122 S. Sasikala et al.number of features, which are shown inTables 2–9are proving the usefulness of applying the proposed MFFS scheme tofeature selection.
Test objectives O3 is aimed at determining the overallquality of our feature selection approach. Test objective O4is formulated to address the impact of the variance coverageof the PCA on the MFFS performance over each model.
To facilitate a logical sequence for the presentation of ourresearch results, the test objectives framed based on the goalsare ordered in a way to glide from the most speciﬁc to a moregeneral case.Tables 2–9summarize the evaluation of testobjectives O1–O4.
4.7. Test results and discussion
InTable 2, the average classiﬁcation accuracy of the chosenalgorithms over unprocessed datasets is provided. Tables 3–8 show the best average classiﬁcation accuracy with the fourclassiﬁers on each dataset and the best accuracy in each caseis highlighted in bold typeface.Table 9shows the averagerunning time in seconds taken by the proposed system.Figs 3–11show the performance of the proposed system.
It can be seen fromTables 2–8, that the classiﬁcation accu-racy based on the selected subsets by the proposed MFFSscheme is better than that based on the original feature set.This indicates that the selected feature subsets are representa-tive and informative and, thus, can be used instead of the com-plete data for pattern classiﬁcation. The list of such selectedfeatures is shown inTable 7.
From the empirical results obtained so far, it is worthnoting that each method has its strengths and limitations. Inparticular, CFS obtains good classiﬁcation accuracy in theleast amount of running time but at the expense of selectingmany more features; PCA selects the least number of featuresbut suffers in terms of classiﬁcation accuracy and also requiresmore running time than others; MFFS attains the best accu-racy and robustness in a reasonable time with lowest numberof features. Considering all these factors, the proposed MFFSscheme shows overall better performance than other methods.Tables 10 and 11summarize and compare characteristics of
0102030405060708090100Classifier Accuracy 
Bio Medical DatasetsAccuracy on Unprocessed DatasetsNaïve Bayes (NB)
Support Vector Machine (SVM)
J48
Multi layer Percetron (MLP)
Fig. 3Performance comparison of different classiﬁers on unprocessed biomedical data sets.
1204008000No. of featureis in log10scale
Bio-Medical Datasets
Fig. 4Number of features selected by the existing systems and proposed MFFS (in log 10scale for uniform scaling) with Naı¨ve Bayes classiﬁer.MFFS 1231204008000No. of featureis in log10scale
Bio-Medical datasets
Fig. 6Number of features selected by the existing systems and the proposed MFFS (in log 10scale for uniform scaling) with SVM classiﬁer.
020406080100Accuracy in %
Bio-Medical Datasets
Fig. 7Classiﬁcation accuracy obtained for the existing systems and the proposed MFFS by SVM classiﬁer.0102030405060708090100Accuracy in %
Bio-Medical Datasets
Fig. 5Classiﬁcation accuracy obtained for existing and the proposed MFFS by Naı ¨ve Bayes classiﬁer.124 S. Sasikala et al.1204008000No. of featureis in log10 scale
Bio-Medical Datasets
Fig. 10Number of features selected by the existing systems and the proposed MFFS (in log 10scale for uniform scaling) with MLP classiﬁer.1204008000No. of featureis in log10scale
Bio-Medical Datasets
Fig. 8Number of features selected by the existing systems and the proposed MFFS (in log 10scale for uniform scaling) with J48 classiﬁer.
020406080100Accuracy in %
Bio-Medical Datasets
Fig. 9Classiﬁcation accuracy obtained for the existing systems and the proposed MFFS by J48 classiﬁer.MFFS 125our proposed method for selective 6 datasets with those ofother previous works in the literature.
5. Conclusion
In this paper, we have proposed an efﬁcient Multi FiltrationFeature Selection (MFFS) method applicable to medical datamining. Empirical study on 6 synthetic medical datasets sug-gests that MFFS gives better over-all performance than theexisting counterparts in terms of all three evaluation criteria,i.e., number of selected features, classiﬁcation accuracy, andcomputational time. The comparison to other methods in theliterature also suggests MFFS has competitive performance.MFFS is capable of eliminating irrelevant and redundant fea-tures based on both feature subset selection and ranking mod-els effectively, thus providing a small set of reliable features forthe physicians to prescribe further medications.
For simplicity, several key points are collected as follows.
(1) It seems that the classiﬁcation performance is necessarilyproportional to the removal of redundant features,heavily dependent on the inclusion of relevant featuresand the ‘‘Accuracy’’ metric is observed maximum withminimum number of features.(2) The proposed MFFS algorithm operates invariably wellon any type of classiﬁer model. This shows the general-ization ability and applicability of the proposed system.(3) Our training and test database collects the popular andbenchmark medical datasets. However, the proposedmethod can be tested and applied on real-world datasettoo.(4) The best accuracy rate achieved by our proposed systemis superior to the existing schemes.
To make our system more practical, future work couldinclude the following.
(a) Fitting the proposed system to classify any other real-world dataset.(b) Applying the proposed method for a multi-labeldataset, where a record may belong to many classessimultaneously.(c) Ensemble with some optimization strategies like Particle
Swarm Optimization (PSO), Ant Colony Optimization(ACO), and Genetic Algorithm (GA) etc.
Summarily, MFFS can be expected to serve as an excellentalternative for feature selection in the ﬁeld of medical datamining.
Acknowledgment
This work is supported in part by the University Grant Com-mission (UGC), New Delhi, INDIA – Major Research Projectunder Grant No. F.No.:39-899/2010 (SR).
Appendix A. Supplementary data
Supplementary data associated with this article can be found, inthe online version,athttp://dx.doi.org/10.1016/j.aci.2014.03.002.
References
[1]I.S.I. Abuhaiba, Efﬁcient OCR using simple features anddecision trees with backtracking, Arabian J. Sci. Eng. 31 (2B)(2006) 223–244
.
[2]P. Bermejo, L.D.L. Ossa, J.A. Gamez, J.M. Puerta, Fastwrapper feature subset selection in high-dimensional datasetsby means of ﬁlter re-ranking, Knowl.-Based Syst. 25 (1) (2012)35–44
.
[3]Y. Chen, S. Yu, Selection of effective features for ECG beatrecognition based on nonlinear correlations, Artif. Intell. Med.54 (1) (2012) 43–52
.
[4]J.Q. Gan, B.A.S. Hasan, C.S.L. Tsui, A ﬁlter-dominating hybridsequential forward ﬂoating search method for feature subsetselection in high-dimensional space, Int. J. Mach. Learn.Cybern. 3 (4) (2012) 1–8
.
[5]A.I. Guyon, A. Elisseeff, An introduction to variable andfeature selection, J. Mach. Learn Res. 3 (1) (2003) 1157–1182
.
[6]H.U. Guz, A hybrid system based on information gain andprincipal component analysis for the classiﬁcation oftranscranial Doppler signals, Comput. Methods ProgramsBiomed. 107 (3) (2011) 598–609
.
[7] M.A. Hall, Correlation Based Feature Selection for MachineLearning (PhD Dissertation), Dept. of Comp. Science, Univ. ofWaikato, Hamilton, New Zealand, 1998.020406080100Accuracy in %
Bio-Medical Datasets
Fig. 11Classiﬁcation accuracy obtained for the existing systems and the proposed MFFS by MLP classiﬁer.126 S. Sasikala et al.[8]J.W. Han, M. Kamber, Data Mining: Concepts and Techniques,Morgan Kaufmann Publishers, 2006
.
[9]Y. Han, L. Yu, A variance reduction framework for stablefeature selection, in: G.I. Webb, B. Liu, C. Zhang, D.Gunopulos, X. Wu (Eds.), Data Mining, IEEE ComputerSociety, Sydney, Australia, 2010, pp. 206–215
.
[10] S. Hettich, C. Blake, C. Merz, UCI Repository of MachineLearning Databases, 1998 <http://www.ics.uci.edu/mlearn/MLRepository.html> (accessed 10 June 2013).
[11]B. Hualonga, X. Jing, Hybrid feature selection mechanism basedhigh dimensional datasets reduction, Energy Procedia (2011)30–38
.
[12]M.M. Jazzar, G. Muhammad, Feature selection basedveriﬁcation/identiﬁcation system using ﬁngerprints and palmprint, Arabian J. Sci. Eng. 38 (4) (2013) 849–857
.
[13] L. Jinyan, L. Huiqing, Kentridge Bio-Medical Data SetRepository, 2002 <http://datam.i2r.a-star.edu.sg/datasets/krbd/> (accessed 10 June 2013).
[14]I.T. Jolliffe, Principal Component Analysis, Springer-Verlag,New York, NY, 1986
.
[15]G. Kim, Y. Kim, H. Lim, H. Kim, An MLP-based featuresubset selection for HIV-1 protease cleavage site analysis, J.Artif. Intell. Med. 48 (2) (2010) 83–89
.
[16]I. Koprinska, Feature selection for brain–computer interfaces,in: T. Theeramunkong et al. (Eds.), Paciﬁc Asia Conference onKnowledge Discovery and Data Mining PAKDD’09, Springer-Verlag, Berlin, Heidelberg, 2010, pp. 100–111
.
[17]S. Lin, S. Chen, Parameter determination and feature selectionfor C4.5 algorithm using scatter search approach, Int. J. SoftComput. 16 (1) (2011) 63–75
.
[18]X. Lu, X. Peng, P. Liu, Y. Deng, B. Feng, B. Liao, A novelfeature selection method based on CFS in cancer recognition, in:L. Chen, X. Zhang, L. Wu, Y. Wang (Eds.), Systems Biology,IEEE Computer Society, China, 2012, pp. 226–231
.
[19]S.A. Mahmoud, S.M. Awaida, Recognition of off-linehandwritten Arabic (Indian) Numerals using multi-scalefeatures and support vector machines vs. hidden Markovmodels, Arabian J. Sci. Eng. 34 (2B) (2009) 429–444
.
[20]M.d. MonirulKabi, M.d. Shahjahan, Murase Kazuyuki, A newlocal search based hybrid genetic algorithm for feature selection,Int. J. Neurocomput. 74 (17) (2011) 2914–2928
.
[21] K. Moutselos, I. Maglogiannis, A. Chatziioannou, Integrationof high-volume molecular and imaging data for compositebiomarker discovery in the study of melanoma, Biomed. Res.Int. (2014),http://dx.doi.org/10.1155/2014/145243.
[22]M. Osl, S. Dreiseit, F. Cerqueira, M. Netzer, B. Pfeifer, C.Baumgartner, Demoting redundant features to improve thediscriminatory ability in cancer data, J. Biomed. Inform. 42 (4)(2009) 721–725
.
[23]S. Paul, P. Maji, Rough set based gene selection algorithm formicroarray sample classiﬁcation, in: Methods and Models inComputer Science, IEEE Computer Society, 2010, pp. 7–13
.
[24]R. Quinlan, C4.5: Programs for Machine Learning, MorganKaufmann Publishers, San Mateo, CA, USA, 1993
.[25]M.L. Raymer, T.E. Doom, L.A. Kuhn, W.F. Punch,Knowledge discovery in medical and biological datasets usinga hybrid Bayes classiﬁer/evolutionary algorithm, IEEE Trans.Syst. Man Cybern. 33 (5) (2003) 802–813
.
[26]T. Ruckstieb, C. Osendorfer, P.V.D. Smagt, Minimizing dataconsumption with sequential online feature selection, Int. J.Mach. Learn. Cybern. 4 (3) (2012) 235–243
.
[27] J. Sanchez-Monedero, M. Cruz-Ramrez, F. Fernandez Navarro,J.C. Fernandez, P.A. Gutierrez, C. Hervas-Martnez, On thesuitability of extreme learning machine for gene classiﬁcationusing feature selection, in: A.E. Hassanien, A. Abraham, F.Marcelloni, H. Hagras, M. Antonelli, T. Hong (Eds.), IntelligentSystems Design and Applications, 2010, pp. 507–512.
[28]A. Shawkat, K.A. Smith Miles, A meta learning approach toautomatic kernel selection for support vector machines,Neurocomputing 70 (1-3) (2006) 173–186
.
[29]Q. Shen, R. Diao, P. Su, Feature selection ensemble, in: A.Voronkov (Ed.), Computing, Springer-Verlag, 2011, pp. 289–306
.
[30]P. Smialowski, D. Frishman, S. Kramer, Pitfalls of supervisedfeature selection, Bioinformatics 26 (3) (2010) 440–443
.
[31]A.K. Tanwani, J.M. Afridi, M.Z. Shaﬁq, M. Farooq, Guidelinesto select machine learning scheme for classiﬁcation ofbiomedical datasets, in: C. Pizzuti, M.D. Ritchie, M.Giacobini (Eds.), European Conference on EvolutionaryComputation, Machine Learning and Data Mining inBioinformatics, Springer-Verlag, Berlin Heidelberg, 2009, pp.128–139
.
[32]L.T. Vinh, S. Lee, Y. Park, B.J. Auriol, A novel feature selectionmethod based on normalized mutual information, Int. J. Appl.Intell 37 (1) (2011) 100–120
.
[33]L.M. Wang, S.M. Yuan, L. Li, H.J. Li, Improving thePerformance of Decision Tree: A Hybrid Approach.Conceptual modeling. Lecture Notes in Computer Science,vol. 3288, Springer, 2004, pp. 327–335
.
[34] Weka 3: Machine Learning Software in Java, 2013. TheUniversity of Waikato Software Documentation < http://www. cs.waikato.ac.nz/_ml/weka> (accessed 10 June).
[35]H.I. Witten, E. Frank, Data Mining: Practical MachineLearning Tools and Techniques with Java Implementations,Morgan Kaufmann, San Francisco, CA, USA, 2000
.
[36]J. Xie, C. Wang, Using support vector machines with a novelhybrid feature selection method for diagnosis of erythemato-squamous diseases, Expert Syst. Appl. 38 (5) (2011) 5809–5815
.
[37]E. Xing, M. Jordan, R. Karp, Feature selection for high-dimensional genomic microarray data, in: C.E. Brodely, A.P.Danyluk (Eds.), Machine Learning, Morgan KaufmannPublishers Inc., San Francisco, CA, USA, 2001, pp. 601–608
.
[38]S. Yazdani, J. Shanbehzadeh, Mohammad Taghi ManzuriShalmani, RPCA: a novel pre-processing method for PCA,Adv. Artif. Intell. 2012 (1) (2012) 1–7
.
[39]M. Zahedi, A.G. Sorkhi, Improving text classiﬁcationperformance using PCA and recall-precision criteria, ArabianJ. Sci. Eng. 38 (8) (2013) 2095–2102
.MFFS 127