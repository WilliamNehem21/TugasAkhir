Deep learning based computer vision approaches for smartagricultural applications
V.G. Dhanyaa,⁎, A. Subeeshb,⁎⁎, N.L. Kushwahac, Dinesh Kumar Vishwakarmad,T .N a g e s hK u m a re, G. Ritika
c, A.N. Singha
aICAR- Indian Institute of Seed Science, Mau, Uttar Pradesh 275101, India
bICAR- Central Institute of Agricultural Engineering, Bhopal, Madhya Pradesh 462038, India
cICAR- Indian Agricultural Research Institute, New Delhi 110012, India
dGovind Ballabh Pant University of Agriculture and Technology, Pantnagar, Uttarakhand 263145, India
eICAR - National Institute of Natural Fibre Engineering and Technology, Kolkata 700040, India
abstract article info
Article history:Received 2 June 2022Received in revised form 22 September 2022Accepted 25 September 2022Available online 30 September 2022The agriculture industry is undergoing a rapid digital transformation and is growing powerful by the pillars ofcutting-edge approaches like artiﬁcial intelligence and allied technologies. At the core of arti ﬁcial intelligence, deep learning-based computer vision enables various agriculture activities to be performed automatically withutmost precision enabling smart agriculture into reality. Computer vision techniques, in conjunction withhigh-quality image acquisition using remote cameras, enable non-contact and ef ﬁcient technology-driven solu- tions in agriculture. This review contributes to providing state-of-the-art computer vision technologies based ondeep learning that can assist farmers in operations starting from land preparation to harvesting. Recent works inthe area of computer vision were analyzed in this paper and categorized into (a) seed quality analysis, (b) soilanalysis, (c) irrigation water management, (d) plant health analysis, (e) weed management (f) livestock manage-ment and (g) yield estimation. The paper also discusses recent trends in computer vision such as generative ad-versarial networks (GAN), vision transformers (ViT) and other popular deep learning architectures. Additionally,this study pinpoints the challenges in implementing the solutions in the farmer ’sﬁeld in real-time. The overall ﬁnding indicates that convolutional neural networks are the corner stone of modern computer vision approachesand their various architectures provide high-quality solutions across various agriculture activities in terms of pre-cision and accuracy. However, the success of the computer vision approach lies in building the model on a qualitydataset and providing real-time solutions.© 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Key words:Agriculture automationComputer visionDeep learningMachine learningSmart agricultureVision transformers
Contents1 . I n t r o d u c t i o n ............................................................... 2 1 22 . C o m p u t e r v i s i o n a n d d e e p l e a r n i n g m o d e l s ................................................. 2 1 32.1. Image classiﬁc a t i o n w i t h C N N a n d O b j e c t d e t e c t i o n m o d e l s ...................................... 2 1 4Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
Abbreviations:AI, Artiﬁcial Intelligence; ANN, Artiﬁcial Neural Network; BP, Back Propagation; C-GAN, Conditional Generative Adversarial Network; CNN, Convolutional Neural Network; COCO, Common Objects in Context; CV, Computer Vision; DCNN, Deep Convolutional Neural Network; DL, Deep Learning; DNA, Deoxyribo Nucleic A cid; RCNN, Region- based Convolutional Networks; FCN, Fully Convolutional Networks; FLDA, Fisher's Linear Discrimination Analysis; GAN, Generative Adversarial Ne twork; GLCM, Grey Level Co- occurrence Matrix; GPU, Graphic Processing Units; HOG, Histogram of Oriented Gradients; KNN, K- Nearest Neighbour; LBP, Local Binary Patterns; LCT F, Liquid Crystal Tunable Filters; LDA, Linear Discriminant Analysis; LIDAR, Light Detection and Ranging; LSTM, Long Short-Term Memory; MHA, Multi Headed Attention; ML, Machine Lear ning; MLP, Multi-Layer Perceptron; NASNet, Neural Search Architecture Network; NLP, Natural Language Processing; OCR, Optical Character Recognition; PEAT, Progressiv e Environmental and Agricultural Technologies; PLF, Precision Livestock Farming; ResNet, Residual Network; RF, Random Forest; RGB, Red Green Blue; SegNET, Semantic Segmentation N etwork; SSD, Single Shot Multibox Detector; SVM, Support Vector Machine; UAV, Unmanned Aerial Vehicle; VGG, Visual Geometry Group; ViT, Vision Transformers; WSN, Wireless Sensor Network; YOLO, You Only Look Once.⁎Corresponding author to: V.G Dhanya, ICAR - Indian Institute of Seed Science, Mau, Uttar pradesh 275101, India.⁎⁎Corresponding author to: A. Subeesh, ICAR - Central Institute of Agricultural Engineering (CIAE), Bhopal, Madhya Pradesh 462038, India.E-mail addresses:dhanya.vg@icar.gov.in(V.G. Dhanya),subeesh.a@icar.gov.in(A. Subeesh).
https://doi.org/10.1016/j.aiia.2022.09.0072589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/2 . 2 . G e n e r a t i v e a d v e r s a r i a l n e t w o r k ( G A N ) a n d V i s i o n T r a n s f o r m e r s ( V i T )................................. 2 1 43. Deep learning driven computer vision –A p p l i c a t i o n a r e a s i n a g r i c u l t u r e .................................... 2 1 5 3 . 1 . S e e d q u a l i t y a n a l y s i s ........................................................ 2 1 53 . 2 . S o i l a n a l y s i s ........................................................... 2 1 63 . 3 . I r r i g a t i o n m a n a g e m e n t ...................................................... 2 1 73 . 4 . P l a n t h e a l t h a n a l y s i s ........................................................ 2 1 83 . 5 . W e e d m a n a g e m e n t ........................................................ 2 1 93 . 6 . L i v e s t o c k m a n a g e m e n t ....................................................... 2 2 03 . 7 . Y i e l d e s t i m a t i o n .......................................................... 2 2 14 . P r a c t i c a l i m p l i c a t i o n s ........................................................... 2 2 15 . C h a l l e n g e s a n d w a y f o r w a r d....................................................... 2 2 36 . C o n c l u s i o n s ............................................................... 2 2 4R e f e r e n c e s .................................................................. 2 2 4
1. IntroductionThe UNDP 2021 report on“leveraging digital technology for sustain-able agriculture”states that global food production needs to be in-creased by 98 percent to feed a burgeoning human population of 9.9billion by 2050 (Burra et al., 2021). This target needs to be accomplishedthrough the effective utilization of available resources viz land, labor,capital, and technology (Ranganathan et al., 2018). Present status on precision agriculture aims to deﬁne the decision support system forfarm management by optimizing the output while consecutively pre-serving the resources applied. Constructively pointing out, the emergingtrend of food security needs to be handled with data-driven farmingthat can increase productivity, efﬁciency, and proﬁts. The key challenges such as food demand, labor shortage, water shortage, climate change(Badrzadeh et al., 2022;Elbeltagi et al., 2022a;Kaack et al., 2022)a n di n - creasing energy demands lead to the need for technology intervention.The opportunity offered by smart agriculture, which encompasses pre-cision agriculture, digital agriculture as well as modern agriculturalpractices, needs prime validation at this point. Smart agriculture is pri-marily based on three platforms viz, science, innovation, and ICT (Infor-mation and Communication Technology) ( Khanna and Kaur, 2019). The traditionally used information and knowledge management system forcollecting and monitoring agricultural data is not only laborious but isalso time-consuming and error-prone. Therefore, the technical ad-vancement in remote sensing, digital applications, sensors, advancedimaging systems, cloud data storage along with intelligent data analysisusing the decision support systems need to be well utilized in makingthe farming sector smarter (Fig. 1). Smart agriculture can leveragecutting-edge technologies like the Internet of Things, Machine learning,Cloud computing, Blockchain,etc., and beneﬁt from these opportunities in improving food production and addressing the emerging challengesin this sector (Sami et al., 2022).Recently, the inﬁltration of computer/ mobile technology even to themost rural pockets, has provided an inimitable facility in connecting therural producers with the city-consumers or the international investors,thereby facilitating better investments and knowledge transfer in agri-culture (Aker, 2011;Karim et al., 2013). Artiﬁcial intelligence (AI) is a game-changing technology that already has proven track records acrossvarious industries, including agriculture ( Adnan et al., 2021;Bhagat et al., 2020;Jamei et al., 2022b;Kumar et al., 2019;Subeesh et al., 2019). The use of machine learning, a subset of arti ﬁcial intelligence, has been covered extensively by researchers in delivering innovativesolutions for modelling complex relationships and further, making pre-dictions on agriculture data (Bhavsar and Panchal, 2012;Heramb et al., 2022;Jamei et al., 2022c;Karbasi et al., 2022
;Malik et al., 2022a;Rai et al., 2022;Rehman et al., 2019;Tantalaki et al., 2019). Computer vi- sion, aﬁeld of artiﬁcial intelligence, is making a machine “see”, using the modern technologies involving a camera and computer instead ofhuman vision, empowering extensive automation capabilities to AI sys-tems. Computer vision collects necessary visual data regarding crops,livestock, farm or garden, allowing us to identify, detect and track spe-ciﬁc objects using visual elements and comprehend complex visualdata for automation tasks. In the past decades, expert and intelligentsystems based on computer vision technology have been well utilizedfor agricultural operations (Foglia and Reina, 2006;Gomes and Leta, 2012;Rico-Fernández et al., 2019). Further, the development of moderntechnologies and hardware supports like Graphic Processing Units(GPUs) and edge devices have diversiﬁed the application of computer
Fig. 1.Components of smart agricultural solutions.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
212vision, thereby making strands to efﬁcient agricultural production (Li et al., 2019;Mochida et al., 2019;Rehman et al., 2019;Vázquez- Arellano et al., 2016). Modern computer vision techniques can help inthe digital quantiﬁcation of different morphological and physiologicalplant parameters along with the qualitative assessment of the sameand are expected to rapidly improve the accuracy of plant phenotyping(Araus and Cairns, 2014;Ghanem et al., 2015). Further, combining the computer vision techniques with the high throughput molecular meth-odologies of DNA sequencing provides an opportunity for genome-wideexploration of useful genes and molecular modeling of the same to un-derstand the complex traits such as plant yield and productivity, stresstolerance, biotic and abiotic stress management etc ( Araus et al., 2018; Shakoor et al., 2017). Thus imaging with computer vision technologyaided by various imaging sensors and algorithms can indeed play amajor role in precision agriculture and in paving the way for smart ag-riculture (Araus et al., 2018). A data driven precision agriculture systemarchitecture consists of sensors deployed on the ﬁelds (sensing layer), network layer that provides connectivity, storage and other services(service layer) and application layer consisting of the end user accessingthe services through mobile/web-based applications ( Fig. 2). Integrative and multi-mode Artiﬁcial Intelligence (AI) models can be deployed topredict crop behaviour under differingﬁeld conditions (Shrivastava and Marshall-Colon, 2018;Waldhoff et al., 2017). The yield perfor- mance of major crops in various regions, along with the ﬁeld conditions for crop production, environmental impact and economic outcome,have been assessed using the algorithms of deep learning and machinelearning (Tantalaki et al., 2019). Deep learning permits the computa-tional models with multiple processing layers to indicate the data inmultiple levels of abstraction (Schmidhuber, 2015). The main applica- tion of deep learning in theﬁeld of agriculture are building models toderive meaningful insights from agriculture data ( Jamei et al., 2022a; Malik et al., 2022b), image analysis including classiﬁcation and object detection, such as the detection of diseases, weed identi ﬁcation, soilanalysis, plant disease detection, etc. ( Kamilaris and Prenafeta-Boldú, 2018).For this study, we have collected more than 100 research papersfrom scientiﬁc databases, including PubMed, Web of Science, andScopus, in the area of deep learning-based computer vision. Further,we investigated all these works that leveraged deep learning-basedcomputer vision technologies to address key agriculture tasks such asplant health monitoring, disease and weed identi ﬁcation, irrigation management, soil analysis, livestock management, yield estimation,etc. The main objective of this study is to evaluate the penetration ofdeep learning-based computer vision approaches in key agriculturalproblems, and this review is intended to be useful to agriculture re-searchers as well as general computer vision researchers who are inter-ested in the application of computer vision solutions to automate andsolve potential agricultural problems. The practical implications of
these technologies along with major challenges in implementinglarge-scale applications were also constructively pointed out in thisstudy.2. Computer vision and deep learning modelsComputer vision possesses dual and interrelated goals. In biologicalscience, computer vision aims to represent the human visual systemusing computational models, and in the engineering perspective, com-puter vision attempts to create autonomous systems that can do tasksthat often human visual systems cannot perform ( Huang, 1993). Com- puter vision imparts visual capability to machines through cameras,data, models, and algorithms rather than retinas and the visual cortex.Optical character recognition (OCR) technology and intelligent charac-ter recognition were some major tasks that employed computer visionto accomplish tasks such as document and invoice processing, vehicleplate detection, etc. In the early stages of computer vision research,the main focus was to build algorithms to detect edges, curves, corners,
Fig. 2.Data-driven precision agriculture system architecture.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
213and other basic shapes. Before the era of deep learning, image process-ing relied on gray level segmentation and this approach wasn ’tr o b u s t enough to represent complex classes. Modern computer vision algo-rithms rely extensively on artiﬁcial neural networks that provide a dra-matic improvement in performance and accuracy compared totraditional approaches for image processing. Deep learning-based com-putation models allow multiple processing layers to learn and infercomplex patterns mimicking the human brain ( O’Mahony et al., 2020; Schmidhuber, 2015;Zhong et al., 2016). It runs and inspects the data over several iterations until it discerns distinctions and identi ﬁes or rec- ognizes the features in the images. The recent surge of interest in deeplearning is due to the fact that it can handle massive amounts of hetero-geneous data (visual, audio, text, etc.) and is capable of embedding so-lutions into several hardwares. DL allows automatic feature extractionand can be utilized in numerous image processing tasks and is wellknown for its effectiveness in handling vision-based activities likeimage classiﬁcation, object detection, semantic segmentation, etc. Infact, these tasks are the backbone for modeling and automating agricul-tural activities such as disease identiﬁcation, weed detection, yield esti- mation, etc (Jha et al., 2019;Subeesh and Mehta, 2021;Tian et al., 2020).2.1. Image classiﬁcation with CNN and Object detection modelsConvolutional neural network-based deep learning architectures arepopular for computer vision tasks like image classi ﬁcation. A convolutional neural network is a type of neural network architecturethat takes input images and extracts relevant features to ef ﬁciently identify and classify images. CNN uses labels to perform convolutionsand generate feature maps. The introduction of imageNet dataset thatcontained millions of tagged images had laid a foundation andbenchmark for building advanced computer vision-based models(Kriegeskorte and Golan, 2019;Miikkulainen et al., 2019;Yoo, 2015). LeNet-5 was one of the earliest CNN proposed by Yann LeCun ( LeCunet al., 1998), led to the development of various CNN models ( Fig. 3). In 2012, AlexNet architecture (Krizhevsky et al., 2012a) was found prom- ising for image recognition, and numerous new architectures such asVGGNet (Simonyan and Zisserman, 2015), ResNet (He et al., 2015), etc. were also introduced by researchers, reducing the error rate and im-proving the performance. Image segmentation approaches are quiteuseful for understanding what an image consists of, by dividing the im-ages into several segments. Image segmentation creates a pixel-oriented mask for each object present inside the image. This eases theimage processing tasks as the important segments alone can be consid-ered for processing tasks.The image classiﬁcation mainly identiﬁes the class, a speciﬁci m a g e belongs to. The image classiﬁcation approach is often not successfulwhen there are multiple objects in the same image. Object detectionaims to detect the location of objects in the image/video. Object detectiontask comprises two major components; class information and location in-formation. The location information is described by bounding boxesaround the target object. Object detection architectures such as YOLO(You Only Look Once) (
Redmon et al., 2016), SSD (Single Shot Multibox Detector) (Liu et al., 2016), Faster-RCNN (Region Convolutional Net-works) (Ren et al., 2016) are widely used for object detection and auto-mation across different domains including agriculture.2.2. Generative adversarial network (GAN) and Vision Transformers (ViT)A generative adversarial network (GAN) is a special type of neuralnetwork used for unsupervised learning. GAN is an approach to genera-tive modeling that can learn to mimic a given distribution of data. Thesemodels effectively reduce the data into its fundamental properties orgenerate new data points with varied properties. The applicationof GANs has achieved state-of-the-art performance in many imagegeneration tasks, such as text-to-image synthesis ( Xu et al., 2017), super-resolution (Ledig et al., 2017), and image-to-image translation
Fig. 3.Architecture of (a) Convolutional neural networks (b) LeNet-5 architecture.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
214(Zhu et al., 2020b). Generally, GAN has two main building blocks (twoneural nets) which compete with each other and are capable of captur-ing, copying and analyzing the variations in a dataset ( Fig. 4). The two networks are usually called Generator and Discriminator. The generatorneural network helps to generate new instances, while the discrimina-tor neural network evaluates the authenticity of the generated images.The discriminator decides whether or not every instance of the data itevaluates belongs to the actual training set and penalizes the generatorfor generating implausible outcomes. The loss of the discriminator isused for improving the generator (Reimers and Requena-Mesa, 2020). The discriminator tries to identify the fake data from the real data, andboth networks work simultaneously to learn complex data. GANs area panacea for the data scarcity problem, which is a serious hurdle in de-veloping robust deep neural network models ( Hiriyannaiah et al., 2020). The realistic images produced by GAN that are different fromthe original training data are attractive in data augmentation of DL-computer vision to reduce the model over ﬁtting. Transformer models have become the de-facto status quo in textprocessing, and recently, the computer vision community has extendedthe concept of NLP (Natural Language Processing) transformer to applyto the image domain with slight modiﬁcation in the implementation to process multiple modalities (e.g., images, videos, etc.) using similar pro-cessing blocks (Dosovitskiy et al., 2021;Khan et al., 2021;Vaswani et al., 2017). Even though the general architecture used in both cases are sim-ilar, ViT uses different approaches for tokenization and embedding(Fig. 5). The overall architecture consists of 3 main components, viz.,patch embedding, feature extraction by stacked transformer encodersand the classiﬁcation head. In ViT, initially, the input image of shape(height, width, channels) is embedded into a feature vector of shape(n+1, d), using a set of transformations. The input image is split into agroup of image patches. Later, these groups of image patches are em-bedded into encoded vectors and fed into transformer encoder network.The transformer encoder learns the features from the embeddedpatches using a stack of transformer encoders ( Wu et al., 2021). The en- coder mainly comprises multi-headed attention (MHA) and a 2-layerMLP with layer normalization and residual connections. The ﬁnal MLP block, called the MLP head is used as an output of the transformer. Inthe case of image classiﬁcation, a softmax on the output generates theclassiﬁcation outputs. ViTs are useful in several vision applications suchas image classiﬁcation, image-to-text, text-to-image generation, imagesegmentation, object detection, etc (Bazi et al., 2021;Li et al., 2022).3. Deep learning driven computer vision –Application areas in agriculture3.1. Seed quality analysisThe commercial seed industry is focused on the supply of the rightquality seeds to the farmers at the right time in the right quantity. Filter-ing out low-quality of seeds from high-quality ones, is not only labori-ous, but it requires sophisticated equipments, infrastructure, and time(Kannur et al., 2011). The testing of seeds for their quality can indeedgain momentum by the use of computer vision technology which canextract the morphological information of different seed lots and gradeit according to the internationally prescribed quality standards ( Bao and Bambil, 2021). The different seed testing modules are likely to ad-dress their physical purity, genetic purity, seed health, vigour, patternsof deterioration etc., which in general may indeed cover the physicalor visually attributable characters such as the seed length, shape, size,visual impairments, and presence of foreign bodies which can indeedbe captured by the advanced computer vision technology (
Granitto et al., 2005).Performance issues of traditional computer vision have greatly beenimproved by deep learning-based computer vision, resulting in largeradoption for seed variety identiﬁcation. The seed quality evaluation pro-cess using computer vision is shown in Fig. 6. Often, spectral imaging techniques are also merged with these approaches to enhance the accu-racy (Qiu et al., 2018;Zhu et al., 2019). In a study conducted byZhu et al. (2019), combining spectroscopy and machine learning –CNN models were found to be effective in identifying the seed varieties. The machinelearning models showed an accuracy of more than 80% in classifying thecotton seeds based on the feature extracted by the CNN and ResNetmodels. In another investigation, SeedSortNet built from computer vi-sion CNN models, was found to be promising, with accuracies 97.33%and 99.56% in sorting the maize and sun ﬂower seeds (Li et al., 2021). CNN deep learning was also utilized for cognizing the viable andnon-viable seeds and was found to be successful with 90% viabilityprediction accuracy for naturally aged seeds ( Ma et al., 2020).
Fig. 4.Overview of training process in GAN (Generative Adversarial Network).V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
215Taheri-Garavand et al. (2021)developed models for automatic identiﬁ- cation of chickpea varieties using seed images in the visible spectrum. Amodiﬁed VGG16 model was used for the identi ﬁcation purpose. As sorting high-quality seeds are vital for increasing yield in the breedingindustry,Zhao et al. (2021)employed seven different computer visionmodels to accurately detect and identify surface defects. MobileNet-V2model had shown excellent detection accuracy for the soybean dataset.There are numerous such studies done by various researchers and theseed industry is hugely getting beneﬁted from advanced computer vi- sion models, achieving a higher level of automation capabilities. Someof the studies in this area are precisely summarized Table 1.3.2. Soil analysisThe preservation and improvement of dynamic soil characteristics isthe main emphasis of soil management in agriculture for increasingcrop productivity (Kushwaha et al., 2022;Suchithra and Pai, 2020). Tra- ditional soil texture analysis entails taking soil samples and bringingthem to a laboratory, where they are dried, crushed, and sieved beforebeing used. For coarse textured or sandy soils, sieving is the most typicallaboratory analytical method, while for smaller textured particles, a hy-drometer or pipette approach based on sedimentation theory is used(Kushwaha et al., 2022;Sudarsan et al., 2016). With the advancement of image processing power and the develop-ment of image acquisition (e.g., cameras) systems in recent years, com-puter vision-based image analysis approaches have gotten a lot ofinterest in a lot of sectors, including soil science. This method collectssoil images (dynamic or static) with cameras and then uses simple com-puter programmes to classify and categorise them ( Fig. 7). For example, after matching textural patterns, the size of the soil particles mightbe estimated straight from the image. In several investigations,various image analysis-based computer vision approaches were tried.
Fig. 5.The architecture of Vision Transformer Model for image classi ﬁcation (Dosovitskiy et al., 2021;Vaswani et al., 2017).
Fig. 6.Seed quality analysis using data-driven models.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
216Haralick et al. (1973)attempted to classify images received from an ae-rial or satellite source using entropy and angular moment-based tex-tural classiﬁcation. Since then, the grey level co-occurrence matrix(GLCM) and its analogues have been used in a variety of remote sensingapplications (Dell’Acqua and Gamba, 2003;Kuplich et al., 2005). How- ever, the greatest resolution satellite can only provide a maximum res-olution of 10 m/square pixel, which is insuf ﬁcient to understand soil particle sizes.Riese and Keller (2019a)implemented three 1- dimensional (1D) convolutional neural networks: the LucasCNN, theLucasResNet and the LucasCoordConv. In addition, for the classi ﬁcation problem at hand, the study tweaks two existing 1D CNN techniques andcompares the CNN techniques against a random forest classi ﬁer to see how well they do. Thereby, study uses the LUCAS topsoil dataset,which is freely available. The CNN method with the least amount ofdepth turns out to be the most effective classi ﬁer. In terms of average ac- curacy, the LucasCoordConv has the best results.Similarly, Zhang et al. (Zhang et al., 2003) proposed a soil texture classiﬁcation system that uses the wavelet transform approach to dis-tinguish between different types of soil. Wavelet transform, which is astrong image and signal analysis method due to its multi-resolution ca-pabilities, is used to extract features. A set of training instances is used tocreate a maximum likelihood (ML) classi ﬁer. This method of MLparameter estimation produces the best results. At the time of trainingand classiﬁcation, the Fisher's Linear Discrimination Analysis (FLDA) isused to optimize and reduce the dimension of the vector. Soil texturessuch as clay, sand, and silt are employed for training and classi ﬁcation. Clay, sand, and silt have 60 percent, 100 percent, and 100 percent cate-gorization rates, respectively. In instance segmentation, Zhang et al.(Zhang et al., 2020) suggested a mask reﬁned R-CNN for reﬁning object details. The goal is toﬁgure out how semantic segmentation of high-level and low-level features affects instance segmentation. The COCO(common objects in context) and cityscapes datasets were used to col-lect the trial results. This approach is reported to be simple to use andeffective. Some of the previous signiﬁcant studies in soil analysis using DL computer vision have been summarized in Table 2.3.3. Irrigation managementIrrigation water management in agricultural production necessitatesconsiderable effort and is crucial in maintaining hydrological, climato-logical, and agronomic equilibrium. Several studies have thus been un-dertaken in gaining knowledge of the biophysical processes includedin the uptake of water through the root zone of the soil and the pro-cesses of transpiration through the plant canopy ( Elbeltagi et al.,Table 1Previous studies on seed quality analysis through application of computer vision and deep learning.Reference Objectives and scenarioof applicationMethodology Crop Results(Javanmardiet al., 2021)Corn varietyclassiﬁcation using 9different varietiesCNN as a generic feature extractor. Classi ﬁcation using ANN, SVM, kNN, boosted tree, bagged treeand LDACorn CNN ANN classiﬁcation has a classiﬁcation accuracy of 98.1%, precision 98.2%, recall 98.1% and F1 score of 98.1%.(Qiu et al., 2018) Variety identiﬁcation inriceKNN, SVM and CNN models Rice CNN outperformed other models with 89.6% accuracy on the training set and 87% accuracy on the testing set. (Gulzar et al.,2020)Seed classiﬁcationusing 14 types of seedsVGG16 architecture for classiﬁcation - 99.9% accuracy over test set with 234 images(Wu et al., 2019) Variety identiﬁcation inoatsDCNN model Oats 99.19% accuracy on testing set.(Gulzar et al.,2020)Seed classiﬁcation inmaize and sunﬂowerCNN model Maize and SunﬂowerCNN based visual model - SeedSortNet developed with 97.33percent accuracy on maize and 99.56% accuracy on sun ﬂower dataset respectively. (Liu et al., 2015) Soyabean seed sorting BP neural network Soyabean 97.25% average recognition accuracy over 857 images of soybean seeds with pest and insect damage. (Veeramaniet al., 2018)Corn seed defectdetectionVGG 19 and GoogleNet Maize -(Dolata andReiner, 2018)Varietal identiﬁcationin barleyCNN Barley Increase in average classi ﬁcation accuracy by 0.6% and sensitivityby 2.3% with respect to view point ignorant architecture of the saidstudy. (Kurtulmuş,2021)Seed classiﬁcation in
sunﬂowerAlexNet, GoogleNet and ResNet Sun ﬂower 95% accuracy with GoogleNet algorithm for classi ﬁcation of 4800 sunﬂower seeds. (Ni et al., 2019) Seed grading in maize DCNN Maize 98.2% prediction accuracy for 408 test images in maize.
Fig. 7.Soil texture analysis using image processing.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
2172022b;Kushwaha et al., 2021). For an effective irrigation schedule, it isnecessary to know the precise amount of water required by the crop(Kushwaha et al., 2016;Vishwakarma et al., 2022). The application of computer vision technologies, as well as the integration and deploy-ment of automated crop production management, plant irrigation, andyield evaluation, thus become critical. Zhang et al. ( Zhang et al., 2018) performed identiﬁcation and monitoring of centre pivot irrigation sys-tems using a Convolutional Neural Networks (CNNs) approach to the al-location of irrigation water. The CNNs with various structures were builtand compared and for data augmentation,training, a sampling strategywas developed. In the testing region, the CNN with the best perfor-mance and the shortest training time was used. To further pinpointthe centre of each centre pivot system, a variance-based techniquewas presented. The proposed approach performed well in the centrepivot irrigation systems identiﬁcation challenge, with a precision of95.85% and a recall of 93.33% of the identi ﬁcationﬁndings. Similarly, Chang and Lin (Chang and Lin, 2018) developed a compact intelligent agricultural machine which is capable of autonomousweeding and variable watering on the cultivated ground, using a combi-nation of computer vision and multitasking. The system classi ﬁes the plants and weeds in real-time so that it can weed and water while main-taining an average herbicidal rate of 90% and a deep soil moisture levelof 80%. This strategy has a lot of potential because it allows for not onlymultitasking integration but also resource utilization in its entirety.Kamyshova et al. (Kamyshova et al., 2022) proposed a computer vision-based technology for optimizing the watering process of cropsutilizing a phyto indication system in low latency mode, the study sug-gested an algorithm-based system for obtaining a maize irrigation map.The system, which comprises 8 IP cameras coupled to a DVR connectedto a laptop, can be mounted on a centre pivot irrigation system. Thereare three steps to the algorithm. Using an integrated excess green andexcess red difference (ExGR) index during the image preprocessingstage. The application of the approach that the study chose based onthe system's operational conditions is the categorization stage. A neuralnetwork trained using the Resilient Propagation method is utilised intheﬁnal stage to calculate the rate of watering of plants in the cur-rent sector of the sprinkler site. Plant identi ﬁcation accuracy was up to 93 percent, and growth stages were up to 92 percent. Low-cost cameras are now being used in all sectors of technology, partic-ularly in agricultural applications. The soil water balance may beprecisely assessed to enable accurate irrigation planning by acquir-ing relevant information on the growth of horticulture crops throughphotographs (Koech and Langat, 2018).Table 3shows the irrigationwater management through the application of computer vision anddeep learning technologies.3.4. Plant health analysisWith the advancement in computer vision and deep learning, newpromising solutions for identifying overall health status of the plantswere introduced. The intelligent decision support system for identifyingcrop diseases (Fig. 8), water stress, and nutrient deﬁciencies would lead to timely control of the panic situations and eradicating the huge losses,ultimately leading to improved plant quality.Plant stress induced by biotic and abiotic factors is expressed in theplant canopy as multiple symptoms. In case of water stress, the plantcloses stomata and delays photosynthesis and transpiration activitiesindicating colour changes in the leaf and temperature ( Nilsson, 1995). Similarly, nutrient deﬁciencies-related symptoms are typically visiblein leaves color and texture (Xu et al., 2011). Image analysis can detect
these changes in a pattern quite effectively. Deep learning-based com-puter vision approaches are viable solutions in addressing timely dis-ease identiﬁcation and avoiding consultation of human experts. Theavailability of a large number of public image datasets such asPlantVillage (Hughes and Salathe, 2016), PlantDoc (Singh et al., 2020) have proliferated the research in the area of disease identi ﬁcation and many works have taken encouraging steps towards disease-free agri-culture (Hassan and Maji, 2022;Ji and Wu, 2022;Nagasubramanian et al., 2019).The PlantVillage dataset has been extensively utilized by various re-searchers for solving disease identiﬁcation problems using deep learn-ing (Amara et al., 2017;Brahimi et al., 2017;Ferentinos, 2018; Mohanty et al., 2016). Several studies reveal that pre-trained modelsquickly and accurately identifying the diseases in terms of precision, re-call and F1 scores (Abbas et al., 2021;Chen et al., 2020b;Coulibaly et al., 2019;Mukti and Biswas, 2019;Thakur et al., 2021). Abbas et al. (Abbas et al., 2021) used synthetic images generated using the ConditionalGenerative Adversarial Network (C-GAN) to build tomato leaf diseasedetection. C-GAN can address the issue of data insuf ﬁciency and provide more generalization to the models ( Mirza and Osindero, 2014). It is worth noting that some investigations were focused on the localizationof the disease spots, giving precise information about the diseases ( Cen et al., 2016;Liu and Wang, 2020;Mathew and Mahesh, 2022;Son, 2021). Several other studies reported research on DL-computer visionbased identiﬁcation of crop stresses, including water stress and nutrientdeﬁciencies (Abdalla et al., 2021;Anami et al., 2020;Jahagirdar and
Table 2Previous studies on computer vision and deep learning technologies for soil properties analysis and management.Reference Objectives andscenario ofapplicationMethodology Results(Riese and Keller,2019b)Soil textureanalysisThe CNN architectures LucasCNN, the LucasResNet and theLucasCoordConvModels The CNN method with the least amount of depth turnsout to be the most effective classi ﬁer (Omondiagbeet al., 2022)Soil texturepredictionEmployed automated deep convolutional neural networks andpopulation-based learning by replacing the random search with a BayesianOptimization.Results show improvements of 5% to 26% for all three soilproperties such as sand, silt and clay.(Pyo et al., 2020) Estimation ofheavy metalconcentrationFrom the soil reﬂectance images, CNN with convolutional autoencoders wastrained to estimate As, Cu and Pb metals. The highest accuracies reported for As, Cu, and Pbestimates were with R
2values of 0.86, 0.74, and 0.82.(Zhong et al.,2021)Soil properties The DCNN architectures LucasResNet-16 and LucasVGGNet-16 models When compared to a single-task DCNN model, the performance of a multi-task DCNN model created basedon LucasResNet-16 was enhanced. (Yu et al., 2019) SoilClassiﬁcationLquid crystal tunableﬁlters (LCTF)-based system and three-dimensionalconvolutional neural network (3D-CNN) for soil classi ﬁcationThe overall accuracy of 99.59% for 3D-CNN-SD-PCA.(Azadnia et al.,2022)Texture Analysis Portable smartphone-based machine vision system using CNN wasdeveloped. The features were extracted using CNN and classi ﬁcation is performed using ANN, SVM, RF and KNN classi ﬁers.Model accuracies at distances of 20, 40 and 60 cm wereof 99.89, 99.81 and 99.58%,(Azadnia et al.,2022)Texture analysis Deep learning models VggNet16, ResNet50, and Inception-v4 models wereused to classify soil aggregates Overall accuracy obtained for CNN networks was 96.2%,97.1%, and 98.7%V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
218Budihal, 2021).Table 4shows the previous studies on deep learningbased computer vision technology on plant health analysis.3.5. Weed managementWeeds are among the major factors that affect agricultural produc-tion negatively. With the focus on improving agricultural productivity,it is evident that more and more chemicals are being dumped into theenvironment with the aim of managing the weed growth. But for im-proving the productivity, it also requires the optimum utilization of re-sources which can only be achieved by the precise spraying on weeds.The traditional robotic weeders generally function by detecting croprow patterns and they do not rely on crop recognition for the weedingoperation. If the weed density and population are large, they may ob-scure the row pattern leading to reduced ef ﬁciency of the weeders.Computer vision approaches come to rescue at this point by accuratelyidentifying the objects as precise spraying of weeds depends on the ac-curate identiﬁcation and location of weeds. Recently, several studieswere carried out by researchers on adaptability of computer vision tech-nology for the agronomic classiﬁcation of plant species at theﬁeld level, viz the classiﬁcation of crops from weeds, off types etc. ( Sau and Ucchesu, 2019;Sau et al., 2018;Subeesh et al., 2022). Detailed applica- tion of the same in the automatic identiﬁcation of plant species based on the leaf recognition pattern has been proposed for preserving andcataloguing plant species (Putzu et al., 2016) along with the botanical characterization of germplasm (Lo Bianco et al., 2017). Methods of achieving weed detection at theﬁeld level mainly include the utilizationof computer vision technology using the traditional image processingand deep learning. When, the conventional methods of computer visionare used, extracting the different features such as colour, shape, textureTable 3Previous studies on irrigation water management through application of computer vision and deep learning approaches.References Objectives and scenario of application Methodology Results(Albuquerqueet al., 2020)Identiﬁcation of malfunctioning in theirrigation systems Mask R-CNN based segmentation on UAV capturedimages Given dataset sizes, the results are satisfactory.(Chen et al.,2020a)Identiﬁcation of water pollution foragricultural irrigation resources Shallow CNN model in combination with decisiontree algorithm trained on NIR data Validation results were 25.47 of RMSEV and 0.914 ofRv. (Zhang et al.,2018)Monitoring and identiﬁcation of canter pivotirrigation system to supply irrigation water CNN based segmentation on UAV captured images Precision and recall of 95.85% and 93.3 percent, respectively, were attained. (Tang et al., 2021) Monitoring the distribution of center pivotirrigation systems Lightweight real-time object detection network(PVANET) based on GoogLeNet and HoughtransformExperiments with Sentinel-2 images achieved aprecision of 95% and a recall of 95.5%,(Kumbi and Birje,2022)Irrigation efﬁciency Sun- ﬂower Atom Optimization-based Deepconvolution neural network (SFAO-DeepCNN)algorithmMaximal accuracy of 92%, speciﬁcity of 91.2% and sensitivity of 94.1%(Kim et al., 2022) Water Level Estimation of Irrigation Channel ResNet-50 image classi ﬁcation and U-Netsegmentation models on irrigation canal's CCTVimagesThe image segmentation model showed a Dice scoreof 0.998 and predicted water levels showed R
2of 0.97
Fig. 8.Deep learning based computer vision approach for plant health analysis.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
219etc., and combining them with the machine learning methods such asthe SVM becomes necessary. But with the improvement in computingpower, the deep learning algorithms can bene ﬁcially extract multidi- mensional and multi-scale spatial and semantic feature information ofweeds through AlexNet, VGGNet, ResNet, etc due to their enhanced ca-pability for image data expression thereby avoiding the disadvantagesof traditional methods of feature extraction. The application of deeplearning in agronomic classiﬁcation of plant species has gained momen-tum after the outbreak of CNN and AlexNet ( Krizhevsky et al., 2012b). Hall et al. (2015)have utilized the CNN architecture in classifying leavesof 32 species of crops and weeds by capturing nearly 1900 images of thesame.Utilization of CNN architecture in the classi ﬁcation and differentia- tion of weeds from different species of wheat, sugarbeet, corn, soybean,sunﬂower, etc. has been proposed byKussul et al. (2017), while the modiﬁed version of VGG16 for the classiﬁcation of barley, grass, oil crops and weeds have been proposed by Mortensen et al. (2016).Table 5shows the previously applied computer vision technology forweed management.3.6. Livestock managementComputer vision approaches are leveraged extensively in precisionlivestock farming (PLF), ensuring optimum output and health of eachindividual animal. Livestock monitoring systems provide real-time in-formation and assist farmers in making strategic decisions ( Fig. 9). The non-invasive computer vision technology has been widely researchedfor its use in recognition of livestock behaviour over the past fewyears (Bello et al., 2021;Kumar et al., 2017;Qiao et al., 2019a;Shen et al., 2020).Xiao et al. (2022)employed a modiﬁed Mask-RCNN model and trained a fusion of Mask-RCNN and SVM to identify cowsin unconstrained barn.Hansen et al. (2018)trained a CNN to recognize pigs via the face using a data set with 1,553 images. The VGG-face modelused in this study achieved an accuracy of 96.7%. Some of theTable 4Previous studies on computer vision and deep learning technologies for crop health analysis.References Objectives and scenario of application Methodology Crop Results(Hassan and Maji,2022)Plant disease identiﬁcation Novel lightweight CNN based on Inceptionand Residual connections with fewerparametersRice,CassavaThe testing accuracies of the proposed model is99.39%,99.66% and 76.59% on Plantvillage, Rice, andCassava dataset (Hati and Singh,2021)Species Recognition (SR) andIdentiﬁcation of Healthy and InfectedLeaves (IHIL)Residual network (ResNet) basedconvolutional neural network (CNN)architecture12 differentplantspeciesSpecies identiﬁcation: Precision 91.84%, Recall 91.67%and F191.49%. IHIL : Precision 84%, Recall 83.14% andF1 83.19% (Ji and Wu, 2022) Black measles disease identi ﬁcation in grape Plant disease evaluation. Imagesegmentation using DeepLabV3 withResNet50 backboneGrape Overall classiﬁcation accuracy of 97.75% on thehold-out test dataset.(Syed-Ab-Rahmanet al., 2022)Citrus diseases classiﬁcation usingleaf images Two-stage deep CNN model Citrus Detection accuracy of 94.37% and an average precision of 95.8%. (Li and Li, 2022) Leaf disease identiﬁcation Vision Transformer-based lightweightapple leaf disease- identiﬁcation model(ConvViT)Apple ConvViT achieved an accuracy of 96.85% on the appleleaf disease dataset(Mkonyi et al.,2020)Early identiﬁcation ofTuta absolutadisease Pre-trained CNN architectures VGG16,VGG19 and ResNet50 Models Tomato VGG16 attained the highest accuracy of 91.9%(Azimi et al.,2021)Stress level detection due to nitrogendeﬁciency Custom Deep learning architecture with 23layers. Sorghum 8.25% better accuracy than traditional machinelearning techniques (Joshi et al., 2021) Viral disease diagnosis Convolutional neural network - VirLeafNet Vigna mungoAccuracies of VirLeafNet-1, VirLeafNet-2, andVirLeafNet-3 were 91.234%, 96.429%, and 97.403% (Shah et al., 2021) Plant disease detection ResTS Architecture with residual
connection 14 crops F1-Score: 0.991(Singh et al., 2021) Pest and disease detection 2D-CNN model with segmented images Coconut treeAccuracy of 96.94% with a Kappa value 0.91
Table 5Previous studies on computer vision and deep learning technologies for weed management.References Objectives and scenario ofapplication Methodology Crop Results(Le et al., 2020) Weed identiﬁcation inCanola, corn and raddish Filtered Local Binary Pattern withContour Maskand Coefﬁcient k (k-FLBPCM), VGG-16,VGG-19,ResNet-50, Inception-v3Canola, corn,radishK-FLBPCM method outperformed other state of theart CNN models.(Osorio et al., 2020) Weed detection in lettuce Compared Mask R-CNN with HOG SVM and YOLO V3 Lettuce 98% accuracy for Mask R-CNN(Chavan andNandedkar, 2018)Weed identiﬁcation in paddyﬁeld Comapred SegNET with FCN and U-Net Rice 92.7% accuracy for SegNet(Chavan andNandedkar, 2018)Weed classiﬁcation atﬁeldlevel Comapred Hybrid network with VGGNet andAlexNet Maize,wheat,sugarbeet98.23% accuracy for Hybrid network(Fawakherji et al.,2020)Crop/weed segmentationusing synthetic images Synthetic image generation using GAN andsegmentation models (UNET, BONNET,SEGNET,UNET-RESNET)Sugar beet All models were performed well with syntheticimages generated using GAN and IoU increaseddrastically using synthetic dataset. (Wang et al., 2020) Weed detection in sugarbeetand oilseeds FCN architecture employed Sugarbeet and oilseeds.Best MIoU value (pixel-wise segmentation) 88.91%and object-wise segmentation 96.12% (Espejo-Garciaet al., 2020)Detection of balck nightshade and velvet leaf intomato and cottonﬁledsCompared Modiﬁed Xception, with Inception -ResNet, VGG-Net, MobileNet and DenseNet Tomato andcottonCombination ofﬁne tuned Densenet and SVM.microF1 score of 99.29%.F1 score≥95% over repeated tests.(Huang et al., 2020) Weed in riceﬁeld FCN Rice Highest accuracy- VGG Net based FCN (VeeranampalayamSivakumar et al.,2020)Weed in soybeanﬁled Compared Single-Shot Detector (SSD), FasterR-CNN
Soybean Faster RCNN as the best model for weed detectionperformance and inference timeV.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
220investigations relied on data collection using unmanned aerial vehiclesto accurately detect and count the cattle ( Andrew et al., 2019; Chamoso et al., 2014;Rahnemoonfar et al., 2019;Rivas et al., 2018). Such detection and counting approach problems, in general, haveadopted either CNN-based probability heat map generation on the loca-tion of the animals or generation of bounding boxes for detection of theanimals. An improved Yolo model called ‘FLYOLOv3’(FilterLayer YOLOv3) based on Filter layer was introduced by Jiang et al. (2019)to ensure accurate detection of key parts of dairy cows. The performanceof this approach was superior to the Faster-RCNN and Yolov3algorithms.Daily activity patterns, food intake, and ruminating are some key in-dicators closely bound to the health and productivity of dairy cows(Huzzey et al., 2007;Weary et al., 2009). Some recent studies underline that traditional methods of direct observation and time-lapse video re-cording are slowly getting replaced by computer vision approaches.Yang et al. (2018)used a Faster-RCNN model to identify individualpigs from a group and subsequently assess the feeding area occupationrate to identify their feeding behaviour. To improve the accuracy offeeding behavior analysis, identify and exclude the non-nutritive visits(NNV) to the feeding area,Alameer et al. (2020)developed a GoogLeNet-based approach. The detection of feeding behaviour washighly accurate with 99.4% accuracy. CNN architectures are also foundto be promising for early cattle disease detection in the animal hus-bandry farm (Rony et al., 2021).Table 6shows the previous studies on computer vision technology for livestock management.3.7. Yield estimationEarly and accurate yield estimation is essential for farmers and otherstakeholders in making strategic decisions on post-harvest planning,policy-making and crop management ( Al-Gaadi et al., 2016; Chlingaryan et al., 2018;Wei et al., 2020). Some of the studies underline that yield estimation using deep learning-based computer vision on ae-rial images is superior to traditional approaches. In a study conducted byYang et al. (2019)rice grain yield from low-altitude remote sensing datawas used to estimate the rice grain yield using convolutional neural net-works. The models were trained on both RGB and multispectral imagescollected by UAV, and results showed that the CNN trained on these im-ages outperformed the VIs-based traditional regression models for grainyield estimation at the ripening stage.You et al. (2017)employed a combination of convolutional neuralnetworks and recurrent neural networks based on the remotely sensedimages to predict the soybean yield. Another investigation carried outbyRussello (2018)utilized satellite images in combination withconvolutional neural networks for crop yield prediction. In case of or-chard crops like citrus, computer vision approaches are quite straight-forward (Fig. 10). The yield can be estimated by directly counting thenumber ofﬂowers or fruits prior to the harvesting stages ( Cheng et al., 2017;Dorj et al., 2017;Kanwal et al., 2019). With an objective of esti- mating yield from citrus orchards, Apolo-Apolo et al. ( Apolo-Apolo et al., 2020)developed a Faster-RCNN model for the fruit detection.The data collected through UAV was used for the model development.In their study, based on the count, yield from orchards was modelledusing the Long Short-Term Memory (LSTM) model. An attempt wasmade by Zhou et al.(Zhou et al., 2020)to deploy the yield estimation models in smartphones as android applications. In his investigation,
four different computer vision models; SSD with MobileNetV2, quan-tized MobileNetV2, InceptionV3, and quantized InceptionV3 weretrained and converted to TensorFlow Lite models. As reported by stud-ies, fruit occlusion caused by leaves and twigs and varying illuminationconditions are some challenging factors in implementing fruit yield es-timation systems based on computer vision ( Maheswari et al., 2021). Table 7shows the previous studies on computer vision technology foryield estimation.4. Practical implicationsDespite being late for digitization, the agriculture sector has ﬁnally seen good momentum for the practical implementation of several arti-ﬁcial intelligence applications, including deep learning-based computervision approaches. Computer vision-powered disease identi ﬁcation ap- plications merge the expertise of genetic resources and arti ﬁcial intelli- gence, allowing farmers and extension workers to act quickly andrescue the crop. This disease detecting computer vision-enabled soft-ware is also being installed inside greenhouses, drones, and otherequipment to identify the issues and provide a faster response in takingpreventive measures. Advancement in computer vision technology hasbeen used by agricultural startups for building solutions that assistfarmers in harvesting, plant health monitoring, pest-weed control, etc.For pesticide application, blue river technology ( Blue river technology) developed a see and spray’technology that works based on camera
Fig. 9.Major applications of DL-Computer vision for livestock managementV.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
221inputs and computer vision algorithms. The algorithm can distinguishweeds from plants and perform targeted pesticide applications. Thestartup, cromai (Cromai) developed AI-driven land and crop diagnosticinformation. They provide a technological solution for georeferencedidentiﬁcation of weeds in the sugarcaneﬁeld using advanced artiﬁcial intelligence approaches. Harvesting robots are widely used in openﬁeld conditions, integrating with machine visions and achieving im-proved precision. Harvest CROO robotics ( Harvest croo robotics) devel- oped a fully autonomous harvester, employing a harvester-mountedLIDAR system to avoid collisions and accurate navigation. The computervision system scans each berry on the plant and determines the ripenessand health before harvesting. ‘Plantix’, the crop damage diagnosis mo- bile application (Plantix) developed by German startup PEAT (Progres-sive Environmental and Agricultural Technologies), uses deep learningand computer vision to help farmers to combat pests and diseases(Goncharov et al., 2018; Tibbetts, 2018) . The application’s functionality enables the end-user to upload crop images and get guidance on the dis-ease affected, symptom descriptions, treatment information, preventivemeasures, etc. With the same objective of identifying a large number ofplant diseases, other applications such as Agrio ( Agrio)w e r ea l s ointroduced to the farming community. Several technology-driven solu-tions were introduced into precision livestock farming to ensure opti-mal health and output of animals also. The technology startupCainthus (Cainthus) offers a computer vision-driven AI system fordairy farmers to monitor their cows and send timely alerts and reportsvia associated applications. Smart cameras are deployed to watch overthe activities of the cows to provide the right amount of feed availableon a timely basis. Similar to this, Piguard ( Piguard), an innovative live- stock management software, leverages deep learning-based computervision approach to monitor the health status and behavioral patternsof animals.Computer vision technology covers a broad spectrum of solutions forfarmers, from small AI-enabled mobile apps for decision support, overin-ﬁeld imaging sensors and remote sensing technologies for data col-lection, and to drones and robots for the automation of processes. Acrossthe globe, farming community has realized the potential of digital tech-nologies and for the past few years, there has been an increase in itsadoption. Some of the key factors inﬂuencing the transformation of farms into digital farms include farm characteristics, operatorcharacteristics, interactions, institutions, attributes to technology, andTable 6Previous studies on computer vision and deep learning technologies for livestock management.References Objectives and scenario ofapplication Methodology Livestock Results(Qiao et al.,2019b)Cattle Segmentation and Contourextraction Mask R-CNN based cattle instancesegmentation and contour line extraction Cattle Cattle segmentation performance with 0.92 Mean PixelAccuracy (MPA) (Achour et al.,2020)Identiﬁcation and feedingbehavior monitoring CNN coupled to Support Vector Machine(SVM) Cow Accuracy 97% for individual identi ﬁcation of cows using multi-CNN. (Xu et al., 2020) Livestock classiﬁcation andcounting Mask RCNN based segmentation on UAVcaptured images CattleandSheepClassiﬁcation Accuracy: 96% and Counting accuracy: 92%(Jung et al., 2021) Cattle Vocal Classiﬁcation andLivestock Monitoring Convolutional neural network (CNN)based cattle vocal classiﬁcationCattle Accuracy of 81.96% after the sound ﬁltering.(Qiao et al., 2022) Behaviour classiﬁcation C3D-ConvLSTM based cow behaviourclassiﬁcation using video data Cow Classiﬁcation accuracy of 90.32% and 86.67% in calf and cowdatasets of 30-frame video length (Abu Jwade et al.,2019)Breed Classiﬁcation VGG16 model for breed classi ﬁcation Sheep Maximum classi ﬁcation accuracy of 95.8% with 1.7 standarddeviation. (Shojaeipouret al., 2021)Automated Muzzle Detection andBiometric IdentiﬁcationTwo-stage YOLOv3-ResNet50 algorithm Cattle Muzzle detection accuracy was 99.13% and biometric identiﬁcation of 99.11% testing accuracy (Brand et al.,2021)Pregnancy status prediction frommid-infrared spectroscopy Genetic algorithm and DenseNet model Cow DenseNet was superior over GA with prediction sensitivity 0.89, speciﬁcity of 0.86, and prediction accuracy of 0.88%.
(Ayadi et al.,2020)Rumination behavioridentiﬁcationConvolutional Neural Networks Cow Average accuracy, recall and precision were 95%, 98% and 98% respectively (Riekert et al.,2020)Position and posture detection Faster R-CNN object detection Pig Pig position detection: Average Precision (AP) 87.4% Pig position, and pig position and posture: mAPof 80.2%.
Fig. 10.Orchard yield estimation using computer vision.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
222psychological factors (Shang et al., 2021). Larger farms are more likely to adopt these technologies by taking advantage of economies of scale,and they can afford the higher initial investment cost. Use of comple-mentary technologies can also lead to better adoption of technologies.For example, the variable rate technology and yield mapping are inter-related, and farmers who are using variable rate technologies are morelikely to adopt yield mapping technologies. Operator characteristicssuch as end user’s education level, age, on-farm digital device such ascomputer usage are also signiﬁcant (Isgin et al., 2008). Operators having higher education levels and innovativeness could adopt the new tech-nologies faster (Aubert et al., 2012).Isgin et al. (2008)found signiﬁcant evidence relating to the impact of urban in ﬂuences on adoption of pre- cision farming technologies in their empirical analysis. Mohr and Kühl (2021)investigated the behavioral factors inﬂuencing the acceptance of artiﬁcial intelligence technologies using a theoretical framework.The results showed that behavioral control and personal attitude ofthe farmers are the two most inﬂuential factors in the acceptance ofartiﬁcial intelligence in agriculture.5. Challenges and way forwardDeep learning for computer vision, the spearhead of arti ﬁcial intelli- gence, is perhaps one of the most promising technologies for meetingthe ever-growing food demand. Several intractable problems in agricul-ture are being solved with the support of DL-computer vision. However,high innovation capability always comes along with some challenges.One major challenge in computer vision using deep learning includesthe requirement of massive processing power, and most deep learningapplications are data-intensive. A possible solution to this is the adop-tion of cloud-based solutions that offer auto-scaling, load balancing, eas-ier maintenance, and high availability features. However, cloudsolutions limit real-time processing due to the latency in access and re-trieval of the data from the cloud. The increased cost of immense dataprocessing and privacy issues are also other concerns. Advanced edgedevices with accelerators are capable of analyzing real-time video in-puts and providing inferences in near real-time. Deployment of thecomputer vision solutions in edge devices can reduce the latency limita-tions. Sophisticated computer vision models in a variety of agriculturaluse cases often do not perform as expected in the production environ-ment. To ensure that a promising model is not becoming a costly liabil-ity, several aspects like data quality check, code inspection, hyper-parameter tuning, code versioning, setting up the right deployment en-vironment, rigorous training and re-training, etc, need to be closelyevaluated.Quality of data is another major concern for developing ef ﬁcient data-driven solutions (Cai and Zhu, 2015;Carletto, 2021). Programmat- ically generating synthetic data is one of the approaches for enhancingthe data quality in deep learning-based computer vision solutions(Fig. 11). Generative adversarial networks and their variations likeCGAN can generate synthetic data for agricultural applications quite ef-fectively (Cui et al., 2021;Olatunji et al., 2020;Zhu et al., 2020a). The performance of a DL-CV model relies heavily on the right hyper-parameter conﬁgurations. There are no simple ways to set hyper-parameters such as learning rate, batch size, momentum, weightdecay, etc, and it demands expertise and extensive trial and error toachieve the best performance. The process of con ﬁguring the hyper- parameter in a high-dimensional space is not a trivial challenge. Com-puter vision problems, more speci
ﬁcally object detection approachesface practical implementation challenges such as viewpoint variation,deformation, occlusion, varying illumination conditions, complex back-grounds, and speed. Viewpoint variation is very common in object de-tection, and segmentation problems, as the object may look atdifferent viewing angles. For e.g., a crop may look different when cap-tured from different angles. The additional complication appears dueto the occlusion.In fruit yield estimation systems, this is a major concern and causessharp declination in the overall accuracy of the system. Varying illumi-nation conditions and extraction of data from complex overlappedand textured backgrounds also make the computer vision task challeng-ing. In real-time video applications, performance in terms of detectionspeed and accuracy are crucial for detecting objects in motion. Researchin computer vision is growing at a faster pace in the agriculture domain.Building a robust computer vision system requires quality data genera-tion, transfer, and processing. The system should have adequate securityto block attacks. Heterogeneity of resources involved in CV solutions in-troduces a lot of security concerns, such as data integrity, privacy issues,Table 7Previous studies on computer vision and deep learning technologies for yield estimation.References Objectives and scenario ofapplication Methodology Crop Results(Khaki et al.,2020)Image-based corn kernel countingand yield estimation Truncated VGGNet backbone and semisupervised deep learning. Corn MAE and RMSE of 41.36 and 60.27 respectively.(Apolo-Apoloet al., 2020)Yield map Generation Region-CNN (RCNN) Model using UAV imagery Apple R-squared value: 0.86, MAE: 10.35 and RMSE: 13.56(Palacios et al.,2020)Detection ofﬂower at bloom foryield estimation CNN SegNet architecture with a VGG19network encoder Grape A determination coef ﬁcient (R2) of 0.91 between theactual and detectedﬂowers. (Faisal et al.,2020)Intelligent harvesting decisionsystem based on date fruit maturitylevel.VGG-19, Inception-v3, and NASNet Models Date Performance metrics of IHDS were 99.4%, 99.4%, 99.7%, and 99.7% for accuracy, F1 score, sensitivity(recall), and precision, respectively. (Yang et al.,2019)Rice grain yield forecasting usingUAV images CNN models with RGB and multispectraldatasets Rice Prediction accuracy: MAPE: 20.4%, RMSE: 0.658 andR-squared: 0.585 (Tedesco-Oliveiraet al., 2020)Yield estimation using objectdetection models Faster RCNN, SSD and SSD Lite Models Cotton Mean percentage error of 8.84%(Chen et al.,2019)Yield prediction by countingnumber ofﬂowers and maturityanalysis, using aerial ortho imagesFaster RCNN model Strawberry The average deep learning counting accuracy was 84.1% with average occlusion of 13.5%.(Bargoti andUnderwood,2017)Yield Estimation using fruitdetection and counting CNN and Watershed algorithm Apple The count estimates using CNN and WS with R-squared value of 0.826(Zhou et al.,2020)Real-time fruit detection and yieldestimation through smartphones. Single shot Multibox Detector withMobileNetV2, quantized MobileNetV2,InceptionV3, and quantized InceptionV3ModelsKiwi MobileNetV2, quantized MobileNetV2, InceptionV3,and quantized InceptionV3 obtained TDR of 90.8%,89.7%, 87.6%, and 72.8%, respectively.(Rahnemoonfarand Sheppard,2017)Fruit counting based on deepsimulated learning Modiﬁed version of the Inception-ResNetModel
Tomato 91% average test accuracy on real images and 93% onsynthetic imagesV.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
223reliability, etc. As these solutions integrate several digital technologiesstarting from the internet, IoT, cloud computing or edge computing,and wireless sensor networks, the system should accommodate securityfeatures for all these technologies and ensure data and device integrity,data accuracy, and availability. From land preparation to harvesting, dif-ferent stakeholders are leveraging new ways to improve the ability toderive insights from images, object detection and tracking, etc. Deeplearning - computer vision models will undoubtedly continue to expandand become more innovative and intelligent, handling more complexcomputations in agriculture with utmost precision. Above all, forobtaining efﬁcient and desirable outputs, strong business cases withthe capability to scale on a larger scale is necessary.6. ConclusionsThe surge of deep learning coupled with computer vision over thepast few years has brought automation capabilities to traditional agri-culture practices. In this paper, we have extensively discussed the roleof deep learning-based computer vision in different agriculture applica-tions. More speciﬁcally, the paper emphasizes seven different applica-tion areas such as seed quality analysis, soil analysis, irrigationmanagement, plant health analysis, weed management, livestock man-agement, and yield estimation. Review of the application of deep learn-ing particularly, the assessment and planning of water resourcesrevealed that the water sector would continue to embrace deep learningat an accelerated rate, and it will play a signi ﬁcant role in the future of water-related research and the wide range of application areas. Tech-nologies powered by deep learning have created a myriad of applicationand research opportunities that have the potential to change hydrolog-ical science and workﬂow. Recent advances in deep learning-assistedimage analysis involving algorithms for image classi ﬁcation, object de- tection, segmentation, etc., have expanded their applications across dif-ferent pre-and post-harvesting activities in agriculture.The following conclusions can be drawn from the study.•Deep learning-based computer vision has tremendous automation ca-pabilities across different applications such as automated plant healthmonitoring, weed detection, irrigation management, livestock man-agement, yield estimation, etc.•Integration of the deep learning computer vision approaches with theUAV, and spectral data can help in building advanced-intelligent solu-tions.•Despite the beneﬁts computer vision and deep learning brought to ag-riculture, signiﬁcant challenges do remain, especially the data qualityissues, the computation power requirement, etc.•The extensive automation across various agriculture activities willcontinue to attract the interest of the deep learning research commu-nity in the years to come.The adoption rate of advanced technologies in agriculture is rela-tively slow, owing to the high initial investment required, lack of techni-cal expertise, and growing concerns about data privacy. However atpresent, the rate of the adoption of these digital solutions has seen a ris-ing curve, thus suggesting that these would not be concerns in movingforward.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.
Fig. 11.Challenges in implementation of deep learning based computer vision.V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
224References
Abbas, A., Jain, S., Gour, M., Vankudothu, S., 2021. Tomato plant disease detection usingtransfer learning with C-GAN synthetic images. Comput. Electron. Agric. 187,106279.https://doi.org/10.1016/j.compag.2021.106279 . Abdalla, A., Cen, H., Wan, L., Mehmood, K., He, Y., 2021. Nutrient Status Diagnosis of In ﬁeld Oilseed Rape via Deep Learning-Enabled Dynamic Model. IEEE Trans. Ind. Inform. 17,4379–4389.https://doi.org/10.1109/TII.2020.3009736 . Abu Jwade, S., Guzzomi, A., Mian, A., 2019. On farm automatic sheep breed classi ﬁcation using deep learning. Comput. Electron. Agric. 167, 105055. https://doi.org/10.1016/j. compag.2019.105055.Achour, B., Belkadi, M., Filali, I., Laghrouche, M., Lahdir, M., 2020. Image analysis for indi-vidual identiﬁcation and feeding behaviour monitoring of dairy cows based onConvolutional Neural Networks (CNN). Biosyst. Eng. 198, 31 –49.https://doi.org/10. 1016/j.biosystemseng.2020.07.019 . Adnan, R.M., Mostafa, R.R., Islam, A.R.Md.T., Kisi, O., Kuriqi, A., Heddam, S., 2021. Estimat-ing reference evapotranspiration using hybrid adaptive fuzzy inferencing coupledwith heuristic algorithms. Comput. Electron. Agric. 191, 106541. https://doi.org/10. 1016/j.compag.2021.106541. Agrio, 2022o. Agrio.https://agrio.app/. (Accessed 6 July 2022). Aker, J.C., 2011. Dial“A”for agriculture: a review of information and communication tech-nologies for agricultural extension in developing countries. Agric. Econ. 42, 631 –647. https://doi.org/10.1111/j.1574-0862.2011.00545.x . Alameer, A., Kyriazakis, I., Dalton, H.A., Miller, A.L., Bacardit, J., 2020. Automatic recogni-tion of feeding and foraging behaviour in pigs using deep learning. Biosyst. Eng.197, 91–104.https://doi.org/10.1016/j.biosystemseng.2020.06.013 . Albuquerque, C.K.G., Polimante, S., Torre-Neto, A., Prati, R.C., 2020. Water spray detectionfor smart irrigation systems with Mask R-CNN and UAV footage. 2020 IEEE Interna-tional Workshop on Metrology for Agriculture and Forestry (MetroAgriFor),pp. 236–240https://doi.org/10.1109/MetroAgriFor50201.2020.9277542 . Al-Gaadi, K.A., Hassaballa, A.A., Tola, E., Kayad, A.G., Madugundu, R., Alblewi, B., Assiri, F.,2016. Prediction of Potato Crop Yield Using Precision Agriculture Techniques. PLOSONE 11, e0162219.https://doi.org/10.1371/journal.pone.0162219 . Amara, J., Bouaziz, B., Algergawy, A., 2017. A deep learning-based approach for banana leaf diseases classiﬁcation. Datenbanksysteme Für Bus. Technol. (Web BTW 2017-Work).Anami, B.S., Malvade, N.N., Palaiah, S., 2020. Deep learning approach for recognition andclassiﬁcation of yield affecting paddy crop stresses using ﬁeld images. Artif. Intell. Agric. 4, 12–20.https://doi.org/10.1016/j.aiia.2020.03.001 . Andrew, W., Greatwood, C., Burghardt, T., 2019. Aerial Animal Biometrics: Individual Frie-sian Cattle Recovery and Visual Identi ﬁcation via an Autonomous UAV with Onboard Deep Inference.https://doi.org/10.48550/arXiv.1907.05310 . Apolo-Apolo, O.E., Martínez-Guanter, J., Egea, G., Raja, P., Pérez-Ruiz, M., 2020. Deep learn-ing techniques for estimation of the yield and size of citrus fruits using a UAV. Eur.J. Agron. 115, 126030.https://doi.org/10.1016/j.eja.2020.126030 . Araus, J.L., Cairns, J.E., 2014. Field high-throughput phenotyping: the new crop breedingfrontier. Trends Plant Sci. 19, 52 –61.
https://doi.org/10.1016/j.tplants.2013.09.008 . Araus, J.L., Kefauver, S.C., Zaman-Allah, M., Olsen, M.S., Cairns, J.E., 2018. Translating High-Throughput Phenotyping into Genetic Gain. Trends Plant Sci. 23, 451 –466.https:// doi.org/10.1016/j.tplants.2018.02.001 . Aubert, B.A., Schroeder, A., Grimaudo, J., 2012. IT as enabler of sustainable farming: Anempirical analysis of farmers ’adoption decision of precision agriculture technology. Decis. Support Syst. 54, 510 –520.https://doi.org/10.1016/j.dss.2012.07.002 . Ayadi, S., Ben Said, A., Jabbar, R., Aloulou, C., Chabbouh, A., Achballah, A.B., 2020. DairyCow Rumination Detection: A Deep Learning Approach. In: Jemili, I., Mosbah, M.(Eds.), Distributed Computing for Emerging Smart Networks, Communications inComputer and Information Science. Springer International Publishing, Cham,pp. 123–139https://doi.org/10.1007/978-3-030-65810-6_7 . Azadnia, R., Jahanbakhshi, A., Rashidi, S., Khajehzadeh, M., Bazyar, P., 2022. Developing anautomated monitoring system for fast and accurate prediction of soil texture using animage-based deep learning network and machine vision system. Measurement 190,110669.https://doi.org/10.1016/j.measurement.2021.110669 . Azimi, S., Kaur, T., Gandhi, T.K., 2021. A deep learning approach to measure stress level inplants due to Nitrogen deﬁciency. Measurement 173, 108650. https://doi.org/10. 1016/j.measurement.2020.108650 . Badrzadeh, N., Samani, J.M.V., Mazaheri, M., Kuriqi, A., 2022. Evaluation of managementpractices on agricultural nonpoint source pollution discharges into the rivers underclimate change effects. Sci. Total Environ. 838, 156643. https://doi.org/10.1016/j. scitotenv.2022.156643.Bao, F., Bambil, D., 2021. Applicability of computer vision in seed identi ﬁcation: deep learning, random forest, and support vector machine classi ﬁcation algorithms. Acta Bot. Bras. 35, 17–21.https://doi.org/10.1590/0102-33062020abb0361 . Bargoti, S., Underwood, J.P., 2017. Image Segmentation for Fruit Detection and Yield Esti-mation in Apple Orchards. J. Field Robot. 34, 1039 –1060.https://doi.org/10.1002/rob. 21699.Bazi, Y., Bashmal, L., Rahhal, M.M.A., Dayil, R.A., Ajlan, N.A., 2021. Vision Transformers forRemote Sensing Image Classiﬁcation. Remote Sens. 13, 516. https://doi.org/10.3390/ rs13030516.Bello, R.-W., Mohamed, A.S.A., Talib, A.Z., 2021. Contour Extraction of Individual CattleFrom an Image Using Enhanced Mask R-CNN Instance Segmentation Method. IEEEAccess 9, 56984–57000.https://doi.org/10.1109/ACCESS.2021.3072636 . Bhagat, M., Kumar, D., Haque, I., Munda, H.S., Bhagat, R., 2020. Plant Leaf Disease Classi ﬁ- cation Using Grid Search Based SVM. 2nd International Conference on Data, Engi-neering and Applications (IDEA). Presented at the 2nd International Conference onData, Engineering and Applications (IDEA), pp. 1 –6https://doi.org/10.1109/ IDEA49133.2020.9170725.Bhavsar, H., Panchal, M.H., 2012. A review on support vector machine for data classi ﬁca- tion. Int. J. Adv. Res. Comput. Eng. Technol. IJARCET 1, 185 –189. Blue river technology, 2022y. https://bluerivertechnology.com/ . (Accessed 5 July 2022). Brahimi, M., Boukhalfa, K., Moussaoui, A., 2017. Deep Learning for Tomato Diseases: Clas-siﬁcation and Symptoms Visualization. Appl. Artif. Intell. 31, 299 –315.https://doi.org/ 10.1080/08839514.2017.1315516 . Brand, W., Wells, A.T., Smith, S.L., Denholm, S.J., Wall, E., Coffey, M.P., 2021. Predictingpregnancy status from mid-infrared spectroscopy in dairy cow milk using deep learn-ing. J. Dairy Sci. 104, 4980 –4990.https://doi.org/10.3168/jds.2020-18367 . Burra, D.D., Hildebrand, J., Giles, J., Nguyen, T., Hasiner, E., Schroeder, K., Treguer, D.,Juergenliemk, A., Horst, A., Jarvis, A., Kropff, W., 2021. Digital Agriculture Proﬁle: Viet Nam (Report). Food and Agriculture Organization of the United Nations.Cai, L., Zhu, Y., 2015. The Challenges of Data Quality and Data Quality Assessment in theBig Data Era. Data Sci. J. 14, 2. https://doi.org/10.5334/dsj-2015-002 . Cainthus, 2022s. Cainthus.https://www.cainthus.com. (Accessed 6 July 2022). Carletto, C., 2021. Better data, higher impact: improving agricultural data systems for so-c i e t a lc h a n g e .E u r .R e v .A g r i c .E c o n .4 8 ,7 1 9 –740.https://doi.org/10.1093/erae/ jbab030.Cen, H., Lu, R., Zhu, Q., Mendoza, F., 2016. Nondestructive detection of chilling injury in cu-cumber fruit using hyperspectral imaging with feature selection and supervised clas-siﬁcation. Postharvest Biol. Technol. 111, 352 –361.https://doi.org/10.1016/j. postharvbio.2015.09.027.Chamoso, P., Raveane, W., Parra, V., González, A., 2014. UAVs applied to the counting and monitoring of animals. Ambient Intelligence-Software and Applications. Springer71–80.Chang, C.-L., Lin, K.-M., 2018. Smart Agricultural Machine with a Computer Vision-BasedWeeding and Variable-Rate Irrigation Scheme. Robotics 7, 38. https://doi.org/10. 3390/robotics7030038.Chavan, T.R., Nandedkar, A.V., 2018. AgroAVNET for crops and weeds classi ﬁcation: A step forward in automatic farming. Comput. Electron. Agric. 154, 361 –372.https://doi.org/ 10.1016/j.compag.2018.09.021 . Chen, Y., Lee, W.S., Gan, H., Peres, N., Fraisse, C., Zhang, Y., He, Y., 2019. Strawberry YieldPrediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoim-ages. Remote Sens. 11, 1584.https://doi.org/10.3390/rs11131584 . Chen, H., Chen, A., Xu, L., Xie, H., Qiao, H., Lin, Q., Cai, K., 2020a. A deep learning CNN ar-chitecture applied in smart near-infrared analysis of water pollution for agriculturalirrigation resources. Agric. Water Manag. 240, 106303. https://doi.org/10.1016/j. agwat.2020.106303.Chen, J., Chen, Jinxiu, Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020b. Using deep transferlearning for image-based plant disease identi ﬁcation. Comput. Electron. Agric. 173, 105393.https://doi.org/10.1016/j.compag.2020.105393 . Cheng, H., Damerow, L., Sun, Y., Blanke, M., 2017. Early Yield Prediction Using Image Anal-ysis of Apple Fruit and Tree Canopy Features with Neural Networks. J. Imaging 3, 6.https://doi.org/10.3390/jimaging3010006 . Chlingaryan, A., Sukkarieh, S., Whelan, B., 2018. Machine learning approaches for cropyield prediction and nitrogen status estimation in precision agriculture: A review.Comput. Electron. Agric. 151, 61 –69.https://doi.org/10.1016/j.compag.2018.05.012 . Coulibaly, S., Kamsu-Foguem, B., Kamissoko, D., Traore, D., 2019. Deep neural networkswith transfer learning in millet crop images. Comput. Ind. 108, 115 –120.https:// doi.org/10.1016/j.compind.2019.02.003 . Cromai, 2022i. Cromai.
https://www.cromai.com/. (Accessed 6 July 2022). Cui, X., Ying, Y., Chen, Z., 2021. CycleGAN based confusion model for cross-species plantdisease image migration. J. Intell. Fuzzy Syst. 41, 6685 –6696.https://doi.org/10. 3233/JIFS-210585.Dell’Acqua, F., Gamba, P., 2003. Texture-based characterization of urban environments onsatellite SAR images. IEEE Trans. Geosci. Remote Sens. 41, 153 –159.https://doi.org/ 10.1109/TGRS.2002.807754. Dolata, P., Reiner, J., 2018.Barley Variety Recognition with Viewpoint-Aware Double-Stream Convolutional Neural Networks. 2018 Federated Conference on ComputerScience and Information Systems (FedCSIS)., pp. 101 –105 Dorj, U.-O., Lee, M., Yun, S., 2017. An yield estimation in citrus orchards via fruit detectionand counting using image processing. Comput. Electron. Agric. 140, 103 –112.https:// doi.org/10.1016/j.compag.2017.05.019 . Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021. AnImage is Worth 16x16 Words: Transformers for Image Recognition at Scale.https://doi.org/10.48550/arXiv.2010.11929 . Elbeltagi, A., Kumar, M., Kushwaha, N.L., Pande, C.B., Ditthakit, P., Vishwakarma, D.K.,Subeesh, A., 2022a. Drought indicator analysis and forecasting using data drivenmodels: case study in Jaisalmer, India. Stoch. Environ. Res. Risk Assess https://doi. org/10.1007/s00477-022-02277-0 . Elbeltagi, A., Kushwaha, N.L., Rajput, J., Vishwakarma, D.K., Kulimushi, L.C., Kumar, M.,Zhang, J., Pande, C.B., Choudhari, P., Meshram, S.G., Pandey, K., Sihag, P., Kumar, N.,Abd-Elaty, I., 2022b. Modelling daily reference evapotranspiration based on stackinghybridization of ANN with meta-heuristic algorithms under diverse agro-climaticconditions. Stoch. Environ. Res. Risk Assess. https://doi.org/10.1007/s00477-022- 02196-0.Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., Vasilakoglou, I., 2020. Towardsweeds identiﬁcation assistance through transfer learning. Comput. Electron. Agric.171, 105306.https://doi.org/10.1016/j.compag.2020.105306 . Faisal, M., Alsulaiman, M., Arafah, M., Mekhtiche, M.A., 2020. IHDS: Intelligent HarvestingDecision System for Date Fruit Based on Maturity Stage Using Deep Learning andComputer Vision. IEEE Access 8, 167985 –167997.https://doi.org/10.1109/ACCESS. 2020.3023894.Fawakherji, M., Potena, C., Prevedello, I., Pretto, A., Bloisi, D.D., Nardi, D., 2020. Data Aug-mentation Using GANs for Crop/Weed Segmentation in Precision Farming. 2020 IEEEV.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
225Conference on Control Technology and Applications (CCTA). Presented at the 2020IEEE Conference on Control Technology and Applications (CCTA), pp. 279 –284 https://doi.org/10.1109/CCTA41146.2020.9206297 . Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.https://doi.org/10.1016/j.compag.2018.01. 009.Foglia, M.M., Reina, G., 2006. Agricultural robot for radicchio harvesting. J. Field Robot. 23,363–377.https://doi.org/10.1002/rob.20131 . Ghanem, M.E., Marrou, H., Sinclair, T.R., 2015. Physiological phenotyping of plants for cropimprovement. Trends Plant Sci. 20, 139 –144.https://doi.org/10.1016/j.tplants.2014. 11.006.Gomes, J.F.S., Leta, F.R., 2012. Applications of computer vision techniques in the agricul-ture and food industry: a review. Eur. Food Res. Technol. 235, 989 –1000.https:// doi.org/10.1007/s00217-012-1844-2 . Goncharov, P, Ososkov, G, Nechaevskiy, A, Uzhinskiy, A, Nestsiarenia, I, 2018. Disease De- tection on the Plant Leaves by Deep Learning. In: Kryzhanovsky, Boris, Dunin-Barkowski, Witali, Redko, Vladimir, Tiumentsev, Yury (Eds.), Advances in NeuralComputation, Machine Learning, and Cognitive Research II. Springer, pp. 151 –159. Granitto, P.M., Verdes, P.F., Ceccatto, H.A., 2005. Large-scale investigation of weed seedidentiﬁcation by machine vision. Comput. Electron. Agric. 47, 15 –24.https://doi. org/10.1016/j.compag.2004.10.003 . Gulzar, Y., Hamid, Y., Soomro, A.B., Alwan, A.A., Journaux, L., 2020. A Convolution NeuralNetwork-Based Seed Classiﬁcation System. Symmetry 12, 2018. https://doi.org/10. 3390/sym12122018.Hall, D., McCool, C., Dayoub, F., Sunderhauf, N., Upcroft, B., 2015. Evaluation of Features forLeaf Classiﬁcation in Challenging Conditions. 2015 IEEE Winter Conference on Appli-cations of Computer Vision. Presented at the 2015 IEEE Winter Conference on Appli-cations of Computer Vision, pp. 797 –804https://doi.org/10.1109/WACV.2015.111 . Hansen, M.F., Smith, M.L., Smith, L.N., Salter, M.G., Baxter, E.M., Farish, M., Grieve, B., 2018.Towards on-farm pig face recognition using convolutional neural networks. Comput.Ind. 98, 145–152.https://doi.org/10.1016/j.compind.2018.02.016 . Haralick, R.M., Shanmugam, K., Dinstein, I., 1973. Textural Features for Image Classi ﬁca- tion. IEEE Trans. Syst. Man Cybern. SMC-3, 610 –621.https://doi.org/10.1109/TSMC. 1973.4309314.Harvest croo robotics, 2022s. Harvest croo robotics. https://www.harvestcroorobotics. com/technology. (Accessed 5 July 2022).Hassan, S.M., Maji, A.K., 2022. Plant Disease Identi ﬁcation Using a Novel Convolutional Neural Network. IEEE Access 10, 5390 –5401.https://doi.org/10.1109/ACCESS.2022. 3141371.Hati, A.J., Singh, R.R., 2021. Artiﬁcial Intelligence in Smart Farms: Plant Phenotyping forSpecies Recognition and Health Condition Identi ﬁcation Using Deep Learning. AI 2,
274–289.https://doi.org/10.3390/ai2020017 . He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition. ArXiv151203385 Cs.Heramb, P., Kumar Singh, P., Ramana Rao, K.V., Subeesh, A., 2022. Modelling referenceevapotranspiration using gene expression programming and arti ﬁcial neural network at Pantnagar. Inf. Process. Agric, India https://doi.org/10.1016/j.inpa.2022.05.007 . Hiriyannaiah, S., Srinivas, A.M.D., Shetty, G.K., Srinivasa, K.G., 2020. Chapter 4 - A compu-tationally intelligent agent for detecting fake news using generative adversarial net-works. In: Bhattacharyya, S., Sná šel, V., Gupta, D., Khanna, A. (Eds.), Hybrid Computational Intelligence, Hybrid Computational Intelligence for Pattern Analysis.and Understanding. Academic Press, pp. 69 –96https://doi.org/10.1016/B978-0-12- 818699-2.00004-4.Huang, T., 1993.Computer vision. Evolution and promise.Huang, H., Lan, Y., Yang, A., Zhang, Y., Wen, S., Deng, J., 2020. Deep learning versus Object-based Image Analysis (OBIA) in weed mapping of UAV imagery. Int. J. Remote Sens.41, 3446–3479.https://doi.org/10.1080/01431161.2019.1706112 . Hughes, D.P., Salathe, M., 2016. An open access repository of images on plant health to en-able the development of mobile disease diagnostics. ArXiv151108060 Cs.Huzzey, J.M., Veira, D.M., Weary, D.M., von Keyserlingk, M., 2007. Prepartum behavior anddry matter intake identify dairy cows at risk for metritis. J. Dairy Sci. 90, 3220 –3233. https://doi.org/10.3168/jds.2006-807 . Isgin, T., Bilgic, A., Forster, D.L., Batte, M.T., 2008. Using count data models to determinethe factors affecting farmers ’quantity decisions of precision farming technology adoption. Comput. Electron. Agric. 62, 231 –242.https://doi.org/10.1016/j.compag. 2008.01.004.Jahagirdar, P., Budihal, S.V., 2021. Framework to Detect NPK De ﬁciency in Maize Plants Using CNN. In: Panigrahi, C.R., Pati, B., Mohapatra, P., Buyya, R., Li, K.-C. (Eds.), Prog-ress in Advanced Computing and Intelligent Engineering, Advances in Intelligent Sys-tems and Computing. Springer, Singapore, pp. 366 –376https://doi.org/10.1007/978- 981-15-6353-9_33.Jamei, M., Karbasi, M., Malik, A., Abualigah, L., Islam, A.R.M.T., Yaseen, Z.M., 2022a. Compu-tational assessment of groundwater salinity distribution within coastal multi-aquifers of Bangladesh. Sci. Rep. 12, 11165. https://doi.org/10.1038/s41598-022- 15104-x.Jamei, M., Maroufpoor, S., Aminpour, Y., Karbasi, M., Malik, A., Karimi, B., 2022b. Develop-ing hybrid data-intelligent method using Boruta-random forest optimizer for simula-tion of nitrate distribution pattern. Agric. Water Manag. 270, 107715. https://doi.org/ 10.1016/j.agwat.2022.107715. Jamei, Mehdi, Karbasi, M., Malik, A., Jamei, Mozhdeh, Kisi, O., Yaseen, Z.M., 2022c. Long-term multi-step ahead forecasting of root zone soil moisture in different climates:Novel ensemble-based complementary data-intelligent paradigms. Agric. WaterManag. 269, 107679.https://doi.org/10.1016/j.agwat.2022.107679 . Javanmardi, S., Miraei Ashtiani, S.-H., Verbeek, F.J., Martynenko, A., 2021. Computer-visionclassiﬁcation of corn seed varieties using deep convolutional neural network. J. StoredProd. Res. 92, 101800.https://doi.org/10.1016/j.jspr.2021.101800 .Jha, K., Doshi, A., Patel, P., Shah, M., 2019. A comprehensive review on automation in ag-riculture using artiﬁcial intelligence. Artif. Intell. Agric. 2, 1 –12.https://doi.org/10. 1016/j.aiia.2019.05.004.Ji, M., Wu, Z., 2022. Automatic detection and severity analysis of grape black measles dis-ease based on deep learning and fuzzy logic. Comput. Electron. Agric. 193, 106718.https://doi.org/10.1016/j.compag.2022.106718 . Jiang, B., Wu, Q., Yin, X., Wu, D., Song, H., He, D., 2019. FLYOLOv3 deep learning for keyparts of dairy cow body detection. Comput. Electron. Agric. 166, 104982. https:// doi.org/10.1016/j.compag.2019.104982 . Joshi, R.C., Kaushik, M., Dutta, M.K., Srivastava, A., Choudhary, N., 2021. VirLeafNet: Auto-matic analysis and viral disease diagnosis using deep-learning in Vigna mungo plant.Ecol. Inform. 61, 101197.https://doi.org/10.1016/j.ecoinf.2020.101197 . Jung, D.-H., Kim, N.Y., Moon, S.H., Jhin, C., Kim, H.-J., Yang, J.-S., Kim, H.S., Lee, T.S., Lee, J.Y.,Park, S.H., 2021. Deep Learning-Based Cattle Vocal Classi ﬁcation Model and Real- Time Livestock Monitoring System with Noise Filtering. Animals 11, 357. https:// doi.org/10.3390/ani11020357. Kaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D., 2022. Aligning ar-tiﬁcial intelligence with climate change mitigation. Nat. Clim. Change 12, 518 –527. https://doi.org/10.1038/s41558-022-01377-7 . Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: A survey. Comput.Electron. Agric. 147, 70–90.https://doi.org/10.1016/j.compag.2018.02.016 . Kamyshova, G., Osipov, A., Gataullin, S., Korchagin, S., Ignar, S., Gataullin, T., Terekhova, N.,Suvorov, S., 2022. Artiﬁcial neural networks and computer vision ’s-based phytoindication systems for variable rate irrigation improving. IEEE Access 10,8577–8589.https://doi.org/10.1109/ACCESS.2022.3143524 . Kannur, Anil, Kannur, Asha, Rajpurohit, V.S., 2011. Classi ﬁcation and grading of bulk seeds using artiﬁcial neural network. Int. J. Mach. Intell. 3, 62 –73.https://doi.org/10.9735/ 0975-2927.3.2.62-73.Kanwal, Z., Basit, A., Jawad, M., Ullah, I., Ali, A., 2019. Overlapped apple fruit yield estima-tion using pixel classiﬁcation and hough transform. Int. J. Adv. Comput. Sci. Appl. 10.https://doi.org/10.14569/IJACSA.2019.0100271 . Karbasi, M., Jamei, M., Ali, M., Malik, A., Yaseen, Z.M., 2022. Forecasting weekly reference evapotranspiration using Auto Encoder Decoder Bidirectional LSTM model hybrid-ized with a Boruta-CatBoost input optimizer. Comput. Electron. Agric. 198, 107121.Karim, L., Anpalagan, A., Nasser, N., Almhana, J., 2013. Sensor-based M2M AgricultureMonitoring Systems for Developing Countries. State and Challenges 5. https://doi. org/10.5296/npa.v5i3.3787. Khaki, S., Wang, L., Archontoulis, S.V., 2020. A CNN-RNN Framework for Crop Yield Predic-tion. Front. Plant Sci. 10.https://doi.org/10.3389/fpls.2019.01750 . Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers in Vi-sion: A Survey. ACM Comput. Surv. https://doi.org/10.1145/3505244 . Khanna, A., Kaur, S., 2019. Evolution of Internet of Things (IoT) and its signi ﬁcant impact in theﬁeld of Precision Agriculture. Comput. Electron. Agric. 157, 218 –231.https:// doi.org/10.1016/j.compag.2018.12.039 . Kim, K.-H., Kim, M.-G., Yoon, P.-R., Bang, J.-H., Myoung, W.-H., Choi, J.-Y., Choi, G.-H., 2022.Application of CCTV Image and Semantic Segmentation Model for Water Level Esti-mation of Irrigation Channel. J. Korean Soc. Agric. Eng. 64, 63 –73.https://doi.org/10. 5389/KSAE.2022.64.3.063.Koech, R., Langat, P., 2018. Improving irrigation water use ef ﬁ
ciency: a review of ad- vances, challenges and opportunities in the Australian context. Water 10, 1771.https://doi.org/10.3390/w10121771 . Kriegeskorte, N., Golan, T., 2019. Neural network models and deep learning. Curr. Biol. 29,R231–R236.https://doi.org/10.1016/j.cub.2019.02.034 . Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012a. ImageNet classiﬁcation with deep convolutional neural networks, in: Proceedings of the 25th International Conferenceon Neural Information Processing Systems - Volume 1, NIPS ’12. Curran Associates Inc., Red Hook, NY, USA, pp. 1097 –1105. Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012b. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. Advances in Neural Information Processing Systems.Curran Associates, Inc.Kumar, S., Singh, S.K., Singh, R., Singh, A.K., 2017. Recognition of cattle using face images. Anim. Biom. 79–110.Kumar, A., Sarkar, S., Pradhan, C., 2019. Recommendation System for Crop Identi ﬁcation and Pest Control Technique in Agriculture. 2019 International Conference on Com-munication and Signal Processing (ICCSP). Presented at the 2019 International Con-ference on Communication and Signal Processing (ICCSP), pp. 0185 –0189https:// doi.org/10.1109/ICCSP.2019.8698099 . Kumbi, A.A., Birje, M.N., 2022. Deep CNN based sun ﬂower atom optimization method for optimal water control in IoT. Wirel. Pers. Commun. 122, 1221 –1246.https://doi.org/ 10.1007/s11277-021-08946-7 . Kuplich, T.M., Curran, P.J., Atkinson, P.M., 2005. Relating SAR image texture to the biomassof regenerating tropical forests. Int. J. Remote Sens. 26, 4829 –4854.https://doi.org/10. 1080/01431160500239107. Kurtulmuş, F., 2021. Identiﬁcation of sunﬂower seeds with deep convolutional neural net- works. J. Food Meas. Charact. 15, 1024 –1033.https://doi.org/10.1007/s11694-020- 00707-7.Kushwaha, N.L., Bhardwaj, A., Verma, V.K., 2016. Hydrologic response of Takarla-Ballowal watershed in Shivalik foot-hills based on morphometric analysis using remote sens-ing and GIS. J Indian Water Resour Soc 36, 17 –25. Kushwaha, N.L., Rajput, J., Elbeltagi, A., Elnaggar, A.Y., Sena, D.R., Vishwakarma, D.K., Mani,I., Hussein, E.E., 2021. Data Intelligence model and meta-heuristic algorithms-basedpan evaporation modelling in two different agro-climatic zones: a case study fromNorthern India. Atmosphere 12, 1654. https://doi.org/10.3390/atmos12121654 . Kushwaha, N.L., Elbeltagi, A., Mehan, S., Malik, A., Yousuf, A., 2022. Comparative study onmorphometric analysis and RUSLE-based approaches for micro-watershedV.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
226prioritization using remote sensing and GIS. Arab. J. Geosci. 15, 564. https://doi.org/ 10.1007/s12517-022-09837-2 . Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., 2017. Deep learning classi ﬁcation of land cover and crop types using remote sensing data. IEEE Geosci. Remote Sens.Lett. 14, 778–782.https://doi.org/10.1109/LGRS.2017.2681128 . Le, V.N.T., Ahderom, S., Alameh, K., 2020. Performances of the LBP Based Algorithm overCNN Models for Detecting Crops and Weeds with Similar Morphologies. Sensors 20,2193.https://doi.org/10.3390/s20082193 . LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to doc- ument recognition. Proc. IEEE 86, 2278 –2324. Ledig, C., Theis, L., Huszar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A.,Totz, J., Wang, Z., Shi, W., 2017. Photo-Realistic Single Image Super-Resolution Using aGenerative Adversarial Network. https://doi.org/10.48550/arXiv.1609.04802 . Li, X., Li, S., 2022. Transformer Help CNN See Better: A Lightweight Hybrid Apple DiseaseIdentiﬁcation Model Based on Transformers. Agriculture 12, 884. https://doi.org/10. 3390/agriculture12060884. Li, Y., Randall, C.J., Van Woesik, R., Ribeiro, E., 2019. Underwater video mosaicing using to-pology and superpixel-based pairwise stitching. Expert Syst. Appl. 119, 171 –183. https://doi.org/10.1016/j.eswa.2018.10.041 . Li, C., Li, H., Liu, Z., Li, B., Huang, Y., 2021. SeedSortNet: a rapid and highly ef ﬁﬁcient light- weight CNN based on visual attention for seed sorting. PeerJ Comput. Sci. 7, e639.https://doi.org/10.7717/peerj-cs.639 . Li, Y., Wu, C.-Y., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer, C., 2022. MViTv2: improved multiscale vision transformers for classi ﬁcation and detection. Presented at the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-nition, pp. 4804–4814.Liu, J., Wang, X., 2020.Tomato diseases and pests detection based on improved yolo V3convolutional neural network. Front. Plant Sci. 11.Liu, D., Ning, X., Li, Z., Yang, D., Li, H., Gao, L., 2015. Discriminating and elimination of dam-aged soybean seeds based on image characteristics. J. Stored Prod. Res. 60, 67 –74. https://doi.org/10.1016/j.jspr.2014.10.001 . Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016. SSD: SingleShot MultiBox Detector. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (Eds.), ComputerVision–ECCV 2016. Lecture Notes in Computer Science. Springer International Pub-lishing, Cham, pp. 21–37https://doi.org/10.1007/978-3-319-46448-0_2 . Lo Bianco, M., Grillo, O., Cañadas, E., Venora, G., Bacchetta, G., 2017. Inter- and intraspeci ﬁc diversity in Cistus L. (Cistaceae) seeds, analysed with computer vision techniques.Plant Biol. 19, 183–190.https://doi.org/10.1111/plb.12529 . Ma, T., Tsuchikawa, S., Inagaki, T., 2020. Rapid and non-destructive seed viability predic-tion using near-infrared hyperspectral imaging coupled with a deep learning ap-proach. Comput. Electron. Agric. 177, 105683. https://doi.org/10.1016/j.compag. 2020.105683.Maheswari, P., Raja, P., Apolo-Apolo, O.E., Pérez-Ruiz, M., 2021. Intelligent fruit yield esti-mation for orchards using deep learning based semantic segmentation techniques —a review. Front. Plant Sci. 12, 1247. https://doi.org/10.3389/fpsyg.2020.513474 . Malik, A., Saggi, M.K., Rehman, S., Sajjad, H., Inyurt, S., Bhatia, A.S., Farooque, A.A., Oudah,A.Y., Yaseen, Z.M., 2022a. Deep learning versus gradient boosting machine for panevaporation prediction. Eng. Appl. Comput. Fluid Mech. 16, 570 –587.https://doi.
org/10.1080/19942060.2022.2027273 . Malik, A., Tikhamarine, Y., Sihag, P., Shahid, S., Jamei, M., Karbasi, M., 2022b. Predictingdaily soil temperature at multiple depths using hybrid machine learning models fora semi-arid region in Punjab. Environ. Sci. Pollut. Res, India https://doi.org/10.1007/ s11356-022-20837-3.Mathew, M.P., Mahesh, T.Y., 2022. Leaf-based disease detection in bell pepper plant usingYOLO v5. Signal Image Video Process. 16, 841 –847.https://doi.org/10.1007/s11760- 021-02024-y.Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Shahrzad,H., Navruzyan, A., Duffy, N., Hodjat, B., 2019. Chapter 15 - Evolving Deep Neural Net-works. In: Kozma, R., Alippi, C., Choe, Y., Morabito, F.C. (Eds.), Arti ﬁcial Intelligence in the Age of Neural Networks and Brain Computing. Academic Press, pp. 293 –312 https://doi.org/10.1016/B978-0-12-815480-9.00015-3 . Mirza, M., Osindero, S., 2014.Conditional Generative Adversarial Nets. ArXiv14111784 CsStat.Mkonyi, L., Rubanga, D., Richard, M., Zekeya, N., Sawahiko, S., Maiseli, B., Machuve, D.,2020. Early identiﬁcation of Tuta absoluta in tomato plants using deep learning. Sci.Afr. 10, e00590.https://doi.org/10.1016/j.sciaf.2020.e00590 . Mochida, K., Koda, S., Inoue, K., Hirayama, T., Tanaka, S., Nishii, R., Melgani, F., 2019. Com-puter vision-based phenotyping for improvement of plant productivity: a machinelearning perspective. GigaScience 8. https://doi.org/10.1093/gigascience/giy153 . Mohanty, S.P., Hughes, D.P., Salathé, M., 2016. Using Deep Learning for Image-Based Plant Disease Detection. Front. Plant Sci. 7.Mohr, S., Kühl, R., 2021. Acceptance of arti ﬁcial intelligence in German agriculture: an ap- plication of the technology acceptance model and the theory of planned behavior.Precis. Agric. 22, 1816–1844.https://doi.org/10.1007/s11119-021-09814-x . Mortensen, A.K., Dyrmann, M., Karstoft, H., Nyholm Jørgensen, R., Gislum, R., 2016. Se- mantic Segmentation of Mixed Crops using Deep Convolutional Neural Network.Mukti, I.Z., Biswas, D., 2019. Transfer Learning Based Plant Diseases Detection UsingResNet50. 2019 4th Int. Conf. Electr. Inf. Commun. Technol. EICT. https://doi.org/10. 1109/EICT48899.2019.9068805 . Nagasubramanian, K., Jones, S., Singh, A.K., Sarkar, S., Singh, A., Ganapathysubramanian,B., 2019. Plant disease identiﬁcation using explainable 3D deep learning on hyperspectral images. Plant Methods 15, 98. https://doi.org/10.1186/s13007-019- 0479-8.Ni, C., Wang, D., Vinson, R., Holmes, M., Tao, Y., 2019. Automatic inspection machine formaize kernels based on deep convolutional neural networks. Biosyst. Eng. 178,131–144.https://doi.org/10.1016/j.biosystemseng.2018.11.010 .Nilsson, H., 1995. Remote Sensing and Image Analysis in Plant Pathology. Annu. Rev.Phytopathol. 33, 489–528.https://doi.org/10.1146/annurev.py.33.090195.002421 . O’Mahony, N., Campbell, S., Carvalho, A., Harapanahalli, S., Hernandez, G.V., Krpalkova, L.,Riordan, D., Walsh, J., 2020. Deep Learning vs. Traditional Computer Vision. In: Arai,K., Kapoor, S. (Eds.), Advances in Computer Vision, Advances in Intelligent Systemsand Computing. Springer International Publishing, Cham, pp. 128 –144https://doi. org/10.1007/978-3-030-17795-9_10 . Olatunji, J.R., Redding, G.P., Rowe, C.L., East, A.R., 2020. Reconstruction of kiwifruit fruit ge-ometry using a CGAN trained on a synthetic dataset. Comput. Electron. Agric. 177,105699.https://doi.org/10.1016/j.compag.2020.105699 . Omondiagbe, O.P., Lilburne, L., Licorish, S., MacDonell, S., 2022. Soil Texture Predictionwith Automated Deep Convolutional Neural Networks and Population Based Learn-ing (SSRN Scholarly Paper No. 4003387). Social Science Research Network, Rochester,NYhttps://doi.org/10.2139/ssrn.4003387. Osorio, K., Puerto, A., Pedraza, C., Jamaica, D., Rodríguez, L., 2020. A deep learning ap-proach for weed detection in lettuce crops using multispectral images.AgriEngineering 2, 471–488.https://doi.org/10.3390/agriengineering2030032 . Palacios, F., Bueno, G., Salido, J., Diago, M.P., Hernández, I., Tardaguila, J., 2020. Automatedgrapevineﬂower detection and quantiﬁcation method based on computer vision and deep learning from on-the-go imaging using a mobile sensing platform under ﬁeld conditions. Comput. Electron. Agric. 178, 105796. https://doi.org/10.1016/j.compag. 2020.105796.Piguard, 2022d. Piguard.https://www.serket-tech.com/Products . (Accessed 5 July 2022). Plantix, 2022x. Plantix.https://plantix.net/en/. (Accessed 5 July 2022). Putzu, L., Di Ruberto, C., Fenu, G., 2016. A Mobile Application for Leaf Detection in Com-plex Background Using Saliency Maps. In: Blanc-Talon, J., Distante, C., Philips, W.,Popescu, D., Scheunders, P. (Eds.), Advanced Concepts for Intelligent Vision Systems.Springer International Publishing, Cham, pp. 570 –581https://doi.org/10.1007/978-3- 319-48680-2_50.Pyo, J., Hong, S.M., Kwon, Y.S., Kim, M.S., Cho, K.H., 2020. Estimation of heavy metals usingdeep neural network with visible and infrared spectroscopy of soil. Sci. Total Environ.741, 140162.https://doi.org/10.1016/j.scitotenv.2020.140162 . Qiao, Y., Su, D., Kong, H., Sukkarieh, S., Lomax, S., Clark, C., 2019. Individual cattle identiﬁ- cation using a deep learning based framework. IFAC-Pap. 52, 318 –323. Qiao, Y., Truman, M., Sukkarieh, S., 2019b. Cattle segmentation and contour extractionbased on Mask R-CNN for precision livestock farming. Comput. Electron. Agric. 165,104958.https://doi.org/10.1016/j.compag.2019.104958 . Qiao, Y., Guo, Y., Yu, K., He, D., 2022. C3D-ConvLSTM based cow behaviour classi ﬁcation using video data for precision livestock farming. Comput. Electron. Agric. 193,106650.https://doi.org/10.1016/j.compag.2021.106650 . Qiu, Z., Chen, J., Zhao, Y., Zhu, S., He, Y., Zhang, C., 2018. Variety Identi ﬁcation of Single Rice Seed Using Hyperspectral Imaging Combined with Convolutional Neural Network.Appl. Sci. 8, 212.https://doi.org/10.3390/app8020212 . Rahnemoonfar, M., Sheppard, C., 2017. Deep count: fruit counting based on deep simu-lated learning. Sensors 17, 905. https://doi.org/10.3390/s17040905 . Rahnemoonfar, M., Dobbs, D., Yari, M., Starek, M.J., 2019. DisCountNet: Discriminating andCounting Network for Real-Time Counting and Localization of Sparse Objects in High-Resolution UAV Imagery. Remote Sens. 11, 1128. https://doi.org/10.3390/ rs11091128.Rai, P., Kumar, P., Al-Ansari, N., Malik, A., 2022. Evaluation of machine learning versus em-pirical models for monthly reference evapotranspiration estimation in Uttar Pradeshand Uttarakhand States. India. Sustainability 14, 5771. https://doi.org/10.3390/ su14105771.Ranganathan, J., Waite, R., Searchinger, T., Hanson, C., 2018. How to Sustainably Feed 10 Billion People by 2050, in 21 Charts.Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You Only Look Once: Uni ﬁed, Real- Time Object Detection. 2016 IEEE Conference on Computer Vision and Pattern Recog-nition (CVPR), pp. 779–788https://doi.org/10.1109/CVPR.2016.91 . Rehman, T.U., Mahmud, Md.S., Chang, Y.K., Jin, J., Shin, J., 2019. Current and future appli-cations of statistical machine learning algorithms for agricultural machine vision sys-tems. Comput. Electron. Agric. 156, 585 –605.https://doi.org/10.1016/j.compag.2018. 12.006.
Reimers, C., Requena-Mesa, C., 2020. Chapter 13 - Deep Learning –an Opportunity and a Challenge for Geo- and Astrophysics. In: Škoda, P., Adam, F. (Eds.), Knowledge Dis- covery in Big Data from Astronomy and Earth Observation. Elsevier, pp. 251 –265 https://doi.org/10.1016/B978-0-12-819154-5.00024-2 . Ren, S., He, K., Girshick, R., Sun, J., 2016. Faster R-CNN: towards real-time object detection with region proposal networks. ArXiv150601497 Cs.Rico-Fernández, M.P., Rios-Cabrera, R., Castelán, M., Guerrero-Reyes, H.-I., Juarez-Maldonado, A., 2019. A contextualized approach for segmentation of foliage in differ-ent crop species. Comput. Electron. Agric. 156, 378 –386.https://doi.org/10.1016/j. compag.2018.11.033.Riekert, M., Klein, A., Adrion, F., Hoffmann, C., Gallmann, E., 2020. Automatically detectingpig position and posture by 2D camera imaging and deep learning. Comput. Electron.Agric. 174, 105391.https://doi.org/10.1016/j.compag.2020.105391 . Riese, F.M., Keller, S., 2019a. SOIL TEXTURE CLASSIFICATION WITH 1D CONVOLUTIONALNEURAL NETWORKS BASED ON HYPERSPECTRAL DATA, in: ISPRS Annals of the Pho-togrammetry, Remote Sensing and Spatial Information Sciences. Presented at theISPRS Geospatial Week 2019 (Volume IV-2/W5) - 10 –14 June 2019. Copernicus GmbH, Enschede, The Netherlands, pp. 615 –621https://doi.org/10.5194/isprs-an- nals-IV-2-W5-615-2019.Riese, F.M., Keller, S., 2019b. Soil texture classi ﬁcation with 1D convolutional neural net- works based on hyperspectral data. ISPRS Annals of the Photogrammetry, RemoteSensing and Spatial Information Sciences. Copernicus GmbH, pp. 615 –621https:// doi.org/10.5194/isprs-annals-IV-2-W5-615-2019 .V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
227Rivas, A., Chamoso, P., González-Briones, A., Corchado, J.M., 2018. Detection of CattleUsing Drones and Convolutional Neural Networks. Sensors 18, 2048. https://doi. org/10.3390/s18072048.Rony, Md., Barai, D., Riad Hasan, Z., 2021. Cattle External Disease Classi ﬁcation Using Deep Learning Techniques. 2021 12th International Conference on Computing Communi-cation and Networking Technologies (ICCCNT), pp. 1 –7https://doi.org/10.1109/ ICCCNT51525.2021.9579662. Russello, H., 2018.Convolutional neural networks for crop yield prediction using satelliteimages. IBM Cent. Adv. Stud.Sami, M., Khan, S.Q., Khurram, M., Farooq, M.U., Anjum, R., Aziz, S., Qureshi, R., Sadak, F.,2022. A Deep Learning-Based Sensor Modeling for Smart Irrigation System. Agron-omy 12, 212.https://doi.org/10.3390/agronomy12010212 . Sau, S., Ucchesu, M., D'hallewin, G., Bacchetta, G., 2019. Potential use of seed morpho-colourimetric analysis for Sardinian apple cultivar characterisation. Comput. Electron.Agric. 162, 373–379.https://doi.org/10.1016/j.compag.2019.04.027 . Sau, S., Ucchesu, M., Dondini, L., De Franceschi, P., D'hallewin, G., Bacchetta, G., 2018. Seedmorphometry is suitable for apple-germplasm diversity-analyses. Comput. Electron.Agric. 151, 118–125.https://doi.org/10.1016/j.compag.2018.06.002 . Schmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural Netw. 61,85–117.https://doi.org/10.1016/j.neunet.2014.09.003 . Shah, D., Trivedi, V., Sheth, V., Shah, A., Chauhan, U., 2021. ResTS: Residual Deep interpret-able architecture for plant disease detection. Inf. Process. Agric. https://doi.org/10. 1016/j.inpa.2021.06.001.Shakoor, N., Lee, S., Mockler, T.C., 2017. High throughput phenotyping to accelerate cropbreeding and monitoring of diseases in the ﬁeld. Curr. Opin. Plant Biol., 38 Biotic in- teractions 2017. 38, pp. 184 –192.https://doi.org/10.1016/j.pbi.2017.05.006 . Shang, L., Heckelei, T., Gerullis, M.K., Börner, J., Rasch, S., 2021. Adoption and diffusion ofdigital farming technologies - integrating farm-level evidence and system interaction.Agric. Syst. 190, 103074.https://doi.org/10.1016/j.agsy.2021.103074 . Shen, W., Hu, H., Dai, B., Wei, X., Sun, J., Jiang, L., Sun, Y., 2020. Individual identiﬁcation of dairy cows based on convolutional neural networks. Multimed. Tools Appl. 79,14711–14724.Shojaeipour, A., Falzon, G., Kwan, P., Hadavi, N., Cowley, F.C., Paul, D., 2021. Automatedmuzzle detection and biometric identi ﬁcation via few-shot deep transfer learning of mixed breed cattle. Agronomy 11, 2365. https://doi.org/10.3390/agron- omy11112365.Shrivastava, S., Marshall-Colon, A., 2018. Big data in agriculture and their analyses. Ency-clopedia of Food Security and Sustainability. Elsevier, pp. 233 –237https://doi.org/10. 1016/B978-0-08-100596-5.22191-4 . Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale image recognition. ArXiv14091556 Cs.Singh, D., Jain, N., Jain, P., Kayal, P., Kumawat, S., Batra, N., 2020. PlantDoc: a dataset forvisual plant disease detection. Proc. 7th ACM IKDD CoDS 25th COMAD,pp. 249–253https://doi.org/10.1145/3371158.3371196 . Singh, P., Verma, A., Alex, J.S.R., 2021. Disease and pest infection detection in coconut treethrough deep learning techniques. Comput. Electron. Agric. 182, 105986. https://doi. org/10.1016/j.compag.2021.105986 . Son, C.-H., 2021. Leaf spot attention networks based on spot feature encoding for leaf dis-ease identi
ﬁcation and detection. Appl. Sci. 11, 7960. https://doi.org/10.3390/ app11177960.Subeesh, A., Mehta, C.R., 2021. Automation and digitization of agriculture using arti ﬁcial intelligence and internet of things. Artif. Intell. Agric. 5, 278 –291.https://doi.org/10. 1016/j.aiia.2021.11.004.Subeesh, A., Kumar, P., Chauhan, N., 2019. Flood early detection system using internet of things and artiﬁcial neural networks. International Conference on Innovative Com-puting and Communications. Springer, pp. 297 –305. Subeesh, A., Bhole, S., Singh, K., Chandel, N.S., Rajwade, Y.A., Rao, K.V.R., Kumar, S.P., Jat, D.,2022. Deep convolutional neural network models for weed detection in polyhousegrown bell peppers. Artif. Intell. Agric. 6, 47 –54.https://doi.org/10.1016/j.aiia.2022. 01.002.Suchithra, M.S., Pai, M.L., 2020. Improving the prediction accuracy of soil nutrient classi ﬁ- cation by optimizing extreme learning machine parameters. Inf. Process. Agric. 7,72–82.https://doi.org/10.1016/j.inpa.2019.05.003 . Sudarsan, B., Ji, W., Biswas, A., Adamchuk, V., 2016. Microscope-based computer vision tocharacterize soil texture and soil organic matter. Biosyst. Eng., Proximal Soil Sensing – Sensing Soil Condition and Functions 152, 41 –50.https://doi.org/10.1016/j. biosystemseng.2016.06.006. Syed-Ab-Rahman, S.F., Hesamian, M.H., Prasad, M., 2022. Citrus disease detection andclassiﬁcation using end-to-end anchor-based deep learning model. Appl. Intell. 52,927–938.https://doi.org/10.1007/s10489-021-02452-w . Taheri-Garavand, A., Nasiri, A., Fanourakis, D., Fatahi, S., Omid, M., Nikoloudakis, N., 2021.Automated in situ seed variety identi ﬁcation via deep learning: a case study in chick- pea. Plants 10, 1406.https://doi.org/10.3390/plants10071406 . Tang, J., Arvor, D., Corpetti, T., Tang, P., 2021. Mapping Center Pivot Irrigation Systems inthe Southern Amazon from Sentinel-2 Images. Water 13, 298. https://doi.org/10. 3390/w13030298.Tantalaki, N., Souravlas, S., Roumeliotis, M., 2019. Data-driven decision making in preci-sion agriculture: the rise of big data in agricultural systems. J. Agric. Food Inf. 20,344–380.https://doi.org/10.1080/10496505.2019.1638264 . Tedesco-Oliveira, D., Pereira da Silva, R., Maldonado, W., Zerbato, C., 2020. Convolutionalneural networks in predicting cotton yield from images of commercial ﬁelds. Comput. Electron. Agric. 171, 105307. https://doi.org/10.1016/j.compag.2020. 105307.Thakur, P., Chug, A., Singh, A.P., 2021. Plant disease detection of bell pepper plant usingtransfer learning over different models. 2021 8th International Conference on SignalProcessing and Integrated Networks (SPIN), pp. 384 –389https://doi.org/10.1109/ SPIN52536.2021.9565945. Tian, H., Wang, T., Liu, Y., Qiao, X., Li, Y., 2020. Computer vision technology in agriculturalautomation—A review. Inf. Process. Agric. 7, 1 –19.https://doi.org/10.1016/j.inpa. 2019.09.006.Tibbetts, John, 2018. The Frontiers of Arti ﬁcial Intelligence: Deep learning brings speed, accuracy to the life sciences. BioScience 68 (1), 5 –10.https://doi.org/10.1093/ biosci/bix136.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł., Polosukhin, I., 2017.Attention is All you Need, in: Advances in Neural InformationProcessing Systems. Curran Associates, Inc.Vázquez-Arellano, M., Griepentrog, H.W., Reiser, D., Paraforos, D.S., 2016. 3-D ImagingSystems for Agricultural Applications —A Review. Sensors 16, 618.https://doi.org/ 10.3390/s16050618.Veeramani, B., Raymond, J.W., Chanda, P., 2018. DeepSort: deep convolutional networksfor sorting haploid maize seeds. BMC Bioinformatics 19, 289. https://doi.org/10. 1186/s12859-018-2267-2. Veeranampalayam Sivakumar, A.N., Li, J., Scott, S., Psota, E., Jhala, J., Luck, J.D., Shi, Y., 2020.Comparison of object detection and patch-based classi ﬁcation deep learning models on mid- to late-season weed detection in UAV imagery. Remote Sens. 12, 2136.https://doi.org/10.3390/rs12132136 . Vishwakarma, D.K., Pandey, K., Kaur, A., Kushwaha, N.L., Kumar, R., Ali, R., Elbeltagi, A.,Kuriqi, A., 2022. Methods to estimate evapotranspiration in humid and subtropicalclimate conditions. Agric. Water Manag. 261, 107378. https://doi.org/10.1016/j. agwat.2021.107378.Waldhoff, G., Lussem, U., Bareth, G., 2017. Multi-Data Approach for remote sensing-basedregional crop rotation mapping: A case study for the Rur catchment, Germany. Int.J. Appl. Earth Obs. Geoinformation 61, 55 –69.https://doi.org/10.1016/j.jag.2017.04. 009.Wang, A., Xu, Y., Wei, X., Cui, B., 2020. Semantic segmentation of crop and weed using anencoder-decoder network and image enhancement method under uncontrolled out-door illumination. IEEE Access 8, 81724 –81734.https://doi.org/10.1109/ACCESS. 2020.2991354.Weary, D.M., Huzzey, J.M., von Keyserlingk, M., 2009. Board-invited review: Using behav-ior to predict and identify ill health in animals. J. Anim. Sci. 87, 770 –777.https://doi. org/10.2527/jas.2008-1297. Wei, M.C.F., Maldaner, L.F., Ottoni, P.M.N., Molin, J.P., 2020. Carrot yield mapping: a preci-sion agriculture approach based on machine learning. AI 1, 229 –241.https://doi.org/ 10.3390/ai1020015.Wu, N., Zhang, Y., Na, R., Mi, C., Zhu, S., He, Y., Zhang, C., 2019. Variety identi ﬁcation of oat seeds using hyperspectral imaging: investigating the representation ability of deepconvolutional neural network. RSC Adv. 9, 12635 –12644.https://doi.org/10.1039/ C8RA10335F.Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L., 2021. Cvt: Introducing con- volutions to vision transformers. Presented at the Proceedings of the IEEE/CVF Inter-national Conference on Computer Vision, pp. 22 –31. Xiao, J., Liu, G., Wang, K., Si, Y., 2022. Cow identi ﬁcation in free-stall barns based on an im- proved Mask R-CNN and an SVM. Comput. Electron. Agric. 194, 106738. https://doi. org/10.1016/j.compag.2022.106738 . Xu, G., Zhang, F., Shah, S.G., Ye, Y., Mao, H., 2011. Use of leaf color images to identify nitro-gen and potassium deﬁcient tomatoes. Pattern Recognit. Lett. 32, 1584 –1590.https:// doi.org/10.1016/j.patrec.2011.04.020 . Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X., 2017. AttnGAN: Fine-
Grained Text to Image Generation with Attentional Generative Adversarial Networks.https://doi.org/10.48550/arXiv.1711.10485 . Xu, B., Wang, W., Falzon, G., Kwan, P., Guo, L., Sun, Z., Li, C., 2020. Livestock classi ﬁcation and counting in quadcopter aerial images using Mask R-CNN. Int. J. Remote Sens. 41,8121–8142.https://doi.org/10.1080/01431161.2020.1734245 . Yang, Q., Xiao, D., Lin, S., 2018. Feeding behavior recognition for group-housed pigs withthe Faster R-CNN. Comput. Electron. Agric. 155, 453 –460.https://doi.org/10.1016/j. compag.2018.11.002.Yang, Q., Shi, L., Han, J., Zha, Y., Zhu, P., 2019. Deep convolutional neural networks for ricegrain yield estimation at the ripening stage using UAV-based remotely sensed im-ages. Field Crops Res. 235, 142 –153.https://doi.org/10.1016/j.fcr.2019.02.022 . Yoo, H.-J., 2015.Deep convolution neural networks in computer vision: a review. IEIETrans. Smart Process. Comput. 4, 35 –43. You, J., Li, X., Low, M., Lobell, D., Ermon, S., 2017. Deep Gaussian process for crop yield pre- diction based on remote sensing data. Proceedings of the Thirty-First AAAI Confer-ence on Artiﬁcial Intelligence, AAAI’17. AAAI Press, San Francisco, California, USA, pp. 4559–4565.Yu, Y., Xu, T., Shen, Z., Zhang, Y., Wang, X., 2019. Compressive spectral imaging system forsoil classiﬁcation with three-dimensional convolutional neural network. Opt. Express27, 23029–23048.https://doi.org/10.1364/OE.27.023029 . Zhang, X., Younan, N., King, R., 2003. Soil texture classiﬁcation using wavelet transform and Maximum Likelihood Approach. IGARSS 2003. 2003 IEEE International Geosci-ence and Remote Sensing Symposium. Proceedings (IEEE Cat. No.03CH37477),pp. 2888–2890.Zhang, C., Yue, P., Di, L., Wu, Z., 2018. Automatic identi ﬁcation of center pivot irrigation systems from landsat images using convolutional neural networks. Agriculture 8,147.https://doi.org/10.3390/agriculture8100147 . Zhang, Y., Chu, J., Leng, L., Miao, J., 2020. Mask-Re ﬁned R-CNN: A Network for Reﬁning Ob- ject Details in Instance Segmentation. Sensors 20, 1010. https://doi.org/10.3390/ s20041010.Zhao, G., Quan, L., Li, H., Feng, H., Li, S., Zhang, S., Liu, R., 2021. Real-time recognition sys-tem of soybean seed full-surface defects based on deep learning. Comput. Electron.Agric. 187, 106230.https://doi.org/10.1016/j.compag.2021.106230 .V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
228Zhong, G., Wang, L.-N., Ling, X., Dong, J., 2016. An overview on data representation learn-ing: From traditional feature learning to recent deep learning. J. Finance Data Sci. 2,265–278.https://doi.org/10.1016/j.jfds.2017.05.001 . Zhong, L., Guo, X., Xu, Z., Ding, M., 2021. Soil properties: Their prediction and feature ex-traction from the LUCAS spectral library using deep convolutional neural networks.Geoderma 402, 115366.https://doi.org/10.1016/j.geoderma.2021.115366 . Zhou, Z., Song, Z., Fu, L., Gao, F., Li, R., Cui, Y., 2020. Real-time kiwifruit detection in orchardusing deep learning on Android
TMsmartphones for yield estimation. Comput. Elec- tron. Agric. 179, 105856.https://doi.org/10.1016/j.compag.2020.105856 .Zhu, S., Zhou, L., Gao, P., Bao, Y., He, Y., Feng, L., 2019. Near-infrared hyperspectral imagingcombined with deep learning to identify cotton seed varieties. Molecules 24, 3268.https://doi.org/10.3390/molecules24183268 . Zhu, F., He, M., Zheng, Z., 2020a. Data augmentation using improved cDCGAN for plantvigor rating. Comput. Electron. Agric. 175, 105603. https://doi.org/10.1016/j. compag.2020.105603.Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., 2020b. Unpaired Image-to-Image Translation usingCycle-Consistent Adversarial Networks. https://doi.org/10.48550/arXiv.1703.10593 .V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 211 –229
229