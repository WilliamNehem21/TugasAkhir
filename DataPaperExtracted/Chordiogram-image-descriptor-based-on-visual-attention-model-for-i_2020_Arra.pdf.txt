Chordiogram image descriptor based on visual attention model forimage retrieval
S. Sathiamoorthya, A. Saravananb,*, R. Ponnusamyb
aTamil Virtual Academy, Chennai, India
bDivision of Computer and Information Science, Annamalai University, Annamalai Nagar, India
ARTICLE INFO
Keywords:Chordiogram image descriptorEdge mapSaliency mapSalient edgesABSTRACT
A novel shape-based image retrieval is presented in this study. The foreground and background contents of imagesare strongly concealed, so they are represented individually to reduce their in ﬂuence on each other in the proposed approach. The Otsu method is employed for segmenting the foreground from the background, and the saliency mapand edge map are then clearly identiﬁed. Saliency reduces the time cost for feature computation, so salient edges are computed for the foreground and background images based on the selective visual attention model.Autocorrelation-based chordiogram image descriptors are computed separately for the foreground and backgroundimages, which are then combined in a hierarchical manner to form the proposed new descriptor. This approachavoids the concealment of foreground and background information, and the new descriptor is rich in geometric andits underlying texture, structure and spatial information. The proposed novel shape-based descriptor performsconsiderably better than conventional descriptors at content-based image retrieval. The proposed shape descriptorwere extensively tested at image retrieval based on the Gardens Point Walking, St Lucia, University of AlbertaCampus, Corel 10 k, and self-photographed image data sets. The precision and recall values were compared for theproposed and state-of-the-art-approaches when applied for shape-based image retrieval from these databases. Theproposed shape descriptor provided satisfactory retrieval results in the experiments.
1. IntroductionAt present, the number of digital images is increasing rapidly due tothe use of various photography devices, such as webcams, mobile phones,and closed-circuit television (CCTV) cameras, and thus the sizes of imagedatabases are growing greatly. Hence, storage, retrieval, and mainte-nance are important tasks for image databases. Image retrieval is broadlydivided into two groups comprising (1) text-based image retrieval (TBIR)[1] and (2) content-based image retrieval (CBIR) [ 2–4]. TBIR wasﬁrst introduced in the early 1970s and it uses manually annotated words todescribe images, which is a difﬁcult, time-consuming, and tedious taskwhen the size of the image database is large, and this method is alsosubject to problems related to visual perception [ 5,6]. To resolve the issues related to TBIR, CBIR was introduced in 1992 by Kato [ 7] and research in this domain of computer vision has continued for more thanthree decades. The need for more effective CBIR systems with high ac-curacy and low time costs has stimulated the development of improvedCBIR systems. CBIR allows the user to retrieve images more ef ﬁciently from image databases by employing feature extraction and matching.Feature extraction is characterized by the utilization of the color, shape,and texture of images, where it must be able to differentiate among im-ages from the same and other classes with very small differences [ 8–11]. Furthermore, the features must be robust to geometric changes such asrotation, scaling, and translation, and photometric changes includingdifferences in illumination and occlusion. Images are characterized using(1) global and (2) local approaches. In global approaches, images arecharacterized by ignoring the local and spatial information in the pictureelements. The global approaches are computationally ef ﬁcient and robust to noise to some extent, but they are not satisfactory at handling issuessuch as variations in illumination and occlusion. However, all of theproblems with global approaches can be addressed by using local ap-proaches where features are computed based on local patches, regions, orselected key points.Shape is an important component used in image recognition andmatching. In the present study, we propose a method based on shapeinformation, and thus we focus on previous methods based only on shapeinformation in the following. Several shape characterization andmatching approaches have been proposed in previous studies. In general,
* Corresponding author.E-mail addresses:ks_sathia@yahoo.com(S. Sathiamoorthy),sjpramoth@gmail.com(A. Saravanan).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100027Received 14 July 2019; Received in revised form 20 April 2020; Accepted 22 April 2020Available online 1 May 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 7 (2020) 100027shape features are computed with: (1) boundary-or contour-based ap-proaches, and (2) region-based approaches, where the former charac-terize the details of the shapes in images using the contours of an objectand the latter use all of the pixels in a region [ 12–15]. We focus our discussion on boundary-based shape characterization methods.Boundary-based shape characterization using chain code histogram[15,16] provides compact representations, translation invariant, preser-ving all of the morphological information. Shape signatures [ 17,18] such as cumulative angle, centroidal proﬁle and chord lengh, and global shapedescriptors such as the area, circularity, eccentricity, bending energy,convexity, major axis orientation, ratio of the principal axis, circularvariance, and elliptic variance, and shape invariants were also employedin previous studies [19–21]. The contour point distribution histogram[22] comprises the distribution of points on an object contour underpolar coordinates and it is a suitable approach for describing shapes withclosed contours but not for images with multiple connected regions.Boundary moments were considered in some previous studies [ 23,24]. The curvature scale space approach employs a corner point detector tosearch for the curvature maxima or inﬂection points on the edges detected using the Canny approach [ 25]. Elastic matching involves an optimization problem based on the pixel to pixel correspondence be-tween two images and it is robust to geometric deformation [ 26,27]. Eigen values are also invariant under rigid motion and scaling [ 28]. A shape-based non-redundant local binary pattern was presented by Yaoand Chen [29]. The local maximum edge binary pattern suggested byMurala et al. [8] is computed based on the local differences among thecenter and eight neighborhood pixels, and it is combined with the Gabortransform to ensure its effectiveness. The local edge pattern proposed byYao and Chen [30] uses the Sobel edge to compute the local edge patternfor segmentation and the local edge pattern for retrieval. Murala and Wuproposed peak valley edge patterns [ 31], where they obtained the local mesh peak valley edge pattern by including the ﬁrst order derivative in the local mesh patterns [32]. The edge histogram descriptor reported byJain and Vailya involves the distribution of the orientations of edgels,and the edges are computed using the Canny approach [ 33]. Another histogram method based on the edge distribution at the local level usesthe Sobel operator for edge detection [ 34], although it obtains sub- standard retrieval results. Thus, a method was developed that uses theabsolute locations of edge and the global composition to enhance theretrieval rate [35]. Jiebo et al. [36] reported a color edge co-occurrencehistogram that calculates the distribution of the separation between pairsof color edges. A method based on the color distributions on directionaland non-directional edges was suggested by Shim and Choi [ 37]. The block variation of local correlation coefﬁ
cients method presented by Chen et al. identiﬁes the edges and valleys in an image, which arecharacterized byﬁrst order moments [38,39], and a method based on the distributions of the edges and valleys was then proposed [ 40]. The edge orientation autocorrelogram method [ 41] computes the correlations among the edgels based on their orientation, and this method is robust todifferences in illumination, viewpoint, translation, and small amounts ofrotation. An enhanced version based on the edge histogram descriptorand edge orientation autocorrelogram methods [ 42,43] employs extremely minute andﬁne edges using a framework based on the fullrange autoregressive model for grayscale and color images, where theedges of color images are computed in the HSV space in order to avoidmissing minute andﬁne edges due to changes in spectral and chromaticdetails.Recently, Toshev et al. [44] proposed a chordiogram image descriptor(CID) that computes the geometric details for a selected set of edgelsobtained by segmentation. This method then employs geometric detailscomprising the distances among pairs of edgels, orientations of pairs ofedgels, and the degree of the angle connecting pairs of edgels and thehorizontal axis. However, the boundaries computed by segmentationmight include fake edgels and this can affect the accuracy of results.Further computing the geometric details for every pair of edgels greatlyincreases the time cost. A subsequently developed method computes theintensity and distance among each pair of edgels [ 45]. Moreover, a bi- nary coherent edge descriptor was presented that characterizes the co-ordinates and orientation of each edgel, and the length of an edge thatpasses through the edgels [46].Wang et al. proposed a new variant of CID that collects the distribu-tions of the chord details for patches in images. The image is divided intoa number of non-overlapping rectangular patches to reduce the in ﬂuence of lighting. The CID method is robust to edge detector and it reduces thecomputational cost by employing predominant edgels. Statistical testsare conducted to identify the predominant edgels in each patch [ 47] and for every pair of predominant edgels in each patch, it is necessary tocompute the distance among every pair of predominant edgels, the ori-entations of each predominant edgel in a pair, and the degree of the anglebetween a line segment among pair predominant edgels and the hori-zontal axis, before combining these geometric details in a local edgelchordiogram (LEC) [47]. The ordered collection of LECs for all patchimages comprises the CID. The CID is robust to differences in illumina-tion, translation, and in-plane rotation, but it is affected considerably bynoise. Thus, the patches with noise are eliminated during the matchingoperation by avoiding higher values in the similarity results obtainedbetween the corresponding patches in the query and target images. CID isan appropriate method for place recognition with illumination changes,while the time cost is low and it can avoid fake edgels because edgedetectors are used for edge identiﬁcation instead of segmentation.In a previous study, we enhanced the efﬁciency of this method by computing the CID using an autocorrelation function to obtain theautocorrelation-based CID (ACID) [48]. The ACID exploits the spatial correlation among identical predominant edgels at distance d, theorientation details for each predominant edgel in a pair of identicalpredominant edgels at distance d, and the degree of the angle along theline segment between a pair of identical predominant edgels and thehorizontal axis. Our method neglects the length between a pair of pre-dominant edgels because the length is always 1 in our approach. Wedemonstrated that ACID performs better than the conventional CID.All of the previous approaches mentioned above compute the shapedetails for either a whole image or objects segmented from an image.However, previous studies have shown that the background and fore-ground details in images are concealed by both the global and localfeatures [49], thereby resulting in poor retrieval performance becausethe user may be focused on objects in the background or foreground, orboth. However, pinpointing the interests of users such as the backgroundor foreground or even a speciﬁc object in the foreground is a challengingproblem for the current CBIR approaches. Furthermore, separating theobjects in the foreground and comparing them with the correspondingobject in a target image is still a difﬁcult issue for the existing CBIR ap- proaches. At present, these problems are resolved using a relevancefeedback approach where users are permitted to choose the images fromthe retrieval results obtained by a query and the selected images are thenjointly employed to reﬁne the query image until it corresponds subjec-tively to a user’s needs in a particular search, and thus this processcontinues until the user is satisﬁed with the results [50]. Recently, ma- chine learning has been combined with the relevance feedback approachto enhance the retrieval rate, but this approach also fails because of thelow number of training images and the unwillingness of users to partic-ipate in the relevance feedback approach for a lengthy period of time[50].Recently, Feng et al. [50] presented a CBIR where the salient edgesand salient regions in an image are used as the retrieval targets becausethey coincide with the interests of users. This method uses the selectivevisual attention model to exploit the salient edges and salient regions inimage, where the salient edge histogram (SEH) and salient adjacencygraphs are computed and used jointly for CBIR to reduce the computationcosts incurred to obtain both the local and global level image features.Another study [49] showed that only considering the features in theforeground can result in poor performance when the images have richcontents in the background, and thus the distinctive features areS. Sathiamoorthy et al. Array 7 (2020) 100027
2computed in both the foreground and background in order to avoid theminﬂuencing each other, thereby obtaining hierarchical feature de-scriptions and achieving more accurate image matching.In the method proposed in the present study, we computed the ACIDbased on the saliency edges for the foreground and background images,and obtain hierarchical feature descriptions by using the ACID to reducethe effects on each other of the foreground and background features. Thismethod captures more geometric and its underlying texture, structure,and spatial details by considering a higher number of more responsivesalient edgels (25% of the total number of edgels) than the conventionalmethod (15% of the total number of edgels). The proposed method se-lects more salient edgels than the conventional approach in order tocapture the rich underlying texture and structure information. Bycontrast, considering less salient edgels will reduce the computationalcost, but this method fails to capture much of the underlying texture andstructure information among the salient edgels. We experimentallyevaluated the proposed approach by considering various subsets ofsalient edgels where each subset varied in terms of the numbers of salientedgels, and the response strengths of the salient edgels were consideredwhen selecting them for a subset. The experimental results demonstratedthat considering 25% of the salient edgels for extracting the rich un-derlying texture, structure and spatial information could obtain moreaccurate results, whereas reducing the number of salient edgels signi ﬁ- cantly reduced the cost but it yielded less accurate retrieval results. Ourproposed method employs the Otsu algorithm [ 49] to segregate the image into foreground and background details, while the Canny operatoris used for edge detection and the selective visual attention model [ 50]t o exploit the salient edges. We comprehensively tested the proposedapproach based on benchmark databases and the results were comparedwith those obtained using CID [47], ACID [48], and SEH [50]. The proposed approach obtained more accurate results than CBIR. The pro-posed retrieval approach is an enhanced version of our previously re-ported method [48].The remainder of this paper is organized as follows. In Section 2,w e describe the approaches incorporated in the proposed CBIR. The exper-imental results and discussion are presented in Section 3. Finally, we give our conclusions in Section4.2. Feature extraction techniquesIn the following, we provide overviews of the selective visual atten-tion model, CID and ACID techniques, proposed CBIR, and featuredescriptor matching method. The architecture of the proposed retrievalapproach is illustrated inFig. 1.2.1. Selective visual attention modelRecently, Feng et al. [50] enhanced the saliency model and deﬁned itas a linear combination of the intensity contrasts in the Gaussian imagepyramid. They reported that their approach can obtain more precisedetails from an image. Thus, let us assume that the image Iis in the RGB color space and a mask with a size of 3/C23 is centered on a given pixel p in an image with M/C2N dimensions in order to compute the saliencyvalue at pixel p based on its adjacent neighbors as follows [ 50]:SVðpÞ¼X
Li¼1X
p2LxLωcSlcðp;qÞþ ωoSloðp;qÞ(1)where the number of levels (L) in the pyramid is three, l represents the lthlevel in the pyramid image, and
ωc;ωo;Scðp;qÞ, andS oðp;qÞrepresent the weight coefﬁcients, color intensity, and orientation contrasts between pand q, respectively. A Gaborﬁlter with four orientations (0
∘;45∘;90∘;135∘Þand four color channels comprising R, G, B, and Y [ 50] is used to compute the orientation and intensity contrast information.According to Feng et al. [50], the contrast at each level of the pyramid iscomputed and combined in a linear manner to frame the ﬁnal saliency map, before a Gaussianﬁlter with a standard deviation of 1 is applied toremove the noise.2.2. Salient edge detectionA previous study showed that all of the edges identi ﬁed by edge de- tectors are not valuable for characterizing an image [ 50]. Hence, several approaches have been reported for computing the salient edges. The termsaliency is used to describe the difference between a pixel and those in itsadjacent neighborhood [50]. Recently, Feng et al. [50] suggested a novel approach based on the visual attention model where the saliency of anedge is measured using its length and the saliency values around it.According to Feng et al. [50], we employ the Canny operator to detectthe edges as:E
I¼/C8eI1;eI2; :::;eIN/C9; (2)where N denotes the total number of segments in an image I. The salient edges are computed as follows [50]: SE/C0e
Ii/C1¼ωLL/C0eIi/C1þωsSA/C0eIi/C1;i¼1; :::;N;(3)whereLðe
IiÞrepresents the length of edgeeIi,SAðeIiÞrepresents the average saliency value for edgee
ibased on the saliency map,and
ωLandωSrepresents the weights forLðeIiÞandSAðeIiÞ, respectively, which are set to 0.3 and 0.7 [50]. The values ofLðe
IiÞandSAðeIiÞare normalized, andSAðe
IiÞis described as [50]:Fig. 1.Architecture of the proposed image retrieval approach.S. Sathiamoorthy et al. Array 7 (2020) 100027
3SA0@eIi1A¼XLðeIi
n¼1X
p2ΘpinSV/C0p/C1/C14L/C0eIi/C1; (4)WhereΘ
pinrepresents a 3/C23 mask centered at pixelpinand SV(p) is the saliency value of pixel p. After computing all of the saliency values for theedges, the following threshold operation is performed [ 50]. T
E¼max/C0SA/C0eIi/C1/C1/C144 (5) Theﬁnal salient edge is then described as [ 50]: Θ
SE¼/C8eIi/C12/C12SA/C0eIi/C1>T
E;i¼1; :::;N/C9 (6)2.3. CIDA given image is divided into several non-overlapping rectangularpatches numbered from 1, 2, 3,…, N. Each patch is characterized usingthe LEC. To compute the LEC, the edges are identi ﬁed using any edge operator and the prominent edgels are identi ﬁed for each patch by applying a statistical hypothesis test, before the local geometric featuresare computed based on the prominent edgels. The LECs obtained for allpatches are then collected in an order to construct a CID. The chorddetails for an image patch are computed based on each pair of prominentedgels coordinated at p and q as follows [ 47]: C
ipq¼/C0ℓpq;ϕpq;θp;θq/C1; (7)where
ℓpq;ϕpq;θp, andθ qdenote the distance among predominant edgels pand q, the angle between the line connecting p and q and horizontalplane, and the degrees of orientation for predominant edgels p and qabout the normal directions, respectively. The values of
ℓpqrange from 0 to the diameter of an image patch, and ϕ
pq;θp, andθ qrange from 0 to 2Π. The values of
ℓpqare discretized in the logarithmic space and they aredivided into
ηdbins where d is the distance, which isﬁxed to 4 [47].ϕpq; θ
p, andθ qare each quantized into eight bins [ 47]. Thus, CID is a four-dimensional histogram that is normalized as described previously[47], and it encompasses the chord details for every pair of prominentedges in an image patch.2.4. ACIDWe propose a novel characterization approach for an image usingACID, which represents an improved version of the approach describedby Wang et al. [47]. ACID computes the spatial correlations among theidentical predominant edgels and explores the variations in the correla-tions at a distance d for each pair of the predominant edgels in asub-image with a size of 3/C23 (one is at the center of the sub-image andother is an identical edgel at distance d from the center of the mask). Wealso compute the orientation of each predominant edgel and the degreeof the angle along the line segment between the pair of predominantedgels and the horizontal plane. The autocorrelations among the iden-tiﬁed identical predominant edgels are depicted in a table and indexedbased on the edgel and distance values. An entry in the table denotes theprobability ofﬁnding a predominant edgel with value i at a correlationdistancedfrom a predominant edgel with value i .Thus, ACID considers the spatial correlation among the identical predominant edgels and itﬁxes the correlation distance to 1 because the lowest correlation distanceprovides the detailed local properties of an image [ 38].Table 1depictsTable 1The representation of autocorrelation of identicalpredominant edgels at d¼1.
Edgel Valu Distance (d)D¼1255 0.019254 0.075253 0.102........
Fig. 2.(a) Sample image of size 6/C26, (3/C23 mask is moved in non-overlapping manner. Center of 3/C23 mask with the salient edgel (value ¼255) is marked in red color and its identical salient edgels at distance 1 is marked in blue color) 2(b). Computation of orientation of edgels with value 255 and angle between thehorizontal axis and line segment of edgels 255 and 255 at distance 1.S. Sathiamoorthy et al. Array 7 (2020) 100027
4the proposed spatial correlations among identical predominant edgels atdistance 1, where theﬁrst and second columns show the edgels withvalue i and the probability ofﬁnding an edgel with value i at correlationdistance 1 from an edgel with value i, respectively. The autocorrelationsamong identical predominant edgels are described as follows [ 48]. LetIbe an n/C2n image and the predominant edgels in Iaree
1;e2;:::;e m. For an predominant edgelE¼ðx;yÞ2I, letIðEÞrepresent its edge value. LetI
e¼ΔfEjIðEÞ¼egHence, the notationE2I eis synonymous withE2I; IðEÞ¼e. For convenience, we use the L
∞norm to assess the distance among predominant edgels [ 48], i.e., for predominant edgels E
1¼ðx 1;y1Þ;E2¼ðx 2;y2Þ,w ed eﬁnejE 1/C0E 2j¼Δmaxfjx 1/C0x 2j;jy1/C0 y
2jgWe denote the setf1;2; :::;ngbyjnj.Deﬁnitions. The histogramhofIis expressed fori2jmjas follows.h
eiðIÞ¼Δn2PrjE2I eij
E2I(8)For any predominant edgel in the image, h
eiðIÞ=n 2is the probability that the value of predominant edgel is e
i. Let a distanced2jnjbeﬁxed a priori. Then, the correlation of Iis expressed fori;j2jmj;k2jdjas follows.γ
ðkÞe
i;ej¼ΔPr
E12Iei;E22I/C2E
22IejjjE1/C0E 2j¼k/C3(9)Given any predominant edgel of value e
iin the image,γðkÞe
i;ejðIÞis the probability that a predominant edgel at distance kfrom the given pre- dominant edgel is of valuee
j. The autocorrelation ofIonly exploits the spatial correlation among identical predominant edgels, and it is de ﬁned as [48]:
αðkÞc¼ΔγðkÞe;eðIÞ (10) To estimate the ACID, a 3/C23 non-overlapping mask is moved over animage from left to right and then from the topmost left corner of theimage for each identiﬁed predominant edgel value. For instance, asshown inFig. 2, when we move the 3/C23 mask over an image to compute the autocorrelation value of the predominant edgel with a value of 255,we obtain an identical predominant edgel with a value of 255 at thecenter of theﬁrst 3/C23 sub-image and it has two identical predominantedgels at distance 1. Thus, we estimate the autocorrelation using Eq. (10) and then estimate the orientation of each predominant edgel in everypair of predominant edgels (center and its identical predominant edgel)asθ
1andθ 2then compute the degree of the angle of the line segmentbetween each pair of predominant edgels and the horizontal axis as ϕ,a s depicted inFig. 2(b). Next, the 3/C23 mask is moved to the left and thereis no predominant edgel with a value of 255 at the center of the mask.Thus, the mask is moved further. In the third 3 /C23 sub-image, there is also no center edgel with a value of 255 but the fourth contains a pre-dominant edgel in the center of the sub-image with a value of 255 and ithas one identical predominant edgel at distance 1. Therefore, we esti-mate the autocorrelations among the identical predominant edgels forthe fourth sub-image usingEq. (10), and we compute the values ofθ
1,θ2, andϕ[48]. This process continues for each predominant edgel valueidentiﬁed in the image. Finally, we obtain the autocorrelations of pre-dominant edgels, as shown inTable 1, and the set ofθ
1,θ2, andϕvalues are represented by separate histograms.In our previous study [48], we demonstrated that ACID obtains abetter retrieval rate because it exploits geometric and its underlyingtexture, structure, and spatial details and the computational cost isequivalent to that of the CID approach [ 47]. We also normalized the histogram for ACID to obtain more lighting variations [ 48]. Therefore, in the method proposed in the present study, we employfour-dimensional ACID for CBIR. In the proposed CBIR method, an imageis segregated into foreground and background images, and ACID is thencomputed for the whole foreground and background images instead ofcomputing it based on the patches in the whole image [ 47,48]. Therefore,the proposed approach avoids the concealment of details in the fore-ground and background, and the computational cost is also signi ﬁcantly reduced.2.5. Proposed CBIR approachDetails are concealed in the foreground and background of an image,where each inﬂuences others to affect the performance of CBIR. Hence, inthe proposed CBIR approach, the query images are divided into fore-ground and background details using the Otsu algorithm [ 49] in order to reduce the effects of the foreground and background on each other. TheCanny operator is then applied to the foreground image to compute theedge map. Subsequently, the saliency map is computed for the fore-ground image using the selective visual attention model proposed byFeng et al. [50]. The saliency edges are computed using the edge andsaliency maps for the foreground image. The Canny operator and theapproach proposed by Feng et al. [50] are used to obtain the edge map and saliency map for the background images, which are then employed tocompute the saliency edges. Based on a trial and error method, weshowed that selecting 25% of the highly responsive salient edgels fromall of the salient edgels in an image to compute the proposed featureobtains good performance.Based on the saliency edgels selected in the foreground and back-ground images, ACID is computed as follows (see Section 2.D).1. Determine the spatial correlations among the identi ﬁed identical edgels and explore the variations in the correlations with distance 1.2. For each pair of salient edgels in a sub-image with a size of 3 /C23 (one is at the center of the sub-image and other is an identical salient edgelat distance 1 from the center of the mask), ACID computes theorientation of each salient edgel.3. Calculate the degree of the angle along the line segment between thepair of salient edgels and the horizontal plane.Thus, the proposed ACID approach based on the selective visualattention model obtains a histogram with four dimensions. In the pro-posed CBIR, the ACIDs in the foreground and background are combinedin a hierarchical manner to describe the image, and thus the proposedCBIR exploits both ACIDs to reduce the inﬂuence of the foreground and background images on each other in CBIR. The algorithm for the pro-posed CBIR approach is described as follows.Input:ImageOutput:Retrieved images1. Divide the image into foreground and background images using theOtsu approach.2. Obtain the edge map using the Canny operator for both the fore-ground and background images.3. Obtain saliency maps for both the foreground and background imagesusing the Gaussian image pyramid.4. Compute saliency edges for both the foreground and backgroundimages.5. Compute ACIDs for the foreground and background images using thestrongly responsive saliency edges.6. Combine the ACIDs for the foreground and background images in ahierarchical manner.7. Measure the similarity between feature vectors in the database andquery image.8. Sort the similarity values between the query and all of the images indatabase.9. Result.2.6. Feature matching methodNext, we present an effective approach for feature matching bycomputing the similarity between the query image and target image. TwoS. Sathiamoorthy et al. Array 7 (2020) 100027
5images are related to each other when the similarity between the twoimages is a small value. Various similarity metrics can be employed tocompute the similarity of two images, but the Euclidean method is morefamiliar and it is used extensively for CBIR. In the proposed method, aftercomputing the features locally in the form of four-dimensional histo-grams, the similarity between the query and target images is computedwith the following equation [50–52]:SðQ;TÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ ﬃX
Ni¼0ðjQ i/C0T ijÞ2vuut; (11)where T and Q denote the input image and target image featuredescriptor, respectively, and N is the number of descriptors in the imagefeature vector. We also compared the performance of other distancemeasures [50–52] such as the Manhattan, Canberra, Chi-square, andChebyshev metrics in terms of the average precision, and the resultsdemonstrated that worst performance was obtained with the Canberrametric and the highest performance using the Euclidean distance metric.The Manhattan metric performed signiﬁcantly worse than the Euclidean metric. Thus, the Euclidean distance is used in our proposed CBIRapproach.Fig. 3shows the average recall results with various distancemeasures.3. Experimental results and discussionIn the experiments, we compared selected methods comprising CID[47], ACID [48], and SEH [50] with the proposed approach. We evalu-ated the performance of these approaches based on the Gardens PointWalking [47], University of Alberta (UA) Campus [ 47], St Lucia [47], Corel 10 k [54], and self-photographed image databases. The superiorperformance of the proposed approach was validated by evaluating theprecision and recall, and based on comparisons with the other ap-proaches with allﬁve image databases. The precision deﬁnes the rela- tionship among the total number of related images retrieved for a giveninput image and the total number of images retrieved from the database,which gradually decreases as we increase the number of retrieved im-ages. The precision is deﬁned as follows [48].PrecisionðPÞ¼
Total No:of correct images retrieved from databaseTotal No:of images retrieved from the database (12) Another common measure used for computing the accuracy is therecall, which is deﬁned as the probability of retrieving a correct relatedimage for a given query and it increases as the number of retrieved im-ages increases. The recall is deﬁned as follows [48].RecallðRÞ¼Total No:of correct images retrieved from databaseTotal No:of images relevant images in the database (13) The average precision is computed as [ 48]:P
CAvg¼1NXNi¼1P; (14)and the average recall is computed as [ 48]:R
CAvg¼1NXNi¼1R; (15)wherecandNdenote the class and total number of images in the data-base, respectively. Thus, the total precision and total recall are computedas follows.P
total¼1CXCi¼1PCAvg (16)R
total¼1CXCi¼1RCAvg (17)To measure the effectiveness of the methods, a query image wasselected from each benchmark database and the top 100 images retrievedby CID [47], ACID [48], SEH [50], and the proposed approach wereconsidered. All of the images in the databases were used as queries toassess the retrieval performance. The state-of-the-art methods and theproposed technique were implemented on a PC with an Intel PentiumCore 2 Duo 2.10 GHz processor and 2 GB RAM.The proposed approach was compared with the state-of-the-art de-scriptors based on the Gardens Point Walking database, which containsthree groups of images recorded by pedestrian. The viewpoint variationswere taken at day (by walking on both left and right) and night (footageon right). Each group contains 200 images captured twice during the dayand night, and the sizes of the images are approximately 960 /C2540 pixels. The precision and recall curves shown in Fig. 4demonstrate the performance of the different descriptors. Clearly, SEH obtained the worstperformance among all of the descriptors and ACID performed betterthan CID and SEH. However, ACID based on the application of the se-lective visual attention model for the foreground and background imagesin a hierarchical manner obtained much higher accuracy than the CID,ACID, and SEH descriptors.We then conducted an experiment based on the UA Campus databasewhere we considered the sequences collected at three different times of
Fig. 3.Average retrieval accuracy attained by various distance measures for theproposed feature descriptor.
Fig. 4.Precision Vs recall of the proposed and existing approaches for GardensPoint Walking database.S. Sathiamoorthy et al. Array 7 (2020) 100027
6day, i.e., at 06:20, 16:40, and 22:15, where each sequence contained 630frames [47]. The results obtained by applying CID, ACID, SEH, and theproposed ACID based on the selective visual attention model to theforeground and background images in a hierarchical manner were rela-tively similar to the results obtained with the Gardens Point Walkingdatabase. The proposed approach performed better than ACID, while theperformance of CID was intermediate and that of SEH was again theworst. The corresponding precision and recall plots are depicted in Fig. 5. Next, we compared the performance of the different descriptors basedon the St. Lucia database. The precision and recall curves obtained for theproposed and existing approaches are depicted in Fig. 6. The St Lucia database contains images acquired atﬁve different times between the early morning and late afternoon during the day, and during the day aftertwo weeks, where it comprises 10 groups of images. In the experiments,the worst performance was obtained using SEH and the best performancewith the proposed method, while CID obtained intermediate perfor-mance and ACID performed well according to the precision and recallplots. The proposed approach is an enhanced version of our previouslypresented method [48] combined with that described by Wei et al. [ 49], and it performed better than the existing approaches with the St.Luciadatabase.The subsequent experiment was conducted based on the Corel 10 kdatabase, which is has been utilized by many researchers for CBIR as-sessments. The Corel 10 k database contains 10000 images in 100 classes,including crab, rhino, panda, cup, tiger, door, ﬁtness, bob, dish, andﬂags, and each class comprises 100 images. The sizes of the images areapproximately 192/C2128 or 128/C2192 pixels. The precision and recallcurves are shown inFig. 7. Clearly, SEH obtained the worst performance,while ACID performed better than CID and SEH, but the highest perfor-mance was achieved using the proposed approach.We also tested the performance of the proposed method, CID, ACID,and SEH based on a self-photographed images database and the corre-sponding results in terms of the precision and recall curves are illustratedinFig. 8. The self-photographed images were acquired at various timesduring the day and they comprised 1140 images with sizes of approxi-mately 1280/C2720 and 1040/C2780 pixels. The results clearly demon-strated that the proposed descriptor obtained better retrievalperformance than all of the other descriptors.The results demonstrated that the proposed approach was morerobust to differences in illumination and occlusion compared with CIDand SEH. The proposed approach and ACID exhibited similar robustnessto differences in illumination and occlusion. However, the proposedapproach obtained better retrieval performance than CID, ACID, andSEH. The low retrieval accuracy of SEH is due to computation at theglobal level and a failure to capture the geometric details of salientedgels. CID obtained good accuracy but it ignores the underlying texture,structure and spatial information of dominant edgels, which is importantfor CBIR. Thus, the retrieval accuracy was reduced with CID. ACIDconsiders the geometric and its underlying texture, structure and spatialdetails of dominant edgels, but foreground and background details areconcealed in images and the method employed to determine the domi-nant edgels for computing the ACID leads to poor performance. Theseproblems with the state-of-the-art methods are addressed in the proposedapproach, and thus it obtains signiﬁcantly better retrieval performance.
Fig. 5.Precision Vs recall of the proposed and existing approaches forUA database.
Fig. 6.Precision Vs recall of the proposed and existing approaches for St.Lu-cia database.
Fig. 7.Precision Vs recall of the proposed and existing approaches for Corel 10k database.
Fig. 8.Precision Vs recall of the proposed and existing approaches for Self-photographed image database.
Table 2Retrieval performance of the proposed, ACID, CID and SEH.
Datasets Proposed ACID CID SEHGardens Point Walking 74.23 72.10 68.93 67.89UA Campus 72.47 70.88 62.91 60.78St. Lucia 69.32 65.83 58.25 55.71Corel 10 k 71.03 67.31 59.20 56.13Self Photographed Images 70.00 65.70 58.80 54.90S. Sathiamoorthy et al. Array 7 (2020) 100027
7Table 2show the average precision and recall values using the proposeddescriptor and the ACID, CID, and SEH approaches based on the GardensPoint Walking, St Lucia, UA Campus, Corel 10 k, and self-photographedimages databases. Examples of images in all of the benchmark databasesare shown inFig. 9. In addition, examples of the retrieval results obtainedby the proposed approach based on the Gardens Point Walking, St Lucia,and self-photographed images databases are presented in Fig. 10. In the experiments, we also computed the proposed feature usingvarious subsets of salient edgels, where each subset (S) varied in terms ofthe number of salient edgels. In particular, if N salient edgels are presentin an image, then the subset S contains P/C2N salient edgels, wherePis the proportion used to determine the size of the subset. In the experi-ments, we started withP¼5%, i.e., 5% of the stronger responsive salientedgels were selected initially, and we then increased Pby 5% incre- mentally. For each value ofP, we computed the proposed feature andmeasured the performance in terms of the accuracy and time cost. Thebest performance was obtained when P¼25%. In the experiments, we changed the value fromP¼5%–100% and the accuracy increasedgradually toP¼25%, but there were no further changes in the perfor-mance with higher values when we increased P, although the computa- tional cost increased, as shown inFig. 11. Therefore, the results clearly showed that the proposed approach could extract rich geometric and itsunderlying texture, structure and spatial information when P¼25%. Any descriptor must perform effectively and the computational costshould also below. Therefore, the central processing unit (CPU) timesrequired by the proposed method, CID, SEH, and ACID were computed byextracting the features ofﬂine (to create a feature database) and online(retrieval). The average CPU times required by the proposed method andthe CID, SEH, and ACID descriptors are shown in Table 3. SEH required the least CPU time whereas the proposed approach consumed the mostCPU time, and it was slightly higher than that by ACID and slightly lowerthan that by CID. However, the fairly high time cost is acceptable becauseof the high accuracy of the proposed method. Thus, the proposedapproach exhibits efﬁcient retrieval performance and it is robust to dif-ferences in illumination and occlusion.4. ConclusionIn this study, we developed a novel shape-based approach for imageretrieval. In contrast to the previously proposed CID descriptor and itsvariants, the proposed descriptor splits the image into foreground andbackground images using the Otsu approach, before computing the sa-liency and edge maps for the foreground and background images. Next,the salient edges computed using the saliency and edge maps areemployed to obtain the ACID by using the salient edgels in the fore-ground and background images in order to avoid missing concealed in-formation, which would inﬂuence the characterization of both theforeground and background images. Furthermore, the proposedapproach considers a higher amount of salient edgels (25%) for featurecomputation to capture more of the rich underlying texture, structure andspatial information than the ACID and CID descriptors. Therefore, theinformation extracted by the proposed approach is more accurate and itis more robust to changes in illumination and occlusion than ACID andCID. However, the computational cost of the proposed approach issigniﬁcantly higher than that of ACID, although this difference is negli-gible due to the higher retrieval accuracy. Experiments conducted basedonﬁve databases conﬁrmed that the proposed descriptor can efﬁciently characterize the shape details and obtain better retrieval resultscompared with the state-of-the-art techniques. In future research,weights can be assigned to the contour-based details determined in theforeground and background images during the feature matching phasebased on the saliency details identiﬁed.Credit author statementS.Sathiamoorthy: Conceptualization, methodology, writing - originaldraft, writing - review and editing, supervision. A.Saravanan: Formal analysis, software and investigation, data curation. R.Ponnusamy: Re- sources, software and investigation, data curation, validation.
Fig. 9.Few sample images of experimental databases.
Fig. 10.For instance, top 4 retrieval results for Gardens point walking, St.Luciaand self photographed image databases.
Fig. 11.Average precision Vs various P% of salient edgels.Table 3Computational complexity of proposed, ACID, CIDand SEH..
Methods Time in secondsSEH 47CID 56ACID 45Proposed 52S. Sathiamoorthy et al. Array 7 (2020) 100027
8Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.References
[1]Ma H, Zhu J, Lyu MRT, King I. Bridging the semantic gap between image contentsand tags. IEEE Trans Multimed 2010;12(5):462 –73. [2]Feng S, Xu D, Yang X. Attention-driven salient edge (s) and region (s) extractionwith application to CBIR. Signal Process 2010;90(1):1 –15. [3]Wang M, Ye Z-L, Wang Y, Wang S-X. Dominant sets clustering for image retrieval.Signal Process 2008;88(11):2843 –9. [4]Murala S, Maheshwari R, Balasubramanian R. Local tetra patterns: a new featuredescriptor for content-based image retrieval. IEEE Trans Image Process 2012;21(5):2874–86.[5]Long F, Zhang H, Feng DD. Fundamentals of content-based image retrieval. In:Multimedia information retrieval and management. Springer; 2003. p. 1 –26. [6]Zhang X, Zhao X, Li Z, Xia J, Jain R, Chao W. Social image tagging using graph-based reinforcement on multi-type interrelated objects. Signal Process 2013;93(8):2178–89.[7]Androutsos P, Kushki A, Plataniotis KN, Venetsanopoulos AN. Aggregation of colorand shape features for hybrid query generation in content based visual informationretrieval. Signal Process 2005;85(2):385 –93. [8]Subrahmanyam M, Maheshwari R, Balasubramanian R. Local maximum edge binarypatterns: a new descriptor for image retrieval and object tracking. Signal Process2012;92(6):1467–79.[9]Zhang J, Ye L. Local aggregation function learning based on support vectormachines. Signal Process 2009;89(11):2291 –5. [10]Qian Y, Hui R, Gao X. 3D CBIR with sparse coding for image-guided neurosurgery.Signal Process 2013;93(6):1673–83. [11]Hu MK. Visual pattern recognition by moment invariants. IRE Trans. Info. Theory1962;8:179–87.[12]Lu G, Sajjanhar A. Region based shape representation and similarity measuresuitable for content-based image retrieval. Multimed Syst 1999;7(2):165 –74. [13]Zhang D, Lu G. Shape-based image retrieval using generic Fourier descriptor. SignalProcess Image Commun 2002;17(10):825 –48. [14]Kim W-Y, Kim Y-S. A region based shape descriptor using Zernike moments. SignalProcess Image Commun 2000;16:95 –102. [15] Freeman, H., Saghri, A., November 7 –10, 1978. Generalized chain codes for planar curves, in: Proceedings of the fourth international joint conference on pattern recog.Kyoto, Japan, pp. 701–703.[16]Iivarinen J, Visa. A. Shape recognition of irregular objects. In: Casasent DP, editor.Intelligent robots and computer vision XV: algorithms, techniques, active vision,and materials handling. Proc; 1996. p. 25 –32. SPIE 2904. [17]Otterloo PJV. A contour-OrientedApproach to shape analysis. Englewood Cliffs, NJ:
Prentice-Hall International (UK) Ltd; 1991. p. 90 –108. [18]Zhang DS, Lu G, January 22–25. A comparative study of Fourier descriptors for shape representation and retrieval. In: Proceedings of the ﬁfth asian conference on computer vision. Melbourne, Australia: ACCV02); 2002. p. 646 –51. [19]Yong J, Bowie Walker, J. An analysis technique for biological shape. Comput GraphImage Process 1974;25:357–70. [20]Peura M, Iivarinen J, May. Efﬁciency of simple shape descriptors. In: Proceedings of the third inter. Workshop on visual form. Italy: Capri; 1997. p. 443 –51. [21]Huang C-L, Huang D-H. A content-based image retrieval system. Image Vis Comput1998;16:149–63.[22]Shu X, Wu X-J. A novel contour descriptor for 2D shape matching and itsapplication to image retrieval. Image Vis Comput 2011;29(4):286 –94. [23]Sonka M, Hlavac V, Boyle R. Image processing, analysis and machine vision.London, UK, NJ: Chapman&Hall; 1993. p. 193–242. [24]Gonzalez RC, Woods RE. Digital image processing. Reading, MA: Addison-Wesley;1992. p. 502–3.[25]Mokhtarian F, Abbasi F, Kittler J. Ef ﬁcient and robust retrieval by shape content through curvature scale space. Int. workshop on Image Databases and Multi-MediaSearch; 1997. p. 51–8.[26]Del Bimbo A, Pala P. Visual image retrieval by elastic matching of user sketches.IEEE Trans Pattern Anal Mach Intell 1997;19(2):121 –32.[27]Attalla E, Siy P. Robust shape similarity retrieval based on contour segmentationpolygonal multiresolution and elastic matching. Pattern Recoginit 2005;38:2229–41.[28]Squire DM, Caelli TM. Invariance signature: characterizing contours by theirdepartures from invariance. Comput Vis Image Understand 2000;77:284 –316. [29]Nguyen DT, Ogunbona PO, Li W. A novel shape-based non-redundant local binarypattern descriptor for object detection. Pattern Recogn May 2013;46(5):1485 –500. [30]Yao CH, Chen SY. Retrieval of translated, rotated and scaled color textures. PatternRecogn 2003;36(4):913–29. [31]Murala S, Wu QM. Peak valley edge patterns: a new descriptor for biomedical imageindexing and retrieval. In: IEEE conference on computer vision and patternrecognition workshops. CVPRW; 2013. p. 444 –9.[32]Murala S, Wu QJ. MRI and CT image indexing and retrieval using local mesh peakvalley edge patterns, Signal Process. Image Commun 2014;29(3):400 –9. [33]Jain AK, Vailaya A. Image retrieval using color and shape. Pattern Recogn 1996;29(8):1233–44.[34]Cieplinski L, Kim M, Ohm J-R, Pickering M, Yamada A, editors. Text of ISO/IEC15938-3/FCD information technology -multimedia content description interface-Part 3: visual. Final committee draft; 2001. ISO/IEC/JTC1/SC29/WG11 (MPEG),document no. N4062.[35]Chee SunWon DongKwonPark, Soo JunPark. Ef ﬁcientUseofMPEG7 edge histogram descriptor. ; 2002.[36]Luo J, Crandall D. Color object detection using spatial-color joint probabilityfunctions. IEEE Trans Image Process 2006;15(6):1443 –53. [37] Shim Seong-O, Choi Tae-Sun. Edge color histogram for image retrievalvol. 3.Rochester,NY, USA: Proc. Int.Conf.onImageProc.; 2002. p. 957 –60.https://doi.org/ 10.1109/ICIP.2002.1039133. [38]Chun YD, Seo SY, Kim NC. Image retrieval using BDIP and BVLC moments. IEEETrans Circ Syst Video Technol Sep. 2003;13:951 –7. [39]Chun YD, Kim NC, Jang IH. Content-based image retrieval using multiresolutioncolor and texture features. IEEE Trans Multimed 2008;10(6):1073 –84. [40]Sathiamoorthy S, Kamarasan M. A novel approach for image retrieval using BDIPand BVLC. Int. Journal of Innovative Research in Computer and CommunicationEngineering 2014;2(9):5897–902. [41]Mahmoudi F, Shanbehzadeh J, Eftekhari AM, Soltanian-Zadeh H. Image retrievalbased on shape similarity by edge orientation autocorrelogram. Pattern Recogn2003;36:1725–36.[42] Seetharaman K, Sathiamoorthy S. An improved edge direction histogram and edgeorientation autocorrlogram for an ef ﬁcient color image retrieval. In: 2013 international conference on advanced computing and comm. Systems. India:Coimbatore; 2013. p. 1–4.https://doi.org/10.1109/ICACCS.2013.6938725 . [43]Seetharaman K, Sathiamoorthy S. A uni ﬁed learning framework for content based medical image retrieval using a statistical model. Journal of King Saud University -Computer and Information Sciences 2016;28(1):110 –24. [44]Toshev B Taskar, K Daniilidis. Shape-based object detection via boundary structuresegmentation. Int J Comput Vis 2012;99(2):123 –46. [45]Kovalev S Volmer. Color co-occurrence descriptors for querying-by example. In:Multimedia modeling, 1998. MMM ’98. Proceedings. 1998. IEEE; 1998. p. 32 –8. [46]Zitnick CL. Binary coherent edge descriptors. In: European conference on computervision. Springer; 2010. p. 170–82.
[47]Wang Xiaolong, Zhang Hong, GuohuaPeng. A chordiogram image descriptor usinglocal edgels. J Vis Commun Image Represent 2017;49:129 –40. [48] Saravanan A, Sathiamoorthy S. Autocorrelation based chordiogram imagedescriptor for image retrieval. In: 4th international conference on communicationand electronics systems (ICCES 2019). Coimbatore, India: PPG Institute ofTechnology; 2019. p. 1990–6.https://doi.org/10.1109/ ICCES45898.2019.9002528. July 17–19. [49] Song Wei, Zhang Yubing, Liu Fei, Chai Zhilei, Ding Feng, Qian Xuezhong, Park SoonCheol. Taking advantage of multi-regions-based diagonal texture structuredescriptor for image retrieval. Expert Syst Appl 2017. https://doi.org/10.1016/ j.eswa.2017.12.006.[50] Feng S, et al. Attention-driven salient edge(s) and region(s) extraction withapplication to CBIR. Signal Process 2009. https://doi.org/10.1016/ j.sigpro.2009.05.017.[51]Sharma Pooja. Improved shape matching and retrieval using robust histograms ofspatially distributed points and angular radial transform. Optik 2017;145:346 –64. [52]Liu Guang-Hai, Yang Jing-Yu, et al. Content-based image retrieval usingcomputational visual attention model. Pattern Recogn 2015;48(8):2554 –66.S. Sathiamoorthy et al. Array 7 (2020) 100027
9