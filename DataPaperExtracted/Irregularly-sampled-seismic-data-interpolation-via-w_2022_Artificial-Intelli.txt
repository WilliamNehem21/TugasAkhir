Artificial Intelligence in Geosciences 3 (2022) 192–202
Available online 31 December 2022
2666-5441/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC
BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available at ScienceDirect
Artificial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artificial-intelligence-in-geosciences
Original research articles
Irregularly sampled seismic data interpolation via wavelet-based
convolutional block attention deep learning
Yihuai Loua,b, Lukun Wuc, Lin Liud, Kai Yue, Naihao Liuf,∗, Zhiguo Wangg,∗, Wei Wangh
aCenter for Hypergravity Experimental and Interdisciplinary Research, Zhejiang University, Hangzhou, Zhejiang 310058, China
bMOE Key Laboratory of Soft Soils and Geoenvironmental Engineering, College of Civil Engineering and Architecture, Zhejiang University, Hangzhou,
Zhejiang 310058, China
cSchool of Software Engineering, Xi’an Jiaotong University, Xi’an, Shaanxi 710049, China
dPetrochina CBM Company Limited, Beijing 100028, China
eCapiticalonline Data Service Co., LTD, Beijing 100012, China
fSchool of Information and Communications Engineering, Xi’an Jiaotong University, Xi’an, Shaanxi 710049, China
gSchool of Mathematics and Statistics, Xi’an Jiaotong University, Xi’an, Shaanxi 710049, China
hPowerChina Huadong Engineering Corporation Limited, Hangzhou, Zhejiang 310014, China
A R T I C L E I N F O
Keywords:
Irregularly sampled seismic data reconstruction
Deep learning
U-Net
Discrete wavelet transform
Convolutional block attention moduleA B S T R A C T
Seismic data interpolation, especially irregularly sampled data interpolation, is a critical task for seismic
processing and subsequent interpretation. Recently, with the development of machine learning and deep
learning, convolutional neural networks (CNNs) are applied for interpolating irregularly sampled seismic data.
CNN based approaches can address the apparent defects of traditional interpolation methods, such as the low
computational efficiency and the difficulty on parameters selection. However, current CNN based methods
only consider the temporal and spatial features of irregularly sampled seismic data, which fail to consider the
frequency features of seismic data, i.e., the multi-scale features. To overcome these drawbacks, we propose
a wavelet-based convolutional block attention deep learning (W-CBADL) network for irregularly sampled
seismic data reconstruction. We firstly introduce the discrete wavelet transform (DWT) and the inverse wavelet
transform (IWT) to the commonly used U-Net by considering the multi-scale features of irregularly sampled
seismic data. Moreover, we propose to adopt the convolutional block attention module (CBAM) to precisely
restore sampled seismic traces, which could apply the attention to both channel and spatial dimensions. Finally,
we adopt the proposed W-CBADL model to synthetic and pre-stack field data to evaluate its validity and
effectiveness. The results demonstrate that the proposed W-CBADL model could reconstruct irregularly sampled
seismic data more effectively and more efficiently than the state-of-the-art contrastive CNN based models.
1. Introduction
Seismic data reconstruction, which is a tough task, plays an impor-
tant role in seismic exploration ( Chai et al. , 2020 ; Liu et al. , 2022b ;
Bai et al. , 2018 ). Seismic data are often sampled irregularly along the
spatial direction due to the constraints on the acquisition condition, the
cost limitations, the dead traces, etc ( Wang et al. , 2020 ). Irregularly
sampled data affects the subsequent seismic processing and interpre-
tation, such as incoherent and coherent noise attenuation ( Yuan et al. ,
2015 ; Dong et al. , 2020 ; Birnie et al. , 2021 ; Liu et al. , 2021c ; Yang et al. ,
2021 ; Wu et al. , 2022c ), normal moveout (NMO) correction ( Zhang
et al. , 2013 ; Biswas et al. , 2019 ; Yuan et al. , 2019 ), seismic inver-
sion ( Gao et al. , 2016 ; Wu et al. , 2020 , 2022b ), seismic horizon and
∗Corresponding authors.
E-mail addresses: lou_yh2021@163.com (Y. Lou), hddsjvae@qq.com (L. Wu), apple1987@petrochina.com.cn (L. Liu), ykkmoon@163.com (K. Yu),
naihao_liu@mail.xjtu.edu.cn (N. Liu), emailwzg@mail.xjtu.edu.cn (Z. Wang), wang_w20@hdec.com (W. Wang).fault interpretation ( Wu et al. , 2019 ; Zhou et al. , 2020 ; Wu et al. ,
2022a ), wavefield solution ( Alkhalifah et al. , 2021 ; Huang and Alkhali-
fah, 2022 ), first arrival picking ( Xu et al. , 2021 ; Liu et al. , 2021a ), etc. It
should be noted that incomplete seismic data can be divided into two
typical cases, i.e., irregularly sampled seismic data and consecutively
sampled seismic data with big gap ( Liu et al. , 2022b ). Moreover, the
regularly sampled case is a special case of the irregularly sampled case.
Therefore, we only consider the case of irregularly sampled seismic data
in this study.
Plenty of methods have been proposed to address seismic data
reconstruction, which can be generally divided into traditional ap-
proaches and machine learning (ML) based approaches. Traditional
https://doi.org/10.1016/j.aiig.2022.12.001
Received 4 November 2022; Received in revised form 21 December 2022; Accepted 21 December 2022Artificial Intelligence in Geosciences 3 (2022) 192–202
193Y. Lou et al.
reconstruction approaches can be then divided into five categories. The
first category is wave equation based approaches, in which seismic
interpolation is regarded as an inverse problem (Ronen, 1987; Fomel,
2003). Note that this kind of methods requires the prior knowledge
of the velocity model. The second, i.e. prediction error filter (PEF)
based approaches (Crawley et al., 1999; Li et al., 2017), utilizes the
low-frequency components to predict the linear events at the high-
frequency. However, the linearity assumptions of seismic data are not
always satisfied, especially for field data. The third one is transform
based methods, which transform seismic data to a specific sparse
domain and then implement seismic interpolation based on the theory
of compressed sensing (Gülünay, 2003; Yu et al., 2007), such as Radon
transform (Thorson and Claerbout, 1985; Kabir and Verschuur, 1995),
curvelet transform (Herrmann and Hennenfent, 2008; Naghizadeh and
Sacchi, 2010), seislet transform (Chen et al., 2014; Gan et al., 2015),
dreamlet transform (Wu et al., 2013; Wang et al., 2015), etc. The fourth
is based on low-rank assumption of seismic data, in which sampled
seismic data can be interpolated by reducing the rank of the analyzed
seismic data (Sternfels et al., 2015; Huang and Liu, 2019). However,
this kind of methods is difficult to determine the optimal rank, which
affects the final interpolation result. The last category is projection-
onto-convex-sets (POCS) image reconstruction approaches (Abma and
Kabir, 2006; Gao et al., 2013), which are based on the Gerchberg–
Saxton iterative algorithm (Trad, 2003), nevertheless, this kind of
approaches suffer from the expensive computing cost. Although tradi-
tional approaches are proposed to address seismic data reconstruction,
they still suffer several apparent limitations. First, these methods often
require the prior assumptions, which is difficult to precisely obtain
when processing field data. The second limitation is the computational
efficiency, especially when facing the massive seismic data. The other
one is that users need to manually define plenty of parameters for
traditional interpolation methods, and these parameters usually are
required to precisely adjust. Certainly, it is a very time-consuming and
difficult task to interpolate seismic data at different seismic survey.
Different with theory-driven or model-driven traditional approaches
as discussed above, machine learning (ML) based approaches are usu-
ally data-driven, which can adaptively build a model for seismic data
reconstruction by learning the characteristics of seismic data itself.
With the development of ML based models, they are utilized to over-
come the limitations of traditional interpolation approaches, such as the
pre-assumptions of the linear events, the sparsity, the low rank, etc (Jia
and Ma, 2017). In addition, ML based approaches often introduce a
trained regression function to guide seismic data interpolation without
the manual parameter tuning, which is accurate and computationally
efficient. Deep learning (DL), as a branch of ML, has been introduced
into seismic exploration, such as seismic fault interpretation (Liu et al.,
2020; Li et al., 2022), impedance inversion (Wu et al., 2021; Chen
et al., 2021), lithology identification (Lin et al., 2020; Liu et al., 2021b),
seismic facies analysis (Zhang et al., 2019; Li et al., 2020), noise
attenuation (Dong et al., 2022; Liu et al., 2022a), etc. In recent years,
DL based methods have been used for addressing irregularly sampled
seismic data reconstruction, the most classic of which is convolutional
neural networks (CNNs). For example, Mandelli et al. (2018) used
convolutional autoencoders to solve irregularly sampled seismic data
reconstruction, first introducing CNNs into deep learning-based seismic
interpolation. Park et al. (2019) presented a U-net model trained with
common shot gathers for regularly sampled seismic data reconstruc-
tion. Wang et al. (2019) designed an eight-layer residual learning
network (ResNet) with better deep back-propagation characteristics for
interpolating sampled seismic data. Yoon et al. (2020) proposed to
reconstruct sampled seismic traces using the recurrent neural network
(RNN). Wei et al. (2021) proposed a cGAN model based on the Pix2Pix
GAN to interpolate irregularly sampled seismic data.
However, current CNN models only focus on the temporal informa-
tion of seismic data, failing to consider seismic features in the frequency
domain, i.e., the multi-scale features of seismic data. Therefore, theperformance of CNN models could be further improved, especially
for reconstructing seismic data with detailed features. The multi-layer
wavelet convolutional neural network (MWCNN) has been proposed to
address the above issues (Liu et al., 2018). MWCNN adopts discrete
wavelet transform (DWT) and inverse discrete wavelet transform (IWT)
in U-Net to avoid the information loss caused by the pooling operations.
It should be noted that MWCNN uses U-Net as the backbone, which
introduces a large number of feature channels in the upsampling part
and allows the network to propagate the context information to the
high resolution layers (Liu et al., 2022b).
In this study, to effectively utilize the information extracted from
seismic data in both time and frequency domains, we propose a
wavelet-based convolutional block attention deep learning network
(W-CBADL) for irregularly sampled seismic data reconstruction. W-
CBADL takes MWCNN as the basic architecture. As pointed previously,
compared with U-Net, MWCNN uses DWT to replace the pooling
operations in the contracting subnetwork. In CNN-based models, the
pooling operation is usually introduced to expand the receptive field,
which may destroy the detailed characteristics of seismic reflections,
which in turn is not conducive to accurate reconstruction of seismic
data (Liu et al., 2022b). By using the downsampling scheme based
on the invertibility of DWT when utilizing the multi-scale feature
representation, the multi-scale features in both time and frequency
domains are extracted, which is conducive to the preservation of de-
tailed features (Liu et al., 2018, 2022b). In the expanding subnetwork,
inverse wavelet transform (IWT) is utilized for upsampling the low
resolution feature maps to the high resolution ones (Liu et al., 2018).
Moreover, the element-wise summation module is adopted to combine
the feature maps of the contracting and the expanding subnetworks,
which can enrich the multi-scale feature representation and reduce
computational burden. Notably, our W-CBADL model introduces a con-
volutional block attention module (CBAM), which is a state-of-the-art
attention mechanism. Traditional attention modules only pay attention
to which layers in the channel dimension will have stronger feedback
capabilities, but does not reflect the meaning of attention in the spatial
dimension. CBAM applies attention to both the channel dimension and
the spatial dimension (Woo et al., 2018). On one hand, CBAM facilitates
each branch to learn ‘‘what" and ‘‘where" in the channel and spatial
dimensions. On the other hand, the multi-scale features at the time
and frequency domains can be precisely represented in the channel and
space dimensions. Therefore, CBAM can learn to emphasize or suppress
the features at the time and frequency domains obtained by DWT in the
channel and spatial dimensions, which is beneficial for obtaining the
multi-scale information. Obviously, this is important for reconstructing
sampled seismic data. In the following sections, we firstly introduce
DWT, IWT, MWCNN, and CBAM in detail. Next, we introduce the
detailed architecture of our W-CBADL model. Afterward, we implement
the experiments on a synthetic data set and a field data set. Comparing
W-CBADL with U-Net and MWCNN in the case of irregularly sampled
seismic data, we present their qualitative and quantitative results and
the detailed analysis. Finally, we provide the discussions and main
conclusions of this study.
2. Methodology
2.1. Discrete wavelet transform and inverse discrete wavelet transform
Discrete wavelet transform (DWT) is formulated in the late 1980s
(Daubechies, 1988). By taking haar wavelet as an example, an image
𝐱can be decomposed into four sub-images using DWT, which are low-
pass image 𝐱𝐿𝐿(average) and three high-pass images, including 𝐱HL
(horizontal), 𝐱LH(vertical), and 𝐱HH(diagonal). These four decomposedArtificial Intelligence in Geosciences 3 (2022) 192–202
194Y. Lou et al.
sub-images are defined as
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪⎩𝐱𝐿𝐿(𝑖,𝑗) =𝐱(2𝑖− 1,2𝑗− 1) + 𝐱(2𝑖− 1,2𝑗)
+𝐱(2𝑖,2𝑗− 1) + 𝐱(2𝑖,2𝑗),
𝐱𝐿𝐻(𝑖,𝑗) = − 𝐱(2𝑖− 1,2𝑗− 1) − 𝐱(2𝑖− 1,2𝑗)
+𝐱(2𝑖,2𝑗− 1) + 𝐱(2𝑖,2𝑗),
𝐱𝐻𝐿(𝑖,𝑗) = − 𝐱(2𝑖− 1,2𝑗− 1) + 𝐱(2𝑖− 1,2𝑗)
−𝐱(2𝑖,2𝑗− 1) + 𝐱(2𝑖,2𝑗),
𝐱𝐻𝐻(𝑖,𝑗) =𝐱(2𝑖− 1,2𝑗− 1) − 𝐱(2𝑖− 1,2𝑗)
−𝐱(2𝑖,2𝑗− 1) + 𝐱(2𝑖,2𝑗),(1)
where 𝐱𝐿𝐿(𝑖,𝑗),𝐱𝐿𝐻(𝑖,𝑗),𝐱𝐻𝐿(𝑖,𝑗), and 𝐱𝐻𝐻(𝑖,𝑗)denote the pixels at the
𝑗th column and the 𝑖th row of 𝐱𝐿𝐿,𝐱𝐿𝐻,𝐱𝐻𝐿, and 𝐱𝐻𝐻, respectively.
Obviously, DWT in Eq. (1) can be considered as a convolution
operation between 𝐱and four 2 ×2 filters in strides of 2, described
as
𝐟𝐿𝐿=[1 1
1 1]
,𝐟𝐿𝐻=[−1 −1
1 1]
,
𝐟𝐻𝐿=[−1 1
−1 1]
,𝐟𝐻𝐻=[1 −1
−1 1]
.(2)
Notice that the pixel-size of the four sub-images generated by using
DWT is reduced to half of the original image, which can replace the
pooling operation (Ronneberger et al., 2015).
Due to the orthogonality of the four filters defined in Eq. (1), we can
fully restore the target image by using inverse wavelet transform (IWT).
Similarly, IWT can be expressed as an inverse convolution operation,
defined as
⎧
⎪
⎪
⎪
⎪
⎪
⎨
⎪
⎪
⎪
⎪
⎪⎩𝐱(2𝑖− 1,2𝑗− 1) =(𝐱𝐿𝐿(𝑖,𝑗) −𝐱𝐿𝐻(𝑖,𝑗)
−𝐱𝐻𝐿(𝑖,𝑗) +𝐱𝐻𝐻(𝑖,𝑗))∕4,
𝐱(2𝑖− 1,2𝑗) =(𝐱𝐿𝐿(𝑖,𝑗) −𝐱𝐿𝐻(𝑖,𝑗)
+𝐱𝐻𝐿(𝑖,𝑗) −𝐱𝐻𝐻(𝑖,𝑗))∕4,
𝐱(2𝑖− 1,2𝑗) =(𝐱𝐿𝐿(𝑖,𝑗) +𝐱𝐿𝐻(𝑖,𝑗)
−𝐱𝐻𝐿(𝑖,𝑗) −𝐱𝐻𝐻(𝑖,𝑗))∕4,
𝐱(2𝑖,2𝑗) =(𝐱𝐿𝐿(𝑖,𝑗) +𝐱𝐿𝐻(𝑖,𝑗)
+𝐱𝐻𝐿(𝑖,𝑗) +𝐱𝐻𝐻(𝑖,𝑗))∕4.(3)
2.2. Multi-level wavelet-CNN model
DWT has apparently adaptive spatial frequency resolution, which
achieves better spatial resolution at high frequency and better fre-
quency resolution at low frequency (Singh et al., 2011). Moreover, IWT
with the orthogonality has been demonstrate to accurately reconstruct
the input image. Therefore, we introduce DWT and IWT to preserve the
feature maps of the convolutional layers, which can promote the ability
and accuracy of seismic data reconstruction.
Liu et al. (2022b) proposed a MWCNN model based on DWT and
IWT, which combines wavelet transform and CNNs. As an improved
U-Net structure, MWCNN introduces wavelet transform to reduce the
size of feature maps in the contracting subnetwork. Furthermore, an-
other convolutional layer is further used to decrease the channels of
feature maps, i.e., the expanding subnetwork. Here, IWT is adopted to
reconstruct the high resolution feature maps. CNNs typically expand the
receptive field at the expense of the computational cost. Although the
dilated filtering solves the problem of the high computational cost of
CNNs, it suffers from the grid effects and the computed receptive field
is only a sparse sampling of the input image with the checkerboard
patterns. MWCNN is able to achieve the sizeable receptive fields with
the limited computational constraints. Moreover, MWCNN can also
be interpreted as a generalization of the dilation filtering and the
subsampling, which is beneficial for image restoration tasks.2.3. Convolutional block attention module
Woo et al. (2018) proposed the Convolutional Block Attention
Module (CBAM), which is proved to be a simple yet effective attention
module for the feed-forward CNNs. CBAM is proposed to improve
the 3D feature maps via model training with channel attention and
spatial attention (Wang et al., 2021b). CBAM contains two separate
sub-modules, which are channel attention module (CAM) and spatial
attention module (SAM). Given an intermediate FM 𝐅∈R𝐶×𝐻×𝑊as
the input, CBAM sequentially infers a 1D channel attention map 𝐌𝐜∈
R𝐶×1×1and a 2D spatial attention map 𝐌𝐬∈R1×𝐻×𝑊, as illustrated in
Fig. 1. Thus, the channel-refined FM and the final FM are computed as
𝐅′=𝐌𝐜(𝐅)⊗𝐅,
𝐅′′=𝐌𝐬(𝐅′)⊗𝐅′,(4)
where⊗denotes the element-wise multiplication. 𝐅′and𝐅′′present
the channel-refined FM and the final refined FM (Woo et al., 2018). The
values are then broadcasted (copied) if the two operands are not with
the same dimension, i.e., the spatial attentional values are broadcasted
along the channel dimension and the channel attention values are
broadcasted along the spatial dimension (Wang et al., 2021a).
2.3.1. Channel attention module
To explain CAM, we first apply the average pooling 𝑓𝑎𝑝and the max
pooling𝑓𝑚𝑝. Then, two features 𝐃apand𝐃mpare computed as
{
𝐃𝐚𝐩=𝑓𝑎𝑝(𝐅),
𝐃𝐦𝐩=𝑓𝑚𝑝(𝐅).(5)
Both 𝐃apand𝐃mpare sent to a shared multi-layer perceptron (MLP)
to produce the output feature maps, which are then merged using the
element-wise summation⨁. Typically, MLP consists of three layers in
Fig. 2(a), including an input layer, a hidden layer, and an output layer.
The merged sum is finally sent to the sigmoid function 𝜎, defined as
𝐌c(𝐅) =𝜎{𝑀𝐿𝑃[𝐃𝐚𝐩]⊕𝑀𝐿𝑃[𝐃𝐦𝐩]}. (6)
To reduce the parameters and simplify the computation, the number
of hidden neurons in MLP is set as R𝐶∕𝑟×1×1, where𝑟is the reduction
ratio. Next, 𝐖𝟎∈R𝐶∕𝑟×𝐶and𝐖𝟏∈R𝐶×𝐶∕𝑟are adopted to represent
MLP weights. Eq. (6) can be further improved and rewritten as
𝐌𝐜(𝐅) =𝜎{𝐖𝟏[𝐖𝟎(𝐃ap)]⊕𝐖𝟏[𝐖𝟎(𝐃𝐦𝐩)]}. (7)
Note that 𝐖𝟎and𝐖𝟏are shared by both 𝐃apand𝐃mp.
2.3.2. Spatial attention module
Fig. 2(b) indicates the simplified architecture of SAM used in this
study. Unlike the channel attention, which focuses on ‘‘what", the
spatial attention focuses on ‘‘where" is the information part, which
is complementary to the channel attention (Woo et al., 2018). Here,
the average pooling 𝑓𝑎𝑝and the max pooling 𝑓𝑚𝑝are applied to the
channel-refined FM 𝐅′. Next, we have
{
𝐄𝐚𝐩=𝑓𝑎𝑝(𝐅′),
𝐄𝐦𝐩=𝑓𝑚𝑝(𝐅′),(8)
where 𝐄apand𝐄mpare both two dimensional feature maps and satisfy
𝐄𝐚𝐩∈R1×𝐻×𝑊∧𝐄𝐦𝐩∈R1×𝐻×𝑊.𝐄apand𝐄mpare concatenated together
along the channel dimension, i.e., 𝐄=𝑐𝑜𝑛𝑐𝑎𝑡(𝐄ap,𝐄𝐦𝐩). Then, the
connected activation map is subjected to a standard 7 ×7 convolution
operation followed by a sigmoid function 𝜎. Afterward, we obtain
𝐌s(𝐅′) =𝜎{𝑐𝑜𝑛𝑣[𝐄]}. (9)
The output 𝐌s(𝐅′)is then element-wisely multiplied with 𝐅′, as defined
in Eq. (4)Artificial Intelligence in Geosciences 3 (2022) 192–202
195Y. Lou et al.
Fig. 1. CBAM and its two sub-modules, i.e., CAM and SAM.
Fig. 2. The simplified architecture of CAM and SAM.
2.4. W-CBADL network
Based on the modules mentioned in the previous sub-sections, we
propose a wavelet-based convolutional block attention deep learning
(W-CBADL) model for irregularly sampled seismic data reconstruction.
Fig. 3 and Table 1 show the simplified architecture and the detailed op-
erations of W-CBADL. The proposed W-CBADL model mainly consists of
the encoder, the decoder, and the connection layer (the Add operations
in Table 1), which is also a typical U-Net structure. We explain the main
parts of the W-CBADL model as follows.
(1) W-CBADL adopts the DWT and IWT to replace the pooling and
general upsampling operations in traditional CNN models. In
term of the orthogonality of the wavelet transform, the DWT and
IWT can retain more seismic data feature information, which
is beneficial for network training and promoting the results of
irregularly sampled seismic data reconstruction.
(2) W-CBADL introduces the convolution block attention module
(CBAM). Note that CBAM focuses on distinguishing which layers
have stronger feedback capabilities at the channel and spatialdimensions. CBAM re-calibrates the feature maps by learning
a set of weights. The channel attention and spatial attention
are also applied to the multi-scale seismic data information
obtained by using the DWT and IWT. Thus, the accuracy and
effectiveness of irregularly missing seismic data reconstruction
can be effectively promoted.
(3) The W-CBADL model adds the layer information of the corre-
sponding coding layer via the Add operation after each IWT,
which can strengthen the feature information. Moreover, its
computational cost is lower than the conventional concatenation
operation.
3. Synthetic examples
To examine the availability of W-CBADL, we first apply it on syn-
thetic data and implement a case, i.e., irregularly sampled seismic data,
which randomly exclude 70% of traces in each patch. Furthermore, we
provide qualitative and quantitative comparisons and explanations with
state-of-the-art U-Net and MWCNN models.Artificial Intelligence in Geosciences 3 (2022) 192–202
196Y. Lou et al.
Fig. 3. The simplified architecture of the proposed W-CBADL model. The ‘‘Conv Block" operation contains two Conv 3 ×3, Batch Norm, ReLU, refer to the Table 1 for details.
Table 1
The detailed operations and hyper-parameters of the proposed W-DBADL model.
Layer name Operation Input size Output size
CB 1 Conv Block (128, 128, 1) (128, 128, 32)
CM 1 CBAM (128, 128, 32) (128, 128, 32)
DWT 1 DWT (128, 128, 32) (64, 64, 128)
CB 2 Conv Block (64, 64, 128) (64, 64, 64)
CM 2 CBAM (64, 64, 64) (64, 64, 64)
DWT 2 DWT (64, 64, 64) (32, 32, 256)
CB 3 Conv Block (32, 32, 256) (32, 32, 128)
CM 3 CBAM (32, 32, 128) (32, 32, 128)
DWT 3 DWT (32, 32, 128) (16, 16, 512)
CB 4 Conv Block (16, 16, 512) (16, 16, 256)
CM 4 CBAM (16, 16, 256) (16, 16, 256)
DWT 4 DWT (16, 16, 256) (8, 8, 1024)
CB 5 Conv Block (8, 8, 1024) (8, 8, 512)
CM 5 CBAM (8, 8, 512) (8, 8, 512)
CBR 5 Conv 3 ×3, Batch Norm, ReLU (8, 8, 512) (8, 8, 1024)
IWT 4 IWT (8, 8, 1024) (16, 16, 256)
ADD 4 Add (CM 4, IWT 4) (16, 16, 256)
(16, 16, 256)(16, 16, 256)
DCB 4 Conv Block (16, 16, 256) (16, 16, 256)
DCM 4 CBAM (16, 16, 256) (16, 16, 256)
CBR 4 Conv 3 ×3, Batch Norm, ReLU (16, 16, 256) (16, 16, 512)
IWT 3 IWT (16, 16, 512) (32, 32, 128)
ADD 3 Add (CM 3, IWT 3) (32, 32, 128)
(32, 32, 128)(32, 32, 128)
DCB 3 Conv Block (32, 32, 128) (32, 32, 128)
DCM 3 CBAM (32, 32, 128) (32, 32, 128)
CBR 3 Conv 3 ×3, Batch Norm, ReLU (32, 32, 128) (32, 32, 256)
IWT 2 IWT (32, 32, 256) (64, 64, 64)
ADD 2 Add (CM 2, IWT 2) (64, 64, 64)
(64, 64, 64)(64, 64, 64)
DCB 2 Conv Block (64, 64, 64) (64, 64, 64)
DCM 2 CBAM (64, 64, 64) (64, 64, 64)
CBR 2 Conv 3 ×3, Batch Norm, ReLU (64, 64, 64) (64, 64, 128)
IWT 1 IWT (64, 64, 128) (128, 128, 32)
ADD 1 Add (CM 1, IWT 1) (128, 128, 32)
(128, 128, 32)(128, 128, 32)
DCB 1 Conv Block (128, 128, 32) (128, 128, 32)
DCM 1 CBAM (128, 128, 32) (128, 128, 32)
OUT Conv 1 ×1, Sigmoid (128, 128, 32) (128, 128, 1)
3.1. Synthetic data set
The synthetic data set used in this study is SEG C3 data set.1The
time sampling number and interval are 625 and 8 ms, while the spatial
interval is 20 m. We randomly select 8000 patches of 128 ×128 from
SEG C3 data set and then all extracted patches are normalized as [0,
1https://wiki.seg.org/wiki/SEG_C3_NA1] by using the Min–Max normalization, which can be expressed as
𝑦=𝑦𝑐− min(𝑦𝑐)
max(𝑦𝑐) − min(𝑦𝑐), (10)
where𝑦𝑐is complete data before normalization, and 𝑦is the normalized
data.. Next, we divide these 8000 patches into 50% as training set
(4000 patches), 25% as validation set (2000 patches), and remaining
25% as blind test set (2000 patches). Fig. 4 shows several examples of
synthetic data.
3.2. Model training
DL models are all built with Keras and Tensorflow deep learning
library on Python 3.6. Specifically, both Keras and Tensorflow are
the version of 2.4.0. All computations are implemented on a graphics
processing unit, i.e., NVIDIA GTX 3090 (24 GB GPU memory). These
models are all trained with a batch size of 40 and a maximum of 500
epochs to make a trade-off between training efficiency and convergence
rate. The commonly used Adam optimizer is selected as optimization
algorithm to minimize the loss function. The learning rate is initially
set as 0.01. In addition, the activation function and the loss function
are set as the Rectified Linear Unit (ReLU) ( Nair and Hinton , 2010 )
and the Mean Square Error (MSE) ( Allen , 1971 ).
After model training, the loss values are presented by the blue,
orange, and gray curves in Fig. 5. It should be noted that these loss
curves are drawn from the 20-th epoch to facilitate the display of
convergence and contrast effects. Obviously, the W-CBAL model and
the contrastive DL models both converge. Moreover, in Fig. 5, it can
be easily found that W-CBADL converges slowly in the early stage
of model training, but accelerates the convergence from the 120-th
epoch, and finally converges to a lower value than U-Net and MWCNN.
This indicates that, after model training, we obtain a more accurate
W-CBADL model than the other two contrastive DL models.
3.3. Evaluation matrix
We introduce several evaluation matrices to quantitatively evaluate
different DL models, including Peak Signal to Noise Ratio (PSNR)
(Huynh-Thu and Ghanbari , 2008 ), Structure Similarity Index Measure
(SSIM) ( Wang et al. , 2004 ), Mean Absolute Error (MAE) ( Chai and
Draxler , 2014 ), and Mean Absolute Percentage Error (MAPE) ( De Myt-
tenaere et al. , 2016 ). Consider that 𝐱𝐢and𝐲𝐢denote the 𝑖th predicted
result and corresponding label, while 𝑛represents the number of the
samples. These matrices are explained as follows.
Peak Signal to Noise Ratio (PSNR) :
𝑃𝑆𝑁𝑅 = 10 ⋅log10(
𝑀𝐴𝑋2
𝐼
𝑀𝑆𝐸)
, (11)Artificial Intelligence in Geosciences 3 (2022) 192–202
197Y. Lou et al.
Fig. 4. The examples of 70% irregularly sampled synthetic data. (a) The ground truth and (b) the incomplete synthetic data.
Fig. 5. The loss curves for different DL models. The gray circle, green star, and cyan diamond curves denote the training loss of U-Net, MWCNN, and W-CBADL; the black triangle,
yellow square, and blue pentagram curves denote the validation loss of U-Net, MWCNN, and W-CBADL.
where𝑀𝐴𝑋2
𝐼denotes the maximum pixel value of the image, 𝑀𝑆𝐸
is the mean square error (MSE) between the predicted result and the
label. The larger PSNR, the less distortion between the predicted result
and the ground truth.
Structure Similarity Index Measure (SSIM) : SSIM is a measure of
similarity between two images ( Wang et al. , 2004 ), presented as
𝑆𝑆𝐼𝑀 (𝐱,𝐲) =(2𝜇𝐱𝜇𝐲+𝑐1)(2𝜎𝐱𝐲+𝑐2)
(
𝜇2
𝐱+𝜇2
𝐲+𝑐1)(
𝜎2
𝐱+𝜎2
𝐲+𝑐2), (12)
where𝜇𝐱and𝜇𝐲present the averages of 𝐱and𝐲,𝜎2
𝐱and𝜎2
𝐲indicate
the variances of 𝐱and𝐲.𝜎𝐱,𝐲is the covariance of 𝐱and𝐲.𝑐1=(𝑘1𝐿)2
and𝑐2=(𝑘2𝐿)2are the constant values used to maintain the stability,
where𝑘1= 0.01and𝑘2= 0.03in this study. 𝐿is the dynamic range of
the pixel values. SSIM between two images is between 0 and 1, when
SSIM is closer to 1, the reconstructed image has less distortion.
Mean Absolute Error (MAE) :
𝑀𝐴𝐸 =1
𝑛𝑛∑
𝑖=1||𝐱𝐢−𝐲𝐢||. (13)When the predicted result is completely consistent with the label, MAE
turns to 0, which stands for obtaining a good model. And, the greater
the error, the greater the value.
Mean Absolute Percentage Error (MAPE) :
𝑀𝐴𝑃𝐸 =100%
𝑛𝑛∑
𝑖=1||||𝐱𝐢−𝐲𝐢
𝐲𝐢||||, (14)
Note that MAPE is actually a percentage. The smaller MAPE, the better
the model effect. It is generally believed that the prediction accuracy
is higher when MAPE is less than 10.
3.4. Synthetic data results
We apply the fine-tuned models to blind test data set. Fig. 6(a)
and 6(b) show ground truth and irregularly sampled synthetic data
randomly selected from blind test data set. Fig. 6(f) and 6(g) are
their𝑓−𝑘spectrum. Afterward, Fig. 6(c), 6(d), and 6(e) show the
reconstructed results computed using U-Net, MWCNN, and W-CBADL.
Meanwhile, the 𝑓−𝑘spectra of U-Net, MWCNN, and W-CBADL are
denoted in Fig. 6(h), 6(i), and 6(j). It should be noted that we nor-
malize these 𝑓−𝑘spectra to make the fair contrast. The imagesArtificial Intelligence in Geosciences 3 (2022) 192–202
198Y. Lou et al.
Fig. 6. The reconstructed results of irregularly sampled synthetic data based on different DL models. (a) Ground truth, (b) incomplete synthetic data, reconstructed data calculated
using (c) U-Net, (d) the MWCNN, (e) W-CBADL, (f)–(j) 𝑓−𝑘spectra, respectively.
reconstructed using different methods in Fig. 6 indicate that there
are several limitations existed in current DL models. First, although
U-Net can restore seismic valid events, a part of seismic events are
still missing. Especially, the pink circles in Fig. 6(c) and 6(h) indicate
that it is difficult to restore the irregularly sampled part with big gap.
Second, compared with U-Net, MWCNN obtains more complete inter-
polated results, but a part of seismic valid events are still losing. For
example, some weak reflections and sampled traces cannot be precisely
reconstructed, highlighted by the yellow and pink cursors in Fig. 6(d).
Moreover, the green cursors in Fig. 6(d) and 6(i) denote that there is
still an unreasonable relationship between the relative amplitudes of
the adjacent traces in the part restored by MWCNN. Finally, W-CBADL
achieves the most reasonable results compared to the contrastive DL
models, whose interpolated result is the closest to ground truth in
Fig. 6(a). Additionally, Fig. 7 show the difference images between the
reconstructed results in Fig. 6(c), 6(d), 6(e), 6(h), 6(i), 6(j) and the
ground truth in Fig. 6(a), 6(f), respectively. By comparing these images,
we have two main observations. First, the difference images of U-Net
and MWCNN show apparent visible differences in Fig. 7(a) and 7(b),
especially for the irregularly missing area. Obviously, the difference
images of their 𝑓−𝑘spectra are larger. This indicates that U-Net and
MWCNN fails to accurately reconstruct sampled data. Although the
difference image of the proposed model also shows seismic reflection
losses in Fig. 7(c) and 7(f), these losses are visibly less than those of
U-Net and MWCNN, benefiting from the time and spatial perception
property of CBAM. Second, the difference images of U-Net and MWCNN
show different mean value shifts, while the former larger than 0 and the
later smaller than 0. This means that these two models cannot maintain
seismic valid reflections when interpolating the missing reflections.
Nevertheless, note that there is not mean value shift in Fig. 7(c)
and 7(f), demonstrating the availability of W-CBADL for reconstructing
the sampled data and preserving the original seismic reflections.
To further verify the interpolation performance of W-CBADL, we
show 1D seismic examples of irregularly sampled synthetic data, which
are extracted from the first row in Fig. 6(a) and the trace number
is 102. In Fig. 8(b), we zoom in the presented traces by the red
rectangle in Fig. 8(a). It can be clearly observed that the restored
trace of W-CBADL, denoted by the blue diamond curve, is the closest
to ground truth presented by the red curve, which is superior to the
other two restored traces calculated by using the contrastive DL models.
Next, the aforementioned evaluation matrices are calculated by using
different DL models, shown in Table 2. It should be noted that the
higher SSIM and PSNR correspond to more suitable DL model, while
the lower MAE and MAPE are related with more accurate DL model.Table 2
Comparisons of different models on irregularly sampled synthetic data.
Model MAE SSIM PSNR MAPE
U-Net 1.1735e −02 0.9519 37.4345 2.2772
MWCNN 6.1965e −03 0.9711 40.7333 1.2464
W-CBADL 4.1270e −03 0.9778 43.0520 0.8337
The comparing result of the evaluation matrices in Table 2 indicates
that the proposed W-CBADL model achieves the best performance on
all evaluation matrices, which further verifies its effectiveness. After
the above analysis, we can conclude that W-CBADL is significantly
better than the comparative DL models in qualitative and quantitative
evaluations, which proves its superiority and availability for seismic
data reconstruction.
4. Field applications
We further adopt field data set to verify the effectiveness of the
proposed model and make detailed comparisons with state-of-the-art
DL models. For field data set, we randomly select 4000 patches from the
Mobil Avo Viking Graben Line 12 field data set,2each of which has a
size of 512 ×112, which are different with synthetic data set. Note that
the spatial sampling interval and the time sampling interval are 25 m
and 4 ms. Next, we divide the selected 4000 patches into 2000, 1000,
and 1000, i.e., 50% as training set, 25% as validation set, and 25%
as blind test set. The images in Fig. 9 indicate several 70% irregularly
sampled field data examples, which is randomly selected from training
data set. Afterward, we aim to adopt different DL models to reconstruct
these incomplete field data.
Fig. 10 and Table 3 denote the results acquired by applying dif-
ferent DL models on irregularly sampled field data for qualitative and
quantitative comparisons. The reconstruction results shown in Fig. 10
indicate that there are several limitations existed in current DL models,
and the reconstruction performance of our proposed W-CBADL model
is superior than U-Net and MWCNN. First, the red cursor and the circle
in Fig. 10(c) highlight that U-Net has difficulty on preserving seismic
valid events at the bottom and produces the significant distortion in
the missing part. Moreover, the pink cursor in Fig. 10(c) presents
that the restored part still has a significant amplitude loss. Second,
compared with U-Net, MWCNN can retain more seismic valid events,
2https://wiki.seg.org/wiki/Mobil_AVO_viking_graben_line_12Artificial Intelligence in Geosciences 3 (2022) 192–202
199Y. Lou et al.
Fig. 7. The difference images between the reconstructed results in Fig. 6(c), 6(d), 6(e), 6(h), 6(i), 6(j) and the ground truth in Fig. 6(a), 6(f), respectively.
Fig. 8. (a) The synthetic traces extracted from Fig. 6 with the trace number of 102 and (b) the enlarged part highlighted by the red rectangle in (a). The red solid, yellow circle,
cyan star, and blue diamond curves denote ground truth and restored traces computed using U-Net, MWCNN, and W-CBADL, respectively.
whereas there is still a significant gap with ground truth in Fig. 10(a).
It should be noted that the image in Fig. 10(d) also has the incomplete
restoration of seismic valid events, indicated by the pink cursor and
the circles. Third, seismic valid events restored by W-CBADL are more
continuous, more complete, and more reasonable than those restored
by U-Net and MWCNN. Furthermore, the 𝑓−𝑘spectra in Fig. 10 lead to
the similar conclusion. Apparently, the 𝑓−𝑘spectra of the reconstructed
results computed using the U-Net and MWCNN models show signifi-
cant amplitude errors, as represented by the pink circle in Fig. 10(h)
and 10(i). Whereas, W-CBADL has the smallest error compared with
the ground truth 𝑓−𝑘spectrum, i.e., Fig. 10(j) is almost identical to
Fig. 10(f). Furthermore, Fig. 11 denote the difference images betweenthe reconstructed results in Fig. 10(h), 10(i), 10(j) and the ground truth
in Fig. 10(f). The difference results of the proposed W-CBADL model are
obviously with less error than the difference results of the compared
DL model. In addition, compared with U-Net and MWCNN, W-CBADL
achieves higher SSIM and PSNR and lower MAE and MAPE, which
can be easily found in Table 3. Therefore, W-CBADL performs best
on all quantitative matrices, which proves its superiority. The above
descriptions show that W-CBADL is an effective model for restoring the
irregularly sampled seismic data.
Furthermore, We further zoom in the corresponding part denoted
by the blue rectangle in Fig. 10(b) and show the cropped image in
Fig. 12. The comparisons in Fig. 12 also indicate that the reconstructionArtificial Intelligence in Geosciences 3 (2022) 192–202
200Y. Lou et al.
Fig. 9. The examples of 70% irregularly sampled field data. (a) Ground truth and (b) incomplete field data.
Fig. 10. The reconstruction results of different DL models on the irregularly sampled field data. (a) Ground truth, (b) irregularly sampled field data, reconstructed data computed
using (c) U-Net, (d) MWCNN, (e) W-CBADL; (f)–(j) the corresponding 𝑓−𝑘spectra.
performance of our proposed W-CBADL is superior than U-Net and
MWCNN. First, the pink cursor and the circle in Fig. 12(c) represent
that, although U-Net can preserve seismic valid events, the preserved
traces are discontinuous and unreasonable. Second, the pink circle andthe yellow cursor in Fig. 12(d) denote that MWCNN restores more
seismic valid events than U-Net, but still has significant amplitude
losses and some weak reflection losses. Finally, seismic valid events
restored by W-CBADL are the closest to ground truth in Fig. 12(a) andArtificial Intelligence in Geosciences 3 (2022) 192–202
201Y. Lou et al.
Fig. 11. The difference images between the reconstructed results in Fig. 10(h), 10(i), 10(j) and the ground truth in Fig. 10(f).
Fig. 12. The zoomed reconstruction results denoted by the blue rectangle in Fig. 10(b). (a) Ground truth, (b) irregularly sampled field data, reconstructed data predicted using
(c) U-Net, (d) MWCNN, and (e) W-CBADL, respectively.
Table 3
Comparisons of different models on irregularly sampled field data.
Model MAE SSIM PSNR MAPE
U-Net 3.3364e −03 0.9734 43.3247 0.7060
MWCNN 3.0002e −03 0.9832 44.9421 0.6076
W-CBADL 2.2755e −03 0.9899 45.7441 0.4620
achieves the most reasonable result in Fig. 12(e). All of the above com-
pared results fully illustrate the versatility, superiority, and reliability
of W-CBADL on irregularly sampled seismic data reconstruction.
5. Conclusion
We propose and train a wavelet-based convolutional block attention
deep learning network (W-CBADL) to interpolate irregularly sampled
seismic data. First, W-CBADL combines the wavelet transform with tra-
ditional CNNs. On one hand, the multi-scale characteristic of DWT and
IWT boosts the accuracy of W-CBADL for seismic data reconstruction.
On the other hand, W-CBADL can restore the target image nonde-
structively by utilizing the orthogonality of wavelet transform. Next,
W-CBADL further introduces a convolutional block attention module
(CBAM). We utilize the CBAM to distinguish which feature maps have
stronger feedback capabilities in both channel and spatial dimensions.
Then, we prepare training data sets which are consisted of synthetic
and field seismic data. Finally, the comparison results demonstrate
that W-CBADL outperforms U-Net and MWCNN both quantitatively
and qualitatively. Moreover, we verify the feasibility, effectiveness, and
superiority of the proposed W-CBADL on irregularly sampled seismic
data reconstruction, especially for irregularly sampled parts with big
gaps and weak reflections.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.Acknowledgments
This research was supported by the National Natural Science Foun-
dation of China under Grant 42274144 and under Grant 41974137.
The authors express their gratitude to the Sandia National Laboratory
and Mobil Oil Company for the open data. The authors deeply appre-
ciate the valuable comments from Associate Editor and another three
anonymous reviewers.
References
Abma, R., Kabir, N., 2006. 3D interpolation of irregular data with a POCS algorithm.
Geophysics 71 (6), E91–E97.
Alkhalifah, T., Song, C., bin Waheed, U., Hao, Q., 2021. Wavefield solutions from
machine learned functions constrained by the Helmholtz equation. Artif. Intell.
Geosci. 2, 11–19.
Allen, D.M., 1971. Mean square error of prediction as a criterion for selecting variables.
Technometrics 13 (3), 469–475.
Bai, X., Chen, J., Yuan, S., Wei, S., Wang, X., Wang, J., 2018. Study on seismic data
acquisition and processing technology of shale gas of complex mountainous in
South China —Taking the analysis of Gui-Guang’s 2D seismic exploration as an
example. Unconventional Oil Gas 5 (02), 1–8.
Birnie, C., Ravasi, M., Liu, S., Alkhalifah, T., 2021. The potential of self-supervised
networks for random noise suppression in seismic data. Artif. Intell. Geosci. 2,
47–59.
Biswas, R., Vassiliou, A., Stromberg, R., Sen, M.K., 2019. Estimating normal moveout
velocity using the recurrent neural network. Interpretation 7 (4), T819–T827.
Chai, T., Draxler, R.R., 2014. Root mean square error (RMSE) or mean absolute error
(MAE)?–Arguments against avoiding RMSE in the literature. Geosci. Model Dev. 7
(3), 1247–1250.
Chai, X., Tang, G., Wang, S., Lin, K., Peng, R., 2020. Deep learning for irregularly and
regularly missing 3-D data reconstruction. IEEE Trans. Geosci. Remote Sens. 59 (7),
6244–6265.
Chen, Y., Chen, K., Shi, P., Wang, Y., 2014. Irregular seismic data reconstruction using
a percentile-half-thresholding algorithm. J. Geophys. Eng. 11 (6), 065001.
Chen, H., Gao, J., Zhang, W., Yang, P., 2021. Seismic acoustic impedance inversion via
optimization-inspired semisupervised deep learning. IEEE Trans. Geosci. Remote
Sens. 60, 1–11.
Crawley, S., Clapp, R., Claerbout, J., 1999. Interpolation with smoothly nonstationary
prediction-error filters. In: SEG Technical Program Expanded Abstracts 1999.
Society of Exploration Geophysicists, pp. 1154–1157.
Daubechies, I., 1988. Orthonormal bases of compactly supported wavelets. Comm. Pure
Appl. Math. 41 (7), 909–996.Artificial Intelligence in Geosciences 3 (2022) 192–202
202Y. Lou et al.
De Myttenaere, A., Golden, B., Le Grand, B., Rossi, F., 2016. Mean absolute percentage
error for regression models. Neurocomputing 192, 38–48.
Dong, X., Lin, J., Lu, S., Huang, X., Wang, H., Li, Y., 2022. Seismic shot gather denoising
by using a supervised-deep-learning method with weak dependence on real noise
data: A solution to the lack of real noise data. Surv. Geophys. 1–32.
Dong, X., Zhong, T., Li, Y., 2020. New suppression technology for low-frequency noise
in desert region: The improved robust principal component analysis based on
prediction of neural network. IEEE Trans. Geosci. Remote Sens. 58 (7), 4680–4690.
Fomel, S., 2003. Seismic reflection data interpolation with differential offset and shot
continuation. Geophysics 68 (2), 733–744.
Gan, S., Wang, S., Chen, Y., Zhang, Y., Jin, Z., 2015. Dealiased seismic data interpo-
lation using seislet transform with low-frequency constraint. IEEE Geosci. Remote
Sens. Lett. 12 (10), 2150–2154.
Gao, Z., Pan, Z., Gao, J., 2016. Multimutation differential evolution algorithm and
its application to seismic inversion. IEEE Trans. Geosci. Remote Sens. 54 (6),
3626–3636.
Gao, J., Stanton, A., Naghizadeh, M., Sacchi, M.D., Chen, X., 2013. Convergence
improvement and noise attenuation considerations for beyond alias projection onto
convex sets reconstruction. Geophys. Prospect. 61, 138–151.
Gülünay, N., 2003. Seismic trace interpolation in the Fourier transform domain.
Geophysics 68 (1), 355–369.
Herrmann, F.J., Hennenfent, G., 2008. Non-parametric seismic data recovery with
curvelet frames. Geophys. J. Int. 173 (1), 233–248.
Huang, X., Alkhalifah, T., 2022. PINNup: Robust neural network wavefield solutions
using frequency upscaling and neuron splitting. J. Geophys. Res.: Solid Earth 127
(6), e2021JB023703.
Huang, W., Liu, J., 2019. Robust seismic image interpolation with mathematical
morphological constraint. IEEE Trans. Image Process. 29, 819–829.
Huynh-Thu, Q., Ghanbari, M., 2008. Scope of validity of PSNR in image/video quality
assessment. Electron. Lett. 44 (13), 800–801.
Jia, Y., Ma, J., 2017. What can machine learning do for seismic data processing? An
interpolation application. Geophysics 82 (3), V163–V177.
Kabir, M.N., Verschuur, D., 1995. Restoration of missing offsets by parabolic radon
transform 1. Geophys. Prospect. 43 (3), 347–368.
Li, C., Liu, G., Hao, Z., Zu, S., Mi, F., Chen, X., 2017. Multidimensional seismic data
reconstruction using frequency-domain adaptive prediction-error filter. IEEE Trans.
Geosci. Remote Sens. 56 (4), 2328–2336.
Li, S., Liu, N., Li, F., Gao, J., Ding, J., 2022. Automatic fault delineation in 3-D seismic
images with deep learning: Data augmentation or ensemble learning? IEEE Trans.
Geosci. Remote Sens. 60, 1–14.
Li, F., Zhou, H., Wang, Z., Wu, X., 2020. ADDCNN: An attention-based deep di-
lated convolutional neural network for seismic facies analysis with interpretable
spatial–spectral maps. IEEE Trans. Geosci. Remote Sens. 59 (2), 1733–1744.
Lin, J., Li, H., Liu, N., Gao, J., Li, Z., 2020. Automatic lithology identification by
applying LSTM to logging data: A case study in X tight rock reservoirs. IEEE Geosci.
Remote Sens. Lett. 18 (8), 1361–1365.
Liu, N., Chen, J., Wu, H., Li, F., Gao, J., 2021a. Microseismic first-arrival picking using
fine-tuning feature pyramid networks. IEEE Geosci. Remote Sens. Lett. 19, 1–5.
Liu, N., He, T., Tian, Y., Wu, B., Gao, J., Xu, Z., 2020. Common-azimuth seismic data
fault analysis using residual UNet. Interpretation 8 (3), SM25–SM37.
Liu, N., Huang, T., Gao, J., Xu, Z., Wang, D., Li, F., 2021b. Quantum-enhanced deep
learning-based lithology interpretation from well logs. IEEE Trans. Geosci. Remote
Sens. 60, 1–13.
Liu, N., Li, F., Wang, D., Gao, J., Xu, Z., 2021c. Ground-roll separation and attenuation
using curvelet-based multichannel variational mode decomposition. IEEE Trans.
Geosci. Remote Sens. 60, 1–14.
Liu, N., Wang, J., Gao, J., Chang, S., Lou, Y., 2022a. Similarity-informed self-learning
and its application on seismic image denoising. IEEE Trans. Geosci. Remote Sens.
60, 1–13.
Liu, N., Wu, L., Wang, J., Wu, H., Gao, J., Wang, D., 2022b. Seismic data reconstruction
via wavelet-based residual deep learning. IEEE Trans. Geosci. Remote Sens. 60,
1–13.
Liu, P., Zhang, H., Zhang, K., Lin, L., Zuo, W., 2018. Multi-level wavelet-CNN for image
restoration. In: Proceedings of the IEEE Conference on Computer Vision and Pattern
Recognition Workshops. pp. 773–782.
Mandelli, S., Borra, F., Lipari, V., Bestagini, P., Sarti, A., Tubaro, S., 2018. Seismic
data interpolation through convolutional autoencoder. In: SEG Technical Program
Expanded Abstracts 2018. Society of Exploration Geophysicists, pp. 4101–4105.
Naghizadeh, M., Sacchi, M.D., 2010. Beyond alias hierarchical scale curvelet inter-
polation of regularly and irregularly sampled seismic data. Geophysics 75 (6),
WB189–WB202.
Nair, V., Hinton, G.E., 2010. Rectified linear units improve restricted boltzmann
machines. In: Icml.
Park, J., Yoon, D., Seol, S.J., Byun, J., 2019. Reconstruction of seismic field data
with convolutional U-Net considering the optimal training input data. In: SEG
Technical Program Expanded Abstracts 2019. Society of Exploration Geophysicists,
pp. 4650–4654.
Ronen, J., 1987. Wave-equation trace interpolation. Geophysics 52 (7), 973–984.Ronneberger, O., Fischer, P., Brox, T., 2015. U-Net: Convolutional networks for
biomedical image segmentation. In: International Conference on Medical Image
Computing and Computer-Assisted Intervention. Springer, pp. 234–241.
Singh, P., Singh, P., Sharma, R.K., 2011. JPEG image compression based on biorthog-
onal, coiflets and daubechies wavelet families. Int. J. Comput. Appl. 13 (1),
1–7.
Sternfels, R., Viguier, G., Gondoin, R., Le Meur, D., 2015. Multidimensional simulta-
neous random plus erratic noise attenuation and interpolation for seismic data by
joint low-rank and sparse inversion. Geophysics 80 (6), WD129–WD141.
Thorson, J., Claerbout, J., 1985. Velocity stack and slant stochastic inversion:
Geophysics. Soc. Expl. Geophys. 50, 2727–2741.
Trad, D., 2003. Interpolation and multiple attenuation with migration operators.
Geophysics 68 (6), 2043–2054.
Wang, Z., Bovik, A.C., Sheikh, H.R., Simoncelli, E.P., 2004. Image quality assessment:
From error visibility to structural similarity. IEEE Trans. Image Process. 13 (4),
600–612.
Wang, S.-H., Fernandes, S.L., Zhu, Z., Zhang, Y.-D., 2021a. AVNC: Attention-based
VGG-style network for COVID-19 diagnosis by CBAM. IEEE Sens. J. 22 (18),
17431–17438.
Wang, Y., Wang, B., Tu, N., Geng, J., 2020. Seismic trace interpolation for irregu-
larly spatial sampled data using convolutional autoencoder. Geophysics 85 (2),
V119–V130.
Wang, B., Wu, R.-S., Chen, X., Li, J., 2015. Simultaneous seismic data interpolation and
denoising with a new adaptive method based on dreamlet transform. Geophys. J.
Int. 201 (2), 1182–1194.
Wang, B., Zhang, N., Lu, W., Wang, J., 2019. Deep-learning-based seismic data
interpolation: A preliminary result. Geophysics 84 (1), V11–V20.
Wang, S.-H., Zhou, Q., Yang, M., Zhang, Y.-D., 2021b. ADVIAN: Alzheimer’s disease
VGG-inspired attention network based on convolutional block attention module and
multiple way data augmentation. Front. Aging Neurosci. 13, 313.
Wei, Q., Li, X., Song, M., 2021. Reconstruction of irregular missing seismic data using
conditional generative adversarial networks. Geophysics 86 (6), V471–V488.
Woo, S., Park, J., Lee, J.-Y., Kweon, I.S., 2018. Cbam: Convolutional block attention
module. In: Proceedings of the European Conference on Computer Vision. ECCV,
pp. 3–19.
Wu, R.-S., Geng, Y., Ye, L., 2013. Preliminary study on Dreamlet based compressive
sensing data recovery. In: SEG Technical Program Expanded Abstracts 2013. Society
of Exploration Geophysicists, pp. 3585–3590.
Wu, H., Li, Z., Liu, N., 2022a. Variable seismic waveforms representation:
Weak-supervised learning based seismic horizon picking. J. Pet. Sci. Eng. 110412.
Wu, B., Meng, D., Wang, L., Liu, N., Wang, Y., 2020. Seismic impedance inversion using
fully convolutional residual network and transfer learning. IEEE Geosci. Remote
Sens. Lett. 17 (12), 2140–2144.
Wu, X., Shi, Y., Fomel, S., Liang, L., Zhang, Q., Yusifov, A.Z., 2019. FaultNet3D:
Predicting fault probabilities, strikes, and dips with a single convolutional neural
network. IEEE Trans. Geosci. Remote Sens. 57 (11), 9138–9155.
Wu, B., Xie, Q., Wu, B., 2022b. Seismic impedance inversion based on residual attention
network. IEEE Trans. Geosci. Remote Sens. 60, 1–17.
Wu, X., Yan, S., Bi, Z., Zhang, S., Si, H., 2021. Deep learning for multidimensional
seismic impedance inversion. Geophysics 86 (5), R735–R745.
Wu, H., Zhang, B., Liu, N., 2022c. Self-adaptive denoising net: Self-supervised learning
for seismic migration artifacts and random noise attenuation. J. Pet. Sci. Eng.
110431.
Xu, k., Lv, J., Pei, G., Li, P., Zhong, H., Liu, Y., 2021. Study on the application of
seismic first arrival pickup method in complex piedmont. Unconv. Oil Gas 8 (04),
1–10.
Yang, Y., Wang, Z., Gao, J., Liu, N., Li, Z., 2021. Sparse inversion-based seismic random
noise attenuation via self-paced learning. Artif. Intell. Geosci. 2, 223–233.
Yoon, D., Yeeh, Z., Byun, J., 2020. Seismic data reconstruction using deep bidirectional
long short-term memory with skip connections. IEEE Geosci. Remote Sens. Lett. 18
(7), 1298–1302.
Yu, Z., Ferguson, J., McMechan, G., Anno, P., 2007. Wavelet-Radon domain dealiasing
and interpolation of seismic data. Geophysics 72 (2), V41–V49.
Yuan, E., Chen, F., Qi, Z., Long, J., 2015. Research and application of SVD in separation
of special interference wave. Unconv. Oil Gas 2 (05), 20–25.
Yuan, S., Wei, W., Wang, D., Shi, P., Wang, S., 2019. Goal-Oriented Inversion-Based
NMO Correction Using a Convex 𝑙2,1-Norm. IEEE Geosci. Remote Sens. Lett. 17 (1),
162–166.
Zhang, Y., Liu, Y., Zhang, H., Xue, H., 2019. Seismic facies analysis based on deep
learning. IEEE Geosci. Remote Sens. Lett. 17 (7), 1119–1123.
Zhang, B., Zhang, K., Guo, S., Marfurt, K.J., 2013. Nonstretching NMO correction of
prestack time-migrated gathers using a matching-pursuit algorithm. Geophysics 78
(1), U9–U18.
Zhou, R., Cai, Y., Zong, J., Yao, X., Yu, F., Hu, G., 2020. Automatic fault instance
segmentation based on mask propagation neural network. Artif. Intell. Geosci. 1,
31–35.