ORIGINAL ARTICLE
Hierarchical simultaneous vertical fragmentationand allocation using modiﬁed Bond EnergyAlgorithm in distributed databases
Hossein Rahimi, Fereshteh-Azadi Parand*, Davoud Riahi
Math and Computer Science Department, Allameh Tabataba’i University, Ahmad Ghasir St., Beheshti Av., Tehran, IranReceived 31 December 2014; revised 2 March 2015; accepted 3 March 2015Available online 23 April 2015
KEYWORDSBond Energy Algorithm;Distributed database system;Data allocation and frag-mentation;ClusteringAbstractDesigning an efﬁcient Distributed Database System (DDBS) is considered as one of themost challenging problems because of multiple interdependent factors which are affecting its perfor-mance. Allocation and fragmentation are two processes which their efﬁciency and correctness inﬂu-ence the performance of DDBS. Therefore, efﬁcient data fragmentation and allocation of fragmentsacross the network sites are considered as an important research area in distributed database design.This paper presents an approach which simultaneously fragments data vertically and allocates thefragments to appropriate sites across the network. Bond Energy Algorithm (BEA) is applied with abetter afﬁnity measure that improves the generated clusters of attributes. The algorithm simultane-ously generates clusters of attributes, calculates the cost of allocating each cluster to each site andallocates each cluster to the most appropriate site. Results show more efﬁcient clustering and allo-cation which gives better performance.
/C2112015 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This isan open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionDistributed databases reduce cost and increase performanceand availability, but the design of Distribute Database Man-agement Systems (DDBMS) is complicated. To make this pro-cess feasible it is divided into two steps: Fragmentation andAllocation. Fragmentation tries to split data into fragments,which should be allocated to sites over the network in the allo-cation stage. The process of fragmentation falls into two cate-gories: Vertical Fragmentation and HorizontalFragmentation. Vertical Fragmentation (VF) is partitioningrelation R into disjoint sets of smaller relations while Horizon-tal Fragmentation (HF) is partitioning relation R into disjointtuples. The allocation problem involves ﬁnding the optimaldistribution of fragmentation to set F on site set S. There arefour data allocation strategies applicable in a distributed rela-tional database: centralized, fragmentation (partition), fullreplication, and partial replication (selective) [10]. When data is allocated, it might either be replicated or maintained as asingle copy. So, fragment allocation can be either non-redundant or redundant. Under a non-redundant allocation
*Corresponding author. Tel.: +98 2188713160.E-mail addresses:s.rahimi@atu.ac.ir(H. Rahimi),parand@atu.ac.ir (F.-A. Parand),d.riahi@atu.ac.ir(D. Riahi).
qPeer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2018) 14, 127–133
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2015.03.0012210-8327/C2112015 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).scheme, exactly one copy of each fragment will exist across allthe sites, while under redundant allocation schema, more thanone copy of each fragment will exist across all the sites [12].I n this work, we combine fragmentation with partial replicationof some clusters of attributes.Allocation and fragmentation are interdependent and efﬁ-cient data fragment allocation requires considering allocationconstraints in the process of fragmentation, but in the mostprevious works these two steps are separated.There are two general approaches toward solving the parti-tioning problem. One is to ﬁnd the efﬁcient solution by consid-ering some of the constraints. In Hoffer [13]the storage capacity and retrieval cost constraints are the role factors.Each of these factors is weighted based on their amount ofeffect. The objective was to minimize the value of overall cost.The weights are calculated using linear programming approachso that the sum of the weights is equal to 1.minðc
1/C3storage costþc 2/C3retrieval costÞð1Þ Another good example of ﬁrst set of approaches is proposed inSchkolnick[21]. The method tries to cluster records within anInformation Management System (IMS) type hierarchicalstructure. The generated hierarchical tree is linear in the num-ber of nodes. Heuristic grouping is used by the method pre-sented in Hammer and Niamir[3]. It starts by assigningattributes to different positions. All potential types of groupingare considered and the one which represents the greatestimprovement over the current grouping candidate becomesthe new candidate. Grouping and regrouping are iterated untilno further improvement is likely. The main issue is the direc-tion of movement, which has a dominant effect on the efﬁ-ciency of the algorithm. Another heuristic approach ispresented in Ma et al.[5]uses a cost model and targets at glob-ally minimizing these costs. The major objective is to fragmentbased on efﬁciency of the most frequent queries. In Hoffer andSeverance[14]clusters of similar attributes are generated usingthe afﬁnity measure between pairs of attributes in conjunctionwith Bond Energy Algorithm (BEA). One of the major weak-nesses is that the number of attributes in clusters are not decid-able, and since it only considers pairwise attribute similarity, itis improper for larger numbers of attributes. Vertical fragmen-tation could also be done in more than one phase. This methodis presented in Navathe et al.[23]. A two-phased approach sep-arates fragments into overlapping and non-overlapping frag-ments. The ﬁrst phase is based on empirical objectivefunction and then it performs cost optimization by incorporat-ing the knowledge of a speciﬁc application environment in thesecond phase. The method presented in Latiful and Shahidul[6]is a methodology for the design of distributed object data-bases that includes an analysis phase to indicate the most ade-quate fragmentation technique, a horizontal classfragmentation algorithm, and a vertical class fragmentationalgorithm. The analysis phase is responsible for driving thechoice between the horizontal and the vertical partitioningtechniques, or even the combination of both, in order to assistdistribution designers in the fragmentation phase of objectdatabases. Baiao et al.[8]presents a three phased methodologyfor the design of distributed database that contains analysisphase, horizontal fragmentation algorithm phase, and verticalclass fragmentation phase. The method illustrated in Abuelya-man[7]experimentally shows that moving an attribute that isloosely coupled in a partition improves hit ratio of attribute inpartition.A method for synchronized horizontal fragmentation andallocation is proposed in Abdalla[4]. This method introducesa heuristic cost model to ﬁnd optimal fragment and allocation.Fragmentation is based on a set of simple predicates, and opti-mal allocation is the one which minimizes the cost function.An adaptable vertical partitioning method is presented in Jinand Myoung[15]. This article reviews Binary Vertical Parti-tioning (BVP)[18]and compares its results with the presentedadaptable vertical partitioning (AVP) which uses a hierarchicalmethod of fragmentation, creates a tree of partitions andﬁnally selects the best result. A heuristic method is imple-mented in Adrian Runceanu[1]. It applies the approach of for-mulating an objective function, named Partition Evaluator [2], before developing (heuristic) algorithms for the partitioningproblem. This approach enables studying the properties ofalgorithms with respect to an agreed upon objective function,and also to compare different algorithms for goodness usingthe same criteria for distributed database vertical fragmenta-tion. A new heuristic algorithm which is based on a decompo-sition technique is developed in Mahmoud and Roirdon [16] that greatly reduces the computational complexity of the prob-lem of ﬁle allocation and capacity assignment in a ﬁxed topol-ogy distributed network. Although using a partial replicationscheme increases database efﬁciency, this beneﬁt comes withsome costs. This cost, which could potentially be high, consistsof total storage cost, cost of local processing, and communica-tion cost[19]. Some fragmentation methods along with queryoptimization , distribution optimization, and join optimizationare covered in Haroun Rababaah, Hakimzadeh [9]. Here we take into account communication and local processing costsin combination with query access and calculate total storagecost separately.Fragmentation and allocation are usually performed sepa-rately while these two steps of Distributed DBMS design areclosely related to each other. The reason for separating the dis-tribution design into two steps is to better deal with the com-plexity of the problem[17].Here we present a method for VF, which applies BEA hier-archically with a modiﬁed similarity measure and simultane-ously allocates the fragments to the most appropriate site.The model notations are listed inTable 1.
Table 1Model notations.
AFF Attribute Aﬃnity matrixQA Query Access matrixCA Clustered Aﬃnity matrixDM Distance MatrixAU Attribute Usage matrixTSC Total Storage CostV Volume of data allocation measured in charactersSC
ij Storage cost of fragmentiin sitej affðA
i;AjÞThe aﬃnity of attributesA iandA j
freqlðqkÞAccess frequency of a querykon sitel acc
lðqkÞAccess per execution of querykon sitel S
ij Similarity measure betweenA iandA j
MQA Minimized Query AccessSC Storage CostIIC Iteration Input Cluster(is fed to next iteration)LC Leaf Cluster128 H. Rahimi et al.The rest of this article is organized as follows. Methods anddifferent inﬂuencing factors are discussed in Section 2. The algorithm is described in details in Section3. Section4draws comparative results of applying both the classic BEA and thepresented method on one database. Finally, conclusion andfuture work are discussed in Section5.2. MethodsAllocation and fragmentation are interdependent problemswhere solving them simultaneously is difﬁcult but results in bet-ter performance of applications. To the best of our knowledge,BEA is not applied to simultaneous fragmentation and alloca-tion. Since in vertical partitioning attributes which are usuallyaccessed together are placed in one fragment, deﬁning a precisemeasure of togetherness is critical. BEA uses afﬁnity of attri-butes to create clusters of attributes, which are the most similar.It starts with Attribute Usage (AU) and Query Access (QA)matrices generates Attribute Afﬁnity matrix (AFF) and ﬁnallycreates Clustered Afﬁnity matrix (CA) by positioning and re-positioning columns and rows of attributes. The afﬁnitymea- sure is too simple. The proposed afﬁnity measure in BEA is basi-cally based on simultaneous access of attribute A
iand attribute A
jof relationRðA 1;A2;...;A nÞby queryqkfor every query in Q¼ðq
1;q2;...;qqÞ. In other words, Two attributes are consid-ered similar if they are accessed by the same query. This is indi-cated in AU byA
ij¼1 andA ik¼1 simultaneously forattributesjandkaccessed by queryiConsidering the afﬁnityof attributesA
iandA jasaffðA i;AjÞ, access frequency of a querykon sitelasfreq
lðqkÞ, and access per execution of querykon site lasacc
lðqkÞ, the equation for afﬁnity presented is as below [14]. affðA
i;AjÞ¼X
kjuseðqk;AiÞ¼1^useðqk;AjÞ¼1X
8SlfreqlðqkÞ/C3acc lðqkÞð2ÞAfter generating AFF using the described afﬁnity measure,clusters of attributes are created using the split function. TheSplitðAFFÞtakes as input the AFF matrix, permutes its rowsand columns, and generates a CA matrix. The permutationis done in such a way to maximize the following globalmeasure.X
ni¼1Xnj¼1affij½aff i;j/C01þaff i;jþ1þaff i/C01;jþaff iþ1;j/C138ð3Þwhereaff
i;0¼aff 0;j¼aff i;nþ1¼aff nþ1;j¼0ð4Þ The last set of conditions takes care of the cases where anattribute is being placed in CA to the left of the leftmost attri-bute or to the right of the rightmost attribute during columnpermutations, and prior to the topmost row and followingthe last row during row permutations. In the process of split-ting the bond between two attributesiandjand the net con- tribution to the global afﬁnity measure of placing theattributekbetweeniandjplay key roles. The bond betweenattributesiandjis deﬁned asbondðA
i;AjÞ¼Xnk¼1affðA k;AiÞaffðA kAjÞð5ÞThe net contribution of placing the attribute kbetweeni andjis deﬁned ascontðAi;Ak;AjÞ¼2bondðA i;AkÞþ2bondðA k;AjÞ/C02bondðA
i;AjÞð6Þ The split function generates the Clustered Afﬁnity Matrix intwo steps:Algorithm 1.Simultaneous VF and allocation
Require:Communication Cost MatrixAttribute Usage Matrix (AU)Query Access Matrix (QA)Number of attributesOutput:Clustered Attribute Matrices as a TreeAllocated clusters to sites1: Optimizing Communication Cost Matrix and generating DM2: Generating Minimized Query Access matrix (MQA)MQA¼P
iP
kDM/C3QA3:IIC AU4:whileNumber of attributes in IIC>3doRun Modiﬁed BEA Algorithm (IIC,MQA)Add LC and IIC to Tree5:end while6: Calculate Storage CostP
mi¼1Xij/C3SC ij/C3V7: Allocate each cluster to site with minimum cost
/C15Initialization:Place and ﬁx one of the columns of AFFmatrix arbitrarily into CA matrix./C15Iteration:Pick each of the remainingn/C0icolumns wherei is the number of columns already placed in CA and try toplace them in the remaining iþ1 positions in the CA.Choose the placement that makes the greatest contributionto the global afﬁnity measure described above. Continuethis until no more columns remain to be placed.Since the clustering result of BEA is the split borderbetween two sets of attributes, BEA does not work efﬁcientlyfor larger databases. Therefore, we need a better approach toidentify more partitioning candidates. As we infer similarityof two attributes when they have concurrent occurrence in aquery, concurrent absence of them for the same query couldalso be considered as a weighted measure of similarity. Fur-thermore, single occurrence of each attribute could be consid-ered as a weighted measure of dissimilarity. Consider n
00;n11, andn
01andn 10be the number of simultaneous absence of attri-butes, presence of attributes, and single occurrence of eachattribute for one query in the Afﬁnity Usage (AU) matrix,respectively. Similarity measureS
ijwhich is described in Xuand Wunsch[20]usesn
11andn 00in the nominator of the frac-tion to show similarity andn
10andn 01in the denominator toindicate dissimilarity.S
ij¼n11þn 00
n11þn 00þw 1ðn01þn 10Þð7Þ This measure computes the match between two objectsdirectly. Unmatched pairs are weighted based on their contri-bution to the similarity. If one considers simple matching sim-ilarityw
1equals to one. In constrained-means clustering [24] the coefﬁcient is considered equal to 2. Gower [11]suggests w
1to be equal to 1=2. It can be concluded, choosing an appro-Hierarchical simultaneous VF and allocation in distributed databases 129priate value for the weightw 1depends the approach and alsoon the structure and deﬁnition of the database itself.Each one of queries can be accessed different times on eachsite. The frequency of query access on each site is described inthe Query Access (QA) matrix. The entry QA
ijindicates the number of times in which queryiis accessed in sitej. On the other hand communication costs between sites of a distributeddatabase play a key role in the performance of a distributedDB. Distance Matrix (DM) is the asymmetric square matrixthat reﬂects these costs which can be minimized using themethod described in Bentley and Dittman [25]. Multiplying DM in QA generates a new matrix in which the inﬂuence ofcommunication costs between sites and query access per siteis considered simultaneously and since DM is minimized dis-tance matrix then the resulted matrix will be the MinimizedQuery Access (MQA) matrix.MQA¼X
iX
kDM/C3QAð8ÞThe Total Storage Cost (TSC) of each attribute in each sitedepends on storage cost for one item and the total volume ofthat site.TSC¼X
mi¼1Xij/C3SC ij/C3Vð9ÞwhereX
ij¼1if fragment i is allocated to site jð10Þ The attribute with minimum storage cost for each site willbe allocated to that site. Eq.9is also applied to the remainingattributes and sites with minimum cost value allocate theattributes.3. AlgorithmThe algorithm (Algorithm 1) works with communication costbetween network sites, QA matrix, AU matrix, and attributescount as inputs and generates the tree of clustered attributesalong with allocating them to sites.The Algorithm works hierarchically and gradually creates acluster tree. In each iteration it generates two sets of attributes.The larger set of more similar attributes which we call it Iter-ation Input Cluster (IIC) is used as input for the next iteration.The other smaller set is called Leaf Cluster (LC) since it is sep-arated and placed as leaf node in the tree. In the ﬁrst step DMis generated by optimizing Communication Cost matrix usingWhitten et al.[25]. Then MQA is generated by multiplying QAin DM matrix. The next step is to initialize IIC by AU matrix.The algorithm continues with iterating on the modiﬁedBEA algorithm, which will be explained later, until attributecount in IIC is equal to 3. Since in each iteration the most sim-ilar attributes group in one IIC, we assume after this numberof iterations, the resulted IIC contains the most similar attri-butes of all therefore there is no need to go further. Next,the storage cost for each attribute on each site is calculatedand ﬁnally based on these costs, each cluster of attributes isallocated to the most appropriate site. The last IIC is allocatedto all sites.The modiﬁed BEA algorithm is actually modiﬁes the afﬁn-ity measure in the original BEA. As it is mentioned beforeBEA is simply using the concurrent occurrence of attributesto create AFF matrix. In the modiﬁed BEA presented here,other possibilities are considered. With Sijborrowed from Xu and Wunsch[20], taking co-absence into account, and cal-culatingn
00;n01, andn 10the new afﬁnity measureS ijis S
ij¼n11þw 1n00
n11þw 1n00þcoefð11Þ The weights ofw
1andw 2are between 0 and 1 sincen 00;n01, andn
10have less positive effect on similarity in comparisonton
11. Furthermore, it can be inferred thatw 1should be greater thanw
2. The approaches to calculate the value of eachweight are dependent on the structure and deﬁnition of thetable and their relations in the database. Gower and Legendremeasure[11]and Rogers and Tanimoto measure[22]are some methods to calculate values of weights. Each of the weights iscalculated considering the structure and deﬁnition of the data-base and queries. The structure of the database gives us someinformation regarding to the relations of different attributes.Therefore, by considering the queries, initial values of theweights are inferred and after generating the elementaryresults, the weights are slightly changed in such a way thatresults reﬂect the true expected relations between attributeswith consideration to the structure of the database.As we mentioned earlier simultaneous absence of attributescan give us some sense of similarity ofAlgorithm 2.Modiﬁed BEA algorithm
Require:Attribute Query MatrixQuery Access MatrixResult:AFF Matrix1:S MQA2:foreach attribute number ido3:QS
i sumðS ijÞ4:end for5:foreach attribute number ido6:foreach attribute number jdo7:initialize n
00;n11;n01;n10by08:ifði¼¼jÞthen9:AFF
ij sumðA jÞ/C3QS10:else11:foreach query number kdo12:calculating n
00;n11;n01;n10
13:ifðn 01¼¼0and n 10>0Þorðn 10¼¼0and n 01>0Þ then14:coef ð/C01Þðn
01þn10Þ/C3w 1
15:else16:coef ðjn
01/C0n10jÞ/C3w 1
17:end if18:S
ij ðn 11þw 2/C3n00Þ=ðn 11þw 2/C3n00Þþcoef 19:end for20:end if21:AFF
ij S i/C3QSi
22:end for23:end for24:call Function SplitðAFFÞ
the attributes. On the other hand, since this effect is marginalin comparison to the effect of simultaneous presence, n
00has some weighted effect on the afﬁnity measure and thereforew
1have a value between 0 and 1.130 H. Rahimi et al.The variablecoefin the denominator is reﬂecting the effectofn
01andn 10. There are four different possibilities. Whenn
01>0 andn 10¼0, it indicates that for two attributes ofA i
andA j, all queries which accessA ido not accessA j. This means that these attributes have some level of similarity. Asa result theS
ijshould get greater values so the weighted mea-sure in the denominator,w
2, should be negative. This is shownin Lines 12 and 13 ofAlgorithm 2. The same is for the case inwhichn
10>0 andn 01¼0. Other possibility is that bothn 01
andn 10are greater than 0. This condition means A iandA j
do not have the same behavior upon different queries whichare accessing them. This has negative effect on the similarity,so the weighted measure in the denominator, w
2, should be positive. This is shown in Line 15. After calculating the AFFmatrix, the algorithm calls the split function which wedescribed in Section2and creates clusters of attribute.4. Case studyIn order to estimate the amount of improvement and correct-ness of our algorithm, we applied both the classic BEA and ouralgorithm on database of Terminal Management System(TMS). TMS is a server which is connected to stores’ (or super-markets’) terminal with a unique serial number. Depending onthe terminal it can download or update terminal informationor operating system. Since stores are located in differentplaces, TMS can obviously work better with distributed data-base. Each terminal has a unique serial number, one task isdeﬁned for each terminal group. These tasks contain one ormore ﬁles which can be associated to a group of terminals.Each Terminal, group of terminal and task has one table. Asimple schema of tables and their relations is illustrated inFig. 4. After reviewing the transactions, eight most frequenttransactions and considering the relations of tables in TMS,eight attributes (illustrated inTable 2) for distributing in sevensites were selected.The AU, QA, and DM matrices are as shown in Figs. 1–3. The query access input for both algorithms were MQA. Theweightsw1andw 2in our algorithm were set to 0.7 and 0.3,respectively. The resulted clustering tree for each algorithm isshown inFig. 5. As it can be observed, both algorithms behavethe same until the fourth iteration. The classic BEA separatesattribute number 2 and puts attributes number 3, 4, 1, and 7 ina cluster. On the other hand the modiﬁed algorithm separatesattribute number 4 and clusters attributes number 2, 1, 3, and7. Considering the conditions applied in our algorithm, thecoefandS
ijare calculated and illustrated inTable 3. It is obvi- ous thatA
4is less similar to other attributes thanA 2therefore it has been separated correctly. We can conclude that the newalgorithm considers better measure and clusters the attributesmuch better.5. ConclusionDistributed databases reduce cost of update and retrieval ofinformation and increase performance and availability, butthe design of DDBMS is more complicated than designing cen-tralized database. One of the major challenges which greatlyaffects DDBS performance is fragmentation and allocationof fragments to sites. Allocation and fragmentation can logi-cally be merged and done simultaneously. In this paper weproposed a method that merges vertical fragmentation andallocation. To achieve this goal we applied Bond Energy Algo-rithm with a modiﬁed afﬁnity measure in a hierarchical processand simultaneously calculated the cost of data allocation foreach site and assigned fragment to the appropriate site. Theuse of the hierarchical process resulted in clustering sets ofmore similar attributes and better data fragmentation. Onthe other hand, by performing simultaneous cost calculationwe took interdependency of data fragmentation and allocationinto account.An extension to the work could cover optimizing the costfunction for data allocation considering the retrieval andupdate frequency for each attribute and applying betterapproach to calculate weights for similarity measure.Fig. 1Query Access matrix (QA) for seven sites.
Fig. 2Communcation Cost Distance Matrix (DM) for sevensites.
Fig. 3Attribute Usage matrix (AU) for 8 queries.Hierarchical simultaneous VF and allocation in distributed databases 131Fig. 4Table relations in TMS.
Table 2List of attributes and their related tables.
No. List of attributes Related table1model/C0name terminal/C0category 2term/C0fid terminal3hw/C0version terminal4pinpad/C0version terminal5flash/C0size terminal6app/C0name download/C0time 7interval/C0date download/C0time 8start/C0time download/C0plan
Fig. 5Hierarchical attribute clustering tree.Table 3The Similarity of attributes.
Attributesn 11 n00 n10 n01 coef S ij
A2and A 1 40220 1A
2and A 3 40220 1A
2and A 7 40220 1A
4and A 1 41210.35 0.93 A
4and A 3 41210.35 0.93 A
4and A 7 41210.35 0.93 A
1and A 3 40220 1A
1and A 7 62000 1A
3and A 7 40220 1132 H. Rahimi et al.References
[1] Adrian Runceanu, Fragmentation in distributed databases,Innovations and Advanced Techniques in Systems, ComputingSciences and Software Engineering, 2008, pp. 57–62.[2]
G.S. Chinchwadkar, A. Goh, An overview of verticalpartitioning in object oriented databases, Comput. J. 42 (1)(1999)
.[3]
M. Hammer, B. Niamir, A heuristic approach to attributepartitioning, in: Proceedings ACM SZGMOD InternationalConference on Management of Data, Boston, Mass., ACM,New York, 1979
.[4]
Hassan I. Abdalla, A synchronized design technique for efﬁcientdata distribution, Comput. Human Behav. 30 (2014) 427–435
. [5] H. Ma, K.D. Schewe, M. Kirchberg, A heuristic approach tovertical fragmentation incorporating query information, in:Proc. 7th International Baltic Conference on Databases andInformation Systems (DB and IS), 2006, pp. 69–76.[6] Latiful A.S.M. Hoque, Shahidul Islam Khan, A New Techniquefor Database Fragmentation in Distributed Systems,International Journal of Computer Applications 5(9):2024,August 2010. Published By Foundation of Computer Science.[7]
E.S. Abuelyaman, An optimized scheme for vertical partitioningof a distributed database, Int. J. Comput. Sci. Netw. Sec. 8 (1)(2008)
.[8]
F. Baiao, M. Mattoso, G. Zaverucha, A distribution designmethodology for object DBMS, Distrib. Parallel Databases 16(1) (2004) 4590
.[9] Haroun Rababaah, H. Hakimzadeh, Distributed Databases:Fundamentals and research, Advanced Database B561, Spring2005.[10] N. Iacob, Data replication in distributed environments, Annalsof the Constantin Brncusi, University of Trgu Jiu, EconomySeries, Issue 4, 2010.[11]
J. Gower, A general coefﬁcient of similarity and some of itsproperties, Biometrics 27 (1971) 857872
.[12]J.O. Hauglid, N.H. Ryeng, DYFRAM: dynamic fragmentationand replica management in distributed database systems,Distrib. Parallel Databases 28 (2010) 157185
. [13]
J.A. Hoffer, An integer programming formulation of computerdatabase design problems, Znf. Sci. 11 (July 1976) 29–48
. [14] J.A. Hoffer, D.G. Severance, The use of cluster analysis inphysical database design, in: Proceedings 1st InternationalConference on Very Large Databases, Framingham, Mass., 1975.[15]
Jin Hyun Son, Myoung Ho Kim, An adaptable verticalpartitioning method in distributed systems, J. Syst. Softw. 73(2004) 551–561
.[16]
S. Mahmoud, J.S. Roirdon, Optimal allocation of resources indistributed information networks, ACM Trans. Database Syst.1 (1976) 1
.[17]
M.T. Ozsu, P. Valduriez, Principles of Distributed DatabaseSystems, Alan Apt, New Jersey, 1999
. [18]
S. Navathe, S. Ceri, G. Wiederhold, J. Dou, Verticalpartitioning algorithms for database design, ACM Trans.Database Syst. 9 (4) (1984) 680710
. [19]
S.K. Rahimi, F.S. Haug, Distributed Database ManagementSystems, A John Wiley and Sons Inc. Publication, IEEEComputer Society, 2010
.[20]
Rui Xu, Donald Wunsch II, Survey of clustering algorithms,IEEE Trans. Neural Netw. 16 (3) (2005)
. [21]
M. Schkolnick, A clustering algorithm for hierarchical structure,ACM Trans. Database Syst. 2 (1977) 1
. [22] Pang-Ning Tan, Michael Steinbach, Vipin Kumar, Introductionto Data Mining, 2005, ISBN: 0-321-32136-7.[23]
Shamkant Navathe, Stefano Ceri, Gio Wiederhold, Jinglie Dou,Vertical partitioning algorithms for database design, ACMTrans. Database Syst. 9 (4) (December 1984)
. [24] K. Wagstaff, S. Rogers, S. Schroedl, Constrained -meansclustering with background knowledge, in: Proc. 8th Int. Conf.Machine Learning, 2001, pp. 577–584.[25]
J. Whitten, L. Bentley, K. Dittman, Systems Analysis andDesign Methods, sixth ed., McGraw-Hill, 2004
.Hierarchical simultaneous VF and allocation in distributed databases 133