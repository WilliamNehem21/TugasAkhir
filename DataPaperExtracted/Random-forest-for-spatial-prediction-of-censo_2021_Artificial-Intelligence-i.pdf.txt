Random forest for spatial prediction of censored response variables
Francky Fouedjio
Kaplan Business School Pty Ltd, Perth Campus, 1325 Hay St, West Perth, WA, 6005, Australia
ARTICLE INFO
Keywords:Censored observationsExact observationsPrincipal component analysisQuadratic programmingSpatial predictionABSTRACT
The spatial prediction of a continuous response variable when spatially exhaustive predictor variables areavailable within the region under study has become ubiquitous in many geoscience ﬁelds. The response variable is often subject to detection limits due to limitations of the measuring instrument or the sampling protocol used.Consequently, the response variable's observations are censored (left-censored, right-censored, or interval-censored). Machine learning methods dedicated to the spatial prediction of uncensored response variables cannot explicitly account for the response variable's censored observations. In such cases, they are routinely appliedthrough ad hoc approaches such as ignoring the response variable's censored observations or replacing them witharbitrary values. Therefore, the response variable's spatial prediction may be inaccurate and sensitive to the as-sumptions and approximations involved in those arbitrary choices. This paper introduces a random forest-basedmachine learning method for spatially predicting a censored response variable, in which the response variable'scensored observations are explicitly taken into account. The basic idea consists of building an ensemble ofregression tree predictors by training the classical regression random forest on the subset of data containing onlythe response variable's uncensored observations. Then, the principal component analysis applied to this ensembleallows translating the response variable's observations (uncensored and censored) into a linear equalities andinequalities system. This system of linear equalities and inequalities is solved through randomized quadraticprogramming, which allows obtaining an ensemble of reconstructed regression tree predictors that exactly honorthe response variable's observations (uncensored and censored). The response variable's spatial prediction is thenobtained by averaging this latter ensemble. The effectiveness of the proposed machine learning method isillustrated on simulated data for which ground truth is available and showcased on real-world data, includinggeochemical data. The results suggest that the proposed machine learning technique allows greater utilization ofthe response variable's censored observations than ad hoc methods.
1. IntroductionWith the increasing development of geoscience data collection plat-forms, the spatial prediction of a continuous response variable usingpredictor variables everywhere available within the study area isarousing much interest in many geoscience disciplines. Machine learningmethods are increasingly used for this purpose. Indeed, the number ofpredictor variables that can help explain the spatial variation in theresponse variable has grown dramatically, making other methodscumbersome to use.Kirkwood et al., 2016a,2022,Taghizadeh-Mehrjardi et al. (2016),Ballabio et al. (2016),Barzegar et al. (2016),Khan et al. (2016),Wilford et al. (2016),Hengl et al. (2015),Appelhans et al. (2015),Li (2013),Li et al. (2011)demonstrated the relevance of machinelearning methods (e.g., random forest, support vector machines, and,neural networks) for spatial prediction in geoscience applications (e.g.,geochemical mapping, soil mapping, hydrological mapping, andenvironmental mapping).Talebi et al. (2021),Sekuli/C19c et al. (2020), Hengl et al. (2018)developed machine learning approaches for spatialprediction, in which the spatial correlation is accounted. This latter playsa crucial role in the realm of geoscience data. Fouedjio (2020)introduced a machine learning technique for spatial prediction, where the responsevariable is exactly conditioned to data.In many geoscience applications, the response variable's observationsoften arise below or above the instrument's detection limit (DL). Theseobservations are referred to as censored observations (left-censored,right-censored, or interval-censored). Censoring refers to a condition inwhich the value of a measurement or observation is only partially known.Left censoring denotes that an observation is below a certain value but itis unknown by how much. Right censoring means that an observation isabove a certain value but it is unknown by how much. Interval censoringindicates that an observation is somewhere on an interval between twovalues. Censored data are a well-known problem when dealing, for
E-mail address:francky.fouedjio@kbs.edu.au.
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2022.02.001Received 31 December 2021; Received in revised form 13 February 2022; Accepted 13 February 2022Available online 23 February 20222666-5441/©2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-NDlicense (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127example, with geochemical data (Sanford et al., 1993). Many analytical results for some geochemical elements are reported under (or above) thedetection limit (DL). That is, concentrations for some samples are re-ported as”less than”or”greater than”a speciﬁc value. Censoring typi- cally results when analytical methods are not sensitive enough to detectsmall quantities of an element or when the technique is so sensitive thatlarge concentrations overwhelm the detection system. Censored obser-vations create difﬁculties for classical machine learning methods forspatial prediction as these latter require a complete set of uncensoredobservations. Approaches to tackle this problem have been relatively adhoc.The deletion and substitution methods are the ad hoc approaches tohandle the response variable's censored observations when using tradi-tional machine learning techniques for spatial prediction. The responsevariable's censored observations are discarded in the deletion procedure,and only uncensored observations are used. The substitution methodreplaces the response variable's censored observations with arbitraryvalues. Typically, censored observations are set equal to some constantvalue. This constant value is some function of the detection limit (e.g.,DL, DL/2, 2DL) and depends on the censoring type (left censoring, rightcensoring, interval censoring). Thus, censored observations are treated asuncensored observations under the substitution method. Traditionalmachine learning techniques for spatial prediction are usually applied tocensored response variables through these ad hoc strategies. Conse-quently, the response variable's spatial prediction may be imprecise andsensitive to the assumptions and approximations involved in those sub-jective choices. In particular, the response variable's spatial predictionmay be inconsistent with censored observations as the response variable'spredicted values at censored sampling locations are not honoringcensored observations. In other words, the response variable's predictedvalues at censored sampling locations could be outside constraint in-tervals. Although ad hoc methods are easy to implement, they can havedifﬁculties to accurately predict values below (or above) the detectionlimit (DL).So far, no alternative methods to the ad hoc techniques have beenproposed for spatially predicting a censored response variable whenspatially exhaustive predictor variables are available within the studyarea. There currently exist spatial prediction methods dedicated tocensored response variables only in the univariate context, where nopredictor (auxiliary) variables are available within the study region.These methods are based among other things on kriging with inequalityconstraints, data augmentation approaches, and Markov chain MonteCarlo (MCMC) algorithms (Ordo~nez et al., 2018;Schelin and Luna, 2014; Toscas, 2010;Fridley and Dixon, 2007;Rathbun, 2006;Abrahamsen and Benth, 2001;De Oliveira, 2005;Militino and Ugarte, 1999;Journel, 1986;Kostov and Dubrule, 1986;Dubrule and Kostov, 1986). This paper presents a machine learning-based method for spatially predicting acensored response variable, in which the response variable's censoredobservations (left-censored, right-censored, and interval-censored) areexplicitly taken into account, that is to say, as they are. Under the pro-posed machine learning method, the response variable's spatial predic-tion is carried out such that the response variable's predicted valuesexactly honor the response variable's observations (uncensored andcensored) at sampling locations. The proposed machine learningapproach naturally accounts for the response variable's censored obser-vations (inequality data).The proposed machine learning method starts with constructing anensemble of regression tree predictors by training the classical regressionrandom forest on the subset of data containing only the response vari-able's uncensored (exact) observations. Next, the principal componentanalysis is carried out to create an orthogonal decomposition of theensemble of regression tree predictors in terms of principal componentcoefﬁ
cients and factors. Then, the response variable's observations (un-censored and censored) are converted into a system of linear equalitiesand inequalities with principal component coef ﬁcients as unknown var- iables. The system of linear equalities and inequalities is then solvedthrough randomized quadratic programming, which allows samplingnew principal component coefﬁcients and then reconstructing regressiontree predictors that exactly honor the response variable's observations(uncensored and censored) at sampling locations. The response variable'sspatial prediction is then obtained by averaging the reconstructedregression tree predictors ensemble. The resulting response variable'sspatial prediction effectively honors the response variable's observations(uncensored and censored) at sampling locations. As a byproduct, theresponse variable's prediction uncertainty is provided. The proposedmachine learning method for spatial prediction of a censored responsevariable is illustrated and compared to ad hoc methods on simulated andreal-world data.The rest of the article is organized as follows. In Sect. 2, different ingredients required to apply the proposed machine learning techniqueare described. Section3demonstrates the proposed machine learningmethod's effectiveness on simulated and real-world data. A comparisonwith ad hoc methods is considered. Finally, in Sect. 4concluding remarks are given.2. MethodologyLet {Z(x):x2G} represents the continuous response variable de ﬁned on aﬁxed continuous geographical domain Gsubset ofRpðp/C211Þ.I n addition to the response variable, there is a set of qpredictor variables {f
1(x),…,f q(x):x2G} exhaustively known in the geographical domainG. We consider the situation where the data collection mechanism is suchthat the response variableZis not fully quantiﬁed due to limitations of the measuring device or the sampling protocol used. For any samplinglocation, the response variableZmay or may not be fully measured,wherein in the latter case, the response variable is known only up to a setof values. Thus, the response variable's observed data consist of ”exact observations”(hard data) measured at some of sampling locations and”interval observations”(inequality data) measured at the other samplinglocations as the result of censoring.The response variable's observed data is denoted by { Z(x
i)2A i,i¼1, …,n}, withfx
i2Ggi¼1;…;n representing sampling locations where exact(uncensored) observations and interval (censored) observations are ob-tained. The two following cases are of common use: A
iis reduced to a single valuez
i(exact or uncensored observations) or A iis an interval whereZ(x
i) is known to belong (interval or censored observations). Threetypes of inequality constraints can be considered for the response vari-able which cover many of the censoring mechanisms encountered inpractice. WhenA
iis an interval, it would be equal to either ( /C0∞,u i], [l i, þ∞), or [l
i,ui] for, respectively, left, right, and interval censoring; l i2R; u
i2R.A ican vary with location (multiple censoring).The goal is to predict the response variable { Z(x):x2G} over the geographical domainGrepresented as a grid ofNlocations, using the response variable's observations (uncensored and censored) and predic-tor variables data. In addition, the response variable's predicted valuesmust honor the response variable's observations (uncensored andcensored) at sampling locations, i.e., ^Zðx
iÞ2A i;i¼1;…;n. The basic ingredients required to implement the proposed machine learningmethod for spatial prediction are described in this section. The imple-mentation is carried out in the R platform ( R Core Team, 2021).2.1. Regression random forestThe starting point of the proposed machine learning method forspatially predicting a censored response variable in the presence ofspatially exhaustive predictor variables is the regression random forest(Breiman, 2001). Regression random forest is a type of ensemble ma-chine learning method that constructs a multitude of regression treemodels on various subsets of the training dataset (bootstrap samples)using different subsets of available predictor variables, followed by ag-gregation. Under random forest, each built regression tree model isF. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
116unique (less correlated with others) due to the bootstrapping of thetraining data and the random selection of subsets of predictor variables.The multiple regression tree models knitted together reduce the predic-tion variance and increase prediction accuracy. The regression randomforest's prediction is obtained by averaging all of the regression tree'spredictions.The random forest popularity for spatial prediction relies on its abilityto efﬁciently deal with many predictor variables, handle complexnonlinear relationships and interactions, require less data pre-processing,and be a non-parametric method (model-free). Regression random foresthas some tuning parameters that can be optimized via a cross-validationprocedure. There are, among others, the number of trees, number ofpredictor variables randomly selected at each node, proportion of ob-servations to sample in each regression tree, and minimum number ofobservations in a regression tree's terminal node. It is usually advocatedto set the number of trees to a large number, allowing the convergence ofthe prediction error to a stable minimum ( Hengl et al., 2018). The R packagesranger(Wright and Ziegler, 2017) andtuneRanger(Probst et al., 2018) implement the regression random forest.The proposed machine learning method for spatial prediction startsby training the classical regression random forest on the subset of datacontaining only the response variable's uncensored (exact) observations.The outcome is an ensemble of regression tree predictorsf~Z
bðxÞ:x2Ggb¼1;…;B , whereBis the number of regression trees. At thisstage, the response variable's censored (interval) observations are not yetconsidered. Also, individual regression tree predictors do not exactlyhonor the response variable's observed values at uncensored samplinglocations. Thus,f~Z
bðxÞ:x2Ggb¼1;…;B will be called”unconditional regression tree predictors”. The next steps aim to generate conditionalregression tree predictors that perfectly honor the response variable'sobservations at censored and uncensored sampling locations.2.2. Principal component analysisThe second step of the proposed machine learning method consists ofperforming principal component analysis (PCA) on the ensemble of un-conditional regression tree predictorsf~Z
bðxÞ:x2Ggb¼1;…;B arranged as a matrixΓ(B/C2N) whose each row represents a single regression treepredictorf~Z
bðxÞ:x2Gg. We obtain the following decomposition inﬁnite dimensions:~Z
bðxÞ¼XLl¼1αb;lψlðxÞ;8x2G;b¼1;…;B;(1)wheref
αb;lgl¼1;…;L are principal component (PC) scores (coefﬁcients) and f
ψlðxÞ:x2Ggl¼1;…;L are principal components (PC) factors (eigen-func-tions);L¼min(B,N).Eq.(1)can be interpreted as a decomposition of a set of imagesn~Z
bðxÞ:x2Go
b¼1;…;Binto a set of eigen-imagesf ψlðxÞ:x2Ggl¼1;…;L
and coefﬁcientsf αb;lgl¼1;…;L . The PC factors are consideredﬁxed, while the PC coefﬁcients are considered random. As one can note, PCA is uti-lized here as an orthogonal decomposition method rather than adimension reduction technique. All PC factors are kept, as shown in Eq.(1). The bijective property of PCA allows reconstructing regression treepredictors from PC coefﬁcients. In other words, an image can be recon-structed back once all the PC factors and coef ﬁcients are used.2.3. Randomized quadratic programmingThe third step of the proposed machine learning method consists ofgenerating new principal component (PC) coef ﬁcients under the PCA decomposition depicted by Eq.(1)such that regression tree predictorsexactly honor the response variable's observations (uncensored and un-censored) at sampling locations. LetZðxÞ¼XLl¼1θlψlðxÞ;8x2G; (2)wherefθ
lgl¼1;…;L are random coefﬁcients andf ψlðxÞ:x2Ggl¼1;…;L are PC factors derived from the PCA of unconditional regression tree pre-dictors as given in Eq.(1). All PC factors are considered, so there is notruncation.We want to generate coefﬁcientsfθ
lgl¼1;…;L such thatfZðxÞ:x2Gg exactly honors the response variable's observations (uncensored andcensored) at sampling locations, i.e., Zðx
iÞ2A i;i¼1;…;n. To achieve that, the response variable's observations (uncensored and censored) at samplinglocations are translated into a set of equality and inequality constraintsusing Eq.(2). Hence, the following system of equalities and inequalities:8<:θ
1ψ1ðx1Þþθ 2ψ2ðx1Þþ⋯þθ LψLðx1Þ2A 1
…θ
1ψ1ðxnÞþθ 2ψ2ðxnÞþ⋯þθ LψLðxnÞ2A n;(3)wherefθ
lgl¼1;…;L are the unknown variables.A i(i¼1,…,n) is equal to either a single valuez
i(uncensored observations), or an interval of thetype (/C0∞,u
i], [l i,þ∞), or [l i,ui] (censored observations). Thus, thesystem of equalities and inequalities deﬁned in Eq.(3)is induced by the response variable's observations (uncensored and censored). In that way,the response variable's censored observations are naturally taken intoaccount. Conditional PC coefﬁcientsθ¼ðθ
1;…;θ LÞTthat exactly honor the response variable's observations (uncensored and censored) aregenerated by solving the following randomized quadratic optimizationproblem (Fouedjio et al., 2021a;Fouedjio, 2021):min
θ2RL/C0ðθ/C0βÞTΣ/C01ðθ/C0βÞ/C1subject tofΨ iθ2A igi¼1;…;n ;Ψ i
¼½ψlðxiÞ/C138l¼1;…;L ; (4) whereβ/C24Nð
μ;ΣÞ. The mean μand the covariance matrixΣof the multivariate normal distribution are computed using unconditional PCcoefﬁcientsf
αb;lgl¼1;…;L derived from the PCA of unconditional regressiontree predictors as shown in Eq.(1). Especially,
μ¼"1BX Bb¼1αb;l#
l¼1;…;L;Σ¼1B/C01X Bb¼1ðαb/C0μÞðαb/C0μÞT;with
αb¼½αb;l/C138l¼1;…;L: (5) For each Monte Carlo sampleβ
t/C24Nð μ;ΣÞðt¼1;…;TÞ, quadratic programming (Goldfarb and Idnani, 1983) is performed toﬁnd a solution θ
tthat satisﬁes the composite constraints (equality and inequality) andminimizes the quadratic objective function de ﬁned in Eq.(4). The covariance matrixΣin Eq.(4)is a diagonal matrix because the PC co-efﬁcients are uncorrelated by construction. Conditional PC coef ﬁcientsθ
t
can be also generated via the Gibbs sampling method ( Fouedjio et al., 2021b). However, this approach can be time-consuming for very largedatasets since Gibbs sampler generate samples that are highly correlated.As one can note in Eq.(3), the number of response variable’obser- vations (uncensored and censored) deﬁnes the number of equalities and inequalities constraints. The number of unconditional regression treepredictorsBshould be large enough to allow good coverage of the so-lution space when solving the system of linear equalities and inequalitiesdeﬁned in Eq.(3). Indeed, the more signiﬁcant is the number of uncon- ditional regression tree predictors, the wider is the solution space of thesystem of linear equalities and inequalities de ﬁned in Eq.(3). Also, too many composite constraints (hard and inequality data) relative to too fewunconditional regression tree predictors will lead to low uncertainty. It isworth mentioning that the number of conditional regression tree pre-dictorsTdoes not depend on the number of unconditional regression treepredictorsBunder randomized quadratic programming. That is to say, T can be smaller or greater thanB.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
117Given conditional PC coefﬁcientsfθ tgt¼1;…;T , regression tree pre- dictors that exactly honor the response variable's observations (uncen-sored and censored) at sampling locations are obtained byreconstruction:Z
tðxÞ¼XLl¼1θt;lψlðxÞ;8x2G: (6) The prediction of the response variable over the geographical domainGis obtained by averaging the predictions from all the individualreconstructed regression tree predictors:^ZðxÞ¼
1TX Tt¼1ZtðxÞ;8x2G: (7)It is important to highlight that in Eq. (7), all the individual recon- structed regression tree predictorsfZ
tðxÞ:x2Ggt¼1;…;T exactly honor the response variable's observations (uncensored and censored) at samplinglocations. Thus, the meanf^ZðxÞ:x2Ggdoes also. In addition to providing predictions, the proposed machine learning method naturallydelivers a quantiﬁcation of the uncertainty associated with the predictionas a byproduct. The prediction uncertainty represents the uncertaintyaround the prediction at a target location, re ﬂecting the inability to exactly deﬁne the unknown value. Assessing the uncertainty about thevalue of the response variable at a target location and of the need toincorporate this assessment in subsequent studies or to support decisionmaking is becoming increasingly crucial ( Fouedjio and Klump, 2019; Szatm/C19ari and P/C19asztor, 2019;Veronesi and Schillaci, 2019). Under the proposed machine learning method, an ensemble of conditional regres-sion tree predictors is produced at any target location. Thus, the condi-tional distribution for the response variable at any target location isavailable. Hence, predictions using, e.g., the expectation, the mode, orthe median can be evaluated and prediction uncertainty using theinterquartile range or the variance of the ensemble conditional regressiontree predictors can be obtained.To summarize, the proposed machine learning method for spatiallypredicting a censored response variable in the presence of spatiallyexhaustive predictor variables is performed using the following pseudoalgorithm:Algorithm 1. Random Forest for Spatial Prediction of CensoredResponse Variables3. Application examplesThe proposed machine learning method's ability to spatially predict acensored response variable is illustrated using simulated and real-worlddata. The prediction performance is assessed using some well-knownprediction accuracy statistics: mean absolute error (MAE), root meansquare error (RMSE), and Lin's concordance correlation coef ﬁcient (CCC). The lower are MAE and RMSE, the better is the predictionmethod. The closer is CCC to 1, the better is the prediction technique. Aprediction performance comparison of the proposed method with two adhoc methods is carried out.3.1. Simulated data exampleThe data-generating process of the response and predictor variables isgiven by the following model:ZðxÞ¼50sinðf
1ðxÞÞ þ3f 1ðxÞf 2ðxÞþ0:5f 3ðxÞ2þ10sinðf 4ðxÞÞ þ ηðxÞ;8x 2½0;100/C138
2; (8) whereZ(⋅) is the response variable. The predictor variables f
1(⋅),f 2(⋅), f
3(⋅), andf 4(⋅), and the latent variable η(⋅) are independent Gaussian isotropic stationary random functions ( Chiles and Delﬁner, 2012) with mean and covariance function speciﬁed inTable 1. The predictor, latent, and response variables are simulated over a 250/C2250 regular grid in the geographical domain [0,100]
2. For background on Gaussian random functions, see Chiles and Delﬁner (2012). TheTable 1Simulated data example - simulation parameters.
Mean Covariance functionType Scale Sillf
1(⋅) 10 Gaussian 11.5 1f
2(⋅) 10 Exponential 6.5 1f
3(⋅) 10 Cardinal Sine 1.5 1f
4(⋅) 10 Cubic 20 1
η(⋅) 0 Spherical 30 100F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
118simulation is performing using the turning bands method in the RpackageRGeostats(Renard et al., 2020). This simulated data example for which the ground truth is available everywhere within the study domainrefers to a situation where there is a non-linear relationship between theresponse variable and predictor variables with some interactions be-tween predictor variables. Also, the response variable shows some spatialauto-correlation, and its distribution is non- Gaussian.Fig. 1displays the simulated data over a 250 x 250 regular grid(62500 observations).n¼300 observations are sampled randomly andtaken as the training data (Fig. 1f) as follows. The response variable'sobservations are divided in three groups: ( /C0∞,λ], [γ,þ∞), and [λ,γ], whereλ¼220.86 andγ¼442.40 are the respectively, 1st and 99thpercentiles. 45 observations are sampled randomly from the group ( /C0∞,λ] and taken as left-censored observations ( Z/C20λ). 45 observations are sampled randomly from the group [ γ,þ∞) and taken as right-censored observations (Z/C21γ). 210 observations are sampled randomly from thegroup [λ,γ] and considered as uncensored observations. Thus, the pro-portion of censored data (left-censored and right-censored) is 30 %in the training data. The rest of data (62200 observations) is kept aside for thetesting.The ad hoc method 1 discards the response variable's censored ob-servations and considers only uncensored observations. The ad hocmethod 2 replaces the response variable's censored observations by thebounds (λandγ). In ad hoc methods 1 and 2, classical regression randomforest is performed with the number of trees equaling 5000. The otherhyper-parameters have been optimized through cross-validation. Under
Fig. 1.Simulated data example - (a), (b), (c), (d) predictor variables, (e) response variable, and (f) sampling locations. black, red, and green points in (f ) represent respectively, uncensored, left-censored, and right-censored sampling locations.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
119Fig. 2.Simulated data example -B¼5000 unconditionalﬁrst four PC scores andT¼1000 conditionalﬁrst four PC scores.
Fig. 3.Simulated data example - prediction maps provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
120the proposed machine learning method, the regression random forestmodel built using only the response variable's uncensored observationsconsists of an ensemble ofB¼5000 unconditional regression tree pre-dictorsf~Z
bðxÞ:x2½0;100/C1382gb¼1;…;5000 . According to the methodology described in Sect.2, PCA is performed on this ensemble, followed byrandomized quadratic programming. T¼1000 new PC scores are generated, thus giving an ensemble of T¼1000 reconstructed (new) regression tree predictorsfZ
tðxÞ:x2½0;100/C1382gt¼1;…;1000 that perfectly honor the response variable's observations (uncensored and censored) atsampling locations. The response variable's spatial prediction, de ﬁned as the average of reconstructed (new) regression tree predictors, also honorsthe response variable's observations (uncensored and censored). It isessential to highlight thatB¼5000 unconditional regression tree pre-dictors are the same as those generated in the ad hoc method 1. Un-conditional PC scoresf
αbgb¼1;…;5000 and conditional PC scores fθ
tgt¼1;…;1000 are presented inFig. 2. The points cloud of conditional PCscores is less scattered than those from unconditional PC scores dueeffectively to the exact conditioning to the response variable's observa-tions (uncensored and censored).Fig. 3presents prediction maps provided by ad hoc methods 1 and 2and the proposed method. The prediction map resulting from the ad hocmethod 1 differs from the two other methods as the former only uses theresponse variable's uncensored observations. The general appearance ofprediction maps resulting from the ad hoc method 2 and proposedmethod looks similar. However, there are some local differences in areasdominated by censored observations due to the exact conditioningcharacteristic of the proposed method. The prediction uncertainty(interquartile range) maps for ad hoc methods 1 and 2 and the proposedmethod are given inFig. 4. The prediction uncertainty map resultingfrom the proposed method differs signiﬁcantly from the others. This is explained by the exact conditioning nature of the proposed method.Under the proposed machine learning method, the response variable'sspatial prediction exactly honors the response variable's observations(uncensored and censored) at sampling locations. Consequently, theprediction uncertainty is zero at uncensored sampling locations (exactobservations) by construction which is not the case for ad hoc methods.As ad hoc methods can provide the response variable's predicted valuesthat are outside constraint intervals at censoring sampling locations, theytend to overestimate the prediction uncertainty.Fig. 5shows the response variable's observed values versus predictedvalues in the testing data, under ad hoc methods 1 and 2 and the pro-posed method. One can notice that ad hoc methods have dif ﬁculties predicting values below (resp. above) the lower (resp. upper) detectionlimit, which is not the case for the proposed machine learning method.Fig. 6provides the histogram of predictions ensemble of the responsevariable at a training location (left-censored) for ad hoc methods 1 and 2and the proposed method. One observes that many predictions aregreater than the lower detection limit (220.86) under ad hoc methods 1and 2. In comparison, all predictions are less than the lower detectionlimit under the proposed method.Fig. 7depicts the histogram of pre- dictions ensemble of the response variable at a training location (right-censored) for ad hoc methods 1 and 2 and the proposed method. Simi-larly, ad hoc methods 1 and 2 provide predictions less than the upperdetection limit (442.40), while the proposed method offers predictionsgreater than the upper detection limit. Thus, the proposed machinelearning method is more consistent with the response variable's censoredobservations than ad hoc methods. In Figs. 6 and 7, one can see that the proposed method provides a more reliable con ﬁdence interval (predic- tion uncertainty) than the ad hoc methods.Table 2shows the predictive performance of ad hoc methods 1 and 2and the proposed method on the testing data (62200 observations). Thesame experiment is repeated for different proportions of censored data(30%, 40%, 50%, 60%, and 70%). One can observe that the proposed
Fig. 4.Simulated data example - prediction uncertainty maps provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
121method provides better predictive performance than ad hoc methods. Inparticular, the ad hoc method 1 is the worst as it only uses a fraction ofthe data available.Table 3shows the predictive performance of ad hocmethods 1 and 2 and the proposed method on the testing data for theresponse values below the lower detection limit (625 observations) andthe response values above the upper detection limit (625 observations).One can note the predictive performance between the proposed and adhoc methods is signiﬁcantly large. Thus, the proposed method handlesbetter the response variable's censored observations than ad hocmethods.3.2. Geochemical data exampleIn this application example, the response variable is Scandium (Sc)geochemical concentration observed at 568 sampling locations across thestudy region in southwest England ( Kirkwood et al., 2016b). The response variable is subject to a lower detection limit of 3 mg/kg (left--censoring). The response variable's censored observations represent /C24 6% of total observations. The observations are divided into a training setð/C2475%Þand a testing setð/C2425%Þ. As we are more interested in theadded-value of censored observations, the testing set consists of
Fig. 5.Simulated data example - response variable's observed values vs response variable's predicted values in testing dataset, for (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method.
Fig. 6.Simulated data example - histogram of predictions ensemble at a training location (left-censored, Z/C20220.86) provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method. The red line represents the response variable's true value.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
122uncensored observations that are geographically close to the censoredobservations as shown inFig. 8a.Fig. 8b provides the spatial plot of the response variable's uncensored observations. Figs. 8c and8d display respectively, the histogram and the variogram of the response variable'suncensored observations. Predictor variables comprise elevation, grav-ity, magnetic, Landsat, radiometric, and their derivatives, totaling 26predictor variables. Some predictor variables are displayed in Fig. 9. In this application example, the ad hoc method 1 ignores the responsevariable's censored observations and considers only uncensored obser-vations. The ad hoc method 2 replaces the response variable's censoredobservations by half the lower detection limit as it is common forgeochemical data. In ad hoc methods 1 and 2, classical regression randomforest is performed with the number of trees set to 5000. The other hyper-parameters have been optimized through cross-validation. The proposedmachine learning method generates an ensemble of B¼5000 uncondi- tional regression tree predictors, followed by an ensemble of T¼1000 conditional regression tree predictors. Unconditional and conditional PCscores are shown inFig. 10. As mentioned in the simulated data example,the size of the envelop containing conditional PC scores is smaller thanthe one containing unconditional PC scores because of the exact condi-tioning to the observations (uncensored and censored). It is important tohighlight thatB¼5000 unconditional regression tree predictors are thesame as the ones generated from the ad hoc method 1.Prediction maps provided by ad hoc methods 1 and 2 and the pro-posed method are depicted inFig. 11. The prediction map produced by the ad hoc method 1 differs from the two others, especially in areasdominated by censored observations. The general appearance of pre-diction maps generated by the ad hoc method 2 and the proposed methodlooks similar. However, one notes some local differences in regionsdominated by censored observations due to the exact conditioning natureof the proposed method.Fig. 12presents the prediction uncertainty(interquartile range) map under ad hoc methods 1 and 2 and the pro-posed method. The prediction uncertainty map resulting from the pro-posed method differs signiﬁcantly from the others. As highlighted in thesimulated data example, this is explained by the exact conditioningproperty of the proposed method. Under the proposed machine learningmethod, the response variable's spatial prediction exactly honors theresponse variable's observations (uncensored and censored) at samplinglocations. In contrast, ad hoc methods can provide the response variable'spredicted values that are outside constraint intervals at censoring sam-pling locations. Consequently, they tend to overestimate the predictionuncertainty.Fig. 13depicts the histogram of predictions ensemble of the responsevariable at a training location (left-censored) for ad hoc methods 1 and 2and the proposed method. One can notice that many predictions aregreater than the lower detection limit (3 mg/kg) under ad hoc methods 1and 2. In contrast, all predictions are smaller than the lower detectionlimit under the proposed method.Fig. 14shows the histogram of pre- dictions ensemble of the response variable at one testing location (un-censored) for ad hoc methods 1 and 2 and the proposed method. One cansee that the proposed method provides a more reliable con ﬁdence in- terval (prediction uncertainty) than the ad hoc methods.Table 4presents the predictive performance of ad hoc methods 1 and2 and the proposed method on the testing data. The proposed machinelearning method shows better predictive performance than the othermethods. Thus, the proposed method can exactly honor the responsevariable's observations (uncensored and censored) at sampling locationswhile achieving good out-of-sample predictive performance. Even in thecase of a small proportion of censored data, the difference can be sub-stantial between the proposed and ad hoc methods.
Fig. 7.Simulated data example - histogram of predictions ensemble at a training location (right-censored, Z/C21442.40) provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method. The red line represents the response variable's true value.
Table 2Simulated data example - predictive performance statistics in the testing datasetcontaining 62 200 observations, under ad hoc methods 1 and 2, and the proposedmethod.
Proportion of Censored Data30% 40% 50% 60% 70%MAE 1 13.15 14.15 14.29 14.45 15.03MAE 2 11.27 12.13 11.97 12.67 13.58MAE 10.32 11.11 11.36 11.61 11.32RMSE 1 17.55 18.78 18.93 19.18 19.81RMSE 2 14.84 16.07 15.95 16.83 17.54RMSE 13.33 14.42 14.72 14.94 14.42CCC 1 0.924 0.911 0.909 0.908 0.901CCC 2 0.952 0.944 0.945 0.944 0.939CCC 0.963 0.957 0.956 0.957 0.960
Table 3Simulated data example - predictive performance statistics in the testing datasetcontaining 625 observations that are below the lower detection limit (220.86)and 625 observations that are above the upper detection limit (442.40), underthe ad hoc methods 1 and 2, and the proposed method.
Proportion of Censored Data30% 40% 50% 60% 70%MAE 1 45.72 46.46 47.24 49.98 49.55MAE 2 19.46 18.70 17.80 17.24 16.51MAE 8.67 8.66 9.55 9.18 8.68RMSE 1 47.87 48.54 49.15 51.54 51.14RMSE 2 22.97 22.52 21.86 21.89 21.22RMSE 12.41 12.49 13.01 12.72 12.03CCC 1 0.893 0.899 0.886 0.872 0.874CCC 2 0.980 0.981 0.982 0.982 0.983CCC 0.995 0.995 0.994 0.995 0.995F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
1234. Concluding remarksThis paper presented a random forest-based machine learning methodfor spatially predicting a censored response variable in which theresponse variable's censored observations are explicitly taken into ac-count. Under the proposed machine learning method, the response var-iable's spatial prediction exactly honors the response variable'sobservations (uncensored and censored) at sampling locations. This isachieved by combining traditional regression random forest, principalcomponent analysis, and randomized quadratic programming. Theeffectiveness of the proposed machine learning method has been show-cased on simulated and real-world data. The proposed method allowsbetter use of censored data than ad hoc methods.The proposed machine learning method has the advantage of natu-rally incorporating the response variable's censored observationscompared to ad hoc methods (e.g., deletion and substitution methods).There is no substitution, imputation, or discarding of censored observa-tions, as with ad hoc methods. It can perfectly honor the response
Fig. 8.Geochemical data example: (a) uncensored vs censored sampling locations, (b) spatial plot of the response variable's uncensored observations, (c) histogram of the response variable's uncensored observations, (d) variogram of the response variable's uncensored observations.
Fig. 9.Geochemical data example - some predictor variables: (a) elevation, (b) Landsat 8 band 5, (c) gravity survey high-pass ﬁltered Bouguer anomaly, (d) Thorium counts from gamma ray spectrometry.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
124variable's observations (uncensored and censored) at sampling locationswhile achieving good out-of-sample predictive performance compared toad hoc methods. It also provides realistic prediction uncertainties of theresponse variable compared to ad hoc techniques. It has the advantage ofallowing a fast updating of the response variable predictive map when afew observations (uncensored and censored) are added. Only the last partof the proposed method, i.e., the randomized quadratic programming,should be performed. The proposed machine learning method is easy toimplement since it combines well-known existing machine learning,Monte Carlo sampling, and optimization techniques. It handles any
Fig. 10.Geochemical data example - unconditional and conditional ﬁrst four PC scores.
Fig. 11.Geochemical data example - prediction maps provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
125censoring type (right censoring, left censoring, and interval censoringobservations) and allows multiple censoring.The proposed machine learning method is built from the regressionrandom forest. The number of unconditional regression tree predictorsshould be large enough for good coverage of the solution space whenperforming the exact conditioning of the ensemble of unconditionalregression tree predictors to the response variable's observations (un-censored and censored). Indeed, the response variable's observations
Fig. 12.Geochemical data example - prediction uncertainty maps provided by (a) ad hoc method 1, (b) ad hoc method 2, and (c) proposed method.
Fig. 13.Geochemical data example - histogram of predictions ensemble of the response variable at a training location (left-censored) provided by (a) ad hoc m ethod 1, (b) ad hoc method 2, and (c) proposed method. The lower detection limit of the response variable is equal to 3.
Fig. 14.Geochemical data example - histogram of predictions ensemble of the response variable at a testing location (uncensored) provided by (a) ad hoc metho d1 , (b) ad hoc method 2, and (c) proposed method. The red line represents the response variable ’observed value.F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
126(uncensored and censored) deﬁne the number of equalities and in-equalities constraints. The more signiﬁcant is the number of uncondi- tional regression tree predictors, the wider the solution space is. Thus,too many constraints relative to a few unconditional regression treepredictors will lead to too small uncertainty. Generating a large numberof unconditional regression tree predictors is not a problem as thisparameter is free.Conﬂict of interestThere is no conﬂict of interest.Declaration of interestsThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsThe author is grateful to the anonymous reviewers and the editor fortheir helpful and constructive comments that helped improve themanuscript.References
Abrahamsen, P., Benth, F.E., 2001. Kriging with inequality constraints. Math. Geol. 33,719–744.Appelhans, T., Mwangomo, E., Hardy, D.R., Hemp, A., Nauss, T., 2015. Evaluatingmachine learning approaches for the interpolation of monthly air temperature at Mt.Kilimanjaro, Tanzania. Spatial Statistics 14, 91 –113. Ballabio, C., Panagos, P., Monatanarella, L., 2016. Mapping topsoil physical properties atEuropean scale using the LUCAS database. Geoderma 261, 110 –123. Barzegar, R., Asghari Moghaddam, A., Adamowski, J., Fijani, E., 2016. Comparison ofmachine learning models for predicting ﬂuoride contamination in groundwater. Stoch. Environ. Res. Risk Assess. 1–14. Breiman, L., 2001. Random forests. Mach. Learn. 45, 5 –32. Chiles, J.P., Delﬁner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley & Sons.De Oliveira, V., 2005. Bayesian inference and prediction of Gaussian random ﬁelds based on censored data. J. Comput. Graph Stat. 14, 95 –115. Dubrule, O., Kostov, C., 1986. An interpolation method taking into account inequalityconstraints: I. methodology. Math. Geol. 18, 33 –51. Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.Artif. Intelligen. Geosci. 1, 11–23. Fouedjio, F., 2021. Classiﬁcation random forest with exact conditioning for spatialprediction of categorical variables. Artif. Intelligen. Geosci. 2, 82 –93. Fouedjio, F., Klump, J., 2019. Exploring prediction uncertainty of spatial data ingeostatistical and machine learning approaches. Environ. Earth Sci. 78, 38 . Fouedjio, F., Scheidt, C., Yang, L., Achtziger-Zupan /C20ci/C20c, P., Caers, J., 2021a. A geostatistical implicit modeling framework for uncertainty quanti ﬁcation of 3d geo-domain boundaries: application to lithological domains from a porphyry copperdeposit. Comput. Geosci. 157, 104931 .Fouedjio, F., Scheidt, C., Yang, L., Wang, Y., Caers, J., 2021b. Conditional simulation ofcategorical spatial variables using Gibbs sampling of a truncated multivariate normaldistribution subject to linear inequality constraints. Stoch. Environ. Res. Risk Assess.35, 457–480.Fridley, B.L., Dixon, P., 2007. Data augmentation for a Bayesian spatial model involvingcensored observations. Environmetrics: Off. J. Int. Environ. Soc. 18, 107 –123. Goldfarb, D., Idnani, A., 1983. A numerically stable dual method for solving strictlyconvex quadratic programs. Math. Program. 27, 1 –33. Hengl, T., Heuvelink, G.B.M., Kempen, B., Leenaars, J.G.B., Walsh, M.G., Shepherd, K.D.,Sila, A., MacMillan, R.A., Mendes de Jesus, J., Tamene, L., Tondoh, J.E., 2015.Mapping soil properties of Africa at 250 m resolution: random forests signi ﬁcantly improve current predictions. PLoS One 10, 1 –26. Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr €aler, B., 2018. Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables.PeerJ 6, e5518.Journel, A., 1986. Constrained interpolation and qualitative information —the soft kriging approach. Math. Geol. 18, 269–286. Khan, S.Z., Suman, S., Pavani, M., Das, S.K., 2016. Prediction of the residual strength ofclay using functional networks. Geosci. Front. 7, 67 –74. Kirkwood, C., Cave, M., Beamish, D., Grebby, S., Ferreira, A., 2016a. A machine learningapproach to geochemical mapping. J. Geochem. Explor. 167, 49 –61. Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016b. Stream sediment geochemistry asa tool for enhancing geological understanding: an overview of new data from southwest England. J. Geochem. Explor. 163, 28 –40. Kirkwood, C., Economou, T., Pugeault, N., Odbert, H., 2022. Bayesian deep learning forspatial interpolation in the presence of auxiliary information. Math. Geosci. https:// doi.org/10.1007/s11004-021-09988-0 . Kostov, C., Dubrule, O., 1986. Interpolation method taking into account inequalityconstraints: II. practical approach. Math. Geol. 18, 53 –76. Li, J., 2013. Predictive modelling using random forest and its hybrid methods withgeostatistical techniques in marine environmental geosciences. In: 11-th AustralasianData Mining Conference (AusDM1 ́3), Canberra, Australia, pp. 73–79. Li, J., Heap, A.D., Potter, A., Daniell, J.J., 2011. Application of machine learning methodsto spatial interpolation of environmental variables. Environ. Model. Software 26,1647–1659.Militino, A.F., Ugarte, M.D., 1999. Analyzing censored spatial data. Math. Geol. 31,551–561.Ordo~nez, J.A., Bandyopadhyay, D., Lachos, V.H., Cabral, C.R., 2018. Geostatisticalestimation and prediction for censored responses. Spatial Statistics 23, 109 –123. R Core Team, 2021. R: A Language and Environment for Statistical Computing. RFoundation for Statistical Computing, Vienna, Austria. URL: https://www.R-project.o rg/.Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and tuning strategies forrandom forest. Wiley Interdisciplinary Reviews: Data Min. Knowl. Discov. https:// doi.org/10.1002/widm.1301. Rathbun, S.L., 2006. Spatial prediction with left-censored observations. J. Agric. Biol.Environ. Stat. 11, 317–336. Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. RGeostats:geostatistical package. URL:http://cg.ensmp.fr/rgeostats.r.package.version.12.0.1 . Sanford, R.F., Pierson, C.T., Crovelli, R.A., 1993. An objective replacement method forcensored geochemical data. Math. Geol. 25, 59 –80. Schelin, L., Luna, S., 2014. Spatial prediction in the presence of left-censoring. Comput.Stat. Data Anal. 74, 125–141. Sekuli/C19c, A., Kilibarda, M., Heuvelink, G., Nikoli /C19c, M., Bajat, B., 2020. Random forest spatial interpolation. Rem. Sens. 12, 1687 . Szatm/C19ari, G., P/C19asztor, L., 2019. Comparison of various uncertainty modelling approaches
based on geostatistics and machine learning algorithms. Geoderma 337, 1329 –1340. Taghizadeh-Mehrjardi, R., Nabiollahi, K., Kerry, R., 2016. Digital mapping of soil organiccarbon at multiple depths using different data mining techniques in baneh region,Iran. Geoderma 266, 98–110. Talebi, H., Peeters, L.J., Otto, A., Tolosana-Delgado, R., 2021. A truly spatial randomforests algorithm for geoscience data analysis and modelling. Math. Geosci. 1 –22. Toscas, P.J., 2010. Spatial modelling of left censored water quality data. Environmetrics21, 632–644.Veronesi, F., Schillaci, C., 2019. Comparison between geostatistical and machine learningmodels as predictors of topsoil organic carbon with a focus on local uncertaintyestimation. Ecol. Indicat. 101, 1032 –1044. Wilford, J., de Caritat, P., Bui, E., 2016. Predictive geochemical mapping usingenvironmental correlation. Appl. Geochem. 66, 275 –288. Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for highdimensional data in Cþþand R. J. Stat. Software 77, 1–17.Table 4Geochemical data example - predictive performance statistics in the testingdataset containing 147 observations.
Criteria Ad hoc Method 1 Ad hoc Method 2 Proposed MethodMAE 3.13 2.89 2.54RMSE 3.91 3.97 3.49CCC 0.65 0.73 0.78F. Fouedjio Artiﬁcial Intelligence in Geosciences 2 (2021) 115 –127
127