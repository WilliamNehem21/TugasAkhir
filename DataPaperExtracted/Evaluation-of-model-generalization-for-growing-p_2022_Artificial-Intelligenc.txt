Evaluation of model generalization for growing plants usingconditional learning
Haﬁz Sami Ullah, Abdul Bais ⁎
Electronic Systems Engineering, Faculty of Engineering and Applied Science, University of Regina, Regina, Saskatchewan, Canada
abstract article info
Article history:Received 20 September 2021Received in revised form 24 September 2022Accepted 25 September 2022Available online 28 September 2022This paper aims to solve the lack of generalization of existing semantic segmentation models in the crop andweed segmentation domain. We compare two training mechanisms, classical and adversarial, to understandwhich scheme works best for a particular encoder-decoder model. We use simple U-Net, SegNet, andDeepLabv3+ with ResNet-50 backbone as segmentation networks. The models are trained with cross-entropyloss for classical and PatchGAN loss for adversarial training. By adopting the Conditional Generative AdversarialNetwork (CGAN) hierarchical settings, we penalize different Generators (G) using PatchGAN Discriminator(D) and L1 loss to generate segmentation output. The generalization is to exhibit fewer failures and perform com-parably for growing plants with different data distributions. We utilize the images from four different stages ofsugar beet. We divide the data so that the full-grown stage is used for training, whereas earlier stages are entirelydedicated to testing the model. We conclude that U-Net trained in adversarial settings is more robust to changesin the dataset. The adversarially trained U-Net reports 10% overall improvement in the results with mIOU scoresof 0.34, 0.55, 0.75, and 0.85 for four different growth stages.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
Keywords:Weed detectionSemantic segmentationAdversarial trainingLate germinationSugar beetCrop segmentationGrowing plantsDomain change
1. IntroductionWeeds are undesirable plants that can be eliminated using herbi-cides. Traditionally, these herbicides are used uniformly over thewholeﬁeld. This uniform spraying eliminates the weeds but affectsthe crop as well. It also consumes an unnecessary amount of herbicide,adversely affecting the environment and human health. The amount ofherbicide usage can be decreased by selective spraying. The agriculturalrobots can perform weed and crop detection to perform precisespraying, which will help reduce herbicides' usage. However, theserobots lack a universal weed detection mechanism making them un ﬁt for commercial use. In addition, typical methods for detecting weedsdo not translate to out-of-distribution data; therefore, each new weedtype orﬁeld needs to be updated, which is time-consuming.Solving the weed detection problem using the colour, skeleton, andarea-based hand-crafted features is prone to error, as the plant's shapeundergoes variations due to growth, time of the day, weather condi-tions, and temperature. Therefore, these hand-crafted features-basedclassiﬁcations are only practical in situations with limited variations inthe data. The Convolutional Neural Network (CNN) based encoder-decoder architectures are reliable for weed detection using semanticsegmentation (Milioto et al., 2017), but a system that can work formultiple stages with different data distribution is still required.This paper proposes a solution for better model generalization. Inthis work, we perform experiments to achieve a more reliable modeltuning that can be implemented on a robot to select weeds. Further-more, the resultant model performs better on the data on which it istrained and other growth stages without any additional training re-quirement. The dataset used for this research consists of images col-lected from multiple stages of sugar beet crop ( Chebrolu et al., 2017). It has nineteen stages which are divided into four sections. The ﬁrst three sections haveﬁve stages each, and the last section consists ofthe remaining four stages.In semantic segmentation, given the limited access to labelleddata and difﬁculty annotating at the pixel level, the generalizationof the model parameters for the unknown stage of the plant becomesdifﬁcult. To articulate that, we explore the capability of differentmodels to segment the crop and weeds from different stages whentrained in an adversarial (using CGAN) and classical fashion. Themain focus is to train different models to adapt to the unseen infor-mation given minimal access to data, i.e., training only for the lastfour stages. The target dataset has varying illumination, multipleweeds, and different soil texture. Using a fully grown stage for under-standing, we are searching for a mechanism to improve the modelperformance for different stages of the crop. In addition, we focusArtiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
⁎Corresponding author.E-mail addresses:hsz248@uregina.ca(H.S. Ullah),abdul.bais@uregina.ca(A. Bais).
https://doi.org/10.1016/j.aiia.2022.09.0062589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/on generalizing the model's parameters for changing soil texture andlighting conditions. In this work, our contributions are:•Providing a thorough study about the effects of changing Generator(G) in CGAN•Finding the potential of one training scheme over another givenlimited access to dataWe compare the performance of different models when trained inclassical fashion, where the model has direct access to the segmenta-tion mask, and in adversarial settings, where the segmentation maskis fed through a discriminator as a condition. The classical training isperformed using weighted cross-entropy loss, whereas the adversar-ial training is conducted using L1 loss and patch-wise discriminatorloss.The CGAN (Isola et al., 2017) is used for adversarial training. TheCGAN in (Isola et al., 2017) consists of two individual models, D and G.The D tries to classify the input as fake or real, and the G's objective isto generate realistic fake images to fool D. Fig. 1shows CGAN settings and how inputﬂows through the model. The experiments are basedon both classically and adversarially trained encoder-decoder models.Firstly, we experimented with different encoder-decoder modelstrained in classical fashion. The input to the model is four channelled(RGB-NIR) images. Secondly, the same models are trained in adversarialsettings. We replace G with the earlier used encoder-decoder models.These encoder-decoder models include U-Net ( Ronneberger et al., 2015), SegNet (Badrinarayanan et al., 2017), U-Net and SegNet with ResNet-50 as a backbone, and DeepLabv3+ ( Chen et al., 2018). A com- parison is made between the models and the training mechanism. Thiscomparison helps to fully understand the capability of models and train-ing settings to perform semantic segmentation for growing plants.The paper is divided intoﬁve sections. InSection 2, we present the related work where methods for weed detection are reviewed. Thedata analysis, experiments, and description of the model architecturesare described inSection 3.Section 4presents the results with a compar-ison of model performance. Finally, we conclude the paper in Section 5.2. Related workExisting weed detection methods could be broadly categorized intotwo categories; feature-based and CNN based. Furthermore, CNN-based methods are divided into two, one based on classical trainingand the other with adversarial training.Haug et al. propose a feature-based detection method usingRandom Forests Classiﬁer (RFC) (Breiman, 2001). First, they extract Normalized Difference Vegetation Index (NDVI) mask for each over-lapping 80 × 80 pixels window, followed by the computation of 15key points based on the shape of the plant and pixel intensities.Next, users draw class-based polygons which assign the class labelto nearby pixels. The features and labels are fed to the classi ﬁer fortraining (Haug et al., 2014). A similar approach is followed by Lotteset al., where Local Binary Patterns (LBP) are used to ﬁnd statistical features over R, G, and NDVI channels. The LBP operator generates abinary pattern based on the centre pixel value within eight neigh-bours. Shape-based features are calculated for vegetation masks.These features are then used to train RFC to detect weeds and plants(Lottes et al., 2016). These methods depend on shape-based features,which vary for the same or growing stage of the plant.Milioto et al. propose CNN based classiﬁer for weed detection. The vegetation mask is used for detecting connected vegetation blobs ofsize 64 × 64 pixels, and bounding boxes are computed around eachblob. The RGB + NIR blobs are used as input to CNN, which classi ﬁes them in a vector with the probability distribution of the input image
patch as a plant or a weed (Milioto et al., 2017). Mortensen et al. pro- pose a modiﬁed version of the VGG-16 (Simonyan and Zisserman, 2014) model to perform the semantic segmentation task achievingpixel accuracy of 79% (Mortensen et al., 2016). Milioto et al. work on weed detection in data having different distributions. Ini-tially, the encoder-decoder-based model, modi ﬁcation of SegNet (Badrinarayanan et al., 2017), is trained for uniformly distributeddata, and performance is measured for the same and cross datasets(Milioto et al., 2018).Cicco et al. propose plant modelling for enhancing the dataset anduse the SegNet model (Badrinarayanan et al., 2017) for semantic seg- mentation of weeds and crops. They design different leaf models andthen model other plants based on the type and number of leaves.These plants with varying sizes of leaves representing various growthstages are randomly distributed over the soil. The respective class isspread with the class semantic mask on a transparent background forground truth. The generated dataset is fed to the SegNet model for seg-mentation (Di Cicco et al., 2017). This solution helps enhance data to im-prove accuracy. McCool et al. use encoder-decoder architecture toperform weed, and crop segmentation tasks ( McCool et al., 2017). Lottes et al. use a decoder with dilated 3D convolution to visualize the weedand crop. This technique uses a sequence of non-overlapping imageschosen using the odometry information. This method proposes thesequential module for extracting the pattern of the plantation. Theyachieve the mDICE score of 0.86 for the cross-validation dataset(Lottes et al., 2018). The common challenges with the above schemesare variant features utilization and lack of generalization for growingplants.Gated-shaped CNN (Takikawa et al., 2019)i m p r o v e st h es e m a n t i c segmentation task by processing information in two streams. Insteadof processing colour, shape, and texture information in a single tradi-tional network, a shape-based stream is proposed. The shaped-basedstream focuses on gradient computation for the given mask. The in-termediate layers in the shape stream are connected using GatedConvolutional Layers (GCL). Finally, features from the regular andshape streams are fused using Atrous Spatial Pooling Pyramid(ASPP). They report an overall mean Intersection Over Union(mIOU) of 80.8% for the cityscape dataset.
Fig. 1.The CGAN for image translation: G represents generator whereas D represents Discriminator, x represents input (RGB and NIR) image, G outputs RGB sema ntic map that is fed to D with the fake label. In the next iteration real semantic map is used along with x input to only train D with a real label.H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
190Asad and Bais propose a novel approach for agricultural image label-ling with promising results for weed detection ( Asad and Bais, 2020). Images from the canolaﬁeld areﬁrst segmented into background andforeground, then the weeds are manually labelled. This labelling mech-anism is not fully automatic, but still, it reduces the complexity of man-ual labelling. The segmentation masks with ﬁeld images are fed to two different models, SegNet (Badrinarayanan et al., 2017), and U-Net (Ronneberger et al., 2015). With mIOU of 82.9%, SegNet based onResNet-50 outperforms U-Net architecture. Model-assisted labellingunder different challenges is adopted in ( Ullah et al., 2021) for crop and weed segmentation in canola. Chen et al. propose an ef ﬁcient way of semantic segmentation. They use the ASPP module to extract multi-scale information. ResNet and Xception-based backbone is used to ex-tract high-level features. These features are placed at multiple scales inthe encoder. The decoder takes low-level information from the featureextractor and combines it with high-level features to target the seman-tic map. They name their model DeepLabv3+, reporting 89% mIOU forCityscapes dataset (Chen et al., 2018).Wang et al. use different image enhancement techniques for im-proving weed, and crop segmentation under varying lighting ( Wang et al., 2020). They use histogram equalization and deep photo enhancer(GAN is used to perform enhancement) for image enhancement. Thendeeplabv3+ (Chen et al., 2018) model is trained reporting 88.91%mIOU.Semantic segmentation can also be done using CGAN. Most of the re-cent research on CGAN focuses on image translation either from text oran image (Karras et al., 2019;Reed et al., 2016;Wang et al., 2018;Zhang et al., 2017). Reed et al. use GANs to convert textual features to generatean image. The randomly distributed noise with text description is fed tothe generator. The discriminator is conditioned to the image with thetext attributes. This idea is useful for generating arti ﬁcial data (Reed et al., 2016). Zhang et al. improve image generation from text usingtwo-stage stack GAN. The stage I generator extracts sketches and thestage II generator reﬁnes the results (Zhang et al., 2017). The text description to image translation can be used as a data enhancementtechnique. In the image-to-image translation, Isola et al. suggest seman-tic segmentation on the cityscape dataset using CGAN. The scene imageis fed to an encoder-decoder-based G, and the output of G is forwardedto D. D is trained for ground truth and generated a segmentation maskimage for a given scene image. With pixel accuracy of 86%, generatedimages look close to the ground truth, but there is noise. The reasonfor this noise is instability in G (Isola et al., 2017). Karacan et al. utilize semantic layout to synthesize realistic outdoorimages. They use CGAN that learns target content to be drawn insidelayout (Karacan et al., 2016). Rezaei et al. propose CGAN for segmentingbrain tumours. Utilizing U-Net (Haug et al., 2014)e20 as generator and Markovian GAN (Radford et al., 2015) based discriminator, they report 0.68 mDICE score (Rezaei et al., 2017). Wang et al. use CGAN for high- resolution image translation. They use 70 × 70 Patch-GAN ( Isola et al., 2017) for discriminator networks. Their focus is to transform thesemantic map into a realistic image. They also report the inverse trans-formation achieving 63.9% mIOU for Cityscapes dataset (
Wang et al., 2018). Regmi et al. work on natural image synthesis and pixel-level seg-mentation using CGAN. The generator is con ﬁgured to take an aerial image and convert it to a street map view and the corresponding se-mantic map. The model consists of one encoder and two decoderstreams. They achieve mIOU of 41.3% ( Regmi and Borji, 2018). Rammy et al. propose retinal vessel segmentation using conditional patch-based GAN. The conditional sample with noise vector z of latent spaceis fed to the generator leading to a synthetic map. This synthetic mapis used in the discriminator as a fake sample update. They report98.84% speciﬁcity for the proposed method (Rammy et al., 2019). To the best of our knowledge, CGANs were not previously used for weedsegmentation. We aim to improve weed detection for the cross stagesdataset without relying on hand-designed features and to reduce Ginstability in CGAN.3. Data analysis and experimentsIn this work, we performed a comparison-based study to understandthe effectiveness of model generalization under adversarial and classicaltraining settings. Our focus is toﬁnd an architecture as well as a trainingscheme that could effectively detect weeds and crops at multiplegrowth stages. In the following subsection, we perform ExploratoryData Analysis (EDA) to understand the underlying pattern and chal-lenges in the data.3.1. Data analysisIn the dataset, images are collected over the growing season of thesugar beet crop.Fig. 2analyzes the percentage of Weed and Crop LeafArea (WLA and CLA) over different growth stages. Eqs. (1) and (2) explains the calculation of percentage CLA and WLA respectively.Percentage Crop Leaf Area¼∑Y crop
Total Number of pixels/C2100ð1ÞY
croprepresents pixels associated with the crop, and the pixel valuecan either be 1 or 0 for the channel associated with the crop.Percentage Weed Leaf Area¼
∑Y weed
Total Number of pixels/C2100ð2ÞY
weedrepresents pixels associated with the weed, pixel value 1 or0 represent weed or no weed, respectively. The pixel value isfundamentally a one-hot encoded vector representing the pixel be-longing to a particular class based on its hot value. For example, thevector with values [1,0,0] means the pixel belongs to the ﬁrst class. InFig. 2, the plots in theﬁrst column indicate the percentage CLA,whereas the second column reports the percentage WLA. In Fig. 2 (i and ii), CLA and WLA of images from Set I, that consist of ﬁve initial timestamps, are reported. The percentage leaf area gives us an indica-tion of changing shape and size. InFig. 2(a), for stage one, the crop leaf area is a small fraction (approx. 0.05%), whereas in (b), the weedpercentage is near zero, i.e., there is no weed at the early stage. This isrepresented with blue inFig. 2(a and b). The weedy plants start poppingout at Stage 3 in Set I (Fig. 2(b)), which is more than the crop as the av-erage leaf area is approximately 0.2% compared to the crop that is 0.11%.As we move forward in Set II,Fig. 2(c and d) show prominent growthfor the crop, but for weeds, there is no consistent growth. The approxi-mate mean CLA is near 0.11% for the crop, whereas for weeds, it is closeto 0.03%. InFig. 2(e and f), the growing trend of percentage CLA con-tinues with an approximate mean of 0.65%, whereas the percentageWLA has an approximate mean of 0.10%. In Set IV, the average WLA isabout 0.82%, whereas for the crop, the average increases to 4.52% asshown inFig. 2(g and h). This analysis aims to understand the underly-ing challenges associated with plant growth, especially when the task isto segment it. Going from a minimal growth stage (where the plant isbarely visible) to a stage where the average CLA in the images fromSet IV containing a crop plant is almost 10% (CLA of Set IV with cropplants), a lot is happening with the plant's shape. This diversity is com-plex to generalize if we do not access any prior information. Anothercritical information that can be extracted from Fig. 2is the percentage distribution of classes of interest. Looking at WLA and CLA, one can tellthat the dataset is highly imbalanced. For example, there are morepixels in the background data than weed and crop. For a model to un-derstand various distributions like the ones explained through Fig. 2, we, unfortunately, sometimes do not have access to enough data.The percentage of pixels for each class and the sets of images isreported inTable 1. For example, Set IV has the highest crop and weedpixels percentage compared to the other three sets. On the otherhand, pixels in Set I mainly belong to the background, and only three-H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
191Fig. 2.Data analysis using percentage crop and weed leaf area distribution. A total of 19 timestamps (stages) are grouped to make 4 sets. The ﬁrst three sets contain 5 timestamps each, whereas the last group (Set IV) has the ﬁnal three stages.H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
192quarters of a percent have weeds and crop. This analysis is essential tostudy as it helps deﬁne the solution to the problem of data imbalance.In this work, we address the discussed problem by exploring thegeneralization capacity of different models under two training settingswith minimal exposure to the shape-based diversity of data.3.2. ExperimentsThe diversity of a plant's shape is a signiﬁcant challenge and hard to address if data is unavailable. In our case, although we have access to thedata from different plant growth stages, the aim here is to limit access tothe data and perform training only over the last few stages. We thenevaluate the technique over the remaining data to understand thecapacity of different models when trained using two methods. First,different state-of-the-art encoder-decoder models with multiple back-bone structures are trained classically. In classical training, the modeldirectly learns the mapping from a given ﬁeld image to a semantic map, i.e. model has exposure to output segmentation mask. The samenetworks are trained in the second mechanism using adversarial train-ing settings. While utilizing the CGAN model, we use the D as describedin pix2pix (Isola et al., 2017) and change G with encoder-decoder archi-tectures described earlier to see the variation in results. In CGAN, G gen-erates images, and D classiﬁes them as real or fake. D splits the imageinto patches and decides the patch of the image to be real or fake. Thisprocess continues until we get the balance between D and G. The back-ground varies between stages, impacting the model performance. Toreduce model dependency on background, we randomly blur theimages using a Gaussianﬁlter of size 5 and standard deviation of 1.The random blurring changes the entire image, not only the soil. Theplants also get an effect of blurriness which helps in mimicking thevibrations in the ground imagery system.In both settings, we train the models for 100 epochs with batch size4 and save the model's best weights. We use Adam optimizer with alearning rate 0.0002 andβ
10.5. In addition, we use one-hot encodedsegmentation masks with weighted cross-entropy for classical training.The weighted cross entropy helps in mitigating the class imbalanceproblem. In adversarial settings, we use binary cross-entropy loss andmean absolute error (L1 loss) for D and G. It is called PatchGAN lossbecause the D generates a patch indicating the image as real or fake.3.3. Objective function for classical trainingSemantic segmentation using classical training is common. The sizeand shape of plants change with growth. If enough data is unavailable,these changes and the soil texture variations are hard to learn. Addition-ally, the dataset is imbalanced as there are more background pixels thanweed and crop.Fig. 3explains the distribution of the entire dataset. Only1.46% of a total number of pixels consists of vegetation with 0.23% forweed and 1.28% for the crop. Almost 98.5% of the pixels are from thebackground class. The weighted cross-entropy loss helps in solving thedata imbalance problem. For an unbalanced dataset, weighted crossentropy assigns loss weightage to each class. The weightage is set suchthat the class with the high number of samples gets a lower weight,and the class with the low number of samples contributes a higher por-tion towards overall loss. The focus of model tuning is directly related tothe penalty for each class. The objective function for classical learningcan be written as:Loss Ytrue,Ypred/C0/C1¼/C0 1P∑Pp¼1∑Cc¼1wcYtrue p,clog Y predp,c/C16/C17ð3ÞwhereY
true∈[0,1] is the ground truth andY pred∈[0,1] is the softmax output. P represents the number of pixels and C denotes the total num-ber of classes.w
cis weight assigned to classc. For this work, thew
cis chosen by taking the inverse of thepercentage of the data distribution.3.4. Objective function for CGANThe main objective while training CGAN is playing the min-maxgame. The training is assisted by generating real and fake labels for D.These labels are produced at each step. The real label corresponds togrid of ones and is only given when the ground truth is used. Zeros rep-resent the fake label. They are given when the prediction from the gen-erator is used. The output of D has a shape of 24 × 32. The labels aregenerated of the same size as the output of D. Initially, D takes ﬁeld image and condition as input. The condition is the ground truth that Gaims to learn. It can be called a real sample update. Then, D takes ﬁeld image and prediction from G as input. This is a fake sample update. Atthe end of each real and fake sample update in D, the weights of G areupdated. While updating the parameters for the G, we perform back-propagation through D without updating D's parameters. In otherwords, we update G with all the layers of D set to non-trainable. Thelearning rate of D is adjusted to half of the G using loss weight con ﬁgu- ration so that G learns faster than D. Collectively the objective of CGANcan be expressed as below (Isola et al., 2017): Loss
CGAN G,DðÞ ¼E X,YtruelogDX,Y true ðÞ½/C138 þE X,Zlog 1/C0D X, G X, ZðÞ /C138ðð½ð4ÞX and Z represent the input image and the operation performed onthe image, respectively.Y
trueis the ground truth and only given to D.For G, we use mean absolute error (L1 loss) for parameter updates. L1loss works well for low-frequency components by improving sparse-ness in the image. The sparsity is linked with having few pixels with ahigher difference to the original. As the D and G work opposite, G re-stricts the high component and resists D to only model high frequency.For G, our objective equation is:Loss
L1GðÞ¼E X,Ytrue,Z‖Ytrue/C0GXð,ZÞ‖ 1 ½/C138 ð5ÞOverall, the end objective is to play the min-max game as givenbelow:G
∗¼arg min
Gmax
DLoss CGAN G,DðÞ þλLoss L1GðÞ ð6Þλis the tuning parameter to assign more weightage to G's loss. Thevalue ofλis set to 100.The best model is chosen based on the equilibrium of minimumlosses of G and D. If D is trained at the same speed as the G, then Dwill outperform G and getting a balance will be dif ﬁcult.Fig. 1shows how the inputsﬂow through the network. G takes the RGB-NIR ﬁeld image and generates the target segmentation mask. The output of Drepresents if eachM×Ngrid in an image is real or fake. In otherwords, it gives a penalty that penalizes the D model at M×Npatch of the image. The model architecture and the improvements made aredescribed in the following subsections.3.5. Model architecturesWe use U-Net (Ronneberger et al., 2015), SegNet (Badrinarayanan et al., 2017), U-Net with ResNet-50 backbone, SegNet with ResNet-50Table 1Percentage-wise pixel distribution of Weed, Crop, Background the data.Set Images Percentage Distribution of PixelsWeed Crop BackgroundI 2679 0.02% 0.05% 99.93%II 3423 0.03% 0.11% 99.86%III 2673 0.10% 0.65% 99.25%IV 2800 0.82% 4.52% 94.66%H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
193backbone, and DeepLabv3+ (Chen et al., 2018) with ResNet-50 back- bone to perform our experiments. The mentioned models are ﬁrst trained using classical training mechanism and then using adversarialsettings.All the models take an input consisting of RGB and NIR channels. Thedimension of the input image is set to 384 × 512. All the models learn tomap the input image to a translation representing the class semanticmask. Each model has two main blocks in the architecture, the encoderand decoder block. The encoderﬁnds the deep features while reducingthe spatial size. The decoder performs inverse operation of mappingthe features to image translation. The difference between SegNet andU-Net is the type of skip connections ( Badrinarayanan et al., 2017; Ronneberger et al., 2015). The U-Net shares more information thanSegNet, where only the max-pooling indices are shared in the futurelayers. The skip connections help to achieve additional stability in spa-tial conﬁguration. In U-Net, the learnable up-sampling is implementedusing transposed convolutional layer. DeepLabv3+ employs the AtrousSpatial Pyramid Pooling (ASPP) module of four different depth-wiseseparable dilated convolutional layers. Using different dilatation ratesin each convolutional layer, this module probes features at multiplescales of the input feature map that helps in maintaining a more diversespatial conﬁguration.Table 2compare the models based on trainableparameters. InTable 2theﬁrst column represents the model name,whereas the second column denotes the number of trainable parame-ters. U-Net requires the highest number of trainable parameters whencompared to other reported models. DeepLabv3+ requires only23.96Mparameters whereMrepresents a million.3.5.1. Generator architecture in CGANIn CGAN settings, we use U-Net (Ronneberger et al., 2015), SegNet (Badrinarayanan et al., 2017), U-Net-ResNet-50, SegNet-ResNet-50,and DeepLabv3+ as G. The U-Net, SegNet, U-Net-ResNet-50, SegNet-ResNet-50, and DeepLabv3 + -ResNet-50 are named as C-U-Net, C-Segnet, C-U-Net-ResNet-50, C-SegNet-ResNet-50, and C-DeepLabv3+when trained in adversarial settings. Using different models as G alsohelps study the variations in the performance for the same D model.The end layer of each decoder is a convolutional layer, after whichtanh activation is applied. The output result is in the range of [ −1,1], which is then shifted and scaled to [0,1].3.5.2. Discriminator (patch wise penalizer)There are altogether six convolutional layers in D with 4 × 4 ﬁlter size and 2 × 2 stride. First, it takes the source and the target image asinput. Then, it uses sigmoid activation toﬁnd a 24 × 32 patch of binary output. The size of the patch can be altered based on the query as givenin (Li and Wand, 2016)a n d(Isola et al., 2017).4. Results and evaluationFor the evaluation of semantic segmentation tasks, the IOU and DICEscore are reliable evaluation metrics to target. We use mean IOU score tocompare our performance. The mIOU score can be calculated usingEq.(5).
Fig. 3.Percentage of number of pixels for each class in the overall data.
Table 2Trainable parameters for different models.Network ParametersU-Net 36.96 MSegNet 29.46 MU-Net-ResNet-50 38.05 MSegNet-ResNet-50 34.88 MDeepLabv3+ 23.96 MH.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
194IOU¼Ytrue∩Ypred
Ytrue∪Ypredð7ÞWe use the sugar beet images from the Bonn ﬁeld captured at various growth stages in our experiments (Chebrolu et al., 2017). The weeds are mainly dicot and grass. The dataset consists of RGB and NIR images witha three-channel corresponding segmentation mask. The samples aretaken over two months, starting from the zero growth stage. Eachnetwork is trained on last stage, it is then tested on unseen data fromthe same and the remaining three stages. We use 2000 images for train-ing, and the rest of the data is used for testing. The data distribution isshown inTable 3.F r o mTable 3it is clear that only 17% data is used fortraining, which is a tiny fraction compared to the 80% testing data.The results inTable 4andTable 5refer to classically and adversarially trained models, respectively. Both tables compare modelperformance based on mIOU score. Together with comparing trainingschemes, the results also give a bigger picture of the models' overallperformance for different plant stages. The C-U-Net results in higher ac-curacy and IOU score for unknown stages. It reports about 10% overallimprovements in mIOU score compared to other models.Deep networks like U-Net and SegNet with ResNet-50 backboneperform better with classical training. This shows that deeper networksare not suitable for G in CGAN. As SegNet has similar architecture as U-Net, the adversarial training in C-SegNet should work similarly to C-U-Net, but U-Net comes with an advantage of enriched skip connectionsthat makes U-Net preserve more information compared to SegNet.The DeepLabv3+, when trained in a classical training fashion, showssigniﬁcantly good results for the trained stage, reporting an 84.5%mIOU score. U-Net-ResNet-50 is the second better performance modelin the classical setting.SegNet trained classically works well for the crop at the thirdstage, reporting a 77% mIOU score. As the plants grow, the mIOUscore tends to increase. The score improves from 32 to 84% fromtheﬁrst stage to the last stage. This increasing trend shows that asthe crop and weeds grow, they exhibit more resemblance with thetraining data.Fig. 4andFig. 5show results from four different stages, with groundtruth and predicted segmentation mask. The weeds are red, green rep-resents the crop, and everything else is classi ﬁed as background. C-U- Net has better overall performance. However, its result is not suitablefor the earliest growth stage. The model has dif ﬁculty in detecting weeds/crop at an early stage and produces noise instead. This is becauseof high variations in soil texture as depicted in the 1st row of Fig. 4and Fig. 5.Additionally, the difﬁculty of differentiating weeds and crops at anearly stage may be the leading cause of poor performance. The soil var-iation can be addressed using artiﬁcial augmentation. Changing bright-ness and adding noise in theﬁeld images can tackle the soil variationseffectively. However, due to the very close resemblance of weed andcrop at this stage, it is hard to differentiate between crop and weed atthe early stage.The white rectangular boxes inFig. 5show some of the false detection (noise). These false detections are the cost of usingCGAN as G does not see the true data distribution, making it un-stable. To help G to get stable, we use noise removing techniqueon random images. Furthermore, we examine that randomly blur-ring images help us achieve better- generalized parameters for the model, making G stable. We refer to blurring as Z input in Eq. (2) and Eq.(3). The drop-out layers also help in approaching a morestable G.There is an interesting fact about the dataset. Some of the pixels in-ground truths are either unlabelled or mislabelled. The connectingweeds are labelled such that the area in between the weeds is marked
as weed class. These pixels belong to the background class. Moreover,some of the stem pixels are labelled as background. The stem pixels ei-ther belong to weed or crop but not the background. Likewise, somepixels are labelled as background, but their actual class is either cropor weed. Nearly all our models detect most of the unlabelled ormislabelled pixel classes in the ground truth. Comparing false labelleddata with good predictions raise a concern about results being betterthan reported. The yellow rectangular boxes in Fig. 4andFig. 5repre- sent the visuals of our claim.The changing moisture levels of soil may lead to late germinationof crops. InFig. 5(t) and (x) the green rectangular boxes highlightsome of the late germinated plants. Our model detects them as aweed because of mislabelled data. The labels in the dataset do notaccount for the late germinated plants. It makes the weed and cropsegmentation confusing and complex for the model to learn. Usingalready labelled data with improvements can lead to more notice-able results.Also, one can see the performance drop of the model in different sets,multiple factors might be hurting the performance, soil moisture,weather, camera settings, Ground Sample Distance (GSD), and time ofthe day images were collected being some of them. GSD normalization,classical domain adaptation (Histogram matching), and droppingsamples to balance the data can be some valuable techniques thatmight elevate the model performance.Table 3Number of images from each stage used in the test, train, and validation sets.Stage Data splitTrain Test ValidationI 0 2679 0II 0 3423 0III 0 2673 0IV 2000 500 300
Table 4Comparison of classically trained models for different growth stages (mIOU).U-Net SegNet U-Net-ResNet-50 SegNet-ResNet-50 DeepLabv3+mean Intersection Over Union (mIOU)Stage I0.32960.3267 0.3128 0.3113 0.3240Stage II0.54380.4792 0.3895 0.4029 0.4063Stage III 0.71990.77190.7412 0.7022 0.7496 Stage IV 0.8088 0.8319 0.8403 0.7924 0.8455
Table 5Comparison of adversarially trained models for different growth stages (mIOU).C-U-Net C-SegNet C-U-Net-ResNet-50 C-SegNet-ResNet-50 C-DeepLabv3+mean Intersection Over Union (mIOU)Stage I0.34340.3357 0.3356 0.3336 0.3344 Stage II0.54880.3886 0.4757 0.4501 0.4967 Stage III0.75000.6756 0.6594 0.5837 0.6183 Stage IV0.84720.7355 0.6905 0.6335 0.7023H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
195Fig. 4.Comparison of models at four different growth stages: 1st row has RGB ﬁeld images, 2nd row is NIR mask, and the rest of the rows follow the sequence of Ground truth, U-Net, SegNet, U-Net-ResNet-50, SegNet-ResNet-50, and DeepLabv3+.H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
196Fig. 5.Comparison of models at four different growth stages: 1st row has RGB ﬁeld images, 2nd row is NIR mask, and the rest of the rows follow the sequence of Ground truth, C-U-Net, C-SegNet, C-U-Net-ResNet-50, C-SegNet-ResNet-50, and C-DeepLabv3 + .H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
1975. ConclusionThis paper studied the impact of using different generators inCGAN. We performed a semantic segmentation task for weed de-tection using classical and adversarial learning. We examined dif-ferent stages of the sugar beet crop by training our models ononly one stage. We discuss possible improvements in a datasetthat can lead to better results. Using U-Net in CGAN settings leadsto more generalized weights than simple classical training, whichis evident from the quantitative and qualitative analysis of resultscollected over different stages. The C-U-Net outperforms in detect-ing the growth variations in the dataset. Compared with other net-works, C-U-Net shows improvements of 10% in mIOU score. Theresults can be improved if the dataset has equal distribution of allclasses and the mislabelled data is corrected. The other way is touse artiﬁcial augmentation to enhance the dataset, which willboost accuracy. There is a need to improve the penalizer. PatchGANloss is not very helpful for training deep models. In particular, ournext research will focus on enhancing D and weed detection foran imbalance class data with a focus on late germination of theplant.CRediT authorship contribution statementHaﬁzS a m iU l l a h :Conceptualization, Methodology, Software,Writing–original draft.Abdul Bais:Supervision, Validation, Conceptu-alization.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgementsThis work was supported by the Natural Sciences and EngineeringResearch Council of Canada Discovery Grant (RGPIN-2021-04171) enti-tled "Crop Stress Management using Multi-source Data Fusion.References
Asad, M.H., Bais, A., 2020. Weed detection in canola ﬁelds using maximum likelihood clas- siﬁcation and deep convolutional neural network. Inform. Proc. Agricult. 7, 535 –545. https://doi.org/10.1016/j.inpa.2019.12.002 . Badrinarayanan, V., Kendall, A., et al., 2017. Segnet: a deep convolutional encoder- decoder architecture for image segmentation. IEEE Trans. Pattern Anal. Mach. Intell.39, 2481–2495.Breiman, L., 2001.Random forests. Mach. Learn. 45, 5 –32. Chebrolu, N., Lottes, P., et al., 2017. Agricultural robot dataset for plant classi ﬁcation, local- ization and mapping on sugar beet ﬁelds. Int. J. Robot. Res. 36, 1045 –1052. Chen, L.C., Zhu, Y., Papandreou, G., Schroff, F., Adam, H., 2018. Encoder-decoder with atrous separable convolution for semantic image segmentation. In: Ferrari, V.,Hebert, M., Sminchisescu, C., Weiss, Y. (Eds.), Computer Vision –ECCV 2018. Springer International Publishing, Cham, pp. 833 –851.Di Cicco, M., Potena, C., et al., 2017. Automatic model based dataset generation for fast and accurate crop and weeds detection. IEEE/RSJ International Conference on IntelligentRobots and Systems (IROS). IEEE, pp. 5188 –5195. Haug, S., Michaels, A., et al., 2014. Plant classiﬁcation system for crop/weed discrimination without segmentation. IEEE Winter Conference on Applications of Computer Vision.IEEE, pp. 1142–1149.Isola, P., Zhu, J., et al., 2017.Image-to-image translation with conditional adversarial net-works. Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 1125 –1134. Karacan, L., Akata, Z., et al., 2016. Learning to generate images of outdoor scenes from at-tributes and semantic layouts. arXiv preprint. arXiv:1612.00215. Karras, T., Laine, S., et al., 2019. A style-based generator architecture for generative adver-sarial networks. Proc. IEEE Conf. Comput. Vis. Pattern Recognit. 4401 –4410. Li, C., Wand, M., 2016.Precomputed real-time texture synthesis with markovian genera-tive adversarial networks. The European Conference on Computer Vision (ECCV).Springer, pp. 702–716.Lottes, P., Hoeferlin, M., et al., 2016. An effective classiﬁcation system for separating sugar beets and weeds for precision farming applications. IEEE International Conference onRobotics and Automation (ICRA). IEEE, pp. 5157 –5163. Lottes, P., Behley, J., et al., 2018. Fully convolutional networks with sequential informationfor robust crop and weed detection in precision farming. IEEE Robot. Automat. Lett. 3,2870–2877.McCool, C., Perez, T., et al., 2017. Mixtures of lightweight deep convolutional neural net-works: applied to agricultural robotics. IEEE Robot. Automat. Lett. 2, 1344 –1351. Milioto, A., Lottes, P., et al., 2017. Real-time blob-wise sugar beets vs weeds classi ﬁcation for monitoringﬁelds using convolutional neural networks. ISPRS Annals of Photo-grammetry, Remote Sensing and Spatial Information Sciences. IV-2/W3, pp. 41 –48. https://doi.org/10.5194/isprs-annals-IV-2-W3-41-2017 .Milioto, A., Lottes, P., et al., 2018. Real-time semantic segmentation of crop and weed forprecision agriculture robots leveraging background knowledge in cnns. IEEE Interna-tional Conference on Robotics and Automation (ICRA). IEEE, pp. 2229 –2235. Mortensen, A.K., Dyrmann, M., et al., 2016. Semantic segmentation of mixed crops using deep convolutional neural network, in. Proceeding of the International Conferenceof Agricultural Engineering (CIGR).Radford, A., Metz, L., et al., 2015. Unsupervised representation learning with deepconvolutional generative adversarial networks. arXiv preprint. arXiv:1511.06434. Rammy, S.A., Abbas, W., Hassan, N.U., Raza, A., Zhang, W., 2019. CPGAN: conditional patch-based generative adversarial network for retinal vessel segmentation. IETImage Process. 14, 1081–1090.Reed, S., Akata, Z., et al., 2016. Generative adversarial text to image synthesis. Interna-tional Conference on Machine Learning, pp. 1060 –1069. Regmi, K., Borji, A., 2018.Cross-view image synthesis using conditional gans. Proc. IEEEConf. Comput. Vis. Pattern Recognit. 3501 –3510. Rezaei, M., Harmuth, K., et al., 2017. A conditional adversarial network for semantic seg- mentation of brain tumor. International Medical Image Computing and ComputerAssisted Intervention (MICCAI) Brainlesion Workshop. Springer, pp. 241 –252. Ronneberger, O., Fischer, P., et al., 2015. U-net: convolutional networks for biomedical image segmentation. International Conference on Medical Image Computing andComputer-assisted Intervention. Springer, pp. 234 –241. Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scaleimage recognition. arXiv preprint. arXiv:1409.1556. Takikawa, T., Acuna, D., et al., 2019. Gated-SCNN: gated shape cnns for semantic segmen- tation. Proceedings of the IEEE International Conference on Computer Vision,pp. 5229–5238.Ullah, H.S., Asad, M.H., Bais, A., 2021. End to end segmentation of canola ﬁeld images using dilated u-net. IEEE Access 9, 59741 –59753.https://doi.org/10.1109/ACCESS. 2021.3073715.Wang, T.C., Liu, M.Y., et al., 2018. High-resolution image synthesis and semantic manipu-lation with conditional gans. Proc. IEEE Conf. Comput. Vis. Pattern Recognit.8798–8807.Wang, A., Xu, Y., et al., 2020.Semantic segmentation of crop and weed using an encoder-decoder network and image enhancement method under uncontrolled outdoor illu-mination. IEEE Access 8, 81724 –81734. Zhang, H., Xu, T., et al., 2017. StackGAN: text to photo-realistic image synthesis withstacked generative adversarial networks. Proceedings of the IEEE InternationalConference on Computer Vision, pp. 5907 –5915.H.S. Ullah and A. Bais Artiﬁcial Intelligence in Agriculture 6 (2022) 189 –198
198