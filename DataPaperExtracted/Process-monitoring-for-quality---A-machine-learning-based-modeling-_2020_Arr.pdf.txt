Process-monitoring-for-quality—A machine learning-based modeling forrare event detection
Carlos A. Escobara,*, Ruben Morales-Menendezb, Daniela Maciasb
aGlobal Research&Development, General Motors, Warren, MI, USA
bTecnol/C19ogico de Monterrey, School of Engineering and Sciences, Monterrey NL, Mexico
ARTICLE INFO
Index Terms:Quality controlManufacturing systemsMachine learningFeature eliminationModel selectionUnbalanced binary dataDefect detectionABSTRACT
Process Monitoring for Qualityis aBig Data-driven quality philosophy aimed at defect detection through binary classiﬁcation and empirical knowledge discovery. It is founded on Big Models, a predictive modeling paradigm that appliesMachine Learning, statistics and optimization techniques to process data to create a manufacturingfunctional model. Functional refers to a parsimonious model with high detection ability that can be trusted byengineers, and deployed to control production. A parsimonious modeling scheme is presented aimed at rarequality event detection, parsimony is induced through feature selection and model selection. Its unique ability todeal with highly/ultra-unbalanced data structures and diverse learning algorithms is validated with four casestudies, using the SupportVector Machine,Logistic Regression,Naive Bayesandk-Nearest Neighborslearning algo- rithms. And according to experimental results, the proposed learning scheme signi ﬁcantly outperformed typical learning approaches based on the l
1-regularized logistic regression and Random Undersampling Boostinglearning algorithms, with respect to parsimony and prediction.
1. IntroductionBecause of ever increasing customer demands, manufacturers are in aconstant competition for improving quality and reliability in products.These attributes are achieved by correct execution of the manufacturingprocess and by an effective process monitoring system.Several researchers have worked on this problem. A framework formultiple release problems using a two-step fault detection procedure andfault removal process was proposed by Ref. [ 1]. A state-of-the-art report of the most important papers in this domain is presented in Refs. [ 2]. A similar project, but based on Bayesian Nets was suggested by Ref. [ 3]. A data mining approach for defect analysis and prevention of industrialproducts in Ref. [4].Process Monitoring for Quality (PMQ) is aBig Data-driven quality philosophy aimed at defect detection through binary classi ﬁcation (good/bad) and empirical knowledge discovery through feature/modelinterpretation [5]. It is a blend of process monitoring and quality controlfounded onBig Models (BM); a predictive modeling paradigm that usesMachine Learning(ML), statistics and optimization techniques, Fig. 1,t o develop a manufacturing functional model: ﬁnal model(classiﬁer). Functional, refers to a parsimonious classi ﬁer with a high detection ca- pacity; parsimony facilitates information extraction, induces model trust[6] and promotes explainability [7]; desired characteristics for a classi-ﬁer to be deployed into production,Fig. 2.PMQhas the potential to solve engineering intractable problems e.g., detecting defects that are notdetected byStatistical Process Controlmethods.Constant product innovation forces manufacturing engineers tolaunch production systems even without a comprehensive understandingof the process. Therefore, the huge amount of process data (e.g., signals)is used to create hundreds or even thousands of features i.e., hyper-dimensional feature spaces. Which frequently include irrelevant andredundant ones [8,9] that tend to hamper the learning process [ 10]. Since most manufacturing companies generate only a few Defects Per Million of Opportunities (DPMO), rare quality event detection is one of themodern intellectual challenges posed to this industry. From ML perspective, manufacturing-derived data sets for binary classi ﬁcation of quality tend to be highly/ultra-unbalanced (minority class count <1%). The problem with these data structures is that the learning algorithmsmisclassify most of the minority class as the majority class, e.g., fail todetect. Since it is harder for the algorithm to learn the minority class.The unbalanced classiﬁcation problem is taking a lot of attention fromtheMLcommunity [11,12
]. Extensive efforts and signiﬁcant progress have been made in recent years to address this intellectual challenge. Themain research efforts are broken down in ﬁve categories: (1)
* Corresponding author.E-mail address:carlos.1.escobar@gm.com(C.A. Escobar).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100034Received 24 January 2020; Received in revised form 31 May 2020; Accepted 1 July 2020Available online 6 August 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 7 (2020) 100034over/under-sampling methods, (2) cost sensitive learning, (3)kernel-based learning, (4) active learning and (5) novelty detection.Where oftentimes the inherent ad hoc data manipulation (e.g., over/-under sampling, miss-classiﬁcation costs, using small pools of data)approach lacks of theoretical foundation and principles to guide thedevelopment of systematic methods that can be ef ﬁciently generalized. In this context, learning from the original data set is identi ﬁed as further research work [12]. Basically, the authors encourage the research com-munity to investigate the development of algorithms/methods that couldlearn from whatever highly/ultra unbalanced data is presented withoutmanipulations. Comprehensive reviews of unbalanced learning are pre-sented in Ref. [12,13].The feature explosion (hyperdimensional feature spaces) combinedwith high conformance production rates (unbalanced binary data) aretwo of the most important challenges of Big Datainitiatives in manufacturing that inspired the development of PMQ-Learning (PMQ-L). TheHybrid Feature Selection and Pattern Recognition (HFSPR) approach proposed in this paper with the capacity to effectively learn from theoriginal data set and to identify the driving features.The proposal is a novel parsimonious modeling scheme, that can beapplied to train theNaive Bayes(NB), SupportVector Machine(SVM),K- Nearest Neighbor(KNN),Logistic Regression(LR) andFisher Linear Discriminant(FLD) learning algorithms. The goal is a rare quality eventdetection through parsimonious modeling, where parsimony is inducedthroughFeature Selection (FS)andModel Selection (MS). The proposed scheme combines the Hybrid Correlation and Ranking- based(HCR) andReliefFﬁltering algorithms to select the most relevantfeatures. To boost parsimony, a set of nested Candidate Models (CM)is developed and then, thePenalized Maximum Probability of Correct Decision(PMPCD) MScriterion is applied to select theﬁnal model. It can be virtually applied to any learning algorithm in which complexity isdeﬁned by the number of features in the model. Its universal applicabilityis demonstrated by analyzing four highly/ultra-unbalanced data setsusing different learning algorithms. Empirical results demonstrate itscapacity ofﬁnding a good quality solution after creating a few CM. This paper is organized as follows: A review of the theoretical back-ground is in section2. Section3describes thePMQ-Lframework, fol- lowed by four binary classiﬁcation empirical studies in section 4.A comparative analysis is given in Section 5. Finally, section6concludes the research.2. Theoretical background2.1. FS methodsFSis the procedure of choosing a subset of good features by elimi-nating irrelevant and redundant ones. From a given data set, evaluatingall possible combinations (2
n) becomes a NP-hard problem as the numberof features grow up [14]. The advantages ofFSmethods are: (1) enable the learning algorithm to train faster, (2) reduce over-complexity of amodel making it easier to understand/interpret, (3) improve general-ization (prediction on unseen data) if the right subset is chosen, and (4)
Fig. 1.Big data–big models.
Fig. 2.PMQ-based quality control.C.A. Escobar et al. Array 7 (2020) 100034
2prevent over-ﬁtting. TheFSmethods broadly fall into 3 classes:ﬁlters, wrappers, and embedded [15].Filter(preprocessing) methods are applied before the learning processto eliminate irrelevant/redundant features. Relevant features areselected using a predeﬁnedﬁtness function, which can be based on de-pendency, distance, consistency or discriminative capacity [ 16]. Computed scores are used to determine the ﬁtness of each feature and are compared with a relevance threshold to select a subset of relevant fea-tures [10]. These methods select features independently of the learningalgorithm.In contrast,wrappersmethods use the learning algorithm as a black-box to evaluate the relative performance of a feature subset [ 17,18]. In this procedure, a set of candidate features are input to the learning al-gorithm, and the prediction performance is used as the objective functionto evaluate the feature subset. Although wrappermethods tend to be computationally intensive, they perform better than ﬁlters, due to the bias induced by the algorithm [15].Inembeddedmethods, theFStask is integrated as part of the learningprocess. TheLASSOwith theL
1penalty andRidgewith theL 2penalty are the most common approaches in this category. They shrink irrelevant andtrivial features to zero or almost zero respectively [ 19,20]. Hybrid approaches have been proposed to take advantage of theparticular characteristics of each method [ 21]. These approaches mainly focus on combiningﬁlteralgorithms with eitherwrappersor regulariza- tion to solve the scalability problem, induce parsimony, and to achievethe best possible learning performance. The basic idea is to break downtheFSproblem into several stages, namely feature ranking,correlation-based feature elimination, and prediction optimization.2.2. ReliefFReliefFranks features according to their discriminative capacity [ 22]. It searches forknumber of nearest neighbors of the same class ( hits), as well as of the different class (misses) to evaluate theﬁtness of each feature. This procedure is repeated mtimes, which is the number of randomly selected instances. Features are weighted and ranked based onthe average of the distances of all hitsand allmisses[23,24].kis a hyperparameter (user-speciﬁed) that provides protection to the effect ofnoise and controls the locality of the estimates. Once all features havebeen ranked, they are selected based on
τ, a signiﬁcance threshold pro- posed by Ref. [22]. Features with an estimated weight below
τare considered irrelevant and therefore eliminated. The proposed limits for
τ
are 0< τ/C201=ﬃﬃﬃﬃﬃﬃ ﬃ αmp[23]; where αis the probability of accepting anirrelevant feature as relevant.ReliefFdoes not eliminate redundant features.2.3. Correlation-based redundancy measureThePearsonproduct-moment correlation coefﬁcientðr
xyÞis used as a measure of redundancy between two random variables [ 25]. It is a measure of strength of linear relationship between two variables ðx;yÞ, and it can take a range of values½/C01;1/C138. A value of 0 indicates there is no linear relationship, while an absolute value of 1 (or close to 1) indicatesstrong linear relationship, and therefore considered highly redundant.2.4. Maximum probability of Correct Decision —A measure of prediction performanceIn the context of binary classiﬁcation, a positive label refers to adefective (bad) item, whereas a negative label refers to a good qualityitem. The prediction performance of a classi ﬁer is summarized in a confusion matrix [26]. This table is used to contrast predicted labels withthe real quality characteristic,Table 1, and to compute relevant measures of classiﬁcation performance.A type-I error (α) is compared with aFPprediction; a type-II (β) error is compared with aFN[25]:
α¼FPFPþTN;β¼ FNFNþTP (1)TheMPCDis a probabilistic measure of classiﬁcation performance that is driven by detection. Since it is very sensitive to FN(missing defective items) in highly/ultra-unbalanced classes [ 5]. The
αandβer- rors are combined to estimate its score:MPCD¼ð1/C0
αÞð1/C0βÞ (2)where higher score indicates better classiﬁcation,MPCD2½0;1/C138.2.5. Penalized Maximum Probability of Correct DecisionIt is aMScriterion based onMPCDthat efﬁciently solves the posed tradeoff between model complexity and prediction ability. The basic ideaof the criterion is to induce parsimony by penalizing for extra features inthe model. The formulation includes a rewarding term based on predic-tion/detectionð1/C0
αÞð1/C0βÞand a light penalization term/C0lnðKÞ=34:55 based on the number of features (K) in the model. And hence,CMwith extra features with negligible contribution to prediction will have asmaller score (and therefore never selected). It is designed to be appliedtoML-based models in which their complexity can be de ﬁned by the number of features, e.g.,SVM,LR,NB,KNNandFLD. Insights about the development and properties of this criterion can be found in Ref. [ 27]. PMPCD¼ð1/C0
αÞð1/C0βÞ/C0lnðKÞ=34:55(3) The model with the highest estimated value on the validation set [ 28, 29] is the preferred one.2.6. Hybrid Correlation and Ranking-based (HCR) algorithmTheHCRalgorithm [30] eliminates redundant features based onPearson’s correlation coefﬁcients and theReliefF-based ranking. Features are eliminated if their correlation score is greater than δ, a user-speciﬁed threshold to determine if the discriminative information between thecompared features is redundant. The basic idea of the algorithm is tokeep thebestfeature–highest ranked–from a set of two or more highly correlated variables.2.7. PMQ-based quality controlRare quality event detection is one of the main applications of PMQ.Table 1Confusion matrix.
Predicted good Predicted badGood item True Negative (TN) False Positive (FP)Bad item False Negative (FN) True Positive (TP)Table 2Confusion matrix.4).Solution, evaluation and discussion : Although the feature combination is subject to combinatorial explosion, 1 :8/C210
16of combinations, thePMQ- Lapproach only required 83 models toﬁnd a solution. To evaluate its relative quality, an exhaustive search was performed with all the possiblecombinations–up to two features–and compared with theﬁnal model. Since noMSis performed, the training set is used to develop the modelsand the testing set to evaluate their generalization ability: (1) 54 ðC
1Þ1- feature models,Fig. 7(top); and (2) 1431ðC
2Þ2-feature models, Fig. 7(bottom).
Predicted good Predicted badGood item 9488 5Bad item 0 7C.A. Escobar et al. Array 7 (2020) 100034
3As depicted inFig. 2, a typical manufacturing process generates only afewDPMO. TheBMlearning paradigm is applied to process data todesign a classiﬁer with high detection capacity to be deployed at theplant, e.g.,ﬁnal model. Since prediction is performed under uncertainty, aclassiﬁer can commitFPandFN(i.e., fail to detect) errors. Whereas agood item predicted as bad (FP) would not generate a critical problem,since in a second level inspection/revaluation would be back in thevalue-adding process, aFNwould become a warranty event. And ifthis“miss”is a critical component/device, then it could have a seriouseconomic impact as well as on the company ’s reputation.3. PMQ-learning—a Hybrid Feature Selection and PatternRecognition approachA new parsimonious modeling method is presented, it is a ﬂexible approach with a unique ability to deal with highly/ultra-unbalanced datastructures and diverse learning algorithms to model linear and no-linearpatterns.Parsimonious modeling is induced through FSandMS,Fig. 3. Since most manufacturing systems are time-dependent, cross-validationmethods are not encouraged. Instead, time-ordered hold-out methodseems to be more appropriate. The data set should be partitioned intothree subsets (i.e., training, validation, testing) [ 29]. And the search space is deﬁned by many candidate pairwise combinations –based on different values ofkforReliefFandδforHCR. The values ofkcan be determined by generating a logarithmically spaced vector [ 31] e.g.,p logarithmically spaced points between decades ½10
a;10b/C138, whereX¼ sumðbadÞin the training set,a¼0 andb¼log
10ðXÞ.1)Feature Selection (FS):The primary purpose of this step,Fig. 3,i st o ﬁnd a small subset of relevant features with high prediction capacity.Since the optimal combination–with respect to prediction–ofkand δis not known in advance, a hyperparameter optimization [ 32], is performed through a grid search [33,34]. Using the training set, irrelevant and redundant features are eliminated by applying ReliefF andHCRalgorithms. Features are ranked based on ReliefFand irrel- evant features are eliminated based onτ–signiﬁcance threshold. From the selected features, high correlations are eliminated based onδ. These two steps are performed in aﬁlter-type approach, where the learning algorithm is not considered.ACMis developed with the subset of features at each pairwisecombination, and the predictiveﬁtness of each model is evaluated toﬁnd theincumbent(best) model–highest validationMPCD. The features in the incumbentmodel are selected and their associated ReliefFranking recorded.2)Model Selection (MS): Although a good feature subset has been ob-tained in theFSstep,Fig. 3, their individual relevance in the model isnot known. To evaluate their prediction-contribution, a set of nnested CMis developed–nis the number of selected features–using the top 1 feature in theﬁrstCM, the top 2 features in the second one, and soon. Finally, thePMPCDof eachCMis estimated and used as aMS criterion to induce parsimony–solve the tradeoff between modelcomplexity and prediction ability. Theﬁnal modelis the one with the highestPMPCDscore.3)Generalization evaluation: To obtain an unbiased estimation (or closestto) of the generalization ability of theﬁnal model, the prediction on testing set (unseen data) should be reported in a confusion matrix, laststepFig. 3.The outcome of this method is a parsimonious classi ﬁer with high detection ability, as the analytical tools used are aimed at analyzinghighly/ultra-unbalanced data structures. Parsimony does not onlyimprove the learning ability, but also helps to identify the few drivingfeatures of the system.In concordance withPMPCD, the application of the proposed learningscheme is limited to classiﬁers in which their complexity is mainlydeﬁned by the number of features in the model, e.g., SVM,LR,NB
,KNN andFLD. Therefore, learning algorithms such as neural networks,random forest, etc. are out of the scope.4. Case studiesFour highly/ultra-unbalanced data sets were analyzed using the SVM, LR,NB, andKNNlearning algorithms [35]. First, a full analysis is pre- sented using theNBalgorithm on a manufacturing-derived data set.Then, the same procedure was applied to three different data sets. Due tospace limitations, only results were reported.4.1. Case study 1The data used for this analysis
1was collected from theUltrasonic Metal Weldingof battery tabs for theChevrolet Volt[5], a well-controlled process that only generates a fewDPMO. It contains 54 features with a binary outcome (good vs badquality), its data structure is ultra-unbalanced–35badwelds out of 40,231 examples (0.09%). Threedata sets are created following a time-ordered hold-out validationapproach: training set (18,495, including 20 bad), validation set (12,236 - 8bad), testing set (9500 - 7bad).
Fig. 3.PMQ-Lframework.
1Privately stored in the Manufacturing Research Lab of General Motors.C.A. Escobar et al. Array 7 (2020) 100034
41)Feature Selection (FS): The search space contains 70 pairwise combi-nations; forReliefF, 7 logarithmically spaced points were de ﬁned– k¼f1;2;3;4;7;12;20g–and forδ, 10 even spaced points–δ¼ f0:50;0:55;…;0:95g. At each combination, feature relevance wasdetermined by comparing their weights with
τ¼0:0329–calculated with an
αof 0.05, andmof 18,495.Fig. 4shows prediction results (validationMPCD) and number of features of eachCM.According to the grid search results, the incumbentmodel has an estimated validationMPCD¼0:8728,Fig. 4(top), and 13 features, Fig. 4(bottom). This model was developed with these relevant hyper-parameters:k¼2;τ¼0:0329;δ¼0:50. AllCMfailed to detect one of the defective items; therefore, the β¼0:125 in all models. And they are basically competing over the
αerror. As displayed by the plots, as thenumber of low quality features included in the model increases, the
α
error increases too. The proposed hyperparameter optimization allowedtoﬁnd a good subset of features.2)Model Selection (MS): To induce parsimony, 13 CM were created, andPMPCDwas used as aMScriterion toﬁnd theﬁnal model.The basic idea is to evaluate the individual prediction-contribution of each ofthe 13 selected features,Fig. 5shows the selected features and theirassociated ranking.CM-1 contains top-1 feature (25),CM-2 contains the top-2 features (25, 5) and so on.According to theMScriterion,CM-2 should be selected, with an estimatedPMPCD¼0:8501,Fig. 6. This analysis, discloses that only twofeatures are needed to approximate the pattern in the manufacturingsystem, since the prediction improvement is not signi ﬁcant if more fea- tures are added to theﬁnal model.Based on exhaustive search, no single-feature model has bettergeneralization ability. Whereas six 2-feature models outperformed theﬁnal model,Table 3summarizes their relevant information. However,Fig. 4.CMinformation (denoted by line intersections).
Fig. 5.Features in theincumbentmodel.Fig. 6.CMusing the top 13 features.3).Generalization evaluation: The testing set (9500 - including 7 bad) was used to estimate the unbiased generalization ability of the ﬁnal model, recognition rates are summarized in the confusion matrix, Table 2. This model includes only two features (25, 5), and it correctly detected theseven defective items with onlyﬁveFPs–MPCD¼0:9995. It is clear that the system can be justiﬁed by only these two features.
Table 3Top models (*PMQ-Lsolution).
Model index Features MPCD FN1032 26, 33 0.9998 21035 26, 36 0.9998 2413 9, 26 0.9997 31042 26, 43 0.9997 31044 26, 45 0.9997 31045 26, 46 0.9996 4Final 5, 25 0.9995 5*C.A. Escobar et al. Array 7 (2020) 100034
5evaluating all possible combinations toﬁnd an optimal solution rapidly becomes unfeasible as the feature space grows up.The optimal solution could be deﬁned as the model with the least number of features and the highest prediction ability. In this case study, ifthere is no other model with an estimated MPCD>0:9998, the optimal solutions would be model indexes 1032 and 1035 Table 3. However, since the number of combinations is huge, a model with more featuresmay have greaterMPCD. Oftentimes due to the tradeoff between modelcomplexity and prediction ability, there is no straight forward optimalsolution, this tradeoff should be solved.Although thePMQ-Ldid notﬁnd the optimal solution, it did promptlyﬁnd a good quality solution–a model that efﬁciently addresses the posed tradeoff.Fig. 7shows the relative location of the solution –ﬁnal model.4.2. Case study 2Sensorless Drive Diagnosis [36], the data set contains 48 numericalfeatures (plus the class label), which are extracted from motor current[37], the motor has good and defective components. This results in 11different classes with different conditions. The goal of this study is todetect only class one. This data set is highly unbalanced (58509 instances- including 5319 class 1) and it is split as follows: training set (33,409 -including 3100), validation set (12,100 - 1319), and testing set (13,00–900). Since the data set does not provide speci ﬁc information aboutthe meaning/name of each feature, they are referred to as feature 1,2, … 48.TheKNNlearning algorithm is applied with the same number ofneighbors, used for theReliefFalgorithm. The search space contains 70pairwise combinations; forReliefFandKNN, 7 logarithmically spaced points are deﬁned–k¼f1;2;4;7;13;24;45g–and forδ, 10 even spaced points–δ¼f0:50;0:55;…;0:95g. Feature relevance is determined bycomparing their weights with
τ¼0:0245. Theincumbentmodel with 3 features (9,11,21) is found withk¼45 andδ¼0:85. Then, threeCMare created (feature 9, features 9,11, features 9,11,21) and the PMPCDis used as aMScriterion to select theﬁnal model. According to the criterion, the 3
rdmodel should be selected (PMPCD¼0:9573),ﬁnal modelhas an estimated testingMPCD¼0:9857.Although the search space of the application of the KNNin this data set is subject to combinatorial explosion –8:7/C210
17(248/C23100)–a good quality solution is found after creating 73 models.4.3. Case study 3Statlog (Landsat Satellite) [38], the original data set contains 36features with 7 classes. Only class 1 is considered –class 1 vs all–, the data set is split as follows: training set (4435 - including 1072), validationset (1000–293), and testing set (1000–168).TheSVMlearning algorithm is applied to the same search spacedescribed inCase Study 2(70 pairwise combinations). Feature relevanceis determined by comparing their weights with
τ¼0:0672. Theincum- bentmodel with 8 features (33,21,25,13,5,14,2,30) is found with k¼7 and. Then, 8 CM are created and the PMPCDis used as aMScriterion to select theﬁnal model. According to the criterion, the 8
thmodel should be selected (PMPCD¼0:8748),ﬁnal modelhas an estimated testing MPCD¼0:9976.4.4. Case study 4Occupancy Detection [38,39], the data set contains 5 features. Togenerate an unbalanced data structure, one out of 10 instances labeled asclass 1 are included in the data set (index 1, 10, 20, etc.) and theremaining nine eliminated, all 0 class are included. The data set is split asfollows: training set (6587 - including 173), validation set (1791 - 98),and testing set (7908 - 205).TheLRlearning algorithm is applied to the same search spacedescribed inCase Study 2. Feature relevance is determined by comparingtheir weights with
τ¼0:0551. TheOptimal Classiﬁcation Threshold with respect to MPCDalgorithm is used to obtain the classiﬁcation threshold of eachCM[30]. Theincumbentmodel with 2 features (feature 3 - CO
2, feature 1 -Humidity) is found withk¼1 andδ¼0:50. Then, 2 CM are created, according to the criterion, the single-feature model should beselected (PMPCD¼0:9681),ﬁnal modelhas an estimated testing MPCD¼0:9879.4.5. DiscussionMost ofBig Datainitiatives are subject to feature combinatorial ex-plosion, a situation that gets aggravated by the hyperparameter tuningprocess e.g.,Case Study 2. One of the main challenges, is to detect whichfeatures actually contain discriminative information, since oftentimesmost of them are either irrelevant or redundant. In the four case studies, agood quality solution is found after creating only a few models –with respect to the feature combination space. All these solutions effectivelyselected only the most relevant features to model the pattern, since eachﬁnal modelis virtually separating the testing data ( MPCD/C251).5. Comparative analysisTo evaluate the performance of the PMQ-Lmodeling scheme, twoFig. 7.MPCD exhaustive search in the 1-feature and 2-feature spaces.C.A. Escobar et al. Array 7 (2020) 100034
6comparative analyses are presented: (1) vs. the l 1-regularizedLRlearning algorithm [19], this algorithm induces parsimony, therefore the goal ofthis analysis is to evaluate howPMQ-Lsolves the posed tradeoff between complexity and parsimony; (2) vs. the Random Undersampling Boosting (RUSBoost)learning algorithm [40], this algorithm is designed specif-ically to analyze highly/ultra unbalanced data structures, but it does notinduce parsimony, therefore prediction analysis is the main goal of thiscomparative study.Five highly/ultra-unbalanced data sets are analyzed, Table 4.I naddition to the data sets of case studies 1,3,4, two publicly available datasets are also included in this analysis.2First, for each data set 70 CM are developed and theﬁnal modelselected using thePMQ-Lmodeling scheme. Then, following the learning approach in Ref. [ 30], the l
1-regularizedLRlearning algorithm is applied to the same data set todevelop 100 CM by varying the regularization values ( λ)[41]. Finally, the Akaike information criterion [42] is used to select theﬁnal model. The solutions of the two learning approaches are evaluated in terms of thenumber ofCMdeveloped, the number of features in the ﬁnal modeland their generalization ability. Results are presented in Table 5. For repro- ducibility purposes the hyperparameters values of the ﬁnal modelsare included.According to experimental results, four (data sets 1,2,4,5) out of ﬁve solutions of the proposed learning scheme outperforms the l
1-regularized LR-based solutions, since they have a lesser number of features andexhibit better generalization performance. In the third solution, thetradeoff is solved differently. Theﬁnal modeldeveloped byPMQ-Lex- hibits slightly lower generalization ability (0.8033 vs 0.8268), but itincludes signiﬁcantly smaller number of features (10 vs 21). Thiscomparative analysis is graphically presented in Fig. 8. A second comparative analysis with a widely used learning algorithmin the category of over/under-sampling method is presented. The RUS- Boostis a combination of random undersampling and AdaBoost[44] speciﬁcally designed to analyze highly/ultra unbalanced data structures.Random undersampling is applied to the majority class to balance theratio between minority and majority classes, then AdaBoostis applied to the balanced-subset to build a model. For this analysis, the RUSBoostis applied to the 4 data sets
3of the case studies presented in Section 4. With a search space of 10–150 trees, the testing results of each of the ﬁnal modelsare summarized inTable 6.In this analysis, highly/ultra unbalanced data structures that exhibitboth, linear and non-linear patterns are analyzed. According to empiricalresults, thePMQ-Ldeveloped better predictive models than RUSBoostin all data sets,Table 6. Since in most of the cases, because of the hyper-dimensional feature spaces, the pattern is not known in advance, there-fore it is recommended to apply all the learning algorithms that can behandled byPMQ-Ltoﬁnd the best one.Table 4Data sets information, positive class count in parenthesis.
Data Description Features Training set Validation set Test set Ratio (overall)1 UMW 54 18,495 (20) 12,236 (9) 9500 (7) 0.09
b
2 Statlog (class 1) 36 4435 (1072) 1000 (293) 1000 (168) 23.82a
3 Credit Card Fraud 29 200,000 (385) 40,000 (52) 44,807 (55) 0.17b
4 Occupancy Detection 5 6587 (173) 1791 (98) 7908 (205) 2.92a
5 HTRU2 8 12,000 (1484) 2000 (91) 3898 (64) 9.16a
aHighly-unbalanced.
bUltra-unbalanced The preprocessing information can be found in Refs. [ 43].
Table 5Solutions by data set, comparative analysis 1.
Data setl 1-regularizedLR PMQ-LFeatures Hyperparameter ( λ) TestingMPCDFeatures Hyperparameters ( k,δ) TestingMPCD1 42 7.168e-07 0.8567 2 12,0.65 0.99562 34 4.721e-05 0.9929 8 1,0.95 0.99763 21 2.738e-05 0.8268 10 1,0.50 0.80334 5 9.839e-06 0.6784 1 1,0.50 0.98795 7 5.520e-05 0.8727 4 1,0.90 0.8758
Fig. 8.Comparative analysis,MPCDand number of features by data set.
2Since the data set of case study 2 does not exhibit a linear pattern (classescannot be separated by a linear classi ﬁer), it is not included in this analysis.3Since theRUSBoostcan handle linear and non-linear patterns the four datasets are analyzed.C.A. Escobar et al. Array 7 (2020) 100034
76. ConclusionsA newHybrid Feature Selection and Pattern Recognition method with the capacity to learn from the original data set was proposed, PMQ-L.I ti s aimed at detecting rare quality events through parsimonious modeling.Although the proposed approach does not guarantee to ﬁnd the optimal solution (if it exists), it did promptlyﬁnd a good quality solution. Its unique ability to deal with highly/ultra-unbalanced data structures anddiverse learning algorithms to model linear and no-linear patterns wasdemonstrated in the 4 case studies, which also exhibited its capacity ofselecting the driving features of the system.According to empirical results, the proposed modeling scheme out-performed widely-used modeling approaches based on the l
1-regularized logistic regression and theRandom Undersampling Boostinglearning al- gorithms in terms of parsimony, generalization ability and the number ofcandidate models needed to develop a good solution.Since rare event detection and information extraction are two of themain modern challenges in the application of MLacross industries, the proposed modeling approach can be generalized to other domains –as supported by the case studies–facing the same challenges.In this research, hyperparameters (k,δ) optimization was performed with respect to validationMPCDonly. If it is considered that greaterseparability between classes is preferred for generalization purposes.Future research along this path, can focus on formulating the modelassessment task as a two objective optimization problem. In whichseparability would be the secondﬁtness attribute to be considered tofurther discriminate between two or more competing models.Credit author statementCarlos Alberto Escobar: Developed the method and led all the sectionsand analysis. Ruben Morales-Menendez: Helped to run the comparativeanalyses and contrast the contribution of the method. Daniela MaciasArregoyta: Collaborated with the literature review of the paper.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.References
[1]Kumar V, Mathur P, Sahni R, Anand M. Two-dimensional multi-release softwarereliability modeling for fault detection and fault correction processes. Int J ReliabQual Saf Eng 2016;23(3):164–78. [2]He Y, Liu F, Cui J, Han X, Zhao Y, Chen D, Zhou Z, A Z. Reliability-oriented designof integrated model of preventive maintenance and quality control policy with time-between-events control chart. Comput Ind Eng 2019;(129):228 –38. [3]Cai B, Zhao Y, Liu H, Xie M. A data-driven fault Diagnosis methodology in three-phase inverters for PMSM drive systems. IEEE Trans Power Electron 2016;32(7):5590–600.[4]Kang S, Kim E, Shim J, Cho S, Chang W, Kim J. Mining the relationship betweenproduction and customer service data for failure analysis of industrial products.Comput Ind Eng 2017;106:137–46. [5]Abell JA, Chakraborty D, Escobar CA, Im KH, Wegner DM, Wincek MA. Big datadriven manufacturing—process-monitoring-for-quality philosophy. ASME J ManufSci Eng Data Sci Enhanc Manuf 2017;139(10) .[6]Ribeiro MT, Singh S, Guestrin C. Why should I trust you?: explaining the predictionsof any classiﬁer. In: Proc of the 22nd ACM SIGKDD int Conf on knowledgeDiscovery and data mining; 2016. p. 1135 –44. [7]Gunning D. Explainable artiﬁcial intelligence (XAI).Defense Advanced Research Projects Agency; 2017.[8]Shao C, Paynabar K, Kim T, Jin J, Hu S, Spicer J, Wang H, Abell J. Feature selectionfor manufacturing process monitoring using cross-validation. J Manuf Syst 2013;10 . [9]Wuest T, Weimer D, Irgens C, Thoben K-D. Machine learning in manufacturing:advantages, challenges, and applications. Prod Manuf Res 2016;4(1):23 –45. [10]Yu L, Liu H. Feature selection for high-dimensional data: a fast correlation-basedﬁlter solution. In: ICML, vol. 3; 2003. p. 856 –63. [11]Haixiang G, Yijing L, Shang J, Mingyun G, Yuanyue H, Bing G. Learning from class-imbalanced data: review of methods and applications. Expert Syst Appl 2017;73:220–39.[12]He H, Ma Y. Imbalanced learning: foundations, algorithms, and applications. JohnWiley&Sons; 2013.[13]Shukla S, B.S. R. Online sequential class-speci ﬁc extreme learning machine for binary imbalanced learning. Neural Network 2019;119(235 –248). [14]Chandrashekar G, Sahin F. A survey on feature selection methods. Comput ElectrEng 2014;40(1):16–28.[15]Ng A. On feature selection: learning with exponentially many irrevelant features astraining examples. In:Proc of the15thint Conf on machine learning. MIT, Dept. of Electrical Eng and Computer Science; 1998. p. 404 –12. [16]De Silva AM, Leong PH. Feature selection. In: Grammar-based feature Generationfor time-series prediction. Springer; 2015. p. 13 –24. [17]Deng H, Runger G. Feature selection via regularized trees,. In: Int J Conf on neuralnetworks; 2012. p. 1–8.[18]Weston J, Mukherjee S, Chapelle O, Pontil M, Poggio T, Vapnik V. Feature selectionfor SVMs. In: NIPS, vol. 12; 2000. p. 668 –74. [19]Tibshirani R. Regression shrinkage and selection via the LASSO,. J Roy Stat Soc B1996:267–88.[20]Ng A. Feature selection L1 vs L2 regularization and rotational invariance. In: Proc. Of the21
stint Conf on machine learning. ACM; 2004. p. 78. [21]Wang F, Yang Y, Lv X, Xu J, Li L. Feature selection using feature ranking, correlationanalysis and chaotic binary particle swarm optimization. In: 5
thint Conf on software Eng and service science; 2014. p. 305–9. [22]Kira K, Rendell L. The feature selection problem: traditional methods and a newalgorithm. In: AAAI, vol. 2; 1992. p. 129 –34. [23]Robnik-/C20Sikonja M, Kononenko I. Theoretical and empirical analysis of ReliefF andRReliefF. Mach Learn 2003;53(1–2):23–69. [24]Kononenko I. Estimating attributes: analysis and extensions of RELIEF. In: EuropeanConf on machine learning. Springer; 1994. p. 171 –82. [25]Devore J. Probability and statistics for engineering and the sciences. CengageLearning; 2015.[26]Fawcett T. An introduction to ROC analysis. Pattern Recogn Lett 2006;27(8):861–74.[27]Escobar CA, Morales-Menendez R. Process-Monitoring-for-Quality —a model selection criterion. SME Manuf Lett 2018;15:55 –8. [28]Arlot S, Celisse A. A survey of cross-validation procedures for model selection. StatSurv 2010;4:40–79.[29]Friedman J, Hastie T, Tibshirani R. The elements of statistical learning, vol. 1.Berlin: Statistics Springer; 2001 . [30]Escobar CA, Morales-Menendez R. Machine learning techniques for quality controlin high conformance manufacturing environment. Adv Mech Eng 2018;10(2):1 –12. [31] The MathWorks Inc. Logspace. [Online]. 2017. Available: www.mathworks.com/h elp/matlab/ref/logspace.html . [32]Kuhn M, Johnson K. Applied predictive modeling, vol. 26. Springer; 2013 . [33]Bergstra J, Bengio Y. Random search for hyper-parameter optimization. J MachLearn Res 2012;13(2):281–305. [34]Claesen M, De Moor B.“Hyperparameter search in machine learning. In: The XImetaheuristics int conf; 2015 . [35]Murphy K. Machine learning: a probabilistic perspective. MIT press; 2012 . [36] Lichman M. UCI machine learning repository. 2013 [Online]. Available: http://arch ive.ics.uci.edu/ml.[37]Paschke F, Bayer C, Bator M, M €onks U, Dicks A, Enge-Rosenblatt O, Lohweg V. Sensorlose zustandsüberwachung an synchronmotoren. In: Proc23th. Workshop computational intelligence, vol. 5; 2013. p. 211. Dortmund, Germany . [38] Dheeru D, Karra Taniskidou E. UCI machine learning repository. 2017 [Online].Available,
http://archive.ics.uci.edu/ml. [39]Candanedo LM, Feldheim V. Accurate occupancy detection of an of ﬁce room from light, temperature, humidity and CO
2measurements using statistical learning models. Energy Build 2016;112:28 –39.Table 6Solutions by data set, comparative analysis 2.
Case studyRUSBoost PMQ-LFeatures Hyperparameter ( trees,learningrate) TestingMPCDFeatures Hyperparameters ( k,δ) TestingMPCD1 54 40,0.1 0.9980 2 2,0.50 0.99952 48 140,0.1 0.8838 3 45,0.85 0.98573 36 140,0.1 0.8377 8 7,0.95 0.99764 5 10,0.1 0.9883 1 1,0.50 0.9885C.A. Escobar et al. Array 7 (2020) 100034
8[40]Seiffert C, Khoshgoftaar TM, Van Hulse J, Napolitano A. Rusboost: a hybridapproach to alleviating class imbalance. IEEE Trans Syst Man Cybern Syst Hum2009;40(1):185–97.[41] T. M. Inc. Lasso. 2011 [Online]. Available: https://www.mathworks.com/help/ stats/lasso.html.[42]Burnham KP, Anderson DR. Model selection and multimodel inference: a practicalinformation-theoretic approach. Springer Science &Business Media; 2003.[43]Escobar CA, Morales-Menendez R. Process-monitoring-for-quality —a model selection criterion for l1-regularized logistic regression. Procedia Manuf 2019;34:832–9.[44]Freund Y, Schapire RE, et al. Experiments with a new boosting algorithm. In: icml,vol. 96. Citeseer; 1996. p. 148–56.C.A. Escobar et al. Array 7 (2020) 100034
9