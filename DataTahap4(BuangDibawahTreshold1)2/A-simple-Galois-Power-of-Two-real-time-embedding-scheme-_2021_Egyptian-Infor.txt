This paper describes how a simple novel Galois Power-of-Two (GPOW2) real-time embedding scheme is used to improve the performance and accuracy of downstream NLP tasks. GPOW2 computes embeddings live on the fly (real time) in the context of target NLP tasks without the need for tabulated pre- embeddings. One excellent feature of the method is the ability to capture multilevel embeddings in the same pass. It simultaneously computes character, word and sentence embeddings on the fly. GPOW2 has been derived in the context of attempts to improve the performance of the SWAM Arabic morphological engine, which is a multipurpose tool that supports segmentation, classification, POS tag- ging, spell checking, word embeddings, sematic search, among other tasks. SWAM is a pattern-oriented algorithm that relies on morphological patterns and POS tagging to perform NLP tasks. The paper demon- strates how GPOW2 led to improvements in the accuracy of POS tagging and pattern matching, and accordingly the performance of the whole engine. The accuracy for pattern prediction is 99.47% and is 98.80% for POS tagging.

Distributional semantics and word embedding techniques [27,25,26] are currently the norm in performing deep learning NLP tasks of any kind. Although these techniques may be traced back to the work of Bengio et al. [3], they only became popular and attracted attention after the surprising success of the Word to Vector (Word2Vec) approach proposed by Mikolov [27,25,26]. Following the success of the Word2vec approach, many other superior approaches have been proposed, including Elmo [32], UMLfit [19] and BERT [8], which capitalized on the power of Bidirectional LSTM models, Transfer Learning, transformer and attention techniques.

The power of GPOW2 is illustrated by applying the method to a real-life problem in Arabic morphology. The SWAM morphological engine [17] grew over the years from a simple morphological ana- lyzer to a multipurpose linguistic tool that addresses spelling cor- rection, sematic search, sentence embedding, plus many other tasks. The algorithm is pattern-oriented and applies a traditional probabilistic language model to capture the contexts of words and morphemes. The main problem of the algorithm is that it is relatively slow due to the exhaustive search process for the appro- priate pattern. Replacing traditional probabilistic language models with GPOW2 to predict word patterns and pos tags substantially improved the performance and accuracy of SWAM.

The paper is organized as follows: Section 2 below states the main problem. The literature review is given in Section 3. Section 4 introduces the novel Galois Power of Two (GPOW2) reversible text transformation and demonstrates how it works. Section 5 describes how GPOW2 embeddings are directly integrated in neu- ral models that predict NLP targets. The neural model is described in detail in this section. Results of experimentation and findings are given in section 6. Section 7 provides a detailed example illus- trating how the model works. Section 8 shows how the proposed neural model substantially improves the performance of tradi- tional SWAM. Conclusions and recommendations are presented in the final section.

Although the results and findings of Bengio et al. were very interesting, research in the area stagnated for some time, presum- ably due to the lack of appropriate computational power at the time. The next milestone in the evolution of word embeddings is the work of Collobert and Weston [6] who developed a unified multitask neural model, which simultaneously performs multiple NLP tasks, based on the distributional feature vectors computed using a deep learning model, similar to the one developed by Ben- gio et al.

The third and, by far, the most important milestone in the evo- lution of word embeddings is the work of Mikolov et al. (Mikolov et al., 2014; Mikolov et al., 2016) who developed the Word to Vec- tor (Word2Vec) approach to compute distributional word repre- sentations. The main difference between the Word2Vec and the previous approaches is the use of shallow neural models to learn the vectors, which made it feasible to train the model on large cor- pora in a short time. Another difference is the use of a symmetric context window around the focus word.

The Word2Vec approach comes in two flavors: Continuous Bag of Words (CBOW) and Skip-gram. In the CBOW version, a semi- supervised neural network is used to compute the embedding vec- tor for a given word. In the Skip-gram version, the Skip-gram model is used to predict the neighbors of a given word.

The work of Mikolov has also been extended to cover sentence embedding [26,27], document embedding [28,30,22], and charac- ter embedding [20]. This made it possible to incorporate the tech- nology into many applications and production settings. A good example is the Fast-text system that integrates character embed- ding with word embeddings [20].

Altowayan and Tao [2] used word embeddings for Arabic senti- ment analysis. They started by compiling a large Arabic corpus from various sources, based on which they computed word embed- ding vectors for both standard and dialectic Arabic. The resulting distributional word representations have then been used to train binary sentiment classifiers to determine subjectivity and senti- ment. The main motivation of the authors was to avoid manual feature engineering, which is a difficult time-consuming task. They reported that their results are slightly better than the traditional approaches based on handcrafted features.

The transformation is based on the simple idea that words and strings may be viewed as integers in a number system whose radix r is a power of two. The characters used to form words and strings are the digits of this number system. The radix r should be large enough to accommodate all characters used to form words and strings. In the current version of GPOW2, the value of r is 64 (26), which accommodates up to 64 characters. Based on this assump- tion, each word or string of length m is represented as a polynomial:

A Special case of Galois Finite Fields are Galois Prime Fields referred to as GF(p). A Galois Prime Field is a finite field with a prime number of elements, i.e. m = p, for some prime p (implying n = 1 in the above relation). In GF(p) every nonzero element has an inverse and arithmetic is done modulo p.

Galois fields have been successfully used in cryptography [23] and communication channel coding [33]. It appears from our initial experiments that Galois finite fields will have some impact in the areas of text encoding and text processing. The main attraction is that we can define two operations + and * corresponding to two abelian groups. One of the implications of Galois finite fields is the possibility of using modular arithmetic to perform embedding operations for sentences and large documents.

Because the radix is a power of 2, the transformation preserves the positions and order of characters in the binary representa- tion of the word. In case of r = 64, each character is represented by six bits. This feature is the main enabler for superimposed character-word embedding.

Over the past few years, some efforts have been made to pre- pare fully annotated Arabic data sets and treebanks. These sets dif- fer in their size, depth, and, more seriously, in the tag sets used. It is unfortunate that there is no agreement on a standard tag set for Arabic. A wide range of tag sets have been proposed [35]. The num-

As expected, replacing the exhaustive search component in the original SWAM with a neuro predictor reduced the execution time by 93.11% while maintaining the same level of accuracy. Both ver- sions of SWAM were applied to analyze the full QAC set. It took

A simple novel Galois Power of Two (GPOW2) scheme for simultaneously computing real time character, word and sentence embeddings has been introduced. The GPOW2 embedding approach is based on a simple power of two transformation that preserves the positions of characters and words in the resulting transformation of a sentence or document. This interesting feature made it possible to design parallel embedding graphs of varying complexity and depth.

To demonstrate the power of GPOW2, the method has been applied to solve a real life Arabic morphological problem related to the performance of the SWAM linguistic tool [13]. SWAM is a pattern-oriented tool that performs a variety of tasks from seg- mentation to semantic search. GPOW2 improved the time perfor- mance of SWAM by more than 90% while maintaining the high accuracy of the engine.

The test accuracies of GPOW2 predictions are 99.24% for Arabic morphological patterns and 98.80% for POS tags respectively. These results beat the SWAM Gold Standard results obtained for the same data set and are much higher than predictions produced by ALSh- gar (2012). More work is needed to extend and improve the emerg- ing GPOW2 model in different directions and apply it to other types of tasks.

ELAffendi MA. The Generative Power of Arabic Morphology and Implications: A Case for Pattern Orientation in Arabic Corpus Annotation and a Proposed Pattern Ontology. In: Alenezi M, Qureshi B (eds.), 5th Int Symp. Data Mining Applications. Advances in Intelligent Systems and Computing, vol. 753. Springer, Cham, 2018.

