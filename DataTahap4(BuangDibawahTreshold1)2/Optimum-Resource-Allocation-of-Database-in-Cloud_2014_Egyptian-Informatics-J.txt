The calibration process in the VDA had been done manually. This manual calibration process is considered a complex, time-consuming task because each time a DBMS has to run on a different server infrastructure or to replace with another on the same server, the calibration process poten- tially has to be repeated. According to the work in this paper, an Automatic Calibration Tool (ACT) has been introduced to automate the calibration process.

Cloud computing is a new generation of computing. It allows users to use computational resources and services of data cen- ters (i.e., machines, network, storage, operating systems, appli- cation development environments, application programs) over the network to deploy and develop their applications [1]. The main feature of cloud computing is providing self-service pro-

The rest of this paper is organized as follow; the related works are described in Section 2. The calibration problem in the VDA is described in Section 3. The proposed automatic calibration tool for DBMS query optimizer is discussed in Sec- tion 4. In Section 5, the optimization problem in the VDA will be handled. In Section 6, the proposed GPSO algorithm will be discussed. In Section 7, the ACT and the GPSO algorithm evaluation results are introduced. In Section 8, the paper is concluded; also a brief outlook into the future work is given.

The virtual design advisor employs a white-box approach for modeling the performance of the DBMS [8,9]. On the other hand, the black-box approach for performance modeling has been used in [13] to drive an adaptive resource control system that dynamically adjusts the resource share of each tier of a multi-tier application within a virtualized data center. The two approaches; black-box and white-box have been used to solve the resources provisioning problem for DBMS on the top of IaaS cloud [20].

For example, without loss of generality, with three shared re- sources (CPU, memory, I/O), that is, M = 3, an allocation of 50% CPU, 30% memory, and 25% I/O to VM1 results in the vector R1 = [0.5, 0.3, 0.25]. We assume that each workload Wi has a relevant cost under resource allocation Ri. This cost is represented by:

static environments [10]. In the real world, however, many applications are non-stationary optimization problems; they are dynamic, meaning that the environment and the character- istics of the global optimum can change timely. Several suc- cessful PSO algorithms have been developed for dynamic environments. One of these algorithms is fast multi-swarm optimization algorithm (FMSO) [24]. It uses two types of swarm; one to detect the promising area in the whole search space and the other swarm is used as a local search method to find the near-optimal solutions in a local promising region in the search space. Another approach is used to adapt PSO in dynamic environments [25]. It is based on tracking the change of the goal periodically. This tracking is used to reset the particle memories to the current positions allowing the swarm to track a changing goal with minimum overhead [25]. Cooperative Particle Swarm Optimizer (CPSO) has been introduced for employing cooperative behavior to significantly improve the performance of the original PSO algorithm [26].

resource allocation for the VMs. It implements a search algo- rithm, such as greedy search and dynamic programming, for enumerating candidate resource allocation. The VDA uses a greedy search algorithm, which is based on iterating until no performance gain can be incrementally achieved [8,9]. Each iteration, a small fraction of a resource is de-allocated from the VM that will get hurt the least and allocated to the VM that will benefit the most. The greedy algorithm makes the decisions of increasing and decreasing the resources allocated to VMs based on the estimated cost of the given workloads.

The worker is the second module in the ACT. It runs in a guest VM. The worker module receives its inputs from the controller module and sends its output back to the controller. It uses the calibration database for executing the input queries.

The experiment described here uses PostgreSQL 8.4.8 database system installed in a machine with Core2 Duo T5870 2.00 GHz processor, 4 GB memory, and CentOS 5.5 operating system. The virtual machine monitor used was Xen [28], which is an open source virtualization platform. Xen-based para-virtual- ization has been used to improve the hypervisor performance when it maps resources directly into the guest operating system [5]. Amazon EC2 is based on Xen virtualization, and thus, this experiment setup is similar to a cloud computing environment.

The GPSO algorithm performance is evaluated by varying the swarm size in two search space boundaries, [0.01% -10%] and [0.1%-10%], to choose the feasible swarm size. The first search space contained 100 points, whereas the second contained 1000 points. Each point in search space represents a value of the share parameter, which is used as a controller of the greedy heuristic algorithm. The GPSO cost improvement over greedy

In this experiment, random TPC-H workloads are generated to test the improvement of overall performance. Twenty que- ries are generated by the same method described in the [8]. Each workload consists of a random combination of between 10 and 20 workload units. A workload unit can be either 1 copy of TPC-H query Q17 or 66 copies of a modified version of TPC-H query Q18 [8,9]. Each VM runs one workload. Each

The combination of the GPSO algorithm with any profiling technique for random workloads characteristics in terms of re- source consumption (e.g., CPU, Memory, and I/O) gives the perception for the intensivity of workloads. This perception can guide the cloud provider to allocate an appropriate

amount of resources to incoming workloads. The provider can arrange the workloads over multiple pools based on the intensively of workloads or use cloud bursting to maintain strict SLA even when some incoming workloads are heavily CPU-intensive. Where cloud bursting means that an applica- tion deployment model in which an application runs in a pri- vate cloud or data center bursts into a public cloud when the demand for computing capacity spikes [35]. The advantage of such hybrid cloud deployment is that an organization only pays for extra compute resources when they are needed [35]. On the other hand, the GPSO algorithm can be used continu- ously to capture the randomness of the dynamic workloads variation by implementing it again periodically or on particu- lar events and changed the resource allocation periodically in each time interval.

