Semantic Dependency Analysis (SDA) has extensive applications in Natural Language Processing (NLP). In this paper, an integration of multiple classifiers is presented for SDA of Chinese. A Naive Bayesian Classifier, a Decision Tree and a Maximum Entropy classifier are used in a majority wins voting scheme. A portion of the Penn Chinese Treebank was manually annotated with semantic dependency structure. Then each of the three classifiers was trained on the same training data. All three of the classifiers were used to produce candidate relations for test data and the candidate relation that had the majority vote was chosen. The proposed approach achieved an accuracy of 86% in experimentation, which shows that the proposed approach is a promising one for semantic dependency analysis of Chinese.

A great deal of research has been done for European languages and in particular, English. Semantic parsing using statistical and machine learning methods [5] has been heavily studied. Annotated corpora such as FrameNet [8] and the proposition Bank [13] have been created.

However, for Chinese much less research has been undertaken. This is largely due to the lack of publicly available semantically annotated corpora. There are corpora such as the work done by Gan and Wong [4] on the Sinica Treebank [7] and the 1,000,000 word scale corpora created by Li et al. [10]. However, these corpora are either not publicly available or have problems for certain researchers.

Some research on partial semantic information assignment has been carried out, such as [26] and [22]. Some research, such as [22], looks at semantic role labeling (SRL). SRL can be thought of as a sub-problem of semantic dependency analysis (SDA) as it is only concerned with the semantic roles between arguments and the main verb. Research in automatic methods for determining semantic relations for a full sentence in Chinese is limited.

This paper presents an integrated multi-classifier approach for semantic depen- dency analysis. It combines three classifiers, Naive Bayesian, Decision Tree and Maximum Entropy, to achieve good accuracy on headword-dependent semantic re- lation assignment. The classifiers are combined using a majority wins selection mechanism and produces better results than using a single classifier.

The rest of this paper will continue as follows. In section 2 the description of dependency grammar and SDA are given. In section 3, related work in SDA will be shown. In section 4, an overview of the integrated multi-classifier approach and each of the individual classifiers will be described. In section 5, information about the semantic tag set and the corpus will be given. In section 6, experimental results are shown. Finally, in section 7 concluding remarks are made and future work discussed.

This section will introduce what a dependency grammar is and what semantic de- pendency analysis involves. First, a brief introduction to dependency grammar will be given. Then, an introduction to semantic dependency analysis is given. Finally, particular aspects of Chinese and semantic dependency analysis are discussed.

Generally, Semantic Dependency Analysis (SDA) builds a dependency tree with the optimal semantic relationship for the parent node (headword) and child node (dependent) between which there is a dependency link according to DG. In semantic dependency grammar, the word that is able to best represent the meaning of the headword-dependent pair is chosen as the headword. The headword of a sentence represents the main meaning of the entire sentence and the headword of a headword- dependent pair represents the main meaning of the pair. In a compound constituent the headword inherits the headword of the head sub-headword-dependent pair and headwords of other sub-headword-dependent pairs are dependent on that headword.

Li et al. [10] built a large corpus annotated with semantic knowledge using de- pendency grammar structure. The selections of semantic relations were taken from HowNet, which is a Chinese lexical database. A computer-aided tagging tool was developed to assist annotators in tagging semantic dependency relations. Manual checking and semiautomatic checking were carried out. Auto-tagging this type of semantic information still has not been completed and is the goal of the current research.

From the above related research, the main difference of SRL and SDA can be sum- marized as follows. SRL only focuses on the main verb of the sentence. SDA focuses on the complete sentence. SRL only describes the relationships between the main verb and its modifiers or complements. SDA determines any relationship for two words or chunks only if there is a dependency link according to DG between them. In SRL, the main verb becomes the unique headword. Since only one headword exists, the structure can be considered as a branch of a tree. In SDA, the headword

is not limited to the main verb. Under the main headword (the main verb), there can be many other sub headwords. SRL is based on predicate-argument structure. SDR is based on semantic dependency structure, which means DG is the supporting grammar for SDA.

The important part is in choosing the selection method. Through testing, which will be shown in the experimental results section, we found that a simple majority wins approached works well. This approach outperformed a probabilistic selection method and the individual classifiers.

The NBC is widely used in machine learning due to its efficiency and its ability to combine evidence from a large number of features [11]. It is a probabilistic model that assigns the most probable class to a feature vector. Even though it relies on an assumption that the features are independent and this is not normally true, it has been shown to generally do well in classification [16]. In text classification it is widely used in spam detection, such as SpamBayes 7 .

Maximum Entropy modeling creates a model based on facts from the underlying data while trying to stay as uniform as possible [1]. As a classifier it uses the principal of maximum entropy to estimate the probability distribution of a model based on observed events. It achieves state-of-the-art results and often performs as well or better than Support Vector Machines. It is also extremely useful for NLP, as [15] shows, and has been widely adopted in the NLP field. For a more in depth explanation we refer the reader to [15].

In this paper, the semantic dependency relation tag set was imported directly from HowNet 8 . HowNet is a Chinese thesaurus that shows the lexical knowledge in semantic network. It is getting more popular in NLP research because semantic hierarchy is constructed between Chinese and English [9].

Semantically labeled corpora for Chinese are still scarce, due to the fact that the corpora that have been created are rarely made publicly available. Because of this, the Penn Chinese Treebank 5.0 was chosen as original data for this research. A portion (4000 sentences) of the Treebank was manually annotated with headwords and semantic relations according to DG, which means that only the words between which there is a unique dependency link will be assigned a relation. This resulted in 27,000 words with 24,487 semantic relations.

In this paper, we presented a multi-classifier approach for analyzing semantic depen- dency in Chinese. It integrated three different classifiers, Naive Bayesian, Decision Tree and Maximum Entropy. We tested two different methods for selecting the best results: majority wins and probabilistic selection. Both methods gave improvements over the individual classifiers. The simpler majority wins selection method outper- formed the probabilistic method.

We have shown that classification based methods are capable of assigning se- mantic dependency relations between headword-dependent pairs. The fusion of in- formation brought together when dealing with multiple classifiers helps to overcome weaknesses in any of the individual classifiers. This allows for a higher accuracy to be obtained.

From these promising results there is a lot of future work that can be done. First, the corpus needs to be enlarged. This will allow us to see if probabilistic selection can perform better than the majority wins approach when it is able to determine the precision based on a larger amount of data. Next, we need to look at new classifiers, specifically support vector machines. In addition to new classifiers we can also explore new features and combining classifiers using different feature sets in the multi-classifier approach.

