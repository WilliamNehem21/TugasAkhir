The fairness evaluation method described in section 2 can be used in simulation of any trust management system, given that the utilities of agents are defined. To demonstrate that the proposed method can be used in practice for consideration of fairness, two case studies are presented. The first study (section 3) concerns the evaluation of a new reputation algorithm, while the second study (section 4) is devoted to the robustness of a simple reputation algorithm to discrimination. The paper concludes with a discussion concerning the relationship of fairness and trust.

In order to consider fairness in the evaluation of trust management system, it is necessary to define fairness. In this section, the concept of fairness will be discussed and defined in an abstract manner. The next two sections will demonstrate how fairness can be used in practice to evaluate reputation systems and algorithms.

If a user decides to post a negative feedback in spite of the negative incentives involved, it is probable that the user is very dissatisfied with his contractor. For that reason, his negative feedback should perhaps have a higher impact on reputation than an ordinary positive feedback. We shall refer this method as stressing of negative feedbacks.

In the design of the simulator, we had to make a decision about a sufficiently re- alistic, yet not too complex model of the auction system, of user behavior, and of the reputation system. We chose to simulate the reputation system almost totally faithfully (the only simplification is that we use only positive and negative feed- backs). The behavior of a user is also quite realistic: the user can take into account reputation when choosing a business partner. The user also decides whether she wishes to report or not, depending on the type of feedback. Users can cheat in feedbacks, as well as in transactions. Users may also use transaction strategies that depend on the history of their individual interactions, as well as on the reputation value of the other participant.

is used by some strategies; and the probability of cheating c that is used by some strategies. We can specify a number of agents and every agent can have distinct parameters. However, we usually partition all agents into two sets that have the same parameters, called the honest and dishonest agents.

The server can compute reputations using all available feedbacks and using any implemented algorithm. The results of the simulation include: the reputations of individual agents and the total payoffs (from all transactions) of all agents. The payoffs are affected by the way the reputation system works: for example, if agents send very few feedbacks, reputations will be random and the payoffs of good agents will drop. The simulator allows to check whether the implemented reputation algo- rithms are effective.

To verify the concept of implicit and stressed negative feedbacks, we have im- plemented the reputation algorithm described above. User feedback behavior was simulated in one of two ways: either all agents always posted feedback truthfully (perfect feedback ), or agents posted feedback according to the following rule (poor feedback ). If the feedback was positive, an agent would post it with probabil- ity r+ = 0.6, and if the feedback was not positive, the agent would post it with probability r+ = 0.05. All feedbacks were always true, if they were posted. The parameters of poor feedback behavior were observed from the analysis of traces obtained from a Polish Internet auction house. The traces contained over 650000 committed auctions made by a sample group of 10000 buyers over a period of six months. While it is clear that the posting behavior of real users is more complex, this approximation is sufficient to evaluate the possibility of using implicit feedback in a simple reputation algorithm.

All experiments were conducted using the Monte-Carlo method. We present average results from 10 simulation runs, together with 95% confidence intervals of results (intervals for which with probability of 95%, all results would belong to the interval. Confidence intervals were obtained using the t-Student distribution). The outcomes of every simulation were the average payoffs and the Gini coefficients of honest and dishonest agents.

In this case study we consider an open market, where a number of agents trades some goods. The majority of agents on the market belong to one group, called the old agents. Newcomers constantly enter the market, but they usually form a minority group, called the new agents. Old agents usually trade fairly with other old agents, but they also usually cheat new agents. New agents usually act fairly (at least initially). They can leave the market any time if their losses are too high, or - after a time - they may join the group of old agents.

The first simulation used an extreme case when the detection of discrimination is simple. Old agents always discriminate against new agents, and as a result, the total payoff of all agents decreases noticeably. However, discrimination can be more subtle. What if old agents discriminate only sometimes? What if new agents cheat sometimes, too? In such a case, how do we evaluate whether discrimination was prevented by the reputation system?

The two case studies have demonstrated that fairness can be taken into account in the evaluation of trust management systems in a laboratory setting. Considering fairness leads to different design of reputation algorithms, and fairness is a useful criterion in the evaluation of reputation systems. However, the consideration of fairness is based on a premise that whenever possible, trust management systems should be designed to provide fair treatment of their users. We would like to go one step further and open a discussion of the questions: should fairness be a goal of trust management systems? If so, how can this goal be achieved in practice?

system goal of fairness. If traders in an Internet auction house can only increase their reputation as a result of fair behavior (for example, if all transactions are supervised by impartial, omniscient arbiters), then the trust management system will be perfectly fair - the distribution of agent utilities should be affected only by legitimate factors. Of course, the real challenge lies in the design of such trust management systems.

be an emergent property that depends on the fairness of individual agents. The trust management system should provide incentives for fair agent behavior. Notice that this is also possible in the case of a trust management system that does not use centralized control and global information. For example, a Peer-to-peer file sharing system that combats free-riding using trust management can also aim for fair treat- ment of users, while it is impossible in such a system to maintain global information and use centralized control. Global information is only used in a laboratory for the evaluation of such a system. The system itself works based on local information only.

It has been pointed out in [1] that trust has yet another dimension: that of norms, values, or principles of human agents. If an agent perceives that another agent has high principles and acts in accordance with social norms, she is more likely to trust him. This observation shows that trust can be affected by the fair behavior of agents.

