(a) Scale: At least two elements of the set of images views have different scales. (b) Occlusion: Is the concept that two objects that are spatially separated in the 3D world might interfere with each other in the projected 2D image plane. (c) Orienta- tion: The images views are rotated with respect to each other.

During the process of searching for documentation on 2D modeling, a lot of work was found that addresses the early fea- ture detection and the posterior image matching. Most of the early implementations developed seemed to work well under certain limited image condition. The real challenge for those authors was to achieve true invariant feature detection under any image such (a) Consistency, detected positions should be insensitive to the noise, scale, orientation, cluttered, illumina- tion. (b) Accuracy, should be detected as close as possible to the correct positions and features; (c) Speed, should be faster enough.

The second attempts towards digital image recognition were limited to the identification of corners and edges. The beginnings of feature detection can be tracked with the work of Harris and Stephen and the later called Harris Corner Detector. Harris was successful in detecting robust features in any given image. But since it was only detecting corners, his work suffered from a lack of connectivity of feature-points which represented a major limitation for obtaining major level descriptors such as surfaces and objects. Almost a decade after the Harris Detector was published; a new corner detector algo- rithm called FAST (Features from Accelerated Segment Test) was presented.

To overcome these limitations, a new class of image match- ing algorithm was developed simultaneously. These algorithms are known as texture-based algorithms because of their capa- bility to match features between different images despite of the presence of textured backgrounds and lack of planar and well-defined edges. One of the first attempts towards this novel approach was undertaken by David Lowe.

Lowe [15] presented SIFT (Scale Invariant Feature Trans- form) for extracting distinctive invariant features from images that can be invariant to image scale and rotation [10,15,16]. Then it was widely used in image mosaic, recognition, retrieval and etc. After Lowe, Ke and Sukthankar used PCA (Principal Component Analysis-SIFT) to normalize gradient patch instead of histograms [12]. They showed that PCA-based local descrip- tors were also distinctive and robust to image deformations. But the methods of extracting robust features were still very slow. Bay et al. SURF (speeded up robust features) and used integral images for image convolutions and Fast-Hessian detector [13]. Their experiments turned out that it was faster and it works well.

SIFT (the Scale Invariant Feature Transform) consists of four major stages: (a) scale-space detection, (b) keypoint localiza- tion, (c) orientation assignment and (d) keypoint descriptor. The first stage used difference-of-Gaussian (DOG) function to identify potential interest points [15], which were invariant to scale and orientation. DOG was used instead of Gaussian to improve the computation speed [15,16,19].

when choosing a feature detection method, make sure which most concerned performance is. The result of this experiment is not constant for all cases. Changes of an algorithm can get a new result, find the nearest neighbor instead of KNN or use an improved RANSAC.

SIFT detected the most feature points on image. This was expected because of the large amount of texture of this image (visually appreciated). On the other hand, SIFT had the lowest detection with image 5 corre- sponding to the first view of the Haddock Boat. This also confirm the hypothesis of SIFT performing at the highest levels when tested on textured images. This can be also checked by looking at the results for the other images.

SURF is also a textured based matching algorithm but it seemed to get confused in textured images with illumina- tion changes. Because of this, it did not have its best per- formance with this pair. The image with most features detected by SURF was the corresponding second one for SIFT because of less feature detection occurred. This proves that SIFTS and SURF being texture-based algo- rithms, do not perform well when tested on planar images.

It was concluded that F-SIFT has the best overall perfor- mance above SIFT and SURF but it suffers from detecting very few features and therefore matches. It is recommended that the properties of this algorithm to be improved by creating a new implementation provided with a matching component. It is nec- essary to improve F-SIFT by increasing the amount of features it can detect. But special care should be taken to preserve the robustness of the algorithm and avoid the detection of useless features. Future research in this area should focus on testing

the accuracy of the algorithms in detecting a single object with- in a scene. Many of the new arising needs in photogrammetric address this problem and this work could be a first step towards a more in depth study. And, also improve semantic techniques with F-SIFT extraction feature algorithm to remove semantic gap between high-level semantic perception of human and low-level features of an image.

