Question answering communities (QAC) are nowadays becoming widely used due to the huge facilities and flow of information that it provides. These communities target is to share and exchange knowledge between users. Through asking and answering questions under large number of categories.

Unfortunately there are a lot of issues existing that made knowledge process difficult. One of those issues is that not every asker has the knowledge and ability to select the best answer for his question, or even selecting the best answer based on subjective matters. Our analysis in this paper is conducted on stack overflow community. We proposed a hybrid model for predicting the best answer. The proposed model is consisting of two modules. The first module is the content feature which consists of three types of features; question-answer features, answer content features, and answer-answer features. In the sec- ond module we examine the use of non-content feature in predicting best answers by using novel rep- utation score function. Then we merge both of content and non-content features and use them in prediction. We conducted experiments to train three different classifiers using our new added features. The prediction accuracy is very promising.

There are many types of social networks that you can use through internet. The types are according to the functionality each network can provides [1]. One of these social networks is collabo- rative social network. Stack Overflow (http://stackoverflow.com), Quora, Yahoo!-answers and Never are examples of this type. In stack Overflow community the question answering process is con- ducting as follows. The user can choose a category to post a ques- tion to. After posting the question the asker waits specific amount of time in order to receive the answers from the expert users. Expert users are the users whose have a great knowledge in this category or sub category of the question field. If the question does not receive any answer, the asker can set a bounty to it. A bounty is

a special reputation award given to answers. This feature was designed to motivate answerers, and help questions get the answers they deserve. Bounty awards are funded by the personal reputation of the users who offer them. Reputation is a rough mea- surement of how much the community trusts the answer author; it is earned if the answer author convinced other users that his answer is the best and this answerer knows what he is talking about. While bounty maker do not need to be the owner of a ques- tion to start a bounty on that question, only one bounty can be active on a question at once, and each user can only have up to three active bounties at once. Users must have specific reputation score to start a bounty, and at least as much reputation as the bounty amount. The bounty award will be subtracted from your reputation when the bounty is started, not when it is awarded.

If the question receives many answers, the users can give an up and down votes to both the question and answers in its answer thread. The most reputation points score is gained when the answer is up voted, it received a bounty, or it is selected as best answer .Also users can add comments to question and answers. Moreover users can set post as a favorite post. There are other activities but to be able to use them it depends on a user privilege under the community. User privilege depends on user reputation score. As an example of these privilege is to mark question or an answer as an offensive post.

In this paper, we focus on the problem of exchanging and shar- ing of the knowledge in stack overflow community. And how to ease knowledge exchange by saving asker wasted time and effort that user exerts to find a satisfied answer. By predicting the best answer to the user, which considered the main problem in ques- tion answering portals.

The work in this paper is organized as follows. In Section 2, related work and a depth analysis to different knowledge exchange approaches will be introduced. Section 3, a study for the model in Ref. [3] which is used to give a local reputation score to answers and our proposed hybrid model for predicting best answer. Sec- tion 4, presents the experiment results and discussion. In Section 5 the conclusion is presented.

In order to improve the mechanism in which question answer portals work under, we need to focus on enhancing the question answer routing approaches and finding best answer techniques... There are a lot of efforts done by researchers in this field to over-

come these problems; we are categorizing these studies to four categories: recommend right experts to a specific question, pre- dicting the best answer, finding group of collaborative experts, and direct questions to an expert. All of these are solutions in order to improve the user satisfaction rate by giving high quality answers to him. Also to minimize the loss of time that is as a result of wait- ing for the right expert to answer the question.

In Ref. [4] they tend to find the right expert to answer specific question under certain category. They proposed a hybrid model to find experts using user reputation, user authority, and user sub- ject relevance. In evaluating their model they used Yahoo! Answer platform in Taiwan. Also called Yahoo! Knowledge plus. They assign different priority to terms according to their place like if the word is in answer post, question post, or in question title. One of the main issues in their technique that they do not consider the quality of posts posted by the expert. Since HITS take only the number of posts as an indication to the authority of that user.

In Ref. [9] they focus on finding best answer in massive online open courses in which users enroll in courses and to further under- stand it they can ask and answer question in the course forum. The experiment is conducted on openHPI MOOC platform. The users used machine learning through train four classifiers. They are bag- ging, naive Bayes, MultiPerceptron, and Random Forest using user features, thread features and content features. They used as a his- torical data the questions that has at least two answers. The train- ing is performed on the answers of 416 questions.

Ref. [10] is a survey that the researchers found that there is a high correlation between posting a high quality question and get- ting a high quality answer. So they studied the features that most important in question to be found in order to get high quality answers. These question related features are tags and terms, length of the question, presence of an example that may help users to

In Ref. [11] they predict best answer using Yahoo! Answers platform historical data. The experiment is conducted on small number of questions with at least five answers they put 13 criteria and asked five Amazon Mechanical Turk workers to rate the answers based on those features. To be sure that they use high cor- related features, they match the rate given by worker with the actual assessment of the asker. The MTurk rating is from 1 to 5. They furthered their investigation by applying logistic regression classifier to their thirteen features. They found that their features are highly correlated. So they decided to use different features, the new features are extracted from answer and question post. Their data set consists from 116 questions and 575 answers.

In Ref. [12] the main aim of their research is to get the factors that affect selecting the best answer. They perform their investiga- tion on 250 stack Exchange community which considered a large scale analysis. They found that users do not evaluate the answers by any criteria to select it as a best answer. They simply depend on the cognitive heuristic such as the space of this answer or the order of it in the answer thread. As the number of answers increases the users rely more in voting on the cognitive heuristic and that lead to less reliable evaluation by users.

In Ref. [14] they propose question answering model to collabo- rative learning. Their system is for Indonesian high school as a part of E-learning process. They used Wikipedia as a free, web-based, multilingual encyclopedia. After posting a question and receiving answers and votes by the students in the system, the answers is evaluated by comparing it with Wikipedia database. And similarity percentage is provided also a link as a source of more information about that answer. Then all previous question and answers are entered into knowledge base to be a reference to new students or students that may will face the same problem in studying.

Ref. [17], this paper the first reporting large scale analysis of answerer behavior on session level. The purpose of this study is to route new questions to experts on the topic of the question. And to reuse large scale data to satisfy asker needs. They answered many questions one of those is when the user tend to answer the question. And how the answerer choose the question to answer for. The answers to these questions help in developing a more reliable question recommendation model. Yahoo! Answer platform is used in.

This large scale analysis. They found that user participate in maximum two communities or categories. And that users chose to answer the questions that face them when entering the plat- form. They applied the analysis on all users, they do not consider super active users.

Our proposed model is consisting of two modules the first mod- ule is to predict the best answers using content features. And the second one is predicting the best answer using non-content fea- tures. After that we combine them in one hybrid model to get the best prediction result.

The model is an enhancement to the work in Ref. [1]. The main difference is that the work in Ref. [1] is conducted on Yahoo- answer platform. Part of their work is to predict the best answer using non content feature which is reputation score of the user that

tion f(uij) is a sigmoid function which provides a high reward to the users with high number of answers and less reward to the users with low number of answers. All answers are under certain cate- gory. Moreover the reward is increased lesly when the number of answers exceeds a certain threshold.

4 windows, each window has different size. Window sizes are 8000, 11,000, 13,000 and 18,000 answer posts. The data is labeled with not-best answer and best answer class. We extracted the answerer information such as number of best answers answered by user in that category.

This function is responsible for measuring the degree of exper- tise of the user under certain category. As in Ref. [1] they found that most of users have little number of answers selected as best answer and the minority have large number of best answers. So the best answer score g(uij) can be driven by applying the sigmoid function as in the participation function.

In this paper, we have dealt with the problem of prediction if the given answer is going to be selected as the best answer or not based on the non-content features of this answer. We con- ducted our proposed model based on the work presented in Ref. [1].

We used precision, recall and accuracy to measure the perfor- mance of our experiments. Precision measurement which is the proportion of the true positives tps versus all the positive results (true positive tps and false positive fps). True positive means the classifier correctly classified the case as best answer class. False positive means the classifier incorrectly classified the case as best answer.

We chose to train Random Forest classifier, Logistic Regression and Naive Bayes using only content features. Random forest classi- fier with 200 trees and 10 fold cross validation while considering 5 random features. The random features are chosen by Weka framework.

The same behavior by classifiers in window size 18,000 except logistic regression is increased but slightly. And still random forest classifier beats the other. Its accuracy increased to be 88.36%. The most accurate prediction accuracy when using random forest clas- sifier. In general in random forest as increasing window size the prediction accuracy increases.

Surprisingly, we found that using stack overflow reputation score in prediction does not make the high improvement in accu- racy. And our reputation model and stack overflow reputation model give almost the same accuracy in prediction. Moreover, our reputation score was slightly higher than the stack overflow one.

We found that as we increase the window size the prediction accuracy increased and to compare hybrid model 1 HCR results with hybrid model 2 HSR, we found that in the first and the second window sizes the hybrid model 2 results is higher by 0.13%. And in the third and the fourth window sizes the opposite occurs. In gen- eral hybrid model 1 and hybrid model 2 are the same in prediction accuracy. And both of them is lower than the accuracy when we train the classifier using only the content features. The higher accu- racy we got is when using the content features only it is 88.36%.

Zhan Justin, Jones Nadia M., Purnell Michael D. Top-K algorithm for recommendation in social networking kingdoms. In: Privacy, Security, Risk and Trust (PASSAT) and 2011 IEEE Third Inernational Conference on Social Computing (SocialCom), 2011 IEEE Third International Conference on. IEEE; 2011.

