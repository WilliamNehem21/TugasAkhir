PAN** (a community that investigated text based tasks) started to add Arabic in their GI task dataset in 2017. The approaches investigated in PAN GI tasks were not specific for Arabic language in addition to the low GI accuracies achieved by their proposed solutions. There is a scarcity in the work that targets GI for Egyp- tian dialect in specific. Previous work in GI investigated several

Hussein, S. et al. [7] proposed a classical machine learning approach for the GI problem considering Egyptian dialect. The authors worked on extracting gender features from the text as semantic features. Their proposed model considered two feature vectors; i) A mixed feature vector combining embedding vector representation and semantic features employed with Random For- est classifier, and ii) N-gram feature vector used with Logistic Regression classifier. They achieved 87.6% accuracy for the GI prob- lem over a labeled dataset they retrieved from Twitter for Egyptian dialect tweets.

Kodiyan et al. [11] investigated gender detection for different languages with multiple dialects, such as English, Spanish, Por- tuguese and Arabic. They investigated the validity of CNN, and the bidirectional Recurrent Neural Network (RNN) with GRU. It turns out that RNN with GRU outperforms CNN in terms of accu- racy. As a result, they achieved 79.03%, 72.57%, 79.5%, and 71.58% GI accuracy for English, Spanish, Portuguese, and Arabic, respectively.

Miura et al. [13] proposed a model for word and character embedding. They considered RNN for the word embedding case, while CNN for character embedding case. Additionally, the authors added multiple attention layers. The embedding layer was initial- ized by a model that was trained through Twitter dataset. For the GI task, the average achieved accuracy for dialectical Arabic was 79.17%.

Deep learning has proven its success in multiple Natural Lan- guage Processing (NLP) problems with different models and archi- tectures. In [3] the authors proposed NN-based models to solve the GI problem. The models were designed based on a mixture of building units that achieved good performance in several text clas- sification tasks. For the GI problem, several architectures such as CNN, LSTM have been studied in the literature and have shown great promise in solving the problem at hand. This work first starts by the simple NN models and utilizes them to solve the GI prob- lem. Afterwards, a level of sophistication is added to the NN mod- els by combining different layers from several NN models in order to achieve higher GI accuracy. In this section, a set of NN architec- tures that were investigated in this work will be presented. For each NN architecture, the parameters have been fine-tuned to opti- mize the GI accuracy.

Gru. The GRU is a modified version of the LSTM layer. It was initially introduced by [5]. It consists of two gates, a reset gate, and an update gate. Intuitively, the reset gate determines how to com- bine the new input with the previous memory, and the update gate defines how much of the previous memory to keep around. The used recurrent activation for the gates is Sigmoid and the activa- tion function for the states (hidden and output states) is Scaled Exponential Linear Unit (SELU).

Model 1 - simple Artificial Neural Network (ANN). This is a model of a simple NN architecture that consists of fully connected layers without the recourse to complex layers such as convolu- tional or LSTM layers. It consists of an embedding layer as described in Section 3.2.1.1, followed by four fully connected layers containing 80, 40, 30, and 1 node(s), respectively. A DO was added with 0.15 dropping probability. The activation function for all the

2. The kernel sizes for the three channels are 2, 4 and 8, respec- tively. The values for the pool size and the kernel sizes are realized after several experiments to achieve the highest GI accuracy for the model. After that, for each channel, there is a bidirectional GRU layer with 40 nodes. Next, the output of the three channels is con-

catenated and fed to a set of two dense layers with 30, and 20 nodes respectively. The dense layers employed the ReLU activation function. The output layer for this model is similar to the one illus- trated in the previous models.

i.e. number of layers, number of nodes per layer and activation functions. Additionally, the input tweet is a single tweet with max- imum length of 50 characters. The GI accuracy with DO probabili- ties 0.05 and 0.1 is in range 70.1% +/-0.3%. In the experiment with

by adding GRU layer to the existing multichannel CNN model, i.e., model 6. vi) Finally, the multichannel convolutional bidirec- tional GRU model achieves the highest accuracy in GI using EDGAD, which is 91.37% for 12 concatenated tweets and 140 input length.

Since the work in this field is still in its initial phases, the exper- iments can be extended to investigate the images posted by each account. The input vector for the NN models can be extended to include language specific features for GI i.e. usage of female suf- fixes and prefixes.

