Ekman et al. [3] defined six basic emotions namly happiness, sadness, surprise, fear, and anger. He proved that human perceive these emotions regardless of their cultures. Emotions could be expressed using two orthogonal dimensions: valence, and arousal as stated by Feldman et al. [4]. He stated that each individual could express his emotions in different way than others. This difference is clearly noted when someone asked to express periodic emotions. Valence ranges from pleasant to unpleasant, and arousal ranges from calm to excited. In the proposed work, authors intend to clas- sify the input instance into its binary combinations of valence, and arousal; low/high valence or low/high arousal.

This paper is organized as follows: In Section 2, previous related works are discussed. In Section 3, the main goal of the proposed approach, and the three main components for face, EEG, and fusion recognition approaches are discussed. Evaluation of the proposed approach on DEAP benchmark, comparison against state-of-the- art approaches, discussion, and analysis for the experimental works are presented in Section 4. In Section 5, conclusion for the proposed works, and future works are presented.

Deep neural networks are explored in multi-modal fusion in several active works [31,30]. Gunes et al. [32] combined the face, and body modalities. Performance evaluation showed that bi- modal fusion outperformed the classification done using the facial modality alone. Baltrusaitis et al. [33] developed a system that inferred emotions from upper body gestures in addition to facial expressions, including head, and shoulder motions. A multi-level Dynamic Bayesian Network (DBN) models the emotional state depending on the probabilities of the gestures. It is possible to improve any module, or add features to it, but the limitation is

The adopted 3D-CNN architecture consists of six basic layers. The input layer. Then, two convolution layers that results in 3D- feature maps. Each convolution layer is followed by a max- pooling layer. The max-pooling layer down-samples the 3D- feature maps from previous layer to decrease the time needed to process volumes with huge dimensions. The last layer is a fully- connected layer to extract the final features. The dimension of the input volume is 5*32*128, where 5 is the number of consecu- tive frames which models the temporal information, 32, 128 are the height, and the width of the input frame from the EEG, and the face domains respectivly. For the EEG input, 32 represents the number of channels, and 128 represents the samples in the

frame segment. For the face input, 32, and 128 are the height, and the width of the face frame. In the proposed network, the con- volution filter has shape of 3*3*3: where 3, 3, and 3 are its height, width, and depth respectively. The number of feature maps in the first layer is 8. The max-pooling layer has a resolution of 2*2*2. The number of feature maps of the second convolution layer is set to

Koelstra et al. [34] developed a database for human emotion analysis using physiological signals (DEAP). It consists of 32- channel EEG signals, and 12 peripheral physiological signals. DEAP dataset rate is 512 Hz, and pre-processed to have a sample rate of 128 Hz. This produces a small number of samples which may affect the performance of any machine learning system to generalize to unseen samples. A data augmentation operation is employed to increase the number of samples. For augmenting the EEG signals, a Gaussian noise signal w is first generated randomly. Then, both the noise signal, and the original EEG signal are mathematically

added to create a new noisy version. The augmentation step is applied in the training phase only. During the testing phase, clean versions of the EEG signals are used. The probability density func- tion (P) of a Gaussian random noise signal w is defined by:

Every 3D-CNN system consists of input data, initial weights, 3D- CNN network. The 3D-CNN which is trained in the training part of the input data, and the initial weights to give a trained model. Then, this trained model is tested on the testing part of the input data to give the final predictions of this system. One of the most effective strategies in the deep learning techniques is to replace the initial weights with some weights from a previously trained model to make use of its learnt information, and save time training from scratch without any emotional background. This strategy is applied in the proposed fusion system which starts training with the initial weights of the EEG, and face systems. This is the main idea of transefer learning which transfers the knowledge from a task to another related task with the aim to reduce the generaliza- tion error, and achieve an improvement in the performance in terms of percentages of accuracy, and processing time.

The final scores are achieved by building a third model that works on the fusion chunks separately. The prediction of this third model is the fusion scores. A further step is added to get the maximum predictions of the three models which are considered as the final fusion scores. Ensemble learning is the art of combining a diverse

set of learners (individual models) together to improvise on the stability, and improve the overall performance of the system. There are several common types of ensemble learning techniques in machine learning including stacking [38], bagging[39], and boost- ing [40]. For the proposed work, two different score fusion meth- ods are adopted; stacking, and bagging.

Each of the proposed face, and EEG emotions systems produces a trained model, and final predictions for valence, and arousal classes. Stacking creates a model to learn from previous trained models. Then, the output score vector is determined by the Map rule which gives the hypothesis that has the maximum probability from the three trained models; EEG, face, and stacking models [41].

5 different splits reserving one split for testing. For each remaining split, a separate model is created producing four trained models. These four trained models are tested using the fifth split which produces four prediction vectors. The final fusion prediction is pro- duced by the Map rule.

The main difference between the proposed weighted sum ensemble, and grid search ensemble methods is the way of finding the best values of weights. In the weighted sum ensemble, the weight values are determined experimentally. While, in the grid search ensemble [42], the weight values are determined by select- ing the values that minimize the recognition error after starting with initial weight values. Grid search is an automatic algorithm that iterates over the different values of hyper-parameters (weights) to update them, and get the best value of hyper- parameter that minimized the error. Grid search takes the model that you would like to train, and the hyper-parameter values. Then it calculates the mean square error of the hyper-parameter values allowing you to choose the best value. Initially, it starts up with one value for the hyper-parameter, and train the model. Then, used different values to train the model. The process is continued, until all set of values of the hyper-parameter is finished. Each model produces an error, the hyper-parameter value that minimizes the error is selected.

posed work since it has the longest duration signals, and contains more multi-modal emotion signals than others [43]. For evaluating the proposed approach, the DEAP dataset which is developed by Koelstra et al. [34] is utilized to model the state of a participant by his Physiological signals. Physiological signals were recorded from 32 participants. the frontal face video data is recorded for only 22 participants. The EEG signals, and peripheral signals were recorded at a sampling rate of 512 Hz.

Tang et al. [44] has suggested two models to classify emotions from EEG signals, and peripheral physiological signals. The first model is referred as Bi-modal Deep De-noising Auto-Encoder (BDDAE) which is an extension of the original De-noising Auto- encoders (DAE). The second model is the Bi-modal-LSTM which models the temporal information in input features from data gen- erated from EEG signals, and eye movement data. Bi-modal-LSTM obtains better performance on both arousal, and valence classifica- tion tasks. Their performance for valence, and arousal are 83.82%, and 83.23% respectively. Liu et al. [48] demonstrated a Bi-modal Auto-Encoder (BDAE) to model the shared representation of EEG

For the fusion of the facial expressions, and the EEG signals for emotion recognition, two methods are proposed in this study; stacking, and bagging. One data set containing facial videos, and EEG data is used to evaluate these methods. Significant results were obtained for both single modalities. Moreover, two fusion methods outperformed the single modalities.

method is explored by applying two different bagging techniques. Moreover, the proposed fusion methods outperform single modal- ities systems. From the experimental results, the 3D-CNN network is demonstrated to extract shared representations that have better performance in terms of accuracy. Based on the conducted com- parative study, it is shown that the proposed scheme can achieve better performance in terms of arousal and valence binary classes since an end-to-end deep leaning framework is performed to map of the video data, and the EEG signals directly to emotion states instead of trying to manually extract features from the input frames, and model the temporal information in video, and EEG data.

Tadas Baltrusaitis, Daniel McDuff, Ntombikayise Banda, Marwa Mahmoud, Rana el Kaliouby, Peter Robinson, et al. Real-time inference of mental states from facial expressions, and upper body gestures, in: Proceedings of the IEEE International Conference on Automatic Face, and Gesture Recognition, and Workshops, pages 909-914, 2011..

