Anomaly detection often searches for anomalies to learn more about the underlying distribution of data. A typical attributed network has a wealth of information, such as user features and connections between users (Khan and Haroon, 2022a). Anomaly detection in attributed networks, is the process of locating nodes in a network that are significantly dissimilar from the rest, has recently attracted considerable interest owing to the growing prevalence of attributed data structures in practical applications like social networks, protein data analysis, and financial services (Khan, 2021; Koutra and Faloutsos, 2022; Kundra et al., 2022). Numerous applications have used the method, including the identification of social spammers (Hu et al., 2014), detection of financial fraud (Huang et al., 2018), and detection of intrusions (Chen et al., 2016; Khan et al., 2023).

Traditional machine learning algorithms like supervised anomaly detection approaches can only work with labelled data and typically only achieve satisfactory results when the data is well balanced (Khan and Haroon, 2022b). However, they are disproportionately affected by the issue of class imbalance. In recent years, researchers have developed a variety of methods to identify anomalies. As obtaining ground-truth anomalies is prohibitively expensive, many systems attempt to detect them unsupervised (Khan et al., 2022d). Unsupervised anomaly detection techniques often classify the least fit examples as outliers on the premise that the rest of the data is typical. Since the introduction of neural networks, various neural network-inspired techniques have been used to

solve the anomaly detection problem (Rasool and Khan, 2015). In terms of performance, autoencoders contributed to the cutting edge of anomaly detection methods. The addition of variational inference to neural networks has made it possible to use probabilistic methods, like those used by variational autoencoders to perform anomaly detection tasks more systematically by relying on reconstruction probability instead of reconstruction error (Kingma and Welling, 2014).

data distribution. When coping with sparse and noisy graph data in the real world, unregularized embedding techniques can quickly lead to unsatisfactory representation because they find an identity mapping that has degenerated to the point where the latent code space is completely unstructured. Regularization of the latent codes, wherein they are forced to conform to a predetermined distribution of the underlying data, is a common solution to this issue (Makhzani et al., 2015). There have been recent advancements in learning robust latent representation using generative adversarial-based frameworks (Donahue et al., 2016; Dumoulin et al., 2017; Radford et al., 2015).

In this study, we suggest a technique, called the adversarial regularized dual graph VAE, to detect anomalies in social networks. Our proposed model is an unsupervised method for dealing with the issue of unknown anomalies in datasets. To solve the issues of data nonlinearity and network sparsity, dual VAEs are employed. Autoencoder-based methods cannot handle variation as their representations of latent variables are deterministic mappings; however, VAE, being a stochastic generative model, may provide calibrated probabilities for doing so. Apart from employing the dual VAE for embedding the structural information and attributes into a vector representation, these dual VAEs are also used as generators for creating fake nodes. We employ the adversarial training approach to guarantee that our latent codes follow a predetermined Gaussian or uniform distribution. To develop an accurate representation of the graph, the discriminator controls the generator while producing latent variables whose distributions are closer to the actual distribution of the data.

Effective anomaly detection has several significant contributions to the field of data science and management. Anomalies, also known as outliers or novelties, are data points that deviate significantly from the norm or expected patterns. Detecting anomalies is valuable across various domains and applications, such as Fraud detection, Cybersecurity, Healthcare, and Network Monitoring. Anomaly detection enhances decision-making by highlighting deviations from normal behavior or expected patterns. It contributes to early detection, reduced risks, improved operational efficiency, better resource allocation, and the ability to respond promptly to emerging issues. In the context of data science and management, anomaly detection is a powerful tool for gaining insights from data and ensuring the reliability, security, and optimization of various processes and systems. To be more precise, this study makes the following valuable contributions:

The adversarial component we introduce to the dual variational graph autoencoder helps to regularize the encoding process by influencing the distribution of the encoded data. This part can distinguish between data obtained from the low-dimensional graph network representation and data obtained from the actual distribution of samples. To obtain an accurate representation of the graph, the discriminator motivates the encoder to produce low- dimensional variables with distributions that are closer to the true distribution of the data.

Autoencoder is one of the newest methods for dimensionality reduction and a widely used strategy for spotting outliers (Aggarwal, 2015). The encoder and decoder of an autoencoder work together to reassemble data samples and calculate an anomaly score based on the reconstruction error (Borghesi et al., 2019). The deep autoencoder proposed by Zhou and Paffenroth (2017) combines robust PCA with deep autoencoders. In doing so, it separates the data into two categories: the part that can be reconstructed by autoencoders and the noise (outliers). The deep autoencoder and Gaussian mixture model work together in the deep autoencoding Gaussian mixture model (DAGMM) to simulate the density distribution of data in several dimensions (Zong et al., 2018).

To reconstruct the topological structure and node properties, Ding et al. (2019) uses a graph convolution network (GCN) to compress the input network into low-dimensional embedding representations. Local representations of nodes are learned in spectral autoencoder for anomaly detection in attributed networks by employing a graph convolution encoder and decoder (Li et al., 2019).

The adversarial strategy of our method is based on GAN, wherein a generator and a discriminator engage in a minimax game to optimize each other (Goodfellow et al., 2020). GraphGAN (Wang et al., 2018) was the first to employ an adversarial strategy for graph learning. As an alternative to traditional network embedding algorithms, Hu et al. (2019) uses GAN as an additional regularization term to impose the distribution of the real data as a prior distribution on embedding vectors. To learn the latent embedding, Makhzani et al. (2015) introduced an adversarial autoencoder, which incorporates the adversarial process within the autoencoder. By contrast, this method works well with simple data and not graph data. Though several adversarial models have found success in computer vision, they are unable to deal with graph-structured data without some modification.

The remainder of this paper is organized as follows. Section 4, introduces the anomaly detection methodology that we have proposed. Section 5, compares and contrasts several assessment measures to demonstrate the practical efficacy of the proposed methods for detecting anomalies in real-world networks. Section 6 concludes the paper.

Flickr is a photo-sharing platform similar to Instagram. By interacting with one another, people create a network that functions much like BlogCatalog. Node properties are defined by tags, which are chosen by the user and reflect their interests (Asperti and Trentin, 2020).

The proposed model was created in Python, and training was performed with 180 epochs throughout all datasets. For this, we employed Adam optimizer with a 0.001 percent learning rate. To maintain consistency across all datasets, we decided to keep the embedding dimension at 128.

Our model is theoretically novel as it combines a GAN with a pair of VAEs. The importance of representing both topological structure and node properties in real-world networks highlights the theoretical merits of this combined approach. This is a significant improvement as it broadens the theoretical arsenal for detecting anomalies in complex data sets.

Results are put into perspective, especially the lower AUC on the Enron dataset, demonstrating a nuanced grasp of anomaly detection. In contrast to some earlier studies, which may have presented results without considering context, this is a significant change. Our study demonstrates a dedication to practical relevance.

This study tackles the problem of detecting anomalies in practical applications, such as online marketplaces and social media networks. This has serious repercussions for companies and groups active in these fields. The proposed model is an example of an advanced anomaly detection technique that management teams can use to detect and prevent fraudulent actions on their platforms. Increased safety, better user experience, and protection of corporate interests are all possible outcomes.

As observed in the Enron dataset, dataset dimensionality can affect model performance, highlighting the need for context-specific investigation. Management must understand that not all datasets are the same and that the data and model they choose must fit the unique characteristics of the problem at hand. This highlights the significance of flexibility in data science projects.

Adversarial training can be used to strengthen models in important ways. The administration needs to understand the value of dependable and secure data-driven apps. Adversarial training, or a method very similar to it, can be used to protect systems from adversarial attacks and guarantee their consistent performance in the real world.

The relevance of regularizing latent codes in the model is emphasized. This idea can be implemented in management theory and practice by emphasizing the value of high-quality, consistent data. The suboptimal representations that may result from using unregularized embedding techniques can be avoided by ensuring that data sources are well-structured and cleansed. Data quality can be managed by allocating resources and implementing processes.

In conclusion, there are several directions in which the management practices of data scientists could make use of the results of this study. They emphasize the need for reliable anomaly detection, high-quality data, and flexibility when dealing with complex real-world data problems. By considering these observations, businesses may boost their data-driven decision-making processes, tighten up their security, and get more use out of their data assets.

The proposed method was evaluated on three different datasets: BlogCatalog, Flickr, and Enron. The experimental findings prove the viability of the proposed model in comparison to the standard approaches. Experiment results demonstrated that GAN models can be used to achieve state-of-the-art performance for anomaly detection on high-dimensional, complex datasets. In future studies, we also hope to perform a deeper analysis of low-dimension datasets and aspire to offer a much more comprehensive analysis of our adversarial approach and its effectiveness in real-world settings.

