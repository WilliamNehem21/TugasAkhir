Deep learning (DL) workloads and their performance at scale are becoming important factors to consider as we design, develop and deploy next-generation high-performance computing systems. Since DL applications rely heavily on DL frameworks and underlying compute (CPU/GPU) stacks, it is essential to gain a holistic understanding from compute kernels, models, and frameworks of popular DL stacks, and to assess their impact on science-driven, mission-critical applications. At Oak Ridge Leadership Computing Facility (OLCF), we employ a set of micro and macro DL benchmarks established through the Collaboration of Oak Ridge, Argonne, and Livermore (CORAL) to evaluate the AI readiness of our next-generation supercomputers. In this paper, we present our early observations and performance benchmark comparisons between the Nvidia V100 based Summit system with its CUDA stack and an AMD MI100 based testbed system with its ROCm stack. We take a layered perspective on DL benchmarking and point to opportunities for future optimizations in the technologies that we consider.

The share of deep learning (DL) scientific applications has steadily increased in the allocation portfolio among High-Performance Comput- ing (HPC) centers. In recent years, it has reached a tipping point that the procurement of next-generation HPC infrastructures has to take the performance of the DL stack into consideration. In the case of DOE leadership class platforms, a Collaboration of Oak Ridge, Argonne, and Livermore (CORAL) has established a set of benchmarks to gauge the hardware/software competitiveness. For the first time in the CORAL- 2 benchmarks [1] suite, DL workloads are included in the evaluation for the acquisition of the systems: Frontier at Oak Ridge, Aurora at Argonne, and El Capitan at Livermore. Ranging from DL kernels to distributed training, the CORAL-2 DL benchmarks consist of micro- benchmarks, such as DeepBench [2], and DL suites including both ResNet50 on ImageNet [3] and application benchmarks such as the can- cer distributed learning environment (CANDLE) [4]. Comparing to the industry-led benchmarking effort, MLCommons HPC (also referred to as MLPerf HPC [5]), the CORAL-2 benchmarks focus more on throughput and fundamental building blocks.

Regardless of the increasing complexities of deep neural net (DNN) models, the compute operations essentially boil down to three types of mathematical kernels, i.e., general matrix multiply (GEMM), convo- lution, and recurrent operation. Considering that distributed training at scale has became a common practice at data centers, the commu- nication operation has to be taken into account as well. The overall performance of DL applications is hence mostly determined by the hardware/software stack for the aforementioned three mathematical and one communication operations. (While I/O is also an important determining factor, the benchmarks we consider here do not face an I/O bottleneck when high-performance node local storage, e.g., SSD, is used for the data and proper pipelining practices are followed.)

The rest of the paper is organized as follows: Section 2 provides general background on differentiating aspects of traditional simulation- based HPC workloads versus emerging DL workloads, as well as an overview of DL benchmarks proposed for the CORAL systems. Section 3 details a layered approach, methodology, and metrics we will use for performance evaluation and comparison. Section 4 presents our results based on the proposed methodology covering compute kernel, model and workloads, frameworks, and applications, which aims to provide an end-to-end perspective on key performance metrics. Section 5 presents our conclusions and discusses opportunities for future work.

Roofline Model In addition to FLOPS, another important metric to gauge the compute and memory performance is the so called Roofline model, which can visually demonstrate the bottleneck of the bench- mark and hardware, i.e., whether it is compute or memory bound. To that end, the arithmetic intensity I, i.e., floating operations per memory load, needs to be calculated. For single-precision GEMM, this is given by,

Resource Utilization Another important way of understanding the performance of deep learning applications is by tracking resource uti- lization. This is typically used to find bottlenecks of the workload and identify operations that need optimization. In this work we use the nvidia-smi for the V100 GPUs on Summit and the rocm- smi for the MI100 GPUs on Spock to monitor the memory used and the GPU utilization for the framework and application benchmarks. Specifically the memory.used and the utilization.gpu flags were used for the nvidia-smi, and the showuse and showmemuse for the rocm-smi.

Communication Kernels Given that distributed training has became common practice to manage ever-growing data and model sizes, the communication kernels play increasingly important roles. For the pop- ular data parallel training (each device has a model replica working on different data batch, and the gradient information is exchanged periodically), allreduce is the dominant communication pattern that is executed each (synchronized) or a few (stale or asynchronized) batch steps. Depending on the implementation, the allreduce can be realized via a single API or a combination of allgather and reducescatter, or reduce and broadcast. The performance depends on device communication libraries (e.g., N/RCCL) and the specific network topology of the platform.

We have presented a layered methodology and metrics to bench- mark DL workloads at scale, involving kernels, models, frameworks, and applications. From the perspective of HPC facilities, we argue that understanding kernel and model level performance, and framework level scalability are more important than application FLOP counts given current scientific DL use cases and patterns. Using the CORAL-2 DL benchmarks, we evaluated the performance of Spock, an early-access testbed system for Frontier. Compared to the V100 based Summit system with CUDA DL stack, the MI100 based Spock with ROCm DL stack shows an edge in single precision performance for most kernel and model benchmarking tasks. However, there is currently a gap in its half precision performance, specifically for TensorFlow. Roofline modeling also indicates rooms for improvement in the ROCm stack, which is still maturing.

We also explored and demonstrated using machine learning an approach to model the relationship between input parameters and benchmark performance outcomes. And through a one-on-one compar- ison of the resource utilization for the two DL stacks on the same DL workloads, we are able to comment on the sources of performance dif- ferences. Although these two ways of gaining insight into performance comparisons are not conclusive in deducing underlying implementa- tions or bottlenecks, our data does shed light on the direction for future optimizations in the DL stacks.

Finally, we do note that Spock is a testbed early access system. Our benchmarking results and comparisons are most useful to concretely present our systematic approach to DL benchmarking. The kernels and frameworks are maturing and will continue to evolve (particularly for the ROCm ecosystem) and, therefore, specific observations reported in this paper are likely to change even if it does not affect the overall methodology that we have presented.

This research was sponsored by and used resources of the Oak Ridge Leadership Computing Facility (OLCF), which is a DOE Of- fice of Science User Facility at the Oak Ridge National Laboratory supported by the U.S. Department of Energy under Contract No. DE- AC05-00OR22725.

