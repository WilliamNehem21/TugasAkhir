Machine learning is concerned about the construction of computer programs that automatically improve with experience [13]. Similarly, pattern recognition is inter- ested in the automatic discovery of regularities in data through computer algorithms and the later application of such regularities as decision making actions, such as dif- ferent data categorizations [3]. One of the well-known classification approaches in machine learning consists of extracting rules. These algorithms generally produce if-then classifiers, with a predictive performance comparable to other traditional classification approaches, such as decision trees and associative classification [22].

A inductor method for rule classification applies an iterative process that covers a subset of training examples and then remove all examples covered by the rule from the training set. This process is repeated until there are no examples left to cover. The final rule set is the collection of rules learned at each iteration [9]. PRISM is one of the rule induction techniques which was developed in [4] and slightly enhanced by others, i.e. [8] and [19]. This algorithm employs separate-and-conquer strategy in knowledge discovery in which PRISM generates rules according to the class labels in the training dataset.

Adaptive technology refers to the use of techniques and devices which are ex- pected to react to given inputs by autonomously modifying their own behavior [14]. In a broad sense, computers learn when there is a behavioral change in order to better perform a specific task. Inspired by previous works on these areas [20,21], we demonstrate how learning systems can be dynamically adjusted by using adaptive techniques. A direct advantage on incorporating adaptivity in learning methods consists on covering a fundamental aspect of learning itself: the dynamic adaptation of a rule set based on interactions with the environment [15]. Additionally, use of adaptive technology tends to be more expressive than traditional methods [14].

Adaptive technology has been successfully used in pattern recognition and ma- chine learning. Pistori and Neto [16] propose a decision tree induction algorithm using adaptive techniques, combining syntactic and statistical strategies. In [17], Pistori presents an adaptive automaton as device for an automatic recognition pro- cess of sign language. Adaptive automata are also reported to be used in syntactic pattern recognition of shapes [6] and in construction of hybrid maps for robot nav- igation [11]. Other applications of adaptive techniques include skin cancer recogni- tion [10] and optical character recognition [7].

As reported by [23], this approach produces rules that are unambiguous in the sense that the order they are executed does not matter. However, the rules are more complex than necessary. As we have just seen, we can obtain if-then rules by learning a decision tree and converting it to rules. Another approach covers the direct rule learning. Decision rules learning works similarly to a decision tree except

As reported by Mitchell [13], Learn-one-rule must return a single rule that covers at least some of the examples. Performance is a user-provided subroutine to evaluate the rule quality. This covering algorithm learns rules until it can no longer learn a rule whose performance is above the give threshold. Common evaluation functions include [13]:

Definition 2.7 [adaptive function] Adaptive actions may be defined as abstractions named adaptive functions, similar to function calls in programming languages [15]. The specification of an adaptive function must include the following elements: (a) a symbolic name, (b) formal parameters which will refer to values supplied as argu- ments, (c) variables which will hold values of applications of elementary adaptive actions of inspection, (d) generators that refer to new value references on each usage, and (e) the body of the function itself.

> 50%, and (c) rules with poor accuracy (mean rank), i.e. <= 50%. When searching for an applicable rule, the algorithm goes over the rules in a topdown fashion starting with the primary rules (higher rank) until reaching rules with a poor rank. Whenever two or more rules have identical performance then the algorithm favors rules with the least number of terms in their P.

Search space reduction for candidate rules. When considering large dimensional datasets, the numbers of candidate rules might significantly increase, and thus impose operational limits to certain applications. A potentially heuristic-based mechanism to reduce the search space might allow better handling of huge datasets.

Numerical attribute handling and support for noisy datasets. A dataset is said to be noisy when it contains incomplete attributes and missing values. An ap- proach to handle and potentially predict completeness of partial information might provide better model convergence. Additionally, the algorithm must offer a dis- cretization strategy for covering attributes in a continuous domain.

