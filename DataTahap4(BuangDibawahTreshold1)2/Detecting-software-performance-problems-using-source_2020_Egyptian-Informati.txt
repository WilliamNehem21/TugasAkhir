This paper makes the following contributions: (1) A novel approach to detect performance regressions across two system versions, while utilizing unit tests using source code analysis tech- niques. (2) An empirical evaluation of the proposed approach is conducted on real performance regressions, across two different versions, for four open source applications, namely: AgileFant, Apache Commons Math, Apache Commons IO, and Xalan. The results show that our proposed approach could successfully iden- tify performance regressions and improvements across the four systems. Even more, our proposed approach relies on finer granu- larity unit tests, rather than randomly generated test input. Accordingly, PerfDetect could detect additional performance regressions that were missed by alternative performance regres- sion detection approaches.

This paper is structured as follows. We start with explaining our problem statement in Section II. Section III introduces our pro- posed approach, whereas Section IV shows our experimental setup. Section V shows the results our study, whereas section VI discusses the remaining issue Section VII concludes the paper, while pointing out future research directions.

Current approaches that aim to identify performance regression issues, and analyze their root causes within the source code suffer from limitations in several directions as follows. (1) Such approaches demand running a complete test suite to compare and detect performance problems across two releases, which is an extremely time-consuming process for large test suites. (2) Existing approaches rely on the presence of execution traces for the analyzed system. Such execution traces can hardly exist, espe- cially if the system is undergoing a major change that hinders hav- ing a completely running version. Furthermore, the quality of the analysis relies on the strength and code coverage of those traces.

Currently, developers must run a complete set of test cases for all the system which is an extremely time-consuming process with huge test suites. Developers must have a completely running ver- sion. Test suites used must be accurate to reveal performance regressions.

Depending only on the black-box technique for finding perfor- mance regression may fail in detecting that such method causes performance regression. Such failure is attributed to the fact that such specific code change might not get executed at all with ran- domized inputs. Randomized inputs may not cover all branches of code in this version.

(1) running a complete test suite across two system versions to identify performance problem which is an extremely time- consuming process for large test suites; (2) relying on the presence of execution traces for the analyzed system, which is hard to exist especially if the system is undergoing a major change that hinders having a completely running version; (3) relying on the quality of the used inputs to create the execution traces. If the execution traces result from inputs that exercise complete system scenarios, such inputs may ignore specific modified source code elements whose modification resulted in performance degradation. Accord- ingly, our approach utilizes different information to avoid execut- ing the whole system to detect performance problems. Such information is (a) the version control information of the two sys- tem versions, (b) the added/modified code across the two system versions, (c) the dependencies between the modified source code and its corresponding test suites.

To apply the proposed approach, a prototype tool named Perf- Detect has been built. The proposed approach starts its first step by comparing two versions of source code to determine which parts of code are changed or added. The methods affected by such changes are then identified to serve as inputs used for analyzing and detecting performance regressions.

[6]. Eclipse Call Hierarchy plugin is used to find the references of a method whether such reference represents the callers of such method, or the callees of such method. This step is represented in Alg. 2 IDENTIFY-RELEVANT-TEST-CASES which takes as input the list of changed methods (M), all the test cases in the two sys- tem versions (T). In each changed method (mj), PerfDetect identi- fies all caller methods of (mj) and detects if each caller method (tdirect) is test case or not by one of these ways: (1) The caller method has the JUnit 4 @Test annotation, (2) The parent class of the caller method is TestCase class.

rect) that call other source code methods, which call the modified methods. PerfDetect starts by checking if the changed method (mj) has relevant direct test cases. If no relevant direct test cases were found, PerfDetect searches for relevant in-direct test case.

In the final stage Alg. 3 EXECUTE-RELEVANT-TEST-CASE takes the list of relevant test cases as input and outputs the list of changes causing the performance regressions. Through the relevant unit test case of each modified method, PerfDetect profiles the execution time of the unit test case across the two versions (tij, ti+1j) with dif- ferent workloads. The differences in execution time (tdj) between the two versions is then calculated. Finally, PerfDetect orders the differences in execution time for all changed methods according to the execution time differences; higher difference are the ones cause performance regression [1].

For modified methods that have no relevant test cases, PerfDe- tect generates new test cases for changed code which have no rel- evant test cases whether directly or indirectly. PerfDetect utilizes a genetic algorithm, through Evosuite [14], for generating test cases to reveal any performance regressions across the two system ver- sions. Evosuite applies a novel hybrid approach that generates and optimizes test cases with different coverage criteria, like individual statements, branches, outputs and mutation testing. The target of Evosuite is generating test cases with assertions to help developers to detect deviations from expected behavior. That target varies from PerfDetect which targets detecting performance variation behaviour. PerfDetect depends on Evosuite for generating JUnit test cases for classes written in Java code. Once those tests are gen- erated by EvoSuite, PerfDetect applies the above mentioned step (See section D) Such step is applied to make those generated test cases to be performance-aware tests, in order to enable them to detect performance regressions.

Within the evaluation, the four open source systems were uti- lized along with their manually written unit tests, to detect perfor- mance regressions using our approach (See Sections V.A, V.B, V.C, and V.D). Additionally, one of those systems (Commons Math) was used another time, along with automatically generated unit

Agilefant is an open-source web application for helping soft- ware engineer to manage agile software development faster. Agile- fant is deployed in server core i5 using Tomcat 7.0.47 as a web server environment, and MySQL as the backend database. Com- mons Math is a lightweight library of mathematics and statistics components addressing common problems not available in the Java programming language.

Versions of Agilefant [17], Commons Math [18], Commons IO [19], and Xalan [20] applications are located in the version control system (Git Repository) [4]. The approach is applicable to any Java- based software systems and pure JUnit 4 tests without any addi- tional framework like EasyMock. EasyMock is used to provide mock objects for interfaces by generating them on the fly so that a dummy functionality can be added to a mock object that can be used in unit testing. PerfDetect needs to test a real scenario for each component without any dummy data. Hence, PerfDetect tests the performance of unit test cases and their impact set rather than those tests functionality only. This is done to get the real exe- cution time of each method in the impact set, and hence identify the cases that can potentially be critical for the performance of the code in the target application context. So EasyMock is inconsis- tent with our approach.

This section analyzes the empirical results of the modified code across two system versions (v3.2 and v3.3) after testing. The results are analyzed for forty-nine changed methods including ten added methods, three deleted methods, and thirty-six modified methods whether from the old version or the new version. That is done by (1) identify the source code differences that have changed between the two system versions, (2) find the related test cases of code differences using the caller hierarchy of eclipse, (3) and

Apache Commons Math is the second application we used to evaluate our approach. In this section, we analyze the results of detecting performance regression problems through two versions (v2.1 and v2.2). In 2010, Commons Math developers submitted a new code version (v2.2) with hidden code portions that caused performance regressions. This problem was discovered and reported9 in 2011. The problem took a long time around one year additional (2012) until developers can detect the code portions caused performance regression and solve it. Our proposed approach in this paper targets helping developers for detecting performance regression faster and earlier.

2.1 especially from version v1.1, developers of Commons IO cre- ated a new method called readFileToByteArray() in class FileUtils which is called another method called toByteArray(InputStream input) in class IOUtils. The problem is the method readFileToBy- teArray() consumed a long time from the time of creation (v1.1) in 2005 until v2.1 in 2011. The reason for this long time is the call- ing of method toByteArray(InputStream input). In 2010, the prob- lem was discovered and reported10 with a new solution which is adding a new method toByteArray(InputStream input, int size) for toByteArray. After this modification method readFileToByteArray() now called the new method for toByteArray(). This reported issue was taken around 6 years to resolve.

In this section, we analyze the results of comparing the perfor- mance of two versions (v2.7.1 and v2.7.2). Some of changed code portions depends on external files as inputs and the test cases we generate cannot cover this kind of code. So we generate manual test cases for them. One of these manual generated test case that we used in Xalan is a changed method called compose() in class ElemLiteralResult. The source code of this changed method rely on composing a template for stylesheet with some attributes to cover all branches of changed code. We generate manual test case to cover all branches of source code for testing the performance of changed code.

We picked two existing approaches [1,3] for performance regression and evaluated our approach against them. Those approaches were selected due to utilizing the same real applica- tions (Agilefant and Commons Math) that we utilized in our eval- uation. We discuss our results against their results for each application.

use source code analysis but the difference is in time of detecting performance regressions. Another approach runs all test cases in the applications and has taken 48 min in v2.2 and 30 min in v2.0. The time of our approach did not exceed 80 s for v2.2 and 11 s for v2.0. That is because our approach depends on running only the test cases related to the changed methods not all test cases in the application. All these results show that PerfDetect can be used to effectively identify the changes that are responsible for performance regressions.

Results of our proposed approach when comparing the source code between two versions (v2.0, v2.1) observed performance improvement. PerfDect detects the changed method readFileToBy- teArray() in class FileUtils and observe that the performance of this method has improved more than 50% comparing to the previous version 2.0. The cause of this performance improvement that the changed method call a new added method instead of old one that was taken a long time.

Results of our proposed approach when comparing the source code between two versions (v2.7.1, v2.7.2) observed performance improvement. PerfDect detects the changed method ElemLit- eralResult.compose() and observes that the performance of this method has improved slightly comparing to the previous version

2.7.1. The cause of this performance improvement that the chan- ged method changed the method used to locate an element of an array. This results prove that PerfDetect can detect the perfor- mance whatever that performance is improvement or regression and identify the cause of performance change.

The proposed approach suffers from one basic limitation which is its inability to identify performance regressions introduced by newly added, rather than changed, source code. This limitation is mainly attributed to the absence of execution time data for such newly added source code. Yet, such limitation remains as an open issue that we plan to address in our future work.

consuming because they demand running a complete set of test cases, this may not scale well for systems with huge test suites. For solving this problem, other approaches (e.g. Yu et al. [23]) use a regression test selection technique, to reduce the number of test cases that must be run on a changed program. These tech- niques prioritize functional tests but are not tailored to select such tests that would reveal performance regressions. Reichelt et al. [24] is depending on regression test selection to define performance changes using artificial test cases.

[33] presented an approach that automatically detects UI perfor- mance degradations in Android apps while considering context dif- ferences. This approach relies on poor GUI responsiveness and number of frames per second for measuring performance regres- sion. Many approaches are interested in detecting performance changes in different languages as Python language [34].

works only locally on the individual statements, they extended EvoSuite with a memetic algorithm enabling a global search algo- rithm to increase branch coverage [15]. Test suite augmentation techniques are used to generate test cases that cover code changes or code elements affected by changes [38,39]. Genetic algorithms used to generate test cases or optimize test suites, but these approaches focus on generating new test cases to achieve high code coverage, rather than reveal performance regressions and they are time-consuming for generating all test cases. existing approaches focus on analyzing complete systems to detect perfor- mance regressions, rather than focusing their analysis on only the modified/newly added portions of those systems on specific sys- tem parts to detect performance regressions.

In this paper we propose a novel approach PerfDetect that aims to identify performance regressions and their root causes across two system versions. The approach depends on: (1) detecting changed code across the two system versions through version con- trol systems, (2) identifying the relevant test cases to the changed code within the current version to compare its performance against a reference version, (3) analysis of performance problems to identify code portions and their root causes that may cause per- formance regression. We evaluated PerfDetect on two open-source applications as real applications written in Java with their test cases. The results demonstrate that PerfDetect can effectively rec- ommend changed code portions caused performance regressions in comparison to other approaches.

