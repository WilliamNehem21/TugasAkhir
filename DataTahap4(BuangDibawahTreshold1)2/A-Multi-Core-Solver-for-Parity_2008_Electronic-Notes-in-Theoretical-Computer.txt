We aimed at an efficient algorithm that is well-suited for a multi-core machine. In this model, a number of processor communicates via shared memory. Our design goals were to maximize memory efficiency, and at the same time ensure maximal parallelism. In particular, our design avoids the storage of predecessor edges, and is lock-free, and almost wait-free. This puts quite some restrictions on the heuristics to select the next vertices to lift; in particular, the absence of direct predecessor information is tough.

For CTL*, a multi-core parallel implementation is described in [21]. The al- gorithm is based on Hesitant Alternating Games and explores the induced and/or graph with a stack to detect cycles. The approach is reported to be close to [8]. The authors also discuss pitfalls for shared-memory implementations.

The least game parity progress measure is computed as the least simultaneous fix point of lifting the measure of each vertex when needed, starting with the 0- vector. Here, with an eye towards the distributed implementation, we allow that a whole set U gets lifted at once. Choosing only singletons for U yields the original algorithm of [25], computing the same fix point. In Sec. 4 we discuss heuristics to choose U in a parallel algorithm. The complete algorithm is given by:

cessors. They can be updated by their respective owners executing line 7 at the same time when Lift(Q, v) reads their values in line 5. For this to be correct, a sufficient condition is that the assignment in line 7 is executed atomically. We can

It is sufficient to store at most |Ui| + 1 different measure vectors in the indexed set for each worker i at any given time. Usually, the number is lower than that, considering for example the initial situation, where all measures are initialized to the zero vector. Hence, this scheme also conserves memory.

On the other hand, the number of measures encountered during the run of the algorithm is much higher than that. This suggests that the indexed set needs to be coupled with a reference counting or garbage collection scheme to be feasible. Larger measures could even benefit from the vector folding described in a different context [7].

For our experiments, we implemented the algorithm for a thread-based multi-core setting (shared memory). We used the Intel Threading Building Blocks (TBB) [22] as concurrency abstraction. It provides high-level operations like parallel_for and parallel_reduce, shielding us from many threading details and allowing a very concise implementation, with close resemblance to the pseudo code presented here.

In Alg. 2, we abstracted from the order in which measures are lifted by means of a permutation H, with allowed repetition. In this section, we will discuss a number of possible choices for H. These are heuristics, meaning that they are not always optimal, but our experiments confirm that they work well in practice.

All our experiments were performed on a computer with two Quad-Core Intel Xeon E5320 processors (1.86 GHz) and 8 GB RAM, running Linux 2.6.18; we ran the algorithm in turn with N = 1, 2, 4, and 8 worker threads enabled, each five times. Because run times of repeated experiments vary little, we use the average run time here. Times represent the wall clock time (in seconds) needed to calculate MG, and the actual computation of progress measures. This being a global algorithm, it is assumed that the parity game graph is already present in memory. Hence, generation time is excluded. Another reason is that parity games can be obtained from a number of different sources with varying performance characteristics.

Onebit Sliding Window Protocol The Onebit SWP [4] is similar to the Sliding Window Protocol, but the window is limited to size 1. While very simple, this protocol is more interesting than, e.g., the alternating bit protocol (ABP), because it allows more parallel behavior than ABP. onebit M denotes Onebit SWP with M distinct data elements.

Concentrating first on the top half of Tab. 1, we can see gains by increasing the number of worker threads (N ) for most of the combinations of case studies and selection heuristics, albeit quite varying. It turns out that the complete asyn- chronicity of workers for most of the run time has the downside of causing extra iterations until a fix point is reached, in particular, if the amount of work for each drops below a certain threshold. In addition, we cannot rule out effects of false sharing, as have been observed by Inggs and Barringer [21].

