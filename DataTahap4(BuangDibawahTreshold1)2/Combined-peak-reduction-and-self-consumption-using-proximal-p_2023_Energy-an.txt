Residential demand response programs aim to activate demand flexibility at the household level. In recent years, reinforcement learning (RL) has gained significant attention for these type of applications. A major challenge of RL algorithms is data efficiency. New RL algo- rithms, such as proximal policy optimisation (PPO), have tried to increase data efficiency. Additionally, combining RL with transfer learning has been proposed in an effort to mitigate this challenge. In this work, we further improve upon state-of-the-art transfer learning per- formance by incorporating demand response domain knowledge into the learning pipeline. We evaluate our approach on a demand response use case where peak shaving and self- consumption is incentivised by means of a capacity tariff. We show our adapted version of PPO, combined with transfer learning, reduces cost by 14.51% compared to a regular hysteresis controller and by 6.68% compared to traditional PPO.

batteries and heat pumps. In their field-test, they achieve a 68 % average self-consumption rate. This means, on average, 68 % of heat pump energy is covered by local PV generation. In a similar field-test, using the same model-based RL approach, but only scheduling the

The capacity tariff DR application, which is considered here, differs from Soares et al . [19, 20] and De Somer et al . [4] as the goal is not to maximise self-consumption. Rather, it is to minimise peak power consumption. In the past, we have already touched upon a capacity tariff scenario [14]. However, the capacity tariff treated here is different, and is as proposed by the Flemish regulator in Belgium, i.e., VREG [24]. The DR use case of peak power reduction is of interest in other regions as well [1].

We test our approach on data from real households, obtained from a field-test in the Netherlands [4]. Although the use case considers the Flemish tariff design, the method is more generally applicable. Moreover, reducing peak power demand is a widespread DR setting.

This paper has been divided in four sections. Section 2 gives a more detailed formulation of the capacity tariff design, formulates the Markov Decision Process (MDP) and lays out the RL algorithm and its modifications. The paper then goes on to Section 3, presenting the experiments and discussing their results. Finally, Section 4 concludes this work and gives future work directions.

An additional challenge considered here is the aim of reducing the need for extensive local PV and demand forecasts. On top of that, the learned control policy will be applied to different residential buildings and households. Therefore, to facilitate policy transfer, the state-space is designed to be independent of environment parameters. For instance, the learned policy should preferably be independent of inverter capacity, as different households will have different PV installations. Preference for policy independence on environment parameters can be illustrated by imagining a simple policy that turns the EWH on whenever PV power production is above 2 kW. When this policy would be transferred to a different household, this absolute number does not apply anymore. Moreover, even within one single household this number would have to change between seasons.

production of that same day. The agent has no (explicit) information on local power demand. The former forecast provide valuable information to determine if water can best be heated at night or not. When there is barely any production forecasted for the day, it is better to heat at night to avoid power peaks during the day. The latter forecast allows the agent to temporarly interrupt a heating cycle when current net power consumption peaks.

In earlier work we have shown transfer learning increases performance for agents applied in a DR setting [11]. Hence, the task is separated in a pre-training and test phase. The pre-training phase only uses readily available data. Three data streams are needed. First, simulated PV production data is taken from the ninja tool developed by Pfenninger and Staffell [15]. Second, electrical load data is generated using Strobe [2]. Third, training phase simulations use Domestic Hot Water (DHW) consumption data from Edwards et al . [5]. These three sets contain data of one year. For pre-training, the agent interracts with the EWH model for a total 15 simulation-years and trains using the collected state-transition tuples.

This part presents the results of the simulations. We start with a visualisation of the three main control approaches (HC, RBC, (expert) RL). Thereafter, we compare their performance differences in a more thorough manner. For the sake of simplicity, only expert RL has been considered at the start. It has been referred to as RL from now on. Only at the final detailed comparison of the main metrics, non-expert RL has been included.

The bottom bar chart shows the final (electrical) energy bill of each household, defined by (3). (Expert) RL is 14.51 % cheaper than HC and 4.59 % cheaper than RBC. For these households and the considered capacity cost, this thus results in an average reduction in cost

Our experiments indicate that data which has been generated/collected for other pu- poses, such as district level simulations or feasability studies, can be used to improve RL performance. This has the potential to facilitate RL-based control uptake. Furthermore, while model-free learning has proven to be promising in DR settings, it has also shown some drawbacks. Our experiments show that by including expert knowledge into the learning pipeline it is possible to mitigate some of these drawbacks. This is in line with the current observations in physics-informed machine learning research.

We believe the introduction of a capacity tariff is an opportunity for Flemish residential consumers to adopt smart control approaches for EWHs (and other TCLs). Aggregators can potentially provide the necessary technology. Therefore, our future work is directed towards incorporating local control for reducing the MMP within an aggregator framework.

