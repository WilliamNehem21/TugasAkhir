Information recognition and extraction of human emotions are necessary for machines to communicate smoothly with humans and to realize emotion communications. We focus on human psychological charac- teristics to develop general-purpose agents that can recognize human emotion and create machine emotion. We comprehensively analyze brain waves, voice sounds and picture images that represent information in- cluded in emotion elements of phonation, facial expressions, and speech usage. We analyze and estimate many statistical data based on the latest achievements of brain science and psychology in order to derive transition networks for human psychological states. We establish a speaker word model for researching computer simulation of psychological change and emotional presentation, developing emotion interface, and establishing theoretic structure and realization method of emotion communication. A new approach for recognizing human emotion based on Mental State Transition Network will be described and one emotion estimation method based on sentence pattern of emotion occurrence events will be discussed, and some new results of the project will be given.

Modern information communication mainly focuses on verbal information and to a lesser degree deals with the human emotions accompanying the message (non- verbal information). Both research and business are developing in the field of hu- man interface technology which covers voice recognition, voice synthesis and virtual reality. However, there are many problems in affective information processing and recognizing human emotion accurately. This makes many people still feel strong resistance toward interacting with machines in business fields of terminal devices (mobile phones and car navigation systems) and medical care systems. Semantic

Our research focuses on human mental features and aims to develop an emotion measurement model for a speaker and emotion simulation model for a computer which work as a multi-purpose agent recognizing human emotions and creating ar- tificial emotions. To be more precise, we analyze information contained in the brain wave [36],[37], sound voice, visual image and speech pattern from the perspective of mental features [34]. We also analyze large statistical data based on the latest result of neurology and psychology in order to derive a mental state transition network. By constructing and using a word model for a speaker, we study how to simulate a change of mental state or emotional demonstrations by a computer. Our study aims to develop an emotion interface and establish a theoretical system and a method for future emotion communication.

The model has two parts, one is the Human Emotion Recognition Engine (HMRE) and the other is the Machine Emotion Creation Engine (MECE). The HMRE consists of linguistic information based emotion recognition module, pho- netic information based emotion recognition module, and expressive information based emotion recognition module. A corpus, an ontology and an individualiza- tion DB are also employed to recognize human emotions. The MECE has three processes: sensibility language process, sensibility sound voice process, and emo- tion face process. A Mental State Transition Network used in both Engines and a psychological questionnaire experiment will be described in next section.

We call an appearance information an emotion energy. At present, the emotion energies are acquired from three parts, the linguistic information, the phonetic in- formation and the expressive information. This section describes a method for acquiring emotion energy from phonetic information.

For judging the expression in an image the minimum distance identification method discussed in [32] is coupled with FACS. The expressions are ranked according to the number of the matched feature values. When the match is not above the given threshold or when the differences between the 1st ranked and other expressions are small, minimum distance identification is used. The minimum distance identifica- tion method is used on the input vector and those in the dictionary. If the FACS expression and the minimum distance identification expression are the same then that expression is the answer. If they differ then the answer will be the one with the larger distance from the 2nd best candidate.

In [24], AUs are mapped to expressions. Each AU is labeled as operating (ON) or non-operating (OFF). The expression is determined by the agreement of ON and OFF AUs. The expression with the highest agreement is chosen as the candidate expressions.

Traditional emotion studies were not focused on integration of internal and exter- nal aspects, because they were technically difficult. The research presented in this paper has developed a new paradigm for human mental state transition network and methods for human emotion recognition, artificial emotion creation, mental state transition of artificial emotion, and affective presentation. These are unseen creations which combine external feature information and physical reactions (men- tal state transition). The new methods are expected to largely contribute to the international society by developing future communication business.

Many thanks to Dr. Shingo Kuroiwa, David B. Bracewell, Junko Minato and all our colleagues participating in this project. The experimental systems described in the paper were developed by Kazuyuki Matsumoto (emotion measurement based on speech pattern), Shunji Mitsuyoshi (emotion estimation based on sound voice), Nobuo Nagao, Takahiro Kuroda and Jia Ma (facial expression recognition), Hua Xiang and Peilin jiang (the psychological experiment).

