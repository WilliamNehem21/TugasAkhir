which contain collections of test cases, are used to test programs. In a test suite, each test case consists of, among others, an input and an expected result, which will be compared with the actual returned result to determine how good the input program is in respect of the test case. We usually consider a good test suite is the one that has small size but has high code coverage, meaning that it not only consumes little memory for running but also can reveal as many behaviors of input programs as possible.

The rest of the paper is organized as follows. Section 2 introduces several metrics for assessing the closeness of two functions. Based on these metrics, Section 3 shows how to generate a better test suite from a given one, and Section 4 points out a method to find the best test cases. We show some experimental results on several mathematical functions in Section 5. Related work is discussed in Section 6. Section 7 concludes.

Here [.] denotes the cardinality of a set. The threshold metric is the percentage of the number of good test cases in a given test suite. If this value is smaller than, say 1%, we say that the function is good up to 99% or good with probability of

Having the metrics in the above section is already quite useful because among many given test cases which are taken from the real data (such as audio and video files) or randomly generated, we can always choose a better or the best ones to test first and stop when the converted function is not good enough. This may allow us to select a smaller set of test cases that can be just as effective in detecting problems for our code. But even the real test data or the randomly generated data may not reveal enough good test cases. Therefore, we propose a method to generate a better test suite from a given one. We start with generating a better test case from a given one.

Among the most well known probabilistic algorithms, we choose Tabu Search (TS) [5] and Genetic Algorithm (GA) [6,19] to solve the problem in the paper; one belongs to neighborhood search methods and the other is in the category of evolution approach. They are two of the most effective, popular, and successful approaches for solving a wide range of combinatorial optimization problems [14]. To discover the global optimizer of l over X, they may complement each other perfectly.

Genetic algorithm (GA), a population-based meta-heuristic optimization method, simulates biotic activities such as crossovers, mutations, and natural selections to gain fittest ones from initial individuals through a number of generations. GA bases on a fitness function, which is l in this case, to decide which individual is better than others. One of the most powerful features of GA is the combination of exploitation (such as crossover process) and exploration (mutation, for instance). The goal of the combination is to find not only new, similar candidates from currently known solutions, but also novel and superior ones.

Because traditional neighborhood-based methods like steepest ascent are easy to be stuck in local optima, it is ineffective to use them to find global optima of complex functions, for instance, Rosenbrock [23]. TS improves this drawback by providing a tabu list to remember which points are recently visited, helping us leave local optima to enter new, promising areas. Also, TS uses intensification and diversification to search not only deeper but also broader, as GA does with exploitation and exploration. In Algorithm 3, we introduce our version of TS to deal with our problem. Note that each time a new initial solution is randomly created, our TS sets a limited computation time for the process of finding the best solution started from the initial one. If the time is exceeded, we stop discovering and prepare for the next start.

Our work also relates to roundoff error analysis, which aims to calculate the difference between an approximate number, function, or program method and their exact value. The difference results from using finite digits to represent numbers in computer. There are several different approaches on the topic. Simulation-based approach [2] attempts to run both types of programs with a selected set of inputs, and then observes the corresponding roundoff error. Although this approach is eas- ier to be carried out, it leads to a trade-off between the accuracy and the speed of the method. Another approach is using mathematical reasoning, in which Giraud et al.

One important technique in our testing method is the use of optimization meth- ods. They are ongoing research topics in not only theoretical fields but also practical applications [15]. Our Steepest Ascent in Algorithm 1 shares the same ideas with the discrete steepest descent method in the paper of Ng et al. [20]. Their method,

entitled the discrete global descent method, can return optimal global minimizers for functions that agree with their three reasonable assumptions. Recently, Hvat- tum and Glover [10] combine traditional direct search methods and Scatter Search to find local optima of high-dimensional functions. However, because our function l is black-box, these inspiring methods are not viable to be used here. To solve our problem, we choose GA and TS, the most popular, powerful probabilistic global optimization methods from which many hybrid algorithms are derived.

For future work, we plan to investigate Quasi-Monte Carlo methods to calculate a metric based on integral of the loss function and some variations of it. We also plan to strengthen our approach by taking into account the implementation of the functions. This source code analysis may help us identify parameters or variables of the functions that have a stronger effect on the precision loss. This information may help heuristic algorithms to converge faster and may provide the programmers some hints to refine or fix their code.

This work is supported by the research project No. QGTD.09.02 (granted by Viet- nam National University, Hanoi) and Panasonic R&D Center Vietnam. We also thank Hong-Hai Nguyen, Ngoc-Khoa Pham, and Hieu Tran at Panasonic R&D Center Vietnam as well as anonymous reviewers for their comments on the earlier version of the paper.

