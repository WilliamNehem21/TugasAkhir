Designing appropriate similarity metrics (distance) and estimating the optimal number of clusters have been two important issues in cluster analysis. This study proposed an improved k-means clustering algo- rithm involving a Weighted Distance and a novel Internal Validation index (WeDIV). The weighted dis- tance, EP dis, was designed by considering the relative contribution between Euclidean and Pearson distances with a weighted strategy. This strategy can effectively capture information reflecting the glob- ally spatial correlation and locally variable trend simultaneously in high-dimensional space. The new internal validation index,RCH, inspired by the Calinski-Harabasz (CH) index and the analysis of variance, was developed to automatically estimate the optimal number of clusters. The EP dis was proved reliable in mathematics and was validated on two simulated datasets. Four simulated datasets representing dif- ferent properties were used to validate the effectiveness of RCH. Furthermore, We compared the cluster- ing performance of WeDIV with 12 prevailing clustering algorithms on 16 UCI datasets. The results demonstrated that WeDIV outperforms the others regardless of specifying the number of clusters or not.

The choice of similarity metric (distance) plays a crucial role in clustering. Euclidean distance is the default similarity metric in most clustering algorithms. Some non-Euclidean distances are also used in clustering. Wu and Yang [56] designed a new similarity metric more robust than the Euclidean norm in c-means clustering and developed two new clustering algorithms, AHCM and AFCM. The km-M+ [57] algorithm gears clusters toward roughly elliptical shapes and uses the Mahalanobis distance for clustering. The i- MWK-means [58] algorithm employing a weighted Minkowski dis- tance could overcome the drawbacks of the k-means that lack defending against noisy features. Nevertheless, optimization of

means [59] algorithm derives a new distance metric S-distance with the S-divergence and is used in k-means. However, the algo- rithm must provide the number of clusters as a parameter. Meng et al. [60] defined a new similarity metric in k-means to capture the difference in trend characteristics between data points by con- sidering the importance of derivative information.

Except for utilizing the internal validation index, some cluster- ing algorithms can automatically determine the cluster numbers. X-means [29] is an extension of k-means by making local decisions for cluster centroids in each iteration of k-means and using Bayes Information Criterion [65] or Akaike Information Criterion [66] to split itself to obtain a better cluster. Nonetheless, the X-means is still affected by the initialization problem. R-EM [20] is a robust EM clustering algorithm based on the Gaussian mixture model. This algorithm solves the problem that EM is sensitive to initial values. C-FS [47] assumed cluster centroids are characterized by a higher density than their neighbors and by a relatively large dis- tance from points with higher densities. This algorithm determines the cluster centroids by finding the density peaks using a heuristic

Minkowski distance in a hierarchical clustering algorithm, making it able to detect clusters with shapes other than spherical, but how to determine an appropriate parameter p and b should be consid- ered. RL-FCM [17] introduces an entropy penalty term to adjust the bias-free of fuzziness index and creates a robust learning- based model to find the optimal number of clusters. However, this algorithm does not allow the provision of the number of clusters.

w is the weight to be optimized with the range of [0, 1]. A smal- ler EP dis represents a stronger similarity between the two data points. EP dis would be Euclidean distance when w = 1 and Pear- son distance when w = 0. Since the ranges of Euclidean distance

An appropriate number of clusters is crucial for clustering per- formance. Milligenet et al. compared 30 internal validation indices comprehensively and found that the CH performed the best in esti- mating the number of clusters [37,38]. CH [35] is a clustering inter- nal validation index proposed by Calinski and Harabasz in 1974, which is defined as follows:

ber of pairs of data points that are in the different R and P. C2 is the number of data points in a dataset. The ranges of CA and Rand are [0, 1]. A larger CA (or Rand) represents a better performance of clustering.

This work was supported by the National Natural Science Foun- dation of China [Grant No. 31701164], the Natural Science Founda- tion of Hunan Province, China [Grant No. 2018JJ3238], the Scientific Research Program of the Educational Department of Hunan Province, China [Grant No. 17A096], the Scientific Research Program of the Educational Department of Hunan Province, China [Grant No. 18A105], and the Scientific Research Program of the Educational Department of Hunan Province, China [Grant No. 18C0171].

Zheming Yuan received the Ph.D. degree in agroecology in 2000 from Zhejiang University, China. He is currently a professor at the Department of Bioinformatics, Hunan Agricultural University. He has been the Principal Investigator of bioinfor- matics research center and the director of Hunan Engineering & Technology Research Center for Agricultural Big Data Analysis & Decision-making, China.

Zhijun Dai received the Ph.D. degree in Bioinformatics in 2014 from Hunan Agri- cultural University, Changsha, China. He is currently an assistant professor in the Hunan Engineering & Technology Research Centre for Agricultural Big Data Analysis & Decision-making, Hunan Agricultural University, China. With main research interests in feature extraction, dimension reduction, support vector classification & regression, and their applications to biological big data.

