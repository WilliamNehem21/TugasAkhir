makes it easier for the bones to move. OA is caused by the rupture of protective cartilage, which causes the bones to rub against one another, resulting in excessive joint pain and stiffness. OA most fre- quently affects the knee, followed by the hip and hand joints. Symptom evaluation and plain radiograph evaluation are used to diagnose OA, but this method is subject to error.

up to 2.5% of the growth of the national product in western nations because of lost productivity and healthcare costs [5]. According to recent research, the global aging population and at least 130 mil- lion people will be affected by KOA [6]. More than 20 billion dollars have been spent on health care because of KOA treatment [7].

pared to popular feature selection algorithms. On a group of 21 selected risk factors, the best-performing model of the random for- est (RF) classifier achieved a classification accuracy of 73.55%. McCabe et al. [17] proposed two validated models for treating radi- ological knee osteoarthritis. A diagnostic and prognostic model of time until the onset of KOA was included. The OAI provided model development and optimization data, and the MOST offered exter- nal validation for both models. AIC, determined from the test data, was used to optimize logistic regression (LR) and Cox regression. The diagnostic model had an AUC of 67% for the validation data and 75% for the test data.

Liu et al. [18] utilized the region proposal network (RPN) and Fast R-CNN. The RPN was trained to generate region proposals that included the knee joint and were then used for classification by Fast R-CNN. Using CNNs for localized classification, the authors fil- tered irrelevant information from X-ray images and extracted clin- ically relevant features. They employed a novel loss function whose weighting scheme permitted to address the class imbalance for further performance enhancements. Additionally, when increasing the input size of X-ray images, more prominent anchors were utilized to address the issue of anchors that did not match the object. The Faster R-CNN achieved sensitivity above 78%, specificity above 94%, and a mean average precision of nearly 0.82. Lim et al.

full-scale model without any preparation. In the fifth step, to get the best performance of the model in the newly applied domain, a process of parameter tuning that involves updating and rebuild- ing the model architecture was implemented during the training phase to obtain the optimal network parameters. In fine-tuning step, we unfreeze the model obtained above and retrain it with a very low learning rate on the new data. Gradually adopting the pre-trained features to the new data may result in significant enhancements. The model was tested and validated on the remain- ing invisible images (the testing dataset). Finally, the DenseNet169 model was deployed after several experiments to determine the KOA severity grading and localization.

TP means true positives, TN means true negatives, FP means false positives, and FN means false negatives. The F1-score is the weighted average of recall (sensitivity) and precision. TP will give the count of the quantity of positive examples that were accurately delegated positives. TN will give the count of the quantity of neg-

2. The training accuracy of the InceptionResNetV2 model was more than 90%, and the validation accuracy of the InceptionResNetV2 model ranged between 85 and 90%. The training loss of the Incep- tionResNetV2 was less than 4, and the validation loss of the Incep- tionResNetV2 ranged between 4 and 5.

tionV3 models was 95%, as they predicted 1319 images correctly from 1382. The accuracy of the Xception model was 93%, as it pre- dicted 1288 images correctly. The accuracy of the ResNet50 model was 91%, as it predicted 1259 images correctly. The accuracy of the DenseNet121 model was 94% as it predicted 1302 images correctly, and the accuracy of the InceptionResNetV2 model was 90% as it predicted 1254 images correctly.

For class moderate, the accuracy of the DenseNet169 model was 86.5%, as it predicted 193 images correctly from 223. The accuracy of the InceptionV3 model was 77%, as it predicted 172 images cor- rectly. The accuracy of the Xception model was 82%, as it predicted 185 images correctly. The accuracy of the ResNet50 model was 87% as it predicted 196 images correctly, the accuracy of the Dense- Net121 model was 85% as it predicted 191 images correctly, and the accuracy of the InceptionResNetV2 model was 88% as it pre- dicted 197 images correctly.

For class severe, the accuracy of DenseNet169, InceptionV3, and ResNet50 models was 84%, as they predicted 43 images correctly from 51. The accuracy of the Xception model was 82%, as it pre- dicted 42 images correctly. The accuracy of the DenseNet121 model was 86%, as it predicted 44 images correctly. The accuracy of the InceptionResNetV2 model was 80%, as it predicted 41 images correctly.

Mohamed Elmogy (Senior Member, IEEE) received the B.Sc. and M.Sc. degrees from the Faculty of Engineering, Mansoura University, Mansoura, Egypt, and the Ph.D. degree from the Department of Informatics, MIN Faculty, University of Hamburg, Hamburg, Germany, in 2010. He worked as a Visiting Researcher with the Department of Bioengineering, University of Louisville, Louisville, KY, USA, from July 2016 to August 2019. He is currently a Professor and the Chair of the Depart- ment of Information Technology, Faculty of Computers and Information, Mansoura University. He has authored/coauthored over 220 research publications in peer-

