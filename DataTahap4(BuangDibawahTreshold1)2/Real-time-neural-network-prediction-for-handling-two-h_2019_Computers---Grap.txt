the fact that there is no confusion among markers. Our work focuses on the former category. Alternative technologies are based on inertial measurement units (IMUs) which, in the 9-axis variant, can track acceleration and angular rate. They can be very precise in a short-term context, but suffer from long-term drifts due to the lack of an absolute positional reference.

The experience of full-body motion control of an avatar into a VR experience is referred to as embodiment. Important limitations for a successful embodiment of the avatar are the discontinuities potentially caused by self-occlusions and the absence of movement of the hands, both leading to a reduced sense of body ownership [1]. In such scenarios, increasing the number of cameras is not always feasible, and does not solve the problem entirely.

As an alternative to inverse kinematics, we propose a real-time algorithm based on machine learning that addresses the issue of occlusions through a predictive model. In the first part of the paper (Section 1.1) we discuss the state of the art and analyze how our method relates to other approaches. In Section 2 we describe the features of the dataset used for training our model. In Section 3, we recall three baseline methods for correcting occlusions and we introduce a more complex model based on neural networks for handling both occlusions and inverse

[4] and [5] focus their work on limbs, and do not address the particular case of fingers. Finally, a large portion of research in this field exploits the assumption that an underlying skeleton model is available, thereby allowing the algorithm to put some constraints on the solution. Recently Alexanderson et al. addressed the prob- lem of labeling markers in a passive system for the fingers and the face [6]. Instead of tracking markers in the temporal domain, it estimates the most likely assignments using Gaussian Mixture Models (GMMs). This allows a fast recovery from occlusions and avoids the so-called ghost markers, i.e. detection of markers that do not exist. This approach, however, does not address the problem of predicting the marker positions during occlusions.

resulting in self-occlusions. Previous work has tried to address this problem, for instance Tkach et al. fit the hand posture using a com- bination of sphere meshes [10] while Mueller et al. use a cascade of convolutional neural networks (CNNs) to first localize the hand center and then regress 3D joint locations [11]. They also employ a synthesized dataset that simulates cluttered environments via a merged reality approach, allowing the model to generalize better. These approaches, however, are still very limited in terms of range of motion as they are optimized for a user facing the camera.

As for tracking with motion capture, Han et al. frame the problem as a keypoint estimation task, which is tackled with CNNs [12]. While their approach allows using a passive system, the authors highlight some shortcomings with multiple occlusions. Other frameworks based on motion capture typically employ some sort of sensor fusion from multiple data sources. Andrews et al. propose a tracking system which uses IMUs and a physics model to recover from sensor dropout [13]. Our approach is related to

Machine learning for inverse kinematics. As an improvement over existing techniques, [14,15] proposed a deep learning framework in which a forward kinematics layer is added to a neural network to constrain the output to feasible postures. Specifically, Zhou et al. [14] focuses on hand pose estimation and [15] on full-body pose estimation. As with the MS Kinect, these techniques rely on regular cameras and computer vision algorithms. As such, they are unable to exploit the potential that a full motion capture system has to offer, in terms of both precision and range of motion.

Inputs: The absolute positions (i.e. 3D points) of the markers. Given that we employ an active motion capture system, each marker is tagged with its own unique ID. Some positions can be missing from the data stream if the correspond- ing markers are not visible from a minimum number of cameras, i.e. they are occluded.

The PhaseSpace motion capture system provides us with ab- solute tracking, whereas the Perception Neuron offers relative tracking. As the data streams between these sources are very dif- ferent, it is crucial to devise a sensor fusion algorithm that yields plausible results. From a high-level perspective, the algorithm is divided into a series of steps.

where Xi = [Xix, Xiy, Xiz, 1] and Yj = [Yjx, Yjy, Yjz, 1]. We are again assuming a row-major vector notation. This problem can be solved using exactly 4 non-coplanar markers. If more markers are available, the problem is underdetermined, as there are infinite solutions to the linear system. Therefore, we apply L2 regulariza- tion, which means that among all possible solutions, we choose the one that minimizes the squared norm of the weight vector. In other words, the predicted position should depend on all the other markers, each of which has a small weight; this leads to a better robustness to noise. We minimize the loss function:

Last known position. The simplest baseline consists in keeping the last known position of an occluded marker. With regard to discontinuities on occlusions, this method is temporally consistent. Moving average. Inspired by Piazza et al. [5], we take velocity into account. We keep a moving average of the velocities of each marker over the last k frames (we use k = 20, i.e. one third of a second) to minimize the effect of noise. When a marker is oc- cluded, this baseline simply moves the marker along the trajectory

[26,27]. The output layer uses a linear activation function, thereby allowing an unbounded output range. All hyperparameters were chosen to minimize the reconstruction error on the validation set, also taking into account performance and latency constraints. We also experimented with varying numbers of layers and discovered that more layers lead to overfitting on this specific task (regardless of regularization).

= 0.001. The learning rate is automatically adjusted once the error reaches a plateau; more specifically, it is halved if the error has not improved over the last 5 epochs. The model is trained only on simulated occlusions, as they are the only ones for which a reliable ground truth can be obtained, and with a batch size of 32 samples.

Having both hands in VR gives the user a more natural way to interact with elements of the virtual world. It allows us to perform simultaneous actions, like changing gears while driving, but also to achieve more complex tasks like handling large objects. The provided video illustrates the actions of opening a drawer to grab an object, fingers crossing and shaking hands.

With regard to related work on this subject, previous methods have mainly addressed passive motion capture and limb recon- struction. Out of the few occlusion-handling solutions targeted at active-marker technologies, we investigated [4], which has already been employed in some studies [33]. However, this method re- quires at least three markers for each segment, which follows from the assumption that the distance between neighboring markers is approximately constant. Given that our hand model comprises at most two markers per finger (tip, and optionally, base), we suggest that a data-driven approach is more suited to this task because it adapts better to the specific domain that should be addressed (hand and fingers reconstruction with only 1 or 2 markers per finger, in our case). This also explains why, in our setting, analytic inverse kinematics perform significantly worse: in the absence of intermediate markers, the algorithm has no knowledge of the priors that constitute a good-looking posture.

previous frames, and they can potentially handle discontinuities without manual corrections, although it is not trivial to enforce temporal consistency on occlusions while keeping the target function differentiable. In the future, we would like to experiment with convolutional, long short-term memory (LSTM), and gated recurrent unit (GRU) architectures. We would also like to add seamless support for passive motion capture systems, perhaps as a pluggable encoder.

With the current architecture, we have a single-hand neural network that is exploited independently for each hand. It means that each hand position is reconstructed only with its own mark- ers positions regardless the relative position of the second hand, leading to possible collisions. In the future, we would like to extend the architecture with a second neural network trained with a dataset of two interacting hands. Another important point to address in future work is to improve the enforcement of consis- tent self-contact as this feature has been shown to be critical for supporting the avatar body ownership [1].

