In this paper, we present a uniform approach to simulate and visualize distributed algorithms encoded by graph relabelling systems. In particular, we use the dis- tributed applications of local relabelling rules to automatically display the execu- tion of the whole distributed algorithm. We have developed a Java prototype tool for implementing and visualizing distributed algorithms. We illustrate the di erent aspects of our framework using various distributed algorithms including election and spanning trees.

Visualization and animation of algorithms can assist in the design, in the debugging, in the validation and also in the explanation of algorithms [4,3]. Particularly, visualization may become extremely important for distributed algorithms because of the complexity of interprocess communication and syn- chronization [19]. In a distributed computation, events occur concurrently at many sites, and the state of each processor depends both on its internal actions and on messages received from other processes. Ability to display the exchange of messages and the current states of processes leads to intuition, to understanding and even to improving distributed algorithms. There is an important pedagogical interest associated with algorithm visualization, which can be used by students individually or in class demonstrations [28,33]

We present in this work a method based on local graph transformations to visualize distributed algorithms. Our work goes beyond the known animation of isolated examples of distributed algorithms. We show that a large class of distributed algorithms, which can be described by some graph transformation systems, can be simulated and visualized automatically. Graph relabelling systems and, more generally, local computations in graphs are powerful mod- els which provide general tools for encoding distributed algorithms, for prov- ing their correctness and for understanding their power [14]. We consider an anonymous network of processors with arbitrary topology, represented as a connected, undirected graph where vertices denote processors, and edges de- note direct communication links. An algorithm is encoded by means of local relabellings. Labels attached to vertices and edges are modi ed locally, that is on a subgraph of xed radius k of the given graph, according to certain rules depending on the subgraph only (k local computations). The relabelling is performed until no more transformation is possible. The corresponding con-

algorithms are presented in [23,25,6]. M etivier et al. [20,21] have investigated randomized algorithms to implement distributed algorithms speci ed by local computations. Intuitively, each process tries at random to synchronize with one of its neighbours or with all of its neighbours depending on the model we choose, then once synchronized, local computations can be done. A synchro- nization between two neighbours is called a rendez-vous, and a synchroniza- tion between a vertex and all its neighbours is called a star synchronization. Procedures implementing synchronizations are given and discussed in [20,21]. We use these techniques to visualize the execution of a distributed algorithm. All random local synchronizations throughout the network are displayed, and messages exchanged during these synchronizations are also shown. Hence, the visualization of the execution of the whole algorithm is carried out until termination. We have developed a prototype tool with an interactive visual

The paper is organized as follows. Section 2 introduces graph relabelling systems, and their use to describe distributed algorithms. Section 3 presents a method to simulate and visualize distributed algorithms coded by graph relabelling systems. Section 4 presents future work and concludes the paper.

All graphs we consider are nite, undirected, simple and connected. A graph G is thus a pair (V (G); E(G)); where V (G) is a nite set of vertices and E(G)  ffv; v0g j v; v0 2 V (G); v0 6= vg is the set of edges. Main notions may be found in [26].

An L labelled graph is a graph whose vertices and edges are labelled with labels from a possibly in nite alphabet L. It will be denoted by (G; ), where G is a graph and  : V (G) [ E(G) ! L is the labelling function. The graph G is called the underlying graph of (G; ), and  is a labelling of G. The class of L labelled graphs will be denoted by GL, or simply G if the alphabet L is clear from the context.

Hence, several vertices may be active at the same time. Concurrent steps will be allowed provided that two such steps involve distinct vertices. The computation stops as soon as all the vertices have been activated. The span- ning tree is given by the 1-labelled edges.

Consider a graph representing a network, where nodes correspond to proces- sors and edges correspond to communication channels. The visualization of a distributed algorithm consists of showing and animating its execution. Data exchanged between processors, as well as status and label updates of proces- sors and of channels are displayed on-the- y on the screen. Of course, other interesting events depend on the algorithm itself. For instance, to determine a spanning tree, it is important to mark edges belonging to the spanning tree.

There are three types of local computations as investigated in [20,21]. Im- plementation of these local computations for an aynchronous message passing system needs randomized procedures [2]. For the purpose of visualization, this randomized implementation is useful because it enables the end-user to observe the entire execution of the algorithm. These local computations are:

  Rendez-vous (RV): in a computation step, the labels attached to vertices of K2 (the complete graph with 2 vertices) are modi ed according to some rules depending on the labels appearing on K2: To implement RV, we con- sider the following distributed randomized procedure. The implementation is partitioned into rounds; in each round each vertex v selects one of its neighbours c(v) at random. There is a rendezvous between v and c(v) if c(v) = v; we say that v and c(v) are synchronized. When v and c(v) are

  Local Computation 1 (LC1): in a computation step, the label attached to the center of a star is modi ed according to some rules depending on the labels of the star, labels of the leaves are not modi ed. The implementation of LC1 is the following randomized local election. it is partitioned into rounds, and in each round, every processor v selects an integer rand(v) randomly from the set f1; :::; N g: The processor v sends to its neighbours the value rand(v): The vertex v is elected in the star centered on d and denoted Sv ; if for each leave w of Sv : rand(v) > rand(w): In this case a computation step on Sv is allowed : the center collects labels of the leaves and then changes its label.

  Local Computation 2 (LC2): in a computation step, labels attached to the center and to the leaves of a star may be modi ed according to some rules depending on the labels of the star. The implementation of LC2 is the following randomized local election. it is partitioned into rounds, and in each round, every processor v selects an integer rand(v) randomly from the set f1; :::; N g: The processor v sends to its neighbours the value rand(v): When it has received from each neighbour an integer, it sends to each neighbour w the max of the set of integers it has received from neighbours di erent from w: The vertex v center of the star Sv is elected if rand(v) is strictly greater than rand(w) for any vertex w of the ball centered on v of radius 2; In this case a computation step may be done on Sv : During this computation step there is a total exchange of labels by nodes of Sv ; this exchange allows nodes of Sv to change their labels.

Java language, the processors are simulated by Java threads. To program a re- labelling system, a library of high level primitives allows the user to implement local computations. In particular, three functions (rendezVous(), starSyn- chro1() and starSynchro2()) implementing the previous synchronizations are provided. Moreover, communications between processors can be expressed by primitives such as sendTo(neighbour, message), and receiveFrom(neighbour). An illustrative example shows the implementation of the spanning tree exam- ple discussed in Section 2.

