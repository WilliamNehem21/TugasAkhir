Software failures can lead to substantial costs for the user. Existing models for software reliability prediction do not provide much insight into this financial impact. Our approach presents a first step towards the integration of reliability prediction from the IT perspective and the business perspective. We show that failure impact should be taken into account not only at their date of occurrence but already in the design stage of the development. First we model cost relevant business processes as well as the associated IT layer and then connect them to failure probabilities. Based on this we conduct a reliability and cost estimation. The method is illustrated by a case study.

Applicability of our method is demonstrated in a case study modeling an in- dustrial maintenance management system from ABB. We support our case study with the Palladio Component Model (PCM) [2] [14]. A distinctive feature of the PCM, compared to other reliability prediction approaches, is its combined consid- eration of software failures and hardware unavailability, as well as the support for service usage propagation through parameterized behavioral specifications [14]. Its relevant inputs are the usage profiles and failure probabilities of individual software components. The PCM is well suited for our approach, since it explicitly takes these parameters into account.

We propose a two-phase method. First, a model of the business processes (called business layer) and its links to the hardware/software system (called IT layer) is created. In a second phase, additional reliability and cost information is added to the layers to determine various forms of cost information.

Our approach models the IT layer using the PCM notation as a source of fail- ure information for the business layer. Therefore, failure probabilities for software components and availability properties for hardware resources are added to the IT layer. The addition of this data finally allows a calculation of probabilities for the different process variants based on the system reliability.

In the next step, the system reliability (i.e., the probability of failure on demand for an IT service) is calculated based on the transformation of the PCM model into a discrete-time Markov chain (DTMC). The results include a distinction between failure modes.

Selected failures along with their calculated probability of occurrence can be combined with the cost information to predict the overall expected cost. The ex- pected cost can be used for comparing design alternatives. In the following subsec- tions, we will further elaborate the different steps of our method.

The Object Management Group has developed a standard for business process modeling, the Business Process Modeling Notation (BPMN). We aim to use this language and augment it with cost information that we attach to each step of the process. Because of space restrictions, we will not explain the details of BPMN [19]. We use the term process step synonymously for the BPMN object activity.

Due to the possible failures, the business layer contains not only a model for the planned business case but also for the deviant processes incurred by a failure. For example, if a server crashes, a member of the IT staff might have to restart the machine. Since there is an almost unlimited number of failure scenarios, a pre- selection has to be made in order to focus on the most relevant options. As our example shows, some failures (i.e. the cost drivers) have a dominant cost impact while others are irrelevant.

In our approach, we use the Palladio Component Model (PCM) for modeling the IT layer. The PCM is specifically designed to model component-based software architectures and evaluate their non-functional properties, in particular performance and reliability. We describe the PCM here very briefly; for a more comprehensive description see [2].

ponents, which are eventually activated depending on the system state and the input parameters of the actual service invocations. The modeler specifies the like- lihood of such failures by annotating certain actions within the control flow with a failure probability. Hardware unavailability may lead to service failure, if a hard- ware device is unavailable just when service execution needs it. The modeler spec- ifies the Mean Time To Failure (MTTF) and Mean Time To Repair (MTTR) of a physical resource, which is used to determine its steady-state availability A = MTTF/(MTTF + MTTR), enabling the analysis of unavailability effects. Furthermore, the PCM considers the possibility of communication link failures, namely failures in transmission of a service invocation or return message sent over a network communication link. Comprehensive tool support exists for specifying all model parts and evaluating system reliability (see Section 2.4).

A planned key feature that will be added to the PCM in the future is a distinction between failure modes. So far, the PCM in essence only calculates the probability of any failure affecting the system as a whole. Effectively, for all the relevant points of failure in the business layer, a failure probability has to be calculated. In the context of this motivational paper, we do the calculations by hand, following the basic principles of the PCM as described above.

Our approach uses the PCM to calculate the reliability of an IT service invoked from the business layer. Thereby, reliability is expressed through the probability of success of the service invocation, which means: the probability that no software and communication link failures occur during service execution, and none of the used hardware devices is unavailable (see Section 2.2).

After the probabilities for the failure scenarios have been determined with Markov models, it is trivial to calculate the expected cost of the failure scenarios. As described in Section2.1, the cost for each failure scenario is the difference to the cost of the baseline scenario. The overall expected cost is therefore the sum of all deviating costs in scenario i :

This section describes a case study to illustrate our approach to integrate reliability predictions with financial impact calculations. Initially the system under study is introduced (see Section 3.1). Afterwards, the method outlined in Chapter 2 is applied step by step: first the business layer (Section 3.3) and IT layer (Section 3.2) are modeled and then annotated with costs (Section 3.4) in order to calculate the reliability (Section 3.5). Based on this the total estimated deviating cost are calculated for the example (Section 3.6).

The probability of the first failure mode (server failure) can also be calculated from the DTMC. It is approx. 0.00011, which is the possibility that the call fails due to a server problem (but not due to anything else). The probability of a lost message is the probability of a failure in the last step (but in none of the steps before), which is approx. 0.0002.

process. Given a single installation and a short time span, the actual average cost per call can differ vastly. However, the numbers can be used to compare this software with design alternatives or to evaluate potential improvements. For example, the decision maker can support the choice for one particular system with the advantages due to better reliability properties expressed in monetary values. He can also decide to substitute an old software component responsible for the execution of a process step with high failure costs. In our example server, crashes are irrelevant when compared to the problem of lost messages. Thus the latter has to be scrutinized in more detail.

Observation of systems at run-time is most suited to represent the perspective of the business user who is confronted with the software product after the develop- ment phase. A software system used in a business environment cannot be regarded to remain unchanged over a longer period of time. Adding or exchanging one of the components represents a not to be underestimated factor in the calculation of downtime induced by failures. Unlike other authors (e.g., [17]), we do not incorpo- rate repair actions and their implications on the failure rate. As bug fixing and the introduction of new faults by system extensions are usually not within the area of control of the end user, we do not consider this aspect. Also the application of fault tolerance mechanisms, e.g. integrated into components or one component taking over the functionality of a faulty one, are out of our scope.

Estimation of the failure probabilities is one of the most critical assumptions. We take a parameterized approach, which relies on estimations of domain experts. The main drawback here is that the assessment of such a domain expert is strongly subjective and an expert may not be available [5]. Even though these findings refer to a different problem domain, expert judgment is valuable for estimation, as recent work in the field of software development work effort forecasting indicates

We also do not provide any insight on the time needed for a component in order to propagate its defective state to another, i.e. the delay between a failure in one component leading to a failure in one or more components that depend on the input of the first one. Markov chains are a well established method to model

So far we have no conclusive method to rate the quality of a scenario, i.e. how well it represents different kinds of impact in practice. A more sophisticated analysis of software failure impact as well as a classification is needed and part of further research.

Component-based approaches mainly rely on probabilistic models to determine reliability. One recent example presented a probabilistic model checking tool for the analysis of performance and reliability properties of system, incorporating different failure probability measures [15]. However no account is given for the consideration of different user profiles. [8] explicitly stress the importance of usage profiles as a precondition to software reliability calculation but employ random testing for the reliability calculation without specifying the dependencies of transition probabili- ties. Our approach explicitly models the call propagation trough the components in the IT layer, supported by the PCM.

In summary, none of the approaches has arrived at bridging the gap between the reliability of a system and software failure induced costs. They are targeted at the costs on the side of the software developer but do not satisfy the needs of the business user. Weyuker addresses this problem by estimating the cost of a software failure and the probability of its occurrence [25]. She emphasizes the importance of testing by measuring risk as a function of tests that have been run and also points out that different usage patterns may cause a significant difference in the behavior of the software. However, she does not elaborate on how these patters might look like. Our approach incorporates the business layer as one layer of IT service reliability. We show how reliability prediction on the IT layer can be associated with costs for the end user, introducing a novel view on cost estimation based on established methods.

We presented a novel approach to the integration of reliability on the IT layer of a software system with costing aspects on the business side implied by the first. By demonstrating how business process steps can be associated with the estimated downtime in case of a defect, we offer a method to calculate software failure induced costs. Reliability analysis is supported by an advanced tool, widening the spectrum of parameters to employ. We can therefore consider concrete component structures on the technical level and diverse process scenarios on the business level.

The method allows users to anticipate the software failure related financial im- pact of a given software system thus enabling a comparison between design alterna- tives. It also helps software developers at design time to identify business process steps which are highly influenced by the reliability values of the underlying IT layer. Business experts can profit from the breakdown of cost structures to channel optimization efforts.

Future work includes the inspection of automated generation of failure scenarios as an approach to accelerate analysis for problems with a known resolution both on the business and the technical level. At the moment our approach is still at an early stage and we have yet to add further formalization to the cost assignment. We plan to integrate more sophisticated measures, such as for example Value at Risk and other financial key performance indicators into our model. Furthermore, a way to handle decisions in the business process is needed.

