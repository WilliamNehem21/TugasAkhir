Citation screening is a labour-intensive part of the process of a systematic literature review that identi- fies citations eligible for inclusion in the review. In this paper, we present an automatic text classification approach that aims to prioritise eligible citations earlier than ineligible ones and thus reduces the man- ual labelling effort that is involved in the screening process. e.g. by automatically excluding lower ranked citations. To improve the performance of the text classifier, we develop a novel neural network-based feature extraction method. Unlike previous approaches to citation screening that employ unsupervised feature extraction methods to address a supervised classification task, our proposed method extracts doc- ument features in a supervised setting. In particular, our method generates a feature representation for documents, which is explicitly optimised to discriminate between eligible and ineligible citations.

Higgins and Green (2011). However, owing to the prolifera- tion of the published literature (Bastian, Glasziou, & Chalmers, 2010), the manual production of a systematic review has be- come a time-consuming process, with an average completion time of approximately 2.4 years (Bekhuis & Demner-Fushman, 2012). Greenhalgh et al. (2014). In addition, Shojania et al. (2007) re-

e.g. sentiment analysis (Cambria, 2016), constitutes a direct ap- plication area of our method. Our experiments demonstrate that the proposed method can substantially improve text classification performance. Moreover, our method could be integrated with text search engines, e.g. Apache Solr (Smiley, Pugh, Parisa, & Mitchell, 2015), in order to learn to identify documents that are relevant to individual user preferences.

The vast majority of existing semi-automatic citation screen- ing methods adopts unsupervised document representation tech- niques, such as bag-of-words, to address an inherently super- vised classification task. Therefore, the induced feature represen- tation of documents naturally ignores the readily available class- membership information of manually labelled citations. In this pa- per, we present a new supervised feature representation technique that leverages the class-membership information of the manu- ally screened citations to generate informative document features. The proposed method uses a multi-layer feed forward neural net- work to learn a latent representation of documents that encodes discriminative and class-specific information about the citation screening task.

More specifically, our proposed feed forward neural network is trained on the manually labelled citations, while the hidden layers of the network are iteratively optimised to better discriminate be- tween eligible and ineligible studies. We then extract an embedded feature representation of documents using the fixed weights of the hidden layers. The document embeddings can be integrated with any classification algorithm used for automatic screening prioriti- sation. Following previous approaches (Cohen et al., 2015; Wallace et al., 2010), we use a Support Vector Machine with a linear ker- nel to assign a classification confidence to each citation set and we rank the citation list in order of relevance to the review.

For evaluation, we conduct a series of experiments to investi- gate the performance of our supervised feature induction method when applied to the citation screening task of 23 publicly available systematic review datasets from the medical domain (Cohen et al., 2006; Howard et al., 2016). Experimental results demonstrate that our proposed feature extraction method can reduce the number of items that need to be manually screened without decreasing the sensitivity of the review, i.e. at least 95% of relevant studies are identified by the semi-automatic screening method. Moreover, our neural network-based feature extraction method shows substantial performance improvements when compared to 10 baseline feature extraction methods. The contributions of this paper can be sum- marised as follows:

Kim & Choi, 2012; Wallace et al., 2010). In the BoW model, each document is represented as a sparse, high-dimensional feature vec- tor, wherein the dimensions of the vector correspond to words or phrases that occur in the document. Bekhuis and Demner- Fushman (2012) demonstrated that an automatic text classifica- tion method trained on BoW features achieved substantial work- load savings of 35%-46% on two medical systematic reviews. More- over, the authors showed that single-word features yielded an op- timal performance when compared to bi-gram or tri-gram fea- tures, i.e. phrases consisting of two or three words, respectively. However, a limitation of the BoW model is that the resulting fea- ture space consists of a large number of word-features and there- fore the model is associated with increased memory and com- putational costs when applied to large-scale systematic review datasets (Forman, 2003).

Feature selection methods, e.g. forward feature selec- tion (Cohen et al., 2015) or information gain filters (Bekhuis & Demner-Fushman, 2012), have been previously used to reduce the size of the BoW space, although Adeva et al. (2014) re- ported that such feature selection methods result in insignificant performance improvements.

In  a  study  closely  related  to  our  work,  Hashimoto et al. (2016) presented a variation of the widely popular paragraph vectors (PV) model (Le & Mikolov, 2014), a document representa- tion technique for extracting informative document features. The PV model is a neural network-based feature extraction method that follows a distributional semantics approach to better account for words and documents semantics. More specifically, the PV model trains a shallow neural network, consisting of one hid- den layer, by maximising the conditional probability of a word given its context and the document that it appears. Hashimoto

et al. (2016) modified the original implementation of the PV method in order to model each document as a distribution of latent topics. The authors further showed that their proposed PV method achieved a superior performance on complex, multi- disciplinary reviews when compared to the LDA topic modelling method. However, a limitation of the PV model is that it follows an unsupervised approach to feature representation and therefore the generated feature space is not explicitly optimised to discriminate between eligible and ineligible studies.

The main advantage of our method when compared to previous feature extraction methods is that it follows a supervised approach to extract discriminative document features. Moreover, our method generates a dense and low-dimensional feature space which is eas- ier to manage when compared to the sparse and high-dimensional feature space produced by the BoW model. We further show that our supervised feature extraction method can enhance the per- formance of semi-automatic citation screening when compared to previously used unsupervised feature extraction methods, includ- ing BoW, bibliographic metadata, a low dimensional projection of the BoW space using the Singular Value Decomposition, a topic- based feature extraction method based on Latent Dirichlet Alloca- tion (Bekhuis & Demner-Fushman, 2012; Howard et al., 2016; Miwa et al., 2014; Mo, Kontonatsios, & Ananiadou, 2015) and a topic- based feature induction method which exploits a shallow neural network (Hashimoto et al., 2016). Moreover, we report statistical significant improvements over the baseline methods in several re- view datasets.

In this section, we detail the methodology that we follow to semi-automate the citation screening process of systematic re- views. Firstly, we describe the automatic screening prioritisation framework that we use to evaluate different feature representation methods. We then provide implementation details of our proposed neural network-based feature extraction method.

The labelled set is manually annotated by a human reviewer with include/exclude codes and it is used by the text classification method to learn to discriminate between eligible and ineligible studies. It should be noted that in our experiments we use pub- licly available datasets which were manually annotated with in- clude and exclude codes in prior work (Cohen et al., 2006; Howard et al., 2016; Wallace et al., 2010).

The text classification method firstly uses a feature extraction component to transform the textual content of citations into a nu- merical representation, i.e. feature vectors. In our approach, we de- velop a new supervised feature extraction method that uses a neu- ral network model to generate a discriminative feature represen- tation of documents. Document features extracted by our method are then used as input to a linear SVM classifier. The proposed su- pervised feature extraction method is described in the following section, Section 3.2.

In our approach, we use a straightforward variation of the one- layer denoising autoencoder (DAE), namely a deep DAE, which simply adds additional intermediate hidden layers into the net- work to learn more complex non-linear projections of the input data (Hinton & Salakhutdinov, 2006). Moreover, we use three dif- ferent DAEs to learn potentially different reconstructions of the BoW space. The experiments that we conducted, presented in Section 4.5.3, demonstrate that a multi-branch model architecture that uses multiple DAE components obtains a statistically signifi- cantly better performance, in comparison to a single-branch archi- tecture that uses only a single DAE component. Each DAE consists of 5 hidden layers, whereas we vary the dimensionality of the first and last hidden layer across the three DAEs to obtain different re- constructions of the BoW space. The reconstructed output of each DAE is then used to initialise the supervised feed forward neural network. This type of unsupervised pre-training, where the feed forward neural network is initialised by deep DAEs, has been previ- ously shown to substantially improve the performance of the feed forward network (Erhan et al., 2010).

b) the size of the dataset in terms of number citations that need to be screened, c) the percentage of eligible citations and d) the availability of bibliographic metadata. Each citation in the review datasets consists of a title, abstract and a classification label, i.e. el- igible or ineligible, associated with that citation. Moreover, 18 out of 23 review datasets include additional bibliographic metadata for

The singular value decomposition (SVD) method is a dimen- sionality reduction technique that projects an input high dimen- sional feature space into a dense, lower dimensional space. SVD is different than the widely used Principal Component Analysis (PCA), as it computes eigenvalues and eigenvectors directly on the input data matrix, whereas PCA computes eigenvalues and eigenvectors on the covariance matrix of the input data (Wall, Rechtsteiner, & Rocha, 2003). In the context of this study, we use the SVD base- line method to derive a low dimensional projection of the BoW feature space. The SVD baseline method is implemented using the Scikit-learn library. It should be noted that no prior work has pre- viously evaluated SVD derived features for semi-automatic citation screening. We therefore identify optimal parameter settings, i.e. di- mensionality of the projected space according to the top K eigen- values, for the SVD method using a grid search method on the same two development reviews that we used to fine-tune our su- pervised feature extraction method. Experimental results showed

MeSH tags are single word or multi-word keywords that are manually assigned to every citation indexed by the Medline bibli- ographic database (Lipscomb, 2000). MeSH tags aim at summaris- ing the textual content of citations using a set of descriptive key- words. Considering that MeSH keywords may not always appear in the title or in the abstract of a citation, MeSH-based features can potentially provide complimentary information to BoW fea- tures (Trieschnigg et al., 2009). In order to retrieve MeSH tags from the Medline database, we use the Biopython library (Cock et al., 2009). We then construct binary feature vectors, where each di- mension of the vectors corresponds to a different MeSH tag, while feature values determine the presence or absence of a MeSH tag in a given citation.

The WSS@95% performance of the DAE-FF method shows a dif- ferent pattern on the larger BPA development review, when com- pared to the performance recorded on the smaller Statins review. Here, the performance of the method continuously improves as the number of DAE epochs increases. An optimal WSS@95% score of 0.792 is observed when training the DAE components for 150 epochs, whereas for 200 epochs the performance of the method slightly decreases to 0.782.

The results show that the composite feature extraction meth- ods improved upon the performance of the BoW single-view base- line. Performance gains in terms of the average WSS@95% range be- tween ~ 1% to ~ 6%. The concatenation of LDA with BoW features (i.e. BoW-LDA) achieved the best average WSS@95% of 0.492 among the two-view composite baselines while the four-view composite method obtained a slightly higher average WSS@95% of 0.5 when compared to the BoW-LDA baseline. The DAE-FF method showed a superior WSS@95% score in 15 out of the 23 review datasets and a statistically significant improved performance over the 5 composite baselines in 7 datasets. Finally, our method increased the average WSS@95% score of the composite baselines by ~ 6% to ~ 11%.

The results that we obtained demonstrate that our neural network-based feature extraction method substantially reduced the screening workload of 23 systematic reviews by approximately 56%. However, the workload savings varied across the 23 reviews from a low WSS@95% score of ~ 9% on the Oral Hypoglycemics review to a higher WSS@95% score of ~ 84% on the PFOA/PFOS review. Moreover, we observed a weak correlation (R2 = 0.279) be- tween the WSS@95% performance and the size of the correspond- ing review dataset which was statistically insignificant (p = .197). This indicates that our method can obtain meaningful workload savings on both small and large review datasets.

views (Howard et al., 2016). Here, threshold estimation techniques, such as the S-D rank optimisation (Arampatzis, Kamps, & Robert- son, 2009), can be used to approximate an optimal threshold value. A second limitation of our method is that the underlying neural network-based feature extraction method is trained independently for each systematic review dataset. As an example, in our exper- iments we produced 23 neural network models corresponding to the 23 review datasets. However, different systematic reviews may share one or more more eligibility criteria (e.g. if included studies are randomised control trials) and thus learned document features could be applied to different reviews. As future work, we plan to investigate the use of domain adaptation and transfer learning in order to domain adapt a single feature extraction model across

We have demonstrated that by initialising the feed forward neural network using multiple denoising autoencoders of varying dimensionality we can improve upon the performance of our fea- ture extraction method. We have further performed a number of experiments to assess the performance of our method across 23 publicly available systematic review datasets. It was shown that for 22 out of 23 review datasets the proposed method achieved signif- icant workload savings on at least 10%, while in several cases our method yielded a statistically significantly better performance over 10 baseline feature extraction methods.

Georgios Kontonatsios: Conceptualization, Methodology, Soft- ware, Writing - original draft, Formal analysis, Writing - review & editing. Sally Spencer: Supervision, Writing - review & editing. Peter Matthew: Software, Validation, Writing - review & editing. Ioannis Korkontzelos: Conceptualization, Methodology, Software, Supervision, Writing - review & editing.

