Our work makes substantial contributions to the field. Firstly, we offer an in-depth analysis of leading benchmarks in conversational AI, providing insights into their strengths and limitations. Secondly, we investigate the applicability of existing ethical standards to ChatGPT and propose adaptive standards that ensure ethical and responsible conversational AI practices. Thirdly, we examine prevalent evaluation methods and propose an innovative, multi-dimensional approach to benchmarking ChatGPT. We also underscore the value of user-centered evaluation, and advocate for the integration of user feedback, subjec- tive assessments, and interactive evaluation sessions into the overall evaluation framework.

General Language Understanding Evaluation (GLUE) [15] and its successor, SuperGLUE [16], are benchmarks designed to evaluate the performance of models across a wide range of NLP tasks. GLUE consists of nine tasks, including question-answering, sentiment analysis, and textual entailment. SuperGLUE builds upon GLUE and includes more challenging tasks, pushing the boundaries of NLP models.

BIG-Bench is a benchmark for evaluating large language models, specifically focusing on assessing their performance across various language tasks [21]. It covers a wide range of tasks such as text clas- sification, summarization, translation, question answering, and more. BIG-Bench utilizes a large-scale dataset to provide a comprehensive evaluation of the models. It is an open-source benchmark that promotes collaboration and reproducibility in the research community.

User feedback provides a qualitative evaluation of a conversational AI system. It involves collecting feedback from users regarding their satisfaction, engagement, and overall experience with the system. Ta- ble 3 presents the comparison of various evaluation criteria. Here are some steps to effectively utilize user feedback for evaluation.

Gather Structured and Unstructured Feedback: Collect both struc- tured and unstructured feedback from users. Structured feedback can be in the form of ratings, rankings, or Likert scale responses, while unstructured feedback can include open-ended comments or suggestions. Structured feedback provides quantifiable metrics, while unstructured feedback captures nuanced insights.

Dialogue coherence is another important aspect often overlooked by traditional metrics. A conversation should maintain a logical and meaningful flow. To evaluate this, we propose a Dialogue Coher- ence Measure, which can quantify the degree of coherence in the conversation flow.

User satisfaction is one of the ultimate goals of any conversa- tional AI system. Traditional metrics often fall short in capturing the subjective experience of users. By incorporating user feedback and human evaluation into our framework, we can gather insights into user satisfaction and the perceived quality of conversations.

Lastly, the relevance of responses is another crucial aspect. A re- sponse may be grammatically correct and similar to reference responses (resulting in high scores in traditional metrics) but may still be irrel- evant or inappropriate in a given context. To capture this, we pro- pose a Relevance Measure, which assesses the pertinence of generated responses.

/lexibility in Design: To promote adaptability, benchmarks, standards, and evaluation criteria should be designed with flexi- bility in mind. This includes accommodating future changes and advancements by allowing for iterative updates and revisions based on emerging research, user feedback, and evolving needs. Incorporating modularity in the design enables easy addition or modification of evaluation components as the field progresses.

Iterative Improvement: Approach the creation of benchmarks, standards, and evaluation criteria as an iterative process. Gather feedback from researchers, developers, and the wider community to refine and enhance the criteria over time. Embrace a growth mindset that welcomes continuous improvement as new insights and techniques emerge, keeping the benchmarks and standards up-to-date and reflective of the latest advancements.

Regular Updates and Versioning: Establish mechanisms for regular updates and versioning of benchmarks, standards, and evaluation criteria. Release new versions that incorporate feed- back, address limitations, and adapt to the evolving landscape. Transparently communicate updates and changes to the wider community, ensuring that stakeholders are aware of the latest developments and can align their practices accordingly.

User /eedback and Human Evaluation: Collecting user feed- back is crucial in evaluating conversational AI systems. Surveys and questionnaires can be designed and distributed to gather feedback on aspects like user satisfaction, usefulness, natural- ness, and perceived biases. User ratings can be obtained by al- lowing users to rate individual responses based on relevance, fluency, coherence, and appropriateness. Conducting preference tests enables users to compare and rank different system re- sponses, revealing their preferences and identifying the most

on predefined evaluation metrics. Online evaluation involves deploying the system in a live setting and collecting real-time user feedback. Through techniques like active learning, users can provide feedback on specific responses, which is then used to update the model and improve its performance over time. It uses the following criteria.

	Online Evaluation: Deploy the system in a live setting and collect real-time user feedback. This can be done through active learning techniques, where the system prompts users for feedback on specific responses. The collected feedback is then used to update the model and improve its performance over time.

Determining the precise number of workloads or the types of work- loads sufficient for comprehensive benchmarking is a complex task. In an ideal scenario, benchmarks should cover a broad spectrum of scenarios that a system could encounter. However, it is impractical and nearly impossible to include every possible workload due to the inherent diversity of real-world interactions and applications. Hence, we propose a balanced and representative selection of workloads.

	Relevance Measure: Ensuring relevance in AI responses is cru- cial for maintaining user engagement and satisfaction. For in- stance, in a digital assistant application where the user asks for weather updates, a response about the latest news headlines, al- though perfect in grammar and syntax, is irrelevant. Applications that demand direct answers to user queries, such as virtual assis- tants or customer support bots, should assign a higher weightage to this metric.

Dialogue Coherence Measure: This metric is essential for ap- plications involving multi-turn dialogues. For instance, a tutoring bot should follow the topic discussed, maintain the continuity of ideas, and avoid abrupt topic switches. A higher weightage could be given to this measure in scenarios involving extended dialogues, such as tutoring, therapy, or general conversation bots.

Traditional Metrics (BLEU, ROUGE, METEOR, /1 score, Pre- cision, Recall, Perplexity): Each of these metrics offers different insights about the linguistic capabilities of the model. For exam- ple, in a language translation bot, metrics like BLEU and METEOR would have higher weightage as they measure how close the translated text is to the reference translation. However, in a question-answering bot, Precision and Recall may have higher weightage as they measure how accurately the bot retrieves the relevant information.

In order to compute a comprehensive score, each metric could be normalized to a standard scale, perhaps between 0 and 1 or 0 to 100, to allow for comparison across different measures. Following this, the overall score could be computed as a weighted average of these normalized scores. The weights assigned to each metric could be decided based on several factors such as the specific use case of the model, user feedback, or empirical evidence from pilot studies.

For instance, if the ChatGPT model is primarily used for customer support, higher weight might be given to TSR, Relevance Measure, and CSI, since these would be critical for the successful operation in a customer service environment. Conversely, if the model is being used for creative writing or storytelling, Dialogue Coherence Measure and traditional language generation metrics (like BLEU, ROUGE, METEOR) might receive higher weighting. Furthermore, these weights could be dynamically adjusted based on user feedback. For instance, if users consistently indicate that they value relevance and coherence over perfect grammatical correctness, the weights for Relevance Measure and Dialogue Coherence Measure could be increased, and weights for traditional metrics like BLEU and ROUGE could be decreased.

Leverage Domain Expert Opinions: Domain experts can pro- vide valuable guidance on assigning weights. For instance, a linguistics expert might suggest a higher weightage for tradi- tional NLP metrics like BLEU and ROUGE for language learning applications. Meanwhile, a psychologist might advise prioritiz- ing ethical considerations and context sensitivity for therapeutic applications.

Use a Data-Driven Approach: Machine learning techniques can be applied to automatically adjust the weights based on empirical evidence. Regression analysis, for example, could be used to find the correlation between different metrics and overall user satisfac- tion. The metrics most strongly correlated with satisfaction would receive higher weights.

	Schedule /easibility: The development of this framework would likely be a time-consuming process. Each component of the frame- work, from the task-specific benchmarks to the user-centric evalu- ation, involves substantial research and development. It is crucial to develop a realistic project timeline that accounts for these complexities.

Bias and Fairness Issues: As ChatGPT learns from large- scale datasets, it may inherit biases present in the training data. Ensuring fairness and mitigating bias in the evaluation process is a critical challenge. Failing to address these issues could lead to biased outcomes and ethical concerns.

Application of Reinforcement Learning in Evaluation: In re- inforcement learning-based evaluation, an agent (in this case, ChatGPT) learns to make decisions by taking actions in an en- vironment to maximize some notion of cumulative reward. For instance, a dialogue manager could be trained to optimize the cu- mulative reward of maintaining user engagement and minimizing harmful or inappropriate responses. We outline a reinforcement learning-based evaluation pipeline in Algorithm 1 and provide implementation details to aid in reproducibility.

Data and Representativeness: The availability of diverse and representative datasets is crucial for benchmarking ChatGPT. However, existing datasets may exhibit biases or lack representa- tion across various demographic and cultural groups, leading to skewed model performance. Future research should focus on cre- ating more inclusive datasets that encompass a wide range of lan- guages, cultures, and perspectives. Additionally, techniques such as data augmentation and debiasing methods can be explored to reduce biases in training data.

	Scalability and Efficiency: As ChatGPT becomes more powerful and complex, scalability and efficiency become critical concerns. Handling high volumes of concurrent conversations and ensuring real-time interactions pose challenges in benchmarking and eval- uation. To address these challenges, future research should focus on developing benchmarks and evaluation methodologies that specifically measure the scalability and efficiency of ChatGPT. Techniques such as distributed computing, parallelization, and model compression can be investigated to improve scalability and reduce inference latency.

	Real-Time User /eedback Integration: Incorporating real-time user feedback into the evaluation process can be logistically challenging. Gathering and processing user feedback in a timely manner to provide actionable insights for model improvement is a complex task. Future research should focus on developing efficient mechanisms to collect and process real-time user feed- back during interactive conversations. Techniques such as natural language understanding, sentiment analysis, and active learning can be leveraged to derive meaningful insights and guide model adaptation in real-time.

	Improved Integration in Real-Time User /eedback: Efficient mechanisms for collecting and processing real-time user feedback during interactive conversations should be developed. This can involve leveraging natural language understanding techniques, sentiment analysis, active learning, and reinforcement learning to derive meaningful insights from user feedback in real-time. The integration of real-time user feedback will provide valuable insights for model adaptation, improvement, and personalized user experiences.

Y. Huang, A. Gomaa, S. Semrau, M. Haderlein, S. Lettmaier, T. Weissmann, F. Putz, Benchmarking ChatGPT-4 on ACR radiation oncology in-training (TXIT) exam and red journal gray zone cases: Potentials and challenges for AI-Assisted medical education and decision making in radiation oncology, 2023, Available at SSRN 4457218.

