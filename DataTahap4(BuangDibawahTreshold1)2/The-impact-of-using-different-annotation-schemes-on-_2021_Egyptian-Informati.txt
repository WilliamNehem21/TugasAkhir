Arabic NER tasks. To redeem this knowledge gap, our research aims to study the impact of using different annotation schemes on the task of Arabic NER. To achieve this aim, we set several objectives which are: building a multi-scheme representative dataset, con- ducting a thorough experiment using five machine learning classi- fiers, and gathering and interpreting the outcomes of the experiment.

Several annotation schemes have been used in the literature. However, choosing the ideal annotation scheme is a complex prob- lem [2]. In this study, seven annotation schemes are explored in terms of their influence on the task of Arabic NER. These schemes are the following:

IO: is the simplest scheme that can be applied to this task. In this scheme, each token from the dataset is assigned one of two tags: an inside tag (I) and an outside tag (O). The I tag is for named entities, whereas the O tag is for normal words. This scheme has a limitation, as it cannot correctly encode consecu- tive entities of the same type.

IOBES: An alternative to the IOB scheme is IOBES, which increases the amount of information related to the boundaries of named entities. In addition to tagging words at the beginning (B), inside (I), end (E), and outside (O) of a named entity. It also labels single-token entities with the tag S.

BIES: This scheme encodes the entities similar to IOBES. In addi- tion, it also encodes the non-entity words using the same method. It uses B-O to tag the beginning of non-entity words, I-O to tag the inside of non-entity words, and S-O for single non-entity tokens that exist between two entities.

The rest of this paper is organized as follows. Section 2 surveys the related work in the literature. Section 3 illustrates the proposed methodology to measure the impact of using different annotation schemes on Arabic NER task. Section 4 presents the results and a discussion of the findings. Section 5 concludes this paper.

Cho et al. [8] presented a method for feature generation, which expands the feature space to include multiple annotation schemes. This allows the assigned model to use the most discriminating fea- tures of a complex annotation scheme but also to avoid the issue of data-sparseness by incorporating the features of simple annotation schemes. This method incorporates several annotation schemes in the place of feature functions to support conditional random fields (CRF). This means that this well-established procedure can be applied to training. This study shows that it is possible to increase the tagging speed of a model by applying multiple annotation schemes so that it matches the speed of a more complex annota- tion scheme. This method has also been evaluated against two NER tasks previously: BioCreative 2 gene mention recognition [9], and CoNLL NER shared task [1].

trary decision. Therefore, the authors conducted several experi- ments to study different segment representations. All the segment representations were tested using CRF and maximum entropy (ME), which are both fairly popular machine learning algo- rithms. The tests were applied in four different languages: Czech, Dutch, English, and Spanish. In addition, BILOU performed the worst for English when using CRF, whereas the IOE-1 and IOE-2 versions seemed to perform the best across all languages and methods. This suggests that the choice of using simpler approaches, such as BIO or IOE, or more complex ones like BILOU, is not as clear-cut as it might appear. This research highlights that the best approach varies according to all the factors.

Malik and Sarwar [11] proposed a tagging scheme (BIL2) that is similar to IOBES. The only difference is that the single-token enti- ties are labeled with the tag L. This scheme was proposed as poten- tially effective for subject object verb (SOV) languages that contain postpositioning. Using Urdu as their case study and applying the hidden Markov model (HMM) and CRF, they compared the results of various tagging schemes: IO, BIO2, BILOU, and BIL2. The results showed that BIL2 achieved the highest F-measure in their experiment.

The performance of these classifiers is measured by well-known metrics: precision, recall, and F-measure. Although these evalua- tion metrics are heavily used, in the NER task, specifically, three standards should be considered when applying these evaluation metrics. These standards are message understanding conference (MUC) [21], computational natural language learning (CoNLL) [1], and automatic content extraction (ACE) [22]. They deal with the issue of boundary errors in multi-token entities.

The MUC [21] gives a partial score when the model correctly predicts parts of the multi-token entities. In contrast, CoNLL [1] is an aggressive metric that does not assign a partial score. An exact match of the entities as a whole and a correct classification must be identified to earn credit. This method of scoring is popular because of its simplicity in calculating and analyzing results. The third stan- dard is ACE [22], which considers other factors, such as mention detection and co-reference resolution.

In this paper, we adopted CoNLL as an evaluation metric due to its abundant usage in the Arabic NER tasks in the literature. According to [23], this evaluation metric is heavily used for Arabic NER due to its simplicity in calculating and analyzing the results. Listing 2 shows a code snippet that details how CoNLL metric was adopted in this work.

results is necessary, depending on the intended use case or the application. Therefore, the results of these metrics are reported. The IO scheme achieved the highest recall score of 79.05%. On the other hand, the precision score of the IO scheme was not the best, and it came in the second place at 2.34% lower than the best annotation scheme.

From the previous results, we can conclude that the IO scheme achieved the best results among the rest of the reported schemes. Despite that, it is not fair to compare the IO scheme with the others because, it cannot inherently recognize consecutive entities, whereas others do. This issue will be discussed in more detail in Section 4.1.

Without considering the IO scheme, the BIES scheme achieved the highest precision and F-measure score at 72.78% and 93.89% respectively. In contrast, the BIES scheme came in the third place in the recall results with a slightly small difference of 0.59% from the best scheme (BI).

In the literature, the majority of the researchers have paid con- siderable attention to the type of classifier used in their experi- ments because the chosen classifier has a significant influence on the performance of the model. While this is a valid consideration, most researchers neglect the type of the annotation scheme, which has been proven to have a significant effect on the results of the classifier, as this study reveals. Regardless of the chosen classifier and to measure the consistency of influence of the chosen annota- tion scheme on NER, we calculated the arithmetic mean for each classifier using the seven annotation schemes. Then, we compared the results of the classifier for each annotation scheme with the classifier mean. Following this approach will allow us to determine whether a consistent effect exists when choosing a specific scheme despite the chosen classifier.

Furthermore, and in an attempt to examine the performance of the classifier regardless of the chosen annotation scheme, we have calculated the arithmetic mean of each annotation scheme sepa- rately. Then, we compared the result of each classifier with the cal- culated mean. The results of this process is shown in 5. Based on that, it is obvious that choosing the appropriate classifier is a cru- cial decision to be made, as there are significant differences between the results of the classifiers, regardless of the annotation

As previously shown in Section 4, the IO scheme outperformed the other schemes in terms of recall, precision, and F-measure. However, an important shortcoming of the IO scheme is its inabil- ity to recognize consecutive entities. This might have affected the results in favor of the IO scheme, as it lacks the ability to do so because there is only one label assigned to the entities. Thus, con-

While our research focused on an often neglected area of NER tasks and revealed promising and interesting findings, there are a few concerns and limitations that need further exploration and investigation. Our dataset is devoted to recognizing a single class of entities which is disease names. Despite that we have a suffi- cient number of disease names, exploring the impact of annotation schemes on other named entities such as drug names and gene names is needed to strengthen the findings and conclusion of this research.

In this research, we studied the impact of using different anno- tation schemes on the performance of NER. Our findings show that the IO annotation scheme, which is the simplest one out of the studied schemes, achieved the highest F-measure score. However, the main limitation of the IO scheme is its inability to recognize consecutive entities. Thus, we explored the ability of more complex schemes to do so. Given the results and the rarity of consecutive entities, the performance of these schemes was promising.

