Labelled Markov Processes (LMPs) are probabilistic transition systems where the state space might be any general measurable space, in particular this includes situations where the state space may be continuous. They are es- sentially traditional discrete-time Markov processes enriched with a notion of interaction by synchronization on labels, familiar from process-algebras. They have been studied intensively in the last few years especially in relation with the question of bisimulation [7,10,11,21,20,12]. This is because they embody simple probabilistic interactive behaviours, and yet are rich enough to encom- pass many examples and to suggest interesting mathematics.

The first approximation scheme that we present in this paper is an im- provement of an unfolding scheme [9] that circumvent the second limitation. The two other approximation notions that we present are variants that over- come both limitations. They have been introduced in recent papers by Danos and Desharnais [5] and by Danos, Desharnais and Panangaden [6]. Concern- ing the former, we take a here a slightly different route and correct a mistake in the original paper. We also strengthen the latter notion by considering L0 extended with fixed points operators.

Actually, one can have the best of both worlds, keeping the flexibility of a customizable approach to approximation and staying at the same time within the realm of LMPs. This what the second approach does. It is based on a radical departure from the ideas of the previous approaches [5,9]. In these approaches one always approximated a system by ensuring that the transition probabilities in the approximant were below the corresponding transition in the full system. Here we approximate a system by taking a coarse-grained discretization (pixellization) of the state space and then using average values. This new notion is not based on the natural simulation ordering between LMPs as were the previous approaches.

Given a state property, one says a pointed LMP has this property if its initial state has it. For instance, one says two pointed LMPs are bisimilar when their two initial states are. After the traditional terminology in Markov chains, the map h is called the kernel or the transition probability function of S. Most of the time, we will write h(a, s, A) simply as ha(s, A). It is a measure of the likelihood that being at s and receiving a the LMP will jump to a state in A.

LMPs differ from standard Markov chains in that the kernels are only asked to be subprobabilities and also depend on an auxiliary set L of actions. This seemingly small difference leads to a very different interpretation for them. They are construed as interactive processes which synchronize on labels and therefore one is interested in various notions of bisimulations and simulations as in non-deterministic process algebras [18].

The intent of this definition is to use pre-LMPs as estimators for LMPs. It is not necessary that the estimation engine be of the same nature as what it tries to estimate. What we are interested in is how easy it is to handle and how well it estimates. Pre-LMPs turn out to be better estimators than LMPs as will be illustrated in Proposition 4.19.

Proof. We have three things to verify according to Definition 4.3 above. The first is obvious. For the second condition, the verification that hR(a, [s], .) is a pre-measure breaks down in two subconditions. (We drop the labels since they play no role in the argument.)

To wrap up, we now know for one thing, that pre-LMP support what seems the natural construction, as summarized in Theorem 4.18, whereas with plain LMPs one has to restrict to finite quotients. And with this last proposition, we see that pre-LMPs also give more accurate finite predictors.

We now extend the results of this section to a logic with fixed points. A more detailed account of this fixed point logic was given elsewhere [4]. This extension will allow us to approximate with respect to a much richer class of properties. More to the point, we have seen in section 3 how the introduction of loops in the approximants allows for quicker convergence when there are loops in the transition graph of the original process. In the present section we have just shown how the approximations may be guided by formulas. However, the formulas used only capture one step transitions and one needs richer formulas

Observe that when we say that a state satisfies a logical property, we ex- pect this state to satisfy at least this property, and that it may satisfy other properties as well. Now that our properties are stated in a labelled transition setting, it is tempting to use the corresponding algebraic notion, that is, sim- ulation. Indeed, if we look back to Example 4.25, we can observe that (the reflexive and transitive closure of) the relation (q1, s0), (q1, s1), (q2, s0), (q2, s2) is a simulation relation.

In this section, we present a customizable approach to approximation and stay within the realm of LMPs. The approach is based on a radical departure from the ideas of the previous approaches. In the previous approaches one always approximated a system by ensuring that the transition probabilities in the approximant were below the corresponding transition in the full system. Here we approximate a system by taking a coarse-grained discretization (pixelliza- tion) of the state space and then using average values. This new notion is not based on the natural simulation ordering between LMPs as were the previous approaches.

Thus another, indirect, way to prove the previous proposition, is to use this sandwiching effect and the fact that the infimum and supremum were proven to give approximations in the same sense as proposition 5.7 [5]. This also makes clear in which sense the average-based approximants are better than the order-theoretic ones.

We believe that the present work is an important step towards model- checking LMPs. For example, if one knows what are the properties that a given continuous process should satisfy, one would prefer to check for these properties on a finite faithful approximant of the process instead of checking each property on the process itself. Our construction achieves this goal since it theoretically ensures exact satisfaction of formulas.

However, we also presented an even faster version of approximants which parallels a former construction [9] and introduces additional loops. The incon- venient of this concrete approximation scheme is that one does not have the choice of formulas, except for their depth and a desired precision. The same properties are satisfied: every formula satisfied by a state is eventually satisfied by the state approximant, and finite processes are eventually approximated by themselves.

Ongoing research is also trying to apply this theory of approximants to other probabilistic models such as continuous time Markov chains and to ex- tend it to a richer logic. One potential application are Markov Decision Pro- cesses that one finds in the field of machine learning. Approximants have been studied in this field, but always with a focus on partitioning the state-space without taking account of the behaviour of processes, that is, of the actual transitions that states can take. As a result, bisimilar or behaviourally close states can be split in the process, whereas our constructions always partition the state-space with respect to satisfaction of formulas.

