With inspection techniques, errors might be found before testing which means that they are found in early development stages. Tool support is rec- ommended for this time consuming task. Using a tool that analyzes software automatically and repeatedly should achieve a constant high level of quality. It is my aim to find errors in the design of software systems automatically and therefore to avoid program structures that can not easily be extended and

The application area of a program under examination is forming the view on a design flaw, too. One could imagine that e. g. a mathematical program contains implementations of complex algorithms. One would allow longer methods here than within a program that implements an event driven user interface and basically relies on delegation to library functions.

My approach starts from a mental model of design flaws. Descriptions of design flaws provide hints on how to recognize each flaw. Often blurry indi- cations or vaguely described program structures hint at a flaw. In addition, suggestions on how to remove a design flaw, using refactoring transforma- tions for example, are provided. Hence, it also seems worthwhile to consider program locations where such transformations are applicable.

This newly built mental model of the design flaw is then described as a set of metrics. Every criterion will be mapped to one or more measurable program properties. The result is a set of object-oriented metrics that characterize each design flaw. At this point there is only a vague idea, about what values are expected for each metric and in what combination they would indicate the presence of a flaw. Furthermore, the relevance of each metric to the design flaw is not known. Thus initially, this is a hypothetical model.

Fowler arguments that programs live long if they consist of very short methods. Fowler adds that a key aspect of good methods are well-chosen method names. One indication for splitting a method into smaller ones is the presence of comments. Whenever something should be commented, Fowler suggests to build a separate method of that program fragment.

Fowler argues that multipurpose classes often own too many instance vari- ables. Typically methods use only a fraction of these variables. Thus, the class decomposes naturally into multiple constituent classes. Fowler further claims that a class containing a large amount of code is suspicious of contain- ing duplicated code. Redundancies should be eliminated by extracting short reusable methods out of long methods. Finally, Fowler looks at the usage of a class. When different users employ disjoint sets of methods, decomposition of the class might be sensible.

When designing a learning method, it is first decided what task the learning program should fulfill. After that, it is necessary to model the experience that the program should draw from and the output it should provide. At last a performance measure is defined to check if the system improves over time.

Decision trees are used for classification. Their input consists of a set of criteria or attributes respectively. An attribute is a name/value pair, whose values may be nominal or numeric. The so-called target attribute describes the desired classification of a set of attributes with concrete values. A set of attributes with concrete values is called an instance.

training set. From this a decision tree is constructed recursively in a top-down fashion. According to a special entropy measure the information gain of each attribute is judged. The attribute with the highest information gain becomes the root of the next recursively constructed subtree. Refer to [28, chapter 3] for a detailed description. The C4.5 method, which is used here, has been introduced by Quinlan [33][34].

Instances not classified so far, namely instances without a value for the target attribute, get classified by interpreting the decision tree. Thereby inner nodes of the decision tree model the decision points based on a single attribute. Starting from the root node, a branch is selected that fits the actual value of the attribute. Leave nodes represent the result of the classification.

For this example the number of training examples and the set of attributes used have been abridged. Only the number of fields, methods, and statements of every method in the class and just 12 training instances are shown. Learning mechanisms of this kind construct decision trees with sufficient accuracy from about 100 training examples.

In the detection phase the user states which parts of the system are to be analyzed. Here, all yet unknown program locations are measured and all decision trees of associated design flaws are applied to the measurements. Every flaw tainted program location is shown to the user.

Now the user verifies each individual case and approves the presumed de- sign flaw or rejects the suggestion. In both cases the program location can be added to the training set as another example. If the user skips verification of a case completely, the training set remains unchanged.

For a start the user compares his own idea of the nature of the design flaw with the decision tree. Possibly there are attributes the user rated as meaningful, but which do not appear in the decision tree at all or they appear only far away from the root of the tree. Attributes with a high impact on the decision would be located near the root by construction.

Quite often, an edge leads directly from a root node or some adjoining node to a leaf node. The associated attribute of the edge represents effectively a knock-out criterion. This clarifies the impact of the attribute for the decision to the user even more.

Future work will show if other learning techniques (like bayesian learning) achieve a better detection accuracy than decision tree techniques. But the main advantage of decision trees is that they can be inspected manually, such that the explanation component and model reflection are feasible at all. This would be lost with black box methods.

The IYC plug-in hooks into the user interface of such views. If a program object is selected, e. g. a package or class, the context menu provides options to start searching for design flaws. Results are presented as a list of potential design flaws within an own view.

Program abstraction models. In the reengineering domain [30] [20] mod- els are used to abstract from concrete programs and programming languages respectively. Model instances reflect basic program properties which allow one to derive higher level properties, including object-oriented metrics. In the literature this model instantiation is termed as fact extraction. The well-known Rigi system [40] follows this approach. To my knowledge only complete programs are analyzed. This leads to hardly practicable memory and time consumption. Whereas my proposed rapid analysis examines only those parts of a program the user is really interested in.

I have introduced a method for detecting design flaws in object-oriented soft- ware. As design flaws are interpreted in very different ways, this method adapts itself to specific usage scenarios. I have combined well-known ap- proaches, based on object-oriented metrics, with machine learning techniques and proposed an adaptive and learning approach.

Based on a prototype implementation an initial case study has been con- ducted. To reach a final conclusion an empiric survey is needed. Therefore the prototype tool will be improved and will be made publicly available. An on-line component will allow collection of models and decision trees from a diverse user base for further design flaws. A user profile in the form of an inquiry about programming style and experience, size of development group and constitution, as well as characteristics of the software system built should give insight into factors that are reflected in different instances of a design flaw.

