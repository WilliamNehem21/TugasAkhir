A significant component in computer-aided medical diagnosis is the automatic detection of lung abnormal- ities from digital chest X-rays; thus it constitutes a vital first step in radiologic image analysis. During the last decades, the rapid advances of digital technology and chest radiography have ultimately led to the development of large repositories with labeled and unlabeled images. Semi-supervised learning algorithms have become a hot topic of research, exploiting the explicit classification information of labeled images with the knowledge hidden in the unlabeled images. In the present work, we propose a new semi-supervised learning algorithm for the classification of lung abnormalities from X-rays based on an ensemble philosophy. The efficacy of the presented algorithm is demonstrated by numerical experiments, illustrating that reli- able prediction models could be developed by incorporating ensemble methodologies in the semi-supervised framework.

Mansoor et al. [25] presented an extended review and explained the capabilities and performance of currently available approaches for segmenting lungs with patho- logic conditions on chest tomography images. Furthermore, they divided the lung field segmentation methods into five broad categories, with an overview of relative advantages and disadvantages of the methods belonging to each group.

The remainder of this paper is organized as follows: Section 2 presents a brief description of the semi-supervised self-labeled algorithms and Section 3 presents a detailed description of the proposed algorithm. Section 4 presents a series of exper- iments carried out in order to examine and evaluate the accuracy of the proposed algorithm against the most popular self-labeled classification algorithms. Finally, Section 5 discusses the conclusions and some research topics for future work.

Self-labeled methods constitute prominent SSL methods which address the shortage of labeled data via a self-learning process based on supervised prediction models. This class of algorithms is characterized by their simplicity of implemen- tation as well as their wrapper-based philosophy. From a theoretical point of view, Triguero et al. [36] proposed an in-depth taxonomy based on the main character- istics presented in them and conducted an exhaustive study of their classification efficacy on several datasets. Next, we briefly describe the most relevant self-labeled approaches proposed in the literature which are divided into two main groups: Self- training and Co-training.

iteration they teach each other the most confidently predicted examples. Nigam and Ghani performed an extensive experimental analysis and concluded that the Co-training algorithm outperforms other self-labeled algorithms when a natural ex- istence of two distinct and independent views exists. Unfortunately, the assumption about the existence of sufficient and redundant views is a luxury hardly met in most real-case scenarios. In general, the self-labeled algorithms proposed in the litera- ture are based on the philosophy of these algorithms while most of them exploit on ensemble ideas and techniques.

The Tri-training algorithm [43] is probably the most representative approach, which is based on the ensemble philosophy, and constitutes an improved single-view extension of the Co-training algorithm. Generally, this algorithm attempts to deter- mine the most reliable unlabeled data as the agreement of three classifiers and it can be considered as a bagging ensemble of three classifiers which are trained on data subsets generated through bootstrap sampling from the original labeled training set [13]. Specifically, in each Tri-training iteration, the labeled set of each classifier is augmented with a unlabeled instance, labeled from the other two classifiers in case it disagrees.

Democratic Co-learning algorithm [42] is based on the idea of ensemble learning and majority voting and follows the multi-view theory but from another aspect. More specifically, instead of demanding for multiple views of the corresponding data, it utilizes multiple algorithms for producing the necessary information and endorses a voted majority process for the final decision. Based on the previous works, Li and Zhou [18] proposed Co-Forest algorithm, which is based on the efficient training a number of Random trees on bootstrap data from the dataset. The basic idea of this algorithm is that the assignment of a few unlabeled examples to each Random tree during the training process. Eventually, the final decision is composed by a simple majority voting. It is worth noticing that the efficacy of Co-Forest is based on the utilization of Random trees, although the number of the available labeled examples is reduced. A rather similar approach was proposed by Hady and Schwenker [13] in which they proposed the Co-Bagging algorithm where confidence is estimated from the local accuracy of committee members. It creates several base classifiers using the same learning algorithm on a bootstrap sample created by random resampling with replacement from the original training set. Each bootstrap sample contains about 2/3 of the original training set, where each example can appear multiple times.

In more recent works, Livieris et al. [24] and Livieris [20] proposed some ensem- ble self-labeled algorithms based on voting schemes. These algorithms combine the individual predictions of three self-labeled algorithms i.e. Self-training, Co-training and Tri-training utilizing a difference combination of voting mechanisms. Motivated by the previous works, in [23] the authors proposed a new semi-supervised learning algorithm, called AAST, which dynamically selects the most promising learner for a classification problem from a pool of classifiers based on a self-training philoso- phy. AAST initially uses several independent base learners and during the training process dynamically selects the most promising base learner relative to a strategy

By taking these into consideration, the proposed algorithm is based on the idea of generating a set C = (C1, C2,..., CN ) of N self-labeled classifiers by applying different algorithms (with heterogeneous model representations) to a single dataset and the combination of their individual predictions takes place through a majority voting methodology.

Our experimental results were obtained by conducting a two phase procedure: in the first phase, we evaluate the performance of the proposed algorithm EnSL against the most popular self-labeled algorithms, i.e. Self-training, Co-training, Tri- training, Co-Bagging, CST-Voting, Co-Forest and Democratic-Co learning, while in the second phase, we performed a statistical comparison between all compared semi- supervised self-labeled algorithms.

CT Medical images dataset : This data collection 1 contains 100 images [3] which constitute part of a much larger effort, focused on connecting cancer phenotypes to genotypes by providing clinical images matched to subjects from the cancer genome Atlas [8]. The images consist of the middle slice of all Computed To- mography (CT) images taken where valid age, modality and contrast tags could be found which results in 475 series from 69 different patients. Furthermore, this dataset is designed in order to allow different methods to be evaluated for examining the trends in CT image data associated with using contrast and pa- tient age. The basic idea is to identify image textures, statistical patterns and

The self-labeled algorithms which constitute the ensemble of EnSL are: Self- training, Tri-training utilizing C4.5 as base learner, Co-training using (SMO) as base learner, Co-Forest and Democratic-Co learning. The motivation for this se- lection is based upon the fact that these algorithms have been reported as the most efficient self-labeled algorithms [36]. We recall that these methods are self- labeled ones, which exploit the hidden information in unlabeled data using different methodologies. More specifically, apart from the number of classifiers used by each method, the key concern is whether they are composed of the same (single) or dif- ferent (multiple) learning algorithms. Self-training, Co-training, Tri-training and Co-Forest are single learning methods while Democratic-Co learning is a multiple learning method.

best-performing self-labeled algorithm is highlighted in bold. Similar observations can be made as well with the previous benchmark. Firstly, it is worth mentioning that the proposed algorithm EnSL demonstrated the best performance. Regarding the F1 and Acc metrics, EnSL exhibited the highest accuracy reporting the top performance in all cases, followed by CST-Voting. Finally, the results clearly show that EnSL increased its classification performance as the labeled ratio increased.

the individual predictions of efficient self-labeled algorithms utilizing a majority vot- ing methodology. For testing purposes, the algorithm was extensively evaluated on the chest X-rays (Pneumonia) dataset and the CT Medical images dataset utilizing Self-training, Co-training, Tri-training, Co-Bagging and Democratic-Co learning to constitute the ensemble. Our numerical experiments indicated the efficiency and the classification accuracy of the proposed algorithm EnSL, as statistically confirmed by the Friedman Aligned Ranks nonparametric test as well as the Finner post-hoc test. Therefore, we conclude that reliable and robust classification models could be developed by the adaptation of ensemble methodologies in the semi-supervised learning framework.

