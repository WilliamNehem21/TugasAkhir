Several theoretical analyses of dropout have been conducted, with various researchers investigating its properties and applications. For instance, Baldi et al. illustrated how dropout functions as adaptive stochastic gradient descent, while Wager et al. examined its role as an adaptive regularizer for generalized linear models (GLMs). Ma et al. sought to quantify the disparity between dropout's training and inference phases, demonstrating the potential for leveraging this discrepancy to regularize standard dropout training loss functions.

In our experiments, we initially applied logistic regression to cancer datasets obtained from the Expression Project for Oncology (EXPO). Subsequently, we trained feedforward neural networks (FNNs) on a dataset from the 1000 Genomes Project to predict individual ancestry based on genetic profiles, with all individuals represented in both datasets by their single nucleotide polymorphism (SNP) profiles.

The application of L1 regularization is likely to result in the removal of certain parameters, thereby serving as a feature selection technique. On the other hand, L2 regularization may lead the loss function to primarily intersect with the vertices of a parallelogram rather than its edges.

Srivastava et al. proposed that applying dropout to a neural network with n units can be conceptualized as sampling 2n sub-networks with weight sharing. During the test phase, an approximate averaging method is employed, given that it is impractical to consider the mean of 2n models. This approximation involves adjusting the weights of outgoing connections to account for the retention probability of neurons. While this approximation has been demonstrated for logistic and linear regression models, there remains an unknown disparity between the expected output of exponential sub-networks and the output of a single deterministic model within deep neural networks (DNNs).

It is recognized that dropout can enhance the performance of training models when combined with other regularization techniques such as batch normalization. Throughout our experiments, stochastic gradient descent was employed as the primary optimization strategy. Two distinct types of datasets, namely cancer datasets from the Expression Project for Oncology (EXPO) and ethnicity datasets from the 1000 Genomes Project, were utilized to train logistic regression and FNN models, respectively.

The incorporation of an autoencoder reconstruction path in the training of a neural network led to improved results; however, training an autoencoder alongside the classification network intensified the complexity of the high-dimensional optimization problem and resulted in a higher classification error. To mitigate this, traditional regularization techniques were utilized, demonstrating greater efficacy in enhancing model prediction accuracy compared to dropout.

In this study, we elucidated the optimization technique of stochastic gradient descent with back-propagation for training deep learning algorithms. We explored various regularization techniques to address overfitting and established a theoretical relationship between dropout and L2 regularization. Experimental findings indicated that when combined with techniques such as batch normalization, max-norm, or unit-norm, dropout outperformed L1 and L2 regularization techniques in terms of model performance.