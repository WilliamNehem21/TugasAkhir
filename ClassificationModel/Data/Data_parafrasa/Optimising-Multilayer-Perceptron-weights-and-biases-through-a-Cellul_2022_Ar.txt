These days, it is impossible to think about advances in healthcare without considering the role of artificial intelligence. The large volume of data from various sources, such as medical images, clinical examination data, and sensors, far exceeds the human capacity to process and analyze. For example, while an average radiologist technician might analyze around 215,000 radiographs over a 40-year period, an artificial intelligence method can process the same amount in about an hour.

One appealing aspect of the multilayer perceptron (MLP) is its capability to serve as a universal classifier that can adapt to different data distributions, features, and complexities. This characteristic is highly desirable in the medical field, where medical data can be noisy, have imbalanced class distributions, and contain registration errors. The effectiveness of MLP relies on its learning process, which involves identifying the weights and biases that minimize classification errors of training samples. The backpropagation algorithm is commonly used to facilitate the learning process, but it has weaknesses such as a tendency to become stuck in local optima. This limitation underscores the need for new genetic operators that can enhance the convergence and efficiency of the optimization process, particularly in large-dimensional solution spaces.

This paper presents a study in which a Cellular Genetic Algorithm (CGA) is employed as an alternative approach to optimize the weights and biases of an MLP for medical data classification. To improve the performance of the CGA, a new crossover operator called Damped Crossover (DX) is introduced. The study aims to provide a reliable method for training MLPs and enhancing classification quality. The contributions of this work can be summarized as follows: (1) exploring the properties of CGA to facilitate better exploration and exploitation, and (2) proposing a novel crossover operator to make the evolutionary process more accurate and consider solutions that have not been previously encountered.

Several bio-inspired algorithms have been used for training MLPs in different contexts. These include the Biogeography-Based Optimization Algorithm (BBO), the Whale Optimization Algorithm (WOA), the Grey Wolf Optimizer (GWO), the Butterfly Optimization Algorithm (BOA), and the Velocity Enhanced Whale Optimization Algorithm (VEWOA). Each of these studies has offered insights into enhancing the performance of metaheuristics specifically designed for MLP training, often through the proposal of new genetic operators or algorithm modifications.

An analysis of the numerical performance of metaheuristics and the time consumed to reach stop criteria is conducted in this paper. The evaluation involves tests on five different benchmark datasets, and the results show that the CGA with DX crossover achieves competitive accuracy results with all the datasets. It is worth noting that the DX variations demonstrated superior performance in optimizing the weights and biases of the MLP across different datasets, indicating their effective task efficiency and ability to enhance classification outcomes.

In addition to the numerical performance analysis, metrics of classification quality demonstrated that the CGA variations with DX crossover produced competitive results in terms of specificity and sensitivity, leading to comparable levels of learning and generalization compared to other approaches.

The study concludes by highlighting the potential of the DX variations to provide noteworthy improvements in optimizing the weights and biases of MLPs, and their ability to improve classification performance across diverse datasets.