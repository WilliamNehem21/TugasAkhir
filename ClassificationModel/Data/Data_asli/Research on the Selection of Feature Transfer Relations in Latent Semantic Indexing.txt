with the development of information technology, a lot of document resources are needed that helps the discovery of the theme, information retrieval, and so on. therefore, text clustering technology came into being. it is a very important part of natural language processing. text clustering technique made great success in document clustering. there are a large number of synonyms, near-synonym and other unique natural language phenomena in document clustering. we will use lsi to explore and resolve these linguistic phenomena to improve the performance of document clustering in the paper.



the corpus tancorpv 1.0 used in the experiment is from the chinese academy of sciences, dr tan songbo and the text classification corpus from sogou lab. 12 classes are randomly selected from the 12 categories in the tancorpv 1.0 which is 2,400 texts in all named as the chinese academy of science corpus 1, the smallest text is 1kb, and the largest one is 14.7kb. one thousand texts are randomly selected in 9 classes from the text corpus of the sogou lab whose largest class contains 200 documents and whose smallest class contains 80 documents. 3000 texts are randomly selected from 60 smaller classes named as the chinese academy of science corpus 2.



in this paper, we think that the transfer number between features has a great impact on the performance of latent semantic indexing. as the feature transfer number increases, some non-existent feature co-occurrence information appear which affects the similarity between features so that affects the performance of the latent sematic indexing. before the decomposing of svd, the feature of document collection is selected by df feature in order to reduce the feature transfer number and non-existent feature co-occurrence information. the df method used by our paper can selected features with documents in document collection and simply filters the transfer number between features. the next step, we will study on the feature selection based on conditional entropy between the features and conditional entropy.



