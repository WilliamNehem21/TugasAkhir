In this paper a new technique is introduced for bias compensation of gyroscopes with a focus on mobiles phones. The standard problem of using gyroscopes is that integration of raw angular rates with non-zero bias will lead to continuous drift of the estimated orientation. To examine the nature of this bias, a simple error model was constructed for the whole device in terms of inertial sensing. For eliminating the bias, a sensor fusion algorithm was developed using the benefits of optical flow from the camera of the device. Our orientation estimator and bias removal method is based on complementary filters, in combination with an adaptive reliability filter for the optical flow features. The feedback of the fused result is combined with the raw gyroscope angular rates to compensate the bias. Various measurements were recorded on a real device running the demanding optical flow onboard. This way a robust and reliable fusion was constructed, which matched our expectations, and has been validated with simulations and real world measurements.

Orientation estimation is a significant and desired feature to be implemented in more and more reliable and robust way. Pedestrian tracking, controlling unmanned aerial vehicles (UAV), professional cinematography with post-processing, and many other fields may take advantage of these techniques. Unfortunately available sensors on the market are not capable of producing stable and reliable output on the long-term. For this

Camera images used for optical flow are rather laggy and imprecise in time, for these are delivered at 30 fps in best circumstances, but this value tends to fall below 20 fps caused by external lighting condition changes and demanding system load on the CPU. For this, an interpolation stage was added to the system to match optical flow samples consistently with real gyroscope values. In our measurements we used the iterative Lucas-Kanade method with pyramids by Bouguet, 2000 [2] to track 25 - 225 features on the images. The feature point selection was done by distributing these features equally on the image in that case, when the count of matched features fall below 65%. From the results of feature displacement of optical flow a vector field was calculated and used for the fusion algorithm. The aggregation of these vectors into a final output vector can be performed in various ways, and is out of the scope of this paper. In case the device has only orientation changes but no physical movement, the output values can reliably be used for absolute, so-called world-frame orientation estimation.

In comparison with others, in [3], authors presents a motion model-based method for robust estimation of orientation via fusing the inertial measurements with the imaging sensor’s data. Their method yields a robust recursive estimator of the gyro error parameters, which is independent of the scene’s structure. The algorithm’s performance is demonstrated using synthetic data as well as visual data extracted from an image stream of high-fidelity computer-generated urban scenes.

The accuracy of optical flow estimation algorithms have been improving steadily as evidenced by results on the Middlebury optical flow benchmark. In [4] the authors attempt to uncover what has made recent advances possible through a thorough analysis of how the objective function, the optimization method, and modern implementation practices influence accuracy. They have found that while median filtering of intermediate flow fields during optimization is a key to recent performance gains, it leads to higher energy solutions. In order to understand the principles behind this phenomenon, they have derived a new objective that formalizes the median filtering heuristic and they have developed a method that ranks at the top of the Middlebury benchmark.

Recently, in paper [5] authors introduce a state estimation framework that allows estimation of the attitude, of an IMU-camera system with respect to a plane. The filter relies only on a single optical flow feature as well as gyroscope and accelerometer measurements. The underlying assumption is that the observed visual feature lies on a static plane. The estimation framework fuses visual and inertial measurements in an Unscented Kalman Filter (UKF). Compared to our work, authors only use one optical feature, and it has to be on a static plane, whereas our approach does not require such limitation.

Simulations were done in MATLAB after all raw data had been recorded on the mobile device. For testing different critical situations, various datasets were generated and also used to measure reliability of the algorithm. In this paper focus is taken on real world measurements.

In order to construct a precise orientation estimator the error model of the device needs to be described. For this, we performed various measurements on the device. The factors that affect all the measurements can be categorized in two groups: external thus independent, and dependent on device orientation and movement. Based on the sensors that have been applied in the sensor fusion, the following model was constructed. It can be safely stated, that all the sensors have different amount of Gaussian noise. All the other error functions have been marked on figure 1.

As it had been already discussed by many [1][6], accelerometer data has a great amount of perturbation in the aspect of orientation estimation, for it not only measures the useful gravity, but has additive linear acceleration that perish the results. This linear acceleration is based on actual device movement for which no estimation can be constructed forward. With the use of an adequately filtered accelerometer, two angles of the device can be estimated on the long term for its low-pass nature.

To find the third absolute angle, magnetometers can be utilized. Measuring the geomagnetic field that is strictly not parallel with the gravity vector, a 3D Earth-based absolute orientation can be constructed using the previous filtered gravity and the geomagnetic data. Although, magnetic field is often perturbed mostly in non- natural scenes, such as buildings, underground, close to wiring or electrical devices. If an orientation estimation algorithm relies on the fact that on the long term magnetometer values are trustworthy, the output angles will be deceiving in the mentioned situations.

Continuing with the gyroscope, it definitely suffers from a couple of errors and perturbation [3]. As it is marked on figure 1, its additive error over a standard Gaussian noise is a function of time and current orientation. Finding the precise error function is not the purpose of this paper, however the basic behavior of the sensor must be known for better filtering, estimation and bias-removal. Based on our measurements, we found that the bias of the gyroscope has changed over time: it always had a different mean at every measurement, repeated 10 times each with 10 minutes delay. Orientation dependent bias effects were also measured: rotating the device 90° on different axes each time, and waiting and measuring in a steady state for a minute, in each direction all the gyroscope values have been changed at every orientation, even at returning to the initial one.

Bias added to an ideal gyroscope will lead to the drift of orientation. However, this bias can be estimated and compensated. For this, it is assumed that a precise orientation estimator is already available using multiple sensors including the gyroscope. As absolute orientation can reliably calculated from magnetometer, accelerometer and optical flow, the advantage of using a gyroscope is that it responds in a more reliable and precise way to fast orientation changes compared to the other sensors. Considering that a bias is added to the ideal gyroscope value, the previous estimated angular rate can be subtracted, thus resulting in the difference to the previous estimation plus the bias. Fortunately, this two can be separated for bias is changing in a slow rate

The basic idea is to take advantage of the calculated optical flow from the camera to refine IMU measurements. Finding the optical flow of an image sequence is a combination of two steps: finding features in the first picture, and tracking them through the new images (see figure 2). In case the number of detected features are lowering below a specified amount, a new feature finding process is executed. In our tests we chose this minimum detected feature count to be the 65% of the initially found ones.

The angular rate estimation from optical flow is calculated from the aggregation of reliable flow vectors. A flow vector (that is a vector between the two consecutive positions of a feature on the images) is considered trustworthy based on how its movement relates to the angular rate of the gyroscope. Without further specifications of this relationship and decision algorithm, it can be stated that flow vectors are marked as reliable or unreliable at every cycle in an adaptive way. This way, the output of the reliability calculation (see figure 3) has less or equal elements as its input, however for angular rate fusion an aggregation is needed to combine these vectors. This aggregation can be a simple averaging or median computation, but the use of more advanced techniques like weighted median may produce more satisfying results. In the measurements of this paper, we used the median in each measured direction. The length of this vector from pixels per sample time can be calculating to radians per sec using the resolution of the image and the field of view.

The filtering algorithm is constructed by three main blocks. The reliability calculation and aggregation has already been discussed. On its input it receives all the feature displacements and the bias compensated angular rate of the gyroscope. Both the result of optical flow and the compensated gyroscope value are processed by a complementary filter. The aim of this block is to create a good balance between slowly changing but on the long-term precise optical flow results, and the fast changing gyroscope angular rates. Complementary filters are easy to implement, quick to be computed, and in this case the results are satisfying. For it is assumed that the final output angular rate will be used by a sophisticated orientation estimator that combines it with acceleration and geomagnetic data, (like extended Kalman filters or the algorithm of Madgwick [1]) it is unnecessary here to apply such for the results. On its input it high-pass and low-pass filters the data, and uses

a standard complementary filter technique to add them with a weighting factor α. The output ω* is a fused, reliable and precise angular rate, that is also used for compensational feedback. In our tests we constructed and used FIR filters with a sampling frequency of 40Hz, order of 30, and cutoff frequency of 0.2Hz.

Compared to the introduction, an additional low-pass filter was added to the bias compensational block. The main reason was to filter out the effects of Gaussian noise of the input ωg from the bias compensations, for these two are completely independent. This way only the bias compensated angular rates are passed to the complementary filter and to the reliability calculation that relies on ideal gyroscope values (see figure 3).

In our measurements we used to rotate the phone on two of its axes with 90° and 60° respectively. The number of feature points of optical flow were altered between each measurement, recording the rotation with 25, 36, 100, and 225 features. While the 25 features could have been tracked with an average frame rate of 15, a more demanding test with 225 features overwhelmed the processor falling between 1 – 2 fps. In the following the results of measurement with 36 feature points are shown. With the above described algorithm and parameters we could compensate the bias, resulting in a very low-drifting angle. On figure 4/a, the recording interval was more than a minute, whereas no significant drift resulted compared to the integral of the raw angular rates. These values are a result of only optical flow and gyroscope fusion in a reliable way, no accelerometer or magnetometer data was used. The only downside of the current version of this algorithm is that integrated final angle not always fall between -45° and 45°, as it supposed to be by the symmetric rotation of 90°. We also measured the same quality of outputs for the other cases. On figure 4/b, the estimated bias compensation level is plotted with the input raw angular rates of the gyroscope.

In this paper we showed that with the use of optical flow from the camera of today’s mobile devices a reliable filter can be built used for bias compensation of the gyroscope. In our test cases we could tune the parameters and filters to provide a satisfactory result. Compared to the raw integration of angular rates from the gyroscope our bias compensation resulted in a long-term stable angle. Although bias compensation was calculated in MATLAB on PC side, the whole heavy optical flow processing was done on the device,

Future plans include testing of other methods for the correlation observation of gyroscope angular rates and optical flow feature displacement values. Other plans include the testing of Kalman filters, and RANSAC for the optical flow processing. The aggregation method of the reliable features has also further possibilities by using more sophisticated methods, like weighted median filters.

