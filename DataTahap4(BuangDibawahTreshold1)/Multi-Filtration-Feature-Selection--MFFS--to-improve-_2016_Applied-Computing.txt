Abstract Selection of optimal features is an important area of research in medical data mining systems. In this paper we introduce an efficient four-stage procedure – feature extraction, feature subset selection, feature ranking and classification, called as Multi-Filtration Feature Selection (MFFS), for an investigation on the improvement of detection accuracy and optimal feature subset selection. The proposed method adjusts a parameter named ‘‘variance coverage’’ and builds the model with the value at which maximum classification accuracy is obtained. This facilitates the selection of a compact set of superior features, remarkably at a very low cost. An extensive exper- imental comparison of the proposed method and other methods using four different classifiers (Naı¨ve Bayes (NB), Support Vector Machine (SVM), multi layer perceptron (MLP) and J48 deci- sion tree) and 22 different medical data sets confirm that the proposed MFFS strategy yields promising results on feature selection and classification accuracy for medical data mining field of research.

prediction of usefulness of surgical procedures, clinical tests, medication procedures, and the discovery of associations among clinical and diagnosis data [37]. The applicability of data mining for healthcare applications is increasingly gaining importance. The availability of diverse-natured medical data for diagnosis and prognosis and of pervasive data mining tech- niques to process these data offers medical data mining a dis- tinctive place to truly assist and impact patient care.

For instance, the physicians can evaluate the diagnostic infor- mation of many patients with identical conditions. In the same way, they can verify their findings too, with the conformity of peer physicians working on similar cases in other parts of the world. The patterns that are discovered denote valuable knowledge that helps medical discoveries, for example discov- ering that a certain combination of features may help in better, and more accurate diagnosis of a particular disease. Accurate diagnosis of diseases and subsequently, providing efficient treatment, form an important part of valuable medical services given for patients in a health-care system.

The unique characteristics of medical databases that pose challenges for data mining are the privacy-sensitive, heteroge- neous, and voluminous data. These data may have valuable information which awaits extraction. The required knowledge is found to be encapsulated in/as various regularities and pat- terns that may not be apparent in the raw data. Extracting such knowledge has proved to be priceless for future medical decision making. Feature selection is crucial for analysing various dimensional bio-medical data. It is difficult for the biologists or doctors to examine the whole feature-space obtained through clinical laboratories at one time. In machine learning, all the computational algorithms recommend only few significant features for disease diagnosis. Then these rec- ommended significant features may help doctors or experts to understand the biomedical mechanism better with a deeper knowledge about the cause of disease and provide the fastest diagnosis for recovering the infected patients as early as possible.

previous models, and uses an independent measure to identify the best subsets for a given cardinality and applies a mining algorithm to select the best subset among all best subsets across different cardinalities. However, the ensemble of a filter based model with another filter based model, once for subset selection and again for ranking proves to be a promising approach, for medical data mining. The ensemble is brought about in a fashion so as to reduce the number of features and also to enhance the classification accuracy.

The objective of this research work is aimed at showing that the selection of more significant features from the available raw medical dataset helps the physician to arrive at an accurate diagnosis. The primary focus is on aggressive dimensionality reduction so as to end up with increase in the prediction accu- racy. The features are subjected to a double filtration process, at the end of which, only the features that increase the accu- racy, and form the subset with the lowest cardinality, with their corresponding rank, are obtained. The method employs an efficient strategy of ensemble feature correlation with rank- ing method. The empirical results show that the proposed Multi Filtration Feature Selection (MFFS) embedded classifier model achieves remarkable dimensionality reduction in the 22 medical datasets obtained from the UCI Machine Learning repository [10] and Kentridge repository [13].

It could be observed that the naive Sequential Forward Feature Selection (SFFS) (pure wrapper approach) [5] is impractical for feature subset selection from a large number of samples of high-dimensional features. Hence Gan et al. [4] proposed the Filter-Dominating Hybrid Sequential Forward Feature Selection (FDHSFFS) algorithm for high dimensional feature subset selection. This method proved to be fast but demanded huge computational complexity. Another variant of the SFFS method called improved F-score and Sequential Forward Search (IFSFS) was proposed by Xie and Wang

[36] for feature selection to diagnose erythemato-squamous disease. This method was designed so as to improve the F-score and measured the discrimination between more than two sets of real numbers instead of measuring between only two sets of real numbers. The method’s applicability to other medical data sets was not reported and hence it was a very specific system targeted at the diagnosis of erythemato- squamous disease only.

Another category of feature selection methods used Mutual Information score. Vinh et al. [32] proposed a novel feature selection method based on the normalization of this well- known mutual information measurement and utilized the information measurement to estimate the potential of the features. The method could not eclipse the strongly correlated features impact on the classification results. Correlated features may be accounted for redundancy and hence a single representative feature from that subset may be selected for further processing.

Ruckstieb et al. [26] and is called as Sequential Online Feature Selection (SOFS). Another Scatter Search-based approach coupled with Decision Trees (SS+DT) is proposed by Lin and Chen [17]. The method acquired optimal parameter set- tings and selected the beneficial subset of features that resulted in better classification results. In [16] Koprinska empirically evaluated feature selection methods for classification of Brain–Computer Interface (BCI) data. A new feature selection method based on rough set theory has been proposed by Paul and Maji [23]. The proposed method identified discriminative and significant genes from high-dimensional microarray gene expression data sets.

Correlation Based Filter [3,18] is another strategy for fea- ture selection. Ensemble methods have also been proposed. Raymer et al. [25] proposed a hybrid algorithm that coupled a genetic algorithm with k-nearest-neighbour classifier and applied it for protein–water binding from X-ray crystallo- graphic protein structure data. MonirulKabi et al. [20] pre- sented a new Hybrid Genetic Algorithm (HGA) for Feature Selection (FS), called as HGAFS. It employed a new local search operation that is devised and embedded in HGA to fine-tune the search in feature selection process. The search process is guided in such a way that the less correlated (dis- tinct) features consisting of general and special characteristics of a given data set are generated in subsequent iterations.

A new approach called Redundancy Demoting (RD) has been proposed by Osl et al. [22]. It takes an arbitrary feature ranking as input, and performs improvement in ranking by identifying redundant features and demoting them to positions in the ranking in which they are not redundant. Hybrid schemes that combine wrapper-based and filter-based approaches are also in the literature [2,11,30] are such schemes where the features are ranked and then selected so as to offer superior classification accuracy. In the first stage, the filter model is used to rank the features by the relief algorithm and then the highest relevant features are chosen to the classes with the help of the threshold. In the second stage, they used shapely values to evaluate the contribution of features to the classification task in the ranked feature subset. Tanwani et al. [31] gave a study on comprehensive evaluation of a set of diverse machine learning schemes on a number of biomed- ical datasets. Sanchez-Monedero et al. [27] studied and pro- posed the suitability of Extreme Learning Machines (ELM) for resolving bio-informatics and biomedical classification problems.

After reviewing the works on feature selection for medical dataset [29] it is observed that most of the existing methods suffer from the following problems: (1) depending on the com- plexity of the search method, the iterations of evaluations are too large; (2) they rely on a univariate ranking that does not take into account interaction between the variables already included in the selected subset and the remaining ones. More- over, a method that produces the best accuracy employs more number of features and hence more running time is involved in the construction of the respective classifiers. Contrarily, a method that outputs the fewest number of features produces inferior detection accuracy. A holistic and universal method that achieves the best classification accuracy with fewest features possible is still an open research problem. This paper makes an attempt to design such a feature selection sequence and it is called as ‘‘Multi Filtration Feature Selection (MFFS)’’.

Let us denote the multi-dimensional dataset in the form of a matrix, A. The dimensions actually represent directions along which the data vary. The feature generation process, which removes the irrelevant features and redundant features, mainly finds an approximate ‘‘basis’’ to the set of directions. Only the crucial dimensions that serve as the corner stone upon which other dimensions are dependent are generated from the given dataset. The redundant-duplicate dimensions are finally elimi- nated with the sense that they can be reconstructed easily from the available set of basis dimensions. This is equivalent to find- ing the dimensions with maximal variance, since the points are found to be constant approximately along other dimensions. Variance factor is an important measure that denotes the degree of data spread in a multi-dimensional dataset. Thus dimensionality reduction is effectively contributed from this first step of filtration by choosing appropriate variance factor at which the system yields the minimum number of features with maximum accuracy.

The basic theoretical idea behind PCA is finding the princi- pal components of the dataset that correspond to the compo- nents along which the variation is the most. This is achieved by finding the covariance matrix, i.e., we find the principal com- ponents of the data, which correspond to the components along which there is the most variation. This can be done using the covariance matrix, AAT for our input matrix A, as follows. Let the eigen values be represented as ki for the covariance matrix. Then, the corresponding diagonal matrix is given in

The suggestion used by the CFS is on the basis that always features strongly correlated with the predicted class form the good feature subset than the features correlated with each other. The feature subset created by the CFS is computed by the merit of the feature subset ‘S’ containing ‘k’ features as in Eq. (4).

It is apparent from Eq. (3) that the columns of this matrix P represents the principal components of the original matrix and hence confines to the directions of most variance [39,38]. PCA employs the entire features and it acquires a set of projection vectors to extract global feature from given training samples. The approach mainly consists of three primary processes such distinction process, binary session and pattern generation [29]. All these flavours make PCA [21] more suit- able for applying on medical datasets, which typically have these characteristics. The variance coverage factor is playing a significant role in deciding the important features and hence this parameter is tuned so as to capture the classifier model

The correlation between each feature and the class and between two features can be measured and best-first search can be exploited in searching for a feature subset of maximum overall correlation to the class and minimum correlation among selected features. This is realized in the Correlation- based Feature Selection (CFS) method [7]. Correlation based Feature Selection is an algorithm that couples this evaluation formula with an appropriate correlation measure and a heuris- tic search strategy. CFS quickly identifies and screens irrele- vant, redundant, and noisy features, and identifies relevant features as long as their relevance does not strongly depend on other features. CFS is a fully automatic algorithm––it does not require the user to specify any thresholds or the number of features to be selected, although both are simple to incorporate if desired. CFS operates on the original (albeit discretized) fea- ture space, meaning that any knowledge induced by a learning algorithm, using features selected by CFS, can be interpreted

In spite of feature extraction and selection, a problem is persis- tent namely the classifier may be biased towards the attributes with more values. Hence this biased nature has to be elimi- nated for which we employ Symmetrical Uncertainty (SU). It overcomes the problem of bias towards attributes with more values, by dividing information gain by the sum of the entro- pies of feature subsets Si and Sj.

Symmetry is a desired property for a measure of correla- tions between features. However, information gain is biased in favour of features with more values. Furthermore, the val- ues have to be normalized to ensure they are comparable and have the same influence. Therefore, we choose symmetri- cal uncertainty. It compensates for information gain’s bias towards features with more values and normalizes its values to the range [0; 1] with value 1 indicating that knowledge of the value of either one completely predicts the value of the other and value 0 indicating that X and Y are independent. In addition, it still treats a pair of features symmetrically. Entropy-based measures require nominal features, but they can be applied to measure correlations between continuous features as well, if the values are discretized properly in advance. Therefore, we use symmetrical uncertainty in this work.

As CFS uses the best-first strategy search method to calcu- late the merit of the feature subset, however there is a necessity to fix the stopping criteria. Due to this strictly needed constrain correlation between features is computed using Symmetrical Uncertainty (SU) as specified in Eq. (6).

The proposed system is validated against standard successful classifier models [35]. Classifiers are constructed with the final subset of features obtained after subjecting the datasets to RFGP, FRP and FRRP steps sequentially. A detailed insight into various classifiers is presented in Section 4.4.

The tests are carried out in a system with Intel i5, 8 GB RAM, DDR3, 500 GB hard drive on a Windows XP operating system. The proposed algorithm is implemented using Weka [34]. WEKA is acknowledged as a landmark system in the field of machine learning and data mining. It has attained wide- spread acceptance among the academia and industry spheres, and has become a widely used tool for data mining research. Another flavour that is highly encouraging is its ‘‘Open Source’’ nature. The free access given to the source code has enabled us to develop and customize the modules matching our work. The stepwise approach is as follows. The input to the system is given in the Attribute-Relation File Format (ARFF). The proposed algorithm is executed and the features in the ranked order are obtained as the output. A result is

and then they are added to the created table. 10-fold cross validation is performed for all classifiers [8]. Fifty runs were done for each classification algorithm on each dataset with features selected by MFFS method. In each run, a dataset was split into training and testing set, randomly. The results obtained are shown in Tables 2–9.

Naı¨ve Bayesian Classifier is a simple probabilistic classifier [35] with an assumption of conditional independence among the features, i.e., the presence (or absence) of a particular feature of a class is unrelated to the presence (or absence) of any other feature. It only requires a small amount of training data to estimate the parameters necessary for classification. Many experiments have demonstrated that NB classifier has worked quite well in various complex real-world situations and outper- forms many other classifiers. Kernel estimation has been used in cases of datasets with numerical attributes. Also supervised discretization is done for converting numerical attributes to nominal ones.

SVM [13,19] finds the hyper plane with maximum margin in between two classes. The Support Vector Machine (SVM) is actually based on learning with kernels some of which form the support vectors. A great advantage of this technique is that it can use large input data and feature sets. Thus, it is easy to test the influence of the number of features on classification accuracy. We implemented SVM classification [28] for two types of kernels: polynomial kernel and Gaussian kernel (Radial Basis Function – RBF). The SVM model with com-

A decision tree [1,33] is a predictive machine-learning model that decides the target value (dependent variable) of a new sample based on various attribute values of the available data. Decision tree’s internal node represents different attributes; the branches between the nodes tell us the possible values that these attributes can have in the observed samples, while the end nodes are the target class labels. The J48 decision tree clas- sifier [24] operates on the basis of constructing a tree and branching it based on the attribute with the highest informa- tion gain. The J48 tree with binary split allowed a confidence factor of 0.25 and reduced error pruning is employed.

An MLP [15] can be viewed as a logistic regression, where the input is first transformed using a learnt non-linear transforma- tion. The purpose of this transformation is to project the input data into a linearly separable space. This intermediate layer is referred to as a hidden layer. We have employed a back- propagation network with 0.3 as learning rate and 0.02 as momentum. The attributes are normalized in the range of (0.1, 0.9). The training was carried out for 500 epochs.

It can be seen from Tables 2–8, that the classification accu- racy based on the selected subsets by the proposed MFFS scheme is better than that based on the original feature set. This indicates that the selected feature subsets are representa- tive and informative and, thus, can be used instead of the com- plete data for pattern classification. The list of such selected features is shown in Table 7.

From the empirical results obtained so far, it is worth noting that each method has its strengths and limitations. In particular, CFS obtains good classification accuracy in the least amount of running time but at the expense of selecting many more features; PCA selects the least number of features but suffers in terms of classification accuracy and also requires more running time than others; MFFS attains the best accu- racy and robustness in a reasonable time with lowest number of features. Considering all these factors, the proposed MFFS scheme shows overall better performance than other methods. Tables 10 and 11 summarize and compare characteristics of

In this paper, we have proposed an efficient Multi Filtration Feature Selection (MFFS) method applicable to medical data mining. Empirical study on 6 synthetic medical datasets sug- gests that MFFS gives better over-all performance than the existing counterparts in terms of all three evaluation criteria, i.e., number of selected features, classification accuracy, and computational time. The comparison to other methods in the literature also suggests MFFS has competitive performance. MFFS is capable of eliminating irrelevant and redundant fea- tures based on both feature subset selection and ranking mod- els effectively, thus providing a small set of reliable features for the physicians to prescribe further medications.

