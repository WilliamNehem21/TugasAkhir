Modifications of model architecture are an essential factor in im- proving the performance of models. The first Deep Learning has ex- perienced considerable modifications up to the present day. Such modifications include structural reformulation, regularization, nor- malization, and parameter optimizations (Alzubaidi et al., 2021). These mainly occurred due to the reorganization of the processing unit and the development of novel blocks. In particular, the most novel results were performed using network depth. Some examples of Deep Learning architecture are AlexNet, ZefNet, Visual geometry group (VGG), GoogLeNet, ResNet, Inception-V3, Inception–V4, DenseNet, ResNet, Xception, Recurrent neural networks (RNN), Long Short-Term Memory (LSTM), Gated Recurrent Units (GRU), Autoencoders, Restricted Boltzmann Machines, Deep belief net- works.

In Machine Learning, optimization is done each time a model is trained: the learning algorithm optimizes the values of the model pa- rameters to minimize the prediction error on the training dataset. However, the hyperparameters are not optimized during training. The number of layers in a neural network, the number of neurons per layer, etc., are hyperparameters that are not optimized during training. The choice of values for the hyperparameters influences the quality of the final model, sometimes strongly. On the other

Scientific articles were searched in databases such as Google Scholar, Science Direct, Web of Science, Scopus, Springer, and Direc- tory Of Open Access Journals (DOAJ). The following keywords were used: ‘Deep Learning’ OR ‘Artificial Intelligence’ in combination: ‘AND’ with ‘fruit disease’, ‘vegetable disease’, ‘fruit stress’, OR ‘vegeta- ble stress’. The last papers were assessed on July 4, 2022. No time interval was specified during the search process since Deep Learning is a relatively new technique, and we want to retrieve as many articles as possible. Therefore, 818 research papers were identified, including journal articles (original and review articles), conference papers, and book chapters. Two hundred forty-six of the documents were dupli- cates. Identification was followed by a screening process that included reading the title, abstract, and content of the paper to ensure that it was related to the use of DL to detect stress on fruits and or vegetables.

By screening the titles and abstracts, 363 other papers were excluded. After the content screening, we removed 50 irrelevant articles, 12 lit- erature reviews, and 15 book chapters. Among the irrelevant articles, some address stress on species other than fruits and vegetables using artificial intelligence or Deep Learning, while others refer to stress but do not use artificial intelligence or Deep Learning methods. Before ex- cluding them from the final selection, we checked references cited in systematic reviews, books, and articles on related topics. Finally, 132 articles were included in the study. Thirteen of the 132 articles were not free of charge. We have therefore used only their abstracts. The methodology described was inspired by the Preferred Systematic Re- views and Meta-Analyses (PRISMA) guidelines (Page et al., 2021) and Fig. 1 summarizes it.

Research questions help to structure better the systematic literature reviews. Our first concern was the objective behind using AI and DL methods to detect stress in fruits and vegetables. Then, we identify the species to which the techniques were applied and the type of stress involved. Data being an essential element for any model, we defined the data's type, size, and source. We also looked at the countries where the authors have installed experiments to collect the data. In addition, several algorithms and implementation frameworks in the literature are used for given tasks. Thus, we have defined them through the selected studies. Moreover, we sought to determine the models' hyperparameters, performances, and evaluation metrics. Finally,

The use of Artificial intelligence in classifying and detecting biotic and abiotic stresses in fruits and vegetables is recent, as shown in Fig. 2. The first papers in this area appeared in 2003, followed by one paper in 2006. A rapid progression was observed from 2019 to 2021, with a peak of 34 papers in 2020. This increase is evidence of re- searchers' interest in using AI and DL methods for early disease detec- tion, as precision agriculture has become a necessity to face the challenges related to food security. Twelve papers were published from January 2022 to July 2022. Most of the papers are journal articles. Conference proceedings account for about 36% of the total number of papers.

Keywords are the words or groups of words that inform the critical aspects addressed by a research paper. For more precision, we elimi- nated inconsistencies by merging similar keywords such as ‘CNN’, ‘cnns’, and ‘convolutional neural networks’. Table 2 shows the top 10 most frequent keywords of the reviewed articles, their number of occur- rences, weight links, and total link strength. The weight links are the number of connections of a keyword with other keywords indicating its importance. The total link strength represents the total strength of the co-occurrence links of a given keyword with other keywords. Fig. 3 completes Table 2 by illustrating the keyword co-occurrence net- work. According to this figure, we distinguish two main periods in the

Training of the DL models requires large input databases. Because data is the most important input of models, it is necessary to ensure their quality. The data used are essentially images (92%). The other data types are climatic data: mean temperature, minimum tempera- ture, maximum temperature, rainfall, wind speed, humidity, solar, and sunlight. The reviewed papers used about 50% of the data they collected in the field, while 28% relied on the plant village databases (www.kaggle.com/charuchaudhry/plantvillage-tomato-leaf- dataset). PlantVillage is a free downloadable web dataset containing images of 54,303 healthy and diseased leaves, divided into 38 categories by species and disease. Species include apple, blueberry, cherry, corn,

grape, orange, peach, pepper, potato, raspberry, soybean, squash, straw- berry, and tomato. Diseases are bacterial, fungal, and viral infections. The remaining 18% used data from AI Challenger 2018, ImageNet, Crowd AI, PlantDisease, Coffee leaf, etc. The sensors used to collect this data are cameras, smartphones, and drones.

Fig. 4 presents the map of the experimental site of data collection from the selected studies. It shows that most papers used data collected in Asia (14, 6, 4, and 3, respectively, from China, India, South Korea, and Bangladesh), followed by the United States of America, Europe (Italy, Germany, United Kingdom, Greece, and Latvia). In addition, there were some efforts from North Africa (Algeria and Egypt), Austral Africa (South Africa), and East Africa (Tanzania). From the selected studies, West Africa was not represented (Fig. 4).

Supervised machine learning. Samajpati and Degadwala (2016) propose the classification of apple fruit diseases from the extraction of features and colors. Random forest classifiers were used for disease clas- sification. Still, on apple, Omrani et al. (2014) were interested in detect- ing three-leaf diseases (Alternaria, black spot, and apple leaf miner) using support vector regression based on the radial basis function. The authors compared ANN and SVM as a classifier and found that SVM per- formed better. Note that the data used are unbalanced between the

three classes considered. Nine classification algorithms are used for jackfruit disease classification in Bangladesh (Habib et al., 2022). Among the algorithms, Random Forest performed better than all other classifiers, with an accuracy of almost 90%. In addition, Vakilian and Massah (2013) implemented a device for detecting two fungal diseases of cucumbers. Inoculated plants are used as input for the ANN backpropagation algorithm, giving an acceptable performance. Zhang and Wu (2012) developed a classification method based on multiclass SVM in several steps. The first step consisted of the background removal of acquired images with a split-and-merge algorithm. The second step

Convolutional neural networks, also known as CNN or ConvNet, are deep learning algorithms that take the input image, assign weights/ biases to the components of the image, and then classify the entire pic- ture. Based on the resolution and size of the image, it is transformed into an array. Each entry consists of a number between 0 and 255 for RGB (Red Green Blue) systems. This number represents the pixel intensity of the image. CNN's structure consists of three layers: the convolutional layer, the pooling layer, and the fully connected (FC) layer. The first layer makes extraction of high-level features. It convolved the image with a filter or kernel using the formula: Z = X∗f , where X and f are

to reduce the spatial size of the image representation and the computa- tional and processing overhead of neural networks. It also extracts key positionally and rotationally invariant features. The last layer uses the flattened output of the previous pooling/convolution layer as input. Flattened means that a 3D matrix or array is expanded into a vector. Specific mathematical calculations are performed for each FC layer. After the vector has passed through all the fully connected layers, the softmax activation function is used in the last layer to calculate the probability that input will belong to a particular class. This process is re- peated for other classes and individual images within those classes. It trains the network and teaches you, for instance, to distinguish between

deploy the models in a mobile application (Orano et al., 2019; Ferentinos, 2018; Sheikh et al., 2020; Pan et al., 2019), the alleviation of training time required (Atabay, 2017), the lower computational com- plexity (Ferentinos, 2018), and the simplicity of the model (Singh et al., 2018). Moreover, several open-source CNN software have been made available. However, unbalanced data often hurt CNN models perfor- mance (Hao et al., 2020). Apart from CNNs, there are other supervised deep learning models, including Recurrent Neural Networks (RNN).

Deep unsupervised learning. This technique allows running the learning process without labeled data available. The model is supposed to organize the data on its own, based on the input data's features, to discover the data's unknown structure or relationship. This approach often includes Generation network technology, dimensionality reduc- tion, and clustering. Some members of the DL family are working well on Nonlinear dimensionality reduction and clustering tasks. These in- clude Restricted Boltzmann Machine (RBM) and Autoencoder.

The Restricted Boltzmann Machine (RBM) or Boltzmann Ma- chine: is a generative unsupervised model that learns a probability distribution from the original dataset and infers data it has never seen before. The RBM has an input layer and one or more hidden layers. It uses a neural network with neurons connected to neurons in the same layer and other neurons in other layers. The nodes are connected in a circle. In contrast to all deterministic network models, the RBM model is called stochastic. It is ideal for system monitoring and handwritten digit recognition (eg: check verification and crimi- nal evidence). The advantage of RBM is its possibility to encode any distribution due to its expressiveness and computational efficiency. In addition, using hidden layer activation as input to other models is a helpful feature to improve performance. This technique is more challenging to train.

Autoencoders are unsupervised deep learning techniques used to learn efficient data coding (Kunapuli and Bhallamudi, 2021). They consist of four or five flat, two symmetric deep belief networks. Half of the network encodes, and the other half decodes. The autoencoder learns essential functions in the data by minimizing re- construction errors between input and output data (Abirami and Chitra, 2020). There are an equal number of neurons in the output layer and the input layer with Autoencoders. Autoencoders are flex- ible due to both linear and nonlinear transformations in encoding (Abirami and Chitra, 2020).

Autoencoder was used to predict and classify early and late phe- nomena resulting from macronutrient deficiencies in tomato plants (Tran et al., 2019) and for detection of lead concentration in lettuce (Xin et al., 2020). In general, the most crucial drawback of unsuper- vised learning methods is their inability to provide accurate

Fig. 6 shows four hyperparameters of DL models: the activation functions, the pooling methods, the optimizers, and the loss functions. The activation functions are added at each layer of the DL model. They enable the algorithm to learn and understand something extremely complicated and non-linear between the inputs and response variables. Relu (Rectified Linear Unit) was preferred mainly (80%) over the other Sigmoid, Tanh, and LeakyRelu. Because of its mathematical simplicity: y = max(0,x), Relu uses less expensive operations. Therefore, its training rate is fast (Kukreja and Dhiman, 2020), and it improves convergence and rectifies vanishing gradient problems. LeakyRelu is an extension of the Relu function. The Sigmoid function is S-shaped and is used for bi- nary classification problems such as 0 healthy and 1 diseased. It is

The optimizers are algorithms used to minimize the loss function. The Stochastic Gradient Descent (SGD) and Adam(Adaptive Moment Estimation) were the most used, respectively, with 52% and 27%. The SGD frequently updates the model parameters and allows using large data sets with less memory. Adam optimizer proposes an adaptive learning rate for each parameter.

The loss function ’Categorical cross-entropy was used in more than 75% of the papers. The categorical cross-entropy function deals with probabilities. Each of the predicted probabilities is compared to the ac- tual value of the class output (0 or 1), and a score is calculated. In con- trast, the Contrastive loss function operates on the data points generated by the network and their positions relative to each other (Khosla et al., 2020). Then, the Categorical cross-entropy seems better for image classification tasks.

The R2 and the root mean square error (RMSE) are the few ob- served regression metrics. The RMSE is the square root of the mean square error. It is used to measure the standard deviation of the re- siduals. The coefficient of determination or R-square is the propor- tion of the variance of the dependent variable that the linear regression model explains. The R2 is a scale-free score, which means that regardless of whether the values are small or large, the R-square value will be less than one. The R2 and RMSE are calculated using the following formulas.

We consider the most used evaluation metric (accuracy) to group the best-performing models reported by the authors into five classes. The five classes are defined as follows: class 1 from 95 to 99.99%, class 2 from 90 to 94.99%, class 3 from 80 to 89.99%, class 4 from 70 to

Applications of Deep Learning in agriculture have been steadily ad- vancing in recent years. This review presented recent work in agricul- ture to identify and classify biotic and abiotic stresses in fruits and vegetables. The effectiveness of the models was evaluated based on the data sources, the models used, the hyper parameters adopted, and the evaluation metrics used. Finally, the limitations of the papers and some perspectives were presented. The finding is that most of the

Zheng, Z., Pan, S., Zhang, Y., 2019. Fruit tree disease recognition based on convolutional neural networks. Proceedings - 2019 IEEE International Conferences on Ubiquitous Computing and Communications and Data Science and Computational Intelligence and Smart Computing, Networking and Services, IUCC/DSCI/SmartCNS 2019,

