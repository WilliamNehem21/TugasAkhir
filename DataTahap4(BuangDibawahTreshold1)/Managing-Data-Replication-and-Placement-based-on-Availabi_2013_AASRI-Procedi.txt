The replication of data across multiple sites of data grid is an effective solution to achieve good performance in terms of load balancing, response time, and improving data availability. To get the maximum gain that can make the data replication, their placement strategy in the system is critical. This paper proposes a replication strategy based on availability. It proposes also a placement and replacement strategies of replicas that ensures the desired availability with the minimum replicas despite the presence of nodes failures and without overloading the system. The results of our experimentations confirm that the proposed approach reaches its objectives.

popularity of data. The most often solution used to solve this problem is the replication [15]. Data replication is a technique of creating identical copies of data (files, databases, etc.) in geographically distributed sites. Each copy is called a replica [7]. The aim of our work is to ensure the desired availability with minimum replicas without degrading system performances. This goal is possible with a placement strategy that takes into account: the desired availability, the stability of nodes in the system and the failures.

We present the following contributions: Section 2 introduces some related researches on the replication and placement of replicas in distributed systems and data grids. In Section 3, we define the used topology. Section 4 presents our contribution, namely a proposal for an efficient dynamic replication approach which takes into account the placement of replicas and failures in the system. Section 5 presents the experimental results of our different simulations. The last section summarizes the paper and gives a short overview of future works.

The root in the topology is used to bind the various clusters with each other. The Cluster-Head (CH), which represents members of the cluster, has a routing table that manages the nodes within the cluster. It also contains metadata and information about the replicas existing in the cluster. The other nodes are storage elements; they contain one or more replicas of various data. In the system, nodes have predictive behavior [4] and the fault detection is based on the messages of life [9]. If a failure is detected, the auto stabilization [2] will be triggered to keep the topology connected, in other words.

The present paper uses a new model of replication and placement of data. The principal objective of this model is to minimize the number of replicas that ensures certain availability degree without degrading the performance of the system. Our strategy takes into account the different and independent stability nodes in contrast to most work in the literature [10] [13] [14] [17]. Each Cluster_Head contains a replication controller to manage the replication and placement of replicas in the cluster.

 pi: the stability of the node i where the replica of data j is stored. 0 pi1. In the rest of the paper, we will make the difference between the stability of the node and the stability of its data, we note the first STAB and the second p. So if data j is stored in node i then:   .

Many of works that exist in the literature [10], [14], [17] assume that the nodes have the same degree of stability in order to use the formula 2. In the work [13], the author used nodes of different degrees of stability, but he proposes to replicate the data in the nodes of the same class of stability. This proposal allows him to use the formula 2 to calculate the number of replicas necessary to meet the desired availability. In our system, nodes have different and independent stabilities; this is the case of existing systems. To calculate the number of necessary replicas  that ensure the desired degree of availability Avail, we have three possibilities:

 Optimistic: is the case where all the replicas of the data i are stored in the nodes of a good stability. So the availability Availi will be assured with the minimum number of replicas. The Op is the number of replicas necessary to assure the desired availability in the optimist case. It is calculated by the formula 2 where p is the best stability in the system.

 Pessimistic: is the case where all replicas of the data i are stored in the nodes of poor stability. So the availability Availi will be assured with the maximum number of replicas. The Pes is the number of replicas necessary to assure the desired availability in the pessimist case, it is calculated by the formula 2 where p is the minimum stability in the system.

 Hybrid is the case where the replicas are stored in nodes of different degrees of stability. So the availability Availi will be assured by crating Hyp replicas in the system. So Hyp will be in the range [Op, Pes]. The number Hyp can not be calculated after specifying the placement of replicas because you have to select the participating pi in the formula 1.

Each Cluster-Head (CH) specifies a certain degree of availability AvailD to ensure in its cluster. The AvailD is estimated using the history of data itself and its importance (popularity) in previous periods. After the calculation of AvailD , the CH compare the actual availability (real availability) of the data AvailR in the cluster with the desired availability AvailD .

The placement of replicas in the system plays an important role. In [17], the author shows that the placement of several replicas of same data in the same node does not improve the availability or fault tolerance. For this reason, it will be useful to store a single replica of the same data in a node. But what are the good candidates to store the data?. In our system, nodes can predict failures. In case of suspecting the failure,

A high degree of responsibility indicates that the node has a lot of bytes to place (move) in case of suspected failure. In this case the recovery time of that node will be high and the fault will be accelerated. The recovery time presents the needed time for moving data from the suspected node to other nodes. We suppose that in the beginning the DR is 1 in all the nodes.

That is to say that the node with a good AF is the node that has good stability STAB, and low degree of responsibility DR. The placement of replicas according to the strategy of availability factor AF guarantees a good distribution of replicas on cluster’s nodes, but does not guarantee a good distribution on the network. There may be cases where many replicas of the same data are stored in neighbors which increase the response time for the other nodes. To avoid this problem, we added the last condition:

So  is the size of the list of different data between data list of the node n and the data list of its neighbors. For example, if  = 0 then all the data in the node n also exist in the 1-neighborhood. Our goal is to distribute the load in the network. In a system where we do not know the frequency of access to the data because:

Figure 3 shows an example of a cluster consisting of five nodes. Assuming that CH wants to store a new replica of the data M and that all nodes have the same AF. If Liste_Cand = {1, 4}, creating a replica of data M in the node 1 does not increase its no-similarity (((1)) = ((1)) ) because this data already exists in its 1- neighborhood. But the addition of this data in the node 4 increases its no-similarity ( =2 >  =1). So the CH replicates this data in the node 4. In case where the CH add a new data S in the cluster, and it has as candidates Liste_Cand = {1, 4}, so it choose the node 4 to store a replicas of the data because this node has

Each node in the system stores the list of nodes requesting access to its data. If there is a failure suspicion, the node moves its data to other nodes to keep the availability in the cluster. For each data, the node selects from the list of nodes requesting this data the nearest node with a good AF. In this way the node minimizes recovery time and keeps the distance between the different data replication.

In order to estimate the behavior of our approach called PD (Placement Dynamic), we used our developed simulator FTSim [13]. The first experiment evaluates the response time using different number of nodes in the system. The results are shown in Figure 4. We note that the response time in the proposed model becomes smaller compared to the random approach (the replicas are placed randomly) if the number of nodes increases in the system. In our approach PD we ensure a good distribution of replicas on the cluster which minimizes the distance between the node and the target data. The second experiment (see Figure 5) calculates the number of replicas in the system for the two placement approaches (PD/Random). We note that the number of replicas of our approach is less than the random approach; despite the availability desired AvailD is the same in both cases because our approach chooses as the placement of replicas the most stable nodes which minimize the number of replicas necessary to ensure AvailD (see § Section 4.1).

The recovery time is a very important parameter in fault-tolerant systems. The recovery time of the system is the sum of the recovery time of each node suspect to crash. The results obtained (see Figure 6) shows that this recovery time increases with the increase in the number of failures in the system, but our approach PD minimizes this time because the replacement of data is done by the node itself without the intervention CH nodes and also chooses the closest nodes for storing data. Failures can cause loss of data or minimizing its

In this paper, we proposed a replication strategy based on availability, we also proposed the placement of replicas in the system in an efficient manner that improves system performance without overloading the system nodes. The inconvenient of this approach it is semi-centralized, that is to say that the decision of replication and placement of replicas is achieved by each cluster-head in each cluster. But since the number of nodes in cluster is limited, the constraint of scalability can be met. In the following phase of the research, we will use Globus to study the comportment of our placement model in real grid. We will also extend this work by considering also the task replication and placement in order to ensure a fast and fault tolerant execution of system jobs.

