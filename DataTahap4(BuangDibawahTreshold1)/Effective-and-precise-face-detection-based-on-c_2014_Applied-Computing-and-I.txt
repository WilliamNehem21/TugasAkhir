Abstract In this work an effective face detector based on the well-known Viola– Jones algorithm is proposed. A common issue in face detection is that for max- imizing the face detection rate a low threshold is used for classifying as face an input image, but at the same time using a low threshold drastically increases the number of false positives. In this paper several criteria are proposed for reducing false positives: (i) a skin detection step is used to reject a candidate face region that does not contain the skin color, (ii) the size of the candidate face region is calculated according to the depth data, removing the too small or the too large faces, (iii) images of flat objects (e.g. candidate face found in a wall) or uneven objects (e.g. candidate face found in the leaves of a tree) are removed using the depth map and a segmentation approach based both on color and depth data.

The above criteria permit to drastically reduce the number of false positives with- out decreasing the detection rate. The proposed approach has been validated on three datasets composed of 180 samples including both 2D and depth images. The face position inside samples has been manually labeled for testing.

Face detection has attracted the attention of many research groups due to its widespread application in many fields as surveillance and security systems, as human–computer interface, face tagging, behavioral analysis, content-based image and video indexing, and many others (Zeng et al., 2009). Face detection is the first crucial step for facial analysis algorithms (i.e. face recognition/verification, head tracking, and facial expression recognition) whose goal is to determine whether or not faces are present in an image and eventually return their location and extent (i.e. a bounding box). It is a more challenging problem than face localization in which a single face is assumed to be inside an image.

Most of the literature in this field deals with frontal face detection from two-dimen- sional (2D) images: the problem is often formulated as a two-class pattern recognition problem aimed at classifying each sub-window of a given size of the input image as either containing or not containing a face (Jin et al., 2004). Then the classification is performed by common technologies for 2D facial recognition such as Eigenface, Fisherface, waveletface, PCA (Principal Component Analysis), LDA (Linear Dis- criminant Analysis), Haar wavelet transform, and so on. The Viola–Jones detector (Viola and Jones, 2001) is probably the most famous approach for frontal 2D detec- tion: it involves exhaustively searching an entire image for faces, with multiple scales explored at each pixel using Haar-like rectangle features boosting classification. Two different face detection strategies based on slightly modified Viola–Jones are proposed in Anisetti (2009). In Ku¨ blbeck and Ernst, 2006 boosting has also been used in con- junction with Modified census transform (MCT), to improve illumination invariance. In (Huang et al., 2007) a method able to detect faces with arbitrary rotation-in-plane and rotation–off-plane angles in still images or video sequences is proposed. In (Jianxin et al., 2008) is designed a classifier that explicitly addresses the difficulties caused by the asymmetric learning goal (the minority class is the face class).

indicating the relative distance of that pixel from the sensor at the time of image capture. Depth information captured by Kinect is not useful to differentiate among different individuals at a distance, due to its very high inter-class similarity, but thanks to its low intra-class variation may be useful to improve the robustness of a face detector by reducing sensitivities to illumination, occlusions, changing of expression and pose. Kinect devices have been extremely popular recently, due to their low-cost and availability, and the first benchmark datasets have been collected for 3D face recognition (Tsalakanidou et al., 2003) or detection (Hg et al., 2012).

This work, similar to other approaches proposed in the literature (Anisetti et al., 2008), is aimed at using depth information to reduce the number of false positive detections and improve the percentage of correct detections. In (Anisetti et al., 2008) the authors use a 2D multi-step algorithm to obtain a coarse-to-fine classifi- cation, then refine the quality of the face location by a 3D tracking approach.

the third filtering rule is defined on the depth map to discard flat objects (e.g. candidate faces found in a wall) or uneven objects (e.g. candidate face found in the leaves of a tree). Combining color and depth data the candidate face region can be extracted from the background and measures of depth and reg- ularity are used for filtering out false positives.

Unfortunately, in the literature there are no freely available large datasets for face detection (with difficult images as complex background, the datasets used in (Shieh and Hsieh, 2013; Mattheij et al., 2012) are quite easy) that contains both the color and the depth map. There are several datasets for face recognition using the depth map, but the face detection step in those datasets is easy. Therefore, the proposed approach has been evaluated on a collected dataset that will be made freely available for further comparisons.

The proposed method is based on the widely used VJ face detector (Viola and Jones, 2001), characterized by a slow training, but very fast classification. VJ involves a very simple image representation, based on Haar wavelets, an integral image for rapid feature detection, the AdaBoost machine-learning method for selecting a small number of important features, and a cascade combination of weak learners for classification. The detection performance of VJ strictly relies on the threshold used to classify an input region as face, this value defines the criteria to declare a final face detection in an area where there are multiple detections around an object. Groups of candidate face regions that meet the threshold are merged to produce one bounding box around the target object. Increasing this threshold may help suppress false detections by requiring that the target object be detected multi- ple times during the multiscale detection phase. Since the original VJ implementa- tion is designed for upright frontal image, in order to handle non upright faces in

ing the faces in order to obtain a set of aligned color images and depth maps. For this purpose, the calibration data for the depth and color cameras of the Kinect are computed using the method proposed in Herrera et al. (2012). This approach com- putes both the intrinsic parameters of the depth and color cameras and the extrinsic parameters between the two cameras. The depth samples positions of the image in the 3D space are first computed by using the intrinsic parameters of the depth cam- era and the 3D samples are then reprojected in the 2D color image reference system by using both the color camera intrinsic parameters and the extrinsic ones. By the end of this procedure, a color and a depth value are associated to each sample.

of the ensemble proposed in Nanni et al. (2013). It is the combination by the sum rule of the methods proposed in Jones (2002), O´Conaire et al. (2007) with three other skin detectors based on the idea proposed in Khan et al. (2011); after the

Since lookup tables2 are used for the classification task this method allows to classify skin pixels of a given image in real time. Please note that since lookup tables have been calculated from several large datasets (Nanni et al., 2013), the system is not over-trained in the dataset tested in this paper.

Another significant information that can be obtained from the depth map is the flatness/unevenness of the candidate face regions. For this filter first a segmenta- tion procedure is applied, then from each face candidate region the standard devi- ation (std) of the pixels of the depth map that belongs to the larger segment is calculated. Regions having a std out of a fixed range [0.15, 4] are removed.

The segmentation of both color and depth map is performed according to the approach of Dal Mutto et al. (2012). This segmentation scheme is based on the normalized cuts spectral clustering algorithm (Shi and Malik, 2000) and jointly exploits the geometry and color information for optimal performances.

pi; i = 1; .. . ; N. After the joint calibration of the depth and color cameras it is pos- Each sample in the acquired depth map corresponds to a 3D point of the scene sible to compute the 3D coordinates x, y and z of pi and to associate to it a 3-D

vector containing the R, G, and B color components. Geometry and color then need to be unified in a meaningful way. Color values are converted to a perceptu- ally uniform space in order to give a perceptual significance to the distance between colors that will be used in the clustering algorithm. The CIELab space has been used for this purpose, i.e., the color information of each scene point is the 3-D vector:

where k is a parameter balancing the contribution of color and geometry. High values of k increase the relevance of geometry, while low values of k increase the relevance of color information. A complete discussion on the effect of this parameter and on how to automatically set it to the optimal value is presented in Dal Mutto et al. (2012).

versity campus in Padova. It includes both outdoor and indoor scenes, framed in different hours during the day, in order to account for the varying lighting conditions. The images capture one or several people performing various daily activities, e.g., working, studying, walking, chatting and so on. Note how most people are not directly looking into the camera, i.e., they did not pose for the frame acquisition but they were doing their activities without being aware of the camera shooting them. Some faces are also par- tially occluded by objects or other people. For these reasons, this dataset is more challenging than the previous ones.

The three sets have been merged to form a single dataset consisting of 233 images containing 251 faces3 (only upright frontal faces with a maximum rotation of ±30° have been considered). Notice that the parameters of the method have been manually selected and are the same for all the testing images despite their dif- ferent origin. The dataset is not ‘‘easy’’: in Figure 5 some samples which are not detected by the VJ method4 are shown.

positives candidates found by VJ using a low threshold (required to reach a high detection rate). It allows to greatly reduce the number of false positives, the other two filtering criteria allow to further reducing the number of false positives. The proposed approach allows to diminish the number of false positives on the consid- ered dataset from 1063 to just 143 almost without affecting the detection performances.

The depth map allows to remove the false positives in many critical situations. In particular it allows to get the actual size of the candidate face allowing to remove objects too small or too large to be a face. It also aids the segmentation step in the proposed method that is a critical point to ensure proper processing in the remaining steps.

Finally, even if the experiments reported in this paper are related to data acquired by the Kinect, there are several other depth acquisition schemes and sen- sors that can be exploited. For example stereo vision systems that get the 3D data from two standard cameras, allow to work at big distances by choosing a suitable baseline. Also there is a wide range of 3D sensors that can work at different dis- tances and with different accuracies. The Kinect is one of the most widespread and the cheapest acquisition sensors but not the best.

