An ontology is very important in describing and sharing knowledge of a domain. This paper proposes a method to automatically generate domain ontologies from Chinese encyclopedias on the web. First, we use the terms appears in category systems of encyclopedias as concepts and resolute synonyms, then derive an original taxonomy from Chinese- Wikipedia and Hudong-Baike; for other concepts not in the original taxonomy, we use a set-theory like method to form a directed graph and generate a tree from the graph via maximum-spanning-tree algorithm, and merge the tree into the taxonomy. Then, we use titles of normal articles as instances and populate them via category labels in them. The attributes of concepts and instances are generated from special structures such as InfoBox modules. We learn a plant ontology successfully and the later experiments show that the learnt ontology has well precision and high coverage.

to build ontologies manually is very expensive, it needs many domain experts and programmers work together for a long time. Moreover, the manual process is tedious and error-prone, and the built ontologies suffer from low coverage and fast aging. There are only a few manually built ontologies; the most famous three are WordNet [Fellbaum, 1998], Cyc [Lenat, 1995] and Gene Ontology [GOC, 2000]. In Chinese, HowNet [Dong and Dong, 2006] provides a knowledge database, but it only has less than 100,000 words and is not free; to the best of our knowledge, there are no large Chinese ontologies published for free yet.

)automatically, which is called ontology learning. According to the coverage of the ontology, ontology learning can be divided into general ontology learning and domain ontology learning. When distinguishing by the source from which the ontologies are learnt, there are ontology learning based on structured data, semi- structured data and unstructured data; more concretely, semi-structured data including h. This paper focuses on learning ontologies from Chinese encyclopedias. With the development of Web 2.0, there are more and more collaborative semi-structured data published on the Internet, the most famous one is Wikipedia [Wales, 2005], which contains more than 3.8 million English articles and 400 thousand Chinese articles. In Chinese, there are other two larger online encyclopedias, namely Baidu-Baike and Hudong-Baike, both of which contain more than 3 million articles.

Text based learning, which learns ontology from unstructured text directly. Text-to-Onto [Maedche and Staab, 2000] is an earlier ontology learning system which learns ontologies from text. TShamsfard and Barforoush [TShamsfard and Barforoush, 2004] proposed an automatic ontology building approach which started from a small ontology kernel and constructed the ontology through text understanding automatically. Lee et al. [Lee et al., 2006] presented a novel episode-based ontology construction mechanism to extract domain ontology from unstructured text document. In general, these methods relied heavily on NLP techniques, structured knowledge such as dictionaries, and sometimes human interaction.

HTML page based learning. Sánchez and Moreno [Sánchez and Moreno, 2004] developed an approach to automatically construct ontology from the Web which started from an initial keyword, they first used search engines to get web pages related to the keyword and then mined concepts from these web pages. Tian et al. [Tian et al., 2009] used an iterative learning approach to learn ontologies with the help of search engine and Protégé-OWL API. Shinzato and Torisawa [Shinzato and Torisawa, 2004] provided a method to acquire hyponymy relations from HTML documents, they used itemization and listing elements in documents. These methods could get shallow information only as lacking of useful structured data.

Encyclopedia based learning. Over 2 million concepts with mappings to over 3 million unique terms were mined from Wikipedia by Gregorowicz and Kramer. [Gregorowicz and Kramer, 2006]. Ponzetto and Strube [Ponzetto and Strube, 2007] derived a large scale taxonomy from Wikipedia by labeling the semantic relations between categories. KOG [Wu and Weld, 2008] is an autonomous system for refining Wikipedia’s InfoBox- class ontology, SVMs and Markov Logic Networks were used to solve the ontology refinement machine learning problem. YAGO [Suchanek et al., 2007] is a high coverage and quality ontology which contains more than 1 million entities and 5 million facts, the ontology was automatically extracted from Wikipedia and unified with WordNet. As the English Wikipedia is really abundant, good ontologies can be learnt. In contrast,

In order to use the three encyclopedias, we should download them first. The Chinese-Wikipedia provides a repository for end users to download it directly; however, we have to write crawlers by ourselves for the other two encyclopedias as they only available via HTML pages. Totally, we get 4,009,755 articles from Baidu- Baike, 2,780,675 articles from Hudong-Baike and 377,635 articles from Chinese-Wikipedia.

Concepts are directly deduced from category systems of the three encyclopedias. As encyclopedias are freely edited by arbitrary end users, there may be some problems. Therefore, we use the following rule to filter the appeared categories: if a category is isolated and has no child categories and instance pages, we discard it. For the remaining categories, we have to merge synonyms. The redirection module in all the three encyclopedias is good clue for synonym resolution. Moreover, we find the fields named “alias” in Baidu- Baike and “alias in Chinese” in Hudong-Baike contain many synonyms of the current article.

Both Hudong-Baike and Chinese-Wikipedia have good category hierarchies, thus an original taxonomy can be deduced from them. However, there are scattered categories. Moreover, all the categories in Baidu- Baike are not well organized. In order to merge these categories into the original taxonomy, we first use the set-theory like method to calculate the belonging degree of them and thus form a directed graph G = (V, E), with weight function , V stands for the set of categories. The weight function is defined as:

The InfoBox modules are good resource for attribute extraction; however, only about ten percent of articles have InfoBox modules. Fortunately, later experiment shows that about eighty percent of plant articles have InfoBox modules. The InfoBox modules are usually defined by special HTML tags and are easily to parse.

In this section, we first learn a plant ontology use the techniques specified above. Then, evaluation about the learnt ontology is described. Usually, there are four ontology evaluation methods, namely “golden standard” based evaluation, expert based evaluation, application based evaluation and domain-data scale based evaluation. As there is no “golden” ontology in Chinese, and we have no application based on our learnt ontology yet, we use the domain-data scale based method and expert based method. We will evaluate the ontology in two aspects, its scale and its precision.

We start with the category labeled with “plant” and extract concept, taxonomy, instance, and attribute step by step. In concept extraction, we extract 3,122, 4,431 and 4,507 concepts from Chinese-Wikipedia, Hudong- Baike and Baidu-Baike respectively; and the numbers of instance from them are 24,482, 54,977 and 56,128 respectively. After doing synonym resolution, the learnt plant ontology contains 4,788 concepts and 59,975 instances in total. Almost all the concepts and 49,023 instances have attributes. Fig 2 shows the information of the learnt ontology in detail, and Fig 3 shows a snapshot of the inner structure of the ontology (we only list part of the concepts and instances).

According to Wikipedia, there are about 350,000 kinds of plants and 287,655 of which have been discovered until 2004. Our learnt plant ontology contains about 6 million instances, covering about 21 percent, which certainly contains most frequently-used plant. Obviously, the scale of the ontology is limited by the corpus.

