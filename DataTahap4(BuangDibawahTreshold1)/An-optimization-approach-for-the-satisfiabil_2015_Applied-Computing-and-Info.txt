Abstract We describe a new approach for solving the satisfiability problem by geometric programming. We focus on the theoretical background and give details of the algorithmic procedure. The algorithm is provably efficient as geo- metric programming is in essence a polynomial problem. The correctness of the algorithm is discussed. The version of the satisfiability problem we study is exact satisfiability with only positive variables, which is known to be NP-complete. ª 2012 Production and hosting by Elsevier B.V. on behalf of King Saud University.

The satisfiability problem is still the most important open problem in computer science. Many scientific disciplines highly depend on efficient solutions of this core problem. These span computer science itself, mathematical logic, pure and applied mathematics, physics, chemistry, economics, and engineering, just to mention some. Approximations and heuristics are more than welcome in applications, since

they oftentimes provide efficient solutions for special cases. Theoretically, how- ever, the computer science community is still lacking a good understanding of the computational difficulties of this problem though many facts about it are already known. Complexity theory tells us that the actual problem is related to the P = NP? question. Indeed, this question is now becoming a complex offered to us by complexity theory. Much effort to solve it has been in vain so far. Regret- tably, no real progress in the discipline of computer science can be made until the problem is solved. Most (senior) computer scientists today rather believe that P is not equal to NP. That is, no matter how much we try, the satisfiability problem, as a prominent NP-complete problem, will persist as a challenging problem with exponential known deterministic worst-case complexity.

It is also disappointing that the NP-complete problems are too many and essen- tial in real applications. It is rather simple to find a new NP-complete problem: just try to solve one of them and describe the main difficulty you encounter as a new problem if possible. So, adding a new problem to the list is relatively easy, but removing one from the list seems to be very hard. The reason is that the list of NP-complete problems degenerates to the empty list as soon as one of the prob- lems is discovered to be outside the list, that is, as soon as one of the problems is proved to be P-complete. This very fact is the main achievement of the theory of NP-completeness.

We favor in this paper the method of mathematical optimization. In another paper, we focused on formulating the satisfiability problem as non-convex, exact, exterior, penalty-based problem with a coercive objective function. The method focused on exact satisfiability (XSAT), which is NP-complete. The method falls into the category of approximation schemes for solving the satisfiability problem and is sub-optimal and partially heuristic in nature. In this paper, we treat the problem by way of geometric programming. We still focus on XSAT or more pre- cisely on 3XSAT, which has the following properties:

To the knowledge of the author, geometric programming has not been used to attack the satisfiability problem yet. Certainly, other optimization schemes are well-known, but geometric programming theory is very promising in the context of satisfiability as we shall see. The method we provide is new and, thus, interesting by its own means. The characteristic feature of geometric programming is that it helps overcome the non-convex nature of the optimization problem under study. This property is of extreme utility in our setting, since direct optimization methods for the satisfiability problem tend to be non-convex. Knowing that general non- convex optimization problems are (still) not tractable, the method of geometric programming offers new perspectives for attacking the problem.

This section is devoted to the introduction of the needed theory of optimization and to fixing our notation. Vector quantities are written in bold if not obvious from the context (e.g. x). Vector components are indexed accordingly (e.g. xi). Unless stated differently, we assume throughout the paper continuity and differ- entiability of used functions. This assumption is not restrictive in our context and is beneficial computationally. An optimization problem (P) is to find the min- imum (or maximum) of a real-valued function f(x), so-called objective function,

is a posynomial and, therefore, permissible as an objective function for (P). In geo- metric programs, an inequality constraint has the form gi(x) 6 1 and any such function gi(x) must be a posynomial. The equality constraints are of the form hj(x) = 1 and the functions hj(x) have to be monomials. Finally, in any geometric program all mentioned variables xk have to be positive. Thus, a geometric pro- gram (P) has the form:

With the two inequalities in hand, a duality theory can be easily developed. The process is sketched (see e.g. Duffin et al., 1967; Peressini et al., 1988 for more details) in the proof of the following main theorem of geometric programming.

Proof. First, the requirement E = £ can be omitted. We will focus on inequality constraints only. The index sets Ik follow from the structure of the used posyno- mials. We will prove the theorem in the case of a single inequality constraint g1(x) 6 1. The case of multiple constraints follows in the same manner. By (AG), we have:

In other words, fact (i) says (as ln(x) is monotone-increasing) that the dual pro- gram (D) is efficiently solvable, since it is in essence a (simple) convex program. Polynomial algorithms for convex programs are known (e.g. interior point meth- ods (Forsgren et al., 2002)). Fact (ii) is saying that once we have solved the dual problem (D), the primal problem (P) is practically solved, too. Although only the minimum of f(x*) is determined by solving the dual problem, a method for deter- mining a minimizer x* is known to be equivalent to a linear program and, there- fore, the problem of determining x* is solvable in polynomial time.

The theory outlined above is called weak duality theory. It is weak in the sense that fact (ii) requires the feasible set £ (P) to have a non-empty interior. Such pro- grams are called super-consistent. On the other hand, if £ (P) is non-empty the pro- gram is called consistent. However, under further mild requirements, the super- consistency assumption can be omitted. This leads to the so-called strong duality theory. The following theorem is the main theorem of strong duality.

This theorem relaxes the requirement of super-consistency: it only requires that the primal is consistent. The price of this relaxation is reflected in the conclusion that just the primal infimum and the dual supremum coincide (if existent). Com- pared with Theorem 2.2 this is a weaker conclusion, since there the primal mini- mum and dual maximum coincide if they exist. However, Theorem 2.3 is only theoretically weaker. In practice, the conclusion is sufficient for devising an algo- rithmic procedure to approximate the minimum of the primal problem and/or the maximum of the dual problem.

Theorem 2.3 opens a new way for geometric programming if used effectively. The idea we now introduce shall try to attack the open problem of solving geomet- ric programs with equality constraints for posynomials. So far, we have allowed equality constraints for monomials only. This requirement is related to the effi- cient solvability of the problem via convex techniques. Theorem 2.3 tells us, how- ever, that consistency of the primal program is sufficient for the mentioned infimum–supremum correspondence if a suitable dual program is defined. As a matter of fact, we are facing the question: Is there a dual program for geometric programs with posynomial equality constraints? The answer to this question is yes as seen in the following theorem.

Theorem 2.4. Let (P) be a primal program with posynomial equality constraints. Then (D) as defined above is the dual program of (P). Moreover, Theorem 2.1 applies for (P) and (D), in particular for feasible x and d we have: f(x) P v(d). h

Proof. The proof of Theorem 2.1 can be reused here without modification for posynomial inequality constraints. For posynomial equality constraints, we just point out that line (2.3) of that proof should include the equality symbol instead of the leftmost inequality symbol. However, this does not impair the validity of other inequalities. Thus, the main conclusion of the theorem, namely, that f(x) P v(d) for suitable x and d, remains true here, too.  h

One remark is here in order. Paradoxically, the preceding theorems show that equality constraints are dealt with as inequality constraints if only the primal is consistent and the optimization is changed so as to find the infimum instead of the minimum. This is so, since Theorem 2.4 shows that the same lower bound is used for equality and inequality constraints. This is rather strange, as the dual program would lose information about the constraint types of the primal pro- gram. This is why the new introduced theorems are rather in the status of con- junctures until more theoretical and/or experimental evidence about them is demonstrated. One way of accepting this paradox, however, is to imagine that proving consistency of the primal program with posynomial equalities is by itself a serious difficulty in practice, so as to make the conclusions of the last theorems of theoretical interest only save perhaps for special (non-)interesting cases.

We finally come back to the original problem of the paper, the satisfiability prob- lem. In the last section, we proved that equality constraints act theoretically like inequality constraints, if the original primal program is consistent. Algorithms for handling equality constraints are known. Actually, one of the inventors of geo- metric programming, Duffin pointed out (Duffin, 1970) that the method of posy- nomial condensation could be of great use in practice to approximate geometric programs by linear ones. This method is useful in our setting, too. The idea is to replace any posynomial equality constraint, e.g. g(x) = 1, by the (in)equalities:

Proof. Step 1 is essentially a perturbed linear programming problem (due to the existence of strict inequalities). So, it is solvable in polynomial time. Step 2 is a convex problem, which is harder to solve, but is still of polynomial complexity. h

Proof. The correctness of the algorithm relies directly on the results of Section 2. We need to emphasize, however, that only the infimum is approximated by the method and to refer to the remarks at the end of last section.  h

The presented algorithm is of use for the exact satisfiability problem (XSAT), if we can convert (XSAT) into an optimization problem of the form (P). With Claim 3.1, we are guaranteed to have a polynomial-time algorithm then. In view of the remarks of the last section, Claim 3.2 is still to be justified experimentally. In any case, a conversion of (XSAT) to (P) is of great importance in our setting. To this end, we first recall that our XSAT instances are required to be positive (i.e. without negated literals). Let C1 = x1 + x2 + x3 be a clause of such an XSAT formula F. We first notice that the following system of equations:

Let the above objective function of clause C1 be called f1(x) and the equality constraint function be called g1(x). Obviously, the same procedure can be used for any clause Ci(1 < —i 6 m) if the formula F includes m clauses, and for each clause Ci functions fi(x) and gi(x) can be defined accordingly. To solve the XSAT problem, we need to combine the equations of the different programs (PCi )’s. This yields to the following formulation of the XSAT problem as an optimization problem

By construction of fi(x) and gi(x), we evidently see that these are posynomials and linear posynomials, respectively. Also, the positivity constraints of the vari- ables can be easily perturbed to strict positivity constraints. Thus, in fact, we need to solve (XSAT) for positive xk only. But now all assumptions of aforementioned algorithm are satisfied, whence the following claim.

The previous procedure is a decision procedure. To obtain a solution vector, bisection in variables’ vector space is the immediate approach. Thus, if F is satis- fiable, the procedure in the previous proof needs to be called O(n) times, where n is the number of variables mentioned in F. We cannot rely here on the approach of geometric programming for determining minimizers (via linear programming), since this method relies on the super-consistency assumption of the problem to be minimized, which is not allowed in our setting.

We revived the theory of strong duality in geometric programming. We easily extended the theory to handle posynomial equality constraints. At this seemed to be only of theoretical interest, but we argued that in special cases (linear posy- nomials) the theory extremely useful in practice and we outlined a polynomial- time algorithm for solving this sort of problems.

The second main contribution is related to satisfiability research. We proposed a new format of the (positive) 3XSAT problem as a geometric program. We then proved that our format adheres to the requirement of linear posynomial equalities. Thus, we were able to apply the proposed optimization algorithm for XSAT.

There is no doubt that both Algorithm 3.1 and the procedure outlined in the proof of Claim 3.4 are polynomial-time algorithms. However, the issue that still needs further investigation is the (practical) correctness of these procedures. We intend to verify correctness experimentally. The main drawback of this approach is that faults in the implementation of mentioned optimization algorithms inevita- bly lead to erroneous conclusions. The paper shows, however, that the theory behind the algorithms is sound and that it predicts polynomial-time performance (with exact upper bounds). Despite this fact, a warning is here in order. Experience with other optimization methods has shown that theoretical investigation is not the whole story in this domain. One need only consider the Ellipsoid Method of optimization, which is provably of polynomial-time complexity, and which, how- ever, has poor performance in practice compared to the theoretically inferior Sim- plex Method with its exponential worst-case complexity.

namely, try to attack the problem via quasi-convex programming. Quasi-convex- ity of constraint functions is sufficient for minimization algorithms based on the Karush–Kuhn–Tucker theory and may be implemented efficiently, if the objective function is convex or at least pseudo-convex. The performance and reliability (i.e. performability) of this approach will be part of our future research.

