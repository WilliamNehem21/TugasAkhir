Independent Hardware Target : The authors in [45] proposed a target-independent compiler so that it can reduce the complexity of implementation. In their technique, the independent target program is linked to a library called the hardware abstrac- tion library (HAL), where this library is implemented for each target. They separate hardware specifications and hardware- independent functionalities to improve portability. However, their method was only implemented for the Intel platform and the Intel data plane development kit (DPDK). In addition, the performance of the compiler is lower than that of hardware- dependent compilers.

PISCES is a software switch in which forwarding behavior is obtained by high-level DSL or P4. PISCES is derived from Open vSwitch (OVS) and is configured by P4. In this protocol, the pro- grammer must specify how to process the packets. For instance, if in P4 we assign PISCES to process IPv6 packets, the program- mer needs to introduce IPv6 packets, including the format and fields of the IPv6 headers. PISCES brings several benefits, such as a personal protocol header, adding/removing a standard header, and easy to add new features [46].

Most of the available P4 programming translators are compilers. However, there exists research for developing interpreters [47], which still needs further attention. Currently, most of the focus is on pro- grammable hardware switches with PISCES that are compiled to a customized software-based POF switch (PVS), where POF stands for protocol oblivious forwarding.

Artificial intelligence (AI) is a progressive branch of computer sci- ence that deals with automation across various fields, and ML is an application of AI. ML is applied to developing systems to learn from patterns and data without explicitly being programmed [48]. The KDN architecture requires adapting an ML technique to optimize and create intelligence for the network. ML applications have been successfully utilized for network analysis, online customer support, search engines,

computer vision, and signal processing applications [49]. As a result, research studies based on ML on various aspects enable the KDN paradigm to access and adapt a suitable ML algorithm for the ap- propriate task. In this section, we survey a number of KDN-relevant ML methods, including SL, UL, RL, NNs, and TL. We briefly explain each algorithm and explore different techniques before using them in Section 6. Therefore, anyone interested in the field will acquire abstract knowledge of ML-based techniques and understand why and how an ML algorithm can be utilized in wireless communication applications.

and classification. In regression problems, we try to predict continu- ous value output, and in classification problems, the prediction is for discrete value output. Classification problems are used to distinguish between different things, such as prediction in image processing, to differentiate between a cat and a dog [50]. In contrast, regression problems cannot be considered as a classification problem [51]. The following are the popular survey works on SL techniques:

Linear Regression: Linear models are provided for prediction as a weighted sum of feature inputs. Linear regression uses gra- dient descent to minimize the loss function and optimize the system performance. Linear regression can only represent linear relationships, which reduces the prediction performance, but de- livers a fast and acceptable result for many applications. Further specifications and information regarding linear regression are available in [52].

Logistic Regression: Classification problems are used for logistic regression to assign observations to a discrete set of input classes. For example, the common use of this algorithm is to predict spam emails, student pass or fail grades, fraudulent websites, and so on. This learning algorithm uses the gradient descent method to optimize the solution. In contrast to linear regression, in the logistic regression model, a number between 0 and 1 is assigned to each data point instead of fitting a straight line or hyperplane. The predicted values are the probabilities generated by the sigmoid function. In other words, from the function out- puts, the learning algorithm decides which category the output data belongs. This algorithm has some drawbacks that can also be applied to linear regression. For instance, if the data are perfectly separated, the algorithm can no longer be trained, or interpretation is more difficult because the interpretation uses multiplication rather than addition. However, some advantages make the algorithm interesting, including multiclass classifica- tion and the probability distribution of the data. More details about the logistic regression are found in [52].

must be selected to increase the marginal space between the two given classes. Furthermore, this algorithm works for non- linearly separable (non-linear classifier) datasets using kernel functions, including Gaussian and polynomial kernels. Although SVM requires a long time to distinguish the alarmed dataset and degrade as the noise increases in the dataset, this method works effectively with high-dimensional spaces. This method uses a fraction of the memory and tightly margins the separation [53, 54].

K-values. The resulting KNN outputs can be used for both classi- fication and regression. In classification, the basic principle is to decide the category of a test point based on the decision made by the majority votes of K nearest neighbors. In the regression, the test point is categorized in a class by the average values of its K nearest neighbors. Additionally, this method can be adapted by assigning weights to neighbors such that the nearest points are more involved than the distant points. The KNN method is illustrated in Fig. 10. A more detailed explanation of KNN can be obtained in [55].

Decision Tree: One of the most common ML algorithms is DT, which is a statistical model for classification problems. This ML technique is used to classify data and formulate a dataset in a hierarchical structure, such as a flowchart representation. This flowchart is usually a tree-based structure, and the algorithm starts from the root and classifies the dataset until it reaches the leaf. The representation of one class DT is as follows: Each split of the tree is based on a condition on a particular attribute, and leaves are the classes. For a simple example of DT, the iterative Dichotomiser 3 (ID3) is explained as follows:

The DT method works for both regression and classification problems with many advantages, such as ease of interpretation, requires fewer data points to learn, and works well with a large dataset. However, minor changes to the large dataset require a new training sequence, and overfitting is the algorithm’s limita- tion. For implementation and further information, refer to [55]. Fig. 11 provides a simple example of a decision tree.

In unsupervised learning (UL), the input data are unlabeled data, where the algorithm has to find patterns and hidden structures to learn a useful function. The enormous data collection by devices and sensors results in a lack of labeling due to the unavailability of funds to pay for manual allocation or the nature of the data itself. The UL is extensively used in clustering and data aggregation [56]. The following algorithms are the most common UL techniques:

and after every iteration, the algorithm corrects itself [55]. The difference between this algorithm and KNN is that in every iteration, every point is assigned to a cluster using the Euclidean distance (closest to the centroid). In each iteration, the algorithm learns and updates the centroids of each cluster. The algorithm operates until the end of the iterations, or when the centroids of the clusters do not change significantly. A basic observation of this algorithm is shown in Fig. 12.

(2) Principal Component Analysis (PCA): This is a statistical proce- dure that transforms a set of correlated variables into a set of linearly uncorrelated variables using orthogonal transforma- tions [55]. The fundamental idea of PCA is to reduce the di- mensionality of the data and optimize it to major components or features. This statistical method represents the datasets in more economical and smaller observed variables for faster data processing in ML. Moreover, PCA works optimally with linear models for feature extraction, data compression, and redundancy of variables, such as image processing, signal processing, com- munications, and control systems/theory [57]. In this algorithm, singular value decomposition (SVD) plays an essential role in computing lower-dimensional data. SVD extracts the eigenvalues from the covariance matrix, which is the best approximation of the original dataset with fewer arguments. For more information on computation and simulation, refer to [55,58].

Q-learning : Q-learning is the most popular RL technique that uses Q-value and Q-function to find optimum action policies. Specifi- cally, the agent interacts with the current given environment to continuously learn the Q-values and maximize this value. This algorithm starts with an initial state and an action, followed by the epsilon-greedy policy. For each performed action, the Q- value is learned through the optimal (greedy) policy, enabling the agent to take any action based on the largest Q-value under the current state. Details of the Q-learning method can be found in [26,60]. Moreover, fuzzy Q-learning is used to deal with con- tinuous state spaces with a certain number of given rules [61]. A flowchart of the Q-learning method is presented in Fig. 13.

of deep learning and RL, which correlates the value function with corresponding actions and states [62]. DRL uses these two principles to approximate the optimal Q-values. Moreover, DRL techniques facilitate an NN with an RL architecture to enable agents to learn the best action in a specific environ- ment. DRL is mostly used in complex decision-making tasks with unstructured environments and can handle large datasets. Recently, DRL has made great strides in vehicle-to-vehicle (V2V) communication [63], wireless communications [64], and video games [65,66]. More details on DRL can be found in [67] for the readers. An overview of this concept is shown in Fig. 14.

Joint Utility and Strategy Estimation Based Learning : The util- ity and strategy estimation-based learning relies on the same concept as the classical RL. However, the main difference is that the agent receives an estimation of the expected utility from the environment and the updated reward. The probabil- ity distributions are modified based on the utility estimation, where the probabilities (also known as strategies) are selected actions [70,71]. Using this algorithm, the regret of each action in the process can be obtained using the received reward and utility parameters. Then, regret can be used to update the strategy. The main advantage is that the algorithm can be fully distributed

when the reward is immediately calculated locally; for instance, the transmission rate between the transmitter and receiver. This algorithm is often referred to as an equilibrium concept in game theory in many survey papers, such as coarse correlated equilibrium and logit equilibrium. The concept of this algorithm is illustrated in Fig. 16.

A neural network is a type of artificial intelligence for information processing that imitates the human brain. The neural network structure consists of thousands of closely connected, simple processing nodes. Neural networks are organized into layers, and in each layer, many nodes move the data. With the development of graphics processing units (GPUs) to accelerate the processing time, NN has attracted con- siderable attention from researchers and companies [72]. The following NN techniques are explained in this study.

Deep Neural Network (DNN): As shown in Fig. 17, the compo- nents of the structure are densely connected by the neurons in the network layer. Each neuron in a layer is connected to the rest of the neurons, resulting in the structure of the DNN. Each neuron corresponds to a weight for the input and an activation function for the output. The input data are transformed from layer to layer, with no direct connection between the two non- consecutive network layers. The main advantage of a DNN is that it can automatically deduct and tune the features to obtain the desired output. For the optimization of network parameters, DNN uses backpropagation (one of the most popular learning techniques for multilayer neural networks) and various gradient descent algorithms, such as Adam and Momentum [73].

1980s [74]. A CNN is designed to adaptively and autonomously learn spatial hierarchies of patterns by using different building blocks, including convolution layers, pooling layers, and fully connected layers. Among them, convolutional and pooling op- erations are the two main building blocks in CNNs, which are feature extraction [75]. Using these two layers enables the CNN to solve relatively complex models based on the progressive nature of the structure, as shown in Fig. 18.

Recurrent Neural Network (RNN): The principle of RNN is to save the previous output and feed it back to the input while having hidden states to assist the algorithm in predicting the outcome of the layer [76]. Additionally, there are connections in the hidden layers of the RNN architecture, where all the inputs, including the current and former inputs, impact the output. Hence, an RNN has the ability to remember [26]. Fig. 19 shows the hidden layers and connections of the RNN architecture.

Autoencoder : The autoencoder aims to model a set of data to learn and approximate the system function. The autoencoder has other variations, such as sparse autoencoder and denoising autoencoder. The autoencoder also applies backpropagation and sets the target output equal to the input. The components and process of the autoencoder are shown in Fig. 20; after the autoencoder is fully trained, the decoder is removed from the network, and only the encoder is kept as a feature extractor. This powerful NN can learn a robust representation of the input with a limited number of neurons such that the network can construct the input at the output. However, the autoencoder has some disadvantages that worth mentioning. First, an autoencoder re- quires an identical distribution of the training data at its input before it can work properly. Second, the number of neurons in the hidden layer has a direct impact on its performance [26].

Extreme Learning Machine (ELM): The extreme learning machine is an NN with multiple hidden layers with randomly chosen parameters, often referred to as a feed-forward neural network (FFNN). This is the simplest and the first artificial neural net- work, which aims to approximate a function to map an input to an output such as the XOR function. Different layers connect each node to other nodes in the network, including the hidden layer, input, and output layers. The information flows only in one direction, and it does not include any feedback, or the nodes do not form a cycle, differentiating it from the RNN.

assist learning more robustly. For deep learning and RL, knowledge can be defined as weights and Q-values, respectively. For instance, when deep learning is adapted for resource management, the ML process can use the weights that have been trained for other resource management tasks as the initial weight. For example, a similar network with the same number of nodes and similar behavior with a trained ML and a fully operational resource management policy can provide knowledge for other similar networks. Moreover, in RL, the Q-values learned from an environment can be used in a similar environment to make better decisions during the initial stage of learning. The specifications for utilizing TL with RL can be further studied in [77]. While there are advantages of having prior knowledge for learning a pattern, there are some limitations and negative impacts on the performance, which needs further attention that exceeds the scope of this study.

Most of the above ML techniques can be deployed in several net- work applications for automation and optimization. ML algorithms provide information and knowledge for different tasks. In the next section, the application of ML in KDN for wireless networks is dis- cussed. First, a general introduction to each application was explored. Second, the important characteristics of each surveyed study with supporting ML algorithms were investigated. Finally, a possible KDN architecture that can be adapted for this specific study is presented. This study was expanded over various parts of a wireless network, including resource management, networking, mobility management, and localization. Therefore, it is crucial to identify the studies within these parts that can be potential use cases in generating knowledge in the KDN paradigm. These studies are the building blocks for achieving a fully knowledge-based network in future wireless networks. Table 2 presents the selected number of surveys with different applications supporting various parts of wireless networks.

and machine-to-machine (M2M) communication requirements make traditional networking intolerable. To overcome such issues, key tech- nological advances in the network, such as the KDN, can support and potentially solve our problems. In the following subsections, four major parts of the communication stack are reviewed based on previous research.

These four criteria are the essential parts of each network, and they need optimization to keep up with standards and demands in future heterogeneous networks. In this section, we discuss recent ML studies in a wireless network. Here, ML-based information is stored as knowledge for that specific network to facilitate other similar networks.

Nowadays, resources among networks are scarce and expensive. Many studies have started to use optimization methods and introduced new ideas for resource management [78]. However, recent studies have utilized ML algorithms to improve the efficiency. These studies focused on spectrum allocation, power management, QoS, BS switching, cache, and backhaul management. Knowledge that KDN provides is beneficial to these standard problems in wireless networks. Once the complex- ity of the network increases in 6G, there needs to be a centralized intelligence that can receive general network information and process that information through an ML algorithm to produce meaningful knowledge. Therefore, it is an excellent opportunity to study research works and identify those with the potential to attain useful knowledge in the KDN framework.

As the number of devices increases in the network, such as IoT, cel- lular, and sensors, spectrum shortages have drawn significant attention. The increase in data traffic now forces efficient spectrum allocation and management strategies to enhance scalability and improve QoS. In the following section, RL is investigated for possible knowledge extraction for spectrum management when using the KDN framework in the network. Table 3 summarizes the studies surveyed in this section. Knowledge derived from reinforcement learning : In [79], spectrum allocation in cognitive radio networks (CRN) is presented, and the non- dominated sorting genetic algorithm-II (NSGA-II) is used as a method to combine the evolutionary algorithm and RL method. As a result, they proposed NSGA-RL for self-tuning and spectrum allocation in an efficient manner. They used the RL algorithm to learn the value of the control parameter during the training phase. Their method evaluates the initial values of the Q-table and updates the Q-values iteratively to obtain optimal values. Based on their algorithm after network initializa- tion, non-dimensional sorting via tournament selection, crossover, and mutation is calculated, representing offspring population generation. They use population as knowledge to find the near-optimal and self- tuning control parameter to allocate spectrum-based and increase the overall network capacity. This algorithm is suitable for the centralized architecture of the KDN, where a multi-objective optimization problem

In [80], the authors developed a dynamic spectrum allocation algo- rithm in a distributed manner using a deep multi-user RL. Their method allocates the shared bandwidth into orthogonal channels, and users ac- cess the spectrum at each time slot based on a random-access protocol. First, users attempt to transmit packets with a certain probability. Then,

each user receives an acknowledgment of whether the packet has been received. Subsequently, each user maps its current state to an action based on the spectrum access obtained by a trained deep Q network. Their algorithm proves that users can learn the best policy based on the objective network utility. The proposed algorithm provides twice the channel throughput compared to slotted-aloha with optimal probabil- ity, which requires full knowledge of the number of users. Hence, this suggests that the output of the Q-learning algorithm in their study can be used as knowledge for KP in wireless networks, where the number of nodes is not visible by the controller, and the controller needs to perform the best action and allocate an efficient channel to users. The proposed algorithm suggested a distributed adaptation of transmission parameters, where the knowledge obtained from a network by users can be used in a distributed manner in the KDN architecture.

As data traffic increases in mobile networks and fixed spectrum allo- cation of operators becomes a major problem, inter-operator spectrum sharing has been proposed as a solution. This solution brings benefits but also introduces new challenges. The authors of [81] introduced a new paradigm called inter-operator proximal spectrum sharing (IOPSS) to intelligently offload users from an overloaded BS to the neighbor- ing BSs based on the spectral proximity. A Q-learning framework is equipped with each BS to dynamically determine the network load and efficiently utilize its spectrum in a self-organizing manner. The state of the system depends on the network load experienced by the BS and is defined as a discretized value. At the same time, the action of each state is determined by the tuple of spectral sharing parameters associated with each neighboring BS. The spectral sharing parameter for a BS includes the required spectrum resources (where spectral resources in this study are considered as resource blocks (RBs)) from a neighboring BS, the probability of a user that the neighboring BS can support with the strongest value of the signal-to-interference-plus- noise ratio (SINR), and the fraction of RBs that need to be reserved as requested. Simulation results demonstrate that the IOPSS-based Q- learning algorithm can intelligently offload users from congested BSs

and help cellular operators to enhance the user’s quality of experience (QoE) and efficiently share spectrum resources. In this study, the IOPSS Q-learning framework was executed in a distributed manner at every BS. This framework is suitable for both centralized and distributed KDN architectures. If the network is seen based on every cluster, where all the BSs in a network are assumed to be a greedy-based BS, then a distributed KDN using the proposed work will maximize each BS’s resources. However, this method can also be assumed as a centralized algorithm for the master controller that manages all the BSs, which eventually suggests a hybrid structure of KDN using the output data resulting from the proposed study.

theory, optimal channel allocation is obtained. The simulation results suggest that the proposed policy outperforms other policies in the literature based on temporal–spatial spectrum reuse. The study in [82] can assist in knowledge extraction and maximize the network benefits for a distributed KDN architecture. In contrast, in [83] the spectrum allocation problem is formulated into centralized and distributed parts, which can best fit our hybrid architecture in KDN.

In heterogeneous networks, the authors of [85] proposed a multi- objective fully distributed strategy based on RL for self-configuration and optimization in LTE small cells. The proposed algorithm uses a joint utility and strategy estimation under QoS constraints to minimize the received intra- and inter-tier interference for femtocells (FCs) and reduce inter-tier interference from FCs to eNBs. Their algorithm utilizes RL techniques to simplify their problem formulation compared to works where global knowledge and complete CSI are unavailable. Hence, we can utilize the RL ability to gather information from the interaction between BSs and users. They identified two sequential learning levels. During the first phase of learning, available spectrum resources were

Power management is a key feature of wireless networks. One of the biggest concerns is that as the number of devices in the network increases, each node will eventually demand a higher data rate, which forces the transmitter to increase its power. However, increasing power increases the interference between the devices. Therefore, the devel- opment of a suboptimal power allocation algorithm is necessary for future wireless networks. Exploiting knowledge from ML algorithms enables us to deploy the KDN architecture in 5G and beyond. In power management problems, knowledge is essential to decrease both the interference and energy consumption. In the following sections, super- vised and reinforcement learning algorithms are explored to examine how knowledge can be extracted and deployed in the KDN. Table 4 summarizes the studies surveyed in this section.

Knowledge derived from SL: In [89], the authors investigated the problems of optimization algorithms, which often lack adequate performance in real-time processing and suffer from complexity. They proposed a centralized SL algorithm to train a deep neural network (DNN). In particular, their algorithm is compared to the weighted minimum mean square error (WMMSE) optimiza- tion algorithm [90], which achieves 90% or higher efficiency. The WMMSE is used in interference management and requires complex matrix operations and bisection in each cycle [90,91]. Further, they used a fully connected NN with channel coeffi- cients at the input layer, multiple hidden layers, and an output layer. The system output is the optimal allocated power value. As a result, their algorithm can be adapted for power allocation in wireless networks and utilized as centralized knowledge to obtain efficient power values and mitigate interference. Having the knowledge of the DNN at the KP layer empowers the system to operate at its best and perform optimization simultaneously.

Knowledge derived from reinforcement learning : Research in [92] introduced a deep Q-learning algorithm for dynamic power al- location based on collected channel state information (CSI) and QoS. Their proposed distributed algorithm is based on the model described in [93], which does not rely on the network size, and it uses a robust technique for the dynamic changes of the network. They considered a single frequency band for transmis- sion with synchronized time slots. A classical power allocation method is utilized in the initial stage of the network. Then, an RL agent interacts with the environment and learns by ob- serving the rewards by trial and error over time [59,94]. To optimize the system and mitigate the problems with Q-learning, a deep Q-network (DQN) is used to estimate the Q-function. Their algorithm showed a fast and suboptimal power-allocation technique for various dynamic wireless networks. This will have a massive opportunity in the future for heterogeneous networks to be implemented and used as prior knowledge and facilitate the users such that optimum power is allocated to all the users quickly and efficiently. This method is more reliable than the other methods because of its response to dynamic changes in the network. A distributed knowledge plan can benefit from this

For heterogeneous networks, the authors of [95] proposed a de- centralized solution for power management and cell association using RL techniques. They have focused on inter-cell interfer- ence coordination (ICIC) using both time-domain and frequency- domain, where macro-cells and pico-cells autonomously learn to optimize their transmission power. In the time domain, the learning agent on pico-BS (PBS) intelligently self-organizes the cell range to effectively offload traffic from the macro-BS (MBS). The action performed by the two agents on both BSs is range expansion and power adjustments on each resource block, where PBS performs range expansion and power adjustment by MBS. The future state of each agent depends on the SINR condition of each UE in the network, while the received cost is the target value of the SINR to serve each UE to meet the total transmit power constraint. In each iteration of learning, PBS chooses an action resulting in the smallest Q-value, and based on this, MBS chooses its action. In the frequency-domain scenario, the action of Q-learning differed from the time domain. It uses single and multi-flow carrier aggregation in which a single UE can be served simultaneously by different BSs over different tiers/layers

(with two different carrier frequencies). Therefore, using Q- learning will enable PBS and MBS to self-optimize system per- formance using ICIC. Moreover, a fully automated multi-agent Q-learning technique was developed to facilitate heterogeneous cellular networks and to model the channel and power levels of D2D pairs [96]. Each pair attempts to maximize the value obtained by the difference between the throughput and power consumption cost, which is achieved via the defined SINR con- straint. The proposed model is formulated using a stochastic non-cooperative game, where each pair of devices becomes a learning agent to learn the best policy from locally observed information. Their simulation results showed an acceptable con- vergence rate and near-optimal performance with a few learning iterations. The study proposed in [95] suggests a distributed policy for KDN to incorporate self-healing, self-optimizing, and self-configuration of the network. Similarly, in [96] users act selfishly to choose the wireless channel and power level to max- imize their throughput, which in fact represents a distributed manner for the knowledge plane. However, the information can be used at the centralized controller to make optimal decisions when congestion in the network is high. Hence, both the dis- tributed and hybrid architectures of the KDN are suitable for this study.

a joint utility and strategy estimation algorithm is proposed to help the SBS reach the desired equilibrium network operating point. In each iteration, the femto-BS selects an action based on its current strategy and receives a reward representing the data rate. The reward of the algorithm ensures that the QoS of the macro-BS users is satisfied. The self-organizing algorithm for interference management is fully decentralized, which suggests a distributed KDN architecture.

Interest in the application of wireless networks is increasing daily. Hence, the future of wireless technology requires better and more reliable communication. Improving QoS with complex and colossal data traffic has always been a challenging research problem. As a result, having prior knowledge to enhance the QoS based on ML algorithms is crucial to ensure scalability. In this section, recent approaches for optimizing QoS based on supervised and reinforcement learning are explained. Table 5 summarizes the studies surveyed in this section.

Knowledge derived from reinforcement learning : In [101], a univer- sal deep neural model (DNM) for predicting multiple-attribute QoS is presented. Two loss functions are used to make accurate QoS predictions: the least absolute deviations and least square errors. During the training, gradient descent was used to de- termine the optimal learning parameters. The predicted QoS values are the knowledge extracted from this algorithm. After the training procedure, their method was compared to the mean absolute error (MAE) and root mean square error (RMSE) for prediction accuracy. The proposed technique achieves a higher prediction accuracy compared to the two methods. Their method can be used in a centralized KDN for a fast user’s QoS prediction, where DNM is deployed in the KP to process information and generate useful knowledge in terms of QoS prediction.

One significant difference between 5G and 4G is the network ar- chitecture and deployment of a large number of SBSs. Because of mmWave signals in 5G cellular networks, BSs need to be closer to users to reduce the propagation loss and improve the channel capacity. However, deploying a large number of SBSs comes at a price and a significant increase in the total energy consumption of the wireless network. One of the promising solutions is BS on/off switching, which saves approximately 36 million kWh per year [102]. Considerable effort has been dedicated to finding the best strategy for on/off switching mechanisms in 5G wireless networks. Among them, ML algorithms have attracted attention for their self-optimization and self-management abilities. The extracted knowledge from a trained ML can be used in a centralized KDN architecture to manage the entire network BS on/off switching. Table 6 summarizes the studies surveyed in this section.

[103] proposed a Q-learning-based algorithm for heterogeneous net- works to reduce the overall energy consumption of SBSs. Users’ infor- mation is utilized with a heuristic algorithm for the implementation information case (HAIIC) with an offline solution to reduce energy consumption. In this work, the on/off switching of SBSs is discussed by gathering complete and incomplete information, which correspond to the future and current information, respectively. With complete information, the critical sections of a cell are defined. Based on an offline approach, the cumulative energy consumption of the SBS is obtained, and the best policy is attained. For incomplete information, the HAIIC was used to categorize the on/off switching policy of the BSs. The HAIIC uses an upper-bound threshold based on the energy consumption ratio (ECR). The learning algorithm defines the SBS on/off switching operation as an action. The state depends on the number of

active users and SBS. Finally, the reward of the learning algorithm is achieved through a switching action in any particular state. The reward of the algorithm is entitled to energy consumption and transmission gain constraints. The procedure is iterative, and the calculated reward updates the Q-value until it converges. From the simulation results, it is concluded that the proposed HAIIC algorithm minimizes energy consumption. Similarly, studies were conducted, such as the work in [104], which used actor–critic learning to control the on/off BS switching. The RL technique defines the BS switching operation as the action and the amount of traffic load on the controller as the state. The controller decides an action based on the traffic load in a stochastic manner to minimize the overall energy usage. The authors of [103] used the coverage of the BS to switch off the SBSs in their vicinity, which suggests a centralized KDN architecture at the main controller. Similarly, in [104], the overlap between the coverage areas of BSs is considered to turn on/off a BS, which also suggests a centralized architecture of KDN.

Moreover, a BS active/sleep scheduling scheme is proposed for k- tier heterogeneous networks, guaranteeing coverage, QoS, and through- put [107]. They used a fuzzy Q-learning method to put the BS in sleep, while there was no user to serve or activate a BS once the user was detected in the cell. To save energy, the algorithm uses an optimal sensing probability strategy for user detection. An SBS is in the sleep mode state when there is no active flow, and it randomly activates to scan the coverage area for possible users based on the tuned output sensing probability action. It is observed that the proposed algorithm can efficiently handle user population fluctuations and increase the energy efficiency. All three [105–107] proposed schemes can be used in a centralized architecture of KDN to provide an energy-efficient algorithm to reduce energy consumption.

As predicted by Cisco [108], wireless networks, especially cellular networks, will produce about 30.6 exabytes of data traffic each month. This is due to the proliferation of smart devices and the appear- ance of high-tech applications, such as ubiquitous social networking, augmented reality, and high-definition live streaming. Faced with un- precedented data traffic, intelligent learning-based caching strategies have been introduced to alleviate backhaul traffic and shorten la- tency [109]. In this section, we investigate ML algorithms to assist in creating knowledge in the KDN paradigm for cache management. Table 7 summarizes the studies surveyed in this section.

Knowledge derived from supervised learning : To construct estima- tion methods to identify the popularity of content in cellular net- works, an ELM neural network is used to improve the QoE and reduce the network traffic [110]. The proposed method adapts mixed-integer linear programming for content replacement. The algorithm uses a perturbation stochastic approximation to select the physical cache size simultaneously and performs efficient cache deployment. In their method, the stochastic approxima- tion reduces the number of neurons for ELM while ensuring an accurate prediction of future content popularity. The pro- posed method utilizes real-world data while making efficient cache decisions compared to the most popular cache deployment schemes, such as K-nearest neighbor [123] and regression [124]. The proposed caching scheme in [110] considers content popu- larity, cache size, network topology, and link capacity to perform efficient content caching and cache deployment. The algorithm has a cache manager to communicate with all BSs, making it suitable for centralized KDN.

The authors of [113] used three different methods to minimize the backhaul load by predicting the cache content from raw video data. First, a 3D CNN is used for feature extraction, in which a single frame of the video is analyzed alternatively. Second, the SVM algorithm is utilized for generating represen- tation vectors of videos, and third, a regression model predicts the video popularity content. After obtaining the popularity of videos, the optimal segment of each video is cached by the BS to minimize the backhaul traffic. The proposed algorithm predicts the popularity of a new video without any statistical information. Here, the BS is connected to the core network, where the core network is connected centrally to the content server, which represents a centralized cache strategy.

lower the requested content and congestion at the BS by en- abling devices to request content from nearby users. The authors of [116] focused on joint content delivery policy and cache content placement. Cache content placement determines the amount of traffic unloaded from the BS to the D2D. This study uses RNN methods, specifically ESN and long short-term mem- ory (LSTM), to predict the users’ mobility patterns and content popularity. Therefore, the algorithm realizes where to cache and which content to cache. Once the user’s local cache content cannot satisfy the content request of the user, the user will establish a D2D link with one of the neighboring users. The process of selecting the most appropriate user was performed

using a DRL agent. The DRL agent optimizes content delivery and makes dynamic decision making for user selection. The simulation results indicate that the proposed content placement and content delivery approaches improve the cache hit ratio and reduce delivery delay and energy consumption. This method is inherently distributed, where nodes find their requesting content from nearby users. However, this information can be used in a centralized manner to monitor the content exchanged and minimize the network traffic on the BS.

Owing to the time-varying nature of the wireless channel, the authors of [117] proposed a deep reinforcement approach to op- timize the cache-enabled interference alignment. They adapted a finite-state Markov channel (FSMC) and used a deep Q-network to develop a caching update scheme. First, a centralized sched- uler collects CSI and cache status information from the users. Then, a deep Q-network feeds the optimal action for the cur- rent instant and stores the agent’s experience of each time instance to optimize the Q-network parameter accordingly. Their method maximizes the Q-learning reward function based on existing knowledge and attempts new actions to acquire new knowledge. The proposed algorithm significantly improves the network performance compared to other studies, in which an invariant channel is assumed. This is a promising technique to be used at KP in a centralized manner, where a controller collects the CSI from the users and sends information to the deep Q- network to obtain the optimal policy for users. The output of the learning algorithm is the knowledge information used in response to wireless networks with similar behavior.

tinger [128] architecture for content caching at the BS. The objective of the algorithm is to maximize the long-term cache hit rate with no information about the content popularity distri- bution. The input of the DRL system is the requested frequency of files and the current file request, and the action of the system is whether to cache the requested content at the BS. The proposed technique is compared with different traditional cache update schemes, namely LRU and LFU [129]. The simulation results prove that the algorithm outperforms both schemes in terms of short-term and long-term cache hit rates. In [119], the authors proposed a DRL approach to enable dynamic or- chestration of caching resources, networking, and computing

resources in vehicular networks. The DRL agent assigns vehicles to BSs and decides whether to cache the requested content by the vehicle at the BS. The proposed algorithm jointly optimizes the problems associated with resource allocation and caching. Simulation results with various system parameters show that the DRL system performs much better than existing methods, such as edge caching, mobile edge computing (MEC) offloading, and virtualization. Based on the requested content in [118] the DRL agent acts as a controller to decide whether to store the

content at the BS local storage. This method suits the centralized architecture of the KDN to serve users directly with a minimum delay. A more sophisticated strategy was adopted in [119] to perform caching, computing, and networking in a systematic, centralized manner using the SDN controller, which is a close research study to initiate knowledge to accomplish different networking tasks.

In heterogeneous networks, SBSs are assumed to have high storage capabilities to cache popular files, such that the user can capture the file in a faster and more efficient manner. In particular, SBSs must distinguish the popularity of files and esti- mate the user demand for that file within a specific time interval. The authors of [122] proposed a TL-based approach to increase the performance of estimating a popularity profile. The central- ized approach uses prior knowledge to compute and estimate the popularity of a file based on requests during a predefined observation period. The estimation was then used to optimize the catching probability. The proposed approach reduced the convergence time of the training phase. In this study, centralized knowledge is generated based on the popularity profile of the cache content.

Content caching at SBSs requires backhaul management, and be- cause of the heterogeneity of backhaul, both wired and wireless back- haul must work together to handle the massive traffic. Wired links use fiber cables, and wireless connections are now deploying mmWave frequencies. Owing to the heterogeneity of backhaul links, the manage- ment of backhaul has attracted significant attention. Different solutions have been proposed to reduce the complexity of backhaul [131,132]. However, new studies have concentrated on ML for reliable backhaul management. In the following section, ML studies that can be deployed in the KDN are investigated. Table 8 summarizes the studies surveyed in this section.

Knowledge derived from reinforcement learning: The deployment of low-power and short-range heterogeneous SBSs and the exponential increase in wireless traffic will cause congestion in backhaul for BSs to communicate with each other. The authors of [133] designed a distributed backhaul management model from a game-theoretic per- spective. They utilized the RL technique to solve the gaming problem using joint utility and strategy estimation. Each SBS is responsible for predicting files to download without compromising the required transmission rate. In this study, different SCNs with several coexisting backhaul systems are connected using wired links, mmWaves, and sub-

With the exponential growth of users and data traffic in the net- work, networking in wireless communication systems requires more advanced solutions. In particular, challenges including the imbalanced distribution of traffic loads among BSs and wireless channel dynamics need to be addressed. Furthermore, emerging vehicular networks and self-driving vehicles introduce new difficulties that are not addressed in traditional networking algorithms. To overcome these issues, the knowledge acquired by ML algorithms can assist networks in building intelligence and automation. Therefore, new ML studies for routing, clustering, data aggregation, and user associations were investigated.

User demands for high-resolution data with enormous sizes have led to new RAN technologies, including cognitive radio networks, C-RANs, and ultra-dense networks (UDNs). To facilitate effective networking, routing strategies and policies play an important role. Traditionally, network upgrades rely on hardware solutions, for example, by improv- ing the core size or increasing the router size to enhance network performance. On the other hand, the software development aspect of traffic management for routing policies has consistently failed because of the varying network environments. Recently, ML has made a major technological breakthrough with efficient routing protocols to enhance processing packets and throughput performance [136]. Accordingly, we provide a detailed summary of the novel ML techniques-based routing for improving the knowledge layer of the KDN to create self- configuration and self-optimization. Table 9 summarizes the studies surveyed in this section.

which consist of heuristic algorithms. After the training pro- cess, the algorithm can identify the path by satisfying system QoS. Once the controller receives new routing requests, the trained ML instantly provides heuristic-like results. The system performance is compared to the classical max–min ant system (MMAS) [154] which has been proven to be an appropriate approach for examining the performance of routing frameworks. MMAS is an upgraded version of the ant colony optimization method taken from ant routing [155]. The proposed study [137] can be adapted to the centralized architecture of the KDN for this particular study. The reason is that the routing framework uti- lizes the SDN functionality to gather global information and then predict a route. Therefore, the proposed protocol is inherently centralized and suitable for the centralized architecture of O- RAN networks. The proposed technique enables the knowledge plan in the KDN to identify routes that maximize the network QoS.

An autonomous vehicle or self-driving car can communicate with other vehicles, roadside units, and infrastructure. This ca- pability is known as vehicle-to-vehicle (V2V) and vehicle-to- infrastructure (V2I) communication to exchange essential in- formation, such as speed, location, environmental conditions, etc., to nearby vehicles and the controller. Authors of [138] proposed a delay-bounded routing framework for vehicular ad hoc networks (VANETs). They focused on delivering messages with user-defined delay parameters and minimum usage of the radio spectrum. The delay-bounded routing protocol uses linear regression to predict the traveling distance and available time for forwarding a message. Their algorithm has two schemes, the greedy and centralized schemes, which are both based on linear regression. The greedy strategy predicts the available time by using current sampling data, and the centralized scheme uses global statistical information to choose the optimal path for routing. The simulation results illustrate that the radio usage is greatly reduced. Moreover, the functionality of using both greedy and centralized-based techniques establishes a connec- tion for the hybrid architecture of KDN to enable the installation of routing protocols in VANETs.

In [139], the authors combined two SL classifiers; decision tree learner and rule learner, for routing optimization in a wireless sensor network. They proposed a MetricMap based on MintRoute, which collects the routing protocol to obtain the link quality. MetricMap uses two components, the first component updates the features for the learning strategy when a packet arrives. The second component controls the link classification with input from the features, and the output values indicate the link quality. For their performance measurements, they considered data latency, data delivery rate, and fairness index. From the evaluation of the 30 sensor nodes in the network, the

MetricMap achieves up to 300% improvement in data delivery with no effect on other performance matrices. As indicated in the study, the training phase is made at a backend server, which suggests a centralized unit that collects all the information, trains a machine, and then instructs the nodes. This means that a centralized knowledge plane is created in this study to guide the nodes. The reason for a centralized KDN is that the authors introduce a two-stage route prediction where the first stage updates the learning strategy, and the second stage identifies the route. The learning strategy requires global information to converge to optimum performance. Therefore, a centralized KDN architecture can provide better information than a distributed architecture. Moreover, the first stage of the learning algorithm is an excellent example of deploying a machine learning in the knowledge plane. The controller uses the second stage to prescribe routing decisions for the network layer.

proposed an intelligent routing scheme using a deep CNN. Their method learns from the previous experience based on conges- tion, and uses this information to train a two-phase procedure, namely cold start period and intelligent running period. The cold start period is the initialization of the training set, where the algorithm only defines a route with a minimum hop path. After this period, the algorithm switches to the intelligent running period, where it performs real-time updating and routing judg- ments. More importantly, a CNN is constructed for each routing decision, which takes the collected information based on traffic patterns from routers, including traffic generation rate, to pre- dict whether the selected routing strategy can cause congestion in the network. This process is periodically updated until it is predicted that the chosen route will cause no congestion. Simu- lation results prove that the proposed algorithm performs much better in terms of E2E delay and packet loss ratio compared to conventional routing strategies, where there is no intelligence. The proposed method is a real-time intelligent network traffic control method that can be adopted in a centralized KDN. A centralized KDN can collect traffic congestion information from the network and tune the deep CNN to converge to optimum per- formance within the timeframe threshold. It is not recommended to use the distributed knowledge plane due to the algorithm’s real-time updates, which require the maximum amount of data from a large number of nodes to make appropriate routing decisions.

In [142], a supervised DNN was proposed for routing optimiza- tion in heterogeneous networks to predict the path from the source to the destination node. Each router in the network uses a DNN to predict the next hop; the DNN takes traffic patterns as in- puts, and based on these inputs, it generates the desired output. The output of the deep learning structure significantly improved the network traffic management. There are three phases to ob- tain a fully functional ML. The first phase is the initial phase, where the traditional routing protocols, such as OSPF, provide the network route, and the network starts to operate. At the same time, the second phase is the training phase to train the deep learning system from the collected information based on the traditional operating system. Finally, the running phase is the stage in which the machine is fully trained and can provide real- time routing strategies. This method has been proven to have a higher throughput and lower overhead compared to OSPF. The proposed study suggests a greedy-based distributed architecture over a knowledge-based network to increase the throughput.

Knowledge derived from unsupervised learning : In [143], the au- thors focused on load balance routing based on PCA and NN for dimension reduction and prediction of the network load status. To obtain intelligence from the network, they combined SDN with ML and data analytics. The use of these algorithms has led to efficient and intelligent routing decisions. This article aims to address the shortcomings associated with the next generation of wireless mobile networks, such as video streaming and online gaming, to mitigate the delay caused by traffic. They proposed a routing strategy based on an ML scheme, where PCA was

used to reduce the dimension of the vector matrix by applying it to the original adjacency matrix of the network topology. Based on the normalization, they designed a queue-utilization routing algorithm for routing prediction. Moreover, routers were continuously updated based on neighbors’ information to select the routers with more resources. In this vein, they explored the current SDN architecture and represented an ML algorithm to predict routes. Nodes can also use the proposed algorithm to reduce the unnecessary information in the routing table and decrease the contention on the nodes in a distributed manner. However, this framework represents a centralized KDN, where the controller collects the information from the data plane and then uses ML techniques to obtain knowledge to identify an effi- cient route. Overall, the representation of SDN with knowledge suggests a centralized KDN.

Owing to the fixed network architecture of some routing proto- cols and the massive volume of data traffic exchanged between devices, the authors of [144] introduced a context-aware routing protocol named KROp. This protocol uses several network fea- tures to make routing decisions based on the network conditions. KROp uses the K-mean clustering algorithm and exploits network features to select the best next hop. This algorithm is based on the knowledge acquired from the node’s behavior to identify a cluster of the best forwarders. The numerical results show su- perior performance in KROp in terms of dropped packets, over- head, and average hop count compared to other routing strate- gies, such as history-based prediction routing (HBPR) [156], and probabilistic routing protocol using the history of encounters and transitivity (PRoPHET) [157]. The proposed protocol uses the entire network information to make routing decisions. This means a centralized unit has the advantage of collecting more useful information compared to distributed. Therefore, this tech- nique is suitable for a centralized KDN to make the next-hop selection decision.

Tang et al. [145] proposed a centralized routing scheme with mobility prediction (CRS-MP) for VANETs. Their method uti- lizes an SDN controller with an ANN to gather information and predict the user’s arrival rate. Based on the arrival rate of each vehicle, RSUs or BSs can model statistical traffic patterns and estimate traffic mobility. Intelligence was also used in this study by integrating the CRS-MP model at the RSU/BS to predict the mobility patterns of vehicles and find vehicle connections. The ANN takes an input according to the number of arrival vehicles at different time instances, and based on the initial random weights, it predicts the vehicle arrival rates. The arrival rate results in the arrival rate function, which is later used to make routing decisions and evaluate the transmission rate and average delay. The numerical results of the CRS-MP scheme outperform other vehicular routing protocols, such as V2I and V2V communication, in terms of overall vehicular service de- lay. Furthermore, the proposed algorithm is independent of the mobility rate, making it more robust to high mobility rates. The proposed routing protocol utilizes multi-hop routing in vehicular systems using an SDN controller that solves the overload on the

BS. Hence, in the near future, centralized routing protocols for mobile devices can use the knowledge plane to decide whether the BS or other devices must route the packet. Centralized VANET protocols have better performance compared to the de- centralized topologies. In centralized VANET protocols, vehicles are supported by the BS and road-side units (RSUs). On the other hand in purely distributed systems, vehicles must relay any vital information across the network using multi-hop communication. Therefore, there is a risk of link failure in highly mobile environ- ments, which might have catastrophic consequences. Generally, VANET protocols are supported by a centralized controller to collect data and inform vehicles. The proposed protocol in [145] uses a centralized controller to collect data and process the data through ML algorithms. Consequently, centralized KDN architecture is usually more suitable for VANET protocols.

In [147], the study tackles the energy-aware routing in wireless sensor networks (WSNs) to transmit data packets using efficient paths within the shortest time such that the lifetime of the network increases. Specifically, they used Q-routing algorithms and extended them to propagate information faster with lower energy consumption [158]. The algorithm uses Q-learning to save the energy levels of the nodes in matrices after a sensor sends a feedback message. When the sensor receives the feed- back messages from neighboring nodes, it modifies the Q-values in the achieved routing table. Once a node has a packet to transmit, it selects the next node from the routing table with the best Q-value in a greedy manner to relay the packet to the des- tination. Their technique has proven optimal routing decisions for low-energy nodes. The greedy-based approach allows nodes to individually select the best route, which suggests a distributed KDN.

Recently, CRN has attracted considerable attention owing to its importance in future wireless communication systems. This technology overcomes the scarcity of the channel spectrum by allowing secondary users or unlicensed users to benefit from underutilized licensed channels. However, the dynamic nature of CRNs makes routing a complicated task. The authors of [148] proposed a clustering mechanism or cluster-based routing to boost network scalability and functionality. Once the cluster heads are identified in the network, each cluster head estimates the Q-value of each neighboring node. The routing table is constructed based on the Q-values, and the largest Q-value is the next chosen node for the next hop. During the learning procedure, the state of the network represents the destination

node, and the decision to select the next hop is the action. Finally, the throughput resulting from the chosen hop is the reward of the system. In this study, the knowledge is derived from each state and action pair, which provides an appropriate action for the next instant. The proposed cluster-based routing is recognized as a distributed method for routing a packet in a CRN. Because the Q-values are estimated locally by the nodes, the highest values are used to build the routing table. This means the routing mechanism is constructed in a distributed manner and is more suitable for distributed KDN architecture.

In [149], the authors studied three route selection schemes in a real testbed environment to improve the performance of multi- CR networks. One of the schemes is based on spectrum leasing, and the other two are based on RL. Spectrum leasing is a new term used for communication between unlicensed and licensed users in CR networks. The two RL algorithms are based on Q-learning to predict the next-hop neighbor. Similar to other studies [148] the next hop is selected based on the highest Q- value. The state action is the destination node and the selected next-hop node for the source node to transmit the data. The reward is the channel-state information. The proposed routing scheme was compared with the highest-channel (HC) protocol in a multihop network and has shown better performance. In the proposed study, we can adapt a hybrid KDN because both centralized and distributed models are utilized. The proposed protocol uses the channel data information to select the next best hop based on the output of the Q-learning. The channel information can be accessed by both the user and the BS, which means the protocol

The authors of [152] added intelligence to the network to miti- gate the complexity of network topologies. They integrated both centralized and distributed network functionality to guarantee high QoS. Their hybrid approach uses AI routers for distributed intelligence and a network mind for centralized intelligence. AI routers are responsible for hop-by-hop IP routing to ease

In wireless networks, nodes/sensors/users have always been clus- tered to describe their distinctive features or differentiate based on their mobility rate, coordinates, etc. Clustering different nodes for different purposes improves the overall performance of the network. It is evident from the introduction of ML techniques that clustering problems are naturally solved using K-mean algorithms. However, other cluster- ing methods have been proposed within supervised and unsupervised learning techniques. Clustering is one of the primary and essential applications of KDN for various purposes, such as traffic classification and data storage. Table 10 summarizes the studies surveyed in this section.

Knowledge derived from supervised learning: One of the problems in ML is class imbalance, where the class distributions are highly separated. This means that the total number of minority or scares classes (also known as positive ones) is far less than the majority class (represented as negative) for a two-class scenario. When we apply a traditional classifier in these scenarios, they are likely to predict everything as a majority or negative class. In [161], the authors used logistic regression for imbalanced problems to improve the performance of the learning procedure. The proposed method is called logistic regression for imbalanced learning based on clustering (LRILC). First, K-mean clustering was applied to the dataset to partition the majority class into small clusters. Logistic regression is then used to overcome the class-imbalance problem. The experimental results show a higher accuracy in clustering the dataset compared to state-of- the-art classification methods. The proposed method can be used in a centralized KDN to solve imbalanced problems with large datasets.

In [162], fraud calls are identified by investigating the user’s behavior. Their method uses the application of SVM alongside fuzzy clustering to identify fraudulent phone subscribers. Fuzzy clustering takes unlabeled input data and clusters the data ac- cording to their similarities. Furthermore, after a trained data algorithm obtains input data, it generates a value between 0 and 1. If the output value is closer to 1, it shows a higher degree of similarity. Their algorithm takes large datasets and

utilizes PCA to reduce the dimension, and then uses the library of support vector machine (LIBSVM) and least square support vector machine (LS-SVM) with fuzzy c-means (FCM) and fuzzy K-means (FKM) to build the user’s profile. If a call pattern does not match any standard pattern, it is classified as a fraudulent call. A comparative study was conducted using different methods of SVM and fuzzy clustering, as specified above. This shows that using LIBSVM and LS-SVM leads to better approximation and accuracy. The proposed algorithm can be utilized in a centralized BS to differentiate between ordinary and fraudulent calls. Accordingly, knowledge can be derived from this study to approximate the genuineness of calls in any BS.

Among the challenges associated with IoT, two challenges pose threats to the overall network connectivity, including battery life and the ability of edge devices to communicate over a long distance. One promising technology among low-power wide- area networks (LPWANs) is LoRa, which operates based on spread-spectrum modulation techniques. In [163] ML algorithms were adapted to edge devices to mitigate two challenges: life expectancy and the ability to communicate over long distances. To achieve this, LoRa is used for low-power transmission, and KNN is used for the activity classification process. They have accomplished an amazing low energy expenditure of 5.1 mJ in power consumption for activity classification, resulting in a battery life of 331 days. A similar technique can be deployed to IoT distributed edge devices to increase the device lifetime in a distributed manner.

Caching popular content at SBS for intelligence gathering in ultra-dense heterogeneous networks effectively decreases redun- dant data transmission and E2E delay. However, dealing with different data content is challenging and time-consuming. The authors of [164] used a clustering-based TDMA transmission scheme for content placement and user association. They used an offline training procedure using DNN to predict the user association for each cluster, where the input to the NN is the user channel gain and user demand. As the number of clusters increases exponentially as the number of users served by the SBS increases, the user cluster information and time duration of serving each cluster are optimized using the DNN. As a result, the user association to each cluster can be quickly identified, and the time required to obtain the optimal user cluster information is reduced. This method enables us to achieve efficient load balancing and user clustering in a centralized KDN.

Knowledge derived from unsupervised learning : As the number of connected devices in IoT for applications, such as smart cities, smart homes, farms, and factories, reaches more than 31 billion, providing secure communication and access control becomes a priority. In [165], the authors proposed a mechanism based on unsupervised clustering techniques to enable reasonable access control throughout the communication history of IoT networks. Their method is called INSTRUCT and has two separate algo- rithms for different types of traffic, one for TCP traffic and the other for UDP traffic. The proposed algorithm uses past communication data to allow access to IoT devices by installing new rules on the switches involved using the clustering tech- nique. K-mean clustering was applied to distinguish between valid and invalid traffic captured by the switches. INSTRUCT achieves 100% classification accuracy for TCP traffic and 95% for UDP based on their comparison with signature-based manual analysis. This algorithm is suitable for a centralized KDN to provide access control to IoT devices.

coordinate their transmissions while reducing energy consump- tion and traffic load. This study formulated the problem as a noncooperative game between clusters, where clusters seek to minimize the cost function to reduce energy consumption. Based on the information regarding the location of SBSs and the capability of handling users and data traffic, the cluster determines their transmission power and on/off situation. The simulation results show improved overall performance when using the cluster-based coordination method in small-cell net- works. The algorithm attempts to reduce the overhead on a centralized controller by allowing the SBS to decide based on their locally acquired information. Hence, this method can be fitted to the distributed architecture of the KDN.

Current wireless communication networks rely on the existence of a cellular architecture. Cellular communication requires BSs for users to request, receive, and upload information. Every BS in the network has a coverage area that supports a specific geographic area with limited users. To increase the capacity of BSs in the cellular network, small cells were introduced to enable service providers to offload users from an overloaded BS to sub-BSs, namely macro BSs, pico BSs, and femto BSs. Therefore, more cellular networks are shifting toward heterogeneous networks (HetNets), enabling flexibility and low-cost deployment of new infrastructure. To associate users with an appropriate cell, the KDN requires reliable methods. Hence, some practical ML-based techniques have been investigated, including studies on SL and RL. Table 11 summarizes the studies surveyed in this section.

Knowledge derived from reinforcement learning : Cell range expan- sion is a technique to increase the coverage area of BSs to either support other users or to cover the blind spots, which increases the coverage area, network throughput, and cell-edge throughput. Expanding the coverage area can be achieved by adding a bias value to the user equipment (EU). Selecting an optimal bias value depends on various factors, including the radio resource ratio between the MBSs and pico-BSs. A dynamic method is presented to determine the bias value of each UE by using Q-learning algorithms [172]. Here, all the UEs learn the bias value independently with the aid of Q-learning to decrease the outage of UEs. The proposed Q-learning algorithm is a multi- agent learning system that allows every user to determine the bias value. The state of the system is the received signal power from both the MBS and pico-BS. The agent’s action is to choose

a bias value, and the reward of the system is determined by the BS when calculating the number of outages. The proposed algorithm increases the throughput and decreases the number of disconnected UEs. This algorithm is a greedy-based cell ex- pansion by every UE, which can be used in a distributed KDN architecture.

Although information, such as channel quality, backhaul capac- ity, and SINR are important attributes of user association, the user’s QoE has the same or higher priority. Pervez et al. [174] in- troduced a distributed user-centric backhaul-aware user associ- ation scheme via a fuzzy Q-learning algorithm to autonomously enable each BS/cell to use QoE and backhaul constraints to max- imize the throughput of the network. Fuzzy Q-learning attempts to learn the optimal bias value (which is the action of the RL algorithm) by the fuzzy rules in an iterative manner by interact- ing with the environment. Specifically, users receive a bias value from different cells, guiding them to associate with the most suitable cell. At the same time, each bias value represents an attribute that satisfies the network performance metrics, includ- ing latency and throughput. The proposed algorithm achieves an optimal performance faster than Q-learning-based methods and, more importantly, improves the user’s performance. Because users will have different requirements in the 5G system, each is associated with different cells based on their needs. Therefore, the above technique is best suited for distributed user association schemes in the KDN architecture.

Owing to changes in network characteristics and the increase in devices with different requirements, researchers have turned to HetNets. Even though it provides several benefits, it presents challenges, such as interference between SBSs, power control, and user association problems. The authors of [175] used a multi-agent deep Q-learning network (DQN) to solve some of these problems. Their algorithm consists of convex optimization and fractional programming, and uses DQN to jointly optimize user association and power management in OFDMA systems. In the Q-learning algorithm, the agent (or UE) must select the

appropriate BS to create communication links and determine the transmission power. The transmission power of the user is the action of the agent throughout the learning procedure. The reward function is the sum energy efficiency of all the UEs. The objective of the learning algorithm is to maximize the expected accumulated reward under QoS constraints. The convergence of multi-agent DQN was analyzed in the simulation results, and it proved to be superior to traditional RL-based techniques. Moreover, the algorithm maximizes the long-term overall network performance and demonstrates efficient energy consumption. The distributed method above shows a solid con- nection to the distributed KDN architecture. Chou et al. used DRL to jointly solve user association and resource management problems in mobile edge computing (MEC) to improve the QoE for online video streaming in 5G networks [176]. The problem is formulated based on the Markov decision process (MDP) and analyzed by a deep deterministic policy gradient (DDPG) algo- rithm based on the supply demand interpretation of the Lagrange dual problems. First, they used the traditional optimization La- grangian approach, where the source of the performance loss in this algorithm was identified as the Lagrangian multiplier update

function. Then, they proposed a pricing function based on MDP for the update function, which was solved by DDPG. Here, the MDP is solved using a DRL, where the supply–demand inspires the agent’s action as the output price of each video from the BS. Then, based on the prices for each video from different BSs, the UE is associated with a BS whose data rate is maximized. The reward function is defined as the sum of each UE’s QoE, and the goal is to maximize it. Simulation results show that the proposed method achieves significant improvements in QoE, particularly in congested networks with low resources. This method can be used in a distributed knowledge plane for users to associate with a BS autonomously.

Symbiotic radio networks (SRNs) have been introduced to enable the coexistence of various networks and utilize resources glob- ally and provide connectivity across multiple networks [178– 180]. The authors of [177] focused on the symbiosis between IoT and cellular networks and the user association problems in SRN. They used the TDMA for cellular communication between users and BSs. Then, each IoT device is associated with one user to exchange information. The dynamic changes in the environment make the collection of real-time channel information difficult.

To overcome this problem, two DRL algorithms were utilized to guarantee optimal user association. One of the algorithms is centralized, which makes decisions for IoT devices based on globally available information. At the same time, the other is distributed and makes decisions based on locally available information. The DRL algorithm can have two states based on the proposed DRL algorithm. The first state is the action space in a centralized DRL-based user association, which is a matrix of cellular users with the associated IoT device. The second state is a distributed DRL-based user association scheme with one IoT device. The immediate reward in both schemes is the sum rate of all IoT devices. The proposed scheme shows optimal user association with high scalability, even when IoT devices increase in the network. This algorithm is suitable for hybrid KDN architectures because it takes advantage of both centralized and distributed algorithms.

As networks are increasing in complexity and become more difficult to manage, embedding intelligence into devices will ease optimization, recommendation, organization, and management. Most studies in the networking area are distributed in nature, which makes it difficult to include ML-based algorithms for controlling the devices. KDN function- ality provides an opportunity to bring intelligence and knowledge to the network. The KDN can collect global information to improve network performance. Traffic classification is a crucial activity in network man- agement, and massive growth in Internet users has brought network traffic classification into attention. Table 12 summarizes the studies surveyed in this section.

Knowledge derived from supervised learning : Raikar et al. inte- grated SDN architecture with SL techniques, specifically SVM, Naïve Bayas (NB), and the nearest centroid is used to classify the network data traffic [181]. First, in the learning phase of their algorithm, the training data are fed to the system to map the network traffic into defined classes. Later, real-time data were captured and mapped based on the trained SL for network traffic classification. In their method, the SDN controller utilizes three different SL algorithms to classify the data into HTTP, mail, and streaming. Their proposed solution was able to obtain high accuracy in all three learning algorithms with the highest accuracy for NB, followed by SVM and the nearest centroid. Their algorithm provides centralized data classification, which offers an opportunity to add intelligence to network devices.

Knowledge derived from transfer learning : To avoid training data from scratch, researchers in [182] method to address multi- class traffic classification problems, and they utilized Maxent as the base classifier in their approach. A new classification task in TrAdaBoost was used to extract labeled data from several network traffic sources. Next, the Maxent model is used to classify and convey traffic knowledge from the source domain to the target domain. The proposed scheme was trained and transferred as prior knowledge for different environments to reveal its performance. They tested their method with two tra- ditional ML algorithms based on Maxnet, known as NoTL and NoTL advance, where TrAdaBoost achieves better performance compared to the other two methods. Their learning algorithm can achieve high accuracy in classifying network data traffic and provide a promising solution for centralized KDN architecture.

of users and their activities can help generate intelligence to increase network performance. Recently, there have been many advancements in mobility prediction and handover management in the field of ML. In the following sections, some ML techniques for the two essential components of mobility management are presented.

Movement is an inherent nature of mobile devices, and predicting the next location of these devices is called mobility prediction. Knowing the next location of the user can improve the network performance, es- pecially in resource management, D2D, and V2V communication [187]. In this section, some promising methods using ML algorithms are presented. These methods provide a huge advantage in the KDN ar- chitecture for predicting user mobility patterns in the next generation of cellular networks. Table 13 summarizes the studies surveyed in this section.

Knowledge derived from supervised learning : Learning and knowing the next location of mobile equipment enables mobile appli- cations and a coherent handover process. Location prediction techniques use the historical trajectory information of mobile users to guess an individual’s next position. Many studies have introduced location prediction methods with acceptable accu- racies [188–190]. However, some of them perform erratically when a user’s activities change in a new area, while others encounter the ‘‘cold start’’ problem when the user’s trajectory information is sparse. The authors of [183] involved the user’s activity patterns and historical data and proposed an SL-based location prediction method. Their technique evaluates the next activity of the mobile user by modeling individual activity pat- terns rather than directly predicting the next location of the mobile user. Then, it predicts the next location of the user based on the obtained next activity. Using real-life GPS trajectory data, the simulation results show a smooth upgrade and robust per- formance based on the proposed prediction procedure. Having knowledge of the user’s activity and location in a centralized KDN architecture improves the network performance, especially when a controller needs to decide on resource allocation and cluster head selection.

Knowledge derived from unsupervised learning : Owing to the dra- matic increase in users across networks, configuration and op- timization have become more complicated. More users push the network to split into small cells and SBSs. More frequent handovers occur once there are more SBS in the network, and because of the complexity of the indoor environment, this task is more delicate in an indoor scenario. Self-organizing networks (SONs) are the key to the next generation of mobile networks for self-healing, self-optimization, and self-organization. Sinclair et al. proposed a modified self-organizing map (SOM) method to make indoor location predictions of users while a handover request occurs [184]. Their method determines whether the indoor user should be connected to another BS or prohibit the handover based on their location information. SON utilizes an unsupervised NN that allows the learning algorithm to generate a low-dimensional output space from high-dimensional input data. The input data depend on the mobile terminal approach, whereas in this scheme, the angle-of-arrival (AoA) of the user and the reference signal received power (RSRP) are fed to the

NN. Based on this information, the SOM is capable of estimat- ing the user’s actual physical location. Accordingly, based on the pre-determined zones, which correspond to prohibited and permitted areas, a handover decision is made. The simulation results demonstrate that the proposed algorithm reduces the total number of handovers by 70% while allowing the nec- essary handovers to proceed. The proposed algorithm has an automatic system that uses monitoring, analyzing, planning,

and execution phases to perform a centralized knowledge-based concept. Moreover, to allow BSs to autonomously discover the RF conditions at the cell edge and their impact on the handover parameters, unsupervised-shapelets and data mining techniques were proposed to recognize patterns in the RSRP measurement reports from users [185]. Their method makes position estima- tion once a handover is triggered. Based on the positioning, the BS discovers new patterns while the network characteristics change and calculates the number of clusters in the network. The simulation results illustrate that even without prior knowledge, the algorithm provides 95% accuracy in clustering the nodes and predicting the user’s location. The proposed algorithm uses cluster heads to identify user’s movements and trajectories. This method is suitable for a centralized KDN for user-trajectory prediction.

Knowledge derived from reinforcement learning : In [186], a mobil- ity prediction model based on DRL at the edge of the network was proposed for mobile users. They designed a DRL framework to offload traffic by training a DQN for mobility prediction. Their method comprises a glimpse mobility prediction model that gathers users’ mobility patterns and trains them in the DRL. The algorithm first assumes that the controller can select the best edge server and apply the DRL. Then, the controller predicts the users’ future locations based on historical data and past user mobility using the DRL agent. The authors used the actual human trajectory and user latency to obtain the perfor- mance of their algorithm. The experimental results show that

the glimpse mobility model outperforms the perfect mobility model and chooses the lowest latency service request. Therefore, it is essential to have a similar model at the edge network to allow the controller to select the best strategy with the least la- tency. Additionally, mobile service providers can determine the expected latency experienced by users to receive data packets in future cellular networks. The low-latency ML-based algorithm in this work suggests a centralized knowledge plane to predict the series of locations and timing of the users.

Recently, the deployment of SBS has contributed to increasing network performance to provide acceptable QoE. However, deploying SBSs means that users will have more frequent handovers, which can negatively impact users’ QoE [191]. In the upcoming section, some of the studies on handover management based on ML techniques are presented to assist future KDN networks. Table 14 summarizes the studies surveyed in this section.

Knowledge derived from supervised learning : Handover in the con- text of wireless communication means passing the control of a UE from its serving eNB to the next nearest eNB without any interruption. With today’s continuous connectivity of users with a high demand for data rate and low latency, handover man- agement is becoming more complex. Additionally, to provide seamless connectivity to users, a handover is required. How- ever, challenges such as security and QoS are expected. Many

studies have used ML techniques for self-organizing networks to improve network performance. Ali et al. presented an SL- based handover management scheme to improve the QoE for LTE users [192]. They utilized historical data to learn how the QoE of users changed when the handover decision was made. In particular, eNB gathers measurement parameters from the users, including the user’s radio link condition of the current eNB, the user’s neighboring eNBs, and the user QoE resulting from past experiences once handover was made. This information is fed to the two-level NN model, where the first level determines the QoE in terms of complete download or incomplete download, and the second level is trained to approximate the file download time. Based on the handover algorithm, the current serving eNB triggers the handover to the next eNB with uninterrupted service to the user. Their algorithm assigns data for users to download and measures the amount of information lost due to handover. The simulation results show that almost 96% of the data were downloaded even when handover occurred. BSs can use the proposed method to create a centralized KP and perform handover management with a low data loss ratio.

Intelligent vehicular networks (IVNs) have attracted many re- searchers because of their real-time road safety services and other essential applications for vehicles. However, the develop- ment of efficient and robust wireless communication in vehicular systems is still challenging for content delivery. This is mainly because of the high mobility rate of vehicles that disrupt con- nectivity. The authors of [195] proposed a two-tier ML-based scheme for intelligent handover management in an IVN. In the first tier, an RNN model is used to predict the receiving signal strength of APs to make a handover trigger decision. A stochastic Markov model is utilized in the second tier to select the next

AP by considering the vehicle flow projection. The handover trigger was divided into three parts: data processing, learning phase, and prediction phase. They used an offline ML technique to model a long short-term memory (LSTM) network (which is an RNN used for time-series sequence prediction [206]) to predict the RSSI of the AP. Based on the acquired RSSI values, the system decides whether to trigger a handover. This decision shows if the handover must occur before its actual execution, which initiates an early handover registration process to avoid any disconnection of the signal while switching between APs. The proposed scheme outperformed the related models in terms of prediction accuracy. The proposed method is suitable for future self-driving vehicles to avoid collisions and can receive information at any instance of time. As a result, a centralized KP is needed to collect all the information and prepare to take handover actions when necessary.

In [199], a two-layer framework for handover control in large- scale wireless networks was introduced. In the proposed frame- work, a centralized controller clusters UEs based on the mobility patterns and utilizes an asynchronous multiuser DRL scheme to control the handover. DRL uses the actor–critic learning method to achieve an optimal policy for handover in each cluster. The algorithm uses online and offline methods, wherein the online method UEs keep processing and fetching the weight parame- ters periodically, while in the offline method, UEs behave as static controllers. Consequently, there are two different con- trollers in the network: one managing the mobile devices and the other managing the controller (main centralized controller). Their work is promising for future wire-less networks as it acts efficiently with the dynamic structure of the network and can train fast even with newly arriving UEs. Therefore, adapting such a system at the KP is essential to address scalability issues and control handovers more robustly.

every iteration based on the algorithm observation. The numer- ical results demonstrate the superiority of the Q-learning-based handover policy prediction compared to the existing heuristic handover decision-making in terms of achieving higher through- put. Consequently, the proposed algorithm for solving handover problems in mmWave systems can be used in a centralized KDN architecture.

Knowledge derived from supervised learning : In [209], the authors presented a feature-scaling-based KNN (FS-KNN) to improve the localization accuracy. The algorithm depends on the measured RSS reported by the MS, which accounts for the actual relation- ship between the signal differences and geometrical distances. To obtain the parameters of the weight function, iterative train- ing was established to tune the parameters. After training the model, the algorithm finds the optimal values corresponding to the actual distance between a newly received RSS vector and each fingerprint. Then, the user location with an average error as low as 1.70 m is determined by solving a weight mean of locations based on K nearest reference points. This algorithm has two phases, which train the system in the offline phase and use it for online location estimation. The offline trained algorithm can

A real-time, precise, and reliable localization system can de- termine the acceptable position of any portable device with opportunities for tracking objects/people, navigation systems, monitoring devices, and other location-based services. Interest in ambient intelligence, which enables people or systems to be aware of the user’s presence, is increasing [221–223]. The main issue in ambient intelligence is determining the position of the user with high accuracy. As a result, there are ongoing investi- gations based on indoor localization to propose new algorithms to improve the accuracy of indoor positioning systems. The

authors of [210] proposed an online independent support vector machine (OISVM) for indoor localization that avoids training from scratch. Their model uses the RSS of WiFi signals to make online predictions and facilitate mobile devices. The proposed model includes two phases: offline and online. The algorithms learn through pre-collected RSS with reference point (RP) labels appended to the corresponding RSS during the offline phase. The offline phase also incorporates kernel parameter selection and data sampling to deal with the imbalanced dispersion of the data samples. In the online phase, new RSS samples are collected by a centralized local AP for online learning and to estimate the location. Compared to traditional SVM methods, their method can balance the accuracy of localization and model size. From the simulation results, the location estimation error decreased by 0.8 m, while the training phase time and period were re- duced considerably compared to the traditional techniques. The proposed technique can be used via a distributed architecture of KDN.

In addition to SVM and RVM, some researchers have used the KNN to achieve acceptable indoor localization. For instance, Xu et al. [213] proposed an optimal KNN positioning algorithm based on theoretical accuracy criteria (TAC) in WLAN indoor environments. In this method, the optimal number of nearest RPs that can locate the user is theoretically analyzed. The KNN-based localization algorithm demonstrated that even with k=1 and

mapping function. In the denoising section, all the nodes’ RSS errors are improved by using a multi-layer denoising archi- tecture. Finally, from the location section, the corresponding square grid labels were detected to estimate the location. They have a two-stage training procedure, where the first stage uses an UL algorithm for pre-training each layer and a fine-tuning stage to minimize the error of the entire network. The proposed algorithm shows higher location accuracy than the maximum likelihood estimation (MLE), generalized regression neural net- work (GRNN), and fingerprinting methods. The proposed works in [217,218] suggest a distributed KDN architecture.

In this section, the terms and conditions associated with KDN prob- lems are generalized for researchers to consider before using knowledge in the network. These conditions are categorized to identify the type of problem KDN is required to solve: implementation complexity, time consumption, training data, and the differences between ML techniques in the same KDN problem. After checking each condition and meeting the requirements, a final decision can be made on whether to adapt the KDN algorithm and which ML algorithm is more cost-efficient.

The first and most crucial step is to discover the type of ML algo- rithm that is most suitable for any particular wireless communication problem. The majority of wireless communication problems are solved within a few different ML algorithms, categorized as regression prob- lems, classification problems, clustering problems, and MDP problems. In regression problems, the ML algorithm is required to predict a continuous value output given an input. In classification problems, the ML algorithm needs to predict a discrete value output, usually answered by a yes or no, and zero or one to essentially identify the class to which the input belongs. The clustering problems are ML techniques, where the data are grouped based on their type or value. Finally,

MDP problems are ML techniques that require taking action in the current system state based on the feedback reward resulting from the previous action. Moreover, all the above ML problems may involve feature extraction, which means that one or more inputs affect the output value. Feature extraction for any problem is performed manu- ally or algorithmically. Consequently, for any wireless communication problem in the above categories, KDN can use an ML technique as a possible solution and derive knowledge from them. For instance, to solve caching problems in wireless networks, one solution is to acquire the content request probabilities from users in the network. This prob- lem can adopt historical data and associate the model with regression problems. It takes the user profile as an input and then predicts the user’s content request probabilities as the output. On the other hand, the same problem can be solved by MDP, where, in every state, the agent can predict the associated cache content request. Furthermore, in user association problems, clustering algorithms are used, and for interactive environmental problems, such as mobility management and resource management, the MDP model is more suitable. Therefore, it is important to consider the advantages and disadvantages of each technique before applying them.

In this subsection, we examine the implementation complexity of ML algorithms in KDN problems. Generally, four factors need to be considered while computing the complexity of an ML algorithm: (1) mathematical operations, (2) dataset, (3) data storage, and (4) software and hardware requirements. Mathematical operations vary from one problem statement to another and are usually based on the features of the ML algorithm that affect the complexity of the mathematical operation. Dataset complexity lies within the collection and use of appropriate data. Hence, collecting the necessary information from different wireless communication tasks has its own difficulties. For instance, in SL problems, data should be presented before finding a solution. In contrast, the data will be collected in RL as the agent interacts with the environment. Next, the complexity that occurs when the collected data or processed data are too large to be stored as knowl- edge in KP. Accordingly, software and hardware complexity arises from simulating the right software and using the correct number of processors or GPUs to process the knowledge, respectively.

One of the critical aspects of KDN is the response time. The ML algorithm tends to spend some time on training to provide near- optimal answers. Hence, it is essential to investigate the two-timing mechanisms that most ML techniques have, including training time and response time. The training time is the amount of time required for each ML algorithm to be fully trained. The response time is when an ML algorithm needs to make a prediction after being trained. Both timings are important for different applications in KDN networks [224].

The training time is most important for supervised and unsuper- vised learning, but it also has significance in RL for making accurate decisions. There are two different ways of training: online and offline. Depending on the application requirements, training may occur in one of the above processes. For example, highly dependent applications will perform online training, such as handover optimization, tuning, and healing in SONs. Applications incorporating NNs require time for adaptation to make accurate predictions, including mobility prediction and clustering problems [225]. Specifically, for applications with NN systems, the training time is usually longer [226]. Hence, the environ- ment may change by the time the training algorithm learns a policy or mapping rule. In addition, training an RL model in complex wireless communication environments can be time-consuming and ineffective because the set of agents and environmental dynamics are different after the learning algorithm is completely trained. Therefore, such

The response time of the trained learning algorithm is more im- portant than the training time. Most applications in wireless networks require a quick response, on a timescale of milliseconds, such as decision making in resource management. Let us consider two differ- ent approaches of ML applied to these wireless network applications, namely NN-based approaches and alternative approaches. The time cost can be discussed as follows:

30 users. Therefore, making resource management decisions within an acceptable time frame is feasible for power allocation problems using a trained DNN [89,229,230]. However, as the network size increased, both the response time and training time exponentially increased. One solution proposed by [231] is to use GPU-based parallel computing to enable NNs to predict within a tolerable range (milliseconds). Furthermore, there is a deep Q network in the DRL, and the deep Q network depends on the output of Q-values from the NN. Hence, the response time mostly depends on the NN process time. However, one promising solution that KDN naturally provides is using the pre-trained data as knowledge, so the response time can be near-optimal and suitable for future wireless network applications.

Alternative approaches: In resource allocation problems, in both RL-based Q-learning and joint utility and strategy estimation- based approaches, the aim is to find a policy or strategy that suites the dynamic nature of the environment. In RL-based meth- ods, after the learning algorithm converges, the policy or strat- egy from the trained algorithm becomes fixed. In Q-learning, the strategy is represented by a set of Q-values, where each set cor- responds to a system state and an associated action. Therefore, a well-trained RL algorithm can respond in milliseconds. Addition- ally, the joint utility and strategy estimation-based learning goal is to choose a probability value that indicates an action. Here, the agent only needs to generate a random number between 0–1 to select the appropriate action. Consequently, a well-trained al- gorithm can accelerate the decision-making process and achieve a millisecond response.

Different training data are collected or generated for supervised, unsupervised, and reinforcement learning, depending on the character- istics of the problem. For instance, in spectrum allocation problems, the authors of [82,83] used non-cooperative game methods where the data were trained based on the collected information from the primary and secondary users. In [86] the same non-cooperative spectrum allocation is modeled with the difference where the data is collected using an RNN model at each BS; here, BSs continuously interact with each other to collect training data. Other authors of [84] used joint utility and strategy estimation-based learning to collect D2D information and gen- erate training data. In power allocation, when the SL method is used, such as in [89,229,230], the authors aim to adapt a neural network to approximate power allocation in a complex environment, including the genetic and WMMSE algorithms. Using these algorithms enables these studies to generate training data under different network scenarios that can later be used in KPs as knowledge. Obtaining appropriate raw data is important for researchers on the same topics. For example, in cache problems, the authors used different datasets for cache management to

reduce the traffic caused by video streaming. In [110], the dataset used for training was acquired using the YouTube Application Programming Interface with 12500 YouTube videos. The dataset used to train the RNN in [111,112] was obtained from Youku [232] (a video hosting ser- vice based in Beijing, China) for content (video) request prediction and traffic management. Moreover, in [113], the dataset is a combination of two datasets, namely YUPENN [233] and UFC101 [233].

In networking scenarios, specifically in routing problems, the col- lected dataset is usually obtained from a simulator. For instance, in [137], the authors implemented a neural network using the Om- net++ simulator for traffic engineering. In this study, the SDN con- troller collects traffic flow reports from the nodes in the networks and separates different features to feed the SL algorithm. In [142], the authors investigated DNN-based routing to solve dynamic routing problems. The training dataset collects traffic patterns and paths that a packet will undertake to reach the destination in the proposed study. The routing paths were obtained using the traditional OSPF strategy from a software simulator. In other studies, such as in [141], the dataset was generated in real-time through online fashion with routers in the network for intelligent route selection.

In ML-based localization techniques, three main strategies are used to collect the required dataset and obtain the position of the object in an indoor environment: RSS, CSI, and ToA. Research studies in [209,210, 215,217,218] used RSS information as a dataset, and studies in [219, 220] used CSI data. The generated dataset was obtained using a cell phone or UWB device from real practical measurements. For example, the authors of [219] utilized the IWL 5300 NIC to mobile devices to read CSI data, while in [209], a client program was used in mobile devices to measure the RSS levels.

In RL, the learning algorithm continuously updates itself through interaction with the environment. In these problems, the learning agent takes action in a particular state. Then, it receives a reward from the en- vironment, where the environment is created as a virtual environment by specific software, such as NS3, Matlab. For example, in spectrum management, researchers in [84] optimized the spectrum usage from reward feedback for each D2D pair. In power management, the authors of [105] used the reward as the difference between the maximum total power consumption and the current total power consumption to perform BS switching on/off to minimize the overall network power consumption.

As previously discussed, ML problems are generally categorized into regression, classification, clustering, and decision-making problems. In comparison, the differences between each lie within the solutions they provide for KDN problems. However, for each problem, there could be different ML solutions. Therefore, it is important to realize and compare ML algorithms, specifically those that can solve the same problem. This subsection provides a guideline for readers to first understand why recent studies have used ML algorithms (in the context of wireless networks) and provide guidance for selecting a suitable ML algorithm.

Techniques for deploy regression or classification: For these two techniques, SVM, KNN, and NN are mostly used [111,112,140, 193,194]. SVM is the most robust prediction method for binary classification with low complexity [113]. At the same time, KNN is a multiclass classifier, mostly known for its simplicity of implementation. In problems where the dataset is not linearly separable, KNN is a better classifier than SVM. In the KNN approach, only the distance metric and K parameter must be selected, whereas in SVM, the regularization and kernel param- eters must be chosen carefully. Although these two algorithms are simple compared to neural networks, NN is more robust in feature extraction and improves overall network performance. For instance, a DNN can handle large datasets and achieve

high accuracy, although the learning procedure can be time- consuming owing to the optimization of various parameters. However, with sufficient training datasets and powerful GPUs, DNNs are more recommended than other learning machines. Furthermore, other neural networks, such as CNN and RNN, can reduce the training time and system overhead. Both algorithms have their own advantages for solving different problems. For instance, CNN is suitable for learning spatial features, including the channel gain matrix, while RNN is good at processing time series problems for feature extraction.

Techniques for deploy clustering: ML models applied to clustering mainly include K-mean clustering. However, there are other supervised and NN studies deployed to clustering problems. K- mean clustering is one of the most popular and simplest methods for clustering data. Generally, the K-mean is used to differentiate between groups with similar data points and patterns. Neural networks are also used to organize the input data. For instance, given a set of images, the NN can organize and provide images with similar content. This process does not provide clusters, but it creates meaningful representations of the dataset, which can be used for clustering. The main difference between these two algorithms is the complexity of implementation. Moreover, in K-mean clustering, the number of cluster centroids (K value) is essential; however, in NN, the design of the hidden layers and other factors must be considered.

After summarizing the terms and conditions associated with ML algorithms, we need to investigate the motivations for applying the appropriate ML algorithm to KDN-based networks. It is essential to ex- amine the reasons and motivations for adapting KDN-based approaches to wireless networks. The subsections below provide the reasons for applying ML and knowledge to the network based on the literature surveyed throughout this study.

Self-organizing networks (SONs) provide self-optimization, coordi- nation, self-organization, and correction for the next generation of wireless networks [240]. In particular, most researchers now consider ML techniques as an official approach to achieve self-organization in the network, proving that KDN will be part of future wireless networks. 3GPP has already started developing protocols and technologies to automate network configurations [241]. In this context, RL is the most recognizable approach for correcting itself based on the envi- ronment and experience. In particular, in load balancing, handover management, routing, etc. In summary, SON can be applied using the following studies [81,85,86,95,99,106,107,114,133–135,184,192,196,

Traditional optimization algorithms can only work for deterministic networks, which have certain characteristics. These algorithms are not practical in current real-life network scenarios, as the network traffic changes every day. In contrast, ML is the capability of machines to learn how to respond to any specific situation. Hence, they are more reliable than traditional algorithms owing to their flexibility and adaptation to new environments. In [80], DRL was utilized for spectrum allocation and was able to provide twice the channel throughput when compared to slotted-aloha with optimal probability. In [89], an SL algorithm was developed to train a DNN for power management, and it was shown to be superior to a state-of-the-art interference management algorithm. In cache management, the authors of [115] used the RL algorithm for content caching at the BS and compared it with two traditional cache update schemes, namely LRU and LFU, where it shows a better long-term cache hit rate. Moreover, in [173], the RL-based algorithm for user association performs much better than traditional dual-decomposition-based approaches. Overall, ML techniques have the potential to provide superior performance to traditional optimization algorithms. Other surveyed works for this motivation are as follows [92, 109,115,121,169,173,219,230].

By utilizing neural networks, hidden patterns in a system can be learned and used to estimate future values or predict the future, which is an advantage in KDN-based networks. In this context, the authors used this NN functionality to improve system performance. In [92], a multi-agent DRL technique was used to observe the spatial features based on the collected CSI and QoS to make a wiser power allocation decision when the network experiences dynamic changes. Moreover, in traditional indoor localization, the system’s performance can be easily affected, resulting in inaccurate positioning owing to the complexity of the environment. As a result, many researchers are now motivated to use NNs to increase the positioning accuracy by learning and updating user patterns [214,219]. Consequently, the motivation of surveyed works utilizing NN can be observed in [12,86,89,92,97,111–113,116, 117,141,164,168,193–195].

One of the important reasons and motivations for using ML al- gorithms is their ability to handle complex problems and datasets. The authors of [86] trained a multi-agent RL-based with ESN for efficient spectrum allocation and load balancing in LTE-U networks. Other researchers in [168] used CNN to achieve a low-complexity and high-accuracy clustering algorithm to classify three different types of waveforms in wireless communication systems. Moreover, the ability to handle high-complexity problems is the main reason why authors use RL. For example, for the on/off sleep mode control of small cells, the authors of [103] used distributed Q-learning to decide on the sleep mode of each BS, which led to a low-complexity sleep-mode control algorithm. Overall, the motivation for providing a low-complexity so- lution for the KDN paradigm can be seen in the literature [12,79,118, 152,167,194,211,216,219].

Some traditional and heuristic approaches for optimization and estimation based on a fixed set of rules are often unable to avoid faulty and unsatisfactory results that have occurred previously. This means that these approaches are unable to learn from their mistakes and correct their decisions. Such problems can be seen in OSPF-based routing strategies, as stated in [141], where OSPF routing will result in some congestion in certain situations at some routers even though it may know the congested router. Hence, in these situations, the OSPF repeats the same action (wrong decision), leading to congestion again. In handover strategies based on RSSI values, a similar problem can accrue, as shown in [244]. Moreover, other similar problems can be observed in the literature for user association based on max-SINR [173] and in the BS on/off sleep mode control strategy [106]. To prevent wrong decision-making in traditional algorithms, RL and deep learning are adapted to learn through historical and new data to prevent any previously incorrect decisions. For instance, deep learning in routing strategies enables the algorithm to avoid mistakes, such as congestion in the network under different traffic patterns. RL can also be utilized in other approaches to overcome the same type of problem [245,246]. In summary, decision making with poor performance outcomes can be avoided using ML strategies in the KDN framework, which can be further observed in the surveyed literature of [63,89,95,98,114,115, 118,133,134,141,150,247–251]

There have been several studies on the applications of ML and its potential advantages for improving the overall network performance in wireless networks. However, few studies have focused on facilitating the ML-generated output data as knowledge, which can be used in similar applications and scenarios to create intelligence. Therefore, there remain many challenges and open issues that require attention from various sources across academia and industry to develop the concept of KDN standard criteria in future wireless networks. In this section, we identify the challenges and discuss future opportunities.

Several studies have introduced the KDN architecture, for instance, the authors of [28] restated the concept of KP in the context of SDN architecture in addition to the three planes of the SDN paradigm. Fig. 1 in this paper shows that the KP is located on top of the control and management planes. The integration of KP generates a behavioral model and reasoning process for decision-making. This architecture enables the KP to fully view and control the network via the control and management planes. Other research studies in [12–14,29] have a similar architecture of KDN. In [14], the same KP is utilized on top of all the layers, but it uses a cross-layer management and monitoring plane with ML algorithms to manage the rest of the planes. This paper [14] utilizes ML-based algorithms in both separate orchestration layers and embedded in the management plane. Therefore, there are different orchestrations of the KDN concept, and a thorough investigation is required to identify the most suitable one in terms of flexibility and performance.

Although ML studies have been a point of discussion over the last few years [253], current research in the wireless communication area is still unripe. This is the main reason why ML has not been practically applied to existing wireless networks. To evolve ML-based algorithms to meet the requirements of future 6G systems, it is essen- tial to standardize AI-embedded communication. The performance of

the current ML-based algorithms is evaluated based on the level of improvement in communication performance. However, to adapt KDN and intelligence across 6G networks, it is far from sufficient to only consider the degree of improvement without considering computation and storage costs. Therefore, in addition to evaluating the performance of ML technologies, the required storage and computation overhead must be considered while designing the standards. Moreover, to realize full intelligence in KDN networks, the compatibility of developed ML algorithms with other network functions is another emerging topic. Further, the intelligence and ML algorithms must be adaptive to any changes in the topology to enable automatic adjustment in 6G networks [254].

One of the crucial aspects in 6G wireless networks is intelligence, and many research studies are now focusing on exploring how knowl- edge and intelligence can be integrated into wireless networks. This survey paper investigated the concept of knowledge-defined network- ing, which aims to combine SDN and ML/AI to create a programmable and knowledge-aware networking architecture. We first introduced emerging technologies to facilitate KDN, specifically the SDN paradigm, network telemetry, and ML algorithms. We then investigated most of the widespread applications of wireless networks. The reviewed studies in network applications were based on the most recent ML-based approached to create automated applications in KDN-based wireless networks. The applications were categorized into the MAC layer, Net- work layer, and Application layer. Resource management problems were distributed within the MAC layer and classified as spectrum allocation, power management, QoS, BS switching, cache, and back- haul management. Networking and mobility management problems were investigated in the network layer. Networking problems were described as routing strategies, clustering, user/BS association, traffic classification, and data aggregation. Mobility prediction and handover management were considered in mobility management. Then, from

the Application layer perspective, various indoor localization tech- niques are presented. Moreover, appropriate ML-based studies were thoroughly explored for each surveyed application, and the most suit- able KDN architecture was suggested. We achieved a comprehensive review of different parts of wireless networks and provided insights into how different algorithms perform, enabling future researchers to adapt the most appropriate ML-based study with the suitable architecture of KDN. Further, the conditions associated with ML-based strategies in the context of KDN were provided, followed by the motivation to apply KDN. Finally, we outlined several unsolved problems and challenges within the KDN paradigm.

Lee D, Choi J, Kim J-H, Noh SH, Min SL, Cho Y, et al. On the existence of a spectrum of policies that subsumes the least recently used (LRU) and least frequently used (LFU) policies. In: Proceedings of the 1999 ACM SIGMETRICS international conference on measurement and modeling of computer systems. 1999, p. 134–43.

Al-Rawi HA, Yau K-LA, Mohamad H, Ramli N, Hashim W. Effects of network characteristics on learning mechanism for routing in cognitive radio ad hoc networks. In: 2014 9th International symposium on communication systems, networks & digital sign. IEEE; 2014, p. 748–53.

Sepehr Ashtari (Graduate Student Member, IEEE) received the B.S. degree in electrical and electronic engineering from Eastern Mediterranean University (EMU), North Cyprus, Turkey, in 2016 and the M.S. degree in telecommunica- tion engineering from the University of New South Wales (UNSW), Sydney, Australia, in 2019. He is currently pursu- ing the Ph.D. degree in telecommunication and information technology at the University of Technology (UTS), Sydney, Australia.

From 2015 to 2016, he was a Research Assistant with the Department of Electrical and Electronic Engineering, North Cyprus, Turkey. He has worked as a research fellow at the Commonwealth Scientific and Industrial Research Organisation (CSIRO) on several wireless communication projects. His research interest includes improvement of wireless cellular communication, routing protocols, software or knowledge-based networking, 5G resource allocation, and machine learning optimization in wireless networks.

B.S. degree in computer science from the University of Sydney, Australia, in 2016. He also received the MBA degree from the University of Technology Sydney in 2019. His research interest includes AI, IoT, cyber–physical system, and blockchain. Currently, he is pursuing a Ph.D. degree at the University of Technology Sydney researching on machine learning-based frost monitoring systems.

Mehran Abolhasan (Senior Member, IEEE) is currently an Associate Professor and the Deputy Head of the School of Electrical and Data Engineering, University of Technology Sydney. He has over 20 years of experience in research and development and serving in research leadership roles. Some of these previous roles include serving as the Director of research programs for the Faculty of Engineering and IT, and the Laboratory Director for the Telecommunication and IT Research Institute, University of Wollongong. He has authored over 160 international publications and has won over four million dollars in research funding. He won a number of major research project grants, including the ARC Discovery Project, ARC Linkage Project, and a number of CRC and other government and industry-based grants. He currently leads the Software-Defined Networks Lab at UTS and his current research interests include software defined networking, the IoT, wireless mesh, wireless body area networks, cooperative networks, 5G networks and beyond, and sensor networks.

Negin Shariati (Member, IEEE) is a Senior Lecturer in the School of Electrical and Data Engineering, Faculty of Engineering and IT, University of Technology Sydney (UTS), Australia. She established the state of the art RF and Com- munication Technologies (RFCT) research laboratory at UTS in 2018, where she is currently the Co-Director and leads research and development in RF-Electronics, Sustainable Sensing, Low-power Internet of Things, and Energy Harvest- ing. She leads the Sensing Innovations Constellation at Food Agility CRC (Corporative Research Centre), enabling new innovations in agriculture technologies by focusing on three key interrelated streams; Energy, Sensing and Connectivity. Since 2018, she has held a joint appointment as a Senior Lecturer at Hokkaido University, externally engaging with research and teaching activities in Japan. She attracted over

$850K worth of research funding across a number of CRC and industry projects, where she has taken the lead CI role and also contributed as a member of the CI team. Negin Shariati completed her Ph.D. in Electrical-Electronic and Communication Technologies at Royal Melbourne Institute of Technology (RMIT), Australia, in 2016. She worked in industry as an Electrical-Electronic Engineer from 2009– 2012. Her research interests are in Microwave Circuits and

Justin Lipman (Senior Member, IEEE) received a Ph.D. in Telecommunications Engineering from University of Wol- longong, Australia in 2004. He is an Industry Associate Professor at the University of Technology Sydney (UTS) and a visiting Associate Professor at Hokkaido University’s Graduate School of Engineering. He is the Director of Research Translation in the Faculty of Engineering and IT and is Director of the RF Communications Technologies (RFCT) Lab — where he leads industry engagement in RF technologies, Internet of Things, Tactile Internet and Software Defined Communication. He serves as committee member in Standards Australia contributing to International IoT standards and Digital Twins. Prior to joining UTS, he was based in Shanghai, China and held a number of senior management and technical leadership roles at Intel and Alcatel driving research and innovation, product develop- ment, architecture and IP generation. He is an IEEE Senior Member. His research interests are in all ‘‘things’’ adaptive, connected, distributed and ubiquitous.

Wei Ni (Senior Member, IEEE) received the B.E. and Ph.D. degrees in Electronic Engineering from Fudan University, Shanghai, China, in 2000 and 2005, respectively. Currently, he is a Group Leader and Principal Research Scientist at CSIRO, Sydney, Australia, and an Adjunct Professor at the University of Technology Sydney and Honorary Professor at Macquarie University. He was a Postdoctoral Research Fellow at Shanghai Jiaotong University from 2005–2008; Deputy Project Manager at the Bell Labs, Alcatel/Alcatel- Lucent from 2005 to 2008; and Senior Researcher at Devices R&D, Nokia from 2008 to 2009. His research interests include machine learning, stochastic optimization, online learning, as well as their applications to system efficiency and integrity.

