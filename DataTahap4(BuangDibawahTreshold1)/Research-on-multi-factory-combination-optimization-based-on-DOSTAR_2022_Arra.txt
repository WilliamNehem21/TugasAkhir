learning (RL) algorithm. Ternary data fusion requires massive amounts of data as a basis. When solving some specific problems, part of the data is missing or the problem to be solved is too complicated. Therefore, in the calculation, these specific data are abstracted as weights, which can greatly optimize the calculation process and ensure the accuracy of the

results. As one of the classic methods in the field of systems engineering, AHP is still widely used in recent years. It is because AHP will produce a series of relatively accurate decision weights in complex supply chain [2]. In addition, when calculating spatial and temporal data, this article imitates the human mindset and converts the specific values of spatio- temporal data into relative weight values. Therefore, spatiotemporal data can be easily incorporated into the final decision-making basis. These weighted data avoid the troubles caused by overly complex spe- cific data for decision-making. It should be explained that the temporal data represents the time point at which the process business occurs, but it is mainly used for the sequence of events in this study. Spatial data represents the business department, that is, the responsible subject of the event, to reflect the flow of business processes between different departments. The line chart with spatiotemporal data is used to visualize multiple event processes and facilitate the comparison before and after optimization.

Knowledge is the foundation of combinatorial optimization. The process of knowledge discovery is consistent with the combinatorial optimization model. There are many methods of knowledge discovery at present, among which the main keywords are data mining. In literature [3], a framework with multiple modules is proposed for knowledge discovery in the processing process. This article also uses a multi-module framework to study knowledge discovery. The premise of accurate reasoning is that the data meets certain specifications, which requires the form of knowledge expression to be more rigorous, such as extending from triples to six-tuples. Reinforcement learning is an intelligent al- gorithm that is more suitable for this scenario. This paper proposes an improved reinforcement learning algorithm to calculate and derive the associated knowledge in the complex manufacturing environment. It provides an associative knowledge discovery method that integrates multi-source spatiotemporal data and multiple sub-models.

In summary, based on the fusion of human-machine-physical ternary data, this paper proposes the DOSTAR method for combinatorial opti- mization. The purpose of this method is to model practical problems in complex manufacturing networks. The following content will focus on these points.

The purpose of this study is to improve the efficiency of quality traceability of water heaters. Business scenarios have the characteristics of multiple data sources, multiple decision makers, multiple spatial and temporal constraints, and strong correlation between data. Therefore, data fusion, decision weights based on AHP, spatiotemporal data, ontology and domain knowledge need to be studied separately. There- fore, relevant research also focuses on these topics.

There are many studies on combinatorial optimization. Generally, the multi-objective optimization problem is transformed into sub- objectives of multi-level or multi-stage, and solved relatively sepa- rately. Then the subsets are combined in a certain way to achieve the goal of optimization. In order to better support the proposed combina- tion model, this paper will analyze the literatures of other researchers from the aspects of group decision-making, knowledge discovery, rein- forcement learning, graph computing, production scheduling, and service-oriented manufacturing.

optimization model for different decision-making preferences for pro- duction and distribution plans. Literature [7] first discovers knowledge from the solution, and then integrates the knowledge into the strategy. In addition, the combination of combinatorial optimization and graph computing is also a hot spot. Literature [8] explores the combinatorial optimization of graph, which uses a machine learning (reinforcement learning) model. In the literature [9], a multi-center variable-scale search algorithm is proposed to solve single-objective and multi-objective combinatorial optimization problems. Literature [10] has developed a set of software systems for exploring multiple combi- natorial optimization problems in complex networks. In Refs. [11,12], the process of solving multi-objective production planning problems is hierarchical. Literature [13] integrates combination optimization and collaborative filtering in a complex service network to improve service efficiency.

The above-mentioned studies are all outstanding and close to the scope of this article. The key factors of the studies are: multiple evalu- ation criteria, multiple layers of constraints, different parameters. However, most of them either did not introduce the relationship be- tween the key factors into the model, or the model was not compre- hensive enough.

Human-in-the-loop simulates Human factors and integrates the data obtained with cyberspace data and Internet of Things data. Although there is no clear fusion of human-cyber-physical data, it emphasizes to quantify the human factor and integrate it into of cyber-physical sys- tems. In order to avoid human error and simplify management, self- managed CPS was proposed in the literature [15]. The human factors are further simulated in the mixed environment of machine and mate- rial. The literature [16] proposes an architecture for seamless integra- tion of factory workers in an industrial network physical production environment, using semantic fusion data, and real-time analysis of data for anomaly detection. The literature [17] put forward that in a manufacturing environment, human beings can supervise and adjust Settings to become the source of knowledge and ability, diagnose situ- ations, make decisions and other activities that affect manufacturing performance, providing additional degrees of freedom for the CPS sys- tem as a whole.

compound methods. In literature [18], an advanced supply chain risk assessment model based on order of magnitude AHP (OM-AHP) was developed to compare the tangible and intangible factors that affect supply chain risk. An illustrative example is given to demonstrate the effectiveness of this assessment model. The evaluation method of machining process scheme based on AHP-GREY correlation analysis is proposed in the literature [19]. Analytic hierarchy process (AHP) is used to analyze the factors that affect the quality of the machining process plan, and the correlation degree is calculated by correlation coefficient and combination weight. Finally, the quality of the process plan is determined according to the correlation degree of the plan.

As mentioned earlier, the spatiotemporal data governance studied in this article is mainly to convert specific spatiotemporal data into weight values. At present, the mainstream time alignment methods mainly include interpolation extrapolation, least square method, Taylor expansion method, etc. In the aspect of space governance, the origin of coordinates is not unified, and the common methods include Kalman filtering [20] and least square method [21]. In addition, there are sys- tematic errors for different descriptions of the same object. Since the benchmark of each description object is different, the results may also have errors. The commonly used methods include least squares, maximum likelihood and so on. In addition, spatiotemporal data governance is inseparable from data mining. A spatiotemporal data mining method based on ontology semantics is proposed in the literature [22]. Through the spatial data analysis method based on event-event and event-place, the information is mined from two aspects of space and time.

mining methods have been used to extract knowledge from solutions generated during multi-objective optimization. These methods are (i) sequential pattern mining, (ii) clustering-based classification trees, (iii) hybrid learning, and (iv) flexible pattern mining. Each method uses a unique learning strategy to generate explicit knowledge in the form of patterns, decision rules and unsupervised rules. In literature [24], data mining and knowledge discovery are carried out together in order to solve complex problems in intelligent production.

forms are triples or variants of triples. In literature [25], it is proposed a multi-agent algorithm able to automatically discover relevant regular- ities (knowledge) in a given dataset. Each agent operates independently by performing a Markovian random walk on a weighted graph repre- sentation. In literature [26], it is proposed a principled knowledge-based model in the form of a computational ontology. The literature [27] proposes a knowledge discovery method based on knowledge graph, which integrates heterogeneous data by introducing knowledge graph.

There are many intelligent algorithms that can be used for knowl- edge discovery. This research believes that reinforcement learning is more suitable for the discovery of related knowledge. Reinforcement learning is one of the paradigms and methodologies of machine learning, which is used to describe and solve problems in which an agent interacts with the environment to maximize returns or achieve specific goals through learning strategies.

In recent years, reinforcement learning has been used to find the path in the knowledge map [28], as well as entity search and relationship search to construct ontology species [29]. Methods based on reinforce- ment learning and semantic fusion selection are proposed in the litera- ture [30] to give Suggestions for decision making. Reinforcement learning is used in the literature [31] to predict the flow of urban spatial and temporal data. The literature [32] studies the related problems of time series data in the IoTs and uses reinforcement learning to solve the problem of mutual information minimization of historical dependence. All these indicate that reinforcement learning has been gradually used in ontology correlation calculation, but due to the lack of in-depth research, the current reinforcement learning has not made significant progress in associative knowledge discovery.

The quality traceability of water heaters is complex, and work effi- ciency needs to be significantly improved. First, it involves multiple responsible parties such as users, after-sales outlets, retailers, and manufacturers; it is difficult for multiple responsible parties to coordi- nate efficiently. Secondly, the data type, data format and value of each responsible party are different; data fusion is more difficult. Third, there are also stakeholders with different goals within the manufacturer who is the most responsible party; the decision-making weight of stake- holders will seriously affect the outcome of the decision. Fourth, the key factors affecting work efficiency should be assigned to multiple sub- models for research, and the correlative knowledge among them should be found to effectively improve overall work performance.

Overall, the research span of this article is very large. The first step is to collect the human-cyber-physical ternary data in the complex manufacturing environment according to the characteristics of the complex network of multiple factories, multiple sales companies, and multiple after-sales service outlets. The second step is based on the integration of ternary data to establish the domain ontology of the complex manufacturing environment. The third step is parallel to the second step. The weight sub-model converts the collected decision basis and results into decision weights, and transforms spatiotemporal data

into time and space weight values; and these weight values are stored in the form of the adjacency matrix. The fourth step is to form the six-tuple, which is to integrate the results of the second and third steps to form a six-tuple data set. The fifth step is the improved reinforcement learning algorithm, which converts the six-tuple into a weighted graph; then the weighted graph is chess boarded; therefore, the reinforcement learning algorithm can run smoothly. The sixth step is the result calculated from the fifth step. This result is a subgraph of the weighted graph of the previous six-tuple. At the same time, it is also a streamlined solution to a specific problem; it shows that associative knowledge is discovered. The above is shown in Fig. 1.

The domain ontology model in this article is mainly established based on factors such as domain knowledge, expert experience, and data relationships. There are many places to study in complex manufacturing environment. This article is mainly based on the analysis of the actual situation of the complex manufacturing environment of Haier water heaters.

manufacturing environment includes: systematic planning, quality assurance and coordination meetings. Systematic planning is divided into single factory scheduling, multi factory scheduling, multi-vendor planning, and multi-D&S (delivery and sales) planning. The sched- uling of a single factory mainly refers to the annual plan. In addition,

between the delivery point and the shipping factory, which is strongly related to the cost and efficiency of transportation, and it is often considered to hand the order to the factory near the delivery point for production. In terms of quality assurance, after the after-sales service outlets receive feedback from users, they need to conduct quality tracing and determine the most suitable maintenance plan. Therefore, we also need to consider regional issues here, that is, considering spatiotemporal data and its weight data. Coordination meeting is an important mani- festation of human data in the entire model. The participants in the coordination meeting are senior experts from important factories, sales companies and after-sales service departments. They will discuss various uncertain factors in order to make correct decisions on specific issues.

which are based on text mining. This article believes that decision- making structure, work flow, management specifications and encyclo- pedia can all be used as the basis for ontology construction. Domain ontology is an important step of this research, but the method of con- structing ontology is not the focus. Therefore, it will not go into too much detail here.

Weight refers to the degree of importance of a certain factor or in- dicator relative to a certain thing. Different levels of importance should be represented by different values. The weight value is a relative value, which mainly indicates the order of the importance of different factors or indicators.

The six tuples are combined as parent node, relationship, child node, space weight, time weight and AHP weight. The parent node, child node and relationship are derived from the triples of the ontology (entity 1, relationship, entity 2). Three weights values are from weight sub-model. By adjusting the weights of similarly related data for different targets and different dimensions, the fusion calculation can be smoother. Based on the above analysis, the six-tuple model is expressed as follows:

maximize the return of the objective function in a live maze of chess- boards. In this paper, the reinforcement learning model is used for associative knowledge discovery. Taking the multi-factory management as an example, we create the corresponding domain ontology based on actual work. Since the composition of this domain ontology is relatively complicated, this article shows the core graph, as shown in Fig. 2.

Obviously, the domain ontology in Fig. 2 is difficult to calculate directly for reinforcement learning. So, we need to transform it. There are several steps to achieve that. Step 1, domain ontology is needed to transform onto “chessboard”. In this step, the domain ontology is put

then, the original segment is removed, such as segment AB in Fig. 3(b). The step 3, it is possible to perform a chessboard simulation of the reward matrix for reinforcement learning model. The values of reward matrix come from weight sub-model. Fig. 4 shows very clearly that all

counted return (reward), such as 80 or even higher. That is, the total value of the cumulative discounted return of the model must be greater than 80, otherwise it is not considered to find a suitable result. But the calculation steps increase rapidly when the number of this constraint is larger. After more than 5000 steps of training (due to the existence of random values, sometimes tens of thousands of steps, and more complex cases requiring even millions or more), the optimal cumulative dis- counted return (reward) can be found for one kind of tracing.

essentially experts representing the interests of location. The individual decision preferences of these three experts will be obtained through AHP first, and then integrated according to the method of cooperative games. After calculation, the subgraph representing the associated knowledge is shown in Fig. 5.

value of the cumulative discounted return of the model must be greater than 150, otherwise it is not considered to find a suitable result. But the calculation steps increase rapidly when the number of this constraint is larger. After more than 20,000 steps of training (due to the existence of random values, sometimes tens of thousands of steps, and more complex cases requiring even millions or more), the optimal cumulative dis- counted return (reward) can be found.

consistency of decision-making results, it is necessary to ensure that CR is less than 0.1. In addition, there are certain conflicts between multiple decision-making preferences. In order to further optimize this situation, different decision preferences need to be weighted to obtain a result that

abilities of interpretability and fusing cross-domain and cross-layer data are quite innovative and remarkable for real business use. If the same data is used, other algorithms cannot perform calculations directly, and some data preprocessing work is also required. In addition, single al- gorithms can only solve sub problems; they cannot solve all the prob- lems corresponding to the algorithms proposed in this article.

Based on the multi-factory case of Haier electric water heater, this paper constructs an DOSTAR fusion model for associative knowledge discovery. This fusion model is divided into four parts: Weight sub- model, domain ontology, six-tuple and improved reinforcement learning. This research integrates various types of big data from multiple dimensions, multiple perspectives, cross-regions, and across time hori- zons throughout the whole process. These data include structured, semi- structured and unstructured data. These data are connected through AHP and domain ontology. The inclusiveness of these connections is very good. In particular, this study fuses human data through the AHP sub-model. The improved reinforcement learning sub-model shows that this research uses artificial intelligence algorithms for associative knowledge discovery. Finally, through the case study, it is obviously that this method can effectively optimize the entire manufacturing network to achieve the purpose of reducing costs and increasing efficiency. It provides innovative ideas for solving related problems.

Due to the relatively short time, there is no time to debug the multi- agent reinforcement learning model. I believe this will greatly shorten the calculation time of the reinforcement learning model. In addition, follow-up research work will further expand the data. More detailed and large knowledge discovery is expected.

