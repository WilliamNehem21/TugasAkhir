The latent semantic index (LSI) has been widely used in many fields of natural language processing in which co- occurrence features can be captured by the transfer relations between the documents and in the documents. Document features with a higher frequency in the collection of the document are more likely to introduce some unreasonable feature transfer relations to the latent semantic space which affects the similarity between features and between documents in document sets in our recent study. In the paper a feature optimize technology in latent semantic indexing that uses feature transfer relation in documents and between documents is proposed. By the complete-link algorithm, the experimental results show that the method effectively improves the performance of latent semantic indexing.

With the development of information technology, a lot of document resources are needed that helps the discovery of the theme, information retrieval, and so on. Therefore, text clustering technology came into being. It is a very important part of natural language processing. Text clustering technique made great success in document clustering. There are a large number of synonyms, near-synonym and other unique natural language phenomena in document clustering. We will use LSI to explore and resolve these linguistic phenomena to improve the performance of document clustering in the paper.

The co-occurrence information of the terms can be captured when Singular Value Decomposition (SVD) is decomposed proposed by [1]. The example preferred in [2] is as shown by table 1 and table 2 is the feature document matrix. The term weight represents word frequency of the term and the matrix is decompose by singular value dropping to a two-dimensional space. By the comparison between the similarity matrix decomposed shown by Table 3 and the one undecomposed, the similarity weights of the table 3 made obvious changes. In table 2 the similarity between the terms is zero, but there does not exist a value of 0 which means co-occurrence information of some terms is improved and some one is weakened. The similarity of 0 is between some terms undecomposed that means there is no or little relations. The undecomposed similarity of the user (t4) and human (t1) is 0 but decomposed similarity is 1.0003. By the changes of the similarity value, we can think "user" and "interface", "interface" and "human", "user" and "human" co-occur. In LSI "user" and "human" projected to the same dimension space.

The degree of similarity between features reflects the correlation of between the terms. The weight value not only reflects the correlation between the features but also embodies the co-occurrence information between the features in SVD space. As can be seen from Table 3, the similarity value of "time (t7)" and "graph(t11)" is 0.4988. These terms are from different classes and the variation of the terms can be considered as the co-occurrence of "time" and "user". Assuming in these nine articles, one common feature is in each document. In this semantic space generated by the document collection, for the mutual transmission between the terms, some feature co-occurrence information that not exists appear in the document so that some non- existent feature co-occurrence information which is noise data will generate between the documents. Fox example,a term X is added to the each document of Table 1 whose feature weight value is 1. Feature

document matrix in Table 1 are decompose by SVD and the similarity values between the features are gotten by the Equation (1) and whose similarity matrix is shown by Table 4. From the Table 4, the weight value of "compute(t3)", "response(t6)" and "time(t7)" are all 0.6925, compared with the corresponding ones in Table 4 that the weight value is weakened. As seen from Table 1, the common feature X added makes the weight value of these words weakened that should be very near.

The similarity degree between the documents mainly depends on the number of the co-occurrence features. In the generating latent semantic space, because of the transitivity between the features, the latent relations will be excavated. There is perhaps high similarity between the documents whose similarity are little or non. The similarity between the documents is calculated by Equation (1) and Table 6 is the similarity matrix after being adding the feature X. As seen from the data of the matrix, a clear distinction exists between the documents. The same type of documents has a higher similarity and different type of documents has a lower similarity. For example, there is a high similarity between "M4" and "c2" whose value is 1.1213. As these documents all have the term "survey", the "survey" weight value of co-occurrence is strengthened.

The corpus Tancorpv 1.0 used in the experiment is from the Chinese Academy of Sciences , Dr Tan Songbo and the text classification corpus from Sogou Lab. 12 classes are randomly selected from the 12 categories in the Tancorpv 1.0 which is 2,400 texts in all named as the Chinese Academy of Science Corpus 1 , the smallest text is 1kb, and the largest one is 14.7kb. One thousand texts are randomly selected in 9 classes from the text corpus of the Sogou Lab whose largest class contains 200 documents and whose smallest class contains 80 documents. 3000 texts are randomly selected from 60 smaller classes named as the Chinese Academy of Science corpus 2.

The features are firstly selected on the corpus. For the different experimental corpus, different thresholds ×FT are set (FT is the total number of documents for each experimental corpus;  is scale factor whose value in the range [0, 1] ; ×FT is rounded).  The feature of DFij >   FT is filtered off, forming a new feature space. The feature weight is calculated by TF-IDF[5], by the filtering feature document matrix generated through vector space model, then decomposing the matrix by SVD [6]. In LSI space, the text similarity is computed by the calculation method of the vector angle cosine. The clustering is by Complete- link algorithm.

As seen from the experimental results of Table 8, clustering performance firstly ascends and then descends with the increasing of  on Sogou corpus and the Chinese Academy of science corpus 1. On Sogou corpus, when  is 0.40, the clustering performance is highest. When  is 1, the clustering performance has similar states on the Chinese Academy of Science corpus 1. However, for the Chinese Academy of Science corpus 2,

the clustering performance ascends, lastly to be the highest. This shows that the selecting of the appropriate threshold and the filtering out features of the document frequency over the threshold can not only reduce the dimension of the feature space but also improve the performance of the clustering.  100% ) means the feature transfer relationship have not been selected. The F-measure value of the clustering results will not change any longer when the feature of document frequency less than 50% FT is as a new feature collection. From these three corpus, when the feature document frequency is reserved between 10% and 15%, some feature transfer relationship can be effectively filtered out in LSI space and unreasonable co-occurrence features and some noise data can be eliminated.

In this paper, we think that the transfer number between features has a great impact on the performance of latent semantic indexing. As the feature transfer number increases, some non-existent feature co-occurrence information appear which affects the similarity between features so that affects the performance of the latent sematic indexing. Before the decomposing of SVD, the feature of document collection is selected by DF feature in order to reduce the feature transfer number and non-existent feature co-occurrence information. The DF method used by our paper can selected features with documents in document collection and simply filters the transfer number between features. The next step, we will study on the feature selection based on conditional entropy between the features and conditional entropy.

