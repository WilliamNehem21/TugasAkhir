In the context of PV, biomedical literature may provide important medical data, from preclinical researches, clinical trials, observational studies, and case reports, which can be missed in regulatory PV ac- tivities due to focusing on coded and structured SRSs data. Addition- ally, drug interactions are frequently reported in pharmaceutical jour- nals and technical reports, making medical literature the most effective source for the detection of DDIs. Therefore, developing NLP methods aiming at mining drug safety information for either single drug-adverse- event combinations or higher-order associations have been grown in re- cent years [35–37]. Despite its merits, this type of data sources has a number of deficiencies that can be summarized as following: (1) Infor- mation comprehensiveness is limited due to restricted financial access to many databases, ignorance of their existence by many people, non- standardization of their interface formats, in addition non-user friendly search engines; (2) Information quality is limited because no uniform guidelines exist for contents of the major text fields in database records (abstracts, titles, keywords, descriptors); (3) Compatibility among the contents of all record fields leading to different perspectives of the same technical topic; (4) Information retrieval is limited because time, cost, technical expertise, and substantial detailed technical analyses are re- quired to retrieve the full scope of related records in a comprehensive process; (5) Unfiltered nature of biomedical literature is too statistically noisy to allow accurate signal detection. These deficiencies can result in two serious limitations: a substantial amount of relevant literature is not retrieved, and a substantial amount of non-relevant literature is retrieved [36]. In summary, it is very challenging to detect drug safety signals due to high statistical noise of the biomedical literature, so high quality text mining algorithms and/or machine learning needed to be developed [37].

There are a number of freely available biomedical knowledge databases, for example, DrugBank, STITCH, and PharmGKB [38–40], support authors to develop representation frameworks targeting the pre- diction of safety signals. This kind of resources provides information about drugs’ chemical structures, biological elements, targets of phar- macological actions, functional/metabolic pathways, etc. Making use of this type of data sources offers some privileges, for example, allowing the opportunity to predict early-stage attrition due to new therapeutic candidates’ toxicities during drug-development process [41]. This type of data sources has limitations represented in: (i) diﬃculty of modeling and simulating multifaceted metabolic pathways for thousands of drug candidates as it is crucial to preprocess molecules extracted from those databases with a set of cleaning rules to be representative dataset; (ii) no available databases completely describe the exact mechanisms by which drugs and genes may interact; (iii) complex informatics methods are required to explore those chemical or genetic data.

Data mining is the process of extracting the patterns, associations or relationships among raw data using different analytical techniques involving the creation of a model to derive useful knowledge [42,43]. In PV, the data mining techniques are dedicated for hypothesis genera- tion of new possible adverse drug events”. Quantitative signal detection practices in large databases (e.g. VigiBase) are founded on data mining algorithms using disproportionality analyses (DPAs) [44,45]. DPAs are

Fig. 1. Venn Diagram illustrating the entries {a, b, c, d} of con- tingency table. a, number of reports containing both the suspect drug and the suspect adverse event; b, number of reports containing the suspect drug with other adverse events (except the event of in- terest); c, number of reports containing the target adverse event with other medications (except the drug of interest); d, number of reports containing other medications and other adverse events.

categorized into two general classes (frequentist and Bayesian). Both categories utilize the entries of confusion matrix (as shown Fig. 1) to compute the magnitude of statistical association between drug-adverse- event combinations (DECs) in a PV database [46]. Their principles depend on estimating observed-to-expected reporting ratios for drug- reaction pairs using the total number of cases in background [47–52]. They quantify the unexpectedness of adverse event being reported to a drug or drug pair. All DPAs share the same basic of disproportion- ate reporting between observed to expected ratios, despite they differ in computation per each. Whenever a DEC’s DPA computation exceeds the predefined minimum thresholds, it will be flagged as a statistical safety signal that requires further clinical assessment. Different thresh- olds are used for these algorithms to identify significant safety signals for DECs.

Besides DPAs, there is an algorithm called association rule mining (abbreviated as ARM) [53]. ARM is considered a well-known min- ing algorithm for disclosing interesting patterns concealed within large databases. This algorithm was first advanced since more than a decade to be applied to the field of computer science, then expanded to various sciences [54–56]. Some studies have adopted ARM-based algorithms to detect adverse DDI patterns using SRSs [57]. Apriori algorithm is a kind of association rules mining which offers an appropriate representation of sparse data for complex computations [53]. Any association rule can

Confounders are hidden covariates that may be hidden factors lead- ing to either flagging spurious safety signals or delaying the detection of significant ones [58]. It is worth mentioning that confounders may infer a risk factor predisposing the adverse reaction or a key to identify higher risky patient subpopulations. A simpler type of confounders can be seen in variables (for example, age, gender, and year). They may be handled effectively by the stratification for each stratum using Mantel– Haenszel adjustments [59,60]. Nevertheless, adjusting huge numerals of possible confounders may result in missing signal detection in a timely manner [61–63]. Another limitation is represented in stratification by gender, age-group, etc. where the number of case reports are low and then infeasible to conduct subgroup analyses. Additionally, there are other forms of confounders called “innocent bystander” responsible for the occurrence of adverse events such as interacting drugs or indica- tions of reported comedications. Unfortunately, using Mantel–Haenszel approaches for adjusting such kind of confounders is ineffective [60]. For large numbers of covariates, adopting logistic-regression (LR) ap- proach is more eﬃcient [61].

LR extends linear regression function by a sigmoid function to a value interval from 0 to 1 [62]. LR computes ROR by categorizing database’s records as case-control records where a case is counted whenever hav- ing adverse-event of interest, whereas controls are counted whenever records having other adverse-events. In the context of PV, two studies have reported adopting LR modeling using SRSs aiming at DDI signal de- tection [63,64]. The comprehension of LR can be expressed according to the following formula:

support. The support of a DEC 𝑆(𝑋) is the observed numerals of reports specific DEC exceeds minimum thresholds of minimum confidence and having 𝑋. While support of an association rule 𝑆(𝑋 → 𝑌 ) can be sym- bolized as 𝑆(𝑋 ∪ 𝑌 ). The confidence of an association rule 𝐶(𝑋 → 𝑌 ) indicates (𝑆(𝑋 ∪ 𝑌 )) ∕ (𝑆(𝑋)) (1). Confidence defines how frequently

Almenoff et al. have examined the Bayesian disproportionality mea- sure MGPS (90% confidence interval) in screening interaction profiles between antihypertensive drug “verapamil” and other classes of car- diovascular medications [69]. Results have shown the usefulness of MGPS disproportionality algorithm in mining DDI interesting patterns in polypharmacy conditions. Schuemie et al. [70] proposed an approach known as longitudinal GPS (LGPS) which is a modification of the orig- inal MGPS approach. LGPS computed the expected number of medi- cal events during drug prescriptions based on an aggregate unexposed patient-time in exposed and unexposed patients. For protecting against confounding from dominating unexposed patients so long as the major- ity of the patients in a population don’t receive specific medicine, au- thors suggested a filter named Observational Profiles of Adverse events Related to Drugs (LEOPARD) to eliminate spurious associations caused by protopathic bias. LGPS has been shown to outperform related meth- ods, including MGPS.

The principal of multiplicative model is based on assuming that a safety concern accompanied with a medication is multiplied to its back- ground estimate, whereas the principal of additive model is founded on adding the safety concern linked to a drug into its background esti- mate. Regarding the multiplicative model, supposing the null hypothesis is real (i.e., non-DDI), the proportional risk of drug-drug-adverse-event

There are different types of unstructured data sources which can be represented in narrative portions of EHRs, biomedical databases, etc. In recent years, researchers have proposed methods for mining such sources to effectively predict adverse DDI signals making up for the lim- itations of spontaneous reporting systems. Algorithms related to text mining and natural language processing (NLP) are commonly utilized to evolve these tasks.

Iyer et al. [75] have proposed an approach of natural language pro- cessing for disclosing DDI signals directly from the embedded verbatim parts of two big corpora of EHRs. They adopted adjusted disproportion- ality measures to detect significant associations of DDIs composed of 1165 distinct drugs and 14 distinct AEs. The authors’ method has shown comparable performance between the DDIs signals discriminated from EHRs and those identified from SRSs.

Jon D. Duke et al. [76] have developed a new approach to mine mechanistic features from PV literature abstracts to predict potential DDI signals. Authors have specifically investigated interesting DDI pat- terns associated with the increased risk of myopathy and related mus- culoskeletal conditions. First, the authors applied two-step Information Retrieval (IR) approach to recognize the expression patterns relevant to DDI from PubMed abstracts. Then, they performed two logistic re- gression analyses to test each DDI effect on myopathy. They validated clinically significant DDI signals via using a database of electronic health records. Five novel DDI signals of increased risk of myopathy along with related cytochrome metabolizing enzymes were predicted.

LePendu et al. [77] proposed an approach for mining textual clini- cal notes corresponding to 1,044,979 patients derived from the Stanford Translational Research Integrated Database Environment (STRIDE) us- ing odds ratio (OR) as frequency-based association measure. In purpose of generating hypotheses of DDIs signals, authors have used unstruc- tured notes of EHRs rather than SRSs. Authors have evaluated their sug- gested approach in terms of sensitivity and specificity using gold stan-

Support vector machine (SVM) is the most popular non-probabilistic ML algorithm which was first developed by Vapnik and Lerner for bi- nary classification problems [85]. The main concept of SVM is to create the optimal hyperplane/decision boundary. A hyperplane is a line/(n- 1) dimensional plane that can separate n-dimensional input space into two classes so that a new data point can be easily categorized into the correct class [84, 86]. There are two types of SVM: linear SVM and non- linear SVM depending on whether data are linearly separable or not. For linear SVM, the distance between the decision boundary and the closest data points is referred to as the margin. As shown in Fig. 2, the optimal hyperplane is the line with the largest margin that can separate the two classes. The margin is calculated as the perpendicular distance from the hyperplane to only the closest points referred to as the support vectors of the hyperplane. Only these points are relevant in defining the line and in the construction of the classifier.

The real-world applications of SVM is subject to non-linearly sepa- rable data where a linear decision boundary cannot be used to classify the dataset. This leads to the emerging of a new generation of learn- ing systems called kernel functions. A SVM Kernel is a function where it takes input training dataset with non-linearly separable decision surface and transforms the low-dimensional feature space into an abstract high- dimensional space. The most common kernel functions used in SMV modeling are Gaussian radial basis function (RBF), polynomial, and sig- moid kernels [87].

In this, we can conclude the merits and demerits of SVM. The key advantage is the kernel trick that enables SVM to create non-linear max- imum margin decision boundaries in the original non-linearly separable feature space and consequently allows SVM to operate well in high- dimensional spaces. While disadvantages are represented mainly in: (i) Long training time for large datasets; (ii) its black-box nature that makes the final model diﬃcult to interpret.

K-nearest neighbors (KNN) is a popular ML algorithm belonging to the family of instance-based learning for the purposes of classification and regression prediction [88,89]. KNN at its core is a distance-based algorithm where the distance between two objects/data rows is calcu- lated. KNN typically requires the calculation of distances between an example and k samples in the training dataset with the smallest dis- tance to determine the prediction. While in the test set, the class label is predicted by only calculating distances for the nearest k neighbors (see Fig. 3). The most commonly used distance metrics in this regard are “Euclidean distance” and “Hamming distance” [90,91]. “Euclidean dis- tance” metric is computed as the square root of the sum of the squared differences between the two vectors as seen in the following formula:

gorithm is applied directly on unlabeled input data to find regularities, and then a model will be built according to the identified patterns. Clus- tering and biclustering algorithms are the most common unsupervised ML techniques [84]. The following subsections from 5.1 through 5.5

The main advantage of KNN algorithm is its capability to arbitrar- ily approximate any data distribution. However its demerits can be abridged as follows: (i) Computationally challenging as KNN uses all data points of the loaded training dataset for conducting queries; (ii) the number of data needed to be scaled grows exponentially with dimension by increasing the dataset size. It is known as the “curse of dimensional- ity”.

Ensemble learning is a general meta-approach that combines predic- tions from multiple ML models to achieve better predictive performance. One of the powerful ML algorithm based on bagging ensemble learning is random forest classifier (abbreviated as “RFC”). It was developed by Leo Breiman [92]. Bagging is one of the main standard strategies used in ensemble learning techniques that fits many decision trees on different samples of the same dataset and averages the predictions (see Fig. 4) [93].

RFC is a supervised learning algorithm that is based on the infras- tructure of decision tree algorithm and to classify either categorical or continuous features [94]. It estimates numbers of decision tree classi- fiers which fit on various subsets of a training dataset to augment the predictive performance and manage overfitting compared to “Decision tree algorithm” (refer to Fig. 5). The considered number of features at each sub-tree equals to the square root of the number of features in the input training dataset. Then, it averages the scores of each decision tree to predict the class of the test dataset instance [95,96].

Artificial Neural Networks (ANN) are a set of algorithms developed to imitate the neurons in human brain that are designed to recognize patterns in data which may be used for classification or clustering prob- lems [99,100]. There are two main categories of ANN: (i) simple neural networks; (ii) deep learning neural networks (refer to Fig. 6). We here outline the general ANN architectures and common activation functions with taking into consideration deep learning neural networks are out of scope of the underlying review where ANNs.

There are two specific architectures for ANNs: (1) Feed-forward neu- ral networks and (2) Feedback/Recurrent neural networks. Feed-forward neural networks are the most common architecture of ANN in practical applications. In this architecture, the information travels in one direc- tion towards the output layer with one or multiple hidden layers. While Feedback/recurrent neural networks (RNNs) are more flexible and much diﬃcult to analyze than feed-forward networks. RNNs can process vari- able length inputs by processing them in time steps, allowing the output of deeper layers to be fed back to previous layers in subsequent time steps. Therefore this type of ANN is typically utilized in sequential and time-series tasks.

Sigmoid is a non-linear activation function which is also known as “Logistic function”. It provides a smooth and differentiated gradient curve. Sigmoid is typically used as the activation function in binary classification problems where its prediction output is normalized into the range [0–1]. Like Sigmoid, Hyperbolic tangent activation function is used in binary classification problems. Nonetheless, it is zero centric

and the output values range from -1 to 1. B is the most popular acti- vation function used in multiclass classification problems besides deep learning neural networks applications. ReLU allows the elimination of negative units in an ANN which diminish the sparse activation of only about 50%. Unlike the previously mentioned activation functions, Soft- max’s output is computed by the probabilities of modulated inputs to predict target class with the highest probability. The sum of the entire probabilities must be equal to 1. This function is commonly utilized in multiclass classification problems.

gies of network-based ML approaches depend on inferring drug simi- larities between network nodes or learning about topological features of the network structure [122,123]. Those knowledge networks can be constructed by extracting and integrating existing drug knowledge from one or multiple data sources (e.g. chemical, biological, target, genomic, pharmacological databases) leading to various shapes of networks (e.g. drug-drug, drug-target, protein-protein, pharmacodynamic, drug-gene, phenotypic, pharmacokinetic, etc.) [124].

The merit of NB modeling is that it is easy to build with no compli- cated iterative parameter estimation. It is particularly useful to imple- ment on large datasets [107]. However, the main limitation of NB is the assumption that all the predictor features are completely independent.

tihypertensive drugs. Three types of experimentally evidenced clinical data sources (FDA-approved drug labels, Database of Clinicaltrials.gov, and published preclinical studies) were utilized. In this study two topo- logical measures were adopted for capturing associations between ele- ments of disease and drug–target networks in the human protein–protein interactome. Network-based proximity measure was used to capture the topological linking between two drug–target modules, whereas, Z-score was used to get closeness among disease and drug networks. As a re- sult, six different relationships of drug-drug-disease modules could be distinguished by combining a number of decision rules.

DDIs using FDA PV database (FAERS). Their similarity-based method depended on capturing chemical and pharmacological features. Their results showed enhanced sensitivity, specificity and precision in com- parison to the data mining algorithm proportional reporting ratio “PRR” traditionally applied on PV databases. Likewise, Sornalakshmi et al.,

This paper investigates pharmacovigilance signal detection for drug- drug interactions from several aspects. Based on the illustration pre- sented above in sections and subsections, herein we concluded a gen- eral framework of informatics-driven modelling that may be used by researchers for the purposes of adverse DDIs discovery (as shown in Fig. 8). One of the main objective of knowledge-based studies is to eval- uate the eﬃciency of a model features in discovering novel DDI sig- nals. Various metrics were used in purpose of evaluating the capability of informatics-based approaches in DDIs prediction. The most common metrics utilized in these studies are precision, sensitivity, specificity, accuracy, F1-measure and AUROC. Sensitivity indicates how well the model distinguishes adverse DDI signals. Specificity measures how well the model can disclose non-DDI signals. Accuracy measures how well the model can predict both DDI signals and non-DDI signals. AUROC metric is mainly used in ML studies to estimate how well specific descriptors can distinguish between two classes (DDI/ non-DDI).

Table 1 displays a comparison of performance metrics for either data mining or machine learning studies – (previously presented in sections 4.0 & 6.0) – in the context of DDI signal detection. There are no state- of-art methods that can be benchmarked. This is due to different subsets size, quality & diverse of data preparation approaches, different valida- tion methods, etc. Henceforth, in Table 1, we focused the comparison on two main items (research context and data source), besides the well- established algorithms per each context (please refer to Fig. 8). From the data mining perspective, hybrid Apriori seems to provide the best performance among data mining algorithms in terms of sensitivity and precision.

Classic PV signal detection practices are focused on adopting DPA algorithms to mine SRS data for constituting hypotheses of single drug- AE combinations that need further investigation to establish evidence– based medicine to confirm or refute causality associations between those pairs. Then, regulatory actions may be taken to protect the public health. However, there is still limitations towards adopting data mining ap- proaches in regular PV practices to generate novel hypotheses for DDI signals. Moreover, modest efforts are in place to mine DDI signals for experimental drugs where there is no or limited data in various data sources. Wherever a novel hypothesis for DDI signal is created, more

Fig. 8. A multi-layered informatics-driven framework for discov- ering drug-drug interactions safety signals. SRS: Spontaneous Re- porting System; EHR: Electronic Health Records; PRR: Proportional Reporting Ratio; ROR: Relative Odds Ratio; RRR: Relative Report- ing Ratio; IC: Information Component; MGPS: Multi-Gamma Poison Shrinker; SVM: Support Vector Machine; KNN: K-nearest neighbor; RFC: Random Forest Classifier; LR: Logistic regression; ANN: Artifi-

experimental/clinical investigations may be needed to provide further evidences for confirming a DDI signal. Several factors have impact on detecting DDI signals where important ones may be missed or spuri- ous associations are more likely to be produced. These influences can be summarized in: minimal demographic data, duplicated reports, no information regarding patient exposure, underreporting that may arti- ficially increase expected risk estimates of the drugs, verbatim medical terms, unstandardized drug names, etc. Accordingly, optimizing current data mining techniques with chosen thresholds aiming at overcoming raw data challenges and capturing DDI signals with higher sensitivity and precision is crucial. Also, researchers have shown ARM and its ex- tended algorithms as good choices for handling the sparse of postmar- keting PV data within SRSs where strong association patterns could be generated to easily recognize higher order of Drug-AE associations. On the other side, adopting EHRs as a complementary source for routine PV practices generally and DDI signal detection specifically is challeng- ing. This may be attributed to the ontology problems, heterogeneity of data from various sources and patients’ information are mainly being in textual formats, besides confounding by co-medications, risk factors and/or comorbidities. Hence, developing NLP approaches and to be in- tegrated with regression methods can be an option to handle ontology problems along with managing confounders. Another avenue of NLP can be adopted in regular PV data in purposes of reducing the large space of drug-drug-event associations inherently present in SRSs and allowing further processing in the context of DDI signal detection [134].

overall performance concerning both accuracy and specificity compared to other methods. While, SVM appears to have the best overall DDI pre- dictive performance in terms of AUROC compared to other supervised ML in predicting DDI signals [135–140]. However, SVM has inherited limitations represented in the time lapse of model running and the black box nature limiting the interpretation of the prediction outcomes. In re- cent years, the dawn of adopting supervised ML in PV signal detection has resulted from the evolution of multiple useful biomedical knowl- edge resources as gold standards of labeled DDIs (e.g. DrugBank [141], KEGG, Micromedex), besides using these resources to assess the predic- tive performance of supervised ML avenues.

The use of supervised ML has its limits such as class imbalance, sparse features, low overlapping and consistency between different DDIs re- sources [142]. Therefore, advancements in incorporating unsupervised ML approaches in DDIs discovery studies, improvements of DDI cor- pora annotation, and establishing standardized guidelines for establish- ing DDI gold reference datasets are probably essential steps to develop more realistic ML frameworks during drug-development, as well as, pre- marketing phases. Other directions of developments to be implemented into routine PV practices are represented in how ML may contribute in boosting DDI signal detection from SRS data and/or predicting their types/change in severity, combining more than one data sources (e.g. EHRs, patient support programs, prospective surveys, randomized con- trolled trials, etc.) rather than restricting to SRS data, assessing ML al- gorithmic performance by constructing reliable test sets, in addition to advancing frameworks for increasing transparency or explainability of ML outputs.

According to the abovementioned illustrations and beyond the struc- tured PV data sources, the exponential increase of the scientific liter- ature complicates the exploration of such biomedical corpora [143]. Biomedical corpora enables constructing learning datasets of medically related terms that embed prior knowledge and aid in the pragmatic use of certain words within specific contexts [120]. Informatics-based meth- ods, in particular text mining and ML frameworks have great applicabil- ity in this regard to aid in extracting, analyzing and classifying biological information designated in scientific publications. During the past years, biomedical corpora have presented a valuable PV data source for the detection and analysis of DDI signals. The unstructured nature of this type of data sources is challenging. Natural Language Processing (NLP) approaches are crucial in this context to annotate, standardize and map

The availability of freely available safety data sources along with the adoption of novel DM and/or ML methods has advanced the PV domain. In this review, we particularly focus on developments in in- formatics techniques in the context of DDI signals. We have shown a portfolio of PV resources, DM and ML approaches techniques proposed to discover adverse DDI signals. Each method may re-stimulate interest to evolve DDI surveillance practices by offering different prospects. To the best we know, the underlying paper is the first review combining machine learning and data mining methods to disclose potential signals of DDIs in drug safety surveillance. Possible causal DDIs associations are experimented during drug development cycle, then monitored via post- marketing PV systems after being in market. Developing better predic- tive methods, to flag potential DDI signals, becomes of great importance to industry and regulatory bodies [144,145].

The black box nature of ML methods is one of their major limita- tions leading to diﬃculty in interpreting of what features are impor- tant and/or interpreting the model results. However, generally, adopting informatics-driven methods to be implemented in regular drug develop- ment process will be valuable. Since this will enable greater linkage between basic experimental platforms and controlled clinical settings early during drug development phases, then empowering the analysis of important safety concerns for investigational drugs at earlier stages lead- ing to more eﬃcient triage for novel drug candidates for further steps in the development process [146]. Boosted predictive approaches can be achieved by integrating structural and biological knowledge with en- riched safety datasets [147]. Previous studies have shown benefits from linking drugs’ phenotypic, therapeutic, structural, and genomic informa- tion in revealing DDI patterns in both drug discovery and postmarketing PV processes [148–150].

Innovative paradigms have arisen from exploiting various data sources compared to conventional PV practices, permitting for the ac- tive monitoring of DDI drug profiles. As spontaneous systems are con- sidered the biggest assembly of real world data for distinguishing DDIs [151–154]. The discovery of higher-order drug–event combinations is more challenging than identifying signals of single drug–event pairs due to the limitation of under-reporting rates in large SRSs. Although the DDI detection research shifted away from SRSs utilization into other data sources in the recent years, this couldn’t replace the main role of SRSs in regular PV. Accordingly, more efforts are required in purposes of evolving methods for routine DDIs discovery across different safety data sources.

Also, the main challenge in DDI signal detection studies is the lack of well-established guidances for assessing informatics-driven methods performance. Principally, this is due to the lack of reference standard for interaction safety profiles of the whole marketed drug products [155]. Hence, researchers should conduct more studies with purposes for ac- quiring better comprehension to the landscapes of multivariate associ- ation measures along with estimating corresponding predictive perfor- mance. Furthermore, developing approaches targeting the estimation of optimal DDI signal detection thresholds that achieve balanced trade-off between sensitivity and specificity with decreasing false signals can be a valuable perspective.

Hartford CG, Petchel KS, Mickail H, Perez-Gutthann S, McHale M, Grana JM, Mar- quez P. Pharmacovigilance during the pre-approval phases: an evolving pharma- ceutical industry model in response to ICH E2E, CIOMS VI, FDA and EMEA/CHMP riskmanagement guidelines. Drug Saf 2006;29(8):657–73.

