Aiming to target occlusion problem in complex scenes, human action recognition via a top-view vision is proposed. However, in this view, the human behavior rotated will be mistakenly identified as another behavior. To address this situation, taking into account the rotation invariance of the moments, human static posture are represented by Hu moments, and using the SVM as trainer and classifier. According to the change of coordinate of the binary image centroid, semantic web of dynamic behavior is established. The experimental results show that this method can accurately identify the human dynamic information and has a high recognition rate.

Human behavior understanding is very active in the field of computer vision, and with the rapid development of computers and other technology, it has made great progress [1]. In recent years, people have started from theoretical research to practical application scenarios. It is great significance in the real world. Now at many public occasions, the camera is located in the corners or top, so in this paper, based on a positive view would be impractical. At present, most of the behavior studies are based on the corner of the camera, while the top of the camera mostly concentrated on the statistics of the number of people [2,3], and action understanding literature is rare [4].

Typically, vision-based human behavior analysis in general compliances with a few basic processes such as feature extraction, motion characterization, action recognition, high-level behavior understanding and scene understanding [5]. Human action characteristics extracted from image sequences are essential to behavior recognition and behavior understanding. The results based on the positive view and corner view in the complex environment of the room or station will be greatly curtailed by kind of occlusion.

Therefore, a method based on the top-view is proposed. The method has the inherent advantage to solve the occlusion problem. However, in top-view, rotation of people makes the common feature representation is invalid. The translation invariance and rotation invariance of Hu moments are used to represent the gesture feature vector [6]. At the same time more high-level semantic information is drawn according to the change in the coordinates of the centroid.

In the experiments we found that the scene transient probability is relatively small so model update rate should be taken a smaller one. We sort the weight of GMM, and take the 65% of sum of weights as a useful model. Moving target can be obtained quickly by the above method. However, because of noise, light, and many other factors, the image will appear hollow noise points, shadows, etc. so image post-processing is required. In this paper, the color model is used to calculate the shaded area [8], and the model is described as follows.

Assume that the shadow of the target pixel Shd in the YCbCr space vector represent as Shd(Cx,Cy,Cz). The background pixels ‘Back’ in the YCbCr space vector is expressed as Back(Bx,By,Bz). Among them, x is the Cb components, y is the Y components and z is the Cr components. The model of YCbCr space is shown in Figure 1.

The shape features need to be same after rotation in the top-view. So, the invariance of the geometric transformation is employed to represent the behavioral characteristics. Certain moments of the image area have the same characteristics such as translation, rotation, scale and other geometric transformations, so it is widely used in pattern recognition.

Use the training data to train SVM. SVM is a learning method based on structural risk minimization criteria and takes training error as the constraints of the optimization problem [9]. SVM classifier with kernel is shown in formula and we can get a classification.

Static posture recognition is far from meeting the needs of people. Accurately our aim is to identify the dynamic information. In recognition of the static posture, you can employ the coordinates of the centroid changes to identify the higher level of semantic information.

In the experiment, a static posture is recorded every five frame. If the frame dissatisfies all the above set of values, it's beginning to find eligible value and record it every three frames. Then you get all the records according to the current record and previous records to identify the dynamic behavior information of Figure 3.

In the experiment, the camera fixed at the top of hall and the lens direction is vertically downward. The distance between ground and hall is about 6m.The experimental environment is VS2010 and Opencv2.2. And experimental set with 30 frames /s video data.

It is only taken a sequence of 100 frames to test each gesture. The first moment of Hu moments is shown in Figure 5. Each gesture contains a rotating action and other factors. And we all know the meaning of first value of Hu moment is area.

The recognition result is shown in Table 1 by literature [4]. It can be seen from the experiment that the recognition rate of lying is 100% using this article method, and seven frames of standing will be detected as squatting and three frames of squatting will be detected as a standing. According to the analysis of the test video sequences, squatting, arms and some stretch will be detected as a standing, and the relatively larger stretch even can be detected as lying. At the same time, leaning forward will be identified as the standing when squatting. If standing with body tilt down, it may be detected as a squatting. The recognition rate of this article has significantly improved than literature 4.

A top-view behavior recognition method is presented in the paper. It is able to identify three static postures and eight high-level dynamic behaviors. The experimental results show that the identification of this article is significantly higher than literature [4]. However, arm lift or not is very serious to behavior recognition, and how to overcome the impact of the arm is the focus of future research. This article is the basis of the behavior recognition of this view and we will focus on the other behavior, people interaction and tracking research.

