The developers’ choices of debugging approaches and tools there- fore have a great influence on their success and pace in fixing the bug. In most cases, this choice depends on the information obtained from the textual bug reports, and the developers’ experience and knowledge. In order to encapsulate and discretize some of this knowledge, we have created a bug classification schema on a high abstraction level. We categorize bugs according to their underlying fault into Concur- rency, Memory, Semantic, and Other bugs. With these categories, we

try to provide an abstraction that encapsulates the different challenges developers face when dealing with a specific bug type including the different tools and approaches for reproduction and localization. To leverage on such a classification schema for practical debugging sup- port, e.g., for tool recommendation, a priori knowledge about a bug’s type is required.

In this paper, we propose a machine learning (ML) based classifier that predicts the fault type of a bug based on its textual bug report. This ML based approach tries to encapsulate a small aspect of what would commonly be considered developer knowledge and experience in debugging. A priori information about the underlying fault type can support inexperienced developers in their choice of debugging approaches and tools.

(3) We perform a user survey to establish a baseline of human clas- sifier performance on this task. (4) We introduce preprocessing steps tailored specifically for bug reports and we apply ensemble learning methodologies on the classification problem. (5) We evaluate the classi- fication performance of various classical ML algorithms, preprocessing steps, and ensemble approaches. (6) We evaluate the performance for inter-project application. Our main findings of this Journal version are:

The remainder of this paper is structured as follows: Section 2 describes the problem of fault type prediction using bug reports. In Section 3, we discuss related work and similar classification endeavors. In Section 4, we discuss the background of this work including exist- ing bug classification schemata, ML based classification, and natural language processing (NLP). In Section 5, we present our experimental setup and approach, followed by our research questions and results in Section 6. In Section 7, we discuss the internal and external threats to validity. Section 8 concludes our work and discusses future research. All datasets and implementations are publicly available (see Section 8).

The classic NLP example of sentiment analysis is based on the assumption that ‘sentiment’ information is inherent to human written text, and the assumption that the inputs contain only human written text. Bug reports are very different from such showcase NLP prob- lems. The information required for correct classification may not be contained in a bug report: Bug reports are textual descriptions of complicated behaviors, states, and outcomes to communicate a problem in a complex system. Such reports are authored by people occupying different roles, functions, and positions in relation to the project. These roles range from end-users unfamiliar with software development, to highly experienced developers with years of experience within the project. Because of this, the authors’ technical expertise and project specific expertise can vary significantly. Further, different roles have inherently different viewpoints and scopes of the bug reports. For example, bug reports may describe only the bugs’ impact in non- technical terms, while other bug reports may describe a problem in technical detail without mentioning possible impacts.

These issues make bug reports a challenging target for NLP ap- proaches. Our task of fault type classification is further complicated by lack of information or misleading information in such bug tickets. We therefore do not expect to see the high classification performance scores known from classic NLP showcase problems.

Multiple researchers have investigated the application of ML and NLP methodologies on textual bug reports for classification problems. Lopes et al. [6] automatically classified more than 4000 bug reports collected from three open-source database applications according to the Orthogonal Defect Classification (ODC) schema. They performed undersampling to avoid imbalanced datasets. Thung et al. [7] per- formed automated ODC defect type classification using semi-supervised learning. Our work distinguishes from Lopes et al.’s and Thung et al.’s works in the used schema. While the ODC schema aims at the analysis and optimization of the software development process, our schema aims to assist developers in choosing the best debugging tool for the bug at hand.

Tan et al. [8] categorized bugs w.r.t the dimensions root cause, impact, and affected component. They applied ML classifiers as support in their data mining efforts. While Tan et al. categorize bugs into con- currency, memory and semantic bugs, we introduce a fourth category (Other ) which covers documentation, build system, configuration and UI resource faults. Since only a small fraction of their manually labeled dataset contains concurrency bugs, they performed a keyword search using keywords such as ‘race’ and ‘lock’ on the textual bug reports. In contrast to their work, we performed the keyword search on the commit messages instead of the bug reports to reduce information leakage.

Ray et al. [10] used ML classifiers to investigate programming language and code quality metrics in open-source projects. They apply five different root cause categories: Algorithmic, Concurrency, Memory, generic Programming, and Unknown. Since their approach is focused on the analysis of fixed bugs, they train their ML approach on com- mit messages (a posteriori approach). However, we are interested to provide information to the developers before they have fixed the bug. Therefore, we train our classifier on the textual bug report instead of the commit messages (a priori approach).

Ni et al. [11] predicted the root cause type from the code changes by converting the code changes into abstract syntax trees (ASTs) and then using a Tree-based Convolutional Neural Network (TBCNN). They distinguished six main root cause categories (function, interface, logic, computation, assignment, and others) and 21 sub-categories. In con- trast to our work, this classification was performed post mortem on the bug fix.

Particularly interesting in the context of fault localization is the approach proposed by Fang et al. [22] that classifies bug reports as informative or uninformative. This approach can be used as a prepro- cessing step in information retrieval (IR) approaches to filter out those bug reports where IR promises little insights.

First, we provide an overview of existing bug classification schemata (Section 4.1). Afterwards, we briefly explain the used classifiers, and statistical methods (Section 4.2) and the most important terms w.r.t. natural language processing (Section 4.3). Finally, we provide the formal definitions of the used performance metrics (Section 4.4).

e.g. severity, impact, and root cause. All classification schemata are of course intended to fulfill a certain purpose, and their dimensions, depth, and detail are selected to achieve the set goal. These purposes range from investigations into process optimization to enable auto- mated triage and prioritization, to research into different areas of the software development process, to the support of techniques such as automated repair.

Polski et al. [24] discussed the application of existing fault classifi- cation schemata and bug distributions for fault injection, and provided an overview on fault classification schemata. Endres [25] performed one of the earliest attempts at bug classification to investigate higher level causes (e.g., technological, organizational, historic).

Chillarege et al. [28] devised the Orthogonal Defect Classification (ODC) schema to form the basis for analysis and optimization of a soft- ware development process. The IEEE Standard Classification for Software Anomalies (IEEE Std 1044–2009) [29] has established a vocabulary for software anomalies as well as a classification schema and attributes for defects and failures.

Only a few classification schemata target the debugging process with the purpose of supporting software developers. Li et al. [9] and Tan et al. [8] studied the characteristics of bugs in open source software to enable more effective debugging tool design and better under- standing of bugs occurring in the real world. Given this focus on debugging tools and debugging processes, they classified bugs along three axes: impact, software component, and root cause. Impact consists of six categories (e.g., incorrect function, crash, or hang). Their root cause dimension comprises three categories: Memory bugs arise from improper memory handling, concurrency bugs occur in multi-threaded programs due to synchronization issues, including race conditions and deadlocks, and semantic bugs are inconsistencies between requirements or programmers’ intentions and the actual software function.

Multinomial Naive Bayes (MNB) are probabilistic classifiers based on Bayes theorem. Naive in this context stands for the assumption that probabilities of features are independent. MNB classifiers are fast and easy to use and can provide a performance baseline to compare other classifiers against.

Support Vector Machines (SVM) are non-probabilistic approaches that can be used for regression and binary classification. SVMs con- struct a hyperplane in the feature space separating the classes. For multi-class classification problems, multiple SVMs are trained simulta- neously in a one-vs-all or one-vs-one setup.

Logistic Regression (LR) are linear classifiers for binary classifica- tion. Logistic regression as well as linear regression are both based on linear models. LR uses the logistic (sigmoid) function to discretize their output—hence the name logistic regression. For multi-class problems, multiple LRs are combined in a one-vs-all or one-vs-one setup.

Imbalanced data in the context of a classification problem means the amount of items per class is not equal for all classes. While there are ML algorithms that are insensitive to imbalanced data, all of the above described classifiers are sensitive to such imbalances. There are basically two strategies to balance a training set, up-sampling and down-sampling. Up-sampling creates synthetic instances for the minority class to scale it up to match the majority classes size. Down- sampling removes instances from the majority class to scale it down to match the minority classes size.

Stemming replaces words with their word stem. For example, the words ‘crash’ and ‘crashing’ have the stem ‘crash’. The resulting stem does not necessarily have to be a word itself. Without prior stemming, ‘crash’ and ‘crashing’ are considered different tokens and therefore result in separate features in a bag of words approach; with stemming

both raters agree (𝑝0), and the proportion of times where agreement is ment by chance. It is calculated based on the proportion of items where expected by chance (𝑝𝑐 ) [34]. 𝜅 values between 0.41 < 𝜅 < 0.60 are considered as moderate, 0.61 < 𝜅 < 0.80 as substantial, and 0.81 < 𝜅 <

Artifact removal discards non-human language artifacts such as stack traces, code snippets, config files, file listings, log outputs and thread dumps from bug reports. Some bug reports in our dataset are as big as 80 kb of text because of such artifacts. Amongst the biggest artifacts are stack traces. While those traces support developers in their debugging efforts, the contained information for classification purposes is mostly limited to name and type of the occurred exceptions.

The wide variety of different formats of artifacts poses a significant problem for automated removal using regular expressions [32]. We therefore employ our custom ML based artifact removal process [33]. The underlying ML model is trained on software projects’ documenta- tion files as well as issue tickets and leverages GitHub markdown for automated training set creation. The resulting classifier model operates on a line-by-line basis and keeps exception names that occur in the artifact.

We use Precision (P), Recall (R), F1-score (F1) in single class ex- aminations to measure and rank our classifiers’ performance, and to enable inter-classifier comparison, and their weighted average F1-score (waF1) and macro average F1-score (maF1) to compare multi-class performance. These metrics can be calculated from the classifiers’ confusion matrices. True Positives (TP) is the number of instances in the predicted class that match the actual class. False Positives (FP) ex- presses the number of instances in predicted class that do not match the actual class. True Negatives (TN) is the number of instances correctly identified as not belonging to the class, and False Negatives (FN) is the number of instances incorrectly identified as not belonging to the class. Precision for a class is TP divided by the total number of instances

Our classification schema should aid developers in the debugging process. We base it on the root cause dimension of Tan et al.’s clas- sification schema [8]. This root cause classification is intended to encapsulate the most promising debugging approaches and specific debugging tools for each category. The top-level of our classification schema is composed of four main categories, Concurrency, Memory, Other, and Semantic. The group of useful debugging tools for Concur- rency bugs will be disjoint from the group of tools useful for bugs in the Other category.

Table 1 provides an overview of our classification schema. A de- tailed documentation including examples and for the increasingly de- tailed subgroups can be found in our online appendix. The purpose of these detailed levels of our classification schema is two-fold: to serve as documentation and education tool for manual classification, and to provide additional information that infers fault and fix patterns providing a clearer picture to the reader. The top-level categories Concurrency, Memory, Other, and Semantic are used in the experiments in the next parts of this paper.

To collect a reasonable number of issue tickets, we mined 101 open- source Java projects hosted on GitHub. These projects cover a wide variety of different software domains, ranging from server side appli- cations, database applications, ML frameworks, testing frameworks, to mobile applications. We added all closed issue tickets of these projects whose labels contain any of the strings ‘bug’, ‘defect’, ‘breaking’, or ‘re- gression’ to our dataset, i.e., 54 755 issue tickets. The resulting dataset is rather noisy due to quirks in GitHub API, bug triaging performed manually by project maintainers, and varying workflows in different projects. Since GitHub API does not differentiate between issues and pull requests, the initial dataset contains both. Further, issues can be closed due to a variety of reasons including duplication, reluctance to fix, or rejected as ‘not a bug’ despite the label. To clean the dataset, we removed all issue tickets that

Managing imbalance. This raw collection of issues tickets is ex- pected to be highly imbalanced w.r.t. our classification target. Other re- searchers identified Memory and Concurrency bugs as minority classes, making up only 2%-16% of all bugs [8–10]. Since our selected ML algorithms are sensitive to such imbalance, we will employ down- sampling to balance our dataset. However, a certain number of data points for the minority classes are required, as the size of these minority classes dictate the resulting training set size. We estimate that our dataset of 11 621 bug tickets contains only a few hundred Memory and Concurrency bugs. Since exhaustive examination of all issues is deemed infeasible, we need to filter and preselect issues for manual examination.

We therefore performed a keyword search on commit messages to identify candidates for manual classification. We used a modified version of the keywords used by Ray et al. [10]. Our keyword set contains 29 keywords and regular expressions, e.g., ‘overflow’, ‘\sleak’, ‘deadlock’, ‘\shangs\s’, ‘\sstarves\s’. The complete list of keywords is available in the online appendix (see Section 8). Commit messages are authored by the developers performing the bug fix. These developers

carry an understanding of the underlying problem, while also carrying a certain level of technical expertise and associated vocabulary. Further, the corpus of commit messages is distinct from the corpus of textual bug reports that constitutes the inputs of our models. This reduces unwanted biases towards certain keywords in our machine learning experiments.

514. Further, we randomly selected and analyzed 158 issues from this group to adequately sample the majority class (Semantic). From the second group, we randomly selected and analyzed 508 issues to adequately sample our Other category. In total, we manually analyzed 1180 issues from 86 software projects. During this manual examination efforts, we removed 418 issues from the dataset for the following reasons:

Dataset quality and verification. Researcher 1 performed the manual classification described above. Six months later, Researcher 1 reclassi- fied 100 randomly sampled and blinded issues from the set for internal verification. In this internal verification step, Researcher 1 scored 0.95 for Cohens Kappa, that suggests almost perfect agreement [35] and a weighted average F1 of 0.96.

Final data sets. In preliminary ML experiments, we identified three projects that are not suitable for our approach of fault type classifi- cation. These projects are: LeakCanary, a Java memory leak detection library, Bazel, a build automation framework, and JHipster, a web application generator. Their domains make it impossible to correctly identify certain bug types using an NLP approach based only on bug reports without knowledge on the projects’ domain and purpose. For example, bug reports from a memory leak detection library utilize a vocabulary otherwise directly connected to memory leaks for all classes of bugs, further reinforced by class names and function names within these software projects. Analog to this, bug reports from a build framework have a vocabulary otherwise associated with Other bugs in any general purpose software project. We therefore removed all items from these projects from the training/test sets and survey answers.

ML algorithms. We have selected Logistic Regression (LR), Multino- mial Naive Bayes (MNB), Random Forrest (RF), and Support Vector Machines (SVM) based on other researchers’ work in similar endeav- ors [6,8,9,17,36], and their ease of use. Further, we employ a LR-based stacking classifier ensemble learning approach to combine the best performing models.

Approach: We performed an online user survey tasking participants to classify textual bug reports according to their fault type in four categories. The survey was promoted via email and direct messages to professional developers at resident software companies and to master students in the field of computer science. It can be assumed that the majority of survey participants were students. Anonymous participation was allowed, while participants disclosing a contact e-mail address could win vouchers of a well-known online store. Multiple submissions from the same person were allowed. For each submission, we tracked IP address, email address (optional), time spent on each bug report, and the corresponding answers for each bug report.

500. The bug reports were formatted the same way as the originals on GitHub, and single choice answers for each bug report were available. While we did not provide a link to the original bug ticket on GitHub, inline images as for example, screenshots, and links in the bug report texts were preserved. Participants were given a short introduction into our classification schema on the survey’s entry page. Short explanations of the four fault types were available during participation inline of each page. Further, a link to detailed explanations and examples of our classification schema were provided. There was no time limit.

Results: We received 51 submissions. We carefully investigated the times spent and IP addresses and did not find any submissions indica- tive of being made with malicious intent. The submissions provide us with a total of 510 manually classified bug reports for our analysis. Removal of bug reports originating from three projects as discussed in , leaves us with 483 classified bugs as the basis for the following discussion.

Fig. 2 shows the normalized confusion matrix of all received an- swers. Other bugs are most frequently misclassified as Semantic bugs by participants. Recall is lowest for Concurrency bugs (0.56) and highest for Memory bugs (0.68). Precision is lowest for Semantic bugs (0.48) and highest for Other bugs (0.70).

Results: Fig. 3(a) shows the macro average F1 Bootstrap confidence intervals and mean performance of the two models. Artifact removal increases the classifier’s mean macro average F1 from 0.62 to 0.65. A one sided T-test of the models’ performance scores confirms that the model with artifact removal is better than the model without this

Results: Fig. 4(a) shows the macro average F1 Bootstrap confidence intervals and mean performance of the resulting models. The mean macro average F1 scores of the classifiers are closely clustered, 0.64 for MNB, 0.65 for SVM, 0.65 for RF, and 0.66 for LR. Only LR scores

However, investigating the models’ performance on specific classes shows that despite their similar macro average F1 scores, there are differences in their capabilities. Fig. 4(b) shows the F1 Bootstrap con- fidence intervals and mean performance of the models for each class. The most apparent difference can be observed in the performance of the LR and RF models on Memory and Other bug classes. The mean F1 performance of the LR and RF models are 0.69 and 0.65 for the Other class, and 0.74 and 0.79 for the Memory class. One sided T-tests on

Fig. 5(a) shows the macro average F1 Bootstrap confidence intervals and mean performance of the resulting ensembles and the on average best performing models from EXP2. The mean macro average F1 per- formances of the ensembles are closely clustered, ranging from 0.68 for Ensemble 1 to 0.69 for Ensemble 3. T-test on the ensembles’ scores

Fig. 6 shows the cumulative confusion matrix of Ensemble 1 col- lected from the 100 Bootstrap iterations. Semantic bugs are the ones most often confused with other categories, most notably the Other cat- egory. To investigate the reasons of misclassifications, we collected all test sets and their predictions from all Bootstrap iterations of Ensemble 1, yielding 22286 bug reports and their predictions. 7072 of which

We compared the length in characters and length in lines, for both the original bug reports, and the bug reports after artifact removal, as well as the lengths in characters and lines of removed artifacts. Further, we compared the number of occurrences of exception names in the bug tickets, as well as the number of bug tickets that contain such exception names. However, none of these metrics show any significant differences between the misclassifications and correct classifications, which can be explained by significant overlap of contained bug tickets in both correct and incorrect sets because of the small original dataset size of 496 bug tickets.

Therefore, we investigated the content of these bug tickets rather than statistical measures on their size. We created a vocabulary from all words included in these bug reports, using case folding and stop word removal, as well as removal of words containing digits and special characters. We counted in how many bug reports each word occurs. Fig. 7 shows the top twenty words for both groups. For the group of always correctly identified items, the corresponding bug reports are explicit especially regarding Memory and Concurrency issues, while the always incorrectly identified items are lacking such expressiveness.

Comparing misclassified bug reports against correctly classified ones does not show any statistical relevant difference in terms of document length or contents in terms of exception names or artifacts. However, investigation into the smaller subgroups of always incorrectly or always correctly identified items shows the latter group’s higher expressive- ness in terms of vocabulary that can directly point towards the bug’s fault type. As expected, Semantic bugs dominate the always incorrectly identified items, while Memory bugs are the most frequent type in the always correctly identified items.

Approach: We measure the performance of our machine learning approach in the scenario that training sets and validation sets are build from different software projects. We generated 100 validation and training splits by random selection of origin software projects from our dataset. These splits were chosen so that validation split size is between 18% and 20% (90–98 bug reports) of the full dataset and validation splits are roughly stratified (number of bug reports for each bug class within 0.5 of the mean of all four classes). The resulting test training splits are comprised of issue tickets originating from different software projects.

Fig. 8 shows the 95% percentiles and mean of macro average F1 scores as boxplots. Again, the mean macro average F1 performances of the ensembles are closely clustered, ranging from 0.71 for Ensemble 1 to 0.72 for Ensemble 3. One sided T-test on the ensembles’ scores shows that we cannot claim an increase of performance due to ensemble size

Analog to our analysis in RQ2, we investigated the bug tickets that were either always correctly classified (99 bugs), or always incor- rectly classified (24 bugs). The always correct classified bug reports are headed by the Other class (36 bugs), followed by Memory (30 bugs), Concurrency (29 bugs), and Semantic (4 bugs) classes. The always incorrectly classified bug reports are headed by the Semantic class (10 bugs), followed by Memory and Concurrency classes (each 5 bugs), and Other class (4 bugs). Manual examination of the always misclassified bugs revealed 6 bugs that we consider hard to classify and 6 that we consider impossible to classify with the given information; the remainder provide enough information and context. The impossible to classify bugs are e.g., reports on improper function that arose from wrong or improper documentation and were fixed as documentation corrections or enhancements, or reports on incorrect function where the root cause was missing UI resource files.

However, besides the discussed test splits, there is another signif- icant difference to our RQ2 experiments regarding the training splits: bug reports from netty are always in the training sets. It is possible that netty bug reports are important to the training process. Unfortunately, we cannot remove netty from the training set while maintaining bal- ance, because of its significant size of 84 items (including 40 memory bugs).

Answer: Examination of the classifiers’ performance on individual software projects shows a wide spread of performance scores, with certain projects performing extremely well (e.g., spring-framework), and others rather poor (e.g., n4js). However, our analysis and the small available sample sizes do not allow an a priori prediction of a project’s suitability for our approach.

The mean performance on 100 validation splits shows that the models are portable and robust, and that the performance results from RQ2 can be maintained in a cross project application. Our smallest ensemble models mean macro average F1 performance from all 100 validation splits is 0.71, our biggest models score is 0.72. Analog to our results from RQ2, these ensembles perform uniformly better than single classifier models (MNB, LR, RF, SVM, scoring 0.68, 0.65, 0.62, 0.65 mean macro average F1 respectively).

Further, hidden biases may reside in bug reports from any single software project. To counter this threat, we sourced this dataset from 71 different open source Java projects covering various organizations, software domains, and deployment targets, ranging from search engines and database applications to small Java libraries and mobile applica- tions. To validate our results and to demonstrate the transferability of our approach, we performed our experiments on test/training splits along software projects, ensuring that bug reports in the test set are from different software projects than the bug reports used in training. A threat to external validity is that our dataset contains only Java bugs. In addition, only open source projects hosted on GitHub were considered in this work. We can therefore not generalize our findings

We utilized labels on GitHub issue trackers to select candidates for our dataset. Labeling is performed manually by the software project maintainers, and is therefore subject to misclassifications. We counter this threat by excluding bug reports that we deem mislabeled, e.g., fea- ture requests wrongly labeled as bugs.

The manual classification performed by researcher 1 is also subject to misclassifications and therefore a threat to internal validity. To counter this threat, a second researcher independently classified a blinded random sample of the dataset, and researcher 1 re-classified a blinded random sample six months after the initial classification. We used these additional samples to calculate inter-rater agreement scores, to quantify the quality of our dataset.

The majority of participants for our survey are master degree stu- dents in computer science. These participants are to be considered non-experts, as they are not involved in the development of the soft- ware projects sourcing our datasets. Further, participants performed classification without provision of the original project context. The resulting scores are therefore on the lower end of human classification performance for the given task.

We have investigated human classifier performance on this task, followed by experiments using classical ML algorithms on this multi- class classification problem. The mean classification performance of non-expert human classifiers was rather low, with a mean weighted average F1 score of 0.62. The best single classifier model (Logistic Regression) has 0.66 macro average F1.

Our investigation into ML approaches highlighted advantages and disadvantages of certain NLP preprocessing steps and different ML algorithms. To exploit the gained insights, we used ensemble methods that combine multiple classifier models and preprocessing pipelines. Using such ensemble methods, we achieved mean macro average F1 scores of 0.69.

Not all types of bugs are equally hard to predict, and our models parallel the strengths and weaknesses of human classifiers: Memory bugs are the class with the highest classification performance for hu- mans (0.63 mean weighted average F1 score), as well as standalone classifier models (0.79 mean macro average F1 for Random Forrest clas- sifiers), and ensemble models (0.77 mean macro average F1). Semantic bugs constitute the class with the lowest classification performance for

Our results show that predicting the correct bug type solely on an initial bug report is difficult. The reasons for this are 1. the noisy nature of bug reports, containing various types of non-human language artifacts, 2. the extreme variety in resulting length and structures, or lack thereof, 3. the different view points and scopes of their reporters, e.g., an end user describing the impact of the bug in non-technical terms, or a senior developer delegating work to other developers— describing the problem in technical detail including the bug’s location and what actions have to be taken to fix the bug.

Our classification schema aims to provide a high level abstraction of debugging approaches and tools necessary for effective debugging of these fault types. Using this schema, we attempt to encapsulate a small portion of senior developer’s knowledge in machine learning models through learning from historical bug reports and their corresponding fixes. In this work, we identify the issues arising in such endeavor, and propose and demonstrate possible solutions for some of these issues.

While our results are promising when compared to our human classifier performance baseline, application of our models as debugging support in a production environment scarcely warranted at this stage and future research in this direction is needed. However, our approach in its current form can support researchers in their effort of creating bug benchmarks of specific bug types for experiments on specialized debugging tools. Our datasets and implementations are made publicly available on GitHub.8 and Zenodo9 We hope that other researchers benefit from our detailed investigation and the provided artifacts.

Li Z, Tan L, Wang X, Lu S, Zhou Y, Zhai C. Have things changed now?: An empirical study of bug characteristics in modern open source software. In: 1st Workshop on architectural and system support for improving software dependability. 2006, p. 25–33. http://dx.doi.org/10.1145/1181309.1181314.

