Initial weight choice is an important aspect of the training mechanism for sigmoidal feedforward artificial neural networks. Usually weights are initialized to small random values in the same interval. A proposal is made in the paper to initialize weights such that the input layer to the hidden layer weights are initialized to random values in a manner that weights for distinct hidden nodes belong to distinct intervals. The training algorithm used in the paper is the Resilient Backpropagation algorithm. The efficiency and efficacy of the proposed weight initialization method is demonstrated on 6 function approximation tasks. The obtained results indicate that when the networks are initialized by the proposed method, the networks can reach deeper minimum of the error functional during training, generalize better (have lesser error on data that is not used for training) and are faster in convergence as compared to the usual random weight initialization method.

random values in the interval [-į, į] (where usually į İ (0,1]). Training of SFFANN has been shown to be sensitive to initial weight choice [1]. The random initialization of weights was proposed by Rumelhart et al. 1987 [2]. They observed that if weights are initialized to equal values they move in tandem/groups during training, and to break this “weight-symmetry”, the random weight initialization method was proposed. Moreover, it has been suggested in literature that hidden nodes act as feature detectors [3]. Thus, it is desirable that each hidden layer node act as a detector of a separate/distinct feature.

Thus, it becomes meaningful to initialize the weights leading into the hidden node (including the threshold of the node), in a manner that for distinct hidden nodes, the weights and thresholds belong to distinct region of the initialization interval. This would lead to the net input to distinct nodes being different by design, and during training allow the nodes to adapt to become detectors of distinct features. This, hypothesis is enshrined in the proposed weight initialization method. The proposed method is compared against four random weight initialization methods (specifically, į = 0.25, 0.50, 0.75 and 1.00), on a set of 6 function approximation tasks.

The hyperbolic tangent function is an anti-symmetric function, and is used as preference has been shown in literature for anti-symmetric function as compared to asymmetric activation function [7]. The schematic diagram of the networks used in this study is represented in Fig.1. The number of inputs to the network is represented by I, the number of nodes in the hidden layer (such a node is called hidden node) is represented by H, and the number of outputs of the network is taken as 1. The jth input is represented by xj and the output of the network is represented by y. The connection strength between the ith hidden node and the jth input is represented by Ȧij, the threshold of the ith hidden node is represented by și, then the net input to the ith hidden node is given by:

The number of hidden nodes was decided on the basis of exploratory experiments conducted in which the number of hidden nodes was varied between 2 to 30 in steps of 1, for 100 epochs of training. The first network of the minimal size that gave satisfactory error during training was taken as the appropriate size for the experiment. The architecture of the networks used for the approximation of the 6 functions is summarized in Table 1.

The proposed method is called the Interval Based Weight Initialization Method (IWI). The method IWI distributes the weights leading in to the ith hidden node in the interval [(2i-1)/(H-1), (2i+1)/(H-1)] (as uniform random numbers in the given interval). While the threshold of the ith hidden node is initialized to 2i/(H-1). The hidden nodes to the output node weights are initialized to deterministic values between [-C,C] where C = H-1/2; that is, for the connection weight between ith hidden node and the output node the weight is Įi = -C + 2iC / (H-1). A similar mechanism, but in a random manner, for the weight initialization of weights based on the fan-in to a node is suggested in[7].

The Resilient Backpropagation algorithm is used for training the network. For the purpose of training 200 input data sets are generated by uniform random sampling of the input domain of the function, and the corresponding output calculated from the function to create the training data set. For testing the generalization capability of the trained network(s), a similar set with 1000 data values is generated and called the test set.

Another set of experiments is performed to measure the convergence speed during training, wherein a goal equal to twice the worst MMSE achieved in the previous experiment is kept as a goal of training, and the number of epochs required is measured. Since the maximum epoch of training is kept at 1000, if a network does not converge during training, its epoch value is kept at 1001 (arbitrarily). This creates a small bias in the mean epoch value, but the number of non-convergent networks is also counted and reported for each instance of function approximation task and weight initialization method.

The result of the training experiment and the generalization experiment is summarized in Table 2. From the Table 2, it can be seen that among the random weight initialization methods (WTRs), there is no single method that gives the best result across the function approximation tasks. It can also be seen that the proposed weight initialization method (IWI) always leads to training and generalization errors that are smaller than any of the random weight initialization methods. We may infer that the proposed method of weight initialization leads to lower value of error after training on an average and up to a factor of 2 deeper minima of the error functional can be achieved on training by the proposed method (IWI), as the variation in the ratio of the best result for the WTR for a specific function approximation task to that of the proposed method lies in the interval [1.12, 2.86].

Moreover, from the generalization experiments we may infer that the networks trained after initialization by the proposed method, have better generalization behavior. That is, the error on data not used for training is lower for networks initialized by IWI. For the generalization experiment, the ratio of the best result for the random weight initialization method (WTR) to the result for the proposed method (IWI) lies between [1.08,2.71] across the function approximation task. Thus, for the generalization experiment also, the proposed method has error on an average across problems that is lower by a factor of about 2 for networks trained using the IWI method.

The summary of the experiments conducted for measurement of speed of convergence is shown in Table 3. From the data, we may infer that the proposed method leads to faster convergence in all case. No single random weight initialization method out of the 4 WTRs can be preferred on convergence speed, as for different function approximation task, a different random weight initialization routine may give better results. The ratio of the minimum average epochs required by any of the 4 random weight initialization method (WTR) to the average epochs required by the proposed method (IWI) for convergence to the specified goal lies in the interval [1.00, 3.19]. Thus, we may infer that the networks trained after initialization by the proposed methods (IWI), have a convergence speed that can be faster by a factor of about 2 as compared to the random weight initialized networks on an average..

Table 3. Convergence Speed Experiment Summary. The goal represents the goal for convergence speed experiment. For the convergence speed experiment the average epoch for convergence is shown the Statistic column. The number of non-convergent networks is also shown as NCN. Goal and the Statistic figures are × 10-3.

In the current work, a proposal for distribution of SFFANN input to hidden weight and thresholds of the hidden weight is made in a manner such that these weights associated with distinct hidden nodes lie in disjoint intervals. On a set of 6 function approximation task, the efficiency and efficacy of the proposed method is demonstrated. That is, networks that are initialized by the proposed method, can be trained to achieve deeper minima as compared to random weight initialization method; they generalize better and are faster in training.

