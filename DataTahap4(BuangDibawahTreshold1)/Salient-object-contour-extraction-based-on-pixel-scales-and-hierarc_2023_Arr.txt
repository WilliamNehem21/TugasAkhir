In order to extract object contour pixels in convolutional neural network, the image pixels confront with classification problems. The pixel scales are classified by piecewise quantization. The number of categories depends on the sizes of receptive fields in the network structure. For example, from the first stage to the fifth stage of the VGG16, as the extracted features become more and more abstract, the

Contour pixel scale piecewise quantization: As shown in Fig. 6(c), the scale of a contour pixel refers to the radius of the maximum inscribed circle that is centered on the skeleton point and is tangent to the contour of the object. In other words, the scale is the distance between the skeleton point and the contour point of the object that is closest to the skeleton point.

corresponds to the sizes of receptive fields with 5, 14, 40, 92, and 196, respectively [19]. According to the ranges of different receptive fields, the contour scale quantization category table is able to be obtained in Table 1. For example, the second category of contour pixels represents the scales of pixels between the range of [12, 26). In Fig. 6(d), the contours with different scales are shown in four kinds of colors, and they are red, yellow, blue, and green from big scales to small scales. Then, the corresponding skeletons are shown in four lighter colors.

/ig. 6. The illustration of contour pixel scale. (a) is the contour of the horse. (b) is the skeleton of the horse. (c) includes the contour and skeleton. The red circle indicates the maximum inscribed circle from the skeleton pixel to the contour, and the green scale of the point of tangency on contour pixel is the radius of the inscribed circle. In (d), the contours with different scales are shown in four kinds of colors, and the corresponding skeletons are shown in lighter colors. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

The normalization of contour scales and loss functions in the regression task: The scale of a contour pixel is a real number that can be predicted in regression tasks. In order to improve the prediction ac- curacy, a normalization method of contour scale is proposed in training steps. The network proposed in this paper consists of five stages, and each stage has independent regression tasks to predict the scales. Noted that there are different numbers of regression tasks at different levels. For example, the H1 level has five regression tasks and the H2 level has four regression tasks. The regression task is used to predict the contour scale at different stages, depending on the size of receptive fields. Therefore, it is necessary to establish the correspondence between the ground truth of contour scales and the values of receptive fields at different stages during the training stage, as shown in Fig. 8.

/ig. 8. The relationship between the ground truth and the sizes of receptive fields. In H2 layer, if the range of receptive fields of ‘‘LR Task1’’ is [0, 14), the size of the field equals to 14; if the range of receptive fields of ‘‘LR Task2’’ is [0, 40), the size of the field equals to 40.

as [𝑆𝑙𝑜𝑤, 𝑆ℎ𝑖𝑔ℎ). For example, if a pixel belongs to category 2, that means tion of Section 3.2, the coarse range of pixel scales can be represented the scale of this pixel ranges at [10, 26), where the value of 𝑆𝑙𝑜𝑤 is 10 and the value of 𝑆ℎ𝑖𝑔ℎ is 26. Combining the predicted values of the

(i.e., 𝑖 = 1), that is to say only one image for one batch is selected. 𝑌𝑖 in one batch of the training dataset. If the parameter of batch-size is 1 represents the total number of pixels in the 𝑖-th image. If the height of the image is ℎ and the width of the image is 𝑤, then the value of 𝑌𝑖 is ℎ × 𝑤. 𝑌𝑖𝑗 represents the number of pixels belonging to class 𝑗.

The HED originally exploits ‘‘side output’’ convolution, which greatly improves the edge extraction accuracy and computational efficiency. Then, the RCF further enriches edge features by fused convolutions. Next, the DFI adopts dynamic feature integration to optimize edge features, which is a state-of-the-art edge extraction algorithm. In this paper, some improvements are presented on the basis of HED and RCF network structures, and we optimize the object contours using pixel scale features.

In experiment procedures, firstly, the contour maps are generated based on the trained models. Then, the refined contours are obtained by the standard non-maximum suppression algorithm [29]. Finally, the refined contour maps are evaluated by PR curves and F-measures. In addition, the performance of the proposed algorithm is verified through comparative experiments on three datasets, and the generalization ability of the new algorithm is proved by cross validation.

The contour extraction accuracy results of several different network structures are compared in detail. Generally, the VGG16-based hierar- chical network maximally has four levels. In the experiments of contour extraction, the methods of different levels are named as Ours1, Ours2, and Ours3. It is verified that for the proposed method, when the level exceeds 2, the accuracy will be reduced, so the Ours3 is adopted at

most. For example, when two levels are used, they are called Ours2. Due to the limitation of the GPU memory, some higher levels of the proposed method cannot be trained. In addition, in order to verify the influence of the variable coefficient loss function and the regres- sion task on contour extraction, relevant independent experiments are also carried out. Assuming that the Ours1A indicates the method of Ours1 with the variable coefficient Softmax loss function method. The Ours1AR indicates the method of Ours1A added with regression task. The definition of other symbols can be analogized. The Table 2 lists the F-measure values of the contour accuracy produced by different network structure experiments.

Tables 5 and 6 list the F-measures of the proposed method com- pared with HED and RCF methods on SK-LARGE, SK-SMALL, and WH-SYMMAX datasets. It is worth noting that if the weighted loss function and the regression task are not used (i.e., Ours1 and Ours2), the performance on the SK-SMALL dataset is slightly worse than that of the RCF method. If only one level and the weighted loss function are used without regression tasks (i.e., Ours1A), the performance on the SK-LARGE dataset is 3.05 percentage points, which is lower than that of the RCF method.

The PR curves in Fig. 10 show that the proposed method is much better than the HED and RCF methods. Specifically, Ours2AR has the best results on SK-LARGE and WH-SYMMAX datasets. The Ours2A is slightly better than the Ours2AR on SK-SMALL dataset, with the F-measure values of 0.715 and 0.711, respectively.

Some typical salient object contour extraction results on SK-LARGE dataset are shown in Fig. 11. From Fig. 11, it can be found that the extracted results of HED and DFI methods contain some non-contour details and background information. In addition, the results extracted by the RCF method lose some contour information. While the proposed method avoids these problems and achieves better results.

other contour extraction methods in terms of salient object contours. Moreover, the salient object contours obtained by the proposed method contain scale information, which can be applied to the subsequent skeleton generation and salient object generation. As a prior knowledge in training, it is also helpful to improve the model performance.

In the future work, the new approach will be extendedly optimized in two possible manners. Firstly, a more effective network structure can be designed for object recognition. Secondly, multi-regressive aiding tasks like classification tasks can be merged into the network, which are not appeared in existing methods. After the fusion step, the regression data can be used to estimate more accurate inscribed circle radiuses between the contour and the skeleton. Finally, the contours are limited to five categories, which reduces the contour accuracy, so better results may be generated by increasing scale categories.

Shen W, Zhao K, Jiang Y, Wang Y, Zhang Z, Bai X. Object skeleton extraction in natural images by fusing scale-associated deep side outputs. In: Proceedings of the IEEE conference on computer vision and pattern recognition. Las Vegas, NV, USA; 2016, p. 222–30.

