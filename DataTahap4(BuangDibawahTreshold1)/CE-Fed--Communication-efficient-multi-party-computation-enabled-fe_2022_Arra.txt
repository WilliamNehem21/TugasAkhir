Federated learning (FL) allows a number of parties collectively train models without revealing private datasets. There is a possibility of extracting personal or confidential data from the shared models even-though sharing of raw data is prevented by federated learning. Secure Multi Party Computation (MPC) is leveraged to aggregate the locally-trained models in a privacy preserving manner. However, it results in high communication cost and poor scalability in a decentralized environment. We design a novel communication-efficient MPC enabled federated learning called CE-Fed. In particular, the proposed CE-Fed is a hierarchical mechanism which forms model aggregation committee with a small number of members and aggregates the global model only among committee members, instead of all participants. We develop a prototype and demonstrate the effectiveness of our mechanism with different datasets. Our proposed CE-Fed achieves high accuracy, communication efficiency and scalability without compromising privacy.

The remainder of this paper is organized as follows. Section 2 describes the background and related work. The CE-Fed framework is presented in Section 3. The experimental analysis and performance evaluations are presented in Section 4, followed by conclusions in Section 5.

has no control over the data as well as the constructed machine learning model, since both are no longer residing at the premise of the data owner. There is a possibility that the centralized service providers can obtain extra revenue on the data as well as on model and use them for some illegal purposes. This cannot be prevented by data owners.

Federated Learning (FL) has been extended to the collaborations across multiple organizations. It is categorized to horizontal FL and vertical FL [16], based on the data distribution over the sample and feature spaces. In horizontal FL, datasets of different organizations like different industrial organizations, have the same features but different sample sizes. In vertical FL, different organizations like banks, or insurance companies that are located with the same city, have collected data with different features.

Secure MPC is a privacy preserving technique that allow for secure computation over sensitive data. It was firstly introduced by Yao in 1986 [22]. Garbled circuits and secret sharing are the two dominated MPC techniques [23] followed today. The secret sharing is a commonly used MPC protocol. It splits the sensitive data into secret shares. The original data is obtained from the combination of these secret shares. The clients cannot learn anything other than final output.

In federated learning, the model parameters and gradients are shared with the server for model aggregation. There is a possibility that the malicious user can intercept the model parameters and could per- form reverse engineering to extract the sensitive data, while it is shared with the server on federated learning [24,25]. To address this issue, Multi party computation methods like additive secret sharing [26] or Shamir secret [27] sharing can be used to encrypt the gradients/model updates before performing the aggregation so that no one will be able to see the gradients.

In our earlier work [38,39], we proposed a two-phase MPC-enabled FL framework to improve scalability. We have carried out preliminary work wherein the committee members are selected randomly. Ran- dom selection may not be efficient as the parties are geographically distributed which could have impact on latency. Further, we consider MPC enabled FL with hierarchical model aggregation and carry out new performance experiments and analyse the results with different datasets in this work.

The proposed CE-Fed selects a few clients as the committee mem- bers who use MPC service to aggregate the local models of all FL clients in a hierarchical manner. Therefore it avoids sharing the model parameter from each client to everyone else in the FL list. Our proposed CE-Fed is executed in two phases. In the first phase, we group the FL clients that are located close to each other. The local models of all FL

clients in the same group are securely aggregated using MPC to form a intra-group model. Based on the latency, one client is elected from each group to form the aggregation committee. In the second phase, the committee members work together to aggregate the inter-group models using MPC. It has three modules:

After grouping the clients, one client from the group is elected as a leader. Instead of letting everyone to share secret shares to every other one, we choose one representative from each group to communicate with other groups. This reduces communication overhead. The group

split into multiple tensors as the secret shares based on the number of clients in the same group. Each client holds one share and exchanges the remaining shares with all the other participating clients in the same group. Then, clients perform secure model aggregation of the secret shares using MPC to obtain their intra-group model.  After learning a

The optimized intra-group models from different groups are finally aggregated by the model aggregation committee members to form the inter-group global model. Each aggregation committee member splits the intra-group model received from different group leaders into m se- cret shares using MPC. It holds one share and exchanges the remaining shares with all the other committee members. The committee members work together to perform inter-group global model secure aggregation using MPC and broadcast the aggregated global model to all the group leaders in various groups. Then, the group leaders broadcast the inter- group model to their followers in their group. The whole process of intra-group model aggregation and inter-group model aggregation are repeated until the model converges. The pseudo code of inter-group model aggregation is shown in Algorithm 3.

To implement and evaluate the effectiveness of our proposed CE-Fed framework, we used PyTorch 1.2.0 and Python 3.74 as machine learn- ing library. We considered three public image datasets, MNIST [44], CIFAR-10 [45] and Fashion-MNIST dataset [41]. We used the CNN

output layer. The model has about 600,000 trainable parameters. In our experiments, we study the performance and effectiveness in using IID (Independent and identically distributed) and non IID datasets. In the IID distribution, the data is shuffled and partitioned across all the clients, whereas in the non-IID distribution, first the data is sorted by the label and then partitioned across the clients in such a way that each party has fixed number of labels and there is no overlap between samples of different clients. To eliminate the randomness caused by client sampling, all clients are participated in each and every round of training. The learning rate is set to 0.01, local epoch number is set to 5 and batch size is set to 64. The stochastic gradient descent (SGD)is used as optimizer. We consider 4 groups and use FedAvg for model aggregation. We use Accuracy and communication cost as performance metrics in our experiments.

using local datasets of individual clients. Centralized training trains the model on centralized data. Experiment results shows that the test accuracy of federated learning is comparable with centralized learning and outperforms local training. It has been observed that FL training achieves an improvement in accuracy of 22%, 33%, 17% on MNIST, CIFAR-10 and Fashion-MNIST respectively when compared with local training. The key reason for the low accuracy in local training is that the size of the local dataset is not large enough.

the communication cost incurred in MPC protocols and also aggregates the models in a privacy-preserving manner without compromising the accuracy. The effectiveness of our proposed CE-Fed framework was demonstrated on various datasets. The above experiments indicates that our proposed CE-Fed is able to reduce the communication cost significantly while achieve the similar accuracy and privacy.

Renuga Kanagavelu received the Ph.D. degree in computer science from Nanyang Technological University, Singapore, in 2015. She is a scientist at the Institute of High- Performance Computing, A*STAR, Singapore. Her research interests include Federated Learning, Privacy-preserving techniques, Industrial IoT and Edge Analytics.

Qingsong Wei received the Ph.D. degree in computer science from the University of Electronic Science and Tech- nologies of China, in 2004. He was with Tongji University as an assistant professor from 2004 to 2005. He is a Group Manager and senior research scientist at the Insti- tute of High-Performance Computing, A*STAR, Singapore. His research interests include decentralized computing, fed- erated learning, high performance computing, emerging non-volatile memory and storage system. He is a senior member of the IEEE.

Zengxiang Li is Executive Vice President of Digital Research Institute and Director of Collaborative Intelligence Lab, ENN Group, Beijing, China. His research group is focusing on Industrial IoT, Blockchain and Federate Learning research and development for trusted ecosystem across multiple in- dustry domains, including smart energy, healthcare, and city governance application domains. He completed his Ph.D. in Nanyang Technological University, Singapore in 2010.

Shangguang Wang is a Professor at the School of Com- puting, Beijing University of Posts and Telecommunications, China. He is a Vice-Director of the State Key Laboratory of Networking and Switching Technology. He has published more than 150 papers, and his research interests include service computing, cloud computing, and mobile edge com- puting. He served as General Chairs or TPC Chairs of IEEE EDGE 2020, IEEE CLOUD 2020, IEEE SAGC 2020, IEEE

