Features are extracted to perform the feature transformation and se- lection for integration. The selected features are semantically associated to classify using the pattern generated through SDRL. Each data records consists of few key data fields such as author, title, keywords and abstract known as metadata of the article. The terms are extracted from the keywords and abstract field to construct a set of terms which will use for the semantic classification to classify the article class for the integration. Semantic classification performs relevance association computation to relate the latent semantic relevancy [28]. Since, the article is technical information, the semantic-based classification [29] will compute the similarity between the set of terms information constructed to identify

them in scalable, big data-capable data storage. Different architectures for data acquisition have been proposed to address the different char- acteristics of big data. Most of data acquisition scenarios assume high- volume, high-velocity, high-variety, but low-value data. This makes adaptable and time-efficient gathering, filtering, and cleaning algorithms as a need to ensure that only the high-value fragments of the data are actually processed for the analysis.

Few techniques [21] are discussed for classification. But The func- tionality of a Naive Bayes Classifiers [25] is completely independent from the class attribute values variations, which generally being termed as Condition-Independence. This approach is constructed based on the assumption for the simplification and generalization of class information in relation to the Naïve Bayes probability assumptions. The constructed class information symbolizes the characteristic of the class attribute relation which supports the classifier in accurate classification. It was experienced that "Bayes approach" is useful in an assured circumstances and it is extremely reliant on the hypothesis of the target classification information for the proficient outcomes. Because of high dependency, a little divergence in the supposition hypothesis constructs a set of inac- curacy in recognition. In the proposed semantic classification method,

[29] as per work done in 2018, where co-occurrence features can be acquired by the transfer relations between the records. The co-occurrence information of the terms can be captured when Singular Value Decomposition (SVD) is decomposed as proposed by Refs. [30] in the year 2019.

occurring in more than one title are extracted. There are two classes of records as, "Computer Interaction (I)" and "Data Mining (M)". This dataset can be described by means of a term by document matrix where each cell entry indicates the frequency with which a term occurs in a record.

The similarity of 0 is between some disintegrated terms, that means there is no or little relations. By the changes of the similarity value, one can think "user" and "interface", "interface" and "human", "user" and "human" co-occur. In LSI [31], the "user" and "human" projected to the same dimension space. Comparison of the feature matrix between the similarity matrix transformed to SVD is shown in Table 4.

The degree of similarity in features reflects the correlation between the terms. The weight value not only reflects the correlation in the fea- tures but also embodies the co-occurrence information between the features in SVD space. The similarity degree in the documents mainly depends on the number of the co-occurrence of features. In generating latent semantic space, the latent relations will be excavated because of the transitivity between the features.

User data access model is designed to evaluate the proposed inte- gration and indexing mechanism. It consists of a user interface to submit a query for mining and visualization of Bibliographic data of the tech- nical article. The search query retrieves the most relevance articles as results and its information as shown in Fig. 4 and Fig. 5.

The aim of Fig. 4 is to show the search result by proposed method. Fig. 5 shows the first page search result along with total number of pages to follow. It also shows total record count of matching through search method supported by total time taken for search operation. User can click on page centric view link provided in front of each search result.

A visualization analysis of data records in relation to the author is presented by an article centric view as shown in Fig. 5. This is detail of page link from previous page (as shown in Fig. 4). It provides details of particular search result, i.e. author names, related article title and key- words. It also gives details and reference of related article with respect to keywords.

The experimental work was carried out over Hadoop Framework for storage the big data. Processing and analysis was performed using Java technologies. The data access visualization is evaluated through a web interface provided by Apache Tomcat Web server. The SDRL mechanism is implemented to generate the required knowledge pattern for integra- tion. Later, the obtained data sources are cleaned and exported to Hadoop for storage.

The Java program performs the integration through semantic classi- fication of data record with the help of knowledge pattern. After completion of integration, the indexing method F-LSA is executed to rank the integrated data group. To evaluate the improvisation in the indexing, a Data Access Visualization interface is built, where multiple queries are submitted to measure the accuracy of the outcome.

Authors assume that each article has keywords after abstract which will be extracted as features along with features from abstract. Title of article is preprocessed to extract keywords from it, if list of keywords are not given after abstract in article. Number of terms and documents are initialized based on selected dataset to find/extract features using pro- posed method to find article. The system configuration used to perform experiment is I5 machine with 8 GB of RAM.

Purity Metric: The evaluation of a measure of purity is a simple and transparent. Each individual cluster is allocated to the class determined by dividing the number of including the number of objects in the preci- sion of the specified object and this exact allocation, and most frequently in the cluster data sets to calculate the purity [34]. The number of clusters is easy to achieve a high purity as shown in equation (5). Therefore, trade-off quality cannot be used for the purity of the number of clusters.

NMI Metric: The measure of NMI metric offers a number of inde- pendent information in the cluster. The measure takes a maximum value [15], when this integration to partition completely consistent with the original partition. The average mutual information of NMI is computed between a pair of clusters and the individual class as shown in equation-6.

Many previous studies have only been used to analyze the purity metric and evaluate the clustering algorithm performance. However, when a larger number of clusters are available for the integration, it is easy to achieve purity measure. Particularly if each object has its own data clusters purity measured as 1. In addition, many of the partitions have the same purity as they are associated with each other. For example,

An experiment result analysis is provided for the integration and indexing through comparing the outcomes in their normal and with the proposed method. In the Normal analysis form, data integration is made simply based on it terms related to the class. In case of proposed inte- gration the integrated data clusters is analyzed against the class articles through measuring it purity and NMI.

The analysis of the integration performance in terms of purity mea- sure and NMI measure is presented in Figs. 6 and 7 respectively. The proposed PFP shows an average of 8% higher purity compared to the existing semantic based associating approaches over the records.

In case of NMI analysis the mutual information is measured, which is shared between the data record and the learned article class terms in proposed PFP and the existing semantic based associating approaches for the integration. Result shows the decreasing in NMI variation in case of TF, Support Vector Machine and Naïve Bayes due it has high number of independent terms with increase number of data records. But the pro- posed PFP probability of relevance of terms with article class increases with the number of mutual information and supports in achieving better NMI in comparison. The SDRL approach makes to relate the anonymity records more precise in order to achieve better purity and NMI in comparison.

The indexing of data helps to retrieve information much faster and accurate. To have an analysis of the indexing approach "Precision", "Recall" and "Accuracy" of the indexed data is measured through an user accessing model. Evaluation of proposed F-LSA based indexing method with TF and LSI based indexing for the integrated data according their article classes is carried out. The outcome of the indexing analysis results are shown in Figs. 8–10 respectively.

The analysis efficiency and ability of the proposed PFP based inte- gration through semantically classification over a Naïve Bayes classifier with F-LSA based indexing (PFP with F-LSA) over the heterogeneous sources is carried out. Evaluation is carried out with the three digital bibliography datasets: "CiteSeerX", "BibText" and "Cora" with the Normal form of integration with LSA based indexing (Normal with LSA). Effi- ciency is tested by submitting few queries to the designed used data ac- cess interface such as, "Software engineering", "Artificial Intelligence" and "Data Security" on the three datasets. Over 100 top retrieved results are analyzed for the query to find the number of true result, number of associated results and total number of related results from the number of search results.

Figs. 11 and 12 show the precision and recall performance for the proposed PFP with F-LSA and Normal with LSA. The obtained result of the proposed PFP with F-LSA shows an average of 13.2% of improvisa- tion in the precision and 10.5% of low recall in compare to the Normal with LSA. The improvisation achieved due to the retrieval of more number of related results, which suggest the improvisation of the inte- gration. The total number of true results suggests the accuracy of the indexing. In case of accuracy measure, an average of 11.8% of improvi- sation is observed due to high number of associated result retrieval as shown in Fig. 13.

Purpose of indexing the records is for faster access and quick retrieval. Accuracy depends on indexing as it depends on feature representation. Good/unique features results in better accuracy. Good features reveal data clearly and any method depends how quality features are extracted for indexing.

Integration and indexing are two principal functions needed to organize and retrieve data faster in today’s big data environment. Hence Implementation is in two folds. Initial line of work is a PFP approach for the efficient integration through semantic classification over a Naïve Bayes classifier. Further an indexing by F-LSA method is implemented on

The selection of features and semantic analyzing is able to enhance the integration in big data. A process of semantic data relation learning (SDRL) method is discussed to learn k-features related to collection of data. These features are analyzed to predict a one-feature class of a data. Further effective grouping is done for Integration. The process of indexing utilizes the latent semantic analysis (LSA) method to under- stand the degree of similarity between features. The correlation between the terms of data records ranks appropriately for indexing.

The integration method associates the features semantically to clas- sify the data record using learned patterns. Further a feature LSA among the integrated class data is implemented to construct the indexing of the integrated data structure. The user access model is presented to evaluate the effectiveness of the integration and indexing based on bibliographical data which includes various technical article published by well known publishers in different domains. Further Performance analysis of two different combinations i,e “PFP with F-LSA” and “TF with F-LSA” is presented. The effectiveness and efficiency of PFP with F-LSA is proved for different data set like Citeseer and Cora apart from Bibtext. There is improvisation in terms of precision and accuracy during indexed data retrieval. Dynamic construction of index during query processing can be carried out in future to minimize the complexity of index column storage.

Madhu Mahesh Nashipudimath would like to thank Principal and Management of Pillai College of Engineering, New Panvel, Navi Mumbai, India for their continue support and cooperation during this research work. Special gratitude to Dr. Satishkumar Varma for his constructive criticism and suggestions.

