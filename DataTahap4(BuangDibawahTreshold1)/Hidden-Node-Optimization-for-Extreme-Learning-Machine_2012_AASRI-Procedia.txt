The number of hidden nodes is a critical factor for the generalization of ELM. Generally, it is heavy for time consumption to obtain the optimal number of hidden nodes with trial-and-error. A novel algorithm is proposed to optimize the hidden node number to guarantee good generalization, which employs the PSO in the optimization process with structural risk minimization principle. The simulation results indicate our algorithm for the optimal number of hidden nodes is reasonable and feasible with 6 datasets on benchmark problems by the accuracy comparisons.

Extreme Learning Machine(ELM)[1], a novel learning algorithm with fast learning speed and good generalization, is a single-hidden-layer feed forward neural networks(SLFNs). As traditional SLFNs, ELM tends to achieve a good generalization with the suitable hidden nodes. EM-ELM[2] pointed that how to choose the optimal number of hidden nodes is still unknown and important. RCGA-ELM[3] employs genetic algorithm (GA) to optimize the number of hidden nodes, input weights(w) and bias(b) in five-fold cross- validation procedures. It is so complex and time-consuming by involving multiple operators. PSO-ELM[4] optimizes only the weight w and bias b, illustrating their value ranges, Root mean square error(RMSE) as the

optimization goals and variance(δRMSE) as the iterative stop criteria. ICGA-SRM-ELM[5] only considers RMSE and hidden nodes, doesn't give a specific form of the solution. Miche[6] presented OP-ELM to choose hidden nodes automatically. However, it needs Multi-response Sparse Regression and Leave-One-Out algorithms to get rid of the useless neurons of the hidden layer, which has to employ multi-certeria mechanism to increase or decrease the hidden node, tending to be more complex. µG-ELM[7] as another solution to optimize the hidden nodes by GA is easy to overfit with RMSE as the only training goal. Those previous works have made some attempts to improve generalization of ELM, but existed some shortcomings like overfitting, heavy time consumption or without specific form of the objective function.

SRM-ELM (Structural Risk Minimization ELM, SRM-ELM) algorithm is proposed in this work to obtain an optimal number of hidden nodes for ELM by PSO, with Structural Risk Minimization (SRM) principle that consist of empirical risk and VC confidence. The most superiority of SRM-ELM is to avoid the overfitting by introduced the VC theory. Moreover, PSO chosen as the optimal tool will reduce the operation time compared with GA or DE (Differential Evolution) algorithm.

where n is the sample number, 0  a≤4 0  b≤2, η  (0,1],and B=1 in binary-class issues. The second summand on the right hand side of Eq.(5) is called VC confidence, whose value depends on VC-dimension h in the case of given samples. According to SRM, minimal R(a) will be obtained by minimizing right hand side of Eq.(5).

VC-dimension h can be used as a measure of the computational complexity for machine learning. A good VC-dimension can improve the generalization of neural network. But, there is not a universal formula to calculate h for neural network. VC-dimension h was deduced with some formulae for the feedforward network with sigmoid activation function[10-12],

where λ is the number of weights in network, l is the number of layers and n is the sum of hidden nodes and output nodes. Some attention could be took on Eq.(7) that h has some relation to λ, and λ also has some relation to the total number of nodes. So, Eq.(7) suppose h can be obtained from the total number of nodes in network. Here, we consider the total number of nodes as the VC-dimension h for ELM. h can be written as,

where η0  (0,1]. We do simulations on binary-class issue Haberman from UCI database to evaluate  (h) in Fig.2. Obviously,  (h) is a concave function, but ƒ (h) is a convex function. So Eq.(17) can be used as the objective function, which consist of Remp and  (h) .

Fig.3 is the flow of SRM-ELM algorithm. After initialize swarm population, the position value p is the hidden node number L, which is the key link between PSO and ELM. p and v will update in every iteration. If NC  itmax , output the optimal number of hidden nodes, else repeat the flow after initialization.

Parameters setting for PSO: population size: n=50, maximum iteration: itmax=30, learning factors: c1= c2=2, dimension: d=1, the maximum and minimum value of inertia weight: τmax=0.9, τmin=0.4, range of the position(number of hidden nodes): [Xmin,Xmax]=[1,300], range of the velocity: [Vmin,Vmax]= [-(Xmax-Xmin),(Xmax- Xmin)]=[-299,299]. Parameters for ELM: choose the sigmoid for the activation function. Parameters for

This work proposed a novel algorithm to optimize the number of hidden nodes for ELM by SRM and PSO. We modified the formula for the VC confidence to reconstruct a concave function for SRM as the objective function. Then we employed PSO to optimize the SRM function for the optimal number for hidden nodes for ELM. The experiment results demonstrate that our algorithm can be used to obtain the effective number of hidden nodes and an excellent generalization.

