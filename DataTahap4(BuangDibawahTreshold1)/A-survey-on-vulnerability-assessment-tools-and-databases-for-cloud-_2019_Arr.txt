The cloud computing success has also led to raising the abstraction level via the supply of extra service types above the infrastructural ones. In particular, platform services are offered to reduce the burden in creating and managing the execution environments within the leashed resources. Further, the scope of such services has been also extended to facilitate cloud application design and development.

While migration of applications to the cloud continues with a fast pace, there are two prohibitive factors that make organisations hesitant to perform it. The first factor is the lack of standardisation, as each cloud provider offers its own APIs and tools to support application development and provisioning. Thus, adopting one cloud provider leads to a lock-in effect as the application owner becomes bound to that provider and cannot easily migrate to a new one, when, e.g., new business opportu- nities arise. Fortunately, various frameworks have been proposed to confront this lock-in issue, which enable applications to move from one cloud to another, thus becoming multi-cloud. Avoiding this lock-in effect introduces greater flexibility as there is plenty of space to optimise ap- plications by exploiting in a combined way those cloud services that more optimally satisfy the application requirements.

The second migration factor is security. By giving more control on third-parties, which might not offer services with suitable security gua- rantees, makes organisations to be reluctant to move to the cloud. Many applications already handle private or sensitive data which will never be moved to the cloud as long as these guarantees are not satisfied. Further, privacy laws might also forbid data movement from certain areas. Yet, a

Virtualisation also impacts security as physical hosts are shared by users in the form of virtual machines (VMs). This can raise issues where malicious users can see the data stored in a VM or take control over all VMs by attacking the hypervisor. Further, as applications are distributed over public clouds, networked attacks can be performed over them which can, e.g., cause crucial information to be leaked during transmission or application overload based on denial-of-service attacks.

Security services can be of a different nature, while they be also reactive or preventive. Reactive services can detect an attack and possibly address it. For example, intrusion detection and protection systems can be used to protect applications from network attacks by, e.g., detecting denial-of-service attacks and blocking their origin IPs. Preventive ser- vices prevent a security incident from happening by detecting application areas that are vulnerable to certain attacks. The latter service kind can take different forms.

Vulnerability scanning tools enable to detect vulnerabilities in different application parts and kinds. Static (code) analysis tools can find code bugs exploitable by security attackers. Audit tools can be utilised to find well-known rootkits, Trojans and backdoors as well as to unveil hidden processes and sockets. Finally, antivirus tools can detect viruses which either attempt to infect or have already infected the underlying operating system (OS).

This article focuses on vulnerability scanning tools and databases. The reason for this is that, while malware and antivirus software is well- known and adopted by both organisations and individuals, vulnera- bility scanning tools are not widely used in practice. Further, it is hard for a practitioner to choose the right vulnerability scanning tool due to the great tool diversity and varied coverage. As such, this article aims to guide the practitioner in making an informed decision about which vulnerability scanning tool and database to select for his/her application. Such a selection relates also to particular challenges which have not been currently confronted in research, thus supplying an invaluable contri- bution to the academic community. In addition, this article contributes towards the identification of those places in the application lifecycle where vulnerability scanning can be performed and supplies valuable insights towards better securing cloud applications. Finally, this article investigates and proves that vulnerability scanning tool orchestration can be the answer towards higher vulnerability detection coverage.

The rest of the article is structured as follows. The next section formally defines what is a vulnerability scanning tool while explicates its architecture and the vulnerability management lifecycle. Section 3 at- tempts to set up and explain the article’s main research goals and methodology. Sections 4 and 5 evaluate the identified vulnerability scanning tools and databases, accordingly, based on a certain evaluation framework and analyse the main results produced. Finally, Section 6 explains the main research challenges involved in the quest for protecting a cloud application from security vulnerabilities. Finally, the last section concludes the paper.

The scanning via a tool can be performed in two different modes. In the internal mode, the tool is installed inside the application and attempts to scan its components, hosted in VMs or containers. Such a mode can address any application component kind due to the potential use of both static and dynamic scanning methods. Thus, its scanning accuracy is increased. Further, it leads to reduced communication overhead as most of the communication occurs within the same hosting environment. However, it can have the greatest impact in terms of interfering with the application performance as it can steal precious resources from the same hosting environment. In the external mode, the scanning is performed externally to the application and relies on the endpoints on which the application web-based components reside. As such, only the latter component kind can be confronted. This scanning mode has the advan- tage of conducting the scanning without interfering internally with the application hosting environment. However, it might not be able to attain the highest possible scanning accuracy as it cannot exploit static scanning methods while it increases the communication overhead in the applica- tion system.

Fig. 1 depicts a reference architecture for a vulnerability scanning tool, showcasing both its main components and different operation modes. The architectural variability that can be observed across all vulnerability scanning tools (in terms of two architecture classes) is also depicted, which is is explained in the next paragraph.

In a client-server architecture, a server remotely orchestrates the execution of different agents, either residing on the application VMs/ containers to perform the scanning internally or on different VMs to conduct an external scanning. Such an architecture can more easily scale while it can accelerate the scanning by orchestrating the parallel agent execution. In a standalone architecture, the scanning tool is installed in one place, either internal or external to an application, and then initiated to perform the scanning. Such an architecture variant/class has the main deficit that the orchestration of the installation, execution and report merging of the respective vulnerability reports burdens the user.

architecture classes, i.e., client-server and standalone, are orthogonal to this mode. This is depicted through the use of different colours in the respective components involved mapping to the four possible architec- ture scenarios (associated with all possible combinations between the two scanning modes and architecture classes).

A scanning tool might also exhibit the functionality to assess the relevant application exposure risk with respect to the kind of vulnera- bilities discovered and their criticality. Such a risk can not only indicate the need to modify the application system but also those parts that need immediate handling. Fig. 1 depicts this tool capability with the sole architectural difference being that in the standalone mode, the report is produced directly while in the client-server mode, the report is compiled by the server after merging the vulnerability results produced by the agents/clients.

As such, vulnerability databases (VDBs) are usually employed to cover the information to be included in a vulnerability scanning report. VDBs usually include some common attributes dedicated to properly identifying and explaining vulnerabilities. They might also include extra information, like which artefact is affected and how the vulnerability could be addressed. Many VDBs also conform to standards that support a kind of standardised identification or determination of the information to be covered.

Fig. 1 highlights two different connection kinds between a VDB and a scanning component. In case of a standalone scanner or server, the continuous lines indicate that the VDB is mainly used to produce the final scanning report. In case of a standalone scanner, the VDB could be also used to define how the vulnerability can be detected. On the other hand, the dash lines between the scanning agents and VDB indicate that the VDB might not always be used by them. In some cases, two main optional usage scenarios can hold: (a) the agent can obtain the scanning rules from the VDB; (b) it might use the VDB to construct its partial report to be sent to the server.

Scanning configuration: the admin then attempts to configure appro- priately the scanning tool. She might use only a subset of all rules or a certain rule set in case of modular tools. She can also point the exact places where scanning needs to be performed.

executor needs not be solely the system admin. The admin is mostly responsible for application deployment and provisioning, e.g., on vulnerabilities covering the OS level or the application execution environment in general. However, the application devop can also be involved, when issues regarding the application code must be confronted.

Once the handling is over, scanning can be re-executed to check that the vulnerabilities in focus have been corrected. Further, scanning might also need reconfiguration. For instance, it might be possible that one vulnerability is too coarse-grained and a more focused scanning needs to be performed to enable detecting more concrete vulnerabilities and subsequently handling them. It might be also possible that the overall handling strategy prescribes that the scanning frequency needs to be aligned with, e.g., the application modification pace.

The above lifecycle analysis highlights that various actors or roles in the user application management are also involved in the vulnerability handling and management. Further, the application management life- cycle is connected to the vulnerability management lifecycle, a logical consequence of the fact that vulnerabilities are not only related to an application but also affect its development and production process. Fig. 3 depicts such a connection, where vulnerability management is part of two application lifecycle activities:

Application Monitoring & Adaptation Once an application has been deployed and executed in a production environment, it must be monitored and adapted according to the current problematic situa- tion. As such, scanning must be continuously performed as there can be configuration changes, OS updates or even security incidents that might have taken place such that they must be checked for vulnera- bilities to be then properly handled. In overall, an application is a living organisation that can be affected by various changes that occur within its system. As such, we could consider scanning as part of dynamic application testing at runtime, already promoted as a kind of preventive application monitoring and adaptation measure in literature.

vulnerabilities only affect the application component code. As such, only certain application components will need to be modified, thus going back to the Development lifecycle activity. Then, the physical management flow will be followed leading to application redeployment and testing to check whether the vulnerabilities have been properly addressed without breaking the application integration.

The second path is followed when vulnerabilities affect multiple components in the application system. In this case, we need to both change application components as well as the way the application is packaged, deployed and configured. This then will affect both Develop- ment and Deployment activities. The changes can be so critical that might affect the application design while they can lead to producing a new application version. This is not so exceptional as we have seen in the past holistic changes on applications that attempt to secure them more than before.

Based on this manual and time-consuming practice, this paper aims at evaluating vulnerability scanning tools against a criteria-based frame- work. Such an evaluation can unveil which tools prevail and according to which aspects. We put special focus in the latter case on the trade-off between scanning accuracy and time to which we add the vulnerability coverage dimension.

We concentrate mainly on open-source vulnerability scanning tools and databases. Our rationale is twofold: (a) open-source model is well adopted in the market where more than 90% of software is available either in this or a dual open-source and proprietary form. This model is critical for both maintaining the software’s sustainability and extending it based on community contributions, especially for small or medium enterprises; (b) for practical reasons, it is impossible to experimentally evaluate a proprietary tool without purchasing it, while such an evalu- ation was required as Section 5 shows.

In practice, each tool might come with a different performance over the three aforementioned dimensions. It might be also specialised over different application or component types. As such, it may be possible that not just one but multiple tools might be required to achieve a certain vulnerability detection goal. Thus, this article also aims at investigating both the tools differentiation and complementarity to assist in their possible orchestration.

A certain procedure for tool search and selection was followed, comprising two main steps: (a) actual search of the tools; (b) selection of the right tools from those discovered based on a set of inclusion criteria. The search relied on a two-front approach. In both fronts, we have relied on a set of keywords deemed as most relevant for the search. This set was slightly differentiated depending on the concerned artefact

As it can be seen, the keywords were split into two or three categories depending on the artefact concerned. The first category is related to the research subject so it maps to the sole term vulnerability. The second category concerned the characterisation of the searched artefact, i.e., the research object. In this case, different alternative terms were used to capture all expression possibilities. The third category was only employed in the context of tool search to explicate the artefact’s specialisation, i.e., what it can perform in terms of the research subject. In this case, we focused on providing alternative terms that would explicate the discovery of a vulnerability.

The first search front was the wide Internet. This front simulated how an application administrator would search for a vulnerability scanning tool or database. Thus, we have employed Google as the well-known and most efficient web search engine to apply the keyword search for each artefact. This was proceeded by snowball search [1] by following the cross-references in a tool’s page to other related tools. While inspecting each tool web page, we had already in mind recording information for those criteria that will regulate the tool/database selection.

The second search front was formulated to cover prototype tools or databases from academia that cannot be found easily from a web search. Such tools could be increasingly improved and extended due to the relative research and development tasks conducted by the research or- ganisations producing them. To discover such tools, we employed well known academic data sources, i.e., Web of Science and Scopus, enabling us to have a full literature coverage.

For the survey on vulnerability databases, we focused on a compar- ative evaluation based on the very nature of research question Q1. Thus, we created a set of evaluation criteria which were either devised by us or drawn from the literature. All criteria were produced by considering two aspects: information coverage, capabilities & support. More details are supplied in Section 4.

The results of Table 1, mapping to the first criteria partition, indicate that the best approach in information coverage is Vulcan followed in 2nd place by 2 other approaches: HPI-VDB and vFeed. Vulcan needs to better handle only the exploit criterion which is partially satisfied. On the other hand, the other 2 approaches need to cover completely 2 criteria. In overall, we can see that Vulcan is very close to an ideal approach for this criteria category, quite normal if we consider that it is ontology-based and thus designed to well integrate and correlate different information pieces.

To unveil some patterns from the results, we can indicate that most approaches focus on addressing the first 5 criteria while the rest are rarely covered. These 5 criteria can be regarded as the most essential for well characterising vulnerabilities. The rest have an added-value focus on clarifying how a vulnerability can be exploited as well as in categorising and relating it with other vulnerabilities. The former is an excellent in- formation source, leading to equipping a certain tool with extra vulner- ability detection capabilities. The latter criteria enable to nicely categorise vulnerabilities for more user-intuitive browsing and explora- tion while also unveil relations between vulnerabilities, to be exploited to conduct more advanced exploit forms. This relationship knowledge can also assist in detecting a vulnerability’s root cause by, e.g., identifying the first vulnerability in the detected vulnerability chain.

Most VDBs focus on any kind of vulnerability; very few have a more restrained scope. For risk assessment, many VDBs adopt Common Vulnerability Scoring System (CVSS) while very few just provide an impact measure. Some VDBs supply both impact and risk information for each vulnerability.

contrasts the other approaches that focus mainly on supplying concrete vulnerabilities at the leaf level of such hierarchy. For instance, Vuldb advertises covering around 114198 leaf vulnerabilities. However, in our view, all hierarchy levels are important. As such, it is imperative that a VDB integrates its content with CWE to enable producing such a rich hierarchy. Such an approach is followed only by very few efforts, including Vulcan, as Table 2 shows.

Table 2 depicts the evaluation results for the capabilities & support criteria partition. These results indicate that there is no clear winner. NVD can be distinguished due to its community support and wide adoption. vFeed.io can be discerned due to its extensive support to standards and its rich interfaces (API & database). Further, it supports rapid threat response development which comes via pinpointing security scripts of vulnerability scan tools, referencing exploit information uti- lisable for automated testing via penetration tools like metasploit, applying effective defence rules (e.g., via Snort) and discovering relevant patches and hot fixes via online web crawling.

Freshness. Community support aligns with this criterion as VDBs with excellent or good community support, also provide frequent updates to their content. However, there is also correlation with the interface mechanism provided as a feed mechanism unveils frequent VDB updat- ing. In overall, the need to provide fresh vulnerability content is recog- nised by all VDB efforts.

freshness: it indicates how frequently the tool is updated, either in terms of improved versions, fixing detected bugs, or extensions. In the context of vulnerability assessment, all these aspects are critical. Bugs can stop a vulnerability assessment tool from functioning. While the reduced or non-existing ability to evolve can mean that the tool is not able to rapidly provide support for detecting new vulnerabilities.

categorisation: One or more categories that characterise a tool’s functionality, derived from the OWASP taxonomy [28], detailed in the appendix. Depending on its functionality, a tool might belong to multiple categories. This can occur not only for single tools but also tool agglomerations, where each component tool might focus on delivering a different kind of vulnerability detection functionality.

Vulnerability Coverage: attempts to evaluate a tool’s with respect to the vulnerability kinds it can detect. In other words, it assesses the per- centage of all possible vulnerability kinds covered. To derive this percentage, we have followed an approach, detailed in Appendix B, comprising (3) main steps: (a) identification of all possible web application vulnerability kinds by relying on SecToolMarket.com; (b) assessment of the percentage of vulnerability kinds covered by a tool;

architecture: identifies the tool architecture. For instance, tools can follow a client-server architecture, where the client is installed in places requiring scanning, while the logic and vulnerability defini- tions are taken from the server side. Each architecture kind can have specific pros and cons. For instance, a tool working in standalone mode does not spend network bandwidth and resources to commu- nicate with a server. However, it cannot receive updates on logic or vulnerability definitions.

usage level: indicates whether a tool can be executed internally or externally to the scanning place. An external execution is non- intrusive, not spending precious resources from the running appli- cation component. However, it might not detect all possible vulner- abilities, especially those related to a component’s internal execution environment. On the other hand, an internal execution can detect more vulnerabilities but can also spend some precious resources, which might prevent the application from delivering a suitable ser- vice level. So, tools flexibly operating in different levels are more preferable as they allow to detect only the most relevant vulnerability kinds and thus achieve a better trade-off between vulnerability coverage and resource consumption.

Resource requirements: Each tool comes with its own minimum resource requirements that must be satisfied to work properly. However, such requirements can affect the application’s resource usage. Thus, in case of resource-intensive applications, it might be better to use tools either lightweight or supporting a non-intrusive, external usage level. Otherwise, especially in case of critical appli- cations, more heavy tools might be required to operate in full mode such that the detection of a high number of different kinds of vulnerability issues can be achieved.

Access control mode: A tool might need to be executed based on certain access control rights to be operationally successful. Thus, if the goal is to find OS issues, the tool should have privileged access to check the OS-specific part of the hosting component (e.g., VM). This criterion also correlates to the usage level as when that level is non-intrusive, the access control mode can be the least critical.

Standards. Both CVE and CWE seem to be mostly supported. This is logical as these standards enable to better characterise a certain vulner- ability and classify it. Next comes CVSS followed by CPE which have slightly more than one third of support. This indicates that the need to associate vulnerabilities to the components concerned has not been well recognised yet while the capability to assess the risk related to a vulnerability seems to be neglected.

It must be noted that one third of the tools do not support any stan- dard. Such tools do not also employ a VDB (see Table 5). This seems a logical correlation as the need to store information in a VDB would have created the requirement to supply a structured way to perform this via standards. In our opinion, support to standards has been recognised by most tool providers as: (a) it can allow any kind of integration or coop- eration between their tools; (b) it facilitates a better identification and assessment of vulnerabilities, thus enabling tool users to really benefit from the extra information retrieved.

We must underline the good performance of academic approaches in this criterion, indicating that researchers have well understood the power of standards such that they tend to adopt them more easily than other tool providers. The sole exception is OpenSCAP which has considered the support to standards as one of its major design requirements. This well justifies the support of this tool to 8 security standards.

Community. We have found that most tools have their own commu- nities, usually maintained by the tool provider. Such communities can interact with the tool provider via emailing lists and blogs, while they can also participate in the tool development. The latter is the cornerstone of open-source communities, usually built around interesting or innovative tools that become sustainable via their participation. As such, security software providers follow faithfully this model which is widely adopted in the software world.

We must appraise the existence of certain organisations, i.e., NIST and OWASP, that promote in the best possible way security software. NIST focuses more on developing standards aiming to bridge interoperability issues, while OWASP focuses on promoting innovative tools, creating tool benchmarks plus classifying both tools and vulnerabilities. Both organisations appear twice as community members in two respective scanning tools. This also indicates that these organisations have a great belief in the dynamics and capabilities of the tools they support.

2 communities targeted. This is not accidental by considering their vulnerability coverage (see analysis below in functional category). OpenSCAP is supported by both NIST and RedHat while it has already engaged multiple OSproviders as these providers have already specified their vulnerabilities and policies in a format acceptable by OpenSCAP. On the other hand, OpenVAS is a fork of the well-known Nessus [49] vulnerability scanner, which has moved from an open-source to a closed-source model. Fortunately, with the support from 3 organisations, OpenVAS has been quite successful as it can be witnessed by the great amount of tools which exploit internally this scanner.

Finally, we must stress that as the last 4 approaches are academic ones, their community is more restricted, usually in form of one or two organisations, where one is affiliated to the other. This restriction, then, impacts the tools’ update frequency which is quite scarce. This underlines the need for the involved organisations to have a more aggressive move towards the open-source community to better support their tools sus- tainability and evolution.

Freshness. More than half of the tools are updated frequently (every some months) or very frequently (every some days) while the rest are either not updated any more (tool provider has stopped investing on them) or the update is infrequent and community-driven (esp. for aca- demic tools). This is a nice result, signifying that the need to update the tools to address the continuously increasing vast set of vulnerabilities has been well recognised.

This result can be correlated with the previous one by considering that frequently-updated open-source tools either rely on more than one community or are pushed by their providers as they see great value in them. In the first case, the update frequency can reach even higher levels depending on the tool community size. For instance, if we consider OWASP, this organisation is both big and well-known for its security expertise. As such, it does not only use its own resources to evolve its tools but it also incorporates an enormous in size member list supporting both the tools updating and maintenance via two alternative feedback mechanisms (merge requests & bug reporting).

Best Tool Nomination for the Support Category. By considering the evaluation results across all criteria of this category, the best performers in the Community criterion, OpenVAS and OpenSCAP, can be discerned based on their community support and update frequency. We further discern OpenSCAP as the topmost performer, due to the great amount of standards it supports. Finally, academic approaches, especially Vulcan and UC-OPS, are placed third, with a good standards coverage.

Similarly, more than half of the tools support information gathering. This is also logical as before any kind of vulnerability assessment is performed, the target object needs to be scanned to find the right places where vulnerability assessment will focus, collected in an information gathering step. Depending on the object of focus, information gathering can take different forms. For pure web application scanners, spider software is exploited to find all actual web pages from which the scan- ning can be performed. For more sophisticated scanners, information gathering involves checking the whole host, finding open ports and attempting to infer the components that run on such ports.

Finally, we must remark that only one tool exhibits application pro- tection capabilities, taking the form of antivirus/anti-malware detection. Such functionality can be considered as complementary to vulnerability detection in the way it is employed. It has the added-value that it is actively enforced, enabling the system to immediately detect and react on a vulnerability that comes in form of a virus/malware. This is essential as continuous security assessment is highly required for most applications/ systems, which constantly evolve over time. Further, we must stress that in contrast to pure vulnerability detection, focusing on discovering but not addressing vulnerabilities, antivirus detection can react on the vulnerability by, e.g., putting infected assets into quarantine or deleting them. Based on this argument, it is recommended that either scanning tools are extended with application protection capabilities or are com- plemented with tools offering such capabilities. Further, by considering application evolution, it is also advocated that vulnerability scanning is not an one-shot but a continuous process with a frequency conforming to the evolution frequency of the respective application.

Object. Almost all vulnerability scanners can operate over a web application. However, as the next paragraph and Section 5.2 will show, each scanner might have a different focus on the web application vul- nerabilities by, e.g., concentrating only on specific kinds, like the most usual ones.

Most old scanners focus solely on web applications as the target scanning object. The main rationale is that application developers would care most how web applications are vulnerable in their operation space (e.g., set of web pages) from the outside. However, as an application might comprise multiple components and might run on vulnerable media, it is apparent that the focus should now move to holistically cover the whole application system. This is actually grabbed by both recent scanners and academic approaches. A good example for the former case is OpenSCAP that covers not only pure web application but also cross-level system vulnerabilities. In the latter case, academic approaches extend an almost complete vulnerability scanner, like OpenVAS, to make it perfect. Vulnerability Coverage. By considering all tools, it seems that only OpenVAS and OpenSCAP have the highest vulnerability coverage along with tools that re-use them. By considering that: (a) the percentage of these tools is less than half of the overall tools; (b) OpenVAS and OpenSCAP do not focus on scanning the whole web application opera- tional space, it is easy to understand that such a result marks the need to

In fact, this might signify that security experts must now move to a new direction: as there exists a sophisticated state-of-the-art tool, the community must focus on both improving and evolving it over time to also detect new vulnerabilities via the production of respective plugins. In fact, this already occurs for the 2 aforementioned tools. In the context of Network Vulnerability Tests (NVTs) or OVAL definitions, OpenVAS and OpenSCAP, respectively, rely on a huge community effort, involving a great amount of OS and software providers contributing to the defini- tion and detection of vulnerabilities.

direction on exploiting such state-of-the-art scanners, has a different focus and attempts to improve or orchestrate them with other tools to achieve both a better coverage extent and breadth plus capture a greater set of vulnerability detection scenarios. For instance, as indicated in the Categorisation criterion, one academic approach, AWS-Vuln, focuses on combining vulnerability scanning with application protection to sustain a smooth, vulnerability- and malware-free application lifecycle. This approach signifies that due to the application system’s complexity, it is not always possible to cover all possible vulnerability kinds and different tools need to be employed to achieve a perfect coverage. Thus, it surely moves to the right direction and paves the way via which vulnerability scanning tools should be utilised in the near future.

Inferencing. Only one tool, Vulcan, supports some kind of inferencing. This academic tool employs ontology-based reasoning to support infer- encing. Restricted to the current tool functionality, this inferencing takes the form of discovering vulnerabilities of component agglomerations apart from those of individual components. However, this ontology- based reasoning approach is quite promising as it could be extended to cover the derivation of extra kinds of vulnerability-related knowledge; these kinds are explained in Section 6.

Counter-Measures. Also correlated to mitigation plan production, this criterion highlights the need to associate vulnerabilities with the ways they can be addressed known as counter-measures. In contrast to the previous criterion, though, the situation is much better as almost half of the tools support this criterion. This means that the need to report this correlation kind to users has been well recognised and realised. Further, most tools exhibiting this feature support standards like CVE or CWE which do provide counter-measure information. This is another benefit related to the support of standards.

OpenSCAP seems to be the sole tool going beyond counter-measure reporting to counter-measure enforcement. In particular, through sup- porting SCAP, this tool can mitigate the vulnerabilities found in some cases via automatic path application. This is the right direction to be followed by all other scanners to really advance their usage and increase their added-value.

Risk Assessment. More than half of the tools offer some risk assessment form. So, they have well recognised this information’s importance, which, by accompanying the vulnerability detection one, enables users to assess their application’s overall risk level and thus supports them in producing a mitigation plan that prioritises more the vulnerabilities with the highest risk level. Similarly to the previous criterion, many tools supporting this criterion, also conform to CVSS, the most prevailing and de-facto standard.

Note that 3 tools claim to support some risk assessment form but it is not clear how. We suspect that they map vulnerability categories to certain risk levels to address this. However, such an approach is too coarse-grained and does not consider a vulnerability’s severity with respect to the consequences it might have and the component(s) on which it applies.

Tool Coverage. Scanning tools re-use a great variety of sub-tools, spanning many OWASP taxonomy categories, including antivirus pro- tection, information gathering, network scanning, and data validation testing. From these categories, we can discern Nessus and OpenVAS, as those sub-tools mostly re-used due to their almost perfect coverage extent, as well as nmap, as the most popular and well-adopted informa- tion gathering/network scanning tool.

In fact, the second factor seems to apply to most (6 out of 9) pure web application scanning tools. This means that such tools have a focus that can be independently realised. This also signifies that possibly these tools’ providers do not want to invest on adopting other tools with the main rationale that this could restrict them technologically in the tool implementation.

Best Tool Nomination for the Functionality Category. In this criteria category, there is not a clear winner. We can actually see 2 main tool partitions: (a) top approaches which do not support inferencing; (b) the Vulcan academic approach which supports inferencing but has a very low coverage and focuses only on web applications as target objects. The latter also indicates that if Vulcan was able to use Nessus or OpenVAS, we would surely nominate it as the best in this category, as this would immediately improve in an ultimate manner its two main deficits.

In the first partition, the academic tool AWS-Vuln is the clear winner. It not only has the best possible categorisation and vulnerability coverage but also re-uses a great sub-tools set, which constitutes another proof of its completeness. From the pure tool world, we can discern OpenVAS, which, however, does not cover additional security scanning categories. From the 2 best result partitions, academic tools can be discerned.

Architecture. A tool’s architecture along with its mode of operation impacts both the vulnerability coverage and the flexibility in the tool runtime administration and execution. In particular, a tool that works in a standalone manner and scans externally an application has the benefit of not interfering with the application’s normal operation, thus taking a non-intrusive approach. On the other hand, this comes with the disad- vantage that vulnerability coverage is low. A tool working in client-server mode that is executed internally to an application is more intrusive but has the benefit of better vulnerability coverage even for the whole application via the well configured concurrent deployment of tool clients that scan all application components in parallel.

Table 5 signifies that most tools can be run in a standalone mode. This is a natural, well-expected result as: (a) this is the desired operation mode for simple, non-expert users who prefer to have a simplified way of launching and interacting with the tool; (b) some tools have low vulnerability coverage and can work only in non-intrusive mode. As such, it is not rationale to make these tools distributed and complicate their administration as this will affect their main competitive advantage: their simplified usage and administration.

Tools that employ a client-server architecture are about one third in number. They usually exhibit a better vulnerability coverage and focus on more advanced, IT-security expert users. As such, in contrast to the previous paragraph argumentation, these tools should employ a client- server architecture as the most natural way to completely cover vulner- abilities across the whole application plus the most flexible and capable way in terms of administering the scanning. Due to these advantages, it is worth sacrificing slightly the simplicity to better attract the main target users, i.e., IT-security experts.

There is also the case of tools (especially Arachni) which enable the alternative use of both architectures. This might be a very good move to extend a tool’s applicability towards all kinds of users and not only the novice ones. As such, the adoption and market share of such tool could be increased.

targets developers of more extensive tools requiring a standardised way to interact with the encompassing tool. An API supply can be also pref- erable in case that the user does not need to be burdened with the tool installation and deployment. In particular, a security organisation can make the API available as a service such that the user can immediately use it without spending resources to deploy, install and maintain it. However, an external API’s supply comes with a maintenance cost for its offering organisation which needs to be accompanied with a certain business model to enable this organisation to gain from it. This business model could take the form of an usage fee or the supply of API usage support or the paid subscription to more advanced API features. How- ever, none of the tools seems to be offered in a SaaS mode. This is natural as all tools considered are open-source so only open-source-based busi- ness models would make sense in this case.

Usage Level. The usage level directly impacts a tool’s ability to cover a suitable coverage level. In particular, a tool operating in external mode cannot access an application component’s internal environment. As such, it might discover vulnerabilities that might jeopardise the component’s operation from the outside but it will not detect other vulnerability kinds that might concern the component (e.g., bugs or modelling mistakes in the source code) and its environment’s elements. On the other hand, for web application kinds with a simplified architecture, this external scan- ning mode might be more suitable as more focused and less intrusive. As such, the operation mode could rely on the type of application targeted, its architecture and its environment.

Vulnerability Database. The existence of a VDB enhances a tool’s reporting capability while assists in better detecting and identifying vulnerabilities. Coupled with the support to standards, the amount and credibility of reported information can reach high levels and thus make the tool more attractive to users. The above arguments must have touched the tool providers as almost half of the tools encompass a VDB in one or another form with varying capabilities. Most approaches encom- passing a VDB also support one or more standards, which enables to exhibit the advantages that were previously mentioned. On the other hand, tools with no VDB seem to be old and clearly not supporting standards. As such, these tools could be considered as outdated and not attractive any more to users. This has thus impacted their communities, also explaining their infrequent updating.

Modularity. A modular tool can be individually extended as needed. It also allows making a focused scanning by using only those modules necessary for detecting the right vulnerabilities for an application. This need for modular tools seems to be picked up by the tool providers as most tools are modular.

exhibited by particular kinds of applications or their components. Partial modularity, on the other hand, is a limited modularity form that implies the existence of one profile from which the user can select some of the scanning rules contained. In some cases, the addition of new rules might be also possible in such a profile.

Based on the above definitions, we can observe that most tools are fully modular. This is a very good result for the prospective of tool users as they have the best possible flexibility in configuring such tools with the right set of scanning rules. This also increases the usability level of these tools.

In most cases, tools which expose a VDB also exhibit modularity. This is a promising result, also enabling possible vulnerability scanning adopters to assess a tool’s coverage level by inspecting the vulnerability kinds it can cover from the available modules and their mapping to the vulnerabilities stored in the VDB. The VDB alone, though, cannot tell the whole truth as it could be made as complete as possible (as in case of NVD VDB) without having a one-to-one mapping of vulnerabilities to the tool’s detection capabilities.

(b) with respect to the scanned component’s execution environment. Obviously, the latter form affects the former as it is expected that when one tool is written to operate in one OS, it is rather strange to apply it to applications written in other OSs. However, this expectation holds mainly for internal-mode tools. As such, this criterion should not be inspected alone when attempting to discover OS restrictions on the application side. On the contrary, by knowing that a tool works in external mode, this tool can be applied for applications operating in any OS. Further, a tool that works in both modes would then be able to cover externally any application and internally only an application operating on those OSs where this tool also operates.

Some tools like Clair are able to operate on the level of images. This then enables them to work on an “external” mode as they just inspect the images for vulnerabilities before they are deployed in the cloud. As such, this kind of tools can support the scanning of images of any OS.

The evaluation results with respect to the first OSsupport type signify that most tools support Linux-based OSs. This is well expected by considering that such OSs have been deemed more secure (at initial deployment time without any extra hardening support [48]). Windows come next with around two third of tools able to operate in Windows environments. OS-X comes third but still having more than half of the tools supporting it.

Around one third of the tools support any OS, which is an encouraging result, signifying that the respective need has been well recognised by tool providers. However, contrary to other criteria, this need is not well reflected in academic tools. This is well anticipated as the internal or community resources dedicated to developing these tools are limited. Rather, a more focused development on improving these tools’ core ca- pabilities is expected.

By combining the results from this and the Usage Level criterion, we can deduce that: (a) tools only operating on Linux have an external or both modes; (b) tools operating on Windows and Linux exhibit both modes; (c) tools operating on any OStend to support only external usage. This result unveils possibly the fact that tools with external usage tend to increase their applicability to improve their market share position by supporting more OSs to alleviate for their non-high vulnerability coverage. It further indicates that tools focusing on a limited OS set tend to be more extensive in terms of their usage level. This is natural as in this case such tools have a better coverage and applicability over these OSs as they invest on these OSs’ popularity.

the usage level criterion. This is due to the fact that as long as a tool is not intrusive, it can be regarded as not demanding for resource requirements. However, this should not be treated as a panacea as there exist non- intrusive tools like ZAP that have high hardware requirements. On the other hand, an intrusive tool should be either invoked irregularly or have low resource requirements to not interfere with the scanned application’s normal operation.

The evaluation results indicate that more than half of the tools have low resource requirements. These tools’ providers have decided to make them lightweight to be more appealing to prospective adopters. How- ever, this design choice seems to affect the tool capabilities, as most tools with low requirements do not have a good vulnerability coverage. On the other hand, sophisticated tools, like OpenVAS, or tools that re-use them are more heavy-weight and require either high-disk VMs (Nessus) or medium CPU & memory VMs (OpenVAS). However, these tools’ usage level is extensive such that they can be executed also in external mode. As such, they are not necessarily intrusive to the application such that they can jeopardise its normal operation. Further, we are discussing mostly requirements at the server side. For the client side, the resource re- quirements can be significantly less. So, the burden in using these tools comes mainly with the operation cost of the server which can be outside the VMs hosting the scanned application.

coverage plus its administration. It might also raise security concerns about how safe are clients operating in admin mode in a client-server architecture. Thus, tools operating in a normal, user mode are less intrusive and do not access critical assets via internal scanning. However, as such assets are uncovered, vulnerability coverage is low. On the other hand, an admin mode enables accessing (possibly infected) critical sys- tem assets and thus caters for a better vulnerability coverage. However, acting in this mode comes with issues that might involve the incautious exchange of credentials and the possible scanner infection thus exposing the respective system to great vulnerability.

The evaluation signifies that most tools support the normal access mode. This is a natural result as: (a) a tool focusing solely on external web application scanning does not require using any credentials for authen- tication and authorisation purposes; (b) less vulnerability to attacks can be caused by the tool’s compromise; (c) possible undesirability of users to provide credentials for this scanning type. On the other hand, almost one quarter of tools support both access modes. We believe that such tools are more flexible in vulnerability scanning as they enable the user to choose the most desirable access control mode according to the current context and application kind. It is not surprising to see that the most represen- tative tools of this kind are OpenVAs and OpenSCAR, while the rest of this kind’s tools just reuse them. This indicates that these two tools are designed based on suitable requirement sets originating from actual users and their needs. This answers well these tools’ excellent performance in many of the considered criteria.

overall evaluation results of this criteria category, OpenVAS and OpenSCAP can be considered as the best. OpenVAS seems better in terms of OS support while OpenSCAP is better with respect to hardware re- quirements. As the OS support can be less important than hardware re- quirements, we might then nominate OpenSCAP as the best. In both cases, both tools seem to support only one kind of architecture (deploy- ment). Thus, we can deduce that there is still room for improvement of these tools.

across all categories plus the derived results from the analysis of each category, we can discern two main tools: OpenVAS and OpenSCAP. OpenVAS is the best in terms of the functional category while OpenSCAP in the support category. While they score equally in the configuration category. However, in our opinion, while OpenVAS might seem better in categorisation and tool coverage as well as OSsupport, we would pro- mote OpenSCAP as the best based on the following justification: (a) OpenSCAP is the sole tool able to mitigate vulnerabilities; (b) it has lower resource requirements; (c) it supports more standards while it has a similar community support with OpenVAS. Further, it seems to have a slight better vulnerability coverage based on the quantitative results re- ported in Appendix B. To this end, OpenSCAP is the best possible tool that can be selected by respective practitioners while OpenVAS could be selected as the second best.

If the analysis is restricted in the academic tool partition, different tools can be distinguished as best in each criteria partition. HPI-Vuln is better in terms of configuration, AWS-Vuln in terms of functionality and Vulcan in terms of support. In overall, we would actually discern two tools for different reasons. AWS-Vuln is discerned not only due to its perfect functional coverage but also as it seems to have good community support and supports well-known standards. By considering its functional coverage, we should highlight its capability to complement vulnerability detection with malware/antivirus protection, a highly-required and innovative feature. However, it still needs to be improved in some aspects like resource consumption, which comes with its design choice to select Nessus as its core vulnerability scanning sub-tool.

On the other hand, we discern Vulcan for three main reasons: (a) its excellent support level to standards; (b) its inference capability; (c) its low resource requirements. However, Vulcan still needs to be improved with respect to its low vulnerability and object coverage plus its limited OSsupport. Further, it needs to be expanded to use a very good vulner- ability scanner, like OpenVAS or OpenSCAP. In any case, we believe that all academic tools should be made publicly available as normal vulner- ability scanning tools to increase their sustainability and potential for improvement and evolution.

As indicated in Section 3, research question Q3 required a special handling to enable assessing the properties of scanning time, accuracy and coverage. This handling is also due to the fact that there is no reporting currently for most tools about their performance for these properties. Nevertheless, even if this reporting was available, it could be old or rely on simple benchmarks.

As such, we have conducted a small web-based survey to identify candidate vulnerability tool evaluation benchmarks. This led to discov- ering 6 benchmarks which are shortly presented in Table 6. From these benchmarks, we have finally selected the OWASP benchmark for the following reasons: (a) it is as realistic as possible as it relies on a web- based application built according to well-known design patterns; (b) it has the widest number of vulnerability areas covered; (c) it enables to assess all three properties. This includes a suitable scanning accuracy metric that considers a tool’s returned evaluation results across all the vulnerability areas covered; (d) it supplies code which employs an easy integration point with respect to the adoption of a scanning tool. The code is also more frequently updated than the other benchmarks; (e) it already includes some scanning tools in its distribution.

Based on the above results, the best tool with respect to the overall scanning accuracy and time is FindSecurityBugs. In fact, this tool ach- ieves a high accuracy on many vulnerability areas. This might be regar- ded as unexpected as this tool only includes a small set of security rules. However, it can be well justified by the fact that this tool operates directly on the source code so it can indicate with a higher accuracy if a certain issue holds.

Similar results were expected for SonarQube. However, this occurred only for scanning time but not accuracy. This could be due to the fact that FindSecurityBugs focuses only on Java source code, on which the OWASP benchmark is based, and is thus more optimal with respect to this pro- gramming language. Thus, as Sonarqube has put equal focus on different

penetration tools. Covers only the SQL injection vulnerability area. It can dynamically create a testbed and map it to ideal assessment results. Tools can be run against this testbed to compare their results with the ideal. 3 aspects can be evaluated: deployment requirement, SQL injection coverage, and certain evaluation parameters.

approach. In particular, we decided to use the tools that the benchmark already provides plus open-source tools, like Zap, which provide guid- ance about how they can be assessed with this benchmark. This would enable us to maintain the assessment effort to the minimum while still enabling to answer appropriately research question Q3. From the reader’s perspective, we continue the assessment of the previous sub-section with the capability to rank the top results against the three major properties of scanning accuracy, time and coverage. To inspect the trade-offs between the three main properties, we also tried to evaluate the scanning time and accuracy by varying a tool’s vulnerability coverage. This was performed by activating or deactivating the respective vulnerability scanning rules, in case this possibility was available.

Based on the results of Table 8, a trade-off exists between scanning time and accuracy only for dynamic vulnerability scanning. In fact, by varying the number of scanning rules, we can reach different levels of scanning time and accuracy. However, when the scanning is statically applied on the application source code, scanning time is very similar, almost independently from the number of security rules considered. This is related to the fact that in all examined static analysis tools the number of security rules is small, such that varying the partitions of the security rule set does not significantly impact scanning time. Thus, it is advocated that static analysis tools should always be used on full scanning mode. On the other hand, dynamic scanning should be more focused, mapping to the need to create suitable scanning profiles to cover different kinds of applications or components. Due to the nature of such tools and the workload they incur on the underlying resources, it is advocated that such tools are used either remotely and infrequently due to their impact on application performance, or internally, when sufficient internal re- sources are available to enable the scanning to be normally performed.

The FindBugs tool [82] enables conducting static analysis to assess Java source code quality. This extension enhances FindBugs with the capability to find security bugs. However, it does not enable filtering the security bug detection rules. Only explored possibility is to either apply only the security rules or all bug detection rules, i.e., including those originally

The results of Table 9 indicate that FindSecurityBugs covers all vulnerability areas. However, this coverage is not always deep. On the other hand, Sonarqube touches just three vulnerability areas and has good accuracy performance in only two of them. This signifies that FindSecurityBugs is recommended, due to its coverage, for use when the application is written in Java. While Sonarqube can be exploited for

areas but exhibits good accuracy performance only in two of them. Compared to FindSecurityBugs, it is slightly better in two vulnerability areas: command injection and path traversal while much better in cross-site scripting and SQL injection. This signifies that the agglomeration of these two tools will enable to have a better vulnerability area coverage and thus reach a much higher overall accuracy level. However, some vulnerability areas are not yet deeply covered, including: command in- jection, LDAP injection, path traversal, trust boundary violation and XPath injection. This outlines the need to consider another scanning tool to complementarily cover more deeply these areas.

All the above observations signify that scanning tools should not be individually used but orchestrated to reach a suitable vulnerability coverage level. Further, it has been highlighted that there is a need for a scanning tool strategy or workflow to enable executing different vulnerability tools based on the kind, nature and content of respective applications. Finally, it has been indicated that tools might require being configured with different rule profiles to enable a more focused scanning to cover only those vulnerability areas relevant for an application. Such rule profiling would also guarantee a suitable scanning time without overloading much the scanned application.

Based on the comparative and empirical evaluation results of the vulnerability scanning tools and databases selected, this section elabo- rates on the consequences of the main findings in terms of on-going challenges and future work directions. The presentation is separated based on the tool kind considered, i.e., a vulnerability DB or tool, in the following two sub-sections.

Most VDBs exhibit just a limited web-search interface. In our opinion, there is a need to go beyond this and offer an API allowing to: (a) pose more advanced query forms; (b) subscribe to certain events (e.g., new vulnerabilities introduction); (c) manage vulnerability information. Such an API could be quite beneficial to both scanning tools and users. The former could retrieve the required information in a sophisticated and precise manner. Further, they could supply different vulnerability reporting levels to users. On the other hand, users would benefit from the extra information that could be retrieved for a certain vulnerability as well as from the existence of an interface for introducing or updating vulnerabilities in case that they concern their own software. Further, such an API could enable to abstract away from the information source technology and enable the VDB to obtain and integrate information from multiple, disparate sources.

Due to the main benefits that semantic technology brings about, we expect a proliferation of semantic VDBs in the near future. Currently, only Vulcan offers a semantic VDB. However, this VDB is not rich enough and coupled with the right rules to allow inferring various knowledge kinds. Further, it seems to manually and not automatically integrate

In some cases, VDBs attempt to draw information from various in- formation sources but only in the context of typical vulnerabilities. In our opinion, a VDB should be equipped with the capability to automatically collect information from multiple sources that cover both vulnerabilities and software bugs. The latter could be drawn from well-known bug re- positories [46] and enable inspecting whether certain application com- ponents are safe to be exploited, unveiling bugs related to software vulnerabilities, and highlighting the most secure and reliable component versions to adopt. The glue between vulnerabilities and bugs could be achieved by using semantic technology, realisable by adopting suitable semantic models and mapping techniques to integrate information from multiple, heterogeneous information sources.

CWE seems to propose a rich vulnerability and threat taxonomy. However, this taxonomy should be extended accordingly to become a semantic security meta-model able to both cover the most important security concepts and their relations. This coverage along with semantic rules incorporation would then enable inferring further relations be- tween security concepts which would never be acquired by a syntactic modelling approach. Apart from this, such relations would improve the scanning accuracy as explicated below.

The first direction indicates that, as there are many special-purpose standards, most of them must be supported by the scanning tools so as to become more complete. Further, this promotes interoperability, especially in terms of tool re-use or orchestration, as it will be shown in the next sub-section. For instance, support to CVE and CVSS can enable to merge vulnerability reporting results from different scanning tools. However, as some standards seem already prevailing while others promising, possibly different priorities must be given to different stan- dards for their adoption. In our opinion, apart from supporting CVE, CWE and CVSS, there is a great need to also support OVAL and SCAP. The first can enable a uniform way to specify and address vulnerabilities while the second their automatic mitigation. The support to SCAP will be further elaborated in a later sub-section.

the use of scanning tools and highlight the criticality of addressing application vulnerabilities; (b) organisation of conferences, workshops and competitions to highlight recent vulnerability scanning advance- ments as well as the top tools possibly categorised under different competition areas; (c) supply of benchmarks or scanner selection tools to assist users in evaluating and selecting the right scanners; (d) supply of interfaces and mechanisms via which valuable user input (e.g., feedback, plugins) can be provided in a more natural and user-intuitive way.

While most tools provide some engagement support, this is not necessarily the case for academic tools. Such tools, while presented in academic forums, are not always made publicly available or offered as open-source. Further, such tools do not supply usual engagement facil- ities like mailing lists and code development portals which hinders their sustainability and further evolution.

As indicated by the evaluation results plus the existence of scanning tools that agglomerate different security tools, there is a high need to orchestrate scanning tools for various reasons. First, as their vulnerability coverage and extent must be enhanced. We have already observed that there exist tools like OpenVAS and OpenSCAP which exhibit a very nice vulnerability coverage. However, due to the their current focus, these tools’ detection recall is not high, especially in terms of web application vulnerabilities. We have recently experienced this when using OpenVAS in the OWASP benchmark where OpenVAS was not able to cover all operation space parts of the benchmark application. This indicates that tools like OpenVAS must be complemented with traditional web appli- cation vulnerability scanners.

Second, as witnessed by the empirical evaluation, even web appli- cation scanners are not so good and must be complemented by source code analysis tools. If fact, as advocated in AWS-Vuln, extra tools (e.g., antivirus) apart from vulnerability scanners are needed. Thus, it is actually advocated that there is a need to orchestrate a great number of different tool kinds to achieve the best possible coverage against the whole application system in a continuous manner. Only in this way, applications can be permanently and fully protected during their whole lifetime as both applications, their components plus penetration methods and techniques continuously evolve over time.

Third, while the orchestration of scanning tools is exhibited in the market, it is not perfect. In particular, such an orchestration does not consider various conflicting factors, including: the complementarity of the tools, their integration level, the user requirements and preferences and the application kinds to be addressed. By focusing on user re- quirements, we see a trade-off between the following properties: scan- ning time, accuracy and overhead. Traditionally, there is an usual trade- off between time and accuracy. Further, as accuracy can lead to a quite intensive processing for retrieving all possible vulnerabilities, it can also affect the scanning overhead in terms of resources needed to support this scanning. This is especially true for the internal scanning mode as precious application resources can be stolen by the respective scanner. We should also not ignore the scanning frequency which could be

There is a need for dynamically agglomerating vulnerability scanners according to the current context. Such a dynamic approach would enable addressing both the evolution of applications, of their re- quirements and of the components that comprise their system. It would also enable checking the current application context and the current ways interference can be achieved to perform the required vulnerability scanning by using the best possible tool agglomeration. For instance, we could sense that the application is under a heavy load such that we might attempt to either postpone its vulnerability scanning or perform it in a very lightweight manner. As another example, if there is a great potential for application penetration, it might be decided to perform a full scan, no matter what is the current application workload.

There is a need for a proper agglomeration strategy encompassing the appropriate use of the right tools of the right kind at the right moment. For instance, vulnerability scanning might be decided not to be performed in conjunction with application protection as a very heavy load could be put on the application, especially as protection testing is performed in an intrusive mode. Further, it could be decided that source code analysis could be performed each time the applica- tion is modified and in random moments in case we need to detect unexpected and irregular modifications in the application source or binary code.

In any case, we believe that vulnerability scanning should start from the very beginning with the checking of the application source code and the images on which it can be deployed. The latter is actually advocated by AWS-Vuln and Clair which indicate that images, no matter who creates them, usually incorporate vulnerabilities that could be due to using old software or misconfiguring system com- ponents (e.g., the OS).

The first form enables producing a suitable mitigation plan, focusing on fixing core vulnerabilities. As such, vulnerabilities are addressed at their very root before they propagate to other vulnerabilities, thus making the whole system highly vulnerable. To achieve the production of this knowledge kind, there is a need to employ and enhance catego- risations like CWE to check possible relations between vulnerabilities which need to be coupled with knowledge about what constitutes the application system and how vulnerabilities can be propagated.

The second form can enhance a tool’s capability to discover new vulnerabilities and thus extend its current coverage. This can also enable the IT world to benefit from such a discovery to more rapidly react to new vulnerabilities before becoming exploitable via the development of respective code by adversaries. A tool, which exhibits such functionality, would become immediately the most utilised one as it would be adopted by most systems in the IT world. To support new vulnerability infer- encing, there is a need to incorporate (semantic) rules attempting to deduce the existence of a vulnerability based on security facts or in- cidents. Such rules could be, for instance, derived by employing event

pattern mining techniques over security logs. While this is a practice currently followed in a manual manner by well-known software pro- viders, it could be an added-value to automate and integrate it in the cloud application management system. This automatic rule production would reduce detection costs and accelerate application evolution to- wards resolving the vulnerabilities detected. By also employing an approach towards properly identifying and publishing vulnerabilities, the community will benefit via: (a) the reduction in effort and time in vulnerability publishing; (b) the rapid addressing of new vulnerabilities.

The vulnerability world is dynamically evolving constantly such that new vulnerabilities are detected each day. As such, a user is usually faced with a great number of vulnerabilities inferred just for a single applica- tion. While a prioritisation of vulnerabilities based on their risk could enable the user to focus more on the most critical ones, we believe that this should be complemented with the capability for automatic vulner- ability mitigation. The latter can enable the user to be burdened only by those vulnerabilities still critical and not automatically solvable.

The way to achieve this is to follow a twofold approach. First, an approach based on SCAP must be followed via which vulnerabilities can be automatically mitigated. This requires vulnerabilities to be coupled with mitigation specifications which can be automatically executed by scanning tools once the vulnerabilities are detected with a high confi- dence. This requires a great community effort, similar to that devoted already for OpenSCAP, involving major software providers and users. The main aim is to produce a global repository for vulnerability mitiga- tion, exploitable by any scanning tool.

The evaluation results indicated that different tools employ different architectures in vulnerability scanning, i.e., the standalone and client- service ones. In our opinion, both architectures are valid and must be supplied by a tool to cover all possible users and the diversification of their requirements. This can certainly improve a tool’s applicability. It can also enable it to scale well to cover scanning big applications. Scanning tools also need to be accompanied with an API due to the great benefits it offers, including: (a) ability to integrate the tool in a stand- ardised way with other tools with similar or complementary function- ality; (b) ability to abstract away from technical specificities and enable a clearer and more user-intuitive tool usage and administration; (c) ability to interface with a tool to create suitable visual vulnerability scanning and risk assessment UIs, enabling users to better browse and inspect in a clearer and user-intuitive way the scanning results.

Apart from abstracting over different vulnerability scanners, the use of APIs could lead to Vulnerability Assessment as a Service scenarios. In such scenarios, users are not burdened any more with tool maintenance, administration and configuration. They can also benefit from this ser- vice’s flexible pricing model to reduce costs by using the scanner only when needed. Such a service can be also easily configurable by users based on their requirements by abstracting from any tool specificities. Such a configuration could come via profiles playing the role of tem- plates, further evolvable by users based on their requirements. Such a service could also encompass an extra-priced tool agglomeration feature, enabling to optimally satisfy user requirements.

Most tools usually report a set of vulnerabilities assigned to a certain risk based on either a manual mapping approach or the existence of already modelled mapping knowledge in form of CVSS descriptions. In both cases, however, we see just the reporting of individual vulnerability risks with no capability to aggregate them to deduce the overall application risk. The latter capability is already exhibited by some prototypes [84,85] but not to the full possible extent covering the whole application system. As such, we expect that research should advance the current risk computation algo- rithms to cover the whole application system while tool providers should borrow the main research results and incorporate them in their tools for advanced reporting reasons. As such, this advanced reporting capability will increase the tool’s added-value can be beneficial for users as: (a) knowledge about the whole application risk can enable selecting the most suitable mitigation strategy; (b) a deeper root cause analysis could be fol- lowed to unveil further vulnerabilities and better resolve them.

While most tools focus on each application component individually, this is not sufficient as illegal behaviour could be detected also in component interactions. As such, there is a need to consider the appli- cation topology to have a holistic view about the whole application, its structure and how its components interact with each other along with their dependencies (e.g., hosting, communication). Further, we should focus on both the current topology description and its extension derived via reasoning. For instance, while examining a component that might be hosted on a certain container, we should not focus just on both of them but also on other components not currently covered by the topology, such as improper or non-initiated system processes. Such information could then need to be exploited towards constructing suitable vulnerability detection rules focusing on scanning the interactions between different application components.

The above direction requires to constantly observe the application’s current, effective topology and to reason about its extension. Such a requirement could only be fulfilled by the proper cooperation between the scanning tool and the (cloud) application management system. The latter, in particular, should have the right, standardised interface via which the tool could obtain the extended application topology. Further, it should possess the right abilities and sensing mechanisms to sense the extra components involved in a certain system to correlate them to the current application topology.

Going beyond topology models, there can be cases where different user applications communicate to each other. Thus, one serious vulner- ability in one application might have the risk to be propagated to another. However, by knowing the topology models of applications that communicate to each other, the scanning process can focus on also checking cross-application vulnerabilities (e.g., side effects of wrong transactions initiated by one, already infected application and handled by another application). This is a novel research direction, not considered before in the literature.

knowledge about which application pairs communicate with each other might not be given by the user. In fact, it is possible that each application is handled by a different management system or different instances of such a system. As such, it should be the vulnerability assessment system that must infer this knowledge; (b) actual cross-application vulnerabil- ities might be hard to be defined and detected while requiring special knowledge to be given by the user (e.g., what is a wrong transaction and how someone can detect it).

The way this can be resolved is twofold. First, by usually inspecting each application from the pair in an individual manner. This relies on the rationale that each application could be considered as an end-user for the other. As such, this end-user could be considered as one source of vul- nerabilities, usually checked by data validation testing and other scan- ning technique kinds. Second, by allowing users to supply their own checks that focus on detecting those inconsistent system states which could make the whole system or an application as vulnerable. This can be considered as a kind of user-specific vulnerability detection, enabling a scanning tool to go beyond the detection of known vulnerabilities to- wards application/domain-specific ones. This would certainly contribute to a better tool completeness and suitability. Coupled also with the ability to define certain mitigation actions when anticipating such in- consistencies would make the vulnerability scanning tool a full, advanced application security protection software, further enhancing its added- value and applicability.

Malware and antivirus software is widely adopted by both organisa- tions and individuals due to the continuous and, in many cases, sophis- ticated risk mitigation support that it features. However, such software takes a reactive approach in dealing with security issues. On the other hand, while vulnerability scanning tools follow a proactive approach by identifying those places in the application system that need improvement to avoid security issues from happening, they are not widely used in practice. Further, even when decided to be adopted, the great tool di- versity and varied coverage makes it hard for a practitioner to choose the right vulnerability scanning tool. To this end, this article offers the following contributions: (a) it guides practitioners towards making an informed decision about which vulnerability scanning tools and data- bases to select for their applications. Such a guidance is supplied through a comparative evaluation approach that relies on a two-level, hierar- chical comparison criteria framework for both scanning tools and data- bases. The evaluation results supplied unveil which are the best scanning tools and databases per each criterion, category of criteria and in overall;

OWASP has proposed a vulnerability tool categorisation/taxonomy which comprises three levels. At the first level, the two top categories of: (a) web application vulnerability detection tools and (b) web application protection tools (e.g., malware and antivirus tools) exist. The former tools are then cat- egorised at the second level into: (i) threat modelling tools, (ii) source code analysis tools (SAST), (iii) vulnerability scanning tools, (iv) interactive application security testing tools (IAST), and (v) penetration testing tools. Penetration testing tools are further classified based on the following partitions: information gathering tools, configuration management testing tools, authentication testing tools, session management testing tools, authorisation testing tools, data validation testing tools, denial of service testing tools, web services testing tools, Ajax testing tools, HTTP traffic monitoring, encoders/decoders, and web testing frameworks.

In this respect, the first approach step involved the proper identification of the different vulnerability kinds that can be detected. Towards realising this step, we have made an investigation of different frameworks that might be available in the literature. Such an investigation ended-up in selecting the framework in sectoolmarket. com which comprises a vast amount of the most frequent web application vulnerabilities categories. The focus of this framework is on web application with the rationale that usually the focus is mainly on this kind of vulnerable object and not all possible ones. In fact, we can actually deduce that there is a lack of frameworks that attempt to categorise vulnerabilities beyond this object kind.

Once the right set of vulnerability categories was identified, the second step of the followed approach involved the assessment of each vulnerability scanning tool considered based on whether it covers each of the categories identified. Towards this goal, we have relied both: (a) on the actual findings2 from a previously conducted research that is reflected in sectoolmarket. com about the coverage of a certain set of open-source vulnerability scanners as well as (b) on an individual evaluation, conducted by us, of those scanners which were not included in the aforementioned evaluated set.

It might be observed that some scanning tools have the same coverage. This is due to the fact that all these tools rely on OpenVAS. As we were not able to assess them easily over their actual coverage, we have considered that their accuracy could not be less than that of the sub-tool that they exploit. In this sense, as the coverage of OpenVAS is the best possible, mapping to the top coverage partition, our consideration is precise according to the scale that has been adopted. Further, we should also note that we were not able to assess Vulcan as its code was not available. However, based on its respective documentation, it relies on a certain limited scanning framework which is expected to have a very low coverage. So, our estimation should be correct here.

