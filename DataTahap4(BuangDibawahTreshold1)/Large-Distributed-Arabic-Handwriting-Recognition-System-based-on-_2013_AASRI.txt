This paper proposes a robust, efficient and scalable distributed Arabic handwriting OCR system based on a parallel FastDTW algorithm via cloud computing technologies. The three techniques Hadoop, MapReduce and Cascading are used to implement the parallel FastDTW algorithm. The experiments were deployed on Amazon EC2 Elastic Map Reduce and Amazon Simple Storage Service (S3) using a large scaled dataset built from the IFN/ENIT database.

Conducted experiments and evaluations on several Arabic handwriting OCR systems show and confirm that : in the first hand, the euclidean distance technique is used for classification. However, this technique is less robustness and more fragile [5]. In the second hand, the Dynamic Time Warp (DTW) algorithm stands among the best techniques for such a mission [6].

The major problem of the DTW is the slowness of its response time because of the enormous amount of computation to achieve [7]. Distributed system, such as cloud computing technologies, provides viable framework to speed up the time of the OCR system based on DTW algorithm. Cloud computing is primarily used to deliver many services such as Infrastructure (I), Platform (P) and Software (S) as services. All these services are available to consumers as registration based services in a pay-as-you-consume model [8].

This paper is organized as follows: an overview on the DTW algorithm and especially the FastDTW and the use of them in Arabic character recognition, is presented in section 2. Hadoop, MapReduce and Cascading models are presented in section 3.The proposed approach is explained in section 4. Experimental and results are presented and discussed in section 5. Conclusion and future work are presented in the last section.

DTW presents many disadvantages such as the time and space complexity which are exponential. This model is practical only for small and medium data sets (<3,000) and time series are often very long [11]. The FastDTW algorithm can be a solution to solve this problem. FastDTW is based on the multi-resolution

FastDTW for Arabic handwriting recognition system consists to prepare a reference database of R trained Arabic alphabet and number in some given scripter, and presented by Ci, i = 1,2, …R. Our approach consists on using the FastDTW pattern algorithm to classify the character to recognize against the template library. Thus the input character is classified as the best character that gives the optimal time alignment p among all R characters.

MapReduce [13] is a tools using to parallelize problems that process large datasets with different computers (nodes) (distributed architecture) like a cluster or a grid computing . Amazon Elastic MapReduce provides the option to analyze vast amounts of data. This advantage is offered by distributing the computational work across a cluster of virtual servers running in the Amazon cloud. All clusters are managed using an open-source framework called Hadoop.

Hadoop [14] is a distributed infrastructure for processing large-scale data. This infrastructure can be used for single machine. The real power of this architecture lies on the ability to use hundreds or thousands of nodes, each with different processor cores. The Hadoop model is also used to share efficiently huge work across different machines”. Hadoop does this by the storage layer that manage large amounts of data, and the running layer that parallelize the execution of the user application using coordinated data subsets.

Cascading website [15] define Cascading “is a framework written with Java language that helps typical developers to easily and quickly develop Data Analytics and Data Management system that can be deployed and managed by a variety of computing environments.” This model is based on a metaphor of data streams called pipes and data operations called filters. Thus, the Cascading API allows the developer to regroup pipe assemblies that do many actions such as the split, the merge,.. of data while applying operations to the different data record.

In our case, the model master–slave and the SPMD (Single Process, Multiple Data) architecture are used on the distribution of the memory .This model is applied to the FastDTW algorithm as the parallelization technique. The distributed FastDTW approach consist on running each copy of the single program on independently processors and Hadoup is the responsible of the communication between processors .

Amazon Elastic MapReduce execute automatically the Hadoop program of the OCR application on different Amazon EC2 instances. First, the map function is applied, this function consist on sub-dividing the huge amount of documents in a job flow into smaller process so that they can be processed in parallel. Second, the reduce function that consist on merging the processed data into the final output is applied (Fig2). The Amazon Simple Storage Service (S3) is in the first hand, the source for the data to process and in the second hand, is the output destination.

To examine the proposed idea, a corpus with 16000 pages (370 characters/page) and a reference database formed of 345 shapes representing approximately the different Arabic alphabet randomly chosen from the Arabic handwritten word images dataset IFN/ENIT[16] are used. For the preprocessing image, the IFN/ENIT dataset was already normalized [16]. Wavelet transform [17] is used as a features extraction technique.

*2, 2 GB of RAM executing a Windows XP operating system, Cygwin [18] is the shell to run Linux command. We used java as a programming language and JDK 1.6 was installed. Eclipse 3.4 was used to program and built our application. 100 MG bits/s was the network capacity.

Based on the state art of the cloud computing technologies [19], Amazon Elastic Computing Cloud have selected for the implementation of our approach. In order to verify that distributed FastDTW functions correctly in cloud technologies, we created six running Jobs flow on the Amazon Elastic Computing Cloud service. We have allocated 100 cores using the three Standard Amazon EC2 Instances. First, the “small” instances each with 1.7 GB of memory, 160 GB of instance storage, and 32-bit platform. Second, the Large

Instance 7.5 GB of memory, 850 GB of instance storage, 64-bit platform and finally the Extra Large Instance 15 GB of memory, 1690 GB of instance storage and 64-bit platform. S3 [20] is used to manage the input and output data.

The average duration of the test time in a sequential mode (a single computer) using DTW and FastDTW are approximately and respectively 9 hours and 8 hours and the average test time in distributed mode and for 100 computers are 0.136 hours and 0.118 Hours. These show that the sequential mode allows recognizing only 18 and 21 characters per second for the two algorithms described above. However, the results in the distrusted mode, illustrated in figures 3and 4, in particular :

