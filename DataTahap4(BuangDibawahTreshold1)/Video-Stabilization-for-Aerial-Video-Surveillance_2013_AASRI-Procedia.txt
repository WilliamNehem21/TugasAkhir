Aerial video stabilization system aims to remove undesired motion in aerial v deo. This motion is the result of undesired movement of mobile sensor. In this article we present a new video stabilization system for Unmanned Aerial Vehicles (UAV). Our system is based on keypoints tracking. We use Scale Invariant Feature Transform (SIFT) keypoint detection, and matching to estimate parameters of affine transformation model. Then, Kalman filter with median filter is applied to remove video noise. A number of real aerials videos surveillances demonstrate that this method can achieve good performance.

Techniques of video stabilization can be divided into four groups: optical approach, mechanical approach, electronic approach and digital approach. In this paper we focused on digital approach. This technique is an image pre-processing. All digital stabilization systems handle three essential aspects. The first one estimates the global motion, the second one concerns the motion smoothing and the last one is related to the motion compensation. Among these components, the step of global motion estimation is the most vital but also the most difficult one [9].

In case of a fixed camera, a strong winds or small vibration from heavy traffic can caused the global motion. In this case background is almost fixed over the long term [15]. Consequently, motion can be calculated by local point tracking [1], or by searching a region that contain few motions [7]. In the case of mobile platform, on which the research is conducted, the global motions include two elements: the intentional and the unwanted motion. Several efforts have been put in case of mobile camera. Block matching techniques improves motion estimation by using different adaptive filters [9]. These method present good results if the video does not contain moving object [8].

In this paper we present a new system to stabilize aerial video surveillance by extracting and matching SIFT point for consecutive frames. In the following, Sec.2 cites some related works on video stabilization; Sec. 3 describes our proposed system. Results achieved by our system are presented in Sec.4. Finally Sec.5 presents summarized conclusions.

Our input is a real video captured from UAV. First of all, SIFT point are extracted and matched for two consecutive frame. Next, inter frames motion is estimated using affine transform model. Finally, both Kalman filtering and median filtering are used in the step of frame compensation. The details are explained as follows.

In our approach, we estimate global motion vector by extracting SIFT points from two successive frame. Next we calculate local motion vector by matching they two sets of invariant features. In other words, local motion vector between frame n-1 and frame n, can be estimated by extracting both the first and the second keypoints Kpoint1 (xpoint1, ypoint1, 1) and Kpoint2 (xpoint2, ypoint2, 1) from these two frames. In this step, we can show how the keypoint has probably moved from two successive frames. Then we use RANSAC (Random Sample Consensus) to select optimal matching. But, by this method, we obtain a whole number of local motion vectors. They sets of vector does not contain helpful indication for real movement of the camera because they include matches related to moving objects in the frame. Deal with this problem we assume that, comparing to other motions, the velocity of moving objects in the scene is very large. For this reason we use a fixed threshold to eliminate moving object. As a result, we can generate the transformation matrix.

Motion can be described either by a 2-D model or by a 3-D model [2]. The various transformations occurring in the 2D plane are Translation, Euclidean or rotation, Similarity and Affine. Thus, in our method we adopt a four parameter 2-D affine estimation model to describe geometric transformation between two consecutive frames. Given a point localized as Pn(xn, yn ,1) in framen, and located as Pn+1 (xn+1,yn+1,1) in framen+1, the transformation model from Pn to Pn+1 can be described as:

In this final step, we need to correct the current frame to obtain stable image. But parameters calculated in equation (1) contain two types of motion: motion of the sensors and normal movement of the UAV. To compensate the current frame we should separate these two types of motion.

We test our system on a variety of scenes from VIRAT Aerial Video dataset [16] containing low resolution sequences of 720 x 480 pixels captured in 30 fps. This aerial dataset is characterized by zooming, varying viewpoints and scale. The results of the proposed system are illustrated in Fig. 2

The PSNR gives a relation between the desired output and the obtained video. In this equation, MSEn measure the Mean-Square-Error between successive frames, IMAX is the maximum pixel value of an image. Frame dimensions are represented by N and M. The PSNR value for each frame of the original video and our stabilized video are shown in Fig 3. Higher PNSR between two stabilized frames represent good quality of stabilized video.

TIF is the average of the PSNR between two consecutives frames. In general this average is used for each value, to obtain an approximate estimation of the quality of the stabilized video. Similar to PSNR, upper ITF values indicate super quality video stabilization. ITF values for three video sequences tested are shown in Table1. This evaluation illustrate that, the ITF of our stabilized videos is superior to the ITF of the original videos. The ITF of our stabilized videos enhances, which is acceptable.

A new system for aerial video stabilization has been introduced in this article. The main idea of this system is to filtered undesired motion by detecting and matching SIFT point in order to predict the interframe motion. To evaluate our system we used real video captured by a camera installed on UAV. The experimental results prove the efficiency and accuracy of our stabilization system. Our future work will concentrate on performing motion estimation by integrating optical flow in the process of local motion detection.

Sangmin O, Anthony H, Amitha P, Naresh C, Chia-Chih C, Jong TL, Saurajit M, J. K. A, Hyungtae L, Larry D, Eran S, Xioyang W, Qiang J, Kishore R, Mubarak S, Carl V, Hamed P, Deva R, Jenny Y, Antonio T, Bi S, Anesco F, Amit RC and Mita D. A large-scale benchmark dataset for event recognition in surveillance video. Computer Vision and Pattern Recognition (CVPR), pages 527 â€“ 528, 2011.

