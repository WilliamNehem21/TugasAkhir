Clustering, also known as unsupervised classification, aims to divide datasets into subsets or clusters according to the similarity measure of the data sample (physical or abstract) such that the data samples within the subset or cluster have a high degree of similarity and that the data samples belonging to different subsets or clusters have a high degree of dissimilarity [1]. Currently, cluster analysis plays an important role in many fields such as social sciences, biology, pattern recognition, information retrieval and so on [2]. It is so useful in machine learning and data mining that many researchers have paid much attention to it. Over the past few decades, a number of excellent clustering algorithms have been developed for different types of applications. Typical algorithms include K-means [3] and K-medoids [4] based on partitioning, CURE [5] and BIRCH [6] based on hierarchy, DBSCAN [7] and OPTICS [8] based on density, WaveCluster [9] and STING [10] based on grids and statistical clustering [11] based on models.

to the same cluster as its nearest neighbor of higher density. The DPC algorithm is simple and efficient, and it can quickly find the high density peak points (cluster centers) without iteratively calculating the objective function. Moreover, it is suitable for cluster analysis on large- scale data. Although the DPC algorithm has obvious advantages over other clustering algorithms, it also has some shortcomings: the accuracy

The first aspect is improving the density measure of the DPC al- gorithm. Du, Ding and Jia [13] proposed DPC-KNN, which introduces the concept of K-nearest neighbors (KNN) to DPC and provides another option for computing the local density. They also employ PCA to reduce the dimensionality of data. However, the method still suffers from the limitations of DPC because it applies the same procedure in determining

the local density ùúåùëñ is positively correlated with the number of points of a point for scanning its neighborhood, which is set by the user. Thus, with a distance from ùëñ less than ùëëùëê . The most obvious difference be- tween the two methods is that for Eq. (1), ùúåùëñ is a discrete value, whereas for Eq. (2), it is a continuous value. However, for both methods, ùúåùëñ is sensitive to ùëëùëê .

The second aspect is to automatically recognize the numbers of clusters and cluster centers. Liang and Chen [15] proposed 3DC, which introduces a divide-and-conquer strategy to determine the ideal number of clusters. However, it ignores the local structure of the datasets which may cause missing clusters. Xu, Wang and Deng [16] proposed DenPEHC, which could automatically detect all possible centers and build a hierarchy presentation for the dataset. Nevertheless, both 3DC and DenPEHC will aggravate the propagation of errors due to the hier- archical clustering strategy. Li, Ge, and Su [17] proposed an automatic clustering algorithm for determining the density of clustering centers. In this algorithm, it is considered that if the shortest distance between a potential cluster center and a known cluster center is less than the

dataset. In Fig. 2(a), we recognize the outer ring as noise and let the cluster number be 2. However, clearly, the result cannot satisfy us. In Fig. 2(b), we recognize the outer ring as a cluster and let the cluster number be 3. Even when we choose the correct cluster center in the outer ring, we cannot obtain the ideal result.

select cluster centers automatically. Finally, the clustering algorithm based on probability propagation can help us allocate all the remaining data points. By doing all these, DPC-PPNNN can be suitable for more complex datasets and distinguish two clusters that are close to each other.

does not require parameters in the selection of neighbors. The size of NNN of every data point may be different according to the distribution of the dataset. The following is a precise description of the natural neighborhood method through the definition of the relevant concepts.

nearest neighbors and the propagation will continue until there is no neighbor that can be infected. At this time, we recognize all the infected data points as a cluster. We call the processes of forming a cluster a round of propagations. Second, we select the data point that has the

In this part, we select a number of synthetic datasets that are widely used to test the performance of clustering algorithms. These datasets are different in terms of the distribution and numbers of points and clusters. They can simulate different situations to compare the performance of various clustering algorithms in different scenarios.

In Fig. 4, we recluster the three counter examples in Section 3.2 by DPC-PPNNN. Clearly, the results are much better than the original DPC which implies that our algorithm can overcome the deficiencies of DPC. Next, we will show some typical clustering results on the synthetic datasets by DPC-PPNNN and the algorithms of the control group. The points with different colors in the figures are assigned to different

In Fig. 5, we can see that the 2d-4c-no9 dataset has four clusters, of which one cluster has a very high density. DPC-PPNNN succeeds in detecting all of them, while the other algorithms of the control group fail to do so.

and DBSCAN can detect the clusters in the Cassini dataset. Although DPC can find the correct cluster centers, it fails to allocate the other remaining data points correctly. The three clusters are not uniform in shape which leads to the wrong cluster results by K-means.

The Fig. 9 shows the results of the four algorithms on the Complex9 dataset. DBSCAN can recognize all the clusters successfully. While for DPC-PPNNN, there are some misclassified points at the tail of a cluster because these points are not connected tightly enough. DPC finds the correct cluster centers but it fails to allocate the other remaining data points correctly. K-means has poor performance.

Fig. 10 displays the results on the Compound dataset. DPC-PPNNN can detect most of the clusters correctly and can also recognize noise. DBSCAN is good at recognizing noise but it also misclassifies many data points as noise. DPC and K-means fail to recognize the clusters with noise.

As shown in Fig. 11, the Dartboard1 dataset has four concentric rings. Both DPC-PPNNN and DBSCAN have perfect performance. DPC cannot find the correct cluster centers because the densities of the data points have little difference. Moreover, K-means has poor performance. For the Jain dataset shown in Fig. 12, only DPC-PPNNN correctly identifies all clusters. DPC has not found the correct cluster centers because the difference between the densities of the two clusters is too large. The reason is the same for the poor performance of DBSCAN.

Fig. 13 displays the results on the R15 dataset. The distribution of points makes it the most straightforward dataset for all the algorithms. Although there are some small defects among them, all the algorithms can recognize both the clusters and centers.

In this section, 8 UCI datasets, as shown in Table 2, are used to demonstrate the performance of the DPC-PPNNN clustering algorithm. These datasets are different in terms of sample number, feature number and cluster number. As shown in Table 4, DPC-PPNNN performs almost the best among the test cases.

In this paper, we proposed an improved probability propagation al- gorithm for density peak clustering based on natural nearest neighbors (DPC-NNN). The new algorithm does not require any parameters and can recognize cluster centers automatically. The final clustering process of the DPC-PPNNN motivated by the epidemic spread performs well especially for distinguishing two clusters that are close to each other.

