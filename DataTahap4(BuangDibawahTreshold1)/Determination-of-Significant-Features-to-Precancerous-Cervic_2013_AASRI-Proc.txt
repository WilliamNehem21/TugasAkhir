examinations. From a practical point of view the authors try to extract patterns from the processed images to classify the existing cervix lesions with diagnostic purposes. The resulting attributes of the image processing were analysed using supervised classification techniques of data mining.

In recent times there have been major advances from a computational point of view in digital image processing and its subsequent analysis for diagnostic purposes. Parallel to this, techniques such as data mining and machine learning can provide a set of methods that could be used to detect patterns of behavior on large amount of data. One such technique for preparing a database for data mining processing is feature selection. Feature selection serves to identify the best subset of features for a particularly given data mining task. Although the minimum number of attributes that can be used is debatable, for instance, in the classification task, we may assume that the more attributes the higher the discriminatory power. However, several experiments with learning algorithms have shown that it is not always so because, as it has been detected, some experiments have had high runtimes, others have had very high occurrence of redundant or irrelevant attributes while showing a degradation in their classification power [11]. Different experiments have shown that feature selection decreases the error rate of classifiers. This is so because through this process we try to choose the minimal subset of attributes according to following two criteria: first that the hit rate does not drop significantly, on the contrary, it is desirable that it increases. Second, that the distribution of the resulting class be as similar as possible to the original class distribution when all attributes are taken into account. In this

Current literature surveys show several works related to feature selection methods focused on search techniques, their applications in classification, comparisons, clustering, introduction of new methods, and combination thereof as indicated in [4], [5]. [6], and [8]. In other medical areas the work of Martin et al. [1] applied feature selection methods available in WEKA [2] to a database containing variables involved in the nutritional status of children aged 6 to 11 years. The purpose of that study was to specify which method determined the factors that contributed the most to nutritional assessment. In another study Blakrishnan [3] tried to find an optimal feature subset of the Pima Indian Diabetes Dataset using Symmetrical Uncertainty Attribute Evaluator and Fast Correlation-Based Filter. Guyon et al [7] studied the problem of selecting a small subset of genes from broad patterns of gene expression data recorded on DNA micro-arrays utilizing Support Vector Machine methods based on Recursive Feature Elimination. The studies just mentioned have the common goal of comparing the performance of attribute selection methods with the results obtained by learning algorithms and thus, determining which method significantly improves the results from different situations, with diversity of information, and high or low dimensionality.

Table 1 shows the algorithms used for feature selection using the data mining tool Weka version 3.6 [2]. The algorithms that evaluate subsets of attributes are distinguished with the letter s. Likewise, the algorithms that evaluate the total set of attributes are distinguished with the letter t. The s-algorithms were combined with

The present study was conducted on two groups of cervix images. One group had images with acetic acid application and the other images with Lugol's iodine application. There were 63 textural features extracted from these images; 21 per each layer Red, Green and Blue (R, G and B). Each subset generated by feature selection methods (scenarios in Table 3) was tested with different classifiers.

For the Lugol’s iodine images, the best classification accuracy was obtained with the S5 scenario that correctly classified 82.46% of the images using the metaclassifier Decorate from decision tree RandomTree. In acetic acid image group it was observed that the use of feature selection methods had no benefit for the classification process. This is due to the fact that the highest percentage of correctly classified instances (71.93%) was obtained with S1. In this latter scenario no feature selection method was used and all attributes were considered in the classification process. From all the images under consideration we can observe that the set of images with Lugol’s iodine provided better accuracy results based on the percentatge of correctly classified instances.

Table 4 shows the results obtained by discriminating between healthy and sick classes with the two types of images. For the Lugol’s iodine images, the highest percentage of rated instances was 89.47%. This was obtained by the metaclassifier AdaBoostM1 from REPTree decision tree using S4. For acetic acid images, the highest percentage obtained was 84.21% using the Decorate metaclassifier from J48 decision tree with a S5.We can also observe that the percentage of correctly classified instances is significantly increased when the AG and BG classes were grouped into a single sick class.

The experiments with the sick class for the two types of images produced the best results, namely, the LADTree decision tree provided a 86.67% instances correctly classified in a S3 for Lugol's iodine images and the metaclassifier AdaBoostM1 and REPTree decision tree providing a 89.47% with a S9 for acetic acid images.

Table 5 presents a summary of the scenarios and classifiers that provided the best performance in each experiment, class, and group of images. The best results were obtained by analyzing the cervix images with Lugols’ iodine, combining the sick classes, and performing the discriminating classification using only two classes in each case. Currently data analysis real life applications clearly show the need to manipulate a reduced number of attributes. The experiments performed in this study shows that the feature selection is a process that provides significant benefits because the obtained models are more understandable and perform better the learning algorithms than when the complete data set is used.

