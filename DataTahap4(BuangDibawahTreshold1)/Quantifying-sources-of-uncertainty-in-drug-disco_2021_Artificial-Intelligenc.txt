Combining these seven components gives the generic formulation of a PPM (Eq. (4)). Although this equation is abstract, it captures where uncertainty can reside. In this section we carefully describe the terms in the equation and then provide a concrete example.

we might represent our uncertainty in 𝑦 for a given compound with a as “is distributed as” or “is generated from”. To make this concrete, Gaussian distribution that has a mean 𝜇 = 2.45 – which represents our best estimate of 𝑦 – and a standard deviation of 𝜎 = 2.1. This would be written as 𝑦 ∼ Normal(2.45, 2.1).

𝑓𝜇 (𝑥; 𝜃𝜇 ) (orange curve) and based on Eq. (5). 𝐺 is the Gaussian data generating distribution with a mean 𝜇 and a constant variance 𝜎, which models the spread of points around the line. Link functions for 𝜇 and 𝜎 are not used and hence are

standard deviation of 𝜎”. But how does 𝑦 depend on 𝑥? The second line shows how 𝑥 enters and how it depends on two parameters: 𝜃1 and 𝜃2 (𝑒 is a constant, not a parameter). We set this equation equal to 𝜇 and can substitute it for 𝜇 in the first line of Eq. (5) giving

A nice feature of PPMs is that they are generative, meaning that they can generate or simulate data. Indeed, simulation and learning are oppo- site sides of the same coin: learning takes the fixed data and infers likely values of the parameters that could have generated the data, whereas simulation fixes the parameters and generates the data. The model in Eq. (5) generated the data in Fig. 2 and we will use it as a running example throughout

To simulate a value for 𝑦, we need to (1) select parameter values, and we use the following: 𝜃1 = 3.25, 𝜃2 = 0.2, and 𝜎 = 0.1; (2) select a value of 𝑥, which enables us to calculate 𝜇; then (3) draw a random number from a Gaussian distribution with a mean of 𝜇 and standard deviation of

𝜎. This can be repeated any number of times to obtain the 𝑃 (𝑦 𝑥) distri- bution, and for different values of 𝑥. The data in Fig. 2 were generated for 100 𝑥 values uniformly distributed between 0 and 1. Note that 𝜃𝜇

that is, we don’t know the form of Eq. (5). Uncertainty in the mean function is also called model uncertainty, but this term is ambiguous because models have multiple components. Choices for the mean func- tion include which predictors, interaction terms, transformations, basis expansions, hierarchies, and time-varying components to include in the model. Assume we only observe the data in Fig. 2, several models we

Fig. 3. Model averaging. Three models fit the data well (A-C), even though none are the true model. They make different predictions at low assay values and when extrapolating to higher values. The mean predictions are superimposed for easier comparison (D). Model averaged prediction (E). Comparison of prediction interval widths (F). Shaded regions are the 95% prediction intervals.

fore capture the main trend in the data. Which model should we use? Typically, only a single model is selected and predictions are made from that. If one model is clearly better than the others, there may be little lost by using one model for predictions. However, if two or more models fit the data equally well, making predictions from only one will under- estimate the prediction uncertainty. Fortunately, we are not forced to choose one model but can fit several and combine their predictions. To illustrate, we will use the quadratic, 2-parameter exponential, and 3-parameter exponential models. The three models are fit to the data (Fig. 3A–C) and predictions are extrapolated to show both how similar the fits are where there is data, and how different the fits are when ex- trapolating. The shaded regions show the 95% prediction intervals (PI), and if a model is suitable, we expect 95% of the data to fall in the shaded region.

five models considered above. Most machine learning methods only use the single best value of each parameter when making a prediction. But since the parameters are learned from the data, they are uncertain, and this uncertainty should be propagated into the prediction. Parameter uncertainty decreases as the sample sizes increases, so to better illustrate the effect of parameter uncertainty on predictions, a smaller dataset was made by taking every eighth data point from the previous example.

val from a Bayesian model that accounts for parameter uncertainty. The dashed black lines show the 95% PI from a classic quadratic regres- sion model which ignores parameter uncertainty, and note how they are slightly narrower. The mean function is identical for both the Bayesian and classic model.

The difference in PI width may seem negligible when focusing on the mean prediction, but ignoring parameter uncertainty gives approx- imately 7% narrower PIs. This may be important with “point of depar- ture” calculations when the tails of the distributions are more important than the means [50]. Assume clinical outcomes above 1.2 are considered

Models typically have many more parameters than this example (the state-of-the-art Generative Pre-trained Transformer 3 (GPT-3) deep learning language model has 175 billion parameters [6]) and simply collecting more data is often not an option to reduce parameter uncer- tainty because more data enables more complex models to be fit (e.g. including nonlinear terms and interactions), which then increases the number of parameters.

Hyperparameters are a diverse set of tunable options that affect the training and predictions. Unlike parameters, they are not estimated from the data but selected by the analyst; examples include the amount of regularisation in a lasso model, the number of trees in a random forest model, or the cost function in a support vector machine. Suitable val- ues are typically found by trying several options and selecting the best using crossvalidation. In the hyperparameter category we can also in- clude options that are rarely part of a formal selection process such as the choice of optimisation algorithm or random number seed for mod- els with a stochastic component. For fully Bayesian models we can also include parameters for prior distributions, which are not updated by the data. These hyperparameters are selected pragmatically to provide good predictions, but other sets of hyperparameter values might give equally good predictions, on average, but slightly different predictions for each test compound. Hence, uncertainty in hyperparameter values is rarely taken into account. A further complication is that most hyperparame- ters are not related to any biological or chemical quantity of interest and hence it is unclear what the uncertainty is actually about. Nevertheless, Lakshminarayanan and colleagues showed that by running many mod- els with a different random seed, the ensemble of predictions performed better than a single model, and the distribution of predicted values pro- vided a measure of uncertainty [31].

hence are uncertain. Furthermore, some predictors such as cLogP are the output of other (imperfect) prediction models and therefore are also uncertain. Finally, some predictors are not measured directly but are es- timated from a standard curve, which introduces additional uncertainty because the curves may not be not perfectly calibrated.

where samples or experimental units are assumed to have the same ex- posure but actually differ. For example, several wells in a microtitre plate are given the same concentration of a compound, but the true concentration may differ due to variations in the amount of compound dispensed, or, wells on the edge of a plate may have greater evapora- tion of the solution and thus have a higher effective concentration of the compound. Compound toxicity classifications can also introduce Berk- son error. A compound may be classified as “severely” hepatotoxic, even though most people tolerate the compound well and only a few expe- rience severe reactions. The class label is therefore defined by a few members of the class instead of the majority response.

Berkson error can be introduced when converting continuous values into bins or groups. For example, compounds are categorised as active versus inactive, despite having a range of activity values. Or, compounds are classified as having no, mild, or severe toxicity, even though com- pounds will have a range of toxicity levels within each category. Binning can also lead to misclassification error, where a compound is placed into the incorrect category. This can occur if the measured assay value dif- fered from the true value and fell on the wrong side of a threshold. Hence, binning is strongly discouraged [33,35]. Misclassification can also occur due to incorrect diagnoses, labelling errors, or data-entry er- rors. The standard response to these known and often large sources of

Data are truncated when values outside of a range are omitted, and the number of omitted values is unknown. For example, for objects to be segmented as a cell in a standard image analysis, they must have a minimum user-defined cell size. Smaller cells will therefore not be in- cluded in the analysis, and hence both the estimated cell size and prop- erties that are correlated with cell size can differ from their true values, thereby introducing both uncertainty and bias.

is the new value to be predicted. Predictions are more uncertain when measurement error in the test data is included (assuming the training data is measured without error; B). Predictions for the test data when accounting for measure- ment error in the training data using multiple generated data (assuming no error in the test data; C). Averaged predictions from the gener- ated data shows greater prediction uncertainty compared with ignoring measurement error in the training data (D). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this ar- ticle.)

Missing data is the final source of data uncertainty and can arise for many reasons. Imputation is a common approach to deal with missing data, where a plausible value is generated and substituted for the miss- ing value. The imputed value is then taken as the true value, ignoring that it was generated and not measured. A simple way to account for un- certainty in an imputed value is to impute many values – known as mul- tiple imputation – and the variation in the imputed values captures the uncertainty [37,65]. Predictions are the made for each imputed value and the predictions combined.

generated value. This is a form of multiple imputation and Blackwell, Honaker, and King describe a more sophisticated method of generat- ing new data by taking the correlations between variables into account [2]. Each dataset is then analysed separately, and the predictions from each analysis are combined. Variations between the different datasets will lead to different parameter estimates, which in turn will lead to

We use the second method to illustrate the effect of ignoring mea- surement error in two ways. First, we assume the training data is mea- sured without error, but the test data is measured with error. Then we assume that the training data is measured with error but the test data is not. In both cases we compare the result to the standard approach of ignoring measurement error in both the training and test data. The test

The blue distribution in Fig. 5C is again the standard analysis, and the five red lines show the slightly different predictions from each of the five datasets that account for measurement error in the training data (the test data was assumed to be error-free). Predictions from the five datasets are averaged and shown as the red distribution in Fig. 5D, which is slightly wider than the standard analysis from the blue distribu- tion. The additional uncertainty appears negligible in this example, es- pecially compared with uncertainty in the test data (Fig. 5B). However, the variation between datasets is expected to increase with (1) greater uncertainty in the variables, (2) more variables with measurement error included in the model, and (3) more parameters in the model with the total sample size remaining fixed. The effect of measurement error in the training data can be assessed during model development and vali- dation, and if the additional prediction uncertainty is negligible, then the final production model might ignore it.

tions are available but the list can be narrowed down based on back- ground knowledge of the outcome. For example, if the outcome is bi- nary such as absent/present, safe/toxic, or alive/dead, then a Bernoulli distribution is appropriate; if the outcome is a count such as the num- ber of seizures, then a Poisson or negative binomial distribution are two common options; if the data are positive values and skewed such as liver enzyme levels, then a log-normal or gamma distribution may be suitable; if the outcome is an ordered category such as none/mild/severe, then an ordered categorical distribution would be appropriate; if the outcome is continuous and unbounded with no outliers, then a Gaussian distribu- tion may be suitable; and if there are outliers, a Student-t distribution might be appropriate. Many Bayesian textbooks have appendices that list the common distributions and their properties [18,36,38].

Choosing between distributions is made easier because many distri- butions are special cases of other distributions. For example, both the Gaussian and Cauchy distributions are special cases of the Student-t dis- tribution, the Poisson distribution is a special case of the negative bino- mial distribution, and the exponential distribution is a special case of a gamma distribution. Hence, we often don’t need to choose between a set mutually exclusive options, but can select the more general distri- bution and allow the model to determine if one of the special cases is more appropriate. The more general distributions usually have only one additional parameter and therefore do not make the model much more complex. However, not all potentially suitable distributions are related (e.g. gamma and lognormal) and hence two or more models may need to be compared. The data for our running example was generated from a Gaussian distribution and which we have been using for all the mod- els throughout. Hence using the more general Student-t distribution will inform us that the Gaussian is suitable, and so the results are not shown. A key consideration when selecting a distribution function is the bounds of the data. In our running example, the clinical outcome has

a minimum value of zero, but is being modelled with a Gaussian dis- tribution. Since a Gaussian distribution is defined for both positive and negative numbers, there is nothing to prevent negative predictions. A model is clearly inappropriate if it predicts impossible values. Fortu- nately, we can easily define truncated versions of standard distributions, and so we could specify a Gaussian distribution with a lower bound of zero. Fig. 6 shows an example using the 2-parameter exponential model

this probability gets redistributed to positive values (Fig. 6D, the small proportion of the distribution below zero is a plotting artefact). Hence, if training or test data are near boundaries, using truncated versions of standard distributions is sensible. However, if the data are bounded but the values are far from the boundaries, then accounting for such bound- aries may be unnecessary.

1. Values returned from the mean function are unconstrained and can lie well outside this range. Hence, a link function is used to transform the values to respect the bounds. The logit, probit, cauchit, and comple- mentary log-log functions all take unconstrained numbers and compress them into the 0–1 range (Fig. 7). There is no “correct” link function and each provides a different mapping from the unconstrained input to the constrained output and hence gives a different prediction, especially for large values of the input. Link functions are also required for the vari- ances, since variances cannot be negative values, and exponential or power links are often used.

proach assumes that uncertainty in 𝑦 is constant (Fig. 8A). However, spread of points around the mean prediction (Fig. 8). The standard ap- when the variance is not constant, a model for 𝜎 is required (Fig. 8B). Just like modelling 𝜇 as a function of 𝑥, we now need to model 𝜎 as a function of 𝑥. This function could be a simple function of one 𝑥 vari- able or a full neural network for all 𝑥 variables [44]. The latter option

Breaking down the sources of uncertainty into seven items enables us to think about them separately and assess their importance when de- veloping a prediction model. A final model may include several sources and they can be easily combined. For example, suppose variance and link functions were not required but two mean functions and two dis- tribution functions performed similarly and therefore four models with each combination of distribution and mean function are fit to the train- ing data and the predictions averaged. If fully Bayesian models are used,

parameter uncertainty is already account for. And if the test data are measured with error, we can use the approach in 5 B to draw multiple samples for each test sample and feed them all through the prediction models. The more sources of uncertainty accounted for the more com- plex the prediction model. Hence, sources of uncertainty that make little contribution to the overall prediction uncertainty can be ignored.

truncation at zero to represent our prior uncertainty in the parameters. All the models were fit to the data with the Turing package in Julia. The No-U-Turn Sampler (NUTS) was used to update the uncertainty in the parameters after conditioning on the data. Three chains with 10,000 samples each were used and convergence was assessed with graphical

blue circles are the “safe” class. The black line is the optimal separat- ing boundary. A logistic regression model is used to separate the classes and the prediction from the model will be a number between 0 and 1, where 1 corresponds toxic and zero corresponds to safe. This prediction is derived from the mean function and is passed through a link func- tion to constrain the predictions to lie between 0 and 1. We’ll call these

shows that the uncertain samples are all close to the decision bound- ary, and that the most uncertain red points lie near the edge of the data where the location of decision boundary itself is uncertain. The shaded grey region in Fig. 9C represents the uncertainty in the decision bound- ary, and note how the uncertainty is wider at the ends compared with the middle.

the boundary (C). Two compounds with the same mean but different uncertainties (D), have the same uncertainty in the final predicted value for 𝑦 (E,F). Outcome (B). The highest variance predictions (red points) are near the decision boundary and at the edge of the data, and high variance predictions (orange points) follow uncertainty is largely explained by 𝜇 (G) and is similar for the two compounds (H). Parameter uncertainty is nearly 3.5 times greater for the compound with the larger 𝜎 (I). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

Uncertainty in the predicted classes is often divided into aleatoric and epistemic uncertainty [25,28]. Aleatoric uncertainty is supposedly due to “inherent randomness” whereas epistemic uncertainty is due to a lack of knowledge. Without starting a philosophical debate, we take the position that all uncertainty is due to a lack of knowledge [5,26]. Nevertheless, we can decompose our uncertainty into two components, which we call the outcome uncertainty and parameter uncertainty, and which correspond to aleatoric and epistemic uncertainty, respectively.

The above examples used a single distribution function, but flexi- bility can be increased by using mixtures of distributions. For example, outliers can be modelled with a mixture of Gaussian distributions: one to account for the regular observations and the second to account for the outliers. Metabolite, gene, and protein levels are non-negative and often positively skewed, and hence gamma or lognormal distributions may be appropriate. But these distributions are only defined for values

greater than zero, and there may be zeros in the data, which are often dealt with by adding a small value to all data points. A better option can be to model the data with a two-part model, one which accounts for the zeros and the other (e.g. gamma or lognormal) which accounts for the non-zero values. Such “hurdle models” provide this flexibility and also return a parameter that estimates the proportion of zeros, which may be scientifically interesting [13]. Taking this idea a step further, Dirichlet Process models allow us to specify as many distributions as needed to model the data. Instead of specifying a single distribution, we specify a prior over distributions, and learn them from the data (yes, we can specify a distribution over distributions! [42]).

tion to make it a covariance function. Covariance functions are not dis- cussed here but they are also useful for modelling hierarchical or nested data [24,48], and for modelling dependencies in time or space. Neu- ral networks [21,56,68] and Bayesian additive regression trees (BART) [12,60] are other options for flexible mean functions.

In addition to providing prediction uncertainty, PPMs have several other benefits. Hyperparameter values are typically selected by trying many options and choosing the combination that performs best. To avoid overfitting, crossvalidation or a similar approach divides the train- ing data into smaller subsets, some of which are used for training and others to assess performance. But with small datasets, crossvalidation can give unstable models and a poor assessment of performance. Many Bayesian approaches can learn values of some hyperparameters using all the training data and have a built-in prevention of overfitting [64,71]. They also incorporate the uncertainty in the hyperparameters in the pre- dictions. Models can still be compared using only the training data by estimating leave-one-out (LOO) crossvalidation performance, without the computational cost of actually retraining the model for each sample [66,67]. Vehtari and colleagues have also developed methods to assess when a LOO estimate is unreliable, and the model can be retrained only for these samples [69].

ing development (although computations are often much quicker when making predictions). Storage for parameter values may be a problem for large models since this equals the number of parameters times number of Markov chain Monte Carlo draws. These approaches may therefore be harder to scale to large datasets, but faster and scalable algorithms is an active area of research. Another solution to large data is to cleverly select a weighted subset of samples that is much smaller than the origi- nal but captures the essential features. This “coreset” approach enables standard PPM methods to be used on the smaller dataset with little loss of information [8,22].

Finally, not all sources of uncertainty can be captured. Many sources of uncertainty discussed above arise because many modelling options are available, and different choices lead to different predictions. All of the choices relate to the prediction model, but many decisions need to be made outside of the model. We refer to these extra-model choices as the project workflow and they include experimental decisions such as the technology, cell-line, assay, antibodies, protocol, and so on. Also included are data processing pipelines where raw data are cleaned, transformed, categorised, coded, and normalised before they are entered into a prediction model. A single workflow is commonly used, with the untested assumption that variations in the workflow will lead to the same predictions and results. However, variations in workflows and an- alytic decisions do lead to variations results [4,23,32,57,58,61,62].

The ultimate aim of prediction models in drug discovery is to en- able better decision making. Thus, not only should predictions be accu- rate with prediction uncertainty adequately represented, but the results should be easy to understand by decision makers. Fortunately, PPMs provide intuitive results for continuous (Fig. 6D), binary (Fig. 9D), cat- egorical, and ordered categorical outcomes [56,71], as well as for com- pound rankings [34,55]. We have found that safety pharmacologists and other project members can easily interpret the predictive distributions provided by PPMs and value the confidence in the predictions that these distributions provide [34,71].

Juliano AC, Kable JW, Kassinopoulos M, Koba C, Kong XZ, Koscik TR, Kucukboy- aci NE, Kuhl BA, Kupek S, Laird AR, Lamm C, Langner R, Lauharatanahirun N, Lee H, Lee S, Leemans A, Leo A, Lesage E, Li F, Li MYC, Lim PC, Lintz EN, Liphardt SW, Vermeer AB, Losecaat Love BC, Mack ML, Malpica N, Marins T, Maumet C, Mc- Donald K, McGuire JT, Melero H, Méndez Leal AS, Meyer B, Meyer KN, Mihai G, Mitsis GD, Moll J, Nielson DM, Nilsonne G, Notter MP, Olivetti E, Onicas AI, Pa- pale P, Patil KR, Peelle JE, Pérez A, Pischedda D, Poline JB, Prystauka Y, Ray S, Reuter-Lorenz PA, Reynolds RC, Ricciardi E, Rieck JR, Rodriguez-Thompson AM, Romyn A, Salo T, Samanez-Larkin GR, Morales E, Schlichting ML, Schultz DH, Shen Q, Sheridan MA, Silvers JA, Skagerlund K, Smith A, Smith DV, Sokol-Hess- ner P, Steinkamp SR, Tashjian SM, Thirion B, Thorp JN, Tinghög G, Tisdall L, Tomp- son SH, Toro-Serey C, Torre Tresols JJ, Tozzi L, Truong V, Turella L, van ’t Veer AE, Verguts T, Vettel JM, Vijayarajah S, Vo K, Wall MB, Weeda WD, Weis S, White DJ, Wisniewski D, Xifra-Porxas A, Yearling EA, Yoon S, Yuan R, Yuen KSL, Zhang L, Zhang X, Zosky JE, Nichols TE, Poldrack RA, Schonberg T. Variability in the analy- sis of a single neuroimaging dataset by many teams. Nature 2020;582:84–8.

Gal Y, Ghahramani Z. Dropout as a bayesian approximation: representing model uncertainty in deep learning. In: Balcan MF, Weinberger KQ, editors. Proceedings of The 33rd International Conference on Machine Learning. Proceedings of Machine Learning Research, 48. New York, New York, USA: PMLR; 2016. p. 1050–9.

Kristiadi A, Hein M, Hennig P. Being Bayesian, even just a bit, fixes overconfidence in ReLU networks. In: I HDII, Singh A, editors. Proceedings of the 37th international conference on machine learning. Proceedings of Machine Learning Research, 119. PMLR; 2020. p. 5436–46.

Pearce T, Leibfried F, Brintrup A. Uncertainty in neural networks: approximately Bayesian ensembling. In: Chiappa S, Calandra R, editors. Proceedings of the twenty third international conference on artificial intelligence and statistics. Proceedings of Machine Learning Research, 108. PMLR; 2020. p. 234–44.

