The advancement of computer technology has led to an overwhelming amount of textual information, hindering the efficiency of knowledge intake. To address this issue, various text summarization techniques have been developed, including statistics, graph sorting, machine learning, and deep learning. However, the rich semantic features of text often interfere with the abstract effects and lack effective processing of redundant information. In this paper, we propose the Multi-Features Maximal Marginal Relevance BERT (MFMMR-BertSum) model for Extractive Summarization, which utilizes the pre-trained model BERT to tackle the text summarization task. The model incorporates a classification layer for extractive summarization. Additionally, the Maximal Marginal Relevance (MMR) component is utilized to remove information redundancy and optimize the summary results. The proposed method outperforms other sentence-level extractive summarization baseline methods on the CNN/DailyMail dataset, thus verifying its effectiveness.

In the era of big data, the rapid development of social media constantly supplies a bulk of information. Text content, as a dominant medium in social media, is an efficient approach to conveying real- time news and opinions. However, the abundance of descriptions and interpretations often obscures the concrete opinion in a content, hin- dering the timely acquisition of vital information. Text summarization is a technique used to condense long text into a shorter abstract while retaining its original meaning [1]. Automatically generating key information from massive text can significantly improve efficiency compared to traditional manual summarization [2]. Automatic text summarization methods can be divided into two categories based on the relationship between the abstract and the original text: extractive summarization and abstractive summarization [3]. Extractive sum- marization extracts keywords from the source document to form a summary. However, this approach may result in a final summary that lacks coherence between sentences and may contain redundant information. While abstractive summarization generates new words to form a summary based on the content of the source document.

do not take contextual information into account. The development of deep learning techniques has facilitated breakthroughs in natu- ral language processing. The BERT model [7] is a pre-trained model that has been trained on large-scale datasets, demonstrating powerful generalization capabilities. BERT uses word-level inputs while extrac- tive summarization is a sentence-level task. Therefore it is impossible to fine-tune the BERT pre-trained model directly for automatic text summarization tasks.

To address the issue, we propose the MFMMR-BertSum model. The primary concept of this method is to incorporate the pre-trained BERT model into the social media text summarization task, modifying its in- put representation to capture sentence features and differentiate them. Subsequently, a classification layer is constructed after the model‚Äôs output to extract the summarized sentences, enabling its application to the text summarization task. Additionally, a de-redundancy component is added to further optimize the summarization results based on the principles of MMR [8]. The key innovations of this model are as follows:

Extractive summarization was the dominant approach to automatic text summarization before the advent of deep learning technology [9]. The basic principle behind statistical methods for extractive summariza- tion involves analyzing word frequency, sentence position, and their weighted combinations to determine sentence importance. According to Luhn, certain words may have greater significance if they appear more frequently in the text. Baxendale found in his research on the summary of sentence position features that it has a strong correlation with the text topic. Edmundson pointed out that some specific words are related to the importance of the sentence. Optimization-based

methods [10] usually formalize text summarization as a mathematical problem with constraints. MMR [8] algorithm is one of the classical methods. It takes the linear combination of the similarity of documents with respect to the query and the similarity with documents that have been previously selected for summarization as the ‚Äò‚Äòedge correlation‚Äô‚Äô, and maximizes this edge correlation value during the retrieval and sum- marization process to gradually obtain the final summary. Cheng [11] ranked each sentence based on the probability that it would become a summary using SVM. The final summary set is obtained by using the improved MMR algorithm to select the sentences at the edge of the summary ratio.

The emergence of deep learning technology has revolutionized the field of extractive text summarization, bringing about significant progress and advancements. Liu [12] applied deep learning to the field of text summarization for the first time and proposed a text summa- rization method based on RBM. The emergence of pre-trained models like BERT [7] has brought natural language processing into a new era. BERT uses the encoder part of the Transformer as the main framework of the model. Through the joint adjustment of the context of each layer to predict the deep bidirectional representation, capturing the bidirectional context relationship in the statement. The BertSum model proposed by Liu [13] is the first BERT-based text summarization model. Some modifications have been made to the embedding of the BERT

model for the purpose of extractive summarization. Yuan [14] added the hierarchical graph mask to BERT to make full use of the structural information between different semantic levels and extract the semantic units at the fact level to obtain a better summary. Srikanth [15] used the K-means algorithm to cluster the sentence representations output by the BERT model and introduced a dynamic method to determine the appropriate number of sentences from the cluster. Ma [16] propose a topic-aware extractive and abstractive summarization model based on

The pre-trained model BERT boasts semantically-rich representation capabilities that effectively address challenges such as polysemy and long-distance dependencies in natural language processing. The input representation of the BERT model consists of token embeddings, seg- ment embeddings, and position embeddings. The body of the model is a multi-layer bidirectional transformer structure and the output can be trained on downstream tasks through fine-tuning by connecting it to neural networks.

Among them, ‚Ñé0 = ùëÉ ùëúùë†ùê∏ùëöùëè(ùëá ), ùëá is the sentence vector output by the BertSum model, and ùëÉ ùëúùë†ùê∏ùëöùëè(ùëá ) represents the position embeddings for vector ùëá . The superscript ùëô represents the depth of the stacked layer. The layer normalization procedure (ùêøùëÅ ) is used to normalize all neurons in a sample‚Äôs same layer. ùëÄùêªùê¥ùë°ùë° is the multi-head attention operation. ùêπ ùêπ ùëÅ is the feed forward network of Transformer.

requirements of extractive text summarization tasks to construct and propose the MFMMR-BertSum model. As shown in Fig. 1. To create our extractive summarization model, we begin by feeding the text content into a modified pre-trained BERT model, which captures sen- tence features through tag-based differentiation. This step yields the sentence vector code. The output is subjected to a combination of linear classification and Transformer classification at the classification layer. This process produces a prediction score, which is then sorted in descending order to generate a preliminary summary. To refine the summary further, we perform a second screening step designed to eliminate redundant components. The resulting summary is our final output.

To reduce redundancy in the abstract, we have incorporated an MMR component into the prediction phase of our model. Traditional MMR only considers word-level features and disregards other aspects, hindering the quality of the final summary. Moreover, the time com- plexity of MMR based on greedy selection depends on the number of summary sentences. Directly applying the original MMR could result in a large increase in the model‚Äôs processing time. Taking on these difficulties, we propose an MFMMR algorithm that utilizes a weighted combination of multiple features as sentence features during the fea- ture extraction process. Additionally, we improve the summary model extraction process by implementing a temporary summary set to reduce the time complexity of the method. By using these techniques, we can generate a more concise and accurate summary while streamlining the computation of the model.

candidate sentences and the temporary summary set is calculated. If the maximum edge correlation falls below a certain threshold, the can- didate sentence is deemed less relevant or redundant and subsequently discarded. On the other hand, if the maximum edge correlation sur- passes the threshold, the candidate sentence is added to the temporary summary set. Once the number of sentences in the temporary summary set reaches a specific level, those sentences are pushed to the final summary set. The time complexity depends on the size of the temporary summary set, allowing for manageable time consumption, because of which it is suited for deep learning-based methods.

For the weighted coefficients ùõº, ùõΩ, ùõæ and ùõø used in this paper, ùõº, ùõΩ, and ùõæ are set to take 0.15 to 0.35 in 0.05 increment, ùõø=1-ùõº ‚àí ùõΩ ‚àí ùõæ and ùõø> 0. Where ùõº=0.25, ùõΩ=0.2, ùõæ=0.2, and ùõø=0.35, ROUGE-1 and ROUGE-L achieve the maximum value, ROUGE-2 is nearly equal to the

to be 100 to 300 in 50 increments. ROUGE-1, ROUGE-2 and ROUGE-L to 0.9 in 0.1 increment, and the word vector dimensions are taken are used as evaluation metrics. The result indicated when ùúÜ takes the value of 0.8, ROUGE-1 and ROUGE-L take the maximum value, and ROUGE-2 is also basically close to the maximum value. At this time,

the sentence score term and redundancy term weights are best assigned. Furthermore, the ROUGE score changes very little under different word vector dimensions. Considering that a word vector with too large a dimension increases the complexity of the model and thus the running time, it is more appropriate when the word vector dimension is taken as 100. Through this experimentation, we determined that the MFMMR algorithm yielded the best summary performance with a hyperparame-

As shown in Figs. 2,3, whether it is a single-sentence or a multi- sentence summary, the MFMMR algorithm considers multiple features as the calculation method of sentence score. On the basis of MMR, it greatly eliminates the negative impact of single feature, and ob- tains the highest score in baseline methods, verifying the algorithm‚Äôs effectiveness in extractive social media text summarization.

ment in the alignment with reference summaries across different sliding window scales, word order, and sentence structure, thus validating the effectiveness of the proposed MFMMR-BertSum model. Meanwhile, the hybrid model SummaRuNNer-PGN achieves better results than the separate one. Accordingly, we speculate that hybrid models combining the advantages of extractive and abstractive summarization have plenty of potential for growth.

In this research paper, we propose the MFMMR algorithm, con- sidering multiple features in the conventional MMR sentence scoring process. Significantly mitigates the adverse effects of relying on single feature for calculating sentence scores. The MFMMR-BertSum model is proposed by modifying the input representation of Bert and adding a classification layer with MMR components to reduce the redun- dancy problem in extractive summarization. Tests were conducted on the CNN/DailyMail dataset, and the results indicate MMR-BertSum has significant improvements compared to the baseline approach on

Junqing /an received his Ph.D. degree from China Univer- sity of Geosciences. He is currently an Associate Professor at the School of Computer Science, China University of Geosciences. He is a CCF member. His research interests include Data Mining, Natural Language Processing, High Performance Computing, and Optimization.

Xiaorong Tian received her B.S. degree in 2022 from China University of Geosciences, Wuhan, China, where she is currently working toward a master‚Äôs degree in Electronic and Information Engineering. Her research interests include Deep Learning, Natural Language Processing, and Big data Technology.

Yuewei Wang received an M.S. degree in Computer Sci- ence and Technology in 2018 from China University of Geosciences, Wuhan, China, where he is currently working toward a doctoral degree in Geographic Information System. His research interests include Machine Learning, Big Data Analyze, Digital Earth, and High Performance Computing.

