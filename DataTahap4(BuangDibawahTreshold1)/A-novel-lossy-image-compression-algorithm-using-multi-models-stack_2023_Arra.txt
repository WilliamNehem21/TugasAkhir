and compare them to known objects. DL can perform all ML tasks from classification to clustering. Also, ML can only perform a few of the DL tasks. This is due to the needless feature extraction in the DL, while the ML needs to perform feature extraction on the data at the preprocessing step before feeding the model. Feature extraction is a step that a domain expert usually does to select or derive an informative and non-redundant set of values from being used for the model. DL is used for image compression using different algorithms. The most widely used algorithm is the convolutional neural network (CNN) which represents the features of the image at different scales and by using pooling layers, which summarize the features of each image region. However, the image compressed and decompressed using a CNN model is found to be blurry due to loss of information during the compression (encoding) process.

This research paper uses a Stacked AutoEncoders (SAE) deep learn- ing model and a content-based image filter to compress the image. SAE is a feed-forward-based type of NN, where the output is the same as the input. It consists of an encoder, a decoder, and a loss function. In the training phase, SAE transforms the bits into an encoded (compressed) format with fewer bits called latent-space representation (code) using the encoder and then reconstructs it into a lossy image using the decoder with the same shape. In the testing phase, the code alone is used in the decoder model to be reconstructed, as the weights and underlying functions (activation functions) are kept in the trained model. The model is trained using Back Propagation (BP) algorithm. BP is a supervised algorithm that tunes and enhances the output by adjusting the hyperparameters. On the another hand, a content-based image filter is used to enhance the quality of the reconstructed image generated using the SAE decoder.

Furthermore, this research adds an image classifier to the proposed compression technique to choose the AE that serves that particular image class. The proposed image compression algorithm reduces data size by removing redundant and excessive information. This process reduces the cost of storing and transmitting images, especially over low-bandwidth networks. The main objective of the proposed approach is to considerably increase the compression ratio beyond the exist- ing compression algorithms, such as Linear Lossy Data Compression (e.g., JPEG), along with an image classifier to identify the class of new images that need compression.

The rest of the paper is structured as follows: Section 2 presents the literature review of image compression. DL algorithms are analyzed, showing the early techniques of image compression till the recent techniques that employed DL. The proposed methodology, including background on the base method used and the complete architecture of the image classifier, compression, and decompression algorithms, are given in Section 3. The experimental results and discussion of the results obtained are provided in Section 4. Finally, Section 5 concludes the research findings and offers possible future directions.

The variational autoencoder VAE is another type of AE used for image compression. VAE uses the mean and variance as input of latent space distributions and has been shown to perform better than simple Autoencoders. Many researchers employed VAE for image compres- sion using nonlinear transforms and uniform quantization techniques. For example, Zhou et al. used VAE with the Challenge on Learned Image Compression (CLIC) dataset for training the model [21]. The model was enormous and complex due to multiple training parameters. Similarly, Chen et al. proposed a VAE-based architecture for high- resolution image compression, which utilized a non-local attention module to improve the training process, but at the cost of increased model complexity [22]. In [23], the authors proposed a VAE for im- age compression, and in [24], VAE was used to compress images and achieve a 4.10 bits per pixel rate, but with a complex model architecture.

To enhance the quality of the reconstructed image, an additional step called the Residual Enhancement Vector (REV) is introduced. By adding the REV, the decompression phase aims to achieve a recon- structed image that is as close as possible to the original image in terms of visual quality and details. This approach helps to enhance the overall performance of the compression/decompression algorithm.

The resulting REV from the compression process is the same size as the original image. In order to reduce the REV size in memory, it was transformed and binarized to have a smaller range of numbers by elimi- nating values with the least density. The following steps are to binarize REV to have a minimum size and then enhance the compression rate.

proposed model has a 23.62% better SSIM score than the JPEG-encoded images and a 3.38 dB better PSNR score. Experiment 4 showed that the proposed model has a 25.18% better SSIM score than the JPEG-encoded images and a 4.48 dB better PSNR score which is the best result.

iments since it is the closest image size to the compressed size of the proposed model. Images decoded using the proposed model are visually better than the JPEG encoded in all four experiments. No enhancement was needed since it was already good enough with high SSIM and PSNR scores.

1.05 dB better PSNR score than the JPEG-encoded images. Experiment 7 showed that the enhanced image has almost the same SSIM score and almost the same PSNR score as the JPEG-encoded images. Experiment 8 showed that the JPEG encoded image is only 0.74% better SSIM score than the enhanced image with less than 0.8% of CR, which is the best CR score for grayscale images. The enhanced image has a 0.16 dB better PSNR score than the JPEG-encoded images.

same CR and a 0.21 dB less PSNR score than the JPEG-encoded image. Experiment 11 showed that the enhanced image has only a 9.95% less SSIM score and a 2.67 dB less PSNR score than the JPEG-encoded images. Experiment 12 showed that enhanced images have a 10.02%

In the experiment, we compared the encoding and decoding times for a 5-layer autoencoder and JPEG compression on 10,000 images from the MNIST dataset. The results showed that the encoding time us- ing the autoencoder was 2.6049 s, and the decoding time was 2.8013 s. On the other hand, the encoding time using JPEG was 1.7122 s, and the decoding time was 1.0762 s. As observed, the autoencoder had slightly

