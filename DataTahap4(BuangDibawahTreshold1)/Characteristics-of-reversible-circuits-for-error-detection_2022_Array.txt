To summarize, reversible circuits bear strong similarities with clas- sical (irreversible) circuits, but there are some notable additional char- acteristics. Chief among them is reversibility itself which implies that information cannot easily escape. Here, we show that this has profound implications for error detection with random inputs. More precisely,

/ig. 3. Typical accumulation effects for multiple errors (log–log plot): number 𝑙 of randomly injected reversible errors (𝑥-axis) vs. average number of random inputs required to detect erroneous behavior (𝑦-axis) in a generic 𝑛 = 20-bit reversible circuit with 4000 gates. Different colors denote worst-case errors of increasing size 𝑘. Solid lines track the theoretical best-case behavior (independent errors, see Eq. (5) below). For small 𝑙, the plot highlights an excellent agreement between typical (diamonds) and best-case (solid lines) behavior.

position’’) and, arguably, more interesting. Circuit diagrams provide a well-established tool that does precisely that. They decompose a possi- bly complicated circuit into a structured sequence of simpler building blocks. We use circuit decomposition on a rather high level to reason

to (non-)equal bit strings. In contrast, the first portion of the circuit 𝑅1 has occurred). Reversible circuits always map (non-)equal bit strings (before the error has occurred) can affect concrete inputs 𝑥⃗ ∈ {0, 1}𝑛. But if 𝑥⃗ is sampled randomly, then 𝑅1(𝑥⃗) will be a different, but still

exposes multiple errors only partially. Everything before the first error (𝑅1) and after /ig. 5. Partial simplification for multiple errors: Simulation with uniformly random inputs the last error (𝑅3) can be safely ignored, but the part in between (𝑅2) does matter.

rors of size 𝑘 permute exactly 2 out of the 2𝑘 possible 𝑘-bit inputs on This probability bound is actually sharp. Worst-case reversible er- which they act. Concrete examples of such a behavior are NOT (𝑘 = 1), CNOT (𝑘 = 2), CCNOT (𝑘 = 3) and, more generally, a (𝑘 − 1)-fold controlled NOT gate on 𝑘 bits (general 𝑘). The numerical simulations

/ig. 6. Best-case scenario for two errors: One of the errors, say 𝐸2, commutes with the relevant circuit part 𝑅2. Reordering allows us to treat the two errors as a single effective error 𝐸̃ = 𝐸1 ◦𝐸2 . In addition, 𝐸1 and 𝐸2 affect disjoint bit collections (independence) and 𝐸̃ factorizes nicely into two disjoint components: Pr[𝑅̃(𝑥⃗) ≠ 𝑅(𝑥⃗)] = Pr[𝐸̃(𝑥⃗) ≠ 𝑥⃗] ≥ 1 − (1 − 2−(𝑘−1))2 (quadratic improvement).

Section 4.1) and maximal masking (worst case, see Section 4.2) turn out to behave in a radically different fashion. Subsequent numerical studies demonstrate that typical error accumulation effects closely follow the best-case trajectory: Multiple errors are typically much easier to detect than a single error.

/ig. 7. Worst-case scenario for two errors: Two bit-flip errors (𝑘 = 1) affect one control line of a (𝑛−1)-fold controlled NOT-gate. These errors do not commute with the relevant circuit part 𝑅2. Quite the opposite: two errors with size 𝑘 = 1 produce an effective error

accumulation effects can occur for more errors (𝑙 ≥ 3) and/or larger random inputs in order to detect the discrepancy. Even worse error error sizes (𝑘 ≥ 2). But already Rel. (6) is almost as bad as it can be. It is only a factor of two away from 2𝑛−1—the absolute worst case for

The multiple-error case is intricate by comparison, because the interplay between error (locations) and underlying circuit geometry starts to matter. We have seen that this leads to strikingly different best- (commuting errors, Sub. 4.1) and worst-case (anticommuting errors, Sub. 4.2) behavior. Concrete problem instances fall into the wide range between these extreme cases. In this section, we employ numerics to

random inputs to detect 𝑙 commuting and independent errors of size 𝑘 each. This bound is sharp. It holds with equality if each of the 𝑙 errors is a worst-case error of size 𝑘, e.g. a (𝑘 − 1)-fold controlled NOT gate.

The next set of numerical experiments pilots us in more interesting territory. Namely, the multiple-error case. We have already teased the results in the introduction and summarized them in Fig. 3. The aver- aged number of inputs highlights an excellent agreement between the observed behavior and the best-case scenario discussed in Section 4.1.

In this work, we have shown the impact of the reversible circuit paradigm on the probability of detecting errors in circuits. Our rigor- ous analysis shows, that, as opposed to classical/irreversible circuits, reversible circuits can never mask single errors and, that the probability of detecting a single reversible error only depends on the error’s size and not at all on the surrounding circuit. Empirical evaluations have shown that, in case of multiple errors, the detection probability is very close to the theoretical best case. Finally, we have observed that, in case the assumption of worst-case errors is dropped, the probability of detecting these errors is increased even more.

