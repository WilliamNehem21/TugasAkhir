PCA is a useful statistical technique that has found application in fields such as face recognition, image compression, dimensionality reduction, Computer System performance analysis etc. It is a common technique for finding patterns in data of high dimension. In this paper, we present the basic idea of principal component analysis as a general approach that extends to various popular data analysis techniques. We state the mathematical theory behind PCA and focus on monitoring system performance using the PCA algorithm. Next, an Eigen value-Eigenvector dynamics is elaborated which aims to reduce the computational cost of the experiment. The Mathematical theory is explored and validated. For the purpose of illustration we present the algorithmic implementation details and numerical examples over real time and synthetic datasets.

Performance evaluation helps us to give an idea how well a system is performing as compared to other systems. Workload is the most crucial part of any performance evaluation process. The entire process can end up in a wrong conclusion if workload is not chosen in appropriate ways. Therefore, workload selection is an integral part of performance evaluation project. Computer architectures are evaluated by running a workload on the computer and measuring the execution time. New computers are designed the same way. As the newly designed computer does not exist, it is not possible to run any workload. This is where workload characterization comes into the fore. The goal of workload characterization is to describe the properties of a workload in terms of abstract performance metrics, called workload characteristics, which predict the final performance [1].

We can find the application of Principal Component Analysis in many fields including data compression, image processing, visualization, pattern recognition and time series prediction [2].Sirvich and Kirby had efficiently used PCA in human faces representation [3-4].This approach leads to decomposition of any images into Eigen pictures so that the image can be reconstructed using a portion of the Eigen pictures and the corresponding projection onto the Eigen picture subspace [5]. The PCA method has also been used in handprint recognition, human made object recognition, industrial robotics and mobile robotics etc [6]. In a workload composition the choice of benchmark is very important. The selection of benchmark for inclusion in

benchmark suit is called workload composition. Smith [7] used a metric, which is based on dynamic program characteristics for the Fortran language..They used squared Euclidean distance to measure the difference between benchmarks. The shortcoming of this procedure is the use of Euclidean distance for measuring the difference. To overcome this Eeckhout et al. [8] proposed Principal Component Analysis (PCA) to get rid of the correlation and dependence between variables. A number of program characteristics are measured for a number of benchmarks on which PCA was applied.

The most computationally expensive part of PCA is the calculation of Eigen values and Eigen vectors of the dataset. In this paper our main objective is to save the computational time of Principal Component Analysis (PCA) by skipping the Eigen vector calculation. Here we propose to bypass Eigen vector calculation, by rather inspecting the Eigen vectors. The understanding of the dynamics of Eigen values and Eigen vectors in the context of Linear transformations and vector spaces plays a crucial role in improving the efficiency of PCA in the workload characterization problem. This will require us to prove/cite important theorems in Linear Algebra.

always the case will be) and is real, symmetric. Therefore, the corresponding eigenvectors will be linearly independent & orthogonal to each other. This enables us to find the eigenvectors by inspection rather than computing step by step via set of simultaneous equations. This saves O (n) computations, crucial computation cost!

The paper aims to improve time complexity of PCA algorithm. The following example will illustrate the principle behind PCA from initial parameters.We have collected synthetic data of the number of packets lost on two different network links. xa is the number of packets lost on link A and xb is the number of packets lost on link B

Now, the correlation matrix in (5) is both real and symmetric and is a candidate for the theorems to be applied. Figure1 below shows the average execution time of PCA algorithm over a sample of 5 different datasets. The execution time has been recorded in milliseconds.

PCA is the simplest of the true Eigenvector-based multivariate analyses. It can be used to reveal internal structure of data in a way that best explains the variance in data. PCA is sensitive to outliers in the data that produce large number of errors. So, before applying PCA it is expected to remove outliers. As a limitation the result of PCA depend on the scaling of variables. The applicability of PCA constrained by certain assumption made in derivation. Our work explores the underlying principles of PCA and exploits the inherent mathematical theory for efficient computation. The figure below conclusively shows that computation time has been reduced to achieve the same results.

