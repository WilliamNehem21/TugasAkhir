Abstract Presented in this paper is a novel system for face recognition that works well in the wild and that is based on ensembles of descriptors that utilize different preprocessing techniques. The power of our proposed approach is demonstrated on two datasets: the FERET dataset and the Labeled Faces in the Wild (LFW) dataset. In the FERET datasets, where the aim is identification, we use the angle distance. In the LFW dataset, where the aim is to verify a given match, we use the Support Vector Machine and Similarity Metric Learning. Our proposed system performs well on both datasets, obtaining, to the best of our knowledge, one of the highest performance rates pub- lished in the literature on the FERET datasets. Particularly noteworthy is the fact that these good results on both datasets are obtained without using additional training patterns. The MATLAB source of our best ensemble approach will be freely available at https://www.dei.unipd.it/node/ 2357.

the same person can vary considerably in time, pose, facial expression, illumination conditions, occlusions, and image quality. Most state-of-the-art face recognition techniques per- form well when facial images are captured in optimal (labo- ratory) conditions where lighting is controlled and samples provide full frontal views, but when facial images are cap- tured in the wild – where pose, age, and facial expressions change and where environmental conditions such as lighting are less than ideal – performance deteriorates. The difficulty lies in teasing out the specific features indicative of identity from the mass of features expressing other conditions. Even the best classifier will fail if an insufficient number of features indicative of identity are isolated. One way to tackle this problem is to use multibiometrics, which recognizes individu- als via biometric fusion [1], whether multimodal, multi- instance, multisensorial [2], or multialgorithmic. Of particular importance to both single trait biometrics and multibiomet- rics is the identification of face descriptors that are discrimi- native yet insensitive to information having nothing to do with identity, such as pose variations, changes in facial expression, and lighting conditions.

Some of the most notable face recognition techniques devel- oped the last five decades [3] include Principal Component Analysis, Elastic Template Matching, Discriminant Analysis, Local Binary Patterns (LBPs), Algebraic moments, Gabor Fil- tering [4], and Neural Networks [5]. One way to categorize face recognition techniques is to look at how a face is represented [3]. Appearance based approaches utilize global texture features such as Eigenfaces [6] or some other linear transformation. In addition to the information found in the texture of a face image, Model based approaches take into account the shape of the face, whether 2D [7] or 3D. Geometry or template based approaches compare an input image with a set of templates constructed using either statistical tools or by analyzing local facial features and their geometric relationships [8]. Neural Networks include approaches based on ‘‘deep learning” where the representation of faces is learned during the training pro- cess [5]. This last class includes approaches that are often referred to as ‘‘deep methods” in opposition to ‘‘shallow meth- ods,” and differs from a second class of approaches where the representation of the face image is derived from ‘‘handcrafted” image descriptors.

Recent developments in the first class of shallow methods include the work of Pinto et al. [9] who describe a set of V1- like features that are composed of a population of Gabor fil- ters. V1-like features are insensitive to view, lighting, and many other image variations. The feature sets proposed by Cao et al.

[10] that encode the local micro-structures of a face into a set of more uniformly distributed discrete codes are excellent examples of a good tradeoff between discriminative power and invariance, as are Patterns of Oriented Edge Magnitudes (POEM), a feature set proposed in [11,12]. POEM is an ori- ented spatial multiresolution descriptor that captures informa- tion about the self-similarity structure of an image. Some feature sets that work well in the wild include those described in [13] and more recently in [14], where monogenic binary cod- ing (MBC) is presented. MBC decomposes an original signal into three components (amplitude, orientation, and phase) that encode local variation. A histogram is then extracted from the local features. This efficient descriptor significantly lowers the time and space complexity compared with other Gabor- transformation-based local feature methods.

Another approach for overcoming variations in pose and illumination is to combine texture-based descriptors with other techniques. For example, in [15] an accurate 3D shape model works by mapping images that vary in pose to a full frontal view. Discriminative models capable of handling aging, facial expressions, low light, and over-exposure are then obtained by comparing billions of faces. One approach described in

[16] trains binary classifiers on sixty-five describable visual traits that were manually labeled on the training set. Another approach based on ‘‘simile classifiers” removes the need for a manually labeled training set by training the binary classifiers to recognize the similarity of faces (using the whole image and patches) to specific reference people. Both approaches exploit the power of simple low-level features (such as image intensi- ties in RGB and HSV color spaces, edge magnitudes, and gra- dient directions). A drawback of these approaches, however, is that they required using affine warping to obtain pose invari- ance. In [17] an identity-preserving alignment is proposed. In this approach, face warping reduces differences in poses and expressions while preserving differences indicative of identity. Binary classifiers are trained both to perform an ‘‘identity-pre serving” alignment and to recognize people.

The Multi-scale Local Phase Quantization (MLPQ) method proposed in [18] is a blur-robust image descriptor. MLPQ is computed regionally and adopts a component-based framework to maximize the insensitivity to misalignment, a phenomenon frequently encountered in blurring. Regional features are com- bined using kernel fusion. The MLPQ representation is com- bined with the Multiscale Local Binary Pattern (MLBP) descriptor according to a supervised fusion that is based on Ker- nel Discriminant Analysis (KDA). This step is necessary to increase insensitivity to illumination. It should be pointed out here, however, that the MLPQ representation in [18] was obtained using a supervised transform and a different testing protocol. Thus, the results reported in [18] on the LFW dataset are not comparable with the approach proposed in this paper.

A real breakthrough in the field of face recognition was the introduction of ‘‘deep methods,” which are based on the appli- cation of deep learning to this pattern recognition problem. The first interesting paper in this area was [5], where a convo- lutional neural network (CNN) was employed to learn a metric between face images. This was a precursor to the recent highly successful application of CNNs to face verification. So power- ful is the deep learning approach that after a decade of study researchers [19] have recently announced that we are now able to close the ‘‘gap to human-level performance in face verifica- tion.” With an approach based on a 3D model for face align- ment and an ensemble of CNNs to find a numerical description of the forward-looking face, DeepFace has achieved 97.25% accuracy on the LFW dataset, which is very close to the human level accuracy of 97.53% in face verification. Another work [20] that is based on Gaussian Processes and multi- source training sets has achieved 98.52% accuracy on the LFW dataset, which is better than human performance.

Many deep learning approaches [21–23] have also signifi- cantly outperformed previous systems based on low level features in face recognition. There are two innovations of note in these deep learning approaches based on low-level features. The first is in face identification, thanks to the last hidden layer, which contains features highly discriminative in performing large-scale face identification. The second is in both face identification and verification, thanks to supervising

The approach presented in this paper can be referred to as shallow, since unlike deep methods, our proposed approach is based on a representation of the face image using handcrafted local image descriptors. The system presented here is based on preliminary results reported in [26] that demonstrate how the performance of the POEM descriptor [12] (one of the most effi- cient and one of the highest performing descriptors recently proposed in the literature) can be enhanced with an ensemble of classifiers that combine different preprocessing techniques that vary a set of feature extraction parameters. However, here we also test our proposed system using a set of ‘‘learned” fea- tures, which have been obtained from the internal representa- tion of a deep method, especifically a Convolutional Neural Network (CNN) trained for the face recognition problem. We want to underscore that the use of ‘‘learned features” does not put the proposed approach in the category of a ‘‘deep method” approach since the training of the classifier is per- formed in a traditional, shallow way.

tures obtains, to the best of our knowledge, one of the highest mean accuracy ratings on the FERET datasets published in the literature. Moreover, the fusion produces very good results on the LFW dataset. The ensemble based on the fusion of learned and handcrafted features improves performance on both the FERET and LFW datasets even further.

The main idea of the proposed approach is to design an ensem- ble of classifiers trained on different descriptors extracted from the face image. Moreover, in order to perturb the information given to the base classifiers and to make the ensemble stronger, we designed several perturbations at different steps in the clas- sification process: in the image preprocessing, feature transfor- mation, and matching steps. The general schema of the complete approach is illustrated in Fig. 1. Detailed descrip- tions of the methods used in each step are provided below in this section.

mined according to the sum rule by summing up the scores/ similarity values (SIMi) obtained from each classifier. In this work, the simple angle distance is used in the FERET datasets, where the aim is identification. Linear SVMs [42] and SML [25] are used on the LFW dataset, where the aim is to verify a given match.

The LBP operator represents the difference between a pixel x and its symmetric neighbor set of P pixels placed on a circle radius of R (when a neighbor does not coincide with a pixel, its value is obtained by interpolation). In this work we use P =8 and R = 1. We also use LBP with uniform bins. The LBP descriptor is extracted from a set of subregions that are

Step 2. Calculate the magnitude accumulation. A local his- togram of orientations is calculated considering all pixels within a local image patch (cell). As a result, each pixel car- ries information about the distribution of the edge direction of a local cell.

Step 3. Calculate self-similarity. In this step the accumulated magnitudes are encoded across different directions using the LBP-based operator within a larger patch (block). Based on previous experimental results [26], the Dense LBP (DLBP) is used in our experiments instead of standard LBP.

The POEM descriptor depends on a large number of parameters that need to be tuned specifically for each applica- tion. In our experiments the number of orientations dis- cretized, and the size of the cell, the size of the block, and the number of neighbors considered in LBP have been set according to [43] (i.e. to 3, 7, 5, and 8, respectively).

Each step in MBC (the multiscale log-Gabor filtering, sub- region histogram computing, and feature combination by LDA) involves several parameters. In our experiments, all parameters have been set according to those in the original paper. However, in this work an unsupervised feature trans- form (PCA), as described below, is used instead of LDA. The final descriptor is composed of three feature vectors, one for each component (amplitude, orientation, and phase) of the original signal, labeled in the experimental section as MBCa, MBCo, MBCp, respectively. The three descriptors are not fused at the feature level but rather at the score level according  to  the  weighed  sum  rule:  MBC = (MBCa + MBCo + MBCp)/3.

HASC [39] is applied to heterogeneous dense feature maps and simultaneously encodes linear relations by covariances (COV) and nonlinear associations through information-theoretic measures, specifically entropy combined with mutual informa- tion (EMI). The basic supposition behind HASC is that linear relations alone are unable to capture the structural complexity of many objects. Using covariance matrices as region descrip- tors is advantageous because it is low-dimensional and robust to noise and pose changes; however, a single pixel outlier can dramatically alter results, making the descriptor highly sensi- tive to impulsive noise. Moreover, the covariance among two features is optimally able to encapsulate the features of the joint PDF only if they are linked by a linear relation. EMI overcomes these limitations. The entropy (E) of a random vari- able measures the uncertainty associated with the value of the variable, and the mutual information (MI) of two random variables captures the generic dependencies (both linear and

nonlinear). HASC takes advantage of these two properties by dividing an image into patches and creating an EMI matrix. Each diagonal entry of the EMI matrix captures the amount of uncertainty or unpredictability related to a given feature whereas off-diagonal entries capture the mutual dependency between two different features.

HASC boosts discriminative performance because the com- bination of COV with EMI captures different features of the joint underlying PDFs. Multiple experiments in [39] demon- strate that HASC is superior in performance to its individual components COV and EMI. This makes HASC a versatile descriptor for a large range of applications. HASC is extracted separately from subregions of the whole image. The subregions

GOLD [40] is a recent improvement of the well-known Bag of Word (BoW) approach [45] for extracting features from an image. The canonical BoW descriptor generates a codebook (via clustering methods on the training set) from a set of extracted local features that are then encoded into codes to form a global image representation. Instead of using a cluster- ing method, GOLD substitutes a flexible local feature repre- sentation obtained by parametric probability density estimation that does not require quantization. Quantization has the drawback of tightly tying dataset characteristics to the feature representation since quantization is learned from the training set, and the cluster centers reflect the training data distribution.

The original LBP does not preserve structural information among binary patterns; therefore, a set of co-occurrences among adjacent LBPs (i.e. a co-occurrence matrix among LBP pairs, or CoALBP) is extracted and converted to a CoALBP histogram feature. The rotation invariance of CoALBP is obtained by attaching a rotation invariant label to each LBP pair [46]. In this work the RICLBP descriptor has been tested using the following LBP parameters: (R = 1, P = 8), (R = 2, P = 8) and (R = 4, P = 8).

siders the intensity of the central pixel; therefore, the final code is obtained from the combination of three codes: CLBP_S, which considers the sign component of the difference (i.e. the standard LBP), CLBP_M, which considers the magnitude component of the difference, and CLBP_C, which considers the intensity of the central pixel. In this work the CLBP descriptor has been tested using the following two LBP config- urations: (1, 8) and (2, 16).

We tested several approaches for dimensionality reduction in our experiments to find the best way of reducing the dimen- sionality of each descriptor before the classification step. According to [48] nearly all spectral methods provide approx- imately the same accuracy when used with the same energy cut. In our experiments, however, the best performance was obtained using PCA [41], one of the most popular methods for unsupervised dimensionality reduction. PCA maps feature vectors into a smaller number of uncorrelated directions calcu- lated to preserve the global Euclidean structure, and it also extracts an orthogonal projection matrix so that the variance of the projected vectors is maximized.

In the classification step, each preprocessed image together with its extracted descriptor induces a different individual clas- sifier or distance measure. Therefore, for each descriptor we have a different score or similarity measure SIMi for the refer- ence image. The final decision of the ensemble is obtained by combining all the scores by sum rule. This is a straightforward method that was selected because the number of classifiers is quite high when including all the preprocessed images and all the descriptors and artificial poses under consideration in this work. Moreover, the simple sum rule does not require a deep analysis of the uncertainty space of ensemble classifiers, as was performed in [49–51].

In the FERET datasets, where the aim is identification, we use the angle distance as the similarity function to compare two faces. The angle distance between two vectors is the size of the angle between the two directions originating from the observer and pointing toward these two vectors. It can be calculated as the angle whose cosine is the ratio between the dot product of the two vectors and the product of their magnitudes. In the LFW dataset, where the aim is to verify a given match, a gen- eral purpose binary classifier can be used to distinguish between genuine and impostor matchings. In this work we test Linear SVM [40] and SML [25].

dimensional feature space. We used different kernels in our experiments, but the best results were obtained with a linear kernel. The SVM classifier is trained to distinguish between genuine and impostor matches. Therefore, a training pattern is the combination x of two descriptors xi and xj and a label

Our proposed system is evaluated on the FERET [53] and LFW [54] benchmark databases. The FERET database con- tains five datasets: Fa (1196 images), Fb (1195 images), Fc (194 images), Dup1 (722 images), and Dup2 (234 images). The gallery set is Fa, and the other datasets are used for test- ing. Fb contains pictures taken on the same day as the Fa images, using the same camera and under the same lighting conditions. Fc is a dataset of pictures taken on the same day as Fa but with different cameras and under different illumina- tion conditions. The Dup1 and Dup2 datasets contain pictures that were taken within the same year as Fa for Dup1 and later than one year for Dup2. The standard FERET evaluation pro- tocol involves comparing images in the testing sets to each image in the gallery set. In our experiments, all FERET gray

celebrities that were collected from the internet (Yahoo news). A total of 1680 faces appear in more than two images. LFW is commonly considered a very challenging dataset for face veri- fication since the faces were acquired in uncontrolled environ- ments. As a result, the images vary greatly in illumination, pose, and image quality, as well as in the age of the different celebrities. Two views are provided in the LFW database. View 1 contains a training set of 2200 face pairs and a testing set of 1000 face pairs and is used for model selection purposes only. View 2 contains 10 nonoverlapping sets of 600 matches and is for performance reporting. View 2 images can be used for 10- fold cross-validation algorithms and for testing the parameters developed on View 1. The classifiers are trained using only View 1. In this work we use preprocessed ‘‘prealigned” and ‘‘funneled” images using commercial face alignment software available on the LFW website.

The official testing protocols of both datasets are employed in the experiments reported in this section. For LFW, the Image-Restricted/No Outside Data Results is used. The perfor- mance indicator is the recognition rate in the FERET datasets and accuracy in the LFW dataset. Accuracy is the proportion of true classification results (both true positives and true neg- atives) in the population.

The first experiment was aimed at evaluating the different descriptors when combined with the preprocessing methods listed in Section 2. The experiments were carried out on the LFW dataset using the complete approach described in Fig. 1 (including the steps of frontalization, pose creation, and feature transformation). The classifier used in these experiments is SML. The results reported in Table 1 show

Taking into consideration the best approach produced in the second experiment (Table 2), the third experiment is aimed at tuning the dimension of the reduced space after the feature transform. The results of this experiment are reported in Table 3: it is clear that a very strong dimensionality reduction is required to maximize performance. This is most likely due to the curse of dimensionality.

In the fourth set of experiments reported in Table 4, we show the performance of some methods and fusions on both the LFW and FERET datasets. To avoid displaying a huge table, we report only the most interesting ensembles. The best tradeoff in performance on both the LFW and the FERET datasets is given by the fusion between the two descriptors POEM and MBC. Table 4 also reports the fusion of our best

approach here with the approach proposed in [25]3 (based on the SIFT and LBP descriptors coupled with SML classifier): the resulting performance of this ensemble is better than the single approaches (see Table 5 for each of these performances). According to Table 4, the fusion between (FM + [25]) and FV produces a performance improvement if the weighing fac- tor is accurately tuned. In Fig. 2 we show the performance of the ensemble based on SML (FM + [25]) and the best ensem- ble based on SVM (labeled FV) combined by weight sum rule as a function of the weighting factor a. Before combining the two methods, the scores are normalized to mean 0 and stan- dard deviation 1. As can be observed, the fusion results in a slight performance gain (the result is up to 92.1% for

Finally, in Table 5 a comparison with the state-of-art for both the FERET and LFW datasets is reported. Examining Table 5, it is clear that system performance has significantly increased the last few years. In the LFW database, we report only those methods that use no outside training data (as in our proposed approach and as is the case with approaches classified in the Introduction as ‘‘shallow”). Notice that our proposed method is the second best approach on the LFW dataset after [55] (whose results are not reproducible, since the method is not available).

As a final experiment, we test the proposed approach using a different set of features, the ‘‘learned” features that were con- trasted in the introduction with ‘‘handcrafted” features. Learned features are not defined in a straightforward manner to measure a specific property of the image (e.g. color and tex- ture) but are obtained from the internal representation of a CNN.

37th and 36th fully-connected layers are used for describing the images. These descriptors, whose dimensionality is 6718, are labeled LF in the following. CNN was trained once in [70], and the same parameters are used in both our experiments on the LFW and the FERET datasets. Please note that in the present work CNN is used only for representation purposes and is trained on tiny cropped faces so that the background is minimally involved in classification. The matching step is performed by training a general-purpose classifier, viz., those reported in Table 6. For classifier training, the same protocols used in the previous experiments and detailed in Section 3.1 are employed (with no external images). Despite our attempts to line up settings for fair comparisons, it can be questioned whether the results reported in Table 6 for the ‘‘learned fea- tures” (LF) can be fairly compared to the results we report above on the LFW database; this is because the LF descriptors have been obtained on a very large training set (even though external images were excluded from classifier training). Since the FERET protocol does not contain any limitations regard- ing the use of external images, the reported results are note- worthy: they are the highest published in the literature on the FERET datasets.

The results in Table 6 demonstrate that the learned features have good discriminant power for this problem and confirm the learning capabilities of CNN. The results for learned features were obtained by aligning the face images so that eyes are cen- tered and by performing a tiny crop of the face (for the hand- crafted features). Moreover, no preprocessing step was per- formed since CNN was used solely for the purpose of feature extraction and was trained using nonprocessed images.

(M) denote the computational costs for the preprocessing steps, the extraction of descriptors, and matching, respectively. T(D) includes the extraction of both descriptors (POEM and MBC) from all nine preprocessed images. The computation time of the feature transform, performed by PCA, is negligible.

In this work, we proposed an ensemble of approaches that obtain good results on the LFW dataset and that produce the best performance on the FERET datasets (see Table 6). Different preprocessing methods are coupled with two texture descriptors to improve performance. In the FERET datasets, where the aim is identification, we use the angle distance to match two faces. In the LFW dataset, where the aim is to ver- ify a given match, we use SVM and SML to match two faces. In the LFW dataset, the approach proposed here obtained 92.1% accuracy, which is the second best result reported in the literature without using outside training data. Unlike the sys- tem proposed in [55] (which is the first), the code of our full

In the future, we plan on testing new texture descriptors to enhance the performance of our approach. Future tests will also be performed using outside training data for comparisons of our approach with state-of-the-art deep learning methods trained with millions of examples. Preliminary results already reported in this work confirm that the ‘‘learned features” are a valid alternative to the ‘‘handcrafted features.” Moreover, since our last experiments were related to features learned for the face recognition task, in the future we are interested in evaluating the possibility of using features learned for differ- ent applications (i.e. object recognition, scene classification, etc.) in order to evaluate the degree of independence of such sets of features and their ability to work with different classifi- cation problems.

