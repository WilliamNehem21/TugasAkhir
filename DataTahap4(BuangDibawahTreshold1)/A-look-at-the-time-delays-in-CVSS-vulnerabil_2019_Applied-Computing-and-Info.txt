There is another viewpoint to the abstract CVE backlog. This viewpoint originates from the so-called switching costs, which are often high for information technology standards [37]. Such the- oretical costs cover also database maintenance: even small changes made to standards may imply a lot of evaluation work par- ticularly in case old information needs to be updated. This concern was raised also during the 2007 introduction of the second revision of the CVSS standard [36]. In other words, updates can be costly in terms of time and resources—given the nearly ninety thousand vul- nerabilities currently archived in NVD. Therefore, it is relevant to ask the following research question (RQ) about the time lags affect- ing CVSS scoring.

excluded. The same applies to CVEs without severity records. At the time of retrieving the NVD content [27], there were 2,218 vul- nerabilities that were published but still lacked CVSS records. Most of these cases relate either to new vulnerabilities that are still in the pipeline for severity assessments, or to already published CVEs

Another question relates to the content of the CVSS standard in terms of the vulnerabilities scored. Reflecting the disagreements among experts about the severity of some vulnerability types [13], it can be hypothesized that the CVSS content itself affects the time delays. Not all vulnerabilities are equally easy (or hard) to classify in terms of severity; hence, some vulnerabilities may take a relatively short (long) time to classify. This reasoning is presented as a second research question, stated as follows.

Two types of covariates are used for modeling the time delays in (1). The first contains the CVSS information itself. The CVSS (v. 2) standard [6] classifies the impact of vulnerabilities according to confidentiality, integrity, and availability (CIA). Each letter in the CIA acronym further expands into three categories that characterize the impact upon successfully exploiting the vulnera- bility in question. Thus, the analytical structure behind the impact dimension can be illustrated with a diagram:

possibly regardless of the impact upon confidentiality, integrity, and availability. There exists also some empirical evidence along these lines [1]. However, the impact and exploitability dimensions both relate to intrinsic characteristics of vulnerabilities; they are constant across time and environments. For instance, EXPLOIT- ABILITY cannot answer to a temporal question about whether an exploit is known to exists for the vulnerability in question [30,43]. The same point extends toward NVD in general [8]. For these and other reasons, the new (v. 3) standard for CVSS enlarges the dimensions toward temporal and environmental metrics.

For the present purposes, however, the impact and exploitabil- ity dimensions are sufficient for soliciting an answers to RQ2. This choice is also necessitated by the paper’s focus on NVD, which does not currently provide full CVSS v. 3 information [29]. Despite of this limitation, a correlation between the six CVSS metrics and Di could be expected due to the fairly detailed criteria used for the manual classification. Complex vulnerabilities with severe impact may require more evaluation work than trivial vulnerabilities; a remote buffer overflow vulnerability is usually more difficult to interpret compared to a trivial cross-site scripting vulnerability. Also the reverse direction is theoretically possible; more effort may be devoted for high-profile vulnerabilities [18]. Either way, RQ2 seems like a sensible hypothesis worth asking.

The three impact metrics measure the severity of a vulnerability on a system after the vulnerability has already been exploited. However, not all vulnerabilities can be exploited; therefore, the CVSS standard specifies also an exploitability dimension for vul- nerabilities. Like with the impact dimension, exploitability expands into three metrics (access vector, complexity, and authen- tication) that can each take three distinct values. The analytical meaning can be again summarized with the following diagram:

so-called dummy variables. For each metric, the reference category is marked with a star in the previous two diagrams. For instance, INTEGRITY is expanded into two dummy variables, INTEGRITY (PARTIAL) and INTEGRITY(COMPLETE), say, the effects of which are compared against INTEGRITY(NONE), which cannot be included in the models due to multicollinearity. The same strategy applies to the metrics used for evaluating RQ1. Namely, the annual

The first model M1 regresses D = [D1, ... , Dn]' against a constant dummy variables present in the (n × 6) matrix XIMPACT. The second represented by a n-length vector of ones, 1, and the six impact model is identical except that further six dummy variables are

Despite of the growing number of CVEs processed from the circa mid-2000s onward [32], the time delays for CVSS processing have steadily decreased over the years. As can be seen from Fig. 4, there have been no extreme outliers in recent years, meaning that most of the right tail in Fig. 2 is attributable to older CVEs. A possible but speculative explanation is that the work done to update old CVEs with CVSS (v. 2) information has mostly been completed.

The strong decreasing trend is likely to support a positive answer to the research question RQ1. Given this prior expectation, the main interest in the forthcoming analysis relates to the statis- tical effect of the impact and exploitability metrics when also the annual trend is modeled. One strategy for evaluating the research question RQ3 is to compare the models M1 and M2 against the full information model M3. If the CVSS metrics provide statistical power for predicting D, this power should be visible also when the decreasing annual trend is controlled for.

However, the model assumes that D is distributed from the Poisson distribution, which, in turn, implies that the mean of the time delays should equal the variance of the delays. As can be con- cluded from the numbers shown in Fig. 2, this assumption is clearly problematic in the current setting. While b is still consis-

the joint significance of the dummy variable groups with a F-test, all groups are significant at a p < 0.001 level. Also the combined forward-stepwise and backward-stepwise algorithm (as imple- mented in the step function for R) retains all coefficients in b^a .

Thus, based on statistical significance, positive answers would be given to all three research questions. This conclusion would be unwarranted, however. Most of the coefficients in the M3 model are close to zero, irrespective of the estimation strategy. Since all covariates are dummy variables (and, hence, have the same scale), this observation can be illustrated in the form of Fig. 5, which plots the OLS coefficients (y-axis) against the corresponding NBM coeffi- cients (x-axis), omitting the constant b^1. As can be seen, there are some differences between the two regression coefficient vectors, but these differences apply mostly to the annual effects. In partic- ular, the coefficients for the impact and exploitability dimensions are very close to zero without notable differences between the OLS and NBM estimates. The largest absolute coefficient values are obtained for the annual effects from 2005 to 2017. These coef- ficients exhibit also the largest differences between the OLS and the negative binomial estimates.

Second, the positive answer to RQ1 is a negative finding in terms of existing academic research; the historically long time delays presumably translate into selection biases in some existing empir- ical studies using CVSS information. Without naming any particu- lar academic study, consider that a hypothetical article published in the late 2000s used a NVD-based dataset of CVE-referenced vul- nerabilities published between 2000 and 2007, say. The long time delays during this period imply that a lot of the vulnerabilities in the dataset could not have had CVSS information. Consequently, some existing academic studies are exposed to difficult questions related to sample selection and missing values, among other issues. This concern is particularly pronounced regarding studies that examine time-sensitive topics such as vulnerability disclosure.

LASSO computations are shown in Figures 6 and 7 for the Gaussian and Poisson specifications. The coefficient magnitudes are shown in the y-axes, the lower x-axes represent different values of k in loga- rithm scale, and the upper x-axes denote the number of coefficients not regularized to zero. The shaded region is based on a 10-fold cross-validation: in each plot, the left endpoint of the region corre- sponds with the value of k that gives the minimum cross-validation error, while the right endpoint is one standard error from this minimum.

particularly with respect to b^b . Although a couple of exploitability metrics retain their magnitudes within the cross-validation region shown in the lower-right plot in Fig. 7, the same conclusion applies more or less also to the Poisson LASSO model. Furthermore, within the cross-validation regions, both b^b and b^c compare well to the OLS and NBM coefficient vectors illustrated in Fig. 5. To conclude: when predicting the time delay from CVE publications to CVSS assignments, the actual CVSS content is largely noise; the most rel- evant readily available information comes with the decreasing annual trend.

This short empirical paper examined the time delays that affect CVSS scoring work in the context of NVD. Three research questions were presented for guiding the empirical analysis based on regres- sion methods. The results are easy to summarize. The CVSS content is correlated with the time delays (RQ2), but the correlations are spurious; the decreasing annual trend affecting the time delays (RQ1) also makes the effects of the CVSS content negiligle (RQ3). Three points are worthwhile to raise about the significance of these empirical findings.

First, the negative answers to RQ2 and RQ3 are positive findings in terms of practical applications using CVSS information. Whether the application context is governmental security intelligence sys- tems or commercial security assessment tools, there is currently no particular reason to worry that a NVD data feed would show significant delays for the CVSS information. Likewise, in 2017, there is no reason to suspect that information for severe vulnera- bilities would tend to arrive later (or earlier) than information for mundane vulnerabilities. However, this conclusion does not

nerability databases has surpassed a point after which statistical significance starts to lose its usefulness for inference in applied research. The current rate of new vulnerabilities archived—about 17 per day in 2016—implies that the problem with statistical sig- nificance is only going to get worse. The point is particularly important in case CVEs are referenced with other datasets, includ- ing big data outputted by intrusion detection and related systems. The regularized regression models used in this paper offer one solution to consider in further applications, but more research is required to assess the existing biases and the potential means for moving forward.

J. Geng, D. Ye, P. Luo, Predicting severity of software vulnerability based on grey system theory, in: Proceedings of the International Conference on Algorithms and Architectures for Parallel Processing (ICA3PP), Lecture Notes in Computer Science, vol. 9532, Springer, Zhangjiajie, 2015, pp. 143–152.

NIST, NVD Data Feed and Product Integration, National Institute of Standards and Technology (NIST), Annually Archived CVE Vulnerability Feeds: Security Related Software Flaws, NVD/CVE XML Feed with CVSS and CPE Mappings (Version 2.0), 2017a. Retrieved in 23 September 2017 from: <https://nvd. nist.gov/download.cfm>.

J. Ruohonen, S. Rauti, S. Hyrynsalmi, V. Leppänen, Mining social networks of open source CVE coordination, in: Proceedings of the 27th International Workshop on Software Measurement and 12th International Conference on Software Process and Product Measurement (IWSM Mensura 2017), ACM, Gothenburg, 2017, pp. 176–188.

