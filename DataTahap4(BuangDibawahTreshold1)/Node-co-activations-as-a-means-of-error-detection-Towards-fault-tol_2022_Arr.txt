Method: We trained four NNs. For each, we studied how often each pair of nodes activates together. In a separate test set, we counted how many rare co-activations occurred with each input, and grouped the inputs based on whether its classification was correct, incorrect, or whether its class was absent during training.

Conclusions: As rare co-activations are more common in unprecedented inputs, they show potential for detecting concept drift. There is also some potential in detecting single inputs from untrained classes. The small difference between correctly and incorrectly predicted inputs is less promising and needs further research.

Large numbers of rare co-activations indicate problems in predic- tions. Rare co-activations are, on average, much more common in inputs from untrained classes than in inputs the model has been trained for. Thus, rare co-activations show good potential in detecting drift in incoming data: should the average number of rare co-activations increase, drift is most likely imminent. However, inputs from trained classes contained outliers with a high occurrence number of rare co- activations as well, and some untrained inputs have a low number of occurrences. Thus, detecting inputs that the model cannot handle and preventing them from being used further down in the system is more problematic. Considering the difference in the average number of occurrences, it may be possible to find systems and contexts where using it is feasible, but the system should be able to deal with some false positives and negatives. Additionally, rare co-activations tended to be more common in incorrectly predicted inputs than in correctly predicted ones, but the difference was both smaller and statistically less significant. Thus, detecting single inaccurate predictions may not be feasible based on the number of occurrences alone, but the approach should at least be fine-tuned to find the most indicative co-activations.

System dependability is threatened by failures, errors, and faults [8]. Failures are deviations from a desired service. They are caused by propagating errors made by the system, i.e. incorrect functioning of the system. Errors are caused by faults that are defects in system components (software or hardware), activated by given inputs in a given state.

The technique responsible for the activation is an activation func- tion [12]. A rectified linear unit (ReLU) is a commonly used activation function. ReLU very closely follows the philosophy of either activating or remaining dormant. Mathematically ReLU is usually formulated as

computations, but if the inputs are very strong, the effect the node has on the following layer is also strong. According to Sharma et al. [12], ReLU has proved to be very effective and is one of the most used activation functions today.

In neural networks, various groups of nodes tend to take responsi- bility of different outcomes [4]. In their work, Tian et al. showed that different groups of nodes in a neural network for autonomous driving tended to activate based on whether the neural network proposed turning to the left or to the right. Xie et al. [5] also suggest that transforming a test input too much will lead to a deformed activation pattern and a wrong result, suggesting that the mutated pattern is related to the incorrect result.

In this section, we introduce the concept of rare co-activations, a novel approach to estimate the typicality of activation patterns in a neural network. Our goal is to show that correctly predicted inputs dif- fer from problematic inputs with regards to rare co-activations within the network. Thus, atypical activation patterns would indicate untrust- worthy predictions. If this is the case, monitoring rare co-activations would show potential in error detection in neural networks.

To produce meaningful results, the set of inputs used to calculate the co-activation rates should be chosen appropriately. The approach we chose was to calculate the co-activation rates after the networks were trained and use the same training set that was used to train them. In this way, the co-activation rates should represent the activation patterns of the input classes that the network should be able to generalize to. Thus, co-activation rates describe the inner workings of the neural networks in cases where the network can reasonably be expected to handle correctly, whereas cases that have no representation in the training set may not produce good results.

The pattern we study is rare co-activations introduced above in Section 3.1. As the activations tend to form patterns [4], it makes sense that nodes in shared groups often activate together. If often activating together implies being in one or more of the same patterns, it may not be unreasonable to think that the disjointed and atypical patter manifested in rare co-activations implies a broken pattern and an untrustworthy prediction. Thus, we try and show whether there is a utilizable connection between rare co-activations and untrustworthy predictions. More specifically, we aim to answer the following research questions:

The aim of RQ1 is to explore whether the idea is valid in the first place. Only a statistically significant difference in the number of rare co-activations allows us to argue that our approach has any potential in building fault-tolerant ML systems. If the distributions between cases where the neural network made a correct prediction and cases where the prediction was wrong or the input never appeared in the training set are not statistically different, we cannot claim that any meaningful conclusions can be drawn from the number of rare co-activations.

In this section, we describe how the experiments were conducted. First, Section 4.1 describes the neural networks from which we gath- ered the data about the rare co-activations, along with how the net- works differ from each other and why. Then, in Section 4.2, we describe what kind of data about rare co-activations in those networks was gathered and how. Finally, in Section 4.3 we describe how the data was analysed to draw conclusions and to ensure statistical significance of our findings. This combination of data triangulation [14] across networks and statistical rigour [15] raises confidence in our findings.

To mimic Scenario 3 from Section 3.2 (a class of inputs was missing from the training set, but appears after training), one class was excluded from the training set, similarly to Ackerman et al. [17]. This way, in the testing phase with a separate test data set, we have both inputs that belong to classes the network was trained to recognize, along with inputs that the neural network should not have extensive knowledge of. Thus, the excluded class represents situations where all inputs do not resemble the data that the neural network was trained with.

CNN-ankle boot9: CNN-ankle boot9 shares most features with CNN-ankle boot except for the number of nodes on the output layer. The filtered class is the same, along with the hidden layers in the neural network. The difference is that the output layer has no reserved output node for the filtered class. This naturally results in only having nine nodes on the output layer.

CNN-shirt: CNN-shirt is structurally identical to CNN-ankle boot. The difference is that the class filtered out from the training set is class 6 (Shirt) instead of class 9 (Ankle boot). The purpose of this is to assess whether the phenomena we find are independent from the filtered class or not. Shirt was chosen, as it is evidently different from ankle boots, whereas, for example, sneakers may not be.

The reasoning behind the inclusion of MLP-ankle boot is twofold. First, including models with different topology provides additional information whether the results are dependent on certain technologies or not. Second, MLP-ankle boot is a smaller network than the other neural networks. This should give us implications of the effect size that the size of the network has on the phenomena.

Data were collected with an experimental set-up utilizing Keras and NumPy.3 First, the networks were trained using a training set, from which one class was entirely excluded. Next, co-activation rates for each node in each neural network were calculated using Algorithm 1, introduced in Section 3.1, and the same data that were used for training the network, still excluding one class. Finally, the number of rare co- activations was computed and saved for each input in a separate test data set that included all the classes.

Additional details were considered before calculating the co-activation rates, e.g. when should a node be counted as activated. An apparent choice would be when the node outputs a non-zero number, as that is de facto how a ReLU activation function works. However,

Furthermore, it is not obvious which output should be counted in the convolutional parts of the CNNs. Activation functions are applied first in the CNNs, after which the strongest activations close to each other are gathered by the pooling layer, while the weakest are filtered out. Thus, there are two consecutive parts that have the outputs of the activation function as their values. We chose to use the outputs of the pooling layer, as they are the ones actually affecting the computations of the following layers.

After the co-activation rates for every neural network were calcu- lated, the number of rare co-activations was counted and saved using Algorithm 2. First, for each input in the Fashion-MNIST test set, we mark down which of the three scenarios the input represents: is the input predicted correctly, incorrectly, or is it untrained (cf. Section 3.2).

Data analysis is based on statistical tests. To answer RQ1 (do the three scenarios differ in terms of rare co-activations), we assessed whether or not the data points in various groups actually originated from different distributions. That is, we are not only interested in whether our samples are different from each other, but we also want to generalize the results to the entire populations from which the samples originate. Using a statistical test, we can determine how certain we can be that not only the samples are different, but the populations behind them as well. Only after this do descriptive statistics, such as the mean, minimum, and maximum, hold strong relevance when comparing the groups. Once the difference is set by tests designed to do just that, these descriptive statistics reveal the nature of the difference.

In this section, we examine our results obtained using the experi- mental set-up. The results are presented for every neural network in their own subsection. In turn, for every network, the results are pre- sented for each activation threshold and each rarity threshold, starting from the lowest one. (see Section 4.2 for more details). Only statistically significant results are presented in detail.

activation rate of less than 5%, 1%, or 0.1% in the training set, manifest differently when the prediction is correct, incorrect, or with unknown inputs. Next, we present the descriptive statistics of the groups to compare how the groups differentiate. The comparison is made for each rarity threshold, as they all hold significance.

However, the highest number of rare co-activations occurred when the model was correct. This applies for the highest rarity threshold, but the number remains relatively high with the lower thresholds as well, even if the highest maximum number is in the incorrectly predicted ones. This suggests that even correct outputs have outliers with large numbers of rare co-activations.

Activation threshold 0.0156: Next, we raise the activation thresh- old to 0.0156. Rare co-activations are more common in incorrectly predicted and untrained inputs than in correctly predicted ones. The average numbers of occurrences are higher and the differences are statistically significant. The chosen threshold is smaller than 99% of all non-zero activations that occurred in CNN-ankle boot in the training set.

Activation threshold 0.112: Next, we raise the activation thresh- old to 0.112, which is smaller than 90% of non-zero activations occur- ring in the neural network with the training set. Rare co-activations are more common in incorrectly predicted and untrained inputs than in correctly predicted ones. The average numbers of occurrences are higher and the differences are statistically significant.

In this subsection, we go through the results for the model CNN- ankle boot9 in a similar manner. This model is otherwise similar to and similarly trained as CNN-ankle boot, but does not have an output node for the class that was filtered out of the training set. See Section 4.1 for more details.

statistically significant. Thus, strong claims relating to the differences should be avoided. We would, however, like to point out that the mean number of occurrences in incorrectly predicted inputs is still higher than in correctly predicted inputs, which does follow the trend set by the higher rarity thresholds.

In this subsection, we go through the results for the CNN-shirt model in a similar manner. CNN-shirt is otherwise similar to and similarly trained as CNN-ankle boot, but the class that was filtered out of the training set was class 6 (Shirt) instead of class 9 (Ankle boot). See Section 4.1 for more details.

predicted inputs. The descriptive statistics show higher averages and the differences are statistically significant. Rare co-activations are arguably more common in incorrectly predicted inputs than in correctly predicted ones as well according to the descriptive statistics but the differences are not statistically significant.

Activation threshold 0.018: For activation threshold 0.018, rare co-activations are more common in untrained inputs than in correctly predicted inputs. The descriptive statistics show higher averages and the differences are statistically significant. Rare co-activations are ar- guably more common in incorrectly predicted inputs than in correctly predicted ones as well according to the descriptive statistics but the differences are not statistically significant.

Activation threshold 0.154: For activation threshold 0.154, rare co-activations are more common in untrained inputs than in correctly predicted inputs. The descriptive statistics show higher averages but the differences are statistically significant only with the lowest rarity threshold. Rare co-activations are arguably more common in incor- rectly predicted inputs than in correctly predicted ones as well accord- ing to the descriptive statistics but the differences are not statistically significant.

in untrained inputs than in correctly predicted ones. Conversely, un- trained inputs average a smaller number of occurrences than incorrectly predicted inputs. The maximum number of occurrences is smaller in untrained inputs than in the other two, with correctly predicted inputs having the largest number. The median and minimum number of occurrences is 0 in every scenario.

In this subsection, we present the results for the MLP-ankle boot model in a similar manner. MLP-ankle boot is unlike the other models, as it is a multi-layered perceptron instead of a CNN and contains much fewer nodes than the other models. See Section 4.1 for more details.

Activation threshold 0.0339: For activation threshold 0.0339, rare co-activations are more common in untrained inputs than in correctly predicted inputs. The descriptive statistics show higher av- erages but the differences are statistically significant only with the highest rarity threshold. Rare co-activations are arguably more com- mon in incorrectly predicted inputs than in correctly predicted ones as well according to the descriptive statistics but the differences are not statistically significant.

The three scenarios (correctly predicted, incorrectly predicted, and untrained inputs, presented in Section 3.2), differ in the occurrences of rare co-activations. For every model, at least some of the chosen combinations of activation and rarity thresholds produced statistically significant differences between the distributions of rare co-activations in the scenarios. This assures that rare co-activations as a concept is something to look into further.

Rare co-activations are, on average, more common in untrained inputs than in correctly predicted ones. This is the most consistent result we obtained across all models. In the vast majority of tests, the difference between the distributions of these two scenarios is statis- tically significant, and not once do correctly predicted inputs have a higher mean value of occurrences. The median can be 0 for both, but it is never higher for correctly predicted inputs. Thus, a larger number of rare co-activations is related to never-seen-before inputs.

The largest difference between CNN-ankle boot and CNN-shirt is that correctly and incorrectly predicted inputs do not differ in CNN- shirt to a degree that is statistically significant with our sample. Es- pecially with lower rarity thresholds, rare co-activations tend to be slightly more common in incorrectly predicted ones, but making any stronger claims on the matter is not possible with our sample. As discussed above, the statistical insignificance in this case may be due to our smallish sample size of incorrectly predicted inputs in CNN-shirt.

Changing the overall structure of the network from a large CNN to a small MLP does not change the overall trend of the results but has a great effect on the number of occurrences. Rare co-activations are still most common in untrained inputs, with the comparison of correctly and incorrectly predicted inputs falling short of statistical significance. One large difference compared with the other networks is that rare co-activations occurred far less overall in MLP-ankle boot. With the two lowest activation thresholds, the maximum number of occurrences per input was 1, with 0 being far more common in every scenario. Occurrences were more common in untrained inputs than in correctly or incorrectly predicted inputs. The results were not much different with the highest activation threshold either.

Another thing to note in the MLP is that, while the rarity threshold in CNN-shirt had to be lowered to find statistical significance with a high activation threshold, in MLP-ankle boot, only the highest rarity threshold produced statistically significant results for each activation threshold. This occurred because there simply were not enough rare co-activations below the lower thresholds, meaning that most nodes in the small network tend to activate together at least sometimes. This suggests that the larger networks have more room for the activation patterns to grow partially or even completely separate, whereas, per- haps unsurprisingly, the nodes in the smaller MLP need to contribute more to each computation.

When considering the usefulness of rare co-activations in error detection to achieve fault tolerance, the types of misbehaviour to be targeted with it must be considered. As discussed in Section 3.2, we consider whether drift in input data can be detected by utilizing the rare co-activations, whether a single input can be detected as something the network is not trained to handle, and whether an incorrect prediction can be detected. Below, we discuss how the rare co-activations would fit these tasks based on our results.

As to detecting drift in incoming data over a period of time, rare co- activations show great promise. For every used network, we found more than one combination of activation and rarity thresholds for which the average number of rare co-activations was largest for untrained inputs and the difference was statistically significant. Not only that, but the difference was often quite large, especially for larger networks, and it was not dependent on the input class that was excluded from the training phase. Based on this, drift in incoming data could be monitored by monitoring the number of rare co-activations: if the numbers per input grow, it could indicate drift.

Detecting incorrectly predicted inputs is less likely to be relevant based on our data. The differences between correctly and incorrectly predicted inputs are so small that they are not statistically significant in every network with our sample. Additionally, even if the differences were significant, they tend to be so small that it is arguable whether they are relevant when used to achieve fault tolerance. In other words, finding an appropriate number to be used as a threshold for flagging the result becomes very difficult and the usefulness of detecting single incorrect predictions suffers.

very close to the chosen level. For this reason, especially for incorrectly predicted inputs, we have noted trends that are quite consistent but not statistically significant in a toned-down manner, hinting at results that could be found in, for example, a larger sample. Coincidentally, the sample size of incorrectly predicted inputs is another thing we consider to be a threat to statistical conclusion validity, especially in the CNNs. Internal validity means the validity of causality: do the treatment and the outcome actually reflect the causality between them? In our pa- per, this would mean whether or not the number of rare co-activations is actually a valid indication of an incorrect prediction or an untrained input. The answer seems to be twofold. According to statistical tests and descriptive statistics, larger numbers of rare co-activations occurring especially in untrained inputs are both significant and, we would argue, relevant. As such, rare co-activations are in some relation to the phenomena we are studying. However, the large maximum number of rare co-activations in correctly predicted inputs suggests that the number of rare co-activations should not be treated as an absolute indication of an incorrect prediction or an untrained input. Thus, there could be more fine-tuned details that are even more indicative of these troubled inputs, as discussed at the end of Section 6.2. Also, we try to avoid making too strong claims of the usefulness of rare co-activations with regards to promoting fault tolerance when the statistical results

Construct validity means the validity of conceptualization and the- oretical generalization, i.e., whether the concepts are properly defined and understood. This type is difficult to assess, as the novelty of this study are the constructs we must deal with. Concepts of co-activation rate and rare co-activations are something we define here, and what implications they may have is something we aim to understand. In other words, gaining a better understanding is our goal. As such, on the one hand, we have full understanding of the concepts, as we are the ones who defined them here. On the other hand, we are only just finding out how they behave in certain situations, and the results we have is all the understanding we have gained of the phenomenon. We

As to the data set used, the choice of using Fashion-MNIST is another threat to generalizability. Fashion-MNIST is widely used in research literature, and many consider it to be somewhat challenging to ML models. However, it is by no means an industrial data set. Because of the wide use of Fashion-MNIST, we do not believe this to be a fatal threat to our results, but we do believe that generalizability would benefit from the approach being applied in an actual industrial network, trained with industrial data.

We have presented a study that defines the concept of co-activation rate, investigates how rare co-activations manifest in correctly or in- correctly predicted inputs a neural network has been trained to handle and inputs it has not been trained for, and, based on the previous note, how the rare co-activations could be used in runtime risk mitigation as a tool to detect errors and promote fault tolerance. To produce the results, we first trained multiple different neural networks, after which co-activation rates were computed for each node. Using a separate test set and the co-activation rates, we counted how many times rare co- activations occurred for every input, and then labelled the input as a correctly or incorrectly predicted one if it belonged to a class that was present in the training set or an untrained input if it belonged to a class that was excluded from the training set.

Rare co-activations are more common in untrained inputs than in inputs that the network was trained to handle, and especially the ones that the network predicted correctly. Thus, monitoring rare co- activations over time could be used to monitor drift in the incoming data. If the number of rare co-activations per input rises, the share of inputs the network was not trained for also rises. However, detecting whether a single input is something the network is trained to handle is a bit trickier. This is mostly because the trained inputs, including the ones the network predicts correctly, also include few inputs with large numbers of rare co-activations.

The difference between correctly and incorrectly predicted inputs is not so clear. There is a tendency that rare co-activations occur slightly more often in incorrectly predicted inputs but the difference tends to be smaller than when comparing with untrained inputs. Thus, detecting incorrect predictions based solely on rare co-activations may not be feasible with this approach. However, as the number of occurrences tends to be slightly higher in incorrectly predicted inputs, trying to find which kind of rare co-activations are the most indicative of incorrect prediction could be a worthwhile research question for the future. This could mean, for example, studying whether rare co-activations on earlier or later layers or rare co-activations across layers is more indicative.

Additionally, the results would benefit from more empirical follow- up studies. Even if Fashion-MNIST is widely used, it is not an industrial data set. An actual industrial setting would provide an even stronger indication of the usefulness of the results we have found.

