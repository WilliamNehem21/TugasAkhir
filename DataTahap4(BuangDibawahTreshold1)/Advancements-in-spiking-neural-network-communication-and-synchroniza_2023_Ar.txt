POLite. Another high-level programming environment toolchain is PO- Lite [28], which is similar to the vertex-centric paradigm [29,30] but supports both synchronous and asynchronous messaging. POLite is a layer of abstraction that manage mapping arbitrary graphs onto the Tinsel overlay. POLite is a C++ light software layer on top of the Tinsel hardware designed to implement a graph-based event-driven abstraction, while it is able to hide architectural details from the user. Similar to Graph Schema, the vertices receive the events and if the state of the vertex is changed, it will send a message via the edge to the other connected vertices. The event messages are treated by the following event handlers:

/ig. 2. (a) One POETS box. Each box has 7 FPGAs, 6 workers connected to an X86 GPP (General Purpose Processor) via one intermediate FPGA. (b) A default configuration of the Tinsel Network on Chip (NoC) on a single FPGA. (c) Default structure of a Tinsel tile. The cores are highly reconfigurable and extendable softcore processors.

Step handler: The step handler is called whenever no vertex in the graph wishes to send, and there are no messages in-flight. It returns a flag, indicating whether the vertex wishes to compute. Typically, an asynchronous application returns a false, while a synchronous one will compute for the time-step, and requesting to send again. It returns true if it wishes to start a new time-step.

Machine learning algorithms have been used to solve tasks in au- tomation, recognition, classification, and prediction without being ex- plicitly programmed [31]. In other words, they are capable of learning from data. Artificial neural networks are a class of machine learning models that are loosely inspired by their biological counterparts [32]. Conventional neural networks continuously transmit real-valued sig- nals between neurons that can be interpreted as firing rates. In contrast, spiking neural networks (SNNs) operate via the transmission of discrete events, referred to as spikes, mimicking action potential generation in the brain. SNNs offer huge energy savings due to their sparse event- driven nature, however, training and simulating such models remains challenging [33].

In event-driven systems, we need a synchronization barrier to man- age when to start a new calculation to separate the current and the next computation steps. This barrier could be a software or hardware barrier. Table 1 lists the response to synchronization barriers. In case of a software barrier no Idle event happens and for a hardware barrier there is no action in the Send event. That is, in the hardware barrier, the neuron accumulates the received spikes and will send when an Idle

/ig. 3. A finite state machine that can be applied for three different synchronization methods in event-driven networks. (a) Clocked synchronization (CS) method, (b) globally asynchronous locally synchronous (GALS) communication, (c) Hardware idle detection (HID). The left side of the figure shows the Finite-state machine (FSM) and the right side shows the network configurations in respect to the synchronization method illustrations.

Although spikes propagate between the nodes asynchronously, we do need to ensure that a Send message is invoked only when all incoming messages from the previous time step have been collected. This is particularly important in POETS, which guarantees the arrival of messages, but not their order of arrival. We may address this problem using different algorithms for synchronization, as depicted in Fig. 3. First, a software barrier known as clocked synchronization (CS), second, globally asynchronous, locally synchronous (GALS) communication and, third, a hardware barrier known as hardware idle detection (HID). In the following, we describe these approaches in more detail.

but also synchronize messaging with neighboring neurons. We define three states for this method: Collecting spikes, Ready to step, and Ready to send spike in a state machine (See Fig. 3.a). In the CS method, the clock device in each state-machine has to send a message to every neuron in each time step, regardless of whether it spikes or not. Hence, the basic considerations of the algorithm are as follows:

For a transition from the collecting spike state, if a tock message arrives and the number of times that neuron has been seen by connected neurons is equal or bigger than the degree of connections in each neuron, the state will change to ready to step. This method is known as a voting mechanism, which means that all connected nodes agree on delivering their sending spikes in the previous time step after tick-tock messaging. In the ready to step state, neurons announce their membrane potential firing state and unconditionally the state will change to ready to spike and another tick will be sent to inform connected neurons. In the ready to spike state, the neuron will send the firing state to its connected neurons.

GALS: Globally asynchronous, locally synchronous (GALS) communi- cation is a method that enables the synchronization of a group of locally clocked modules to facilitate effective communication among [35]. GALS has more degree of freedom compared to the CS method by eliminating the need for a global clock. GALS systems, with their localized clocks, provide better fault isolation. If a module fails, it primarily affects its neighboring modules, minimizing the impact on the entire system. GALS systems are highly scalable as they can easily accommodate the addition or removal of modules without affecting the overall system timing. This scalability is particularly beneficial in large-scale systems where CS becomes more challenging. By allow- ing individual components to operate at their own clock frequencies, power can be saved by avoiding the need for high-frequency oper- ation throughout the entire system [36]. The following is a list of fundamental considerations for GALS methods in our system:

in larger networks, the number of messages surpasses that of clocked synchronization (CS). Nevertheless, removing global synchronization will accelerate computation, especially in non-fully connected networks running on distributed, federated or heterogeneous systems can facili- tate parallelization. Running this method on our platform, in addition to reviewed benefits of GALS, we predict more capability to speed up the computation and communication due to the lack of global clock connections.

Hardware idle detection (HID): In this research, we introduce HID for the neuromorphic event-driven system synchronization based on termination detection [28]. A hardware barrier synchronization will synchronize the event-based neurons in the HID method. It means a neuron does not wait for any other neurons to receive messages. The HID method is designed for globally-asynchronous applications, and it uses a signal to ensure there is no undelivered message in the system. HID identifies an event when there is no thread in the system with

pending send, receive or computation tasks, known as hardware idle. This hardware idle state keep the neurons synchronized. The state machine is shown in Fig. 3.c which represents Collecting spike and Wait to step states. HID implements the following in our neuromorphic system:

communication is simple and generates a smaller number of messages in the network compared to CS and GALS. Furthermore, there is no synchronization except an idle detected hardware signal, which makes this method faster compared to the other two methods. However, we need to provide a handler which is called for all nodes. Hence, any node which takes longer to reach the barrier due to having more messages to handle, will enforce other nodes to sit idle. It takes more execution time, particularly if the tasks to be executed are Heteroge- neous. Consequently, as the synchronization barrier will happen in the hardware, there will be less control and debugging facilities if there is an error in the system. Moreover, the advantage of employing HID is substantial when dealing with small graphs. However, this advantage diminishes as the computational load per time step increases, whether by assigning more vertices to each thread or by augmenting the fan- out and consequently the quantity of messages handled by each thread [37]. Conversely, if the computational load per time step decreases, such as through the addition of more cores, we can anticipate an enhanced benefit from HID.

POETS is designed to efficiently simulate highly scalable event- based models. We will focus on simulating spiking neural networks that implement parallel distributed processing at large scale. In the previous section, we presented a generic model that could be used for any neuron model. Here, we demonstrate how the Izhekevich model can be mapped on hardware. The balance of excitatory to inhibitory neurons ratio in the spiking network is 80% to 20% respectively.

Routing algorithms serve the purpose of directing data packets, while each algorithm being a software tasked with determining the most efficient path for transmitting a packet. Achieving effectiveness and efficiency in routing stands as a paramount aspect within NoC- based (Network on Chip) neuromorphic systems [38]. Message delivery in POETS system is guaranteed by the hardware provided that all threads eventually execute the messages available to them. Threads communicate with each other via mailboxes. Two different methods have been used for message communication between threads, namely unicast and multicast. In the unicast method, there is a point-to-point communication between two threads in which a single packet is sent to a single destination. The aim of multicasting is to send the same message to multiple destinations while minimizing messaging traffic in the system. Messages first will be delivered to the programmable routers, which automatically propagate messages to any number of destination threads distributed throughout the cluster. If a router sup- ports multicast routing, then neurons with a high fan-out can be communicate efficiently with minimizing inter-FPGA bandwidth while offloading work from the processing cores. Tinsel provides both unicast and multicast communication by having a programmable router on each FPGA board to support global multicasting.

The SNN is mapped as a graph on POETS where the neurons rep- resent the device vertices and the edges of the graph represent plastic synaptic connections between neurons. The POETS ecosystem supports mapping different feed-forward, including local circuitry similar to convolutional neural network (CNN) or recurrent network topologies. After mapping the network topology graph onto the hardware, the connections remain fixed during the running time of SNN on hardware. Firstly, we define the number of neurons and the connection between the neurons. The neuron model will be defined, and the parameters will be initialized subsequently. In the next step, the high-level software design will be compiled using Graph Schema or POLite to be transferred to FPGA as Verilog code and data. Finally, the code will be mapped onto the POETS hardware. Fig. 4 shows the steps from high-level modeling to the hardware mapping using the POLite application to simulate a neural network on POETS. The same steps are presented using a Graph Schema API in previous work for simulation of SNNs on POETS [20].

In this work, the FPGA-hardware implementation for up to two million neurons on one cluster (box) and up to 8 million neurons on 8 clusters is performed while each neuron is connected to 100 to 1000 other neurons. Most of the SNN mapping is implemented using one POETS box which includes six FPGA boards. By employing an extensive setup, we aimed to push the system to its limits, testing its performance and scalability across 8 interconnected POETS boxes using all 48 FPGAs and 49,152 threads uncovering the full potential of the platform. It provides valuable insights into capabilities of system in tackling com- plex computational tasks and addressing large-scale problems. We have also compared the speed performance of the network with a different number of neurons using 1, 2, 4, and 8 of POETS boxes. The properties of one FPGA board are listed in Table 3.

Each box is hosted by an x-86 machine with 28 cores Intel(R) i9-7940X CPU@3.10 GHz. Although in previous work, we have used MNIST data set as an output due to establishing a learning algorithm on the hardware [19]. In this research, the input spiking data is generated randomly using a normal distribution for different networks to verify the network capability running different number of neurons. We implemented a random recurrent neural network with sizes from 100 to 8 million nodes and placed on one to 8 hardware boxes. The testing benchmark is similar to [13] but in more scalable sizes. The critical check points for the number of nodes are 100, 200, 500, 1k, 10k, 50k, 100k, 500k, 1000k, 2000k, 8000k. The number of synapses per neuron has been defined to verify a normal and extreme number of connections, 100 and 1000 synapses per neuron. Due to more robustness of results while using the random seeds inputs, we average

value from a Gaussian (normal) distribution with a mean of zero and a standard deviation that corresponds to the chosen weight range. For connection weight modification, The system supports STDP (Spike Timing-Dependent Plasticity) and reward-based STDP which have been presented in our previous works running on the same platform [19,20]. In this work, we focus on neuron activities and message communication while STDP rule is used for weight modification without decoding any input data pattern.

The POETS ecosystem mapping strategy works efficiently depends on the number of vertices and edges in the mapped graph. If the vertices could be assigned parallelly into the threads of one physical board, it will not use two boards to address high-speed communication and energy saving challenges. The threads on neighboring mailboxes communicate faster compared to threads on further mailboxes, caus- ing these to spend more time and consume more bandwidth in the system. Therefore, it is important how neurons communicate with each other on the same thread, or different threads while still sharing

the same mailbox, or different threads sharing different mailboxes on one or more FPGA boards. The system supports efficient parallel communication and computation if each neuron is assigned to one thread, instead of several neurons to one thread. However, for large- scale networks in which the number of neurons is larger than the number of threads, several neurons will be mapped on the same thread. We can map neurons on threads in different boards, but in this case we need to consider inefficient communication due to distant threads and mailboxes. Hence, there will be a trade-off between parallelism and communication distance. Fig. 7 (left) shows for networks with a different number of spiking neurons that neurons on different threads have the highest communication rate.

There is a trade-off between offloading work from the cores and overloading the programmable routers. Messages which are closer to the destinations consume the less space and energy on the network. As shown in Fig. 7 (right), the effectiveness of using the multicast method compared to the unicast method is most pronounced for medium-sized networks. In our experience, a mix of local sending in unicast methods and offloading via the programmable router is an efficient solution. For very large number of neurons that occupy all eight boxes and a few neurons that could be run on single FPGA boards using the programmable router is less advantageous comparing to the average number of neurons between the lowest number of neurons and the highest system scalability limitation. For more information regarding the routing and message-passing, we refer to [40].

To assess the speed performance of POETS, we need to compare it with other simulators and platforms. We compare the speed of running the network simulation on POETS with the Brian simulator and the SpiNNaker [4] platform using the same number of nodes starting from 100 up to 2 million neurons implemented on one box and 8 million neurons on eight boxes. As demonstrated in Fig. 10, the simulation time for a network consisting of two million neurons is 8.16 and

4.67 s using one box and eight POETS boxes respectively. We used the same networks with the same number of connections for Brian simulation. Results show that the hardware implementation on POETS is more than twenty times faster than the Brian simulator. Additionally, comparing the system with one 48-chip SpiNNaker node shows that POETS is at least 16 times faster. This speed comparison is made using optimal communication parameters such as HID synchronization, multicast routing, and considering homeostasis for POETS as discussed previously.

/ig. 8. The left side shows a comparison of the variety of firing rates in networks of neurons with fixed firing threshold and local adaptive firing threshold. The right side shows the impact of increasing the firing rate on the run duration of the network.

advanced RISC ARM968 processing cores next to custom routing infras- tructure circuits which dedicate 96 kB of local memory besides 128 MB of shared DRAM as depicted in Fig. 11.a. It has a remarkable flexibility, capable of simulating millions of neurons with biologically realistic connectivity supporting learning algorithms. However, the system still uses a von Neumann architecture with a large memory hierarchy as found in conventional computers. Although it uses low-power ARM processors dedicated to power-efficient platforms, the largest machine incorporating over a million ARM processor cores, still requires up to 75 kW of electrical power. The current version has no floating-point support. The improved version named SpiNNaker2 is a 10-million core machine which uses 22 nm scale fabrication technology and supports floating-point operations [43].

NeuroGrid uses analog/digital mixed-signals for modeling neural network components. Similar to TrueNorth, Neurogrid has a non-von Neumann architecture. Neurogrid emulates four neural network com- ponents, namely: axon, dendrite, soma and synapse. Axons are modeled by a digital circuit and the other components are modeled in using analog circuits. NeuroGrid consists of 16 standard CMOS ‘‘NeuroCores’’ integrated on a board that works using 3 W of power. The synaptic circuits are shared among the neurons, while different spikes can be assigned to the same synapse (Fig. 11.c). The long time constant limitation (tens of milliseconds) causes difficulties in using typical VLSI for design and implementation.

spikes across the network [44]. Neurogrid and BrainScaleS similarly use the temporal dynamics of memory elements to store the state of the network, with the capability of local learning. The second version of BrainScaleS is developed as BrainScaleS-2. This version is a multi- chip system building upon existing BrainScaleS wafer-scale system components. The architecture is implemented in a single-chip ASIC

Loihi [10] is a recent neuromorphic platform developed by In- tel [46]. Loihi is an event-driven neuromorphic chip fabricated in Intel’s 14-nm process that uses a discrete-time model for computation distributed over 128 cores that are integrated into an asynchronous mesh. This neuromorphic platform supports variable precision synaptic weights. Each core has 128 kB synaptic state, and 20 kB of routing tables that can be assigned to its 1024 neurons The implementation of 131,072 leaky-integrate-and-fire neurons and more than 130 million synapses has been reported in [10]. Different Loihi systems have been designed by using a multichip mesh architecture, ranging from two chips in a USB device to a rack-mounted system enclosing 768 Loihi chips capable of implementing more than 100 million neurons. Loihi

particular SNN simulation to deliver optimized performance. A 6- FPGA Neuroflow system can simulate a network of 600,000 neurons. One FPGA Neuroflow outperforms a speedup of up to 33.6 times the speed of an 8-core processor, as well as 2.83 times the speed of GPU- based platforms. Similar to BrainScaleS and SpiNNaker, it uses PyNN, a simulator-independent neural network description language for the user interface. More details about NeuroFlow are presented in Table 4. There exist yet other neuromorphic platforms such as Darwin [49],

Messaging delivery. One of the important characteristics of POETS is a guaranty of message into the destination, which for example SpiNNaker suffers from lack of this messaging guaranty. It means there is no lost message on the fly. SpiNNaker has emergency packet re-routing, which allows packets to be sent along an alternative route when a link is detected as failed. In TrueNorth the chip communicates and processes data packets called spikes. Each core can be connected to any other core on the chip by a two-dimensional mesh network, and therefore messages are delivered from each neuron on the crossbar to any axon on the chip. BrainScaleS uses a cyclic redundancy verification to find corrupted messages. In Loihi, neurons communicate via spike events, 32-bit messages containing destination addressing, source addressing and graded-value payloads to distribute between cores. Each core sends generated spikes to down stream fan-out neurons based on configured routing information that allows adapting to the pipeline activities de- lay. Neurogrid uses address-event bus to build networks with thousands of neurons with a few hundred synaptic connections that shared wires communicating addresses that signal arrival of an action potential, or spike.

Synchronization is one of the most important methods in designing neuromorphic event-driven systems in simulating SNNs. Three synchro- nization methods including clocked sync, GALS (Globally Asynchronous Locally Synchronous), and Hardware Idle Detection (HID) have been discussed and analyzed in this work that the HID is introduced in this work as a novel synchronization method for neuromorphic system. To implement these algorithms on hardware, we introduced POETS as a new large-scale neuromorphic system which is flexible using FPGA clusters, easily scalable by adding more FPGA boards, reliable with a guaranty of receiving messages, and fast due to the parallel processing of data and high-speed interconnection bandwidth. Running the SNN on POETS hardware, we demonstrated that HID is the best synchronization approach considering speed and spiking accuracy.

The routing methods for spiking message-passing has been stud- ied in this work too. Results demonstrate the effectiveness of using multicast method of routing (using the programmable router) over the unicast (point-to-point communication) for moderate number of neurons is more visible rather than small or very large number of neurons. The adaptive threshold has been used in spiking neurons, to make the system more stable and showing better results rather than fixed-firing threshold. Connecting this adaptive threshold to the firing rate, we showed that increasing firing rate has less than linear-slope impact on the speed of simulation.

Using the best parameters e.g., synchronization, communication, and routing, we run SNN on POETS hardware in the range of up to two million spiking neurons on one cluster and up to 8 million spiking neurons on eight clusters. Regarding the number of synaptic connections in SNN modeling, up to one billion on one cluster and up to 4 billion synaptic connections on all eight clusters are the maximum numbers that have been modeled in this work. A speed comparison demonstrates that POETS simulates the SNN 20 time faster than Brian simulator and 16 times faster than SpiNNaker using the best approaches for synchronization and communication. This paper provides an archi- tecture overview of current large-scale neuromorphic systems, with a primary focus on investigating synchronization, communication, and routing methods employed in these systems. The future work could be a next generation of this neuromorphic hardware named POETS-

Shahsavari M, Thomas D, Brown A, Luk W. Neuromorphic design using reward- based STDP learning on event-based reconfigurable cluster architecture. In: International conference on neuromorphic systems 2021. ICONS 2021, New York, NY, USA: Association for Computing Machinery; 2021, p. 1–8. http://dx.doi.org/ 10.1145/3477145.3477151.

Rafiev A, Morris J, Xia F, Yakovlev A, Naylor M, Moore S, et al. Practi- cal distributed implementation of very large scale Petri net simulations. In: Koutny M, Kordon F, Moldt D, editors. Transactions on petri nets and other models of concurrency XVI. Berlin, Heidelberg: Springer Berlin Heidelberg; 2022,

Cheung K, Schultz SR, Luk W. A large-scale spiking neural network accelerator for FPGA systems. In: Villa AEP, Duch W, Érdi P, Masulli F, Palm G, editors. Artificial neural networks and machine learning. Berlin, Heidelberg: Springer Berlin Heidelberg; 2012, p. 113–20.

Malewicz G, Austern MH, Bik AJ, Dehnert JC, Horn I, Leiser N, et al. Pregel: A system for large-scale graph processing. In: Proceedings of the 2010 ACM SIGMOD international conference on management of data. New York, NY, USA: ACM; 2010, p. 135–46. http://dx.doi.org/10.1145/1807167.1807184.

