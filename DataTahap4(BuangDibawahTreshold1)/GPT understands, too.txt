However, we observe that manual discrete prompts suffer from a large degree of instability. As shown in Table 1, with a frozen language model, changing a single word in the prompt might result in substantial performance drop. As we will show in Section 3, when the language model is tuned, the instability problem is alleviated but the perfor- mance difference between different prompts is still sizeable, especially in the few-shot setting. Such an instability issue of discrete prompts poses a crit- ical challenge in practice. Recent approaches of automatic prompting have attempted to search for a better-performing prompt given a task (Shin et al., 2020; Gao et al., 2020; Jiang et al., 2020b), but these methods do not change the unstable nature of discrete prompts.

To reduce the instability of discrete prompts, we propose a novel method P-Tuning that em- ploys trainable continuous prompt embeddings in concatenation with discrete prompts. Specifically, given a discrete prompt as the input, P-Tuning con- catenates continuous prompt embeddings with the discrete prompt tokens and feeds them as the input to the language model. The continuous prompts are updated by backpropagation to optimize the task objective. The intuition is that continuous prompts incorporate a certain degree of learnability into the input, which may learn to offset the effects of mi-

that the task could be reformulated as filling in the blanks of the input text. For example, for the task of predicting a country’s capital (LAMA-TREx P36), a prompt could be “The capital of [INPUT] is [LA- BEL].” With a piece of labeled data “(Britain, Lon- don)”, the reformulated text would be “The capital of Britain is [MASK].”, where “[MASK]" should predict the given label “London”. Both discrete prompts and discrete data are together mapped into input embeddings:

However, as is discussed in Section 2.1, such discrete prompts tend to be extremely unstable and might not be optimal with back-propagation. Therefore, we propose P-Tuning that uses contin- uous prompt embeddings to improve and stabilize prompting. Let [Pi] be the ith continuous prompt embedding. The prompt template for P-Tuning is as follows:

Table 2: Task settings and summary of results in our experiments. P-tuning shows improvement over base- lines on all task settings, and can stabilize performance on LAMA and Few SG. For Full SG, the gap between discrete prompts is not large and training is stable even without P-Tuning. (Full SG: fully-supervised learn- ing on SuperGLUE; Few SG: few-shot SuperGLUE; Improved: overall performance improved; Stabilized: training stabilized by minimizing difference between discrete prompts).

Datasets and vocabulary. LAMA enforces all answers in single-token format. We first adopt the original LAMA-TREx dataset, consisting of 41 Wikidata relations and altogether 34,039 test- ing triples (namely LAMA-34k, which covers all BERT vocabularies). Since different pretrained models share distinct vocabularies, to allow direct comparison, we follow previous work (Shin et al., 2020) to adopt a subset that covers the intersection of GPT’s and BERT’s vocabularies. This is caled

Comparison methods. We experiment with P- tuning on both unidirectional and bidirectional pretrained models, i.e., GPT and BERT. We include four variants BERT-Base, BERT-Large, GPT2-Base, and GPT-medium. For each model, we compare standard classification finetuning, PET (Schick and Schütze, 2020) (a typical fine- tuning method based on manual discrete prompts) and our P-tuning.

Configuration.  We use the same metrics as in (Wang et al., 2019b). For fully-supervised learn- ing, we use a large training set to finetune pre- trained models and use a development set for hyper- parameter and model selection. Specifically, the AdamW optimizer with a linearly decayed learn- ing rate is used for training. We use a learning rate of {1e − 5, 2e − 5, 3e − 5}, a batch size of

Baseline and Hyper-parameter. In few-shot learn- ing, we again compare P-tuning with PET (Schick and Schütze, 2020), which was shown to out- perform GPT-3 on some of the tasks. Similar to (Schick and Schütze, 2020), we use ALBERT- xxLarge as the base model. For hyper-parameters that are shared by PET and P-tuning (e.g., learn-

Number of Prompt Tokens We also study the in- fluence of the number of prompt tokens and show the results in Table 7. By comparing #3, #6, #7, and #8, we can conclude that the number of prompt tokens has a great impact on the few-shot perfor- mance. However, it is not that a larger number of prompt tokens would always be better. We conjec- ture that it could be that due to the limited training data, it becomes difficult to learn the parameters when excessively increasing the number of contin- uous prompt tokens. In practice, it is suggested to search for the best number of prompt tokens through model selection.

2021; Zhao et al., 2021b) as a way of prompting to transfer knowledge from pretraining to downstream tasks. Schick and Schütze (2020) proposed to use cloze patterns, which removes the constraint that the masked token is the last token of the sentence. This further minimizes the gap between pretrain- ing and downstream tasks. To improve prompting for NLU, recent works have proposed methods to automatically search for high-performing prompts by mining the training corpus (Jiang et al., 2020b), gradient-based search (Shin et al., 2020), or using pretrained generative models (Gao et al., 2020). Our approach is different from these prior works in that we resort to using continuous prompt em- beddings, which are found to be complementary to discrete prompts in our experiments.

Recently, some concurrent works also proposed the use of continuous prompts. Prefix-tuning (Li and Liang, 2021) adds continuous prompts at the beginning of the sequence for each layer. In con- trast to our work, prefix-tuning targets natural lan- guage generation tasks.

In the area of NLU, a few concurrent methods were proposed based on continuous prompts, fo- cusing on improving knowledge probing (Qin and Eisner, 2021; Zhong et al., 2021). Lester et al. (2021) showed that with large pretrained models, only tuning continuous prompts with a frozen lan-

In this paper, we present a method P-Tuning that uses continuous prompts in concatenation with dis- crete prompts. P-Tuning improves performance and stabilizes training for pretrained language model adaptation. P-Tuning is effective with both tuned and frozen language models under both the few-shot and fully-supervised setings.

Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading com- prehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 252–262.

