We propose in this paper an object-based method for on-the-fly extraction of key frames summarizing the salient visual content of videos. This method is based on spatial segmentation of each frame in order to detect the important events. Thus, key frame detection is facing a much more semantic criterion so that each key frame presents an important event such as the appearance and the disappearance of relevant objects. Realized experiments on challenging videos demonstrate the efficiency of the proposed method, which is able to catch the semantic content of a video shot, while preventing redundancy of the extracted key frames and maintaining minimum requirements in terms of memory space.

Nowadays users are faced with an ever-growing amount of videos and efficient tools are thus more and more needed for videos archiving, indexing and retrieval. Video summarization is an important research topic which aims to create automatically a compact and representative summary of video content in terms of still images. Most of existing video summarization methods are content-based, since keyword-based ones pose severe problems of subjectivity and feasibility. Moreover, in order to overcome the “semantic gap”, between the description of a video in terms of low-level features and its semantic content, recent methods combine low- level criteria with the semantic notion of objects. There are two classes of video summarization methods based on objects. For the first group, the concept of object is used to extract the most representative frames (key frames) which represent the salient content and the second group includes methods providing background- foreground segmentation [3]. We focus our attention here on video summarization with a minimum amount of data by extracting relevant key-frames. However, key frame extraction has strong limitations in the case of long

In this paper we combine object segmentation with low-level features in order to propose a higher level of description in terms of semantic primitives. The visual content of a shot is summarized on-the-fly into key frames, such that each one represents a new event. The first frame of the shot is automatically selected as key frame. Then, each received frame is segmented into salient objects, and a position-based criterion is combined with a shape-based one to reject irrelevant objects. Next, many-to-many correspondence between objects of the current frame and those extracted in previous key frames allows to decide if the current frame corresponds to a new event. The main contribution of the proposed method is the summarization on-the-fly of a shot while extracting key frames illustrating relevant events. Many tests on standard videos showed that the proposed method is able to preserve the overall content of a shot with minimum data, even when the camera returns to parts of the scene already visited before. Next section describes the proposed method and experimental results are presented in section 3 to prove objectively the effectiveness of the method using standard metrics.

The proposed method for key frames extraction is mainly based on shot boundary detection and object- based event detection. In fact, after partitioning the input video into shots [2], key frames are selected on-the- fly from each shot while looking for important events corresponding to the appearance and the disappearance of significant objects. For that purpose, the first frame F1 in each input shot is automatically considered as key frame KF1 and is also segmented into salient objects using a fuzzy coarse region segmentation technique [1].

Fig. 1. Object extraction. The first image is the original frame and the following ones are the extracted objects: only the 2nd region is considered as relevant, since the 3rd and 4th ones (resp. 1st) are excluded by the position-based criterion (resp. the shape-based criterion).

correspondence between the correspondent frames is formulated as a many-to-many linear assignment problem between t 1. To resolve this problem, we applied the shortest augmenting path algorithm while looking to find the association between the objects that maximize the amount of the corresponding similarities. This many-to-many assignment permits to handle correctly complex interactions and occlusions between objects, without considering these situations as new events. The many-to-many correspondence between objects allows the comparison of the objects at different levels of granularity what overcomes the over- and under- segmentation effects. If more than one key frame is already extracted (Card()>1), a received frame is assumed to represent a new key frame only if it cannot be associated with all key frames already extracted up to that instant. Indeed, given the set ={KF1,…,KFj} of the already extracted key frames, the frame Ft is assumed to represent a key frame (   Ft) only if it represents a new event comparatively to KFj, to KFj-1, . . .and to

KF1, while giving priority to recent key frames (initially, ref=j=Card()). Thus, Ft is considered as key frame only if some objects are totally added to, or removed from, the scene in this frame (2). In addition to the implicit integration of the temporal behavior with the visual appearance of salient objects, the comparison of each frame with the already selected key frames avoids the consideration of the temporally appearance or disappearance of objects as new events. In particular, the given priority to recent key frames allows the detection of the repetitive events without testing a huge set of frames. Besides, the many-to-many assignment avoids the consideration of occlusion effects as new events, what minimizes the redundancy of the final key frames. Once the decision is performed for Ft, the same object-based event detection procedure is applied on- the-fly for the next received frame Ft+1 and so on until the end of the shot. We note that from the second shot, each frame Ft, including the first one of the shot, must be compared not only to the key frames already extracted from this shot but also to those selected within all precedent shots.

We presented an efficient object-based method for key frame extraction while maintaining convenient memory requirements, even under loop-closure situations. This method is mainly based on the detection of significant events while analyzing the spatio-temporal behavior and visual appearance of salient objects. The detection on-the-fly of the key frames allows to integrate implicitly the temporal content within the input shot without having to process the whole shot. This allows a properly capturing of the underlying dynamics of the input frames. To the best of our knowledge, not much attention was paid for the realization of this task on-the- fly. Moreover, the suggested method avoids the complexity of existing methods based on clustering or optimization strategies. It captures efficiently the underlying dynamics of frames without requiring a priori knowledge of the number of frames representing each shot. The preliminary recorded results and an objective comparative study with many existing key frame extraction methods have shown the efficiency of our unsupervised content-based method, in terms of redundancy, compression rate and recall/precision imetrics. As perspectives, we propose to use a visual dictionary, which can be formed based on low-level attributes of the relevant objects of interest in all available frames, in order to provide a model vector that describes each frame based on the types of objects it contains.

