Traditional ML methods such as logistic regressions and naïve Bayes Algorithms succeed in simple classification tasks, contingent on pre-defined data representations as inputs, but demonstrate declining performance when features are unknown [9]. Representation learning constitutes an ML method used to automate representation mapping and feature detection, utilising encoder-decoder (AutoEncoder) functions to convert inputs into new representations [11]. However, emulating the decision-making process of the human brain requires integrating visible and non-visible features, while simultaneously ranking their impor- tance, automatically overlooking irrelevant variables [9]. Defining such a representation is exceedingly complex, thus evoking the establishment of DL, a type of ML characterised by hierarchical nested layers, with multiple interconnected representations aiming to map abstract and complex concepts [9,12].

The classical perceptron, proposed by Rosenblatt in 1958, initialises the concept of numerical weights, scalar values attached to each feature reflecting their importance [13]. This has since become quintessential to DL architectures, creating the foundation of the multilayer perceptron (MLP), the most basic form of Neural Networks (NNs) [13,14]. MLPs consist of an input and output layer connected by a variety of hidden layers, with each layer composed of multiple neurons (Fig. 3). The input layer is a vector of predictable values, with the number of neurons equivalent to the number of predictable values [15]. This layer stan- dardises the range of the vector values and distributes them across the neurons within the hidden layers [15]. Additionally, a bias is projected onto each hidden layer, this is a constant that is added to the product of the input and weight, aiming to balance the results [15]. The hidden layer multiplies the projected inputs by randomly initialised weights, the resultant sum is passed through an activation function contributing to the calculation of an output [15,16]. The function of the hidden layers is influenced by the learning algorithm, where the model must deter- mine how the layers can produce the most accurate representation of the objective function [9]. Hence, network behaviour is not pre-specified by the training data, unlike in traditional ML methods [9]. The dimen- sionality of the hidden layer influences model “width,” while the

the number of hidden layers within a model, one layer is sufficient un- less the available data has discontinuities [17]. However, determining the number of neurons within a hidden layer requires striking a metic- ulous balance, where too few neurons make it impossible to model complex relationships, while too many neurons put the model at risk of overfitting [15]. Overfitting is a phenomenon that occurs when the model is too complex for its data, giving disproportionate importance to noisy, insignificant data, generating very accurate results on training data, but performing poorly on unseen test sets (low generalisation) [15]. The values obtained from each hidden layer are multiplied by weights, and summed, producing a vector that passes through a trans- formation function, generating output values [15]. Subsequently, output values are either utilised for back-propagation during model training, or for decision-making during testing [16]. When training an MLP the primary objective is to establish a set of weights that when multiplied produce output values closest to the target value [15].

In the context of image segmentation, the image would be the input, with each pixel representing a feature (Fig. 4a). The principal outcome is to build a NN that can accurately predict the class of each pixel using examples from both the training and test set [9]. Hence, a balance be- tween under-fitting and over-fitting must be struck, equating the

actual value) [18]. Minimising the loss function is achieved using opti- misation algorithms, teaching models how to adjust their parameters (model weight and capacity), aiming to reach a point of convergence [18]. Back-propagation is the process by which weights are individually updated to reduce the loss function following every iteration; the gradient of the loss function of each weight is computed and adjusted accordingly [19]. Hyperparameters encompass manually adjustable

architectures used in cardiac image segmentation, advanced building blocks that can be applied to enhance results, and commonly employed loss functions. Then, we describe the methodology of the literature re- view and present a simplified version of the results. Finally, we sum- marise the top performing NN architectures across various cardiac segmentation tasks, delineate key challenges currently encountered by state-of-the-art segmentation models, and suggest areas of future investigation.

Convolutional layers, the defining characteristic of a CNN, consists of an input, a second argument (also referred to as the kernel), and an output (feature map) [9]. The input and kernel are multidimensional arrays of data and parameters respectively [9]. The convolutional kernel is classically followed by a normalisation layer, and a non-linear acti- vation function, such as a rectified linear activation function (ReLu), releasing an output corresponding to the input (Fig. 4c) [9]. All outputs are decimated through pooling layers, usually down-sampling by a factor of two, aiming to optimise efficiency, accuracy, and general- isability by excluding redundant features [20]. Max-pooling operations determine the highest value within a section of a feature map and use this to create a down-sampled feature map (Fig. 4d). Consequently, the output becomes invariant to minimal changes in the input. This is key in certain image segmentation tasks where the presence of a feature is

Fig. 4. (a) Basic feedforward neural network using a CMR image as input; (b) U-Net model designed for CMR segmentation, comprised of contracting and expansive pathways using max-pooling and up-sampling to generate pixel-wise predictions; (c) Graphical representation of the ReLu activation function graph; (d) 2 × 2 Max- Pooling operation.

more important than its location, significantly increasing computational efficiency [9]. For example, when segmenting cardiac images, at times it may be important to merely recognise the location of the major heart chambers, however, it might not be necessary to view these structures with optimal pixels [9]. Furthermore, pooling is indispensable to image processing tasks that deal with inputs of disparate sizing [9]. Next, the fully connected layer establishes the features most vital to successful prediction, and thus decreases the dimensionality of features from the preceding layer [20]. Finally, a fix-sized vector is produced as the model output [20].

There are three key benefits that arise from convolutional layers, namely sparse interactions, parameter sharing and equivariant repre- sentations [9]. Firstly, sparse interactions are a feature of CNNs that reduce the kernel to a size smaller than the input, differing from classical NN architectures as each output unit does not need to interact with each input [9]. This poses an apparent advantage in image segmentation, as instead of storing the millions of pixels that may be associated with an input image, only meaningful features are processed in kernels composed of tens or hundreds of pixels [9]. As a result, processing ef- ficiency is significantly increased, while running time is reduced [9]. Additionally, in deep CNNs, reducing kernel size allows the receptive field to increase by augmenting the number of convolutional layers, this causes an indirect interaction with a greater proportion of inputs enabling complex, multifactorial correlations to be captured [9,20]. Secondly, parameter sharing (or tied weights) causes the value of one

position, allowing only one set of parameters to be learnt for all locations [9]. Equivariance is a product of parameter sharing, where a change to the input produces the same transformation to the output [9]. This is advantageous to image processing tasks as the first convolutional layer typically detects image edges [9]. Since, images typically share borders, equivariance enables effective parameter sharing [9].

however, despite this, the model is required to train on each patch individually [20]. Furthermore, localisation accuracy is spared at the cost of maintaining context. Larger patches oblige a greater number of pooling layers, compromising the localisation accuracy but preserving context, while smaller patches have a greater localisation accuracy but lack context, as they cover smaller image areas [21]. Thus, traditional CNNs are typically utilised for object localisation in cardiac image seg- mentation. More recently, modifications to traditional CNNs have been proposed that enable complete pixel-wise segmentation [20].

FCN encompass a ground-breaking variant of CNNs, designed to undertake pixel-to-pixel prediction tasks without systematic inefficiency [22]. Long et al. developed the FCN for semantic segmentation, aiming to overcome shortcomings associated with traditional CNNs [88]. FCNs do not contain any “dense” or fully connected layers, thus, exclusively

consisting of convolutional layers [22,23]. Every data point with an FCN consists of three dimensions, h x w x d, where h and w represent height and width (spatial dimensions), while d represents the feature/channel dimension [22]. FCN input images can have variable sizes that will be encoded into feature representations, and then decoded using spatial information via a sequence of up-sampling (deconvolution) and con- volutional layers [20]. Up-sampling increases the magnitude of minority classes by adding duplicate data points to that class, enabling a balanced data set [89]. In contrast to the traditional CNN patch-based approach, FCNs can train and make predictions on entire images [20]. Nonetheless, FCNs encoder-decoder approach causes the elimination of notable fea- tures and contextual information during the pooling layers [20]. Hence, updates to traditional FCNs have been proposed, aiming to transmit features between encoding and decoding layers, preserving spatial context, and improving segmentation accuracy [20]. most widely employed architecture for biomedical image segmentation is the U-Net, a variant of the FCN [20]. This model’s U-shaped architecture in-

pathway and an expansive (decoder) pathway (Fig. 4b) [21]. The con- tracting path is like traditional CNNs, composed of two convolution layers with associated ReLu and max pooling operations, where the number of features is doubled at each down-sampling step [21]. The expansive path uses up-sampling at each step followed by a convolution, halving the number of features [21]. All feature maps are then cropped due to the loss of border pixels at each convolution [21]. Concatenating skip connections are present between contracting and expansive path-

tation precision [20]. The cropped feature maps from the contracting pathway are thus connected to the expansive pathway, and projected into two convolutions, each proceeded by a ReLu function [21]. The final layer is marked by a single convolution that maps the output [21].

2D and 3D U-Net models are powerful variants of traditional U-Nets, utilising similar architectures with alterations to the kernel size and convolutional layers to reflect the dimensionality of the input [24]. The fundamental difference between both data types is that 2D models train and make predictions based on a single slice, whereas 3D models can make inter-slice predictions [24]. While this enables more complex and insightful segmentation, 3D models have an increased cost of computing, requiring patch-based processing [24]. In general, both 2D and 3D U-Nets are effective at biomedical image segmentation, with their accuracy varying with task complexity [24].

V-Net is a deep learning methodology used for semantic segmenta- tion, designed to overcome the deep and wide nature of CNN layers [25]. This architecture employs a reversible mechanism and asymmetrical convolutions maintaining image size and quality [25]. As a result, V-Net can train high-quality images on a single GPU [25]. This model compiles Contextual Pyramid Pooling modules and versatile modules [25].

built secondary to the dependencies, available for prompt deployment and application without increasing computational load [26]. Three distinctive U-Net models are then automatically generated, a 2D model, 3D model, and a 3D cascaded U-Net, and the best-performing is selected [26]. Hence, nnU-Net proposes an end-to-end automated segmentation methodology with state-of-the-art performance standards.

Transformer encompasses a DL model that was initially developed for natural language processing but has recently been introduced to the image processing domain [27]. Transformers alone do not employ CNN-based architectures; however, modifications have been performed leading to the creation of TransUNet [27]. This model utilises a Vision Transformer (VIT) as the encoder and a CNN as the decoder [27]. The VIT deploys the transformer architecture onto fix-sized patches present within the image, the linear embeddings provided by these patches are then input into a Transformer model [28]. Thus, TransUNet overcomes the lack of spatial context produced by only VIT models [27].

module [29]. Resultantly, Swin Transformer limits computation to the non-overlapping windows, improving efficiency while enabling pro- cessing at various scales and image sizes [29]. This model offers a general backbone for image classification tasks, differentiating from other vision Transformers’ low resolution [29].

[17]. Although models such as U-Net have integrated de-convolutional layers (up-sampling layers) to maintain spatial resolution, DeepLab utilises an alternative mechanism entitled atrous convolution [17]. Atrous convolution is analogous to down-sampling layers in CNN models, however, it broadens the receptive field while preserving feature map spatial dimension [17]. DeepLab employs Atrous Spatial Pyramid Pooling (ASPP) to aid in handling multi-scale images, con- trolling feature response density to obtain multi-scale context [17]. Resultantly, while FCNs and U-Net are more commonly used in biomedical image segmentation, DeepLab provides a deeper model ar- chitecture with a greater number of features, potentially better suited to complex segmentation tasks [31].

GAN proposed by Goodfellow et al. encompass a variation of generative models specialised for the synthesis of images from real data [20,32]. GANs are composed of a generator and discriminator NN con- nected through back-propagation [20,33]. The generator network cre- ates false images, and the discriminator is tasked at differentiating between fabricated and real images [20]. The discriminator network’s

In the context of image segmentation, replacing the generator network with a segmentation network enables the GAN to differentiate between predicted segmentation tasks and the ground truth [20]. However, this approach is associated with difficulties in training, and maintaining segmentation quality [34]. Resultantly, GAN variants have been developed, with one of the most successful being the segmentation adversarial network (SegAN), using a fully convolutional GAN for pixel-to-pixel segmentation [34].

A recent innovation that combines Swin Transformers with GANs is the Swin Transformer-based GAN for multi-modal medical image translation entitled MMTrans, coined by Yan et al. in 2022 [35]. MMTrans is composed of a generator based on the SwinIR architecture (Swin Transformer that can predict deformable vector fields), skilled at generating images within the same category of the modality of choice [35]. After the generator, there is a registration network that corrects any minor mismatches between source and target domain images [35]. Finally, MMTrans contains a discriminator, built using a CNN that dis- cerns whether the target image is most like the generator or the real image [35].

used RNN variants [38]. Within cardiac image segmentation, RNNs are beneficial in imaging series such as cine CMR and Echo sequences, establishing connections between current and previous outputs. In addition, they are often combined with FCNs to optimise inter-slice knowledge and improve segmentation [20].

ASPP is designed to capture wide image context in segmentation tasks through convolutional feature layers with filters that have various sampling rates and fields-of-view (Fig. 5) [42]. Residual connections are skip-connections that allow gradient flow directly through the network [20]. Dense connections concatenate the feature map of the current layer with outputs from the previous layer [20].

Attention Gates (AGs) offer a solution to the computationally expensive nature of traditional CNN models, aiming to use model pa- rameters and intermediate feature maps more efficiently [43]. AGs enable automatic structural focus with minimal supervision, delineating the features most relevant to a specific task, and repressing less relevant features and regions [43]. Resultantly, AGs eradicate the need for external structural localisation without compromising prediction accu- racy, simultaneously reducing the computational overload associated with CNNs [43]. Multiplicative and additive attention are the two existing types of AGs that can be embedded into any CNN architecture [43].

Deep Supervision Modules (DSV) generate multiple segmentation maps at all levels of resolution, transposed to build secondary segmen- tation maps [44]. This is accomplished by up-sampling the element-wise sum of adjacent resolution segmentation maps until the highest resolu- tion is reached [44]. Resultantly, DSV improves the number of features that can be learnt and optimises model convergence [45].

most popular loss function in image classification and segmentation [47]. Cross-entropy can be used to summarise probability errors in pixel-wise segmentation [20]. Mean-dice loss is another widely employed segmentation-specific function, built as an adaptation to the dice coefficient, a value that calculates the similarity between two im- ages [47]. Weighted cross-entropy and weighted dice-loss, form two variations of the loss functions, using weighted loss terms to overcome class imbalance, and include rare classes or objects [20]. Unified focal loss generalises both dice loss and cross-entropy loss to tackle class imbalance within data sets [48]. This function allows a single hyper- parameter to be fine-tuned as opposed to the six hyperparameters associated with traditional focal loss functions, making it more efficient [48].

aiming to identify studies meeting the pre-determined eligibility criteria (see Fig. 6 for search queries). As outlined in Fig. 4, a total of 277 papers resulted from the initial search, upon automatic and manual exclusion, 64 eligible studies were identified and included in the final data set.

Tables 2, 3, 4, 5 and 6 present the results of CMR segmentation of the ventricles and myocardium, atria, pericardial adipose tissue, and the whole heart, respectively. Tables 7, 8, 9, 10 and 11, delineate segmen- tation results for CT-based segmentation of the ventricles and myocar- dium, atria, adipose tissue, aorta and coronary arteries, and whole heart, respectively. Tables 12, 13, and 14 demonstrate segmentation results from Echo, X-Ray Angiography, and multi-modal inputs, respectively. Studies are presented from highest to lowest mean DSC for each seg- mentation target, highlighted in the “Ranking” column.

U-Net was by far the most popular backbone architecture (n = 44), followed by CNN (n = 9) and FCN (n = 6), the frequencies of backbone architectures are expressed in Fig. 7. Of the four top-performing models presented in the subsequent section, three utilise a U-Net backbone, while one employs a nnU-Net foundation, representing a U-Net variant.

The Automated Cardiac Diagnosis Challenge (ACDC) comprises CMR images obtained from 150 patients at the University Hospital of Dijon obtained over a six-year period from two MRI scanners with magnetic strengths of 1.5T and 3.0T [111]. Patients were divided into five equally

This dataset was used in the MICCAI 2017 Conference in a challenge terminating in 2022. The training dataset was comprised of 100 patients (66.67%), 20 patients from each group, while testing data consisted of 50 patients (33.33%), 10 patients from each group. Ground-truth seg- ments were developed by two experienced cardiologists. The segmen- tation targets were LV, RV and Myo. Results and architecture details of the three top-performing segmentation models from the ACDC challenge

The Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Image Segmentation Challenge (M&Ms) encompasses a data set used in the MICCAI 2020 Conference [115]. This dataset is composed of 375 CMR datasets obtained from four MRI scanners across six hospitals in three countries. Thus, when compared to previous challenges such as ACDC, M&Ms provides a heterogeneous compilation of data aiming to reflect the high degree of variability between images obtained from different vendors and locations. Patients included demonstrated diverse cardiac pathologies including hypertrophic cardiomyopathy, dilated cardio- myopathy, coronary heart disease, abnormal RV, myocarditis, ischaemic cardiomyopathy, and healthy volunteer.

The dataset was divided into 175 training cases (46.67%), 40 vali- dation cases (10.67%), and 160 testing cases (42.66%). The top-three performing models within this dataset all employed a nnU-Net back- bone architecture in addition to various data augmentation techniques (see Table 16). Utilising data augmentation methods such as parameter variation and intensity transformation helped build new training im- ages, ultimately improving model generalisability. However, domain adaptation, typically combined with U-Net backbones, did not yield results superior to nnU-Net models without domain adaptation.

achieving a mean dice score of 96.70% for LA and RA segmentation [90]. The outlined approach suggests an improved U-Net, characterised by U-shaped upper and lower sampling layers, built using residual the- ory (ResNet) as the selected encoder-decoder. The suggested residual module aims to limit model depth by delaying gradient convergence during network propagation [90]. Furthermore, sampling modules aid

in maintaining accuracy when increasing feature complexity, by con- necting the prior U-Net decoder with the subsequent U-Net encoder [90]. Additionally, this module builds numerous paths for data trans- mission, utilising features of FCNs in the U-Net paths [90]. In addition, deep deconvolutions are incorporated into training and testing stages to provide a supervised learning method [90]. Therefore, the combination of augmented complexity and various connected pathways creates an

image region encompassing the whole heart. This is coupled with multi-atlas segmentation and label generation to create an annotated training dataset. Following this, corrective segmentation alters the la- bels generated in the preceding step to differentiate cardiac tissue from other intra-thoracic structures [97]. Reverse ranking is used to assess the quality of the computer-generated labels obtained through CMACs, and the selected labelled images are inputted into the U-Net segmentation model. Therefore, the model is trained on computer-generated exam- ples, but is validated using manually annotated, real labels [97]. DeepHeartCT can segment up to 12 structures simultaneously, utilising a dice loss function and ReLu activation function after each layer [97]. Thus, DeepHeartCT successfully overcomes obstacles to adequately an- notated and high-quality training data, without compromising training time or segmentation accuracy. nnU-Net is a fully automated aorta, aortic valve and LV outflow tract segmentation model achieving mean

DSCs of 97% using cardiac CT images [103]. This study is the first to employ nnU-Net for cardiac image segmentation, providing the pivotal benefit of automatic input data pre-processing, and parameter and hyperparameter fine-tuning within a U-Net structure [103]. Thus, nnU-Net can augment training data through image cropping, resam- pling, and data normalisation to reduce artefact [103]. In addition, No-New-Net provides the additional benefit of easy conversion to ster- eolithography files required for 3D evaluations prior to trans-catheter aortic valve interventions, giving it a direct clinical advantage [103].

an obstacle when building deep NNs in cardiac image segmentation. As a result, models are prone to over-fitting, and efficient classification re- quires the deployment of further advanced techniques. For example, two regularisation techniques, weight regularisation and dropout, are often used to optimise learning. The former involves adding weight penalties to the loss function based on the relevance of the input, while the latter

As an attempt to increase the magnitude of training data, strategies such as cross-modality image segmentation have been proposed. This method uses feature adaptation to alter an input image from an unde- sired imaging modality to the modality of choice, indicating that CT images could be used to train a CMR segmentation model [116]. Multi-atlas-based segmentation proposes an alternative approach where an anatomical atlas library containing pre-segmented cardiac structures is transformed into target images for the segmentation model [117].

The nature of medical imaging is dynamic and inextricably influ- enced by involuntary organ motion, patient movement or breathing, and challenges with image acquisition [84]. Resultantly, acquired images suffer from a variety of artefacts that can hinder segmentation, causing misleading and inaccurate results [84]. Image de-noising is a domain of medical image processing separate to segmentation, however, combining both approaches in one framework can help overcome challenges in image quality. Oksuz et al. suggests an end-to-end pipeline for artefact detection, correction, and segmentation [84]. This method reconstructs high quality CMRs (Cardia Magnetic Resonance) using a joint loss function, then leverages a data consistency term (k-space line detection network) to reconstruct under-sampled images [84]. Hence, proposing a framework to ensure high quality data despite inevitable motion artefacts [84].

accuracies approaching 100% [118]. Muller et al. suggests that evalu- ation bias represents a severe obstacle to widespread clinical applica- bility and proposes a guideline to evaluate research reliability in image segmentation [118]. To safely implement image segmentation models within clinical settings, doubts regarding evaluation bias and true model accuracy must be addressed.

tation have only reached mean dice scores of 87.1% [120]. While this model provides a key advantage as it can classify motion artefact and perform image segmentation in one step, increasing training data and employing data augmentation techniques will improve segmentation accuracy [120].

To overcome limited access to large, annotated datasets weakly su- pervised, or unsupervised approaches to training are necessitated. Despite the proposition of attempts to reduce supervision such as few- shot learning and weakly-supervised learning, these methods have several disadvantages when implemented [121]. Few-shot learning, a semi-supervised data augmentation technique, is prone to noisy results that place too great emphasis on the available labelled data points [121]. In addition, weak supervision is not effective in leveraging the full potential of high-quality images [121]. Therefore, novel weakly supervised or unsupervised training approaches are required. Hooper et al. suggests a framework combining few-shot learning and weak su- pervision to overcome their respective limitations, however further modifications can fortify the performance of weakly supervised models

[121]. For example, introducing self-supervised pre-training modules able to gain global and local insight through domain and problem-specific cues using contrasting learning strategies [122]. Furthermore, building networks that can automate ROI and extreme point selection can help reduce the supervision-level required for suc- cessful model deployment [123].

As innovation within the biomedical imaging domain progresses, user-friendly, end-to-end image processing frameworks are becoming increasingly necessary. Medial Open Network for AI (MONAI) is an open source, easily operated, end-to-end biomedical imaging platform, enabling image labelling, transformation, segmentation, and model deployment [124]. Thus, MONAI aims to integrate state-of-the-art findings in biomedical imaging DL solutions into a single platform, driving scientific progression in the field [124].

Over the last decade there has been a steep rise within the field of cardiac ML. Initiatives such as euCanSHare (http://www.eucanshare. eu/) have led to the establishment of international, multi-cohort car- diovascular research platforms, driving innovation, and increasing the prospect of clinical application. Cardiac image segmentation has been fundamental to this, aiming to reach a highly personalised, patient- centred, accurate and time-efficient approach to the diagnosis and management of cardiac pathologies. Image segmentation forms a field of biomedical ML with high clinical acceptance, as it reduces clinician workload without significantly intervening in their decision-making process [5].

Bui V, Hsu LY, Chang LC, Sun AY, Tran L, Shanbhag SM, et al. DeepHeartCT: a fully automatic artificial intelligence hybrid framework based on convolutional neural network and multi-atlas segmentation for multi-structure cardiac computed tomography angiography image segmentation. Front Artif Intell 2022; 5:1059007.

