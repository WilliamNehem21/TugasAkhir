While modernized simulation frameworks superficially appear simi- lar, upon closer inspection, they are complimentary in nature. To make this clearer, in Fig. 3, we compare modern simulation frameworks, i.e., those that support pre-trained DNN conversion and TF/PyTorch integration in more detail, using radar charts. As it is shown, there is not a large overlap amongst the simulation frameworks which have been compared: RAPIDNN, PUMA, DL-RSIM, Tiny but Accurate, Ultra-

simulate the training routine of the VGG-8 [57] network architecture, and the inference routine of the GoogLeNet [58] network architecture. Both training and inference routines were evaluated using the CIFAR-10 dataset. Two separate network architectures were used for evaluation, as larger and more complex networks could not be reliably trained using existing simulation frameworks with Compute Unified Device Architecture (CUDA) support when utilizing a single GPU, even with 32 GB of Video Random-Access Memory (VRAM). Moreover, not all simulation frameworks supported convolutional layers with non-zero groups (connections between inputs and outputs), meaning that many ResNet-based architectures could not be implemented.

was modeled by sampling ùëÖON and ùëÖOFF from normal distributions with differential weight mapping scheme, and device-to-device variability mean values of 10 kŒ© and 100 kŒ©, and standard deviation values of 1000 and 10,000, respectively, i.e., ùëÖOÃÑ N = 10 kŒ©, and ùëÖOÃÑFF = 100 kŒ©.

Devices were assumed to have a finite number (6) of conductance states, and ADCs were assumed to operate at a 6-bit resolution. For inference routine simulations, 10 runs were conducted, and mean and standard deviation values were reported across all runs. For training routine simulations, mean and standard deviation values were reported across all training epochs. All codes used to perform comparisons are made publicly-accessible,1 and can be modified to perform compar- isons using different hardware technologies, network architectures, and hyper-parameters.

All simulations were conducted using a High Performance Comput- ing (HPC) cluster with the following run-time hardware configuration set using the Simple Linux Utility for Resource Management (SLURM) workload manger: 1 node and 8 CPU cores (Intel Xeon 6132 series CPU sockets), 100 GB DDR4 3200 MHz Random-Access Memory (RAM), and one PCI-E 32 GB Volta V100 GPU. torch.cuda.Event and timer.time() were used to determine the execution time of various

In addition to simulating training and inference routines using MemTorch, DNN_NeuroSim_V2.1, and the IBM Analog Hardware Ac- celeration Kit, baseline training and inference routines were simulated using the native PyTorch ML library for comparison. For all base- line implementations, the exact same hyper-parameters were used. torch.cuda.amp was used to quantize all network parameters to 16-bits to improve performance.

In Fig. 4, the performance of training routines for the VGG-8 net- work architecture using the CIFAR-10 dataset are compared. For Neu- roSim and the IBM Analog Hardware Acceleration Kit, default non- linear weight update parameters were used. All networks were trained for 256 epochs with a batch size of 128 using Stochastic Gradient De- scent (SGD) with momentum and cross-entropy loss. An initial learning rate of 0.1 was used with fixed momentum value of 0.9. Optimizers that support adaptive learning rates were not used, as these were not supported by DNN_NeuroSim_V2.1. Instead, during training, the learning rate was decayed by one order of magnitude at epochs 100, 200, and 250 (these schedules were determined empirically), to prevent stagnation.

The functionality of each simulation framework has previously been investigated and validated [51,55,56]. Consequently, training and test set losses and accuracies were not reported or compared, as they have no bearing on the performance of each simulation framework. As can be seen in Fig. 4, the IBM Analog Hardware Acceleration Kit consumed the most RAM and GPU VRAM. While DNN_NeuroSim_V2.1 consumed more RAM than the baseline implementation, interestingly, it con- sumed notability less VRAM. This can be largely attributed to the large number of operations being performed on CPU and/or sequentially on GPU, rather than in parallel, and can be used to explain the relatively large elapsed time per training epoch reported by DNN_NeuroSim_V2.1, as depicted in Fig. 4(c).

To quantify the performance trade-off between GPU VRAM usage and training time, Fig. 4(f) was constructed. The baseline training routine clearly exhibits the best performance trade-off. Our findings suggest that DNN_NeuroSim_V2.1 is capable of simulating the training routine of larger and more complex network architectures, however, it does not fully utilize CUDA, and is much slower than other simulation frameworks. In contrast, the IBM Analog Hardware Acceleration Kit

Fig. 5(c), the IBM Analog Hardware Acceleration Kit is capable of simulating inference routines significantly faster than the MemTorch and DNN_NeuroSim_V2.1 simulation frameworks. This is while con- suming more VRAM and approximately the same amount of RAM. We largely attribute this to the fact that the IBM Analog Hardware Acceleration Kit is unable to simulate modular crossbar tiles, which are difficult to parallelize using CUDA. When modular crossbar tiles are not simulated, when sufficiently small WL voltages are used to encode inputs, conventional VMMs can be used to determine output currents when 1T1R crossbars are modeled.

As can be seen in Fig. 5(d), our findings suggest that the IBM Analog Hardware Acceleration Kit is able to utilize VRAM to the greatest extent, however, it is unable to simulate modular crossbar tiles. DNN_NeuroSim_V2.1 is able to simulate inference routines significantly faster than MemTorch, however, it is not as customizable, as it utilizes proprietary weight mapping and data flow schemes, which cannot be easily modified.

It is evident that MDLS and memristive simulation frameworks are becoming increasingly useful and popular. While the reliable, large- scale operation of reconfigurable MDLS is still arguably an open prob- lem [61], modernized simulation frameworks and tools enable re- searchers from a variety of disciplines to rapidly and accurately model the behavior and operation of MDLS without specialized circuit-level SPICE simulation expertise. This is in addition to the ability to work in tandem with existing modernized ML libraries. As these simulation frameworks and the models used to simulate non-ideal circuit and device characteristics mature and grow in popularity, the development cycle and production of innovative device technologies and MDLS architectures will also continue. These new devices and architectures can be conveniently integrated into the existing tools, facilitating their quick large-scale adoption.

modernized simulation frameworks. Furthermore, we provided an out- look/perspective into the future of CAD tools for modeling and sim- ulating MDLS. We demonstrated that modern simulation frameworks are complimentary in nature, and can be used by a variety of users with different requirements to facilitate current research efforts in the domains of IMC and unconventional computing.

Corey Lammie: Conceptualization, Methodology, Software, Valida- tion, Formal analysis, Investigation, Resources, Data curation, Writing ‚Äì original draft, Writing ‚Äì review & editing, Visualization, Funding ac- quisition. Wei Xiang: Writing ‚Äì review & editing, Supervision. Mostafa Rahimi Azghadi: Conceptualization, Methodology, Validation, Writ- ing ‚Äì review & editing, Supervision, Project administration, Funding acquisition.

Kund M, Beitel G, Pinnow C-U, Rohr T, Schumann J, Symanczyk R, Ufert K, Muller G. Conductive bridging ram (cbram): an emerging non-volatile memory technology scalable to sub 20nm. In: IEEE internationalelectron devices meeting, 2005. IEDM technical digest. 2005, p. 754‚Äì7. http://dx.doi.org/10.1109/IEDM.

Xia L, Li B, Tang T, Gu P, Yin X, Huangfu W, Chen P-Y, Yu S, Cao Y, Wang Y, Xie Y, Yang H. MNSIM: Simulation platform for memristor-based neuromorphic computing system. In: 2016 design, automation test in europe conference exhibition (DATE). 2016, p. 469‚Äì74.

Paszke A, Gross S, Massa F, Lerer A, Bradbury J, Chanan G, Killeen T, Lin Z, Gimelshein N, Antiga L, Desmaison A, Kopf A, Yang E, DeVito Z, Raison M, Tejani A, Chilamkurthy S, Steiner B, Fang L, Bai J, Chintala S. PyTorch: An imperative style, high-performance deep learning library. In: Wallach H, Larochelle H, Beygelzimer A, dAlch√© Buc F, Fox E, Garnett R, editors. Advances in neural information processing systems 32. Curran Associates, Inc.; 2019, p. 8024‚Äì35.

Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Man√© D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Vi√©gas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X. TensorFlow: Large-scale machine learning on heterogeneous systems. 2015.

Ankit A, Hajj IE, Chalamalasetti SR, Ndu G, Foltin M, Williams RS, Faraboschi P, Hwu W-mW, Strachan JP, Roy K, Milojicic DS. PUMA: A programmable ultra-efficient memristor-based accelerator for machine learning inference. In: Proceedings of the twenty-fourth international conference on architectural sup- port for programming languages and operating systems, ASPLOS ‚Äô19. New York, NY, USA: Association for Computing Machinery; 2019, p. 715‚Äì31. http://dx.doi. org/10.1145/3297858.3304049.

Lin M-Y, Cheng H-Y, Lin W-T, Yang T-H, Tseng I-C, Yang C-L, Hu H-W, Chang H-S, Li H-P, Chang M-F. DL-RSIM: A simulation framework to enable reliable ReRAM-based accelerators for deep learning. In: Proceedings of the international conference on computer-aided design, ICCAD ‚Äô18. New York, NY, USA: Association for Computing Machinery; 2018, p. 1‚Äì8. http://dx.doi.org/10. 1145/3240765.3240800.

Yuan G, Ma X, Ding C, Lin S, Zhang T, Jalali ZS, Zhao Y, Jiang L, Soundarajan S, Wang Y. An ultra-efficient memristor-based DNN framework with structured weight pruning and quantization using ADMM. In: 2019 IEEE/ACM international symposium on low power electronics and design (ISLPED). 2019, p. 1‚Äì6. http:

Rasch MJ, Moreda D, Gokmen T, Le Gallo M, Carta F, Goldberg C, El Maghraoui K, Sebastian A, Narayanan V. A flexible and fast pytorch toolkit for simulating training and inference on analog crossbar arrays. In: 2021 IEEE 3rd international conference on artificial intelligence circuits and systems (AICAS). 2021, http://dx.doi.org/10.1109/AICAS51828.2021.9458494.

Corey Lammie is currently pursuing a Ph.D. in Computer Engineering at James Cook University (JCU), where he completed his undergraduate degrees in Electrical Engineering (Honors) and Information Technology in 2018. His main research interests include brain-inspired computing, and the simulation and hardware implementation of Spiking Neural Networks (SNNs) and Artificial Neural Networks (ANNs) using ReRAM devices and FPGAs. He has received several awards and fellowships including the intensely competitive 2020‚Äì2021 IBM international Ph.D. Fellowship, a Domestic Prestige Research Training Program Scholarship (the highest paid Ph.D. scholarship in Australia), the 2020 Circuits and Systems (CAS) Society Pre-Doctoral Grant, and the 2017 Engineers Australia CN Barton Medal awarded to the best undergraduate engineering thesis at JCU. Corey has served as a reviewer for several journals and conferences including the IEEE Internet of Things Journal and the IEEE International Symposium on Circuits and Systems (ISCAS).

Wei Xiang is currently Cisco Chair of AI and Internet of Things and Founding Director of Cisco-La Trobe Centre for AI and Internet of Things. He is also Director of Higher Degree Research Program at SmartSat CRC. Prior to joining in La Trobe, he was Foundation Chair and Head of Discipline of Internet of Things Engineering at James Cook University, Cairns, Australia. Due to his instrumental leadership in establishing Australia‚Äôs first accredited Internet of Things Engineering degree program, he was selected into Pearcy Foundation‚Äôs Hall of Fame in October 2018. He is an elected Fellow of the IET in UK and Engineers Australia. He received the TNQ Innovation Award in 2016, and Pearcey Entrepreneurship Award in 2017, and Engineers Australia Cairns Engineer of the Year in 2017. He has been awarded several prestigious fellowship titles, including a Queensland International Fellowship, an Endeavor Research Fellowship, a Smart Futures Fellow, and a JSPS Invitational Fellow. He is the Vice Chair of the IEEE Northern Australia Section.

Mostafa Rahimi Azghadi received the Ph.D. degree in electrical and electronic engineering from The University of Adelaide, Adelaide, SA, Australia, in 2014. From 2012 to 2014, he was a Visiting Ph.D. Student with the Neuromorphic Cognitive System Group, Institute of Neuroinformatics, University and Swiss Federal Institute of Technology (ETH), Z√ºrich, Switzerland. He is currently a Senior Lecturer with the College of Science and Engineering, James Cook University, Townsville, QLD, Australia, where he is researching neuromorphic engineering and brain-inspired architectures and developing custom hardware and software solutions for a variety of engineering applications ranging from medical imaging to precision agriculture. Dr. Azghadi was a recipient of several national and international awards and scholarships, such as the 2020 JCU Award for Excellence in Innovation and Change, the Queensland Young Tall Poppy Science Award in 2017, and the South Australia Science Excellence Awards in 2015. He was a recipient of the Doctoral Research Medal and the Adelaide University Alumni Medal in 2014.

