Abstract In this paper, we propose a general framework for an intelligent recommender system that extends the concept of a knowledge-based recommender system. The intelligent recommender system exploits knowledge, learns, discovers new information, infers preferences and criticisms, among other things. For that, the framework of an intelligent recommender system is defined by the following components: knowledge representation paradigm, learning methods, and reasoning mechanisms. Additionally, it has five knowledge models about the different aspects that we can con- sider during a recommendation: users, items, domain, context and criticisms. The mix of the com- ponents exploits the knowledge, updates it and infers, among other things. In this work, we implement one intelligent recommender system based on this framework, using Fuzzy Cognitive Maps (FCMs). Next, we test the performance of the intelligent recommender system with special- ized criteria linked to the utilization of the knowledge in order to test the versatility and perfor- mance of the framework.

which provides suggestions of items for users [15]. Various techniques for recommendation have been proposed. From the domains such as artificial intelligence, data and semantic mining, information retrieval, approaches of RS have emerged. The RS traditionally have been classified as content-based, collaborative, knowledge-based, and hybrid.

A knowledge-based recommender system only exploits the knowledge naively. We argue that a recommender system has an intelligent behavior if it has the next set of capabilities: knowledge representation, learning capabilities, and reasoning mechanisms. The mix of these capabilities can exploit largely knowledge, update them, and infer them, among other things. Based on these ideas, in this paper we propose a new type of recommender system, called Intelligent Recommender System

(IRS), which is an extension of the knowledge-based RS. The IRS considers learning algorithms, knowledge representation mechanisms, and reasoning motors, among other aspects. In this paper, we define an IRS, and describe its components, and the relationships among them, among other things.

An IRS can use any intelligent technique (fuzzy logic, onto- logical approaches, etc.) for its implementation. Additionally, we give an example of its application using the FCMs. The FCMs have been used in different domains [1–3]. The FCMs are based on the Cognitive Maps (CMs) theory, to model sys- tems based on concepts that describe the main characteristics of the modeled system (variables or states of the system), and the causal relationships between them. FCMs are based on the fuzzy logic theory to define their structure and their inference process from a given data input. FCMs have been applied to diverse field such as supporting group-decision, political analysis [2].

In the following section we present some backgrounds about the RS. Section 3 presents the theoretical bases of our approach and then Section 4 presents the IRS framework. Sec- tion 5 presents details of the description of the knowledge models of the IRS. Section 6 presents the implementation of IRS using FCMs. Lastly, the next sections present a case study, the utilization of IRS based on FCMs, the experiments, and the analysis of the results.

neural network has the ability to act as a regularizer of the item representations to be recommended. In [22] a personalized rec- ommendation system is introduced based on accurate models which capture the user preferences. They propose a picture- based approach: they use a set of travel related pictures selected by a user, and an individual travel profile is deduced. This is accomplished by mapping those pictures onto seven basic factors, which reflect different travel aspects. This model constitutes the basis of their recommendation algorithm. In

[23] a semantic recommendation approach of pedagogical resources is proposed within a learning ecosystem. This approach is based on a voting system, where each member of the ecosystem evaluates the pedagogical resources found in his/her sharing space. In this way, they define a coherent learning ecosystem that promotes collaborative learning, which allows exchanging and sharing knowledge and/or skills. Finally, [15] is a book which present trends, concepts, method- ologies, challenges and applications on RS. This book describes the classical methods, as well as novel approaches such as Context-Aware RS and RS in the social web.

human-RS interaction. The types of information used for the recommendation generation techniques are very differ- ent, for example, the item selected by the user, the descrip- tion of the context of the query. Also, the transaction can include an explicit feedback of the user. Normally it is called criticity such as its rating of an item.

The concept of critiquing is very important in our frame- work. It is based on the idea that users specify their requests as goals not satisfied by the recommended ongoing item [9]. Critiquing-based RS articulates preferences without forcing users to specify concrete values for item properties. The major steps of a critiquing-based RS are [9]:

Item reviewing. In this step the user reviews the recom- mended item, in order to accept the recommendation or select a critique, which restart a new critiquing cycle. If a critique has been chosen, only the items that fulfill the cri- teria defined in the critique are further taken into account (it reduces the candidate item set).

Our IRS extends the ideas behind the classical RS with this concept [5,6], because it tries to understand users, discover their interests, etc., through the creation of knowledge, reason- ing, etc. During this process, we use the notions of ‘‘unit cri- tique” and ‘‘compound critiques”. Unit critique operates

over one specific property of an item. It defines the change requests of a single item property. For example a unit critique in a PC IRS can infer that the user is interested in a PC with more memory than the normally recommended PC; ‘‘more memory” is a critique over the memory feature. There are cri- tiques that operate over multiple properties, called compound critiques. For example a compound critique in our IRS for the PC domain can infer lower price, faster CPU, and more mem- ory. The compound critiques are very important because they reduce the number of critiquing cycles and allow a faster nav- igation into the item space.

The IRS exploits all the knowledge, which is obtained auto- matically (by learning mechanism), and is modeled appropri- ately, in order to be used by reasoning mechanisms to infer how much the user needs the item. If the user’s goals are sat- isfied by the current items. Basically, two aspects must be defined in our IRS. The two aspects are its architecture, and the knowledge to model. The general architecture is shown in Fig. 1.

The main component is the semantic knowledge model, which stores the different types of knowledge that it uses for recommending items. This knowledge must be updated because it requires learning mechanisms. Finally, in order to exploit the knowledge, it uses a reasoning mechanism, which is responsible for recommending items using all the knowledge available. Now, listed below are the different components of architecture:

– Knowledge modeling: the main aspect was to define the paradigm of knowledge representation. There are a lot of paradigms [1,2,4,10,12]: ontologies, fuzzy rules, conceptual maps, etc. The main points to select one are the capabilities of representation of all the knowledge available, and the possibility to define reasoning mechanisms with them. In general, our recommendation paradigm requires different types of knowledge such as the user model, the contextual model, the domain model, the item model that have been recommended, and the rest information about the behavior

Knowledge acquisition: this phase is defined to learn about the current situations, etc. There are a lot of approaches (supervised, unsupervised, etc.) [21], but the main point is to define an approach that allows all the knowledge avail- able to be discovered in a given moment. In general, the dif- ferent machine-learning techniques can be potentially used, according to the context, for example: information online available and real time information. The sources of knowl- edge are very varied and they can be structured (For exam- ple, the transactional database, or data-warehouse, of an organization), semi-structured (For example, xml files) or unstructured (For example, GPS tracking information, audio streams, etc.) data, which represent information about the users, context, etc. The knowledge is acquired through learning mechanisms based on data mining, seman- tic meaning (web mining, text mining, ontological mining), among other techniques [6,9]. The learning mechanisms to be used depend on the knowledge model and the source of data. For example, if the knowledge model is ontology and the source of data is the web, we can use semantic min- ing techniques to extract knowledge. Additionally, in this case, data science tasks are very important, in order to explore, clean, transform and reduce the data, before apply- ing the learning techniques to extract the knowledge [15].

Reasoning mechanism: according to the paradigm choice, there are specific reasoning mechanisms which can be used. The main point is that these mechanisms must allow for inferences. The user needs if the user goals are not satisfied by the current items, considering all the knowledge avail- able. There are three main reasoning mechanisms that can be used: induction, abduction, and deduction. Each one can be used for different tasks, such as analyzing why a rec- ommendation can be given, predicting an item that can be interesting for a user and that means, it allows various types of reasoning: find a ‘‘relaxation” or ‘‘compromise” (for example, What if the user’s requirements cannot be ful- filled?), find a ‘‘diagnosis” (for example, Why a certain item is recommended), carry out a ‘‘verification” and ‘‘repara tion-debugging” (for example, What if the user require- ments are inconsistent?), and so forth. The idea is to define logical explanations about the different aspects to be con- sidered during a recommendation process, using the avail- able knowledge. There are classic RS responses only to some of them.

Criticality system: it is an automatic system to infer the user preferences without asking users about them, only using the knowledge stored in the RS [9]. In each cycle of a recom- mendation session there is a reasoning phase where the aspects are deduced to accept or criticize. These cycles con- tinue until a recommendation is carried out. The learning mechanisms provide the knowledge necessary like a feed- back procedure, to deduce the preferences of the users. They provide the knowledge necessary to deduce them to infer the rating of the items, built over a series of recom- mendation cycles. If a new cycle has been triggered, then the only items that fulfill the criteria defined in the critique are further taken into account (reduction in the candidate items set, in order to reduce the space of search: candidate items). In general, this process continues until the reasoning

The main component of our IRS is knowledge. An IRS must exploit all the knowledge available, and to do that, all the advances must be used in different domains (such as informa- tion retrieval, data mining), in order to extract this knowledge. Classically, a RS estimates the similarity among the item prop- erties and the user preferences, or estimates the ratings for the items that have not been seen by a user. IRS exploits knowl- edge to infer the rating of the items, to infer the preferences of the users, and to match the item properties with the user preferences. For that, it defines different types of knowledge:

An extended user profile (including its opinions, critiques, etc.): Normally, the information that is modeled is about his/her preferences, his/her personal information (age, gen- der, profession, and education), etc. [8,17,22]. Here, we pro- pose to extend with new information about his/her opinions, critiques; his/her relationship with other users (his/her friend groups, etc.). The user model profiles, with the rest of semantic model of IRS must infer the preferences and needs of the users.

An extended item profile: it represents a full description of an item based on four dimensions: (i) the general descrip- tion of the product (name, branches that produce the items, etc.); (ii) the functional information about the item (its func- tions, etc.), (iii) the structural information about the item (its components, the relationships among them, depen- dence, etc.) and finally, (iv) the operational information about the item (how can be used, etc.). Some of this infor- mation can be learned, or inferred from the information stored.

Context and domain knowledge: it is very important to know the domain where the items will be used, the context where the individual is going to make the decision, etc. The con- textual knowledge is all the knowledge that explains a given situation [15,20]. The domain knowledge is the knowledge of an area of a discipline, a human activity, etc. [15,23]. This type of knowledge is not currently considered, or the RS must be customized to be used in a specific context, chang- ing part of its structure. Here, we propose to model explic- itly these aspects.

A critical knowledge: This is a knowledge that must be dis- covered, based on the transactions over the RS (relations between users and items). A transaction may describe the context of the recommendation, may refer the item chosen, and may include the feedback the user has provided, among other things [9,15]. Normally, this knowledge describes the behavior-based knowledge, and must be discovered using machine-learning technique in order to obtain interesting patterns. This knowledge normally represents the interests of the users. In our IRS, this knowledge must be learned or discovered.

In general, our IRS must use the knowledge available in a given moment without degrading its performance. That is, depending on the application domain and the usage scenario, may be only parts of the previous knowledge are available, and the IRS must continue to carry out recommendations. In this way, the definitions of the different types of knowledge are very important, but our IRS is very robust in order to work with the knowledge available. The main difference with the Knowledge-based RS is that this requires knowledge engineer- ing, and an IRS uses learning mechanisms from different sources: product databases, social media, etc. Our system can exploit the different techniques of machine learning, semantic mining, in order to build the knowledge that is need. For example, if the knowledge model is ontology, then we can apply the merging and alignment of ontologies (two types of semantic mining techniques) to enrich the model [24]. In this way, our IRS avoids the problem of acquisition of knowledge of the knowledge-based RS. Additionally, it has not the ramp- up or cold-start problem because IRS can draw inferences about the users or items, even when it has not yet gathered suf- ficient information. Its recommendations do not depend only on the user ratings (case of collaborative RS), or on gathering information about a particular user or item (case of content- based RS), because it infers this information. We dedicate the rest of this section to explaining this knowledge.

Domain knowledge: It is the valid knowledge used in a given area of human activity, in a specialized discipline, etc. Nor- mally, the experts use and develop their own domain knowl- edge. In IRS, it refers to the specific area/domain where it will be used.

One of the advantages of our system is that its implementation can be carried out using any intelligent techniques. The intelli- gent paradigm (ontologies, based on the evolutionary process, etc.) can be chosen according to the source of information, tools available, etc. In this Section, we give one example of implementation based on the FCM.

Cognitive Maps (CMs) are directed graphs that model a real system as a set of concepts and causal relationships between them [1–3]. Each concept is a node in the graph, and represents a characteristic/state of the system. The causal relationships are positive or negative signs, with specific weights. The value of a node is the activation degree of a concept in a given time. This value is defined as the sum of all values of the concepts at the preceding state and the incoming edges.

The user’s behavior is modeled in order to determine his/her preferences, opinions, trends, etc. [10]. The model represents the opinions, critiques (see Table 5). Particularly, the user opinion (critique) about the items recommended is stored, as a result of an inference process, a learning process, a calculation (estimation), or asking the users elicitation. This model is navigated (it is the space of the critique to the prod- ucts) in order to be used during the recommendation process and represents the discovered preferences. In this way, the IRS can recommend items that satisfy the ongoing critique, are similar to the previous recommendation, satisfy the majority of the previous critiques, etc. Our system infers the user preferences and works through a cycle of recommen- dations to build it.

CMs have been extended by Kosko, considering fuzzy logic and neural network theories [2]. This approach has been called FCM, and often is defined by concepts that can be defined as fuzzy sets, and causal relationships between the concepts that can be defined by fuzzy implications. Also, the threshold func- tion of the weighted sums can be fuzzy. There are different learning algorithms for a FCM in the literature. There are two examples of learning algorithms. One is based on the opin- ion of the experts and another is based on the historical data. In [1,3] an exhaustive presentation of different learning algo- rithms is given.

We propose a FCM based on two levels, the first level contains the concepts inferred, which represent the knowledge about the critiques of the products, the preferences of the users, the rec- ommendations, among other concepts. The second level repre- sents the description of the current situation: the information about the items and users, the context, etc. They are defined according to our models presented in Section 5. In this way, the first level is the knowledge generated by our system, which can be used in different ways: to recommend, to discover infor-

In this level, there are concepts inferred from the concepts of the second level or from concepts on the same level. Mainly, they represent concepts linked to the recommendations (our system can recommend different things, for example interest- ing items, similar items), but additionally, other types of con- cepts about information inferred about the products or items (for example, users opinions, or punctuation of the item). In general, the attributes of the critical model belong to this level, and they reduce the space of candidate items. The concepts on this level are defined in Table 6.

In this level, the concepts represent the different attributes of the user and item profiles, extended by the knowledge about the context and domain (see Section 5). They are grouped according to the knowledge that is represented (users, prod- ucts, etc.).

The relationship between the concepts is according to the causal relation between them. They determine the dependent relationships among the concepts. In this case, the learning process adapts the relationships among the concepts: deleting, updating or adding. In this way, our FCM is reconfigurable according to the quality of recommendations given.

Fig. 2 shows the initial FCM. In the first level we can see the concepts inferred listed in Table 6, as the preferences, usability, Use Level (‘‘nivel the uso” in Spanish), etc. In the second level the concepts are defined in Section 5 about users profile defined in Table 2 (Languages (‘‘idioma” in Spanish), Sex, User type (‘‘tipo de usuario” in Spanish), etc.), and item

profile defined in Table 3 (Name (‘‘Nombre” in Spanish), Localization, etc.), among others. The initial values of the arcs have been defined by a group of experts on learning resources, and they have been adapted using the learning mechanisms of the FCM Designer Tool [7].

Particularly, we can see that in FCM the similarity is inferred due to the relationships between the concepts of the item profile and users profile with the concepts inferred (in spe- cial, with the ‘‘Item of interest” and ‘‘Item preferred by user” concepts). The recommendation and preference concepts are examples of elements of the critical model included in the FCM model which are also inferred. In this way, all the ele- ments of the different models are included naturally in the FCM.

We verify whether this information is included in the user profile defined previously (see Table 7). In Table 7 we can see that our profile contains the main attributes of the IMS LIP specification and of the Reusable Definition of Compe- tency or Educational Objective (RDCEO) specification. The RDCEO allows defining competencies in the learning domain. Any additional information to be included in our user profile is specified like part of the domain knowledge of the IRS. In our case, initially we do not add more information. Finally, our FCM will use the same set of concepts, see Table 7.

We use the IMS standard, which describes the general infor- mation to be collected about a student or a producer of learn- ing content, to customize the user profile [8]. The IMS Learner Information Package (IMS LIP) specification defines the inter- operability of internet-based Learner Information systems, with other Internet systems used by learning processes. Our IRS must consider this information. The main aspects to con- sider from the IMS standard are affiliations, competencies, goals, identifications, interests, qualifications, certifications, accessibilities, activities, and relationships.

Based on the same idea of the previous Section, we need to adapt the item’s profile to the context of application of the IRS. In this case, we need to compare the items to be recom- mended for a standard in the domain of the Learning Resource (LR). Specifically, we use the LOM-IEEE standard [11]. The LOM-IEEE defines 9 categories: (a) General category (GC): general information about the LR, (b) Lifecycle Category (LC): metadata related to the history and current status of the LR, (c) Metadata Category: information about the meta- data itself, (d) Technical category (TC): metadata about the technical requirements of the LR, (e) Educational category: metadata for educational uses of LR, (f) Rights Category: metadata about property rights and intellectual material, (g) Relation Category: metadata used to establish relationships between the LRs, (h) Annotation Category: annotations and comments on the LR, and (i) Classification Category: LR clas- sification like taxonomies.

The general category of the LOOM-IEEE standard includes nine types of metadata, such as: Identifier (ID), Title, Lan- guage, among others. The technical category includes Format, Size, Location, etc. Educational category groups the metadata: Interactivity Type (‘‘Active”, ‘‘Expositive”), LR Type (exercise, simulation, questionnaire, slide, experiments, lecture, etc.), Interactivity Level (low, high, etc.), Context, Difficulty, Typical Learning Time, among others. For the rest, see [11].

In the Sections 7.1 and 7.2 we show two examples of imple- mentation of the user and item profiles of our IRS model. In this study case, we have used two standards of the domain of learning to define these profiles: the LOM standard for the item profiles, and the IMS standards for the user profile. We confirm that the general user and item profiles defined in Section 5 are part of these standards. In this way, the FCM implementation of the IRS defined in Section 6 is correct.

In this section, we evaluate the capabilities of our FCM-based IRS in terms of the learning methods and reasoning mecha- nisms. In the previous sections, we have defined the knowledge models used by the FCM-based IRS. The knowledge model based on FCM is defined by the concepts that represent the information about the users, items, domain, context and criti- cisms (see Sections 5–7). In this section, we evaluate the learn- ing and reasoning capabilities.

Our RS can be used in two cases: to recommend an LR for a given student, and to infer quality aspects in an LR. In this sec- tion, we are going to test both capabilities. To recommend LR, we only need infer concepts linked to preferences, and to infer the quality of a learning resource, we only need infer concepts linked to the characteristics of the learning resources (see Table 9).

If we like to study the preferences (first case), then we need to keep all the student concepts, and some of the concepts about learning resources (such as item ID, type, goal). The attributes related to the educational characteristics of the item are important in this case (see Table 11). To define the prefer- ences of a student we need all of its concepts (see Table 10), but to infer the quality of the learning resources only the student concepts about its physical characteristics (sex, disability, etc.) are of interest (see Table 10).

To recommend learning resources. Now, we present an example of the FCM to recommend an LR. In this case, the IRS infers whether a student is interested or not in an LR, whether it has a preference for it, whether the LR is useful for the student, etc. For this experiment, an example of the val- ues of the student attributes is given in Table 12, and of the LR in Table 13.

To infer the quality of a learning resource. In this case, we test the FCM to infer the characteristics of usability and interoperability. An example of the input of the pair LR and student is given in Tables 15 and 16.

This FCM can predict the characteristics of usability and interoperability of a LR. This is an interesting use of the FCM, in order to determine whether a LR can be interesting for a given course (that can be very important in courses where there are students with physical disability). For example, for the case of the Tables 15 and 16, the first column in Table 17 describes the inference. In this case, the FCM infers a high usability and interoperability of the LR. Particularly, the FCM infers the LR is easy to use for the students with the pro- file of Table 15 (usability), and can be integrated into other platforms (interoperability).

In the Section 8.1, we have evaluated the learning and reason- ing capabilities of the FCM-based IRS. Particularly, the rea- soning capabilities allow different types of inferences not only to recommend but also to infer other types of information such as the quality of the learning resource. With the models and results obtained in Sections 6,7 and 8.1, we have evaluated the entire FCM-based RS as an IRS.

Now, we compare the FCM-based IRS approach with other knowledge-based RS. In order to determine the compar- ison criteria, we define some questions about how the knowl- edge is managed by the different approaches in the context of learning resources. These questions are as follows: How can our IRS be evaluated? How can we measure the quality of the IRS? In general, we are going to use metrics that try to determine how the knowledge in our IRS is exploited. For this reason, we propose the next criteria [9,15]:

In general the IRS gives good results. It gives a good expla- nation of its recommendations and advices based on its infer- ence process (see Section 7.1) and its knowledge model about the domain of interest (see Sections 5–7). Due to that this information can persuade to the users in a transparent way in order to help them in their processes of making decisions. The only criterion where our IRS is not good is in efficiency, because the user needs to customize the input and to interpret the inference process of the IRS. The FCM-based IRS increases the decision-making effort of the users that is due to the technique used to test our IRS (FCM), and not to the IRS framework, because the user needs to know the FCM the- ory to understand the process followed by the RS.

The works [14,16] do not give explanations about their rec- ommendations because their recommendations are not based on a knowledge model. The work presented in [10] has the same problem of efficiency as our work because it is based on the same technique, but it has not a learning process and the users must update manually the cognitive map (trans- parency). Additionally, the knowledge model of [10] is very simple. It does not consider aspect as the context and the crit- icisms, and its user and item profiles are very simple. These limitations in the knowledge model or the inexistence of the knowledge model in [14,16], reduce their persuasive capabili- ties and for the same reasons in some cases the recommenda- tions do not help users for making high-quality decisions. We like to highlight that [10] uses the same technique that our example of IRS, but its knowledge representation is very simple and has not a learning mechanism. Our IRS framework introduces some aspects such as the knowledge representation, the learning and the reasoning mechanisms, all of which improve the recommendation process.

[10] uses the same technique that we have used to implement our example of the IRS, but in [10] it is defined as a knowledge-based RS. In the Section 8.2, we have determined that our system has better behavior due to these characteristics used simultaneously.

The criteria about the management of knowledge defined in Section 8.2, are well achieved by our approach. Our approach obtains two types of results: qualitative (cognitive map) and quantitative (inferences). With these results it meets the crite- ria. [14,16] do not reach the validity and efficiency criteria because they do not explain how the results are obtained and

the students/teachers must reason about why to use the learn- ing resources recommended (the validity is not reached). Also, [10,14,16] are not persuasive and effective due to the knowl- edge model used which only cover partial information about the students or learning resources or to the inexistence of a knowledge model. This does not allow convincing a person, because it is not possible to define arguments based on knowledge.

Further works must test the IRS in other types of problems with other intelligent techniques, like the ontologies, the fuzzy logic, in cases which require large knowledge about the domain and context. Also, more experiments are necessary to deter- mine other uses of the knowledge stored by the IRS in the con- text of RS (e.g., for diagnosis).

