Support for nonconforming meshes relies on the p4est software library [38,39] and its bridge to PETSc’s DMPlex [40]. Leveraging the capa- bilities of the PETSc library allows support for different mesh formats including fluent, Exodus II, CGNS and GMSH. Triangle/tetrahedral meshes are converted on the fly into quadrilateral/hexahedral ele- ments; uniform and non-uniform mesh refinements algorithms are also available.

Amazon EC2 offers a variety of instance types that are suitable for a wide range of applications. It is possible to mix and match CPU, memory, storage, and networking resources by using different instance types. Depending on the workload, we can choose from a variety of instance sizes available for each type of instance. AWS Cloud provides highly flexible computing platforms that are suited for performing HPC applications in terms of resource availability and configurability. CFD usually necessitates a large amount of computer resources, and its software architecture makes it suited for parallel processing; this requires the use of an HPC or cloud computing infrastructure. In order to deal with parallel computation needs, an adequate resource management system is also necessary. AWS ParallelCluster is one of the possible solutions to the aforementioned requirements. Here, we will briefly describe the compute environments of Amazon EC2 cloud and on-premise resource Ibex cluster.

of RAM ranging from 360 GB to 700 GB. The network connectivity on Ibex depends on InfiniBand HDR Director Switch. It has the following features and benefits: Mellanox HDR Infiniband is capable of 200 Gbps, it uses in compute and storage nodes with HDR-100 at 100 Gbps speed. Table 1 summarizes the main features of the Ibex’s architectures used in this work. We highlight that Ibex has a daily occupancy of about 75%, with Intel architectures in high demand.

The purpose of this study is to analyze the performance of the SSDC solver on Ibex and Amazon EC2 clusters for complex flow problems and provide the current cost per core hour. The default number of CPU cores for an EC2 instance depends on its type. We only allowed the instance to use the available physical CPU cores. This means we disabled hyperthreading. In the following sections, we report the arithmetic average of the wall-clock time (WCT) in seconds of three independent runs and the associated estimated cost in United States Dollars (USD).

/ig. 2. Flow past two spheres in tandem: Wall-clock time in seconds for each simulation against the number of nodes for Intel CPU architectures. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

D, held fixed in a rectangular domain located at a separation distance of We simulate the flow past two equally sized spheres with a diameter 10D [41]. The Reynolds and Mach numbers are set to 𝑅𝑒𝐷 = ∞D∕𝜈 = 104 and 𝑀𝑎∞ = 0.1, respectively. The Prandtl number is set to 𝑃 𝑟 = 0.7.

Regardless of the geometry’s simplicity, capturing the flow in this regime is relatively difficult. The relevance of such flows around sev- eral bodies, specifically around two spheres, is considered significant in many practical applications, as it allows a better understanding of the effect of the wake behind a leading bluff body on the flow around a trailing one, for instance. A non-exhaustive list of important applications ranging from industrial fluidized beds to bio-reactors, to the combustion of aerosols, could be liquid–gas two-phase flows [42], suppression of icing on the solid surface [43], and oil droplets [44]. The complexity of the flow fields is shown in the lower panel of Fig. 1

We perform the numerical simulations using one of the grids TandemSpheresHexMesh2Pm provided by Steve Karman of Point- wise for the HiOCFD5 [41]. An illustration of the grid structure is shown in the upper panel of Fig. 1. In this study, we use this mesh

nodes are non-exclusive, with a daily occupancy of the Ibex cluster of approximately 75%. Thus, the more computing nodes, (1) the greater the likelihood that computing resources will be shared with other users, and (2) the more partitioned the job, the more likely it is that network bandwidth will be shared. We highlight that this behavior can also be observed in the case of exclusive access to the nodes. In fact, for a ‘‘sufficiently’’ large number of CPU cores count and hence, sufficiently smaller local problems, communication between partitions cannot be hidden behind computations. Thus, the solver’s performance degrades and departs from the ideal behavior.

/ig. 4. Flow past two spheres in tandem: Wall-clock time in seconds for each simulation against the number of nodes for AMD & Arm CPU architectures. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)

eight computing nodes, in particular, results in a speed-up factor of about 5.6 for both cases (8 being the perfect scaling factor). Overall, the c5d.9xlarge EC2 instance delivers the shortest time-to-solution. Notably, although the on-premises cluster’s wall-clock time increases as the number of compute nodes increases, up to two Ibex nodes running

computing nodes. Therefore, it seems that utilizing half of a node’s physically available cores has a favorable effect on wall-clock time. In fact, only half of the cores utilize the shared intra-node network, and more crucially, the workload per core permits better communications to be hidden behind computations.

As observed from Fig. 2, the results are favorable for simulations conducted on 8 EC2 nodes. In particular, the c5d.9xlarge EC2 instance delivers the results in the least amount of time. However, in terms of cost, Fig. 3 leads to a different conclusion: For any number of nodes and order of accuracy tested, it is cheaper to run on the Ibex cluster.

In Figs. 5, we show the cost of simulating the test case with AMD and Arm architectures. Compared to the AWS ParallelCluster, the Ibex cluster offers significantly more affordable computations. While between all AWS EC2 instances, the Arm nodes are the least expensive for c6 g.4xlarge and c6 g.8xlarge. Finally, it is worth noting that the cost and performance of the EC2 c6 g instances are similar to those of the AWS EC2 c5d instances for all orders of accuracy.

radius leading edge configuration, 𝑟𝐿𝐸 ∕𝑐̄ = 0.0015, where 𝑐̄ = 0.653 m. ternational Vortex Flow Experiment. In this work, we use the medium The delta wing has a mean aerodynamic chord of 𝓁 = 0.667 m, a root chord length of 𝑐𝑟 = 1.47𝓁, and a wing span of 𝑏 = 1.37𝓁. Furthermore,

coordinate system is positioned at the delta wing’s apex with the 𝑥1 the central region is flat, and it has no twist or camber. A Cartesian coordinate pointing downstream, the 𝑥2 coordinate pointing in a span- wise direction, and the 𝑥3 coordinate perpendicular to the flat plate. We consider the sting as part of the setup up to the position 𝑥1∕𝑐𝑟 = 1.758.

is ≈ 1.435 × 107. The simulations are carried out for an angle of attack solution and the number of cells in each block, the number of DOFs of AoA = 13°, a Mach number 𝑀𝑎 = 0.07 and a Reynolds number of

The performance of the simulation for both on-premises cluster Ibex using Intel Cascade Lake architecture and AWS ParallelCluster using EC2 c5d instances are present in Fig. 7. The performance of the on-premise cluster is degraded as the number of cores increases. This is because of the interplays of two factors: the high occupancy and non-exclusivity of the Intel Cascade nodes on the Ibex cluster, and the considerable increase of the fraction of the communication time on the overall computational time for smaller and smaller local problems (communication cannot be hidden behind computations). When the size problem becomes ‘‘sufficiently’’ large, we observe that an increment of the number of nodes corresponds to a decrease in time- to-solution, as shown in Fig. 7(d). However, the performance degrades again for eight nodes, where we observe a growth of the time-to- solution. In contrast, for the AWS EC2 c5d instances, the simulation is speed-up as the number of nodes increases for all orders of accuracy.

/ig. 6. (a) Geometry (top) and solution polynomial degree distribution (bottom) for the 65° swept delta wing test case; 𝑝 = 2 in the far-field region (yellow), 𝑝 = 5 in the region surrounding the delta wing and its support (blue), and 𝑝 = 3 elsewhere (green). (b) Average flow field past the 65° swept delta wing: the 𝑄-criterion colored by the normalized velocity magnitude (left) and mean axial velocity (right) at 𝑥1∕𝑐𝑟 = 0.2, 0.4, 0.6, 0.8, and 0.95; the wing surface is colored using the time-averaged pressure coefficient.  (For

Fig. 8 shows the cost analysis of running on Ibex and the c5d instances. The runs performed with the on-premises cluster using 40 and 20 MPI threads show better cost-efficiency than the AWS EC2 c5d instances. In fact, Ibex provides up to 70% better pricing performance than c5d instances, regardless of solver accuracy and the number of nodes.

As done for the flow past the two spheres in tandem, we also study the performance of the solver and the cost of each simulation for the AMD and Arm architectures. Fig. 9 shows the wall-clock time in seconds for the on-premise cluster with the AMD Rome architecture, the EC2 AMD c5a instances, and the EC2 Arm c6 g instances. The performance results of the Ibex and AWS clusters is mostly matching the results obtained for the previous test case, i.e., the wall-clock time decreases substantially by increasing the number of nodes. In particular, by doubling the number of nodes we observe a speed-up factor of about

1.7 for all the runs. As for the previous application, moving from the c5a.4xlarge to the c5a.8xlarge instances lead to a reduction in the wall-clock time by around 35%-to-40%. Furthermore, the smallest wall- clock time is obtained using the EC2 Arm c6 g instances, for all orders of accuracy. In addition, almost perfect scaling is observed. Doubling

A detailed cost analysis is shown in Fig. 10. Among the EC2 in- stances, the compute-optimized c6 g instances perform better than the c5a instances for all the number of nodes and order of accuracy. However, on the on-premises cluster, using the largest number of physical cores possible is always the most convenient solution.

In the next section, we further assess the performance of the solver on-premises cluster Ibex and the AWS ParallelCluster by considering two more complex industrially-relevant flow problems. Precisely, we will simulate the NASA junction flow experiment and the flow past a Formula (1) front wing. The accurate simulation of these indus- trial problems via large eddy simulation (LES) is representative of the type of simulations that exascale will allow performing in a 24- hour turnaround, a typical requirement for industry standards (see, for instance, [46]). In our context, we use these test cases to explore the influence of the problem size on the performance of both clusters.

represents a different degree of approximation (𝑝) for the solution field. in Fig. 11(a). The grid is broken up into three blocks, each of which Specifically, we utilize 𝑝 = 1 in the far-field region (dark green), 𝑝 = 3 in the region surrounding the model (dark orange), and 𝑝 = 2 in the

In Fig. 12, we show the WTC for different numbers of nodes for the Ibex cluster and the AWS EC2 instances. For all the type of nodes except the Intel Cascade Lake, doubling the number of nodes yields an efficiency of about 95%. For the Intel Cascade Lake architecture with 40 MPI threads, moving from one to four nodes leads to a rapid decrease in efficiency. Eventually, for eight nodes, the time to solution increases. The fastest time-to-solution is delivered by the Cascade Lake with 20 MPI threads per node and the c6g8xlarge AWS EC2 instance. In terms of cost, the on-premise cluster simulations done on Intel architectures are much cheaper than the AWS EC2 cd5 instances, as illustrated in Fig. 13. For the AMD-Ibex nodes and the c5a and c6 g instances, the on-premises cluster still delivers the smallest cost.

The computational domain is divided into 3.4 × 106 hexahedral ele- ments with a maximum aspect ratio of approximately 250. The solution polynomial degree is set to 𝑝 = 2 — a formally third-order accurate scheme. Thus, the total number of DOFs is approximately 9.18 × 107.

The grid is constructed using the commercial software Pointwise V18.3 released in September 2019; solid boundaries are described using a quadratic mesh. The panel of Fig. 14(a) illustrates an overview of the front wing geometry and the mesh, where the contour plot of the time- averaged pressure coefficient on the surface of the front wing is shown in Fig. 14(b).

In this work, we evaluate the performance of a prototype of next generation high-order entropy stable solvers for compressible flows on unstructured grids on the Amazon Web Services Elastic Compute Cloud and the on-premise resource Ibex cluster hosted at KAUST. The study aims to establish the possibility of using Amazon’s cloud-based high-performance computing service to address complex computational fluid dynamics industry flow problems and propose a set of Elastic Cloud Computing instances that provide the fastest time to the solution and offer more affordable computations. In terms of time-to-solution, the Amazon Web Services Elastic Compute Cloud delivers the best performance, with the Graviton2 processors based on the Arm archi- tecture being the fastest. However, the results also indicate that the nodes based on the AMD Rome architecture of Ibex deliver very good performance, close to those observed for the Amazon Cloud service. In addition, we found that the simulations on the Ibex cluster are currently less expensive than those performed on the cloud for all orders of accuracy. Future work will demonstrate the performance at higher core counts and include post-processing elements of the computational fluid dynamics process.

R. Al Jahdali: Conceptualization, Data curation, Formal analysis, Investigation, Methodology, Project administration, Resources, Soft- ware, Validation, Visualization, Writing – original draft, Writing – review & editing. S. Kortas: Resources, Software. M. Shaikh: Re- sources, Software. L. Dalcin: Resources, Software, Review & editing.

Parsani M, Boukharfane R, Nolasco IR, Del Rey Fernández DC, Zampini S, Hadri B, Dalcin L. High-order accurate entropy-stable discontinuous collo- cated Galerkin methods with the summation-by-parts property for compressible CFD frameworks: Scalable SSDC algorithms and flow solver. J Comput Phys 2020;424:109844.

Mengaldo Gianmarco, De Grazia Daniele, Witherden Freddie, Farrington Antony, Vincent Peter, Sherwin Spencer, Peiro Joaquim. A guide to the implementa- tion of boundary conditions in compact high-order methods for compressible aerodynamics. In: 7th AIAA theoretical fluid mechanics conference. 2014, p. 2923.

Balay S, Abhyankar S, Adams MF, Benson S, Brown J, Brune P, Buschelman K, Constantinescu E, Dalcin L, Dener A, Eijkhout V, Gropp WD, Hapla V, Isaac T, Jolivet P, Karpeev D, Kaushik D, Knepley MG, Kong F, Kruger S, May DA, McInnes L Curfman, Mills R Tran, Mitchell L, Munson T, Roman JE, Rupp K, Sanan P, Sarich J, Smith BF, Zampini S, Zhang H, Zhang H, Zhang J. PETSc/TAO users manual. Technical report ANL-21/39 - revision 3.16, Argonne National Laboratory; 2021.

