Abstract Generating multi-label rules in associative classification (AC) from single label data sets is considered a challenging task making the number of exist- ing algorithms for this task rare. Current AC algorithms produce only the largest frequency class connected with a rule in the training data set and discard all other classes even though these classes have data representation with the rule’s body. In this paper, we deal with the above problem by proposing an AC algorithm called Enhanced Multi-label Classifiers based Associative Classification (eMCAC). This algorithm discovers rules associated with a set of classes from single label data that other current AC algorithms are unable to induce. Further- more, eMCAC minimises the number of extracted rules using a classifier building method. The proposed algorithm has been tested on a real world application data set related to website phishing and the results reveal that eMCAC’s accu- racy is highly competitive if contrasted with other known AC and classic classi- fication algorithms in data mining. Lastly, the experimental results show that our algorithm is able to derive new rules from the phishing data sets that end-users can exploit in decision making.

In the last few years, a learning strategy which applies the association rule in classification data called associative classification (AC) emerged (Thabtah et al., 2010; Thabtah et al., 2011; Wang et al., 2011). Most AC algorithms like MAC (Abdelhamid et al., 2012), CMAR (Li et al., 2001) and others usually apply an association rule technique to discover the rules, and then filter out the rules to include only those which their consequent is the class attribute. Experimental research works (Jabbar et al., 2013; Jabez, 2011) indicated that AC algorithms fre- quently build more accurate classifiers than classic classification approaches such as the probabilistic approach (Witten and Frank, 2002), decision tree (Quinlan, 1993) and rule induction (Cohen, 1995). The algorithm proposed in this article is part of the AC family.

Limited research attempts in AC have been conducted to produce rules with more than one class, i.e. Lazy AC (CLAC) (Veloso et al., 2011) and Multi-label Multi-class AC (MMAC) (Thabtah et al., 2004). The rest of the existing AC algo- rithms is unable to deal with discovering multi-label rules from single label data sets, and normally derive only the largest frequency class connected with the attri- bute value(s) and ignore all other classes even if these classes have large frequen- cies with the attribute value(s). For instance, this condition occurs if an attribute value such as <A> is associated with two class labels (cl1, cl2) in different places (examples) within the training data set with frequencies equal to 44 and 45 respec- tively. A typical AC algorithm like CBA will only take on class ‘‘cl2’’ simply because it has a larger frequency than cl1 with <A> and ignores class cl1 even if this class is statistically significant with <A>. This surely makes the selection of (<A>, cl2) questionable. In the proposed algorithm we pick the two class labels and construct a multi-label rule rather than removing class cl1. This enables

the decision maker to obtain knowledge missed by current AC algorithms. The primary motivation of this paper is to deal with the problem of generating multi-label rules via AC from single label data sets. In other words, we intend to discover all class labels associated with the attribute values bringing up novel and useful knowledge normally missed by current algorithms.

In this paper, a new multi-label rule based AC called Enhanced Multi-label Classifiers based Associative Classification (eMCAC) is proposed. This algorithm extracts from data sets not only rules with the most obvious class but rules that are associated with a ranked set of class labels. When an attribute value in a training data set is connected to more than one class in different locations with certain fre- quencies, the proposed algorithm extracts and sorts all of them in the rule conse- quent according to their frequencies. Thus, later in the prediction step, there can be more alternatives (classes) when the rule is used in predicting the class for a test case.

The proposed algorithm generates rules from the complete training data set and without performing recursive learning as the MMAC algorithm, which requires learning from parts of the training data set. This means MMAC ends up with sev- eral single label classifiers that are then merged in a separate step to make the final classifier. Another main distinction between eMCAC and MMAC is that our algo- rithm’s way of computing the confidence and the support for a multi-label rule is based on the average confidence and support values of all (Items, Classes) con- tained within the rule, whereas, MMAC assigns the top ranked class confidence and support to the multi-label rule.

The proposed algorithm employs a rule pruning method that considers a rule part of the classifier if its body is contained within the training example. This is done without considering the class similarity of both the evaluated rule and the training case, thus ensuring a high rule coverage with respect to the training cases and consequently a smaller number of extracted rules. Section 4 demonstrates the applicability of the proposed algorithm on real world application data related to phishing that was collected from phishy and legitimate websites (www. phishtank.com) (www.millersmiles.co.uk).

The AC problem, its related basic concepts and relevant literature are given in Section 2. Section 3 surveys common approaches in the literature and the pro- posed algorithm is presented in Section 4. Experimentations and result analysis are demonstrated in Section 5, and lastly conclusions are given in Section 6.

Let T denote the domain of the training cases and C be a list of classes. Each case t e T may be given a class c1, c2, .. ., ck for ci e C, and is represented as a pair (t, (ck)) where ck is a class from C associated with the case t in the training data.

Let H denote the set of classifiers for T → C where each case t e T is given a class and the goal is to find a classifier h e H that maximises the probability that h(t)= c for each test case. In our algorithm, and in case of multi-label data we assume that it is transformed to a single label data format after applying the copy transforma- tion method (Section 3.1). The proposed algorithm deals with the single label data format only. The multi-label data set is displayed in Table 1. Table 2 denotes the data obtained after applying the copy transformation method in Table 1 and before the mining process starts.

Several data transformation methods exist in the literature to convert multi-label data into one or more single label data. To demonstrate these data transformation methods we use the data set of Table 1 which consists of nine training cases that belong to the following class set {cl1, cl2, cl3}. We summarise some of the common methods from (Tsoumakas and Katakis, 2007) and use our own example to fur- ther simplify them.

The first data transformation method simply removes any multi-label case from the training data set. Therefore, from Table 1, cases located in TID (1,2,5,6,8,9) are discarded. Another data transformation method selects one class of each case either arbitrarily or subjectively by the domain expert. So from Table 1 a single associated class for each of the multi-label cases may be picked. Another more realistic method transforms every multi-label case into a single label one by replac- ing the multi-label case (xi ,Yi) with |Yi| cases. After that a number of methods could be applied such as copy-weight which associates a weight of (1/|Yi|) to each of the transformed cases. Table 2 shows the copy transformation method after applying it against Table 1, which our algorithm employs when the input data are multi-label .

Finally, a common data transformation method used in image classification that derives a single label binary classifier for every class in the class set is called Binary Relevance (BR) (Boutell et al., 2003). It transforms the original multi-label data set into |L| data sets which contain all the cases. This method gives a positive indicator for a class if it is associated with a case in the training data set and a

The majority of existing AC mining algorithms use rules learnt from the training data set for constructing a single label classifier which in turn is utilised for predict- ing test data. Thus, there are limited numbers of research articles related to multi- label rules in AC. Hereunder, we shed the light on two approaches and other techniques related to traditional multi-label classification in data mining. It is worth to note that the traditional classification algorithms surveyed in this section are related to multi-label data sets and they assume each training example to be associated with more than one class. This is unlike the proposed algorithm that assumes each training example to be linked with a single class but produces multi-label rules.

Veloso et al. (2011) proposed a multiple label AC algorithm that adopts the lazy classification approach in which it delays the reasoning process until test data are given. Unlike binary classification which does not consider the correlation among classes, the lazy algorithm takes into account class relationships. Furthermore, it deals with the small disjuncts (rules that cover limited number of training data), this may reduce classification accuracy. This lazy approach has been compared with BoosTexter (Schapire and Singer, 2000) on three medium size data sets from ‘‘http://portal.acm.org/dl.cfm’’ with respect to error rate. The results produced show that this method is competitive to BoosTexter.

Another AC algorithm called MMAC was proposed to find multiple label rules from single label data sets. It has been reported that MMAC was able to generate higher quality classifiers than CBA and decision trees on a number of UCI data sets in regard to classification accuracy. One obvious limitation of the MMAC is that the classifier produced is extracted from parts of the training data set and the requirement of the recursive phase to find the multi-label rules.

Wang et al. (20110 proposed an enhanced Emerging Pattern (EP) algorithm called ADA that constructs rules from both the input training data set as well as the classified resources such as text documents. ADA classifier gets amended on the fly after the classified resources reach a certain amount. The authors have updated the classifier by refining the newly discovered knowledge using the exist- ing rules. Moreover, ADA uses the maximum entropy approach to classify test cases where multiple rules that are applicable to the test case contribute to the pre- diction decision. Overall, ADA can be considered a semi-incremental algorithm since few training examples or users set of frequent patterns (keywords) are only necessary to build the classifier instead of the complete training examples. Then, the classified examples as well as the rules are employed to update the classifier by adding or removing rules. Limited experiments on four data sets from the UCI data repository (Merz and Murphy, 1996) have been conducted using

In image classification, pictures may belong to multi-labels, i.e. different objects within a view. This problem is called class overlapping where a scene may contain multiple labels. A scene classification method called cross training was developed in (Boutell et al., 2003). This method trains on each available label in an image in turn during the training step in order to consider all available labels. The results produced reveal that the developed scene classification algorithm performs well with respect to classification accuracy.

Tsoumakas and Katakis (2007) surveyed common learning approaches related to multi-label classification in several domains. The authors have firstly presented the multi-label data transformation methods used in the literature, and then sur- veyed the different multiple label learning approaches including the adaptation and the transformation methods. Experimentation using three different data transformation methods and various learning algorithms with respect to different evaluation measures, has been conducted. The accuracy results of the compared algorithms revealed that the ‘‘PT3’’ transformation method (considers each differ- ent set of labels that exist in the multi-label data collection as a single label) when used with a learning algorithm generates the highest results in each of the consid- ered data collections.

Our algorithm consists of three main steps: Rule discovery, classifier building and class assignment. In the first step, eMCAC iterates over the training data set in which rules are found and extracted. In step (2), redundant rules are discarded which means that rules do not have training data coverage. The outcome of the second step is the classifier which contains rules. The last step involves testing the classifier on the test data set to measure its predictive rate. Details on eMCAC steps are given in the subsequent sections. The proposed algorithm assumes that the input attributes in the training data set are categorical (having distinct values), and for each of these attributes, all possible values are mapped to a set of positive integers. For continuous attributes any discretisation method can be employed before the training phase.

This step is optional and only required when the input data are multi-label. In this case, we copy each training example with each of its connected class labels. So, if there is a training example linked with two classes, this example is repeated with each class. One of the reasons behind our selection of this method is that we would like to treat each class inside the multi-label example equally. This is since class labels are not sorted in the first place within the multi-label training data set for each case and therefore we do not have prior knowledge on the best class for each training example. Later, when the classifier is constructed we will be able to sort classes within each rule generated based on their frequency with the rule’s attribute values in the training data set after the transformation. Another reason for select- ing this data format method is that we do not want to lose any knowledge that might be useful to the decision maker by ignoring class labels particularly when other data transformation methods are chosen including ‘‘largest frequency class’’, ‘‘class random selection’’, ignore multi-label case’’, etc.

In the last few years, some scholars (Abdelhamid et al., 2012; Thabtah et al., 2005) have developed AC algorithms which employ vertical data layout. A training data set in the vertical data layout consists of a group of attribute values, where each attribute value is followed by its locations (Tids) in the training data set. Table 3 shows the vertical layout of attribute values ‘‘y’’, ’’z’’, and ‘‘b’’ from Table 1. A number of research studies (Thabtah, 2007; Abdelhamid et al., 2012) revealed that the vertical data format is more effective for representing data than the horizontal format since it makes the process of identifying frequent attribute values efficient specifically the task involving the support counting. This is simply because vertical algorithms use simple TIDs intersection among attribute values to accomplish the

The eMCAC algorithm uses a rule discovery method that utilises fast intersection among attribute values TIDs to discover the rules. The TID of an attribute value holds the locations (row Ids) that contain the attribute values and its associated class labels in the training data set. The learning method of the proposed algo- rithm discovers the frequent attribute value of size 1 (F1) after scanning the train- ing data set once. In particular, for each attribute value linked with a class, its support is computed from its TIDs list in which the size of the subset of the TIDs list that is associated with the largest frequency class of an attribute value divided by the size of the training data set denotes the attribute value support. In cases where an attribute value is connected with more than one class it will end up with more than one support. This ensures the production of the multiple label rules since the algorithm allows an attribute value to be associated with multiple labels as long as they are frequent.

When F1 is generated, the algorithm simply intersects the TIDs of the disjoint attribute values in F1 to discover the candidate attribute values of size 2, and after finding F2, the possible remaining frequent attribute values of size 3 are obtained from intersecting the TIDs of the disjoint attribute values of F2, and so forth. Since this frequent ruleitems discovery approach iterates over the training data set once, it is highly effective according to several experimental studies in the lit- erature of data mining community especially with regard to processing time and memory usage. More details on the advantage of vertical algorithms over tradi- tional ones are given in (Thabtah, 2007).

When frequent attribute values are identified, eMCAC generates any one as a rule when it passes the MinConf threshold. This is accomplished in a straightforward manner since all necessary information for calculating the rules confidence values are stored in the attribute value TIDs. Any frequent attribute value that holds a confidence value smaller than the MinConf gets discarded.

For any attribute value connected with many classes and which becomes fre- quent, eMCAC generates a multi-label rule for it when it passes the MinConf threshold. For example, the attribute value <a> of Table 2 is linked with two class labels, e.g. (cl1, cl3) 4 times each in the training data set. Assume that the MinSupp and MinConf are set to 4/15 and 40% respectively. This means (<a>, cl1), and (<a>, cl3) have higher support and confidence than the Min- Supp and MinConf thresholds and therefore two rules can be produced in this case: a → cl3, and a → cl1. For this example, a typical AC algorithm such as CBA only derives the rule that has higher coverage in the training data, meaning any of the above single label rules can be produced. On the other hand, the pro- posed algorithm does not discard any useful knowledge and for the above example it produces a multi-label rule R: a → cl1 v cl3, where normally class labels are ranked based on their count with the attribute values. In the above example the class rank within the rule is random since cl1 and cl3 have the same count when linked with <a> in the training data set.

Once the complete set of rules is derived a rule sorting procedure is invoked to ensure that rules with high confidence and support values are given higher priority to be selected during building the classifier. The rule sorting procedure utilised considers different criteria to favour among rules. The criteria order is: rule’s con- fidence, support, length and class frequency. This ordering of rules has been used since it reduces rule random selection in the prediction step when no rules are found to be applicable to the test case which positively affects the classification accuracy of the classifier.

After rules are sorted from which a subset gets chosen to comprise the classifier. Precisely, and for each training case, eMCAC iterates over the complete set of rules discovered (top-down fashion) and marks the first rule that corresponds to the training case to be part of the classifier. A rule gets inputted into the classifier if it covers at least a single training case. The rule coverage does not necessitate the similarity between the rule’s class and that of the training case. This results often in more training coverage for each rule since all training data belonging to the rule body are removed during evaluation. This surely reduces overfitting and usually ends up with less number of rules. The same process is repeated until all training

The proposed algorithm fires the first sorted rule in the classifier applicable to the test case and assigns its class to the test case. The rules attribute values must be contained in the test case in order to be chosen for classifying the test case class. When there is no rule fully applicable to the test case then we take on the first rule that partly matches the test case attribute value. Unlike the majority of current prediction procedures in AC mining that takes on the default class when no rules are applicable to the test case our prediction procedure minimises the utilisation of the default rule in class assignment process of test cases which normally improves upon the resulting classifier performance. This is since default rule has been cre- ated with high error from the remaining unclassified training data cases while

In all experiments, tenfold cross validation testing method has been employed for fair evaluation of the classifiers derived by the algorithms considered and to reduce overfitting. Furthermore, six dissimilar classification algorithms which utilise a variety of rule learning methodologies have been considered for contrasting pur- poses with the eMCAC. These algorithms are MMAC (Thabtah et al., 2004), CBA (Liu et al., 1998), PART (Frank and Witten, 1998), MCAR (Thabtah et al., 2005), RIPPER (Cohen, 1995) and C4.5 (Quinlan, 1993).

The experiments were conducted on an I3 machine with 2.3 Ghz. The experi- ments of C4.5, PART and RIPPER were carried out in Weka software (Witten and Frank, 2002). For AC algorithms, we have selected CBA, and MCAR for sin- gle label classifier comparison and MMAC for multi-label classifier. CBA and MMAC source code has been obtained from their prospective authors and the proposed algorithm and MCAR were implemented in Java.

Finally, we have set the MinSupp and MinConf thresholds for the AC algo- rithms (CBA, MCAR, MMAC, eMCAC) to 2% and 50% respectively for all experiments. The main reason for giving the MinSupp 2% is that previous research works, e.g. (Thabtah et al., 2005), have suggested that MinSupp values ranging between 2% and 5% may balance between the number of rules generated and the predictive accuracy of the classifier. On the other hand and for the MinConf parameter, it has been set to 40% since it has minor effect on the performance of the classifiers.

Phishing features can be extracted in a number of ways one of which is manual extraction where users derive features and judge their legitimacy. In this method, users have to spend a lot of time studying the up-to-date phishing collection tech- niques which is an infeasible approach for the majority of the users. The second method employed in extracting phishing features is the automatic extraction. This is accomplished by examining the webpage and extracting a set of patterns related to phishing and legitimate type web pages. This involves examining the webpage properties and all its features. Webpage properties are typically derived from HTML tags, URL address and Javascript source code.

One main reason for achieving higher predictive accuracy by the eMCAC algo- rithm is its ability not only to extract one class per rule but also all possible class labels in the form of a disjunctive multiple label rule. This extra useful knowledge is usually missed by the majority of existing AC algorithms and can contribute positively in predictive power as well as serve the need for the end-user. This can be clearly obvious in real world applications such as website phishing. Fig. 2 lists the number of rules generated by all algorithms against the phishing data set. The figure stresses the point that the AC algorithm especially MCAR still generates alarge number of rules if contrasted to decision trees, rule induction or hybrid classification.

For the multi-label classifiers, we contrasted the proposed algorithm with MMAC multiple label AC algorithm. Fig. 3 illustrates the two measures values derived by the eMCAC and the MMAC algorithms named ‘‘Label-weight’’ and ‘‘Any-label’’ (Thabtah, 2007). Hereunder are the equations for calculating the two evaluation measures:

To further clarify how the label-weight evaluation measure works, consider for case a rule R: X ( Y → l1 v l3 where attributes value (X, Y) is associated 30 and 20 times with class labels l1and l3 in the training data respectively. This is why class l1 precedes class l3 in R. The label-weight technique assigns the predicted class weight to the test case if the predicted class matches the actual class of the test case. On the other hand, ‘‘Any-label’’ evaluation measure considers 100% correct classifica- tion when any of the multi-label rule’s class matches the test case class. So for the rule R if the test case class is either l3 or l1 the test case will be given as ‘‘1’’. This explains its higher rate within Fig. 3. In the same figure eMCAC algorithm outper- formed the eMMAC algorithm in both label-weight and any-label evaluation mea- sures for the phishing data.

To signify the importance of the additional knowledge produced by the pro- posed algorithm Fig. 4 displays the number of multi-label rules with respect to their consequent part (class labels on the right hand side). The proposed algorithm was able to extract multiple label rules from the phishing data set solving an important problem in classification data mining regarding rules overlapping class labels. In particular, Figure 10 shows that the eMCAC algorithm generated 24 multiple label rules that represent ‘‘Legitimate OR Phishy’’ website class. This multi-label class is in fact websites that are suspicious and mainly classified by cur- rent classification algorithms as ‘‘Phishy’’ since they do not account the class over- lapping problem. In other words, the eMCAC algorithm was able to extract rules that current AC algorithms and traditional classification algorithms ignore bring- ing up interesting useful information for the end-user. The fact that the eMCAC algorithm finds this additional knowledge is an indicator of the ability of the algo- rithm to discover new data insights most current AC algorithms are unable to detect.

In this paper, a new multi-label rule-based classification algorithm based on AC mining called eMCAC has been proposed. The originality of the proposed algo- rithm is its ability to generate rules with multiple class labels from single data sets and without recursive learning in current AC methods like MMAC. Experimental results against crucial applications named website phishing have been conducted to evaluate the performance of the proposed algorithm in classifying websites. The measures of evaluation are label-weight, any-label, accuracy and number of

rules and the contrasted algorithms are CBA, MCAR, MMAC, PART, C4.5 and RIPPER. The results of the experiments showed that the proposed algorithm out- performed the considered algorithms on the real world phishing data with respect to accuracy. Further, the label-weight and any-label results of the proposed algo- rithm are better than those of the MMAC algorithm for the same data. The eMCAC algorithm was able to produce multi-label rules from the phishing data where each training example is associated with one class. We have identified a smaller effective feature set for detecting the type of the website after applying Chi-square feature selection method. The results of all considered algorithms other than eMCAC have been consistent in detecting the phishing website. In the near future, we intend to apply the eMCAC algorithm on unstructured data related to text categorisation.

