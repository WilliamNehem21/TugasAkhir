In another proof-of-concept study, we have recently predicted activ- ity cliffs (ACs), which are formed by pairs of active structural analogues with significant potency differences [14], on the basis of image data [15]. ACs are of particular interest in medicinal chemistry because they capture small chemical modifications having large biological effects and are thus rich in structure-activity relationship information [14]. For image-based AC prediction, a CNN architecture was also used [15]. AC prediction represents a special task because in this case, test instances are compound pairs, rather than individual molecules. Accordingly, in AC prediction, the negative class consists of pairs of active structural analogs with small or no differences in potency. ACs were first correctly

Transfer learning [17,18] refers to the process of learning a new task by transfer of knowledge from a related task for which models have already been derived. Transfer learning is applied in machine learning to derive models in the presence of related predictions tasks, in particular, when limited amounts of training data are available for individual tasks. To facilitate the analysis, a deep CNN architecture was pre-trained and fine-tuned in different ways. Pre-training was carried out using com- pound images to learn FG chemistry and accurately classify compounds based on the presence or absence of FGs. Only the final layers of these CNN models were then fine-tuned for the complex task of AC predic- tion based on transfer learning. Alternatively, all layers of CNN models pre-trained on compound images or general image data were re-trained using AC data. As a control, models initialized with randomized weights

Substructures representing commonly observed FGs were extracted from compounds using the FG identification algorithm introduced by Ertl [19]. The method identifies all heteroatoms in a molecule together with carbon atoms connected by non-aromatic double or triple bonds to other carbons or any heteroatom, acetal carbons, and oxirane, aziridine and thiirane rings, and combines subsets of connected marked atoms into FGs [19]. FGs are then extracted together with atom environment information (i.e., bonded carbon or hydrogen atoms) and assigned to

ing, dropout, and dense layer. Max-pooling was used as pooling layer to compute the maximum value in each patch of each convolved feature map. A dropout layer was added to avoid overfitting. To train the SCNN model on compound images or condensed graph of reaction (CGR) rep-

stride 2. For transfer learning, the IV-3 architecture was slightly modi- fied by replacing the last fully connected layer with three fully connected layers of output dimensions of 500, 1000, and 2000 neurons, respec- tively. As final layer activation functions, sigmoid and softmax were used for FG multi-label classification and AC prediction, respectively. Models were trained using the Adam optimizer to minimize binary cross en-

MMPs can be represented in a single graph using the condensed graph of reaction (CGR) approach [31]. The CGR formalism was origi- nally conceived to combine reactants and products graphs based upon a superposition of invariant parts [31]. The resulting CGR is a completely connected graph in which each node represents an atom and each edge a bond. In a CGR, the shared core of an MMP and the two exchanged

substituent fragments are represented as a single pseudo-molecule. MMP CGRs were generated using an in-house Python script and con- verted into a pseudo-molecule using the RDKit API. The larger fragment was connected with the core via a single bond and the smaller frag- ment a hypothetical zero-order bond [32]. For each pseudo-molecule,

that were involved in the formation of most FGs. On the other hand, the I-IN model detected more general chemical features covering both the core structure and FGs. These observations provided a rationale for successful transfer learning by the I-FG model, given its ability to specif- ically recognize FGs that distinguished between compound forming ACs and non-AC MMPs.

Abadi M, Barham P, Chen J, Chen Z, Davis A, Dean J, Devin M, Ghemawat S, Irv- ing G, Isard M, Kudlur M, Levenberg J, Monga R, Moore S, Murray DG, Steiner B, Tucker P, Vasudevan V, Warden P, Wicke M, Yu YZX. TensorFlow: a system for large-scale machine learning. 12th USENIX Symposium on Operating Systems De- sign and Implementation (OSDI 16), Savannah, GA; 2016.

