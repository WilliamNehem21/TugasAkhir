To design a complex algorithm, a top-down approach is taken and the analysis proceeds by successive refinements. By so doing, we stay away from any implementation, the concrete representation of data is not fixed; refers to the abstract data type. We give a notation that describes the data, the applicable operations to these data (primitives), and the properties of these operations (Semantic). According to [28], an abstract data type (ADT) is a mathematical specification of a set of data and a set of operations that can be performed on them. This type qualifies as abstract because it corresponds to a set of specifications that a data structure must then put in the work. It allows to define non-primitive data types which are not available (not already imple- mented) in current programming languages. However, the framework for formally defining the types and operations on objects of these types is that of algebraic specifications. More precisely, the semantics of an algebraic specification consists of the definition of one or many

Note that the name sort is more specific than the name of the type. While a sort is a syntactic object, a type is both a set of data and a set of features working on these data [29]. Also, several different types can be associated with the same set of data according to the functionality considered. Moreover, the number of elements the function requires is called the arity [29]. The term function here represents any operation or any definition requiring operations.

characterizes a face. Therefore, the more parameters we have, the more precise will be the representation. Moreover, in real-life, the processed images are still not of good quality as is often the case in datasets that presuppose that the images are of good quality. This, therefore, introduces new sets of constraints that can be related to illumination, rotation, resolution, occlusion, etc. As we cannot tackle all these issues simultaneously, we have chosen the ones which are convergent, includ- ing illumination, rotation, and resolution. Consequently, we represent a face by a vector of invariant characteristics by any operation affecting the texture (illumination, etc.) and geometric transformation (rotation, translation, etc.), and introduce a parameter which is a function of the image quality such as the resolution.

According to [30], a signature is defined by a set of sorts, S, and by a set, A, of operation names, each provided with an arity on S. The operation names must be distinct two by two. It is not required that the set A must be finite, although in practice it is always the case (if only to be able to write a specification in a finite number of characters).

obtained with this representation, that is why it has been and is used in most recognition approaches based on deep learning. However, this approach still requires a large amount of training data to be effective. In addition, a size 128 vector is used, giving the intuition that an increase in the size of this vector will lead to an improvement in the results, that is why we have focused our thinking on this track. Recently, a new approach is under study which involves representing a face with an embedding vector of size 512 and 2048 [32,33] with a corresponding neural network. These treatments are carried out using the assumption that the images are of good quality which is not always the case in real-life. In this perspective, [34] proposes a method allowing to improve the quality of the image, that is, to transform an image of small resolution to a high-resolution image. As we mentioned in the previous

equation calculates the distance between the vertical landmarks while the denominator calculates the distance between horizontal reference points. The weighting of the denominator is because there is only one set of horizontal points but two sets of vertical points. The use of this equation avoids the techniques of image processing and simply depends on the ratio of the distances of the points of marker to determine if a person is blinking. Thus;

which will be used to verify the characteristics of human presence (second part of the authentication) as described in Section 4.3. So, if all these checks are correct, one can indeed conclude that it is a real person and not a spoofed person that has been identified.

The usual approach entails using a dataset of random images to train the model in as much as the images are of good quality (resolution). However, this model performs poorly in the specific case of human faces, that is why we decided to train our model specifically on face images. To do so, we replace the dataset of objects with human faces. In our case, the faces have been retrieved in the label faces in the wild

testing, respectively. This gave us a set of 9924 images for training and that we have separated into two sets with a ratio of 3:1, for training and 3309 images for testing. The model was then trained on the dataset with 20 epochs.

The fundamental idea behind this network usage is to reconstitute an image of better resolution by the intuition developed by convo- lutional neural networks. The resulting process differs from a simple zoom because its algorithm adapts values of the image matrix on a new dimension which generally conduce to loss of certain feature elements. On the other hand, the super-resolution approach uses an initial image of little resolution and reconstitutes a similar image of better resolution. It, therefore, highlights hidden details enhancing in this case, features extraction.

The idea is to illustrate that the super-resolution principle increases the performance of face detection. Consequently, the summary of images detected as well as their corresponding percentages are reported in Table 1. We notice that in each case, the number of faces detected with super-resolution is greater than the number detected without. We also notice that the higher the input image resolution, the better the output image after the super-resolution and an acceptable image resolution is

This module is essentially dedicated to features extraction on input images enabling unique representation of each one. We have seen in the literature that many models exist but two have captured our attention: FaceNet, VGG and Resnet. This choice is motivated by the ratio between the results obtained and the time taken. Consequently, we did tests on the LFW with three models FaceNet, VGG and ResNet-50 the results obtained are presented in Table 2

This is the easiest step once the features have been extracted. It involves matching a features vector of an input image to a features vector belonging to a class in the knowledge base. The native approach used to this effect is the KNN (K-nearest neighbor) algorithm which computes the distance between each input image features vector and all images in the knowledge base and returns the class with the lowest distance. Another approach is the use of a support vector machine (SVM) trained to efficiently separate data for further use that can be either classification or regression. Even if its execution time is negligible once the model has been trained, its performance decreases considerably when the number of classes increases or when data are not linearly separable. We, therefore, choose to use this approach which is the use of a trained softmax layer that takes into consideration details of the features vector and uses less time for execution as well as SVM.

the training softmax layer with normal situation and parameters, the second is the training with the introduction of the margin layer, the third is the training with the use of the CRelu module as the activation function, and the fourth is the combination of the previous two. Fig. 4 and Fig. 5 respectively present accuracy and loss evolution during training and validation steps. We can easily notice a better stabilization of our combined approach in the two cases (evolution of precision and dropping of loss).

If the images contain some degree of tilt or rotation the network model chosen which is Resnet-50 model is used. This model handles any rotation, pose and age variation. It was downloaded then pre- trained, and after a fine-tuning process is done on our data set and a fully connected layer for feature extraction is added. Consequently, the features extracted are independent of transformations.

Formally, authentication comprises the verification that an indi- vidual is effectively the one he claims to be. In the frame of video surveillance, we will understand authentication to be any mechanism targeting the attestation of effective presence of the identified indi- vidual as well as any reinforcement decision-making principle. To do this, we first proceed to a verification of information. Concretely, we use features extracted by the FaceNet network, compute the distance (Euclidean) between this vector and the feature vector of the individual in the knowledge base. Note that a feature vector has previously been computed for everyone in the knowledge base. If the computed

The architecture of the network is presented in Fig. 7. It is an architecture inspired by the Alex network where pooling layers have been modified so that it can take a low image dimension (64 in our case) to increase the execution speed. Also, rather than training on one dataset, three famous datasets have been used namely, NUAA (Nanjing University of Aeronautics and Astronautics) dataset, Replay attack dataset and Casia dataset. This led to the increase of network generalization capacity and even outperforms one recent model with a very deep architecture. The choice of Alex network is motivated by the work of [39] which focuses on the sizes of the different kernels, com- pared to [40] which gained all its efficiency from its number of layers. We go in the same direction by also focusing on the dataset used which is why to increase the generalization capacity, we use a combination of several datasets. Moreover, this choice was also motivated by the training time and the execution time which are measured according to

Finally, a third principle is applied which entails the detection of eye blinks. We use the principle stating that a person cannot stay for a certain amount of time in front of the multimedia sensor without blink- ing his/her eyes. Consequently, every image that does not contain one or more eye blinks within a given amount of time will be considered as a spoofed image or video. To achieve this, the landmarks approach is used to localize opened/closed eyes, so, by counting the number of blinks, we are done. Also, we extend the normal principle stating that eyes blink together by considering the blink of each eye separately because one can be injured on one eye or cannot have both eyes at all. Furthermore, we need to insist on the fact that this principle is applied if and only if the face has previously been detected as genuine. This gives rise to the flow chart in Fig. 6 illustrating how the proposed blinking principle works compared to the native one.

Third step: The Casia dataset has in the training set, 160 fake videos and 80 real videos, and 240 fake videos and 120 real videos in the testing set. Proceeding the same way, we extracted 11 images per fake video and 21 per real video. This gave a total of 1760 fake images and 1680 real images. These images were later

/ig. 8. Evolution of accuracy and loss during training phases in four scenarios; (A) Training using Inception model and NUAA dataset, (B) Training using adapted Alex model and NUAA dataset, (C) Training using adapted Alex model, NUAA and Replay dataset, (D) Training using adapted Alex model, NUAA, Replay and Casia dataset.

rather than valid. This process has progressively been reinforced by the Replay attack and Casia datasets. The results of different tests after training are presented in Table 7. Also, the training time and the execution frequency (frame per second) of each method is recorded in Table 6.

We can see that, with a lighter architecture as Alex network, we were able to achieve good performance compare to a deeper one such as Google Inception architecture. However, recent models exist in the literature were one of the most common is Resnet family architecture but due to their depth there are not yet widely used in liveness detection because usually and architecture with less layer is sufficient to have good results. Nevertheless, we train our dataset on one of it variant which is the Resnet-50 [12] architecture and the results obtained during training is presented on Fig. 9.

identified by a diagnostic test. It suggests how good the test is at iden- tifying normal (negative) condition. Finally Accuracy is the proportion of true results, either true positive or true negative, in a population. It measures the degree of veracity of a diagnostic test on a condition.

we do not have a real person in front of the multimedia sensor. The number of seconds is detected empirically and according to the type of application we intend to develop. Figs. 12 and 13 illustrate how we have applied the formula to detect a blink on our input image with the normal blinking process and with the proposed blinking process. As we indicated previously, we do not rely on the strict application of its formula, instead, we first check if the face is genuine or spoofed. If it is genuine, rather than considering both eyes blinking together, we

While FAR and FRR focus on the capacity of the approach to reject positive samples and should equal to 0 in a perfect scenario, TPR and TNR on the other hand focus on the ability of the model to accept true samples. This should be equal to 1 in a perfect scenario. We can easily see that our proposed approach outperforms the inception model in all the cases. Also, the more we combine datasets, the better the results in terms of accuracy; this is highlighted in Fig. 10 where we have successfully tested the NUAA dataset, Replay dataset and Casia dataset on Inception train with NUAA, native Alex train with NUAA, Alex model train only with NUAA, train with NUAA and Replay and train with NUAA, Replay and Casia.

Another name of TPR is Sensitivity. It is the proportion of true positives that are correctly identified by a diagnostic test. It shows how good the test is at detecting a disease. Also another name to TNR is Specificity which is the proportion of the true negatives correctly

It can be noticed that in the proposed approach, we can detect a blink when one eye has an eye aspect ratio less than the threshold aspect ratio (Fig. 13 Line 3) which is not the case in the original approach that focuses on two eyes blinking (Fig. 13 Line 2). On the other hand, in case both eyes have an eye aspect ratio greater or equal to the threshold aspect ratio, blink detection is achieved by the approaches without any problem (Fig. 12 Line 2 and 3). The input images used are frames taken from the talking face dataset [41] which is a dataset constituted of 5000 frames taken from a video of a person

studied related work on face representation. Later, the FaSIVA proposed method was detailed, where each of its parts was clearly illustrated. This considers three parameters namely the quality of the image, the pattern extracted from the image which is the combination of the pattern extracted from two popular models: the Resnet-50 and Facenet, and finally, a proposed authentication mechanism that takes into consideration two metrics; eye blinking and spoofing verification. A section dedicated to the implementation of FaSIVA and all the steps we went through to train our models which were followed by a testing phase were presented. It was proven that the proposed signature is valid, efficient and robust. This is because it no more focuses only on recognition but also handles operations that enhance image quality as well as authentication mechanisms to reinforce the recognition process and prevent spoofing attacks. From the results obtained, we are able to improve face recognition pipeline by enhancing image quality in case of low resolution images and prevent spoofing attacks of the system using liveness detection. For future work, we intend to investigate more on blinking detection, to accurately detect blinks even in non-frontal faces and in occluded faces. Now that the signature is well defined we are working inserting the ability of detecting adversarial attacks.

