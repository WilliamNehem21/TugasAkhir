1992) was used to develop a model for forecasting weekly solar radia- tion (Prasad et al., 2020), and the non-dominated sorting genetic algo- rithm (Deb et al., 2002) was used in the development of a long-term precipitation model (Ali et al., 2020).

In summary, crop models are widely used to predict plant growth, water input requirements, and yield. However, the existing models are very complex and require hundreds of variables to perform accu- rately. Due to these shortcomings, the large-scale applications of crop models are limited. Machine learning techniques, such as DL, can be used to address some of these limitations; however, their applications are currently limited to qualitative assessments, such as computer vi- sion and speech recognition (Liu et al., 2017). Here, we tried to address the existing problems with crop models by not only developing a DL model for predicting yield and water requirements, but also by reducing the number of input variables from hundreds to only a few. This way, the DL crop models can be used for real-time and large-scale applica- tions, which are not currently possible. In this study, we evaluated the reliability of the DL crop model using about 10 million tested scenarios, which to the best of our knowledge has not been attempted before. This effort will mainstream crop model applications for predicting yield and water use at the regional and national scale. Crop models can help guide policymakers toward achieving sustainable water and food security in the 21st century.

Fig. 1 presents an overview of this study. First, 100 weather scenarios were combined with 100,000 random irrigation applications to create 10,000,000 scenarios in which crop production could be examined using a crop model. Next, a DL model was trained and tested based on these 10 million scenarios. Eight hundred input variables were intro- duced within the DL model, including four varying environmental vari- ables during a 200-day crop growing season. Three commonly used variable reduction techniques were then used to develop additional deep learning models based on the reduced number of input parame- ters and smaller structures. These models were tested and compared with the original deep learning model to identify the best new models with the lowest number of input variables. The impact of variable reduc- tion methods on the performance of DNN models with various architec- tures were discussed at length. In each round of the experiment, the number of input variables was reduced (400, 200, 100, 50, 40, 30, 20, and 10 variables), and the model's DNN structure was downsized (com- binations of 600, 400, 200, 100, 50, 40, 30, 20, 10, 8, 6, and 4 neurons by 50, 40, 30, 20, 10, 9, 8, 7, 6, 5, 4, 3, 2, and 1 layers). The process was iter- ated until a minimum number of architectures with comparable accu- racy to the original model was identified. Each computed model was

identified by three numbers in the form of Inputs-Neurons-Layers. For example, model 50–40-6 indicates a DNN with 50 input variables, 40 neurons per layer, and 6 layers. Finally, the best DNN model was com- pared against a new DNN model developed using a recent feature selec- tion method.

In this study, several environmental input variables were consid- ered, including precipitation and irrigation, maximum temperature, minimum temperature, and solar radiation. 800 variables were intro- duced for a 200-day growing season. Irrigation was added in combina- tion with precipitation to generate the total water applied in mm per day. To generate the model's training data, 100 different climate scenar- ios were considered. These climate scenarios were combined with 100,000 random irrigation scenarios and applied to a crop model, which generated 10,000,000 records of maize yields. Data related to cli- mate variabilities were produced by weather generators as described below.

Providing daily precipitation, temperature, and solar radiation in ad- dition to irrigation amounts, is essential for assuring the accuracy of crop model outputs. One-year field weather data was collected and used to generate different weather realization scenarios. The monthly temperature, solar radiation, and precipitation were collected from the nearest weather station to the study site (PRISM, 2011). A weather sto- chastic disaggregation tool was used to generate daily weather informa- tion from the environmental variables' monthly historical records (Hansen and Ines, 2005). This tool generated daily weather information by disaggregating average monthly data from historical records. To ac- curately simulate weather in the study area, 30 years of data were

used in the weather generator. The weather generator uses stochastic approaches to generate daily information for the locations of interest. In regards to precipitation, three conditions above the average, the aver- age, and below the average levels of precipitation are considered. Pre- cipitation occurrence is modeled by the Markov chain and the amount is sampled from hyperexponential distribution or probability density function of the random variable x (precipitation occurrence) given by Eq. (1).

and Melbourne, 2002). Less than 30% of the annual rainfall occurs from February through May, which is the main growing season (Fig. 2). The 30-year minimum and maximum temperatures of the loca- tion were 14.3 °C and 26.7 °C, respectively. On average, the highest tem- perature ranged from 32.7 °C in July to a low of 19 °C in January. Millhopper fine sand was the major soil type found in this region, known to drain moderately well.

After removing duplicate data from the 10,000,000 generated re- cords, 8,970,685 unique maize production data remained (Table 1). These data were used for further developing the DL model. Despite the fact that two computers were used in the study, a 24 core of Intel® Xeon® CPU E5-2680 v3 @ 2.50GHz with Quadro M6000 GPU and an Intel® Core™ i7-4770 CPU @ 3.40GHz with GeForce GTX 1080 GPU, the GPU memory did not allow for all available records to be used in training the DNN. To reduce the number of records and maintain diversity, the data were categorized by maize yield into 12 categories, each 1000 kg apart, then sampled to create 10 similar population distri- bution datasets. Table 1 shows the number of data records in each dataset.

namically model over 40 different crops and has been widely used in the past 30 years by researchers and academic institutes worldwide (Hoogenboom et al., 2015; Jones et al., 2003a, 2003b; Nurudeen, 2011). According to the DSSAT website (Hoogenboom et al., 2019), the model has been used by more than 14,000 researchers, educators, consultants, extension agents, growers, and policy/decision-makers in over 150 countries. DSSAT's software application package includes soil, weather, crop management tools, and experimental data. DSSAT simulates and models crop growth, development, and yield as a func- tion of the soil, weather, and plant dynamics.

To obtain data for the purposes of DNN model training, a maize irri- gation experiment was setup in DSSAT and calibrated based on the re- sults from the experimental study site at the Irrigation Research Park. The growing season comprises 200 days (Hoogenboom et al., 2015). The DSSAT input files were setup using maize cultivar (McCurdy 84aa) with planting and harvesting dates of February 16 and May 7, re- spectively. Ten irrigation applications were selected to generate random scenarios within the growing season to further train the DNN model ir- rigation applications. Irrigations occurred within the growing season, with amounts ranging from 10 mm to 250 mm of water per day.

Extensive knowledge of climate, geology, and agricultural manage- ment practices is needed to accurately operate typical crop models. In addition, the application of these types of models on a large-scale is lim- ited by model complexity. To address these limitations, this study was aimed to show the potential for applying DL techniques to crop models. The DL architecture that was used in this study was a DNN with a Multi Layers Perceptron (MLP) architecture (Fig. 3). MLP is a feed-forward DNN and was selected for this study, since it has been shown to success- fully generate solutions for classification problems (Deng and Yu, 2014). In this large-scale analysis, we were interested in estimating the pro- duction class rather than the actual yield due to the high level of uncer- tainty for individual fields. To make the production estimation more reliable, the maize production level was grouped into 12 classes (ranges from 0 to 12,000 kg/ha), with each class of input representing a range of 1000 kg/ha yield. For example, class 0 represents 0 to 1000 kg/ha, and

class 1 represent 1000 to 2000 kg/ha. Different combinations of hidden layers with neurons (computation nodes) within each layer were used in the DNN to form the crop models. Activation functions of the same type were assigned for all neurons. The TanH (tangent hyperbolic) acti- vation function was used for all hidden layers and the SoftMax (normal- ization constraint on the total output probability function) activation function was used for the output layer (Costa, 1996). Once the DNN structure was created, training is necessary and was performed as detailed in the next section.

All models were trained using the first dataset of 900,000 records and 10% of the dataset (90,000 records) were used for validation throughout the training process. The backpropagation (Liu et al., 2017) method and the gradient descent (Baldi, 1995) algorithm were also used to train the network by minimizing a defined cost (Baldi, 1995). In this study, the negative log-likelihood equation Eq. (3) was used as a cost function (Friedman, 2002). The learning rate was fixed at 0.01, and the mini-batch size was 1000 records in all training runs. After reaching the lowest validation error, training was continued for 100 epochs to ensure that it was not trapped in the local minimum.

To improve training performance and reduce the computational power required for developing the DL models. Among the numerous variable reduction techniques, some of the most commonly used tech- niques in environmental and agricultural studies were applied (Woznicki et al., 2015). The following three variables reduction methods were evaluated for preprocessing the data.

In theory, a Bayesian model explains a response variable (output) with a (large) number of explanatory variables (inputs). The Bayes- ian Variable Selection method selects a small subset of variables that can be inferred and used to explain a large fraction of the variation present in the response. In many cases, the variable selection is done by specifying the variables; the variable selection task is to es- timate the marginal posterior probability of whether the variable should be included in the model or not (O'Hara and Sillanpää, 2009). Several Bayesian Variable Selection software tools are currently available, such as BayesFactor, BayesVarSel, and BMS (Forte et al.,

To compute the required time for each epoch in the DNN structure with l layers and n neurons per layer, it was assumed the total calcula- tion time of each layer in parallel for the feed-forward and feed- backward process on the GPU is Tl second. To feed the next layer, the neuron's output between GPU memory and machine main memory was transferred in Tn second. Considering the training process, Eq. (4) calculated the time consumption for one epoch.

Spearman Rank Correlation method calculates the ranking cor- relation between each variable and the output (Eq. (5)). In this method, it was assumed that the variables with higher cor- relations would have a greater impact on outputs and should be considered first in the model.

Principal Component Analysis (PCA) feature extraction is an orthog- onal transformation for converting correlated variables to a smaller set of uncorrelated variables. PCA feature extraction method uses ei- genvalues of XT X to calculate linear transformations between these two sets. This method is called feature/variable extraction (Khalid et al., 2014).

In this study, a python library (sklearn.decomposition.PCA) was used to perform this analysis. This library used the LAPACK imple- mentation of the full Singular Value Decomposition (SVD) or a ran- domized truncated SVD, a method introduced by Halko et al. (2011).

Min-redundancy and max-relevance (mRMR) (Menger et al., 2018) is a novel and popular method that was initially introduced by Peng et al. (2005) and then improved by Bugata and Drotar (2020). The mRMR algorithm selects a set of explanatory variables with the highest relevancy and the lowest redundancy level to describe the output vari- able. Consequently, the most dependent variables are identified in a large set of variables, which ultimately results in better classification. Through reduced input variable redundancy, a smaller model with equal or better performance can be obtained.

Research by Bugata and Drotar (2020) showed that the max- redundancy is not always equivalent to max-dependency as was assumed by Peng et al. (2005). Therefore, Bugata and Drotar (2020) suggested that by adding an objective to the algorithm to maximize de- pendency, the overall performance of the mRMR algorithm can be im- proved. In fact, applying the revised mRMR algorithm resulted in

The original DL model was trained and tested using 800 inputs. The inputs consisted of four environmental variables varying over a 200-day crop growing season and all models were trained on the first 900,000 recordsets. Fig. 4 and Table S1 (Supplementary Materials) present the accuracy of the original DL models with different structures. The predic- tion accuracy results are shown in the “Training Set” columns and the average accuracy rates from the nine test sets are shown in the “Test Sets” columns. The results showed that in smaller DNN structures (50 to 100 neurons per layer), more layers resulted in lower model accu- racy. This behavior was previously observed by Schmidhuber (2015), who noticed decreasing accuracy from exploding/vanishing gradients. The accuracy rates in the DNNs with small structures were significantly lower (30% to 70%) than those with larger structures (400 to 1000 neu- rons per layer) (75% to 80%), which can be seen in Fig. 4. This behavior shows that a DL model based on a large dataset with 800 inputs vari- ables needs a large structure to perform reasonably well.

In this study, the accuracy reduced at a lower rate in DNNs with more than 400 neurons per layer compared to smaller DNNs with an in- creasing number of layers. However, larger structures require more computational time and more powerful hardware to perform at an ac- ceptable level. It should be noted that accuracy rates in training sets and test sets were identical, indicating that the models were not overfitted.

Table S2 and Fig. 5 show the accuracy training and test sets of the DNN model predictions with 400 input variables. Evidently, the result with the Bayesian and Spearman Rank Correlation methods in the train- ing set and the average of the nine test sets were almost identical. The similarity indicated that the models were not overfitted. Furthermore, the PCA feature extraction method displayed greater accuracy with the training set but poor performance with the test sets. As an unsuper- vised feature extraction method, PCA identifies and extracts the least correlated features from the input dataset (Calesella et al., 2021). Mean- while, the developed DL models based on this extraction method were overfitted during the training process.

Additionally, Fig. 5 shows that the 400 neuron DNN with many layers had a lower accuracy with the Bayesian Variable Selection method. Similar results were observed with the Spearman Rank Corre- lation, but the Bayesian Variable Selection method showed a decrease in accuracy when the number of layers increased, while the Spearman Rank Correlation method did not. This indicated that the Spearman Rank Correlation method was less affected by the vanishing/exploding gradients issue. The PCA feature extraction method showed almost no sensitivity to an increasing number of layers. Although it reached a sig- nificant prediction accuracy (more than 97%) with the training set, poor performance on test sets showed the model was highly overfitted. Bayesian Variable Selection and Spearman Rank Correlation methods had the same prediction accuracy on both training and test sets of DNNs with 600 neurons per layer. For the PCA feature extraction method, the prediction accuracy on training sets were increased on 600 neurons per layer, demonstrating how the method became less reliable due to overfitting with larger DNN structures.

number of layers in the DNN with 200 neurons per layer. However, for 400 and 600 neurons per layer, the accuracy rate remained almost con- stant for all three variable reduction methods. It can be observed in Fig. S2 that 100 input variables' accuracy rates were constant for all the different DNN structures except for the PCA feature extraction method, which had a comparable accuracy rate to the 400 inputs models. This illustrates that improvements in accuracy were established by increasing the number of layers in the training set without improving test set performance.

Fig. S3 show the accuracy validation result for these models on the train- ing sets and test sets with 50 input variables, which turned out to be identical to the results of 100 input variables. This result shows that the three variable reduction methods successfully reduced input vari- ables and can be expected to produce models with small DNN structures at acceptable prediction accuracy levels. In the next step, the DNN struc- ture was reduced in order to find out if these methods would work with smaller DNN structures.

To understand the impact of the number of hidden layers and vari- able reductions on the DNN model's accuracy, the number of layers was limited to less than 10 (1 to 9) for three different sets of neurons (i.e., 200, 400, and 600) and 200 input variables (Table S6).

Fig. S4 indicates that with 200 inputs and the proper number of neu- rons per layer, the accuracy with 1 hidden layer is almost the same as the large DNN structures with several layers. This shows the robustness of all three chosen variable reduction methods on the system. In other words, a shallow DNN (with less than 10 layers) with a higher number of neurons per layer (200, 400, and 600) performed as well as a deep DNN (with more than 10 layers) with several hidden layers (10, 20, 30, 50 hidden layers). However, similar to previous results, the models developed based on the PCA feature extraction method had excellent accuracy on the training sets and poor performance on the test sets.

Meanwhile, DL models based on the Bayesian and Spearman vari- able reductions methods are more consistent in performance between testing and training sets, while the DL models developed using the PCA variable reduction method are less robust. For example, in section C of Fig. S5 the test set shows a considerable drop in accuracy compared to the training set.

Large DNN structures require significant time for training their net- work (Sun et al., 2019). Therefore, the goal of this section was to identify the minimum number of input variables that can be used to develop a DL model with similar accuracy as the original model, with 800 input variables. Here we considered four models with 10, 20, 30, and 40 var- iables. Table 2 shows models by 10 to 40 inputs and 1 to 9 layers predic- tion accuracy results. The highest accuracy for all three variable reduction methods was obtained using the DNN with 10 neurons per layer, as highlighted in Table 2. Among these three methods, only the

The model with 30 inputs, 10 neurons, and 5 layers (50 computa- tional units) created by the Bayesian Variable Selection method had the same accuracy as a model with 800 inputs, 400 neurons, and 10 layers (Table S1 with 4000 computational units). This suggests that the DNN structure developed using the Bayesian Variable Selection method, which was 80 times smaller, with 1/27 the number of input variables, could have the same accuracy as the larger model. However, it took three weeks to calculate the posterior probability vector using 900,000 simulated crop yield datasets. This hurdle makes the Bayesian Variable Selection method less useful for fast applications.

The Spearman variable selection method, with more than 200 input variables, was acceptable with larger DNN structures since they had the same accuracy as the Bayesian Variable Selection model with 200 neu- rons per layer. However, the Spearman produced model reduced the prediction accuracy when the number of neurons per layer was re- duced. This method was found to work well with larger input variables and DNN structures compared to the Bayesian Variable Selection method. In addition, the processing time for variable selection was much faster (~10 min) than the Bayesian Variable Selection method.

0.86 s, which is 193 times faster. A decrease in the overall runtime of the DNN crop model can help with the application of these types of models at the large scale that is necessary for policymakers to make an informed decision at the national and international levels.

ables from the 800 inputs to develop the DNN models comparable with the most accurate model developed by the Bayesian variable selection method. Fig. 6 shows the comparison between these two methods. The DL models computed by mRMR are usually around 10% less accu- rate than the similar models developed based on the Bayesian variable selection method. The lower accuracy established a clear superiority of the Bayesian variable selection methodology to the mRMR.

A cropping system modeled by DL with many input variables pro- duces a large DNN structure, which is very computationally intensive. Additionally, the training process required for large DNNs can be very time-consuming. This shortcoming has limited their usage for large- scale applications. However, the utilization of an efficient deep learning crop model can be a game-changer. For example, a regional-scale irriga- tion scheduling system can be developed using a hybrid system com- prised of DL-based crop models, a weather forecasting system, and an optimization algorithm.

In this study, we examined the possibility of developing a simpler deep learning model with comparable accuracy through the application of different variable reduction methods (i.e., Bayesian Variable Selec- tion, Spearman Rank Correlation, and PCA variable extraction method). The Bayesian Variable Selection method was identified as the most robust method of the three evaluated in this study. However, calculating the posterior probability for each variable is very time-consuming. The Spearman Rank Correlation was ranked the second best with similar ac- curacy to the Bayesian Variable Selection method. The performance of these models were also examined against the recently improved mRMR technique. In general, the models based on the mRMR technique are less accurate than the ones based on the Bayesian and the Spearman Rank Correlation techniques. Finally, the DNN models that were devel- oped based on the PCA feature extraction method had the highest accu- racy during the training tests, but the lowest levels during testing.

Nurudeen, A.R., 2011. Decision Support System for Agro-technology Transfer (DSSAT) Model Simulation of Maize Growth and Yield Response to NPK Fertilizer Application on a Benchmark Soil of Sudan Savanna Agro-ecological Zone of Ghana (Doctoral Dis- sertation). Kwame Nkrumah University of Science and Technology Kumasi.

Seligman, N., Van Keulen, H., 1981. Papran: a simulation model of annual pasture produc- tion limited by rainfall and nitrogen, simulation of nitrogen behavior of soil-plant sys- tems: papers of a workshop. Models for the Behavior of Nitrogen in Soil and Uptake by Plant, Comparison Between Different Approaches, Wageningen, the Netherlands, January 28–February 1, 1980. Centre for Agricultural Publishing and Documentation, Wageningen, Netherlands, p. 1981.

