Each obtained model instance was trained on the prepared dataset, in parallel with a five-fold cross validation procedure. The latter consists in training the instance on five-split configurations. In each one, one split (1/5) is reserved to validate the model after being trained on the four other ones, which provides more realistic performance estimates for the evaluation (Refaeilzadeh et al., 2009). A variant of an iterative gradient-based optimization algorithm has been adopted i.e., the Adam optimizer (Kingma and Ba, 2017). For each model instance, the parameters were randomly initialized; afterwards, a series of iterations (or epochs) –whose number is also a hyperparameter– were per- formed. Each series comprises:

A Rectified Linear Unit (ReLU) activation function expressed as fol- lows f : z → max (z, 0) was implemented in all the hidden layers' neu- rons. ReLU is a widely used activation function in machine/deep learning models since it involves less computations compared to other used functions such as sigmoid and tangent hyperbolic (Apicella et al., 2021). Since the model has to output a vector of binary numbers, each

The core defining hyperparameters within the architecture of an MLP are the number of layers and neurons within each layer, especially the hidden ones. In this study, two groups of models have been con- structed: (1) The first with one hidden layer where the number of neu- rons has been tuned between 1 and 9; (2) The second with two hidden layers where the number of neurons within each layer has also been tuned likewise. The total number of trainable parameters of each group are presented respectively in Tables 3 and 4.

The architecture design phase aimed at finding the model that will score the highest mean accuracy (averaged over the five-fold cross val- idation procedure) along with being the less complex i.e., with the min- imum number of parameters for a further implementation in a microcontroller. Tables 3 and 4 show how increasing the layers' number and the contained neurons generate more parameters. For the case of two-layer MLPs, the modifications induced in the hidden layers operate symmetrically. A tuple of parameters (a, b), chosen respectively for layers 1 and 2, generates identical parameters as the inversed tuple (b, a).

As shown in Fig. 8, compared to a traditional labeling approach, multi-labeling enables a huge gain in terms of neurons within the out- put layer, which in turn, reduces the number of generated parameters by the layer substantially. For 5 climatic input variables (as it is the case in our work), adopting a traditional labeling approach would have resulted in MLPs with 31 neurons in the output layer. This number would have escalated quickly to 1024 neurons for 10 input variables. Based on this, we content that multi-labeling is a highly efficient ap- proach for designing lightweight and scalable models within the ad- dressed thematic.

For the first and second model groups, the heatmaps in Figs. 9 and 10 illustrate respectively the final mean values of the cost, the validation cost, the accuracy and the validation accuracy during the training/five- fold cross-validation procedure. For Fig. 9, the upper axis represents the number of neurons within the hidden layer. For Fig. 10, it represents the number within hidden layer 2, while the left axis contains the neu- rons' number per hidden layer 1.

For both groups, the models' performance improves with the con- struction of more complex MLPs. More neurons yield generally better performance and vice versa. This observation is quite logical since hav- ing a more complex architecture enables to fit more complex data pat- terns. On the other hand, trying to fit a very simple model to a complex data distribution yields poor performance, since the trainable parameter number is insufficient. We will denote the model instances within group 1 by model (i) where i is the number of neurons within the hidden layer. For group 2, each instance will be denoted by model (i, j) where i and j are respectively the number of neurons per hidden layers 1 and 2.

Fig. 10. Summary of the obtained (a) cost, (b) validation cost, (c) accuracy and (d) validation accuracy for the second group of models in function of the implemented neurons' number within hidden layer 2 (upper horizontal axis) and hidden layer 1 (left vertical axis).

the evolution of its mean cost and accuracy along the training/cross- validation process. The mean loss converges to a value of 0.16 while the mean accuracy to 0.97. No generalization gap is observed between the training and cross-validation curves which evolve tightly close to each other. This confirms that the model's learning has been performed within an optimal regime without underfitting nor overfitting the data. A further evaluation of model (7,8) on the test set yielded an accuracy of

0.96. By featuring only 5 neurons in the output layer, the model has a total number of 151 parameters. This number would have been 385 if a traditional labeling approach has been adopted. The latter would have required 31 output neurons to be implemented to handle the five-action control strategies.

This work focused on developing a tinyML-oriented solution for a machine learning-based autonomous greenhouse microclimate control based on multi-variate sensed data (5 variables). The MLP architecture has been selected and various hyperparameters have been experimented while training 90 model instances. A robust five-fold training/cross-validation procedure on a balanced dataset was also car- ried out to find the optimal and best-performing model. From an evalu- ation metric-based perspective, multiple MLPs can fulfill the task; however, from a tinyML perspective, only the model with the less pa- rameter number has to be kept for optimized computations and

advantage thanks to the adopted tinyML approach), it still can be cate- gorized –from a broader definition perspective– under the deep learn- ing field. The used multi-labeling approach enabled a parameter reduction of 60%. Thanks to its lightness, the model also eliminates the need for using any regularization technique to prevent overfitting the data.

Although the obtained performance metric can be contended as satis- factory. We consider that it may be ameliorated further. Herein we suggest: (1) A model centric avenue, which focuses on modifying the parameters and hyperparameters; or a (2) Data-centric avenue which focuses on collecting additional and more refined data.

Ilham Ihoume: Data curation, Formal analysis, Writing – original draft. Rachid Tadili: Supervision, Conceptualization, Methodology, Resources, Validation. Nora Arbaoui: Writing – review & editing. Mohamed Benchrifa: Writing – review & editing. Ahmed Idrissi: Writing – review & editing. Mohamed Daoudi: Writing – review & editing.

