Recent years have seen rising research interests in automatic description of images and videos in natural language using deep learning techniques. Recent methods are inspired by the encoder-decoder framework used in machine translation [1]. These techniques use Con- volutional Neural Networks (CNNs) as encoders to compute fixed/variable-length vector representations of the input images or videos. A Recurrent Neural Network (RNN), e.g., vanilla RNN [2], Gated Recurrent Units (GRU) [3] or Long Short Term Memory (LSTM) networks

In this work, we present the first systematic analysis of the encoder- decoder framework components with the aim of revealing the contri- bution of each component on the quality of the generated captions. Our analysis is performed by studying the effects of popular choices for each component while keeping the remaining components fixed. We also include the choices of important hyper-parameters in our analysis. The main contributions of this paper are as follows:

We evaluate and quantify the role of CNN architecture employed to extract features in the video captioning framework. We analyze 5 CNN models encompassing varying depths and structures. We observe that networks with stronger expressive ability perform better. We empirically demonstrate that the choice of CNN model in the video captioning framework can lead to performance gain up to

Most recent methods incorporate various adjustments in the encoder- decoder framework. Wang et al. [29] proposed multi-model memory to model the long-term visual-textual dependency. Chen et al. [30] proposes a frame picking module to select a compact frame subset to represent the video. This enables the encoder-decoder architecture more applicable to process the real world videos. GRU-EVE [25] employs Short Fourier Transform on the encoder output to enrich the video representation with temporal information. Zhang et al. [31] models salient objects with their temporal dynamics to improve the architecture performance. Zheng et al.

[32] propose syntax aware action targetting module to explicitly learn actions. In order to understand the strength of each technique employing encoder-decoder architecture, it is important to understand the role of each module in encoder-decoder. For that matter, we put together the performance contribution of each module, forming the basis of encoder-decoder architecture, towards overall framework performance.

We first introduce the setup used in our empirical analysis of the video captioning framework. For evaluation, we divide the framework into four core components, namely CNN model - that encodes visual features of videos, feature transformation - that transforms visual features to be used as inputs by the language model component, word embeddings - that provides numerical representation of words in the vocabulary, and the language model component, which decodes the visual features into natural language descriptions. Extensive experiments are carried out by varying the methods for each component of the framework and analyze the captioning performance of the overall pipeline.

Description Evaluation (CIDEr) [36]. These metrics are known to comprehensively evaluate the quality of automatically generated cap- tions. We briefly discuss each metric in the below text. For details on each metric, pros and cons, and for their comparisons, we refer the readers to original papers and survey paper [37].

We perform experiments on the popular video captioning dataset MSVD [41]. This dataset comprises 1, 970 YouTube short video clips, primarily containing single action/event in a video. Each clip duration varies from 10 to 25 s. Each video is associated with multiple human annotated captions. On average, there are 41 captions per video clip. For bench-marking, we follow the data split of 1, 200, 100, and 670 videos for training, validation and testing respectively. This is a widely employed protocol for evaluation using MSVD dataset [8,9,29].

of generated sentence is an exact match to the reference sentence. In order to compute corpus level score, aggregated values of the constituent components i.e., P, R, and p are taken. Moreover, highest METEOR score is selected if there are multiple reference sentences against a generated sentence.

In this section we briefly describe the vanilla encoder-decoder ar- chitecture which is backbone of most video captioning methods. Encoder encodes the input video frames by employing pre-trained convolutional neural network(s). The extracted features from encoder are then utilized as input to RNN decoder to generate the caption. Formally, for a given video

Convolutional Neural Networks (CNNs) can be readily applied to images and videos. In deep learning based encoder-decoder framework for captioning, CNNs dominate the encoder part. Due to the significance of a encoder role, the choice of CNN models can affect the overall captioning performance significantly. Hence, we first analyze the five most commonly used CNN models in captioning, namely; C3D [44], VGG-16 [45], VGG-19 [45], Inception-v3 [46], and InceptionResNet-v2 [47]. Among these models, C3D - a popular example of 3D-CNN, is a common choice [23,48] because it can not only process individual frames, but also short video clips. This is possible due to its ability to process tensors with an extra time dimension.

Moreover, we observe that the effect of CNN architecture in the video captioning framework follows the similar trend as seen in image classi- fication tasks. It is generally believed that deeper networks tend to perform better, we see that VGG-Nets from 16 to 19 layers slightly improve the performance of captioning framework. However, after a certain depth is reached in the CNN, increase in number of layers do not increase the classification accuracy of the model. The problem was resolved by introducing residual blocks that resulted in networks with stronger expressive ability and hence performed better in classification tasks. Similarly, we see the same trend in our experiments where we achieve better performance by employing these networks in the video captioning framework.

It is also evident from the results that for temporal encoding, the performance of different models show a similar behavior relative to each other, which is also the case for the mean pooled features. For instance, with temporally encoded features, the best performing architecture still remains the best and vice versa is also true. The temporal encoding is providing a significant positive offset to the performance.

In this encoder-decoder framework, a word embedding is a vector representation for each word in the available vocabulary for video caption generation. Word embeddings are much more powerful low- dimensional representations for words as compared to the sparse one- hot vectors. More importantly, unlike one-hot vectors, word embed- dings can be learned for the captioning tasks. In captioning literature, two methods are commonly used to compute these vectors. The first approach is to learn the vectors from the training dataset while the lan- guage model is trained. In this case, one can initialize the embedding vectors randomly and compute the embeddings tailored to the captioning task. However, such vectors often fail to capture rich semantics due to the fact that captioning corpus size is often small for the purpose of training a language model. The second way to obtain these vectors is to use pre- trained embeddings that are learned for a different task and select those according to the vocabulary of the current task.

vectors. The other word embeddings do not have this property. For instance, with 9, 914 words of corpus vocabulary size in FastText, 8, 846 tokens are extracted from the pre-trained embeddings and the embed- dings for the remaining 1, 068 tokens are generated using character n- grams of out-of-vocabulary words. The resulting vectors are then merged to produce the final embedding vector. This strategy is certainly better than random initialization of the out-of-vocabulary words. With FastText at the top, glove840B and Word2Vec performs almost at par. Among all the pre-trained embeddings, glove6B proved to be the weakest.

In language models, given the type and size of data, depth of the model plays the pivotal role in effective learning. Where lower layers of a model learn to represent the syntactic information (parts of speech, grammatical role of words in each sentence etc.), semantic information (meaning of the words, contextual information) is better captured at the higher layers. As each layer learns different type of information, depth of models becomes important for effective language modelling. However, the modelling performance may start to deteriorate at a certain depth due to the data size limitation.

Appropriate hyper-parameter setting and model fine-tuning are well- known for their role in achieving the improved performance with deep networks. Here, we provide a study of a few important hyper-parameters relevant to the captioning task under the encoder-decoder framework. The reported results and findings can serve as guidelines for the com- munity for training effective captioning models.

We also experimented by fine tuning the model for 10 epochs on the pre-trained word embeddings. It was observed that in this case, the performance on BLEU and CIDEr metrics improved slightly with the fine tuning. However, performance on ROUGEL metric remained negligible. METEOR metric value showed mixed behaviour with no regular patterns. Dropout in Recurrent Layers: Dropout is a technique used in neural networks to prevent overfitting of the model during training. In recurrent

kens), dropout therefore does not have a significant effect on language model performance for this dataset, or the datasets of similar scale. We employed dropout in the recurrent layers of language model. However, it was observed that application of dropout did not improve the perfor- mance. In fact, it sometimes resulted in slight deterioration of the model performance. Based on the observed behavior, we can confidently recommend to avoid the use of recurrent dropout in a GRU language model, given the training data of MSVD size (or comparable) and model

60 % for CIDEr metric, if we choose the right visual feature encoding model. Similarly, when comparing among 2D CNNs only (second row), we see there are significant performance variations. These variations only resulted from varying the CNN model. Hence, we can conclusively argue that superior CNNs (with better representation power) can result in significant performance improvement for the captioning techniques.

training with random initialization. Moreover, we also experimented with fine tuning of the pre-trained embeddings for 10 epochs for the captioning task. However, we observed that fine tuning does not result in any drastic performance gain. We noticed that the performance of word2vec and glove840B mostly remain at par with each other. Compared to the visual feature encoder selection, we can see the per- formance gain by the informed selection of word embeddings are not negligible either. However, the right CNN model does have a dominant effect on the performance gain as compared to the word embedding selection.

