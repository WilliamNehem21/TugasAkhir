Transfer learning. One approach, a transfer learning technique, is based on the knowledge gained from a training dataset and is used for training a different but relevant task or field (Weiss et al., 2016). In this deep learning process, the first few layers are trained to define the characteristics of the task. The last few layers of the trained network can be removed and retrained with new layers for the target task. It re- fers to the situation whereby what has been learned in one setting is adopted to improve the optimization in another setting. With limited computational requirements, ResNeXt-101 achieved state-of-the-art power and time speed (Albawi et al., 2017).

Training a deep learning model with a small dataset is often insuffi- cient for its model's performance. Transfer learning is a process of pre- initialize a model using the weights obtained by training a different model on a larger, different, dataset. In the work conducted by Karimi et al. (2021), it was reported that although transfer learning reduced

Ensemble technique. Ensemble learning is one of the deep learning technologies that combine multiple primary learners through a fusion strategy to improve overall generalization performance (He et al., 2015). Ensemble learning has attracted a lot of attention because of its easy-to-understand structure and promising classification performance by combining more than one CNN model. Ensemble learning is a tech- nique that incorporates multiple models for final decision-making The ultimate goal of an ensemble is that by combining multiple models, the errors of a single model can be corrected (compensated for) by other models, making the overall score (prediction and classification) of the ensemble better than any individual participating model (Kawasaki et al., 2015).

The third approach is the digital image processing techniques of McNeely-White et al. (2020); Atila et al. (2021); Chambon et al. (2021). Zhou et al. investigated a technique for assessing the extent of hop disease in rice crops, using a fuzzy C-means algorithm to classify re- gions into one of four classes: no disease, light disease, moderate dis- ease, and severe disease. Their study achieved an accuracy of 87% in distinguishing cases in which a planthopper did or did not occur, while the accuracy in distinguishing four groups was 63.5%. Chambon et al. (2021) was to identify and classify six types of mineral deficiencies

in rice. The study used features such as texture and colour for a devel- oped specific multi-layer neural network. Both networks consist of a hidden layer with a different number (40 for texture and 70 forcolourr) of neurons in the hidden layer, in which 88.56% of the pixels were cor- rectly classified. Similarly, the same authors proposed another similar work that successfully identified two types of diseases (blast and brown spot) affecting rice plants (Chambon et al., 2021).

Zhou et al. (2013) reported an automatic identification and diagno- sis of rice diseases using CNN as a deep learning method. Using a dataset of 500 natural images of diseased and healthy rice leaves and stems cap- tured from the rice experimental field, a CNN network was trained to identify 10 common rice diseases. Under the 10-fold cross-validation strategy, the proposed CNN-based model achieved an accuracy of 95.48%.

Sanyal and Patel (2008) suggested a faster R-CNN approach, which seemed to be ideal for the detection of rice diseases due to its good speed and high accuracy. Shrivastava et al. (2019) also applied a CNN al- gorithm for rice plant disease classification using a transfer learning of deep convolution neural network. Using an AlexNet CNN model, the model was able to classify rice diseases with a classification accuracy of 91.37%.

Asfarian et al. (2014) developed a CNN approach for detecting dis- eases and pests (five classes of diseases, three classes of pests, and one class of healthy plants and others) from rice plant images. A total num- ber of 1426 images were collected that were captured using four differ- ent types of cameras and the system achieved a mean validation accuracy of 94.33%.

Akhter et al. (2019) also suggested a new stacked CNN architecture that used two-stage training to substantially reduce the model size while retaining a high classification accuracy. Several CNN architectures, such as MobileNet, NasNet Mobile, and SqueezeNet, were used. Experi- mental results showed that the proposed architecture achieved the de- sired accuracy of 93.3% with a significantly reduced model size, for example, 99% smaller than that of VGG16.

Despite the fact, Phadikar et al. (2012) observed that computer- aided rice disease detection and classification have received special at- tention, Asfarian et al. (2014) criticized for low accuracy rates using the rice disease detection models. Our literature review in this study also suggest that the classification accuracies by most of the existing methods are between 50% and 95% (Asfarian et al., 2014). Moreover, those achieving higher accuracies were usually tested with fewer dis- eases. The performance would deteriorate if more diseases were in- cluded. (Acharya et al., 2020) and (Huang et al., 2017) discussed the gap between the current capabilities of image-based methods for auto- matic rice disease identification and the real-world implementation needs.

The experiments in this study were conducted based on Google CoLab using the Keras library. TensorFlow which is one of the best Py- thon deep learning libraries available for working with machine learn- ing methods on Python was used. In this study, the original, transfer learning and ensemble models were trained using google collab Tesla graphics processing unit (GPU). TPU is available through the Google Collaboratory framework by Google. Initially, the colab framework pro- vides up to 12 GB random access memory (RAM) and about 360 GB GPU in the cloud for research purposes.

Data collection was the exceedingly cardinal quest for our research. We have put a vast effort to gather a great number of datasets. Since this research aimed to detect rice diseases, that mainly occurred in Bangladesh, most rice epidemic diseases found in the country were con- sidered. Therefore, the data collected from rice leaf images included a combination of the Rice Leaf Disease Dataset from the University of Cal- ifornia Irvine (UCI) Machine Learning Repository, a dataset from pub- licly available respiratory and a dataset collected from Bangladesh Rice Research Institute (BRRI). An example of rice leaves with various dis- eases is given in Fig. 2.

The final combined dataset contains nine (9) classes of rice diseases, with each class having one hundred (100) images for each type of disease. Images in the dataset are coloured images of various sizes and have a white background. The original images were divided into training and test sets with a ratio of 70:30 (see Table 1).

Image Acquisition: In this step, we downloaded the images from the targeted sites to provide as input. Images in the dataset were checked manually to identify if they had a white background. In the case where images (mainly from the BRRI) had coloured backgrounds, images were placed on a white background. If dis- ease symptoms such as spots, diseased colour, and diseased shape were not visible in an image, the image was removed from the dataset.

Image Augmentation: Image augmentation is used in this step. Image augmentation is the procedure by which an existing dataset is expanded by transforming the original dataset to cre- ate more new data, and in such a way that new data are also label-preserving (Sankupellay and Konovalov, 2018, Meeras Salman Al-Shemarry et al., 2019). The goal is to increase the var- iance of the dataset while ensuring that new data are meaningful and do not merely add unnecessary volume to the dataset (Sankupellay and Konovalov, 2018). When used in a machine- learning context, it can improve model generalization, make trained models more robust to unseen data, and increase model accuracy (Sankupellay and Konovalov, 2018).

With these aims, we conducted data augmentation in the training data. However, position augmentation such as scaling, cropping, flip- ping, rotation, and colour augmentation such as brightness, contrast, and saturation was deployed. Random rotation from −15 degrees to 15 degrees, rotations of multiple of 90 degrees at random, random dis- tortion, shear transformation, vertical flip, horizontal flip, skewing and intensity transformation were also used as part of the data augmenta- tion process. In this way, 10 augmented images from every original image have been created. Random choice of a subset of the transforma- tions helps augment an original image in a heterogeneous way.

In this study, each pixel value of images in the original and aug- mented images was first normalized dividing by 255. The images were then resized to a default size accepted by each model. In our experi- ment, input image resolutions were necessarily resized for all models of EfficientNet architecture due to our hardware limitations. Through trial and error, it was seen that the maximum allowed input size that our hardware resources were sufficient for the training of the EfficientNet model which has the highest number of parameters of 132 × 132. Therefore, the input size for all models of EfficientNet archi- tecture was set as 132 × 132 to evaluate all models under the same con- ditions. Table II summarizes the default image resolutions and the number of parameters defined for deep learning models.

All the models were trained for 175 epochs (iterations) with Early Stopping callbacks (patience = 10 iterations) Patience is the number of epochs with no improvement after which training will be stopped. An Adam optimizer, a combination of Stochastic Gradient Descent (SGD) with momentum and RMSProp (Root Mean Squared Propaga- tion, or RMSProp, is an extension of gradient descent and the AdaGrad version of gradient descent that uses a decaying average of partial gra- dients in the adaptation of the step size for each parameter.) were used for faster convergence with the parameters like learning rate was set at αα = 0.0001, β1β1 = 0.9, β2β2 = 0.999 and ϵ = 1 × 10 − 7ϵ = 1 × 10 − 7. The same optimizer was used for all three models and then the models were saved as .h5 files. The time taken for model training is −31 s (s)/epoch (Iterations) for DenseNet201 and 17 s/epoch for each of the models ResNet50V2 and Inceptionv3.

In this research standard deviation was used as a model performance metric since the dataset used in this experiment does not have any major imbalance. Categorical cross-entropy was used as a loss function for all CNN architectures since this work deals with multi-class classifi- cation. All intermediate layers of the CNN architectures used in this work have relu as the activation function while the activation function used in the last layer was softmax. The hyperparameters used are as fol- lows: the dropout rate was 0.3, the learning rate was 0.0001, the batch size was 64, and the number of epochs was 275. An adaptive moment estimation (Adam) optimizer was used for updating the model weights. All the images were resized to the default image size for each prior architecture.

Classiﬁcation: In this step, neural networks (DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, resnext101, and Xception) were used in the automatic detection of leaf dis- eases. The neural network was chosen as a classification tool due to its well-known technique as a successful classifier for many real applications. After the training model, the evaluation model was built for rice disease detection based on the highest probability of occurrence, the images of rice leaves were classi- fied into different disease classes using a softmax output layer.

Recall or Sensitivity is the accuracy of positively predicted instances describing how many were labelled correctly (Kawasaki et al., 2015). Recall tells us how many of the actual positive cases we were able to predict correctly with our model. The recall is a useful metric in cases where FN trumps FP. The formula of recall is given below (3):

Accuracy is one metric for evaluating classification models. Infor- mally, accuracy is the fraction of predictions our model got right. For- mally, Accuracy is the ratio of correctly labelled images to the total number of samples (Kawasaki et al., 2015). The formula for accuracy is given below (1):

F1-score, as an additional measure for classification accuracy, con- siders both precision and recall. F1-score is a harmonic mean of Preci- sion and Recall, and so it gives a combined idea about these two metrics. It is maximum when Precision is equal to Recall.

Specificity refers to the ability of a diagnostic test to correctly iden- tify a rice leaf that is healthy or free from disease. It measures the per- centage of true negative results. A highly specific test has a low false positive rate. However, in case of a highly specific test can be interpreted

Training loss is a measure of how well a model fits the training data. It quantifies the discrepancy between the predicted output of the model and the actual target values in the training set. The goal during training is to minimize this loss, which indicates that the model is learning to ac- curately represent the relationship between the input data and the cor- responding output targets.

Validation loss, on the other hand, assesses how well the model gen- eralizes to new, unseen data. It measures the discrepancy between the model's predictions and the true target values in a validation set or a portion of the training data that is held out for evaluation. The validation loss helps determine if the model has learned meaningful patterns or if it is overfitting.

Overfitting occurs when a model becomes too complex or too closely fits the training data. In such cases, the model may start capturing noise or irrelevant patterns from the training set, making it less effective at generalizing to new data. Overfitting is often characterized by a low training loss but a high validation loss, indicating that the model is not performing well on unseen data.

To combat overfitting, techniques such as regularization, dropout, and early stopping can be employed. Regularization methods help pre- vent the model from excessively fitting the training data by introducing penalties or constraints on the model's parameters. Dropout randomly deactivates a portion of the neurons during training, reducing the model's reliance on specific features or patterns. Early stopping stops the training process when the validation loss starts to increase, prevent- ing the model from further overfitting.

A confusion matrix is a table that summarizes the results of a classi- fication model by comparing the predicted labels with the true labels of a dataset. It provides a comprehensive view of the model's performance by displaying the counts of true positive (TP), false positive (FP), true negative (TN), and false negative (FN) predictions. Confusion matrices are valuable tools in evaluating and comparing different models, selecting appropriate thresholds, and understanding the trade-offs be- tween various performance measures. They offer a clear and concise summary of the model's predictive performance and are widely used in machine learning and classification tasks.

Support refers to the number of actual occurrences or instances of a particular class within a dataset. It represents the frequency or preva- lence of a specific class. Imbalanced support occurs when there is a sig- nificant disparity in the number of instances between different classes in the training data. For example, if one class has a much larger number of instances compared to another class, the dataset is considered imbal- anced. Imbalanced support can pose challenges in training classifiers and evaluating their performance. Classifiers tend to be biased towards the majority class due to the larger number of instances, resulting in lower accuracy or performance metrics for the minority class. This im- balance can indicate potential structural weaknesses in the reported scores of the classifier, as the overall performance may not accurately reflect its ability to correctly classify all classes. To address imbalanced

support, various techniques can be employed. Stratified sampling is one approach that ensures each class is represented proportionally in the training and evaluation datasets. This helps provide a more balanced representation of classes during model training and evaluation. Rebalancing techniques, such as oversampling the minority class or undersampling the majority class, can also be used to mitigate the effects of imbalanced support during training.

In this section, the performances of the six original individual CNN networks (DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, and Seresnext101) are presented. The classification perfor- mance of the models is first presented. The overall measures for those models are then discussed. Gathering in addition to the descriptors, pos- sible causes, and areas of opportunity for improvement of results.

The Precision, Recall, F1-score and Specificity obtained by the DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, and Seresnext101 models for each of the classes are shown in Table 3. Considering the precision values for each on the test dataset, DenseNet121, Inceptionv3, and MobileNetV2, architectures provide the best performance. The above table suggests that the DenseNet121, Inceptionv3, and MobileNetV2 models classified Blight, Leaf Blast and Tungro diseases with 99% accuracy. The Seresnext101 performed low precision having the lowest identification of the Bacterial blight leaf with only 56% accuracy. All models identified Hispa with an average ac- curacy by the six models. Seresnext-101 requires large amounts of im- ages for training to learn accurate representations compared with DenseNet121, Inceptionv3, MobileNetV2, resNext101, and Resnet152V. If the network receives less testing data, it provides lower classification accuracy. Moreover, the architecture and hyperparameters of the SEResNeXt-101 model could impact its performance. If the model archi- tecture is not suitable for the specific image classification task, or if hyperparameters such as learning rate, batch size, or regularization set- tings are not properly tuned, it could result in lower accuracy. Lastly, since the SEResNeXt-101 model is too complex and has too many pa- rameters relative to the size of the training dataset, leading to overfitting and reducing performance on new images (See Table 4).

In this section, the performance of four transfer learning CNN archi- tectures is presented. Table 6 shows the accuracies obtained in the test sets by DenseNet121, Seresnext101, EfficientNet and, Xception models. The test accuracies shown in Table 5 were calculated as the ratio of the number of correctly classified samples to the number of all samples. The DenseNet1121 model achieved the highest accuracy of 97%. However, the accuracy improvement from the original network to transfer learn- ing by the SeresNext101 network is mentionable. The network

The associated TP, FN, FP, and TN are shown in Table 8. For rice disease detection and classification, we applied the seresNext101 Model with a transfer learning approach as the model received the lowest accuracy in earlier experiments (Without transfer learning). In addition to the SeresNext101 model, we also selected DenseNet121, EfficientNet and Xception models for rice leaf disease detection and classification. As these are deep convolutional networks and we were interested to see if the models are useful for small-scale datasets. The confusion matrix of DenseNet121, Seresnext101, EfficientNet and Xception is shown in Fig. 4.

In this research, the ensemble stack is developed on three different original CNN models, Densenet121, EfficientNetB7, and XceptionNet. To accelerate the training process, we adopted a transfer learning strategy. In addition to this, the output from these models was sent to a post-processing block containing a fully connected layer followed by a dropout layer and a final logit layer for classifying the image. For better convergence of our models, we used a learning rate decaying strategy which divided the learning rate by 10 only when the loss stops decreas- ing for three continuous epochs and an early-stopping strategy that halts the training process after the learning rate decayed 5 times (Kawasaki et al., 2015).

The precision on ensembling suggests that the model received 99% on Bacterial blight, which was 98% with transfer learning and 56% on the original CNN model (see Table 8). Even though the F1-score had the lowest accuracy (53% in the case of Brown Spot using Seresnext101) the ensemble model had 95% in that case). However, the Precision, Re- call, f1 and Specificity result of CNN networks with the ensemble is shown in Table 8.

Fig. 5 shows the confusion matrix of the ensemble model. Fig. 6 shows the training accuracy and validation accuracy of the ensemble model of Densenet121, EfficientNetB7 and XceptionNet, where the x-axis represents the number of epochs and the y-axis represents the accuracy and loss percentages. Fig. 6 indicates that the training and validation data are split appropriately with no over-fitting.

Fig. 7 shows the training loss and validation loss over epochs by the ensemble technique. A loss function is used in CNN to optimize an architecture. The loss is calculated on training and validation and its in- terpretation is based on how well the model is doing in these two sets. It is the sum of errors made for each example in training or validation sets.

In this research, we performed an in-depth investigation of the per- formances of original individual CNN, transfer learning, and ensemble models. We compared the results of six different CNN-based models of DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V and SeresNext101 by applying them to the nine classes of rice diseases (see Fig. 8 for accuracy). The dataset used includes 14,118 rice leaf images. After image expansion through rotation, we obtained 34,992 images for training and 7884 images for testing. Among the original in- dividual networks, Densenet121 provides the best classification results in identifying rice leaf diseases. Bari et al., 2021; Nayak and Singh, 2021 also support the findings that Denesenet121 delivers relatively high ac- curacy. This is because, in DenseNet, each layer obtains a “collective knowledge” from all preceding layers as layers receive inputs from all preceding layers and pass them on to the next layers.

Our investigation suggests that transfer learning of deep learning models provides slightly improved accuracy than the original individual networks for small datasets (Number of imageless than 2000). In this case, only after careful training including transfer learning, the accuracy was higher than the original CNN architecture. The transfer learning strategies in this research were based on using the pre-trained model for training and extracting features. Surprisingly we found that seresNext101 has improved by 17% of accuracy after a transfer learning process. This is consistent with the results from the study conducted by Oloko-Oba and Viriri (2021) that SE-ResNeXt-101 normally would in- volve more parameters and was computationally expensive but has shown good results on the ImageNet classification tasks. Performing transfer learning from images trained on Imagenet (general images such as cats, dogs, etc.) or MURA (X-ray images on different parts of the body but not the chest) improved results compared to scenarios when transfer learning was not used at all.

This research offers several key contributions. Firstly, this research experimented using nine types of rice diseases. Secondly, in this re- search, a comparison of six original CNN architectures (DenseNet121, Inceptionv3, MobileNetV2, resNext101, Resnet152V, and Seresnext101) was conducted. Thirdly, we applied a transfer learning approach on DenseNet121, MobileNetV2, Resnet152V, Seresnext101, and an ensem- ble model called DEX (Densenet121, EfficientNetB7, and Xception) to draw a comparison among the original CNN networks, transfer learning,

In the future, we want to create a user interface for the detection and localization of rice leaf diseases for farmers. This interface would not only detect but also provide a guide on how the diseases can be con- trolled. As mobile phones are seen as a preferred technological device among developing country users, we aim to develop a mobile phone- based rice leaf disease detection application tool.

Densenet121, EfficientNetB7 & XceptionNet was found to produce the highest accuracy in classifying rice diseases from rice leaves. The success of the proposed architecture was compared with the transfer learning and six state-of-the-art individual CNN architectures. Experimental studies were conducted in both original and augmented versions of the image dataset. Considering both the average accuracy and the aver- age precision metric on both the original and augmented datasets, the DEX model was found to be superior to other CNN architectures.

