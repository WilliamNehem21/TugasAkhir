As mentioned at the beginning of this section, our main model eval- uation strategy is a 10-fold cross validation. We do not, however, compute accuracy, precision, or recall values independently for any fold, but collect and aggregate the raw confusion matrices (the true positive, true negative, false positive, and false negative values). This enables us to calculate the higher level measures once, at the end. Our primary mea- sure and basis of comparison is the F-measure – i.e., the harmonic mean of a model’s precision and recall – but in the case of the best models per algorithm, we calculated additional ROC curves (Receiver Operating

Normalization vs. Standardization. As a preprocessing step for the 60 features – or, predictors – we compared the results of the default algo- rithms on the original data (none) vs. normalization and standardization, introduced in Section 3.2. A comparison of the techniques is presented in Table 3.

The results suggest that standardization almost always performs well – as expected from previous empirical experiments. Even when it does not, it is negligibly close, and it is also responsible for the largest improvement in our deep neural network strategy. As there are already many dimensions to cover in our search for the optimal bug prediction model, with many more still to come – and even more we could have added – we decided to finalize this standardization preprocessing step for all further experimentation.

Resampling. Similarly to preprocessing, we compared a few resam- pling amounts in both directions. The results in Table 4 show the effect of altering the ratio of bugged and not bugged instances in the training set on predicting bugs in an unaltered test set. The numbers in the header column represent the percentage of resampling in the given direction, as described in Section 3.1.

We ended up choosing the 50% upsampling because it was the best performing option for our sdnnc strategy and produced comparably good results for the other algorithms as well. Similarly to above, it is also considered a fixed dimension from here on out so we can concentrate on the actual algorithm-specific hyperparameters. We do note, however, that while it was out of scope for this particular study, replicating the experiments with different resampling amounts definitely merits further research.

Simple Grid Search. In our first pass at improving the effectiveness of deep learning, we tried fine-tuning the hyperparameters that were already present in the default implementation, namely the number of layers in the network, the number of neurons per layer (in the hidden layers), and the number of epochs – i.e., the number of times we traverse the whole training set. Note that the activation function of the neurons (rectified linear) and the optimization method (Adagrad) were constant throughout this study, while the batching number could have been varied – and it will be in later stages – but were kept at a fixed 100 at this point. The performance of the different configurations is summarized in Table 5, where a better F-measure can help us select the most well-suited hyperparameters.

As the F-measures show, the best setup so far is 5 layers of 200 neurons each, learning for 10 epochs. It is important to note, however, that these F-measures are evaluated on the dev set, as the performance information they provide can factor into what path we choose in further optimization. Were we to use the test set for this, we would lose the objectivity of our estimations about the model’s predictive power, so test evaluations should only happen at the very end.

Initial Learning Rate. The next step was to consider the effects of changing the learning rate – i.e., the amount a new batch of information influences and changes the model’s previous opinions. These learning rates are set only once at the beginning of the training process and are fixed until the set number of epochs pass. Their effect on the resulting model’s quality are shown in Table 6.

Early Stopping and Dynamic Learning Rates. Our most dramatic improvement was reached when we introduced validation during training, and instead of learning for a set number of epochs, we imple- mented early stopping. This meant that after every completed epoch, we evaluated the F-measure of the in-progress model on the development set and checked whether it is an improvement or a deterioration. In the case of a deterioration, we reverted the model back to the previous – and, so far, the best – state, halved the learning rate, and tried again; a strategy called “new bob” in the QuickNet framework [50]. We repeated this loop until there were 4 consecutive “misses”, signaling that the model seems

The performance impact of this change is meaningful, as shown in Table 7. Note that both the above limit of 4 for the consecutive misses and the halving of the learning rates come from previous experience and are considered constant. We will refer to this approach as cdnnc, for “customized deep neural network classifier”.

Regularization. At this point, to decrease the gap between the training and dev F-measures and hopefully increase the model’s generalization capabilities, we tried L2 regularization [51]. It is a technique that adds an extra penalty term to the model’s loss function in order to discourage large weights and avoid over-fitting.

In our case, however, setting the coefficient of the L2 penalty term (denoted by β) to non-zero caused only F-measure degradation (as shown in Table 8), so we decided against its use. Note that we also tried β values above 0.05, but those also lead to complete model failure.

Another Round of Hyperparameter Tuning. Considering the meaningful jump in quality that cdnnc brought, we found it pertinent to repeat the hyperparameter grid search paired with the early stopping as well, netting us another +0.45% improvement. The tweaked parameters were, again, the number of layers, the number of neurons per layer, the batching amount, and the initial learning rate (that was still halved after every miss). The results, which are also our final results for deep learning in this domain, are summarized in Table 9.

The highest generalization on the independent test set goes to the random forest algorithm, although the highest train and dev results belong to our deep learning approach according to both F-measure and AUC figures. The numbers also show a fairly relevant gap between the performance of the two best models (forest and cdnnc) and the rest of the competitors. Additionally, while their evaluation times are at least comparable – with others meaningfully behind – training a neural network is two orders of magnitude slower.

Algorithm Comparison. To get some perspective on how good the performance of deep learning is, we needed to compare it to similarly fine-tuned versions of the other, more “traditional” algorithms listed in Section 3.3. Their possible parameters are listed in the official scikit-learn documentation [48], the method we used to tweak them is the same grid search we utilized for deep learning previously, and the best configura- tions we found are summarized in Table 10 in descending order of their test F-measures. Note that although we used F-measures to guide the optimization procedure, we list additional AUC values belonging to these final models for a more complete evaluation. We also measured model training and test set evaluation times, which are given in the last two columns, respectively.

One interesting aspect we noticed when comparing our cdnnc approach to random forest was that although they perform nearly iden- tically in terms of F-score, they arrive there in notably different ways. Taking a look at the separate confusion matrices of the two algorithms in Tables 11 and 12 shows a non-negligible amount of disagreement be- tween the models. Computing their precision and recall values (shown in the first two columns of Table 14) confirm their differences: cdnnc has higher recall (which is arguably more important in bug prediction any- way) at the price of lower precision, while forest is the exact opposite.

This prompted us to try and combine their predictions to see how well they could complement each other as an “ensemble” [52]. The method of combination was averaging the probabilities each model assigned to the bugged class and seeing if that average itself was over or under 0.5 – instead of a simple logical or on the class outputs. The thinking behind this experiment was that if the two models did learn the same “lessons” from their training, then disregarding deep learning and simply using forest is indeed the reasonable decision. If, on the other hand, they learned different things, their combined knowledge might even surpass those of the individual models’. Tables 13 and 14 attest to the second theory, as the ensemble F-measure reached 55.27% (a 1.56% overall

improvement) while the AUC reached 83.99% (a 1.01% improvement). Moreover, the corresponding ROC curves provide a subtle (yet useful) visual support for this theory. As we can see in Fig. 1, CDNNC and Forest learned differently, hence the differences in their curves. CDNNC slightly outperforms Forest at lower false positive rates, but the relationship is reversed at higher rates. Combining their judgments leads to the dotted

Another auxiliary experiment we tried was based on the assumption that “deep learning performs best with large datasets”. And by “large”, we mean data points in at least the millions. While our dataset cannot be considered small by any measure, – it is the most comprehensive unified bug dataset we are aware of – it is still not on the “large dataset” scale. The question then became the following: how could we empirically show that deep learning would perform better on more data without actually having more data? The answer we came up with inverts the problem: we theorize that if data quantity is proportional to the “domi- nance” of a deep learning strategy then it would also manifest as a faster deterioration than the other algorithms when even less data is available. So we artificially shrank – i.e., did a uniform stratified downsampling on – the full dataset three times to produce a 25%, a 50%, and a 75% subset to replicate our whole previous process on. The results are summarized in

The table consists of three regions, namely the various F-measures evaluated on their test sets (left), the difference between the best deep learning strategy and the current algorithm (middle), and the same dif- ference, only normalized into the [0,1] interval (right). The normalized relative differences are also illustrated in Fig. 2, where the slope of the lines represent the change in the respective differences. So we track these relative differences over changing dataset sizes, and the steeper the incline of the lines, the less influence dataset sizes have over their cor- responding algorithms compared to neural networks.

And what we see in Fig. 2 is not far off from this theoretical indicator. In the case of logistic vs. cdnnc, for example, growth in the differences means that cdnnc is leaving logistic farther and farther behind as more data becomes available. While in the case of forest vs. cdnnc, it means that cdnnc is “catching up” – since the figures are negative, but their absolute values are decreasing.

As most tendencies of the changing differences empirically corrobo- rate, more data is good for every algorithm, but it has a bigger impact on deep learning. Naturally, there are occasional swings like SVM’s decrease at 75% – possibly due to the more “hectic” nature of the technique – or KNN’s “hanging tail” at 100%. If we assume a linear kind of relationship, however, even these cases show overall growth. This leads us to speculate that deep neural networks could dominate their opponents – individu- ally, even without resorting to the previously described model combi- nation – when used in conjunction with larger datasets. We also note that scalability should not be an issue, as larger input datasets would affect only the training times of the models – which is usually an acceptable up- front sacrifice – while leaving prediction speeds unchanged.

Throughout this study, we aimed to remain as objective as possible by disclosing all our presuppositions and publishing only concrete, repli- cable results. However, there are still factors that could have skewed the conclusions we drew. One is the reliability of the bug dataset we used as our input. Building on faulty data will lead to faulty results – also known as the “garbage in, garbage out” principle – but we are confident that this is not the case here. The dataset is independently peer reviewed, accepted, and is compiled using standard data mining techniques.

Another factor might be – ironically – bugs in our bug prediction framework. We tried to combat this by rigorous manual inspections, tests, and replications. Additionally, we are also making the source code openly available on GitHub and invite community verification or comments.

Yet another factor could be the study dimensions we decided to fix – namely, the preprocessing technique, the preliminary resamplig, the number of consecutive misses before stopping early, the 0.5 multiplier for the learning rate “halving”, and even the random seed, which was the same for every execution. Analyzing how changes to these parameters would impact the results – if at all – was out of the scope of this study.

Our greatest contribution is the thorough, step by step description of our process which – apart from the underexplored coupling of concepts – leads to a deep neural network that is on par with random forests and dominates everything else. Additionally, we unveiled that an ensemble model made from our best deep neural network and forest classifiers is actually better than either of its components individually, – suggesting that deep learning is applicable right now – and that more data is likely to make our approach even better. These are two further convincing argu- ments supporting the assumption that the increased time and resource requirements of training a deep learning model are worth it. Moreover, we open-sourced the experimental tool we used to reach these conclu- sions and invite the community to build on our findings.

Our future plans include comparing the effectiveness of static source code metrics to change-based and vector embedding-based features when utilized with the same deep learning techniques, and to quantify the effects of different network architectures. We would also like to replicate the outlined experiments with extra tweaks to the parameters we considered fixed thus far (e.g., the random seed or the preprocessing methodology), thereby examining how stable and resistant to noise our

results are. Additionally, we plan to expand the dataset – ideally some- what automatically to be able to reach an official “large dataset” status in the near future – and to integrate the current best bug prediction model into the OpenStaticAnalyzer toolchain to issue possible bug warnings alongside the existing source code metrics. In the meantime, we consider our findings a successful step towards understanding the role deep neural networks can play in bug prediction.

This work was partially supported by grant 2018–1.2.1-NKP-2018- 00004 “Security Enhancing Technologies for the IoT” funded by the Hungarian National Research, Development and Innovation Office. Ministry for Innovation and Technology, Hungary grant TUDFO/ 47138–1/2019-ITM is acknowledged. The Titan Xp used for this research was donated by the NVIDIA Corporation.

Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A, Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y, Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Mane´ D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P, Vanhoucke V, Vasudevan V, Vi´egas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X. TensorFlow: large-scale machine learning on heterogeneous systems. software available from: tensorflow.org. 2015. URL, htt p://tensorflow.org/.

