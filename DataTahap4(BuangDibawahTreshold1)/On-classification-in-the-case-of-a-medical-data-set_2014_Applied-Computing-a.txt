Abstract In one of our earlier studies we noticed how straightforward cleaning of our medical data set impaired its classification results considerably with some machine learning methods, but not all of them, unexpectedly and against intui- tion compared to the original situation without any data cleaning. After a more precise exploration of the data, we found that the reason was the complicated variable distribution of the data although there were only two classes in it. In addition to a straightforward data cleaning method, we used an efficient way called neighbourhood cleaning that solved the problem and improved our clas- sification accuracies 5–10%, at their best, up to 95% of all test cases. This shows how important it is first very carefully to study distributions of data sets to be classified and use different cleaning techniques in order to obtain best classifica- tion results.

In our earlier research we developed a signal analysis method for nystagmic eye movements investigated in otoneurological tests (Juhola et al., 2009, 2011). For the automatic analysis of such signals poor or invalid nystagmic eye movements should correctly be separated from valid nystagmic eye movements, because valid eye movements can only be used for the data analysis needed for the diagnostics of otoneurological patients. Typically, invalid nystagmic eye movements are cor- rupted by noise or artefacts. Thus, we have also studied the classification of nys- tagmic eye movement candidates into invalid and valid, hereafter called the rejected and accepted, on the basis of machine learning methods (Juhola et al., 2013). We then observed how their complicated distribution made the classifica- tion task difficult and attempted to reduce the greater subset (class) of the rejected eye movement candidates in a learning set, which was performed by cleaning away a part from them. Surprisingly, a simple cleaning process impaired classification results of some of the machine learning methods applied. We realized that the rea- son for such a seemingly conflicting situation originated from the complicated var- iable distribution of the data (Juhola et al., 2013).

In order to define which distribution of two classes is seen as simple or compli- cated we refer to Fig. 1. A simple distribution is where the centre (computed as means for all variables) of each class is located inside its own area including most elements of that class. This is described in Fig. 1(a). A complicated distribution is depicted in Fig. 1(b), where the centre of one class is outside its own area. Such complicatedness could be defined in various ways, but it is essential that we cannot then base data cleaning on distances from the class centres.

Fig. 1 Let us assume that there are two hypothetical classes A and B in a variable space. Elements of the majority class A close to B and possibly from their overlapping area would be useful to be cleaned out. (a) A simple distribution in which the majority class A (more elements) and minority class B partially overlap and in which element x from A is closer to the centre of B (square) than to the centre (circle) of its own class A. Thus, x can clearly be cleaned on the basis of the distance criterion. (b) A complicated distribution in which x from A is closer to its own centre than to that of B. Further, the centre of A is outside its actual area. Element x cannot be cleaned on the basis of the distance criterion.

Since the 1970s, dozens of different nystagmus detection algorithms have been presented (Abel et al., 2008; Augustyniak, 1996; Hertle and Dell’Osso, 1999; Hosokawa et al., 2004; Juhola, 1988; Tominaga and Tanaka, 2010; Wall and Black, 1982). Most of them have been on the basis of applying digital filters,

Fig. 2  A hypothetic nystagmic beat describing its main variables and shape from which some other variables are derived. A beat is composed of the slow and fast phases represented with their amplitudes given in angular degrees and durations in seconds. In reality, there may be noise or other unevenness in nystagmus signals. Such phenomena are estimated by computing, for example, linear correlation between an actual slow phase and its ideal component, straight line y.

Fig. 3 Here a three-dimensional nystagmic eye movement signal is depicted as three one-dimensional component signals so that their features can be visualized and understood. The component signals are horizontal (blue), vertical (green) and torsional (red). The signal is 10 s long sampled at 50 Hz. Segments (1)-(5) were deemed due to be rejected nystagmic beats because of signal corruption. Segment (1) was corrupted both in the horizontal and torsional components. Segments from (2) to (5) were dropouts of video camera images in the torsional component, in other words, the camera system had momentarily failed to identify the eye in its successive images. Peaks with lower amplitudes than 1° were seen noise or unevenness being neither valid nor invalid nystagmic beats.

thresholding, other signal analysis and tuning parameters. Nevertheless, it is not only to detect nystagmic beat candidates, but the separation of acceptable beats from those corrupted or noisy is a necessary stage in processing before computing nystagmus variable values from all accepted beats. Typically, this stage has been grounded on conventional signal analysis methods and separately on every single nystagmic eye movement. However, this is difficult, because poor nystagmic beats may vary remarkably between patients and also depend on measurement devices. Recently, we developed an approach based on machine learning and collected a data set of nystagmic eye movement beats, both accepted and rejected, in order to form a training set for machine learning and classification of nystagmus signals (Juhola et al., 2013). To our knowledge, no such attempt has previously been made for nystagmus data. Nonetheless, the classification task was difficult because the data distribution appeared to be of a rare form in which the two classes of the data were very close to each other or partially overlapped so that accepted nystag- mic beats were, in a way, surrounded by those rejected beat candidates (Juhola et al., 2013). This resulted in an extraordinary outcome that our then straightfor- ward data cleaning technique impaired the classification results computed with some machine learning methods. In the present study we applied a more efficient

The data set included one signal from each of 107 patients suffering chiefly from acute, unilateral, peripheral loss of vestibular function, in other words, vestibular neuritis or having had a surgery for acoustic neuroma. Nystagmus of a patient sit- ting in a chair in the darkened room was measured with two small eye movement video cameras that detected eye movements with an image processing system. Each signal was 30 s long and included 20–80 acceptable nystagmic beats.

Every signal was analysed with a nystagmus detection algorithm (Juhola et al., 2011) and nystagmic beat candidates were, at the same time, separated either into accepted or rejected beats according to our algorithm (Juhola et al., 2013). Inde- pendent of this automatic separation, an expert manually explored all nystagmic

In our original cleaning procedure (Juhola et al., 2013) we assumed that there would be two classes in the data having their class centres in different areas. This assumption was reasonable in the sense that the poor nystagmic beat candidates were marked to be rejected (manually or automatically) typically since some of their variable values were above some upper bounds. For instance, segments

(2)–(5) in Fig. 3 would create too high torsional mean velocities for slow or fast phases of nystagmus because of the very steep spikes in the torsional signal. Clean- ing was made first by computing the class centres and second by deleting those re- jected nystagmic beats that were the closest to the class centre of the opposite class until the class size of the rejected nystagmic beat candidates became as small as the opposite class.

A more precise data analysis showed, however, that the class distribution was complicated so that no two clearly separate class centres appeared. The distribu- tions computed with principal component analysis are given after the automatic rejection and acceptance of nystagmic beat candidates in Fig. 4(a) and similarly in Fig. 4(b) after first reducing the larger class of the rejected candidates according

Fig. 4 (a) Before neighbourhood cleaning and (b) after this. The two first (most important) principal components were used to show the distributions of the accepted and rejected nystagmic beats after the automatic selection to the accepted and rejected beats. To make the scatter plots clear, one tenth of all beats only were drawn since their occurrences overlap considerably, particularly in the ‘‘left corner’’ of either distribution. The two first principal components accounted for 80% of variance in (a) before cleaning and 87% in (b) after cleaning.

After the manual selection of nystagmic beat candidates there were 2171 ac- cepted and 3818 rejected beats. After the automatic selection these numbers were 2517 and 3472, and after using the both ways jointly 1645 and 4344, respectively. Thus, after cleaning, i.e., reducing the larger class of the rejected beats, the num- bers were 2171, 2517 and 1645 for each of the classes in these three situations.

The above straightforward cleaning procedure even impaired the classification accuracies from 80–90% (without cleaning) down to half for nearest neighbour searching, naı¨ ve Bayes rule and logistic discriminant analysis (Juhola et al., 2013), whereas linear and quadratic discriminant analysis and support vector machines with the linear and quadratic kernels obtained minor improvements. For the sake of these unexpected negative effects we abandoned the preceding cleaning procedure in the present study and used another, more sophisticated method (Laurikkala, 2001) that was based on nearest neighbour searching to determine which rejected beat candidates were the best to drop out from that larger class in order to balance the class sizes and, most of all, to use cleaning in order to improve classification.

The cleaning procedure acts locally, not globally as the previous that was con- structed on the basis of the class centres. Thus, the neighbourhood cleaning meth- od of (Laurikkala, 2001) is not hampered by a complicated distribution of more or less overlapping, mixing classes or one class surrounding the other. Since the cleaning procedure processes data cases on the basis of their nearest neighbour- hood approach, it functions locally, independent of the ‘‘global’’ properties of data. It also cleans not only according to the majority class of the rejected beats, but also by means of the minority class. Nevertheless, in our nystagmus data cleaning is only directed to the majority class. This is very natural particularly

We tested with nearest neighbour searching, Naı¨ ve Bayes rule, and linear, qua- dratic and logistic discriminant analysis, and support vector machines with differ- ent kernel functions. Results are presented in percents as classification accuracies, i.e., ratios of true positive added with true negative rates to the number of all test cases. Since the leave-one-out method was applied to tests, the number of tested cases was equal to all nystagmic beat candidates, 5989, or less when the data were cleaned.

Nearest neighbour searching and logistic discriminant analysis gave better clas- sification results when the data were first standardized by subtracting the mean of each variable and then by dividing with its standard deviation. For those other classification methods, standardization (being not useful) was not used. Without standardization, the cleaning procedure removed 1313, 1680 and 1364 nystagmic beat candidates from the class of the rejected, respectively, after the three selec- tions. With standardization, these were only 998, 717 and 732. Consequently, the class of the rejected beats remained as the majority class for all other situations than that without standardization for the automatic selection.

In Table 2, the results produced with k nearest neighbour searching are shown. The nearest neighbour number of k equal to 15 was used here since earlier it gave us at least 1% better results than odd values of less than 10 or greater than 20 (Juhola et al., 2013). To experimentally study the influence of the neighbourhood cleaning procedure (Laurikkala, 2001) described above, we also ran cleaning by

leaving out a half or quarter from the class of the rejected beats as systematically dropping out every two or four beat candidates from those of the rejected. Let us call these ‘random cleaning’ because no assessment criterion for cleaning was em- ployed. The last row in Table 2 includes the results when, after neighbourhood cleaning, artificial beats as many as cleaned beats from the rejected were inserted into the class of the rejected.

According to Table 2, we see that it is quite useless to run random cleaning, at least for this complicated data distribution, since the results were not better than those original of the uppermost row. (Random cleaning was made only to show that more efficient cleaning is necessary.) When strong cleaning that left the half out of the rejected was run, we even obtained partially poorer results. Of course, we also have to notice the different class sizes here that affect a priori probabilities in classification. Doubtless did neighbourhood cleaning affect positively by increasing classification accuracies by 5–6% for all three selection ways of nystag- mic beat candidates. Instead, the extension of artificial, rejected beats after neigh- bourhood cleaning had only a minor effect, less than 0.5%. In principle, it might sometimes even impair results because there is no guarantee that such extension would always ‘‘improve the quality of data’’. Therefore, it is not reasonable to ap- ply for tests of the subsequent tables.

In Table 3 there are results of linear, quadratic and logistic discriminant analysis and Naı¨ ve Bayes rule. The different sizes of the classes of the rejected beats for the different classification methods came from whether the data standardization was used or was not used. Again neighbourhood cleaning was effective improving accuracies by 4–10% compared to the results of not cleaned situations.

We also experimented with support vector machines the results of which are given in Table 4. Suitable parameter values were extensively studied as in (Juhola et al., 2013) and those giving the best average accuracy results were chosen for the three selection methods of nystagmic beat candidates (three rightmost columns in Table 4): (1) box constraint 8.9 for linear kernel and 0.1 for quadratic kernel, and box

Ultimately, we still studied the use of greater k values than 3 for the neighbour- hood cleaning procedure. Results obtained are given in Table 5. These are only shown for the classification set-up where nearest neighbour searching was applied to classification since similar phenomena were also expected for the other classifi- cation methods. Increasing the number up to 11 or 13 of the nearest neighbours searched for in the cleaning procedure improved classification accuracies slightly, by 1–2%. This came from the stronger cleaning, in other words, with greater k val- ues more elements were left out from the class of the rejected beats. Therefore, the more intensive data cleaning may be a reasonable approach if there are abundant elements in the majority class as were here.

It was obvious that increasing k in neighbourhood cleaning did not much im- prove the results obtained compared to those of k equal to 3, because the latter were already high, over 90%, although more and more elements of the majority class were left out. Note two lowest rows in Table 5. According to them, it seemed in the present data that increasing k over 11 did not improve classification accuracies.

In all of our results, neighbourhood cleaning was an efficient way to refine data and reduced the size of the majority class, rejected nystagmic beats. Simpler clean- ing ways could hardly function as effectively, at least neither that was used earlier (Juhola et al., 2013) nor in Table 2.

Logistic discriminant analysis after cleaning was 2–12% better than the others in Table 3 and 2–6% worse than the accuracies of the nearest neighbour searching after neighbourhood cleaning as in Table 2. However, the best support vector ma- chines (with the quadratic kernels) were 2–5% poorer than the nearest neighbour accuracies after neighbourhood cleaning as in Table 2. This is a slightly surprising conclusion since our experience among some other data sets has been the opposite, frequently support vector machines have been subtly better than others. We as- sume that the possible reason was now that neighbourhood cleaning favoured the nearest neighbour searching classification. Since the nearest neighbour search- ing was applied in both, cleaning could ‘‘favour’’ its ‘‘relative classification meth- od’’ more than the others.

The automatic selection was better in some cases than the manual and auto- matic ones together. Apparently, this stemmed from the fact that manual selection criteria may vary a little from time to time. Instead, the automatic selection always functions stably.

The use of greater nearest neighbour numbers (5, 7, 9 or 11) than 3 originally used improved the classification results slightly further since more elements were cleaned out from the majority class compared to the situation of 3 nearest neigh- bours. However, no such conclusion could be drawn that this phenomenon would typically be present. After all, the properties of data and their distribution are essential.

A future research detail in using the neighbourhood cleaning method could be to attempt to also clean the minority class. In general this is not perhaps sensible, but for such data sets as ours here where both classes were fairly large, it might be useful since the classes were ‘‘mutually overlapping’’, some rejected nystagmic beats among the accepted and vice versa, as seen in Fig. 4. On the other hand, cleaning accepted beats should be made very carefully and ‘‘conservatively’’, not to deteriorate the validity of the data set as a training set for nystagmus analysis viz., the accepted nystagmic beats represent physiologically plausible and possible nystagmus variable values, whereas the rejected beats are more or less values out- side physiologically possible boundaries. However, these boundaries are not exact, because they may vary between subjects.

We can conclude that neighbourhood cleaning refined data efficiently and im- proved accuracies throughout the tests accomplished. Its character is rather local than global and it can purify noise-like occurrences from data. It had also good influence on the current data set with the complicated distribution. To clean data sets with unknown distributions, it is best to first explore their data distributions

