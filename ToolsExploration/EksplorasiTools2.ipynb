{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "44eb4b82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import library\n",
    "import re\n",
    "import sys\n",
    "import os\n",
    "import shutil\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from gensim.models import Word2Vec\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.metrics.pairwise import cosine_similarity\n",
    "import numpy as np\n",
    "import time\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "e85a82a3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file asli sebanyak 200\n",
    "directory_path_real = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_asli'\n",
    "\n",
    "files_real = os.listdir(directory_path_real)\n",
    "list_files_real = []\n",
    "counter = 0\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_real:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_real.append(file)\n",
    "        \n",
    "all_text_real = \"\"\n",
    "for file in list_files_real:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_asli/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        lines = content.splitlines()\n",
    "        if counter < 200:\n",
    "            counter += 1\n",
    "            # sumber file\n",
    "            source_file = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_asli/' + file\n",
    "            \n",
    "            # file tujuan\n",
    "            destination_folder = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_asli'\n",
    "            \n",
    "            # menyalin file ke folder baru\n",
    "            shutil.copy(source_file, destination_folder)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "91feb6b1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file parafrasa sebanyak 200\n",
    "directory_path_paraphrased = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_parafrasa'\n",
    "\n",
    "files_paraphrased = os.listdir(directory_path_paraphrased)\n",
    "list_files_paraphrased = []\n",
    "counter = 0\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_paraphrased:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_paraphrased.append(file)\n",
    "        \n",
    "all_text_paraphrased = \"\"\n",
    "for file in list_files_paraphrased:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_parafrasa/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        lines = content.splitlines()\n",
    "        if counter < 200:\n",
    "            counter += 1\n",
    "            # sumber file\n",
    "            source_file = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/Data/Data_parafrasa/' + file\n",
    "            \n",
    "            # file tujuan\n",
    "            destination_folder = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_parafrasa'\n",
    "            \n",
    "            # menyalin file ke folder baru\n",
    "            shutil.copy(source_file, destination_folder)\n",
    "        \n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "3ad0b4da",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method untuk menghitung nilai relatedness\n",
    "def relatedness(model):\n",
    "    # Baca file rw untuk evaluasi word embedding dengan teknik relatedness cbow\n",
    "    dir_file = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataEvaluasiGitHub/word-similarity/monolingual/en/rw.csv'\n",
    "    list_model_similarity = []\n",
    "    words_exist = []\n",
    "    words_not_exist = []\n",
    "    list_human_score = []\n",
    "    \n",
    "    df = pd.read_csv(dir_file)\n",
    "    list_human_score = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        word_1 = row[\"word1\"].lower()\n",
    "        word_2 = row[\"word2\"].lower()\n",
    "        try:\n",
    "            wv_1 = []\n",
    "            wv_2 = []\n",
    "            wv_1.append(model.wv[word_1])\n",
    "            \n",
    "            if word_1 not in words_exist:\n",
    "                words_exist.append(word_1)\n",
    "            \n",
    "            wv_2.append(model.wv[word_2])\n",
    "            \n",
    "            if word_2 not in words_exist:\n",
    "                words_exist.append(word_2)\n",
    "                \n",
    "            similarity = model.wv.similarity(word_1, word_2)\n",
    "            list_model_similarity.append(similarity)\n",
    "            \n",
    "            human_score = float(row[\"similarity\"])\n",
    "            list_human_score.append(human_score)\n",
    "            \n",
    "        except:\n",
    "            if word_1 in words_exist: # kata 2 yang tidak ada di model\n",
    "                if word_2 not in words_not_exist:\n",
    "                    words_not_exist.append(word_2)\n",
    "            \n",
    "            if word_2 in words_exist: # kata 1 yang tidak ada di model\n",
    "                if word_1 not in words_not_exist:\n",
    "                    words_not_exist.append(word_1)\n",
    "        \n",
    "    \n",
    "   \n",
    "        \n",
    "\n",
    "    rho_model, p_model = spearmanr(list_model_similarity, list_human_score)\n",
    "    print(\"banyak kata ada di model\", len(words_exist))\n",
    "    print(\"banyak kata tidak ada di model\", len(words_not_exist))\n",
    "    return rho_model\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "b876df7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# method untuk menilai model dengan teknik analogi kata\n",
    "def word_analogy(model):\n",
    "    # Baca file word_analogy untuk evaluasi word embedding dengan teknik word analogy (menggunakan library)\n",
    "\n",
    "    dir_file = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/DataEvaluasiGitHub/word-analogy/monolingual/en/semeval.csv'\n",
    "    df = pd.read_csv(dir_file)\n",
    "    true_predicted = 0\n",
    "    false_predicted = 0\n",
    "    words_exist = []\n",
    "    words_not_exist = []\n",
    "    \n",
    "    for index, row in df.iterrows():\n",
    "        word_1 = row[\"word1\"].lower()\n",
    "        word_2 = row[\"word2\"].lower()\n",
    "        word_3 = row[\"word3\"].lower()\n",
    "        word_4 = row[\"target\"].lower()\n",
    "        word_predicted_curr = \"\"\n",
    "        try:\n",
    "            word_predicted_curr = model.wv.most_similar(positive=[word_2, word_3], negative=[word_1], topn=1)[0][0]\n",
    "         \n",
    "            if word_predicted_curr == word_4:\n",
    "                true_predicted += 1\n",
    "            else:\n",
    "                false_predicted += 1\n",
    "            \n",
    "            if word_1 not in words_exist:\n",
    "                words_exist.append(word_1)\n",
    "            \n",
    "            if word_2 not in words_exist:\n",
    "                words_exist.append(word_2)\n",
    "            \n",
    "            if word_3 not in words_exist:\n",
    "                words_exist.append(word_3)\n",
    "            \n",
    "            if word_4 not in words_exist:\n",
    "                words_exist.append(word_4)\n",
    "\n",
    "        except:\n",
    "            continue\n",
    "            \n",
    "        # untuk menambahkan word_1 jika tidak ada di model    \n",
    "        try:\n",
    "            word_vector = model.wv[word_1]\n",
    "        except:\n",
    "            if word_1 not in words_not_exist:\n",
    "                    words_not_exist.append(word_1)\n",
    "        \n",
    "        # untuk menambahkan word_2 jika tidak ada di model    \n",
    "        try:\n",
    "            word_vector = model.wv[word_2]\n",
    "        except:\n",
    "            if word_2 not in words_not_exist:\n",
    "                    words_not_exist.append(word_2)\n",
    "\n",
    "        # untuk menambahkan word_3 jika tidak ada di model    \n",
    "        try:\n",
    "            word_vector = model.wv[word_3]\n",
    "        except:\n",
    "            if word_3 not in words_not_exist:\n",
    "                    words_not_exist.append(word_3)\n",
    "        \n",
    "        # untuk menambahkan word_4 jika tidak ada di model    \n",
    "        try:\n",
    "            word_vector = model.wv[word_4]\n",
    "        except:\n",
    "            if word_4 not in words_not_exist:\n",
    "                    words_not_exist.append(word_4)\n",
    "\n",
    "    print(\"true_predicted\", true_predicted, true_predicted / (true_predicted+false_predicted))\n",
    "    print(\"false_predicted\", false_predicted, false_predicted / (true_predicted+false_predicted))\n",
    "    print(\"banyak kata ada di model\", len(words_exist))\n",
    "    print(\"banyak kata tidak ada di model\", len(words_not_exist))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "bb407863",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file asli dari folder\n",
    "directory_path_real = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_asli'\n",
    "\n",
    "files_real = os.listdir(directory_path_real)\n",
    "list_files_real = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_real:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_real.append(file)\n",
    "        \n",
    "all_text_real = \"\"\n",
    "for file in list_files_real:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_asli/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_real += content\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c29124a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil file parafrasa dari folder\n",
    "directory_path_paraphrased = '/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_parafrasa'\n",
    "\n",
    "files_paraphrased = os.listdir(directory_path_paraphrased)\n",
    "list_files_paraphrased = []\n",
    "\n",
    "# Ambil daftar nama file\n",
    "for file in files_paraphrased:\n",
    "    if '.DS_Store' not in file:\n",
    "        list_files_paraphrased.append(file)\n",
    "        \n",
    "all_text_paraphrased = \"\"\n",
    "for file in list_files_paraphrased:\n",
    "    with open('/Users/williamnehemia/Documents/Skripsi/TugasAkhir/ToolsExploration/Data/Data_parafrasa/' + file, 'r') as fileNow:\n",
    "        content = fileNow.read()\n",
    "        all_text_paraphrased += content\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "76d4cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Melakukan lowercase lagi\n",
    "all_text_real = all_text_real.lower()\n",
    "all_text_paraphrased = all_text_paraphrased.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "795b23b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['computer-aided structure-activity based prediction studies in drug design helps to treat diseases with novel biomarkers', ' prediction of activity spectra for substances(pass) database comprised 46,000 biologically well-known active drugs and screening are performed before the establishment of an in vitro experiment', ' pass gives the significant bioactivities of chemical compounds as pa(probable activity) and pi(probable inactivity) values to mention the compounds, whether they are active are inactive', ' the pa values higher than 0', '7 indicated this compound would be active in experiment and pi values indicate theirs inactivate possibilities', ' the admetsar chemoinformatics based tool used to predict absorption, metabolism, excretion, and toxicity of the particular compound', ' based on these criteria, the outcomes of an in vitro experiment will lower the risk of negative results[13,14]', '\\n\\n\\n\\nclc-pred tools performed to predict cytotoxicity of tumor cell lines, and it is based on structure-cell line cytotoxicity relationships designed by pass special training sets with leave-one-out cross-validation procedure', ' the accuracy of in silico prediction results significantly 96% matches with the results of in vivo experimental', ' the efficiency of compounds against cancer could be found and optimized using this pass based clc-pred database in the future to develop potential anti-cancer drugs']\n",
      "['the detection of pedestrians has broad applications in various fields such as autonomous driving, intelligent monitoring, and robotics, and has long been a focal point in the domain of computer vision', ' in recent years, there have been significant advancements in pedestrian detection technology due to the evolution of deep learning and the introduction of numerous large pedestrian datasets', ' these developments have notably enhanced detection accuracy and speed', ' however, contemporary pedestrian detection methods still struggle to match human performance, particularly when addressing challenges like occlusion and scale variations', ' consequently, solving the issues of occlusion and scale is a crucial objective in pedestrian detection research', '\\n\\nthis paper aims to review the progress in pedestrian detection research, focusing on methods for calculating the confidence of bounding boxes in regression and utilizing multiple convolution layers with different scales to improve multi-scale object detection and enhance accuracy', ' notably, the integration of convolutional neural networks (cnn) into object detection, as exemplified by the introduction of r-cnn by ross girshick et al', ' in 2014, has been pivotal', ' subsequent developments, such as the introduction of the region proposal network (rpn) in 2015, and the adoption of deep learning technologies in methods like faster r-cnn, mscnn, yolov4, csp, and saf-rcnn, have further advanced pedestrian detection capabilities', ' these newer approaches leverage deep features, providing superior generalization ability and robustness compared to hand-designed features']\n"
     ]
    }
   ],
   "source": [
    "# Memisahkan text per kalimat (menggunakan delimiter titik)\n",
    "list_of_sentences_real = []\n",
    "list_of_sentences_paraphrased = []\n",
    "\n",
    "list_of_sentences_real = all_text_real.split('.')\n",
    "list_of_sentences_paraphrased = all_text_paraphrased.split('.')\n",
    "\n",
    "print(list_of_sentences_real[0:10])\n",
    "print(list_of_sentences_paraphrased[0:10])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "c1f774e0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['computer', 'aided', 'structure', 'activity', 'based', 'prediction', 'studies', 'in', 'drug', 'design', 'helps', 'to', 'treat', 'diseases', 'with', 'novel', 'biomarkers'], ['prediction', 'of', 'activity', 'spectra', 'for', 'substances', 'pass', 'database', 'comprised', 'biologically', 'well', 'known', 'active', 'drugs', 'and', 'screening', 'are', 'performed', 'before', 'the', 'establishment', 'of', 'an', 'in', 'vitro', 'experiment'], ['pass', 'gives', 'the', 'significant', 'bioactivities', 'of', 'chemical', 'compounds', 'as', 'pa', 'probable', 'activity', 'and', 'pi', 'probable', 'inactivity', 'values', 'to', 'mention', 'the', 'compounds', 'whether', 'they', 'are', 'active', 'are', 'inactive'], ['the', 'pa', 'values', 'higher', 'than'], ['indicated', 'this', 'compound', 'would', 'be', 'active', 'in', 'experiment', 'and', 'pi', 'values', 'indicate', 'theirs', 'inactivate', 'possibilities'], ['the', 'admetsar', 'chemoinformatics', 'based', 'tool', 'used', 'to', 'predict', 'absorption', 'metabolism', 'excretion', 'and', 'toxicity', 'of', 'the', 'particular', 'compound'], ['based', 'on', 'these', 'criteria', 'the', 'outcomes', 'of', 'an', 'in', 'vitro', 'experiment', 'will', 'lower', 'the', 'risk', 'of', 'negative', 'results'], ['clc', 'pred', 'tools', 'performed', 'to', 'predict', 'cytotoxicity', 'of', 'tumor', 'cell', 'lines', 'and', 'it', 'is', 'based', 'on', 'structure', 'cell', 'line', 'cytotoxicity', 'relationships', 'designed', 'by', 'pass', 'special', 'training', 'sets', 'with', 'leave', 'one', 'out', 'cross', 'validation', 'procedure'], ['the', 'accuracy', 'of', 'in', 'silico', 'prediction', 'results', 'significantly', 'matches', 'with', 'the', 'results', 'of', 'in', 'vivo', 'experimental'], ['the', 'efficiency', 'of', 'compounds', 'against', 'cancer', 'could', 'be', 'found', 'and', 'optimized', 'using', 'this', 'pass', 'based', 'clc', 'pred', 'database', 'in', 'the', 'future', 'to', 'develop', 'potential', 'anti', 'cancer', 'drugs']]\n"
     ]
    }
   ],
   "source": [
    "# melakukan tokenisasi dan menggabungkan teks asli dan parafrasa\n",
    "\n",
    "\n",
    "list_of_sentences_for_model = []\n",
    "\n",
    "tokenizer = RegexpTokenizer(r'[a-zA-Z]+')\n",
    "\n",
    "for sentence in list_of_sentences_real:\n",
    "    sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "    list_of_sentences_for_model.append(sentence_tokenized)\n",
    "\n",
    "for sentence in list_of_sentences_paraphrased:\n",
    "    sentence_tokenized = tokenizer.tokenize(sentence)\n",
    "    list_of_sentences_for_model.append(sentence_tokenized)\n",
    "\n",
    "print(list_of_sentences_for_model[0:10])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "94544820",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total training time: 0 hours 0 minutes 14 seconds\n"
     ]
    }
   ],
   "source": [
    "# Membuat model word embedding dengan teknik word2vec dengan cbow 1\n",
    "\n",
    "# waktu mulai\n",
    "start_time = time.time()\n",
    "\n",
    "model = Word2Vec(sentences=list_of_sentences_for_model, vector_size=10, min_count=1, workers = 10, compute_loss=True,  window=5, sg = 0, cbow_mean = 0, alpha = 0.01, seed = 10, epochs = 100)\n",
    "\n",
    "# waktu selesai\n",
    "end_time = time.time()\n",
    "\n",
    "# hitung waktu training\n",
    "training_time = end_time - start_time\n",
    "\n",
    "# convert ke jam, menit, detik\n",
    "hours = int(training_time // 3600)\n",
    "minutes = int((training_time % 3600) // 60)\n",
    "seconds = int(training_time % 60)\n",
    "\n",
    "print(\"Total training time:\", hours, \"hours\", minutes, \"minutes\", seconds, \"seconds\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "id": "be904c34",
   "metadata": {},
   "outputs": [],
   "source": [
    "# save model word embedding dengan teknik word2vec dengan cbow 1\n",
    "model.save(\"/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/ModelWord2vecSetelahPembagianData/word2vec_cbow_1.model\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "0ef8369b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load model word embedding dengan teknik word2vec dengan cbow 1\n",
    "model = Word2Vec.load(\"/Users/williamnehemia/Documents/Skripsi/TugasAkhir/WordEmbeddingModel/ModelWord2vecSetelahPembagianData/word2vec_cbow_1.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd951226",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
