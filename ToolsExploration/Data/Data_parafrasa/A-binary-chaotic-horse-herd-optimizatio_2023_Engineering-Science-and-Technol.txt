The original research introduced the Binary Horse Herd Algorithm (BHOA), which is specifically designed to handle large-scale data effectively. The study focused on adapting the BHOA for the feature selection problem using the wrapper method (BHOAFS), and involved an extensive review of literature to achieve this conversion.

The primary objective of the study was to maximize classification accuracy while minimizing the number of selected features. To assess the performance of the proposed algorithm, various criteria such as average fitness values, average accuracy values, average number of selected features, and CPU time were evaluated.

The paper is structured as follows: Section 2 provides the foundational research and details the proposed binary BHOAFS method. Section 3 describes the proposed FS-based binary chaotic BCHOAFS methods. Section 4 presents the experimental results and discussions, while Section 5 outlines the concluding results.

The algorithm adopts a population-ranking approach, where horses with good fitness values are selected in descending order. The top 10% of ranked horses are selected first, followed by 20% (b horses), 30% (c horses), and 40% (d horses) for subsequent iterations.

To adapt the feature selection problem to the continuous space of HOA, the algorithm leverages transfer functions such as S-shaped (sigmoid), V-shaped, and U-shaped functions. The U-shaped transfer function facilitates the conversion from continuous to discrete space.

A similarity measure function (SMF), designed to enhance the local search strategy, aims to improve the positions of individuals within the population and advance them to the next iteration. SMF calculates the distance between candidate horse solutions, generating a new solution likely to be in a better position.

The algorithm utilizes the Pareto law to generate a random population in the feature selection process, allocating 80% zeros and 20% ones in the initial binary vector array. The datasets are split into training and test datasets, with 90% for training and 10% for testing, and classifiers such as k-NN and SVM are employed. The fitness function in the feature selection problem considers both classification accuracy and the number of selected features.

Chaotic maps, including piecewise, Singer, logistic, tent, and sinusoidal maps, are evaluated for their accuracy and the number of features selected. Piecewise and Singer maps performed best, demonstrating the effectiveness of the proposed BCHOAFS3 and BCHOAFS4 versions.

The study assessed the proposed BCHOAFS and BHOAFS versions using 18 standard datasets from the UCI repository, comparing their performance with other optimization algorithms such as GA, PSO, ALO, GWO, SSA, as well as binary optimization algorithms like BGA, BPSO, BALO, BGWO, and BSSA. The BCHOAFS3 and BCHOAFS4 algorithms were found to outperform the other methods in terms of classification accuracy and feature selection.