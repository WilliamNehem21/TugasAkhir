In recent decades, the rapid expansion of businesses has led to an increased demand for office desktops. However, the lack of proper evaluation of system performance and a limited understanding of practical usage conditions often hinder efforts to select the best desktop among multiple alternatives. In order to improve the evaluation process of desktop performance in centralized procurement, we introduce cpsmark+, a comprehensive benchmark system that assesses office desktop performance based on simulated user experience. This system includes scenario-oriented workloads representing typical user behaviors in modern office settings, and it employs adaptable metrics that properly reflect end-user experience across different types of tasks.

A comparison with the state-of-the-art benchmarks shows that cpsmark+ is highly sensitive to various hardware components, such as the CPU, and exhibits high repeatability with a coefficient of variation of less than 3%. In a practical case study, we demonstrate the effectiveness of cpsmark+ in simulating user experience of tested computer systems, particularly under modern office-oriented scenarios, with the aim of improving the quality of office desktop performance evaluation in centralized procurement.

In contrast, existing benchmarking approaches like Sysmark 2018 utilize real-world third-party software as workloads to assess overall computer performance, but they are limited in their ability to model cooperative workflows across tasks in a common work setting. Furthermore, these approaches often focus on CPU-intensive workloads, neglecting the importance of graphics and I/O performance that are significant to end users in daily use. Additionally, they may isolate system responsiveness and program start-up measurements, which diminishes the practical reference value of benchmarking results.

Historically, computer performance evaluation relied on comparing hardware specifications and typical metrics such as latency and throughput. However, with advancements in computer architecture, the relevance of these traditional benchmarks has diminished, as they do not account for the evolving user experience requirements and usage scenarios in modern office environments. Balancing the relevance and universality of benchmarks is crucial in accurately reflecting user experience and describing system performance based on specific purchase demands.

Our approach to benchmarking office computer performance in centralized procurement focuses on scenario-oriented applications and software manipulations that replicate real user behaviors in modern office settings. Specifically tailored workloads closely correlate to the behaviors and intended usage of end customers, which is essential in the context of centralized procurement where end users have limited influence on purchase decisions.

In our methodology, we adopted an API-level implementation approach to automate the execution of workloads, which allows for more accurate reflection of user experience for performance evaluation compared to UI-level implementation. This decision offers tangible benefits in terms of providing a clear view of workload conditions and reducing the influence of the benchmarking system on system performance.

Moreover, our benchmark system incorporates weighted modules for different usage scenarios to ensure that the evaluation reflects the priorities and expectations of real end users. The weights of these modules are determined based on the average values derived from survey results from the real end users, ensuring that the benchmarking system aligns with actual user preferences and requirements.

In conclusion, our work presents a method for designing a comprehensive benchmark system, cpsmark+, which emphasizes the importance of simulating user experience in evaluating office desktop performance for centralized procurement. Our findings demonstrate that this new approach, compared to traditional benchmarks, has the potential to contribute to improvements in the bid evaluation process and the overall quality of office desktop procurement. In the future, we plan to explore the introduction of benchmark scores in traditional bid evaluation methods in order to improve the centralized procurement of office desktops further.