disease information. through the epidemic intelligence(ei) concept, their mainstreaming into surveillance systems has been a paradigm shift for disease surveillance and control. driven by the international health regulations(ihr)(who, 2005), ei integrates two components in a single surveillance system: indicator-based surveillance(collection of structured data through traditional surveillance systems) and event-based surveillance(collection of unstructured data from informal sources). combining these two components has proven to enhance surveillance systems' performance by increasing outbreak detection timeliness and number.



informal sources cover a diverse spectrum, but they all share the information in textual format. peculiarities of textual data include linguistic ambiguities, redundant and noisy information, a lack of normalisation, etc. besides, daily amounts of such information can rapidly overwhelm surveillance systems, including moderation steps performed by experts. event-based surveillance(ebs) systems thus



the motivation of this work is to use simple and effective statistical measures to extract epidemiological events that are easy to analyse by experts. the measures studied and extended in this paper use weak knowledge based on the number of examples in textual data without the need to determine counter-examples. the main objective is to determine which parameters that we need to integrate. we focus on 2 main parameters associated with our statistical methods and unsupervised approaches:(i) what textual context to use(i.e. document, sentence, and word) for extracting pairs of elements to define an epidemiological event for animal disease surveillance and(ii) which pairs of entities and generalisation to apply for event extraction. this type of results could be relevant insights for integration in embedding approach architectures.



section 2 presents the related work on event-based surveillance systems, entity extraction, entity normalisation and linking and event extraction applied to animal disease surveillance. section 3 presents our global process in order to extract relevant events. sections 4 and 5 discuss the results obtained with different strategies.



ebs systems were pioneered in 1994 by the international society for infectious diseases(isid), through the program for monitoring emerging diseases(promed). promed is a human-curated system that relies on an extensive network of experts worldwide who detect and share reports on disease outbreaks using a common platform(carrion and madoff, 2017). moderators validate the information.



biocaster and the platform for automated extraction of animal disease information from the web(padi-web) rely on fully automated pipelines. biocaster was a public health surveillance system supported by the university of tokyo from 2006, with a priority focus on the asia-pacific region. biocaster is no longer operational, but it is included in our review because it relied on a unique and well-documented ontology-based approach. padi-web was created in 2016 to monitor online animal health-related news for the french epidemic intelligence system(feis).



between these two extremes of pure automation and pure manual data collection and analysis, other prominent systems combine automated text-mining based steps and a dedicated team of curators to assess and verify the outputs. semi-automated systems include healthmap, founded by the boston children's hospital in 2006, the canadian public health agency global public health intelligence network(gphin), the european union medisys, argus and aquatichealth.net.



i.e. nonspecific domain entities(dates, locations, etc.). recently, neural network training algorithms have shown great success in several nlp tasks, including named entity recognition. these models have achieved state-of-the-art results while alleviating the burden of the amount of feature pre-processing(chiu and nichols, 2016). the rnn-based



thematic entities are usually normalized to their canonical form(e.g. disease acronyms are converted into the full disease name). gphin provides a link between the detected entities and umls terminology and definitions. biocaster ontology provides access to term definitions, synonyms and translations in eight languages, along with a link to medical ontologies(including icd-10, meddra, mesh and snomed-ct).



event extraction methods have been extensively studied in many domains such as business and financial, biomedical(zhu and zheng, 2020), and outbreak-event detection domains.(xiang and wang, 2019) propose a comprehensive and synthetic survey of event extraction methods. briefly, they include pattern-based methods, machine learning methods(supervised or semi-supervised), deep learning methods, and unsupervised methods. in the studied ebs systems, healthmap, padi-web, biocaster and medisys include an event extraction step, all of which rely on a different



biocaster event extraction uses a simple rule language(srl), inspired by the so-called declarative information analysis language(dial). srl creates sophisticated matching patterns combining entity classes, string literals, regular expressions, entity types including verbs of infection, common victim expressions, occupation names, etc.



to address this issue, we propose association mining methods(i.e. unsupervised machine learning approaches) in order to extract entity co-occurrence in news articles. more precisely, our approach involves two steps:(i) the detection of pairs of entities based on their relative powhere px is the probability of occurrence of x, py is the probability of occurrence of y, and pxy is the probability of co-occurrence of x and y(joint probability). mutual information is sensitive to rare and specific co-occurrences.



to evaluate the proposed approach, we first annotated a corpus of news articles with events(section 4.1). we further used the list of annotated events as a gold-standard to automatically determine the relevance of the retrieved pairs of entities(section 4.2). the quality of the



an epidemiologist read each of the 438 documents to detect all disease events contained within them. to ensure consistent and reproducible annotation, events found in the documents were compared to a gold-standard database, i.e. the emergency prevention system for priority animal and plant pests and disease(empres-i) database. empres-i is a publicly available animal disease information system created by the food and agriculture organization of the united nations(fao). among other sources, empres-i stores the official notifications from the world animal health organization(oie). each detected event was labelled using the unique empres-i identifier. when the epidemiologist could not link an event to an official one, she created a new event identifier and manually recorded the epidemiological features(location, date, disease and host). the final corpus annotated with the event identifiers is hereafter referred to as the event corpus.



overall, 771 events were detected in the corpus. among them, 70%(n= 541/771) were reported in a single news article. the events present in several news articles were reported in up to 11 news articles(median number of 3 news articles).



in the following experiments, we selected only news articles containing at least one event(a corpus of 229 documents). even if still modest in size, our corpus is highly specialized regarding both its domain(i.e. animal health) and its nature(i.e. online news articles).



we extracted all the disease-host and disease-location pairs using the co-occurrence parameters described in section 3.1. the word window size ranged from 1 to 200 words on each side(left, right, and both). this window was chosen by for an event extraction task, assuming that most relevant information would be present in the first 200 words. the sentence window size ranged from 0 to 20 sentences per side. we ranked the retrieved pairs in decreasing order based on their mutual information or dice values. we evaluated the quality of the ranked list according to the parameters and association measures' ability to assign a better rank to relevant pairs than to irand two spatial generalisation levels. for left side, distances are converted to their positive value. horizontal lines correspond to the normalized f-measure values at document-level. level 0: no generalisation, level 2: generalisation at the country level.



(on both sides), and the scores slightly decreased with the largest window sizes. at the country level, the mi f-measure remained below 0.70 while that of dice ranged from 0.80 to 0.88. the dice ranking reached maximum values between 100 and 125 words(both sides) and remained increased for all window sizes.



the statistical approaches used in this paper(i.e. dice and mi) detected event-related pairs of epidemiological features with a good trade-off between precision and recall. our results showed that using a window of words outperformed document-based and sentencebased methods while reducing the probability of detecting false pairs.



the use of the bert(bidirectional encoder representations from transformers) model for event extraction could represent an attractive future work as discussed in section 6. but the use of pre-trained language models has some limitations with respect to the specific domain addressed in this paper. for example, we studied the use of embedding methods like the word2vec model that consists in a 2-layer neural network. we used a pretrained corpus(i.e. google news corpus with 3



several gold-standard disease-location pairs were not detected due to a linkage between geonames and the global administrative unit layers(gaul) used by the empres-i database. in the latter, taiwan and hong kong are considered distinct countries. on the contrary, the geonames hierarchy considers them as administrative units of china. used a manual procedure to link both databases.



were about the foot-and-mouth disease in endemic areas, and the last article referred to a suspected case of african swine fever. such cases were rare in the studied corpus. still, it should be noted that the host attribute is not necessarily communicated in news content, either because the disease is yet well known(e.g. endemic disease in an area) or because the event is only a suspicion. the host is thus implied because this information is secondarily compared to spatial features. this behaviour may bias models that determine news articles' relevance based on the presence or absence of a host's name.



before applying extraction methods, expansion of text content could be proposed using word embedding architectures like the bert model. bert produces word representations that are dynamically informed by the words around them(i.e. context dependent). this model achieved new state-of-the-art results for several nlp tasks.



