as software systems become increasingly massive, the advantages of automated transformation tools are clearly evident. these tools allow the machine to both reason about and manipulate high-level source code. they enable off-loading of mundane and laborious programming tasks from human developer to machine, thereby reducing cost and development timeframes.



although many transformation tools can retain commenting[4, 10], they often cannot be replaced in their original position(e.g. they may appear on different lines). comments that have not been accurately replaced can, in certain circumstances(e.g. protocol field identification), be hazardous to any later maintenance activities.



a key challenge in forming the ll-ast is ensuring that the representation is semantically correct. the use of conditional directives(i.e.#ifdef,#if,#elif,#else,#endif) is particularly pertinent to this issue. for example, consider the following fragment of code where conditional directives are used to form parallel branches:



given a transformation objective of ensuring that values returned from function f() are assigned to variables of type t1, whilst values returned from function g() are of type t2, one cannot correctly transform the above example without associating the appropriate declaration of variable x to each of the assignments. for this reason, maintaining parallel branches in the ll-ast significantly increases the complexity of the transformation logic. we therefore argue that conditionals should not be integrated into the ast grammar directly.



y. furthermore, the symbol x can be used in an arbitrary position and yet still be valid. effectively, this means that any input symbol can be replaced by any other symbol, resulting in a language that cannot be captured as a context-free grammar. consider the following excerpt which is perfectly valid to the preprocessor, but does not conform to the c++ grammar(due to the statement x(10)z), and therefore cannot be parsed into an ast.



rect ast. for example, macros may be used in a form that can be parsed by the c/c++ grammar and expressed as part of the ast(macros used for predefined constants or as function calls fit in this category). however, incorporation of macro usage into the grammar will likely result in mis-representation in the ast. for example in the previous excerpt, x(10) is not a function call, but rather a for loop.



although proteus expands macros, it still permits transformations on their uses. proteus uses annotations on the ll-ast to indicate that a piece of program text is the result of an expansion(see section 2.2.1). if a given transformation alters an instance of a macro body beyond simple parameter alteration, then the macro will not be replaced in the reconstruction process(one could of course define a new macro for the modified version). the metadata tag information also includes the original parameters. this helps in cases where defined parameters are not used in the macro body. in addition, a special form of meta-data tag is used for macros that are defined as null; these are left in comment form and not converted to annotations.



include directives present an additional problem similar to that of macro usage in that they can also be viewed as a form of symbolic replacement. for example, the following excerpt is perfectly valid c code. although this form of program is valid, in our experience it is very rarely seen. in the 6 million lines of code that we tested proteus on, this construct did not occur. as a result, we made the decision not to address this phenomena.



with respect to the c and c++ grammars, proteus uses its own implementations based on the language standards. the grammars are written in sdf(syntax definition formalism) and used as input to the sglr parser. modifications have been made to the sdf tools so that signatures, that allow one to manipulate a tree of the given grammar, can be generated with the literal and layout terms included.



as discussed previously in section 2.1.3, comment-based meta-data tags are used to record macro expansions on the code. tags in the ll-ast are parsed as layout and hence embedded in layout nodes. this form is particularly susceptible to disruption during the transformation process for instance when layout is being intelligently manipulated by the system(refer to section 3.3). loss of integrity in sme,eme tag pairing can lead to problems in later stages of macro replacement.



by applying annotations across all of the expanded macro text the annotations become more resilient to disruption. for example, if a given transformation changes only part of an expanded macro tree, lets say a parameter, then the macro can be still be replaced(see section 4.2).



the ll-ast is the basic subject of transformation; tools that are built using proteus perform transformations directly upon them. the complexity of the ll-ast is hidden from the developer by a transformation language that we developed called yatl(yet another transformation language), which provides abstractions known as super-types. unfortunately extensive discussion of yatl is outside the scope of this paper.



the following example illustrates the use of the functioncall super-type. the objective is to replace calls to function boo with calls to a function foo. the replacement of the identifier is made in-place, leaving the surrounding terms, including the parameters and layout unchanged.



the current implementation supports the use of free-text for statement and expression construction(the intention is to also extend its use to allow construction of trees for matching). to assist in formatting the text, yatl allows the programmer to mark the left hand edge with a colon. this defines the left hand margin for relative indentation.



free-text ll-asts are generated by the system building a dummy program from the specified text. the dummy program is parsed externally with the sglr parser, either by the yatl compiler at compile time(when the fragment is static) or by the transformation tool at run-time(when yatl variables are used in the fragment). compiling the free-text statically leads to better run-time performance. the ll-ast sub-tree corresponding to the fragment is extracted from the larger ll-ast.



proteus tries to leave existing layout whenever possible. otherwise, layout consistent with surrounding context is used. this is significantly different from traditional pretty printing where existing layout is completely ignored. in comparison, proteus embeds the original layout information in the tree and uses it as a reference point for laying out newly inserted code.



however, the normal proteus behaviour is to only delete layout that is directly associated with the statement that is being deleted, i.e. those on the same line. therefore the deletion of the data field will only lead to the removal of layout on line 5. hence, the/* optional fields*/ comment will be left in tact, only the/* data*/ comment will be removed; of course proteus provides means to retain this comment should it be necessary.



in certain situations it is useful to transform preprocessor directives that have been embedded in layout(refer to section 2.1.1). a prime example is the insertion of new#include statements into a programthis is particularly useful for software migration applications. one could pattern match on the embedded strings and manipulate them as necessary. however, this can quickly become very complicated.



the second-level tree is reconstructed. second-level parsing is performed at tranformation time. as transformation on preprocessor directives is not particularly common, it is more beneficial to perform it on demand rather than on all pre-processing directives at ll-ast construction time.



the next step is to replace preprocessor macros and unmask conditional directives in the re-constructed program text. we have developed the recond tool for this purpose. macro replacement is performed through the use of reverse regular expression pattern matching on meta-data tag pairs. the regular expressions for the reverse match are constructed from the macro definitions previously extracted by the decond tool. as mentioned earlier, the current macro expansion meta-data does not necessarily map to a single instance of a macro definition. instead, all definitions of a macro are tried until one reverse-matches. to allow macro replacement when macro parameters have changed, the regular expression includes wildcard expressions in parameter use positions. unused parameters are replaced with the parameter identifier itself.



if macro replacement fails, the expanded text is left in place without the tags and an optional comment is inserted to indicate that replacement was not possible. partial replacements can also occur when some nested macros can be replaced but one or more outer macros cannot. finally, the recond tool is also responsible for removing any masks introduced during the slicing process(refer to section 2.1.1) and replacing line continuation characters.



the last stage of the re-construction process is to use the proteus merge tool to combine transformed slices into a single unified version. differences in all slices of the same source file were included in the merged version and surrounded with appropriate pre-processing conditionals. if the merged file is branch-sliced again, we will obtain transformed code for that specific slice. the merge process only introduces new changes where code has indeed been modified. otherwise, the original program remains intact.



upon investigating the file differences, we found out most(about 90%) of the changes were due to spacing changes in function like macro invocations(e.g. m(a, b) being reconstructed to m(a,b)). other differences between the source and transformed files result from the use of macros that concatenate parameters(a## b) in their definition. doing so results in the inability to define partitioning of the expanded form, and hence reconstructing the original form is not possible. we are cuurently working on an enhancements to proteus to address these problems.



the basic idea of using source text markup to retain important aspects of transformed program code was also proposed by dean et al. in their system for cobol language transformation. although the basic principles can be transferred to other programming languages, their solution does not readily address the complexities of c and c++. in comparison to our own solution, their solution generates multiple parallel versions(known as factors) of the source text with appropriate markup annotations. the individual factors(including one that contains commentary text) are then combined in a posttransformation stage. this post-transformation phase relies on performing matches across the original and transformed versions(essentially formulating a mapping), a process which is inherently error prone in more complex multi-line transformations.



work carried out by cox et al. has also looked at using a markup language to record modifications on original code caused by preprocessing. their solution uses xml as the markup language. the focus of their solution is on cross-referencing program elements from parser-based analyzers back to the original source code.



another body of work relevant to that presented in this paper, is the work done by baxter et al. of semantic designs. although their solution, dms, is clearly related to our own, their design focus has been different. for example, their solution is aimed at supporting multiple target programming languages as well as cross-language transformations. they do not place any importance on high-fidelity transformation capabilites and make little comment on how they deal with code versioning system problems that typically arise from pretty printing. problems relating to preprocessing have not yet



in this paper we have discussed how the proteus c/c++ transformation system is able to perform high-fidelity transformation. we have shown how careful construction of a specialised form of ast, the ll-ast, allows useful lexical detail to co-exist with higher-level abstractions. furthermore, we have illustrated how proteus is able to attain semantically correct programs through recorded macro expansion coupled with slicing and merging of parallel conditional branches.



the solution presented in this paper has already been successfully applied to over 6 million lines of commericial source code in a version managed environment. we believe that addressing the problems of practicality is absolutely vital to the progression of automated transformation technology. now we can look to the exciting opportunities that this unveiling field of software engineering has to offer.



