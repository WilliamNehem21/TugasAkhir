being a valuable source of information for organizations, the need to make accurate sa is an important issue. most opinions gathered from arab social media is in dialectal arabic, as the use of modern standard arabic(msa) in social media is rare. research work that handles dialectal arabic is still in its infancy and the accuracy still needs tuning. also dialectal arabic is a dynamic language where new words and terms are continuously being introduced by new generations, besides words usages changes over time. the proposed approach is based on the hybrid method that builds lexicons and uses them to train a machine learning based algorithm. it introduces a semi-automatic learning system that copes with the dynamic nature of the dialectal arabic as it crawls twitter for new tweets and extends the lexicon by extracting words that do not exist in it and try to guess their polarities. hence, the name hilatsa(hybrid incremental learning approach for arabic tweets sentiment analysis). the main contribution of this work is introducing a sentiment analysis tool for arabic tweets along with its ability to cope with the rapid change of words and their usages. as a part of the proposed approach some essential lexicons are built(words lexicon, idioms lexicon, emoticon lexicon and special intensified words lexicon). besides, two lists including the intensification tools and the negation tools are created. all these will be available for public use to overcome the problem of lack of dialectal arabic resources. the paper also investigated the effectiveness of using levenshtein distance algorithm in sa to cope with different word forms and misspelling.



the rest of the paper is organized as follow. section two describes briefly the related work done in the field of sa. section three provides the details of the methodology along with the used datasets, lexicons and classifiers. section four presents the results achieved on the different datasets and discusses them. finally, section five provides the conclusion of the whole work.



building a sentiment analysis tool for social media is an emergent task due to its wide spread usage and the valuable information that can be produced from it. while a lot of work was done on english, the research on arabic is still in its early stages and many of the resources are not available for public use. although some of the built lexicons could be extended, to the best of our knowledge none of them were proved to be able to cope with the dynamic nature of the language over time without the need of retraining. this section provides a quick overview of some of both arabic and english tools.



ibrahim et al. built a system for msa and colloquial arabic. they created two lexicons one for words and another one for idioms. they detected negation tools, intensifiers, wishes and questions. a support vector machine(svm) classifier was used to detect the sentence polarity.



duwairi et al. converted the emotions as fx1, fx2 to their corresponding words. also, they converted both dialect sentences and franco arab words to their corresponding msa. their highest accuracy was achieved by using naive bayes(nb) classifier.



zhang et al. proposed a new entity-level sentiment analysis method for twitter with no manual labelling. first of all a lexicon was created manually in order to determine the tweet sentiment polarity. then, opinion indicators as words and tokens were extracted using chi square based on the lexicon. finally, a classifier was trained to identify the tweet sentiment polarity.



nodarakis et al. proposed a distributed parallel algorithm in spark platform. in order to avoid the intensive manual annotation, tweets were labelled based on the emotions and the hashtags. a bloom filter was applied to enhance the performance after building the feature vector. finally, all k nearest neighbour(aknn) queries are used to classify the tweets.



poria et al. created a seven layer deep convolutional network for the aspect extraction sub-task. the system tags each word in the document as aspect or not aspect. their approach gives better accuracy than both linguistic patterns and conditional random fields(crf).



in this work svm, l2 logistic regression and recurrent neural network(rnn) classifiers are used. svm is a linear classifier which creates a model to predict the class of the given data. it solves the problem by finding a general hyperplane with the maximum margin. it supports many formulas for classifications. this work uses c-svm as the classifier where c is the penalty or the cost of wrong classification. the cost limits the training points of one class to fall in the other side(each side of the hyperplane represents a different class) of the plane. the higher the cost, the lower the number of points will be on the other side which in some cases may lead to over-fitting issue. also, the lower the cost the higher will be the possibility of wrong classifications. when the cost function is high, the classifier selects a hyperplane(boundary) with a small margin which may give a good result regarding the training data, but may cause an overfitting issue and low accuracy regarding the testing data. on the other hand, when the cost function is low the classifier selects



logistic regression is a discriminative classifier. it solves the problem by extracting weighted features, in order to produce the label which maximizes the probability of event occurrence. regularization produces more general models and reduces overfitting by ignoring the less general features.



using 10-fold cross validation with astd, mastd, arsas, gs and the syrian corpus training datasets the word lexicon is built(different lexicon for each fold excluding the testing part) and the classifiers are trained. this section describes the results in terms of accuracy and average f1 score(avg. f1). in addition, comparisons of the achieved results with other available work are also provided.



in summary, svm tends to have better accuracy in most cases which is consistent with. that is due to the kernel trick which allows a better accuracy with higher dimension data. in addition to that, it is less sensitive to outliers and less prone to overfitting. also, 3-class sentiment analysis is more challenging than 2-class which is expected. in addition, in most cases f1 score is higher for balanced datasets than unbalanced datasets.



using the dataset(artwitter) from a classifier is trained using the same lexicons from the previous part(built from datasets other than artwitter). after that, the tweets are classified using 10fold cross validation for training and testing. then the words learner is used to learn words and update the lexicon. finally, the tweets are classified again after updating the lexicon.



