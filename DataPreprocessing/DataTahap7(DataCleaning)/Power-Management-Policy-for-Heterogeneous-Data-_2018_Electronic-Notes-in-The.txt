several studies as in[23,10,17,5] show a significant augmentation of energy consumption and digital pollution caused by recent expansion of clouds and data centers. more than 1.3% of the global energy consumption is due to the electricity used by data centers. additionally, one data center server can produce more than 10 kilogram of co2 per day, rates that are increasing, revealed by surveys conducted in[23,40], saying a lot about the increasing evolution of data centers. thus, needs for energy saving is emerging to consider a power management strategy face to the energetic problems and digital pollution issues. data centers are designed to support the expected peak traffic load, however the global load is about 60% of the



studies like[26,3] show that much of the energy consumed in the data center is mainly due to the electricity used to run the servers and to cool them(70% of the total cost of the data center). thus, the main factor of this energy consumption is related to the number of operational servers. an important effort was focused on servers and their cooling. efforts have been made to build low-energy-consumption processors and better components, more efficient cooling systems, optimized kernels, and more efficient energy network. against this background, complementary saving energy approach is to consider a power policy to manage the switching-on/off of servers in a data center to ensure both a better quality of services offered by these data centers and reasonable energy consumption.



it is known that the size of an mdp is important, and the computation of the optimal policy can be hard even impossible for big models. indeed, it is crucial to analyze the structural properties of the optimal policy to speed-up the computation. several works as in[46,29,43,19,38] investigate the structure of optimal policy and give conditions to check double-threshold structure.



the rest of the paper is organized as following. section 2 shows how the arrival jobs can be modeled by discrete distribution obtained from real google traffic traces[47,41]. we also suggest a method to build a discrete identically independently distributions. section 3 models the system by simple queue. then, section 4 formulates the optimization problem as discrete time mdp. after that, in section 5 we prove some structural properties related to the optimal policy. finally, section 6



example 2.1 assume that, per slot, we have a probability of 0.14 to receive 3 arrival jobs, 0.19 to receive 7 arrival jobs, and 0.67 to receive 11 arrival jobs. in this case, arrivals are modeled by histogram ha=(sa, pa) where sa={3, 7, 11}, pa(3)= 0.17, pa(7)= 0.19, and pa(11)= 0.67.



a data center is composed of heterogeneous servers grouped essentially into several levels of energy consumption. to simplify we consider the set of the following levels g={high, med, low}(med for medium). the number of operational servers of type g is denoted by mg and maxg is the maximal number of servers of level g.



(i.e. waiting and loss jobs) and the energy consumption(i.e. number of operational servers). however, as the number of servers changes with time, the system becomes more complex to analyze. the number of servers may vary according to the traffic and performance indexes. more precisely, depending on n, l, and a particular cost function some decisions are taken to change mhigh, mmed, and mlow.



as shown previously, the size of the mdp is important, and the computation of the optimal policy can be hard, and even impossible for a big data center. in fact, it is essential to analyze the structural properties of the optimal policy to make the computation efficient. in particular the double-threshold structure was proved in the case of a homogeneous data center in several models like in. in the following we will be interested in some properties around the optimal policy, and we show in particular that the property of the double-threshold structure does not hold for our heterogeneous data center model.



this case study uses real traffic traces based on the open cluster-data-2011-2 trace[47,41]. as in, we model arrival jobs, and additionally service rates, by discrete distributions build from the job/machine events corresponding to the requests destined to a specific google data center for the whole month of may 2011. this traffic trace is sampled with a sampling period that ensure the i.i.d-ness assumption. thus, we consider frames of 136 second to sample the trace and construct empirical distributions.



to write a specification for our mdp model which can be written easily if the system parameters were small, but when we consider real systems with big parameters it will be very hard. as an example the prism specification associated with a data center with 10 heterogeneous servers is a file of several thousands of lines, all lines are different from each other. trying to write our specification without automated generation is time consuming, gives rise to making easily mistakes, forgetting some cases, losing time for updating or maintaining. indeed, as the specification of our heterogeneous model is huge and not obvious to write(see section 4), we have coded a tool that helps us to generate automatically the prism specification file.



results show that heterogeneous model saves more energy(12% better then homogeneous model). however, the size of the three homogeneous models is relatively small compared to the size of the heterogeneous one which is huge. the size of heterogeneous model is due to the large number of different action combinations.



