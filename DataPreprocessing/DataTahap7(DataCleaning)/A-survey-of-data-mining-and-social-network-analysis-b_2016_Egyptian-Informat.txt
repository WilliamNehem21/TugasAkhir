abstract with the increasing trend of online social networks in different domains, social network analysis has recently become the center of research. online social networks(osns) have fetched the interest of researchers for their analysis of usage as well as detection of abnormal activities. anomalous activities in social networks represent unusual and illegal activities exhibiting different behaviors than others present in the same structure. this paper discusses different types of anomalies and their novel categorization based on various characteristics. a review of number of techniques for preventing and detecting anomalies along with underlying assumptions and reasons for the presence of such anomalies is covered in this paper. the paper presents a review of number of data mining approaches used to detect anomalies. a special reference is made to the analysis of social network centric anomaly detection techniques which are broadly classified as behavior based, structure based and spectral based. each one of this classification further incorporates number of techniques which are discussed in the paper. the paper has been concluded with different future directions and areas of research that could be addressed and worked upon.



degrees in 1997, online social networks such as twitter, linkedin and facebook have attracted large number of people. at present, almost every domain is linked in one form or the other with the social networks. be it entertainment, education, trading, business, communication etc., osn has made an influence on each of them. for example, mostly companies have started promoting their brands and products on social networking sites to increase the popularity of their products which in turn enhances their sales.



the presence of anomalies in our data poses many problems which need to be tackled carefully. for example, some sort of malicious users may construct a set of false identities and use them to communicate with a large random set of innocent users. hence, detection of these anomalous activities in a network is a big concern as their presence may lead to heavy losses. for example, in a computer network an anomalous traffic pattern could mean that a hacked computer is sending out sensitive data to an unauthorized destination. nowadays, not only the detection but the reason why these activities took place along with the methods to prevent these behaviors is on the rise. here in this paper, various techniques used to detect and handle the anomalous behavior are covered. at first, a generalized view of various data mining techniques applicable to multiple domains and applications is given and then a special reference is given to some of the popular anomaly detection methods applicable to social networks.



the paper is organized into different sections. section 2 contains the novel categorization of anomalies on the basis of number of parameters. the major data mining and social network techniques for anomaly detection have been discussed in sections 3 and 4 respectively. finally, section 5 presents conclusion along with some future directions that could be addressed.



behavior attributes: characteristics of an object are defined using these attributes and in a way help to identify the anomalous behavior of an object with respect to its context. in the temperature example, temperature, humidity etc. can be considered as behavior attributes.



has evolved in social networks which depict the presence of anomalies based upon the different sources of data available. for example, the same user may be present in different communities on different social networks. similarly, a user may have similar kinds of friends on number of social networks(e.g. facebook, google+) but completely different kinds of friends for another social network(e.g. twitter). this depicts an unusual activity which can be considered as anomalous.



unlabeled anomalies are related only to the network structure. no attribute of a node or an edge is taken into consideration. their classification is mostly studied as follows and different techniques have been developed and deployed to detect these types of anomalies. a number of such techniques have



static labeled anomalies are used in spam detection, for example, to detect opinion spam(which involves the fake product reviews). a set of hidden labels are usually assigned to the vertices and edges which are iteratively updated. in the product review system, a bipartite graph with one subset of vertices as users and other as products is taken in which the edges between the subsets represent the product reviews. hidden labels are assigned to both users and products. for users the label can be in the form of honest or fraudulent and for the products it could be either good or bad. a normal honest user will give accurate results i.e. for good products they give positive response and for bad ones they will give negative reviews whereas fraudulent users are understood to do the reverse.



this type of anomaly arises when we have dynamic networks that change with time. behavior of the data object is different with respect to previous time period relative to the network structure. for example while considering only the pattern of interactions, there are maximum of six ways in which a maximal clique can evolve: shrinking, growing, splitting, merging, appearing or vanishing. all of these involve studying the network structure with respect to the network structure prevalent at some previous time period. sometimes, the normal behavior does not result in any network change; then, any neighborhood changes may also predict an anomalous behavior.



it arises when one data object deviates significantly from other observations resembling the basic anomaly definition. for example, while examining the student record, if a record is found where height of a student is entered as 56 ft, which is impossible, then it is taken as a white crow anomaly. these anomalies are mostly detected as particular nodes, edges, or subgraphs representing the abnormal behavior.



a data set or a network may contain more than one kind of anomaly. some of these anomalies can be clubbed together to form a hybrid set. as an example, savage et al. studied the classification of anomalies as a combination of static/dynamic and labeled/unlabeled.



deletion: deletion involves the absence of an expected vertex or an edge. sometimes, it even incorporates the concept of dangling edges i.e. with the deletion of a particular vertex all the adjacent edges to it may also have been deleted.



the major task involved in classification approach of supervised methods is to make the classifier learn. a classifier can be constructed in numerous ways. for example, it can be neural network based[22,23], support vector machine[15,16,24], bayesian network based[25,26] etc. supervised anomaly detection methods should keep in consideration the following two aspects: mal are available. using the small amount of labeled data a classifier can be constructed which then tries to label the unlabeled data. hence, a model for normal data objects is built which is used to detect the anomalies in a way that the objects not fitting the normal model are classified as anomalies. this is the simplest approach called self-training used under semi-supervised model. another method called as co-training can be employed where two or more classifiers train each other. self-training is more sensitive to errors than co-training. it is known as semi-supervised as it partially functions as supervised methods because only the normal class is taught and the algorithm learns to identify anomalies by itself.



for higher dimensions the approach can be improved using hilbert space filling curve. the multi-dimensional space in grid based approach is extended by angiulli and pizzuti to handle the high dimensional data more efficiently. hilbert space filling curve is used along with hilout algorithm, an algorithm defined to choose the anomalies based on their aggregate score with their neighbors rather than one absolute score. for each object o, weight w, is computed as:



as the dimensionality increases the question about why and up to what extent the data object is an anomaly is of more concern rather than just predicting out anomalies. one of the simplest approaches toward it is to compute sparsity coefficient. the more negative its value is, sparser a cell(hypercube) is and more likely the objects in c are anomalies.



the major problem associated with distance based methods is its failure to detect local anomalies which can be easily overcome by density based methods. density based approaches work by comparing the density of an object with density around its neighbors. for a normal object both densities are assumed to be same whereas for anomalous objects they are different. the concept of relative density is often used to measure the degree of anomalous behavior of an object.



in order to make the approaches computationally less expensive some sort of statistical measures are added to them. for example, instead of using the densities as it is the computation of standard deviation of densities led toward an approach named as multi-granularity deviation factor(mdef) as suggested by papadimitriou et al.. similarly, local outlier probability(loop) method also makes use of the statistical measures and estimates the probabilistic lof as a factor of ratio of densities to finally compute the measure called as loop.



for this case, simply the density based clustering approaches can be used simplest of which is the dbscan and its numerous variants. dbscan checks the density around each object and the one being isolated or of lower density than others is considered as an anomaly. one of the striking features of this method is that it can detect the clusters of arbitrarily any shape. a number of improved variants of dbscan such as, fdbscan, l-dbscan, c-dbscan, etc. out of all such measures the prominent one is the shared nearest neighbor(snn) method in which the similarity between the data points is identified based upon the number of nearest neighbors shared and hence the core points around which the clusters are to be built are identified. this approach helps to identify the dense as well as medium and sparse clusters.



no doubt, the proposed methods help to identify the anomalies but they focus more toward finding the clusters and considering any point not related to any cluster as noise which in a way is assumed to be anomalous. some of the cluster based methods also avoid finding the degree of anonymous behavior shown by each data object. in order to encounter such problems, numerous advanced approaches have been proposed. for example, cluster based local outlier factor(cblof) and the corresponding algorithm findcblof are used to mine the encountered anomalies. cblof is measured as a factor of both the cluster size to which object belongs and its distance from the cluster it is closest to. findcblof uses the squeezer algorithm which constructs the clusters out of which a set of large and small clusters are formed and cblof is calculated for every data point. in a similar fashion a number of other techniques using different distance measures have also been proposed like, self-organizing maps(som) an unsupervised method proposed by kohonen, k-means clustering[54,55], k-means++[56,57]. as these techniques involve the computation of distance factor, therefore, they are a good way to handle the second case. some of the semi-supervised methods proposed by wu and zhang, vinueza and grudic



this case is handled by defining a threshold value for the clusters and the objects belonging to low value clusters are considered as anomalous. findcblof algorithm detects both the individual objects and points belonging to small clusters as anomalous by computing the similarity between the objects in the small cluster and the closest large cluster. cblof value for such points comes out to be very low. apart from this, other applicable approaches are described in[52,60,61]. distance or densities of the small clusters generated are compared with those of large clusters and anomalies are detected. numerous efficient techniques such as k-d trees and cd-trees are used to partition the data into clusters.



in one class model, only a single labeled class is defined i.e. classifier is constructed to only define the normal class and all those data objects that belong to that class are treated as normal whereas the ones that do not fit in the defined class are treated as anomalies. some of the examples of one class models used for anomaly detection are one-class svm, gaussian model description(gaussd), principal component analysis description(pcad), parzen window classifier(pwc) etc. in each of them a decision boundary is set up. the data objects falling outside the decision boundary are treated as anomalous. one-class models help to detect new anomalous objects that are far from the other anomalous objects present in the given training set.



the other set of model called the multiclass model is used when the available data objects not only belong to a single class but to multiple classes. for example, the classification of a set of images of fruits into the probable classes of apples, oranges or mangoes. every data object may be assigned only one label. just like, a fruit may be classified as either one out of the three categories but not more than one at the same time.



kruegel et al. studied the detection of anomalous behavior using bayesian classifiers. using this approach, all the previously unknown attacks are also identified but with the generation of a number of false positives. a model of the normal behavior is available and deviations from these behaviors are identified as anomalies. there are a number of models which evaluate different set of features and return different probabilistic values as anomalous scores which are aggregated into a single value. but in every such model the final decision process is conducted using a bayesian classifier.



it makes use of a hyperplane as a decision boundary to separate the tuples of different classes from one another. the major task involved in svm is the selection of best separating hyperplane from among several of them. an approach toward this issue is the use of maximum marginal hyperplane(mmh) i.e. the one with largest margin is considered most accurate for classification. svm as a classification measure can be used to detect the anomalies in various applications.



cases, pruning of the networks to include only most relevant relationships is done. in most of the cases, the presence of an anomaly is considered as a binary property in which anomaly is either present or not, but in some applications the extent to which anomaly is present is considered by giving degree of being an outlier to each object in the data set. for example, this degree has been referred to as local outlier factor(lof) by breunig et al..



structure context-based similarity: it is a local cluster or neighborhood based similarity in a way that nodes having similar neighborhood are considered as similar. for example, in social networks, different users getting recommendation about a page or a community etc. from a number of mutual friends usually make similar decisions and help in determining how close they are.



similarity based on random walks: this type of similarity could be well described by this example. suppose, an information or message needs to be forwarded to multiple users. but at an initial stage it is sent to only two users a and b who forward it to their friends. now, the closeness or similarity could be measured by the simultaneous receipt of the message from both a and b to the nodes. so, here similarity is addressed as a random walk measure over the network.



cluster building. the proposed technique is a faster and efficient way to identify fake accounts as it only uses the attributes entered by a user during registration i.e. profile creation. the employed technique is a first in its form to detect the clusters of fake accounts usually created by a single user on a particular social network, thereby superseding the existing techniques which only work and make deduction for a single account. the system was found to restrict around 2,50,000 fake accounts.



the structural properties have been used by most of the researchers working in social network domain to define a number of new approaches for identifying anomalies in online social networks. as an example, link mining, used by getoor and diehl studies the structural properties of the networks to predict different behaviors of individuals in social networks. for instance, a normal tendency shows that consumers, whose friends spend a lot, spend a lot themselves. the concept of link analysis is applicable for both homogeneous and heterogeneous networks, but in the concerned work the graphical structure of heterogeneous networks with different types of nodes or edges is given more focus. by analyzing the association between different nodes it is usually found that the linked objects often have a set of correlated attributes. in other words, connectivity of two users can be checked by examining the common properties as what is usually observed is that the objects sharing some sort of common features are often found to be linked with each other. getoor and diehl covered eight link mining tasks with their respective algorithms and grouped the defined tasks under three categories, namely object-related, link-related and graph-related. most of the structure based link prediction methods show poor performance because of the involvement of prediction of future relationships likely to occur. earlier also a number of advanced tasks such as anomalous link discovery(ald) were proposed which involved only the prediction of anomalous relationships rather than all the involved relationships. it was seen that almost every prediction model performed quite well for ald.



many already existing node-based and egonet-based features were studied recursively by henderson et al.,. some aggregate values were calculated on the already existing characteristics. neighborhood information was retrieved using both node and egonet-based features and behavioral information was extracted using recursive features.



akoglu et al. utilized another structure based approach in which a number of pattern and law discoveries were used by to detect different types of anomalies in social network graph. to spot some strange nodes especially in weighted graphs an oddball algorithm was proposed in which a number of new rules(power laws) were discovered to detect the deviation from the known normal behavior. a set of features were grouped into certain set of carefully chosen pairs and anomalous behavior was analyzed by examining the group structure. groups were formed where the patterns of normal behavior(power laws) were observed and the points deviated from discovered patterns were flagged out to be considered as anomalous. a number of anomalous relationships were observed namely near stars or near cliques, heavy vicinities and dominant edges.



the concept of betweenness centrality formulated by freeman is modified to work for edges instead of vertices to find the number of shortest paths between a set of vertices that pass through the edge under consideration. the implication used is that the edges with high value of betweenness centrality state the points where a network is expected to break and hence are separated. generally, in online social networks high betweenness centrality is found to be at the intersection of densely connected network groups. as a result, a number of significant groups could be determined by removing the set of links from a graph, a concept also used by newman.



randomness tests are applied which involves the computation of different non-random measures specially the nonrandomness for nodes. this non-randomness characteristic is used by a popular algorithm known as spctra to identify the anomalous users. a number of different subgraphs are created where attackers or anomalous nodes are likely to have dense subgraphs. from these subgraphs, a set of nodes are chosen for rla groups. finally, all the dense subgraphs formed by regular nodes are removed and hence, the only ones left are the subgraphs of attacking nodes. the proposed approach supersedes the previous approaches because of the effectiveness of spectral characteristics.



for example, savage et al. surveyed on different techniques applicable for each of the static/dynamic unlabeled/ labeled anomalies. like the various techniques discussed in structure based approaches are used to identify static unlabeled anomalies. in a similar fashion, for detecting dynamic unlabeled anomalies apart from link prediction, other techniques such as bayesian analysis and scan statistical approaches(mainly applicable to hypergraphs) are used with each approach having its own application and benefits. in case of labeled anomalies, a number of techniques have been proposed for static as well as dynamic networks. a number of approaches were discussed in the survey paper for static labeled anomalies such as contextual anomalies, heavy vicinities, and opinion spam. as an example, for the detection of opinion spam a belief propagation method has been applied which deals with a set of hidden labels. one more approach called trust rank was discussed that follows a link analysis perspective in which it is assumed that good nodes would never point to bad nodes. two basic principles followed are as follows: as well. the only constraint imposed is the addition of information regarding the attributes. in most of the approaches the network structure is considered as static for a fixed time period and in order to add the dynamic concept the behavior of different nodes/modules is compared at different time intervals. signal processing works on such principles by using the probabilistic features.



similarly, akoglu et al. gave a survey of different graph based anomaly detection methods covering both the static/dynamic and labeled/unlabeled constraints. in each network structure, different quantitative and qualitative techniques have been very well categorized into different sub modules such as structure based, window based, community based and feature based. moreover, researchers have described a number of real-world applications where graph based anomaly detection methods could be fit, for example, opinion spams, auction networks, social networks, telecommunication networks, trading networks, cyber crimes, security networks to name a few.



recently, there has been an inclination toward detecting anomalies in dynamic networks. therefore, a number of researchers are adding dynamic concept into their research work. for example, a number of anomaly detection techniques specially related to dynamic networks are recently surveyed by ranshous et al.. for instance, a scoring function is used to identify various types of anomalies. categorization of anomalous behavior is based upon the scoring function being used along with the application area under consideration.



also, the most significant and pertinent subset of nodes is used by vigliotti and hankin to detect anomalous patterns in huge dynamic networks. in their work the experiments were performed on the temporal networks. temporal information from two data sets namely vast data set(2008) and twitter data set was taken. in vast data set, the telephonic calls among different nodes are examined. also, the already available techniques are used to predefine an anomalous pattern and the projected approach is just validated over the working data set. but for the twitter network being used no prior knowledge pertaining to anomalous patterns is already known, anomalous patterns and nodes need to be assumed and it has to be tested whether the stated hypothesis regarding anomalous or non-anomalous behavior is true or not.



but gao et al. proposed an advancement in the above approach by integrating both the network and data object information to detect the community anomalies. the proposed approach is called community outlier detection algorithm(coda) which makes use of a probabilistic mixture model designed for multivariate data objects(objects with multiple attributes). statistical anomaly detection approaches were used to detect the community anomalies in which depending upon the type of data associated, different distributions were analyzed where normal data objects were assumed to follow the defined distribution whereas anomalous objects deviate from it or follow some other distributions. in the proposed technique, two types of data objects were usedcontinuous data and text data and for normal behavior they were found to follow gaussian and multinomial distribution respectively. it was found that any encountered anomaly followed a uniform distribution. a set of hidden variables for data objects and hidden markov random field(hmrf) for the network links are worked upon by the defined icm and em based algorithms. in order to make it more effective a set of hyper graph parameters like, threshold(indicating few anomalies for its high value and more anomalies for the low value), link importance(for the prediction of confidence level), number of components(small determining global anomalies and large the local ones) were also defined and used.



the paper is structured into five major sections. section 1 described the importance and growing trend toward social networks along with the presence of anomalous activities in it. a set of widely accepted formal definitions of anomaly have been tabulated. section 2 classified the anomalies into various categories based upon different parameters. finally, sections 3 and



kisilevich s, mansmann f, keim d. p-dbscan: a density based clustering algorithm for exploration and analysis of attractive areas using collections of geo-tagged photos. in: proceedings of the first international conference and exhibition on computing for geospatial research& application; 2010. p. 38.



