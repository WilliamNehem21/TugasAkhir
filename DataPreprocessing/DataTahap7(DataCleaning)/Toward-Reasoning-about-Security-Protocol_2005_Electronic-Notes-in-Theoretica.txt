as a case study illustrating our approach, we will consider the so-called sra three pass protocol and prove a property of it. it is not our intention to prove that the protocol is completely secure(as it is not in full generality), but we will prove that if the agents participating in the protocol are honest, then an intruder observing the communication does not learn anything about the plain-text messages in a single run. furthermore we show what the intruder is able to learn about the agents participating.



the first type of update typically runs as follows: in an open network agent a sends a message to agent b. from a security perspective, it is customary to assume the dolev-yao framework, in which all agents in the network can read this message too. however, also in open networks private learning, the second type of update, can take place. for example, agent b receives a message{x}k from agent a. here{x}k denotes a message with content x encrypted with the(symmetric) key k. if b possesses the key k, then b privately learns the message content x(assuming that the key k is shared among a and b). the final type of update, learning about knowledge of others, is probably the most interesting. it is realistic to assume that the steps in a protocol run are known to all agents. therefore, observing that an agent receives a message will increase the knowledge of the other agents. for example, if agent a sends a message{x}k to agent b, then agent c learns that b has learned the information contained in the message{x}k, but typically, c does not learn x if c does not possess the key k.



stronger types of updates we do not consider here. for example, we will not update the beliefs of an honest agent such that it learns that an intruder has learned about others. in the present paper, we restrict ourselves to updating beliefs about objective formulas and beliefs about objective formulas.



in this section we describes various types of updates in detail. we will start by defining an update for propositions in subsection 3.1. in subsection 3.2 we will define a belief update for agents that learn something about the belief of others. we do this in two slightly different ways by varying in the functions that describe a side-effect for an agent.



we can see that the belief of agent a has not changed: it still considers its old worlds possible. the belief of agent b however, has changed. it now only considers the state u possible where p holds, hence b beliefs p.



a technical obstacle is that states can be shared among agents. it is obvious that if we change a state with the intention to change the belief of one agent, then the belief of the other agents that consider this state possible, is changed as well. therefore, the first thing to do, is to separate the states of learning agents from the states of agents that do not learn. this procedure will be called unfolding. the functions newb and orig are generalizations of new and old from the previous section, but the function orig is only defined



the idea is that once the belief is completely separated, we can not only safely change the belief of a certain agent, but also preserve the kt45 properties. in particular, we change locally the knowledge of an agent a, e.g. regarding uncertainty of another agent b about a propositin p. the operation atomsplit(p,b) below removes the arrows of b between states that have a different valuation for p.



the fact that the agents of type a are the only agents that learn at all, is clear. the other agents consider their old worlds possible; their belief has not changed. with this in mind, we present a few properties of the side-effect function.



part(a) of the above lemma states that agent a obtains derived knowledge of an agent b. part(b) states that no object knowledge is learned. part(c) phrases that agent a considers the rest of the agents as smart itself. finally, part(d) captures that other agents do not learn.



property(c) is a reasonable assumption of a about the other agents in the context of open communication networks(as is the setting of sections 4 and 5). if one agent believes that another agent knows the value of p, then it is reasonable to assume that another agent will believe the same. on the other hand common knowledge might be too strong to assume.



for simplicity of presentation, we assume that there is exactly 1 agent present in each group, i.e. a={a, b, c, d}. we define the new side-effect operation 0-unfold(where 0 refers to zero-knowledge). note that the 0-unfold operation depends on the particular partinioning of agents a. since the operation s0-ide-effect will depend on the unfold operation, the side-effect function is also taken with respect to some chosen partitioning of the agent set.



so instead of completely separating the knowledge of te agent a with the other agents, we share the knowledge of a about d with the other agents. since the other agents do not learn anything, a does not gain knowledge about d. we present a lemma similar to lemma 3.7.



the selection mod(m, w) is organized in such a way that no transition rule is applied over and over again. the recursive definition of d therefore is well-defined, since it stops if no fresh transition rule can be applied. in the definition of mod it is checked if the belief of the agents changes under the transition rules, preventing an infinite chain of rewrites for dp. note that because of the results of the previous section, the order of applying these transition rules does not matter.



