testing and debugging are very important tasks in software development. fault localization is a very critical activity in the debugging process and also is one of the most difficult and time-consuming activities. the demand for effective fault localization techniques that can aid developers to the location of faults is high. in this paper, a fault localization technique based on complex network theory named flcn-s is proposed to improve localization effectiveness on single-fault subject programs. the proposed technique diagnoses and ranks faulty program statements based on their behavioral anomalies and distance between statements in failed tests execution by utilizing two network centrality measures(degree centrality and closeness centrality). the proposed technique is evaluated on a well-known standard benchmark(siemens test suite) and four unix real-life utility subject programs(gzip, sed, flex, and grep). overall, the results show that flcn-s is significantly more effective in locating faults in comparison with other techniques. furthermore, we observed that both degree and closeness centrality play a vital role in the identification of faults.



this work builds on previous work where we have proposed a new fault localization technique based on complex network theory(flcn). statements behavioral anomalies and the distance between program statements in both passed and failed tests execution are the two variables that the technique takes into account. degree centrality and closeness centrality were adopted for fault diagnosis and a ranking formula was also proposed to aid in identifying fault location. the technique locates faulty statements irrespective of whether the statements were



the rest of the paper is organized as follows. section 2 highlights some existing related work. section 3 presents the proposed fault localization technique. section 4 describes the experimental setup, results, and discussion. lastly, the study is concluded in section 5.



former was built for programs with a single fault, while the latter is for programs containing multiple faults. the result of the study showed that o and o<sup>p</sup> are more effective in localizing faults than tarantula. shu et al. proposed a fault localization method based on



an important sampling of program statements was proposed. by utilizing probability updates and sampling, the approach can help identify statements that have a high likelihood of being faulty. the approach was found to be more sensitive to failed test inputs than passed test inputs. in addition, wong et al. proposed two machine learning-based techniques for fault localization, fault localization based on bp(back-propagation) neural network and fault localization based on rbf(radial basis function) neural network to localize faults effectively. the result shows that these techniques are effective in locating program faults. however, these techniques have problems of paralysis and local minima. in another study by zheng and wang, a fault localization based on deep neural network(dnn) was proposed to tackle the problems of paralysis and local minima. dnn was found to be very effective in comparison to other machine learning-based techniques.



to improve the effectiveness of the former fault localization technique(flcn) in the single-fault context, we proposed a technique named fault localization based on complex network theory for single-fault programs(flcn-s), which is a variant of our initial technique in ref. coined flcn. flcn localizes faults with the utilization of both passed and failed test inputs. however, from our initial results, we observed that by utilizing both test inputs(passed/failed) in a single-fault context, the technique(flcn) effectiveness reduces significantly. at best, flcn aids developers in locating 40% of the faults by checking less than 10% of the program code in single-fault subjects. this is mainly due to the high sensitivity of flcn with program statements executed by passed test inputs. hence, the effectiveness is not convincing in comparison with similarity coefficient-based techniques such as ochiai, jaccard, execution is labeled as 1, it indicates that the statement is executed by the test input in that test run, and 0 otherwise. for the test result of each test input, 0 means the test input has passed while 1 means the test input has failed. out of the six available test inputs, two of which are failed test inputs tf=(t5, t6) while four are passed test inputs tp=(t1, t2, t3, t4). in order to model the complex network, failed test inputs tf=(t5, t6) is to be utilized. for the first failed test input t5, there is an edge from m1 to m2, m2 to m3, m3 to m5, and m5 to m12. for the second failed test t6, there is an extra edge from m5 to m6 and m6 to m8, respectively.





step 1: in this step, the faulty program p will be executed by all the available test inputs in t and the execution data will be collected. the execution profile of statements with respect to each test input will be collected. the set of passed and failed test inputs will be identified.



and four unix real-life utility programs(gzip, sed, flex, and grep) to evaluate the proposed technique. generally, various studies have utilized these subject programs for fault localization[9,18,19]. siemens test suite programs are utilized because the programs contains single fault each, while unix real-life utility programs contains both real and seeded faults. all of these subject programs are also written in c programming language.



siemens test suite is composed of seven subject programs, namely schedule, schedule2, print_tokens, print_tokens2, replace, tot_info, and tcas where each of the subject programs has more than 1000 test inputs. for the unix real-life utility program, gzip program is utilized for file compression and decompression. the program is normally utilized to decrease the size of name files. the input of gzip program comprises of 13 options with a list of files as well. the program has 6573 lines of code and 211 test inputs. the sed program performs simple changes in an input stream. it is basically utilized to parse textual input and also to apply a specified user changes to the input. the program has 12,062 lines of code and 360 test inputs.



the flex program is a lexical analyzer. it reads a file and produces a c source file called scanner. the input files contain sets of consistent expression and c code, called rules. the program has 13,892 lines of code and 525 test inputs. the grep program has two input parameters which are patterns and files. the program prints lines in each file that contains a match of any of the patterns. the program has 12,653 lines of code and



top-n. top-n symbolizes the percentage of faults a fault localization technique ranks for all faulty statements among the top n(n= 1, 5, 10) positions in the ranked list. hence, the smaller the value of n in top-n, the stricter the metric. for instance, top-5 metric demands that all faults are ranked within the top 5 positions in the ranked list.



in part(c) and(d), the effectiveness score of print_tokens is presented. we observed that, by examining less than 10% of the program code, flcn-s can only locate 65% of the faulty versions in the best case and 35% in the worst case. ochiai(the second best) can locate 60% of the faults in the best case, and 25% in the worst case. moreover, the percentage for tarantula(the third best) is 55%(best case), and 15%(worst case). however, looking at the curves in part(c) and(d), the two techniques(jaccard and sncm) are the least effective on print_tokens program faulty versions.



few scenarios have confidence level that is lesser than 95%, for instance, flcn-s being more effective than ochiai with 90.00% confidence for the best case of tcas, 93.33% confidence being better than ochiai for the best case of print_tokens2, and with 92.31% confidence being better than ochiai for the worst case of print_tokens. in summary,



in our earlier work, we studied the impact of degree centrality and how statements degree relates to faults. we concluded that statement degree is vital in identifying faulty program statements, especially in a multiple-fault context. in this paper, we observed that on siemens test suite programs, 23% of all faulty statements have degree centrality of 3 while 77% have a degree centrality of 2. hence, we found out that in the single-fault context, closeness centrality plays a vital role in ranking and identifying program statements. moreover, on both siemens test suite and unix real-life utility programs, both degree centrality and closeness centrality play a critical and vital role in the identification of faulty program statements.



generally, the effectiveness of a given technique is not always constant and can change depending on the subject program used. we observed that by utilizing failed test inputs alone, the effectiveness of the proposed fault localization technique(flcn-s) has increased on singlefault programs. in our initial work, we utilized both test inputs(passed/failed) to localize single faults, the accuracy was not convincing, where we achieved 40% exam score by checking less than 10% of the program faulty versions on siemens test suite programs. this is mainly due to the high sensitivity of the technique(flcn) with program statements executed by passed test inputs. therefore, we concluded that by utilizing failed test inputs alone, the accuracy of our proposed technique(flcn-s) increases in the context of single fault due to the minimal faultto-failure complexity that affects localization on multiple-fault programs. our technique can effectively localize 65% of all faulty versions on siemens test suite subjects by checking less than 10% of the program code and is largely more effective on unix real-life utility program in comparison with other techniques in both best and worst case scenarios. finally, we also observed that both degree centrality and closeness centrality plays a vital role in the identification of faulty program statements.



in this paper, we presented an automated debugging technique, coined flcn-s to improve localization effectiveness in a single-fault context. the proposed technique is a variant inspired by our previous work which uses both passed and failed tests executions. our previous technique has proven to be less effective on single-fault subject programs where both test inputs(passed/failed) are taken into account. in contrast, flcn-s diagnoses and rank program statements based on their behavioral anomalies and distance between statements in failed test inputs.



the proposed technique is evaluated on a well-known standard benchmark(siemens test suite) and four unix real-life utility programs(gzip, sed, flex, and grep). we compared our technique with four fault localization techniques, namely ochiai, tarantula, jaccard, and sncm. overall, the results show a significant improvement on the single-fault subjects where 65% of all the faulty versions can be localized by checking less than 10% of the program code on siemens test suite programs and is largely more effective on unix real-life utility program in comparison with other techniques in both best and worst case scenarios. furthermore, we observed that both degree centrality and closeness centrality play a vital role in the identification of faults.



for future work, we will like to explore other centrality measures for fault localization. and we plan to further explore the effectiveness of our technique on larger datasets to further substantiate our claims. moreover, the proposed fault localization technique(flcn-s) is considerably more effective in localizing faults in a single-fault context in comparison with our previous work and other techniques compared with.



