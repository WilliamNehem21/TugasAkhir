developing efficient and automatic testing techniques is one of the major challenges facing software validation community. in this paper, we show how a uniform random generation process of finite automata, developed in a recent work by bassino and nicaud, is relevant for many faces of automatic testing. the main contribution is to show how to combine two major testing approaches: model-based testing and random testing. this leads to a new testing technique successfully experimented on a realistic case study. we also illustrate how the power of random testing, applied on a chinese postman problem implementation, points out an error in a well-known algorithm. finally, we provide some statistics on model-based testing algorithms.



producing secure, safe and bug-free programs is one of most challenging problem of modern computer science. in this context, two complementary approaches address this problem: verification and testing. on one hand, verification techniques mathematically prove that a code or a model of an application is safe. however, complexity bound makes verification difficult to apply on large-sized systems. on the other hand, testing techniques do not provide any proof but are relevant, in provide(in section 4) some statistics on test suites generated in a pure model based testing approach based on fsm coverage. such statistics may be relevant in order to help the validation engineer to choose among different existing testing techniques. we think such statistics can be also useful in order to have a better understanding of coverage criteria.



given automaton and a given coverage criterion implemented into an test generation algorithm, a given number of tests. this approach reuses the results of previous section on the statistics of test suites. notice that this approach can not be used to reduce a test suite; its goal is to increase the number of tests that would have been obtained using the selected test generation algorithm on the original automaton.



from previous research and teaching experiments we had designed a formal model of the system, written as a b abstract machine anda java implementation, along with a number of mutants of this implementation. each mutant is a variation of the original implementation, in which a mistake has been introduced on purpose. this technique is used to evaluate the quality of a test suite: the more mutants are killed, the more efficient is the test suite.



when running the tests, we are looking for a non-conformance between the results obtained by the implementation, and the expected results given by the model. this conformance relationship is based on observing the outputs of the different commands, that are supposed to return a status code indicating if the command succeeded or failed, and why, depending on the value of the error code. a test fails if the codes do not correspond at a given step of the execution of the test.



these results shows that the average length of the tests suites is relatively similar for a given number of final states. the computation time decreases with the increase of the number of final states. intuitively, this is due to the fact that adding final states add backward transitions that simplify the search for the optimal path in the chinese postman algorithm. however, the resulting test cases are longer, and



the main idea of random testing is that randomness is not influenced by the tester. in this context, a crucial issue is to perform uniform generation, i.e. every element has the same chance to be selected by the algorithm. otherwise, selected values are related to the chosen algorithms, what is precisely opposed to the main idea. the result presented in is therefore interesting, opening many possibilities to test algorithms that manipulate labeled graphs(e.g. the traveling salesman algorithm).



we randomly generate strongly connected finite automata and we ask the tested program to provide a minimal path starting from the initial state and using all transitions. generating 30 deterministic automata with 8 states on a 20 letters alphabet, we point out an automaton making the program fail(a smaller automaton causing the program failure and obtained with a larger test suite is provided in appendix a). it is not the purpose of the paper to discuss why there is a problem in the code, just notice that it is the indexing of an array out of its bounds. we fixed this bug and we did several random tests on both the implementations, that did not reveal other errors on the original program nor side-effects introduced when fixing the bug.



first, we have explored an original combination of random and model based testing, through a technique that makes it possible to augment the size of a test suite. second, we have illustrated how random testing can be employed to detect bugs, even on a well-known and widely-spread algorithm. third and finally, we have provided some experimental data and statistics on several test generation algorithms based on automata.



the work proposed in this paper is based on a random approach[9,16]. even if such an approach is usually presented as one of the poorest way of generating data, it has been experienced as an efficient way for finding errors, confidence into the software. random testing can be employed for generating test data, such as in dart. the dart approach consists in combining static and dynamic program analysis in order to test software.



recently, random path generation has been explored in several testing works. in, dwyer and al. expose how to include a random process in depth-first search algorithms in order to get better test suites. the work show how perform random walks in a model is deeply influenced by the topology of the model. to tackle this problem, an algorithm to generate uniformly a path of a given length is provided and successfully experimented on very large models.



the test generation technique that we propose differs by proposing a test sequence length-guided approach. it can be seen as the automated test generation of test purposes for the tgv tool, motivated by the goal of providing a user-defined number of tests(increasing the number of tests that would have been obtained) when the test generation process is applied. to the best of our knowledge, this approach has never been targeted before.



