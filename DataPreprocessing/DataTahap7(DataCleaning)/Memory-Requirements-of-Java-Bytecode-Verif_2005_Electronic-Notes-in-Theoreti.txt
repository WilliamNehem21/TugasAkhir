bytecode verification forms the corner stone of the java security model that ensures the integrity of the runtime environment even in the presence of untrusted code. limited devices, like java smart cards, lack the necessary amount of memory to verify the type-safety of java bytecode on their own. proof carrying code techniques compute, outside the device, tamper-proof certificates which simplify bytecode verification and pass them along with the code. rose has developed such an approach for a small subset of the java bytecode language.



type safety of program code is an essential issue. it has to be guaranteed that every bytecode instruction operates always on program objects which have the correct type, e.g. that arithmetic instructions do never operate on object references because pointer arithmetic would compromise the security of program execution.



the remainder of this paper is structured as follows. first, we concentrate on the fundamental extensions of the rose approach and our new technique for memory optimizations. then we present the results of our evaluation. the section on related work focuses on alternative approaches to bytecode verification in resource-constrained environments.



extensions to the type system itself, while the special size of the data types long and double influences the implementation of the verifier slightly. in addition, we discriminate references to objects that have not been initialized yet within our type system. this is an elegant way to solve a special verification task completely transparent to the verification algorithm.



definition 2.2 a type transfer function of an instruction maps a given frame type to a frame type that characterizes the effect of execution according to the type based semantics of the instruction. additionally, the type transfer function states the preconditions that have to hold if the instruction is executed.



input and output frame types correspond to the situation right before and after an instruction while the final input frame type denotes the type inference result for an instruction. if all instructions can be executed on their final input frame type the program is type safe. otherwise, the verification fails.



over the bytecodes only, because the final result of the data flow analysis, i.e. the final input frame type of the actual control flow node, is always at hand. an additional off-card phase determines the certificates by the classical data flow analysis without the resource restrictions of the java card. the certificate that is transmitted to the card holds the pieces of information that are essential to reconstruct the data flow analysis result from a initial input frame type computed by the verifier. usually, the certificate entries are empty



however, this simplicity comes at the cost that the correctness of the information in the certificate has to be checked: every final input frame type has to comply with the final output frame type of all predecessor nodes. the merge process guarantees this condition for the direct successor node because the difference information in the certificate can only produce a more restrictive final input frame type. additionally, the verifier has to ensure that the final input frame types of all other successor nodes are more restrictive than the actual final output frame type, too. these final input frame types have either been computed already or they are computed later in the verification process. thus, the verifier has to store some frame types temporarily.



every edge in the control flow graph causes a test for compliance because the final output frame type of the source node has to be checked against the final initial frame type of the target node. such a check can be done as soon as both frame types have been computed i.e. when their nodes have been verified. hence, a frame type has to be stored for every edge that crosses the cut because then one of the participating nodes is not computed yet.



are represented by a single edge. this edge determines the lifetime of its frame type. the frame type remains in memory as long as the edge crosses the cut of the actual transition. the number of cut edges is the number of simultaneously stored frame types of the actual transition. the cut with the maximum number 5 of edges determines the memory consumption of the whole verification pass when frame types are stored just as long as they are needed.



the verifier uses the type lattice in two different ways. first, it ensures that the type transfer function is evaluated only if the precondition holds that the actual input frame type contains subtypes of the operand types. moreover, the verifier merges frame types by applying the leastupperbound relation successively to all stack elements and local variables.



on the bytecode level the creation of new objects is decomposed into two distinct instructions. the new instruction allocates memory for the object while its constructor is invoked later with the invokespecial instruction. the verifier has to ensure that references to an uninitialized object are only used in a very restricted way. for example, other method invocations than constructor calls with invokespecial are not allowed.



introduce new uninitialized types into the type system. these new types make up a completely new part of the lattice which is not related to the original class hierarchy. the verifier will reject the code if uninitialized references are used as operands of conventional instructions because the subtype checks fail. the type transfer functions of the new and invokespecial-instruction produce and consume the uninitialized variants of types.



the conversion of uninitialized types into their initialized counterparts has to convert all copies of an uninitialized reference. to find these copies in the actual input frame every new instruction produces an own type. these types are distinguished by the offset of the new instruction.



we focus our investigations on the worst cases because they must remain manageable within the few kilo bytes of memory in a java card. the average case is usually not a challenge because between 40% and 60% of the analyzed methods contain only a single control flow node. we have analyzed the standard api supplied by the java card development kit, two packages of the standard java runtime library, and the jdfa data flow analysis framework on which our certifier prototype is based.



presence of multi-branches that arise from switch-instructions the original idea of propagating the output frame type to all successor nodes would lead to copies of the same frame type. again our model determines the lifetime of every output frame type and stores it until its last use. the effect is show by the java card api where the number of result frame types of the worst case decreases significantly.



however, all rosepass worst cases suffer from a large number of output frame types that arises from the depth first traversal. the comparison to the optpass values show the expected effects if an optimizer determines a flexible traversal in combination with our lifetime model. the worst case of the java card api shows the most significant improvement that arises from the early choice of nodes with many predecessors like exception handlers.



