such situations have precarious impacts to the maintainability of software systems. in december 2004 a dangerous security hole was detected in the gdi+ component, which renders jpeg files on windows platforms(see). as multiple applications deployed the component as a dynamic linked library(dll) locally, so that it was installed in parallel in many cases, the security hole could not be fixed by simply substituting one centrally installed component.



one real-world example of such situations is the so called dll-hell(cf.[5,14]). the setup procedure of different applications on windows platforms overwrote commonly used components by their own, not necessarily newer versions. accordingly, the new applications worked as expected, but some of the already installed programs behaved unexpected, if operating at all.



there are a couple of approaches, which are able to check the conformity between different evolutionary states of a component and automatically detect incompatible changes. the most promising systems in this area have been compared by us in. depending on the system, more or less contract levels(syntax, behavior, synchronization, quality) are included in the analysis, if a certain component is in the position to replace another completely. the upgrade is allowed, if the conformance check yields the substitutability for all components, the administrator wants to replace.



both systems and their tools rest upon different heuristics that are pragmatic but minimize the solution space for finding compatible systems. in case of an update, tools like apt or rpm will always try to install newest packages only. if a well-defined system state could only be achieved by moving some packages back to an older version, these tools are not the best choice. furthermore it is accepted that debor rpm-packages are always backward compatible to older versions. packages that would violate this rule are simply renamed(e.g. with the new major-revisionnumber in the package name) and from then on exist in parallel with older version in the package-repositories.



components. especially against the background of the objective function defined above, we have to punish the usage of otherwise unnecessary components in a system which increases the version sum. therefore, we have to define another objective function for ensuring a minimal system regarding the count of used components by accumulating the sum over all decision variables xri. for the example this leads to:



we now have multiple objective functions against which we want to optimize our solution. it is provable that solutions for the linear relaxation of single objective functions are always located on a vertex of the n-polyhedron, given by the constraints of an optimization problem(cf. section 3.2). in a best case, the relaxation is also an integer solution. otherwise we found a good upper bound for a search(see section 4.1). in case of multiple objective functions, the optimal solution is located on the edge of the n-polyhedron between the vertices of the single optimal solutions.



in our example, we would first solve the problem against t1(xi) where we derive a maximum sum for the version-numbers of 5. now we can solve the problem against t2(xi) but additionally trying to minimize the difference between the version sum and the resulting version sum of the result of t2(xi). by changing the order of applied objective functions and weighting the detected differences to prior calculated results, we can influence the optimization process in a smooth way.



hence we need a mechanism that cuts non valuable branches off the search space, reducing the search complexity to a minimum. branch-and-bound(see) is one representative of such mechanisms. the core idea of branch-and-bound is to calculate an upper bound z(in case of a maximization problem) for an objective function to restrict the search to interesting branches. such an upper bound is calculated in each search state. in combination with a measurement for unexpanded branches, we can reduce the search space to a minimum.



in our case this bound could be the sum of maximum version numbers of a valid system. during the search for valid combinations, only those branches are expanded that have a sum of at least the upper bound calculated so far. if we find a solution with a greater version sum, we found a new upper bound. so we can remove other branches from the list.



if the model has a feasible solution, the constraints of the model can be interpreted as the border of an n-polyhedron with non empty volume. thus the search for an optimal solution can be reasoned by a geometric perception. in principle, an optimal solution can be found on vertices of that polyhedron. simplex algorithms(cf.) move from vertex to vertex via the edges of the polyhedron to find the optimal solution while interior point algorithms(cf.) also use interior points of the polyhedron to find an optimal solution. both methods guarantee to compute an optimal solution after a finite number of calculation steps.



from a theoretical point of view interior point algorithms are superior to simplex algorithms. there are interior point algorithms(e.g.) that solve the linear optimization problem in worst-case polynomial time while the existence of a worstcase polynomial running simplex algorithm remains an unsolved problem 6. in practice simplex algorithm perform very well and are successfully used in lots of optimization tools. furthermore, simplex algorithms are better suited for branchand-bound. through the relaxation of the 0/1 constraints, we derive continuous values for the decision variables and an upper-bounds estimation. if the calculated bound in a certain search state is less than a bound calculated before, then we do not have to expand this branch.



in section 3.1 we assumed that the current system consists of the versions a1, b1 and c2. the mission is now to update component a1 by a newer, bugfixed version a2. the described mechanisms solve this problem and generate a system that downgrades component c2 to c1 as the only possible solution. we know of no other approach that would be able to generate such a solution.



tutability of an old version against a new one. if substitution is not possible, we can use componentor, to analyze the reasons of this conflict and try to find valid combinations using the sketched optimization mechanisms. if all this fails, the tool is able to generate advices, how different components could be changed in order to ensure compatibility again.



to apply our methods, the components have to form direct dependencies. the more generic software becomes, the less applicable are our methods. dependencies that arise from dynamic instantiation and the use of reflection are not seizable by static code analysis. such dependencies could only be discovered by dynamic analysis, e.g. during unit testing.



