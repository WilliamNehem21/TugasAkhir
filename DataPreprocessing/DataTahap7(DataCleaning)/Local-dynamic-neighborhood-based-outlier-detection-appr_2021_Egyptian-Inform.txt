however, traditional local algorithms usually obtain lowquality results and are sensitive to neighborhood parameter, such as parameter k. most traditional algorithms use knn to quantify their neighborhood of data objects. as is well known, knn quantifies a rounded or spherical local region, therefore, it is a rough neighborhood measurement and not feasible for datasets with non-spherical clusters. moreover, it is hard to set k value without burden of them grows dramatically. for example, given a dataset with 1 million samples, the n naffinity matrix will need 7,450.58 gb of memory, which could lead to the memory bottleneck of a common computer, not to mention the next phase of detection process.



is a commonly partition method due to its low computational and space complexity. for example, clustering-based local outlier factor(cblof) is calculated by the distance of each data object to its respective cluster center after partitioning. similar to cblof, ldcof separates a dataset into clusters first by using k-means and then computes the ldcof scores by dividing the distance of an object to its cluster center by the average distance. additionally, the histogram-based outlier score(hbos) is a very fast anomaly detection algorithm to compute the feature probabilities of each data object. zhao et al. proposes the scalable unsupervised outlier detection(suod) for high-dimension large datasets by integrating several classical detection methods.



in this section, the proposed algorithm(ldnod) and its framework(ldnod-km) are introduced in details. the ldnod can produce high-quality and robust detection results. meanwhile, the detection framework by integrating ldnod with k-means can handle larger-scale datasets efficiently without sacrificing accuracy.



because close data objects have similar even same drnn neighborhoods, we construct a common neighborhood for near data points. in other worlds, all data objects within a drnn neighborhood has a common neighborhood. it is worth noting that different drnn neighborhoods can share neighbor or neighbors to maintain natural features of them. therefore, for a dataset x with n data



traditional detection algorithms usually use a top-n manner where they output the top scored n data objects as outliers. the parameter n is often not available without prior knowledge, and their detection quality heavily relies on it. however, for the ldnod algorithm, we can intuitively set the lnof threshold value, such as 3 or 5, although no consensus has been reached at present.



unlike traditional manners, the lnof score each local neighborhood instead of each data object, that is why we named it local neighborhood based outlier factor. on the one hand, it deal with a local region at a time more than a data object, which can also potentially improve the efficiency of the proposed algorithm. in fact, near data objects have similar outlier degrees, thus there is no need to compute an outlier scores for all data points. on the other hand, it is able to recognize single outlier and multiple outliers(a group of outliers).



purpose of this step is to get a small set of partitions with respect to original data instead of correct clusters. generally speaking, m is much larger than the real number of clusters for a large-scale dataset. therefore, pure partitions could be obtained by k-means, namely it contains only data objects from the same class besides outliers.



each dataset. due to robustness of ldnod, we set j to 30 for all datasets for convenient. the lnof threshold is also set to 3. the other parameters in the baseline methods are set as suggested by the corresponding papers or codes.



in this paper, a new local detection algorithm(ldnod) and its framework(ldnod-km) have been proposed. the ldnod is insensitive to neighborhood parameter due to the stability of drnn, which constructs neighborhood of an instance based on dynamic reference objects. moreover, sharing neighborhoods of close objects and scoring each local region are designed, which can potentially reduce the running time of ldnod. because the ldnod-km combines the benefits of both ldnod and k-means, it is able to handle large-scale datasets efficiently without sacrificing accuracy. finally, experimental results have demonstrated the effectiveness of ldnod and its framework. in the future, we will further improve ldnod-km and apply it to handle larger scale and high dimensional datasets.



