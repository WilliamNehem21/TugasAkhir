to address the abovementioned issues, this paper proposes the attcl method, which integrates hyperbolic representation learning and contrastive learning. this method involves embedding knowledge into a hyperbolic space. for samples with limited hierarchical characteristics and insufficient feature information, a method of overlaying adversarial noise is employed. the loss function of the embedded samples is backpropagated to the embedding vectors. perturbations are added and adjusted in the gradient direction to make them smoother and more localized. simultaneously, a hyperparameter is introduced to fine-tune the adversarial strength when constructing adversarial samples for



embedding is a technique commonly employed for addressing the knowledge graph completion problem. it involves mapping entities and relations into a lower-dimensional vector space, allowing for the quantification of entity similarity and relation distance. effectively capturing the interconnectedness among entities while retaining the integrity of the original information[10,11]. the transe model uses the euclidean distance to measure the matching between entities and relations; it combines entity vectors with relation vectors to capture interentity relations, but it has difficulty handling complex relations. the distmult model uses the dot product to measure entity and relation similarity; it avoids illegitimate entity combinations by restricting the relation vectors, but it is limited in its representation ability. the complex model extends the distmult model to the complex domain and uses complex vectors to represent entities and relations and compute their similarity, but it has many parameters and is computationally complex. graph neural network models focus on capturing complex entity and relation interactions to forecast and recover missing data. the conve model uses convolutional neural networks to learn the interaction patterns of entities and relationships, which are then applied for information prediction and filling.



in contrastive learning, by comparing the similarities and differences between positive and negative sample pairs and increasing the diversity of the training samples, it is possible to better distinguish between different entities and relationships to obtain more semantically informative representations. simctg is a contrastive learning framework proposed for neural text generation tasks that addresses the problems encountered in decoding methods, encourages diversity and coherence, and improves the calibration of the representation space of language models. simkgc uses a dual encoder architecture, introduces three harmful sample types, and uses the cosine similarity for tail entity prediction to improve the learning efficiency. kge-cl number of relationally connected entities in the knowledge graph is low, att-cl makes a judgement on each triple. suppose that the number of connected entities is lower than the average. in that case, adversarial samples are generated via the introduction of adversarial noise for data augmentation, and the generated adversarial samples are added to the sample pairs for contrastive learning.



in knowledge graph completion. owing to the comparatively modest dimensions of the wn18rr dataset and the fact that the relations it contains mainly involve associations between word meanings, it is primarily focused on semantic relations. therefore, the att-cl model shows greater improvement on the fb15k-237 dataset than on the wn18rr dataset.



2.2%, 1.6%, and 2.62% in mrr, hits@1, hits@3, and hits@10, respectively. furthermore, further enhancement of the embedding space by integrating hyperbolic representation learning and contrastive learning mechanisms can yield superior results and performance in the task of completing knowledge graphs.



