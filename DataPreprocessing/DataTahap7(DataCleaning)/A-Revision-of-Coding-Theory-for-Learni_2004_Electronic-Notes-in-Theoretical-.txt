finitary stochastic processes form a special class of stochastic processes which can be generated by models with finite or short-range memory in the finite number of distinctly distributed observed or hidden variables. some examples are markov models, hidden markov models(hmms), shortrange hamiltonian systems in physics. probabilistic context free grammars(pcfgs) probably are also elements of the class(see appendix a). for finitary processes, the structure of random variable dependencies in the best model inferable from data almost stabilizes. the size of the best model grows at most logarithmically w.r.t. the dataset size.



following, let assume that any admissible description for any data consists of two parts:(i) the codebook, being the definition of some decoding procedure c.(ii) the encoded data, being some argument a for procedure c. c(a) is the original, unencoded data. in a simplified symbolic approach discussed in section 4, c is a set of some codeword definitions and a is a string of the codewords. no such assumption is made now. let n be the length of unencoded data and let d(n; c) be the length of their description using procedure c.



3 contains a mistake of identifying h(n)= d(n). furthermore, there is an unsolved problem of estimating(n) as the length of really optimally encoded data. estimating(n) as shannon entropy h(n) of the sample is dangerous. for 6= 0,(n) being average code-length for n-tuple used apart is significantly bigger than h(n)being average code-length for n-tuple of data immersed in the infinite ensemble which is all coded. it is also why recursive definitions introduced in section 4 do profit. at last, there may be no naive infinite ensemble as the usual law of large numbers can be hardly seen experimentally for some objects(problem of absolute probabilities for words rather than ranks).



both e(n) and(n) meet the desirable conditions for so called complexity measures discussed in plectics(studies of complexity). they remain finite for easily formalizable deterministic or finitary systems(homogeneous substances) while possibly growing infinitely w.r.t. the system size for those that are extremely laborious to describe(life, language, society).



(n) is the size of the theory which is most appropriate for describing the n-tuple of data.(n) measures only the remaining randomness. for parole, unbound growth of(n) is reasonable. the receiver of parole can profit from inferring new portions of generalizable knowledge from the incoming signal infinitely. otherwise, it would be very difficult to explain what for the humans keep on communicating to each other in the longtime perspective.



each codeword standing for a composition of atomic symbols can be defined either in terms of codewords for atomic symbols only or also in terms of codewords for more complex compositions which together yield the same composition. the first kind of definitions will be called simple, the second one will be called recursive. for 1 6= 0 and fully reversible compression, the choice between using simple or recursive definitions does not change significantly the total description length for infinite data. for never can be neglected w.r.t. to the size of the encoded data. recursive definitions shorten both the codebook and the encoded data simultaneously and significantly. some induced codewords may be used only in the codebook for defining other codewords. even for 6= 0 and 1 6= 0, the use of recursive definitions enlarges the codebook significantly and improves its predictive quality although it does not change significantly the total description length for infinite data.



an implementation of the presented principles is de marcken algorithm. in this algorithm, set x consists of two kinds of symbolic transformations:(a) defining a concatenation of two codewords with undefining 0, 1, or 2 elements of the pair,(b) undefining a codeword. defining an object means introducing a new codeword, replacing all occurrences of the object with the codeword(both in the data and in the codebook) and enrolling the definition of the codeword as the object into the codebook. undefining the codeword is the reverse. de marcken algorithm produces a codebook consisting of recursive and mostly meaningful definitions of often syllables, morphemes, words and fixed phrases. since more distant(and some internal) correlations are not pure concatenations, then it stops enlarging the codebook with compression rate about 2 bits/character for english texts. an example from is:



there are parallels between the discussed reversible compression formalism and recent hypotheses in neurosciences. argues thoroughly for a functional distinction between neocortex and hippocampus(parts of brain), which can be resumed that neocortex(larger one) stores mostly the codebook while hippocampus(smaller one) stores the recent encoded data. the recursive form of codebook by de marcken algorithm resembles also the neurolinguistic stratificational model presented in.



the substantial progress of machine language structure acquisition is not a question of improving the quantitative side of the learning scheme but rather considering more complex reversible symbolic transformations of the descriptions. what de marcken algorithm does is rewriting its input as a nearly shortest context-free grammar(cfg). this cfg is restricted to one-to-one rewriting rules where each codeword on the left-hand-side must be precedent to all those on the right-hand-side accordingly to a partial order relation. to conform fully with cfg format, the encoded data should be treated as the rewriting for the initial symbol.



one might think that in general, all entries in the codebook might be the rules rewriting any strings of codewords as any strings of codewords. then these rules needed to be also one-to-one, and because each rule had to contribute to the overall compression, their left-hand-sides had to be shorter than their right-hand-sides if counted in binary symbols. it means that the codebooks would have a restricted form of context-sensitive grammars(csgs). maintaining the condition of unique derivation of the initial symbol(compression reversibility) for a csg is problematic. besides that, csg codebooks are too weak to generalize independent behavior of strings into paradigmatic definitions(for morphemes having phonetic alternations and harmonies, words having inflections, phrases instantiated as words).



with being very low in the beginning and apparently growing later during the life, human ability to perform non-local search might be learnt during data processing. points out that many cases of what has been considered global optimization and learning in biology, later appeared to be an effect of genetic algorithms(species evolution, immunological reaction). proposes the same mechanism also for inconscious brain processing in the real time.



deterministic interpretation may be applied as the single method only for fully learnt, purely finitary processes. asymptotically there is no learning in them and the past can be forgotten with no risk. the child may be more eager to forget these parts of its current representation of parole which it can interpret deterministically, while remembering more often those which cause troubles for its deterministic interpretation. when the child is learning, its discrimination between these two classes evolves.



two remarks are worth making for the future research:(i) pure minimumdescription-length formalism cannot contribute to clarifying the notion of deterministic non-interpretability. this formalism is just a construction of the shortest system of hierarchical rules and exceptions in one which exists for adding any new exceptional instance.(ii) if the counts of all codewords are memorized independently from their linear order in the description, forgetting the order of some codewords can be done with or without decreasing their memorized counts. not decreasing the counts stabilizes the entries for the codewords the order of which is forgotten.



0<< 1, the optimal codebook grows infinitely. the first hypothesis was that the learning child memorizes only the optimal codebook. the natural optimality criterion would be(n)= max, which yields the optimal dependent on n with 1 for n. the largest codebook could be extracted from sufficiently large amounts of almost complete noise!



theoretical statistical physicists have been conscious of acute problems that are met by maximum shannon entropy method in inferring the macroscopic behavior of long-range hamiltonian systems. for short-range hamiltonian systems, the method predicts the macroscopic behavior excellently. to deal with long-range hamiltonian systems, nonextensive thermodynamic formalism was proposed several years ago. non-extensive thermodynamics modifies the definition of entropy for which the maximum entropy method is applied. as a result, the method produces power-law distributions for the same constraints which yield gaussian distributions when applied to shannon entropy. while gaussian distributions abound in inanimate nature, power-law distributions abound in complex systems such as life, language and economy. a new wave of interest in physics is whether some extensions of non-extensive thermodynamics could not only reproduce the power-laws but also explain consistently all large-scale behavior of the complex systems. the question of plausible links between linguistics and thermodynamics is discussed a little more in.



analogies and differences between this theory and nonextensive thermodynamics. the analogies and differences may stimulate further bidirectional transfer of ideas between the community of physics and plectics and the community of linguistics and computer science. influential can be also neuroscientific inspirations.



gives processes generated by hidden markov models(hmms) as instances of finitary processes and remarks that non-finitary processes may be generated by context-free grammars(cfgs). this remark may be false if the probabilistic cfgs(pcfgs) are meant. pcfgs consist of a finite set of context free rules with probabilities of derivation depending only on the currently expanded node. given these probabilities, a1d(n)> h(n)> b1 d(n)> 0 and a2 n> d(n)> b2n> 0, where d(n) is the



number of derivation rules used in a derivation tree for n-terminal production. h(n) for typical pcfgs may have a large linear component and possibly only very small sublinear ones. analogical reasoning is valid also for hmms, which are probabilistic regular grammars and form a subset of pcfgs. there is little evidence at the moment that hmms and other pcfgs belong to different classes of entropic sublinearity generators.



