with the growing market of cloud platform, its scale has become enormous. however, the prosperity of cloud platforms also brings significant challenges to site reliability engineers(sres) in detecting and diagnosing faults within large-scale cloud platforms. the computing infrastructures providing services need to guarantee service level agreements(slas) to customers. unexpected service downtime can greatly impact stability objectives and lead to substantial financial losses.



critical for maintaining service level agreements. thus, there is a high demand for algorithms to accurately and efficiently detect metric anomalies with an online manner. an effective online anomaly detection algorithm must continually process incoming data streams and update its model online to ensure accurate and reliable detection. in the situations when metrics experience significant changes in data distribution, known as concept drift, algorithms need to adapt to these changes promptly to prevent false alarms from occurring.



metrics serve as a fundamental component of observable data objects in cloud platforms. a metric is a numerical value or counter that represents the state of the system, which is atomic and cumulative. each metric can be regarded as a logical measurement unit, typically representing data statistics updated over time. although the specific metrics monitored by vary cloud platforms can be different. take google cloud metrics as an example. a typical set of cloud platform metrics includes five categories, including cpu, system, memory, block, and network, which can cover various aspects of the cloud platform.



streamad can serve as a logical unit that is dedicated to utilizing machine learning technology for metric monitoring. it is capable of subscribing to message queues, allowing it to receive and analyze streaming data based on algorithmic processing logic. once the observed data has been analyzed, it will be scored accordingly. those data with a high anomaly score are then sent to the alert exporter, and then further notify the users in time. sres can trace the metrics records via a customized dashboard and deal with the faults in the cloud platform.



tods[13,19] constructs a full-stack automated machine learning system for anomaly detection. it is a benchmark that identifies four multivariate real-world datasets from different domains and benchmarks nine algorithms on synthetic and real-world datasets. tods also publishes preprocess and synthetic scripts, as well as algorithm implementations.



nab[20,21] focuses on scenarios of online anomaly detection in practical applications. although all algorithms in this benchmark are designed for online anomaly detection and use a scoring algorithm designed for streaming data, it only evaluates univariate anomaly detection algorithms and lacks the discussion of multivariate time series data. furthermore, this benchmark is not currently maintained and does not cover new online anomaly detection algorithms.



exathlon is a benchmark that focuses on time series data. it provides a new analytical perspective on time series anomaly detection, which is the interpretability of the detection results. it focuses on spark application monitoring and provides an end-to-end pipeline for explainable time series anomaly detection. however, this benchmark is for offline analysis.



utsd is a benchmark that focuses on univariate time series and provides a user-friendly visual interface for those series. this benchmark contributes a large number of datasets and their variants. however, the use case of this benchmark directly applies tabular data anomaly detection algorithms to time series data, which has great limitation on modeling the features of time series data.



xstream tackles anomaly detection tasks for feature-evolving streams. as a density-based ensemble anomaly detection algorithm, it approximates the density of a point by counting its nearby neighbors at multiple scales. rshash employs randomized hashing to score data points and features an elegant subspace interpretation. hstree is a fast one-class anomaly detector for evolving data streams. utilizing mass as a measure to rank anomalies, it can construct a ranking with small samples, enabling the anomaly detector to learn quickly and adapt to changes in data streams promptly. loda recognizes that the probability of observed samples valuable in determining their



aiops_kpi is a large-scale real-world public dataset, consisting of 27 key performance indicators(kpis) for artificial intelligence for it operations(aiops). this dataset is collected from five large internet companies, including sougo, ebay, baidu, tencent and alibaba. the duration of each kpi data ranges from two to seven months, and each kpi is labeled by experienced sres in these companies. the kpi patterns in this dataset are various.



micro consists of metrics data from a public microservices monitoring dataset. it contains fine-grained metrics, including container, linux system, oracle, and redis. the attributes of the spans on each component are aggregated into kpis that reflect the overall status of each component. anomalies in this dataset are simulated by fault



gaia is comprised of one-month cloud platform monitoring data, selected from a login-action scenario in a business cloud platform system. the monitoring data includes zookeeper, redis and mysql. this dataset covers different types of time series data, including change point, concept drift, periodic and stationary data. this dataset with rich variety of anomaly types can provide more comprehensively validation scenario for anomaly detection.



for time series anomaly detection evaluation, there are several measures have been proposed to assess the quality of anomaly detection algorithms. in general, these evaluation criteria can be classified into two categories, point-aware evaluation and series-aware evaluation. for these evaluation methods, precision and recall are both considered as evaluation criteria. precision measures the proportion of relevant instances among the retrieved instances, and recall measures the proportion of relevant instances that were successfully retrieved. streamad includes both point-aware and series-aware evaluations to ensure a comprehensive assessment of the detection methods.



datsets. as described in section 4.4, streamad includes five public real-world datasets, focusing specifically on cloud platform applications. due to the online detection paradigm employed by various detection algorithms, we allocate the first one hundred points of each streaming data for algorithm initialization. these detection algorithms are primarily designed for the transductive setting, and outputting anomaly scores for the incoming data.



hyperparameters. each anomaly detection algorithm in streamad(described in section 4.2) has its own hyperparameters, such as the observation window for knn-cad algorithm and the number of trees for rrcf algorithm. to ensure a fair comparison, we used the default hyperparameter settings from the original papers for all algorithms in streamad.



evaluation criteria. the benchmark of streamad incorporates both point-aware and series-aware evaluation methods. the pointaware evaluation adheres to the standard definition of evaluation criteria, while the series-aware evaluation accounts for the frontend bias introduced in cloud platform metric evaluation scenarios, as discussed in.



additionally, point-aware evaluation and series-aware evaluation lead to significantly different results on the micro dataset. this is attributed to the short and concentrated anomaly duration of the micro dataset, as its average length is 228 and the anomaly rate is 1.26%. the experiments on this dataset demonstrate the differences between point-aware and series-aware evaluations.



in summary, selecting the best algorithm to detect cloud platform metrics anomalies is a challenging task. it is difficult to guarantee that an algorithm can cover all application scenarios. users still need to try it out based on their specific use cases. as the promising effectiveness from our benchmark results, we recommend using spot for univariate data streams and xstream for multivariate data streams as the first attempt method.



with the development of the online anomaly detection community, the construction of benchmarks is a long-term process. in our future work, we are going to follow the state-of-the-art work, and integrate them into streamad. in addition, we plan to evaluate benchmarks from more perspectives, such as the impact of hyperparameters on the detection performance of different algorithms, and the interpretability of various algorithms. we hope that these future work can provide a more comprehensive view for benchmark evaluation.



in this work, we propose streamad, a cloud metrics-oriented benchmark for unsupervised online anomaly detection. streamad comprises eleven anomaly detection algorithms and conducts comprehensive experiments on five existing public datasets. the benchmark includes comparisons for the effectiveness, efficiency and memory resource consumption for various algorithms. streamad is open-source, and it provides a user-friendly api to help sres evaluate anomaly detection applications in their specific use cases. researchers can even develop new algorithms with streamad, which can facilitate further research in this area.



