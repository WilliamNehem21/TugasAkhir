with the volume of data increasing exponentially, there is a growing interest in helping people to benefit from their data regardless of its poor quality. one of the major data quality problems is the imbalanced distribution of different categories existing in the data. such problem would affect the performance of any possible of analysis and mining on the data. for instance, data with an imbalanced distribution has a negative effect on the performance achieved by most traditional classification techniques. this paper proposes tgt(train generate test), a novel oversampling technique for handling imbalanced datasets problem. using different learning strategies, tgt guarantees that the generated synthetic samples reside in minority regions. tgt showed a high improvement in performance of different classification techniques when was experimented with five imbalanced datasets of different types.



a collection of data is called imbalanced if one class instanceswas higher in number than the other. the class with more instances is referred to as the majority class, and the one with fewer instances is called the minority class[1,2]. numerous recent researches on imbalanced datasets have generally agreed that because of this skew distribution of classes, the classifiers are biased towards the majority class and give very low classification accuracy towards the smaller classes. classifier may also classify any sample as the majority class and ignore the minority class.



tion. the idea of sampling is based on changing the dataset so that a more balanced class distribution is created. methods of sampling can be subdivided into oversampling and undersampling. undersampling eliminates the number of instances of the majority class while oversampling generates minority class synthetic instances during preparation.



the rest of this paper is organized as follows. section 2 presents the related work while section 3 introduces the proposed technique to handle imbalanced data classification problem. sections 4 and 5 show the details of the performance evaluation and concludes the paper.



to resolve the smote shortcomings on nonlinear problems through oversampling in the svm feature space. wk-smote modifies smote for non-linear separable data by producing the synthetic instances in the classifier feature space rather than the input data space. in comparison with the other baseline approaches on several metrics, the suggested oversampling algorithm together with a cost sensitive svm model have demonstrated an enhancement in performance. therefore, a hierarchical structure for multiclass imbalanced issues with a progressive class order is created. the suggested wk-smote and the hierarchic structure are tested on real world industrial fault detection system. two main variables are defined in smote: n oversampling value and the k-neighbors. nevertheless, in practical implementations, the two variables randomly selected by users cannot be opti-



mized. however, the imbalance ratios of the data are completely distinct, making it more difficult to pick parameters in smote. the authors in proposed a new oversampling method relying on smote to address the problem. this turns the problem of parameter selection into a multi-target optimization problem in smote. to achieve their optimal solution, a new selection technique called absolute dominance-based selection was introduced to search best values for smote parameters.



in, the authors are presenting a novel technique of selforganizing map based oversampling(somo), that generates two-dimensional representations of the input space by applying a self-organizing map to enable the effectiveness of artificial data point generation. somo consists of three main steps: first a selforganizing map gives the original, typically high-dimensional, space a two-dimensional representation. then it produces artificial instances in a cluster and then produces synthetic instances between clusters. they also performed empirical experiments which enhanced the performance of methods, when somo produces artificial data.



a new method for oversampling is proposed in which uses the real value negative selection(rns) method for generating synthetic minority instances with no real minority data necessarily available. the generated minority instances with rare actual minority data are merged with the majority data in order to provide a method for the binary classification learning. in their experiments, they show the efficacy of rns to prevent the over-sampling problems faced by conventional approaches such as noise generation and redundant samples in the same clusters. however, we noticed from the results it performed well only for severely imbalanced datasets.



the authors in are suggesting radial based oversampling methods(rbo), which can identify areas in which minority class artificial instances are to be produced on a radial base depending upon the imbalance distribution estimate. they take into consideration data got from all classes, in contrast to traditional multi-class over-sampling methods that use only minority class data. experiment results conducted on a typical dataset indicate that the rbo artificial over-sampling technique offers a promising alternative to the current imbalanced dataset solutions. a three-way decision model(ctd) is introduced in where the costs of choosing main samples are taken into consideration. first, the ctd uses constructive covering algorithm(cca) to separate minority instances into multiple coverage. each cover is then selected and divided into three regions based on the coverage density. finally, according to the model of cover distribution on minor-



the authors in introduced generative adversarial minority oversampling(gamo). the idea is that producing synthetic points near the borders of the minority class will let the classifier to learn class boundaries which are more robust to class imbalance. the convex generator produces synthetic points as convex combinations of the existing points from the minority class. they introduced also an additional discriminator which ensures that the generated points belong to the real distribution of the intended minority class.



the first step is training step which is utilized to train two classifiers on the given data: a decision tree and a neural network. the decision tree is trained to get the minority class classification rules which will later be used to generate the synthetic samples. then, a neural network is trained to classify the minority and majority classes data. the neural network will be later used for testing the generated samples. despite the training on imbalanced data, the two classifiers are expected to extract and use all the available knowledge about the minority class in the data. this would guarantee that the new generated samples follow the distribution of the minority class data even though its scarcity.



training step details is described in algorithm1. it contains two functions. first one is builddt, which takes the training data along with class label, uses gini index for selecting the attribute that best classifies the instances and outputs arrays containing the upper and lower value for each attribute. the second function is nn which takes the majority and minority sets to be trained and then used later in verifying generated samples.



the last step is the testing step. in this step, the generated samples are tested or verified using the trained neural network. if classifier shows that the sample belongs to minority class, then it will be kept in new synthetic samples array. otherwise, it will be discarded. the neural network is considered to be an unexplainable classifier that identify the class based on the finding the right weights of network neurons in order to find the right classification. using such classifier would ensure that the generated sample of minority class through explainable classifier is verified by another classifier that works in a completely different and unexplained method.



the objective of the evaluation is to prove the effectiveness of the proposed adversarial guided oversampling technique. the proposed technique aims to balance the class distribution of data by making guided oversampling though using minority class rules driven from training the decision tree on the dataset. and then checking the generated samples using well-trained neural network to assure they all belong to minority class. towards this goal, the proposed technique is evaluated on different datasets over different classifiers against smote method which is the baseline oversampling approach in the literature and also against one of its very recent smote variations which is modified smote. the following subsections describes the evaluation details.



[23,24]. accuracy is the most common performance metric in practice, particularly for binary and multi-class classification issues, as seen in various studies[25,26]. sensitivity determines the amount of real positive that is correctly classified as such, while specificity determines the amount of real negative that is correctly identified. in other words, specificity metric is used to measure the fraction of negative patterns that are classified correctly. consequently, sensitivity takes into account the prevention of false negatives, and for false positive specificity does likewise.



three different classifiers, k-nearest, fuzzy k-nearest, and support vector machines classifications, were used for performance evaluation. the knn classifier takes only one parameter and the best results got when k= 10 while the fknn classifier takes two parameters k and m where k= 10 and m= 0.5. classifiers were trained on different settings to compare the proposed oversampling technique against smote and modified smote. they were trained once on data without oversampling, once with data after oversampling it with smote, once with data after oversampling it with modified smote and finally with the data after oversampling it with tgt.



algorithm is variant on the five datasets. this may be related to the ratio between the majority and minority classes in the original data. the results show that the proposed technique performs extremely well when the ratio between the majority and minority classes in the original data is high as in indian diabetes and kc2 software fault prediction datasets.



the outperformance of the proposed technique against smote algorithm and modified smote may be related to the difference between how the three methods work. traditional smote algorithm generates synthetic samples in the space of the minority data space, the modified smote algorithm generates synthetic samples in the space between minority and majority data. both methods generate a new synthetic sample by analyzing a randomly chosen sample of minority class. on the other hand, tgt generates the synthetic samples guided by the classification rules of the minority class derived by training the dataset of the decision tree. after the guided generation, these generated samples will be verified using the well-trained neural network to assure that all synthetic samples belong to minority class. otherwise, they will be discarded. those double-checked new samples generated by the guided adversarial oversampling technique have led to better classifications which proves our initial argument. in addition, the interaction between explainable and non-explainable classifier and the analysis of the whole dataset has shown to be effective enough to generate new synthetic samples better than those generated by smote and modified smote which depend on the analysis of individual samples to generate new samples.



this paper presented an adversarial guided oversampling technique(tgt) for handling the imbalanced datasets. the proposed technique utilizes two classifiers to extract and model the knowledge about the minority class data. a decision tree is trained on the given data to model the minority class data as set of classification rules where those rules are used to generate new samples of minority class. then, a neural network is trained on the given data and used to verify that all the generated samples belong to the minority class data distribution. the proposed technique showed a higher performance when was evaluated against standard and recent data oversampling techniques over different datasets and using different classifiers.



