we can fix the proposition and apply narrowing search for an unknown proof. the general search procedure will then try to construct proofs of the given proposition. one approach to develop a proof construction tool for a given higher-order formalism is to add meta variables(or logical variables) to the abstract syntax of the proof language. these are used as place holders for unknown data. when searching for a proof, a single meta variable is created at the start, indicating that the entire proof term is initially unknown. the meta variable is then instantiated step-by-step, adding new meta variables representing unknown sub-terms. while instantiating, one keeps track of the semantics of the formalism, back-tracking whenever the cur-



there would be several advantages of the proposed way of attaining proof search from a proof checker. instead of implementing an often intricate and tedious proof search algorithm, the proof checker, which presumably already exists, is reused. apart from saving work, this entails that there is little potential for inconsistency. in other words, given that the narrowing search procedure is sound and complete, the same properties are inherited by the proof search. thus the correctness of the search is in principal directly provided. another advantage would be that meta variables are handled by the narrowing algorithm. hence, there is no need to add meta variables to the term syntax, and functions like evaluation and comparison do not need to be aware of them. also, since the narrowing is first-order, the search algorithm is pretty simple. in this work narrowing is used to construct higher-order proofs. to make this possible the terms are represented in a first-order abstract syntax.



the potential drawback of the approach which could overshadow all these benefits is of course that the resulting search procedure, although working in theory, is not efficient enough for practical use. we have investigated this by implementing a narrowing search algorithm and a proof checker. our aim has been to achieve a proof construction tool which automates the construction of proofs which are rather small. it is supposed to be useful in an interactive proof construction environment as an aid for filling in not too complex sub-terms in a proof. it is not meant to compete with advanced algorithms for constructing higher-order proof terms. instead, the intended point of the approach is to add automation to a formal system for higher-order logic at a low cost.



the formalism we have chosen is a logical framework with dependent types, and recursive data-types and function definitions. hence the proof checker is in fact a type checker, and will we from now on use the terminology of the curry-howard correspondence, i.e. refer to proofs as terms and propositions as types. we have implemented a type checker for the formalism in haskell. applying the narrowing search on this type checker indeed does not at first give a proof construction tool which could be used in practise. however, our experiments indicate that, by adding a couple of general features to the narrowing search, as well as introducing some small modifications to the type checker, the performance is substantially improved. we will present the most crucial(in our experience) of these general features and modifications. a central idea of the work is that the same code should in principle be able to serve as a description for both checking and searching. hence, the modifications introduced to the type checking algorithm should preserve its meaning as a haskell program.



our underlying search procedure is based on narrowing. a survey of various narrowing strategies is found in. a notion of parallel evaluation was presented by antoy et. al. in. we have used a slightly different notion of parallel evaluation, which is described in. higher-order term construction using first-order narrowing was investigated by antoy and tolmach. our work is related to this in the sense that we have also looked at using first-order narrowing to construct higher-order terms. the difference is that antoy and tolmach focused on constructing terms in the declarative language itself, whereas we encode a new language in a first-order data-type and search for objects of that data-type.



finding efficient strategies for automatically constructing proof terms is an extensively studied area. the concept of uniform proofs largely reduces redundancy in proof search and is implemented in e.g. the formal system twelf[11,12]. focused derivations is a development of this which further improves performance by detecting chains of construction steps for which the search can proceed deterministically. tabling is another technique for narrowing down search space. it is based on memoizing subproblems in order to avoid searching for the same proof more than once. our approach cannot compete with these advanced proof construction strategies. nonetheless we do add some restrictions to the type checker in section 5 which remove certain kinds of redundancy. more refined restrictions could probably be introduced, and a tabling mechanism could possibly be added to the general search procedure. however, such advanced features are outside the scope of this investigation.



with a lazy narrowing strategy the expression for which the search is performed is evaluated in a lazy evaluation order, i.e. from the outside and in. whenever an uninstantiated meta variable is encountered it is said to be in the blocking position. the meta variable in the blocking position is chosen for refinement. it is nondeterministically refined to one of the constructors in its type. for each constructor, fresh meta variables are inserted at the argument positions. then the evaluation of the expression proceeds until a new blocking meta variable is encountered or a value is reached. if the value is true a solution has been found. if it is false a dead-end has been reached and the search back-tracks.



proceeding with the example, we start evaluating the given expression lazily. since tc does a case distinction on the third argument, we need to know the head constructor of the term. but the term is?1, which thus blocks further evaluation. there are three possible refinements of?1. the refinement?1:= app?2?3 yields an expression with(tiv empctx?2) surrounding the potentially blocking position. assuming that tiv returns nothing for the empty context regardless of its second argument, the evaluation proceeds without further refinement with the result being false, which means no solution. the refinement?1:= pi?2?3?4 immediately yields false since the type is not set. finally, the refinement?1:= lam?2?3 results in the expression



however, the example could very well be formalized in a system without dependent types. in a logical framework with dependent types, argument types and the output type may depend on a previous argument value in applications. this leads to some complications which were not exposed in the example. in the following two sections these complications will be discussed along with suggestions for how to deal with them.



one complication which did appear in the example was the treatment of variables. when constructing variable occurrences, the description of the search was rather irregular, stating that two variables should be the same rather than instantiating a single meta variable. also, the variables which were never used were left uninstantiated. these problems are however easy to avoid. when representing variable occurrences by de bruijn indices, no arbitrary choices need to be made, and no uninstantiated variables at binding position will appear. furthermore, if recursively defined numbers are chosen to represent the indices, the narrowing algorithm will itself limit the search to the set of possible indices for a given context.



this section describes the two main non standard features of the general search algorithm we have investigated. as stated above, the search procedure targets weakly orthogonal trss. in order to deal this class of rewrite systems efficiently, a concept of parallel evaluation has been proposed. in a somewhat different notion of parallel evaluation is presented, which is used in our implementation. using this feature for the purpose of parallel conjunction is discussed in section



as mentioned, the hazard of giving conjunctions a parallel meaning is that an intentional partiality in the right conjunct is exposed. the implications are different for non-definedness and for non-termination partiality. non-definedness can be easily handled by treating a undefinedness error in the right conjunct of parallel conjunction as false.



with the needed narrowing strategy, there is always a unique meta variable to branch the search on. however, in the presence of parallel conjunction, or parallel evaluation in general, there is no longer a single meta variable blocking the evaluation. the order in which to instantiate meta variables must hence be further specified. a natural choice is to store them in a collection and extract them in either a queue or a stack manner. in our experience the queue is in general, but not always, the better choice regarding performance. the order of instantiating blocking meta variables can also be controlled in more refined ways, which we will come back to in section 5.2.



parallel conjunction was introduced in order to check several properties in parallel during the incremental instantiation of a term. this seems to be beneficial in various situations where the properties constrain the same part of the term. however, for problems where there are sub-terms with no dependencies in between, it seems undesirable to interleave the search for their solutions. consider the situation where p and q have no meta variable occurrences. interleaving the construction of a proof of p and a proof of q is unnecessary and should lead to a larger search space than if the to subproofs were constructed separately. this is a drawback of switching to parallel conjunction as discussed in sec 4.1. moreover the situation is very common. it can appear whenever attempting to prove a proposition by case



a solution to this drawback could be to add a feature of subproblem separation to the general narrowing search. one part of the mechanism would be to detect the presence on independent subproblems, i.e. unconnected graphs where parallel conjuncts and meta variables constitute the vertices and the edges represent meta variable occurrences in the conjuncts. when such a partitioning has been detected, a local search for each subproblem is spawned. when performing the search for several independent subproblems backtracking one of them should not affect the other ones, and when the search space is exhausted for one of them, itself and all of its sibling subproblems are cancelled. we have implemented this feature, but it should be considered a prototype.



the features presented in the previous section, parallel conjunction and subproblem separation, do not result in a proof search which is of practical use. in order to improve the performance, we have also experimented with some modifications of the type checker. in this section we will discuss a few such modifications which have proved to be important in our experiments.



most logical frameworks allow writing essentially the same proof in many different ways. by restricting the type checker to accept fewer terms for a given type, a reduction of the search space can be accomplished. one restriction is to only allow normal terms. it can be easily achieved by adding a side condition checking that no sub-term is reducible. imposing this restriction does not compromise the completeness of the search. another possibility could be to add restrictions which allow only uniform proofs.



one can of course also come up with a large number of restrictions, which do limit the completeness. these can be seen as representations of different heuristics. one example is to restrict induction so that no generalizations may take place. this clearly makes the search incomplete, but also contributes a lot to performance.



as mentioned in section 4.1, the search is based on keeping a queue of blocking meta variables and instantiating them one at a time. by slightly annotating the code of the type checker, one can introduce a notion of priority which refines the scheduling of the meta variable instantiation. controlling the order of instantiation this way does not affect the completeness, apart from the possibility that an otherwise finite search space could become infinite.



in order to be able to prioritize proof and non-proof terms differently, there must be a way to tell them apart. one option is to distinguish between dependent and non-dependent function types. application arguments which stem from dependent function types are treated as non-proof terms and those from non-dependent function types as proof terms. we have chosen this approach in our implementation.



the search must guess?1 to be add?2?3. this is of course a possible solution, but we would like to make better use of the information that is given in the initial type defining the problem. the meta variable on one side should be able to mimic the term on the other side, just like in unification.



we have performed some experiments with the presented approach. the implementation consists of a general narrowing search system and a type checker for a logical framework written in haskell. the narrowing system is constituted by a compiler of haskell programs and a run-time system implementing the search algorithm. the narrowing algorithm accepts weakly orthogonal trss by implementing the parallel evaluation discussed in section 4.1 and presented in more detail in. it also includes a prototype of the subproblem separation feature presented in section 4.2. the type checker is based on the one presented in section 3, but modified to enable parallel conjunction and the performance enhancing features presented



an inductive proof containing some kind of generalization serves well as an illustration of the order of instantiation which is imposed by the prioritization presented in section 5.2. the example is to construct a proof of the proposition stating that a given function, sort, always returns a sorted list.



in order to save space we will omit the definitions of sorted and sort, but merely state that sort implements insertion sort. the reader thus cannot confirm that the proof is correct. nevertheless, the mechanisms of the proof search should be clear. the function sort is defined in terms of sort' which, in turn, uses insert. the names list and nat refer to the standard recursive definitions of lists and natural numbers. the proof search is provided an elimination constant for lists, by adding a couple of non-standard features of the narrowing, as well as a number of small and rather general modifications of the type checker. we have made an implementation to get some empirical evidence of the usability of the approach. the preliminary conclusion is that the approach could be useful in situations where low cost is important rather the high performance. the experiments are however too limited to be able to give a more solid conclusion/



e.g. insertion sort. but the main obstacle seems to be that, although we restrict the search to terminating functions, a lot of very inefficient candidates are still generated. this could be amended by adding some notion of function complexity to the type system.



