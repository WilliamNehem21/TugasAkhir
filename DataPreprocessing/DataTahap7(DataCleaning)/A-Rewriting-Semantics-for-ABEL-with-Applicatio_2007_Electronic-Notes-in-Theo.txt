contemporary research into embedded system design[22,13,7] has advocated a significant change to the process outlined above, whereby the major functionalities are first modeled at a high level before being mapped to specific technologies, such as a dsp or asic. modeling is realized through a collection of formalisms covering a variety communication, concurrency, and computation schemes. in the ptolemy ii system design environment, for example, an engineer can describe computations using combinations of the many supported models, including data-flow network and discrete-event models. by separating functionality from implementation, tools such as ptolemy ii attempt to coordinate the system design process and produce verified designs more quickly by leveraging modularity and abstraction. in limited cases, automated tools have been able to produce implementations directly from these high-level models.



the rest of this paper is organized as follows. section 2 presents, by way of abel, a high-level overview of how the syntax and semantics of a hardware description language are specified in maude. section 3 explains in detail our framework for specifying and trace checking co-designs and illustrates our proposed method through two in-depth case studies. section 4 discusses related work, and section 5 presents some conclusions.



synchronous digital circuits need to respect two separate notions of execution order. first, during each clock cycle, the topology of the combinational(non-state carrying) network must be respected: each internal node value within the circuit is considered accurate only after its inputs become accurate, and some amount of time has elapsed to compute the value from the stabilized inputs. second, latches should respect a global clock and should all be updated in parallel.



clearly, the verification of a co-design project should include the interactions between hardware and software; both to ensure system wide correctness and to validate the engineering choices that led to the partitioning scheme. if, for example, the instruction throughput is never high enough to take advantage of all of the functional units, then it might be worthwhile to use this information to reevaluate the hardware partition and scale back the design. if the software is complicated and depends on many subtle interactions with the available hardware, then this inefficiency might be very hard to uncover without an implementation to test on, or without formal analysis. if, as a second example, the co-design effort tried to minimize power consumption, then it would be useful to formally validate how the power saving features perform when the software is executing on the chip. hence, validation of the co-design decisions after implementation may be necessary to guide redesign or other efforts in the future.



creating a maude specification for the semantics of an hdl such as abel represents our first step in an effort to facilitate co-verification. the strong modeling properties of rewriting logic, and maude in particular, allow a user to take both the hardware and software components of a design and instantly embed them into a unified mathematical framework. in addition, meta-level properties of the system(e.g. software data structures) can also be conveniently modeled and related to the rest of the design. this allows the engineer to specify properties of his/her system in the most appropriate and natural language, hence promoting a modular verification effort that abstracts away unnecessary details and helps avoid error-prone encodings.



cycle in the fetch stage. all of these are interesting features that can be monitored during simulation. if there is a violation of one of these properties, the executing software can help to put the trace into a more understandable context. on the other hand, if a piece of software is producing incorrect results, but the property passes the test, then the programmer might want to look at the software itself.



would generally depend on the application binary interface(abi) for argument and stack conventions, and also on the array implementation assumed. it would also rely on the specific register allocation map used. the abi would be defined before any circuit design work is started, and the register map would be known by the engineer who wrote the assembly code or the compiler that generated it.



in trying to schedule the instructions for high performance, we initially made a mistake that caused the program to compute incorrect results. this was caught by ps-02 and we updated the code. when we fixed the code, a load-use dependency was unintentionally created and caused a pipeline stall, this was immediately found by ps-01 and fixed.



trace checking tool for digital circuits. it can check simulation traces for violations of properties specified in a linear temporal logic augmented with first-order variables, arrays, and queues. the data structures permit property specification at a higher level of abstraction, thus improving usability and reducing low-level specification errors. however, neither of these tools can easily be used to understand hardware/software interactions.



in the embedded design space, there are industrial tools that allow a certain degree of hardware/software debugging. the xilinx edk, for example, allows the user to debug his/her software with gdb and to scope the internal hardware signals when a breakpoint is triggered. however, it does not work in the other direction: hardware events cannot be used to stop the software. furthermore, formal analysis is not supported by these debugging tools.



ing the properties related to their interactions with such environments, is as crucial as specifying and verifying the hardware/software system itself: both tasks should be done together. although we have addressed some real-time and performance issues in our case studies, a full modeling of environments, though very important, is beyond the scope of this paper. the natural approach for modeling such environments is viewing embedded systems as real-time systems that can be hybrid, and can even be both stochastic and hybrid. therefore, from a rewriting logic perspective the natural techniques and tools to use will include real-time rewrite theories, probabilistic rewrite theories, the modeling of stochastic hybrid systems, and tools such as real-time maude and the upcoming pmaude and shymaude. this should enable us to handle mechanical and sensing interfaces, such as those in many control systems, for example, anti-lock brakes systems. in the terminology of this paper it will also provide a natural extension of the meta-level properties that can be formally specified and analyzed.



