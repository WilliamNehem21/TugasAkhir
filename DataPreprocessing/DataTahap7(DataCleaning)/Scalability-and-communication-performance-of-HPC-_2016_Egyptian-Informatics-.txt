virtualization technology introduces attractive techniques to manage and multiplex computing resources. it contributed to the presence of cloud computing. cloud computing has become an alternative platform to fill the gap between scientists growing computational demands and limited local computing capabilities. in addition to virtualization, cloud computing has many benefits which are introduced to hpc applications users as elasticity of resources and elimination of cluster setup cost and time. in virtualized hpc system, computing nodes are deployed via individual virtual machines communication on razi(gigabit ethernet) and haitham(infiniband) clusters by using skampi, imb and mpbench applications. then, they compared output results from these applications and analyzed for validation. they reached that the architecture of the clusters itself might also affect the results independent of type of interconnect.



belgacem et al. connected ec2 based cloud clusters located in usa to university clusters located in switzerland, and ran a tightly coupled, concurrent multi-scale mpi based application on this infrastructure. then, they measured the overhead induced by extending their hpc clusters with ec2 resources. their results show that applying multi-scale computation on cloud resources can lead to low performance without a proper adjustment of cpus power and workload. nevertheless, by enforcing a load-balancing strategy one can benefit from the extra cloud resources.



the evaluation performance metrics of the npb kernels are mops and speedup. mops is the million operations per second of a program, while speedup is the ratio of serial program execution time to parallel program execution time. the larger the value of mops and speedup, the better performance we have. a npb class c workload has been selected because its performance is highly influenced by efficiency of the communication middleware and the support of the underlying networks.



the cg kernel showed a maximum performance(both mpos and speedup) at a cluster of 4 vms(i.e. 32 cores), then a degraded performance after that. this degradation indicates a virtualized network performance bottleneck that could be solved by better nic card virtualization implementation such as sr-iov or using faster network fabrics such as infiniband. an optimum performance of is kernel was achieved at clusters of one vm. ft kernels showed similar maximum performance at a cluster of 1 and 16 vms, under mpich2 and openmpi respectively.



the analysis of the npb kernels performance shows that the evaluated libraries obtain good results when running entirely on shared memory(on a single vm) using up to 8 cores in a10 vms, respectively, due to the higher performance and scalability of intra-vm shared memory communications. however, when using more than one vm, the evaluated kernels scale poorly, experiencing important performance drawbacks due to the network virtualization overhead. the poorest scalability has been obtained by is kernel where the highest performance was achieved at 8 cores(i.e. one vm). on the other hand, the most scalable kernel was mg, where it achieved an optimum performance at a cluster of 8 vms(128 cores).



