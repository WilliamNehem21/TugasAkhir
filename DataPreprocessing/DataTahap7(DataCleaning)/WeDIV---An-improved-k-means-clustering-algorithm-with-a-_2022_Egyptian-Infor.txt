designing appropriate similarity metrics(distance) and estimating the optimal number of clusters have been two important issues in cluster analysis. this study proposed an improved k-means clustering algorithm involving a weighted distance and a novel internal validation index(wediv). the weighted distance, ep dis, was designed by considering the relative contribution between euclidean and pearson distances with a weighted strategy. this strategy can effectively capture information reflecting the globally spatial correlation and locally variable trend simultaneously in high-dimensional space. the new internal validation index,rch, inspired by the calinski-harabasz(ch) index and the analysis of variance, was developed to automatically estimate the optimal number of clusters. the ep dis was proved reliable in mathematics and was validated on two simulated datasets. four simulated datasets representing different properties were used to validate the effectiveness of rch. furthermore, we compared the clustering performance of wediv with 12 prevailing clustering algorithms on 16 uci datasets. the results demonstrated that wediv outperforms the others regardless of specifying the number of clusters or not.



the choice of similarity metric(distance) plays a crucial role in clustering. euclidean distance is the default similarity metric in most clustering algorithms. some non-euclidean distances are also used in clustering. wu and yang designed a new similarity metric more robust than the euclidean norm in c-means clustering and developed two new clustering algorithms, ahcm and afcm. the km-m+ algorithm gears clusters toward roughly elliptical shapes and uses the mahalanobis distance for clustering. the imwk-means algorithm employing a weighted minkowski distance could overcome the drawbacks of the k-means that lack defending against noisy features. nevertheless, optimization of means algorithm derives a new distance metric s-distance with the s-divergence and is used in k-means. however, the algorithm must provide the number of clusters as a parameter. meng et al. defined a new similarity metric in k-means to capture the difference in trend characteristics between data points by considering the importance of derivative information.



except for utilizing the internal validation index, some clustering algorithms can automatically determine the cluster numbers. x-means is an extension of k-means by making local decisions for cluster centroids in each iteration of k-means and using bayes information criterion or akaike information criterion to split itself to obtain a better cluster. nonetheless, the x-means is still affected by the initialization problem. r-em is a robust em clustering algorithm based on the gaussian mixture model. this algorithm solves the problem that em is sensitive to initial values. c-fs assumed cluster centroids are characterized by a higher density than their neighbors and by a relatively large distance from points with higher densities. this algorithm determines the cluster centroids by finding the density peaks using a heuristic



minkowski distance in a hierarchical clustering algorithm, making it able to detect clusters with shapes other than spherical, but how to determine an appropriate parameter p and b should be considered. rl-fcm introduces an entropy penalty term to adjust the bias-free of fuzziness index and creates a robust learningbased model to find the optimal number of clusters. however, this algorithm does not allow the provision of the number of clusters.



w is the weight to be optimized with the range of[0, 1]. a smaller ep dis represents a stronger similarity between the two data points. ep dis would be euclidean distance when w= 1 and pearson distance when w= 0. since the ranges of euclidean distance



an appropriate number of clusters is crucial for clustering performance. milligenet et al. compared 30 internal validation indices comprehensively and found that the ch performed the best in estimating the number of clusters[37,38]. ch is a clustering internal validation index proposed by calinski and harabasz in 1974, which is defined as follows: ber of pairs of data points that are in the different r and p. c2 is the number of data points in a dataset. the ranges of ca and rand are[0, 1]. a larger ca(or rand) represents a better performance of clustering.



this work was supported by the national natural science foundation of china[grant no. 31701164], the natural science foundation of hunan province, china[grant no. 2018jj3238], the scientific research program of the educational department of hunan province, china[grant no. 17a096], the scientific research program of the educational department of hunan province, china[grant no. 18a105], and the scientific research program of the educational department of hunan province, china[grant no. 18c0171].



zheming yuan received the ph.d. degree in agroecology in 2000 from zhejiang university, china. he is currently a professor at the department of bioinformatics, hunan agricultural university. he has been the principal investigator of bioinformatics research center and the director of hunan engineering& technology research center for agricultural big data analysis& decision-making, china.



zhijun dai received the ph.d. degree in bioinformatics in 2014 from hunan agricultural university, changsha, china. he is currently an assistant professor in the hunan engineering& technology research centre for agricultural big data analysis& decision-making, hunan agricultural university, china. with main research interests in feature extraction, dimension reduction, support vector classification& regression, and their applications to biological big data.



