automated theorem proving consists in automatically(i.e. without any user interaction) discharging proof obligations which arise when applying rigorous methodologies for designing critical software systems. recent developements in the so-called lazy approach in the integration of boolean satisfiability with decision procedures for decidable theories of first-order logic have provided new means to efficiently prove or refute such proof obligations. in this paper, we present the first(known) attempt to design a distributed version of lazy theorem proving on a network of computers so that the available processing power can be used more effectively and avoid that automated reasoning be the bottleneck of the application of formal methods. experiments clearly show the viability and the benefits of the proposed approach.



formal verification tools and techniques are challenged by increasingly complex software systems. in particular, checking that a property is met by a system is the bottleneck in the application of virtually any formal design approach. one recurring approach to this problem consists of building a conservative abstraction of the system and check if it is satisfied. if this is the case, the original model also satisfies the property; otherwise, we are not allowed to conclude and a refinement step is undertaken. the abstract model is changed to take into account more details and it is checked if such a model satisfies the property of interest. this abstract-check-refine cycle is repeated until we are allowed to conclude or the available computational resources are exhausted. state-of-the-art model checkers for software verification(see e.g.[8,2]) are typical examples of this approach to verification.



the construction of a heterogenous, distributed system is a challenging task, both from a design and implementation point of views. the toolbus provides an elegant solution to implement robust distributed systems, using a process algebra as language to describe the protocol between the different components, and a uniform, term-like, communication data type called aterms, which is also used in harvey to represent logic formulas. in this section, we provide the reader with just the information necessary to understand how harvey components may be distributed and interconnected with the help of the toolbus. further details can be found in



first(lines 12 to 15), m can emit a new assignment to be checked for unsatisfiability by some slave first-order reasoning tool s. s is then initialized and signaled this new verification task. m is sent an acknowledgement as soon as the verification has been started.



the verification in the slave tools is done asynchronously with respect to the master tool. it is up to the master to create new assignments and dispatch them to slaves that have previously been connected to the tool bus. a detailed description of the inner workings of the master-slave tool is presented in the next section.



the distributed version of harvey, as described in the previous section, has been implemented and tested in a small network of workstations. we report on two experiments that we have carried out. first, we present in section 5.1 different approaches to choose the assignment. as a matter of fact, as this choice has a direct impact on how much the propositional formula gets pruned, it may affect the efficiency of the verification. second, in section 5.2, we present the speed-up obtained with the distributed version of harvey considering a varying number of available slaves.



we have developed several approaches to choose a(satisfying) assignment from the propositional abstraction of a formula. as distributed harvey only uses bdds, we have developed these approaches for bdds. we recall that an assignment is represented in a bdd as a branch from the root node to the leaf node for the constant true. traversing the bdd to find an assignment is straightforward and basically consists in recursing down the graph up to a node that has a true child node. the complexity is thus o(n), where n is the number of atoms in the verified formula.



by picking the right child. the random approach choses randomly one of the two child nodes. the zigzag approach choses alternatively the right or the left child. finally, in the alternate approach, the traversal produces alternatively the rightmost or the leftmost branch.



when two slaves deliver a result concomitantly, the master might receive them in a different order. this may result in different prunings, and cause the master to dispatch a different assignment at the next iteration, which ultimately explains the difference in the number of generated assignments. in our experiments, we repeated each verification several times and report here an average value of the measures.



thus, for each of the 50 examples in our benchmark, we measured the number of dispatched branches with one(b1), two(b2) and four(b4) slaves, using the four assignment choice approaches presented in the preceding section. to illustrate the impact of distribution, we computed the estimated speedup sn=(n.b1)/(bn), where n is the number of slaves.



finally, note that, in some experiments(mainly happening in those conducted with the random approach), distributed theorem proving achieves super-linear speedups. this happens when the proof of unsatisfiability of an assignment is so general that it prunes a relatively large part of the search tree. this may happen for the class of formulas such that the value of a relatively small subset of the atoms causes unsatisfiability. our experiment shows that the probability of achieving super-linear speedups is larger in the random choice assignment approach.



the tool interaction protocol and is used to generate the code responsible for the communication and synchronization, as well as the interfaces that the tools shall implement to participate in the interaction. also, we developed different approaches that impact on how the workload is distributed. we validated our approach via a series of experiments that show, first, that the distributed version does indeed achieve an interesting speedup in average and, second, that the so-called random assignment choice approach is the most efficient policy.



