compound sentences which female actor played in casablanca and is married to a writer born in rome?. estimates that 9% of the total errors in queries generated by their ganswer tool is due to the fact that it does not handle queries with unions or filters.



sparql query is composed of a set of triple patterns known as graphs patterns. the graphs patterns are placed directly after the where key word or after the target variables in the sparql query. the triple patterns are of the form of< subject>< predicate>< object> where the subject, predicate and object may be variables(sparql working group, 2013). the idea therefore in this section is to process a user submitted query to identify potential triples that will be used to construct the sparql graphs. triples identified from the user query are referred here as user triples. to identify user triples from the submitted query, we categorize it into either::hasriver river}. to capture both these possibilities, for each user triple we extract, we create a second one where the concepts in the subject and object position are interchanged. therefore, for the user triple{rivers traverse mississippi} we create another{misssippi traverse river} where the concepts mississippi and rivers are interchanged. in this example we generate the following user triples.



a non-relational query is a query(sentence) which has no relational phrase linking any of its nominals. for instance, the sentence what is the area of the most populated state? is a non-relation based query. to identify triples that exist in this category of queries, we use algorithm 2.



the nobel prize?, since it is a relation based query, it is first processed based on the discussion in 3.3.1, the generatetriple(s) will extract the triples{(chemist won nobel)or(nobel won chemist)}. adding this to the triple generated based on adjective, the final user triples for the query is shown in listing 6.



to develop this kind of a lexicon, we adopted lemon(lexical model for ontologies)(mccrae, spohr,& cimiano, 2011) which is a model for lexicons that are machine readable. it allows information to be represented relative to the underlying ontology. lemon was a natural choice since it is rdf based and uses the principles of linked data. it can also be extended easily to capture the information needed. to reduce the work of generating the lexicon manually, we adopted the technique proposed in(walter, unger,& cimiano, 2013). we exploited the technique to generate the lexicon in lemon model semi-automatically. we designed the lexicon such that it preserved the structure of the underlying ontology. for each lexical entry in the lexicon we specify the following information:



an ontology triple, the position a term occupies in the user triple should match the position of the term it is mapped to in the ontology triple i.e. a term in the user triple that occupies the subject position must be mapped to a term in the ontology triple that occupies subject position. by doing this, we narrow the search space hence reducing the mapping time significantly. a user triple can be in any of this forms



the non-scalar adjectives need no special handling apart from those discussed in section 3.3.2. however, scalar adjectives help to further narrow down the query hence need addition processing on top of those discussed in section 3.4.1. the syntactic constrains of scalar adjectives are defined in section 3.3.1. to process the scalar adjectives, we execute three steps



to process the negation, we first remove the negation part and extract triples contained in the positive query. for example, in the query which river does not traverse alaska or mississippi? its positive form is which river traverses alaska or mississippi?. the user query is then processed normally to generate initial sparql query as ahown in listing 23.



for complex questions, we used two datasets, the first dataset was from the 9th challenge on question answering over linked data(qald-9)1. we specifically evaluated the parot system on the test dataset. the questions contained in the dataset are of different complexity, including questions with counts, superlatives comparatives and temporal aggregators. the second type of dataset used was that provided by mooney2 which has been used previously by panto for similar evaluation. we specifically used the dataset that is composed of geography data in the united states. the dataset is accompanied with 880 queries where each query has its expected response in prolog format. from this dataset, we converted the prolog format into owl ontology. we then selected queries that were compound in nature and contained negation. to evaluate the performance of parot on simple questions, we used 200 questions and their corresponding answers from the dataset proposed by.



we then computed the macro and micro f-measure of parot over all test questions. to compute micro-f-measure, we summed up all true and false positives and negatives and calculated the precision, recall and f-measure at the end. for the macro-measures, we calculated precision, recall and f-measure per question and averaged the values at the end. the results were compared with those of ganswer tool, which was the top performing tool in qald-9 challenge.



ber of variety of questions i.e. the wide coverage of the syntactic based heuristics. parot performs 18% better that ganswer in this task. its high coverage is depicted by its comparatively higher recall value. its high precision shows that the heuristics are able



to resolve user questions into correct sparql queries. however, an optimum performance of parot was inhibited by its inability to answer questions that start with when. it also can only partially handle aggregation. when it comes to query processing time qanswer has a significant lower response time as compared to parot. the significantly slow query response time of parot is attributed to its elaborate query analysis and categorization step which takes



all persons who have made substantial contributions to the work reported in the manuscript(e.g., technical help, writing and editing assistance, general support), but who do not meet the criteria for authorship, are named in the acknowledgements and have given us their written permission to be named. if we have not included an acknowledgements, then that indicates that we have not received substantial contributions from non-authors.



