work. for instance, latent semantic indexing converts documents and source code to a series of term-document matrices, which represent the underlying meaning of those texts, and then the term-document matrices are used to compute the similarity between documentation and source code. source code contains many special tokens, such as class names, method names, namespace names, and comments, and documentation can be categorized into different hierarchies, such as summary design documents and detailed design documents. those special tokens in source code and the hierarchical structure of documentation can be regarded as useful characteristics of software engineering, and then we can use such characteristics to refine the traceability links extracted by ir approaches.



the paper is organized as follows: section 2 gives an overview of recovering methods based on ir techniques. the main framework of recovering process is shown in section 3. section 4 presents the special characteristics of documentation and source code and proposes four strategies to improve current recovery model. section 5 compares our approach with previous work through two experiments and gives the detailed analysis of parameter tuning. finally, section 6 draws conclusion and outlines future work.



extracting the relationships between documentation and source code can be regarded as an ir query process, in which source code is converted to query terms and software documentation constitutes literature library. first, documentation and source code need to be preprocessed for information retrieval, such as retrieval items extraction, conversion between lowercase and uppercase, stop words removal, and etyma generation. second, similarity is computed by applying some ir models, and then a list of documents sorted descendingly by correlation degree is generated. finally, the records with the correlation degrees above the threshold value are selected from document list. when ir approaches are applied to the recovery of traceability links, there must be an important premise that most of the identifiers in source code should be named with meaningful words, which also exist in software documentation. to validate the hypothesis, we manually checked 50 source code files from linux software repository and 5 commercial software products, respectively. almost 97% of identifiers in source code are meaningful except some local variables.



vector space model was proposed by salton for smart information retrieval system at cornell. either a query or a document is viewed as a vector of terms(or words), and the lingual similarity of free text is transformed to spatial similarity, that is to say, the similarity among text vectors is applied to document retrieval. theoretically, the smaller the vector angle is, the higher similarity between a query and a document is. when vsm is applied to recovering the relationships between documentation and source code, every characteristic of documentation or source code can be viewed as one dimension of text space, and then the vector space is the set of those characteristics. any retrievable document is expressed as a vector in the



the low rank approximation of term-document matrix provided by lsi can filter a lot of noises and has a better spatial and temporal efficiency. lsi takes the relationships among words into account, so synonymies and polysemies can be processed correctly. moreover, etyma generation is unnecessary, which simplifies the preprocessing step.



the experiments conducted by antonial et al. showed that probabilistic model was better than vector space model with 30% precision and 70% recall. then, marcus et al. applied lsi model to the recovery of traceability links, and precision can even reach 70% while recall is about 60%. when the recall keeps invariant, the recover traceability links, the results are not as good as applied them to information retrieval. documentation and source code are different from plain text, and they have many special characteristics with respect to software engineering, for instance, documentation might contain data dictionary, uml diagrams, and class explanations; structural entities in source code also have call relationships, inheritance relationships, and dependence relationships. previous recovering methods based on ir techniques treat documentation and source code as plain text, but those special characteristics can be used to make improvement on the precision and the recall of traceability recovery. our recovering process of traceability links is divided into



firstly, the identifiers, such as class names, method names, field names, and comments, are extracted from source code. then, the identifiers that contains more than one word, for example, list polygon unite, height of window, are broken into a list of words, here, two lists are l1=(list polygon unite) and l2=(height of window). in addition, the keywords corresponding to specific programming language and stop words must be removed in the preprocessing step, while capital characters are converted to lowercase.



secondly, all the documents with multiple formats, such as word, html, rtf, and pdf, will be converted to plain text files. moreover, in order to achieve better results, the text files are partitioned into small documents with approximately same size. documents can also be partitioned according to chapters, sections, or paragraphs, and then stop words removal and uppercase conversion are operated on the small documents as same as on source code.



low-level documents are usually closely related to source code. class names, method names, or comments are often included by low-level documents. moreover, low-level documents can be considered as further detailed versions of high-level documents, such as, algorithm designs, concept explanations, and implementation details. therefore, low-level documents can be regarded as a bridge between source code and high-level documents, and an iterative refining process can be constructed according to the hierarchical structure of documentation. first of all, the traceability links of low-level documents and source code are extracted, and then they are used as feedback to revise current ir model. new query vectors are generated through learning and accumulating the concepts of low-level documents, and then those concepts will be used to retrieve relationships from higher-level documents. that iterative process will be terminated when the highest-level documents are processed.



relationships in source code. as a result, there are only 88 and 144 correct traceability links, respectively. however, our approach does not have those restrictions, and there are totally 150 correct links retrieved. the decrease of recall is due to the different constraints of experiments accordingly.



when threshold value method is applied to filter result list, we used 0.5 as initial threshold. there are 143 correct links out of 247 links retrieved, and 57.89% precision and 95.33% recall are reached. in, when threshold was set to 0.6, lsi module can get 71.01% recall and 42.98% precision. therefore, compared to traditional lsi method, alsi can improve precision and recall greatly.



different weights are assigned to comments according to their types, such as, class comments, method comments, and general comments. when the weight of class comments is increased, precision and recall will fall after rising, which is the same as method comments and general comments. when the weight of class comments is set to 2 and all the other weights keep invariant, peak value of precision and recall is reached. moreover, the weights of peak values of method comments and general comments are 1.5 and 1.5, respectively. the weight of comments can not be increased arbitrarily. tf-idf weight calculator computes the weight of a word not only in current document but in whole corpus, so any weight that is relatively too high will make tf-idf algorithm fail.



though similarity thesaurus can process synonymies and abbreviations, it still needs some manual efforts. if a developer is not familiar with the legacy system that he/she is maintaining, it is very difficult for him/her to input correct entries to the thesaurus. moreover, the preprocessing step of documentation and source code is still a tricky and challenging job, and the classifying of documents according to the hierarchical structure of documentation also needs manual interventions. therefore, more attention should be payed on the preprocessing step, and the automatic classification of documents is necessary. in the future, we will exploit more characteristics of documentation and source code with respect to software engineering to improve the precision of retrieving traceability links. in addition, more experiments on large open source projects, for instance, apache, eclipse, and gcc, will be performed with our improved ir approach.



