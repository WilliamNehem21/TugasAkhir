networks. today, support vector machines and along with other learning based-kernel algorithms show better results than artificial neural networks and other intelligent or statistical models, on the most popular benchmark problems. the scarcity of the model results from a sophisticated local learning that matches the model capacity to the data complexity ensuring a good performance on the future, previously unseen, data. they gave a single solution characterized by the global minimum of the optimized functional and not multiple solutions associated with the local minima as in the case of neural networks. moreover, support vector machines do not rely so heavily on heuristics, i.e. an arbitrary choice of the model and have a more flexible structure.



in this study, a new kernel function called gaussian radial basis polynomial function(grpf) is introduced that could improve the classification accuracy of support vector machines(svms) for both linear and non-linear data sets. the aim is to train support vector machines(svms) with different kernels compared with back-propagation learning algorithm in classification task. moreover, we compare the proposed algorithm to algorithms based on both gaussian and polynomial kernels by application to a variety of non-separable data sets with several attributes. it is shown that the proposed kernel gives good classification accuracy in nearly all the data sets, especially those of high dimensions.



the rest of this study is organized as follows: in section 2, the svm classifier is described. the multi-layer perception classifier is designed in section 3. in section 4, the svms are presented with a new kernel function. the kernel parameters are optimized in section 5. section 6 gives comparison results between support vector machines and multi-layer perception classifier. our conclusion is presented in section 7.



network architecture in use today both for classification and regression. mlps are feed forward neural networks which are typically composed of several layers of nodes with unidirectional connections, often trained by back propagation[34,35]. the learning process of mlp network is based on the data samples composed of the n-dimensional input vector x and the m-dimensional desired output vector d, called destination. by processing the input vector x, the mlp produces the output signal vector y(x, w) where w is the vector of adapted weights. the error signal produced actuates a control mechanism of the learning algorithm. the corrective adjustments are designed to make the output signal yk(k= 1, 2,..., m) to the desired response dk in a step by step manner.



where r is a statistic distribution of the probability density function of the input data; and the values of r(r> 1) and d can be obtained by optimizing the parameters using the training data. the proposed kernel has the advantages of generality. however, the existing kernels such as prbf and proposed in zanaty et al., gaussian and polynomials kernel function by setting d and r in different values. for example if d= 0, we get exponential radial when r= 1 and gaussian radial for r= 2 and so on. moreover various kernels can be obtained by optimizing the parameters using the training data.



in this study, we have constructed svms and computed its accuracy. we held a comparison between the svms and mlp classifier. we have used different sizes of data sets with different attributes. then, we have compared the results of the svm algorithm to mlp classifier. the proposed grpf kernel has achieved the best accuracy, especially with the data sets with many attributes.



in mlps classifiers, the tested data sets need more hidden units and the complexity is controlled by keeping the number of these units small, whereas the svms complexity does not depend on the dimension of the data sets. svms based on the minimization of the structural risk, whereas mlp classifiers implement empirical risk minimization. so, svms are efficient and generate near the best classification as they obtain the optimum separating surface which has good performance on previously unseen data points.



however, the main difference is in the complexity of the networks. the mlp network implementing the global approximation strategy usually employs very small number of hidden neurons. on the other side the svm is based on the local approximation strategy and uses large number of hidden units. the great advantage of svm approach is the formulation of its learning problem, leading to the quadratic optimization task. it greatly reduces the number of operations in the learning mode. it is well seen for large data sets, where svm algorithm is usually much quicker.



