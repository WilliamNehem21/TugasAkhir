in the last step, a counterexample is generated and also optimized. that is, loops and other unneeded parts are removed to ease its comprehension. the counterexample is presented in the assembly code, the c code, the control flow graph of the assembly code, and as a state space graph.



the algorithms described in the remainder of this section differ in the implementation of the functions used to access the state space. in our algorithms, two different approaches to load balancing and one partitioning function are used. moreover, communication between threads is implemented using different data structures and synchronization primitives.



for static load balancing, only the structure of the processed state has an influence on the assignment of a state to a thread. stern and dill propose using an evenly distributed hash function. the static load balancing function implemented in[mc]square is the static partitioning function described in sect. 3.1.1.



a different approach to static load balancing is described by lerda and sisto. in their approach, the load balancing function is not based on the complete state but only on small parts. the idea behind this approach is that only small parts of a successor state are changed in a single transition. this approach increases the probability that successors of a state s are processed by the same thread as s. developing an evenly distributed balancing function for this approach is challenging and sometimes impossible.



this section describes the implementation of four different parallel algorithms. for the implementation, we have evaluated the performance of the following java containers for storing states: hashmap and treemap, accessed using explicit synchronization, as well as concurrenthashmap and concurrentskiplistmap. we observed results similar to the experiments of goetz et al., that is, concurrenthashmap being the fastest solution for parallel access.



balancing. this structure corresponds to a bidirectional producer-consumer pattern. successor states are generated by n threads, which are consumed by the master thread and stored in the state space. the master thread uses the partitioning function part(s) to determine the thread that processes state s. that is, the master thread fills the queues of the invariant checkers.



this section describes an algorithm for distributed state space generation, which is based on the approaches of stern and dill, lerda and sisto, and holzmann and bosnacki. we use the term node for each process in the distributed network. one master node is used, which starts the other nodes and detects termination of the distributed algorithm. it performs a different task than the master thread for parallel invariant checking.



in the distributed algorithm, each node runs three threads. the main thread executes invariant checking, that is, it has exclusive access to the state space, and performs load balancing. in our distributed approach, static load balancing and static partitioning are used as in the parallel case. two threads, a sender and a receiver, are used for communication of states between nodes,



we used two programs for the atmel atmega16 microcontroller to evaluate the performance of the presented algorithms. the program adc.elf implements a distance measurement using an infrared controller. it consists of 467 lines of assembly code, which result in 434,756,686 states with all optimizations such as delayed nondetermism or dead variable reduction enabled in[mc]square. the program window lift.elf implements a controller for a powered window lift used in a car and consists of 288 lines of assembly code. without any optimizations, this program leads to creation of 2,589,681 states. we have chosen different abstraction techniques to uncover a potential influence on the performance of the parallel and distributed algorithms.



we identified two reasons for these results. first of all, we believe it stems from the inefficient synchronization primitives in java as it can be observed in all implemented parallel algorithms. similar results were observed by inggs and goetz et al.. another problem is the structure of our multi-core system. although the server has a shared-memory architecture, existing libraries. in the end, we only kept the implementation of the best parallel algorithm, namely static load balancing and local access to the state space, and the distributed algorithm. both algorithms can be tuned by users by adjusting the number of threads or nodes used.



more than 5 processors barely paid off and sometimes even caused a slowdown due to the synchronization overhead. comparable numbers were also observed by others such as inggs or goetz et al. when dealing with parallel java programs. it could be the case that this observation is caused by the communication between the data structures used in java. there are two possible solutions to this problem. first, we could implement the important methods in c or c++ and then use the java native interface to use these methods. another solution could be to use the new java 7 version as it will include several performance improvements.



state spaces for invariant checking can also be used for model checking. for model checking, we plan to extend our global ctl model checking algorithm. we expect that this allows to use more than a single search front for state space building. in the local model checking algorithm, this is not efficient as it contradicts the local character of the algorithm.



