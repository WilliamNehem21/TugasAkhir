cloud computing is emerging as a high performance computing environment with a large scale, heterogeneous collection of autonomous systems and flexible computational architecture. many resource management methods may enhance the efficiency of the whole cloud computing system. the key part of cloud computing resource management is resource scheduling. optimized scheduling of tasks on the cloud virtual machines is an np-hard problem and many algorithms have been presented to solve it. the variations among these schedulers are due to the fact that the scheduling strategies of the schedulers are adapted to the changing environment and the types of tasks. the focus of this paper is on workflows scheduling in cloud computing, which is gaining a lot of attention recently because workflows have emerged as a paradigm to represent complex computing problems. we proposed a novel algorithm extending the natural-based intelligent water drops(iwd) algorithm that optimizes the scheduling of workflows on the cloud. the proposed algorithm is implemented and embedded within the workflows simulation toolkit and tested in different simulated cloud environments with different cost models. our algorithm showed noticeable enhancements over the classical workflow scheduling algorithms. we made a comparison between the proposed iwd-based algorithm with other well-known scheduling algorithms, including min-min, max-min, round robin, fcfs, and mct, pso and c-pso, where the proposed algorithm presented noticeable enhancements in the performance and cost in most situations.



e.g. grids and clouds for their powerful capabilities in modeling a wide range of applications, including scientific computing, multiprocessors system, multi-tier web, and big data processing applications. the problem of workflow scheduling in cloud has become an important research topic due to the development of cloud technology.



fashion. in general, the two most important objectives of workflow schedulers are the minimization of both cost and makespan. the cost involves not only the computational costs incurred from processing individual tasks, but also data transmission costs, where potentially large amounts of data must be transferred between compute and storage sites.



as they collect some soil from the path; this increase is inversely proportional to their travel time, where the travel time duration depends on both the velocity of the stream and the distance travelled. the amount of soil in the path is a major concern when deciding the chosen path to go through; where water drops always prefer the easier path, i.e. the path with less soil. the iwd algorithm constructs probabilistic solutions for the best path(s), where the parameters of the algorithm are updated iteratively in order to converge to high-quality solutions. optimum approximation algorithms are used for finding approximate solutions to optimized solution. authors of identified three reasons for the significance of the iwd algorithm:(i) it converges to the solution faster than other techniques,(ii) it converges to high quality solutions using average values,(iii) it is flexible with the dynamic environment and incorporates pop-up threats easily.



authors in proposed a market-oriented hierarchical scheduling, which considers and optimizes both the task-toservice assignment, and the task-to-vm assignment in local cloud data centers. the cat swarm optimization(cso) was used by the authors of for workflow scheduling in the cloud. the algorithm aims at reducing wastage of energy, where its policy is based on updating positions of cats. the algorithm proved to be more efficient than pso as it converges to the optimal solutions in less number of iterations. in, a resource-aware hybrid algorithm was proposed to schedule both batch jobs and workflows in the cloud, where tasks are first assigned to group of cloud resources; then classical scheduling is applied for each group to schedule its assigned tasks.



from node i to node j. condition(j) identifies whether the node j has child node(s) or no. if it has one or more child; then, it is assigned a value of 1; otherwise, it is assigned 0. the method subgraph(j) counts the number of nodes below that particular node



this phase represents an extension to the original iwd algorithm, it starts after finishing the second phase, in which the whole workflow is traversed and all the iwd paths are discovered. in this phase, in order to assign the workflow tasks to the cloud vms, the scheduler assigns the workflow tasks level-by-level guided by the best discovered iwd paths in order. as shown in the algorithm, the scheduler traverses all the tasks tpath on the best discovered path piwd_best from its root to its leaves one by one. for the task at the root, i.e. first level, of the piwd_best, the algorithm searches for the fastest idle machine vmselected and selects it to be assigned it to this task; if no idle vm exists, the algorithm finds the next vm to be free and selects it for executing this task. in the next levels, all descendent tasks in the same path are assigned by default to the same vmselected of the task at their root, as it remains always the fastest free vm. at each level of the best path, after assigning the task tpath on the piwd_best at this level, the algorithm assigns all tasks tlevel on its level belonging to other connected paths as well from left to right, where for each task tlevel, the next fast idle vm is assigned to it; otherwise, if no free vm is available, the schedule finds the next vm to be free and assigns it to this task. this important as it ensures the respect of the precedence relationship among the tasks, i.e. no task at any level is assigned to be executed by a vm before its parent task. it is possible that after finishing the above steps some tasks remains unscheduled in the workflow, e.g. if the workflow has several unconnected paths, once the whole piwd_best is completely traversed, the same steps are repeated on the next best iwd path containing unscheduled tasks in the same workflow.



iwdc algorithm, piwd_best, is: 0? 2? 4? 6. next, the same above calculations is repeated in a second iteration, where the iwdc restart again from node(0) after purging the pre-visited paths. this results in the second best path which is: 0? 2? 4? 7. then the process is repeated again until all paths in the workflow graph is discovered, which results in the following best paths in order: 0? 2? 4? 8, 0? 2? 3, 0? 2? 5 and finally 0? 1.



in the previous sections, we compared our algorithm with a set of common heuristic algorithms. in this section, we compare it with schedulers based on the meta heuristic optimization techniques pso and its variant c-pso. pso and c-pso are workflow cloud-scheduling algorithms that rely on the social behavior of a swarm of particles and c-pso a variant of pso that is inspired by the catfish effect observed by norwegian fishermen when catfishes were introduced into a holding tank of sardines. we used workflowsim to run our scheduler to schedule 16 different workflows on a cloud environment with 6 vms similar to the environment paying around the double price which is fair enough for people which want to balance between the speed and the cost. hence, from these results, it can be seen that using m1.small and c4.large instance types can be the best choice for scheduling many workflows, where m1.small can be better for workflows that have the cost as a main concern over the makespan, while c4.large instance type is better for workflows that have the makespan as a main concern over the cost.



