a rising amount of research demonstrates that artificial intelligence and machine learning approaches can provide an essential basis for the drug design and discovery process. deep learning algorithms are being developed in response to recent advances in computer technology as part of the creation of therapeutically relevant medications for the treatment of a variety of ailments. in this review, we focus on the most recent advances in the areas of drug design and discovery research employing generative deep learning methodologies such as generative adversarial network(gan) frameworks. to begin, we examine drug design and discovery studies that use several gan methodologies to evaluate one key application, such as molecular de novo design in drug design and discovery. furthermore, we discuss many gan models for dimension reduction of single-cell data at the preclinical stage of the drug development pipeline. we also show various experiments in de novo peptide and protein creation utilizing gan frameworks. furthermore, we discuss the limits of past drug design and discovery research employing gan models. finally, we give a discussion on future research prospects and obstacles.



posed to employing artificial neural networks that only employ a single individual layer. deep learning algorithms have achieved a broad variety of applications in drug design and discovery[9,21]. these approaches are based on the most advanced computer technologies now available, such as general-purpose computing performed on graphics processing units. there is an enormous need to employ software tools in deep learning frameworks for various drug development tasks in order to address the demanding challenges we face today in the field of drug design and discovery. these challenges are presented in the form of complex problems that must be solved. to be more specific, deep learning frameworks are utilized in order to serve as tools in order to fulfill the applications of drug design and discovery, such as molecular de novo design, dimension reduction of single-cell data in pre-clinical development, compound property and activity prediction, reaction analysis, synthesis prediction, and biological image analysis. these applications include molecular de novo design, dimension reduction of singlecell data in pre-clinical development, reaction analysis, synthesis prediction, and biological image analysis.



designed to emulate that of the human mind through learning from the surrounding environment and/or experiences. when working on machine learning, there are three focuses of the model: the class of the task, the factor by which the performance must be improved by, and the source of the overall experience. using these three factors, machine learning models have the optimization goal of minimizing their learning error through the use of a variety of algorithms such as gradient descent and backpropagation. gradient descent looks at the error curve that the model generates compared to what it should be outputting and finds the right parameters for the model that minimizes the error. these algorithms have been applied in a very broad range of environments consisting of, but not limited to: computer vision, entertainment, spacecraft engineering, finance, and computational biology.



putting a discrete number of classes or regression outputting a continuous value or set of values. for example, a machine learning model such as linear regression classifying a given image as containing a cat or a dog would first need to be separately shown images of a cat and a dog to learn the features within the images to then compare to the features of a new image. for regression, a model could be trained on the previous sales of a company to then create a numerical prediction to forecast future sales and growth of the next year.



one of the biggest inspirations for gans came from deep boltzman machines that likewise utilized two separate processes running at the same time while training which included a positive and negative phase. the positive phase loaded in data and made it more likely, and the negative phase drew samples and made them less likely. through iterative training, the distribution of the samples becomes closer to the distribution the model represents. the parameters of the model are updated at the same time as new samples are being generated. however, it was found that deep boltzman machines lacked the capability to scale past the greyscale mnist digits dataset, and for example, were unable to generate color image samples in a timely manner. this is caused by the negative phase being unable to keep up with the speed of the positive phase generating samples which prevents deep boltzman machines from converging to a useful state.



plausible data, and the discriminator distinguishes between what part of the data is real or fake. the generator uses the feedback from the discriminator to update its weights and biases in order to generate data that can better fool the discriminator by looking more real. the discriminator will then begin to perform worse on the data and will have to update itself to perform better. after this, the generator will be able to use the feedback from the new discriminator to better improve its own data generation. this cycle continues until convergence. for image generation applications, the discriminator is typically a convolutions neural network(cnn) and is able to adapt to the underlying distribution of data. it will perform a binary classification to distinguish between the real and the generated data.



a year later, denton et al combined the cgan with a laplacian pyramid to break up the generation of images through multiple cnns into successive refinements. the laplacian gan(lapgan) was the first gan architecture to be able to generate high-resolution images by first generating images at a low resolution and then scaling to successive higher resolutions. this was done by taking the difference between the generated images and their blurred counterparts to produce a learnable filter.



researchers from google created the famous transformer architecture that connects the encoder, the generator, and the decoder, the discriminator, through an attention mechanism. an attention mechanism looks for the most relevant parts of a dataset through using a weighted combination of input vectors with the most relevant input vectors granted the highest weights. this same architecture was then applied to a convolutional based gan model called the self-attention generative adversarial network(sagan). sagans are able to generate data with details across the entire high resolution feature map as opposed to previous models only generating data with details in local points from lower resolution representations of the data. with sagans being attention-driven from utilizing the self attention architecture in both the generator and discriminator, they are able to converge must faster than other gan architectures and achieve state of the art results with the imagenet dataset.



for example, imagine that the generator creates images that are classified as real by the discriminator 25% of the time. therefore, the generator has a 25% accuracy, and the discriminator has a 75% accuracy. now imagine that the generator improves itself with feedback from the discriminator, so that the generator is now 80% accurate. therefore, the discriminator must be 20% accurate. in other words, the generator has improved by 55%, and therefore, the discriminator must have lost 55% points of accuracy.



used to help resolve are the issues involving the heavy cost and overall time for drug research, but one issue that has come up for gans itself involves the limitation when working to explore some regions of chemical space. we previously referenced how gans work better than other traditional methods as well as newer deep learning when it comes to chemical space, but innovations in gans look to expand and solve this shortcoming of not having complete access to chemical space. a fully quantum gan could accelerate the training process for gans, as well as offer better training samples which could allow more exploration of chemical space but lacks the ability to process over two dimensions. in the future, a fully quantum gan could be viable and successful, but more innovation in the technology is currently needed. in the



lin et al continue to describe another group of researchers, karimi et al. who investigated the gcwgan(guided conditional wasserstein general adversarial network) structure and its applications in peptide folding. this conditional wasserstein gan structure is derived from the wasserstein gan with gradient penalty. the wasserstein gan, as proposed by arjovsky et al., is a type of gan architecture which uses



a research used gans with adaptive training data for drug discover. during the training period, the model saved new and valid compounds it generates. the training data then updated using a replacement technique that may be either directed or random. the technique was repeated when the training resumes. the findings suggest that this method can counteract the decline in new molecules created by a normal gan during training. in addition, using recombination between created compounds and training data to boost novel molecule discovery. by including replacement and recombination into the training process, gans may be used for larger drug discovery searches.



continues, the discriminator maps the inputs utilizing a feature space by a local feature extractor. this local feature extractor promotes sample classification, which learns the difference between real and fake proteins and similes. after the first round is completed, the local feature extractor will be used as the feature representation of the new model. the second round is to build upon the regressor. this uses the feature extractor from the first round along with the labeled data to train the



the convolutional regression model conducts convolution operations with kernel size 4 to create feature maps of the input data. the dimensions of the convolution layers are 16x4, 32x4, and 48x4. the output layer is a linear function that obtains the continuous value. this is on a network that is trained to decrease the loss of function defined by the



in order to target these cannabinoid receptors, a study utilized a deep convolutional generative adversarial network(dcgan) model for de novo designing target-specific compounds. de novo is one of two strategies for identifying the compounds of a specific target. in this study, four types of targets, or fingerprints, were used to describe the small molecules with different structural features: ecfp6, maccs, atompair count, and atompair. a convolutional neural network(cnn) was the architecture for the deep learning model used in this study. it was first built using the four aforementioned types of targets as input data. then it was applied to the discriminator of the dcgan. afterward, the generator was created through reverse convolution. this strategy minimizes the discriminative and generative loss simultaneously.



input with the label of zero. the discriminator chooses between these fingerprints and the authentic data which is labeled as one. the second step is minimizing the generative loss. here the discriminator is not trainable and the generative loss is recorded to show how well the generator fools the discriminator. the learning epochs were 50, 100, and 200, while the batch size was 128. when comparing the different cnn architectures, the lenet-5-based architecture and the atompair fingerprints performed the best. based on the roc curves, this duo attained the highest auc score across each test set for both types of receptors, cb1 and cb2. this is the optimum combination using the first as the generation for the discriminator and the second as the input data. this strategy lets the machine produce fingerprints of a potential targetspecific compound without any effort from the user.



the study does point out two concerns regarding the gan architecture. the first challenge is to optimize both the discriminator and the generator simultaneously. however, this instability can lead to a gradient favoring one over the other. this could result in an improved score for d, but not g, or vice versa. secondly, there can only be a restrictive set of outcomes to be generated, also known as mode collapse. with the restrictive set, the generator can only produce one type of outcome or a small sample. only a limited chemical space is to be covered as well as a lack of structural diversity.



data frameworks. drug discovery relies on a wide variety of data kinds, many of which are far from exhaustive. one example is that not all protein folds and structures are fully understood, and the data space is only partially covered. therefore, even if significant progress has been made, applications in which these structures are projected are not yet as good as in other fields. for small molecule synthesis, where the complete chemical space is unknown, this holds true as well for reaction prediction.



concerns about the veracity and reproducibility of findings. as a means of avoiding potential pitfalls, understanding the current state-of-the-art of research reproducibility in computational drug discovery is crucial. this will guarantee that the underlying work is of high quality and will withstand the reproduction of the described methodology by the external research group. in this overview, we looked at the various methods



transparency in implementations and easy access to validate the analyses are both benefits of open source. it might be challenging to maintain track of the various analysis tools and settings while dealing with data and modeling. as a result, drug discovery workflow solutions are becoming increasingly popular. they aid in producing more trustworthy multi-step computations, as well as in establishing a trail of evidence and making results easy to reproduce. common workflow language is one example of an initiative seeking to standardize and promote interoperability among workflow specification languages.



the usage of public or shared computing infrastructures(hpc/cloud) is necessitated by the ever-increasing amounts of data, but this introduces a new layer of complexity for computational reproducibility. the usage of virtual machines and software containers has made it possible for all data analysis tools to be used across different platforms. high levels of automation and, by extension, increased repeatability, are possible when workflow systems are integrated with the container and virtual machine infrastructure. for example, when deploying models as networked services, virtual infrastructure and containers allow for more dependable and reproducible service delivery.



