this paper introduces an approach to dynamic software composition in the context of scientific computing where high demands performance seem to prevent such flexible solutions. in our concrete however, dynamic software composition is rather a way to high-performance than an obstacle to it. we achieve this by combining dynamic architectures and task graph scheduling.



in the world of scientific computing a common problem is the mapping of one or more computations onto a grid of interconnected machines. the efficiency of the application or even the feasibility of running the computation is very much dependant on this mapping. unfortunately, finding an optimal mapping is an np-hard problem.



this paper presents a dynamic service infrastructure for high-performance applications solving problems like those discussed above. it is originally developed for the lois space antenna it-infrastructure. our solution is influenced by a number of different fields within computer science, for instance optimization of parallel programs, dynamic software architectures and software composition.



on this it infrastructure, we execute the"experiments", i.e. applications processing data from the sensors. these applications are data parallel programs. their input are either sensor input values or the output of other applications. if the input of an application a is the output of an application



for components we assume a high performance fortran(hpf)-like programming model, with data parallel synchronous program but without any data distribution. for simplicity, we further assume that the programs operate on a single composite data structure which is an array, a. the size of an input a, denoted by|a|, is the length of the input array a.



array a is either the array of input values or the output of another component. note that in our scenarios, the number of input values is fixed. we may assume that the output size of an application is a function of the input size. by induction, it follows that the input size is fixed for all applications in our systems.



x. in many cases of practical relevance it only depends on the problem size n. we call these program oblivious and denote its task graphs by gn. we write g instead of gn if n is arbitrary but fixed. the height of a task v, denoted by h(v), is the length of the longest path from a task with in-degree 0 to v.



the computation of a optimal logp-schedule is known to be np-hard. however, good approximations and heuristics, c.f. our own contributions in task scheduling, e.g.[9,8,7], guarantee a small constant factor in delay. in practice, the results are even closer to the optimum. the techniques can be



so far, we defined the components as data-parallel applications, translated to task graphs and scheduled to the it infrastructure. for each component, we can determine an upper time bound for its execution. each component implements a function mapping an input array ai to an output array ao.



these task graphs. adding a new application e' is still specified by calling its corresponding function with the result of an existing application e. however, only the new application e' is translated into a new task graph. the input tasks of this task graph are connected with the output nodes of the task graph of e and then handed over to the scheduling unit. inversely, removing e' leads to disconnecting the corresponding task graphs and deleting transitively



while reusing task graphs is straight forward, reusing schedules is not since a optimum schedule(or its approximation) does not necessarily keep the schedules for the different task graphs distinct. instead, it might merge task of different task graphs into one process. moreover, optimum schedules of individual task graphs(or their approximations) are, in general, not part of the optimal schedule for a composed system(or its approximation).



so far our components are modelled as malleable tasks. these tasks are composed to systems modelled by malleable task graphs. the systems are mapped to the it infrastructure by reusing the pre-computed mapping of the individual malleable task graphs. it remains to show how the schedules



the second assigned task is event management. events generated externally(e.g a new schedule arrives) and internally(e.g increased sample rate) will be responded to proper actions taken. this is governed by a coordinator application. this coordinator is also responsible for feeding information back to the conceptual coordinator, for instance forwarding events that initiates a new lookahead schedule.



for system level initiation, coordination is supported on different levels. coordinator components present in different applications connect to coordinator entities on the system level. the network of probe and actuator instances also work on this level. making these entities available for both on-line and off-line



the main contribution of the paper is the introduction of a dynamic service infrastructure for data-parallel applications, cf. section 5. we built on a static composition and optimization model of data-parallel programs, we introduced a system infrastructure allowing both high performance computing and dynamic change of the systems architecture. key to our solution are precomputations of possible changes in the architecture based on run-time events. these evolved system architectures are translated, optimized in parallel to the execution of the actual system. on occurrence of a change triggered by any of those events, the new optimized system can be deployed without further preparations. additionally, such an event triggers the translation and optimization of the new generation of systems the currently running will possibly evolve to.



