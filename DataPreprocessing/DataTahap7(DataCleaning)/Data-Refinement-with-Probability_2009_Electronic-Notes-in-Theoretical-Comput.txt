in the transformer model the two forms of simulation are again sound, but this time upwards simulation is by itself complete. the reason is that downwards simulation is able to be expressed by a galois connection. that is the basic technique used here. the proof in the relational setting that the two simulations generate all data refinements reduces in the transformer setting to the stronger statement that a composition of upwards simulations(again an upwards simulation) does so.



r. in that setting, the counterpart of the relational model is the distributional model of he et al. and that of the transformer model is the expectation-transformer model of morgan et al.. again there is a galois connection between them.



in section 3 we consider data refinement for probabilistic programs. we begin by establishing structure on the spaces of simulations then show that in the distributional model, neither kind of simulation is alone complete for data refinement. further progress is beset by the weakened laws of probabilistic programming. we conclude with a discussion.



the pair(i, f) enjoys further properties. for instance hi and hf bound upwards simulation. for convenience we write[hi, hf] for the set of functions satisfying refinement(2)(with no intention to imply that it is totally ordered).



in proposing a complete technique for data refinement in the probabilistic setting, much remains to be done. what might play the role of the power-set construction in the probabilistic case? theorem 3 shows that, unlike the standard case, it does not suffice to consider deterministic datatypes. moreover the result is crucially dependent on the semantic model of computations and even on the manner in which the semantics of a module is cast. the former is inherited from the standard case but the latter is new.



