the calibration process in the vda had been done manually. this manual calibration process is considered a complex, time-consuming task because each time a dbms has to run on a different server infrastructure or to replace with another on the same server, the calibration process potentially has to be repeated. according to the work in this paper, an automatic calibration tool(act) has been introduced to automate the calibration process.



cloud computing is a new generation of computing. it allows users to use computational resources and services of data centers(i.e., machines, network, storage, operating systems, application development environments, application programs) over the network to deploy and develop their applications. the main feature of cloud computing is providing self-service pro-



the rest of this paper is organized as follow; the related works are described in section 2. the calibration problem in the vda is described in section 3. the proposed automatic calibration tool for dbms query optimizer is discussed in section 4. in section 5, the optimization problem in the vda will be handled. in section 6, the proposed gpso algorithm will be discussed. in section 7, the act and the gpso algorithm evaluation results are introduced. in section 8, the paper is concluded; also a brief outlook into the future work is given.



the virtual design advisor employs a white-box approach for modeling the performance of the dbms[8,9]. on the other hand, the black-box approach for performance modeling has been used in to drive an adaptive resource control system that dynamically adjusts the resource share of each tier of a multi-tier application within a virtualized data center. the two approaches; black-box and white-box have been used to solve the resources provisioning problem for dbms on the top of iaas cloud.



for example, without loss of generality, with three shared resources(cpu, memory, i/o), that is, m= 3, an allocation of 50% cpu, 30% memory, and 25% i/o to vm1 results in the vector r1=[0.5, 0.3, 0.25]. we assume that each workload wi has a relevant cost under resource allocation ri. this cost is represented by: static environments. in the real world, however, many applications are non-stationary optimization problems; they are dynamic, meaning that the environment and the characteristics of the global optimum can change timely. several successful pso algorithms have been developed for dynamic environments. one of these algorithms is fast multi-swarm optimization algorithm(fmso). it uses two types of swarm; one to detect the promising area in the whole search space and the other swarm is used as a local search method to find the near-optimal solutions in a local promising region in the search space. another approach is used to adapt pso in dynamic environments. it is based on tracking the change of the goal periodically. this tracking is used to reset the particle memories to the current positions allowing the swarm to track a changing goal with minimum overhead. cooperative particle swarm optimizer(cpso) has been introduced for employing cooperative behavior to significantly improve the performance of the original pso algorithm.



resource allocation for the vms. it implements a search algorithm, such as greedy search and dynamic programming, for enumerating candidate resource allocation. the vda uses a greedy search algorithm, which is based on iterating until no performance gain can be incrementally achieved[8,9]. each iteration, a small fraction of a resource is de-allocated from the vm that will get hurt the least and allocated to the vm that will benefit the most. the greedy algorithm makes the decisions of increasing and decreasing the resources allocated to vms based on the estimated cost of the given workloads.



the worker is the second module in the act. it runs in a guest vm. the worker module receives its inputs from the controller module and sends its output back to the controller. it uses the calibration database for executing the input queries.



the experiment described here uses postgresql 8.4.8 database system installed in a machine with core2 duo t5870 2.00 ghz processor, 4 gb memory, and centos 5.5 operating system. the virtual machine monitor used was xen, which is an open source virtualization platform. xen-based para-virtualization has been used to improve the hypervisor performance when it maps resources directly into the guest operating system. amazon ec2 is based on xen virtualization, and thus, this experiment setup is similar to a cloud computing environment.



the combination of the gpso algorithm with any profiling technique for random workloads characteristics in terms of resource consumption(e.g., cpu, memory, and i/o) gives the perception for the intensivity of workloads. this perception can guide the cloud provider to allocate an appropriate amount of resources to incoming workloads. the provider can arrange the workloads over multiple pools based on the intensively of workloads or use cloud bursting to maintain strict sla even when some incoming workloads are heavily cpu-intensive. where cloud bursting means that an application deployment model in which an application runs in a private cloud or data center bursts into a public cloud when the demand for computing capacity spikes. the advantage of such hybrid cloud deployment is that an organization only pays for extra compute resources when they are needed. on the other hand, the gpso algorithm can be used continuously to capture the randomness of the dynamic workloads variation by implementing it again periodically or on particular events and changed the resource allocation periodically in each time interval.



