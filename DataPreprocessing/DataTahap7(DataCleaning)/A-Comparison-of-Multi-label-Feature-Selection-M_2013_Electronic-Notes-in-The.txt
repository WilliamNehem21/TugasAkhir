feature selection is an important task in machine learning, which can effectively reduce the dataset dimensionality by removing irrelevant and/or redundant features. although a large body of research deals with feature selection in single-label data, in which measures have been proposed to filter out irrelevant features, this is not the case for multi-label data. this work proposes multi-label feature selection methods which use the filter approach. to this end, two standard multi-label feature selection approaches, which transform the multi-label data into single-label data, are used. besides these two problem transformation approaches, we use relieff and information gain to measure the goodness of features. this gives rise to four multi-label feature selection methods. a thorough experimental evaluation of these methods was carried out on 10 benchmark datasets. results show that relieff is able to select fewer features without diminishing the quality of the classifiers constructed using the features selected.



1 this research was supported by the brazilian research council fapesp. the authors would like to thank the anonymous referees for their insightful comments on this paper. we would also like to thank victor augusto moraes carvalho and antonio rafael sabino parmezan for their help in additional analysis.



for single-label learning, where each example(or instance) in the dataset is associated with only one class, feature selection has been studied for many years. however, few results in feature selection on multi-label learning have been reported. unlike single-label learning, each example in multi-label learning is associated with a subset of labels, i.e., each example can simultaneously belong to multiple classes. in addition, these labels are usually correlated. multi-label learning is an emerging research topic due to the increasing number of applications where examples are annotated using more than one class, such as bioinformatics, emotion



this work proposes and experimentally evaluates four multi-label feature selection methods, which use the filter approach. in this approach, the goodness of a feature is evaluated irrespective of any particular classifier. we use the standard multi-label feature selection approach, which consists of first transforming the multi-label data into single-label, which is then used to select features.



the binary relevance approach transforms the multi-label dataset into many single-label datasets, one for each individual label in the multi-labels. one disadvantage of this transformation, is that it does not take into account label dependence, an important aspect in multi-label learning. after this transformation, the contribution of each feature according to each individual label in the multi-labels is measured and the average of the score of all features across all labels is considered. finally, features with an average score greater than a threshold are selected.



it should be observed that the first method, rf-br, was initially proposed in, although it was evaluated in few datasets. the evaluation of rf-br using more datasets was carried out in, where it was experimentally compared with another feature selection method which directly measures the feature goodness in the multi-label dataset. besides these two pieces of work, we are not aware that relieff has been used for multi-label feature selection.



the rest of this work is organized as follows: section 2 briefly presents multilabel learning and section 3 addresses feature selection for multi-label learning, as well as related work. the filter methods proposed are described in section 4 and their experimental evaluation in section 5. section 6 concludes and highlights future work.



multi-label learning methods can be organized into two main categories: algorithm adaptation and problem transformation. the first one consists of methods which extend specific learning algorithms in order to handle multi-label data directly. the brknn algorithm used in this work is in this category. the second category is algorithm independent, allowing the use of any state of the art single-label learning algorithm to carry out multi-label learning. it consists of methods which transform the multi-label classification problem into either several binary classification problems, such as the binary relevance approach, or one multi-class classification problem, such as the label powerset approach. recall that single-label learning is called multi-class classification whenever there are more than two class values, and it is called binary classification when the class values are yes/no. both approaches, br and lp, are used in this work and are described next.



this approach decomposes the multi-label learning task into q independent binary classification problems, one for each label in l. in other words, the multi-label dataset d is first decomposed into q binary datasets dyj, j= 1..q which are used to construct q independent binary classifiers. in each binary classification problem, examples associated with the corresponding label are regarded as positive and the other examples are regarded as negative. finally, to classify a new multi-label instance br outputs the aggregation of the labels positively predicted by the q independent binary classifiers. as br scales linearly with size q of the label set l, it is appropriate for a not very large q. however, it suffers from the deficiency that correlation among the labels is not taken into account.



this approach transforms the multi-label learning task into a multi-class learning task. to this end, lp considers each unique combination of labels in a multi-label dataset as one class value of the correspondent multi-class dataset. in other words, each ei=(xi, yi), i= 1..n, is transformed into ei=(xi, li) where li is the atomic label representing a distinct label subset. in this way, unlike br, lp takes into account correlation among the labels. however, as the number of class values of the correspondent multi-class dataset is given by the number of distinct label subsets in d, the main drawback of this approach is that some class values in the multiclass dataset may be associated with a very small number of instances, making the multi-class dataset unbalanced.



to improve the predictive performance and to tackle directly the multi-label problem, the extensions brknn-a and brknn-b were proposed in. both extensions are based on a label confidence score, which is estimated for each label from the percentage of the k nearest neighbors, containing this label. brknn-a classifies an unseen example e using the labels with a confidence score greater than 0.5, i.e., labels included in at least half of the k nearest neighbors of e. if no label satisfies this condition, it outputs the label with the greatest confidence score. on the other hand, brknn-b classifies e with the[s](nearest integer of s) labels which have the greatest confidence score, where s is the average size of the label sets of the k nearest neighbors of e. in this work, we use the brknn-b extension.



unlike single-label classification where the classification of a new instance has only two possible outcomes, correct or incorrect, multi-label classification should also take into account partially correct classification. to this end, some measures, called example-based, were specifically defined for multi-label task, while others, called labeled-base, are adaptations from the single-label classification problem. a complete discussion on the performance measures for multi-label classification tasks is out of the scope of this work, and can be found in. in what follows, we briefly describe the six multi-label evaluation measures used in this work.



feature selection has been an active research topic in supervised, semi-supervised and non-supervised machine learning, with a large number of related publications and comprehensive surveys[12,15,31]. however, most of the research related to supervised feature selection has been mainly to support single-label classification, and few results on multi-label classification have been reported. this was confirmed by a systematic review process related to multi-label feature selection we carried out in. despite the growing interest on this research topic in recent years, less than 60 related papers were found by the systematic review process. some of these papers are addressed next.



however, most papers propose a previous transformation of multi-label data to single-label data, i.e., to multi-class data or binary data using respectively the label powerset or the binary relevance approach. whenever the br approach is used, features are independently selected in each binary data and the results are combined using, for example, an averaging approach.



currently, methods which perform feature selection considering label correlation have also been proposed. the chi-square measure is applied according to the lp approach in. in an evaluation measure which concerns the ranking quality between output labels is used. the mutual information measure is applied in according to a modified lp approach, which also considers label dependence. the symmetrical uncertainty measure is extended in to find relationships between all pairs of features and labels. furthermore, in it is proposed to simultaneously do feature selection and learn the labels correlation during label ranking.



the specific versions of the br and lp problem transformation approaches used in this work, as well as the algorithm brknn-b described in section 2.4, are the ones available in mulan. brknn-b was executed with k=5. weka provides the implementation of relieff and information gain, which are used by the proposed



initially, for each dataset, the classifier constructed by brknn-b using all features was evaluated using the multi-label measures described in section 2.5. these results were used as a baseline to evaluate the goodness of the feature selection methods proposed. for each dataset, the four feature selection methods were executed and the classifier constructed with the selected features was evaluated.



as can be observed, aside from these 5 cases, which were all obtained by information gain as a feature importance measure, the average feature reduction shows a high variation. it goes from 0.00%(all features were considered important by the feature selection method) up to 99.55% for dataset 6-enron(only 0.45% of the features were considered important by the ig-br method). moreover, for some datasets such as 6-enron, there is a high feature reduction variation, going from 99.55% for ig-br, down to 0.00% for ig-lp.



as can be observed, rf-lp shows in general good performance, followed by rf-br and ig-lp. on the other hand, ig-br is the one that failed most(4 cases) in finding important features, followed by ig-lp(1 case). furthermore, the few measure values worse than the baseline are concentrated in three datasets: 3corel16k001, 4-corel5k and 6-enron. observe that these three datasets have a high number of different combinations of labels in common.



in general, the experimental results suggest a relative superiority of the methods which use relieff as importance measure, compared with the ones that use information gain. this could be due to the fact that relieff considers the interaction among features. moreover, there is little difference between the measures obtained by the feature selection methods built using the lp or br approaches and the same importance measure, i.e., relieff or information gain. however, it was expected that methods using the lp approach would show better results than the ones using the br approach, as lp takes into account label interaction. indeed, it is expected that methods that take the interaction among labels into consideration should lead to better results.



nevertheless, there are two aspects that should be jointly considered when feature selection methods are evaluated: the reduction in the number of features versus the performance measure values of the classifier generated with the features selected. this sort of evaluation is better carried out by a graphical analysis. in what follows, this analysis is illustrated in datasets 8-medical and 1bibtex. graphs for all the datasets used in this work can be found at http: all features across all labels is considered. finally, features with an average score greater than a threshold are selected. the second method transforms the multilabel data into single-label data using the label powerset problem transformation approach, in which any single-label feature selection method can directly be applied.



the methods were experimentally evaluated using 10 multi-label benchmark datasets. to this end, we use the brknn-b multi-label learning algorithm, using the selected features to construct the classifiers for each dataset. in addition, the classifiers constructed by brknn-b using all features are considered as baseline for each dataset.



as future work, we plan to broaden the experimental evaluation of relieff using synthetic datasets. furthermore, the analysis of potential relieff extensions for feature selection in order to tackle the multi-label feature selection problem directly, i.e., without any problem transformation, will also be considered.



