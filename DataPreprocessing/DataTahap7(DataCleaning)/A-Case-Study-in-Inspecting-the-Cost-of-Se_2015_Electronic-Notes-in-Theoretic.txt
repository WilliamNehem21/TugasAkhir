the growing demands of business and the competition in the provision of services has led to many enterprises outsourcing it provision using cloud computing to handle business processes and data management. ideally, the benefits that are offered by cloud computing technology can accommodate the rapidly increased demands of the organizations and individual customers. however, the usage of cloud computing has potential security concerns. the proposed research will put forward an approach for investigating the cost of security in cloud computing. the proposed method is based on a multi-level security model, which uses the distribution of partitioned workflows upon hybrid clouds. furthermore, the pepa eclipse plug-in tool will be used to create a cost model that uses the valid deployment choices that generated by the multi-level security model. the outcomes that can be obtained by means of implementing this research will used to describe the evaluation of the performance. thus, predictions of the performance can be derived as well as the potential cost of deployment under different options and scenarios.



over the last few years cloud computing has become a valuable option for a considerable number of organizations. the reasons behind this move are the growing capability of outsourced solutions and the high cost of buying and maintaining infrastructure. this allows organizations to exploit the advantages offered by cloud computing such as: high performance, availability, scalability and the low cost(i.e. pay on demand).



to migrate toward cloud computing to cope with their rapidly increased processes and related data. services offered by an organization are often business processes presented as workflows. the deployment of workflows on internal resources(i.e. private cloud) can affect the performance of services where the resources are limited. on the other hand, cloud computing(i.e. public cloud) can overcome the limited resources problem that is facing enterprises and can offer high performance with cost-saving.



although the challenges facing large scale computing have been understood for some years, cloud computing has highlighted several security concerns, for instance where the organizations data will be stored and how ensure the confidentiality and privacy of information. correspondingly, the security aspects are one of the main concerns for many organizations[5,4]. moreover, according to surveys that conducted by idc in 2008 and 2009, security is still the top challenge for cloud services. thereby, some companies tend to use a combination of public and private clouds based on the sensitivity of data; private cloud as they perceive more secure and public clouds to gain the benefit of high performance, scalability and availability.



corporations can obtain many significant improvements in security by using cloud computing. based on the multi-level security model presented by watson, this research will investigate the cost of security for the deployment of the partitioned workflow in cloud competing. moreover, the pepa eclipse plug-in tool will be used to create a cost model that relies on the valid selections generated by means of watsons approach. furthermore, a new cost model will be developed to evaluate the variety of distribution possibilities, where each option has its characteristic to deploy partitioned workflow on federated clouds.



mach and schikuta introduced a new cost model, where they stated that their economic cost model can provide a beneficial information for cloud provider and consumer based on the calculation of the servers energy consumption. they assume that a cloud cost model can be considered as a traditional production process that has production factors and produced goods. as well, the benchmark specpower ssj2008 that developed by spec is used to test performance and power consumption. the produced information can be used to make right decisions for the business strategies that might be implemented and its impact. in spite of some similarities with watson method as well as to the work presented by pearson and sander, mach and schikutas work lies in operational cost that are related to power



a further approach has been presented by nada et al for partitioning bpel, a workflow description language. a program technique is used in order to partition automatically. this approach states that the distribution of data brings several improvements on performance for example reducing network traffic. in addition, the method of nada et al has attempted to reduce the cost of communication between partitions.



from the consumers perspective dillon et al argue that, numerous models of cost can be raised specially with the use of hybrid cloud distribution model where, enterprise data needs to be transferred from its source(private cloud) to its destination(public cloud) and vice versa. the integration of consumer data with the cloud providers is shown to add additional cost. likewise, leinwand also discusses the cost of using cloud computing. the charge of an application that generates a lot of bandwidth using windows azure platform has been presented as an example. additionally, it has been suggested that, if size of data in excess of 50 gigabytes the cloud consumer should buy his own bandwidth.



an experimental methodology is introduced by cerotti et al, where the performance of multi-core systems in the cloud have been evaluated. several types of benchmarks such as: dacapo and iozone have been implemented on virtualbox and amazon ec2 platforms. hence, the obtained outcomes are used to acquire estimations of the response time. although numerous of experiments are implemented on real platforms, the cost only mentioned from the provider side, whereas their findings show that the provision of many single-core machines is more cost-effective than single machines with many cores.



the importance of modelling business processes is stated by adriansyah et al, through an approach that provides an analysis of the performance. the techniques that have been used in their work starting with create a process model via yawl language and then replay the developed event logs framework against the model. open source tool have been used such as: open source framework process mining prom and extensible event stream xes. some similarities to our proposed approach can be observed in modelling the business process and performance analysis. however, the aim is different, as the proposed work will concentrate on the cost in cloud taking into account the importance of evaluation of performance of the modelled systems.



workflows modelling languages acting essential role in abstracting business processes, modelling and analyzing. therefore, several workflow modelling languages including yawl and bpel will be investigated in order to create a new cost model for the deployment of partitioned workflow over hybrid clouds. in addition to this, extensive experiments will be implemented to simulate the performance behaviour of completion of workflow activities. also, a comparison between the results those will be obtained from the afore mentioned modelling languages will take place.



public and private instances, n2 and n3 respectively, respresent the number of servers available in the public and private clouds. in option 1 the activities read and anonymize will be distributed on a private cloud whereas analyze and write will be deployed on public clouds. option 2 differs in that only the write activity will be deployed on the public cloud and in option 3 only the analyse activity will be deployed onto the public clouds. in option 4 all the actions activities are deployed only on the private cloud and so no public component is defined in this case.



public). in the terminology of pepa, this is referred to as active-active cooperation. the reason for specifying the rates in this way is because we wish to exploit the definition of the apparent rate in such a way to investigate the scalability of the cloud provision. in this way the apparent rate of the action anonymize, for example, will be given by the product of s multiplied by the minimum of the number of service1 and private components currently enabled. that is, an individual instance of the anonymize action will never be be served at a rate greater than s, regardless of how many instances there are or how many servers are available. this condition might not always be preserved if we were to use passive actions in our model.



the first observation to make from this set of experiments is that option 1 and option 3 perform almost identically in each case. this is not surprising given that these options differ only in whether the relatively fast writeresults action is performed in the public or private cloud. a further observation is that option 2 has a consistently lower throughput than either of the other options. this is unsurprising given that the relatively slow analyze action is being performed in the(non-scaled) private cloud in option 2, whereas it is performed in the(scalable) public cloud in options 1 and 3. in each case there is no increase in throughput beyond 20 servers, because there are only 20 workflow instances. in some cases the maximum throughput is reached with fewer servers as the low capacity of the private cloud means that actions performed there become a bottleneck in the system.



in this paper we have presented some initial work in modelling workflow deployment using pepa. the aim of this work is to explore the costs associated with different security decisions. this paper has been motivated through an example of a healthcare application. we have shown that a simple form of model with standard analysis tools can provide insight into the behaviour of the system in question.



the model we have developed to study this application also has some limitations. the most significant of these is that we have not modelled any data transfer costs. clearly it would be a simple matter to add some network delays between actions being undertaken in different locations. this would enable a clearer comparison between the performance of different deployment options in public and private clouds where different transfer costs will be evident.



options there is wide range of performance that might be experienced. predicting exactly what a provider will offer on any specification is not generally possible in the current evolution of cloud systems. however it would be relatively straight forward to give different performance characteristics for different systems(at least public and private) and compute the average performance at different service levels.



clearly to have practical value approaches such as the one described here need to be validated against real implementations. performing such experiments in a rigorous way is a difficult and time-consuming business. our aim therefore is to further develop the modelling approach to show the potential of this line of investigation before undertaking work to validate the results.



