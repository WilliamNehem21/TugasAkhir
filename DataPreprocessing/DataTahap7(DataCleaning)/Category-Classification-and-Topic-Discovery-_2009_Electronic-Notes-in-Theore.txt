we define topic analysis for news as identifying not only the topic, but also the category of a news article. for news, categories are high level groupings that allow for easier navigation of articles. newspapers and internet news sites are broken down by category. for example, a newspaper will have a sports page, business page, etc. we define topics to be the main themes of news articles. topics are also a part of newspapers and internet news sites.



this paper will continue as follows. first, in section 2 background information and related work will be examined. then, in section 3 the algorithm for category classification will be given. in section 4 the algorithm for topic discovery and topic classification will be shown. next, in section 5 experimental results are given. finally, in section 6 concluding remarks are made and future work discussed.



in recent years research has been done on automatically discovering topics and groups in an existing document collection. wang et al introduced a method for discovering groups and topics from the relations in text. their group-topic model was designed to aid social network analysis. presents work on an unsupervised topic discovery using a document clustering technique.





news topics, unlike categories, are created daily as news happens. therefore, it is more difficult if not impossible to assign an initial set of topics that can cover all articles in the foreseeable future. this means that not only is topic classification needed, but also topic discovery(also called new topic detection or new topic creation).



like the category classification algorithm, the topic classification algorithm is also based on the keyword extraction algorithm described in. it calculates a similarity between each known topic and the given article using the keywords. then it assigns the most similar article as the conditionally assigned topic.



if both of the thresholds are met then the conditionally classified topic becomes officially assigned to the article. otherwise, a new topic is created and the article is the first source of training data. training is done in the same way as category classification.



the algorithm is very simple, but meets the strict needs of topic classification for news articles. the newtsim and other thresholds were determined through extensive experimentation. the results shown in the next section come from documents that were not used to determine the thresholds.



the results show high recall and precision for both japanese and english. the japanese results were slightly better than those of english. this could be the result of the keyword extraction algorithm working more effectively on japanese. while not directly comparable, the results are similar to those of other researchers, such as, were able to achieve with support vector machines on other corpora. this algorithm, though, has the advantage of being able to be easily updated.



a number of tests were performed for topic discovery and classification. first, since the topic classifier must work well even with sparse training data we compared it to other classifiers when trained using sparse data. second, we performed tests on two different english corpora(reuters and one created by us using various online news sources). finally, we experimented with japanese.



the first experiment was training with sparse data in an offline environment. as new topics are found, in the online environment, the initial training samples are small. even with sparse training data the classifier must be able to accurately determine the topic of the news article. for comparison purposes, a naive bayesian classifier(nbc), decision tree(dt) classifier, and maximum entropy(me) classifier were used.



each of the standard classification algorithms used all the keywords extracted from the training articles as features. the feature vector was made up of the keyword scores. for a fair comparison the proposed algorithm did not use online learning to improve its results.



state-of-the-art classifiers, like maximum entropy, are not capable of accurately classifying when there is only sparse training data. this is seen in the results. the naive bayesian and decision tree classifiers are able to perform much better. the proposed method does achieve better results for the most part. plus, there is no obvious way of doing online training for the naive bayesian and decision tree classifiers.



the first english test used a 1,000 article subset of the reuters corpus. this subset was made up of 11 topics. starting with no known topics the news articles were fed into the system in random order. for evaluation, we used four measures: recall, precision, f-measure, and fragmentation factor. the recall, precision, and f-measure are used to evaluate the ability to classify. in this case we were only interested in how well the articles grouped together as topics were really in topic. because of this we combined the false alarms or created topics that only contain



the second test used 500 randomly extracted articles from various online english news sites, including yahoo! news, the washington post, bbc and cnn. while the sites are predominantly from the u.s., we do not think this would make much a difference. this test shows results for articles that are more likely to be encountered in a real world system. the article set had topics manually assigned and resulted in 13 different topics. the experimentation was started with no known topics.



the final test used 1,000 randomly extracted articles from various online japanese news sites, including mainichi shimbun, asahi shimbum and yomiuri shimbun. the article set had topics manually assigned and resulted in 10 different topics. the experimentation was started with no known topics.



measure for japanese were all very high. the fragmentation factor, though, was also high at 11.3. the results were a little worse than those of the english non-reuters test. this is possibly due to the japanese use of chinese characters(kanji). these characters help to disambiguate words, but can also make naive word matching difficult.



