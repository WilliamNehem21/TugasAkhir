owing to the hard obtain of supervised data, numerous active learning algorithms have referred to select the most meaningful data from a collection of unlabeled data instances. active learning algorithm typically selects an instance at one time to query an oracle, and then repeats this procedure until the accuracy of the algorithm or the labeling budget is achieved. the strategy of selecting instances is crucial for algorithms with limited budgets. a good query strategy exposes the most common problem encountered when selecting a batch of instances is that the selected instance is informative but homogeneous. since the detail provided by similar instances is almost identical to the learning model, the oracle provided by expert is wasted. hence diversity is the key to active learning in addition to informative batch processing mode.



however, the existing bmal algorithms are mainly limited to two problems: first, the measurement of similarity between instances has a significant impact on the performance of the bmal algorithm. for example, svmactive maps instances to the kernel space, and then simply uses eigenvectors to calculate the similarity between instances. this is less diverse than the algorithm that learns with the classification model.



our research aims to design an effective batch mode active learning algorithm, aiming to enhance the performance of classifiers in situations where the amount of labeled data is limited and the cost of labels is high. the problem we need to address is how to select a fixed number of instances to maximize overall information while minimizing redundancy proactively. we need to overcome the two key challenges mentioned above. firstly, we need to find a better way to measure similarity. secondly, based on this measure of similarity, we need to develop a strategy that can select instances that are both informative and diverse. we propose an adaptive bmal method that uses deep neural networks to learn similarity and balance exploration and exploitation. exploration strategy represents gaining more information, while exploitation strategy means reducing redundancy.



in our method, convolutional neural network is used to classify the instance. the outcome of the last but one layer of the network can be considered as the learned instance feature representation. the inner product of feature vectors can be used to obtain the similarity between instances. our neural network training process has two objectives: 1) improve the accuracy of training models, 2) improve the accuracy of similarity measurement between instances by mapping instances to a new space.



as far as we know, contributions 2 and 3 have not been explored in previous articles. it is the first attempt in bmal. differences between this article and previous conference versions. based on the conference version, this paper does the following work which is not in the conference version: multi-class active learning method is proposed by yan et al. to automatically label video data. this approach evaluates several practical sample selection strategies and advances active learning to handle multi-class problems. chattopadhyay et al. present a batch processing strategy that chooses a group of instances based on the distribution of unlabeled data. there are also some studies combine uncertainty sampling andrepresentative sampling[6,10]. the algorithm discussed above focuses on selecting the instance with the largest amount of information at each query. however, this is an inefficient way to tag instances one by one.



in batch active learning, it is important to decrease redundancy when selecting instances. hoi picks a set of least redundant instances with minimal fisher information. joshi proposed an active learning framework based on multi class image classification system. schohn space. xia uses a clustering algorithm to cluster unlabeled instances, and then selects instances from each cluster. guo et al. proposed an active learning algorithm for discriminating batch patterns, which defined the case selection task as a continuous optimization problem. zhu et al. in order to minimize the risk of harmonic energy minimization function, gauss random field model is used for active learning and greedy query from unlabeled data. shi presents maximum impact, minimum redundancy and maximum uncertainty criteria to measure the informativeness of a collection of data. he transformed the bmal problem into maximizing the objective function by selecting a collection of informative and diverse instances. dasarathy proposes a binary label prediction algorithm called s2 for active learning on a grap.



proposed an active learning strategy based on bayesian form, which minimized the hypothesis of data and affected the exploration and development of input data space. osugi introduced an active learning algorithm to balance exploration and refine decision boundaries by dynamically tuning the exploration chance of every iteration. cebron et al. propose a new prototype based active learning(pbac) approach with self-controlled exploration/exploitation strategy



and question classification. as the neural network becomes more and more complex, more training data are needed. but at the same time, a lot of labeled data are difficult to obtain. we aim to use active learning algorithm to reduce the workload of manual annotation and ensure the accuracy of the network.



deep neural networks have been successful in many areas, including object recognition, text classification, image classification and speech recognition etc. many networks have been explored in the field of image recognition. simonyan et al. studied the influence of the depth of convolution network on the accuracy of large-scale image recognition settings. the work of proposes a residual learning framework to simplify the training of networks deeper than those previously used. deep neural network has also achieved excellent performance except for image recognition. target detection has been a big driver in the work of[28 29]. zhu et al. proposed a novel model, which can be used to assess patient similarity using deep convolution neural network. kim



the purpose of active learning is to boost the classifier effecting by marking as few instances as possible. the key to the problem is how to choose a batch of b instances to maximize classifier performance. the selected set of instances should be both diverse and informative. each time the instance is selected and labeled, the classifier will retrain with the new labeled data instances set l. the basic model is described in algorithm 1.



the method is to divide the budget into multiple batches and retrain the learners for each batch. there are two problems in this process that need to be solved. the first is how to select the set of instances with the most information, and the second is how to use labeled data when retraining learners. due to the training of early labeled data in each iteration, the deep neural network classifier used in the experiment is prone to overfitting in small-scale labeled datasets. these will cause problems during training as illustrated in the following sections.



located at the decision boundary. the probability that x belongs to the ith class can be denoted as hi(x). e(s) represents the sum of uncertainties for all data points in set s, which is the entropy of set s.



selected instances. so the criteria for selecting redundancy of set s needs to be updated. we need to measure the similarity between instances in an appropriate way to select the different types of instances. similarity is usually measured in feature space. kernel space is often used in svm. the feature space used by the algorithm is the result of the penultimate layer of



we use adaptive strategies to combine exploitation and exploration in bmal. we divide each iteration of algorithm 2 into three parts: adaptive phase, exploration phase, and exploitation phase. at the beginning, we assume that there are very few labeled instances and they cannot represent the true distribution of all instances. thus, the algorithm should focus more on exploring to discover new areas better. the exploration and exploitation strategies are executed respectively at each iteration. we denote m and k-m as the numbers of instances to be selected in exploration and exploitation respectfully, which control the



as the labeled instance set becomes larger, the possibility of redundancy between the selected instance and the labeled instance set increases. if the value of m is fixed, it is difficult to balance exploration and exploitation throughout the entire process of the algorithm. we expect to initialize a relatively large m value and gradually optimize it during the iteration process based on the performance of the exploration and exploitation strategies until the end of the iteration.



k) to verify the performance of current learners in exploration and exploration. this means that we select a instances to test learners by exploration strategy. we can obtain the test result mp, which is matching probability. based on the test result mp, we adjust the value of m adaptively.



for convenience, bmal, em, kff, random, ee are used to represent the baseline mentioned above respectively, and aeep(adaptive exploration-exploitation-pairwise model) to represent our method. these five baseline methods can represent the current approach to solving such problems in a comprehensive way. kff is focused on exploration, so early on they performed much better than others. bmal is more development-oriented, so it stands out in later stages. entropy maximization method is also a very classic method to deal with problems. it sets an entropy value for each point to be amplified, and recalculates the entropy after output a point, and outputs the point with the maximum entropy. besides, random selection is also chosen as a baseline method.



the same length, and the filled content is meaningless in medical terms. we use ehr data to train a word embedding model, which can convert medical events into fixed length vectors. each patient can be represented as a fixed size matrix.



we evaluate our framework on mnist dataset. the classic mnist dataset contains a large number of handwritten digital images. for more than a decade, researchers from the fields of machine learning, machine vision, artificial intelligence, and deep learning have used this data set as a benchmark against which to measure algorithms.



we use a classic seven layer neural network to classify mnist images. the neural network consists of three convolutional layers, two pooling layers, and two fully connected layers. the output of the network is a 10 dimensional vector, corresponding to 10 categories. the entire network uses relu as the activation function. our active learning algorithm is applied in the model.



before. this is because with the increase of batch size, more instances can be labeled at the beginning, and the effect of network training will be enhanced. when these active methods run to the later stage, the amount of labeled instances is already very large. at this time, if you still



in this paper, we address two common issues with batch mode active learning. we propose an adaptive batch mode active learning algorithm with deep similarity and successfully integrate it with deep neural networks. during training, we use pairwise deep neural network to extract the features of the instances and get the similarity between the instances. this enhances the diversity of selected data in active learning. in the active learning stage, we use an adaptive strategy to dynamically adjust the balance between maximum uncertainty(exploration) and diversity(exploitation) in order to select informative and diverse instances.



