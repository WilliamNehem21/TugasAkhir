hyperspectral measures are alternative approach to derive discriminatory information regarding spectral signatures. these measures are simple and computationally light. they are able to capture the degree of similarity between two spectrums. for example, in spectral angle mapper(sam), the similarity value ranges from 0(highly similar) to 1(highly dissimilar). the similarity value should, for instance, be less than



the proposed approach aims to develop learnable hyperspectral measures as replacement for static threshold hyperspectral measures. this is done through using hyperspectral measures values as similarity patterns and employing a classifier. the classifier acts as an adaptive similarity threshold. the derived similarity patterns are flexible as they are able to capture the specific notion of similarity that is appropriate for each spectral region. two similarity patterns are proposed. the first pattern is the cosine similarity vector for the second spectral derivative pair. the second pattern is a composite vector of different similarity measures values.



version 1.1 calculates the cosine similarity vectors for the second order derivatives of the spectral signature pairs. the vectors form similar and dissimilar patterns. the resulting patterns are classified by svm that acts as an adaptive similarity threshold. version 1.1 is applied on the full hyperspectral space of the spectral signature. in this section, we describe the steps of version 1.1.



c class mean vector. the result is five cosine similarity subspace vectors forming one combined similar pattern. for each sample k not in class c, the cosine similarity is calculated between the five subspaces of k and the corresponding five subspaces in c class mean vector. the result is five cosine tral measures to form similar and dissimilar patterns. the resulting patterns are classified by svm that acts as an adaptive similarity threshold. version 2.1 is applied on the full hyperspectral space of the spectral signature. combining similarity values means consolidating the different statistics derived by similarity measures. the resulting composite vector of similarity values is used to discriminate each spectrum pair. in this section we describe the steps of version 2.1.



the second version calculates different similarity values using nine similarity measures to form similar and dissimilar patterns. svm classifies the resulting patterns to act as adaptive similarity threshold. each version has been implemented twice using full hyperspectral space and subspaces. mathworks matlab version r2009b has been used for implementing the hyperspectral measures. libsvm, a support vector machines tool, has been used to handle the multi-class svm types. the used svm parameters have been derived from a research



according to analysis conducted by wu and chang in on the test dataset, the spectral signatures of classes(2, 3, 4, 7, 10 and 12) are so close to each other, and the same condition for classes(1, 8, and 11). for classes(5, 14, and 15), they have less similar signatures. for classes(6, 13 and 16), their signatures are dissimilar. classes 5 and 11 are highly mixed. signal-tonoise ratio(snr) at the time of data acquisition was lower than current aviris standards. this means the noise level is high.



ova separated each class from the rest classes, and developed a classification model. such procedure was not appropriate for highly mixed classes. many of the separated classes contained spectral signatures that were close to spectral signatures of other classes. therefore, svm failed to discriminate the similarity patterns efficiently. the training complexity was high as each ova classifier was trained using all available samples. as a result, the performance of ova was poor.



ovo was much better than ova as each ovo classifier was trained using samples of two classes only. the low number of samples causes smaller nonlinearity, shorter training times and significant information discrimination. as a result, ovo achieved better results than ova.



the previous experiments have been applied on nine classes out of 17. this is because pca approach avoids classifying the remaining eight classes as the samples of these classes were relatively small. by applying nca on the neglected classes(1, 4, 7, 9, 13, 15, 16 and 17) using distlearnkit,1 the classification accuracy for each neglected class was calculated. the following hypotheses have been set for right-tail z-test: h0(p1 6 p) and h1(p1> p). p and p1 are the average classification accuracies for the 17 class samples achieved by nca approach and version 2.2 of the proposed approach respectively. h0 is accepted when the calculated z(zc) 6 the tabular z(zt). h1 is accepted when the calculated z(zc)> the tabular z(zt). the calculated z(zc) is defined as: subspaces. the calculated cosine weights for spectral features kept its power as they were normalized across small spectral regions. both versions 1.1 and 1.2 were concerned only with the geometry of the spectral signatures. they did not capture any other discriminatory information such as: orthogonal projections information, correlation coefficients, and probability distributions produced by the spectral signatures. versions 2.1 and 2.2 have combined all of these characteristics.



using hyperspectral measures values as similarity patterns and employing a classifier. the classifier acts as an adaptive similarity threshold. two similarity patterns are proposed. the first pattern is the cosine similarity vector for the second spectral derivative pair. the second pattern is a composite vector of different similarity measures values. the resulting patterns are classified by svm. the proposed approach is applied on full hyperspectral space and sub-spaces.



the experiments have been applied on one of the most challenging hyperspectral datasets. this is done to test the robustness of the proposed approach compared to the best competitive approaches applied on the same dataset. the experimental evaluation showed that the proposed approach outperformed pca and nca approaches. by conducting a right-tail z-test to compare the significance of version 2.2 of the proposed approach to the best competitive approach(nca approach), the calculated z value was 1.7047 and the one-tailed p-value was 0.0441. this means the two classification accuracies were significantly different.



pca performance was poor. this is because pca kept high variance bands and ignored low order bands containing discriminatory information. in addition, pca failed to classify small size classes of the test dataset. unlike pca which is not directly related with the final classification performance, nca was designed to directly optimize the expected leave-one-out(loo) classification error on the training data. therefore, nca performance was far better than pca. nca developed a learnable distance metric by finding a linear transformation of input data to enable knn to perform well in this transformed space. although nca achieved good results, it is computationally expensive.



the training time of pca and nca was so high compared to the proposed approach versions. therefore, the larger the number of training samples, the longer the time needed to build classification models for both pca and nca. the results imply that using simple learnable hyperspectral measures overcome complex or manually tuned techniques used in classification tasks.



