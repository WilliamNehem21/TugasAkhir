the major challenge facing computer science at this juncture is to protect and secure data from risks, as well as to protect it(and its genuine users) from malicious attacks. in-depth defense, such as multifactor authentication and the building of secure systems, does achieve protection, but it cannot detect whether a company is experiencing a zero-day attack or not. furthermore, as well as avoiding attacks, systems must protect and gather data as evidence, as well as generally employing existing solutions such as firewalls, encryption, and intrusion detection systems. however, there are two essential problems with the existing solutions:



jalil et al. compared the efficiency of different ai approaches(such as neural networks(nns), svm, and dt-j48 algorithms) based on different metrics(detection rate, false alarm rate and accuracy) using a well-known dataset called a kdd-cup dataset. the experiments concluded that the dt-j48 algorithm outperformed the neural network and support vector machine(svm) algorithms.



hamid et.al examined several approaches(rule-based, base rule, functions, lazy learners, tree, meta-algorithm, and input mapped classifier) using weka tools and the kdd cup 1999 dataset. the comparative analysis was based on several metrics(accuracy, recall, precision, f-measure, tp rate, tn rate, roc area, kappa statistic, and mean absolute error). the authors used this approach both fully and with the reduction of features, proving that classification methods do not need all features, but the main limitation in this research is the limited amount of data.



haripriya and jabbar examined various machine learning algorithms(supervised and unsupervised, reinforcement learning, etc.). the authors used method, dataset, and metrics to compare between ml approaches, but they did not study other approaches, such as fuzzy logic, ant-colony, and deep learning.



behavior of other attacks. the second limitation was that it did not mention the number of features that were input into the model, nor did it determine the continuous or categorical data. furthermore, the optimization phase did not give details on parameters such as the optimization function, and the preprocessing was performed via black box. furthermore, the authors did not say whether the data were normalized or not, how they dealt with missing and categorical values, or anything about whether the data were balanced or not. additionally, the model suffers from overfitting.



this paper presents a hybrid method for evaluating network behavior throughout the intrusion detection process, combining supervised techniques(dnn), unfiltered measurements(pca), and statistics measures to increase the accuracy of detection of stealth attacks. moreover, the canadian institute of cyber security(cic) provided the cse-cicids2018 dataset, which has only been used by a few researchers as of the time of writing. this dataset is used at the flow level, and statistical measurements of the flow level features are collected. the proposed method offers a basic solution to the problem of volumetric attack mitigation in the context of attacks such as ddos, http, and floods.



although various standard datasets are widely available, many of them include offensive, non-modifiable, outdated, and nonreproducible attack scenarios. in an effort to solve these shortcomings and create more modern traffic patterns, the cse-ci-unb dataset was developed by. the correctly marked cse-ci-ids 2018 dataset exhibits real network behavior and includes a variety of intrusion scenarios. in addition, it is distributed as a complete network set with all internal paths to evaluate the loads for prepacket inspection.



the adapted dataset includes ten days of normal and malicious network activity. the dataset was produced from two files containing abstract images of events and activities in the network. for example, communication between the source and the receiver host can be visualized over http through packets sent or received and endpoint properties. this imaging creates one profile. these profiles can provide real data for protocols such as ftp, pop3, imap, smtp, ssh, and http. agents or individual users can use them to inject different situations into the network. in order to revitalize network activity for premium applications by changing these profiles, personal files are also exchanged with real school datasets.



in this attack, a large number of infected devices called zombies are connected to each other under a cc empire. typically, these hosts are infected by malware. when the cc launches an order, the whole army participates in the attack; usually, these bots are used in ddos and cryptocurrency mining and distributed processing.



the network testbed architecture comprises 500 machines divided into five separate lans for building a realistic network topology, namely: department of r&d, department of management department of technicians, secretary and department of operations, department of it, and database spaces. in this dataset, the authors deployed lists of different microsoft operating systems, such as windows 8.1 and 10, for all organizations except the it department; all devices in the it department are linux os. various ms windows servers, such as 2012 and 2016, were implemented for server farms. despite the various hacking scenarios in the real-life network, there are some limitations to this dataset. for example, a large portion of the ip packets generated by data management and network monitoring operations are un-named and anonymous. additionally, when flow records are retrieved from the dataset, some streams are found to contain infinity and nan values.



this section describes the methodology of the proposed model architecture. firstly, a summary of the workflow of the system is presented, with a brief description of the architectural models(i.e., pca and dnn) that inspired our multi-layer architecture. the proposed architecture and its implementation are subsequently described. as previously stated, this research proposes an anomaly detection system that analyzes flow-level traffic and captures attacks. the proposed system combined four well-known techniques:



network traffic information is often collected in the form of initial packet capture(pcap) in network switches or routers[5,35]. pcap information includes full tcp/ip packet data sent or received on a specific network computer. while in some cases full packet capture information may be useful, this has a high storage cost.



the main difference between the cic ids 2018 dataset and the iscx ids 2012 and cic ids 2017 datasets is the number of classes and network flow features available compared to the iscx ids 2012 and cic ids 2017 datasets. the iscx ids 2012 dataset has 14 total features, while the datasets labeled cic ids 2017 and cse cic ids 2018 have 81 available features each. these additional features consist primarily of comprehensive flow statistics and packet information calculations for each flow created using the cicflowmeter open source tool.



once the data are in the form of flow data, the further cleaning, conversion, and preparation of the data for use as input into the deep learning algorithm are performed. this step includes tasks such as making sure that the data do not contain invalid characters, removing fields with empty values, removing or changing values(not the number), and removing duplicate columns. the main reason for preprocessing is that the data are in different formats and are collected from different locations; it also ensures the accuracy and effectiveness of the model being trained on this data.



dnns are amongst the most commonly used machine learning approaches. they can reverse increasingly complex functions by merging more layers and more units per layer into a neural network. a dnn can be used to identify normal and malicious traffic patterns hidden within large quantities of structured data.



the well-known technique of pca is a mathematical procedure that uses orthogonal transformation to convert a set of observations of possible associated variables into a set of non-linearly related variable values, called principal components. pca is applied primarily in dimensionality reduction. it is a non-supervised projection method that can translate from a complex larger area to a smaller new area with minimal information loss. if the origi-



in this section, a dnn technique with and without dimensional reduction(pca) applied to the csecicids 2018 dataset will be briefly summarized. the experimentations were conducted on a 64-bit windows 10 pc with 16 gb ram and 2.60 ghz cpu, and deep learning was performed using python 3.7.3 and numpy 1.16.2.



sumption was calculated in this experiment, as this is a fundamentally important factor. this involves checking how fast the different codes and algorithms are running using a timestamp provided by the training computer. it was expected that binary results would take less time to generate than multiclass results because of the lower number of classes available for identifying the samples.



which achieved 98% as well. therefore, training a dnn by omitting features allows it to identify the source, destination, traffic type, etc., and give it statistical and network features. it will save costs in the training phase; hence it may be usable in a pre-trained model, of a kind not seen before.



