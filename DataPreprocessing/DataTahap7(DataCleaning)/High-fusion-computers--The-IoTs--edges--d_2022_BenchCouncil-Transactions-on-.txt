we take hfc as a pivotal example of the open-source computer system plan. we abstract reusable functions(funclets) across system stacks among iots, edges, and data centers. based on funclets, we rebuild the iots, edges, data centers, and humans-in-the-loop as a computer in a structural manner, with full-fledged functions of autonomic resource discovery, management, programming, workload scheduling, and coordinated collaboration between software, hardware and human components. our plans are three-fold. first, we value the importance of benchmarks and funclet-based standards in evaluating and building the systems. second, we emphasize the methodology and tool to facilitate the workload-driven exploration of the system and architecture design space. third, we will provide the first open-source implementation of the funclet architecture of hfc systems.



we organize the rest of this paper as follows. section 2 explains the motivation. section 3 illustrates the hfc challenges. section 4 explores the hfc software and hardware design space. section 5 describes our plan. section 6 summarizes the related work. section 7 concludes.



the medical system can reasonably allocate medical resources for rapid rescue. it is worth noticing that medical experts play a decisive role in the system. medical emergency management systems consider medical professionals a reliable external component in the control loop, which we call reliable-human-in-the-loop. in this scenario, the system may make recommendations, but the medical expert takes the responsibility, and the decision made by the system is reversible.



the computation patterns follow the observe, fuse, recommend, and train patterns. the iot devices observe the data of the patients at different levels. the system may fuse various observations at the edge or data center. the data centers or edges will train and update an ai model through the widely collected and labeled data. the iot or edge makes a recommendation like alert and further-taken actions. the medical experts make a final decision.



task types: highly-automated and mainly safety-critical. the future autonomous driving would be highly-automated, even fully-automated, and consider no human in the control loop. the corresponding system needs to perceive and collect multi-source and multi-dimensional data in real-time and respond within several milliseconds. considering the properties of high autonomy, hard real-time, and potentially destructive effects, the decision and action made by the system are irreversible. it will be a system failure in hard real-time when missing a deadline. in auto-driving, missing a deadline will be catastrophic.



various iot devices generate a massive volume of heterogeneous data. autonomous driving depends on a large number of sensors; even a single car may deploy multiple kinds of sensors[28,29] including cameras, ultrasonic radar, millimeter-wave radar, lidar, imu(inertial measurement unit), etc., to obtain the environment information comprehensively. the input data are multi-source and heterogeneous; thus, the system should be able to fuse multi-sensor data for quick processing.



organizability and manageability challenges. unlike a traditional computer system, e.g., a supercomputer or warehouse-scale computer, an hfc system is geographically distributed, consisting of iots, edges, data centers, and humans-in-the-loop. moreover, they are dynamic. for example, in smart defense systems and applications, sensors or terminal control units can be dispersed by airdrop, inserted by artillery, and/or individually placed by an operation team. in an extreme case, the spatial realm even has no bound. for example, unmanned spacecraft may have no bounded destination in interplanetary exploration applications.



the effective evaluation challenges. generally, we need to deploy a system in a real-world environment and run a real-world application scenario to evaluate the performance and provide optimization guidelines. however, the real-world environment and emerging/future application scenarios are inaccessible and costly in assessing and verifying an hfc system. hence, benchmarks as proxies of emerging/future



simulation and validation challenges. at the early stage of system and architecture evaluation, the simulator plays a vital role due to the vast manufacturing investment of time and money and the immaturity of the corresponding ecosystem. for example, the effectiveness of the improved processor design, memory access technologies, etc., is evaluated on a simulator. considering the cost of building a real-world hfc system, a simulation or validation system supporting the whole environment simulation and technology verification is significant. however, the complexity and diversity of application scenarios pose substantial challenges in building such a simulator.



second, simulation accuracy is a crucial metric. high accuracy means the simulator can reflect similar running characteristics to the real world and exhibit running differences under different system environments. considering the difficulties of multiple-level or multiple-scale simulation, including hardware level, e.g., processor chip, cache, memory hierarchy, disk, and software level, e.g., operation system, ensuring the simulation accuracy is necessary but challenging.



cpu benchmarks. we present cpu benchmarks covering typical workloads from emerging and future application scenarios from the low-level architecture benchmarking perspective. constructing cpu benchmarks adopts a traceable methodology, managing the traceable processes from problem definition, problem instantiation, solution instantiation, and measurement.



for the second step, we attempt to define the ideal chiplet architecture for different iot, edge, data center layers and different analyzed patterns. specifically, according to the classifications in step 1, we define the computation, memory, networking chiplets for iot, edge, and data center, respectively. each layer of iot, edge, or data center will contain multiple chiplets for different patterns of computation, memory access, networking, etc. additionally, each pattern may contain multiple chiplet designs according to the classifications of workload characteristics.



for the third step, we validate the chiplet architecture design and further performs improvements according to the feedback. we adopt fpga-based simulation and evaluate the scenario, iot, and cpu benchmarks to conduct the functionality and performance validation. further, we explore the upgrades and design optimizations based on the validation results.



processor). we will implement the datapath processor as an accelerator. however, the network packet will be the first-class citizen in the datapath processor. the packet stream leaves the register file of the primary cpu and arrives at the datapath processor directly without going through a complex memory hierarchy. the datapath processor has full functionality, including access to the cache and main memory. thus, the datapath processor can hold and process complex state information necessary for the data plane. the datapath processor also has accelerating units on the local bus; data exchange between the datapath processor core and accelerator can be low latency, highly paralleled, and fine-grain. we have not seen such the structure of the datapath processor before, but fortunately, open-source processors, like risc-v based, allow us to design a novel processor architecture freely.



the design of a dss needs to consider the characteristics of storage devices. we conclude two development trends of storage devices. on the one hand, the devices will be increasingly faster with microsecondscale or even lower latency. storage devices have experienced several technological breakthroughs in the past twenty years, such as the development of commercial ssd products, nvm-based ssd products(intel optane ssd), and persistent memory products(intel optane pm). compared to hdd and ordinary ssd, nvm-base devices have much lower latency. in addition, emerging fast networks(e.g., 200 gbps and 400 gbps infiniband) have round-trip latency of less than



the optimization object is uncertain. finding the performance bottleneck in an hfc system is non-trivial. users may feel confused about the optimization objects because of the optimization possibilities on iots, edges, or data centers and the complex hierarchies of algorithms, frameworks, software, and hardware.



automatically co-optimization is non-trivial because of the vast optimization space. there are thousands of optimization dimensions of the algorithm, software, and hardware, and the values of the variate vary in an extensive range. as a result, the optimization space is too huge to complete the search. reinforcement learning has shown powerful capabilities for the problem of searching for optimal policies in a vast space. evaluating is expensive in co-optimizing the algorithm, software, and hardware across the iots, edges, and data center. we will investigate the state-of-the-art learning algorithms and evaluation strategies and develop the corresponding tools for automatic optimization.



we abstract reusable functions(funclets) across system stacks among iots, edges, and data centers to guide the hfc system design and evaluation. we first propose to define a series of benchmarks and funclet-based standards and then build the tools to facilitate the workload-driven exploration of the system and architecture design space. finally, we provide open-source implementations of an hfc system. we will perform system co-design from the vertical and horizontal dimensions throughout the process. vertically, we comprehensively explore the algorithms, runtime systems, resource management, storage, memory, networking, and chip technologies. horizontally, we deeply discover the collaboration and interaction among iot, edges, data centers, and humans-in-the-loop.





