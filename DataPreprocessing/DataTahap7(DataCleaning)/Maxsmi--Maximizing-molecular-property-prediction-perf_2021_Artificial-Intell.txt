drug design is a time-consuming and costly process[1,2] with high attrition rates. it can be supported with in silico methods by guiding the design process, optimizing compounds, and discarding those with undesired properties at an early stage of development. in this context, computer-aided drug design(cadd) has become central in the drug discovery pipeline and is widely adopted in research and development in both academia and pharmaceutical companies.



encoding molecular compounds in both humanand computerreadable formats is a necessary step in cadd. a convenient encoding is smiles, or simplified molecular-input line-entry system. as the name suggests, smiles is a linear notation of a molecule based on atom and bond enumeration, as well as branch, ring closure, and disconnection specification. several advantages arise from this compact representation.



the implementation of a smiles given a compound can be described as follows: from any starting atom in the molecule, enumerate the atoms and bonds following a path in the molecular graph. two aspects of this construction lead to the non-uniqueness of smiles: 1. the atom to start the enumeration from, and 2. the path to follow along the graph.



therefore, one molecule can have many different valid smiles, simply by starting the enumeration from a different atom or by choosing a different path. nevertheless, in some settings, having a bijection between a molecule and its smiles notation may be sought. for example, when determining the overlapping molecules from two data sets. in this context, most cheminformatics tools have their own algorithm implemented allowing them to always retrieve the same smiles given a molecular graph, such a smiles is called canonical.



0.12 increase with respect to the canonical model. from then on, several studies have built on the same idea, applying smiles augmentation in qsar modeling. moreover, convolutional neural networks have successfully been applied in the context of smiles augmentation, outperforming models using traditional molecular descriptors[22,23]. such augmentation techniques have also emerged in related fields, such as retrosynthesis[25,26] and generative modeling[27,28]. while all these studies show the benefit of augmenting the data, none of them focus, to the best of our knowledge, on a systematic analysis on how to augment the data set best, and most decide a priori on an augmentation number. this study aims at filling this gap by offering a systematic augmentation approach, both in the augmentation strategies and by how much the data should be augmented. moreover, a command-line interface is available for users interested in the predic-



in this section, we first describe different augmentation strategies which can be used for data augmentation when dealing with smiles. second, we illustrate how smiles augmentation can be viewed as an ensemble learning technique when it comes to prediction. we then examine the deep learning models that are trained in this study.



ing to the augmentation number, i.e. the number of times a sample is drawn from the valid smiles space, and the size of the molecules in the data set. a disadvantage of such an augmentation strategy is that small molecules, which presumably possess fewer unique smiles representatives, will be under-represented in the data set and could create a bias in model training.



the final strategy described is augmentation with estimated maximum, which aims to cover a wide range of the valid smiles space for a given compound, or in other words, to generate a number of unique smiles that depends on the compound. in our study, the implementation of this augmentation strategy randomly samples smiles corresponding to a compound, and the sampling process is stopped once the same smiles string has been generated a pre-defined number of times. the experiments of this study set 10 generations of the same smiles as a stopping criterion. it is noteworthy that the number of smiles this method generates is highly dependent on the



in this study, comparing deep learning models and how they perform with respect to data augmentation is one of the key focuses. to this end, three types of models are architectured and trained, namely 1d and 2d convolutional neural networks(conv1d, conv2d) as well as a recurrent neural network(rnn). the architecture of the recurrent network consists of an lstm layer, followed by two fully connected layers of 128 and 64 units, respectively. it was inspired by bjerrum, in which an lstm layer is followed by a single 64 unit fully connected layer. using a similar approach, a single 1d convolutional layer of kernel size 10 and stride 1 is applied in the conv1d model. two fully connected layers follow the convolution. the conv2d adheres to the same pattern but instead of using a 1d convolution, a 2d convolution operation is performed using one single channel. finally, all three architectured models stay consistent in the depth of the network and remain shallow.



in this study, all deep learning models are trained for 250 epochs, using mini-batches of size 16, where the mean squared error is the considered loss. optimization is done with stochastic gradient descent and a learning rate of 0.001. note that a fixed number of epochs is used in this study, but for sake of completeness, three sample models with early stopping were also run. the results with and without early stopping did not change significantly(data not shown). moreover, some models were trained by adapting the number of epochs with respect to the augmentation number, but this only proved to overfit the training set and yielded the same results on the test set as training with 250 epochs(data not shown).



measured water solubility is referred to as the esol data set. the raw data contains 1 128 data points. this data set is further processed to only include molecules with at most 25 heavy atoms for experimental setup and is referred to as esol_small.





ensemble learning is applied on each test set and the mean is used as aggregation. however, a user could easily adapt it to another function, such as the median. the standard deviation is stored for each compound in the test set.



moreover, a random forest(rf) model is used as a baseline, with all default parameters from scikit-learn. the inputs to the model are the morgan fingerprints of radius 2 and length 1024. augmentation strategies as discussed above are not applicable in the context of fingerprints.





and their ability to extract relevant features in compounds based on onehot encoded smiles input. this also implies that although applying 2d convolutions to smiles is programmatically feasible, 1d convolutions are better suited than 2d convolutions, the latter having shown great success in image classification. indeed, when considering the one-hot encoded matrix, smiles are more similar to words, in which the position of the atoms is important, rather than to images.



using the maxsmi models established above, we look into more details at the information gained from ensemble learning for molecular prediction, and more specifically at the average and standard deviation computed from the per smiles prediction. feeding different smiles representations to the model and aggregating the prediction for each smiles variation to obtain a single prediction per compound is valuable not only from a practical point of view where molecular prediction is more informative than a smiles prediction, but it also allows the model to merge information coming from different perspectives of the same compound. moreover, the standard deviation associated with the smiles predictions allows to quantify the uncertainty of the prediction of the model toward a given compound. the higher the standard deviation for a molecule, the less concurrent are the predictions by the model, and thus, less confident.



as an outlook, we observe that strategies that keep all, or a fraction of duplicates, may help the model to learn inherent symmetry in a compound. indeed the same random smiles representation will certainly be generated multiple times for a symmetric molecule even though the initial atom and the path along the graph are different. in this sense, smiles duplication is not an artificial construction, and keeping replicas could retain important information about the underlying symmetry of a compound.



(the lipophilicity data) is still above 0.3 when only keeping the 10% of compounds with the highest confidence. showing a relationship between high confidence and small mean prediction error. although also generally decreasing, the mean prediction error in the right plot



