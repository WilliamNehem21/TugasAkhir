the non-functional challenges facing large scale computing provision are generally well documented. amongst these the cost of energy has become of paramount concern. energy costs now dominate it infrastructure total cost of ownership(tco), with data centre operators predicted to spend more on energy than hardware infrastructure in the next five years. the u.s. environmental protection agency(epa) attribute 1.5% of us electricity consumption to data centre computing, and gartner estimate the ict industry was responsible for 2% of global co2 emissions in 2007. with western european data centre power consumption estimated at 56 twh/year in 2007 and projected to double by 2020, the need to improve energy efficiency of it operations is imperative.



has become a big trend in web services and information processing. the most significant advantage of the cloud is its flexibility. it offers the chance of shifting capital expenditure to operational expenditure, which is ideal for starting a new service. furthermore, since there is an increase in the quantities of data being collected for commercial, scientific or medical purpose, the big capacity of data centre is ideal to process such massive volume of data. as the cloud offers users an illusion of infinite computing resources on-demand, cloud computing is in fact essential to gather useful data from that enormous amount of information.



one of the more challenging problems in managing energy consumption in distributed systems is in handling variability of workload. there are a number of measures which can be applied to manage the effect of variable supply and demand. for example, there are a variety of load balancing techniques and traffic shaping measures which can be utilised to manage demand so that resources do not become excessively over-utilised when demand is high. an alternative approach is to dynamically manage the supply of service capability by making more servers availible during periods of high demand. slegers et al[18,19] considered the problem of finding the optimal share of servers to different services under variable load in order to minimise a performance-based cost function.



the remainder of this paper is organised as follows. in the next section we explain the context of this work in relation to other work on energy reduction. in section 3 we describe the system model and introduce six heuristic strategies for controlling the number of servers powered on and off. this is followed in section 4 by a brief description of the simulation environmen and we then a present and discuss the results of our experiments. finally we present some conclusions and of holding a job in the system for one unit of time, in other words the need of processing jobs quickly. moreover, there was also cpow which reflected the energy saving benefit of keeping a server down for one unit of time. again, those were only relative cost, i.e. cpow= 1 and cjob= 2 simply meant that keeping a job in the system was double as expensive as the energy saving gained by powering down a server. to sum up, a data centres state could be described as follows:



mdown and f are the number of servers which were currently up, down, powering up, powering down and at fault mode respectively. furthermore, the sum of servers in all modes was always n(i.e there is no loss). a system state could move to the next state by the following possibilities:



the same procedure is also true for f. it can be increased with the rate mf as long as there are servers being powered on or off. if the duration of fault mode for a server reaches zero, f will be decreased.



this method employs the concept of making no changes in the number of active servers. in other words, the heuristic decided on the best possible number of servers to switch on, and made no additional switching after that. unless there were faults occurred in the process of powering on, then the heuristic would decide to switch on more servers to compensate for the lost ones. first, the heuristic determined the average rate for both arrival periods by adding the jobs loads and then divided it to the mean duration of both periods: ing and faults can be excluded. therefore, the energy efficiency is only dependent on the power savings and the job holding cost. then the process can be easily repeated for all possible number of servers in order to determine the best efficiency value and therefore the optimal number of servers.



in order to fix the disadvantage of the static allocation heuristic, the semi-static policies is introduced. this method works with the same principle as the static allocation, but it treats the two arrival periods separately. in other words, the semi-static heuristic uses the erlang c formula to find out the best number of servers for the high and low arrival periods separately. therefore, those optimal numbers of serv-ers will be able to keep up with the arrival rates of both periods without the waste of turning on too many servers.



unlike the static allocation method, the semi-static still needs to turn servers on or off between periods. however, since it uses the same mechanism as the static allocation, the semi-static does not take the switching time into account. therefore, when the system is erratic, i.e. the durations of periods were short, or when the switching time is long, this method clearly shows a poor performance. in some extreme cases, the period might be over before the switching finished. nevertheless, because switching times are often small in comparison to the periods duration and the length of jobs, the semi-static still has a good overall performance.



basically, the high/low heuristic is based on the job processing speed to estimate the average time when the job is finished. if the system did no switching, then it is trivial to estimate the time that the jobs were finished. on the other hand, if the system decided to switch on more servers, then after the switching time, the processing speed would increase and the time would be shortened. on the contrary, the processing speed would be slowed down and the time would increase if the system switched off servers.



a simulation fora data centre model was implemented using java jdk. it used an additional library of jfreechart 2 to display the real-time running of the system, along with the statistical results. from this experiments were undertaken to better understand the performance of the various heuristics introduced above. each simulation run lasted 10000 units of(simulated) time, and the costs were calculated from an average of 50 runs.



quickly was given slightly more priority than saving power. the cost of powering servers on and off was accumulating from zero to 10 to show the changes of the two heuristics in comparison with each other. furthermore, the threshold was set to be 5 to make the result easier to distinguish.



one of the most important factors when considering the performance of a system is its consistency. it is also true with data centres. in this case, the consistency denoted the ability to avoid unnecessary switching, which tended to trigger faults. the system in section 6.1 was measured again to get the average number of faults for each heuristic after a loop of 10000 units of time and 50 runs, while the fault rate of switching was calculated with a probability of 0.1%.



our approach here has a number of limitations. firstly we have only considered delays which are negative exponentially distributed, whereas in reality this may not be the case. given that we are simulating the model, there is no real reason why we could not consider general distributions to better understand the effects that different distributions might have on system performance. we have also assumed that all servers are identical, whereas in practice this may not be the case. not only would different servers have different processor speeds, but they would conceivably have different energy consumptions. it would be feasible and interesting to model more than one type of server and to consider, for example, the impact of utilising n fast but energy inefficient servers or m slower but more efficient ones.



