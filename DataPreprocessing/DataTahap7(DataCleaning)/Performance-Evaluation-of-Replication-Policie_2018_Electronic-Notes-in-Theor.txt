nowadays applications tend to be executed on distributed environments provisioned using on-demand infrastructures. the use of techniques such as application containers simplifies the orchestration of complex systems. in this context, microservices based architectures offer a promising solution for what concerns software development and scalability. in this paper, we propose an approach to study the automatic scalability of microservices architectures deployed in public and private clouds. a fluid petri net model describes the characterise of the platform, and a real trace drives the approach to consider a realistic scenario. our focus is on evaluating the performances, costs and energy consumptions from both the service provider and infrastructure provider point of view.



the goal of our modeling process is to evaluate the performances, costs and energy consumptions related to the provisioning of a mbsa from both the service provider and infrastructure provider point of view. in particular, we divide the evaluation into two parts: first we consider the autoscaling strategy, then we focus on the provisioning scheme. the autoscaling strategies will be described using pseudo-code algorithms, while the provisioning schemes will be modeled with fluid stochastic petri nets(fspn).



autoscaling strategies are characterized by two important features: the initial allocation, and the consolidation policies. the initial allocation policy defines the main logic according to which the different mss are mapped onto available vms when the system starts the operations. the consolidation policy defines how the mss are mapped onto available vms according to the dynamics of the workload, i.e. the growth or decrease of demand for the various instances for the different mss.



in the second step, the available cloud resources and their management policy are considered. as costs are dictated by the usage of resources in time and volume, the number of vms in use are the most relevant cost factor. two main cases are in the scope of our study: private clouds and public clouds.



in the case of public clouds, instead, costs are standardized according to the number of vms used per time billing unit(e.g., one hour): the minimization of costs is thus connected to a better usage of the minimal number of vms possible, but with the maximum exploitation of resources in the time billing unit.



vms are characterized by a startup time tup and a shutdown time tdown. in this phases, vms are running, but cannot be used to serve any of the incoming traffic. since they are running, they consume energy: note that the instances of the mss running over them are considered to be inactive and do not share the workload with the other instances. startup phase is modeled by deterministic transition ready(characterized by firing time tup) and place starting. shutdown is modeled by deterministic transition off(with firing time tdown and place suttingdown). both transitions are infinite server, since more vms can be starting or shutting down at the same time.



increasing the cost, by exploiting the time-slot based billing policy usually applied by providers. in this study, we have set tbill= 8 h., again slightly larger than the one commonly used by cloud providers, to emphasize its effect. as it can be seen, in many cases keeping the vms active until the next billing period, can leave the system ready to accomodate new workload fluctuations and generally give more resources than the one actually needed, allowing to provide a better quality of service.



identified by run. as expected, actual purchased vm time grows as a step function, due to quantization of billing periods. as expected, the full consolidation policy, strategy c, is the one that provides the least cost in term of vms running hours, and the non-consolidating policy is the one that results to be the most expensive.



