all previous approaches, however, deal with the static version of the investigated problem. in this paper, we are concerned with the dynamic version of the above mentioned scenario; namely, with the case where the graph may dynamically change over time as streets may be blocked, built, or destroyed, and trains may be added or canceled. in this work, we present new algorithms that dynamically maintain geometric containers when the weight of an edge is increased or decreased(note that these cases cover also edge deletions and insertions). we also report on an experimental study with real-world railway data. our experiments show that the new algorithms are 2-3 times faster than the naive approach of recomputing the geometric containers from scratch.



if n denotes the number of nodes, a graph(without multiple edges) can have up to n2 edges. we call a graph sparse, if the number of edges m is in o(n), and we call a graph large, if one can only afford a memory consumption in o(n). in particular for large sparse graphs, n2 space is not affordable.



note that further nodes may be part of a target container. however, at least the nodes that can be reached by a shortest path starting with e must be in t(e). we will refer to the additional nodes as wrong nodes, since they lead us the wrong way.



proof. obviously, the edge(x, y) must be part of this path pnew. let psy denote the sub-path of pnew from s to v. as a sub-path of a shortest path psy is also a shortest path. in particular, psy is a shortest path that ends with the edge(x, y). for symmetric reasons, the first edge of a shortest x-t-path is(x, y).



proof. since wnew(x, y)< wold(x, y), the new distance dnew(s, t) must be shorter than the old distance dold(s, t). the new shortest path pnew does contain the edge(x, y) in contrast to the old shortest path from s to t. therefore



for each graph, we increase the weight of 100 random edges to a large value(i.e. the sum of all weights in the graph). this is similar to removing the edge from the graph. after every weight change, the containers are updated according to section 4.1. a second set of containers is determined from scratch to compute the quality and compare the computation time.



for the evaluation of decreasing edge weights, we start with the graph where 100 random edges have been set to a large weight. the weights are then decreased to their original values. again, the updated containers are compared to newly computed containers.



compute the container from scratch. the result is slightly different from recomputing all containers from scratch, because some may shrink but are not updated. however, the distances of all nodes to x and from y to all nodes are not needed in this case and their computation can be omitted.



enlarge the container to infinity(without any further computation). if the entire graph is inside the container, it is certainly consistent. however, the quality of the containers is going down rapidly. again, the distances of all nodes to x and from y to all nodes are not needed in this case and their computation can be omitted.



it would be interesting to find other simplifications that guarantee consistent containers, but realize a good compromise between optimality and running time. furthermore, our results suggest that it should be possible to get a speed-up factor of about 2 with an(provable) optimal update strategy. finally, it might be possible to combine edge weight increases and edge weight decreases in a single algorithm.



