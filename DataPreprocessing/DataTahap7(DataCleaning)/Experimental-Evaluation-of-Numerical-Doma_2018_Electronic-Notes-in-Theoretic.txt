many numerical abstract domains have been defined in the literature with the aim of discovering relations among numerical variables in imperative programs. these abstract domains differ on the shape and number of constraints on program variables which may be represented. the most common numerical domains are the interval, octagon and polyhedra abstract domains.



according to the expressive power, we have poly> oct> int and poly> par> int, while octagons and parallelotopes are incomparable. obviously, the more expressive a domain is, the more accurately it may track, at least in principle, the values of program variables during the program execution. if we compare two analyses performed using the domains a and b with a> b, we would expect the analysis using a to find more constraints or more precise bounds than the analysis using b. however, expressiveness of a domain does not tell the whole story. at least other two factors may influence the result of the analysis: abstract operators and widening.



the widening h79 maintains all the constraints of the polyhedra in the previous iteration, under the condition that the constraint is satisfied by all the points in the polyhedra of the current iteration. the widening bhrz03 improves on the standard widening by combining four different heuristic techniques, derived from upper bound operators. both widenings present cases where they lose precision in such a way that the resulting analysis is less precise than what may be attained even with the much simpler interval domain. a detailed example appears in.



from the theoretical point of view, it is easy to study the computational complexity, in space and time, of the abstract operators. most operations on intervals are linear in the number of variables. for octagons and parallelotopes operations are at most cubic on the number of variables, while polyhedra have a worst-case exponential complexity on the number of variables.



however, just knowing how each abstract operator behaves does not give a complete account of the performance of a domain in a real analysis. in particular, predicting the behavior of polyhedra is difficult because the cost of operations heavily depends on the complexity of the polyhedra found during the analysis. therefore, there are cases when polyhedra are faster than octagons, and cases in which they are much slower.



in this paper, we will focus on comparing the precision of analyses run with the same algorithm but different domains. however, we do not compare directly the results as returned by the analysis, for two reasons. first of all, we would get many cases of incomparable results. second, domains such as parallelotopes and polyhedra find many complex constraints involving a lot of variables which, although may be useful to track the execution of the program, are not particularly useful in the final result.



generally, simpler constraints are more easily applicable. for example, interval constraints may be used to prove that some run-time errors, such as division by zero or out-of-bound access to array, do not occur in practice. in case it is needed, simple program transformations may replace complex expressions with new synthetic variables, so that every interesting constraint in the original program becomes an interval constraint in the transformed one. moreover, interval constrains are the largest set of constraints which can be explicitly represented in all the domains. for these reasons, we think that evaluating the precision of the analysis only on the interval constraints is a valuable approach.



we also include for completeness a different comparison on octagonal constraints. these constraints are useful, for example, to check for out-of-bound array accesses when arrays are created with a dimension known at run-time only. however, since new synthetic variables may be created to transform all problems to interval checking problems, we think this comparison is not as relevant as the one on interval constraints.



benchmarks were performed using the jandom static analyzer on the alice benchmarks. jandom is an analyzer for simple imperative programs, linear transitions systems and java bytecode. intervals, parallelotopes and their product are natively implemented in jandom. for octagons and polyhedra we use the implementation in the ppl.



the test-suite comprises a total of 108 models(linear transition systems) with a total of 326 locations, 161 of which are loop heads. each model has at most 11 different locations, 4 loop heads and 10 variables. most of the models(102 out of 108) are part of the alice benchmarks, the remaining 6 are taken from our previous work.



for each model a classical two-phase analysis is performed, consisting of an ascending chain with widening and a descending chain with narrowing. widening and narrowing are applied on all loop heads. for polyhedra, the trivial narrowing which always returns the previous value of the descending chain is used. a delay is applied for both widening and narrowing. we have experimented with different values of the delay: for widening, we have used values between 0 and 6, while for narrowing values between 0 and 3. further experiments carried out with bigger narrowing delays are not shown here, since there are practically no improvements.



the case with narrowing delay 0 is very unfavorable for polyhedra, since it means that no descending chain is performed at all. it is shown only for completeness, but it is not particularly interesting in practice. actually, if we exclude the polyhedra domain, delayed narrowing seems to have a very marginal benefit. results for intervals and octagons, in particular, do not show any improvements with delayed narrowing. the fact that descending chains are generally quite short was already observed in[1,2].



which also loses precision when narrowing delay is 0 and widening delay increases from 4 to 5. finally, polyhedra with h79 widening hardly combines with delayed widening: precision is lost moving from delay 0 to delay 2. from delay 3 onward the analysis recovers some lost precision, but it never comes back to the precision it had with delay 0.



from the point of view of performance, the execution time of intervals, octagons and polyhedra are as expected. intervals are much faster than anything else. for low values of widening delays, speed of octagons and polyhedra is comparable, but for high value of delays, octagons are faster. parallelotopes and their reduced product with intervals are the slowest domains. although this contrasts with the theoretical results, actually it is due to the fact that while octagons and polyhedra are part of the ppl, which is written in c++ and highly optimized, parallelotopes are written in scala with a functional style which is not particularly well suited for this kind of application.



we have compared the relative precision of polyhedra, intervals, octagons, parallelotopes and a reduced product of parallelotopes and intervals w.r.t. the interval constraints on the alice benchmarks using the jandom static analyzer. we have shown that, although the polyhedra domain is theoretically the most precise for inferring linear relationships, in practice the less expressive domains can find more precise results, in particular the reduced product of parallelotopes and intervals. we have also shown that delayed widening generally improves precision of the results up to a certain value(around 3, 4) with the exception of the polyhedra domain with standard widening, where it has a detrimental effect. finally, we have shown that delayed narrowing has no significant effect on the precision of the analysis, with the exception of polyhedra domain which, lacking a narrowing operator, needs at least a delay of one during the descending phase to take a step.



