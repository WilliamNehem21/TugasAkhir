clustering can be defined as the phenomenon of managing data objects into group of classes, which are disjoint in nature. objects in a particular cluster share the same properties, whereas, object in different clusters share dissimilar properties. document clustering are responsible for auto-grouping of documents into groups. clustering is a type of unsupervised classification. unsupervised classification is the process of classification which does not interact with previously defined classes and training sets. bioinformatics, pat-



the whole process of clustering can be split into two categories, these are: hierarchical algorithm and partition algorithm. hierarchical algorithm decomposes the dataset into clusters gives rise to a child clusters in a hierarchical pattern, which is responsible for the generation of clusters named as a dendrogram. each cluster gives rise to a child cluster. partition algorithm is responsible for decomposition of datasets into smaller units in a step. hierarchical algorithm can be further divided into two, those are: agglomerative and divisive clustering. agglomerative clustering involves generation of singleton cluster and later it combines more clusters recursively. divisive clusters initiate with a single cluster and recursively divided into more clusters. this loop aborted when termination condition is satisfied.



the traditional clustering schemes, instances with similar properties are grouped under same cluster, whereas; instances having dissimilar properties are grouped under different category of clusters. these types of clustering schemes are known as hard clustering schemes. in case of soft clustering schemes, instances may be grouped in different clusters simultaneously.



in decision trees classification models, divide-and-conquer is followed to create a tree, in which instances are combined with other attributes. decision tree contains nodes and a leaf node with the output of this algorithm is either true or false. both the path and nodes are considered to form pre-conditions so constraints are formed by the path from root to leaf. tree pruning helps to discard unwanted preconditions and redundancy. one of the best among classification algorithms is random forest algorithm classifies the huge amount of data with high accuracy. in decision trees the models are generated by creating a large number of decision trees. the result contains modal class which is predicted by indi-



genetic algorithms(ga) are categorized as evolutionary and stochastic algorithm which gives an optimal solution. crossover and mutation operations are performed to encode candidate solutions. to create offspring, solutions are selected according to the fitness function. the initial population is randomly constructed; every candidate solution is evaluated to gain a fitness score at each generation. in hierarchical clusters, decomposition occurs and smaller datasets are formed with respect to the different hierarchical pattern. clusters form child clusters or leaf clusters are constructed which are known as dendograms. the process of document classification plays a vital role in recent days. for decades the vast amount of research has been carried out to propose a new integrated approach combing document clustering and classification to achieve optimized performance rate.



the main contribution of the proposed model is included, the gene based features clustering and classification, which is responsible for construction of predictive classification models. in this model, feature based relational gene clusters are used to find the probability of a document belonging to a specific gene related clusters. if a document has a low similarity index in one cluster, then it is grouped in another cluster which is having a high similarity index. there is an effective index which helps in extraction of related documents from previous index. the proposed method reduces the estimated cost of clustering and classification.



rojcek developed a new technique to achieve uncontrolled fuzzy clustering and fast fuzzy classification model on limited dataset. they implemented the concept of kmart neural network for document classification, which is an innovative approach used to integrate clustering with fuzzy classification. clustering as well as classification algorithms share common initial weights. this uncontrolled system is based on two basic methods, those are: 1. involves plasticity, 2. involves stability. this approach can be implemented on classification of labelled ones with limited feature set. these methods are formed without affecting the pre-defined structure(stability) of the training set. this model was tested on small datasets and the algorithm shows better performance than that of existing conventional approaches for document clustering and classifications with limited instances and dimensions. the main limitations in this model include high misclassification rate and less true positive rate on the high dimensional datasets.



chan and chong gave emphasize on non-text based classification scheme and identified the most common issues in the classification model. they classified hypertext-based non-textual information for the purpose of classifying unknown documents. in the traditional approaches of classification, many search engines processes only text-based documents and unable to retrieve the graphic or diagrammatic documents. in this paper, a new algorithm for non-textual diagrammatic document classification was implemented on document sets to analyze the similarity index of the documents. various feature vectors for classification model are presented and tested. this method analyzes and compared with the other methods(like hac) and showed that the proposed approach is far better than that of others in terms of performance. the main issue in the non-textual based classification scheme is, this model failed to process large number of document sets for clustering and classification.



curtis et al. proposed a novel unsupervised model to handle huge volume of documents, also it is quite hard for the algorithms to retrieve and classify massive documents. this model is based on supervised learning, but the idea of unsupervised algorithm is more feasible and effective one for large scale data. a large number of clustering schemes have been developed in the last few years using unsupervised learning. in this model, a novel twothreshold method for hierarchical decomposition of features was implemented on un-structured datasets with missing values and class labels. it discards all the clusters that do not belong to the same cluster. the main problems in this model include the updating of static threshold and parameter initialization in each iteration.



lin and chen developed a new genre classification scheme for musical documents with different melodic patterns consisting of keywords, statistical and low-level features. the proposed classification depends on correlation analysis of musical patterns grouped by appropriate clustering is implemented. performance is increased remarkably by smoothing methods. they considered five different types of musical patterns(such as jazz, lyric, rock, classical) are transformed into symbolic patterns and achieved 70.67% of accuracy. the patterns are transformed into symbolic patterns. in biomedical document classification, k-nearest neighbor technique is a special type of machine learning model based on the document feature extraction and key phrase extraction; it does not build a training model until the new instances are classified. the basic idea of the knn model is to classify a new document based on the similarity measure between the document feature vector and the training document set. after training the model, it gets k documents as majority voting based on the highest similarity measure.



(frecca) for identifying interrelated sentence clusters, which uses fuzzy clustering for this method. it compares the chosen sentences and calculates a similarity index. further works may be done in this algorithm by extending it to gain better performance than that of frecca. genetic algorithm takes input data sets according to random classes. after processing the resulting solution must contain strongly interlinked clusters.



and classification, which is responsible for construction of predictive classification models. in this model, feature based relational gene clusters are used to find the probability of a document belonging to a specific gene related clusters. if a document has a low similarity index in one cluster, then it is grouped in another cluster which is having a high similarity index. there is an effective index which helps in extraction of related documents from previous index. the proposed method reduces the estimated cost of clustering and classification.



gene based document features are extracted using the gene mutual information measure. this measure is implemented in the mapper phase. as the size of the biomedical documents increases, this measure finds the filtered candidate sets with highest probabilistic measure. this measure takes clustered documents as input and finds the topmost features from inter and intra clustered document sets.



in the mapper phase, each biomedical document is processed to find the sparsity and null values. initially, training dataset d is prepared using the medline and pubmed repositories. each document in the training data is represented in vector format. here, document gene, disease and mesh terms are extracted using gene term entity discovery procedure. for each document in the vector docuis used to find inter and intra cluster gene documents for feature selection measures. here, proposed gene mutual information and chi-square feature selection measures are used to find the gene dependencies on the disease patterns. finally, documents with highest gene-disease document patterns are extracted as candidate sets to reducer phase.



ment set, term similarity and weights are computed in vector format as weighted vector document sets wvd. in the next step, the similarities between the gene documents are computed using gene database and gene-synonyms extraction method. documents with highest gene-disease patterns are extracted for document clustering and feature selection procedure. agglomerative cluster method



debian, cent os, gentoo linux, oracle linux, and freebsd. amazon plans to include several additional operating systems to ec2 instances in future. for successful operation of this work, a 64-bit ubuntu server 12.04 is implemented. generally, ami uses t1.large, as it supports both 32-bit as well as 64-bit operating system.



