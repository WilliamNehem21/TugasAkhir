abstract in this paper, an algorithm is proposed to integrate the unsupervised learning with the optimization of the finite mixture models(fmm). while learning parameters of the fmm the proposed algorithm minimizes the mutual information among components of the fmm provided that the reduction in the likelihood of the fmm to fit the input data is minimized. the performance of the proposed algorithm is compared with the performances of other algorithms in the literature. results show the superiority of the proposed algorithm over the other algorithms especially with data sets that are sparsely distributed or generated from overlapped clusters.



in this paper, an algorithm is proposed to determine both the number of components in the fmm and its parameters for fitting an input data set that may be sparsely distributed or generated from overlapped clusters. as it learns the fmm parameters the proposed algorithm minimizes the mutual information among components of the fmm while keeping the reduction in the likelihood of this fmm to fit the input data minimum. the rest of this paper is organized as follows: section 2 presents an algorithm that is proposed to integrate the unsupervised learning and the optimization of the fmm using data sets that may be sparsely distributed or contain overlapped clusters. section 3 presents a comparison study of the proposed algorithm and other algorithms such as the mi, the mml, and the bic algorithms based on their results in clustering the input data and determining the number of fmm components. section 4 presents the conclusions and can be deleted only if dec(r)< avg(dec(k:r+ 1))+ 3std(dec(k:r+ 1)), where avg and std denote the average and the standard deviation. since the tumi algorithm is independent of the number of mixture parameters it is less sensitive to the number of features in the input data set than the algorithms that use penalized-likelihood criteria. therefore, it can handle sparse data sets more accurately than these algorithms. in addition, tuning the mutual information theory allows the tumi algorithm to fit data sets that are generated from overlapped



the mutual information is a symmetric measure to quantify the statistical information shared between two distributions. based on this fact this measure is used to quantify how good the clustering results obtained by a clustering algorithm for a certain data set is by comparing it to the true classification of this data set. let x and y be two random variables represent the true class labels[1...m] for a certain data set and the cluster labels[1...k] resulting from a clustering algorithm where h(x) and h(y) denote the entropy of x and y. the nmi has the value of 1.0 when there is a one to one mapping between the clusters obtained and the true classes(i.e., k= m) of a given data set. since this measure is not biased toward large k, it is preferred to compare different data partitions[36,37].



algorithm is better than the performance of the mi algorithm with all data sets. this is because of many reasons; first, deleting from the mixture model the smallest component that has positive mutual information with the rest of the mixture components causes the model fitting to the given data set to be minimally reduced. on the other hand, the mi algorithm deletes the component that has the maximum positive mutual information with the rest of the mixture components. this



parameters that are more likely to be biased due to the sparse distribution of the feature vectors in the data space. although the mi algorithm is mathematically more efficient than the tumi algorithm results show that it can be highly inaccurate, and therefore this efficiency gain can be worthless. third, using the likelihood function to tune the tumi algorithm allows it to estimate the number of mixture components with high accuracy when the given data set is generated from partially overlapped clusters. on the other hand, this sort of tuning is not found in the mi algorithm and therefore it tends to underestimate the number of mixture components when the given data set is generated from partially overlapped clusters.



