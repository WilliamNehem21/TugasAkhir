the replication of data across multiple sites of data grid is an effective solution to achieve good performance in terms of load balancing, response time, and improving data availability. to get the maximum gain that can make the data replication, their placement strategy in the system is critical. this paper proposes a replication strategy based on availability. it proposes also a placement and replacement strategies of replicas that ensures the desired availability with the minimum replicas despite the presence of nodes failures and without overloading the system. the results of our experimentations confirm that the proposed approach reaches its objectives.



popularity of data. the most often solution used to solve this problem is the replication. data replication is a technique of creating identical copies of data(files, databases, etc.) in geographically distributed sites. each copy is called a replica. the aim of our work is to ensure the desired availability with minimum replicas without degrading system performances. this goal is possible with a placement strategy that takes into account: the desired availability, the stability of nodes in the system and the failures.



we present the following contributions: section 2 introduces some related researches on the replication and placement of replicas in distributed systems and data grids. in section 3, we define the used topology. section 4 presents our contribution, namely a proposal for an efficient dynamic replication approach which takes into account the placement of replicas and failures in the system. section 5 presents the experimental results of our different simulations. the last section summarizes the paper and gives a short overview of future works.



the present paper uses a new model of replication and placement of data. the principal objective of this model is to minimize the number of replicas that ensures certain availability degree without degrading the performance of the system. our strategy takes into account the different and independent stability nodes in contrast to most work in the literature. each cluster_head contains a replication controller to manage the replication and placement of replicas in the cluster.



pi: the stability of the node i where the replica of data j is stored. 0 pi1. in the rest of the paper, we will make the difference between the stability of the node and the stability of its data, we note the first stab and the second p. so if data j is stored in node i then:.



many of works that exist in the literature,, assume that the nodes have the same degree of stability in order to use the formula 2. in the work, the author used nodes of different degrees of stability, but he proposes to replicate the data in the nodes of the same class of stability. this proposal allows him to use the formula 2 to calculate the number of replicas necessary to meet the desired availability. in our system, nodes have different and independent stabilities; this is the case of existing systems. to calculate the number of necessary replicas that ensure the desired degree of availability avail, we have three possibilities:



optimistic: is the case where all the replicas of the data i are stored in the nodes of a good stability. so the availability availi will be assured with the minimum number of replicas. the op is the number of replicas necessary to assure the desired availability in the optimist case. it is calculated by the formula 2 where p is the best stability in the system.



pessimistic: is the case where all replicas of the data i are stored in the nodes of poor stability. so the availability availi will be assured with the maximum number of replicas. the pes is the number of replicas necessary to assure the desired availability in the pessimist case, it is calculated by the formula 2 where p is the minimum stability in the system.



hybrid is the case where the replicas are stored in nodes of different degrees of stability. so the availability availi will be assured by crating hyp replicas in the system. so hyp will be in the range[op, pes]. the number hyp can not be calculated after specifying the placement of replicas because you have to select the participating pi in the formula 1.



each cluster-head(ch) specifies a certain degree of availability availd to ensure in its cluster. the availd is estimated using the history of data itself and its importance(popularity) in previous periods. after the calculation of availd, the ch compare the actual availability(real availability) of the data availr in the cluster with the desired availability availd.



the placement of replicas in the system plays an important role. in, the author shows that the placement of several replicas of same data in the same node does not improve the availability or fault tolerance. for this reason, it will be useful to store a single replica of the same data in a node. but what are the good candidates to store the data?. in our system, nodes can predict failures. in case of suspecting the failure,



a high degree of responsibility indicates that the node has a lot of bytes to place(move) in case of suspected failure. in this case the recovery time of that node will be high and the fault will be accelerated. the recovery time presents the needed time for moving data from the suspected node to other nodes. we suppose that in the beginning the dr is 1 in all the nodes.



in this paper, we proposed a replication strategy based on availability, we also proposed the placement of replicas in the system in an efficient manner that improves system performance without overloading the system nodes. the inconvenient of this approach it is semi-centralized, that is to say that the decision of replication and placement of replicas is achieved by each cluster-head in each cluster. but since the number of nodes in cluster is limited, the constraint of scalability can be met. in the following phase of the research, we will use globus to study the comportment of our placement model in real grid. we will also extend this work by considering also the task replication and placement in order to ensure a fast and fault tolerant execution of system jobs.



