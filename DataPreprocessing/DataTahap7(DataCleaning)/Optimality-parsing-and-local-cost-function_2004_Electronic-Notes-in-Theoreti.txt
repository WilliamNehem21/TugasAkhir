also to richard hudson, don c. mitchell, owen rambow, lars konieczny, line mikkelsen, sten vikner, sabine kirchmeyer-andersen, and alex klinge for fruitful discussions, and to two anonymous reviewers for highly valuable comments. this work was made possible by a grant from the danish research council for the humanities.



in the lexicon, each type is defined by a lexical entry that specifies an ordered list of its immediate super types, and declares its variables. variable declarations and values are inherited by default from super types. variables are marked as either atomic or set-valued. the value of an atomic variable is either specified locally, or inherited from the first super type in which it is defined(ie, by default priority inheritance). the value of a set-valued variable can be locally specified using different operators: v= a is used to set the value of variable v to the set a, whereas v=+a b sets the value of v to set a plus all values for v in super types minus set b.



in the simplified example below, verb is defined as subtype of type word with variable comps having set value[subj:noun](that is, a verb licenses by default a complement of type noun with edge type subject). eat is defined as subtype of verb and pres(present tense), and its value for comps is[subj:noun, dobj:noun] after inheritance(that is, it also takes a noun as direct object). rains overrides the inheritance by setting comps to[subj:it].



dg constraints are defined in terms of cost functions, which return a cost that measures the number of times a syntactic condition is violated, weighed with its severity. cost functions are expressed in terms of the realand set-valued operators shown below. each operator is stated in the context of a word w in a graph g. dg defines the following set-valued operators(we let n denote the type join of the node n with all of its parent edges):



in the following, we will demonstrate how the operators above can be used to encode cost functions for a wide range of syntactic constraints, ie, to express what counts as a violation of the constraint. in a real lexicon, the cost functions must be assigned relative weights by multiplying them with a fixed non-negative cost. however, for simplicity, we will pretend that all cost functions have unit cost.



a landing site has a left field and a right field that contains its landed nodes. some violations in the right verbal field are listed below. in danish and english, these violations are always ungrammatical and must be assigned a high cost. in german, they are only marked and must be assigned a lower cost.



5 np-hard formalisms include optimality theory with unrestricted constraints, stochastic tree-substitution grammars and data-oriented parsing, lfg, and constraint-based formalisms such as hpsg. cfg and tag are exceptions since they can be parsed in o(n 3) and o(n6) time, respectively, although there are linguistic phenomena that they cannot account for. moreover, chart parsers for cfg and tag create a packed chart that may contain an exponential number of parses(e.g., consider the grammar x! x x with terminal rule x! x). thus, given a linguistically plausible cost measure on top of cfg and tag(say, a measure that checks semantic and pragmatic plausibility), it is quite conceivable that exact optimality parsing in cfg and tag will turn out to be np-hard.



the parsing algorithm in dg is based on local search. the search space is defined as the set of all partial parses of a sentence, and the solution space consists of all partial parses that contain all input words. thus, even if dg parsing fails, it will return a maximal partial parse of the input. the starting point is the empty parse, and the total cost of a partial parse is defined as the sum of all violation costs imposed by the words in the parse. this gives the algorithm for optimality parsing:



the local neighbourhood around each partial parse is defined in terms of a set of basic parsing operations that compute a set of new graphs from the existing graph. if there are any unread words in the input, the operation that reads the next word and adds it to the graph is always a valid parsing operation. the identity mapping is never a valid parsing operation, ie, a graph is never included in its own neighbourhood.



ideally, the parsing operations must be chosen strong enough to recover from suboptimal analyses that humans recover from, and weak enough to be trapped in suboptimal analyses that humans are trapped in. preferably, the operations must be computationally efficient as well. there is no reason to suppose that all humans have the same set of parsing operations. on the contrary, humans seem to revise their parsing strategies after exposure to garden-path sentences, so it is reasonable to suppose that parsing operations are learned, and hence that there is some variation between different people and language communities.



when testing an optimality parser, we need to distinguish between grammar failures and parser failures. grammar failures arise if the cost measure defined by the grammar is wrong, either because an undesired analysis is deemed optimal by the grammar, or a desired analysis is deemed non-optimal. parser failures arise when the parser fails to find an optimal analysis. obviously, one cannot blame the parser for any grammar failures, but parser failures are serious counter-evidence against the parser, unless humans suffer from the same problem.



there are three different kinds of monotonic operations: lex-operations add the next unread word to the graph, land-operations add a landing site edge to a surface root, and dep-operations add a governor edge(and possibly a landing site edge, if none exists already) to a deep root. the land-operation is used to attach a node to a landing site in cases where the governor is not yet available, such as a subject temporarily landing on a complementizer until its governing verb is encountered. in the german example below, steps 1, 2, 4, and 6 are lex-operations, step 3 is a dep-operation, and step 5 is a land-operation.



parsing operations that affect at most k nodes in the graph, either by adding a node to the graph or by changing its governor and landing site, are called k-change operations. clearly, any parsing operation is a k-change operation for large enough k. in particular, the repair operation needed in the garden-path example is a 2-change operation.



a k-change operation has time complexity o(n2k), so if we want parsing to be almost linear, k-change is too expensive. however, it is conceivable that humans employ 2-change and 3-change as higher-level repair operations, which are only used as a last, unreliable resort if all normal parsing operations fail. this may also account for how humans learn a set of parsing operations: initially, they have to resort to 2-change and 3-change operations to find an optimal parsing operation. but after a while, they notice a pattern in the optimal operations which allow them to formulate a more efficient class of parsing operations, such as land, dep, repair1, repair2, and repair3.





the purpose of the implementation was to examine the precision of the algorithm. when the parser returned a suboptimal parse, it was either a garden path sentence where humans also failed, or the parse was only suboptimal because a landing site was chosen too high in the structure, and optimal in all other respects. since landing sites play no role in the semantic interpretation, this slight suboptimality is unimportant. apart from landing site suboptimality, the test showed that the parser returned an optimal analysis in all other cases than strong garden paths. thus, even though the parser is heuristic, it does not seem to be more significantly more error-prone than an exact parser in practice.



local cost functions in discontinuous grammar provide a flexible and efficient means to measure the syntactic, semantic, pragmatic, and probabilistic plausibility of a parse. moreover, the optimality parser is very fast, with theoretical time complexity o(n log4 n) under linguistically reasonable assumptions, and tests with a simple implementation of an optimality parser and a grammar shows that the proposed parsing operations are strong enough to provide the correct analyses for a wide range of discontinuous sentences, although it fails on garden path sentences where humans fail. thus, optimality parsing seems to provide a fast and accurate model of human parsing and its imperfections.



some issues have not been addressed in this article, in particular, the question of how to deal with ambiguity, and the question of how to improve the averagecase complexity of the dg parser by using sophisticated search techniques. new parsing operations may have to be added to the set of admissible parsing operations in order to account for some phenomena, such as adverbial reanalysis. finally, a large-scale grammar and evaluation of dg would be highly desirable as well.



