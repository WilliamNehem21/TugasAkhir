service-oriented architecture(soa) for enterprise systems is a relatively new, but rapidly proliferating paradigm in software engineering. the business functionality of the individual systems is published as services with well-defined interfaces that are specified in a standard, platform-neutral way, such as web services description language(wsdl). using elementary services, composite services can be created, or even complex workflows can be orchestrated that are capable of partly or completely automating business processes. among the several advantages of soa based systems, we find flexibility, platform-independence and better reuse. however the of xml serialization is hence composed of the cpu and network demand. in service-oriented applications, the service interfaces, including the message types, are described in wsdl, before the services are implemented. we show that using only the wsdl, and an estimation about the length of the dynamic data structures such as collections, the cost of xml serialization on a concrete system can be estimated if we measure this cost only for primitive types on that concrete system. the rest of this paper is organized as follows. section 2 summarizes related work. section 3 contains the explanation and the validation of the cost model of xml serialization. finally, conclusions are drawn in section 4, and future work is



the following questions are relevant when we want to take the overhead of xml serialization in a performance model of a service-oriented application into account: how does the cost of serialization depend on the data type of the messages to be serialized? how does the serialization overhead depend on the length of a dynamic data structure, such as an array? how does the length of a string affect the cost of its serialization? how can we estimate the serialization overhead of a known composite data type if we know the serialization cost of the composing primitive data types? to answer these questions, we performed measurements, built a model, and validated the goodness of fit against the measurement results, as presented in subsections 3.1, 3.2 and 3.3 respectively.



the first part of the test cases serializes a specific object 10, 000 times into the same network stream(i.e., sends the serialized xml over the network), and measures the average execution time of the serialization. the measurement is performed library in java that provides a high resolution timer using the native timer capabilities of the operating system. the second part of the test cases measures the cost of deserialization by serializing an object into the memory, and deserializing it back 10, 000 times. the cpu usage during the test cases is recorded as well, the cpu and i/o cost can be separated this way. the test cases were executed on a pc with microsoft windows server 2003 r2 service pack2, a 2.4 ghz intel pentium 4 processor and 1 gb memory. the target of the serialization is another pc with windows xp, and a 3 ghz intel pentium 4 hyperthreading processor and 2 gb memory. the machines are connected via a 100 mbit/s switched lan. both the



well. it can be seen that this value is very close to 1, indicating an excellent fit of the linear model. the linearity itself is not a surprising result, but determining these coefficients is important when we try to find a general formula to calculate the resource demand of composite types in subsection 3.3.



.net framework can be disassembled(decompiled) using.net reflector. we found that the writing of characters to the output stream is buffered at multiple points. in.net for example, a buffer of 256 and a buffer of 1024 bytes are used. to prove that this causes the more flat line for small string lengths, we wrote a similar code to that found in the.net framework. it turned out that removing the 256 sized buffer removes the cut-off point, hence we conclude that the buffering causes the better performance of the xml serialization for small string lengths, and we place the cut-off point at 256. in java, a similar buffer is used with a size of 128. the lines for deserialization are less steep, and do not contain any cut-off points in



the coefficients retrieved with the multiple linear regression for composite types. for the other three cases(.net serialization and deserialization, and java deserialization) our starting point is confirmed, namely, that the serialization cost of one additional integer or double value(represented by the appropriate coefficients) is the same, the value is either a member in an array or a field in a composite object.



this observation has a very important practical implication as well. if we want to know the cost of xml serialization or deserialization on a given machine for any composite type containing integer and double fields, it is enough to measure the serialization or deserialization cost of the array for some lengths(in our case 1, 10, 20,...1000), perform a linear regression, and the resulting coefficients can be reused in calculating the serialization overhead of any composite object.



