in theory, program result checking has been established as a well-suited method to construct formally correct compiler frontends but it has never proved its practicality for real-life compilers. such a proof is necessary to establish result checking as the method of choice to implement compilers correctly. we show that the lexical analysis of the gnu c compiler can be formally specified and checked within the theorem prover isabelle/hol utilizing program checking. thereby we demonstrate that formal specification and verification techniques are able to handle real-life compilers.



program checking has been introduced to improve the reliability of programs. it assumes that there exists a black box implementation p computing a function f. a checker for f checks for a particular input x if p(x)= f(x).



we check the lexical analysis of the gcc compiler by recomputing its result via a formally verified implementation and by comparing this correct result with the one produced by the gcc, cf. subsection 2.3. for this purpose, we specify the lexical analysis within the isabelle/hol theorem prover and use



in the remainder of this subsection, we discuss the most interesting parts of both specification possibilities for the above example automaton. the specification of the scanner recomputing the results of the gcc follows these lines directly, except that it is much larger, cf. subsection 4.2. in discussing the smaller example first, we can illustrate the underlying principles more clearly.



to check the lexical analysis of the gcc, we need to have access to the tokens and their values computed by it. in subsection 5.1, we describe the modifications of the gcc code necessary for that. in subsection 5.2 we explain our overall checking architecture, in particular how we have connected the isabelle the list of expected tokens and their values. this list is written back by the function write token list to the format in which also the modified gcc analysis outputs its tokens and their values. in this format, each token and its value are separated by a whitespace and closed with a new line.



altogether, our checking architecture integrates well into the gcc system. we only need very simple modifications of the gcc code to extract the computed tokens and their values. also, the isabelle checker can easily be connected with the gcc code.



program checking has been used in the construction of correct compilers, most prominently in the verifix project. it has proposed program checking to ensure the correctness of compiler implementations. program checking has been applied in the context of frontend verification, as already discussed in section 2. the program checking approach has also been used in further projects aiming to implement correct compilers. shows how some backend optimizations of the gcc can be checked. in[12,11], the problem of constructing correct compilers is also addressed, but only for very limited applications. only those programs consisting of a single loop with loop-free body are considered and translated without the usual optimizations of compiler construction. those programs are translated correctly such that certain safety and liveness properties of reactive systems are sustained. in more recent work



formal verification of compiler frontends has been investigated e.g. in where proof fragments for the correctness of lexical and syntactic anlysis have been formalized in the mechanical theorem prover nqthm. formal verification of lexical analysis has also been addressed in. this work shows the formal verification of a very simple lexical analyzer generator that takes a regular expression and yields a functional lexical analyzer. emphasis in this work is placed on the formal verification of transforming a regular expression into an initially nondeterministic automaton and then into a deterministic one. in future work we plan to connect our checking architecture with the theory described in this work. in doing so, we would only need to extract the regular grammar for the lexical analysis and could construct the corresponding deterministic automaton as described in this related work.



with the results presented in this paper, we have set up an architecture for checking results of frontend computations, in particular for the lexical analysis. we have shown how to specify the task of scanning tokens within isabelle/hol. furthermore, we have demonstrated how to access the intermediate results of the gcc system by minimally modifying its c source code. in doing so, we have shown that program checking can handle large nonacademic compilers which are extensively used in practice. the presented checking method is not specific to the gcc nor to the c langugage and can be applied to other compilers as well. in future work, we plan to connect our checking architecture with the results of which provides a formal proof in isabelle/hol for the generation of a functional lexical analyzer(i.e. a finite automaton) given a regular grammar. since the regular grammar for the c tokens is directly given in the c language specification(as is the case for the specifications of many other programming languages as well), this will yield a completely verified result of the lexical analysis.



