schemes, as well as the integration of multiples data sources into a single data set. in general in data cleaning, near-duplicates may exist within a single source, whereas in data integration near-duplicates may exist within or across various data sources. however, both cases have the same common goal, detecting near-duplicates and the closeness of similarity among entities in a large collection accurately and efficiently. that is, to determine which records/objects in the same or different databases refer to the same underlying real-world entity.



insights and multi-dimensional information from business, financial(i.e., credit cards), and healthcare data. a comprehensive evaluation and an umbrella of techniques have been investigated. a common factor between these algorithms is they cannot guarantee finding all nearduplicates and also cannot guarantee the accuracy. vogal et al. provided an annealing standard to evaluate near-duplicate detection results. in other words, the accuracy and completeness of duplicates should converge incrementally and interatively to a gold or silver standard that defines which records represent the same real-world entities.



our proposed set of algorithms do not presume a specific application domain, but in contrast they are tuned toward any domain-independent applications. monge-elkan's(me) algorithm is relatively domainindependent with the purpose of integrating and matching web scientific papers from multiple sources, typically an alphanumeric domain class. the parameters used in me are mapped to such a class of applications with only a restricted possibility of tuning the threshold values to provide a better accuracy. in addition, the heuristic method of me minimizes the number of pairwise record comparisons with potential record duplicates and integrates some key concepts such as the minimum edit-distance of sw.



the rest of the paper is organized as follows. section 3 reviews and summarizes the effectiveness of monge-elkan and smith-waterman algorithms. section 4 addresses the metric measures used in detecting nearduplicates. section 5 explains how to calculate and choose between precision and recall using f-measure. the choice, adjustment, and threshold tuning are defined in this section.



ferences between near-duplicate records often arise because of many abbreviations or extra-string insertions and omissions, the affine-gap model produces a better similarity and more accurate results than most the other edit distance metrics. moreover, the affine-gap algorithm performs well to detect similarities when records have minor syntactical differences, including typographical errors, abbreviations, and truncations. in fact, the monge-elkan's algorithm approximates the solution to the optimal assignment problem in combinatorial optimization. this approximation is a reasonable trade-off between accuracy and complexity.



in the paper, we adopt a modified version of the smith-waterman similarity edit distance as inter-token similarity measure. formally, let c(xi, yi) denote the cost of the edit distance that aligns ith character of string x to jth character of string y. then the sw algorithm computes a cost matrix m that represents a maximum-cost string alignment by the following recurrence rule based on monge-elkan's algorithm.



representatives which retain the most relevant syntactic and semantics features of the records in the cluster where comparisons take place with cluster representatives, instead of all records, thus the search space can be reduced with the improvement of both true and declared subspaces. in other words, the reduction of false positives and in particular false negatives have an impact on the accuracy. a high recall means no false misses and indicates high accuracy of the duplicate detection results. a high reduction ratio achieves an even more effective search space reduction. a high precision means few false matches and has the opposite



cluster. however,(r1, r3) and(r2, r3) would be non-duplicates. now, consider a new record r4 added to r3 with sim(r1, r4)= 0.75, sim(r2, r4)= 0.88, and sim(r3, r4)= 0.96. then,(r2, r4) and(r1, r4) are classified as



similarly 1^ 1r and 1^ 2 r are defined. if two records(i.e., representatives) are in the same cluster then they are considered to be near-duplicates or exactly similar, and if not they are dissimilar. for instance, let r3={r1; in a same manner. lines 4 and 5 in the function compare(c^ 1;c^ 2) should be substituted by lines 4 and 5 in the function check-similarity(c^ 1;c^ 2), respectively. in the same way, lines 11 and 12 should be substituted by lines 4 and 5. for the case of no similarity, lines 4 and 5 in the function



3: flip the first and last attributes(i.e., flip the first and last names in the lists) 4: duplicate a random character. this might be done to more than one character 5: abbreviate randomlykeep the first character but this might be duplicated or



cora2 cora data set contains bibliographic records and citations in scientific papers classified in several classes. the cora citation data set, which consists of a data set of original references and research papers, is often used in the duplicate detection community. additionally, we augmented and topped the cora data set to 21,152 references and 32,005, a substantial larger data set for our experiments.



names(s), title of the publication, some keywords, an abbreviated reference format citation(i.e., journal, book, editor). it consists of 43,935 real objects, both for relational and xml data. additionally and for our experiments, we augmented the data set to 63,553 references.



with an extensive experimental analysis, the purity, inverse and fmeasure achieved a value of nearly 1.0, a precision which outperforms the seminal work of monge-elkan. for instance, in the me algorithm which is based on jaro-winkler's metric and using the attribute name provided by dblp, the maximum f-measure is at threshold 0.8 and robust up to 0.9. the precision drops steadily below 0.8 due to many false positives. however, with the same algorithm based on smith-waterman's metric, the maximum f-measure is at threshold 0.9 and the sw precision



i acknowledge the collaboration of dr. maamir allaoua who collaborated with me on several interrelated published papers. he is now retired from the dept. of computer science, university of sharjah, sharjah uae. i would also like to thank two graduate students, sepideh pashami and serveh ghaderi, who spent one entire term carrying out the experiments and programming parts reported in this paper. i also thank professors a. elmagarmid and v. verykios for providing me with some of the original benchmark data sets.



