a controlled environment based on known properties of the dataset used by a learning algorithm is useful to empirically evaluate machine learning algorithms. synthetic(artificial) datasets are used for this purpose. although there are publicly available frameworks to generate synthetic single-label datasets, this is not the case for multi-label datasets, in which each instance is associated with a set of labels usually correlated. this work presents mldatagen, a multi-label dataset generator framework we have implemented, which is publicly available to the community. currently, two strategies have been implemented in mldatagen: hypersphere and hypercube. for each label in the multi-label dataset, these strategies randomly generate a geometric shape(hypersphere or hypercube), which is populated with points(instances) randomly generated. afterwards, each instance is labeled according to the shapes it belongs to, which defines its multi-label. experiments with a multi-label classification algorithm in six synthetic datasets illustrate the use of mldatagen.



in practice, the effectiveness of machine learning algorithms depends on the quality of the generated classifiers. this makes fundamental research in machine learning inherently empirical. to this end, the community carries out extensive experimental studies to evaluate the performance of learning algorithms. synthetic(artificial) datasets are useful in these empirical studies, as they offer a controlled environment based on known properties of the dataset used by the learning algorithm to construct the classifier. a good classifier is generally considered to be one that learns to correctly identify the label(s) of new examples with a high probability. thus, synthetic datasets can be used instead of real world datasets to derive rigorous results for the average case performance of learning algorithms.



the remainder of this work is organized as follows: section 2 briefly describes multi-label learning concepts and strategies to generate synthetic multi-label datasets. the proposed framework is presented in section 3 and illustrated in section 4. section 5 presents the conclusions and future work.



multi-label learning methods can be organized into two main categories: algorithm adaptation and problem transformation. the first one consists of methods which extend specific learning algorithms to handle multi-label data directly, such as the multi-label naive bayes(mlnb) algorithm. the second category is algorithm independent, allowing one to use any state of the art single-label learning method. methods which transform the multi-label classification problem into several single-label classification problems, such as the binary relevance(br) approach, fall within this category. specifically, br transforms a multi-label dataset into q single-label datasets, classifies each single-label problem separately and then combines the outputs.



the evaluation of single-label classifiers has only two possible outcomes, correct or incorrect. however, evaluating multi-label classification should also take into account partially correct classification. to this end, several multi-label evaluation measures have been proposed, as described in. in what follows, we briefly describe the example-based and label-based evaluation measures used in this work. all these performance measures range in[0..1].



thus, the binary evaluation measure used is computed on individual labels first and then averaged for all labels by the macro-averaging operation, while it is computed globally for all instances and all labels by the micro-averaging operation. this means that macro-averaging would be more affected by labels that participate in fewer multi-labels, i.e., fewer examples, which is appropriate in the study of unbalanced datasets.



synthetic datasets with different properties are generated in to study multilabel decision trees. in this study, the power to identify good features, related to the feature selection task, was also verified. the strategy used to generate the datasets considers several functions to define feature values related to the labels.



to illustrate a new multi-label learning algorithm, a synthetic dataset is specified in. given three labels and a covariance matrix, instances are labeled according to seven gaussian distributions, such that each distribution is related to one multilabel. the number of instances per multi-label is arbitrarily defined.



it should be emphasized that mldatagen extends the strategy proposed in by enabling the user to choose the number of different kinds of features, i.e., relevant, irrelevant and redundant, the maximum and minimum radius of the small hyperspheres, as well as the noise level of the datasets generated.



as the possible range to define each coordinate of ci reduces whenever a new coordinate is set, it is necessary to avoid determinism during the generation of the ci coordinates of the hypersphere hsi, i.e., to avoid always generating ci1 as the first coordinate and cimrel as the last one. to this end, the index of the coordinates j to be set, j= 1..mrel, is randomly defined.



to ensure that the multi-labels contain at least one of the q possible labels, the generation of the n instances is oriented, such that, for each instance ei, the point with coordinates(xi1, xi2,..., ximrel) is at least inside one small hypersphere. by using this procedure, none of the instances will have an empty multi-label. all small hyperspheres hsi, i= 1..q, are populated using this criterion.



algorithm 2 summarizes the generation of the instances xk, k= 1..n, inside the small hyperspheres hsi=(ri, ci), i= 1..q. similar to algorithm 1, the updateminx(x) and updatemaxx(x) functions refresh respectively the lower and upper bounds of the next xk coordinate to be defined inside the corresponding hypersphere hsi. the already set coordinates are also taken into account to randomly generate the remaining coordinates.



after generating the n points related to mrel, mirr and mred features are set by adding mirr irrelevant features, with random values, and mred redundant features. the features to be replicated as redundant are chosen randomly. in the end, the n points are in rm, m= mrel+ mirr+ mred.



different from hyperspheres, the possible ranges to define each coordinate cij are the same for all coordinates. thus, algorithm 3 is simpler than algorithm 1, as the functions updateminc(x) and updatemaxc(x) are not needed to generate the hypercubes hci=(ei, ci), i= 1..q.



algorithm 4 summarizes the generation of the instances xk, k= 1..n, inside a small hypercube hci=(ei, ci), i= 1..q. this algorithm is simpler than algorithm 2 by discarding the functions updateminx(x) and updatemaxx(x), as the coordinates range in the same domain.



as hyperspheres does, after generating the n points related to mrel, the mirr and mred features are set by adding mirr irrelevant features, with random values, and mred redundant features. the features to be replicated as redundant are chosen randomly. in the end, the n points are in rm, m= mrel+ mirr+ mred.



mldatagen was used to generate 6 synthetic multi-label datasets, 3 using the hyperspheres strategy and the other 3 the hypercubes strategy. to generate the datasets, different values of the mrel, mirr and mred parameters were used. these values were chosen to analyze how the number of features(m= mrel+ mirr+ mred) and the number of unimportant features(mirr and mred) influence the performance of the multi-label brknn-b learning algorithm, available in mulan. in what follows, information about the synthetic datasets generated, brknn-b and the classification results are presented.



lazy algorithms are useful in the evaluation of datasets with irrelevant features, as the classifiers built by these algorithms are usually susceptible to irrelevant features. the multi-label learning algorithm brknn is an adaptation of the single-label lazy k nearest neighbor(knn) algorithm to classify multi-label examples proposed in. it is based on the well-known binary relevance approach, which transforms a multilabel dataset into q single-label datasets, one per label. after transforming the data, knn classifies each single-label problem separately and then brknn aggregates the prediction of each one of the q single-label classifiers in the corresponding multilabel, which is the one predicted. despite the similarities between the algorithms, brknn is much faster than knn applied according to the br approach, as brknn performs only one search for the k nearest neighbors.



to improve the predictive performance and to tackle directly the multi-label problem, the extensions brknn-a and brknn-b were also proposed in. both extensions are based on a label confidence score, which is estimated for each label from the percentage of the k nearest neighbors having this label. brknn-a classifies an unseen example e using the labels with a confidence score greater than 0.5, i.e., labels included in at least half of the k nearest neighbors of e. if no label satisfies this condition, it outputs the label with the greatest confidence score. on the other hand, brknn-b classifies e with the[s](nearest integer of s) labels which have the greatest confidence score, where s is the average size of the label sets of the k nearest neighbors of e.



it can be observed that between the two datasets with m= 5 features, dataset f, which was created using the hypercubes strategy, shows the best results. among the four datasets with m= 20, dataset b obtained the best example-based measure values for hamming-loss(hl) and subset-accuracy(sacc), while dataset d obtained the best results for the remainder of the example-based measures and for all the label-based measures considered. both datasets were also created using the hypercubes strategy.



as stated before, for m= 20 the classifiers build with datasets b and d show better performance than the ones build with datasets a and c. moreover, for m= 5 the classifier build with dataset f shows better results than the one build with dataset e. observe that these datasets(b, d and f) were generated using the hypercubes strategy.



is reached for m= 5, decreasing afterwards. this problem is well described in. on the other hand, in the hypercubes strategy the volume of the main hypercube hc always increases with the dimensionality. both cases are related to the curse of dimensionality.



to illustrate mldatagen, multi-label classifiers were built from six synthetic datasets. the results suggest that the datasets generated by the hypercubes strategy provide better classification results than the ones based on hyperspheres. in fact, despite both strategies being sensitive to the curse of dimensionality, the volume of a hypercube always increases with the dimensionality, showing better behavior than a hypersphere, whose volume decreases from one dimension upwards. as future work, we plan to implement more strategies into the mldatagen framework and make them available to the community.



