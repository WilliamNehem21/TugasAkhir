in the past years, arabic nlp has been significantly lagging behind its english counterpart, but recent advancements in natural language processing have made it possible for arabic to catch up and show promising results for a multitude of tasks. complex tasks such as short answer scoring, have been widely researched mainly for english, leveraging machine learning and state-of-the-art deep learning techniques. in this paper, we introduce the first deep learning-based system for arabic short answer scoring, in efforts to provide a reliable system that can help teachers in the arab world better utilize their time in other teaching activities that would elevate the quality of learning in the region. we empirically study different techniques and propose the best performing system based on our results, where we have achieved state-of-the-art performance, achieving a qwk score of 0.78 and showing how powerful and robust recent arabic nlp tools have become.



automated grading systems have been efficiently reducing the huge amounts of resources and time spent in grading exams, allowing teachers to better utilize their time and effort in other core tasks that would yield a better educational experience for their students. due to the precise nature of the grading process, extensive research has been conducted to ensure that a fair platform is being offered. such platform should be able to properly justify replacing manual grading methods.



there was a gap for short answer scoring in the arabic language. most of the available systems were reference based. the two approaches were explored by. it was found that with sufficient amount of human rated answers then the response-based approach is likely to perform better than the reference-based one. but this remained unexplored in the arabic language. the only response-based approach for arabic short answer scoring was presented in. however, the existing work for arabic short answer scoring systems never explored deep learning. with the performance that deep learning systems offer, the work in this paper tries to fill in this gap by contributing a deep learning response-based system for arabic short answer scoring.



short answer scoring. section 3 gives an overview about the dataset, preprocessing steps and the evaluation metrics used for this work. section 4 discusses the applied methodology. the experiments one are shown in section 5. the results are shown in section 6 discusses the results of the experiments. further discussion is done in section 7. we then conclude with directions to future work.



in this section we will briefly discuss challenges in the arabic language, such as morphology, dialectal variations, and lexical ambiguities. traditional nlp methods are then discussed in addition to deep learning and transfer learning, leveraging transformer language models such as bert and how they contributed to the rise of nlp. finally, previous work regarding automated grading systems will also be discussed.



arabic is known for being a complex language. it contains many subtleties and ambiguities that sets it apart from other languages, making it a challenging language in the context of nlp. one of the challenges is the complex morphology when compared to english, where arabic words can take on several forms resulting in numerous variations of a single word that can often times translate to a sentence in english. all verbs in arabic have a root usually from three letters making it highly derivational. therefore, it is much harder to build arabic stemming/lemmatizing tools, and adapt tokenizers such as wordpiece and sentencepiece tokenizers used in bert for arabic. and. moreover, lexical ambiguities is also highly present in arabic, with context playing a huge factor in determining the meaning of a word or sentence.



one other challenge in the arabic language is dialectal variations. each region has its own version of arabic. while the general structure of arabic is preserved in most dialects, usually each dialect comes with an additional set of custom rules and vocabulary. unfortunately, this can hinder the quality of arabic datasets. for example, a model can perform extremely well on the egyptian dialect, but perform poorly on the tunisian dialect. thus, in order to build generalized arabic models, different dialects must be taken into account when collecting the data.



rich language. for our specific case(automated grading), the dataset we are using is an arabic translated version. as a result, diacritics are completely disregarded. moreover, students usually do not include diacritics in their answers for ease of writing, with the exception of formal arabic exams, as human graders can generally understand the context without diacritics. however, this poses a difficult challenge for automated nlp systems, since any collected dataset for this task will most likely not include diacritics.



pretrained word embeddings have also surfaced in the past decade, such as word2vec and glove. they are trained on a large corpus for the desired language, and a high-dimensional vector is produced for each word, taking into account semantic similarities[17,18]. as a result of representing each word as an independent vector, context is also not taken into account, when a word can have different meanings depending on the context. such models also suffer from not being able to handle unseen words. this can cause problems when dealing with text that contains specific content, such as scientific content that is abundant in short answer scoring datasets.



they propose replaced token detection instead of the mlm task present in bert. akin to a gan training process, a generator randomly replaces tokens with fake high-quality words. a discriminator is then trained to predict whether each token is original or replaced. thus, electra learns from all input tokens rather than the masked tokens only, which resulted in a more cost and computationally efficient model that outperforms bert in many downstream tasks. it is important to note that this method is not adversarial since the generator producing corrupted tokens is trained with maximum-likelihood.



transformer-based language models have carved the path for transfer learning in nlp. such models have been fine-tuned for downstream tasks and achieved state-of-the-art results in most nlp tasks. deep learning has only been minimally researched in developing short answer scoring systems, with a few utilizing models such as bert and electra. bi-lstms with attention showed to be a powerful candidate for the task at hand. they have achieved promising results reaching a quadratic weighted kappa(qwk) of 0.723 on the kaggle asap-sas dataset.1 on the same dataset, used different architectures such as cnns, lstms, and bert, reporting a qwk of 0.62, 0.65 and 0.71 respectively, with bert achieving the best performance.



only a few arabic short answer scoring systems exist, where most of them being based-off the reference-based method. authors in[25,26] presented a promising reference-based arabic short answer scoring system with effective feedback based on the similarity the score. they proposed string-based similarity and corpus-based similarity and a hybrid approach to compare the similarity between the student answer and the reference answer.



different preprocessing steps have been conducting for our experiments. for our baseline model, consisting of a tf-idf vectorizer and svm, we first cleaned the text from any numbers and symbols, and some artifacts from the translation process, where we found some random english letters and words that have not been translated. the next preprocessing step is lemmatizing using farasa. lemmatizing is a crucial normalization step, that greatly reduces the size of the tf-idf vectors to avoid sparsity as much as possible.



light preprocessing was performed when running our experiments on bert and electra. the same cleaning process was used, but we have not performed any lemmatization. we argue that performance gains from lemmatization will be insignificant when fine-tuning transformer models, as they are usually pretrained on raw text. another basis for our choice is that byte-pair encoding is used when tokenizing input



multiple rnn-based models were used such as standard rnn, lstm, and bi-lstm. all models consist of 5 layers. the first layer consists of 128 units with a sigmoid activation function, followed by a dropout layer of 0.2. the third layer consisted of 64 units with a sigmoid activation function, followed by another dropout layer of 0.2. the fifth layer was a dense layer with 4 output units that correspond to the 4 possible classes in the dataset, with a softmax activation function. pretrained sentence embeddings were generated using universal sentence encoder.



authors in[7,8] introduce arabic versions for both bert and electra. they are pretrained on 77 gb of raw text. they used the arabic wikipedia dump from 2020/09/01, the 1.5b words arabic corpus, the oscar corpus, and the osian corpus.



another interesting observation is that upon inspecting our plots, we find that the validation loss always starts with a smaller value than the train loss. this is not the usual case when training a model, but we speculate that it could be due to the different setups between training mode and evaluation mode. both bert and electra use dropout as a means of regularization, which is used only while training. thus, resulting in a greater loss than the validation, as dropout is not used in evaluation. although, further investigations needs to be conducted. the optimal set of hyperparameters for bert was the following: a learning rate of 5e-5 was used. the batch size was set to 32 and the



we can observe that bert and electra perform much better than the baseline counterpart and show a significant improvement over the standard deep learning models. the mean qwk for the baseline model is 0.503 which is much lower than the suggested qwk value for automated systems, making it a subpar system that will probably not be of great interest to academic facilities.



the standard rnn, lstm, and bi-lstm have similar scores both when trained on each question and the whole dataset. the mean score when training on each question is around 0.65 and training on the whole dataset showed a better qwk score around 0.7.



both bert and electra showed a significant increase in performance both when trained on each question and the whole dataset. training on individual question resulted in a qwk score around 0.7, and training on the whole dataset at once resulted in a qwk score around 0.77.



in the presented work, different models were investigated to implement the grading system using an arabic translated version of the kaggle asap-sas dataset. firstly, the baseline experiments consisting of a tf-idf+ svm and cosine similarity reference-based system showed to be lacking in performance. we argue that the main reason for the reported results is due to the failure of the baseline methods to accurately represent the answers and build a relation between them.



rnn-based models showed a significant increase in performance compared to the baseline model. while lstms and bi-lstms usually perform better than standard rnn models, their scores are very similar to each other. this is probably due to the answers in the dataset being relatively short in length, making the standard rnn sufficient for the task.



transformer-based language models, namely bert and electra. accordingly, we report the best system for the task at hand which was achieved using electra with a qwk score of 0.78. our main contribution is investigating the first deep learning-based arabic short answer scoring system using deep learning pretrained language models, while also highlighting the advancements of recent arabic nlp tools that presented impressive performance on our task.



moreover, further pre-training language models on domain-specific corpora has shown to be of great potential, such as the work presented in where they further pretrained bert on educational content in english. the same can be done for arabic models by collecting arabic educational/scientific books as pretraining data and other models such as electra, resulting in a model that can be used in a wide array of educational applications. the only downside being the computational costs of pretraining.



