plug-in hybrid electric vehicles(phevs) showed significant improvement in fuel consumption and co2 emissions over the last decade. however, the performance superiority depends on the quality of the on-board energy management strategy(ems). the emerging technologies such as connected vehicles and automated driving increased the complexity of the ems control objectives. traditional hand-crafted rule-based approaches are robust, reliable and computationally affordable. however, they do not guarantee optimality with several objectives in a multi-domain, nonlinear and time-varying systems. accordingly, the need for more intelligent controllers becomes vital for future vehicles.



saving 10% in the fuel consumption in comparison to the charge-depletion charge-sustaining(cdcs) strategy. the aforementioned approaches offered a feasible solution to real-time hybrid control units(hcus), however the trade-off between control objectives, controller performance in real-time, tuning effort, and vehicle hardware capabilities are difficult to balance. reinforcement learning(rl)-based energy management approaches in phevs took immense attention of the artificial intelligence(ai) community due to the ability to learn control policies through interaction without being explicitly programmed on a certain strategy.



1) conventional drive(cd{0}) where ice is the only propulsion source and supplying the low voltage auxiliaries. 2) optimum generation(og{3}) where the demanded torque is low and the ice load point can be increased to a better fueleconomy location. the em works as a generator for charging the hv battery with the leftover power from the ice. 3) electric drive(ed{6}) which enables the em to propel the vehicle.



the agent collected 50 k experience tubles, following a random policy, which were postprocessed to accumulate the episodic reward and calculate the q-value for each tuble. afterwards, several nn architectures were trained and tested on different datasets in a supervised learning fashion to calculate the root mean square error(rmse) as training and validation errors. the aforementioned architecture was selected due to achieving the minimum rmse among others which indicates the best ability for the nn to learn and approximate the q-function efficiently.



the double deep q-network(ddqn) algorithm, introduced by hasselt et al., is used to reduce the observed q-values overestimation bias and stabilize the training by decoupling the action selection from the target q-value estimation using two different nns; policy and target networks. lillicrap et al. proposed an will be selected according to the rl policy. other approaches suggested including the agent only in the free modes and bypass it in the fixed modes segment. this approach was tested, and the agent faced difficulties in learning the correct value function that represents the future cumulative rewards, as the agent is not aware of part of such rewards if bypassed, hence it never converged. the advantage of our approach is that the agent will have full-observa bility/controllability over the environment even with the action



several scholars proved that advanced ai-based strategies such as reinforcement learning emss can significantly improve the fuel economy of phevs. this study introduced an adaptive online learning rl agent into the existing hcu architecture. dp results are used to benchmark the developed rl algorithms which solved the ems for near-optimal solutions. the development process began with formulating the control problem mathematically as an infinitehorizon optimal-control problem. the mathematical formulation is a function of the battery soc, driver torque demand, vehicle speed, remaining trip distance, and the engine on/off status. the objective is to minimize the total fuel consumption and the frequent engine switch. accordingly, the design process reasonably gration with the intelligent transportation systems(itss) to construct a smart city or a smart grid is coming soon with more comprehensive and complicated optimization control objectives. in the future connected environment, distributed and multi-agent drl systems are necessities for cooperative learning between vehicles on the road. the sooner the transition towards intelligent control systems by automotive oems, the better they are prepared and qualified for the upcoming challenges. the research presented an initial step overlooking the way towards realizing an intelligent adaptive energy management system for phevs. the upcoming research efforts shall investigate more the following spots:



the author would like to thank prof. thomas schlechter and prof. stefan winkler for their support during the master thesis research. moreover, sincere gratitude to the avl dsp team: patrick teufelberger, huan chen, evgeny korsunsky and bhargav adabala, for their collaboration and valuable feedback. furthermore, deep thanks to the editors and anonymous reviewers for providing insightful suggestions and comments to improve the quality of research paper.



