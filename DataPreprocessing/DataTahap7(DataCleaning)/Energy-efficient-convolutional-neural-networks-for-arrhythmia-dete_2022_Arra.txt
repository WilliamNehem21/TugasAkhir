be energy efficiently implemented in analog or digital hardware. the parameters of the 7 biquad blocks of the second order stage(sos) architecture can be determined to achieve the wanted transfer function of the bandpass filter. the high order of the filter enables a very good suppression of baseline errors and noise. the future hardware implementation will of course need a proper rearrangement of the second order stages to keep the amplitude in between the filter stages in a reasonable range.



considering the complexity of the input signals, the linear nature of the convolution cannot capture all the underlying information. therefore, the activation functions serve as a mapping of the previous layer to the next one in a non-linear manner. however, the application of multiple filters on the same input often dramatically increases the dimensions of the feature maps, thus the pooling operation is responsible of condensing the complexity of the cnn simply by down-sampling information. commonly, the generated features of the cnn are fed into fully connected layers with dense connections between them. the number of the layers, the kernel, pooling size and the number of nodes in the fully connected layers are some of the hyperparameters, that define the structure of the cnn and should be chosen appropriately with regard to network performance and learning ability.



using one of the cnn architectures for the classification of arrhythmia in a continuous fashion, for example 12 h while wearing a smart watch, would be very energy inefficient. in practice, it makes sense to limit the detection of arrhythmia to short repeated intervals, here we are using 7 s intervals, but this may differ. ideally, the classifier can decide about arrhythmia or non-arrhythmia, without exploring the whole 2 min.





chaur et al. generated similarly an 1d cnn for the detection of atrial fibrillation. the cnn architecture consists of 10 layers of convolutions followed by pooling operations and 2 fully connected layers followed by one softmax layer output. the number of filters at each convolutional layer varies in the range of 32 to 512, which results in 3,933,634 trainable parameters. this denotes 34,505 times more parameters than our proposed model 4.



in the present study, we are proposing energy efficient recurrent cnn architectures for long time series and our approach is tested on the detection of atrial fibrillation on ecg signals. our workflow suggests the development of lightweight, fully-segmented models with drastically fewer model parameters than previous studies. the inclusion of the energy consumption as an additional metric for the evaluation of the performance, allows us to generate architectures that can be easily embedded on physical small hardware devices.



tional layers with 1, 2 and 2 filters respectively and 1 fully connected layer. the total number of parameters of the model is 114, which is millions of times smaller than model sizes that others have suggested. after energy optimization our model achieved an accuracy of 95.3% on our test set of 4800 ecgs. the use of the optimal energy classifier permitted us to reduce the energy by 81% for the classification of 2 min signals. specifically, only an average of 3.09 signal segments of 7 s, or approximately 21 s, were needed for the classification of the whole 112 s. mistakes due to wrong segment-wise decisions are avoided by recurrently using the information of previous segments.



