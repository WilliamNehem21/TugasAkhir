the radical adoption of cloud computing technology has exposed a significant overhead in maintaining its infrastructure, which has become a major issue for the cloud providers due to the associated high operational costs, such as energy consumption. it has been stated that a data centre may consume about 100 times more energy than a typical office of the same size. so, efficiently managing the power consumed by the servers would improve the overall consumption; in the sense that as the servers consume less power, the heat generated by these servers would be reduced, which would then reduce the need for cooling resources that consume large amount of energy as well and result in more energy savings.



recently, some works have started to measure the energy consumption in more detail, like measuring energy consumption for each vm in a physical machine. research conducted in introduces a vm power model to measure the estimated power consumption of vm with using performance events counter. they argue that the results of their proposed model can get on average about 97% accuracy.



this proposed system architecture would have the rmu to dynamically collect the energy consumed by the hardware components and observe the number of assigned vms. after that, epu would have appropriate algorithms to calculate the energy consumed by each vm and hardware components, and it would then profile and populate these measurements as kpis to a database. this data can be further analysed by the reporting and analysis unit to provide the software developers energy-aware reports in order to enhance their awareness of the energy consumption when making programming decisions. for example, it might be interesting to know whether the cpu or the memory of the hardware component would consume more energy, so that the developer can create applications that would depend more on components with less energy consumption, without compromising performance.



from a hardware perspective, the leeds testbed is comprised of a cluster of dell commodity servers. for the purpose of this research, four of these were used with energy meters. each server consists of a four core x3430 intel xeon cpu, running at the default clock speed of 2.40ghz and a total of 8gb of ram(four modules of 2gb ddr3 at 1333mhz). additionally, each server utilised a single 3.5 inch western digital re3 250gb sata hdd(model: wd2502abys), with 16mb of cache and a spindle speed of 7200 rpm. the machines connect via gigabit ethernet using a broadcom pci-e nic(model: bcm95722). this connectivity provides shared access to a nfs share running on the cluster headnode. the nfs share is backed by four 500gb hdds running in raid 0, providing a total of 2tb storage for vm images.



the energy profiling unit has several key features; it is primarily aimed at reporting at both host and vm level energy usage data. these values are reported as: a historical log, current values or future predictions. the historical log provides values for energy consumed over a specified time period and the average power, while the current values report power alone. the future predictions are based upon linear regression of cpu utilisation vs power consumption during a training phase on a per host level. the profiling unit provides automatic calibration features to achieve this.



it is clearly shown that increasing the number of vms from one up to four vms in a single host has an impact on the overall power consumption for that host. the power consumption shows a linear growth with the increment of vms. increasing the number of vms means increasing the usage of physical resources, like cpu, disk, and memory, assigned to these vms. so, as more physical resources are used, the power consumption increases accordingly.



the results shown in this experiment 3 are similar to those shown in experiment 2; but here the results are shown for three physical hosts running simultaneously, whereas experiment 2 was run only on a single physical host. so, the power consumption in this experiment 3 increases linearly with the increment of vms running on each host. this experiment also shows that an application consisting a number of tasks can run across multiple vms hosted by different physical host machines at the same time.



