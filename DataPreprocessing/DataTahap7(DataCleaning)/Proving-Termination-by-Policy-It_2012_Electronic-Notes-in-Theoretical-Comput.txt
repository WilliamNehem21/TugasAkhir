in this paper, we explore the adaptation of policy iteration techniques to compute greatest fixpoint approximations, in order to find sufficient conditions for program termination. restricting ourselves to affine programs and the abstract domain of template constraint matrices, we show that the abstract greatest fixpoint can be computed exactly using linear programming, and that strategies are related to the template constraint matrix used. we also present a first result on the relationships between this approach and methods which use ranking functions.



this paper describes the use of policy iteration techniques to discover sufficient termination conditions. as a first work in this direction, we restrict ourselves to affine programs and to the template constraint matrices abstract domain, a sub-domain of polyhedra. policy iteration techniques were already used in this framework to approximate the set of reachable states, hence we need to adapt these results to greatest fixpoint computations.



we first present the relationships between approximating fixpoints and proving termination. then we give an overview of the policy iteration approach. in section 4, we explore the extension of these approaches to the computation of an abstract backward semantics designed to prove termination properties. finally we give a first result on the relationships between our approach and ranking function synthesis, showing that programs admitting a linear ranking function can be treated with our approach.



the three approaches are related: with a ranking function, one can prove formulas(1) and(2). reciprocally, proving formula(1) or(2) proves that a ranking function exists. in fact, some lower fixpoint induction methods(e.g. in[4, sect. 11]) directly use some kind of ranking function. however, if the approximation is proved with other methods, it may be difficult to make the ranking function explicit. hence, techniques which compute directly a fixpoint appear as interesting alternatives to infer termination properties.



the use of policy iterations(also called strategy iterations) to compute the least fixpoint of a self-map f in static analysis was first introduced in. the principle of this techniques is to describe f as the minimum(or the maximum) of a set s of simpler maps. a strategy(or a policy) is a selection of an element of s. the least fixpoint of this element is computed. if this fixpoint is a fixpoint of f, the algorithm terminates, otherwise a new strategy is selected during the strategy improvement step, and the algorithm iterates.



since our goal is to overapproximate greatest fixpoints, it seems more natural to approach them from above, hence to use a dual version of the second approach. in this paper, we mainly follow the method presented in and restrict ourselves to affine programs and template constraint matrix domains. before summarizing the method, we introduce a few notations.



this paper is intended to show how policy iteration techniques can be applied to termination analysis. we performed only a few experiments, and more comparisons with related work needs to be done. recently, bozga et al. presented several results on the decision of conditional termination. their framework is more restrictive(as they restrict themselves to integer variables), but may give more precise results as it is not limited by the precision of the abstract domain.



to improve the analysis, we can extend previous work on policy iterations for other abstract domains(e.g. quadratic zones). although we dealt only with affine programs, we can also add linearization and non-determinism to extend the framework. finally, an interesting feature of greatest fixpoint overapproximation, compared with least fixpoint overapproximation, is that the abstract domain can be refined during the computation. to refine the precision of the analysis, we can add new template constraints at any time, using for example decreasing iterations on the domain of polyhedra.



