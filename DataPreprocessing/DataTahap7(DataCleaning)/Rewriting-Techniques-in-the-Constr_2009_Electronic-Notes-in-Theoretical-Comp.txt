the constraint solver is a symbolic cryptographic protocol security analysis tool that is based on a unique term rewriting approach. several of the design characteristics of this tool, and the reasons for them, are discussed and placed in perspective with other approaches. the constraint solver uses a free message algebra and a bounded-process network model with a dolev-yao attacker. these choices yield simplicity and decidability.



in the last few years, considerable effort has been put into reconciling symbolic and computational approaches, beginning with the abadi and rogaway paper. the idea is to prove that symbolic conclusions are valid if the real cryptographic operations satisfy sufficiently strong properties. a fine-grained categorization of relevant security definitions is given in, including such properties as indistinguishability under chosen-ciphertext attacks. a library of operations proved to have strong enough properties has been proposed. as pointed out by blanchet, however, using strongly secure libraries is not as easy as one might hope, because the operations in them do not exactly match the ones axiomatized in most symbolic models, and additional assumptions and restrictions might apply to the



the goal of proving protocols correct was carried on in the second thread, using various types of state-transition specifications of protocols as the object of inductive proofs that data was shared properly among authorized participants. such proofs must take into account attacker actions and multiple sessions. both hand proofs and software tool-supported proofs were demonstrated; see, for example,[9,41,44,46,13]. it turns out that a rigorous security proof for a protocol is a long, arduous, errorprone procedure, with a similar flavor to a program proof. the first tool-supported proof that was well enough explained to encourage imitation using other provers was by paulson.



a few researchers hit upon the idea of using a model checker to explore the state space of a cryptographic protocol. roscoe might have been the first. using a model checker generally means making some approximation or imposing some theoretically severe limitation on the protocol and environment model. but this disadvantage is counterbalanced by the ease of specifying the protocol and the ability to explore modifications in the protocol and environmental assumptions quickly and easily. and over the years, model checkers and analysis techniques have improved. model checking runs took an hour or so at first, which was still better than the days needed to update an inductive proof after a modification, but now a few seconds will usually do.



model checking in the general sense was being done from quite early on, in the sense that there were specialized tools for exploring the protocol state space. the real boost for protocol analysis came from the use of general-purpose model checkers built for other applications. some early examples of this approach are[29,39,16]. this way, many groups of researchers could use well-supported, or at least locally supported, tools that they already had, rather than having to import experimental software with which they were unfamiliar. also, the limitations imposed by practical model checking were clear and easily understood, while specially designed analyzers were too complex to understand exactly what attacks they might miss. in fact, the use of general-purpose model checkers had the side effect of raising the standards for symbolic state-exploration protocol analysis, so that newer specialized tools and newer versions of older tools are much improved in their conceptual clarity.



a rule to derive m from mpk(e) is always assumed, since e is the attacker. so this constraint is solved. but the solution is trivial, because it says only that the initiator has deliberately chosen to share m with e. this is not a compromise.



we can avoid this spurious solution by introducing another constant b to instantiate b, making b distinct from a and from the attacker e. in effect, we have had to pin down the secret m and the two principals who are expected to share it, a and b. and we did this by instantiating parameters of only that role instance that created the secret m.(we might also want to consider a scenario with a= b= a, since that is not excluded by the protocol specification.)



with the starting substitution a= a, b= b, m= m, are the two constraints solvable? we leave it to the reader to check that they are, by setting a'= e, b'= b, m'= m. there is no reason to exclude this solution. it reflects a masquerading attack by e against b, which works because the message received by b is not authenticated.



the branching means that the solver performs a tree search to find a solution. it stops at the first solution. if it does not find one after searching the whole tree, it reports failure(which is often a good thing, when a solution demonstrates a compromise).



be derived from any nonempty t. constraints like this are called simple and are not transformed, even by unification. a constraint system that is either empty or has only simple constraints is solved, since there is at least one successful substitution for all the remaining variables.



as simple as the constraint solver is, there are several design choices in it that have a significant effect, and such choices are made by other protocol analysis tools as well. the bounded-process limitation has already been mentioned. there are others that the reader has probably noticed that merit some discussion.



to compromise m, the attacker records the first message from a, and substitutes it for m in a new session between the attacker e and b. then b returns[[m pk(b), a]pk(e), b]pk(e) to e. the part of the reply encrypted by pk(b) is then fed back by e to a new instance of b, who re-encrypts it for e.



the constraint solver does not distinguish different subsorts of the message sort. although we talk about principals and public keys, each operation accepts any message for any of its arguments. there are two encryption operators, one for public-key encryption and the other for symmetric-key encryption.



this could be viewed as a type flaw, since a is asked to accept[nb, b] as a nonce. but the vulnerability also hinges on recognizing[x,[nb, b]] as a binary concatenation. if the protocol had been specified to use a ternary concatenation[na, nb, b] in the second message, the attack fails, because a is expecting a binary concatenation in the first message of the responder role.



does not introduce extra vulnerabilities if all encryption is received with context. in other words, if every encrypted term has some structure(i.e., it is not just a variable), there are no vulnerabilities that are hidden from analysis using a free algebra. this condition was called ev-freedom. the result in was proved only for symmetric encryption. it was extended to public-key encryption(with pevfreedom) by lynch and meadows.



the msr(multiset rewriting) notation is a formal meta-notation motivated by a linear logic concept. the multiset is a global bag of facts about the current state of each role instance and the attacker knowledge. additions to attacker knowledge and replacement of role instance states are both easily represented as term rewriting rules. linear logic offers a special formalism for uniquely originating nonces, using an existential quantifier that is assumed to generate new symbols.



current frontiers in protocol analysis are in three main directions. one is to support a wider choice of encryption operators. much work has been done already to handle exclusive-or encryption and diffie-hellman key distribution, based on modular exponentiation[31,24]. these operators have a commutative character that is challenging to handle algebraically in an efficient way. two of the more recently considered unusual operators are bilinear pairings, as used in elliptic curve cryptography, and zero-knowledge proofs.



