mlc is divided into two classification types: flat and hierarchical. in flat classification, a set of predefined labels are classified without considering the hierarchy of the relationship between the labels. in contrast, in hierarchical multi-label classification(hmc) a single instance may have multiple labels concurrently, and these labels are structured in a hierarchy. this type of classification, presents a more complex classification problem than flat classification, given that the classification algorithm has to take into account hierarchical relationships between labels and be able to predict multiple labels for the same instance.



mlc can be divided into flat and hierarchical classification. flat classification can be conducted using a problem transformation(pt) or algorithm adaptation technique. a pt technique simply aims to transform mlc problem into single-label problems, after which a traditional single-label classification algorithm is used to perform the classification task. an algorithm adaptation technique is concerned with adapting single-label classification algorithms to deal with the mlc problem directly. examples of such techniques include a multi-label lazy learning(ml-knn) algorithm and a multi-label decision tree(ml-dt) algorithm.



the pt technique includes two general methods. the first of which involves transforming the multi-label problem into a set of binary classification problems. examples of this method include binary relevance(br), classifier chains(cc), and ranking by pairwise comparison(rpc). the second method converts the multi-label problem into a multi-class classification problem. examples of this method include label powersets(lp), pruned sets(ps), and random k-labelsets(rakel).



on the other hand, hmc is considered an extension or variant of mlc in which a hierarchical structure is considered on the multilabels. the output of the classification algorithm for a given multi-label problem is a set of labels structured in a hierarchy. the hierarchical multi-label problems are classified typically as a tree-hierarchy or a directed acyclic graph(dag). generally, the classification algorithms proposed for hmc are more capable of dealing with large sets of labels than those proposed for flat classification.



ahmed et al. conducted a study on binary and multi-class classification transformation methods using several multi-label classifiers. they transformed the mlc of arabic data into singlelabel classification using meka2 tool to implement lp, br, and ranking and threshold-based(rt) approaches. the standard single-label machine learning algorithms applied as base classifiers were: a support vector machine(svm), a k-nearest neighbor(k-nn) algorithm, a naive bayes(nb) method, and a decision tree(dt). the collected dataset on which the evaluation was performed consisted of 10,000 news articles classified into five labels(sports, arts, economy, politics, and science). the results of the evaluation showed that using svm as a base classifier with the lp method achieved the best ml-accuracy with 71%.



different sets of single-label machine learning classifiers including svm, nb, and k-nn. the evaluative experiments were performed using the same dataset collected in. the experiment results showed that using a br method consisting of various sets of single-label classifiers(svm, nb, and k-nn) achieved the best results.



shehab et al. conducted a study that focused on arabic news articles. three multi-label classifiers were adapted to deal with mlc problems: random forest(rf), dt, and k-nn with k= 5(5-nn). the researchers conducted experiments on a collected dataset of 10,997 news articles classified with multiple labels, such as economics, sports, world, middle east, science& technology, and miscellaneous. the evaluation results showed that the dt classifier achieved a better performance than the rf and 5-nn classifiers.



al-salemi et al. conducted a study that sought to investigate mlc problems by conducting an in-depth comparison of the most common mlc algorithms used in the pt method, such as br, cc, lp, and calibrated ranking by pairwise comparison(crpc). these methods were trained using three base classifiers(svm, knn, and rf). four algorithm adaptation techniques were also evaluated. these were: ml-knn, rfboost, binary relevance knn(brknn), and instance-based learning by multi-label logistic regression(iblrml). the algorithms were evaluated using the rtanews dataset3 which is a multi-label arabic dataset of 23,837 arabic news articles distributed over forty categories. a comparison was performed to investigate the effectiveness of the introduced dataset(rtanews) in mlc tasks. the experiment results showed that both rfboost and lp with svm outperformed the other mlc algorithms. moreover, the algorithm adaptation methods performed faster than the other pt algorithms except for the lp method.



elnagar et al. conducted a study that introduces two new arabic datasets for both single-label and multi-label text classification tasks. the datasets called sanad(single-label arabic news articles dataset) and nadia(multi-label news articles dataset in arabic). both datasets were collected from news sources and available online on mendely.4 further, they conducted an extensive comparison of several deep learning models, to investigate the effectiveness of the introduced datasets on the arabic text classification tasks. the results showed that all models achieved good results when evaluated using sanad dataset, cgru(convolutional gated recurrent unit) achieved the lowest accuracy of 91.18%, whereas hangru(hierarchical attention network-gated recurrent unit) achieved the highest performance of 96.94%. concerning nadia dataset, hangru has the best overall accuracy of 88.68%.



notably, most previous research has focused mainly on flat mlc methods, and to the best of our knowledge, only one study has investigated dealing with hmc problems in arabic texts using machine learning. this study employed a homer algorithm with its default classifiers(br and nb classifiers). the study did not investigate empirically the impact of feature selection methods and feature set dimensions on the model. in addition, the dataset used in that research has not been published online.



there is a lack of publicly-available multi-labelled arabic datasets, especially those with hierarchical multi-labels. for this reason, the dataset used in this study was a raw hierarchical multilabel arabic dataset taken from a study performed by zayed et al., which was applied in the same domain.



stemming. stemming aims to stem or return the derived features to their roots or stems. transforming semantically similar features to their root form reduces the feature space, decreases the morphological variance of words, and improves the classification performance of the model[44,45].



information, which may have affected classification performance. finally, stemming was carried out using a snowball stemmer. snowball is a small text processing language used to create the stemming algorithm, it supports many languages including the arabic language. it aims to return words to their stems by stripping off common prefixes(e.g.,) and suf-



tion performance by increasing computational complexity. therefore, an additional feature selection method was needed to reduce the features number by selecting a subset of the most relevant and highest-ranking features. using such a method effectively reduces the dataset dimensionality by removing irrelevant or redundant features without decreasing the classification performance. hence, it can improve the learning process and reduce computational complexity.



the most relevant features, the standard method of feature selection in multi-label data, as illustrated by spolaor et al., is based on applying a pt classifier(e.g., lp, br, cc) to transform mlc problems into single-label problems, after which a traditional feature ranking method can be employed to perform the feature selection. one of the most common methods of multi-label text classification is using a br method to determine the discriminative power of each feature concerning each label independently of the rest of the labels. after this, the computed scores are aggregated to obtain



in the mlc context, each combination of pt classifier and feature ranking method is considered a separate feature selection method. thus, this study used an abbreviation of the pt classifier followed by a feature ranking method abbreviation separated by a hyphen symbol to denote the feature selection method in the multi-label classification context. for example; lp-ig referred to an independent feature selection method consisting of an lp classifier and ig feature ranking method, respectively.



the feature selection phase was examined by investigating different sets of feature selection methods and examining different sets of the high-ranking features, to obtain the best method and feature set dimensions for optimizing the model performance. the details of this phase are presented in the results and discussion section(see section 5.1 and section 5.3).



this paper investigates applying the homer algorithm in a domain with a large set of labels such as the islamic context. the homer algorithm is an effective hierarchical multi-label classifier that uses a divide-and-conquer approach. it can efficiently handle mlc problems with a large number of labels by constructing a tree-shaped hierarchy of simpler mlc problems. first, homer automatically organizes the large set of labels into a tree-shaped hierarchy. this is accomplished by applying a clustering algorithm which repetitively partitions the labelset into a number of nodes(clusters). then, it employs a multi-label classifier(e.g., br) for each node, where each classifier can handle a small number of labels instead of handling a large labelset.



of all nodes in the tree. a multi-label classifier s was employed for each node to predict the meta-labels of its children. the default structure of the homer algorithm was implemented by employing a br-nb multi-label classifier and balanced k-means clustering algorithm. where the abbreviation br-nb refers to the br multilabel classifier implemented using nb base classifier.



mlc models are evaluated using evaluation metrics that are commonly used in the mlc field. several multi-label evaluation metrics were proposed which divided into two main approaches: example-based(instance-based) and label-based metrics. the first approach is measured for each test instance and then averaged over all test instances. whereas the second approach is measured for each label and then averaged over all labels.



the most common example-based metrics that are used to evaluate mlc models are described in the following. suppose that: m refers to the total number of instances in the test dataset, i indicates an instance in the test dataset(where 1 6 i 6 m), l= kj: j= 1... q is the set of labels, where q is the total number of labels, zi and yi refer to the predicted and actual labels, respectively.



the primary aim of this work was to incorporate the preprocessing techniques, feature selection methods, and homer algorithm to obtain a competitive hmc model for the arabic language. thus, several experiments were conducted, which primarily focused on investigating the impact of the feature selection method and the dimension of the selected feature set on the classification performance of the model. in addition, the effect of the homer parameters(multi-label classifier, clustering algorithm, and the number of clusters) on the model performance were also examined.



the dataset was prepared in arff file format, along with the xml file which are compatible with the mulan tool. as the purpose of this study is to address the lack of available multi-label arabic datasets, the processed version of the dataset used in the experiments has been made publicly available online.6 it consists of 26,470 text instances distributed over 578 hierarchical multilabels. it also contains different sets of high ranking features



each feature selection method was used to select a subset of high-ranking features from the total number of features(11,000). accordingly, the classification model was evaluated using the 2000 features selected by each feature selection method. also, the homer was implemented using its default multi-label classifier(br-nb) and balanced k-means algorithm with four clusters.



the second experiment sought to optimize the homer algorithm by investigating different sets of multi-label classifiers in order to examine their effect on the predictive performance of the model. three pt methods were employed as multi-label classifiers since they are among the most widely-used methods of the pt technique. these were: br, cc, and lp classifiers. the pt methods were employed with three base classifiers(nb, svm and j48). accordingly, homer was run with each different multi-label classifier set(br-nb, br-j48, br-svm, cc-nb, cc-j48, cc-svm, lp-nb, lp-j48 and lp-svm).



shows that outcomes obtained for feature sets ranging from 1000 to 8000 features were approximately the same, with small differences in favor of the 4000-feature set, which obtained the best results in terms of hamming loss, h-loss, ml-accuracy and micro-averaged f-measure. in the average ranks, the 4000 and 5000 features obtained the highest value, which was 2.29. consequently, the 4000-feature set was employed with the evaluated models in order to avoid model overfitting and improve predictive performance.



homer variations. then, the three clustering algorithms were compared with a high number of clusters(k= 8 clusters). label distribution performed using the balanced k-means algorithm obtained the best results for hamming loss(0.0045) and ml-accuracy(0.7581), random clustering performed best regarding hamming loss(0.0045), h-loss(1.7149), micro-averaged precision(0.8832), and micro-averaged f-measure(0.8536), and the k-means algorithm obtained the best result for subset accuracy(0.302).



in the average ranks, the balanced k-means algorithm scored the highest(2.57), followed by random clustering and the kmeans algorithm with 2.71 and 3.57, respectively. this indicates that performing even label distribution using a balancing clustering algorithm(balanced k-means) is more effective in this domain than performing label distribution using a simple clustering algorithm(k-means). as stated in, this may be because balanced clustering is more useful in domains with a large number of labels, and balanced k-means clustering algorithm with eight clusters. in addition, the br, cc, and lp models were implemented using svm as a base classifier. furthermore, the fatwa model was implemented, in accordance with, by running the homer algorithm using the br-nb multi-label classifier.



moreover, cc suffered from random order chain. the results also indicated that the chaining property in cc may cause error propagation at classification time. furthermore, lp experienced an exponential increase in computational complexity in relation to the number of labels. additionally, it could only predict labelsets observed in the training data and it also suffered from model overfitting.



in contrast, the hmatc, which was built based on the homer algorithm, efficiently dealt with a large number of labels by constructing a tree-shaped hierarchy of simpler mlc problems, in which the root node was constructed to contain all labels in the tree. then, the balanced k-means algorithm was employed to distribute large labelsets into balanced clusters that represented new nodes. for each cluster, the lp-svm multi-label classifier was employed(since it achieved the best results) to handle labels in that cluster only. if the predicted label was in the meta-labels of the child node, only the multi-label classifier of that node was activated.



furthermore, the effect of each different set of methods was investigated in each phase. then, the hmatc model was implemented using the methods which obtained the best performance. this led to improved model performance and reduced total computational cost.



the researchers intend to extend this work in the future by investigating the hmatc model with other multi-label classifiers, such as rakel, rpc, and ps. they also aim to apply different structured methods for selecting the number of clusters in the clustering algorithm(k). regarding the feature selection phase, the authors intend to examine a principal component analysis(pca) method instead of transforming the multi-label problem using a pt method. finally, they plan to give more attention to the preprocessing phase by employing word embedding techniques like word2vec and fasttext as text representation methods, applying different stemming algorithms, and preparing a list of arabic human names so they can be easily removed during the stop word removal phase.



nawal aljedani received the b.s. and m.s. degrees in information technology from king abdulaziz university, jeddah, saudi arabia. she is currently a collaborative teaching assistant with the faculty of computing and information technology, king abdulaziz university, jeddah, saudi arabia. her research interests include machine learning, data mining, natural language processing, and multi-label classification.



reem alotaibi received the ph.d. degree in computer science from the university of bristol, bristol, u.k., in 2017. she is currently an assistant professor with the faculty of computing and information technology, king abdulaziz university, jeddah, saudi arabia. her research interests include machine learning, data mining, and multi-label classification.



mounira taileb received the ph.d. degree in computer science from the university of paris-sud, paris, france, in 2008. she is currently an associate professor at the faculty of computing and information technology, king abdulaziz university, jeddah, saudi arabia. her research interests include image retrieval, image annotation, machine learning, data mining, and text classification.



