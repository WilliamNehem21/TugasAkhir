to better understand real-world failures and the potential limitations of state-of-the-art tools, we first conduct an empirical study on 277 user-reported storage failures in this paper. we characterize the issues along multiple dimensions(e.g., time to resolve, kernel components involved), which provides a quantitative measurement of the challenge in practice. moreover, we analyze a set of the storage issues in depth and derive focus on measuring the observations that the tools enable developers to make(i.e., observability), and derive concrete metrics to measure the observability qualitatively and quantitatively. our measurement demonstrates the different design tradeoffs in terms of debugging information and overhead. more importantly, we observe that both tools may behave abnormally when applied to diagnose a few tricky cases. also, we find that neither tool can provide low-level information on how the persistent storage states are changed, which is essential for understanding storage failures. to address the limitation, we develop lightweight extensions to enable such



in addition, we find that neither tool can directly provide low-level information(e.g., storage device commands) on how the persistent storage states are changed, which is crucial for understanding host-device interactions in the storage stack. to address the limitation, we explore different ways to extend both ftrace and panda, and shows that it is possible to enhance both of them with such low-level observability without relying on special hardware(e.g., bus analyzer[36,37]).



complementary to the existing efforts, we focus on the effectiveness of failure diagnosis. specifically, we propose to measure the observability of debugging tools, which is a concept proposed recently for improving system reliability. the observability includes three desired properties(i.e., visibility, repeatability, and expressibility) for debugging failures. intuitively, the concept describes the observations that a tool allows the developers to make, which is critically important for debugging. while the concept of observability is well known, there is no practical methodologies to measure it to the best of our knowledge. we demonstrate how to measure the observability using realistic cases and concrete metrics in this work.



threats to validity. the characterization results presented in this section should be interpreted with the methodology in mind. in particular, the dataset was refined via critical keywords and manual efforts, which might be incomplete. also, only a limited subset of linux end users are aware of linux bugzilla and only a limited subset of them would report issues. therefore, it is likely that there are more storagerelated issues occurred in the wild but not captured in this study. nevertheless, we believe our effort is one important step toward addressing the challenge.



fourth, 37 out of 136(26.3%) resolved issues involve multiple os distributions or kernel versions. the manifestation symptoms of the issues often differ on different systems, which suggests that the software environment(e.g., kernels, libraries) is critically important for reproducing the failures for diagnosis.



fifth, only 5 out of 136(3.7%) resolved issues were caused by hardware. this implies that software remains the major source of storage failures, which is consistent with previous studies. also, it suggests that observing the behavior of the storage software stack is critically important for failure diagnosis.



both ftrace and panda may provide useful information for the majority of the cases evaluated. on the other hand, both of them may behave abnormally when diagnosing some tricky cases. we elaborate on the experimental results of ftrace and panda in section 4.1 and section 4.2, respectively. in addition, we find that both tools fall short of providing low-level information on how the persistent states are changed. we discuss our extensions to improve their low-level observability in section 4.3.



panda(platform for architecture-neutral dynamic analysis) is an open-source platform for program analysis. by leveraging virtualization(i.e., qemu) and the llvm compiler infrastructure, panda can help understand the behavior of the entire storage software stack. we focus on measuring its major feature(i.e., record& replay) and 4 related plugins(i.e., show instructions, taint analysis, identify processes, process-block relationship) in this subsection because they are most relevant for diagnosing storage failures.



since panda records the full state of a target system hosted in qemu as well as all non-deterministic events in snapshots, it can achieve full-stack, all-instruction observability by design(i.e., all executed instructions are observable by replaying). therefore, we do not calculate the function-based metrics as used in measuring ftrace(section 4.1). instead, we qualitatively measure if the target features are applicable in diagnosing failures.



level of information communicated between the storage software and the storage device, i.e., the device commands(e.g., scsi). such command-level information is valuable in diagnosing storage failures because the persistent storage states are changed by the device commands directly. traditionally, bus analyzers[36,37] are used to capture such command-level information. but as mentioned in section 2.2, bus analyzers are hardware-based tools which are not as convenient as software tools. we introduce software extensions to capture bus-analyzerlike command information and enhance the low-level observability of both ftrace and panda in this subsection.



16 bytes from the buffer to the command log and use the opcode to identify valid bytes. qemu classifies scsi commands into either direct memory access(dma) commands(e.g., read_10) or admin commands(e.g., verify_10), and both are handled in the same way in our extension since they share the same data structure.



we focus on measuring the observability of the debugging tools in this work because this is one of the most important metrics for debugging failures. we envision many opportunities for further improvements based on the initial effort. for example, we recognize that observability is only one desired property proposed recently for improving system reliability. there are other important properties from the modern cloud-native environment which supports looselycoupled microservices and enables flexible integration of monitoring, tracing, logging, etc. services for observability, the storage stack in the monolithic linux kernel has more constraints. how to improve the observability for the monolithic kernel with minimal intrusion remains an open question. our effort on measuring the observability of state-of-the-art tools is one first step toward addressing the challenge. finally, we would like to point out that our goal of measuring the observability of different debugging tools in this work is not to imply which one is better. instead, we hope to identify the potential limitations of the state-of-the-art tools in the context of diagnosing realistic storage failures, and inspire further improvements to address



coincidentally, there is an early work by lu et al. which is called bugbench. the authors collected 17 bug cases in c/c++ applications, and proposed to evaluate bug detections tools based on the bug cases. different from bugbench which includes applicationness, which is different from traditional performance-oriented metrics. therefore, we view the two efforts as complementary to each other. we hope that by reviving the concept of benchmarking for observability and other important reliability-oriented properties of computer systems, this work will inspire follow-up research and help improve the robustness of systems in general.



6 linux file systems and identified evolution trends; lu et al. studied 105 concurrency bugs from 4 applications and identified a number of common bug patterns(e.g., atomicity-violation and orderviolation); duo et al. studied 1350 pm-related kernel patches and identified a number of pm bug characteristics including pm patch



to the best of our knowledge, this work is the first effort to measure the observability of debugging tools. the work demonstrated in this paper suggests many opportunities for further improvements such as reproducing and packaging other types of bugs cases, deriving additional metrics for other desirable system properties, and measuring other tools or features, which we leave as future work. we hope that our initial effort will inspire follow-up research in the communities and help measure and improve the robustness of computer systems in general.



we thank the anonymous reviewers for their insightful feedback. we also thank researchers from western digital including adam manzanares, filip blagojevic, qing li, and cyril guyot for valuable discussions on the internals of the storage stack and the latest storage technology. in addition, we thank wei xu, om rameshwar gatla, prakhar bansal, runzhou han, and philip ma for the help on reproducing and/or analyzing failure reports and bug patches. this work



kang-deog suh, byung-hoon suh, young-ho lim, jin-ki kim, young-joon choi, yong-nam koh, sung-soo lee, suk-chon kwon, byung-soon choi, jin-sun yum, jung-hyuk choi, jang-rae kim, hyung-kyu lim, a 3.3v 32mb nand flash memory with incremental step pulse programming scheme, in: ieee j. solid-state circuits(jssc), 1995.



haryadi s. gunawi, thanh do, agung laksono, mingzhe hao, tanakorn leesatapornwongsa, jeffrey f. lukman, riza o. suminto, what bugs live in the cloud? a study of issues in scalable distributed systems, in: proceedings of the acm symposium on cloud computing, socc, 2014.



haryadi s. gunawi, mingzhe hao, riza o. suminto, agung laksono, anang d. satria, jeffry adityatama, kurnia j. eliazar, why does the cloud stop computing? lessons from hundreds of service outages, in: proceedings of the acm symposium on cloud computing, socc, 2016.



