most existing methods for handwriting recognition use two stages: text segmentation and text recognition. the first stage involves segmenting the text using a hidden markov model(hmm)[23,24], while the second stage was focused on text recognition. although each stage could produce good results independently, they have some fundamental flaws. for instance, manually constructing ground truth segmentation and transcription labels at the line level is costly and time-consuming. moreover, any segmentation mistakes can lead to recognition errors,



the study conducted experiments on three public datasets of handwritten paragraphs, rimes, read 2016, and iam. it achieved competitive results with state-of-the-art models that use ground-truth paragraph segmentation. as for the rest of the paper, section 2 discusses related methods and modeling choices. section 3 describes the proposed modification and provides system details. section 4 presents the experiment results. section 6 presents a brief discussion that compares the proposed system with other methods, suggests potential improvements, and discusses the challenge of applying it to full documents.



bluche and messina, and puigcerver followed the trend toward a more parallel architecture by replacing the mdlstm encoder with cnn while still relying on ctc. the segmentation and recognition text line tasks are presented as two distinct networks by inspired by object recognition methods. by focusing on modular techniques, the authors proposed a pipeline and described it in detail in. the passages distinguish handwritten text areas, apply object recognitionbased word-level segmentation is made, and the words are arranged logically. in contrast to, which focuses on line-level recognition, the authors of, taking an approach similar to that of, offers an end-to-end paradigm based on word-level recognition rather than line-level recognition.



to train directly on paragraph images. whereas most existing htr models need prior image segmentation to extract text components such as characters, words, and lines, these approaches have several flaws. these methods of image-text segmentation rely on heuristics or feature engineering that break under severe data changes. many segmentation-based methods help correct slanted text, curvature, and bold, using wrong properties and heuristics. more critically, it is challenging to separate text units cleanly in real-world handwriting. lines, for example, may be warped or merged with non-textual symbols and other visual elements. the literature has further information on these limitations[25,26]. synthesizing a general transcription from external transcribed text segments considers a different technique that might introduce errors and decrease performance.



need stitching because text portions are typically concatenated with a space or new line. thus, the formatting and indentation may be lost. this limitation indicates that the smaller text size may overlap and intersperse with the more significant parts of information that have not been transcribed. luckily, the proposed model avoids the issues by implicitly processing and learning from data end-to-end. so, our proposed model is easy to implement with the best choice of hyperparameters. they can jointly perform segmentation and recognition tasks using cutting-edge methods of training and searching. without human intervention, the training is carried out automatically on paragraph images.



the encoder and decoder modules are coupled with a multi-head self-attention mechanism and positional encoding. this enables the model to focus on the current time step on specific feature maps. by leveraging joint attention and a segmentation-free model, the neural network computes split attention weights on the visual representation, allowing for implicit line segmentation. images are processed using a cnn for features extractor, then the transformer encodes and decodes these features using multi-headed self-attention layers. a joint attention module decodes the paragraph image at the character level. the proposed method uses the backbone resnest50 as a convolutional network to extract the 2d features representation. before feeding data through a transformer encoder, we add the 2d positional encodings to the feature vector. we applied four encoders and six decoder layers; each has self-attention and feed forward neural network(ffnn).



positional encodings are generated and concatenated with the feature maps to capture spatial information. the concatenated feature maps and positional encodings are input to the transformer, comprising encoder and decoder components. the transformer utilizes multi-head self-attention mechanisms in the encoder and decoder blocks to capture dependencies between features and positional encodings. the target text is embedded and processed through a 1d positional encoding layer. the decoder applies multi-head self-attention to the feature maps and positional encodings. the final output is the predicted text.



to sknet, the split is accomplished by performing convolution with various kernel sizes to select the optimal kernel size channel-wise for each. resnest, conversely, does convolution with kernels of the same size as a branch selection. several models are generated and then assembled using divided attention by resnest.



the iam handwriting dataset is a collection of handwritten text documents widely used for research in handwriting recognition. it consists of more than 1500 pages of handwritten text, including various handwriting styles and languages. the documents in the dataset have been manually transcribed and annotated, making it a valuable resource for training and evaluating handwriting recognition algorithms. in our study, we utilized this dataset which consists of handwritten copies of text passages taken from the lob corpus. the dataset includes grayscale images of english handwriting at a resolution of 300 dpi. it provides segmentation at the page, paragraph, line, and word levels and their corresponding transcriptions.



the read2016 handwriting dataset was proposed for the icfhr 2016 competition on handwritten text recognition. it comprises a subset of the ratsprotokolle collection used in the read project and includes color images of early modern german handwriting. the dataset provides segmentation at the page, paragraph, and line levels. we follow the preprocessing steps of, so we eliminated the charevaluation. however, for comparison with other works, we utilized the last 100 training images for validation, as is commonly done. the dataset provides segmentation and transcription at the paragraph, line, and word levels, and we used paragraph segmentation levels in this study.



the proposed method utilizes a resnest50 convolutional neural network and a transformer network to learn the underlying structure of input paragraph images and their corresponding transcriptions through supervised learning. during the training phase, the model is trained on a training dataset, while in the testing phase, the trained model weights are used to predict transcriptions for a testing dataset. the model is designed to take paragraph-level images as input and is trained endto-end on a given dataset. the model weights are then saved based on the optimal training period and used by the paragraph recognition model to extract features. this method offers a powerful approach to handwriting paragraph recognition, leveraging convolutional and transformer-based architectures to capture complex features in the input images and corresponding transcriptions.



