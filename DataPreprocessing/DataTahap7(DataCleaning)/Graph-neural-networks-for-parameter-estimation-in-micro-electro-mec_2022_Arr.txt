micro-electro-mechanical systems(mems) are of great importance in a broad range of applications including vehicle safety and consumer electronics. during the testing of these devices, large heterogeneous data sets containing a variety of parameters are recorded. aiming to substitute costly measurements as well as to gain insight into the relations among the measured parameters, graph neural networks(gnns) are investigated. thus, the questions are addressed whether for inference of mems final module level test parameters, working on graph structures leads to an improvement of the predictive performance compared to the analysis via standard machine learning approaches on tabular data and how the graph structure and learning algorithm contribute to the overall performance. to evaluate this, in an empirical study different graph representations of the acquired test data were set up. on these, four different state-of-the-art gnn architectures were trained and compared on the task of raw sensitivity prediction for a mems gyroscope. whereas the gnns performed on par with a light gradient boosting machine, neural network and multivariate adaptive regression splines model used as baseline on the complete data set, in the presence of sparse data, the gnns outperformed the baseline methods in terms of the overall root-mean-square error(rmse) and achieved distinct improvement in the maximum error when trained on data with similar sparsity rates as observed during the validation.



the rest of the paper is organized as follows. section 2 contains related work on predictive modeling in mems and integrated circuit(ic) testing and graph-based representation learning in the context of manufacturing and testing. section 3 provides a short introduction to learning on graph structured data in general and to the considered gnn architectures. in section 4, the use case of sensitivity estimation during ft and the experimental setup are specified. section 5 describes the results which are then discussed in section 6. section 7 provides the conclusion.



manufacturing in general offers a wide range of applications for graph-based methods. components get assembled from sub-components, which can be represented as entities within graphs as well as the single process stages. a general overview of potential applications of graphs in manufacturing with focus on process and assembly planning is given by weise et al.. transferring methods from graph theory



another representation of relational information are knowledge graphs. especially for data sets with numerous types of entities and relations, setting up triplets of two entities connected via a relation is common. however, learning methods operating on numerical attributes in knowledge graphs are scarce[34,35]. as knowledge graphs can be reformulated in the graph schema defined above and most common gnn methods operate on the latter, knowledge graphs and their specific learning methods are not regarded further within this paper.



a normalized adjacency with self-connections. symmetric-normalized aggregation is applied to avoid numerical instabilities that can arise during the training process on graphs with a wide range of node degrees[32,40]. however, whereas countering the risk of overfitting, this self-loop update prevents the distinction between information of the considered node and that of neighboring nodes. gcns can also be reformulated as spatial method, where the features of a node neighborhood and of the node under consideration are aggregated via mean pooling[32,39]:



the nodes and/or edges taking the aggregated information as well as the features of the own instance or relation into account. the aggregation function might simply average the features, but it might as well be provided by recurrent neural network units or other types of nns[32,39]. a similar variety exists for the combination function, which can be realized as a non-linear activation function, a weighted sum or others as long as the function is permutation invariant and invariant to the amount of input nodes[24,32,46,47]. in a general form, the message passing scheme can be formalized as:



nation function evaluations defines the number of layers in the gnn. the more iterations are carried out, the more information from distant nodes is propagated to the nodes of interest. however, it has been shown, that the use of too many layers often leads to overfitting and, therefore, the number of iterations is often limited to two or three layers in practice[48,49]. finally, the last step constitutes the readout of the feature vectors of interest.



to keep models simple for the evaluation of the case study and to limit the computational effort during the search for the best suited graph variant, package, asic and process information were disregarded. additionally, in a first step the feature set was reduced to five wlt and ft parameters selected by the highest importance scores of the light gradient boosting machine(light gbm) used as baseline. on these simplified graphs the four gnn architectures gcn, gat, rgcn and hgt were compared. the best performing pair of graph variant and gnn architecture was then evaluated further by adding supplementary information to the graph structure and by investigating the effects of changes to various hyperparameters of the gnn. the two best performing gnns were than compared to a mars model, a light gbm, and a standard deep neural network(dnn) and the performance of the models was evaluated on sparse data sets. for this comparison, all models were trained on the complete training set and evaluated on validation sets with different sparsity rates. for a missing ratio of 0.2, 20 different initializations per method were applied and compared. additionally, for the rgcn and hgt training was also conducted on similar sparsity rates as present during the evaluation. in the following, first the graph construction and afterwards the application of the learning methods is described.



the reason for setting up the graph this way and not concatenating all parameters measured for one die into the die node feature vector is that in such a case missing features would again have to be imputed. even though this could be handled via embedding or by adding additional entries to the node feature vectors indicating whether a parameter has been measured or not, one would lose the advantage of the graph structure as such a procedure could also be used as pre-processing step for standard ml methods. in the homogeneous graph variants, the node types were encoded in a 1-of-k scheme and concatenated into the node feature vector.



be minimized if neighboring dies and their attributes are taken into account for the predictions as well. for the showcased task of sensitivity estimation, this observation was true up to a missing rate of 0.4 where the performance heavily decreased. possible reasons for this decrease are firstly that the hyperparameters of the hgt were optimized for a complete training set and secondly that with lots of missing data during training the relations among the features become harder to identify, i.e. the difficulty of the learning task increased. for practical application this means that for each parameter during training a tradeoff has to be found between providing the model enough information to capture the underlying relations and maintaining a similar node degree distributions during training and inference.



except for the hgt, which showed a similar performance to the light gbm regardless of the specific graph variant used, all other gnn architectures compared showed a strong dependency on hyperparameters and on the underlying graph variant. this especially applied to the gat architecture. in particular, for the gat the graph variants with high node degrees of the die nodes, v2a and v3a, impeded the learning process. when training took place though, the attention mechanism outperformed the bare gcn due to the limited expressive power of the latter. shchur et al. reported similar performance drops in gats concluding that the gat architecture can be sensitive to weight initialization. therefore, it is possible that for the variants with high node degrees, which seem to complicate training, the importance of weight initialization increases.



however, cause an informational bottleneck leading to an oversmoothing over connected nodes as observable for the gat operating on v2a and v3a. the hgt performed best on v2b. remarkably, this variant had the lowest average centrality coefficient of all graphs where dies were connected both within and between wafers. this suggests that the algorithm makes use of information hold by the inter-wafer relations, but also results in a trade-off between a sufficient number of connections to represent the relationships whereas at the same time preventing over-smoothing.



another peculiarity is the performance deterioration on graph variants where positional information was represented by separate position nodes(i.e. in addinfd and addinfe, see appendix b). the reason might again be the increased complexity of the graph which makes it harder to extract the relevant information together with the high node degrees of the position nodes, making the gnn prone to over-smoothing as discussed above.



however, an improvement in performance became visible when positional identifier were added to die nodes(posenca). this leads to the consideration that an even better performance could possibly be achieved by position-aware gnns calculating node embeddings by aggregate messages from sets of randomly chosen anchor nodes and considering the distance of two nodes within the message passing. an alternative could be the use of anchor nodes as presented by chu et al., who selected nodes as anchors which provide a large amount of information to other nodes.



presented approach mainly data has been integrated into the graph structure which is also usable by mars and a light gbm even if the positional relation cannot be represented there. therefore, to fully leverage the advantages of the graph structure, a next step could be to add information from system simulation which classic ml methods can use at most for transfer learning approaches. however, this additional information can be captured by the graph structure. another possibility to increase the performance of graph-based approaches might be a modification of the loss function towards a semi-supervised loss, not only training the gnns to correctly predict the target parameter, but also to encourage the capturing of relations between other non-target parameters and therefore to better represent actual inter dependencies.



for the positional encoding four variants were tested. in the first variant posenca, each wafer position was assigned to an unique identifier which was set as node feature of each die node. for the second variant posencb, separate position nodes were created and each die was connected to its regarding positional node. in the third variant posencc, again position nodes were used, but instead of connecting neighboring dies, neighboring position nodes were connected. for the fourth variant posencd, the die-die connections of v3a were replaced by position nodes, which in turn were connected introducing the new node type distance with the reciprocal of the euclidean distance between the two positions as node feature. in addinfa, additional node types site number ft, site number wlt, load id, and card id were added to the graph. augmenting the graph by six types of inline parameters







monika e. heringhaus was born in ulm, germany, in 1995. she received the m.sc. degree in medical engineering from the university of stuttgart, germany, in 2019. she is currently pursuing the ph.d. degree with robert bosch gmbh, reutlingen, germany, in cooperation with the university of stuttgart. her focus is on machine learning methods for mems sensors.



alexander buhmann received the dipl.ing. and ph.d. degrees in microsystems technology from the university of freiburg, freiburg, germany. he was involved in system design, simulation, and algorithms. in 2008, he was with the corporate research department, robert bosch gmbh, reutlingen, germany, where he designed, analyzed, and assessed new mems components. since 2012, he has been the project and team manager of the development department. he is currently chief expert and director responsible for systems engineering, for algorithm& data science and for embedded software development of mems.



