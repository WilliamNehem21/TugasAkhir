techniques can be used to transform automatically composed conversion functions as well as other value-level operations. these additional transformations enable optimization of conversion functions and migration of queries. section 5 discusses how two-level transformation can be made constraint-aware, in the sense that constraints on transformed types can be propagated and introduced during type-level transformation. we discuss related work in section 6. in section 7 we summarize the contributions of the 2lt project and we indicate avenues of future work.



similar coupled transformation of data types and corresponding data instances are involved in the scenario of data mapping. such mappings generally occur on the boundaries between programming paradigms, where for example object models, relational schemas, and xml schemas need to be mapped onto each other for purposes of interoperability or persistence. data mappings tend not to be evolutionary, but rather involve fully automatic translation of entire data structures, carried out during system operation.



in the first transformation step, the nested type x is transformed by transformation t1, witnessed by to1 and from1. to pull the nested transformation to the level of a itself, the map operator associated to f is applied, which results in the transformation f t1(where we once more overload the symbol f) which converts fx into the intermediate type fx'. the witnessing conversion functions are lifted to fto1 and ffrom1. this is an example of structural composition of two-level transformations.



when developing a framework for two-level data transformation, the challenge arises to drive composition at the value level by composition at the type level. in other words, from a compositional specification of the transformation of one type into another, it should be possible to derive compositional specifications of the value-level transformation functions that convert between values of these types. moreover, the derivation should be dynamic, in the sense that the target type of the type-level transformation can not be assumed to be known before hand, but is only arrived at by actually carrying out the transformation. likewise, the types of the derived conversion functions, as well as their compositional specification are computed dynamically.



type-safe manner by judicious use of dynamic types. using this approach, valuelevel transformations are statically checked to be well-typed with respect to the type-level transformations to which they are associated, and well-typed composition of type-level transformation steps induces well-typed compositions of value-level transformation steps. the approach will be reviewed in section 3.



firstly, the dynamically computed compositions of value-level functions resulting from two-level transformations fulfill the task of converting source values into target values and back. but they do not necessarily perform this task in the best possible way. in particular, these functions may include redundant intermediate steps and may not perform their steps in optimal order. thus, a further challenge arises to post-process conversion functions after composing them, in such a way that conversion steps are reordered and fused and more optimal conversions are derived. secondly, data processing operations may exist on the source type of a two-



generally, schema definitions consist of a structural description augmented with constraints that capture additional semantic restrictions. for example, sql database schemas and xsd document schemas may declare referential integrity constraints, grammars include operator precedences, vdm specifications contain datatype invariants. when a data schema is transformed, the corresponding constraints must also be adapted.



in, we showed that constraint-awareness can be built into our framework for two-level transformation in a straightforward manner. in this approach, constraints are represented in a similar manner as data conversion functions and queries. unlike these data processing operations, the functions that represent constraints are embedded into representations of types. the approach is explained in more detail in section 5.



in this section, we explain how data refinement theory, combined with typed strategic term rewriting can be used to provide an initial framework for two-level transformation. this initial framework addresses the first challenge of two-level transformation(formulated in section 2.1) of driving composition at the value level by composition at the type level.



thus, a rewrite rule consumes a representation of type a, and returns a triple, embedded in monad m. the monad is used to represent partiality(success and failure of rewrite rules). the triple contains a representation of type b into which a is refined, as well as the two witnessing functions to and from that convert between a and b. thus, a rewrite rule does not simply transform values into other values, as is the case in normal rewrite systems. rather a rewrite rule is defined as a transformation of one type representation into another, witnessed by value-level transformations. the universal quantifier expresses that rewrite rules are polymorphic in a, i.e. they can be applied to representations of any type. the existential quantifier expresses that the target type of the refinement is computed dynamically, are function composition and the identity function. apart from these, every type constructor, such as binary product, disjoint sum, lists comes with its own associated set of operators. the laws for these operators describe properties such as associativity and commutativity, but also expansion and cancelation properties.



here, m is again a monad. thus, rewrite rules are basically monadic functions on point-free representations, additionally parameterized with a type representation. this additional parameter of rules is used for type-directed rewriting, i.e. it allows us to create rewrite rules that decide their applicability on the basis of the type of their input expression.



a constraint on a datatype can be modeled as a unary predicate, i.e. a boolean function which distinguishes between legal values and values that violate the constraint. to associate a constraint to a type, we will write it as a subscript:



atanassow et al show how canonical isomorphisms(corresponding to laws for zeros, units, and associativity) between types can induce the value-level conversion functions. they provide an encoding in the polytypic programming language generic haskell involving a universal representation of types, and demonstrate how it can be applied to mappings between xml schema and haskell datatypes. beyond canonical isomorphisms, a few limited forms of refinement are also addressed, but these induce single-directional conversion functions only. a fixed strategy for normalization of types is used to discover isomorphisms and generate their corresponding conversion functions. by contrast, our type-changing two-level transformations encompass a larger class of isomorphism and refinements, and their compositions are not fixed, but definable with two-level strategy combinators. this allows us to address more scenarios such as format evolution, data cleansing, hierarchicalrelational mappings, and database re-engineering.



on the level of problem statement, a basic difference exists between lenses and two-level transformations or refinements. in refinement, a(previously unknown) concrete representation is intended to be derived by calculation from an abstract one, while lenses start from a concrete representation on which one or more abstract views are then explicitly defined. this explains why some ingredients of our solution, such as representation of types at the value level, statically unkown types, and combinators for strategic rewriting, are absent in bi-directional programming.



several systems have been developed for performing program transformation in calculational form using fusion laws. among these, mag and yicho are prominent, but both are targeted towards haskell programs written in the pointwise style. in order to cope with fusion laws for generic recursion patterns both resort to advanced higher-order matching algorithms. we do not need such techniques sions, which was used to optimize expressions resulting from a program transformation tool that translates pointwise haskell code into point-free style. the main improvement of our approach is typing: we can use type representations to guide the rewriting process and rewrite rules are guaranteed to be type-safe. in his introductory book to haskell programming, bird presents a functional calculator that can also be used to simplify point-free expressions. unfortunately, the expressions are not typed and, likewise to mag, it uses a fixed rewriting strategy, which makes it difficult to apply in our scenarios.



a notion of xml functional dependency(xfd) was introduced by chen et al, based on path expressions. mapping algorithms are provided that propagate xfds to the target relational schema and exploit xfds to arrive at a schema with less redundancy. davidson et al present an alternative constraint-preserving approach, also using path expressions. in contrast, our constraints are not restricted to relational integrity constraints. we have expressed constraints as point-free functions, which can be converted automatically to and from structure-shy programs including path expressions.



barbosa et al discuss generation of constraints on relational schemas that make xml-relational mappings information preserving, i.e. isomorphic. nonstructural constraints on the initial xml schema are not taken into account. constraints and conversion functions are expressed in(variations on) datalog, which can be(manually) rewritten to normal form in a mechanical way.



our approach so far has been limited to a number of fundamental type constructors, sufficient for modeling relational databases and most constructs found in xml schemas. however, a number of further constructs for data type construction would be desirable to include, such as mutual recursive datatype definitions, inheritance, and parametric polymorphism. these enhancements would enlarge the scope of 2lt to data formats such as grammars and object-oriented data models.



at the level of behaviour, the 2lt project has focussed on point-free functional programs as conversion functions, queries, and constraint definitions. for these programs, calculation laws are readily available and highly developed. however, it would be desirable to include other kinds of behavioural descriptions. for example, structure-shy query specifications as found in xpath have been shown to be amenable to calculation by converting them to and from point-free structuresensitive programs. also, point-wise functional programs, as well as imperative programs with side effects need to be brought within scope. this would allow the application of the 2lt approach to more general model-transformation problems.



a particularly interesting challenge, would be to extend the 2lt approach to components and services. the challenge here would be to formalize and support the coupled transformation of components such as clients and servers, in such a way that wrapper and glue components can be introduced automatically and to some extent fused into the various components. techniques that might be employed to meet this challenge include refinement of co-algebras, automata, and other component models. this avenue of elaboration could for example find application in evolution of web services.



