according to software performance engineering(spe) practices, performance analysts doing performance testing are expected to enforce that the system under test(sut) is executed in isolation and following a specific test plan according to a well-defined test objective[2,3]. respecting spe practices adds a layer of responsibilities, new roles(human resources such as performance analysts and similar capacities) invariably leading to additional costs, and also new risks to software projects. however, it is not always possible to follow spe guidelines rigorously due to the low priority(or impossibility) that is associated with pure performance testing in software projects. in contrast, functional testing is considered more important for stakeholders and thus, has a higher priority level. in software projects, one must balance the advantages of using spe to devise responsive and performant applications in respect to the effort to be spent.



the remainder of this paper is organized as follows. section 2 addresses software testing and performance testing generally. our target application is explained on section 3 as well as its architecture and internal operational details. in section 4 we discuss stochastic modeling and the formalism known as stochastic automata networks. section 5 presents our model and an analysis of our results. finally, in section 6, we present our final considerations and future works.



software testing is a crucial task in current information technology(it) organizations because it helps ensuring the delivery of high quality products to end customers. there are two aspects of testing that must be considered for every type of software project(small, medium and huge sizes), i.e., failure analysis(or functional software testing) and performance testing. functional software testing(fst) is a process that follows a rigid set of rules allowing testers and developers to repeat error conditions and fix issues, hopefully, in a timely manner. the main interest of fst is to ensure if a functionality is producing the expected output for given input.



performance testing is an important component of software performance engineering(spe) practices. in contrast to fst, it is directed towards the nonfunctional aspects of systems, e.g., availability, security, reliability, responsiveness, among other attributes. there are three major objectives to test the performance of an application:



the interest is to evaluate the quality of the product that is subjected for consideration. to test a given application in terms of performance, it is recommended to follow guidelines and principles according to a methodology. the methodology must relate to a precise objective, for instance, discover the major application bottleneck or assess the network impact on performance indices. it also describes the workload that must be characterized to assess the overall performance.



slas are high-level contracts established by stakeholders, e.g., service providers and customers. the main objective to set up an agreement stems from the need to guarantee that quality of service is present and ensured throughout a business relation. defining slas between interested parties helps the understanding of responsibilities and conditions to deliver performant services. slas relates to non-functional application testing since it helps devising a compromise in terms of expected quality of service.



when applications are fully tested for both functional and non-functional specifications, they are ready to be deployed in a production environment. there is a huge research effort to characterize the behavior of applications and map to distributions in order to enhance the comprehension of how the system will behave under certain conditions, anticipating and efficiently reacting to problems. next section discusses related works regarding stochastic models of multi-tier architectures and also some approaches where slas are under consideration.



slas were discussed in seminal works regarding service oriented computing[7,8,9,10]. dealing with slas and cost models associated with contracts was researched by several authors in a recent past. ashok et al. investigated location-aware sla contracts and quality of service measurements whereas liu et al. build a cost model to analyze the impact of sla to maximize profit. cost models for slas are



the research presented here distinguishes itself from related works from previous authors on the description and analysis of a stochastic model specially tailored for applications under performance testing subjected to external workloads and observance of slas. it is important to map the amount of external influence or unknown workloads to predict if the contract established by the service agreement will not be met. case the time to complete jobs in the main load balance servers is taking a time that is superior to the threshold computed by our stochastic model, given the external load, it indicates that the slas will probably never be met. it is reasonable to consider that external influences are the main cause for the test fail, not because of some bottleneck problem. after some time or if the main server experiences less amount of loads, one could restart the performance testing process and resume sut operation.



in fact, the bottleneck for such types of environments is directed towards the main server acting as a dispatcher that distributes the workload among the remaining servers. as stated before, after the transactions pass this server, they are executed in a clean and dedicated environment, where more reliable usage statistics are enabled. thus, it is possible to populate a stochastic model with parameters measured after the transaction passed the main server. one clear advantage of such stochastic models is the possibility of computing the average time necessary to process a given transaction, using the sla deadline to calculate the available time that can be spent in the main server. an online monitoring tool could keep observing the process that are under execution in the main server and decide whether or not the sla will be met. if a break of contract is imminent, one could stop the performance test and resume afterwards, to continue searching for application bottlenecks or some other performance testing objective.



once the transaction is successfully processed by prf01 and routed to prf02 or prf03, the execution becomes dedicated for the sut. all three web servers run on a pentium iv 2.66ghz machines with 16 gbytes of ram, dualcore, running weblogic 8.1 as application server and windows 2003 server edition. the dbms runs on a pentium iv 3.2ghz, quadcore, with the same amount of ram and running linux redhat enterprise edition and oracle 10i with dedicated execution. transactions that disrespects the sla are stored for counting reasons(for subsequent quality assurance purposes) and exits the system. ideally, no transaction ought to pass the higher limit of the agreement ensuring high qos to the system and certainty that every transaction runs below specified thresholds.



the measurements ruled out the network as an application bottleneck since every server is present in the same environment and the monitoring tool was able to attest that the system was robust and healthy for the totality of the test plan execution. the critical aspect of the architecture is towards the prf01 server since it encompasses every demand for every application that is running. since the sla must comply with a set of rules, our main strategy is to divide the system into two distinct blocks and analyze them separately, in order to isolate our problem into more manageable pieces.



processes). occasionally, a process is loaded to memory and remains executing indefinitely. this can be mitigated by a soft reset(only a few processes are destroyed) or, more extremely, a hard reset(the whole machine is reinitialized). in production, the same problems related to server instabilities may or may not appear, depending on the execution profile. since we are testing the application in the most problematic situation, our effort in this work will present a worst case scenario analysis.



for this particular problem, one could model the reality choosing standard queueing networks(qn)[21,22]. qn is a very powerful formalism to model systems and extract performance indices such as average state permanence probabilities, transient behavior or scalability analysis. it has been used in the past in many applications with significant results, from economic models to distributed computing. however, our reality implements a unique behavior to balance transaction routing that is hard, if not impossible, to model with qns. the problem under analysis needs a more sophisticated manner to convey the fact that transactions are routed following patterns that must know the apparent load within each server.



san is used to model realities where parallel and synchronizing behavior(resource sharing, for instance) is expected to occur. it is specially suited for distributed systems but can be applied to several application domains. it has been successfully used to extract useful performance indices of global software development realities, non-uniform memory access architectures, master/slave parallel computing platforms and mobility patterns to name a few. in a mathematical point of view, san uses tensor algebra properties to compute the probability vector that withholds the performance indices. it basically multiplies a vector by a non-trivial structure called a markovian descriptor, i.e., a list of small sized matrices that captures the occurrence of every event present in a given model. these matrices are operated with tensor sums for local events and tensor products for synchronizing behavior. one of the greatest advantages of using san to represent and solve stochastic models is due to its power of description and efficient storage mechanism.



this section presents how we calculate the time needed to process the phase ii, i.e., the total response time for this phase(tphaseii). the main idea is to profit from the isolation of the application and database servers in terms of execution. once the transaction arrives in this phase, it is processed in dedication with full use of available processing power and memory. we are assuming the servers with a high level of stability, i.e., needing system restorations and management operations only occasionally.



in this paper, once the main application server routes transactions forward, we assume that on the worst case scenario a maximum theoretic value for the response time of precisely 8.8 seconds, i.e., we assume the worst case scenario, tphaseii= 8.8 seconds. this value will be used to derive the final sla time needed to complete the transaction.



we proceed our analysis on studying the response time for phase i(tphasei). the model presented here was described by a san instead of a qn because our reality has several different behaviors that must be captured in terms of external influence modeling. next, in section 5.2.1 we explain our stochastic model in detail and its results are presented in section 5.2.2.



we conducted several performance tests and monitored important resources such as processor time and available memory(only to name a few) for all application servers and the database servers. we used loadrunner as the main tool to perform the load testing procedure on our sut, where the verified throughput(x) was 50 tps. following a methodology, our performance testing objective was to verify that at least 90% of the total number of transactions were being met by the sla.



for the rest of our analyzed case scenarios where we varied the workload intensity, we were able to compute response times within the available sla time, i.e., cases where the external application influence over service time enables meeting the time constraint. our stochastic model was able to help us understand the fact that depending on the application execution profile we can anticipate if the response time for our sut will fall under the sla threshold. this is very interesting because the model could forecast, for instance, the unfavorable or advantageous conditions present on the environment to allow the execution of performance testing. case such conditions are causing external delays, it is safe to conclude that it is not our



the present work proposed stochastic models and sla assurance applied in the context of multi-tier web services with external workloads. there is an increasing interest for practical applications of stochastic modeling for performance evaluation. we modeled the reality and performed a worst case scenario analysis to verify if the contracts between users and service providers were being respected. our performance indices computations presented means to decide if the sla would be executed below its deadline and the impact of external workloads in this time.



as future works, we consider applying the same model ideas for busy environments with more intense workload variation. such work may demand a deeper analysis of stochastic distributions leading to the extensions of the model to consider more complex distributions. one option is the inclusion of phase-type transitions to approach some non-exponential phenomena like timeouts. in this case, some prior studies on phase-type representation for san formalism could be used.



in another interesting future work we could also develop and install a daemon with both a stochastic model and a numerical solver(e.g., gtaexpress or similar) in the application servers of interest to monitor the execution and selfparameterize a model with the obtained data to decide whether or not the sla will be met in a timely fashion. this research will allow decision makers to stop the execution of some external workloads or to control the incidence of external applications that are allowed to run. this will undoubtedly help to assure a higher level of availability to the web service, avoiding economical losses and ensuring user satisfaction.



the authors thank dr. alberto avritzer from siemens corporate research(scr), princeton/nj/usa, for insightful discussions that led to the elaboration of this paper and the performance testing research team, a collaboration between the faculty of informatics at pucrs and dell brazil. this work has also support from fapergs/cnpq.



