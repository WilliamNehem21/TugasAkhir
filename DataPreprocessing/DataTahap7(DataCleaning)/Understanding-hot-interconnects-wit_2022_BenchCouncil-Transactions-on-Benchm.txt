with the trend of hardware evolution and the new interconnects being created, there are several issues that the application developers need to pay attention to. with the hardware upgrading, the developers need to re-evaluate the performance of different generations of hardware to design the proper systems software based on the improved data transfer rates. also, many new interconnects are emerging with the development of novel hardware features. these features may potentially impact application performance and need to be systematically investigated.



on the other hand, different types of data center applications represent various performance characterizations, like hpc workloads, deep learning training and inferences, big data analytics, and cloud-based microservice. the impacts of new interconnects on these different workloads should be evaluated separately and carefully. therefore, we believe this is high time to investigate the performance characterizations of modern data centers and hpc interconnects via standard benchmarking experiments under different application scenarios. this observation motivates us to extensively survey hot interconnects on modern data centers and hpc clusters and the associated representative benchmarks to help the community better understand these advanced interconnects.



investigated fourteen deep learning benchmarks. zhou et al. discussed seven microservice benchmarks. gao et al. compared fifteen big data and ai(artificial intelligence) benchmarks. however, we did not find such a survey that can extensively cover a broad range of the latest advanced interconnects in modern data centers and the associated representative benchmarks for different application scenarios. therefore, this paper addresses the need to survey different hot interconnects deployed in modern data centers and the corresponding benchmarks to expose their performance characteristics.



ethernet is one of the most traditionally utilized interconnects for hpc and data center clusters. at the early stage, 1 gb/s ethernet(1-gige) was widely used. however, with the advancement of cpu performance and i/o speed, the 1-gige has become the bottleneck. with the demand for higher bandwidth and data transfer rate, ethernet with 10-gige, 25-gige, 50-gige, and even 100-gige, has been developed. as of june 2022, 25-gige is the most widely used interconnect in the top500 list, and the ethernet interconnect family is the majority in the list, taking up nearly 50%.



taken the advantages of rdma, rdma over converged ethernet(roce) is developed, which is a network protocol that allows rdma to operate over ethernet networks. roce is designed to support rdma over ethernet on layer 2 networks, and its extended version roce v2 enables transportation on layer 3 networks. traditionally, ethernet has left the congestion control to the tcp(transmission control protocol) layer. with the development, the first algorithm proposed for the ethernet network is pause frame in 1996. congestion control on roce uses an extension to the tcp/ip protocol called ecn(explicit congestion notification). other techniques, such as the qcn(quantized congestion notification), were developed afterward. both traditional ethernet and roce are available for various interconnect topologies. in 2019, amazon announced efa(elastic fabric adapter) for its ec2(elastic compute cloud) instance. the libfabric interface on efa provides up to 100 gbps speed and reduces overhead with techniques like operating system bypass.



provided by nvidia, infiniband(ib) is an industry-standard switch fabric and the second most popular interconnect family in the top500 list. as of june 2022, 32.4% of the top500 clusters are interconnected by ib, especially for top10 clusters such as summit and sierra. besides the higher bandwidth(up to 400 gbps) and lower



specifically, rdma-capable networks(like infiniband) typically support four types of transport modes: reliable connection(rc), reliable datagram(rd), unreliable connection(uc), and unreliable datagram(ud). send and recv operations are supported by all modes, while the rdma write operation is unsupported by ud, and the rdma read operation is unsupported by ud and uc. the most commonly used network topology for ib is fat-tree, but it also supports other topologies like dragonfly+. the ib standard includes a congestion control mechanism to detect and resolve congestion by using two relay messages: fecn(forward explicit congestion notification) and becn(backward explicit congestion notification). when applying ib to gpu, cuda 5.0 first introduced gdr(gpudirect rdma). gdr allows ib adapters to directly access the gpu memory while also bypassing the host. gdr can significantly increase data communication performance among gpus, which further benefits the increasing number of redesigned classical hpc and machine/deep learning applications.



communication pattern to evaluate the throughput and latency of basic rdma operations. we can also use it to compare different transports by specifying the transport as rc, uc, ud, raw ethernet, and even mellanox dct(dynamic connected transport) and aws srd(scalable reliable datagram) transports that are not specified in the standard ib specification. besides the basic operations and transports, perftest also supports the gpudirect feature for direct intergpu communication through gpudirect rdma and the aesxts feature for data encryption and decryption scenarios using rdma. perftest is designed without emulating any real application traffic or traffic probability distribution. it does not allow users to choose the traffic pattern but only with a parameter to specify the message size in each test. these tests are mainly helpful for hardware or software tuning as well as for functional testing.



network_load_test measure the performance of an mpi application with network congestion. this simulates the scenario when running on multi-tenant hpc networks. each congestor has a unique random ring, and the communication patterns include point-to-point incast, all-to-all, one-sided rma incast, and one-sided rma broadcast. two measurements execute in the random ring infrastructure: point-to-point latency measurement by sending and receiving 8 bytes messages from and to two sides, point-to-point bandwidth with synchronization by sending and receiving eight 128k bytes messages from two sides.



the default settings are intended to be utilized in general production scenarios. it reports the mean and 99th percentile latencies as well as the bandwidth per rank. with congestors, it also reports the congestion impact metric, which is defined as the ratio of congested latency or bandwidth divided by the uncongested latency or bandwidth. the congestion impact metric is an indicator to study the impact of congestion across systems with different networks.



with the development of hardware technology, much new storage hardware is produced, such as the nvme ssd. storage systems rely on different drivers and libraries in data center, with interactions between the processors and the storage devices via interconnects. intel spdk provides nvme perf as an nvme ssds benchmarking tool with minimal overhead in benchmarking. nvme perf provides several runtime options to support the most common workload. users



the above-mentioned application-level benchmarks can represent a broad range of data center applications, including hpc, big data, ai, and cloud computing. in this survey, we choose mpi and pgas based benchmarks as application examples and run them on different interconnects. mpi and pgas based benchmarks have been designed and maintained for many years with a lot of contributed optimizations from the community. our experience also reveals that they are easy to deploy and convenient to run. the experiment results are shown in section 5.



benchmarking big data systems: many benchmarks are proposed to evaluate the big data systems with the big data boom. hibench and mrbench are designed for evaluating mapreduce systems. textbends is applied to evaluate the performance of hive, spark, and mongodb on a textual corpus. the tpc organization designed benchmark standards what were data-centric benchmark and disseminated verifiable data to the industry. wang et al. discussed the challenges of using the widely-used benchmarks(tpc-c and ycsb) for systems evaluation. dcqcn and dscp-basedpfc



this work was supported in part by the nsf research grant ccf#2132049. part of this research was supported by the exascale computing project(17-sc-20-sc), a collaborative effort of the u.s. department of energy office of science and the national nuclear security administration, and by the u.s. department of energy, office of science, under contract de-ac02-06ch11357. we gratefully acknowledge the computing resources provided on bebop, an hpc cluster operated by the laboratory computing resource center at argonne national laboratory, usa. part of this research was conducted using pinnacles(nsf mri,#2019144) at the cyberinfrastructure and research technologies(cirt) at university of california, merced.



