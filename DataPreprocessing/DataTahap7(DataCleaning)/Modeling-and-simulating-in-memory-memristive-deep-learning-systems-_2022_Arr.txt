traditionally, machine learning(ml) and deep learning(dl) systems are trained and deployed using hardware platforms adopting the von neumann computing architecture. while in recent years, graphics processor units(gpus) have been used to massively parallelize and accelerate the performance of these workloads, they are still prone to performance bottlenecks caused by the amount of data being moved back and forth between physically separated memory and processing units. imc is a novel non-von neumann approach, where certain computational tasks are performed in the memory itself, which has the potential to alleviate this bottleneck.



in contrast to conventional spice-based simulation, modern cad simulation frameworks adopt modern software engineering methodologies. moreover, they are able to accurately model non-ideal device characteristics, peripheral circuitry, and modular crossbar tiles while being interfaceable using high-level language apis. we confine the scope of this paper to mdls, i.e., memristive imc systems for dl system deployment, and provide a survey of existing simulation frameworks and related tools used to model large-scale mdls.



the rest of the paper is structured as follows. in section 2, preliminaries related to modeling and simulating in-memory mdls are presented. in section 3, existing cad tools for in-memory mdls are over-viewed. in section 4, comparisons of modern simulation frameworks for in-memory mdls are made, and two mdls architectures are simulated. in section 5, we provide an outlook for mdls simulation frameworks. finally, in section 6, the paper is concluded.



resistance/conductance values, and inputs as wl voltages. tiled crossbar architectures contain several modular crossbar tiles connected using a shared bus. these are also connected to additional circuitry used to realize batch-normalization, pooling, activation functions, and other computations that cannot be performed, or are not efficient, in-memory. modular crossbar tiles consist of crossbar arrays with supporting peripheral circuitry. we refer the reader to for a comprehensive description and overview of imc accelerators for dl acceleration.



simulate the training routine of the vgg-8 network architecture, and the inference routine of the googlenet network architecture. both training and inference routines were evaluated using the cifar-10 dataset. two separate network architectures were used for evaluation, as larger and more complex networks could not be reliably trained using existing simulation frameworks with compute unified device architecture(cuda) support when utilizing a single gpu, even with 32 gb of video random-access memory(vram). moreover, not all simulation frameworks supported convolutional layers with non-zero groups(connections between inputs and outputs), meaning that many resnet-based architectures could not be implemented.



devices were assumed to have a finite number(6) of conductance states, and adcs were assumed to operate at a 6-bit resolution. for inference routine simulations, 10 runs were conducted, and mean and standard deviation values were reported across all runs. for training routine simulations, mean and standard deviation values were reported across all training epochs. all codes used to perform comparisons are made publicly-accessible,1 and can be modified to perform comparisons using different hardware technologies, network architectures, and hyper-parameters.



in addition to simulating training and inference routines using memtorch, dnn_neurosim_v2.1, and the ibm analog hardware acceleration kit, baseline training and inference routines were simulated using the native pytorch ml library for comparison. for all baseline implementations, the exact same hyper-parameters were used. torch.cuda.amp was used to quantize all network parameters to 16-bits to improve performance.





