a prominent algorithm for learning such devices was developed by angluin. we have implemented this algorithm in a straightforward way to gain further insights to practical applicability. furthermore, we have analyzed its performance on randomly generated as well as real-world examples. our experiments focus on the impact of the alphabet size and the number of states on the needed number of membership queries. additionally, we have implemented and analyzed an optimized version for learning prefix-closed regular languages. memory consumption is one major obstacle when we attempted to learn large examples.



code generation and model-based test generation[4,12]. they all assume that a formal model of the system under study is available. such formal models are assumed to be developed during the specification phase of system development, or a posteriori from an existing implementation.



one approach to overcome these limitations is to develop techniques for generating formal models with less manual effort and more automated support. in the extreme case, a formal model could be generated a posteriori, from the developed system. if no model of the system under development was present, this model can be used to analyze and validate the implementation. if a formal model was available a priori, the generated model can be compared with this one to show conformance of the implementation with respect to its specification.



it can be shown that this construction yields a minimal dfa accepting l, i.e., the number of states is minimal among all dfa accepting l. furthermore, it can be shown that every minimal dfa is isomorphic to the one we constructed.



we studied 6 transition systems of ccs processes. they are simple examples like buffers, vending machines, or several examples of schedulers and mutual exclusion protocols. their number of states lie between 2 and 13 and the number of letters between 3 and 6. note that we learned minimized dfa representations of the given protocols.



we failed to learn some larger protocols, namely some instances of parameterized schedulers, the jobshop(77 states, 7 letters) and an atm protocol(1715 states, 27 letters). the reason is that we did not invest effort into a good algorithm for finding counterexamples; it took too long to find counterexamples for the protocols in question.(note that the execution time which we measure is independent of the time spent for finding counterexamples.) the atm, though, failed due to lack of memory.



among the conclusions we draw from our experiences is the fact that random prefix-closed automata are harder to learn in comparison to completely randomly generated automata. for our random examples, the number of membership queries can roughly be described as linear in the number of transitions. membership queries for our prefix-closed examples, in comparison, are approximately quadratic in transitions.



moving deeper into the domain of prefix-closed automata we conclude that it is possible to reduce the number of membership queries by using an optimization specially shaped for these automata. the optimization reduces the number of membership queries considerably. for the randomly generated prefix-closed automata we measured a reduction of about 20%.



turning our attention to the real-world examples we see that the optimization works much better, saving 60% membership queries relative to unoptimized learning. we also compared the result of learning real-world examples with randomly generated prefix-closed examples of the same size, in order to investigate if they behaved in the same manner. the result reveals a better performance for the real-world examples in terms of membership queries, especially with the optimization. this seems to imply that our real-world examples have a more suited structure for learning. hopefully this observation can be used to optimize the learning process further.



