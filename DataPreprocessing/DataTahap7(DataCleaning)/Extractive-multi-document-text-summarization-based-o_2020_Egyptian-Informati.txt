this abundance of data availability undoubtedly enhances human life, but at the same time makes the quick access of accurate information increasingly difficult. automated text summarization technologies are therefore increasingly studied by researchers so as to achieve greater efficiencies through enhanced or new methods. however, despite all of the studies conducted in the field of document summarization, the need for improvement and innovation has not diminished. automated document summarization is a significant subtopic of natural language processing(nlp), which has the objective to present long text documents in a compressed and comprehensible form. document summarization



the remainder of the study is organized as follows: in section 3, information is provided on the general stages of the proposed summarization method, textual graph, proposed text preprocessing tool, and graph independent sets. in section 4, information is given on the dataset and evaluation metrics used, the experimental results of the proposed method for summarizing the texts are presented, and the proposed model is compared with the stateof-the-art methods. finally, in section 5, the experimental results are discussed and interpreted.



although most researchers have focused on extractive text summarization, some have worked effectively on abstractive summarizing. in ref., abstractive multi-sentence summarization was performed using the rnn structure. similarly, refs.[34,35] were also works in the field of abstractive summarization.



today, the problem of finding maximum independent sets still remains as a np-hard problem. the process of finding maximum independent sets has not yet reached an optimal result, and can be seen in the current study as a weakness. new methods to arrive at a solution for this problem will be pursued meticulously.



in this study, a graph-based generic, extractive and multidocument summarization method is presented to extract appropriate summaries from the given texts. the proposed document summarization method consists of three main stages. in the first stage, non-discriminatory stop words(e.g., pronouns, prepositions, conjunctions) were removed from the dataset. a number of preprocesses are performed by the developed kush text preprocessing tool. when forming graphs of close-meaning words that differ from each other in terms of their spelling but are semantically derived from a common word root, they are prevented from being processed as if different words. in the second stage, word commonalities between the phrases are represented mathematically and graphically. in addition, this stage includes determination of the nodes forming the maximum independent set and the removal of the sentences corresponding to these nodes from the main graph. the final stage is the weighting of the sentences that make up the documents using the eigenvector node centrality method and the selection of important sentences. at this stage of the proposed method, the top n phrases are combined and 200and 400word summaries created separately. the success of the framework was then tested in detail using a number of different rouge performance metrics.



in addition, when forming graphs of close-meaning words that differ from each other in terms of their spelling but semantically derived from a common word root, they are processed as if they are different words, which makes it difficult to identify connections and relationships between the sentences. it is predicted that this problem will significantly affect summarization performance, hence a text processing tool was developed within the framework of the proposed model. this software tool, which we named kush, was developed using c# on the.net platform. the steps for the proposed kush preprocessing tool are given in algorithm 1, and the pseudocode of the best alternative search function is given in algorithm 2.



a node is added to the representative graph for each sentence in the texts. for the edges between the nodes, the edge weight was added by considering the number of intersecting words of the phrases in the text. the kush algorithm regulates the semantic accuracy of the relationship between sentences, interconnecting the nodes based on meaningful relationships. thus, all the sentences were associated with each other in terms of their common content that covers all combinations, ensuring that the relationships between the sentences can be accurately transferred to the graph. the steps for the textual graph are given in algorithm 3.



the proposed method obtains the graph from the text. the independent set is obtained from the graph and, after the independent set is removed, the eigenvalue is obtained and summarized with eigenvalue centrality. in the proposed summarization approach, based on the assumption that the sentences that should be excluded from the summary will be represented as independent set, the mentioned nodes are determined and removed from the representative graph. thus, in the experimental process leading to the summary, a method which has not previously been used in any document summarization study is presented. in order to find effective nodes, ineffective nodes were first identified. as shown in algorithm 6, the remaining nodes after the ineffective nodes have been removed, eigenvector node centrality is weighted with metric. for the obtained top n node scores, 200and 400word summaries were obtained.



the higher the score, the more shared content is available with the model summary; and for the proposed method, the automated summary was considered to be better and more informative. in refs.[47,48], lin revealed a high correlation between rouge scores and the scores given by individuals. in the current study, we used rouge-n(n-1, n-2), rouge l, rouge-w-1.2 and sets(duc-2002 and duc-2004) were used to test the accuracy of the proposed method. the duc-2002 dataset contains documents for both abstractive and extractive summarization; however, the extractive summarization files were used based on the summarization method proposed in the current study. nist produced 60 reference sets, with each containing documents, single-document summaries, and multi-document abstracts/extracts, and defined criteria such as event sets and biographical sets.



for the duc-2002 dataset, three tasks were defined. task 1, fully automated summarization of multiple newswire/newspaper documents(articles) on a single subject, with 60 sets of approximately 10 documents each provided as system input. task 2, automated summarization of multiple newswire/newspaper documents(articles) on a single subject. task 3, one or two pilot



after the preprocessing stage, the developed software tool called kush was used to provide the most accurate transfer of relationships between word phrases and textual graphs. owing to its simple and efficient algorithm, the software assigned substitutes, chosen from the text to be summarized, for the changed words. as a result, graphs with high levels of representation were obtained.



maximum independent set, which has not previously been used in any summarization study, was utilized for the current study. based on the assumption that sentences corresponding to the nodes in the independent set should be excluded from the summary, the nodes forming the independent sets on the graphs were determined and removed from the graph. thus, prior to the effect of the nodes on the global graph being quantified, a limitation was applied to the summary. this limitation prevented the repetition of word groups being included in the summary, thereby resulting in more comprehensive summaries being generated. in addition, the experimental processes affected the approach for removing the independent sets, which has been used for the first time in a summarization study, as an encouraging step to include sentences with minimum words that intersect with each other in the main text. the values reported throughout the experimental processes of the study clearly demonstrate the contributions of this innovative method.



1.2, and rouge-su4 average scores for all eight approaches, including the proposed method. all of the comparisons in the graph are summarized separately for the 200-word summaries based on the recall, precision and f-score values. the proposed method, based on a rouge-1 and f-score assessment, reported a 25% higher value than random, 8% more than luhn, 33% more than lsa, 8% more than textrank, 5% more than lexrank, 9% more than



prior to the weight of the nodes on the global graph being mathematically sorted, a unique approach is performed, which has not been used in any other document summarization study. the approach isolates the nodes forming independent nodes from the main graph and then creates the summary. this limitation prevents the repetition of word groups from being included in the summary, and thereby generating much more comprehensive summaries. in addition, the experimental processes have been used as a preliminary assessment step which encourages the removal of the independent sets, and first used in a summarization study. so, in the summaries, the sentences with maximum relations with each other are included.



