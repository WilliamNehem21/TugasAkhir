resnet-50 has dropped exponentially, and the result of nvidia shows that it now can be done in under half a minute. from the benchmarking perspective, such a short running time does not allow for a thorough and endurable evaluation. furthermore, the fixed amount of computation is distributed on the hpc system with a growing scale, which makes the resource utilization of each computing node extremely unsaturated.



and v2.0 and mlperf(hpc) fail to tackle the scalability issue and focus on selecting typical hpc ai applications and parallel-based optimizations. hpl-ai and aiperf manage to achieve scalability but bring other problems. hpl-ai evaluates hpc systems by performing mixed-precision lu decomposition at the kernel level. same to hpl,



the ensemble learning idea is to solve a common problem by combining the predictions of a group of base models. rather than making decisions depending on a single model, a group of models makes it possible for ensemble learning to reduce the variance of predictions, so-called the wisdom of crowds. bagging(bootstrap aggregating) is a fundamental paradigm of ensemble learning. as its name suggests, bagging consists of two parts: bootstrapping and aggregating. bootstrapping is essentially a data sampling process with replacement from the original dataset. the data generated through this process is called the bootstrapped dataset. the training process of bagging is highly parallel as each base model in the ensemble is trained based on its corresponding bootstrapped dataset rather than the original dataset. after finishing the training, the final decision is aggregated by averaging all the predictions of the base models.



overhead of dp is the main reason for this phenomenon because all the model copies of dp need to be synchronized globally at the end of each training step. the base model in the model ensemble of hpc ai500 v3.0 is trained highly independently without synchronization, so the communication overhead is avoided.



