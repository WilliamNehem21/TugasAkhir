i.e. they start the same general-purpose function but become specialized with training. while dendrites are the site of reception of synaptic inputs, axons convey electrical signals over long distances. inputs to neurons cause a slow potential change in the state of the neuron; its characteristics are determined by the membrane capacitance and resistance allowing temporal summation.



studies showed that the organization of the cortex can be regarded as an association of columnar units[14,15], each column being a group of nodes sharing the same properties. learning in the human brain is mainly performed using plastic connections, repeated exposures and firing and inhibition of neurons. in a simplified manner, information flowing in the cortex causes connections in the brain to become active, over time, with repeated exposures these connections are strengthened creating a representation of the information processed in the brain. moreover, inhibition of neuronsphysically defined as prohibiting neurons from firingpartly account for the forgetting process.



at a nodal level, ann started with the simplified mccullochpitts neural model(1943), which was composed of a basic summation unit with a deterministic binary activation function. successors added complexity with every iteration. at the level of activation functions, linear, sigmoid, and gaussian functions came into use. outputs were no longer restricted to real values and extended to the complex domain. deterministic models gave way to stochastic neurons and spiking neurons which simulated ionic exchanges. all these additions were made to achieve more sophisticated learning models.



one of the major and most relevant contributions in that field was made by edelman and mountcastle. their findings lead to a shift from positioning simplified neuron models as fundamental functional units of an architecture to elevating that role to cortical columns, collections of cells characterized by common feedforward connections and strong inhibitory inter connections. this provided a biologically feasible mechanism for learning and forming invariant representations of sensory patterns that earlier ann did not.



dnn are deeper extensions of shallow ann architectures that are composed of a simplified mathematical model of the biological neuron but do not aim to faithfully model the human brain as do ca or some other ml approaches. dnn are based on the neocognitron, a biologically inspired image processing model, that attempt to realize strong ai models through hierarchical abstraction of knowledge. information representation is learned as data propagates through the network, shallower layers learn low-level statistical features while deeper layers build on these features to learn more abstract and complex representations. lacking clear skills for logical inferences, dnn need more morphing to be able to integrate abstract knowledge in a human manner. recurrent and convolutional neural networks, first introduced in the 1980s, can be considered predecessors of dnn and were trained using back-propagation which has been available since 1974.



ca are a deep artificial neural network model, which borrows several concepts and aspects from the human brain. the main inspiration is drawn from the findings of edelman and mountcastle[15,7], which state that the brain is composed of cortical columns arranged in six layers. he also uses the concept of strengthening and inhibiting to build a computational training algorithm capable of extracting meaningful information from the sensory input and creating invariant representations of patterns. further description of the ca model and its biologically plausible aspects can be found in[61,51,62].



in summary, the overall computational cost of dbn and ca depends on the architecture of the network: the number of layers and the number of neurons per layer which affect the number of connections. the activation function can be fixed for both dbn and ca. sigmoid is a common activation function used in both algorithms. after training, some of these connections will have a weight of zero. based on the previous sections, we notice that for a fixed network architecture, ca will have less non-zero weights due to the pruning algorithm. however, the depth of the best networks for each algorithm vary based on the data. six-layer archinetwork. the threshold is not set to a fixed value since the weights range of weights varies based on the input data, i.e. for some datasets all the weights might be less than this set threshold even though this threshold might be very small. furthermore, the threshold is not set to zero since some weights will not exactly zero but significantly smaller than the other weights in the network and their contribution is insignificant. the classification



