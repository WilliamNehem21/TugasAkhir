
FULL-LENGTH ARTICLE
Music emotion recognition: The combined evidence of MFCC and residual phase

N.J. Nalini *, S. Palanivel

Dept of Computer Science and Engineering, Annamalai University, Tamil Nadu 608002, India

Received 27 September 2014; accepted 8 May 2015
Available online 1 September 2015

Abstract The proposed work combines the evidence from mel frequency cepstral coefficients (MFCC) and residual phase (RP) features for emotion recognition in music. Emotion recognition in music considers the emotions namely anger, fear, happy, neutral and sad. Residual phase feature is an excitation source feature and it is used to exploit emotion specific information present in music signal. The residual phase is defined as the cosine of the phase function of the analytic signal derived from the linear prediction (LP) residual and also it is demonstrated that the residual phase signal contains emotion specific information that is complementary to the MFCC features. MFCC and residual phase features are used to create separate models for each emotion. The evidence from the models is combined at the score level for each emotion and it is used to recognize the emotion. The proposed method is evaluated using music files recorded from various websites and the method achieves a performance of 96.0%, 99.0%, 95.0% using AANN, SVM, RBFNN, respectively.
© 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).



Introduction

Music plays an important role in human history and almost all music is created to convey emotion. Music emotion recogni- tion (MER) got much development these years, and appar- ently, it will play an important role in digital entertainment and harmonious human–machine interaction [1]. However,

* Corresponding author.
E-mail addresses: njncse78@gmail.com (N.J. Nalini), spal_yughu@ yahoo.com (S. Palanivel).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
the connection between music and feelings (emotion) is probably the simplest and profound. The emotion’s crucial role in musical experience has long been the object of philosophical reflection and empirical study.
Many issues for music emotion recognition have been addressed by different disciplines such as psychology, physiol- ogy, musicology and cognitive science [2–4]. Emotion recogni- tion from music signal is a difficult task due to the following reasons: First, emotion observation is basically subjective and people can recognize different emotions for the same song. Second, it is not easy to express emotion in a worldwide way because the adjectives used to describe emotions may be unclear, and the use of adjectives for the same emotion can vary from person to person. Third, it is still hard to know how music evokes emotion. Computational systems for music mood recognition may be based upon a model of emotion,


http://dx.doi.org/10.1016/j.eij.2015.05.004
1110-8665 © 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

2	N.J. Nalini, S. Palanivel


although such representations remain an active topic of psy- chology research.
An important step in MER is feature extraction and classi- fication. The importance in determination of feature extraction from music signals is in the sense that they represent the music well and computation can be carried out efficiently. Much of work on extraction of features from music devoted to timbre features. MFCC is the well known timbre texture feature or spectrum features which is the highest performing individual feature used in speech recognition, can be examined for modeling of music. Among regular features used in MER, in this work, there is a search for new features for recognizing the musical emotions.
Residual phase (RP) features is an excitation source feature used by very few researches for speaker recognition. In this work it can be used to exploit emotion specific information present in music signal. It is defined as the cosine of the phase function of analytic signal derived from LP residual. LP resid- ual demonstrates the presence of audio specific information obtained after removing the predictable part of the signal [5]. It is known that the residual of a signal is less subject to degra- dations as compared to spectral information. So systems built using the residual may be robust against degradations. This emphasizes the importance of information present in the LP residual of music signal.
The MFCC features and residual phase features are com- bined at the score level to enhance the performance of the sys- tem and it is used with pattern classification techniques such as autoassociative neural network, support vector machine and radial basis function neural network for music emotion recog- nition. The recognition rates from the models, evidence that the emotion specific information is present in residual phase signal. The paper is organized as follows. In Section 2, a review of some of the existing methods for music emotion recognition is described. The spectral and excitation source feature extrac- tion for music emotion recognition is presented in Section 3. In Section 4 techniques used in MER is described. Experimen- tal results and conclusion are discussed in Sections 5 and 6
respectively.

Related works
Determining the emotional content of music is not only through signal processing and machine learning, but also requiring an understanding of auditory perception, psychol- ogy, and music theory [6]. The related works are given based on two aspects: the features related to the music moods or emotions and the classification schemes.
The content-based acoustic features are categorized as timbre texture features, rhythmic content features, and pitch content features, harmony and temporal features [7]. The timbre features include cepstral features such as mel frequency cepstral coefficients (MFCC). Rhythmic features contain the regularity of the rhythm, the beat and tempo information, such as beat per minute (BPM) and rhythm histogram. Pitch fea- tures deal with the frequency information of the music signals. Harmony features include chromogram and temporal features include temporal centroid. In [8] author proposed a music clas- sification based on spectral and cepstral features (fusion) achieves higher classification accuracy (86.83%) than the winner of the ISMIR 2004 music genre classification contest with a classification accuracy of 84.07%.
In [9], MFCC was used for their content based music simi- larity search and emotion detection based on SVM classifier and achieved the performance about 70.0%. In [10] Daubechies wavelet coefficient histograms (DWCH) was introduced for music feature extraction in music information retrieval. The histograms are computed from the coefficients of the db8 Daubechies wavelet filter applied to 3 s of music. A compara- tive study of sound features and classification algorithms on a dataset compiled by Tzanetakis shows that combining DWCH with timbral features (MFCC and FFT), with the use of multiclass extensions of support vector machine, achieves approximately 80.0% of accuracy. The chromagram is a well-established method for estimating the western pitch class components within a short time-interval [11]. Jonathan Foote’s music retrieval database to do experiments and final results show that the retrieval accuracy can reach over 96.7% using chromagram as features even when the signal-to-noise ratio is 0 dB [12]. In [13] beat tracking and tempo analysis are measured for identifying the songs from large database and reported an accuracy of 51.0% and mean reciprocal ranking (MRR) of 0.53%. Overgoor [14] gives a nice overview of which beat detection algorithm can be used in what application.

Feature extraction for music emotion recognition

Mel frequency cepstral coefficients

Spectrum features are features computed from the short time Fourier transform (STFT) of an audio signal [4]. MFCC has pro- ven to be one of the most successful spectrum features in speech and music related recognition tasks. The mel-cepstrum exploits auditory principles, as well as the decorrelating property of the cepstrum. The MFCC features for a segment of music file are computed using the following procedures [15]:

The music waveform is first windowed with analysis window and the discrete short time Fourier transform (STFT) is computed.
The magnitude is then weighted by a series of filter frequency responses whose center frequencies and band- widths roughly match those of the auditory critical band filters. These filters follow the mel scale whereby band edges and center frequencies of the filters are linear for low frequency and logarithmically increase with increas- ing frequency as shown in these filters are mel-scale filters and collectively a mel-scale filter bank. This filter bank, with triangularly shaped frequency responses, is a rough approximation to actual auditory critical band filters covering a 4000 Hz range.
The log energy in the STFT weighted by each mel-scale filter frequency response is computed.
Finally discrete cosine transform (DCT) is applied to the filter bank output to produce the cepstral coefficients. This is represented by
Cepstrum(frame)= IDFT(log(|DFT(frame)|))	(1)
For acoustic feature extraction, the music signal is divided
into frames of 20 ms, with a shift of 10 ms. In this work 39th order MFCC is used to capture the static and dynamic features of music signal. It contains 13th order static coefficients, 13th

Music emotion recognition	3


order delta coefficients and 13th order acceleration (delta–delta) coefficients results in a 39 dimensional MFCC feature vector for each frame.

Residual phase

The basic idea behind the linear predictive analysis [16] is that a given music sample at n, s(n) can be estimated as a linear
samples    s_(n)   is    given    by combination of the past p music samples. The predicted
p
s_(n)=	aks(n — k)	(2)
k=1
where p is the order of prediction and the coefficients {ak}
k = 1, 2, .. ., p is the set of linear prediction coefficients (LPCs). The prediction error e(n) is defined as the difference between the actual value s(n) and the predicted value and is given by
p
e(n)= s(n)— s_(n)= s(n)—	aks(n — k)	(3)
k=1
The LPCs are obtained by minimizing the mean squared
prediction error over the analysis frame. This error e(n) is called the linear prediction (LP) residual r(n) of the music signal. The LP residual contains much information about the excitation source. The phase of the analytic signal derived from the LP residual contains better speaker specific informa- tion [17]. In this work residual phase is used for extracting the emotion specific information present in the excitation source signal. The analytic signal ra(n) corresponding to r(n) is given by
ra(n)= r(n)+ jrh(n)	(4)
where rh(n) is the Hilbert transform of r(n) and is given by
ra(n)= IFT[Rh(x)]	(5)
where
  —jR(x);	0 6 x < p

In this work music signal sampled at 44.1 kHz and the LP order 16 is used for deriving the LP residual. The LP residual is extracted from emotional music signal by pre-emphasizing the input music data using first-order digital filter and 20 ms frame size with an overlap of 50 percent between adjacent frames. The highest Hilbert envelope around 40 samples for each frame is extracted. A segment of music file from anger emo- tion, its LP residual, the Hilbert transform of the LP residual, the Hilbert envelope, and residual phase are shown in Fig. 2 and the 40 dimensional residual phase features extracted from music signal for a single frame for five emotions are shown in Fig. 3.

Techniques for music emotion recognition

Autoassociative neural network (AANN)

Neural network models can be trained to capture the nonlinear information present in the signal. In particular AANN models are basically feed forward neural network (FFNN) models which try to map an input vector onto itself. It consists of an input layer, an output layer and one or more hidden layers [18,19]. The number of units in the input and output layers is equal to the size of the input vectors. The number of nodes in the middle hidden layer is less than the number of units in the input or output layers. The middle layer is also the dimension compression hidden layer.
The activation function of the units in the input and output layers is linear (L), whereas the activation function of the units in hidden layer can be either linear or nonlinear (N). Studies on three layer AANN models show that the nonlinear activation function at the hidden units clusters the input data in a linear subspace [20]. Theoretically, it was shown that the weights of the network will produce small errors only for a set of points around the training data. When the constraints of the network are relaxed in terms of layers, the network is able to cluster the input data in the nonlinear subspace. Hence a five layer



Here R(x) is the Fourier transform of r(n) and IFT denotes
the inverse Fourier transform. The magnitude of the analytic
signal ra(n) is given by
qﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2ﬃ

	

Emotion recognition using AANN model is basically a two
stage process namely, (i). Training phase and (ii). Testing phase. During training phase, the weights of the network are adjusted to minimize the mean square error obtained for each

and the cosine of the phase of the analytic signal ra(n) is given by
for one epoch. During testing phase (evaluation), the features extracted from the test data are given to the trained AANN

cos(h(n)) = Re(ra(n)) = r(n)
(8)
model to find its match.

|ra(n)|	he(n)

In [5] it is demonstrated that the residual phase signal
contains speaker specific information that is complementary to the MFCC features. The residual phase is defined as the cosine of the phase function of the analytic signal derived from the linear prediction (LP) residual of a music signal. In this work, the emotion specific information from the residual phase features is analyzed using AANN, SVM and RBFNN. The recognition rates from the models, show that the emotion specific information is present in the music signal. The residual phase extraction is described in Fig. 1.
Support vector machines (SVM)

The support vector machine [21] is a useful statistic machine learning technique that has been successfully applied in the pattern recognition tasks [5,22]. If the data are linearly non-separable but nonlinearly separable, the nonlinear support vector classifier will be applied. The basic idea is to transform input vectors into a high-dimensional feature space using a nonlinear transformation / and then to do a linear separation in feature space as shown in Fig. 4.

4	N.J. Nalini, S. Palanivel















Figure 1	Extraction of residual phase from music signal.






















Figure 2 (a) Music signal. (b) LP residual signal. (c) Hilbert transform of residual signal. (d) Hilbert envelope. (e) Residual phase.
Figure 3	40 dimensional residual phase features for (a) Anger.
(b) Fear. (c) Happy. (d) Neutral. (e) Sad emotions.


A nonlinear support vector classifier implementing the optimal separating hyperplane in the feature space with a kernel function K(x, xi) is given by

f(x)= sgn

Nsv
i=1
aiyi K(x; xi)+ b!
(9)

where x ∈ RNf is the Nf dimensional input vector, {Xi}NSV are
the support vectors that are obtained from the training set by
vectors and yi ∈ {—1; 1} are their associated class labels. ai and an optimization process, NSV are the total number of support b are the model parameters. K (.,.) is the kernel function defin-
ing the inner product between x and xi and is given by

K(x; xi)= UT(x)U(xi)= (x; xi)	(10) the first layer selects the basis K(x; xi) from the given set of The SVM has two layers. During the learning process, bases defined by the kernel; the second layer constructs a linear
function in this space. This is completely equivalent to con- structing the optimal hyperplane in the corresponding feature space. SVM based supervised technique was proposed in [22]






Figure 4 SVM kernel function /(x) maps 2-Dimensional input space to higher 3-Dimensional feature space. (a) Nonlinear problem. (b) Linear problem.


the remaining other emotions as (—1) class for training an by labeling the music data of one emotion as (+1) class and or                         (—1). SVM hyperplane and then classified each emotion as (+1)

Music emotion recognition	5

Radial basis function neural network (RBFNN)
The radial basis function neural network (RBFNN) is a feed forward architecture with an input layer, a hidden layer and an output layer [23,25]. RBFNN solves the classification problem by transforming the input space into a high dimen- sional space in a nonlinear manner [24]. The RBFNN uses Gaussian basis function as the radial basis function in the

confidence score when compared to one or more other emotion models. Generally, FAR and FRR depend on the system threshold t. The error at which the two curves intersect represents the EER.

5.2.2. Accuracy
Accuracy or recognition rate is defined as
Number of correctly predicted testing

hidden layer [25]. The significance of the radial basis function is that the output is a function of radial distance. The Gaussian
Accuracy =
Total number of testing

basis function for the ith hidden unit for an input vector xj is
given by
The emotion recognition performance using MFCC, resid-
ual phase and combined MFCC and residual phase features

g (x )= exp
 —x — l 2!
(11)
with AANN, SVM and RBFNN is shown as a consolidated table in Table 2.

i  j	2r2

where mi and r2 are the mean and standard deviation of ith cluster. A clustering algorithm such as k-means clustering is used to cluster the training vectors into Nh clusters, where Nh is the number of units in the hidden layer. k-means clustering algorithm is employed to determine the centers. The algorithm is composed of the following steps:
Randomly initialize the samples to k means (clusters) l1,
.. ., lk
Classify n samples according to nearest lk.
Recompute lk.
Repeat the steps 2 and 3 until no change in lk.


Experimental results

Music database

In this work MER categorizes emotions into a number of classes such as anger, fear, happy, neutral and sad. The data- base used in this work is perceived emotion of music signals with 44.1 kHz sampling frequency and 16 bit monophonic PCM wave format. For each emotion 20 music files are collected of 10 s duration from various websites in .wav format, totally 100 files are collected for this work. Ten music files are randomly selected for training and the remaining ten music files are used for testing.
During testing phase, each 2 s duration of music signal is used for testing the model. The experiment is repeated once to generate 500 test cases. The performance of emotion recog- nition is assessed in terms of equal error rate (EER) and in terms of accuracy. For acoustic feature extraction, the music signal is divided into frames of 20 ms, with a shift of 10 ms.

Performance measures

The performance of emotion recognition is assessed in terms of equal error rate (EER) and accuracy or recognition rate (RR).

Equal error rate (EER)
The EER is the result obtained by adjusting the system thresh- old such that FAR and FRR are equal. A false acceptance rate (FAR) is defined as the rate at which an emotion model gives high confidence score when compared to the test emotion model. A false rejection rate (FRR) is defined as the rate at which the respective model for the test emotion gives low

MER using AANN

MFCC
During the training phase a single AANN is trained separately for each emotion. The AANN structure 39Lu 50Nu 16Nu 50Nu 39Lu achieves an optimal performance in training and testing the MFCC features for each emotion. The structure is obtained from the experimental studies. The performance of emotion recognition is obtained by varying the second (expansion layer) and third layer (compression layer) of AANN model. The effect of changing the neurons in the com- pression layer Nc on the performance of recognizing emotional music is studied. There is no major change in the performance if Nc is between 15 and 20 and the emotion recognition perfor- mance (ERP) decreases if it less than 15 or more than 20. The results are shown in Table 1. Similarly the performance is obtained by varying the number of units in the second layer (expansion layer) keeping the number of units in the compres- sion layer to 16.
The MFCC feature vectors are given as both input and out- put. The weights are adjusted to transform input feature vector into the output. The number of epochs needed depends upon the training error. In this work the network is trained for 500 epochs, but there is no major change in training error after 400 epochs and it is shown in Fig. 5.
During testing phase the MFCC features extracted from test samples are given as input to the AANN and the output is computed. The anger emotion test sample of 5 s duration is tested against all the five models and the result is shown in Fig. 6. It shows that anger emotion model gives high confi- dence scores compared to other models. The output of each model is compared with the input to compute the normalized squared error. The normalized squared error (e) for the feature
vector y is given by, e = ||y—o||2 where o the output vector is
||y||
dence score (c) using c = exp (—e). The average confidence given by the model. The error (e) is transformed into a confi- score is calculated for each model.
The category of the emotion is decided based on the highest confidence score. The performance of the music emotion





6	N.J. Nalini, S. Palanivel




















Figure 5	AANN training error Vs number of epochs for MER.


recognition using MFCC features is evaluated in terms of accuracy and equal error rate. Accuracy is shown in terms of confusion matrix.
The average emotion recognition performance for the five emotions is about 92.0%. An equal error rate (EER) of 8.0% is obtained by evaluating the performance in terms of FAR and FRR at threshold 0.69 and is shown in Fig. 7.

Residual phase
The AANN structure 40Lu 60Nu 20Nu 60Nu 40Lu achieves an optimal performance in training and testing the residual phase features for each emotion. The residual phase feature vectors are given as both input and output. The weights are adjusted to transform input feature vector into the output. The number of epochs needed depends upon the training error.
Figure 7 FAR and FRR curves using MFCC features and AANN.

During testing phase the residual phase features of test samples are given as input to the AANN and the output is computed. The output of each model is compared with the input to compute the normalized squared error. The error (e)
(—e). The average confidence score is calculated for each is transformed into a confidence score (c) using c = exp model.
In Fig. 8 the test samples of 5 s duration of anger emotion music signal are tested against all other AANN models. By evaluating the performance in terms of FAR and FRR, at threshold of 0.63, an equal error rate (EER) of 40.0% is obtained and it is shown in Fig. 9(a). The average emotion recognition performance is about 60.0%.
























Figure 6 MFCC features extracted from anger music emotion are tested against all the five AANNs. (a) Anger. (b) Fear. (c) Happy. (d) Neutral. (e) Sad.
Figure 8 Residual phase features extracted from anger emotion are tested against all the five AANNs. (a) Anger. (b) Fear. (c) Happy. (d) Neutral. (e) Sad.

Music emotion recognition	7




















Figure 9	FAR and FRR curves for (a) residual phase and (b) combined features using AANN.

Combined features
The excitation (residual phase) and spectral features (MFCC) are combined because of its complementary nature. The two features are combined at score level using
c = ws1 + (1 — w)s2	(12)
where s1 and s2 are the scores or output of the model for
MFCC and residual phase features, respectively and w is the weight, 0 6 w 6 1. In this work it is observed that for the weight 0.6 (w = 0.6) an EER of about 4.0% is achieved using AANN by combining the features and it is shown in Fig. 9(b). The overall recognition performance for the combined MFCC and residual phase features is about 96.0%. The emo- tion recognition performance of the combined system is increased than individual system due to the complementary nature of residual phase with MFCC and also it shows that
residual phase contained emotion specific information.

MER using SVM

Figure 10	FAR and FRR curves using MFCC features and SVM.

SVM is trained to distinguish the acoustic features (class label ’+1’) of an emotion and all other acoustic features (class label





















Figure 11	FAR and FRR curves for (a) residual phase and (b) combined features using SVM.

8	N.J. Nalini, S. Palanivel

MFCC
During testing the MFCC features extracted from the test utterances are given as input to the SVM model. The distance between each of the feature vectors and the SVM hyperplane is obtained. The number of positive score is calculated for each emotion model. Based on the score, the category of emotion is decided. This process is repeated for all the test utterances of emotions. The average emotion recognition performance of 94.0% is obtained as a result
By varying the threshold FAR and FRR are obtained from the scores. The EER of 6.0% is obtained at threshold 0.49 and it is shown in Fig. 10. A false acceptance rate (FAR) is defined as the rate at which an emotion model gives high score when compared to the test emotion model. A false rejection rate (FRR) is defined as the rate at which the respective model for the test emotion gives low score when compared to other emotion models.


Figure 12	Music emotion recognition error rate with respect to number of mean centers for each emotion.
Residual phase
In this work LP order 16 is used for deriving the LP residual. The LP residual is extracted from emotional music signal by pre-emphasizing the input music data using first-order digital filter and 20 ms frame size with an overlap of 50 percent between adjacent frames. The highest Hilbert envelopes around 40 samples for each frame are extracted. A single SVM is developed for one emotion. Five SVM models are created for residual phase features. The average emotion recognition performance of about 56.0% is obtained after testing all the test utterances.
By evaluating the performance in terms of FAR and FRR from the scores, EER of 44.0% is obtained at threshold 0.16 and it is shown in Fig. 11(a).

Combined features
It is observed that an EER of about 1.0% for the combined features by evaluating the FAR and FRR values and it is shown in Fig. 11(b). The overall emotion recognition system is obtained by combining the evidence of MFCC and residual phase features and their average performance is about 99.0% is obtained.





Figure 13 Testing MFCC features of anger music emotion using RBFNN. (a) Anger. (b) Fear. (c) Happy. (d) Neutral. (e) Sad (five outputs of RBFNN).



’—1’) in the training set. The MFCC features extracted from each frame are of 39 dimensional and residual phase feature
extracted from each frame is of 40 dimensional is given as input to SVM model. For each music emotion a single SVM model is created. All music emotions belong to same category is merged into a single category. Totally five SVM models are created. In the training phase, SVM is trained to distinguish acoustic features of one emotion (+1) and acoustic features
of all other emotions (—1). The SVM is trained with Gaussian,
polynomial and sigmoidal kernel functions, among which
Gaussian kernel gives better emotion recognition performance.










Figure 14	FAR and FRR curves using MFCC features and RBFNN.

Music emotion recognition	9



Figure 15 Testing residual phase features of anger music signal using RBFNN. (a) Anger. (b) Fear. (c) Happy. (d) Neutral. (e) Sad (five outputs of RBFNN).
MER using RBFNN
The music emotion recognition system is evaluated in terms of radial basis function neural network model. Five types of emotions namely anger, fear, happy, neutral and sad are studied in this work. All five emotions are recorded for 10 s at 44,100 samples per second. For music emotion recognition, different music emotion signals are analyzed by dividing it into 20 ms with a shift of 10 ms. A 39 dimensional MFCC feature vector and 40 dimensional residual phase feature vectors are extracted from each frame of the music signal. All the feature vectors (MFCC/residual phase) belong to a same category are merged into a single file. The file is given as input to the RBFNN model. k-means algorithm is used to find the RBF centers and the weights are calculated using least square algorithm. The value of k varies from 1 to 10 in this work for each emotion. The system gives an optimal performance for k = 6 for MFCC and residual phase feature as shown in Fig. 12. For training

50,000 feature vectors are extracted from input file (10,000 × 5). The size of the G matrix is 50,000 × 31 (last column of G matrix is bias value of RBFNN). The size of the weight matrix is 31 × 5 is calculated using least square algorithm.

MFCC
During testing phase the MFCC features extracted from test samples are given as input to the RBFNN and output is com- puted. The anger emotion test sample of 5 s duration is tested and the result is shown in Fig. 13.
It shows that anger emotion is recognized better than other emotional music samples. By evaluating FAR and FRR for various values of threshold, EER is obtained. In Fig. 14 an EER of about 9.0% at threshold of 0.54 is obtained.

Residual phase
Other than the spectral features, excitation source feature is
matrix of size 31 × 5 is calculated using the least square algo- used to evaluate the performance of RBFNN. The weight rithm. For evaluating the performance of the system the test
data of 5 s music signal is used. The 500 frames anger music signal is tested against other emotions using RBFNN and it is shown in Fig. 15.
The overall recognition is reported about 53.0% and another performance measure EER of about 47.0% is obtained from FAR and FRR by varying the threshold and it is shown in Fig. 16(a).

Combined features
For evaluating the RBFNN model, it is observed that an EER of about 4.0% by evaluating the FAR and FRR values is shown in Fig. 16(b). The overall recognition performance of 95.0% is obtained by combining the evidence of MFCC and residual phase features.
The consolidated music emotion recognition performance using AANN, SVM and RBFNN is shown in Table 2. The experiment results show that MFCC with residual phase fea- tures gives an optimal performance of 99.0% and EER of 1.0% using SVM, when compared to other models. It indicates that residual phase contains emotion specific information in music and the combined MFCC and residual phase increases the rate of recognition.





















Figure 16	FAR and FRR curves for (a) residual phase and (b) combined features using RBFNN.

10	N.J. Nalini, S. Palanivel









Conclusion

In this work, methods were proposed for combined mel frequency cepstral coefficients (MFCC) and residual phase (RP) features for emotion recognition in music (audio). Emotion recognition in music considers the emotions namely anger, fear, happy, neutral and sad. For music emotion recog- nition, MFCC (spectral features) and residual phase features (excitation source) were extracted from the music, and were used to create models for each emotion using AANN, SVM and RBFNN. The MFCC and residual phase features were combined, due to the complementary nature of emotion speci- fic information present in the residual phase in comparison with the information present in the conventional MFCC which increase the emotion recognition performance. AANN is used to capture the distribution of the acoustic feature vectors and is very effective in recognition of emotion in music with an accuracy of 96.0%. A nonlinear support vector machine learn- ing algorithm is applied to obtain the optimal class boundary between the various emotions namely anger, fear, happy, neu- tral and sad and obtained the recognition accuracy of 99.0%. The distribution of the acoustic features of music was captured using a Gaussian distribution in RBFNN shows the recogni- tion accuracy of 95.0%. SVM shows the highest performance for MER with MFCC and residual phase features.

References

Yang Yi-Hsuan, Lin Yu-Ching, Ya-Fan Su, Homer H. A regression approach to music emotion recognition. IEEE Trans Audio Speech Lang Process 2008;16(2):448–57.
Thayer RE. The biopsychology of mood and arousal. New York: Oxford University Press; 1989.
Yang Yi-Hsuan, Chen Homer H. Music emotion recognition. CRC Press; 2011.
Zhu Bin, Zhang Kejun. Music emotion recognition system based on improved GA-BP. Comput Des Appl 2010:409–11.
Koolagudi Shashidhar G, Sreenivasa Rao K. Emotion recogni- tion from speech: a review. Int’l J Speech Technol 2012;15:99–117.
Kim Youngmoo E. Music emotion recognition: a state of the art review. In: Eleventh international society for music information retrieval conference; 2010. p. 255–66.
Kaminskas Marius, Ricci Francesco. Contextual music informa- tion retrieval and recommendation: state of the art and challenges. Comput Sci Rev 2012;6:89–119.
Lee Chang-Hsing, Shih Jau-Ling. Automatic music genre classi- fication based on modulation spectral analysis of spectral and cepstral features. IEEE Multimedia 2009;11(4).
Li Tao, Ogihara Mitsunori. Content-based music similarity search and emotion detection. In: Int’l conference on acoustics, speech and signal processing; 2004. p. 705–8.
Li Tao, Ogihara Mitsunori. Toward intelligent music information retrieval. IEEE Multimedia 2008;8(3).
Schmidt EM, Turnbull D, Kim YE. Feature selection for content- based, time varying musical regression. In: Int’l conference on multimedia information retrival; 2010. p. 267–74.
Yu Xiaoging, Zhang Jing, Yang Wei. An audio retrieval method based on chroma-gram and distance metrics. In: Int’l conf on audio language and image processing; 2010. p. 23–5.
Ellis D, Poliner PW, Graham E. Identifying cover songs with chroma features and dynamic programming beat tracking. In: IEEE conf on acoustic, speech, and signal processing, vol. 4; 2007.
p. 1429–32.
Overgoor JM. An evaluation method for audio beat detectors. In: Fourth twente student conference on IT; 2006. p. 23–9.
Jothilakshmi S, Ramalingam Vennila, Palanivel S. Unsupervised speaker segmentation with residual phase and MFCC features. Expert Syst Appl 2009;36(6):9799–804.
Rabiner Lawrence, Schafer Ronald. Theory and applications of digital speech processing. England: Pearson Education; 2010.
Sri Rama Murty K, Yegnanarayana B. Combining evidence from residual phase and MFCC features for speaker recognition. IEEE Signal Process Lett 2006;13(1):52–5.
Palanivel S. Person authentication using speech, face and visual speech, Ph.D thesis, Department of Computer Science and Engg. Madras: Indian Institute of Technology; 2004.
Yegnanarayana B, Kishore SP. AANN: an alternative to GMM for pattern recognition. Neural Networks 2002;15:459–569.
Nicholson J, Takahashi K, Nakatsu R. Emotion recognition in speech using neural networks. Neural Comput Appl 2000;9:290–6.
Vapnik V. Statistical learning theory. New York: John Wiley and Sons; 1998.
Shen Peipei, Pan Yixiong, Shen Liping. Speech emotion recogni- tion using support vector machine. Int’l J Smart Home 2012;6(2).
Haykin S. Neural networks, a comprehensive foundation. Singa- pore: Pearson Education; 2001.
Yegnanarayana B. Artificial neural networks. New-Delhi: Pren- tice-Hall of India; 1999.
Koh LH, Ranganath S, Venkatesh YV. An integrated automatic face detection and recognition system. Pattern Recogn 2002;35: 1259–73.
