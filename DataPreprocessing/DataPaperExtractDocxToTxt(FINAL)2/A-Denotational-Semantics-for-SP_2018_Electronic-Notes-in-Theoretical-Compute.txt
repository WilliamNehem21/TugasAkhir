Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 336 (2018) 223–239
www.elsevier.com/locate/entcs
A Denotational Semantics for SPARC TSO
Ryan Kavanagh1 Stephen Brookes2
Computer Science Department Carnegie Mellon University Pittsburgh, PA, USA


Abstract
The SPARC TSO weak memory model is defined axiomatically, with a non-compositional formulation that makes modular reasoning about programs difficult. Our denotational approach uses pomsets to provide a compositional semantics capturing exactly the behaviours permitted by SPARC TSO. Our approach facilitates the study of SPARC TSO and supports modular analysis of program behaviour.
Keywords: SPARC TSO, denotational semantics, pomsets, concurrency, weak memory models.

Introduction
A memory model specifies what values can be read by a thread from a given memory location. Traditional concurrency research has assumed sequential consistency, wherein memory actions operate atomically on a global state, and a read is guaranteed to observe the value most recently written to that location globally. Consequently, “the result of any execution is the same as if the operations of all the processors were executed in some sequential order” [10]. However, sequential consistency negatively impacts performance, and modern architectures often provide much weaker guarantees. These weaker guarantees mean that classical concurrency algorithms, which often assume sequential consistency, can behave in unexpected ways. Consider, for example, the Dekker algorithm on a system using the SPARC instruction set. The Dekker algorithm seeks to ensure that at most one process enters a critical section at a time. Executing the following instance of the Dekker algorithm on a sequentially consistent system from an initial state where memory locations w, x, y, z are all zero will ensure that we end in a state where not both z and w are set to

1 Email: rkavanagh@cs.cmu.edu. Funded in part by a Natural Sciences and Engineering Research Council of Canada (NSERC) Postgraduate Scholarship.
2 Email: brookes@cs.cmu.edu

https://doi.org/10.1016/j.entcs.2018.03.025
1571-0661/© 2018 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

one:
(x := 1; if y =0 then z := 1 else skip)  (y := 1; if x =0 then w := 1 else skip). However, the SPARC ISA provides the weaker SPARC TSO (total store ordering) memory model. Under SPARC TSO, it is possible to start from the aforementioned
initial state and end in a state where both z and w are set to one, thus violating mutual exclusion.
Weak memory models are often described in standards documents using natural language. This informality makes it difficult to reason about how programs will behave on systems that use these memory models. The SPARC Architecture Manual [12] gives an axiomatic description of TSO using partial orders of actions, and we present this axiomatic description in Section 2. Even with the formality of this axiomatic account, reasoning about the behaviour of programs is difficult, because the axiomatic approach is non-compositional and precludes modular reasoning. We address this problem by presenting a denotational semantics for SPARC TSO in Section 3. Our denotational semantics assigns to each program a collection of pomsets. Pomsets are generalisations of traces and were first used by Pratt [11] to give denotational semantics for concurrency, and later by Brookes [6], with some modifications, to study weak memory models. We illustrate our semantics by validating various litmus tests and expected program equivalences. To ensure our denotational semantics accurately captures the behaviour of SPARC TSO, we show in Section 4 that from every denotation of a program we can derive a collection of partial orders satisfying the axiomatic description of Section 2 and, moreover, that we can derive every such partial order from the denotation.
An Axiomatic Account
The SPARC manual [12] gives an axiomatic description of TSO in terms of partial orders of actions. Unfortunately, this description is incomplete because it fails to specify the fork and join behaviour of TSO. In this section, we fully axiomatise the SPARC manual’s account of TSO. Before doing so, we give an informal description of TSO to help build intuition.
A system providing the TSO weak memory model can be thought of as a collection of processors, each with a write buffer. Whenever a processor performs a write, it places it in its write buffer. The buffer behaves as a queue, and writes migrate out of the buffers one at a time, and shared memory applies them according to a global total order. Whenever a processor tries to read a location, it first checks its buffer for a write to that location. If it finds one, it uses the value of the most recent such write; otherwise, it looks to shared memory for a value. Because of buffering, it is possible for writes and reads to be observed out of order relative to the program order.
Program Order Pomsets
To make the above intuition precise, we must formalise the notion of a program order, i.e., the ordering of read and write actions specified by a program. We do so by means of partially-ordered multisets.

A (strict) partially-ordered multiset or pomset (P, <P , Φ) over a label set L consists of a strict poset (P, <P ) of “action occurrences” and a function Φ : P → L mapping each action occurrence to its label or “action”. Denote by Pom(L) the set of pomsets over L.
We do not usually make the poset P explicit, because the structure of the pomset is invariant under relabellings of the elements of P . Consequently, we identify pomsets (P, <P , Φ) and (P j, <j , Φj) if there exists an order isomorphism φ : P → P j such that Φ = Φj ◦ φ. We usually denote the elements of the pomset using just their labels, but we sometimes need to specify their exact occurrence, in which case we write lp, where l = Φ(p).
It is useful to draw a pomset P as a labelled directed acyclic graph, where multiple vertices can have the same label and we have an edge a → b if a <P b. For clarity, we omit edges obtained by transitivity of <P . For example, the following graph depicts the pomset where P = {0, 1, 2, 3}, the order is given by 0 < 1, 1 < 2, 0 < 2, and 0 < 3, and Φ is given by Φ(0) = a, Φ(1) = b, Φ(2) = a, and Φ(3) = c:

c ,r  a
zb  ,	za .,

We assume a countably infinite set of locations Loc, ranged over by metavariables x, y, z, . . . , and a set of values V , ranged over by v. In our examples, we will take V to be the set of integers. We call x := v a global write action, x = v a read action, and δ a skip action. Let Aw and Ar be the sets of global write actions and read actions, respectively.
A program order is a pomset P over the set of action labels APO = Aw ∪Ar ∪{δ}
with the (locally) ﬁnite height property, that is, such that for all b ∈ P , the set
{a ∈ P | a <P b} is finite.
Intuitively, the program order
x := 2	zy ,=1	y := 1	zy ,=1 
describes the parallel execution of writing 2 to x before reading 1 from y, and writing 1 to y before reading 1 from y, with no other ordering constraints.
TSO Axioms
A global state is a finite partial function from locations Loc to values V . We let ΣPO = Loc ~fin V be the set of global states, and use σ to range over ΣPO.
A list L determines a strict total order <L on its elements, where λ <L λj if and only if λ appears to the left of λj in L. Given any set S and partial order <S on it, every element s ∈ S determines a set s↓S = {sj ∈ S | sj <S s}∪ {s} called its lower closure. Write s #S sj to denote that s and sj are not comparable under <S and s /= sj, and write s ⊥S sj to denote that they are comparable.
Definition 2.1 Let P be a program order and <T be a strict partial order on the elements of P . We say <T is TSO-consistent with P from (the initial state) σ if it satisfies the following six axioms:
(O) Ordering: <T is a total order on the write actions Aw of P .
Value: for all reads (x = v)r in P , either

	there exists a write (x := vj)w maximal under <T amongst all writes to x in (x = v)r↓T , all writes to x in (x = v)r↓P are in (x := vj)w↓T , and v = vj; or
there exists a write (x := vj)w maximal under <P amongst all writes to x in
(x = v)r↓P , and both (x = v)r <T (x := vj)w and v = vj; or
there are no writes to x in (x = v)r↓T or (x = v)r↓P , and σ(x)= v.
(L) LoadOp: for all reads r ∈ P and all actions a ∈ P , r <P a implies r <T a.
(S) StoreStore: for all writes w, wj ∈ P , w <P wj implies w <T wj.
(F) Fork: if α1 <P α2, α1 <P α3, and α2 #P α3, then α1 <T α2 and α1 <T α3.
(J) Join: if α1 <P α3, α2 <P α3, and α1 #P α2, then α1 <T α3 and α2 <T α3.
We simply say <T is TSO-consistent with P if there exists some initial state σ from which they are TSO-consistent.


Axioms (O), (Va), (Vb), (L), and (S) are directly adapted from the formal specification given in Appendix K.2 of [12]. We introduce axiom (Vc) to simplify our presentation of examples. By requiring that programs first write to any locations from which they read, it can be omitted, and apart from examples, we will assume throughout that our TSO-consistent orders do not require (Vc). Though the formal specification does not provide axioms (F) and (J), they are consistent with the behaviour intended by Appendix J.6 of [12]. Intuitively, axiom (Vb) requires that whenever a processor reads from a location, it must use the most recent write to that location in its write buffer (if it exists), and if there is no such write in its write buffer, but we have observed a global write to that location, then (Va) requires that the most recent such write be the one read. Our presentation differs slightly from the formal specification. In particular, we do not consider instruction fetches or atomic load-store operations, and we do not consider flush actions, because they can be implemented as a derived action in our semantics by forking and immediately joining. To be consistent with our pomset development, we also assume the order to be strict. If <T is TSO-consistent for P , then there exists a (not necessarily unique) total order on P that is TSO-consistent with P and contains <T . This is consistent with our physical intuition: given an actual execution, we can imagine taking a temporal linearisation of the operations. As a result, we can view all orders that are TSO-consistent with P as weakenings of total orders that are TSO-consistent with
P . One should be careful not to conflate linearisations and TSO-consistent total orders. Consider, for example, the program order
x := 2   zx ,=2   x := 3   zx ,= 3 .
The linearisation x := 2 < x := 3 < x =3 < x =2 is not TSO-consistent with the program order because it violates (Va); the order x =2 < x := 2 < x =3 < x := 3 is not a linearisation of the program order but is TSO-consistent with it.
When we have a write followed by a read in the program order, but swapped in the linear order, as in this example, we can imagine the write having gotten stuck in the write buffer, and observing the read before the write.

A Denotational Account
So far we have dealt with program orders as an abstract concept. However, there exist program orders having no TSO-consistent orders. Consider, for example, the program order {x = 1 < x = 2}. We therefore restrict our attention to program orders for well-defined programs in the simple imperative language given below. These program orders are defined in Section 3.2.
Restricting our attention to program orders of well-defined programs raises the question of compositionality. The key is to find a way to derive TSO-consistent orders for a sequential composition c1; c2 or parallel composition c1  c2 given TSO- consistent orders for c1 and c2. This is infeasible with the axiomatic approach, which requires reasoning about whole programs and is inherently non-compositional. In contrast, a denotational approach using pomsets is compositional: it allows us to derive the meaning of a program from the meaning of its parts.
Our denotational semantics has two components. The first associates to each program a set of TSO pomsets, which serves as the abstract meaning or denotation of the program. This component is described in Section 3.3. The second associates to each pomset a set of executions, which describe its input-output behaviours. This is described in Section 3.4.2.
A Simple Imperative Language
We express our programs using a simple imperative language. This formalism avoids the complexity of high-level languages, while still capturing the programs we are interested in. In the syntax below, e ranges over integer expressions, b over boolean expressions, c over commands, and p over programs. We distinguish between commands and programs, because although commands can be composed to form new commands, programs are assumed to be syntactically complete and executable.
v ::= ..., −2, −1, 0, 1, 2,... 
e ::= v | x | e1 + e2 | e1 ∗ e2 | · · ·
b ::= true | false | ¬b | e1 = e2 | e1 < e2 | b1 ∨ b2 | b1 ∧ b2 | · · ·
c ::= skip | x := e | c1; c2 | c1  c2 | if b then c1 else c2 | while b do c p ::= c
Let VExp denote the set of integer expressions, BExp the set of boolean expressions, and Cmd the set of commands.
PO Pomsets
Given a command c in our language, we must now compile it down to its set PPO(c) of program order pomsets. We need operations for the sequential and parallel composition of pomsets over the same set of labels. The sequential composition (P0, ≤0, Φ0); (P1, ≤1, Φ1) of pomsets is (P0, ≤0, Φ0) whenever P0 is infinite, and otherwise is (P0 P1, ≤, Φ) where P0 P1 = {0}× P0 ∪ {1}× P1 is the disjoint union, (i, a) ≤ (j, b) if and only if i = j and a ≤i b, or i = 0 and j = 1, and Φ((i, a)) = Φi(a). The parallel composition (P0, ≤0, Φ0)  (P1, ≤1, Φ1) of pomsets is (P0  P1, ≤, Φ) where (i, a) ≤ (j, b) if and only if i = j and a ≤i b, and Φ((i, a)) = Φi(a). The empty

pomset 0 = (∅, ∅, ∅ → L) is the unit for sequential and parallel composition. Given a pomset P on a set of labels L and a subset Lj ⊆ L, the restriction P TL′ of P to Lj is the pomset on Φ—1(Lj) whose ordering is induced by P . The deletion of Lj from P is P TL\L′ . We lift these operations to sets of pomsets in the obvious manner.
Because δ has no effects, we identify program orders P and P j whenever there exists a non-empty pomset Pδ that can be obtained by deleting a finite number of δ actions from P and also by deleting a finite number of δ actions from P j.
For expository convenience, we identify lists and linear pomsets, where we call a pomset linear if its underlying poset is linear. Explicitly, we identify [ ] with 0 and [ λ1,..., λn ], with the pomset {λ1}; ··· ; {λn}. To minimise notation, we leverage this identification and write L; Lj to denote the concatenation of the lists L and Lj.
We begin with the program order denotation of expressions. To each expression
e, we assign a set PPO(e) of tuples of program orders and corresponding values:
PPO : VExp → ℘(Pom(APO) × V )
PPO(v)= {({δ}, v)}
PPO(x)= {({x = v}, v) | v ∈ V }
PPO(e1 ⊙ e2)= {(P1  P2, v1 ⊙ v2) | (Pi, vi) ∈ PPO(ei)}
where ⊙ ranges over binary operations. Read expressions x are associated with arbitrary values in V for reasons of compositionality: we do not know with which writes the read may eventually be composed, and so we need to permit reading arbitrary values. We chose to evaluate binary operations e1 ⊙ e2 in parallel; one could just as legitimately have chosen to sequentialise the evaluation and written P1; P2. We assume v1 ⊙v2 ∈ V to be the result of applying the binary operation ⊙ to v1 and v2. We handle program orders for unary expressions analogously, and assume
¬bj is the result of negating the boolean value bj. To simplify the clauses involving conditionals, we give helper functions Ptrue and Pfalse to pomsets corresponding to the given boolean values.
PPO : BExp → ℘(Pom(APO) × Bool)
PPO(b)= {({δ}, b)}	(b ∈ {true, false})
PPO(¬b)= {(P, ¬bj) | (P, bj) ∈ PPO(e)}
PPO(e1 ⊙ e2)= {(P1  P2, v1 ⊙ v2) | (Pi, vi) ∈ PPO(ei)} Ptrue(b)= {P | (P, true) ∈ PPO(b)}
Pfalse(b)= {P | (P, false) ∈ PPO(b)}
Note that in the case of boolean binary operations, the ei might be integer or boolean expressions, and the corresponding semantic clause for PPO(ei) should be used.
We give the program order denotation of commands in a similar manner, this time associating sets of program orders to each command phrase:
PPO : Cmd → ℘(Pom(APO))
PPO(skip)= {{δ}}
PPO(x := e)= {P ; {x := v}| (P, v) ∈ PPO(e)} PPO(c1; c2)= PPO(c1); PPO(c2)

PPO(c1  c2)= PPO(c1)  PPO(c2)
PPO(if b then c1 else c2)= Ptrue(b); PPO(c1) ∪ Pfalse(b); PPO(c2)
∞
PPO(while b do c)=	In(b, c) ∪ Iω(b, c),
n=0
where	I0(b, c)= Pfalse(b)
In+1(b, c)= Ptrue(b); PPO(c); In(b, c)
The only interesting clause is for while b do c. Here, we take union of all of the finite unrollings In(b, c) of the loop. We must also consider the case of an infinite loop. This is captured by Iω(b, c), which describes the infinite pomset obtained by unrolling the loop arbitrarily many times. The while b do c clause also illustrates why we associate the pomset {δ} instead of 0 to values: otherwise, we would have PPO(while false do c)= {0}, and this would break both our input-output executions and our intuition that this program should be denotationally equivalent to skip.
To illustrate the above semantic clauses, we return to the Dekker program from the introduction. This program has pomsets of each of the following forms, for each choice of v /= 0 and vj /= 0:

x := 1
y := 1 ,	x := 1
y := 1 ,	x := 1
y := 1 ,	x := 1
y := 1 .

,	, 
,	, 
,	, 
,	, j

y =0  x =0 
,	, 
y = v	x =0 
, 
y =0 x = v
, 
y = v x = v

z := 1 w := 1
w := 1
z := 1

The first program order describes an execution where we read both y = 0 and x =0 and where Dekker fails. The next three forms of program orders describe executions in which one or both reads obtain a non-zero value.
TSO Pomsets
As described above, we wish to assign a set PTSO(c) of TSO pomsets to each command c. Doing so will involve the careful modelling of write buffers. For compactness, we will write P instead of PTSO in this section’s semantic clauses.
We introduce a set BLoc = {x¯ | x ∈ Loc} of buffer locations and let the set of

buffer write actions be Ab = {x¯
:= v | x¯
∈ BLoc, v ∈ V }. An action x¯
:= v

by a thread denotes adding a write x := v to the thread’s write buffer. The set of TSO actions ATSO then consists of APO extended with Ab. A TSO pomset will then be a pomset in Pom(ATSO) satisfying the finite height property.
To capture the effects of buffers, we parametrise our semantic clauses with lists of global write actions, which represent the writes currently in our buffer. We let Ls = Aw list be the set of all such lists. The intuition is that write buffers behave as queues under TSO, and we can use a list L ∈ Ls to model a queue by dequeuing from the head of the list and enqueuing at the end of the list. We use lists to model queues rather than some abstract data structure because of our convenient identification between lists and linear pomsets.
The semantic clauses are given in two strata. The semantic clauses BL for “basic TSO pomsets” capture the meaning of the syntactic phrases in a manner very similar to the program order definitions in Section 3.2. BL assigns to each command phrase

a set of pairs of TSO pomsets and buffer lists. The pomset component captures the meaning of the phrase in the presence of the buffer L, while the buffer component captures the state of the buffer after performing the actions associated with the phrase. In the second stratum, we use PL clauses to capture the meaning of the phrase in the presence of buffer flushing. Flushing a write consists of dequeuing a global write x := v from L and inserting it in the pomset. PL again takes command phrases to subsets of Pom(ATSO) × Ls.
To generate TSO pomsets, we modify the semantic clauses generating program orders in four key places to get our basic pomsets. The first is for write commands x := e. Starting from a buffer L ∈ Ls, we get the pomset P and associated value v for e using PL(e). The buffer L may have changed to a buffer Lj while we were evaluating e, and PL(e) also gives us this Lj. Instead of immediately making a global write to x as we would have in the program order clause, we enqueue the global write on the buffer Lj:
BL(x := e)= {(P ; {x¯ := v}, Lj; {x := v}) | (P, v, Lj) ∈ PL(e)}.
We must also change the semantic clauses for read expressions. By axiom (Vb), whenever we read from a location x, we must use the most recent value available for it in the write buffer, if available. We use the following helper function to convert a buffer L ∈ Ls to a partial function βL : Loc ~fin V giving us the value of the most recent write in L to a given location:

β (x)= undeﬁned,	β
(y) =  v	if x = y

Then, the semantic clause giving us the basic pomsets for reads is
BL(x)= {({x = v}, v, L) | βL(x)= v}∪ {({x = v}, v, L) | x ∈/ dom(βL),v ∈ V }. The first part tells us to use the value associated with x in the buffer L, if available. The second part uses arbitrary values if unavailable, as with program orders.
The third major change involves parallel composition. We explain parallel composition of expressions; parallel composition of commands is analogous. By axioms (F) and (J), we must flush our buffers before every fork and join. We therefore begin by flushing our entire buffer, i.e., by taking L and placing it at the beginning of our pomset. Having flushed the buffer, we then evaluate the ei with empty buffers and get back pomsets Pi and vi. Because we can only join threads if their buffers are empty, we require that these Pi and vi be associated with empty buffers in P[ ](ei). We then proceed as for the program order, and add the parallel composition of the Pi to our pomset, and compute the value v1 ⊙ v2. Because we just joined two empty buffers, our resulting buffer is empty:
BL(e1 ⊙ e2)= {(L; (P1  P2), v1 ⊙ v2, [ ]) | (Pi, vi, [ ]) ∈ P[ ](ei)}.
Finally, when we sequentially compose two commands c1 and c2 (assuming no forking or joining), c2 continues executing from the buffer c1 finished with. We express this using the polymorphic helper function a:
a : ∀A.∀B.℘(Pom × A) → (A → ℘(Pom × B)) → ℘(Pom × B) S a f = {(P ; Pj, b) | (P, a) ∈ S ∧ (P j, b) ∈ f (a)}

Taking A = B = Ls, sequential composition can be expressed using as
BL(c1; c2)= PL(c1) a P—(c2),
where we take P—(c) to be a function of type Ls → ℘(Pom(ATSO) × Ls). Explicitly, this means BL(c1; c2)= {(P1; P2, L2) | (P1, L1) ∈ PL(c1), (P2, L2) ∈ PL1 (c2)}. This
idiom of chaining pairs of pomsets and buffers together using a will be useful
throughout. We make a polymorphic so that we can handle, e.g., the case of A = Ls
and B = V × Ls below.
The remainder of the basic clauses are analogous to those for program order pomsets, subject to the modifications described above:
B : VExp → Ls → ℘(Pom(ATSO) × V × Ls) BL(v)= {({δ}, v, L)}
B : BExp → Ls → ℘(Pom(ATSO) × Bool × Ls) BL(¬e)= {(P, ¬b, Lj) | (P, b, Lj) ∈ BL(e)}
PL,true(b)= {(P, Lj) | (P, true, Lj) ∈ PL(b)}
PL,false(b)= {(P, Lj) | (P, false, Lj) ∈ PL(b)} B : Cmd → Ls → ℘(Pom(ATSO) × Ls)
BL(skip)= {({δ}, L)}
BL(c1  c2)= {(L; (P1  P2), [ ]) | (Pi, [ ]) ∈ P[ ](ci)}
BL(if b then c1 else c2)= PL,true(b) a P—(c1) ∪ PL,false(b) a P—(c2) BL(while b do c)= [ In(b, c) ∪ Iω(b, c)
where	I0 (b, c)= PL,false(b)
In+1(b, c)= PL,true(b) a P—(c) a In(b, c)
L	—
We now turn our attention to flushing. The intent is that a thread can flush arbitrarily many of its writes at any point in its execution. Thus, the pomsets associated with flushes for a buffer L are the prefixes Lj of L, and the resulting buffers are the remainders of L. We use split(L) to denote these prefix-suffix pairs:
split : Ls → ℘(Pom(ATSO) × Ls) split(L)= {(Lj, Ljj) | L = Lj; Ljj}
We introduce a variant of a to cope with triples of pomsets, values, and buffers, and will rely on types to disambiguate the version needed in any given situation:
a : ∀A.∀B.℘(Pom × A × B) → (B → ℘(Pom × B)) → ℘(Pom × A × B) S a f = {(P ; Pj, A, Bj) | (P, A, B) ∈ S ∧ (P j, Bj) ∈ f (B)}
We define the TSO pomsets P in terms of B and split. P composes split and B in a manner that we can flush some writes from the buffer, then evaluate e or perform c, and then flush some writes at the end:
PL(e)= split(L) a B—(e) a split
PL(c)= split(L) a B—(c) a split
We can validate various expected equivalences by unfolding these definitions. For ex-

ample, sequential composition of commands is associative, because PL(c1; (c2; c3)) = split(L) a ¥—(c1) a split a ¥—(c2) a split a ¥—(c3) a split = PL((c1; c2); c3). Us- ing the identity 0; P = P and the fact that parallel composition of pomsets is asso- ciative, one can show that parallel composition of commands is associative, i.e., that PL(c1  (c2  c3)) = PL((c1  c2)  c3). The parallel composition of pomsets commutes, so the parallel composition of commands commutes, i.e., PL(c1  c2)= PL(c2  c1).
Because we expect programs to be run from an empty buffer and to only stop after emptying all buffers, we let the set of TSO pomsets associated with a program p be given by PTSO(p)= {P | (P, [ ]) ∈ P[ ](p)}.
We illustrate the constructions by giving four example families of TSO pomsets for the Dekker program from the introduction, again assuming v /= 0 and vj /= 0:

x¯ := 1
, 
x := 1
, 
y¯ := 1 ,
, 
y := 1
, 
x¯ := 1
, 
y =0 
, 
y¯ := 1 ,
, 
x =0 
, 
x¯ := 1
, 
x := 1
, 
y¯ := 1 ,
, 
y := 1
, 
x¯ := 1
y =,v 
, 
y¯ := 1 .
, 
y := 1
, j

y =0 
, 
z¯ := 1
, 
x =0 
, 
w¯ := 1
, 
x := 1
, 
z¯ := 1
, 
y := 1
, 
w¯ := 1
, 
y = v	x =0 
, 
w¯ := 1
, 
x := 1 x = v

z := 1 w := 1
z := 1 w := 1
w := 1

In the first family of pomsets, we flush the writes immediately after inserting them in the buffers, while in the second, we flush the writes to x and y after reading y and x. In the third family, we flush x right after placing its write in the buffer, but fall into the false case of the conditional after reading some value v /= 0, thus taking the skip branch. In the fourth pomset, we read y after placing the write x := 1 in the buffer, but before it gets flushed, and both threads fall into the skip branch.
Executions
Our TSO pomset semantics gives an abstract account capturing families of possi- ble executions. However, compositionality comes with its price: we associate to programs some pomsets that cannot in any real sense be “executed”. Consider
for example, the pomset x := 2 → x¯ := 2 → x = 1 → · · · for the program
c = (x := 2; if x = 1 then c1 else c2). In no circumstance do we expect to execute c1 when this program is run alone, and so the above TSO pomset has, in a sense made precise later, no executional meaning. However, compositionality requires this pomset be associated with the command c, because one could execute c1 if our program were instead c  x := 1. Our notion of execution filters out these pomsets with no executional meaning and yields an input/output behaviour for programs built from their pomset semantics.
Buffered States
Our notion of execution requires the concept of a buffered global state, i.e., a global state with a write buffer per thread. We execute threads individually. Each thread’s execution starts from a state with a buffer, which in combination reflect that thread’s view of shared memory. Let Locs = BLoc ∪ Loc be the set of all locations. To model the combination of a global state and a buffer, we use elements of Σ= (BLoc ~fin (V ×N)/≈)×(Loc ~fin V ), where ≈ is the least equivalence relation

generated by (v, 0) ≈ (vj, 0) for all v, vj ∈ V . Because Loc and BLoc are disjoint, we can and will identify Σ with the its obvious inclusion in Locs ~fin ((V × N)/≈∪ V ). The intuition is that if σ(x¯)= (v, n), then there are n writes to x in σ’s write buffer, and the most recent write to σ was the value v. We need to keep track of the number n of writes to x still in the buffer to know whether we should continue reading x from the buffer after a flush. We identify (v, 0) and (vj, 0) for all v and vj because one should not be able to observe a value for a write that is no longer in the buffer, and this identification allows us to “forget” the value by setting n to 0. For xi ∈ Locs and vi in the corresponding subset of ((V × N)/ ≈) ∪V , we denote by [x1 : v1,..., xn : vn] the buffered state with graph {(x1, v1),..., (xn, vn)}. For compactness of notation, we write vn for the equivalence class of (v, n) in (V × N)/ ≈.
Footprints
Footprints are the first step towards filtering out pomsets with no executional meaning. Informally, a footstep (σ, τ ) ∈ Σ × Σ of an action λ is a minimal piece of state σ required to be able to perform λ, and a description τ of the effects of performing λ. For example, to perform a global write x := v, we need to have x in

the domain of the initial state and present in the buffer, so σ = [x : vj, x¯ : vjj
] for

some vj and vjj, and the result is setting the global value of x to v while removing one occurrence of x from the buffer, so τ = [x : v, x¯ : vjj]. Though v and vjj are unrelated, this gives the correct behaviour in the context of command pomsets because global writes to x occur in the same order as buffer writes to x. To perform a read action x = v, we must either have no entries for x in the buffer and have x : v in the global state, or we must have x in the buffer with value v, i.e., x¯ : vn for some n > 0. We call the set of footsteps associated with an action its footprint. Similarly, pomsets have footsteps and footprints.
TSO footprints for actions are given as follows:
Jx = v) = {([x : v, x¯ : vj ], [ ]), ([x¯ : vn+1], [ ]) | vj ∈ V ∧ n ∈ N}
Jx¯ := v) = {([x¯ : vj ], [x¯ : vn+1]) | vj ∈ V ∧ n ∈ N}
Jx := v) = {([x : vj, x¯ : vjj	], [x : v, x¯ : vjj]) | vj, vjj ∈ V ∧ n ∈ N}
Jδ) = {([ ], [ ])}
To give footprints to pomsets, we need to know when it makes sense to combine two footprints sequentially or in parallel. We say two states σ1 and σ2 are consistent, σ1 ⇑ σ2, if for all x ∈ dom(σ1) ∩ dom(σ2), σ1(x)= σ2(x). In this case, σ1 ∪ σ2 is also a state. Let σ \ dom(τ )= σTdom(σ)\dom(τ ). Then the result of updating σ by τ is [σ | τ ] = (σ \ dom(τ )) ∪ τ . For subsets S1 and S2 of Σ × Σ, we define the associative operation a to be:
S1 a S2 = {(σ1 ∪ (σ2 \ dom(τ1)), [τ1 | τ2]) | (σi, τi) ∈ Si ∧ [σ1 | τ1] ⇑ σ2}.
To account for global writes occurring elsewhere during the program, we parametrise the clauses assigning footprints to actions and pomsets by a list Λ containing a linearisation of the pomset as a subsequence, combined with any num- ber of other global writes that represent flushes from foreign buffers. Formally, given a pomset P , we let Lin(P ) be the set of its linearisations. Then the clauses are

parametrised by Λ ∈ Γ(P )= {Lin(P  L) | L ∈ Ls}, where given some global-write environment Λ ∈ Γ(P ), we identify P with its image in Λ. Given some L ∈ Ls, let JL)∗ be inductively defined on L as follows:
J [ ] )∗ = {([ ], [ ])}
Jx := v :: L)∗ = {([x : vj], [x : v]) | vj ∈ V } a JL)∗
The intuition here is that a foreign buffer flush should only affect the global part of the state and have no effect on our buffer.
We will need to know if buffered states have empty buffers. We let ζ(σ) hold if and only if for all x ∈ dom(σTBLoc), σ(x)= v0 for some v. Thus, for example, both ζ([ ]) and ζ([x : 1, y¯ : 20]), but neither ζ([x : 1, x¯ : 25]) nor ζ([y¯ : 11]).
The footprint JP )Λ of a pomset P under a global-write environment Λ ∈ Γ(P ) is the set inductively defined according to the three following rules:
(Act) If P = {λ} for some action λ and Λ = Λ1; P ; Λ2 for some Λ1, Λ2 ∈ Ls, then
JP )Λ = JΛ1)∗ a Jλ) a JΛ2)∗.
(Seq) If P = P1; P2 and Λ = Λ1; Λ2, then JP1)Λ1 a JP2)Λ2 ⊆ JP )Λ.
(Par) If P = P1  P2, Λ1 is the result of deleting the read and buffer write actions of P2 from Λ, Λ2 is the symmetric restriction, (σi, τi) ∈ JPi)Λi , ζ(σi), ζ(τi) (i = 1, 2), and σ1 ⇑ σ2, then (σ1 ∪ σ2, τ1 ∪ τ2) ∈ JP )Λ.
This definition is inspired by Lamport’s “happened before” relation [9]. In the case of (Act), for a given P = {λ} and Λ ∈ Γ(P ), there exists a unique splitting of Λ into Λ1 and Λ2, and the intuition is that Λ specifies that the global writes in Λ1 appeared before λ, and that λ was followed by some global writes in Λ2. In the case of (Seq), for JPi)Λi to be well-defined, we are implicitly assuming that Λi ∈ Γ(Pi). (Seq) tells us that the result of sequentially executing a program in the presence of global writes should be the same as executing the pieces sequentially in the presence of the appropriate subset of global writes. Finally, in (Par), the restrictions of Λ are such that both parallel components observe all writes in the same order, and this is how we simulate the effects of writes to a global state.
Because global writes are observed in the same order, we can show by induction on the derivation of a footstep that the second component of the footsteps of a pomset is determined by the global-write environment whenever its buffer is empty. This validates the intuition that the final state should be determined by the total order imposed on the writes:
Proposition 3.1 For all P, Λ ∈ Γ(P ), and (σ, τ ) ∈ JP )Λ such that ζ(τ ), we have
τ TLoc = βΛTAw .
Executions
We say that a finite pomset P is TSO executable if there exists a Λ ∈ Lin(P ) such that JP )Λ is non-empty. The set of TSO executions of a finite pomset P is given by the set E (P )= {(σ, [σ | τ ]) | Λ ∈ Lin(P ), (σj,τ ) ∈ JP )Λ, σj ⊆ σ, ζ(σ),ζ(τ )}; we take E (P )= ∅ if P is infinite. These executions take all of the states σ containing a minimal fragment σj required to execute P to a state updated with the effects τ of

P . The set of TSO executions for a program p is then E (p)=	P∈rTSO (p) E (P ). We illustrate TSO pomset executions by validating the IRIW litmus test, i.e.,
by showing that all writes appear in the same order to all threads. For example, starting from a state initialised to zero, executing the program
x := 1  y := 1  (w1 := x; w2 := y)  (z1 := y; z2 := x)
under TSO should never give a state consistent with [w1 : 1, w2 : 0, z1 : 1, z2 : 0]. To show this, it is sufficient to show the following pomset P is not executable:
x¯ := 1	y¯ := 1	x =1	y =1 
,	,	,	, 
x := 1  y := 1  y =0  x = 0.
Consider some Λ ∈ Lin(P ). Without loss of generality, assume x := 1 <Λ y := 1. To get an execution, we must apply (Par), and eventually we will need to compute JP4)Λ4 where P4 is y =1 → x =0 and Λ4 is such that x := 1 <Λ4 y := 1. To be able to execute y = 1 and still get a footstep with an empty initial buffer, we need Λ4

to satisfy y := 1 < y = 1. But then JP4)Λ4
= J[ x := 1,y := 1 ])∗ a J{y = 1})[ y=1 ] a

J{x = 0})[ x=0 ], and there are no footsteps in J[ x := 1,y := 1 ])∗ a J{y = 1})[ y=1 ] that can be combined with those in J{x = 0})[ x=0 ] to get states with empty buffers. This means we cannot combine footsteps from P4 to get footsteps for P using the (Par) rule, and so JP )Λ will be empty.
As discussed in the introduction, the Dekker mutual exclusion algorithm fails under TSO. Indeed, the second pomset for Dekker on page 10 can be executed from an initial state having both x and y set to zero. To do so, we take a Λ such that y = 0 <Λ y := 1 and x = 0 <Λ x := 1, and apply (Par) followed by (Seq).
In contrast, the Peterson algorithm successfully enforces mutual exclusion under TSO. Consider the following instance of the Peterson algorithm:
(x := 1; if x =2 then l := 1 else skip)  (x := 2; if x =1 then r := 1 else skip). Starting from the initial state [x : 0,l : 0,r : 0], one cannot execute the above under TSO and reach a state where both l and r are 1. In showing this, we can safely
ignore all pomsets where a read from x appears before the global write to x, because whenever we have x¯ := v → x = vj → x := v in a command’s TSO pomset, we must have v = vj. This implies that if a thread reads x before it does the global write to x, it will take the skip branch of the conditional. It is then sufficient to show that the following pomset is not executable:
x¯ := 1  zx ,:= 1  zx ,=2 
x¯ := 2  zx ,:= 2  zx ,= 1.
Consider some Λ ∈ Lin(P ). Without loss of generality, assume x := 1 <Λ x := 2. To get an execution, we must apply (Par) and derive a footstep for the bottom row P2 under some Λ2 where x := 1 <Λ2 x := 2. To compute this footstep, we must repeatedly apply (Seq), and will eventually reach the stage where JP2)Λ2 =
{([x : 0] , [x : 2])} a Jx = 1)[ x=1 ]. But this footprint must be empty, because
[x :0 | x : 2] is not consistent with [x : 1]. We thus cannot apply (Par) and we conclude that the pomset is not executable. It follows that the Peterson algorithm enforces mutual exclusion under TSO.

Fences
We can extend the above semantics to deal with fences. This extension will not be referenced in subsequent sections, and we include it here merely to emphasise the flexibility of our general development.
A fence constrains the reordering of memory actions. To capture fences, we first introduce a command fence. Under TSO, fences cause all actions before the fence to be observed before any actions after the fence. It is sufficient to flush the thread’s buffer to ensure this, giving rise to the semantic clause PL(fence)= {(L; {δ}, [ ])}.
Soundness and Completeness
We show that our denotational account of TSO in Section 3 is sound and complete relative to the axiomatic account of Section 2. Soundness implies we capture only behaviours permitted by the axiomatic account; completeness implies we capture all behaviours permitted by the axiomatic account. Because all TSO-consistent orders are contained in TSO-consistent total orders and can be obtained by weakening these, it is sufficient to show that we capture all TSO-consistent total orders. We identify total orders and lists.
Soundness
We call a function f : Pom(APO) → ℘(APO list) sound when for every program p
and finite pomset P ∈ PPO(p), if L ∈ f (P ), then L is TSO-consistent with P .
We will construct such an f and show that it is sound in this subsection, and we will show that it is complete in the next subsection.

Lemma 4.1 For every program p and pomset P ∈ PTSO(p), there exists an order isomorphism ω : P TAb → P TAw such that if ω(x¯ := v)= (y := w), then y = x and w = v, and such that for all b ∈ P TAb , we have b <P ω(b).

For programs, this means that global writes appear after the corresponding write to the buffer, that global writes occur in the same order as the writes to the buffer, and that all writes to the buffer give rise to global writes. We call a TSO pomset for which such an ω exists well-balanced. Given a program p, all TSO pomsets in PTSO(p) are well-balanced, and finite P ∈ PTSO(p) have a unique such ω. We assume in the rest of this section that our TSO pomsets are well-balanced.
Consider the function U : Pom(ATSO) → Pom(APO) that takes each TSO pomset to its underlying program order. It does so by first deleting all global write actions, and then relabelling all buffer write actions x¯ := v by corresponding global write actions x := v. We identify all reads in P with the corresponding reads in U (P ) and (x := v)= ω(x¯ := v) with the write x := v below x¯ := v. We can imagine U and the identifications as being given in the following diagram, where dashed arrows indicate identifications, solid arrows indicate the pomset orders, and the λj

are arbitrary read actions:
···	zλi, zx¯ ,:= v 	zλi,+2 	z· ·,·	zλk,

zx ,:= v 	zλk,+2 	z· ·,·	P

U
,	,	,	,	,	, 
···	zλi, zx ,:= v 	zλi,+2 	z· ·,·	zλk,	zλk,+2 	z· ·,·	U (P )
By observing that the PO pomset clauses are essentially special cases of the TSO pomset clauses, we have that for all programs p, U (PTSO(p)) ⊆ PPO(p). This inclusion is actually an equality, because given any program order P for p, we can construct a TSO pomset P j such that U (P j)= P by immediately flushing the buffer with split after every write, i.e., by replacing all occurrences of x := v in P with x¯ := v → x := v to get P j.
Let the set T (P ) of TSO-consistent total orders of P ∈ Pom(ATSO) be given by

T (P )= 
P′∈U−1(P )
{ΛTAPO 
| Λ ∈ Lin(Pj) ∧ JP j)Λ /= ∅}.

Informally, T (P ) captures the linearisations of TSO pomsets in U—1(P ) that give rise to TSO executions of pomsets.
Theorem 4.2 The function T is sound.
Proof. Let p be an arbitrary program, and let P j ∈ PPO(p), P ∈ U—1(P j), and Λ ∈ Lin(P ) be arbitrary such that there exists a (σ, τ ) ∈ JP )Λ. Because P is well- balanced, we can assume without loss of generality that ζ(σ). Let L = ΛTAPO . It is straightforward but tedious to check the six axioms to show that L is TSO-consistent for U (P )= P j from σ. We show the proof for (S) as an example.
Axiom (S). Lemma 4.1 implies there exists an order isomorphism between P TAw
j	j
and U (P )TAw , so w <P w implies w <U (P ) w . Linearisation preserves order, so
w <U (P ) wj implies w <Λ wj as desired.	2
Completeness
We call a function f : Pom(APO) → ℘(APO list) complete when for every program
p and finite pomset P ∈ PPO(p), if L is TSO-consistent with P , then L ∈ f (P ).
Our goal is to show that T is complete. To do so, we first construct a pomset s(P, L) ∈ U—1(P ) for any P ∈ Pom(APO) and total order L that is TSO-consistent with P . Let the underlying set s(P, L) be given by {0}× P ∪ {1}× P TAw . Let Φs(P,L) be given by Φs(P,L)((i, p)) = (x¯ := v) if both i =0 and ΦP (p)= (x := v), and Φs(P,L)((i, p)) = ΦP (p) otherwise. Intuitively, <s(P,L) merges the global writes into the program order in the places specified by L. Let <s(P,L) be the least strict partial order generated by the following collection of inequalities: (i) (0, p) <s(P,L) (0, pj) if p <P pj; (ii) (1, w) <s(P,L) (1, wj) if w ⊥P wj and w <L wj; (iii) (0, w) <s(P,L) (1, w)
if w ∈ P TAw ; (iv) (0, p) <s(P,L) (1, w) if p ⊥P w and p <L w; and (v) (1, w) <s(P,L)
(0, p) if w ⊥P p and w <L p.
The proof of completeness can be broken down into three lemmas as follows:
Lemma 4.3 If p is a program, P ∈ PPO(p), and L is TSO-consistent with P, then
s(P, L) ∈ PTSO(p) and U (s(P, L)) = P.

Lemma 4.4 If P ∈ Pom(APO) and L is TSO-consistent with P, there exists a
Λ ∈ Lin(s(P, L)) such that ΛTAPO = L.
Lemma 4.5 If p is a program, P ∈ PTSO(p), L0 ∈ Ls, L is TSO-consistent with
U (P )  L0, Λ ∈ Lin(P  L0), and ΛTAPO = L, then JP )Λ /= ∅.
The presence of L0 in the statement of Lemma 4.5 lets us use an induction hypothesis in the case where P is a parallel composition of pomsets, because when we want to apply the induction hypothesis to one of the pomsets, we need to be able to reference the global writes in the other pomset.
Theorem 4.6 The function f is complete.
Proof. Given a program p, a P ∈ PPO(P ) and a L that is TSO-consistent with P , we have by Lemma 4.3 that s(P, L) ∈ U—1(P ). By Lemma 4.4, we have a Λ ∈ Lin(s(P, L)) such that ΛTAPO = L. By Lemma 4.5 with L0 = [ ], Js(P, L))Λ /= ∅. So L ∈f (P ), and we conclude completeness.	2
Related Work
Other approaches to semantics for weak memory models mostly use execution graphs and operational semantics. Execution graphs [1,2] serve to describe the executional behaviour of an entire program, an inherently non-modular approach. We see our denotational framework as offering an alternative basis for program analysis, compositional and modular by design. Boudol and Petri [3] gave an operational semantics framework for weak memory models that uses buffered states. Jagadeesan et al. [8] adapted a fully abstract, trace-based semantics by Brookes [4] to give a fully abstract denotational semantics for TSO.
Pratt [11] was the first to generalise from traces to pomsets in the study of concurrency. He introduced the parallel composition operations we presented in Section 3 and he used pomsets in the study of concurrent processes. Building on Pratt’s work, Brookes [5,6,7] introduced a pomset framework to study weak mem- ory. This framework used Pratt’s parallel composition operator, and its sequential composition is a variant of Pratt’s concatenation operation. The TSO semantics given above builds on Brookes’s work. The key technical differences involve adapting pomset semantics to incorporate state equipped with abstract buffers, with careful accounting to deal properly with order relaxations allowed by TSO. Our formal axiomatisation of SPARC TSO, including full treatment of forks and joins, is a crucial part of the set-up, allowing us to be precise about the relationship between our abstract denotational semantics and the more concrete and informal characterisation of TSO that appears in the manual.
Conclusion
Our denotational semantics accurately captures the behaviours of SPARC TSO, and its compositionality enables us reason modularly about programs. The main strength of the pomset approach is its conceptual simplicity. Unlike trace semantics, which include irrelevant orderings of actions, our pomset semantics specifies only relevant

orderings of actions. The simplicity of the pomset approach also makes it readily adaptable to other memory models. For example, to capture SPARC PSO, which relaxes TSO to provide only per-location global orders on writes, we conjecture that it is sufficient to replace the single buffer parameter L in the semantic clauses ¥L of Section 3.3 to instead use families {Lx}x∈Loc of buffers, and to then modify the buffer flushing clauses in the obvious manner and to handle store barrier “stbar” commands. To capture other memory models, it should be sufficient to specify the correct set of actions, and to modify the semantic clauses generating pomsets and footsteps accordingly. As a result, we believe pomset semantics provide fertile ground for future research in semantics for weak memory models.
References
	Batty, M., S. Owens, S. Sarkar, P. Sewell and T. Weber, Mathematizing C++ concurrency, in: Proceedings of the 38th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’11 (2011), pp. 55–66.
	Boehm, H.-J. and S. V. Adve, Foundations of the C++ concurrency memory model, in: Proceedings of the 29th ACM SIGPLAN Conference on Programming Language Design and Implementation, PLDI ’08 (2008), pp. 68–78.
	Boudol, G. and G. Petri, Relaxed memory models: An operational approach, in: Proceedings of the 36th Annual ACM SIGPLAN-SIGACT Symposium on Principles of Programming Languages, POPL ’09 (2009), pp. 392–403.
Brookes, S., Full abstraction for a shared variable parallel language, Information and Computation 127
(1996), pp. 145–163.
Brookes, S., Partial order semantics and weak memory (2015), Invited talk, Domains XII, Boole Symposium, University of Cork.
Brookes, S., A denotational semantics for weak memory concurrency (2016), Invited talk, Mathematical Foundations of Computer Science.
Brookes, S., A denotational semantics for weak memory concurrency (2016), Midlands Graduate School in the Foundations of Computing Science.
URL https://perma.cc/2PHM-A9ZP

	Jagadeesan, R., G. Petri and J. Riely, Brookes is relaxed, almost!, in: Proceedings of the 15th International Conference on Foundations of Software Science and Computational Structures, FOSSACS’12 (2012),
pp. 180–194.
Lamport, L., Time, clocks, and the ordering of events in a distributed system, Commun. ACM 21
(1978), pp. 558–565.
Lamport, L., How to make a multiprocessor computer that correctly executes multiprocess programs, IEEE Trans. Comput. 28 (1979), pp. 690–691.
Pratt, V., Modeling concurrency with partial orders, Int. J. Parallel Program. 15 (1986), pp. 33–71.
SPARC International Inc., “The SPARC Architecture Manual,” Menlo Park, CA, USA (1992), version 8, revision SAV080SI9308.
