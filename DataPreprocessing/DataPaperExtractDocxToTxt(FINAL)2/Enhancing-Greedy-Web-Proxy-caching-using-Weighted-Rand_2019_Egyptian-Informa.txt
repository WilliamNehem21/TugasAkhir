Egyptian Informatics Journal 20 (2019) 117–130











Enhancing Greedy Web Proxy caching using Weighted Random Indexing based Data Mining Classifier
Julian Benadit Pernabas a,⇑, Sagayraj Francis Fidele b, Krishna Kumar Vaithinathan c
a Department of Computer Science and Engineering,Faculty of Engineering, CHRIST(Deemed To be University), Kengeri Campus, Kanmanike,Bangalore,560074, Karnataka,India.
b Department of Computer Science and Engineering, Pondicherry Engineering College, ECR, Pillaichavady, Pondicherry 605014, India
c Department of Computer Engineering, Karaikal Polytechnic College, Varichikudy, Karaikal-609609, India



a r t i c l e  i n f o 

Article history:
Received 8 March 2017
Revised 7 December 2018
Accepted 7 January 2019
Available online 14 January 2019

Keywords: GDS GDSF
GD*
Random indexing Clustering
Proxy
Data Mining
a b s t r a c t 

Web Proxy caching system is an intermediary between the Web users and servers that try to alleviate the loads on the origin servers by caching particular Web objects and behaves as the proxy for the server and services the requests that are made to the servers. In this paper, the performance of a Proxy system is measured by the number of hits at the Proxy. Higher number of hits at the Proxy server reflects the effec- tiveness of the Proxy system. The number of hits is determined by the replacement policies chosen by the Proxy systems. Traditional replacement policies that are based on time and size are reactive and do not consider the events that will possibly happen in the future. The performance of the web proxy caching system is improved by adapting Data Mining Classifier model based on Web User clustering and Weighted Random Indexing Methods. The outcome of the paper are proactive strategies that augment the traditional replacement policies such as GDS, GDSF, GD* which uses the Data Mining techniques.
© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

The World Wide Web and its usage are growing at a rapid rate and this has resulted in overloaded Web servers, network conges- tion, and consequently poor response time. Multitudes of approaches are continuously being made to overcome these chal- lenges. ‘Web caching’ is one of the approaches that can enhance the performance of the Web [1]. A Web cache is a buffered repos- itory of the Web objects that are most likely to be requested fre- quently and in the near future. The general architecture of the World Wide Web is shown below in Fig. 1. consists of the client users, the Proxy Server, and the Origin Server. Whenever the client requests the Web object, it can be retrieved from the Proxy server immediately, or it can be retrieved from the origin server. There-

* Corresponding author.
E-mail addresses: benaditjulian@gmail.com (J.B. Pernabas), fsfrancis@pec.edu (S.F. Fidele), vkichu77@gmail.com (K.K. Vaithinathan).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.


fore, whenever a user’s request is satisfied from the Proxy server, it minimizes the response time as well as it reduces the overload of the Web origin server. Typically, the Web cache may be located
[1] at the origin server cache, at the Proxy server cache, or the cli- ent cache.
The overall objective of the research work is to improve the per- formance of the Greedy Web Proxy caching algorithms by aug- menting the Web user clustering and Data Mining Classifier model based on the random indexing methods and Weight assign- ment policy mechanism. Sections 2 address the work in Traditional Greedy Web Proxy Caching algorithms and Data Mining based Web caching Methods. Section 3 details the Overall working model for the Web Proxy caching system. Section 4 presents the Performance Evaluation Used in Clustering and Classification
Section 5 presents the Generic Model for Web Proxy Caching algorithm using Data Mining Classifier Model, Section 6 elaborates the Performance measures of web proxy cache replacement algorithms.


Related work

The methodologies for Web caching may be categorized into two groups. The first category of methods is Traditional Greedy in the sense they use computationally simple parameters for cache


https://doi.org/10.1016/j.eij.2019.01.001
1110-8665/© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. World Wide Web architecture.


replacement. The second category of the methods combines Data Mining techniques with the traditional approaches for the enhancement of the Web caching system.

Summary of traditional Web Proxy caching algorithms

This section summaries the traditional Greedy Web Proxy cach- ing algorithm [2,3] based on the key parameters and Table 1.Sum- maries the Key factors for the eviction and its limitations for the traditional replacement policies like Greedy-Dual Size (GDS), Greedy-Dual Size Frequency(GDSF), and Greedy Dual⁄(GD⁄) [3].

Data Mining based Web caching methods

Several policies have been proposed in the literature for aug- menting the traditional Web caching methods with Data Mining techniques. In this Section, the Data Mining based Web caching Methods are broadly categorized as follows:

Regression based Web caching methods
Logistic Regression (LR) models [4] was applied in an adaptive Web cache, where the model predicts the Web objects that may be requested in the future and the LR policy replaces the Web object with the lowest re-visited value. Multinomial Logistic Regression (MLR) technique is also used as an early content classification scheme for Web cache objects [5] and is simple to
implement, but results in low classification accuracy with reduced cache performance. A semi-intelligent web cache system with a lightweight machine learning technique was used for Proxy cache replacement [6]. It uses a novel caching scheme called east Worthy Evict–Least Recently Used (LWE-LRU) for cache replacement.


Classification based Web caching methods
KORA (Khalid Obaidat Replacement Algorithm) to enhance the Web cache performance [7] uses a neural network to identify tran- sient and shadow cache lines. The temporary lines are the new web object arrivals to the cache, whereas the shadow lines are the arri- vals of the Web object that were replaced recently from the cache in favor of some other lines. The KORA algorithm performs well compared to the conventional algorithm by having lower miss ratio. An adaptive web cache access predictor technique [8] uses Back-Propagation Neural Network (BPNN) to improve the perfor- mance of Web caching by predicting the most likely re-accessed objects. A non-linear model using object features [9] analyzes the Web cache optimization with Multilayer Perceptron (MLP) Net- work and predicts the value of the object based on the syntactic features of the HTML document. The Neural Network Proxy Cache Replacement (NNPCR) [10], uses the Back Propagation to adjust the weight factors in the network. Here, an object is selected for replacement based on the ratings returned by the Back Propagation Neural Network (BPNN).





Table 1
Summary of traditional Web Proxy caching algorithms.

S.no	Algorithms	Parameters	Evictions	Limitations

GDS	Object size Sp . Object cost Cp . Inflation –Value L.
GDSF	Object size Sp . Object cost Cp . Inflation -ValueL.
Number of non-aged references Fp .
GD*	Object size Sp .
Least valuable objects with a Key value
Kp = L + Cp /Sp .

Least valuable objects with a Key value.
Kp = L + Cp × Fp /Sp


Least valuable objects with a Key value.
It does not consider the frequency value of the Web object.


It does not consider the inter-access time of the Web object.




Low hit ratio under constant cost model.

Object cost Cp . Inflation-Value L.
Temporal correlation measures b. Number of non-aged references Fp .
Kp =

L + (Cp × Fp /Sp )b .






Fuzzy rules [11] were also used to identify the Web pages to be removed from the cache. The variables describing each web page are first fuzzified, and the output is de-fuzzified to replace the Web page. Artificial Neural Network algorithm (ANN) combined with PSO [12] was used for improving the neural network perfor- mance. The neuro-fuzzy system is partitioned client-side Web Cache which uses the Adaptive Neuro-Fuzzy Inference System (ANFIS) method for binary classification and for classifying the Web objects into cacheable or uncacheable objects. Here, the trained Neuro-fuzzy has been employed with the LRU algorithm in cache replacement decision. The Intelligent Naïve-Bayes approach [13] were also used to identify whether the Web object can be re-accessed in the future or not. In [14,15,16] various clas- sification algorithm improves the performance of a proxy cache using Tree Augmented Naïve Bayes approach followed by very fast decision tree algorithm and Expectation Maximization with Naive
Bayes Classifier for improving the Web proxy cache using sliding window mechanism and Data Mining classification performance. This method is integrated with greedy replacement algorithms like GDS, GDSF, and GD⁄ which consider multiple factors like cost, size, frequency to form a novel Web caching.

Clustering based Web caching methods
A Cluster-based prefetching scheme on a Web cache is used to partition the clusters of correlated Web pages to identify the user access pattern [17]. In this method, when the user requests a Web object, the proxy server retrieves all the objects, which are in the same cluster with the requested object. In this approach, the proxy residues are represented by using the navigational graph algorithm. Moreover, this method has some limitations on the esti- mation of the number of clusters due to the complex nature result- ing in low performance.





Fig. 2. Overall working model for the Web Proxy caching based on Data Mining Classifier Model.



Evolutionary based Web caching methods
An Evolutionary technique for Web caching applies the Genetic Algorithm (GA) for cache replacement decision [18]. Here, each Web object’s ‘‘strength” is determined by the specific properties related to object’s staleness, access frequency, and retrieval cost. The Finite State Machines (FSM) is coupled with an evolutionary algorithm shows a good prediction rate for web prediction [19]. GA-Based cache replacement policy has also been used for user side cache replacement [20]. In this process, the algorithm uses a fitness function, and the replacement policy considers the download time and number of references to a Web object along with its size.


Working principle of Web Proxy caching based on Data Mining Classifier Model

Clustering is one of the widely used data mining technique that is used in learning systems. The strategy presented in this chapter
augments the traditional Web Proxy caching by clustering web user and amalgamating it with classifier model based on Weighted Random Indexing and Weight assignment policy. The overall work- ing flow model consists of different phases as shown in Fig. 2. These working methods are classified as given below:

Web Proxy server datasets collection

In this section, the Web Log files for the Web Proxy cache sim- ulation are obtained from the National Lab for Applied Network Research (NLANR) [21]. Each data set represents a proxy server located in a particular location.

Data Pre-Processing

In this method, the datasets are pre-processed and its converted into a structured format for reducing the time of simulation. The




Fig. 3. Filtering phases for the Web Proxy datasets.



techniques undergo few pre-processing [22] steps to remove irrelevant requests, and to extract useful information. The steps involved in data pre-processing are Proxy Logs Filtration, Data cleaning, and Data Abstraction.

Proxy logs filtration
In this technique, the recorded proxy log files obtained from the NLANR have undergone the basic filtration process to reduce the size of the log data sets as well as the running time of the simula- tion. Thus, only the three proxies log datasets are considered for the filtration process [23] as shown in Fig. 3. This filtration module is based on the methods such as a Latency based method, SIZE based method, Dynamic based method, Content-based method. The Fig. 3. Illustrates the steps involved in various filtering meth- ods [23] and its details are described below. After these filtering, the size of the proxy log files is reduced to have some unique requests, which have obtained after the filtering phases, i.e., 823649 for the UC datasets, 674352 NY datasets, and 2406556 SD datasets as shown in Table 2.
From the filtration results obtained, the URL convention and the required HTTP requests are sorted based on the Timestamp. Now the input trace file from UC, NY, SD is ready to undergo a certain data pre-processing steps for the simulation of the Data Mining Classifier.


Data cleaning
Data cleaning is the process of removing the irrelevant entries in the proxy log file [22]. Here only the relevant HTML file is con- sidered and all other irrelevant log entries that were recorded by requesting graphics, sound, and other multimedia files, etc. are discarded.


Data Abstraction

Data Abstraction is the process of abstracting the log entries based on the User and Session Identification.




Table 2
Statistics for the trace file.



Fig. 4. Algorithm for User Identification.


Fig. 5. Algorithm for Session Identification.



User identification
The goal of user identification [24] is to identify each user in the Proxy datasets. This method can be carried out easily if the user
The formula for the Frequency [25] of the Web object p is given in the Eq. (1)
	Number of times(p) accessed	

provides his or her registration information. There are several ways
to identify the individual users in the Proxy datasets, which are collected from proxy servers. The algorithm for identifying user
a(p)=	Number of times p accessed
WebPage ∈ Visited Web Pages
(1)

identification is given in Fig. 4.
Similarly, the Duration [25] of the Web object p is given by the
Eq. (2) as shown below:

Session identification
The goal of session identification is to divide each user into dif-

b(p)= X
TotalDurationof (p)/Size (p)
TotalDuration (p)/Size(p)

(2)

ferent segments based on the user’s pattern, which is called a ses- sion. Session identification approaches identify the user’s session by a maximum time limit. A new session is created, when the dif- ference in time of a Web page is more significant than the thresh-
old limit of access time, or if the time spent in the same session
Max WebPage ∈ Visited Web Pages

From, these two measures, we can obtain the High-Interest Web object set and this high-interest object set value is normalized to 0 or 1 based on the Eq. (3)given below:

exceeds the maximum limit. Based on the empirical findings, the maximum time limit has been set to be 30 min [24] for our simu-
v p	2 × a (p) × b(p)
a (p)+ b (p)
(3)

lation. The algorithm for session identification is shown in Fig. 5. After the proxy datasets are pre-processed; it is further ana-
Once the high-interest Web object p is obtained from the URL,P = {URL , URL , .. . URL } a navigation pattern profile is gener-


User modeling based on Weighted Random Indexing

The user modeling based on the Weighted Random Indexing consists of the following methods such as High-Interest Page Set, Segmentation of URL, and Random Indexing based on weight, Weight Function, and Navigation set of users.

High-Interest page set
Once the dataset has been preprocessed, it is segmented to find out the high-interest Web object, based on these two measures, i.e., Frequency and Duration [25]. Let P be the set of a Web object P  p1, p2 , p3 ... pn accessed by the users in the Proxy server logs. Here the session is transformed into ‘n’ a dimensional vector for the weight is assigned as S = {w(p1, s), w(p2 , s) ... w(pn , s)} where, w pi , s is a weight assigned to ith Web page p visited in a Session
S. The parameters involved in obtaining the High-Interest Web
object [25] is shown in Table 3. The frequency of the Web object
p is calculated by the number of times the Web object p is accessed.
Table 3
Parameters for High-Interest Web Object.

Symbols	Parameters for High-Interest Web Object


p	Web object.
a	Frequency of the Web object p.
b	Duration of the Web object p.
Size	Size of the Web object p.
k	High-Interest of the Web object p.

ated from a set of the individual user, which is given by U  U1, U2, .. . Un and these given URLs are segmented based on the user interest Object set p.

Segmentation of URL
The high-interest Web page p obtained from the URL is segmented based on the hierarchical structure of the Websites. The URL is composed of different levels, which are segmented by '/'based on the sequence obtained in the Proxy log.

Weighted Random Indexing
Random Indexing performs well with a small volume of data, but when the volume of data increases, the performance degrades. So to overcome this limitation, the random indexing method is modified with the weight function [26] to improve its perfor- mance. In Random Indexing Weights are generated using the sta- tistical information based on the frequency of each term and its contexts. In this case, each index vector in the context is multiplied by the context vector generated. Therefore, the context vector of each term t is calculated as shown in Eq. (4)
Ct = X R(r, t').weight(t, r, t')	(4)
(r,t' )∈(t,*,*)

where,

R r, t' is the index vector of the context r, t'
Weight t, r, t' is the weight function for the term t' and its context.




Fig. 6. Algorithm for K-Means clustering.



In this caching process, the list of web documents are obtained and in each web document, a context window size of 2 + 2 is used for it’s constituent words. Using the formula, the weight for each word is generated. This weight is used to generate the index vector. Once generated, the weight function is applied to it to determine the context vector and this is applied iteratively for each word in the document. For example, Christuniversity.in/studentlogin.html/C S433, we can segment the URL using ‘/’ to obtain the different sec- tions of the webpage. Using these various segmented sections, the index vectors can be used to determine personalized high interest pages for each user, where a navigation path can be explained in a n × d dimensional matrix.
Table 4



Table 5
Performance Metrics used in Data Mining Classifier.

Metric	Description	Formula





Web user clustering

The main objective Web user clustering [26,27] is creating the common patterns and grouping of user access behavior. Thus, the clustering system would group the Web documents and organize
them according to their user interests. Once the random indexing
Precision (p)	It is defined as the ratio of the
number of relevant Web pages that were retrieved and the total number of retrieved Web pages
Recall (r)	It is defined as the ratio between the number of relevant Web pages retrieved and the total number of relevant Web pages.
Precision p	 TP 
TP+FP





Recall r	 TP  TP+FN

process is generated, an efficient clustering algorithm called the K-Means algorithm is used. It is one of the most popular partitions based clustering algorithms, which is widely used.
K-Means is one of the well-known clustering algorithms from unsupervised learning.The K-Means algorithm partitions the given
data into k clusters. Each cluster has a cluster center, called the
Correct
Classification Rate (CCR)




Table 6
Correct Classification Ratio(CCR) is a good measure for evaluating Classifier .
CCR	TP TN
+TN+FP+

cluster centroid. The centroid usually used to represent the mean of all the data points in the cluster, which gives the name of the algorithm, i.e., since there are K clusters, thus it is called K- means. The algorithm for K-Means clustering is given in Fig. 6.

Data Mining classification

K-Nearest Neighbor Classifier [28] is one of the supervised machine learning algorithms used in a variety of application. The K-Nearest Neighbor classifier is working under the principle of dis- tance measures. The KNN algorithm trains not only the data set but also the classification for each training example. This indicates that
in the KNN algorithm the training samples are used to build classi-
Statistics of Data preprocessing datasets.

Attributes	Generated
No	Values


Total entries accessed by the users.	1,248,675.
Number of filtered access entries.	116,200
Number of different access users.	12,931.
4	Maximum no of users (Considered for simulation).	100.
User –ID	1 to 100.
Session Duration Time limit (Si).	30 min.
Duration of the Web page.	10 min.
Minimum Frequency access of the Web page (Fi)	10 times.
Total number of accessed Web pages.	800.
Number of the Web page accessed with Frequency	600.
P10 times.
Total Number of Identified sessions.	24,000.

fication models. In KNN classifier the learning process occurs when a test sample needs to be classified. The algorithm for the KNN Classifier is given in Fig. 7

Performance evaluation used in clustering and classification
Number of Identified sessions with Session duration (30 min).
Number of Identified sessions with Page duration (10 min).



Experimental results and discussion
12,000.

12,500.

Precision and Recall [29] are the performances suited in many machine learning applications. These two parameters can be mea- sured based on the result obtained from the confusion matrix, which is shown in Table 4. This confusion matrix has been modified according to the correctly or un-correctly classified Web pages which are classified by the classifier. This performance metrics which is shown in Table 5 is mainly used to find out the optimal Data Mining Classifier Model for enhancement of the Web Proxy caching strategies. The following measures are classified as follows:

The experimental results in this section are classified as follows:

Data preprocessing results
In the pre-processing procedure, the original Web proxy data- sets are cleaned, formatted, and grouped into user’s sessions. This experimental simulation is carried out by the Web Utilization Miner tool (WUM) [30].Table 6 presents some statistics of the experimental data sets. From Table 6, it is shown that 116,200




Fig. 7. KNN classifier algorithm.



clean entries are extracted. In this, 800 Web pages were visited, and 600 were accessed more than ten times. Also, the total session identified as 24,000, from which the identified session reduced 12,000 with session duration threshold to limit as 30 min. Simi- larly, the page duration of the session is identified as 12,500 with page duration threshold limit set 10 min.

Clustering result
The parameters used in the Weighted Random Indexing exper- iments have some values assigned, i.e., +1, —1 in index vector as e and the context window size are denoted as l.d as a dimension of the index vector. So here,e is set toe = 10 as proposed [26] and l = 1 as the URLs is short k value for the K-Means clustering. The
maximum value of the cluster number kmax and be chosen as  n
(n is the data size).From the above Table 7, the values are assigned


Table 7
Parameters for Weighted Random Indexing




Table 8
Results of Performance Metrics used in Cluster.
to the parameters, and the experiments were conducted using the simulation tool Text Matrix Generator tool (TMG) [31]. Therefore, after data preprocessing and the Random Indexing for the prepro- cessed Proxy datasets, a High-Interest page set P is obtained con- taining 97 requested URLs Web pages. These URLs are split by ‘/’ to get the segment set ‘S’, which comprises 152 different segments. Similarly, the dimension of the index vector in RI is selected to 300. Finally, 100  300 the matrix is constructed X  X1, X2, .. . X100
for the single user pattern matrix based on the Weighted-RI
method, and it takes it as input to the K-means clustering algorithm.
The K-Means clustering algorithm generates the common user’s requests and groups these 100 users in the different Clusters Num- ber (C-N) (Varying 1 to 9) based on the Weighted Random Indexing methods. After obtaining the common user’s requests, the individ- ual clusters are evaluated based on the performance metrics as shown in Table 8.
From this Table 8, the no of Pre-URL required for each user is identified, and a common navigation pattern profile will be created for each cluster. The parameters Precision and Recall evaluates the performances of clusters. According to the clustered results obtained, the precision is defined as the ratio of hits to the number of URLs that are cached. Similarly, recall is the ratio of hits to the number of URLs that are requested. Fig. 8 signifies the clustering results based on the performance metrics.





Fig. 8. Results of Precision Recall, and Hits in clusters.



Classifier results
The Data Mining classification (KNN), which aims to evaluate the experimental results of classification [25]. The evaluation of the classification is about whether the Web page belongs to the class of common user navigation pattern profile. This navigation pattern profile is generated based on the session duration, high- interest page, the frequency of access, duration of the Web page. The different classifier trains these values whether the requested Web page belongs to a common user navigation pattern profile. So, if the requested Web page belongs to the common user profile pattern it assigned as class one otherwise zero. Therefore, if the class value of the Web page is 1, it indicates that Web page may request in future and this type of Web page is considered cacheable
requests. In this section, different classifiers [25] like Support Vec- tor Machines (SVM), Decision Tree (J48), Naïve Bayes (NB) classifier and K-Nearest classifier (KNN), for classification purpose. So among the different classifier, the KNN classifiers has a better clas- sification accuracy which is shown in Fig. 9


Generic Model for Web Proxy caching algorithm using Data Mining Classifier Model

In this Section, a generic model for Web Proxy caching strate- gies integrated with Data mining Classifier Model was introduced. The algorithm for Generic Web Proxy caching algorithm integrated




Fig. 9. Comparisons of Accuracy for Data Mining Classifier.



Fig. 10. Generic model for Web Proxy caching replacement algorithms using the DMCM algorithm.



with Data Mining Classifier Model is shown in Fig. 10. In (line 1), an initial Data Mining Classifier Model (KNN) is built on history Weblogs. For each Web page p requested from the Proxy cache t that contains the Web page p and then the Web page p is returned to the Web Client. Concerning this performance, measures (line 7– 11), in this case, it is considered as Cache Hit.Also, the Number of bytes transfers back to the client is counted for the weighted hit rate measure. Once the data are transferred back to the client, the Proxy cache is updated (line 12) by the Data Mining Classifier Model (KNN) based on the weight assignment policy, (Class value of the Web page, i.e., whether the Web page p that can be revisited in future or not).On the contrary, if the requested Web page p is not

Table 9
Parameters for Weight Assignment Policy.

Parameters  Description


L	Inflation factor to avoid cache pollution in the proxy cache t. f (p)	Previous frequency access of Web object p in the proxy cache t. F(p)	Current frequency access of Web object p in the proxy cache t. Kn—1(p)	Previous key value of the Web object p in the proxy cache t.
DTt (p)	Difference in time between the current requests and previous requests for the Web object p in the proxy cache t.
Ct (p)	Current reference time of the Web object p in the proxy cache t. Lt (p)	Last reference time of the Web object p in the proxy cache t. S(p)	Size of the Web object p.
  Kn (p)	Current key value of the Web object p.	
available in the Proxy cache t or it is stale, a Cache Miss occurs, i.e., the Web page q is deleted from the cache (line 16), in this case, the Proxy server forwards the request to the origin server S (line 17), and a fresh copy of the Web page p is retrieved from the origin ser- ver S and pushed into the Proxy cache t (line 18). The push method consists of assigning the class value of the Web page p by the Data Mining Classifier Model (KNN) based on the Weight assignment Policy.Also, if the cache space t exceeds the maximum cache size N (line 19), the Web page q from the cache is popped out from the cache (line 20), based on the Class assigned and lowest key value based on the weight assignment policy by the Data Mining Classifier Model (KNN). Such an approach is known as weight assignment cache replacement policy (line 21–24), i.e., each time when the cache gets overflows. Finally, the Data Mining Classifier Model periodically updates the key value of the remaining Web page is stored in the Proxy cache t. This process continues itera- tively when the cache performance decreases. Also, notice that the update of the Data Mining Classifier Model (line 28), is dissoci- ated from the online caching of the Web page, and it can be per- formed in parallel.


Weight assignment policy for cache replacement

The weight assignment policy [32] of the Web object p in the proxy cache t is expressed in Eq. (5), and the parameters are shown




Fig. 11. GDS replacement algorithm based on weight assignment policy and DMCM.



in Table 9. From the above strategy, the key value used in the cach- ing system is applied to the Greedy family replacement algorithm
(KNN) and Weight assignment policy of the caching system, i.e.
F(p)×C(p)+Kn 1 (p)×  DTt (p)  

(GDS, GDSF, GD⁄) and the key factor of the replacement algorithm
Kn(p)= L +
—
S(p)
Ct (p)—Lt (p)
. Similarly, if the Web page p is

is modified according to Eq. (5) as shown below.
F(p)+ Kn 1(p)×  DTt (p)  
not available in the Proxy cache t then Cache Miss Occurs and the Web page p retrieved from the origin Server S and if there is no

Kn(p)= L +
—
S(p)
Ct (p)—Lt (p)
(5)
space in the Proxy cache t the Web page q has to be replaced based on the DMCM (KNN) and lowest key value assigned by the weight

Therefore, whenever the cache replacement occurs, the replace- ment algorithm replaces the Web object based on the key value used by the weight assignment policy.
From the above strategy, the key value used in the caching sys-
tem is applied to the Greedy family replacement algorithm (GDS,
assignment policy of the caching system, i.e. Web page q with min- imum key value (minq∈cache{k(q)|q}) in the cache is chosen among the other Web pages resident in the Proxy cache t. Subsequently, the values are reduced by Kmin, and the key value of the Web page
F(p)×C(p)+Kn 1 (p)×  DTt (p)  

GDSF, GD⁄) [2] and the key factor of the replacement algorithm is
p is updated as Kn(p)= L +
—
S(p)
Ct (p)—Lt (p)
and it is pushed

modified according to the equation as shown above. Therefore, whenever the cache replacement occurs, the replacement algo- rithm replaces the Web page based on the key value used by the weight assignment policy.

Integration of GDS replacement with the Data Mining Classifier Model

The GDS policy [2] is adapted with Data Mining Classifier Model when there is a need for cache replacement. The algorithm illus- trates GDS cache replacement with Data Mining Classifier Model. In this method, GDS associates a value key value K p with each Web page p in the cache. When a Web page p is requested in the Proxy cache t. and it is already available in the Proxy cache t then Cache Hit occurs, and the Web page p is pushed onto to the top of the cache. Also, the key value is updated based on the DMCM
to the top of the cache. Also, the Data Mining Classifier Model updates the remaining Web pages. The algorithm for the GDS replacement based on weight assignment policy and DMCM is given in Fig. 11.

Integration of GDSF replacement with the Data Mining Classifier Model

The GDSF policy [2] is adapted with Data Mining Classifier Model when there is a need for cache replacement. This algorithm illustrates GDSF cache replacement with Data Mining Classifier Model. In this method, GDSF considers variability in cost and size of a Web page p by choosing the victim based on the ratio between the cost and size of documents. GDSF associates a value key value k p with each Web page p in the cache. When a Web page p is requested in the Proxy cache t, and it is already available in the




Fig. 12. GDSF replacement algorithm based on weight assignment policy and DMCM.



Proxy cache t then Cache Hit occurs and the Web page p is pushed onto to the top of the cache, also the key value is updated based on the DMCM(KNN) and weight assignment policy of the caching sys-
(F(p)+f (p))×C(p)+Kn 1 (p)×  DTt (p)  
correlation in a Web page. GD⁄associates a value key value k p with each Web page p in the cache. When a Web page p is requested in the Proxy cache t and it is already in the Proxy cache t then Cache Hit occurs and the Web page p is pushed onto to the

tem  i.e.K(p) is  set  to  Kn(p)= L +
—
S(p)
Ct (p)—Lt (p) .
top of the cache, also the key value is updated based on the DMCM

Similarly, if the Web pagep is not available in the Proxy cache t then Cache Miss Occurs and the Web page p retrieved from the origin server S and if there is no space in the Proxy cache t the
(KNN) and Weight assignment policy of the caching System k p is set to i.e.

Web page q has to be replaced based on the DMCM, and the lowest	2
  DT p   31


key value assigned by the Weight assignment policy of the caching system, i.e. Web page p with minimum key value and class	n value as 0 is chosen among all the other Web pages resident in
the Proxy cache t. Subsequently, these values are reduced
(F(p)+ f (p)) × C(p)+ K
( )= +	S(p)
n—1
(p) × 
t ( )	b
t (p)—Lt (p) 5

by kmin, and the key value of the Web page p is updated as
(F(p)+f (p))×C(p)+Kn 1 (p)×  DTt (p)  
Similarly, if the Web page p is not available in the Proxy cache t then
Cache Miss occurs and the Web page p retrieved from the origin ser-

Kn(p)= L +
—
S(p)
Ct (p)—Lt (p)
and it is pushed to the top
ver S and if there is no space in the Proxy cache t the Web page q has

of the cache. Also, the Data Mining Classifier Model updates the remaining Web pages. The algorithm for the GDSF replacement based on weight assignment policy and DMCM is given in Fig. 12.
Integration of GD⁄ replacement with the Data Mining Classifier Model
to be replaced based on the weighted assignment policy and DMCM, i.e.Web page q with minimum key value minq∈cache{k(q)|q} with class value 0 in the cache is chosen among all the other Web page resident in the Proxy cache t. Subsequently, the values are reduced bykmin and the key value of the Web page p is updated as
2		 31


(F(p)+f (p))×C(p)+K
n—1
p	 DTt (p) 	b
Ct (p)—Lt (p)

The GD⁄ policy [2,3] is adapted with Data Mining Classifier
Kn(p)= L + 4
S(p)
5 and it is pushed to the

model when there is a need for cache replacement. The algorithm illustrates GD⁄ cache replacement with Data Mining Classifier Model. In this method, GD⁄ captures both popularity and temporal
top of the cache. Also, the remaining Web pages are updated by the Classifier Model. The algorithm for the GD* replacement based on weight assignment policy and DMCM is given in Fig. 13




Fig. 13. GD* replacement algorithm based on weight assignment policy and DMCM.



Experimental setup for the Web Proxy cache simulation

For the simulation of the Web Proxy cache algorithm, the win- dow based cache is simulator is used for the integration of Data Mining Classifier Model (KNN). Results obtained from the Classifier are taken as input to the Web Proxy cache simulator [33]. The experimental setup is carried out based on the following parame- ters like Trace file name, cache size, replacement scheme, the con- tent type used. The Trace file name includes the following attributes Timestamp, URL-ID, object size, etc. The cache size used in these experiments may vary in size from 5% to 45% (Maximal volume of the cached content) and the replacement scheme used are, GDS, GDSF, GD*.

Experimental results for the Web Proxy cache simulation

In this section, the results obtained for the Data Mining Classi- fier Model-based Web proxy cache replacement algorithms are compared with the traditional cache replacement algorithms. From
the experimental results, it is shown that the performs measures of hit and byte hit ratio for the DMCM (KNN) based. Web proxy cach- ing algorithms outperform the traditional caching algorithms in all aspects. Here the experimental results are graphically illustrated, and the results are shown below. The experimental result com- pares Greedy family algorithms under constant and packet model based on the Data Mining Classifier Model which is shown in Figs. 14 and 15. Respectively

Conclusion

The various working modules of the overall work flow as imple- mented in the paper are data pre-processing, Weighted Random Indexing, clustering the web users, followed by a Data Mining Clas- sifier Model. The results obtained show significant improvement over generic caching and are described further discussed. The KNN classifier outperforms the other machine learning classifier by improving the classification accuracy to 76% which is compara- tively higher than the other machine learning algorithms. The





Fig. 14. Comparison of the Overall Hit Ratio of GDS(p), GDSF(p), GD*(p) vs DMCM based replacement.



Fig. 15. Comparison of the Overall Byte Hit Ratio of GDS(p), GDSF(p), GD*(p) vs DMCM based replacement.



average precision and recall value for all the clusters generated by the K- means algorithm is around 80.1% and 46% respectively.
The integration of Web user clustering and classifier methods with traditional Web Proxy caching improves the performance of overall hit and byte hit ratio. The hit ratio of GDS, GDSF, and GD⁄ under constant cost model based on DMCM has been improved to 11.4%, 12.8%, 13.7%. The byte hit ratio has been improved to 30.2%, 29.1%, 30.2%. The hit ratio of GDS, GDSF, and GD⁄ under packet cost model based on DMCM has been improved to 26.7%, 23.3%, and 24.4% respectively. The byte hit ratio has been improved to 34.1%, 34.2%, 33.1%. In future we would like to implement advanced clustering and classification algorithms for improving the performance of the model. Also we would like to incorporate our work for CDN (Content Distribution Networks).

References

Baentsch M, Baun L, Molter G, Rothkugel S, Sturn P. World Wide Web caching: the application-level view of the internet. IEEE Commun Mag 1997;35
(6):170–8. doi: https://doi.org/10.1109/35.587725.
Balamash A, Krunz M. An overview of Web caching replacement algorithms. IEEE Commun Surv Tutorials 2004;6(2):44–56. doi: https://doi.org/10.1109/ COMST.2004.5342239.
Jin S, Bestavros A. Greedy-dual⁄ Web Caching Algorithm. Int J Comput Commun 2001;24(2):174–83. doi: https://doi.org/10.1016/S0140-3664(00) 00312-1.
Foong AP, Hu Y-H, Heisey DM. Logistic Regression in an adaptive Web Cache. IEEE Internet Comput 1999;3(5):27–36. doi: https://doi.org/10.1109/ 4236.793455.
Sajeev GP, Sebastin MP. A novel content classification scheme for Web Caches. Evol Syst 2011;2(2):101–18. doi: https://doi.org/10.1007/s12530-010-9026-6.
Sajeev GP, Sebastin MP. Building semi-intelligent Web cache systems with light weight machine learning technique. Comput Electr Eng 2013;39 (4):1174–91. doi: https://doi.org/10.1016/j.compeleceng.2013.02.005.
Khalid H, Obaidat M. KORA: a new cache replacement scheme. Comput Electr Eng 2000;26(3):187–206. doi: https://doi.org/10.1016/S0045-7906(99)00041- 5.
Wen Tian, Ben Choi, Vir Phoba. An Adaptive Web cache access predictor using network. Developments in Applied Artificial Intelligence, Lecture Notes in Artificial Intelligence, vol. 23. Springer; 2002. p. 450–9. doi: https://doi.org/ 10.1007/3-540-48035-8_44. no. 58.
Koskela T, Heikkonen J, Kaski K. Web Cache optimization with non linear model using networks object features. Comput Networks 2003;43(4):805–17. doi: https://doi.org/10.1016/S1389-1286(03)00334-7.
Cobb J, ElAarag H. Web Proxy cache replacement scheme based on back- propagation neural network. J Syst Software 2008;81(5):450–9. doi: https:// doi.org/10.1016/j.jss.2007.10.024.
Sabeghi M, Yaghmaee MH. Using Fuzzy logic to Improve cache replacement decisions. Int J Comput Sci Network Security 2006;6(3):182–8.
Ali W, Shamsuddin SM. Neuro-Fuzzy system in partitioned client-side Web cache. Expert Syst Appl 2011;38(12):14715–25. doi: https://doi.org/10.1016/j. eswa.2011.05.009.
Ali W, Shamsuddin SM, Ismail AS. Intelligent Naïve Bayes approaches for Web Proxy caching. Knowl Based Syst 2012;31:162–75. doi: https://doi.org/ 10.1016/j.knosys.2012.02.015.
Julian Benadit P, Sagayaraj Francis F, Muruganantham U. Improving the performance of a Proxy Cache using tree augmented Naive Bayes classifier.
Procedia Comput Sci 2015;46:184–93. doi: https://doi.org/10.1016/j. procs.2015.02.010.
Julian Benadit P, Sagayaraj Francis F. Improving the performance of a proxy cache using very fast decision tree C classifier. Procedia Comput Sci 2015;48:304–12. doi: https://doi.org/10.1016/j.procs.2015.04.186.
Julian Benadit P, Francis Sagayaraj F, Muruganantham U, Improving the Performance of a Proxy Cache Using Expectation Maximization with Naive Bayes Classifier. In Jain L, Behera H, Mandal J, Mohapatra D. (eds) Computational Intelligence in Data Mining - Volume 2. Smart Innovation, Systems and Technologies, vol 32. pp. 355–368 Springer, New Delhi. https://doi.org/10.1007/978-81-322-2208-8_33.
Pallis G, Vakali A, Pokorny J. A Clustering Based–prefetching scheme on a Web cache environment. Comput Electr Eng 2008;34(4):309–23. doi: https://doi. org/10.1016/j.compeleceng.2007.04.002.
Vakali A. Evolutionary techniques for Web caching. Distrib Parallel Databases 2002;11(1):93–116. doi: https://doi.org/10.1023/A:1013385708178.
Bonino D, Corno F, Squillero G. A Real Time Evolutionary algorithm for Web prediction. In Proceedings of the IEEE/WIC International Conference on Web Intelligence; 2003. p. 139–145. https://doi.org/10.1109/WI.2003.1241185.
Chen Y, Li Z, Wang Z. A GA-Based cache replacement policy. In: Proc. of the 3rd International Conference on Machine Learning and Cybernetics; 2004. p. 263–
265. https://doi.org/10.1109/ICMLC.2004.1380674.
NLANR, National Lab of Applied Network Research (NLANR), Sanitized Access Logs. [Online] http://www.ircache.net/2010.
Cooley R, Mobasher B, Srivastava J. Data preparation for Mining World Wide Web Browsing Patterns. Knowl Inf Syst 1999;1(1):5–32. doi: https://doi.org/ 10.1007/BF03325089.
Kastaniotis G, Maragos E, Douligeris C, Despotis DK. Using Data envelopment analysis to evaluate the efficiency of web caching object replacement strategies. J Network Comput Appl 2012;35(2):803–17. doi: https://doi.org/ 10.1016/j.datak.2006.06.001.
Sule Gunduz Oguducu. Web Page Recommendation Models: Theory and Algorithms. Morgan and Clay Pool; 2011. https://doi.org/10.2200/ S00305ED1V01Y201010DTM010.
Liu H, Keselej V. Combined mining of Web server logs and Web contents for classifying user navigation patterns and Predicting users’ future requests. Data Knowl Eng 2007;61:304–30. doi: https://doi.org/10.1016/j.datak.2006.06.001.
Wan M, Jonsson A, Wang C, Li L, Yang Y. Web user clustering and Web prefetching using Random Indexing with Weight functions. Knowl Inf Syst 2012;33(1):89–115. doi: https://doi.org/10.1007/s10115- 011-0453-x.
Jin Hua Xu and Hong Liu, Web user clustering analysis based on K-Means algorithm. In: International Conference on Information, Networking and Automation (ICINA), Kunming, vol. 2, pp. noV2-6-V2-9, 2010 https://doi.org/ 10.1109/ICINA.2010.5636772.
Wu X, Kumar V, Quinlan JRJ, et al. Top 10 algorithms in Data mining. Knowl Inf Syst 2008;14(1):1–37. doi: https://doi.org/10.1007/s10115-007-0114-2.
Hall M, Frank E, Holmes G, P fahringer B, Reutemann P, Witten I, Weka Data Mining Software; 2009. [Online version Weka 3.7.10]. http://www.cs.waikato. ac.nz/ml/weka.
WUM: ‘‘A Web Utilization Miner, 2003” [Online] http://wum.wiwi.huberlin. de.
Zeimpekis D, Gallopoulos E. TMG: A MATLAB Toolbox for Generating Term- Document Matrices from Text Collections. In: Kogan J, Nicholas C, Teboulle M, editors. Grouping Multidimensional Data. Berlin, Heidelberg: Springer; 2006.
p. 187–210. doi: https://doi.org/10.1007/3-540-28349-8_7.
Bonchi F, Giannotti F, Gozzi C, Manco G, Nanni M, Pedreschi D, Renso C, Ruggeri S. Web log Data Ware housing and Mining for Intelligent Web Caching. Data Knowl Eng 2001;39(2):165–89. doi: https://doi.org/10.1016/S0169-023X (01)00038-6.
Gonzalez-Cante FJ, Casilari E, Trivino-cabrera A. A Windows Based Web Cache Simulator Tool. In Proc. of the 1st International conference on Simulation tools and techniques for communications, networks and systems & workshops; 2008. p. 1–5, 2008. https://doi.org/10.4108/icst.simutools2008.2933.
