

Electronic Notes in Theoretical Computer Science 253 (2009) 119–141
www.elsevier.com/locate/entcs

An Interval-based Abstraction for Quantifying Information Flow
Chunyan Mu 1	David Clark 2
Department of Computer Science King’s College London
The Strand, London, UK

Abstract
In a batch program, information about confidential inputs may flow to insecure outputs. The size of this leakage, considered as a Shannon measure, may be automatically and exactly calculated via probabilistic semantics as we have shown in our earlier work. This approach works well for small programs with small state spaces. As the scale increases the calculation suffers from a form of state space explosion and the time complexity grows. In this paper we scale up the programs and state spaces that can be handled albeit at the cost of replacing an exact result with an upper bound. To do this we introduce abstraction on the state space via interval-based partitions, adapting an abstract interpretation framework introduced by Monniaux. The user can define the partitions and the more coarse the partitions, the more coarse the resulting upper bound. In this paper we summarise our previous contribution, define the abstract interpretation, show its soundness, and prove that the result of an abstract computation is always an upper bound on the true leakage, i.e. is a safe estimate. Finally we illustrate the approach by means of some examples.
Keywords: Security, Abstraction, Information Flow, Measurement, Language


Introduction
Information-flow security enforces limits on the use of information propagation in programs. The goal of information flow security is to ensure that the information propagates throughout the execution environment without security violations so that no secure information is leaked to public outputs. The traditional theory based reasoning and analysis of software systems are not concerned with security mea- surement. Quantitative information flow (QIF) proposes to determine how much information flows from conﬁdential inputs to public outputs. It is concerned with measuring the amount of information flow caused by interference between variables by using information theoretic quantities. Quantitative analysis therefore relaxes

1 Email: chunyan.mu@kcl.ac.uk
2 Email: david.j.clark@kcl.ac.uk

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.10.009

the well known non-interference [14] property by introducing a new policy: the pro- gram is secure if the amount of information flow from confidential inputs to public outputs is not too big.
In [26], we presented an automatic analyzer for measuring information flow within software systems. We considered the mutual information between a high security variable at the beginning of a batch program and a low one at the end of the program, conditioned on the initial values of low, as the measure of how much of the initial secret information is leaked by executing the program [4]. We incor- porated the leakage computation into Kozen’s probabilistic semantics to build an automatic analysis tool. This provided a more precise analysis. It took as input a probability distribution on the initial store and calculated a probability distribution on the final store when the program sometimes terminates. In fact it calculates a probability distribution at each program point. These can then be used to calculate the exact leakage. Specifically, while-loops were handled by applying the more gen- eral definition of entropy of generalized distributions [27] and related properties in order to provide a more precise analysis when incorporating elapsed observed time into the analysis. The drawback of this approach is that it is lacking in abstraction techniques, and time complexity can become large in certain circumstances. One of the time complexity issues is from mutual information computation [26]. An- other one is introduced by applying concrete semantics to while loop, which can be improved by applying the abstraction considered in this paper.
We introduce here an approximation method based on measurable partitions and Monniaux’s abstract probabilistic semantics [25]. We define a basic abstract domain using interval-based partitions with weights to abstract the measure space. We then define abstract semantic functions which transform the abstract state space in the abstract domain. The definition of leakage upper bound due to such seman- tics is presented. We introduce uniformalization to provide upper bounds on the leakage computation in the framework of information theory. The security property we consider is an a priori bound on leakage, that is, the program is regarded as sufficiently secure if the leakage is less than the a priori bound. Since the actual leakage in the program is required to be less than or equal to the a priori bound, the analysis is safe if we find an upper bound on the leakage [4].
The rest of the paper is organized as follows. Section 2 explains the concrete semantics and leakage computation used in our earlier work. In Section 3, we present a method to approximate our concrete leakage analysis and prove some properties of this. Finally, we present related work and draw conclusions in Section 4 and 5.



Leakage Analysis via Probabilistic Semantics

We have developed an automatic leakage analysis system in [26] without abstraction. This previous work is summarized here for the sake of clarity.

Measure space and programs
Assume that the tuple of program variables X→


range over the state space Ω. The

denotational semantics of a command is a mapping from the set X of possible environments before a command into the set X' of possible environments after the command. These spaces updated by semantic transformation functions, can be used to calculate leakage at each program point. Some useful definitions from [30] are reviewed as follows. A measure space is a triple (Ω, B, μ), where Ω is a set, B is a σ-algebra of subsets of Ω, μ is a nonnegative, countably finite additive set function on B. A σ-algebra is a set of subsets of a set X that contains ∅, and is stable under countable union and complementation. A set X with a σ-algebra σX defined on it is called a measurable space and the elements of the σ-algebra are the measurable subsets. If X and X' are measurable spaces, f : X → X' is a measurable function if for all W measurable in X', f −1(W ) is measurable in X. A positive measure is
a function μ defined on a σ-algebra σX , which is countable additive, i.e. if taking
(En)n∈N a disjoint collection of elements of σX , then μ( ∞  En) = Σ∞  μ(En).
A probability measure is a positive measure of total weight 1. A sub-probability measure has total weight less than or equal to 1.
The language
The language we considered is standard, presented in Table 1.

Table 1 The language


Probabilistic semantics and leakage computation
In [26], we used Kozen’s probabilistic semantics for a while language [17] to calculate the probability distribution on the final state from knowledge of the initial state. We then used this to calculate the quantity of leakage from any initial variable to any final variable.
Information theory introduced the definition of entropy, H, to measure the av- erage uncertainty in random variables. Shannon’s measures were based on a loga- rithmic measure of the unexpectedness of a probabilistic event (random variable). The unexpectedness of an event which occurred with some non-zero probability p
was log2 1 . Hence the total information carried by a set of events was computed
as the weighted sum of their unexpectedness: H = Σn	pi log2  1 . Assume we
have two types of input variables: H(confidential) and L(public), and the inputs
are equipped with probability distributions, so the inputs can be viewed as a joint

random variable (H, L). The semantic function for programs maps the state of inputs to the state of outputs. We present the basic leakage definition due to Clark, Hunt and Malacaria [4] for programs as follows.
Definition 2.1 (Leakage) Let H be a random variable in high security inputs, L be one in low security inputs, and let L' be a random variable in the output ob- servation, the secure information flow (or interference) is deﬁned by I(L'; H|L),
i.e. the conditional mutual information between the output and the high input given knowledge of the low output. Note that for deterministic programs, we have I(L'; H|L)= H(L'|L), i.e. interference between the uncertainty in the public output given knowledge of the low input.
The denotational semantics for measure space transformations are in the follow- ing forms:
V al  ⟨Ω, B, μ⟩	Σ  X→ → V al
C[[·]] : Cmd → (Σ → Σ)	E[[·]] : Exp → (Σ → Val)
B[[·]] : BExp → (Σ → Σ)
Consider the program as a state-transformation machine, and assume that the

tuple of program variables X→
range over the state space Ω. The program variables

X→ satisfy some joint distribution μ on program inputs: μ(X→ ) assigned to each
X→ ∈ Ω; μ(X→ ) ≥ 0; ΣX→ ∈Ω μ(X→ )= 1. Let [· ] denote any measure space transformer
given by a semantic function, precisely, [·]] means [[CB]] where CB ∈ Cmd ∪ BExp, i.e.
[[·]]:Σ → Σ'. In Kozen’s probabilistic semantics, a program transformation function
[[· ] maps distributions μ on X→ to distributions μ' = [[·]](μ) on X→ . Let X ranges over
Ω in Σ, X' ranges over Ω in Σ', μ is in Σ, and μ' is in Σ', i.e. X and X' denote the

measure space over X→
before and after [· ]. Consider the general form of an action

of one elementary operator:
X → (X' = [[·]](X))
μ → μ'


'	'	Σ
,⎨ δa,b =1 iff a = b

μ (X )= 
X∈Ω μ(X)δX' , [[·]](X) where,
, δa,b =0 iff a /= b

We concentrate on the distributions and present the space transformation functions offered by semantic functions by using Lambda Calculus and the notation of inverse function following [25] in Table 2.
Due to the measurability of the semantic functions, for all measurable W ∈ X', [[x := e]](W ) is measurable in X. The function [B]] for boolean test B defines the set of environments matched by the condition B: [[B]](μ)= λX.μ(X ∩ B), which causes the space to split apart, X ∩B denotes the part of the space X which makes boolean test B to be true. Conditional statement is executed on the conditional probability distributions for either the true branch or false branch: [c1]] ◦ [[b]](μ)+ [c2]] ◦ [[¬b]](μ). In the while loop, the measure space with distribution μ goes around the loop, and

at each iteration, the part that makes test b false breaks off and exits the loop, while the rest of the space goes around again. The output distribution [while b do c]](μ) is thus the sum of all the partitions that finally find their way out. Note that these partitions are part of the space when the loop partially terminates, which implies the outputs are partially observable and hence produce an incomplete distribution. The incomplete distribution incorporates the non-termination part (the rest of the space) to be a complete distribution finally. For the case that the loop is always non-terminating, ⊥ is returned and leakage is 0.
In what follows we present some of concrete semantic operations with leakage analysis.
Assignment Assignment updates the state such that the measure space of as- signed variable x is updated to become the measure space for expression e. For example, if there is no low input, the secure information leaked to low security variable x after command [x := e]] is give by L[[x:=e]] = H(μe). The secure infor- mation contained in e is considered as the entropy of its distribution μe.
Sequential composition command The distribution transformation function for the sequential composition operator is obtained via functional composition: [[c1; c2]](μ)= [c2]] ◦ [[c1]](μ).
Conditionals Conditional tests cause a partition of the measure space into values that make the boolean b true and those that make it false: let [b]]s denote the standard state transformer of boolean test, P0 ⊆ X, ∀x ∈ P0, [[b]]sx = tt, i.e. k makes test b be true; P1 ⊆ X, ∀x ∈ P1, [[b]]sx = ff, i.e. k makes test b be false, and P ∪ P = X ∧ P ∩ P = ∅. Let μ (tt)=    μ(x) denote the probability that b is true, and μb(ff) =  x∈P1 μ(x) denote the probability that b is false under
the current space. Let P0 = {p0 = μb(tt)}, P1 = {p1 = μb(ff)} denote the

partitions due to the test b, and Ql
= {q00,... , q0m}, Ql
= {q10,... , q1n} denote

the normalized distribution of the low security variable l inside the partitions. The semantic function is given by [if b then c1 else c2]](μ) = [c1]] ◦ [[b]](μ)+ [[c2]] ◦ [[¬b]](μ). The leakage into l due to the if statement can be computed by:
L[[if]] = H˜(P0 ∪ P1)+ H˜(Ql ∪ Ql | P0 ∪ P1)
where H˜ denotes the entropy of generalised distributions [26].
Example 2.2 To show how the method works, let us consider a simple program

Table 2
Probabilistic Denotational Semantics of Programs

[26].
if (h==0) then l=0 else l=1;
Assume h is a 32-bit high security variable with possible values 0,... , sk − 1 under uniform distribution, l is a low security variable. The boolean test splits

the original space and we get P0 = {  1  }, P1 = {1 −  1 }, and Ql
= {μl(0)} =

232	232	0
{  1  }, Ql = {μ (1)} = {1 −  1 }. The resulting set of distribution transformation
232	1	l	232
is obtained as:
⎛	⎡ ⟨0, ⊥⟩	→ 1/232 ⎤⎞	⎛	⎡	⎤⎞

⎜	⎢	⎥⎟
0 → 1/232

[[if]]	⟨h, l⟩ '→	...	... 
⟨232 − 1, ⊥⟩ → 1/232
⎥⎦⎟⎠ = ⎝l '→ ⎣
1 → 1 − 1/232 ⎦⎠

The distribution of low security variable l after the execution of the program is as

⎡ 0 → 1/232 follows: l '→ ⎣
⎤
32 ⎦, and the information leakage due to this example

1 → 1 − 1/2
can be computed by:
H[[if]] = H˜(P0 ∪ P1)+ H˜(Ql ∪ Ql |P0 ∪ P1)
0	1
1	1	−9
= H˜({ 232 }∪ {1 − 232 })+0 = 7.8 × 10

It is clear that this example just releases a little bit information to the public. The leakage analysis result also agrees with our intuition: the possibility of h =0 is quite low and the uncertainty of h under condition h /= 0 is still big, i.e. only small information released to the public.
While Loop In the while loop, the distribution goes around the loop, at each round, the part that makes test b false breaks off and exits the loop, and the rest goes around again. Command [while b do c ] can be rewritten as [[if ¬b then skip else {c; while b do c} ]. The operator for loops can be de- scribed in a recursive way, we have:
⎨ [[while b do c]]0(μ)= μ
, [[while b do c]]n(μ)= [[¬b]](μ)+ [[c]] ◦ [[b]]([[while b do c]]n−1(μ))
For the always terminating loops, the output distribution is the sum of all the sub-distributions that fail the conditional test on each iteration and find their way out of the loop untill [b]](μ)= ∅. The loop is considered as partially/completely non-terminated when no new partitions are produced but the test is still satisfied with respect to the partial/whole space. Consider a terminating loop as a sub- measure transformer which builds a set of accumulated incomplete distributions,
i.e. due to the kth iteration, P([[while b do c]]) =	0≤i≤k Pi([[while b do c]]),
where k ≤ n, and n is the maximum number of the partitions produced by the terminating loops. Let

, b0 = ff,	i =0 
Pi = {pi} = {μ(ei)}, where ei = ⎨ b0 = tt ∧ b1 = ff,	i =1 
,,, b0 = tt,... , bi−1 = tt ∧ bi = ff, i > 1
where ei is the event that the loop test b is true until the ith iteration, bi denotes the value of the boolean test b at the ith iteration. Consider the union of the decompositions
P = (P0 ∪ P1 ∪ ... ∪ Pk)0≤k≤n = ({p0}∪ {p1}∪ ... ∪ {pk})0≤k≤n
the events P0,... , Pk build the out-going partitions of the states for a while loop. The leakage computation of the loop given by addition of the entropy of the union of the boolean test for each iteration and the sum of the entropy of the loop body for each weighted sub-probability measures by applying the definition of entropy of partitions [29,27]:
Lwhile(k)= H˜(P0 ∪ ... ∪ Pk)+ H˜(QL ∪ ... ∪ QL|P0 ∪ ... ∪ Pk)

Σn  (pi log
1 )
Σn	Σm
k
qij log
qij

=	n	+
i=0 pi
n i=0
m j=0
qij

where Pi = {pi}, thus H(Pi)= log2 pi . The leakage computation formula works for terminating, partially terminating, and non-terminating loops. Further details and examples refer to [26].

Abstraction on Information Flow Measurement
To address the problem of tractability when the size and number of variables get large, we present a method to help us approximate our concrete analysis for leakage. Firstly, we define an abstraction on the measure space. The abstraction technique considered is partitioning of the concrete measure space into blocks. Each block is described by intervals and has a coefficient which is the upper bound on measures. Secondly, abstract semantic operations are applied on these partitions. The abstract transformation behavior of the distributions is described by the abstract transition function [·]] on the abstract objects based on Monniaux’s [25] abstract probabilistic semantics. To guarantee security, the analysis is safe if we find an upper bound on the leakage. Our analysis is required to over estimate the leakage and never under estimate it. Therefore, we introduce uniformalization to estimate the abstract spaces to provide safe bounds on the entropy computation. The leakage computation is obtained by estimating the abstract space to maximise the entropy and thus give safe (i.e. upper) bounds on the entropy.
Measurable Partitions and Abstract Domain
In the first step, we aim to provide an abstraction on the measure space transfor- mation which provides a basis for the leakage computation. The basic principle of abstraction here is to collapse sets of concrete states into single abstract states such

that concrete states are simulated by abstract ones. We give our basic abstract domain expressing the above idea formally, and take Monniaux’s [25] framework of abstract probabilistic semantics to construct abstract operations on such abstract objects.
Our abstract domain is based on partitions of the measure space. Any collection of non-empty disjoint sets that cover measure space X is said to be a partition of X. A partition of space X is defined as a family ξ = {Ei|i ∈ I} of nonempty disjoint subsets of X, whose union is X:  Ei∈ξ Ei = X. The sets Ei are the blocks of ξ.
This can be equivalently considered as a surjection φ : X → X/ξ, where the points
of the space X/ξ correspond to the sets {Ei} of the partition. An element of ξ is the inverse image φ−1(e) of a point e ∈ X/ξ. The quotient σ-algebra on X/ξ is defined as the pushforward under φ of the σ-algebra on X, i.e. a subset M ⊂ X/ξ is measurable if φ−1(M ) ⊂ X is measurable. φ is called a measurable partition function. Suppose we have a measure μ on X and a measurable partition ξ. The quotient measure μ' on X/ξ is defined as the pushforward of μ under the natural projection, i.e. for a subset M ⊂ X/ξ, define μ'(M )= μ(φ−1(M ')).
Partitions of sets can be viewed as random variables: if X is a finite set and
ξ = {E1,... , En} is a partition of X, W (X) denotes the weight of X, then p =
( W (E1) ,... , W (En) ) is a discrete probability distribution. The Shannon entropy of p
W (X)	W (X)
equals to the Shannon entropy of ξ according to Rokhlin [29]. This relation allows
the transfer of certain probabilistic and information-theoretical notions to partitions of sets, where we can take advantage of the partial order between partitions.
Definition 3.1 The partial order relation on Ξ(X) is deﬁned by ξ ≤ ξ' for ξ, ξ' ∈ Ξ(X) if H(ξ) ≤ H(ξ'). It is easy to get that if every block of ξ is included in a block of ξ' then ξ ≥ ξ'.
The largest element of this lattice is the partition αX = {{x}|x ∈ X}; the least one is the partition ωX = {X}. The set of partitions of X is denoted by Ξ(X). A partial order relation on (Ξ(X), ≤) is actually a bounded lattice.
Definition 3.2 (Abstract domain) Let Iv = [av, bv], where v in X→ . Then let

[Ei]
def
=	Iv v in
X→ } be the interval-based partition of X. The abstract domain

is deﬁned as: X  d=ef
{⟨αi, [Ei]⟩}0<i≤n, where n is the number of the partitions.

A component of the abstract domain X is deﬁned as a pair ⟨αi, [Ei]⟩, where αi denotes the weight on [Ei], [Ei] is a partition on concrete space X and described by a set of intervals on possible values of each variable v in the vector of program variables X→ : ∃ intervals {Iv = [av, bv]v∈X→ } on partition Ei, where av = min{Val v },
bv = max{Val v }, and Valv denotes the possible values of v inside Ei.
Intuitively, a single partition is constructed by choosing an interval on possible values for each variable. The partitions can be arbitrary, but certain human in- tervention can optimise the analysis. All different strategies of partitioning on the space can produce safe leakage bounds, but with different precision with regard to the resulting upper bound. The abstract space can be viewed as X⟨α, γ⟩X , where X denotes the concrete space, X denotes the abstract space, α denotes the

abstraction function, and γ denotes the concretisation function as usual.
Definition 3.3 (Abstraction funtion) The abstraction function α is a mapping
from concrete space X to the sets of interval-based partitions X : X −w→ [X/ξ] with
weights w over the interval-based partitions [X/ξ], where [X/ξ]= {⟨αi, [Ei]⟩}0<i≤n
(assume n is the number of the partitions).
The measurability of this collection of partitions follows from their disjointness. However, non-injective semantics [· ] can produce non-disjoint sets in some cases. For the case of non-disjoint partitions created by semantic function [·]] : X → X, we extend the space to eliminate the collisions. The method we considered is putting an index i on the overlapped part to distinguish them, where i is from the index of the partition Ei in which it is located. Since the initial partitions {Ei|i ∈ I} before semantic functions are disjoint, there exist measures μi such that  μi = μ( Ei), and for all i, μi is concentrate on Ei and μi ≤ αi. It is clear that such scenario will never underestimate the leakage computation due to the definition of leakage upper bound in Definition 3.12 (see Proposition 3.15).
The concretisation of such abstract objects is thus the set of all measures match- ing the above conditions. Consider a sub-partition η on each element of the final abstract space X to concretize it.
Definition 3.4 (Concretisation function) The concretisation function γ is thus a mapping: X →  {x|x ∈ [Ei/η]}, where Ei is the blocks of the abstract object X , η is a sub-partition on each block under uniform distribution.
The sub-partition η will be further discussed in section 3.4 for the purpose of safe leakage computation.
Proposition 3.5 X⟨α, γ⟩X  is a Galois connection.
Proof. For complete lattices X and X , to prove the pair ⟨α, γ⟩ form a Galois connection, we need to prove that for all C ∈ X and A ∈ X , C ±X γ ◦ α(C) and α ◦ γ(A) ±X A.
The concrete space X with measures (q1,... , qm) can be viewed as a composition of two partitions denoted by: ξη0, where ξ = {E1,... , En} with measures μ1,... , μn is the same partition as the abstraction X given by α. Sub-partition η0 is the partition on ξ which recovers the abstract space X to be the original concrete space,
i.e. assume Nk is the number of elements of Ek (where 1 ≤ k ≤ n and therefore

n k=1
Nk ), according to the definition of entropy of partitions [29,27], for all

C ∈ X, we have
n
H(C)= H(ξη )= H(ξ)+ H(η |ξ)= H(μ ,... ,μ )+ Σ μ H( pi1 ,... , piNi )

0	0	1

n	i	μ	μ
i=1

where p11 = q1, ... , pnNn = qm is the measures over X, and m = Σn	Nk.
The abstract space X can be viewed as a partition ξ over X given by abstraction function α. The concretisation on Xsharp is given by sub-partition η (uniformal- ization) on each block. Therefore γ ◦ α can be considered as the composition of partitions ξη, i.e. for all A ∈ X , we have

H(γ(α(A))) = H(ξη)= H(ξ)+ H(η|ξ)= H(μ ,... ,μ )+ Σ μ H(  1 ,... ,  1 )
1	n	i	N	N
i=1
since uniform distribution maximise the entropy, we have
1	1	pi1	piNi
H(	,... ,	) ≥ H(	,... ,	)	for all 0 < i ≤ n
Ni	Ni	μi	μi
By (1)(2)(3) it is easy to get that H(C)  ≤  H(γ(α(C))), and similarly
H(α(γ(A))) ≤ H(A). Therefore, C ±X γ ◦ α(C) and α ◦ γ(A) ±X A.	 
Abstract Transformation
We have presented an abstract domain to approximate the concrete measure spaces. We need abstract semantic functions to produce the transformations of such abstract spaces. The leakage upper bound Uv on variable v will be discussed in Section 3.4. We just concentrate on the abstract space transformation here. Assume [·]] is the abstract semantic function, which drives the abstract spaces to transform. For the purpose of leakage computation, the abstract transformations are required to keep the measurability and properties of entropy computation. We concentrate on the abstract semantic functions over measurable partitions. It is easy to extend to abstract transformation over interval-based measurable partitions defined in our abstract domain.
Proposition 3.6 If φ : X → X/ξ is a measurable partition function on the measur- able space X and [[·]] is the measurable space transformation function on X given by semantic functions, then [[·]] (the abstract analogue of [[·]]) is a measurable function on X = φ(X).
Proof. To prove [·]] is measurable, we need to prove that for all M ⊂ X/ξ, [[·]] −1(M ) is measurable.
[[·]] −1(M )= {x|φ([[·]](x)) ∈ M } = {x|[[·]](x) ∈ φ−1(M )}
i.e. [[·]] −1(M ) is the set of all those elements of X which are mapped into M by [·]].
Since φ is measurable, φ−1(M ) is a subset of elements of X, the measurability of the set
[[·]] −1(M )= {x|[[·]](x) ∈ φ−1(M )} = [[·]]−1(φ−1(M ))
follows from the measurability of [·]].

Arithmetic Expressions
In concrete analysis, we denote the probability function of variable v as μv. Let μv(c) be the probability that the value of v is c, the domain of μv will be the set of all possible values that v could be. The computation function e(μ) of an arithmetic expression e defines the measure space that the arithmetic expression can evaluate to in a given set of environments.
Now consider the abstract arithmetic operation e(x, y)  = z . We define the

abstract arithmetic operation ⟨αi, {[av, bv]	→ }i⟩ → ⟨α' , {[a' , b' ]
→ }i⟩ as: z  =

v∈X
i	v	v v∈X

e(x, y) (⟨αi, {[av, bv]v∈X→ }i⟩), and for all i ∈ N , we take,
α' = αi
,, if v = z	a' = min{e(zx, zy)|ax ≤ zx ≤ bx, ay ≤ zy ≤ by}

,⎨
v
' = max{e(zx, zy)|ax ≤ zx ≤ bx, ay ≤ zy ≤ by}

,,, otherwise a' = av, b' = bv
The arithmetic expression returns the probable values on the abstract lattice with the current context: the weight of each abstract element is unchanged, i.e. α' = αi; the interval of variable v is updated by the intervals on probable values of the expression e if v suppose to be the assigned variable.
Example 3.7 Consider expression [x+2y ], assume the initial distribution for ⟨x, y⟩
is μ⟨x,y⟩ (the distribution function on a set M is viewed as a mapping: M → [0, 1]):
⎛ ⟨0, 0⟩→ 0.1, ⟨1, 1⟩→ 0.1, ⟨2, 1⟩→ 0.1 E1	⎞
μ	'→ ⎜ −−−− −− − −−− −− −−−− −− −−− ⎟

⟨x,y⟩
⟨3, 2⟩→ 0.2, ⟨4, 2⟩→ 0.2, ⟨5, 2⟩→ 0.1
⟨6, 3⟩→ 0.1, ⟨7, 3⟩→ 0.1	E2

We take the partition: ,⎨ E1⟨[0, 2]x, [0, 1]y⟩ : α1 = 0.3
, E2⟨[3, 7]x, [2, 3]y⟩ : α2 = 0.7


Apply the arithmetic expression μ


Assignment


[[x+2y]]
, we get: ,⎨ E1⟨[0, 4][[x+2y]]⟩ : α1 = 0.3
, E2⟨[7, 13][[x+2y]] ⟩ : α2 = 0.7

Assignment operation [x := e]] updates the state such that the abstract domain of assigned variable x is mapped to the domain of expression e. The upper bounds on
the weight of partitions remain unchanged: α' = αi, and the interval of assigned
variable x is updated by the interval of the expression e: ax = min{Val e}, bx =
max{Val e}.
Sequential command
The sequential operator can be obtained by composition: [c1; c2]] (X ) = [c2]] ◦
[[c1]] (X ). Assume X ' = [[c1]](X ), and X '' = [[c1]](X '), then X '' is the new state

space by executing the sequential command over X : X
[[c1;c2]] 
−→	X
 ''.

Conditional
The tests split the abstract space into two parts according to the result of boolean test over the space. The weight on each part is the probability of the part of the space that makes the boolean test be true or false. The if statement returns the sum

of the two parts by executing the statements under true and false branches, therefore the intervals on variables are updated but the weight on each abstract block remains unchanged. Let + be an abstraction of the sum operation on measures, the abstract semantic function of tests are given by:
[[if b then c1 else c2]] (X )  [[c1]] ◦ [[b]] (X ) + [[c2]] ◦ [[¬b]] (X )
We define the abstract operation (X ,XT ,XF ) → X' , where XT denotes the
abstract object under true branch: XT ⟨αT , {[aT , bT ]	→ }i⟩ = [[c1]] ◦ [[b]] (X ), and
i	v	v v∈X
XF denotes the abstract object under false branch: XF ⟨αF , {[aF , bF ]	→ }i⟩ =

[[c2]] ◦ [[¬b]] (X ). We have, ∀v ∈ X→ :
i	v	v
v∈X

(⟨αi, {[av, bv]}i⟩, ⟨αT , {[aT , bT ]}i⟩, ⟨αF , {[aF , bF ]}i⟩) → ⟨α' , {[a' , b' ]}i⟩

i	v	v
for all v ∈ X→ , i ∈ N , we take:
α' = αT + αF = αi;
i	v	v
i	v	v

i	i	i
a' = min(aT , aF ), and b' = max(bT , bF ) on each interval-based partition [Ei].
v	v	v	v	v	v
Example 3.8 Consider program
if(x==0) then y:=0 else y:=1;
Assume x is a 3-bit high security variable with distribution
⎛ 0 → 0.1 1 → 0.1 2 → 0.1 3 → 0.1 ⎞
⎝ 4 → 0.2 5 → 0.2 6 → 0.1 7 → 0.1 ⎠
y is a 3-bit low security variable under any distribution. Let us take partition as: ⎨ E1⟨[0, 3]x, [0, 7]y ⟩ : α1 = 0.4 .  After applying the abstract operation for if
, E2⟨[4, 7]x, [0, 7]y⟩ : α2 = 0.6
,⎨ E' ⟨[0, 3]x, [0, 1]y⟩ : α' = 0.4
, E' ⟨[4, 7]x, [1, 1]y⟩ : α' = 0.6
Loops
The concrete semantics of loops are given as infinite sums of all the partitions which have found their way out in a recursive way (see Section 2.3). In order to consider the approximation of the fixpoint of some monotone operators on the concrete lattices and study the abstract operations, we rewrite the semantic function of loops as [while b do c]](X)= [[¬b]](limn→∞ Xn) following [25], where Xn is defined recursively: X0 = λX.0, and Xn+1 = Xn + [[c]] ◦ [[b]](Xn). We shall deal with it using fixpoints on the abstract lattice (X , ±). Let us take X as the abstraction of measure space X, assume lfp : (X mo−no→tone X ) → X is an approximation of the least fixpoint, i.e. if [·]] : X → X is monotonic then we have the approximation relation [·]] (lfp (f)) ± lfp (f). Let + be an abstraction of the sum operation on measures, ⊥ be an abstraction of the null measure, the abstract function is given by: [while b do c]] (X )   [[¬b]] (lfp χ '→ X H [[c]] ([[b]] (χ ))). We define the

abstract operation: (⟨αi, {[av, bv]
→ }i⟩, ⟨α' , {[a' , b' ]
→ }i⟩) → ⟨α'', {[a'', b'']
→ }i⟩

v∈X
i	v	v v∈X
i	v	v v∈X

as the least upper bound: ⟨αi, {[av, bv]
→ }i⟩ H ⟨α' , {[a' , b' ]
→ }i⟩, and ∀i ∈ N , we

take:
α'' = max(αi, α' )
v∈X
i	v	v v∈X

i	i

a'' = min(av, a' ), and b'' = max(bv, b' ), for all v ∈ X→
on [Ei]

v	v	v	v
Furthermore, to figure out the approximation of least fixpoint operation lfp , we need an operation to be applied using widening operators ∇ to guar- antee termination of the increasing sequences, and the feasibility of the fixed point computation.  By widening, the abstract function (X, X') → X'', i.e.

(⟨αi, {[av, bv]
→ }i⟩, ⟨α' , {[a' , b' ]
→ }i⟩) → ⟨α'', {[a'', b'']
→ }i⟩ can be noted as

v∈X
i	v	v v∈X
i	v	v v∈X

⟨αi, {[av, bv]	→ }i⟩∇⟨α' , {[a' , b' ]
→ }⟩ as standard. By induction, the upward it-

v∈X
i	v	v v∈X

eration sequence is defined as X '	= X '∇X 
as usual, which is required to be

n+1	n	n
eventually stationary for every ascending chain elements and limited by a sound

upper approximation of lfp for any sequence (X )
. Since our abstract domain

is based on partitions of the measure space with coefficients of intervals of variable values, i.e. interval-based approximation, we choose the ordinary widening operator for intervals on the integers ∇I [8]: x∇Iy = ∞, if x < y, otherwise x∇Iy = y. The intuition behind applying this widening operator is that if a sequence of coefficients Iv keeps ascending, an upper approximation of it is obtained by using +∞. For all i ∈ N , we take:
α'' = max(αi, α' )
i	i

Intervals Iv = [av, bv]v∈X→
on [Ei] are given as standard:

⊥∇I [av, bv]	= [av, bv]
[av, bv]∇I ⊥	= [av, bv]
[av, bv]∇I[a' , b' ]= if a' < av then −∞ else av

v	v	v
if bv < b'
then + ∞ else bv


Weight on each abstract elements is the maximum one between before and after the current iteration. By applying the widening operation of intervals ∇I , we enforce the convergence and induce termination for intervals on each abstract blocks.

Soundness of Abstract Functions
It is clear that, for a Galois connection, X⟨α, γ⟩X , and semantic functions [·]] :
X → X, [[·]] : X → X , X is a sound approximation of X iff
(α ◦ X )(S) ±X  (X ◦ α)(S), for all S ∈ X

iff

([[·]] ◦ γ)(A) ±X (γ ◦ [[·]] )(A), for all A ∈ X 
Furthermore, given the Galois connection, X⟨α, γ⟩X , and operation [·]] : X →

X, the most precise [·]] 
best
: X → X , which is sound with respect to [·]] is [[·]]	=

α ◦ [[·]] ◦ γ.

Lemma 3.9 (Soundness of composed functions) Assume function f 
is a

sound abstraction of function f1, and f  is a sound abstraction of f2, then the ab-
stract composed function f ◦ f is sound with respect to concrete composed function
1	2
f1 ◦ f2.
α ◦ f1 ± f ◦ α ∧ α ◦ f2 ± f ◦ α =⇒ α ◦ (f2 ◦ f1) ± (f ◦ f ) ◦ α
1	2	2	1
Proof.
f ◦ f ◦ α ± (α ◦ f ∗ ◦ γ) ◦ (α ◦ f ∗ ◦ γ) ◦ α	(because α ◦ f ∗ ◦ γ ± f )
1	2	1	2
= α ◦ f ∗ ◦ (γ ◦ α) ◦ f ∗ ◦ (γ ◦ α) ± α ◦ f ∗ ◦ f ∗ (because γ ◦ α ± Idc)
1	2	1	2

Theorem 3.10 (Soundness of abstract functions) [[·]] is sound with respect to concrete semantic functions [[·]].
Proof. To show the soundness of the abstraction is equivalent to prove that [·]] is sound with respect to [·]] iff

[[·]] 
(A) = α ◦ [[·]] ◦ γ(A) ±X
 [[·]] (A), for all A ∈ X 

Assignment: [[x := e]]  is sound with respect to [x := e]] iff [[x := e]]	=
α ◦ [[x := e]] ◦ γ ±X →X [[x := e]] . Assume we concentrate on interested variable
v with interval [a ,b ], let M = e (X ) = {⟨α , [a ,b ]⟩}	, we have γ(M ) = 

i  i	e
i	i  i
i∈N	e

  i∈N {αi/Ni, si|ai ≤ si ≤ bi, Ni = bi − ai + 1}, then we have:
[[x := e]]	(x, M ) (X )
= α ◦ [[x := e]](x, γ(M )) (X )
= α([[x := e]](x,  {⟨αi/Ni, si⟩|ai ≤ si ≤ bi, Ni = bi − ai + 1}) (X ) i∈N
= X (  {⟨αi, Ei(x '→ [ai, bi])⟩})
i∈N
± [[x := e]] (x, M ) (X )	(by Proposition 3.5)
Compositional operator The soundness proof of sequential abstract function is straightforward by Lemma 3.9.
Conditional To prove soundness of the abstract function for if statement, we need to show that α ◦ [[if b then c1 else c2]] ± [[if b then c1 else c2]] ◦ α, i.e., for all S ∈ X:
α([[if b then c1 else c2]])S ± [[if b then c1 else c2]] (α(S)) By Proposition 3.5, it is clear that,
α([[b]])S =  {⟨αT , {[aT , bT ]	→ }⟩} ± [[b]] (α(S)) ◦ α

i
i∈N
v	v v∈X

α([[¬b]])S =  {⟨αF , {[aF , bF ]
→ }⟩} ± [[¬b]] (α(S)) ◦ α

i
i∈N
v	v v∈X

By Lemma 3.9 and IH, it is easy to get that,
[[c1]] ◦ [[b]] + [[c2]] ◦ [[¬b]] ± [[c1]] ◦ [[b]] + [[c2]] ◦ [[¬b]] 

Loop To prove soundness of [while b do c]] with respect to [while b do c]], we need to show α ◦ [[while b do c]] ± [[while b do c]] ◦ α, i.e. for all S ∈ X
α([[while b do c]](S)) ± [[while b do c]] (α(S))
Let F = λχ.X ∪ [[c]]([[b]]χ), and F = λχ.X H [[c]] ([[b]] χ) and thus
lfp F = lfp lim (λχ.X ∪ [[c]]([[b]]χ))n
n→∞
lfp F = lfp lim (λχ.X H [[c]] ([[b]] χ))n
n→∞
Starting with ⊥, we apply the function F over and over again to build up a sequence of partial functions F0, F1, ... , Fn, similar to F . By IH, we have:
The base case is obvious when the function F is applied 0 time:
α ◦∅ ± ⊥ ◦ α =⇒ α ◦ (λχ.X ∪ [[c]]([[b]]χ))0 ± (λχ.X H [[c]] ([[b]] χ))0 ◦ α
IH: Assume 6 n < k, α ◦ Fn ± F ◦ α
then, α ◦ Fk−1 ± F	◦ α	by IH (k — 1 < k)
and α ◦ F1 ± F ◦ α	by IH (1 < k)

by Lemma 3.9 and definition of Fn
and F , we have

α ◦ (Fk−1 ◦ F1) ± (F	◦ F ) ◦ α	iff	α ◦ Fk ± F ◦ α
k−1	1	k
Therefore, soundness of abstract function F is obtained as:
α ◦ (λχ.X ∪ [[c]]([[b]]χ))n+1 ± (λχ.X H [[c]] ([[b]] χ))n+1 ◦ α
By Lemma 3.9, we have
α ◦ [[чb]](lfp λχ.X ∪ [[c]]([[b]]χ)) ± [[чb]] (lfp λχ.X H [[c]] ([[b]] χ)) ◦ α
i.e. α([[while b c]](S)) ± [[while b c]] (α(S)). This completes the proof of the soundness of abstract semantic function for loops.

Entropy of Measurable Partition and Leakage Computation
Sections 3.1 and 3.2 have described an abstraction on the measure space and the abstract semantic operations. In order to measure the information flow, we also need to consider an approximation on the leakage computation. Let ξ be our measurable
partition of space X and E1, E2,... , En be elements of ξ of positive measure (where
n is the number of partitions), and μi such that Σn	μi = μ(  Ei) and μi is
concentrated on Ei. The entropy of ξ is given by Rokhlin [29]: if μ(X —  Ei) = 
0, 7(ξ) = — Σn	μi log2 μi; if μ(X —  Ei) > 0, 7(ξ) = +∞. 7(ξ) is called the
entropy of ξ.
We have introduced that if ξ, ξ' ∈ Ξ(X) and ξ ≤ ξ', then 7(ξ) ≤ 7(ξ'). How- ever, the partition entropy may underestimate the leakage computation. To get a conservative leakage analysis, we need a re-approximation on the final abstract space aimed to provide a safe (upper) bound on the leakage computation at the end. The basic method here is that we do a subpartition η on each element of the final abstract objects obtained by the abstract operations to maximise entropy. The sub- partition we considered is performing a uniformalization over each element of the

final partition ξ' = [[·]]ξ according to the fact that uniform distribution maximises entropy. We call a block uniform if all its states inside have the same probability,
i.e. all possible values of the distribution within that block are uniform.
Definition 3.11 (Uniformalization) Uniformalization is deﬁned as a transfor- mation of each block of space of a variable into a space with uniform distribution on each block, i.e. each partition is sub-partitioned under uniform distribution due to the values of an interested (low) variable.
Based on the definition of uniformalization on measurable partitions, next con- sider the leakage upper bound Uv due to our abstract domain. Let us consider a subpartition η under uniform distribution on the final partition ξ' in which any ele- ment E' ∈ ξ' comprises of Ni sub-elements with equal probability (where [·]]ξ = ξ', and ξ is the initial partition at the begin of the program), and let us attach to this partition the finite discrete uniform probability distribution whose corresponding entropy will be log2(Ni), where Ni denotes the size of the final abstract element E' due to the low security variable considered. We consider the entropy of the compo- sition of partitions ξ' and η denoted by 7(ξ'η), which is defined as the sum of the well-defined entropy 7(ξ') and the mean conditional entropy of η with respect to ξ': 7(η|ξ') [29], i.e.
7(ξ'η)= 7(ξ')+ 7(η|ξ')
The conditional entropy 7(η|ξ') is a non-negative measurable function on the factor
space X/ξ, which is defined by [29]: 7(η|ξ') = Σn	μi7(η), where μi denotes
the measure of each partition given by ξ'. The leakage upper bound is therefore defined as the entropy on the compositional partitions ξ'η. It is clear that the entropy of a measurable sub-partition η on partition of ξ' into Ni sets is less than or equal to log2(Ni) if and only if every element of the sub-partition has measure 1/Ni. This meets the intuition of our method: computation time is saved by doing abstract transformations on abstract partitions ξ of the measure space, the safety of the leakage computation is guaranteed by considering mean conditional entropy of uniform partition η with respect to the final abstract partition ξ' returned by the execution of the program.
Definition 3.12 (Leakage upper bound) The leakage upper bound is given by the entropy of paritition ξ'η, according to the deﬁnition of entropy on partitions and deﬁnitions of ξ' and η, we have:
Uv = 7(ξ'η)= 7(ξ')+ 7(η|ξ')
= 7(μ ,... ,μ )+ Σ μ 7( μi/Ni ,... , μi/Ni )
1	n	i	μ	μ
i=1
n
= 7(μ ,... ,μ )+ Σ μ 7(  1 ,... ,  1 )
1	n	i	Ni	Ni
i=1
n
= 7(μ1,... , μn)+ Σ μi log2(Ni)
i=1

where Ni is the size of the partition Ei, i.e. Ni = (bv — av + 1) for partition Ei ∈ ξ'.


Example 3.13 Consider [l := h + 1 ] as an example to show some intuition of the
⎛ 1 → 0.3 2 → 0.4 ⎞
method. Assume the initial distribution of h is ⎜ 3 → 0.1 4 → 0.1 ⎟, and the dis-


⎛ 0 → 0.5 ⎞
⎝ 5 → 0.1	⎠

tribution of l is
⎝ 1 → 0.5
⎠. Consider the partitions ξ1 on the joint distribu-


tion:
, E1⟨[1, 1]h, [0, 1]l⟩ : α1 = 0.3
E2⟨[2, 2]h, [0, 1]l ⟩ : α2 = 0.4 .  The abstract semantic functions transform
,,, E3⟨[3, 5]h, [0, 1]l⟩ : α3 = 0.3

,, E' ⟨[1, 1]h, [2, 2]l⟩ : α' = 0.3

the above abstract objects into ξ' :
1
E' ⟨[2, 2]
1
, [3, 3] ⟩ : α'
= 0.4 . In the next step,

1	2	h	l	2
,,, E' ⟨[3, 5]h, [4, 6]l⟩ : α' = 0.3
we perform uniformalization on ξ' , and concentrate on the low security variable l:

⎛ [2, 2]l → 0.3 ⎞
⎛ 2 → 0.3/1 ⎞
⎜ 3 → 0.4/1 ⎟

ξ' ⎜ [3, 3]l
→ 0.4 ⎟ Uniform=a⇒lization ξ' η1 ⎜ 4 → 0.3/3 ⎟

⎝ [4, 6]l → 0.3 ⎠
5 → 0.3/3
6 → 0.3/3


Finally we do the leakage computation due to such spaces based on the leakage definition: the leakage upper bound Ul = 7(0.3, 0.4, 0.3) + 0.3 ∗ log2 3= 2.0464.


In order to show that the strategy of partitioning affect the precision of leakage computation, let us consider another strategy way of partitions ξ2:
⎨ E1⟨[1, 3]h, [0, 1]l ⟩ : α1 = 0.8 .	The abstract semantic functions transform the

, E2⟨[4, 5]h, [0, 1]l⟩ : α2 = 0.2
above abstract objects into ξ' :
,⎨ E1⟨[1, 3]h, [2, 4]l⟩ : α1 = 0.8 . After doing uni-
, E2⟨[4, 5]h, [5, 6]l⟩ : α2 = 0.2

formalization, we have:

' ⎛ [2, 4]l
ξ ⎝





→ 0.8 ⎞ Uniformalization  '
⎠	=⇒	ξ η

⎛ 2 → 0.8/3 ⎞
⎜ 3 → 0.8/3 ⎟
⎜ 4 → 0.8/3 ⎟

2
[5, 6]l → 0.2
2 2
5 → 0.2/2
6 → 0.2/2

and the leakage upper bound of variable l is given by:
Ul = 7(0.8, 0.2) + 0.8 × log2 3+ 0.2 × log2 2= 2.1899
Furthermore, it is easy to get that the exact leakage due to this simple example is L = 7(0.3, 0.4, 0.1, 0.1, 0.1) = 2.0464. This simple example suggests that unifor- malization on abstract objects preserves the safety of leakage computations, and the strategy of partition may affect the precision of the computation: the first strategy of partition in this example is much better than the second one since the first one is closer to the exact leakage. But both of them provide safe leakage computation,
i.e. never underestimate the leakage analysis. The precision of partitioning strategy depends on the initial distribution of high security inputs and also the programs, but not in general. One can image that the analyzer can produce more precise result if the partitions produced by the partitioning strategy and abstract operations in the abstract lattice is closer to the concrete partitions produced by the programs (especially conditional tests and loops) in the concrete lattice.
Example 3.14 Consider a loop program
l:=0; while(l<h) do l++;
⎛ 0 → 0.1, 1 → 0.1 ⎞
assume h is a 3-bit high security variable with distribution	2 → 0.1, 3 → 0.1	, l
4 → 0.2, 5 → 0.2
6 → 0.1, 7 → 0.1
is a 3-bit low security variable with an initial value of 0. Let us take partition ξ as:
⎨ E1⟨[0, 3]h, [0, 0]l ⟩ : α1 = 0.4 . According to the above abstract operation, we can
E2⟨[4, 7]h, [0, 0]l⟩ : α2 = 0.6
get that,
' = 0.4, [ah, bh]= [0, 3], [al, bl]= [0, 3]
' = 0.6, [ah, bh]= [4, 7], [al, bl]= [0, 7]
Finally we compute the entropy of y by leakage definition after uniformalization: leakage upper bound Ul = 7(0.4, 0.6) + 0.4 ∗ log2 4+ 0.6 ∗ log2 8= 3.57. That is of course not as good as the exact computation:
L = 7(0.1, 0.1, 0.1, 0.1, 0.2, 0.2, 0.1, 0.1) = 2.92

but still offers a reasonable upper bound. This example also shows our abstraction techniques improves the time consuming problem of leakage analysis. Note that the concrete analysis needs 8 time-unit(iteration) to get the result while the abstract analysis needs 2 units.

The measurement produced by our technique should be an upper bound on the actual information flow, so that our technique can overestimate the amount of infor- mation leaked, but can never underestimate it. To address this concern, Proposition
3.15 is presented to show the soundness of our approximation for abstract leakage analysis formally.
Proposition 3.15 Assume the ﬁnal abstract space obtained by the semantic func- tions [[·]] be a ﬁnite measurable partition ξ. After performing uniformalization sub- partition η on these output abstract objects ξ, we get safe leakage upper bound by computing the entropy of ξη.

Proof. Consider a distribution μ over a set of events E containing N elements and let ξ = {E1,... , En} be the partitions of this set, η be a sub-partition on ξ. Assume
Nk is the number of elements of Ek, we have N = Σn	Nk, and let μ(Ek) be the
weight of the partition Ek(k = 1,... , n). Let the elements of E be labeled from 1 to N , i.e. E = {e1,... , eN } and the elements of Ek(k = 1,... , n) be labeled from 1 to Nk. According to the definition of entropy of partitions [29,28], the entropy of such space over E can be computed by:
7(ξη)= 7(ξ)+ 7(η|ξ)= 7(μ ,... ,μ )+ Σ μ 7( μi1 ,... , μiNi )
1	n	i	μ	μ
i=1
Now consider that the η is under uniform distribution over each element of partition ξ, since the number of elements in partition Ek is Nk, the entropy of the space after uniformalization on the partitions can be computed by:
7¯(ξη)= 7(μ ,... ,μ )+ Σ μ 7(  1  ,... ,  1 )
1	n	i	N	N
i=1
Since uniform distribution maximum entropy, it is clear that
n	n
Σ 7(  1 ,... ,  1 ) ≥ Σ 7( μi1 ,... , μiNi )

Ni
i=1
Ni	μi	μi
i=1

Therefore we get 7¯(ξη) ≥ 7(ξη), i.e. we show that uniformalization on the partitions keeps the entropy computation safe.	 

We have already introduced a method to provide an approximation analysis of our automatic leakage analysis system presented at [26]. The method produces safe leakage bounds on variables to address the scalability problem of concrete leakage analysis.

Related Work
Quantitative information flow has recently become an active research topic in the computer security community. The precursor for this work was that of Denning in the early 1980’s. Denning [10] presented that the data manipulated by a pro- gram can be typed with security levels, which naturally assumes the structure of a partially ordered set. Moreover, this partially ordered set is a lattice under certain conditions [9]. However, he did not suggest how to automate the analysis. In 1987, Millen [24] first built a formal correspondence between non-interference and mutual information, and established a connection between Shannon’s information theory and state-machine models of information flow in computer systems. Later related work is that of McLean and Gray and McIver and Morgan in 1990’s. McLean pre- sented a very general Flow Model in [23], and also gave a probabilistic definition of security with respect to flows of a system based on this flow model. The main weakness of this model is that it is too strong to distinguish between statistical correlation of values and casual relationships between high and low object. It is also difficult to be applied to real systems. Gray presented a less general and more detailed elaboration of McLean’s flow model in [15], making an explicit connection with information theory through his definition of the channel capacity of flows be- tween confidential and non-confidential variables. Webber [32] defined a property of n-limited security, which took flow-security and specifically prevented downgrad- ing unauthorized information flows. Wittbold and Johnson [33] gave an analysis of certain combinatorial theories of computer security from information-theoretic perspective and introduced non-deducibility on strategies due to feedback. Gavin Lowe [19] measured information flow in CSP by counting refusals. Aldini and Di Pierro [1] introduced a method of quantifying information flow on a probabilistic process system, The method is based on process similarity relation with regard to an approximation of weak bisimulation of CCS. Backes [2] gave a definition for mea- suring the quantity of information flow within interactive settings by measuring the distance between different behaviours of high user from low user’s views. McIver and Morgan [22] devised a new information theoretic definition of information flow and channel capacity. They added demonic non-determinism as well as probabilis- tic choice to while program thus deriving a non-probabilistic characterization of the security property for a simple imperative language. There are some other attempts in the 2000s: Di Pierro, Hankin and Wiklicky [11,12,13] gave a definition of prob- abilistic measures on flows in a probabilistic concurrent constraint system where the interference came via probabilistic operators. Clarkson et al. [7] suggested a probabilistic beliefs-based approach to non-interference. This work might not works for most of situations. Different attackers have different beliefs, therefore the worst case is required. Boreale [3] studied the quantitative models of information leakage in the process calculi. Clark, Hunt, and Malacaria [4,5,6] presented a more complete reasonable quantitative analysis for a particular program in imperative languages but the bounds for loops are over pessimistic. Malacaria [20] gave a more precise quantitative analysis of loop construct but it is hard to automate. McCamant and Ernst [21] investigated techniques for quantifying information flow revealed by com-

plex programs by building flow graphs and considering the weight of the maximum flow over it. K¨opf and Basin [16] developed a quantitative model for assessing a system’s vulnerability to adaptive side-channel attacks.

Conclusions and Future Works
We aimed to develop automatic and high quality leakage analysis tools for programs. In [26], we devised a method that implements Kozen’s [18] concrete probabilistic semantics, and applied this semantics to calculate leakage. In this paper, we present an approach to provide an approximation on such leakage analysis.
We are currently integrating the abstraction techniques into our implementation of concrete leakage analysis tool. We also expect to improve our analysis based on the experimental results of the influence of certain parameters over the quality of approximation. We propose to incorporate more parameters to obtain a better approximation result, especially due to the time complexity issues introduced by the calculation of mutual information. Another possible plan is to produce an abstract backward analysis for quantitative information flow. The idea is that we start from a description of an output event, and compute back to the description of the input domains describing their distribution of making the behavior happen. This allows the effective computation of upper bounds on the probability of outcomes of the program. Furthermore, it is necessary to attack the problem of computation expense of mutual information. We consider to develop a chopped information flow graph for representing information flow of program by capturing and modeling the information flow of data tokens on data slice. Smith [31] introduced a new foundation based on a concept of vulnerability recently, which measures uncertainty by applying R´enyi’s min-entropy rather than Shannon entropy. Our method is based on probability distribution, we may also consider to adapt our method to different information theoretic measures as Smith’s [31] definition.
Acknowledgement
We thank Geoffrey Smith, Ton Coolen, and the anonymous reviewers for useful com- ments. This research was partially supported by the EPSRC grant EP/C009967/1 Quantitative Information Flow.

References
Alessandro Aldini, Alessandra Di Pierro, and Ra Di Pierro. A quantitative approach to noninterference for probabilistic systems, 2004.
Michael Backes. Quantifying probabilistic information flow in computational reactive systems. In Proceedings of 10th European Symposium on Research in Computer Security (ESORICS), volume 3679 of Lecture Notes in Computer Science, pages 336–354. Springer, September 2005.
Michele Boreale. Quantifying information leakage in process calculi. In ICALP (2), pages 119–131, 2006.
David Clark, Sebastian Hunt, and Pasquale Malacaria. Quantitative analysis of the leakage of confidential data. In Electronic Notes in Theoretical Computer Science, volume 59, Elsevier, 2002.

David Clark, Sebastian Hunt, and Pasquale Malacaria. Quantitative information flow, relations and polymorphic types. J. Log. and Comput., 15(2):181–199, 2005.
David Clark, Sebastian Hunt, and Pasquale Malacaria. A static analysis for quantifying information flow in a simple imperative language. Journal of Computer Security, 15:321–371, 2007.
Michael R. Clarkson, Andrew C. Myers, and Fred B. Schneider. Quantifying information flow with beliefs. Journal of Computer Security, 2007.
Patrick Cousot and Rahida Cousot. Abstract interpretation and application to logic programs. J. Log. Program., 13(2-3):103–179, 1992.
Dorothy Elizabeth Robling Denning. A lattice model of secure informatin flow. Communications of the ACM, 19(5):236–243, May 1976.
Dorothy Elizabeth Robling Denning. Cryptography and Data Security. Addison-Wesley, 1982.
Alessandra Di Pierro, Chris Hankin, and Herbert Wiklicky. Approximate non-interference. In CSFW, pages 3–17, 2002.
Alessandra Di Pierro, Chris Hankin, and Herbert Wiklicky. Approximate non-interference. Journal of Computer Security, 12(1):37–82, 2004.
Alessandra Di Pierro, Chris Hankin, and Herbert Wiklicky. Quantitative static analysis of distributed systems. J. Funct. Program., 15(5):703–749, 2005.
Joseph Goguen and Jose Meseguer. Security policies and security models. In IEEE Symposium on Security and privacy, pages 11–20. IEEE Computer Society Press, 1982.
James W. III Gray. Toward a mathematical foundation for informatin flow security. In IEEE Security and Privacy, pages 21–35, Oakland, California, 1991.
Boris K¨opf and David Basin. An information-theoretic model for adaptive side-channel attacks. In CCS ’07: Proceedings of the 14th ACM Conference on Computer and Communications Security, pages 286–296, New York, NY, USA, November 2007. ACM SIGSAC, ACM Press.
Dexter Kozen. Semantics of probabilistic programs. In 20th Annual Symposium on Foundations of Computer Science, pages 101–114, Long Beach, California, USA, October 1979.
Dexter Kozen. Semantics of probabilistic programs. Journal of Computer and System Sciences, 22(3):328–350, 1981.
Gavin Lowe. Defining information flow quantity. Journal of Computer Security, 12(3-4):619–653, 2004.
Pasquale Malacaria. Assessing security threats of looping constructs. In Proceedings of the 34th annual ACM SIGPLAN-SIGACT symposium on Principles of programming languages, pages 225–235, Nice, France, 2007. ACM Press.
Stephen McCamant and Michael D. Ernst. Quantitative information flow as network flow capacity. In Proceedings of the ACM SIGPLAN 2008 Conference on Programming Language Design and Implementation, Tucson, AZ, USA, June 9–11, 2008.
Annabelle McIver and Carroll Morgan. A probabilistic approach to information hiding. In Programming methodology, pages 441–460, New York, NY, USA, 2003. Springer-Verlag New York.
John McLean. Security models and information flow. In Proceeding of the 1990 IEEE Symposium on Security and Privacy, Oakland, California, May 1990.
Jonathan Millen. Covert channel capacity. In Proceeding 1987 IEEE Symposium on Resarch in Security and Privacy. IEEE Computer Society Press, 1987.
David Monniaux. Abstract interpretation of probabilistic semantics. In SAS’00: Proceedings of the 7th International Symposium on Static Analysis, pages 322–339, London, UK, 2000. Springer-Verlag.
Chunyan Mu and David Clark. Quantitative analysis of secure information flow via probabilistic semantics. In ARES’09: The International Dependability Conference on Availability, Reliability and Security, volume 0, pages 49–57, Los Alamitos, CA, USA, 2009. IEEE Computer Society.
Alfred Renyi. On measures of entropy and information. In Proceedings of the 4th Berkeley Symposium on Mathematics, Statistics and Probability, pages 547–561, 1961.
Alfred Renyi. Probability theory. North-Holland Publishing Company, Amsterdam, 1970.
Vladimir Abramovich Rokhlin. Lectures on the entropy theory of measure-preserving transformations.
Russian Mathematical Surveys, 22(5):1–52, 1965.

Walter Rudin. Real and Complex Analysis. McGraw-Hill, 1966.
Geoffrey Smith. On the foundations of quantitative information flow. In FOSSACS, pages 288–302, 2009.
Douglas G. Weber. Quantitative hook-up security for covert channel analysis. In CSFW 1988, Franconia, NH, 1988. IEEE.
J. Todd Wittbold and Dale M. Johnson. Information flow in nondeterministic systems. In IEEE Symposium on Security and Privacy, pages 144–161, 1990.
