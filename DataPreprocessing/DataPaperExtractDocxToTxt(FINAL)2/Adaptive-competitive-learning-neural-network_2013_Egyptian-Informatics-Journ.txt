
ORIGINAL ARTICLE

Adaptive competitive learning neural networks
Ahmed R. Abas *

Department of Computer Science, College of Computer in Lith, Umm Al-Qura University, Makka Al-Mukarrama, Saudi Arabia Department of Computer Science, Faculty of Computers and Informatics, Zagazig University, Zagazig, Egypt

Received 30 April 2013; accepted 26 August 2013
Available online 14 September 2013

Abstract In this paper, the adaptive competitive learning (ACL) neural network algorithm is pro- posed. This neural network not only groups similar input feature vectors together but also determines the appropriate number of groups of these vectors. This algorithm uses a new proposed criterion referred to as the ACL criterion. This criterion evaluates different clustering structures produced by the ACL neural network for an input data set. Then, it selects the best clustering structure and the cor- responding network architecture for this data set. The selected structure is composed of the minimum number of clusters that are compact and balanced in their sizes. The selected network architecture is efficient, in terms of its complexity, as it contains the minimum number of neurons. Synaptic weight vectors of these neurons represent well-separated, compact and balanced clusters in the input data set. The performance of the ACL algorithm is evaluated and compared with the performance of a recently proposed algorithm in the literature in clustering an input data set and determining its number of clus- ters. Results show that the ACL algorithm is more accurate and robust in both determining the number of clusters and allocating input feature vectors into these clusters than the other algorithm especially with data sets that are sparsely distributed.
© 2013 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University.



Introduction

Cluster analysis is an important task in pattern recognition. It is interested in grouping similar feature vectors in an input

* Address: Department of Computer Science, Faculty of Computers and Informatics, Zagazig University, Zagazig, Egypt. Tel.: +966 580016325, +20 1009013037.
E-mail addresses: arabas@zu.edu.eg, armohamed@uqu.edu.sa.
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
data set into a number of clusters. Feature vectors in one clus- ter are similar to each other more than other feature vectors in the other clusters. Different clustering algorithms are proposed in the literature such as the competitive neural networks (CNN) and the finite mixture models (FMM) [1–5]. The neu- rons of the competitive networks learn to recognize groups of similar input feature vectors. The FMM produces a cer- tainty estimate of the membership of each feature vector to each one of the clusters in the input data set. However, these algorithms have some limitations. First, they produce sub- optimal results because they converge to the nearest local op- tima of the objective function to the starting point [6]. Second, they produce biased values for cluster centers when clusters are poorly separated or when cluster sizes are not balanced, i.e.,


1110-8665 © 2013 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. http://dx.doi.org/10.1016/j.eij.2013.08.001

data are sparsely distributed [7,8]. These algorithms require the number of clusters to be given [5].
Determining the optimal number of clusters for an input data set is a difficult problem in cluster analysis [5,9]. Several criteria are proposed in the literature for determining the num- ber of clusters with FMM [see for review, [10–12]]. However, these criteria tend to overestimate the number of clusters when cluster shapes are not Gaussian [8] and underestimate the number of clusters when clusters are overlapping or when the size of the given data set is small [13]. Also, these criteria have poor performance with sparsely distributed data [10,11]. On the other hand, there is no criterion proposed in the lit- erature for automatic determination of the number of neurons, which represent clusters, in the CNN to the best of our knowl- edge. Alternatively, the number of neurons in the CNN is determined manually by removing the neurons that have not win the competition in representing the input feature vectors after convergence [14]. These neurons are called the dead neu- rons [5]. However, this method is sensitive to the selection of the learning rate [5]. Another method determines the number of neurons or clusters by finding the difference between the synaptic weight vectors of the CNN, and if the difference be- tween the weight vectors is greater than a specified value, then the number of clusters is increased by one, otherwise the clus-
ters are merged [15].
In this paper, a new algorithm that is referred to as the adaptive competitive learning neural network (ACL) algorithm is proposed to integrate the unsupervised learning and the optimization of the CNN. It learns and evaluates different CNN architectures for clustering an input data set using a new proposed criterion that is referred to as the ACL criterion. The CNN architecture corresponds to the minimum value of the ACL criterion is considered the most efficient CNN archi- tecture, in terms of its complexity. This CNN is composed of the minimum number of neurons that represent compact and balanced clusters in the input data set. These clusters have the minimum within-component variation among them. The rest of this paper is organized as follows: Section 2 presents the architecture of the CNN. Section 3 presents the proposed criterion and the proposed ACL algorithm. Section 4 presents an evaluation and a comparison study of the ACL algorithm and the EMCE algorithm [12]. This study shows the perfor- mance of each algorithm in determining the optimal number of clusters and allocating input feature vectors into these clus- ters using data sets of different properties. Section 5 presents discussion of the results obtained. Finally, Section 6 presents the conclusions and the future work.

The architecture of the CNN

The CNN is a single-layer neural network in which each output neuron is fully connected to the input nodes. The output neu- rons compete to become active when an input feature vector is presented to the network [1,2,5,6,16]. The activation of the neuron with the largest net input is set to 1 and the activation of all other neurons is set to 0. This condition is known as ‘‘winner takes all.’’ If a neuron wins the competition, its weight vector is updated according to the competitive learning rule shown in the equation
wj(n + 1) = wj(n)+ n(n)(pi — wj)	(1)
where n is the learning rate and wj is the weight vector of the winning neuron j for the input feature vector pi. The network initializes the weight vectors of its neurons randomly. The neu- rons learn by moving their weights toward the input feature vectors when presented to the network. Feature vectors in the data set are presented randomly to the network a number of times called epochs. After training, each output neuron should represent a cluster in the input data set by moving its own synaptic weight vector to the center of that cluster. In other words, the neurons in a competitive layer distribute themselves in the feature space to recognize frequently pre- sented input feature vectors to the network. Therefore, the CNN performs clustering for the input data set. However, the performance of the CNN is dependent on the number of the output neurons and the initialization of their weight vec- tors [6]. Different initial weight vectors may lead to different final clusters because the update rule in Eq. (1) only moves the weight vector of the winning neuron toward its nearest fea- ture vectors. Another drawback of the CNN is that it may split one cluster into many small clusters [6]. The architecture for the CNN is shown in Fig. 1.
The input vector p and the input weight matrix W are fed to block ||dist||. The number of features of the input vector is R, while the number of neurons of the CNN is S. The output of block ||dist|| is a vector having S elements. The elements are the negative of the Euclidean distances between the input vec- tor p and vectors formed from the rows of the input weight matrix W. The net input of a competitive layer n is composed by finding the negative distance between input vector p and the weight vectors and adding the biases b. When all biases are zero, the maximum net input for any neuron is 0. This occurs when the input vector p equals that neuron’s weight vector. The competitive transfer function block CTF has the net input vector as input for the layer and produces neuron outputs of 0 for all neurons except for the winner whose output is 1. The winner neuron is the one with the most positive element of the net input vector. If all the biases are zero, then the neuron whose weight vector is the nearest to the input vector, i.e. the neuron with the least negative net input, wins the competition and its output becomes 1.
During learning of the weight vectors in the CNN, a prob- lem referred to as the dead neurons occurs when some neurons may not get allocated or win the competition for almost all fea- ture vectors in the input data set. This happens when the weight vectors of some neurons start out far from any input feature vectors, and therefore, these neurons never win the


Input	Competitive Layer



Figure 1  The architecture of the Competitive neural network.











1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
EMCE

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ACL

0	0	0.2	0.4	0.6	0.8	1
Feature 1
0
0	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 1

(a)	(b)



2.5


2


1.5


1


0.5
EMCE

1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
ACL

0	0.21	2	3	4	5	6	7	8	9	10
K	k
(c)	(d)
Figure 2	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the First data set.



competition, and their weight vectors do not get to learn. These neurons do not perform a useful function in the CNN. In order to prevent the occurrence of the dead neurons, biases are used to give neurons that rarely win the competition an advantage over neurons that win often. A positive bias added to the negative distance makes a distant neuron more likely to win. To do this job, the percentage of times the output of each neuron is 1 is stored. This percentage is used to update the biases, so that the biases of frequently active neurons will be smaller, and the biases of infrequently active neurons will be larger. The learning rate for updating the biases is called the conscience learning rate, and its value is set smaller than the competitive learning rate n. In the experiments shown in this paper, the competitive learning rate is set to 0.01, and the con- science learning rate is set to 0.001, respectively. Integrating biases learning with the competitive learning for the neurons of the CNN is called learning with conscience [17]. This learn- ing process allows the infrequently active neuron to respond and move toward more input feature vectors. Eventually, the neuron will respond to an equal number of feature vectors as other neurons. As a result of learning with conscience, the dead
neurons problem is resolved, and each neuron in the CNN is forced to represent roughly the same number of input feature vectors [18].

The proposed adaptive competitive learning neural network

The adaptive competitive learning (ACL) neural network algo- rithm uses a new proposed ACL criterion to determine the optimal number of output neurons in the CNN for clustering an input data set. This criterion is based on the theory that the best CNN architecture for clustering an input data set should produce a cluster structure that is composed of dense, well-separated and balanced clusters that have the minimum number of parameters to be estimated. To realize this theory, the ACL criterion selects the CNN that represents minimum within-cluster variations and minimum value for the product of the relative weights of clusters in the given data set. To introduce the notation, let D = {p1, p2, .. ., pn} be a given data set that consists of n feature vectors that are independently and identically distributed in R-feature space. The values on each feature are scaled such that they range from 0 to 1. This re-




1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
EMCE

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ACL

0
0	0.2	0.4	0.6	0.8	1
Feature 1
(a)
00	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 1
(b)




2.5
EMCE

1.5
ACL



2

1
1.5


1


0.5

0.5



0
20  18  16

14  12  10	8	6
K

4	2	0
01	2	3	4	5	6	7	8	9	10
k

(c)	(d)

Figure 3	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Second data set.

duces the sparsity of the input data and increases the accuracy of estimating its cluster structure [10]. The cluster structure is

1	k
ACL(k) = E(k)+ 2k

log10
  1 




(4)

revealed using the CNN that represents the input data set. Each neuron in output competitive layer is assumed to repre- sent a cluster in the input data set. Then, using a CNN that contains k neurons in its output layer, the average within-clus- ter variations in this data set are defined as:
max j=1	j

where kmax is the maximum number of neurons in the output layer in the CNN. The optimal architecture for the CNN that is composed of the optimal number of output neurons for best clustering a given data set corresponds to the minimum value

n  k	of the ACL criterion. Minimizing the first term in the ACL cri-

E(k) = 1 XXr p — w 2	(2)



terion produces the minimum within-cluster variation, which




where pi e D, wj is the weight vector of neuron j and rij is set to 1 if neuron j wins the competition when input vector pi is pre- sented to the network, otherwise it is set to 0. The relative weight of a certain cluster in the data set represented by a neu- ron j in the CNN is defined as:
1 Xn


tion, minimizing the second term produces the minimum prod-
uct of cluster relative weights that corresponds to the minimum number of well-separated clusters that have roughly equal weights. This in turn results in the most efficient CNN in terms of its complexity. Therefore, minimizing the ACL criterion produces the most efficient CNN that represents compact, well-separated and balanced clusters in the given data set.



Then, the proposed ACL criterion for evaluating different
architectures of the CNN that are composed of different num- bers of neurons is defined as:
maximum value. As k decreases, the first term increases be- cause the sizes of the clusters increase, while the second term decreases because relative weights of the clusters become




1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
EMCE

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ACL

0	0	0.2	0.4	0.6	0.8	1
Feature 3
0 0	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 3

(a)	(b)



2.5
EMCE

1.4
ACL




2


1.5


1


0.5


0
20
















18  16
















14  12	10	8	6	4	2	0
K
1.2

1

0.8

0.6

0.4

0.2

01	2	3	4	5	6	7	8	9	10
k

(c)	(d)
Figure 4	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Iris data set.



larger. The minimum ACL criterion value assures the best compromise between compactness of the clusters and their number, i.e., complexity of the CNN.
The ACL algorithm uses learning with conscience [17] in or- der to resolve the dead neurons problem during competitive learning of the weight vectors of the output neurons. In addi- tion, this learning process assures that these neurons represent balanced clusters, i.e., clusters of roughly equal number of in- put feature vectors. The algorithm starts with a CNN of large number of neurons kmax that is set to 10 in the experiments shown in this paper. Input feature vectors are presented ran- domly to the network for a number of epochs. The number of epochs in experiments shown in this paper is set to 10. Learning with conscience [17] is used to learn weight vectors of the output neurons. After convergence, the ACL criterion value of the current CNN architecture is computed. The num- ber of neurons is decremented, and weight vectors of the new CNN architecture are learned. This process continues until there is one neuron in the CNN architecture. Finally, the CNN architecture that has the minimum ACL criterion value is considered the optimal CNN for clustering the input data set, and its number of neurons is considered the optimal num- ber of clusters in this data set. This CNN has the minimum number of neurons that represent well-separated and balanced clusters. These clusters have the minimum within-cluster vari- ation. This CNN is efficient because it has a small number of neurons, i.e., small complexity and it represents compact,
of clusters in it. The EMCE algorithm proves superiority to other algorithms in the literature in clustering an input data set and determining the number of clusters in it [12]. It uses mixture models and a criterion that is based on likelihood and mutual information theory for evaluating different mixture models with different numbers of clusters. The com- pared algorithms are implemented, and experiments are carried out using the MATLAB software. Different data sets with different cluster structures are used. These data sets are described in Section 4.1. The measure used to quantify how good the clustering results obtained from each algorithm is described in Section 4.2.

Data sets

Data sets used in the experiments shown in this paper have dif- ferent cluster structures in terms of number of clusters, cluster separation and data sparsity in the feature space. These data sets are described as follows:

The first data set
This data set is artificially generated such that it consists of 150 feature vectors each of which is a vector in 10-feature space. These feature vectors are generated from five separated Gaussian-shape clusters with equal probabilities. The centers of these clusters are l1 = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]T,  l2 = [6, 2, 2, 2, 6, 6, 2, 2, 6, 6]T,
l = [2, 6, 6, 6, 2, 2, 6, 6, 2, 2]T, l = [4, 4, 4, 4, 4, 4, 4, 4, 4, 4]T and

balanced and well-separated clusters. Finally, the steps of the	3	T  4

proposed ACL algorithm are shown as follows:
l5 = [6, 6, 6, 6, 6, 6, 6, 6, 6, 6] , while their covariance matrices are
identical and equal to R = 0.5I10. The purpose of using this data set is to test the algorithms compared when data clusters are separated and when the number of features is large compared to the number of feature vectors, i.e., the data set is sparsely distributed.
The second data set
This data set is artificially generated such that it consists of 90 fea- ture vectors each of which is a vector in 10-feature space. These feature vectors are generated from three poorly separated Gauss- ian-shape clusters with equal probabilities. The centers of these

clusters are l1
= [0, 0, 0, 0, 0, 0, 0, 0, 0, 0]T, l2
=[- 2,- 2, - 2,

- 2, - 2, - 2, - 2, - 2,- 2, - 2]T and l3 = [2, 2, 2, 2, 2, 2, 2, 2, 2, 2]T,
while their covariance matrices are identical and equal to R = I10. The purpose of using this dataset is to test the algorithms compared when data clusters are poorly separated and when the number of features is large compared to the number of feature vectors i.e., the data set is sparsely distributed. Both of the first and the second data sets are used in the literature in comparing different clustering algorithms [12].










Experimental results and evaluation

The performance of the ACL algorithm is compared to the performance of the recently proposed EMCE algorithm [12] in clustering an input data set and determining the number
The Iris data set
It consists of 150 feature vectors each of which is a vector in four-feature space. These feature vectors represent three clus- ters of equal sizes. Two clusters are overlapped in the data space. The purpose of using this data set is to test the algo- rithms compared when data clusters are poorly separated and when the number of features is small.

The Seeds data set
It consists of 210 feature vectors each of which is a vector in seven-feature space. These feature vectors represent three clus- ters of equal sizes. These clusters are partially overlapped in



the data space. The purpose of using this data set is to test the algorithms compared when data clusters are poorly separated and when the number of features is moderate.

The Wine data set
It consists of 178 feature vectors each of which is a vector in 13-feature space. These feature vectors represent three clusters whose sizes are 59, 71 and 48 feature vectors. The clusters are separated in the data space. The purpose of using this data set is to test the algorithms compared when data clusters are sep- arated and when the number of features is large compared to the number of feature vectors.

The Breast Cancer data set
It consists of 683 feature vectors each of which is a vector in 9- feature space. These feature vectors represent two unbalanced clusters whose sizes are 444 and 239 feature vectors, respec- tively. To be composed of balanced clusters, the size of this data set is reduced to 478 feature vectors that represent two clusters of equal sizes. The clusters are overlapping in the data space. The purpose of using this data set is to test the algo- rithms compared when data clusters are overlapping and when
the number of features is moderate compared to the number of feature vectors.

The Pima Indians Diabetes data set
It contains 768 feature vectors each of which is a vector in eight-feature space. These feature vectors represent two unbal- anced clusters whose sizes are 500 and 268 feature vectors, respectively. To be composed of balanced clusters, the size of this data set is reduced to 536 feature vectors that represent two clusters of equal sizes. These clusters are largely overlap- ping. Correlations between different pairs of features of this data set are too weak. The purpose of using this data set is to test the algorithms compared when data clusters are largely overlapping and when the number of features is moderate compared to the number of feature vectors.

The Glass Identification data set
It contains 214 feature vectors each of which is a vector in nine-feature space. These feature vectors represent two unbal- anced clusters whose sizes are 163 and 51 feature vectors, respectively. To be composed of balanced clusters, the size of this data set is reduced to 102 feature vectors that represent




1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
EMCE

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ACL

0	0	0.2	0.4	0.6	0.8	1
Feature 1
00	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 1

(a)	(b)



2.5


2


1.5


1


0.5

EMCE


1.4

1.2

1

0.8

0.6

0.4

0.2

ACL



00	2	4	6	8	10  12  14  16  18  20
k
(c)
01	2	3	4	5	6	7	8	9	10
k
(d)


Figure 5	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Seeds data set.




1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
EMCE

1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
ACL

0	0	0.2	0.4	0.6	0.8	1
Feature 1
00	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 1

(a)	(b)


6	EMCE

5

4

3

2

1

1.6

1.4

1.2

1

0.8

0.6

0.4
ACL



0
20	8	6	4	2	0
K
(c)
0.21	2	3	4	5	6	7	8	9	10
k
(d)


Figure 6	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Wine data set.


two clusters of equal sizes. These clusters are separated in the feature space. The purpose of using this data set is to test the algorithms compared when data clusters are separated and when the number of features is large compared to the number of feature vectors i.e., the data are sparsely distributed. All these real data sets are commonly used in classification analysis [19].
probability of class i and Pj is the probability of cluster j. Since this measure is not bounded by the same constant for all data sets, a normalized version that ranges from 0 to 1 is proposed for easier interpretation and comparison [21]. This normalized version is called the Normalized Mutual Information (NMI) and is computed as follows:
I(X; Y)


The evaluation criterion for clustering results
NMI(X; Y) = 
pHﬃﬃﬃﬃ(ﬃﬃXﬃﬃﬃﬃ)ﬃﬃHﬃﬃﬃﬃ(ﬃYﬃﬃﬃﬃ)ﬃ
(5)


The Mutual Information is a symmetric measure to quantify the statistical information shared between two distributions [20]. Therefore, this measure is used to quantify how good the clustering results obtained for a certain data set is by com- paring it to the true classification of this data set [21]. Let X and Y be two random variables represent the true class labels [1..m] for a certain data set and the cluster labels [1..k] resulting from an algorithm for the same data set, respectively. The mu- tual  information  between  X  and  Y  is  defined  as
I(X; Y) = Pm Pk P log (P /P P ), where P is the probabil-
where H(X) and H(Y) denote the entropy of X and Y. The
NMI has the value of 1 when there is a one to one mapping between the clusters obtained and the true classes (i.e., k = m) of a given data set. Since this measure is not biased to- ward large k, it is preferred to compare different data parti- tions [21,22].

Discussion of results

Table 1 shows the performances of the algorithms compared with each one of the data sets used. The performance of each





1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

EMCE















0	0.2	0.4	0.6	0.8	1
Feature 6


1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1
0

ACL


0	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 6

(a)	(b)



2.5


2


1.5


1


0.5
EMCE

1.3
1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
ACL

00	2	4	6	8	10	12	14	16	18	20
k
(c)
0.4
1	2	3	4	5	6	7	8	9	10
k
(d)


Figure 7	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Breast Cancer data set.


the number of clusters corresponding to the optimal value of the criterion used by the algorithm and the average ± the stan- dard deviation resulting from 100 experiments. Each experi- ment has different random initialization values of the neuron weight vectors in the ACL algorithm and the mixture compo- nent parameters in the EMCE algorithm. This repetition of the experiments removes the effect of initialization values on the results of the algorithms [11,12]. The shaded cells in this table represent the maximum values of the NMI among all algo- rithms and the correct number of clusters with each data set. Figs. 2–9(a and b) show examples of the FMMs obtained from the EMCE algorithm, represented by blue ellipses, and neuron weight vectors (cluster centers) obtained from the ACL algo- rithm, represented by red circles, with each one of the eight data sets used. The ellipses in these figures are isodensity curves of each component in the FMM. These figures (c and d) also show the distribution of the EMCE and the ACL criteria against the number of clusters k through the runtime of the corresponding algorithms.
Table 1 shows that the ACL algorithm results in the correct optimal number of clusters in all data sets used, while the
EMCE algorithm fails in producing the correct optimal num- ber of clusters with the Breast Cancer, the Pima Indians Dia- betes and the Glass Identification data sets. In addition, the ACL algorithm produces robust results as its average number of clusters over 100 experiments is approximately equal to the correct number of clusters, while the standard deviation is too small with all data sets used. On the other hand, the EMCE algorithm does not produce robust results as its average num- ber of clusters over 100 experiments is far from the correct number of clusters, and its standard deviation is large with all data sets used except the Iris and the Seeds data sets.
Regarding the clustering results, the ACL algorithm produces the highest and the most robust results of the NMI measure with all data sets except the Iris and the Seeds data sets. With the Iris and the Seeds data sets, the EMCE algo- rithm produces the highest and the most robust results for the NMI measure as these data sets have overlapping among their clusters. In general, results of the ACL algorithm are accurate and robust in determining the number of clusters in all data sets used. With respect to clustering results, the ACL algorithm is accurate and robust with data sets that do not



1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

EMCE


1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

ACL

0
0	0.2	0.4	0.6	0.8	1
Feature 1
00	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 1

(a)	(b)



2.5


2


1.5


1


0.5
EMCE

1.2
1.1
1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
ACL

00	2	4	6	8	10  12  14	16  18	20
k
0.21	2	3	4	5	6	7	8	9	10
k

(c)	(d)
Figure 8	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Pima Indians Diabetes data set.



have overlapping among their clusters and are sparsely distrib- uted. These results show that the ACL algorithm is less sensi- tive to the curse of dimensionality than the EMCE algorithm. This is because the ACL algorithm depends on the Euclidean distance in the feature space, while the EMCE algorithm de- pends on the probability density function in clustering an input feature vector.
Figs. 2–9 show that the ACL algorithm produces correct and robust clustering results either the number of clusters or the allocation of input feature vectors to these clusters with all data sets used. In addition, drawing the ACL criterion against k shows that it is convex and does not have overshoot- ing or flat areas as the EMCE criterion does. This shows that the ACL criterion is not only accurate but also robust espe- cially with sparsely distributed data sets.

Conclusions and future work

In this paper, the competitive neural network (CNN) is devel- oped in order to be able to cluster an input data set and deter- mine its number of clusters. The resulting algorithm is referred
to as the adaptive competitive learning (ACL) neural network. It is based on a new proposed criterion for evaluating the clus- tering structure produced by the CNN, called the ACL crite- rion. This criterion evaluates the clustering structure by compromising the compactness of the clusters produced by the relative weights of these clusters. It prefers the cluster struc- ture that is composed of the minimum number of clusters that are compact, well-separated and balanced in their sizes. The ACL algorithm is compared with the recently proposed EMCE algorithm that proves superiority to other algorithm in the lit- erature in clustering an input data set and determining its num- ber of clusters. Different data sets with different cluster structures in terms of number of clusters, cluster separation and data sparsity in the data space are used. Results show that the ACL algorithm produces more accurate and robust results than the EMCE algorithm especially with data sets that are sparsely distributed. The ACL algorithm does not impose a certain cluster shape on the data set, while the EMCE imposes the Gaussian shape on data clusters. In addition, it overcomes the problem of the EMCE algorithm when the input data are sparsely distributed. This is because the ACL algorithm uses



1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

EMCE


1
0.9
0.8
0.7
0.6
0.5
0.4
0.3
0.2
0.1

ACL

0	0	0.2	0.4	0.6	0.8	1
Feature 3
0
0	0.1  0.2  0.3  0.4  0.5  0.6  0.7  0.8  0.9	1
Feature 3

(a)	(b)



2.5
EMCE

1.6
ACL




2


1.5


1


0.5


00	2	4	6	8	10	12	14	16	18	20
k
(c)
1.4

1.2

1

0.8

0.6

0.4

0.2 1	2	3	4	5	6	7	8	9	10
k
(d)


Figure 9	The clusters obtained from the EMCE and the ACL algorithms and the distribution of the criteria used in these algorithms against the number of clusters k with the Glass Identification data set.


the Euclidean distance in the feature space rather than the probability density function as the EMCE algorithm does in allocating each input feature vector to a certain cluster. Both of the algorithms compared are similar in producing a single frame for determining the number of clusters and allocating input feature vectors in a certain data set to these clusters.
In the future, the ACL criterion may be tested for other competitive learning algorithms such as the fuzzy competitive learning neural networks [23], the fuzzy c-means algorithm
[24] and the k-means algorithm [25].

References

Rumelhart DE, Zipser D. Feature discovery by competitive learning. Parallel distributed processing. MIT Press; 1986, p. 151– 193.
Kohonen T. Self-organization and associative memory. 2nd ed. Berlin: Springer-Verlag; 1987.
Duda RO, Hart PE, Stork DG. Pattern classification. 2nd ed. USA: John Wiley & Sons; 2001.
Webb A. Statistical pattern recognition. 2nd ed. UK: John Wiley & Sons; 2002.
Budura G, Botoca C, Miclau N. Competitive learning algorithms for data clustering. Electron Energetics J 2006;19(2):261–9.
Lei JZ, Ghorbani AA. Network intrusion detection using an improved competitive learning neural network. In: Proceedings of the second annual conference on communication networks and services research; 2004. p. 190–197.
Chinrungrueng C, Sequin CH. Optimal adaptive K-means algo- rithm with dynamic adjustment of learning rate. IEEE Trans Neural Networks 1995;6(1):157–69.
Biernacki C, Govaert G. Using the classification likelihood to choose the number of clusters. J Comput Sci Stat 1997; 29(2):451–7.
Guo P, Chen CLP, Lyu MR. Cluster number selection for a small set of samples using the Bayesian Ying-Yang model. J IEEE Trans Neural Networks 2002;13(3):757–63.
Rafat A. An adaptive approach for clustering incomplete data sets: an application to health inequality analysis. Germany: Lambert Academic Publishing; 2010.
Abas AR. An algorithm for unsupervised learning and optimiza- tion of finite mixture models. Egypt Inform J 2011;12(1):19–27.



Abas AR. On determining efficient finite mixture models with compact and essential components for clustering data. Egypt Inform J 2013;14(1):79–88.
Yang ZR, Zwolinski M. Mutual information theory for adaptive mixture models. J IEEE Trans Pattern Anal Mach Intell 2001;23(4):396–403.
Xu L, Krzyzak A, Oja AE. Rival penalized competitive learning for clustering analysis. rbf net and curve detection. IEEE Trans Neural Networks 1993;4:636–64.
Sowmya B, Rani BS. Color image segmentation using fuzzy clustering techniques and competitive neural network. J Appl Soft Comput 2011;11:3170–8.
Behnke S, Karayiannis NB. Competitive neural trees for pattern classification. IEEE Trans Neural Networks 1998;9(6):1352–69.
Desieno D. Adding a conscience to competitive learning. In: Proceedings of the second IEEE international conference on, neural networks (ICNN-88), vol. 1; 1988. p. 117–24
Demuth H, Beale M. Self-organizing and learning vector quantization nets. In: Neural network toolbox for use with MATLAB  User’s  Guide,  The  MathWorks  Inc.,  2002.
<www.mathworks.com>.
UCI Repository of machine learning databases, Irvine, CA: University of California, Department of Information and Com- puter Science, March 2013. <http://archive.ics.uci.edu/ml/>.
Cover TM, Thomas JA. Elements of information theory. Wiley; 1991.
Strehl A, Ghosh J. Cluster ensembles – a knowledge reuse framework for combining multiple partitions. J Mach Learn Res 2002;3:583–617.
Fern XZ, Brodley CE. Random projection for high dimensional data clustering: cluster ensemble approach. In: Proceeding of the 20th international conference on machine learning (ICML 2003); 2003. p. 186–93.
Madiafi M, Bouroumi A. A new fuzzy learning scheme for competitive neural networks. J Appl Math Sci 2012;6(63):3133–44.
Zanaty EA. Determining the number of clusters for kernelized fuzzy C-means algorithms for automatic medical image segmen- tation. Egypt Inform J 2012;13:39–58.
Ossama O, Mokhtar HMO, El-Sharkawi ME. An extended k- means technique for clustering moving objects. Egypt Inform J 2011;12:45–51.
