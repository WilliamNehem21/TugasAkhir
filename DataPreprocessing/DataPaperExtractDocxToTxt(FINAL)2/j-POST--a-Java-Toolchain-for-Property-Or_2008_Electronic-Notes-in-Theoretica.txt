

Electronic Notes in Theoretical Computer Science 220 (2008) 29–41
www.elsevier.com/locate/entcs

j-POST: a Java Toolchain for Property-Oriented Software Testing
Yli`es Falcone1 Laurent Mounier1 Jean-Claude Fernandez1
Verimag
Universit´e Grenoble I, INPG, CNRS Grenoble, France
Jean-Luc Richier1
LIG Laboratory Universit´e Grenoble I, INPG, CNRS
Grenoble, France

Abstract
j-POST is an integrated toolchain for property-oriented software testing. This toolchain includes a test designer, a test generator, and a test execution engine. The test generation is based on an original approach which consists of deriving a set of communicating test processes obtained both from a requirement formula (expressed in a trace-based logic) and a behavioral specification of some specific parts of the software under test. The test execution engine is then able to coordinate the execution of these test processes against a distributed Java program. j-POST was applied to check the correct deployment of a security policy for a travel management application.
Keywords: property testing, tool, composition, Java


Introduction
Testing is a validation technique aimed to find defective behaviours on a system either during its development, or once a final version is issued. It remains one of the most feasible methodologies to ensure the expected behaviour of a software. This is notably due to its ability to cope with continual growth of system complex- ity. However, reducing its cost and time consumption remains a very important challenge sustained by a strong industrial demand.
In previous work [5,6] we have presented a black-box test generation method able to construct abstract test cases from a formal requirement (a property that

1 Email: FirstName.LastName@imag.fr

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.11.004

the system is expected to fulfill). This method (implemented in a prototype tool) is based on a test calculus allowing the method to be compositionally and formally defined. In this framework, a requirement is expressed by a logical formula built upon a set of (abstract) predicates. Each predicate corresponds to a (possibly non- atomic) operation to be performed on the system under test, and is user-provided as a test module indicating how to perform this operation on the actual implementa- tion, and how to decide whether its execution succeeds or not. The test generation step consists in building, by composition of test modules, a set of communicating test processes from this property. In this paper we present a significant step from this previous work. First off, we present formally how the previously generated test can be executed. Besides we present j-POST, an integrated toolchain for property- oriented software testing. In addition to a full implementation of the test generation tool, we present the associated test designer and test execution engine resulting in a fully integrated toolchain. The test designer helps the user to provide inputs to the test generator. The test execution engine is able to coordinate the execu- tion of the generated processes against a possibly distributed program, leading to a satisfiability verdict with respect to the given requirement.

Comparison with classical model-based testing.
This approach offers several advantages over more classical model-based test generation techniques [15] implemented in several existing tools (e.g. TGV [9], TorX [16], see [2,7] for more exhaustive surveys). First, j-POST is able to deal with piecewise speciﬁcations restricted to specific functionalities. We strongly believe that this feature is really important in practice, especially in application domains where formal modeling of software is not a common practice. Specifying only some global requirement and some specific implementation features in an operational way (i.e. the test modules) seems much easier for test engineers than building a complete model of a software. As a consequence, the test generation step will not require the exploration of such a complete model, avoiding the well-known state explosion problem. Furthermore, this toolchain remains open in the sense that various logics can be considered to express the requirements, and new logic plugins can be easily added. Finally, this toolchain integrates a large spectrum of the whole test process, from the test design to the test execution.


The remainder of this paper is organized as follows. Sect. 2 describes the un- derlying theory of j-POST and Sect. 3 describes the toolchain itself. In Sect. 4, we depict one of the experiments conducted with j-POST on a travel agency applica- tion. Sect. 5 exposes some conclusions and perspectives opened by this work.

Underlying testing theory
This section briefly presents the background of j-POST, namely how to produce and execute test cases from a formal requirement following a syntax-driven approach.

More details can be found in the research reports available in [8].
We consider in the following that the behaviour of the software under test (SUT) can be modelled using a labelled transition system (LTS), noted Sut, namely a quadruplet (QSut, ActSut, →, q0) where Q is a set of states, ActSut a set of actions (labels), →⊆ QSut × ActSut × QSut the transition relation and q0 ∈ QSut the initial state. In black-box testing this behaviour can be accessed only through a SUT interface, namely a set of visible actions Actvis ⊆ ActSut. Non visible actions are supposed to be labelled by τ . We will denote by p −a→ q when (p, a, q) ∈→, and

by p
=τ ∗a
when there exist p , p , ..., p
s.t. p
−τ→ p	and p
−a→ q. Finally,

1  ⇒ q
2	3	n
i	i+1	n

we define the execution sequences of Sut as the set of ﬁnite sequences of visible actions that can be performed from its initial state: Exec(Sut) = {a1.a2. ··· .an |

∃q ,...,q 
s.t. q
τ∗ai
∧ q = q0}.

1	n+1
i =⇒ qi+1	1


The properties to test
We assume in the following that the properties to test are expressed using a logic
L . Formulas of L are built upon a finite set of n-ary operators Fn and a finite set of predicates {p1, p2,..., pn}. The abstract syntax of such a logic could be defined as follows: formula ::= Fn(formula1, formula2,..., formulan) | pi.

Formulas of L are interpreted over ﬁnite execution sequences. However, this se- mantics also takes into account two other important features:
First, this semantics is defined on two levels. Predicates are not atomic, i.e. they do not necessarily correspond to occurrences of single visible actions, but rather of (concrete) sequences of visible actions. Operators Fn are then interpreted over abstract execution sequences, i.e., sequences of predicates.
Second, since our objective is to test either the validity or the non-validity of a formula ϕ, the semantics of ϕ defines three kinds of execution sequences, corre- sponding to the possible verdicts delivered by a tester: the ones that satisfy ϕ (pass), the ones that do not satisfy ϕ (fail), and the ones for which we cannot conclude about the satisfiability of ϕ (inconc).
More formally, a triplet of finite languages (LP , LF , LI ) is associated with each
pi	pi	pi
predicate pi. These three languages define respectively concrete execution sequences that satisfy pi, that do not satisfy pi, and for which the satisfiability of pi is unknown. The following assumptions are required:

LP , LF and LI are defined over an alphabet Ap ⊆ Actvis. Intuitively, Ap
is the

pi	pi	pi	i	i
set of visible actions whose occurrences influence the truth value of pi.
This set of three languages defines a partition of (Ap )∗.
For two distinct predicates pi, pj, Api and Apj are disjoint.
The semantics of a non-atomic formula ϕ(p1, p2,..., pn) is then defined by three

sets [ϕ]]P , [ϕ]]F , [ϕ]]I , inductively computed from LP , LF
and LI
for each pi

appearing in ϕ.
pi	pi	pi

Finally, we say that an LTS S satisfies ϕ (we note S |= ϕ) iff all sequences of Exec(S) belong to [ϕ]]P , and we say that it does not satisfy ϕ iff there exists a sequence of Exec(S) that belongs to [ϕ]]F .

A set of communicating test processes
The test cases we aim to produce consist of a set of sequential communicating test processes. Roughly speaking, each test process is built from classical programming primitives such as variable assignment, sequential composition, (non-deterministic) choice, and iteration. It can also perform communications with the other test pro- cesses, and interact with the SUT. This sequential behaviour can be modelled by an LTS extended with variables.
Test processes run asynchronously and communicate with each other either by “rendez-vous” on dedicated communication channels or through shared variables. The semantics of a whole test process Tϕ can be expressed by an LTS STϕ . A complete syntax and semantics of such a “test calculus” can be found in [5], but other classical process algebra could be used as well.

Test generation
The purpose of the test generation phase [6] is to produce a test case Tϕ (i.e. a set of communicating test processes) associated to the L -formula ϕ under test. We distinguish two kinds of test processes (which are both LTSs):
test modules tpi , provided by the user, and associated with the predicates pi of
ϕ. Their purpose is to produce a test verdict indicating whether a given concrete
execution sequence belongs either to LP , LF or LI .  Examples of such test
pi	pi	pi
modules are given on Fig. 5 in Sect. 4.
test controllers tF n , associated with each n-ary operator Fn of the logic L . Their purpose is to control the execution of the test process associated to each of their operands by means of basic signals (start, stop, loop), and to collect their verdicts in order to produce a resulting verdict corresponding to this instance of operator Fn. One can find controllers for several logics in the research reports provided in [8].
This test generation technique can be formalized by a function called GenTest L , such that GenTest L (ϕ) = Tϕ. This function is inductively defined on the syntax of L in the following way:
If ϕ = pi, then GenTest L (ϕ) returns the test module tpi (associated with the predicate pi) extended with the communication operations required to make it controlable by another test process (see Fig. 3 in Sect. 3).
If ϕ = Fn(ϕ1, ··· , ϕn), then GenTest L (ϕ) returns a parallel composition between (recursively defined) test processes tϕ1 , . . . , tϕn and an instance of the (generic) test controller tF n .

Finally, a special test process tmain is added to launch the whole test execution and collect the final verdict. According to this generation technique, the architecture of a test case Tϕ exactly matches the abstract syntax tree corresponding to formula ϕ: the root is tmain, leaves are test modules corresponding to predicates pi of ϕ, and intermediate nodes are controllers associated with operators of ϕ (see Fig. 6 in Sect. 4.2 for an example).

Test selection and execution
From a formal point of view, the test execution sequences are the execution se- quences of a parallel composition between the LTS S modelling the SUT behaviour and the test case STϕ , with a “rendez-vous” synchronization on the visible actions appearing in STϕ .
However, this LTS product may still contain a bunch of possible test executions (due to possible non-determinism both inside the test modules and introduced by the parallel composition). Moreover, the test generation function only ensures that
the verdicts produced by the test execution are sound with respect to the initial
formula ϕ: it does not help to select the interesting test executions that are likely to exhibit an incorrect behaviour of the SUT. To solve this problem we propose to use behavioural test objectives, already introduced in several model-based testing tools (e.g. in [9,14]). Their purpose is to inject some execution scenario in the test cases produced by the test generation phase, either by enforcing the execution order of some visible actions, or by introducing other additional visible actions to lead the SUT into some particular state. Most of the time, in specification based testing, this test selection is performed during the test generation phase, by pruning the undesired test executions from the whole SUT specification. In our approach, the selection is not performed during test generation, but during the test execution (similarly to walk guidance in TorX). This is due to the fact that we do not rely on such a specification. So, the test selection phase is combined with the test execution: the test objective is expressed by an LTS with accepting states, and the test sequences leading to such states are privileged during the text execution.
This approach is formalized below. In a LTS S, for two states q, q' we note q' ∈ ReachS(q) the fact that q' is accessible from q in S. Also, when q' ∈ ReachS(q), we note dS(q, q') the distance between q and q', i.e. the minimal length of the existing paths between q and q'.
Definition 2.1 [Behavioural test objective] A test objective O relatively to a test case t which semantics can be expressed by a LTS (QSt , ActSt , →S , qSt ) is a de-
t	0
terministic LTS (QO, ActO, →O, qO) complete wrt. ActSt (i.e. ∀q ∈ QSt , ∀a ∈
0
S	'	O	a	'	O	O	O

Act t , ∃q ∈ Q  · q →O q ). Q	contains two sink states Accept	and Reject
, and

ActO ⊆ Actvis.
Using a test objective, it is possible to operate “on the fly” a test selection on the test case during the execution.
Definition 2.2 [Selection using a behavioural test objective] Let t be a test case





Informal Requirement

SUT
Interface
Test Objective




Verdict for F
(pass,fail,inc)   Logging information

Fig. 1. Abstract view of the j-POST testing toolchain
which semantics can be expressed by St = (QSt , ActSt , →S , qSt ), and a behavioural
t	0
test objective O = (QO, ActO, →O, qO).  The execution of t guided by O can be

defined as a synchronous product SO
= St
× O such as ActSO
= ActO, QSO  ⊆

QSt × QO ∪ Inc, and →SO is defined by the following rules. Note that control and observation actions are not distinguished.

t →a S
t'	AcceptO ∈ ReachO(o),o →a O o'	dS (o', AcceptO) < dS (o, AcceptO),a ∈ ActSt

(1)

(t, o) →a
O (t', o')
t

AcceptO ∈ ReachO(o),o →a S
o'	dS (o', AcceptO) < dS (o, AcceptO),a ∈/ ActSt

(1')

(t, o) →a
O (t, o')
t

t →a S
t'	AcceptO ∈ ReachO(o),o →a O o'	dS (o', AcceptO) ≥ dS (o, AcceptO)

(2)

(t, o) →a
O (t', o')
t

o →a O RejectO
(t, o) →a	Inc
t

(3)

Some priorities are associated with these rules to favour the execution of tran- sitions bringing closer to an Accept state. The rules (1) and (2) are of the highest prioriy, then is rule (2), and at last the rule (3) is of the lowest priority.
Finally, the set of test execution sequences obtained from an SUT S and a test case STϕ when taking into account a test objective O is defined as the execution sequences of the parallel composition between the SUT S and the LTS STϕ × O. Note that when the Inc state is reached in this composition, the whole test execu- tion is stopped and an inconclusive verdict is issued. This general framework has been instantiated for two particular logics, namely LTL-X, and extended regular expressions (see Sect. 3.2).
Architecture and functionalities of j-POST
The architecture of the toolchain is depicted in Fig. 1. It is built upon three main components, a test designer, a test generator and a test execution engine. Two interfaces are provided: a command-line mode and a graphic interface.
The purpose of j-POST is to check through black-box testing whether a Java application fulfills a given requirement. To do so, the test designer (step 1) helps the user both to formalize this requirement in a trace-based logic and to elaborate
a test module library. Each test module (corresponding to a predicate used in the requirement) is obtained by combining some of the actions offered by the SUT interface. The test modules are used by the test generator (step 2), according to a




Informal Requirement



SUT Interface
Formal Requirement	ATM Library
F(pi) ∈ ERE, LTL	pi ↔ mi






Graph representation
(png, jpeg, . . . )
Requirement Parser

Test Tree Builder




Test

LTL
ERE
Instanciation

plugin plugin
Test Generator



Formal requirement
F(pi) ∈ ERE, LTL
ATM Library
pi ↔ mi
Test case
m1  ···  mn communicating

Fig. 2. Abstract view of the j-POST test designer and test generator architecture
logic plugin, to produce a test case as a set of communicating test processes. Finally, this test case can be launched by the test engine (step 3), taking into account a test objective to select the more promising test sequences.

Test designer
The test designer of j-POST is a user assistant that helps to elaborate the formal re- quirements and the corresponding test modules through dedicated editors available within the Eclipse Modeling Framework. Each test module is stored into an XML file (their j-POST internal representation). Moreover, the test designer provides a tool (based on GraphViz [1]) to vizualise them in a more intelligible way. This avoids any error-prone manipulations of XML files from the user.

Test generator
The j-POST test generator consists mainly in implementing the GenTestL function. It produces a test case following the syntax-driven approach recalled in Sect. 2.3 in two stages:
The first stage is the construction of a communication tree obtained from the abstract syntax tree of the formula. This tree expresses the communication archi- tecture between the test processes that will be produced by the test generator. Its leaves are abstract test modules (ATM) corresponding to the atomic predicates of the formula, taken from the library. Its internal nodes are (copies of) generic test controllers, corresponding to the logical operators appearing in the formula (they are obtained from a finite set of generic controllers provided by the logic plugin). Finally, the root of this tree is a special test process, called testCaseLauncher, whose purpose is to initiate the test execution and deliver the resulting verdict.
The second stage consists of instantiating the communication tree by associating fresh channel names to each local communication between test processes. It relies on a traversal of this communication tree in order to modify the test modules. In particular the test modules provided by the user are automatically extended with additional communication actions to be managed by the test controllers, e.g. starting signal, verdict emission (see Fig. 3). The resulting test case is a set of XML



Fig. 3. Instantiation of an abstract test component tp

Abstract test case
m1  ···  mn communicating
Test Objective





Scheduler
Objective Engine


Test case Loader


ATC Engine
execution information


Logger

Execution graphs Execution traces

abstract external actions
Concretisation Wrapper

Mapping
RMI	JMS
communication
SUT

plugin
plugin

Test Execution Engine


Verdict for F (pass,fail,inc)
Fig. 4. Abstract view of the j-POST test execution engine
files, one per test process.
Generic test controllers and test generation algorithms have been defined for different specification formalisms. So far, j-POST TestGenerator supports two common-use formalisms, by means of logic plugins:
Temporal logics [12] like LTL are frequently used in the verification community to express requirements on reactive systems. We consider here fragments of such logics whose models are set of ﬁnite execution traces. We did not include the next operator in order to be insensitive to stuttering [3]. The complete definition of the variant of LTL-X we use is given in [6].
Extended Regular Expressions [10] are another formalism to define behavior pat- terns expressed by finite execution traces. They are commonly used and well- understood by engineers.

Test execution engine
The purpose of the test execution engine is to produce a verdict for the initial requirement. It takes as inputs the test case produced by the test generator, a test objective, and a mapping describing how to execute SUT interactions used in the test modules.
The architecture of the engine is depicted in Fig. 4. First the test case (a set of

XML files) is loaded using the test case loader. Each test process is executed in a separate Java thread. A centralized scheduler implements both the internal commu- nications between the test processes (based on “rendez-vous” and shared variables), and solves the priority conflicts between their actions (according to a predefined policy). Moreover, interactions to be performed on the SUT transit through a Con- cretisation Wrapper. This component is in charge of transforming these interactions into executable operations on the SUT (depending on the communication medium used, e.g. Java RMI). This transformation may also add some parameters omitted at the test module level (for the sake of simplicity), but mandatory for the test execution. Finally, the test selection operation described in Sect. 2.4 is performed by the Objective Engine. When the test execution terminates a verdict is issued and the Logger produces some execution traces that help the diagnostic phase.

j-POST at work
We describe in this section the use of j-POST on an example. Tests are designed, generated, executed using the j-POST toolchain to check some properties on a travel agency application [4], called Travel. We take as inputs an informal requirement extracted from the functional specification of Travel and the application interface. The requirement we choose for the demonstration purpose is informally expressed as “it is impossible to create a mission in Travel before being connected”.

Test design
We start by presenting the test design stage, that is the requirement formalization and the edition of test modules.

Requirement formalization.
A possible understanding of our requirement could be that a behaviour in which it is possible to create a mission before performing the identification ac- tion is not desired. In other words, we can say that we require no mission cre- ation until a connection is open. This informal statement refers to two abstract operations: “create a mission”, and “open a connection”. In the following we respectively designate these two operations by the predicates missionCreation() and connection(). The requirement can be expressed formally by an LTL formula: ( ¬missionCreation())  U  connection().


Test module edition.
Test	modules	have	to	be	created	by	the	user	for	the	predicates
missionCreation() and connection(). Each of this module should describe:
how to perform the abstract operation using the Travel interface;
what is the test verdict obtained (depending on how Travel reacts).













Fig. 5. Test modules for predicates connection and createMission

Possible test modules are proposed in Fig. 5, produced with the j-POST test designer. The connection test module (left-hand side) contains three possible ex- ecution sequences: a correct call to the connection method identify (the user is “Falcone”, the correct password is “azerty”, which corresponds to a registered user of Travel ), an incorrect one (the password is “qwerty”, it is not valid), and an execution where the connection procedure is never called. Note that the call to the identify() method returns an identification number which is stored in a shared variable (between test components) called id. The createMission test mod- ule (right-hand side) consists of calling the missionRequest() method, supplying the shared variable id as an identification number. Depending on the return value (createOk), it delivers the corresponding verdict.
Inside the toolchain these modules are represented using an XML format, but, from a practical point of view, they can be written and viewed using the j-POST test designer.

Test generation
The requirement stated, and the test modules designed (Fig. 5), we are now able to perform the test generation. In order to illustrate such a process, we give an insight of the generated test case on Fig. 6. The structure of this test case follows the structure of the formula. It contains a test controller for each operator appearing in the formula (Until and Not ), and a test module for each predicate (missionCre- ation() and Connection()). The testCaseLauncher is in charge of managing the execution of the testcase and emitting the final verdict. The c start (resp. c stop, c loop, c ver) channels are used by the processes to perform starting (resp. stopping, rebooting, verdict transmission) operations.


Test execution
The next operation to perform is to choose a test objective in order to restrict the set of potential test executions. Regarding the requirement we consider (“no mission creation until a connection is open”), an interesting objective is to try to falsify this requirement in order to exhibit an incorrect behaviour of the software under test. Falsifying such a requirement means for instance producing an execution sequence where :









c start1 ,c stop1 , c loop1 ,c ver1
Controller not
rootTestCase
c start0 ,c stop, c loop0 ,c ver
Controller until
c start2 ,c stop2 ,
c loop2 ,c ver2 ,
connection

c start3 ,c stop3 , c loop3 ,c ver3
missionCreation
Fig. 6. Test case produced from ¬(missionCreation) U connection
the verdict delivered by missionCreation() is pass (possibly after several previous
fail results) ;
in the meantime, the verdict delivered by connection() remains always fail.
Such a test objective can be obtained from the test modules given on Fig. 5. How- ever, obtaining a fail verdict for a connection operation can be fully controlled by the test execution engine (e.g., by supplying an incorrect password), whereas the verdict returned by a mission creation cannot be controlled (it only depends on the SUT behaviour).


Three versions of the Travel application have been tested:
Experiment 1. In the first (erroneous) version of Travel a mission creation request is always accepted, therefore our requirement is false (a mission can be created by a non connected user). The test execution engine detects this error (it delivers a fail verdict) and produces the test execution traces and graphs for the test case and each module.
Experiment 2. In the second (erroneous) version of Travel a mission creation request is accepted either if the identification number supplied is correct (it cor- responds to a return value of a connection request), or if it is the third attempt to create this mission. Therefore our requirement is still false: if a non connected user tries repeatedly to create a mission, it eventually succeeds. This error is detected by the test engine, which delivers a fail verdict.
Experiment 3. Finally, the third version of Travel always refuses a mission request as long as the identification number supplied is invalid. Thus, the only way for a non connected user to create a mission is to “guess” a correct identification number. This cannot be achieved by our test execution engine, which delivers here a pass verdict.

Conclusion and perspectives
This paper presents an original approach for property-oriented software test- ing (POST). Starting from a formula expressed in a trace-based logic, the user

first provides a test module (using the test designer) dedicated to each predicate appearing in this formula. The test generation phase then consists of producing a test case as a set of communicating test processes by combining the test modules with some test controllers associated to each logical operator. This test case can be executed by a test engine, able to take into account a test objective to constrain the set of test sequences to execute. This whole testing approach has been implemented in a working tool and applied to some non-trivial case studies. The architecture makes it open, and easily allows the toolchain to support new logical formalisms by adding logic plugins.
The main advantage of this approach is that it does not require a “global” behavioural specification of the software under test, as is the case in many model- based testing approaches. In fact the user only needs to make explicit the evaluation of a predicate in the test modules. The test generation phase is therefore rather straightforward and does not suffer from state explosion limitations. Of course, the test case produced may encompass many possible test executions, but the use of test objective allows the user to select the most interesting scenarios. This approach seems particularly relevant to dealing with security or robustness testing, where the functional model of the SUT can be very large (and hence not easily available as a single formal specification), and where the requirements to be checked only concern specific parts of this model. In fact, one of the motivations for this work was the validation of the correct deployment of security policies within the French Politess [13] project.
The Travel case study allowed many enhancements for j-POST and opens several research perspectives. In particular, it appears that the design of test modules could be facilitated by the use of abstract domains (e.g., at the test module level one only needs to distinguish between correct passwords and incorrect ones, without referring to a concrete value). These abstract domains could then be concretized only at the test execution level by selecting relevant values within a concrete domain (which may depend on the SUT’s current state). This concretisation could be performed, for instance, according to coverage criteria that could be defined with respect to the requirement under test. It seems particularly worthwile to relate this work with [11].

References
AT&T Research, Graph Visualization Software, http://www.graphviz.org (2007).
Belinfante, A., L. Frantzen and C. Schallhart, Tools for Test Case Generation., in: M. Broy, B. Jonsson, J.-P. Katoen, M. Leucker and A. Pretschner, editors, Model-Based Testing of Reactive Systems, LNCS 3472, 2004, pp. 391–438.
Clarke, E., O. Grumberg and S. Peled, “Model Checking,” The MIT Press, 1997.
Falcone, Y., A Travel Agency Application, Technical report, V´erimag (2007).
Falcone, Y., J.-C. Fernandez, L. Mounier and J.-L. Richier, A Test Calculus Framework Applied to Network Security Policies., in: FATES/RV, LNCS 4262, 2006, pp. 55–69.
Falcone, Y., J.-C. Fernandez, L. Mounier and J.-L. Richier, A Compositional Testing Framework Driven by Partial Specifications, in: TestCom/FATES, LNCS 4581, 2007, pp. 107–122.

Hartman, A., Model Based Test Generation Tools Survey, Technical report, AGEDIS Consortium (2002).
j-POST Reference Page (2007). URL http://www-verimag.imag.fr/~async/jpost.html

Jard, C. and T. J´eron, TGV: theory, principles and algorithms, A tool for the automatic synthesis of conformance test cases for non-deterministic reactive systems, Software Tools for Technology Transfer (STTT) 6 (2004).
Kleene, S. C., Representation of events in nerve nets and finite automata, in: C. E. Shannon and
J. McCarthy, editors, Automata Studies, Princeton University Press, Princeton, New Jersey, 1956 pp. 3–41.
Lestiennes, G. and M.-C. Gaudel, Testing processes from formal specifications with inputs, outputs and data types, in: ISSRE (2002), pp. 3–14.
Manna, Z. and A. Pnueli, “Temporal verification of reactive systems: safety,” Springer-Verlag New York, Inc., New York, NY, USA, 1995.
Project Politess, ANR-05-RNRT-01301 (2007). URL http://www.rnrt-politess.info

Schmitt, M., M. Ebner and J. Grabowski, Test Generation with Autolink and Testcomposer, in: 2nd Workshop of the SDL Forum Society on SDL and MSC - SAM’, 2002.
Tretmans, J., Test Generation with Inputs, Outputs and Repetitive Quiescence, Software - Concepts and Tools 17 (1996), pp. 103–120.
Tretmans, J. and E. Brinksma, TorX: Automated Model Based Testing - Coˆte de Resyste, in:
Proceedings of the First European Conference on Model-Driven Software Engineering, 2003, pp. 13–25.
