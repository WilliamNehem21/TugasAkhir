Electronic Notes in Theoretical Computer Science 119 (2005) 51–65  
www.elsevier.com/locate/entcs


New Algorithms for Solving Simple Stochastic Games
Rafal- Somla1 ,2
Computing Science Department, Uppsala University, Box 337, SE-75105, Uppsala, Sweden

Abstract
We present new algorithms for determining optimal strategies for two-player games with proba- bilistic moves and reachability winning conditions. Such games, known as simple stochastic games, were extensively studied by A.Condon [2,3]. Many interesting problems, including parity games and hence also mu-calculus model checking, can be reduced to simple stochastic games. It is an open problem, whether simple stochastic games can be solved in polynomial time.
Our algorithms determine the optimal expected payoffs in the game. We use geometric interpre-
tation of the search space as a subset of the hyper-cube [0, 1]N . The main idea is to divide this set into convex subregions in which linear optimization methods can be used. We show how one can proceed from one subregion to the other so that, eventually, a region containing the optinal payoffs will be found. The total number of subregions is exponential in the size of the game but, in practice, the algorithms need to visit only few of them to find a solution.
We believe that our new algorithms could provide new insights into the difficult problem of deter- mining algorithmic complexity of simple stochastic games and other, equivallent problems.
Keywords: infinite graph games, parity games, simple stochastic games, finding optimal strategies, successive approximation.


Introduction
Many problems studied in computer science have an elegant presentation in a form of two-player graph games with various winning conditions. This in- cludes verification of open components, controller synthesis and also theory of alternating automata. Hence a question of finding efficient algorithms for solv- ing graph games, i.e., for deciding which player possess a winning strategy and

1 Email: rsomla@it.uu.se
2 This work is supported by Games for Processes project (VR grant 621–2002–455).


1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.07.008


possibly finding this strategy, becomes important. The problem was exten- sively studied for games with a wide range of winning conditions: from simple safety/reachability objectives to ω-regular ones expressed by either Bu¨chi/co- Bu¨chi or most general parity conditions [9,15,14,4,8,13,5]. A long standing open question in this area is whether it is possible to solve games with parity winning conditions in polynomial time. Through known reductions [12], a positive answer to this question would also mean that the mu-calculus model checking can be done in polynomial time.
In this paper we focus on simple stochastic games [2]. These are two-player, turn-based games with random moves. The objective in the game is to reach a final position (a sink) with the best possible associated payoff. Thus, rather than looking for a winning strategy, we want to find an optimal strategy, that is a strategy which guarantees the best expected payoff for a player.
We are interested in this kind of games because, on the one hand, other important graph games, like parity and mean-payoff games, can be easily reduced to simple stochastic games. On the other hand, simple stochastic games are instances of general stochastic games which have a rich and well developed theory. We believe that this link between an old area of operational research and the current studies can provide new insights into the problem of the complexity of graph games. Simple stochastic games are also interesting as a model for open, probabilistic components. Efficient algorithms for solving simple stochastic games can be used for verification of such components or even for synthesis of components meeting given specification.
Over the years, many algorithms has been proposed, which solve (simple) stochastic games. Many of them were later shown to be incorrect [3]. The correct ones, usually don’t have any satisfactory complexity analysis. The two main methods used in these algorithms are the strategy improvement method and solving the local optimality equations.
Strategy improvement was developed by Hoffman and Karp for general stochastic games [7]. In this method an initial strategy for one of the players is improved in each iteration by switching it at positions at which choices are not locally optimal.
The other method is based on solving a system of constraints for the op- timal expected payoffs in the game, which we call local optimality equations. Having optimal payoffs, one can easily reconstruct optimal strategies. For one-player games, the local optimality equations are linear, hence such games can be solved in polynomial time using linear programming techniques [6]. For two-player games the constraints are no longer linear and thus other methods are used, usually some form of iterative approximation.
We propose two algorithms which are based on the second method. For a


game with N positions the vector of optimal expected payoffs is an element of a hyper-cube [0, 1]N . Moreover, it is a maximal point of a set W of feasible vectors, described by the local optimality equations. Our main idea is to divide set W into subregions in which the equations become linear. This allows one to find in a polynomial time a maximal element in each subregion. Using this fact we show how to iterate through the subregions in such a way that eventually the subregion containing the optimal vector will be found. We prove that this must happen after at most an exponential number of iterations. In practice, we couldn’t find any examples, including those known from the literature [3,10], which would require more than a polynomial number of iterations. To evaluate the efficiency of our algorithms we have implemented them and run on example simple stochastic games. In these test runs we show that our algorithms perform comparably to the strategy improvement methods.
The rest of this paper is organized as follows. Section 2 contains basic definitions and properties of simple stochastic games. The existence of the optimal value and memoryless determinacy of the game is stated here. We present first of our algorithms in Section 3. We prove its convergence to the op- timal solution and also prove that the number of iterations of the algorithm is bounded by the number of different strategies in the game. Section 4 describes our second algorithm. It is a modification of the first algorithm which replaces costly solving of linear optimization problems by much simpler computations. We prove convergence of this simplified version of the algorithm and argue that the number of iterations is at most exponential in the size of the game. In Section 5 we describe the strategy improvement algorithms which we use as yardsticks to measure performance of our algorithms. Section 6 summarizes results of our experiments.

Simple Stochastic Games
Simple stochastic games are played by two players max and min on a game board G consisting of a finite directed graph of game positions. An edge in G indicates a possible move in the game. If a play reaches a sink s of G then it stops and player max wins from player min a payoff p(s) ∈ [0, 1] associated with that sink. 3 Let S be the set of all sinks of G. The remaining positions
V are divided into strategic and average (or random) ones. At a strategic position one of the players chooses the next move. Let Vmax and Vmin be

3 In the standard definition of a simple stochastic game there are only two sinks—the 0-sink and the 1-sink. Player max wins a play if it ends in the 1-sink. Here we use a generalized version of the game (cf. [3, p. 53])




1/32
1/128
7/8
1/8

1	0	1
Fig. 1. Example game board. Black circles represent min-player nodes, white ones are max-player nodes. Gray squares represent average nodes. There are three sinks with pay- offs 1, 0 and 1, respectively.

sets of positions where player max and min, respectively, makes a decision. At an average position x ∈ Vavg the next move is chosen randomly with a given probability distribution q(x, ·) over successors of x. Figure 1 presents an example game board with three average positions x, y and z, one min position a and one max position b. Three sinks on this board are labelled with their respective payoffs. Edges going out of average nodes are labelled with probabilities of their successors.
For each (nonterminal) position x game G(x) starts at x and is played until a sink is reached. Hence a play of G(x) is a maximal path x, x1, x2,... in the graph G. If it ends in a sink s then the outcome of the play is p(s) and player max is interested in maximizing this outcome while min wants to minimize it.
A strategy for a player describes the choices which that player makes during a play of the game. In this paper we consider only deterministic, memoryless strategies which select the next move based on the current position, ignoring the history of a play.
Definition 2.1 [strategies] A strategy for a player P ∈ {max, min} is a function σ : VP → V ∪ S such that x → σ(x) for all x ∈ VP . A play x0, x1,... of the game G(x0) conforms to σ if xi+1 = σ(xi) for all xi ∈ VP .
In what follows, we will often consider valuations v : V → [0, 1] of game positions. When necessary, we extend such a valuation to game sinks with the payoff function p. The extension of v will be written as v¯. Given a valuation we can speak about greedy strategies which make locally optimal choices with respect to that valuation.
Definition 2.2 [greedy strategies] Let v : V → R be a node valuation. A
max player strategy σ is v-greedy at x ∈ Vmax if v¯ σ(x)  = maxx→y v¯(y).
Finally, a strategy for a player P is v-greedy if it is v-greedy at each x ∈ VP .


Even when both players fix their strategies there are many possible plays due to the random choices made at average positions. Let σ and τ be strategies for players max and min, respectively. In a standard way edge probabilities induce a probability measure over plays conforming to σ and τ . Let qσ,τ (x, s) be the probability that a play of G(x) conforming to σ and τ ends in a sink s.

Definition 2.3 [expected payoffs] Let σ and τ be strategies for max and min, respectively. The expected payoff vσ,τ (x) ∈ [0, 1] in the game G(x) when players use strategies σ and τ is defined by
vσ,τ (x)= Σ qσ,τ (x, s) · p(s).
s∈S
Observe that implicit in this definition is the fact that an infinite play results in a payoff of 0.
Looking at the probability distribution over plays of G(x) it is not hard to verify the following fact.
Proposition 2.4 The vector vσ,τ of expected payoffs is a ﬁxed point of an operator Fσ,τ : (V → [0, 1]) → (V → [0, 1]) given by
,,v¯  σ(x)	if x ∈ Vmax,
,Σx→y q(x, y) · v¯(y)	if x ∈ Vavg.
Optimal strategies are defined in the usual way. It turns out that for simple stochastic games it is enough to consider memoryless strategies.
Definition 2.5 [optimal strategies/values] Strategies σ∗, τ∗ are optimal at x
if
vσ,τ∗ (x) ≤ vσ∗,τ∗ (x) ≤ vσ∗,τ (x)
for any σ and τ . The expected payoff vopt(x)= vσ∗,τ∗ (x) is called an optimal value of the game G(x) and is easily shown to be unique if it exists. Strategies σ and τ are optimal if they are optimal at every position in G.
The existence of the optimal value was proven by Shapley for general stochastic games [11] and later, by Condon, for the class of simple stochastic games [2]. The important assumption in these proofs is that a play of a game is finite with probability 1. 4

4 Using more advanced proof techniques, it is possible to obtain similar results also for non-stopping games. See e.g. a chapter on Positive Stochastic Games in [6].


Definition 2.6 [stopping game] G is a stopping game board if	s∈S qσ,τ (x, s)= 1 for any x, σ and τ . That is, for any x, a play of G(x) stops at a sink with probability 1 regardless of what strategies are used by the players.
The vector of optimal values of a stopping simple stochastic game is the only solution to the local optimality equations, as stated in the following theorem.
Theorem 2.7 (Shapley,Condon) Let G be a stopping game board and let
F : (V → [0, 1]) → (V → [0, 1]) be given by
,,maxx→y v¯(y)	if x ∈ Vmax,

F (v) x =
minx→y v¯(y)	if x ∈ Vmin,
,,Σx→y q(x, y) · v¯(y)	if x ∈ Vavg.

The operator F has a unique ﬁxed point v∗ and v∗(x) is the optimal value of
G(x) for all x ∈ V .
Proof (sketch). If G is a stopping game board with N non-terminal positions then the probability of reaching a sink in the first N steps of a play is at least mN , where m is the least edge probability in G. This implies that an operator FN is contracting, that is, FN (v) − FN (w)  ≤ (1 − mN )  v − w  , where the norm v  of v : V → R is defined by v  = maxx∈V |v(x)|. It follows that FN , and hence also F , has a unique fixed point v∗.
Let σ∗ and τ∗ be v∗-greedy strategies. For any strategy σ for player max
we have
Fσ,τ∗ vσ∗,τ∗ x ≤ F vσ∗,τ∗ x = Fσ∗ ,τ∗ vσ∗,τ∗ x = vσ∗ ,τ∗ (x)
for all x. Observe that Fσ,τ∗ is monotonic with respect to a partial order on
V → [0, 1] defined by v ± v' iff v(x) ≤ v'(x) for all x. Hence, by Knaster- Tarski theorem, the unique fixed point of Fσ,τ∗ , which is vσ,τ∗ , must be ± than vσ∗ ,τ∗ . By similar argument we show that vσ∗,τ∗ ± vσ∗ ,τ for any min player strategy τ . This proves that strategies σ∗ and τ∗ are optimal. Since vσ∗,τ∗ = Fσ∗ ,τ∗ vσ∗,τ∗  = F vσ∗ ,τ∗  and F has a unique fixed point, it follows that v∗ = vσ∗,τ∗ .	 
We note the following useful facts about optimal values and strategies.
Proposition 2.8 Let G be a stopping game board and let vopt(x) be the opti- mal value of G(x) for each x. The following are equivalent:
strategies σ and τ are optimal,
vσ,τ = vopt,

strategies σ and τ are vσ,τ -greedy.
strategies σ and τ are vopt-greedy,
Proof. Implication (a) ⇒ (b) follows by the uniqueness of the optimal value of a simple stochastic game. From (b) it follows that Fσ,τ (vσ,τ )= vopt = F (vσ,τ ), i.e., σ and τ are vσ,τ -greedy. If (c) holds then F (vσ,τ ) = Fσ,τ (vσ,τ ) = vσ,τ . Hence, by Theorem 2.7, vσ,τ = vopt and thus (d) also holds. Implication
(d) ⇒ (a) was proven as a part of the proof of Theorem 2.7.	 
Implication (d) ⇒ (a) of Proposition 2.8 shows how one can reconstruct optimal strategies knowing the optimal values of the game. The equivalence
(a) ⇔ (c) gives a polynomial procedure for deciding whether given strategies σ and τ are optimal: find the vector vσ,τ by solving linear equations v = Fσ,τ (v) and then check if σ and τ are vσ,τ -greedy.
Corollary 2.9 The problem of ﬁnding optimal strategies/values of a stopping simple stochastic game is in NP.
From now on we restrict our attention to games played on a stopping game board.

Finding Optimal Values
From Theorem 2.7 we know that the vector of optimal values of a stopping game G is the unique fixed point of the operator
,,maxx→y v¯(y)	if x ∈ Vmax,

F (v) x =
minx→y v¯(y)	if x ∈ Vmin,
,,Σx→y q(x, y) · v¯(y)	if x ∈ Vavg.

Since F is ±-monotonic it follows, by Knaster-Tarski theorem, that its fixed point is a supremum of all the pre-fixed points of F . In other words, the vector of optimal values is a maximal point in a region W of the hyper-cube [0, 1]V
defined by
W =  v : V → [0, 1]  v ± F (v)}.
In order to find this maximal point, we propose to divide W into subregions in which F is linear. This way the linear programming techniques can be used in each subregion to speed-up successive approximation of the optimal values.
For given strategies σ and τ , let
Wσ,τ = v ∈ W  ⟨σ, τ ⟩ are v-greedy}.


Observe that F restricted to Wσ,τ is the same as Fσ,τ which is linear. Note also that different subregions Wσ,τ have disjoint interiors and that all the subregions sum up to W . The number of different sub-regions is the same as the number of different strategies and is exponential in the size of the game. Our algorithm iterates through the subregions of W until it finds one corresponding to optimal strategies. In each iteration a new subregion is visited and this new subregion is determined using a maximal point of the
current one. This maximal point is found by solving a linear program.
Algorithm 1 (improved iteration I)
Start with v1 = F (0).
Find vi-greedy strategies ⟨σi, τi⟩. Stop if ⟨σi, τi⟩ are optimal.
Find a valuation v which maximizes	x v(x) and satisﬁes the linear con- straints:
vi ± v,
strategies ⟨σi, τi⟩ are v-greedy,
v ± Fσi,τi (v).
Take vi+1 = F (v) and repeat.
The convergence of the first algorithm to the optimal values easily follows from the following observations.
Lemma 3.1 Suppose that vn ∈ W . If vn ± v ± F (v) and vn+1 = F (v) then
F (vn) ± vn+1 and vn+1 ∈ W .
Proof. This is a trivial consequence of F being monotonic: F (vn) ± vn+1
follows from vn ± v and vn+1 ± F (vn+1) follows from v ± vn+1 = F (v).	 
Proposition 3.2 If {vn} ⊆ W and F (vn) ± vn+1 then limn→∞ vn = vopt.
Proof. For each x the sequence vn(x) is bounded and monotonic, hence it converges to some value v(x) ∈ [0, 1]. Since F (vn−1) ± vn ± F (vn) it follows that v = F (v) and therefore, by Theorem 2.7, v = vopt.	 
Next, we show that the algorithm never visits the same subregion twice, from which it follows that it terminates after at most an exponential number of iterations.
Proposition 3.3 Let v0 ± v1 ± v2 ± ··· ± vn be the sequence of valuations constructed by Algorithm 1 such that none of vi is the optimal values vec- tor. Let Wi be the subregion containing vi and corresponding to the vi-greedy strategies chosen in step 2 of the algorithm. Then Wi /= Wj for i /= j.


Proof. Suppose that Wi = Wj = Wσ,τ for some i < j. We have vi+1 = F (v) and vj+1 = F (v') where v and v' are maximal solutions to the same set of linear constraints. Moreover, v ± vi+1 ± vj ± v' and therefore v = v'. It follows that v = vi+1 = F (v) so that vi+1 is the fixed point of F and hence, by Theorem 2.7, also the optimal values vector. This contradicts our assumption.	 
Observe that the maximal points of the subregions, found in step 3 of the algorithm, form a ±-monotonic sequence. Therefore the sequence of subre- gions traversed by the algorithm is a chain in a partial order induced by ±-wise ordering of maximal points of these subregions. We expect that the maximal length of such a chain is much smaller than the total number of subregions, but so far we are unable to prove this formally.

Simplified Version
In each step of Algorithm 1 a linear optimization problem must be solved. This can be done in polynomial time but the solution can be costly to compute. In the next algorithm, we propose how to replace the linear optimization problem by much simpler computations at the expense of performing only sub-optimal improvements in each iteration.
Having the current valuation vi and a pair of vi-greedy strategies ⟨σi, τi⟩ we compute v˜i = vσi,τi which we call the limit vector. This limit vector sets a direction in which the current valuation is increased.
If ⟨σi, τi⟩ are v˜i-greedy then, by Proposition 2.8, they are optimal and the algorithm can terminate. Otherwise, we look for the maximal point vt∗ in the segment [vi, v˜i] such that ⟨σi, τi⟩ are still vt∗ -greedy and we take vi+1 = F (vt∗ ). Such a choice of vi+1 guarantees the convergence of the constructed sequence to vopt.
Proposition 4.1 Suppose that v ∈ Wσ,τ and that v ± v˜. Let vt = (1−t) v+t v˜ for t ∈ [0, 1]. The maximal t∗ such that vt∗ ∈ Wσ,τ can be found in time O(K) where K is the number of edges in the game graph.
Proof. For each position x we find tx such that strategy σ/τ is vt-greedy at
x for t ∈ [0, tx] in the following way. Let x ∈ Vmax have successors y1,... , yk and σ(x) = y1. Note that v0(y1) ≥ v0(yj) for all j, since σ is v-greedy at x. Inequality vt(y1) ≥ vt(yj) holds for all t ∈ [0, tj] where tj = δj · (δj − δ˜j )−1,
δj = v(y1) − v(yj) and δ˜ = v˜(y1) − v˜(yj) (if δj = δ˜ we put tj = 1). Strategy
σ is vt-greedy at x iff vt(y1) ≥ vt(yj) for all j. Hence tx = minj tj, and in the same way we can find tx for x ∈ Vmin. Clearly t∗ = minx tx.	 
This leads to the following algorithm.

Algorithm 2 (improved iteration II)
Start with v1 = F (0).
Find vi-greedy strategies ⟨σi, τi⟩ and the limit vector v˜i = vσi,τi. Stop if
⟨σi, τi⟩ are v˜i-greedy.
Let vt = (1 − t) vi + t v˜i. Find t∗, the maximal t such that ⟨σi, τi⟩ are
vt-greedy.
Take vi+1 = F (vt∗ ) and repeat.
The convergence of the constructed sequence of valuations to the optimal values follows from Proposition 3.2 and the following lemma
Lemma 4.2 Let v ∈ Wσ,τ and v˜ = vσ,τ . Let vt = (1 − t) v + t v˜ for t ∈ [0, 1]. If strategies ⟨σ, τ ⟩ are vt-greedy then vt ∈ Wσ,τ .
Proof. We need to prove that vt ± F (vt). The limit vector v˜ = vσ,τ is a fixed point of Fσ,τ . From v ∈ Wσ,τ it follows that F (v)= Fσ,τ (v) and v ± F (v). By linearity and monotonicity of Fσ,τ we get
vt = (1 − t) v + t v˜ ± (1 − t) Fσ,τ (v)+ t Fσ,τ (v˜)= Fσ,τ (vt)= F (vt) .
The last equality follows from the assumption that ⟨σ, τ ⟩ are vt-greedy.	 
Proposition 4.3 Algorithm 2 ﬁnds an optimal pair of strategies after at most an exponential number of iterations.
Proof. Let vi be the sequence of valuations computed by Algorithm 2. As- suming vi ∈ W we see that vi ± vt∗ and, by Lemma 4.2, vt∗ ± F (vt∗ ). There- fore, by monotonicity of F , vi+1 = F (vt∗ ) ± F (vi+1) and hence vi+1 ∈ W .
Clearly v1 = F (0) ∈ W thus vi ∈ W for all i. Also, from vi ± vt∗ it follows that F (vi) ± vi+1 for all i. By Proposition 3.2, limi→∞ vi = vopt.
We have F (vi) ± vi+1 ± F (vi+1) and v1 = F (0), hence Fi(0) ± vi ± vopt for all i. Thus the convergence rate of vi is no worse than the convergence rate of the sequence Fi(0).
As noted in the proof of Theorem 2.7, operator FN is α-contracting where N is the number of non-terminal positions in the game, α = (1−m)N and m is the least edge probability in the game board. It follows that for i = O (1/m)N the values Fi(0), hence also vi, are so close to the limit vopt that any vi- greedy strategies must be also vopt-greedy and thus optimal. At that point the algorithm will terminate.	 
Our experiments show that the number of iterations needed by Algorithm 2 to solve example games is slightly bigger but comparable with that of Algo-


rithm 1. Hence, in practice, replacing the exact solution of the linear con- straints problem by a heuristic choice of vt∗ seems to work well. Unfortunately, with this approach, the argument of Proposition 3.3 is no longer valid and, at least in principle, it is possible that Algorithm 2 traverses several times the same subregion during its search for the optimal values.
We note that a similar modification of the value iteration method is de- scribed in [1] in the context of Markov decision processes (i.e., single player stochastic games) as “generic rank-one corrections”. 5 In this scheme, the current valuation is increased along a fixed vector d in each iteration, so that vi+1 = F (vi + γd). However, the difficulty of this method lies in the correct choice of vector d which entails guessing the optimal values and correcting this guess during iterations.
In contrast, our algorithm uses the easily computable limit vector to deter- mine the direction in which to improve the current valuation in each iteration. We also use a simple and general criteria (Proposition 3.2) which guarantees convergence of the modified sequence to the optimal values and works for single as well as two-player games.

Strategy Improvement Algorithms
None of the known methods for solving simple stochastic games has satisfac- tory complexity analysis. It seems though, that one of the simplest methods, the strategy improvement, works particularly well in practice. We decided to use strategy improvement algorithms as yardsticks to measure efficiency of our new algorithms.
The strategy improvement method is based on improving an initial, ar- bitrary strategy for one player, say max, by updating it at nodes at which it doesn’t make optimal choices. Given a strategy σ, let vσ(x) be the best payoff player max can achieve in a game starting at x in which he uses σ. A position x ∈ Vmax is switchable if there exists a successor y of x such that vσ(y) > vσ(x). If this is the case, then the current strategy σ is updated to choose position y at x. After updating the current strategy at all switchable positions the whole process is repeated.
From this general schema we get different algorithms by using different methods for determining the values vσ(x). One method is to use the same strategy improvement technique to solve a one-player game resulting from fixing max choices according to the strategy σ. This leads to the following algorithm:

5 I would like to thank the anonymous referee for pointing out this reference.


Algorithm 3 (strategy improvement I)
Choose arbitrary strategies σ and τ for max and min, respectively.
Find an optimal min counter-strategy for σ by updating τ in the following process:
compute vσ,τ by solving linear equations v = Fσ,τ (v),
for any min position x having a successor y such that vσ,τ (y) < vσ,τ (x) set τ '(x)= y,
set τ '(x)= τ (x) for all remaining x ∈ Vmin,
if τ ' /= τ then set τ ← τ ' and repeat from (b).
Update strategy σ based on the valuation vσ,τ . That is, for any x ∈ Vmax if x has a successor y with vσ,τ (y) > vσ,τ (x) then set σ'(x)= y. Otherwise set σ'(x)= σ(x).
If σ' = σ then strategies σ and τ are optimal. Otherwise set σ ← σ' and repeat from 2.
The advantage of this version of the algorithm is its simplicity. Besides solving the system of linear equations, one needs only to compare values of game positions and update strategies accordingly. On the other hand, nothing is known about the worst case complexity of this method. In principle, each improvement of the max strategy σ can cost exponentially many iterations of the inner loop in step 2. But in practice , despite its simplicity, the algorithm performs surprisingly well.
The other version of the algorithm uses linear programming to find payoffs vσ(x) in polynomial time. Determining the optimal values of a single-player stochastic game, also known as a Markov decision process, by solving a sys- tem of linear constraints is a well established technique which can be directly applied here.
Algorithm 4 (strategy improvement II)
Choose arbitrary strategy σ for max.
Find expected payoffs vσ for player max using strategy σ by solving the following optimization problem: maximize  x vσ(x) under constraints
vσ(x) ≤ v¯σ(y)	for x ∈ Vmin and x → y,
vσ(x)= v¯σ σ(x)	 for x ∈ Vmax, vσ(x)= Σ q(x, y) v¯σ(y)	for x ∈ Vavg,
x→y
vσ(x) ≤ 1	for all x.






SN n


SN 2



1/2	1/4
s1	s2

s1	s2	sn−2	sn−1	sn



Fig. 2. Sorting games SG n. For readibility edges connecting each average position xi to the corresponding “output position” yi were omited.
Update strategy σ to σ' based on the valuation vσ found in 2.
If σ' = σ then vσ = vopt. Otherwise set σ ← σ' and repeat from 2.
As with our algorithms, each iteration of Algorithm 4 takes a polynomial time. But we note again, that the cost of solving linear constraint problems can be high in practice.

Experiments
We tested our algorithms on a family of simple stochastic games which we call sorting games. These games seem to be non-trivial for solving by various methods.
A sorting game board SGn has n average positions x1,... , xn and n sinks s1,... , sn. The strategic positions of SGn are arranged into a sorting net- work SN n which copies values of the average nodes to the “output” positions y1,... , yn, sorting them into increasing order. Figure 2 shows how the sorting network SN n is constructed from SN n−1. As can be easily seen, network SN n consists of n(n − 1) nodes and hence board SGn has n2 non-terminal positions and n sinks. Each average position xi in SGn is connected to the sink si with probability 1/2i and to the corresponding position yi with probability 1−1/2i. For games SG 3, SG4 and SG 5 we searched for sink payoffs which make the algorithms perform a large number of iterations. We abstracted from the details of the implementation of the algorithms and we compared only the


number of iterations each algorithm needs to solve a game. This means that we treated solving a linear constraint problem as an atomic operation. For Algorithm 3 we counted the total number of strategy updates, taking into account also the costs of the inner loop of step 2.
Results of our experiments are presented in Table 1.
Table 1
Number of iterations taken by each of the algorithms on example sorting games.



Summary
We present two algorithms for determining optimal values and strategies in a simple stochastic game. The algorithms search for the solution of the local optimality equations in the set W of feasible valuations. Search space W is divided into subregions Wσ,τ corresponding to different pairs of strategies. Our first algorithm uses linear optimization techniques to advance to a new subregion in each iteration. The second algorithm replaces exact solutions of the optimization problems by easily computable heuristics.
We prove correctness of both algorithms. A single iteration of each of the algorithms can be done in a polynomial time. Hence the complexity of the algorithms depends mainly on the number of iterations required to find the solution. We provide exponential bounds for this number and give some indications why it could be much smaller in practice.


We also tested our algorithms on example games and compared them with the well known method of strategy improvement. We demonstrate that in practice their performance is similar.
Our main contribution is a new technique for finding the optimal values of a simple stochastic game which is comparable with the strategy improvement method. The technique is based on a geometric interpretation of node valua- tions as points in the hyper-cube [0, 1]N and identification of player strategies with the subregions of this cube. In this setting we show a different method for improving the current pair of strategies by advancing from one subregion to the other in a monotonic way.

References
Dimitri P. Bertsekas. Dynamic Programming and Optimal Control, volume II. Athena Scientific, 1995,2001.
Anne Condon. The complexity of stochastic games. Information and Computation, 96(2):203– 224, 1992.
Anne Condon. On algorithms for simple stochastic games. In Jin-Yi Cai, editor, Advances in Computational Complexity Theory, volume 13 of DIMACS Series in Discrete Mathematics and Theoretical Computer Science, pages 51–73. AMS, 1993.
Luca de Alfaro, Thomas A. Henzinger, and Orna Kupferman. Concurrent reachability games. In Proc. 39th IEEE Symp. on Foundations of Computer Science, pages 564–575, 1998.
Luca de Alfaro and Rupak Majumdar. Quantitative solution of omega-regular games. In ACM Symposium on Theory of Computing, pages 675–683, 2001.
Jerzy Filar and Koos Vrieze. Competitive Markov Decision Processes. Springer-Verlag, New York, 1997.
A. J. Hoffman and R. M. Karp. On nonterminating stochastic games. Management Science, 12:359–370, 1966.
Marcin Jurdzin´ski. Small progress measures for solving parity games. In Procedings of STACS, volume 1770 of LNCS, pages 290–301, 2000.
Robert McNaughton. Infinite games played on finite graphs. Annals of Pure and Applied Logic, 65(2):149–184, 1993.
Mary Melekopoglou and Anne Condon. On the complexity of the policy improvement algorithm. Technical Report 941, Univ. of Wisconsin – Madison, June 1990.
L.S. Shapley. Stochastic games. Proceedings of the National Academy of sceinces USA, 39:1095–1100, 1953.
Colin Stirling. Bisimulation, modal logic and model checking games. L. J. of the IGPL, 7(1), 1999.
Jens V¨oge and Marcin Jurdzin´ski. A discrete strategy improvement algorithm for solving parity games. In CAV’00: Computer-Aided Veriﬁcation, volume 1855 of LNCS, pages 202–215. Springer-Verlag, 2000.
Wies-law Zielonka. Infinite games on finitely coloured graphs with applications to automata on infinite trees. Theoretical Computer Science, 200(1-2):135–183, 1998.
Uri Zwick and Mike S. Paterson. The complexity of mean payoff games on graphs. Theoretical Computer Science, 158(1–2):343–359, 1996.
