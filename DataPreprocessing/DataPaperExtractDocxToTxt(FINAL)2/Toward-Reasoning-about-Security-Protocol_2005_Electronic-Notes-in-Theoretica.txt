Electronic Notes in Theoretical Computer Science 126 (2005) 53–75  
www.elsevier.com/locate/entcs


Toward Reasoning about Security Protocols: A Semantic Approach

Arjen Hommersom1
Nijmegen Institute of Information and Computing Sciences Radboud University, Nijmegen, the Netherlands

John-Jules Meyer
Institute of Information and Computer Science Utrecht University, the Netherlands

Erik de Vink
Department of Mathematics and Computer Science Technische Universiteit Eindhoven, the Netherlands
Leiden Institute of Advanced Computer Science Leiden University, the Netherlands


Abstract
We present a model-theoretic approach for reasoning about security protocols, applying recent insights from dynamic epistemic logics. This enables us to describe exactly the subsequent epistemic states of the agents participating in the protocol, using Kripke models and transitions between them based on updates of the agents’ beliefs associated with steps in the protocol. As a case study we will consider the SRA Three Pass protocol.
Keywords: modal logic, belief revision, semantic updates, security protocols, dynamic epistemic logic



1 Corresponding author: arjenh@cs.ru.nl



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.11.013

54	A. Hommersom et al. / Electronic Notes in Theoretical Computer Science 126 (2005) 53–75
1	Introduction


In today’s world of e-commerce and the Internet, the role of security protocols is getting increasingly important. The design of security protocols is difficult and error-prone [20,2], which makes (automated) verification of protocols of crucial importance. Since the late eighties, one line of research, amongst oth- ers, for reasoning about security protocols is based on the use of the so-called BAN logic, proposed by Burrows, Abadi and Needham in [5]. This is an epistemic logic augmented by constructs that are relevant for reasoning about security, such as the property of having the disposal of a cryptographic key to be able to decode a message and therefore to know its contents. Although many useful results having been reported (e.g., [16,1,21]), due to their com- plexity and their semantic underpinning the use of BAN logics to prove the correctness of security protocols has so been of limited success (cf. [3,6,22,19]). In this paper we will apply insights from dynamic epistemic logics as re- cently developed by Gerbrandy [13,14], Baltag and Moss [8,4,7], Van Dit- marsch [10,11], and Kooi [17]. Moreover, contrary to the traditional BAN logic approach, our approach is semantic or model-theoretic. We use Kripke models to represent the epistemic state of the agents involved in a protocol, similarly to the S5 preserving approach of Van Ditmarsch to analyze certain kinds of games involving knowledge. From Baltag and Moss’ action models we import the idea to describe belief updates of the agents by semantic oper- ators transforming the Kripke models at hand by copying and deleting parts of these models, although we use traditional Kripke models rather than the more involved action models. To this end we need also operators for unfolding models, which is in turn inspired by Gerbrandy’s work on possibilities. The difference being that in our approach only partial unfolding is called for. We furthermore propose a language to express belief updates in the context of security protocols as well as properties of these updates, and give a seman- tics of this language in terms of the models mentioned and the operators on them. Since our approach is model-theoretic, we believe that it may serve as
a starting point for the tool-supported verification of security protocols.
As a case study illustrating our approach, we will consider the so-called SRA Three Pass protocol and prove a property of it. It is not our intention to prove that the protocol is completely secure (as it is not in full generality), but we will prove that if the agents participating in the protocol are honest, then an intruder observing the communication does not learn anything about the plain-text messages in a single run. Furthermore we show what the intruder is able to learn about the agents participating.

In this section we briefly discuss some preliminaries and background regarding the updates we will handle and the epistemic model we will use. First, we define the notion of an objective formula and the concept of o-seriality.
Definition 2.1 Fix a set P of propositional variables, also referred to as atoms. The class of objective formulas is the smallest class such that
all propositional variables p ∈ P are objective;
if φ is objective, then ¬φ is objective;
if φ1 and φ2 are objective, then φ1 ∧ φ2 is objective.
So, objective formulas do not involve beliefs. For our purposes it is important that every agent distinguishes a world with the same ‘objective’ information. This leads to the notion of an o-serial model.
Definition 2.2 A Kripke model M = ⟨S, π, R1, ..., Rm⟩ is o-serial iff for all i, 1 ≤ i ≤ m, and w ∈ S, there exists v ∈ S such that (w, v) ∈ Ri and for all objective formulas φ it holds that (M, w) |= φ ⇔ (M, v) |= φ.
We use a, b, c, etc. as typical agents, taken from a class A. We use the no- tation {x}ka to denote a message x encrypted with the cryptographic key ka of agent a. Furthermore, B is used as a doxastic modal operator. For ex- ample, Baφ should be read as ‘a believes φ’. We interpret formulas on stan- dard Kripke models (M, s) = (⟨S, π, R1, ..., Rm⟩, s), where (M, s) |= Biφ iff
∀t ∈ S: Ri(s, t) → (M, t) |= φ.  The state s is referred to as the actual world.
We require the relations Ri to be o-serial, transitive and euclidean. This yields a class of models that we will call Kt45, a proper subset of the class of models of the well-known doxastic logic KD45. The lower case t refers to the axiom
(t)	Biφ ⇒ φ,
for every objective φ. The system Kt45 is sound with respect to the class of o-serial, transitive and euclidian models [15]. We will show that the operations we introduce preserve Kt45. The point is that in worlds of Kt45 models, we cannot both have Biφ and Bi¬φ, for an objective formula φ. This is reasonable from our assumption that agents are conscious about the protocol. Therefore, they will not infer objective contradictions. This objectivity is captured locally for each state. As a consequence, the operations that we introduce can restrict the set of states without destroying objective information.
For the analysis of security protocols below, we assume that we are om- niscient about the values of the variables in different runs of a protocol. For


example, the program variable p in a protocol run has the value [p]]. In the real world it is, obviously, always true that p = [[p ]. However, it is cumbersome to keep track of what is the real world in the operations on Kripke structures that we employ below. Therefore, we assume that an interpretation [·]] is given, that provides the ‘real’ values of the program variables when needed. It might very well be the case that p /= [[p ] in a certain state. From now on, we will abbreviate p = [[p]] to p. (Thus transforming a program expression into a propositional variable.) Similarly, чp is an abbreviation of p /= [[p]]. For example, agent a that learns Bbp V Bbчp, learns that agent b has assigned a value to the program variable p.
The types of updates we consider are (i) public announcement of a variable,
(ii) the private learning of a variable and (iii) the private learning about the knowledge of other agents.
The first type of update typically runs as follows: In an open network agent a sends a message to agent b. From a security perspective, it is cus- tomary to assume the Dolev-Yao framework [12], in which all agents in the network can read this message too. However, also in open networks private learning, the second type of update, can take place. For example, agent b receives a message {x}k from agent a. Here {x}k denotes a message with content x encrypted with the (symmetric) key k. If b possesses the key k, then b privately learns the message content x (assuming that the key k is shared among a and b). The final type of update, learning about knowledge of others, is probably the most interesting. It is realistic to assume that the steps in a protocol run are known to all agents. Therefore, observing that an agent receives a message will increase the knowledge of the other agents. For example, if agent a sends a message {x}k to agent b, then agent c learns that b has learned the information contained in the message {x}k, but typically, c does not learn x if c does not possess the key k.
Stronger types of updates we do not consider here. For example, we will not update the beliefs of an honest agent such that it learns that an intruder has learned about others. In the present paper, we restrict ourselves to updating beliefs about objective formulas and beliefs about objective formulas.

Update constructions
In this section we describes various types of updates in detail. We will start by defining an update for propositions in subsection 3.1. In subsection 3.2 we will define a belief update for agents that learn something about the belief of others. We do this in two slightly different ways by varying in the functions that describe a side-effect for an agent.

Objective updates
The belief update of objective formulas we will use is based on [18,8]. The construction works as follows: We will make copies of the states of the model such that the old worlds in old(S) correspond to the information in the original model and the new worlds in new(S) correspond to the new information.
Definition 3.1 Let a world (M, w)= (⟨S, π, R1, ..., Rm⟩, w), a group of agents
B, and an objective formula φ be given such that (M, w) |= φ. Then


update(φ,B)(M, w)= (⟨S', π', R' , ..., R'
⟩, w')

1	m
where
S' = old(S) ∪ {new(s) | (M, s) |= φ}
w' = new(w)
π'(old(u))(p)= π'(new(u))(p)= π(u)(p) for all p ∈ P 
for 1 ≤ i ≤ m the relation R' on S' is minimal such that
R'(old(u), old(v))	⇔ Ri(u, v)
R' (new(u), new(v)) ⇔ Ra(u, v)	if a ∈ B 
R' (new(u), old(v))  ⇔ Rb(u, v)	if b ∈/ B 

Note that φ holds in each state in the new part. The following example shows how this works on a concrete model.
Example 3.2 Consider the model (M, s) in Figure 1a where π(s)(p) = true and π(t)(p) = false.  The operation we execute is that b learns p, i.e. update(p,b). This results in the model (M, u) in Figure 1b where new(s) = u, old(s) = v and old(t) = w and π(u)(p) = π(v)(p) = true and π(w)(p)= false. The world new(t) is unreachable from the actual world and is therefore omitted from the figure. (In fact, in all figures in this paper, we will omit the unreachable worlds.)
We can see that the belief of agent a has not changed: it still considers its old worlds possible. The belief of agent b however, has changed. It now only considers the state u possible where p holds, hence b beliefs p.
The update update(φ,B) is based on a formula φ and a set of agents B. Roorda et al. [18] have given a characterization of the formulas that are al- tered by such an operation with a single learning agent. Here we extend the definition for multi-agent purposes.


a, b	a, b



a, b	a, b
s	a, b	t
b
a.	(M, s)	b.	(M, u)
Fig. 1.
Definition 3.3 An update function (·)[φ, B] is called proper if

(M, u)[φ, B] |= α) if b ∈ B 
Following Roorda et al. we have that update(φ,B) is proper. Moreover, update(φ,B) is uniquely characterized by Definition 3.3 upto elementary equiv- alence, i.e. if (·)[φ, B] is a proper update function, then (M, w)[φ, B] and update(φ,B) are elementary equivalent.
We collect the following properties of update(φ,B).
Lemma 3.4
For all formula φ it holds that (M, w) |= φ ⇒ update(φ,B)(M, w) |= BBφ.
If a model (M, w) satisﬁes the Kt45 properties and φ is objective, then the model update(φ,B)(M, w) satisﬁes the Kt45 properties as well.
The models
update(ψ,C)(update(φ,B)(M, w)) and update(φ,B)(update(ψ,C)(M, w))
are bisimilar.
Note that in Lemma 3.4a φ ranges over arbitrary formulas, including non- objective ones. However, for a non-objective formula, Biφ say, it can happen that, unintended, an agent increases the objective knowledge encapsulated by the formula φ. This is illustrated by the next example.




a, b

a, b	a, b
a, b
a, b	a, b


s	a	t	b	u

(M, s)	b.	(M ', v)
Fig. 2.
Example 3.5 Suppose we are interested in agent a learning the formula BbpV Bbчp, but not p itself. Consider the Kripke model (M, s) in Figure 2a, where π(s)(p) = π(u)(p) = true and π(t)(p) = false. This models that b knows that p is true. Agent a does not know p or чp, and it does not know if b knows p.
If we apply the definition of the update operation, it results in the model (M, v) from Figure 2b, where π(v)(p) = π(s)(p) = π(u)(p) = true and π(t)(p) = false. The reason that it turns out like this, is because the only state where Bbp V Bbчp holds, is the state s. Thus, all the other states have no corresponding new states that are reachable from the new point v. Figure 2b illustrates that a has learned Bbp V Bbчp, but also that a has learned p itself.
In the next subsection we will define a side-effect function such that a, referring to the situation above, will learn about others, but does not learn any objective formulas itself.

Side-effects
The main reason that in Example 3.5 the update of Bbp V Bbчp for agent a fails, is that it actually deletes the wrong arrows. The update operation deletes arrows of a to gain the states that satisfy the updating formula. This is not what we intend. We want a to keep all the states it considers possible, but at the same time we would like to update all the possible states of a such that the formula Bbp V Bbчp holds in these states. Moreover, we do not want to change the knowledge of other agents. In this section we define the functions that accomplish these requirements.
A technical obstacle is that states can be shared among agents. It is obvious that if we change a state with the intention to change the belief of one agent, then the belief of the other agents that consider this state possible, is changed as well. Therefore, the first thing to do, is to separate the states of learning agents from the states of agents that do not learn. This procedure will be called unfolding. The functions newB and orig are generalizations of new and old from the previous section, but the function orig is only defined

at the point of the model, i.e. the actual world.










Definition 3.6 Given a model (M, w) with M = ⟨S, π, R1, ..., Rm⟩, a parti- tioning X of the set of agents A, we define the operation unfoldX (M, w)= 
(⟨S', π', R' , ..., R' ⟩, w'), where

1
S' = ( 
m
newB(S)) ∪ orig(w)

w' = orig(w)
π'(newB(v))(p)= π(v)(p), π'(w')(p)= true for all p ∈ P, B ∈ X
the relation R' on S', 1 ≤ i ≤ m, is minimal such that
R'(newB(u), newC(v))	⇔	Ri(u, v) ∧ (B = C)
R'(orig(w), newB(u))	⇔	Ri(w, u) ∧ i ∈ B 
where B, C range over X .











So for every group of agents B there is copy of the original states (viz. newB(s) for every s ∈ S). This operation does indeed preserve our Kt45 properties and it models the same knowledge, which is captured by the following lemma.


a, b	a, b	a, b






a, b




a, b	a, b
s'	a
a
t'	b	u' a
s


s	a	t	b	u

s''
b	''
t

u''

a, b
a	b
a, b	a, b

(M, s)	b.	(M ', s)



Lemma 3.7
Fig. 3.

If (M, w) is a Kt45 model and X a partition of ma, then unfoldX (M, w)
is a Kt45 model as well.
For every model (M, w) and partition X of A, it holds that the models
(M, w) and unfoldX (M, w) are bisimilar.
Example 3.8 Consider the Kripke model (M, s) in Figure 3.a with π(s)(p)= π(u)(p)= true, π(t)(p)= false. So, b knows that p is true, while a does not. Furthermore, a does not know if b knows p. Now the operation we perform is unfold{{a},{b}}(M, s) which results in the model (M ', s) in Figure 3b (with valuation as expected).
So we have split the knowledge of a and b. The state s is the original state, the primed states model a’s knowledge and the double primed states model b’s knowledge. So the upper half of the model represents the knowledge of a, and the lower half represents the knowledge of b. Note that no states are shared, in particular because the point of the model is not reflexive.

Next, we give some preparatory definitions leading to the notion of a side-effect operation in Defintion 3.14. First we define the notion of a submodel.
Definition 3.9 A model M  = ⟨S, π, R1, ..., Rm⟩ is a submodel of M ' =
⟨S', π', R' , ..., R' ⟩, written as M ± M ', iff S ⊆ S', π(s)(p) = π'(s)(p) for
1	m
all s ∈ S, p ∈ P and Ri ⊆ R'.
Next, we construct a submodel that represents the knowledge of an agent a.
Definition 3.10 Given a model (M, w) = (⟨S, π, R1, ..., Rm⟩, w) and a par- tition { {a}, B } of A such that (M, w) = unfold{{a},B}(M ', w') such that
{{a}, B} for some model (M ', w'), the a-submodel suba(M ) of M for a is
given by (M ', w')= ⟨S', π', R' , ..., R' ⟩ where
1	m



Fig. 4. a-submodel outline
s ∈ S' ⇔ newa(s) V orig(s)
π'(newa(s))(p)= π'(orig(s))(s)= π(s)(p) for all p ∈ P 
R'(s, t) ⇔ Ri(s, t) ∧ s, t ∈ S'.
Clearly an a-submodel is a submodel in the sense of Definition 3.9. The restmodel is the part of the model that is complements the submodel with respect to the accessibility relation.
Definition 3.11 Given a model (M, w) = ⟨S, π, R1, ..., Rm⟩ and a submodel
N = ⟨S'', π'', R'', ..., R'' ⟩ of M , the restmodel restN (M ) of M with respect
1	m

to N is given by restN (M )= ⟨S', π', R' , ..., R'
⟩ where

1	m
s ∈ S' ⇔ s ∈ S ∧ ∃(u, v) ∈ R': u = s V v = s
π'(s)(p)= π(s)(p) for all p ∈ P 
R'(s, t) ⇔ Ri(s, t) ∧ чR''(s, t)
i	i

We can see the a-submodel and restmodel definitions in action by taking the model of Example 3.8 and applying the above definitions. See Figure 4. This exactly corresponds to the idea of two submodels that represent knowledge of different agents.
Now we would like to update the belief of some agents. To this end, we want to replace the submodel that represents their belief by a new model. For this, we introduce the replace operation.
Definition 3.12 Given a model N = ⟨S, π, R1, ..., Rm⟩, a model M , a sub-

model N ' such that N ± N ' ± M with restN' (M ) = ⟨S', π', R' , ..., R'
⟩, we

1	m
define replaceN' (N, M )= ⟨S'', π'', R'', ..., R'' ⟩ where
1	m
s ∈ S'' ⇔ s ∈ S V s ∈ S'
π''(s)(p)= π(s)(p) for s ∈ S'' for all p ∈ P 



a, b

s'	a
a
a, b

t' a s


s''
b	t''

u''

a, b
a	b
a, b	a, b
Fig. 5. (M '', s)

R''(s, t) e (s, t) ∈ Ri V (s, t) ∈ R'.
i	i
The idea is that once the belief is completely separated, we can not only safely change the belief of a certain agent, but also preserve the Kt45 properties. In particular, we change locally the knowledge of an agent a, e.g. regarding uncer- tainty of another agent b about a propositin p. The operation atomsplit(p,b) below removes the arrows of b between states that have a different valuation for p.
Definition 3.13 Given a model M = ⟨S, π, R1, ..., Rm⟩, we define a function

atomsplit(p,b)(M )= ⟨S', π', R' , ..., R'
⟩ as follows:

1	m
s ∈ S' e s ∈ S
π'(s)(p)= π(s)(p) for all p ∈ У 
R'(s, t) e Ri(s, t) (i /= b)
R' (s, t) e Rb(s, t) Λ π(s)(p)= π(t)(p).
Finally, we are in a position to define the actual side-effect function that ties these things together.
Definition 3.14 For a model (M, w) put (M ', w')= unfold{{a},A\{a}}(M, w) and N = suba(M '). The model side-effect(p,a,b)(M, w), the side-effect for a learning b knowing p, is given by
side-effect(p,a,b)(M, w)= (replaceN (atomsplitp,b(N ),M '), w').
Example 3.15 We continue Example 3.8. To the a-submodel of M we now apply atomsplitp,b which results in the model (M '', s) in Figure 5. The arrow (t', u') has disappeared, since π(t')(p) /= π(u')(p). Therefore, u' is not reachable anymore from the actual world, and can be dropped. Notice that a believes Bbp V Bbчp, while a has learned nothing about p itself.
We have the following results.


Lemma 3.16
If (M, w) is a Kt45-model, then side-effect(p,a,b)(M, w) is a Kt45-model as well.
Given a model (M, w), agents a, b, c, d, two propositions p, q З У, it holds that the models
side-effect(p,c,d)(side-effect(q,a,b)(M, w))


and
are bisimilar.
side-effect(q,a,b)(side-effect(p,c,d)(M, w))

Given a model (M, w), agents a, b, c, a proposition p З У and an objective formula φ, it holds that the models
side-effect(p,c,d)(update(φ,a)(M, w))


and
are bisimilar.
update(φ,a)(side-effect(p,c,d)(M, w))


Next, we consider how the formulas are altered by the side-effect function. We will partially answer this by presenting a few interesting formulas that hold in the resulting model. We will look at groups of agents instead of a single agent:
the group of agents that learn about other agents, ranged over by a;
the group of agents that is learned about, ranged over by b;
other agents, ranged over by c.
The fact that the agents of type a are the only agents that learn at all, is clear. The other agents consider their old worlds possible; their belief has not changed. With this in mind, we present a few properties of the side-effect function.
Lemma 3.17 Let a, b and c be three different agents. Given a model (M, w)
and the model (M ', w')= side-effect(p,a,b)(M, w), it holds that
(M ', w') |= Ba(Bbp V Bbчp)
(M ', w') |= Baφ iff (M, w) |= Baφ	for φ objective
(M ', w') |= BaCabc(Bbp V Bbчp)
(M ', w') |= Biφ iff (M, w) |= Biφ for i /= a.


Part (a) of the above lemma states that agent a obtains derived knowledge of an agent b. Part (b) states that no object knowledge is learned. Part (c) phrases that agent a considers the rest of the agents as smart itself. Finally, part (d) captures that other agents do not learn.
Property (c) is a reasonable assumption of a about the other agents in the context of open communication networks (as is the setting of Sections 4 and 5). If one agent believes that another agent knows the value of p, then it is reasonable to assume that another agent will believe the same. On the other hand common knowledge might be too strong to assume.
We show how to refine the approach above by distinguishing between agents that are considered to have equal learning facilities and thos that do not. I.e., we split the former group of other agents in those that commonly learn and thos that will remain ignorant. We represent this by linking a’s beliefs of those other agents back to the original (unmodified) states. We now distinguish four different type of groups of agents.
the group A of agents that learn about other agents, ranged over by a;
the group B of agents that is learned about, ranged over by b;
the group C of agents of which agents a believe they have commonly learned about agents in group b with, ranged over by c;
the group D of agents of which agents in a believe they have learned nothing about, ranged over by d.
For simplicity of presentation, we assume that there is exactly 1 agent present in each group, i.e. A = {a, b, c, d}. We define the new side-effect operation 0-unfold (where 0 refers to zero-knowledge). Note that the 0-unfold oper- ation depends on the particular partinioning of agents A. Since the operation s0-ide-effect will depend on the unfold operation, the side-effect function is also taken with respect to some chosen partitioning of the agent set.


Definition 3.18 Given a model (M, w) such that M = ⟨S, π, Ra, ..., Rd⟩, we define a function 0-unfold(M, w)= (⟨S', π', R' , ..., R' ⟩, w'), where
a	d
S' = newa(S) ∪ newbcd(S) ∪ orig(w)
w' = orig(w)
π'(newB(v))(p) = π(v)(p) and π'(orig(w))(p) = π(w)(p) for all p З У, B З {{a}, {bcd}}
R' on S', i З { a, b, c, d }, is minimal such that


R' (newa(u), newbcd(v))	e	Rd(u, v)
R'(newa(u), newa(v))	e	Ri(u, v) (i /= d)
R'(newbcd(u), newbcd(v))	e	Ri(u, v)
R' (orig(w), newa(v))	e	Ra(w, v)
R'(orig(w), newbcd(v))	e	Ri(w, v) Λ i = b, c, d
So instead of completely separating the knowledge of te agent a with the other agents, we share the knowledge of a about d with the other agents. Since the other agents do not learn anything, a does not gain knowledge about d. We present a lemma similar to Lemma 3.7.
Lemma 3.19
If (M, w) is a Kt45 model, then so is 0-unfold(M, w).
For	every	model	(M, w)	it	holds	that	the	models	(M, w)	and
0-unfold(M, w) are bisimilar.
Due to the case distinction for R' (orig(w), newB(v)) in Definition 3.18 above, it holds that the knowledge of a about b is not related to the knowledge of other agents about b or of b itself. So, as before, we can ‘cut out’ the submodel containing the b arrows from the belief of a:
Definition 3.20 Given a model (M ', w') = ⟨S', π', R' , ..., R' ⟩ such that
a	d
(M ', w')= 
0-unfold(M, w), for some (M, w), define b-sub(M ') = ⟨S'', π'', R'', ..., R''⟩
a	d
where
S'' = { newa(s) | s З S }
π''(s)(p) e π(s)(p) for all p З У 
R''(s, t) e R' (s, t) for s, t З S''
b	b
R'' = ∅ (i /= b).
The operation 0-side-effectp is then given by
0-side-effectp(M, w)= (replaceN (atomsplit(p,b)(N ),M '), w') where N = b-sub(M ').
Again, the operation respect the Kt45 -properties.
Lemma 3.21  If	(M, w)	is	a	Kt45-model,	then	so	is	the	model
0-side-effectp(M, w).


a, b, c	a, b, c
a, c
s'	t'


a, b, c, d


a, b, c, d	a, b, c,d 
a	a
d	s	d
c
''	b, c, d ''	''

s	a, c	t	b	u
s	t	u
a, c	b

a, b, c, d	a, b, c, d	a, b, c, d
a.	(M, s)	b.	(M ', s)
Fig. 6.
Similarly to Lemma 3.16 and Lemma 3.17 we have the following result.
Lemma 3.22
If (M, w) is a Kt45-model, then so is 0-side-effectp(M, w).
Given a model (M, w) and the model (M ', w')= 0-side-effectp(M, w), it holds that
(M ', w') |= Ba(Bbp V Bbчp)
(M ', w') |= Baφ iff (M, w) |= Baφ for φ objective
(M ', w') |= BaCabc(Bbp V Bbчp)
(M ', w') |= Biφ iff (M, w) |= Biφ for i /= a
(M ', w') |= BaBdφ iff (M, w) |= BaBdφ
Parts (i) to (iv) compare to the properties given in Lemma 3.17. Part (v) states that the knowledge of agent a about agent d has not changed, exactly as desired.

Example 3.23 Recall the model (M, s) from Example 3.8 (Figure 3.a). We now present this model with four agents {a, b, c, d} in Figure 6.a such that π(s)(p) = π(u)(p) = true and π(t)(p) = false.  Now, we apply 0-side-effectp(M, w) and we obtain the model (M ', s) from Figure 6.b such that π(s)(p) = π(s')(p) = π(s'')(p) = π(u'')(p) = true, π(t')(p) = π(t'')(p)= false. Note that in the latter model, in agreement with clause (v) of Lemma 3.22b a still knows exactly the same about d as it did before.

A logical language for security protocols
In this section we exploit the ideas of the previous section for a logical language to reason about security protocols. The expand and side-effect operations are used for its semantics. The usage of the language is illustrated for the


case of the SRA Three Pass protocol. We describe how the various steps of the protocols change the initial knowledge of the agents involved and what Kripke-structure is finally obtained.

Definition 4.1 Fix a set of proposition У, ranged over by p, and a set of agents A of m elements, ranged over by a. The language LC is given by
φ ::= p | чφ | φ1 Λ φ2 | Biφ | [σ]φ
σ ::= Priv(i → j, p) | Pub(i, p) | σ; σ'

The σ symbol denotes a (possibly composed) communication action. The action Priv(i → j, p) is a private or peer-to-peer message p from i to j; the action Pub(i, p) means a public announcement or a broadcast by i about p. In the latter every agent on the network learns p, whereas in the former only j learns p. The bracket operator [σ]φ has the interpretation that after executing the communication action, φ holds.
The subscript in LC refers to a set of so-called transition rules C. The transition rules capture the updates, i.e. the updates and side-effects, necessary for the interpretation of the constructs Priv(i → j, p) and Pub(i, p). The transitions rules enforce consistency among the propositions that hold. For example, if an agent believes that the value of a message m is [m ] and possesses a key k(a), then it must believe that the value of the encryption Ek(a)(m) of m has a value that corresponds with [m]].
A transition rule has either the form Bip ⇒ β or Hip ⇒ β. Bip and Hip are conditions, expressing that p must be believed by agent i or that p has been delivered to agent i, respectively. The body β of a transition rule is a sequence of actions α1; ... ; αn. Actions come in two flavours, viz. LBp and Si,jp. Here, an action LBp expresses that p is learned among the agents in the set B and corresponds to belief update, whereas an action Sa,bp expresses the side-effect that the agent a has learned that agent b now knows about p.
As an example, we will have the transition rule Bb{x}k ⇒ Lab{x}k, when agents a and b share the key k and a sends b the message {x}k. A typical application of an Hip ⇒ β transition rule is in a protocol that uses a challenge. In the situation described above, agent a sends the message x to agent b and agent b returns the message {x}k. Since it is shared, a already can compute
{x}k itself, so the delivery of {x}k does not teach a anything about this value. But, since it must come from b, the agent a does learn that b has the shared key and authenticates b toward a. The transition rule in this case is Ha{x}k ⇒ Sa,bk, stating that agent a, on observing {x}k, learns that agent b knows the

right value of the key k.
The semantics for the language LC, provided in the next definition, follows the set-up of [8,10]





Definition 4.2 Let C be a set of transition rules. For σ З LC the relation [[σ]] on models for A over У is given by
(M, w)[[Priv(i → j, p)]](M ', w')
e (M, w) |= Bip ⇒ (expandp,j(M, w) dp (M ', w'))
(M, w)[[Pub(i, p)]](M ', w')
e (M, w) |= Bip ⇒ (expandp,A(M, w) dp (M ', w'))
(M, w)[[σ; σ']](M ', w')
e (M, w)[[σ]](M '', w'')[[σ']](M ', w') for some model (M '', w'')
(M, w) dp (M ', w') e
if (x ⇒ β) З Mod(M, w, p)
then (M, w) ⟨β⟩ (M '', w'') dp (M ', w') for some (M '', w'')
else (M, w)= (M ', w')
(M, w) ⟨⟩ (M ', w') e (M, w)= (M ', w')
(M, w) ⟨LBp; β⟩ (M ', w') e expand(p,B)(M, w)⟨β⟩(M ', w')
(M, w) ⟨Si,jp; β⟩ (M ', w') e side-effect(p,i,j)(M, w) ⟨β⟩ (M ', w')
(M, w) |= p e π(w)(p)= true
(M, w) |= чφ e (M, w) $ φ
(M, w) |= φ Λ ψ e (M, w) |= φ and (M, w) |= ψ
(M, w) |= Biφ e (M, v) |= φ for all v such that wRiv
(M, w) |= [σ]φ e (M ', w') |= φ if (M, w)[[σ]](M ', w')


Mod(M, w, p)= 
{ Bip ⇒ β З C | (M, w) ⟨β⟩ (M ', w'),
(M ', w') |= φ ↔/  (M, w) |= φ, (M, w) |= Bip }∪ 

{ Hip ⇒ β  З C | (M, w) ⟨β⟩ (M ', w'),  (M ', w') |= φ ↔/
(M, w) |= φ }

The operation dp applies a number of transition rules after the execution of a communication action. For the transition rules of type Bip ⇒ β it is checked if the precondition Bip holds.
The selection Mod(M, w) is organized in such a way that no transition rule is applied over and over again. The recursive definition of d therefore is well-defined, since it stops if no fresh transition rule can be applied. In the definition of Mod it is checked if the belief of the agents changes under the transition rules, preventing an infinite chain of rewrites for dp . Note that because of the results of the previous section, the order of applying these transition rules does not matter.

The SRA Three Pass protocol
In this section we discuss how the machinery developed above works out for a concrete example. Preparatory for this, in order to keep the models within reasonable size, we employ two helpful tricks. The first one is the disregarding of propositions not known to any agent. Thus, if a proposition is not part of the model, then the interpretation is that no agent has any knowledge about it. What we than have to specify is how we add that proposition into the model. We accomplish this by making two copies of the original states. One of them we assign ‘positive’ and the other ‘negative’. In the positive states, the proposition will be true, and in the negative states, the proposition will be false.
Definition 5.1 Given a model (M, w)= ⟨S, π, R1, ..., Rm⟩ we define the func-
tion addatomp such that (M ', w')= addatomp(M, w)= ⟨S', π', R' , ..., R' ⟩
1	m
where
S' = pos(S) ∪ neg(S)
π'(pos(s))(q)= if p = q then true else π(s)(q)
π'(neg(s))(q)= if p = q then false else π(s)(q)
R'(α(s), β(t)) e Ri(s, t), α, β = pos, neg
w' = pos(w)
We have the following property.


Lemma 5.2 Given a model (M, w) and (M ', w')= addatomp(M, w) it holds that
(M ', w') |= p
(M, w) |= φ e (M ', w') |= φ for p /З φ∗ with φ∗ the closure under subfor- mulas of φ
for all i З A: (M ', w') |= Bip.
The second trick helps to prevent the useless applying of rules which keeps the model in a reasonable size.
Lemma 5.3 Given a model (M, w), the model (M ', w') such that
(M, w)⟨Lip; Sjip⟩x[Pub(i, p)](M ', w')
(for some x, i, j) and the model (M '', w'') such that (M, w)⟨LAp⟩(M '', w'') are bisimilar.
That is to say, if an agent i learns p and then all other agents learn about i that it has learned p, followed by the action where everyone learns p (commonly), then it is equivalent to say that they have just learned p commonly.
Shamir, Rivest and Adelman have suggested the three-pass protocol [9] for the transmission a message under minimal assumptions for commutative en- cryption. It is known to be insecure and various attacks have been suggested. However, it serves an illustrative purpose here. The protocol has the following steps:
a → b : {x}ka
b → a : {{x}ka }kb
a → b : {x}kb
Both agent a and b have their own encryption key, ka and kb, respectively. The encryption key can be a symmetric key or a the private key of a private key-public key-pair for that matter. Agent a wants to send message x to agent b through an insecure channel and therefore wants to send x encrypted to b. It does this by sending x encrypted with its own key. Next, b will encrypt this message with b’s key and sends this back. Since the encryption is commutative, a can now decrypt this message and sends this to b. Finally, b can decrypt the message it has just received and learn the value of x.
We consider three agents {a, b, i} where a and b are honest agents that will run this protocol and i is the intruder that looks at the messages that are being transmitted through the network. So we’re interested, if i does not actively attack the protocol, what i can learn during the run of this protocol.


Next, we define the transition rules. The first transition rule models the fact that agents can encrypt with their own key: BjmK ⇒ LjmK+j; SijmK+j and, for j З K, BjmK ⇒ LjmK−j; SijmK−j, for all j З A, where K is some set of agents, + a function that adds an agent and − a function that deletes one. Here, mK represent a message successively encrypted with the keys of the agents in K.  In the modeling, we limit ourselves by defining the list of useful propositions. The propositions we want to consider here are У =
{m, ma, mb, mab} where ma abbreviates {x}ka = [[{x}ka ]] and mab abbreviates
{{x}ka }kb = [[{{x}ka }kb ]].
We assume that
encryption is commutative, i.e. {{x}ka }kb = {{x}kb }ka (in the actual world);
every agent has its own key not possessed by any other agent;
during the run of the protocol, the value of the keys do not change.


a, b, i

m, ma
b, i
b, i

b, i




b, i
b, i m, ¬ma
b, i





a, b, i	b, i




b, i

¬m, m b, i
¬m, ¬ma
b, i
m, ma
b, i
¬m, ma

a.	starting point	b.	after Pub(a, ma)
Fig. 7.
The next assumption we must make is about the knowledge state of the agents before the run of the protocol. We will assume that a is the only agent that knows m and ma. Furthermore, we will assume that the other agents know this about a. The corresponding Kripke structure is in Figure 7.a.
The first step is executed. That is, ma is propagated on the network, so all agents will learn this value. Thus, we execute the action Pub(a, ma). If we discard the states that become unreachable, this results in the model of Figure 7.b Note that this model models Bbma. This triggers one of the transition rules, that is, it triggers Bbma ⇒ Lbmab; Sibmab since the antecedent holds in the point now.
We have not modelled mab yet, so this is the first step. We will not repeat ma in the figure since this holds in any state of the model. The function addatommab results in the model of Figure 8.a. Observe that in the next step of the protocol LAmab (Pub(b, mab)) is executed, since the message is



a, b, i
m, mab b, i b, i
a, b, i




b, i
b, i
¬m, mab

a, b, i





a, b, i	b, i




b, i

m, ¬m b, i
¬m, ¬mab
b, i
m, mab
b, i
¬m, mab

a.	added mab	b.	after Pub(b, mab))
Fig. 8.

being transmitted to all agents on the network. So with Lemma 5.3 it is justified to skip the steps that are required by the transition rules. Thus, we get the model in Figure 8.b. This results in the triggering of the transition rule: Bamab ⇒ Lamb; Siamb. This is in fact a completely similar case as in the previous step of the protocol. Again we dismiss the mab proposition since every agent has learned this.

a, b, i	b, i


m, mb
b, i
¬m, mb


Fig. 9. after P ub(a, mb)


We introduce mb and execute Pub(a, mb) because we can again skip the actions in the transition rules (Lemma 5.3). So we end up with the model in Figure 9. The last transition rule that is triggered is Bbmb ⇒ Lbm; Sibm. Again, we discard the proposition that holds in every state: mb. We now only focus on the most interesting proposition m. First b learns m which results in the model in Figure 10.a.
The second action for the transition rule is that i learns that Bbp V Bbчp. If we execute this on the model we get the model (M ', w') which can be seen in Figure 10.b. Recall that we have

(M ', w')= [Pub(a, ma); Pub(b, mab); Pub(a, mb)](M, w).

It holds that: (M ', w') |= чBim, (M ', w') |= Bbm and (M ', w') |= Bi((Bbm V
Bbчm) Λ Ba(Bbm V Bbчm)).


b	a, b, i

a, b, i	a, b, i
b	a, b, i	a, b, i
a.	after Lbm	b.	after Sibm
Fig. 10.


Conclusion

Inspired by recent work on dynamic epistemic logics, we have proposed a log- ical language for describing (properties of) runs of security protocols. The language contains constructs for the two basic types of epistemic actions that happen during such runs. The semantics of the language is based on tradi- tional Kripke models representing the epistemic state of the agents involved in the protocol at hand. Changes in the epistemic state of the agent system as a result of the execution of a protocol are described by means of transi- tion rules that precisely indicate what belief updates happen under certain preconditions. These belief updates give rise to modifications of the models representing the agents’ epistemic state in a way that is precisely given by semantic operations on these models. We have illustrated our approach for a well-known security protocols such as the SRA Three Pass protocol. We also have analyzed the Needham-Schr¨oder public key protocol and Andrew RPC, see [15]. It should be noted, that we focus here on single protocol runs with passive intruder. A further research goal is to extend our approach to deal with a setting of multiple protocols/multiple runs and active intruder.
The semantic updates we used operate on traditional Kripke models as opposed to updates in the approaches of Gerbrandy and Baltag. We believe that this will make it less troublesome to integrate these updates into existing model checkers, which hopefully will lead to better and new tools for the verification of properties of security protocols.
Although future research will have to justify this, we are confident that our method can be employed for a broad class of verification problems concerning security protocols because of the flexibility of our approach using transition rules for epistemic updates.

References
N. Agray, W. van der Hoek, and E.P. de Vink. On BAN logics for industrial security protocols. In B. Dunin-Keplicz and E. Nawarecki, editors, From Theory to Practice in Multi-Agent Systems, pages 29–38. LNAI 2296, 2001.
R.J. Anderson. Security Engineering: A Guide to Building Dependable Distributed Systems. Wiley, 2001.
M. Abadi and M. Tuttle. A semantics for a logic of authentication. In Proc. PODC’91, pages 201–216. ACM, 1991.
A. Baltag.  A logic for suspicous players: Epistemic actions and belief-updates in games.
Bulletin of Economic Research, 54:1–46, 2002.
M. Burrows, M. Abadi, and R.M. Needham. A logic of authentication. ACM Transactions on Computer Systems, 8:16–36, 1990.
A.  Bleeker  and  L.  Meertens.	A  semantics  for  BAN  logic. In DIMACS Workshop on Design and Formal Veriﬁcation of Protocols. Rutgers University, http://dimacs.rutgers.edu/Workshops/Security, 1997.
A. Baltag and L.S. Moss. Logics for epistemic programs. Synthese: Knowledge, Rationality and Action, 139:165–224, 2004.
A. Baltag, L.S. Moss, and S. Solecki. The logic of public announcements, common knowledge and private suspicions. In Itzhak Gilboa, editor, Proc. TARK’98, pages 43–56, 1998.
J.A. Clark and J.L Jacob. A survey of authentication protocols 1.0. Technical report, University of York, 1997.
H.P. van Ditmarsch. Knowledge games. PhD thesis, ILLC Dissertation Series 2000-06. University of Groningen, 2000.
H.P. van Ditmarsch.  The semantics of concurrent knowledge actions.  In M. Pauly and
G. Sandu, editors, Proc. ESSLLI Workshop on Logic and Games, Helsinki, 2001.
D. Dolev and A.C. Yao. On the security of public-key protocols. IEEE Transaction on Information Theory, 29:198–208, 1983.
J. Gerbrandy. Dynamic epistemic logic. Technical Report LP–97–04, ILLC, 1997.
J. Gerbrandy. Bisimulations on Planet Kripke. PhD thesis, ILLC Dissertation Series 1999-01. University of Amsterdam, 1999.
A.J. Hommersom. Reasoning about security. Master’s thesis, Universiteit Utrecht, 2003.
V. Kessler and H. Neumann. A sound logic for analyzing electronic commerce protocols. In J.-
J. Quisquater, Y. Deswarte, C. Meadows, and D. Gollman, editors, Proc. ESORICS’98, pages 345–360. LNCS 1485, 1998.
B. Kooi. Knowledge, Chance, and Change. PhD thesis, ILLC Dissertation Series 2003-01. University of Groningen, 2003.
J.-W. Roorda, W. van der Hoek, and J.-J.Ch. Meyer. Iterated belief change in multi-agent systems. Proceedings of the ﬁrst International Joint Conference on Autonomous Agents and Multi-Agent Systems: Part 2, 2002.
P.F. Syverson and I. Cervesato. The logic of authentication protocols. In R. Focardi and
R. Gorrieri, editors, Foundations of Security Analysis and Design, Tutorial Lectures, pages 63–136. LNCS 2171, 2000.
B. Schneier. Secrets and Lies: Digital Security in a Networked World. Wiley, 2000.
S.G. Stubblebine and R.N. Wright. An authentication logic with formal semantics supporting synchronization, revocation and recency. IEEE Transactions on Software Engineering, 28:256– 285, 2002.
G. Wedel and V. Kessler. Formal semantics for authentication logics. In E. Bertino, H. Kurth,
G. Martello, and E. Montolivo, editors, Proc. ESORICS’96, pages 219–241. LNCS 1146, 1996.
