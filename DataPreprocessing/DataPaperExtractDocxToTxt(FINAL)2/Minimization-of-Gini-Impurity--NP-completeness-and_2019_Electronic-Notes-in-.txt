Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 346 (2019) 567–576
www.elsevier.com/locate/entcs

Minimization of Gini Impurity: NP-completeness and Approximation
Algorithm via Connections with the k-means Problem
Eduardo Laber,1 Lucas Murtinho2
PUC-Rio
Rio de Janeiro, Brazil

Abstract
The Gini impurity is a very popular criterion to select attributes during decision trees construction. In the problem of finding a partition with minimum weighted Gini impurity (PMWGP ), the one faced during the construction of decision trees, a set of vectors must be partitioned into k different clusters such that the partition’s overall Gini impurity is minimized.
We show that PMWGP is APX-hard for arbitrary k and admits a randomized PTAS when the number of clusters is fixed. These results significantly improve the current knowledge on the problem. The key idea to obtain these results is to explore connections between PMWGP and the geometric k-means clustering problem.
Keywords: Impurity, Gini, k-means, NP-completeness, Approximation.


Introduction
Decision Trees and Random Forests are among the most popular methods for classi- fication tasks. It is widely known that decision trees, especially small ones, are easy to interpret, while random forests usually yield more stable/accurate classifications. A key decision during the construction of these structures is the selection of the attribute that is used for branching at each node. The standard approach for this selection is to evaluate the ability of each attribute to generate “pure” partitions, that is, partitions in which each branch is very homogeneous with respect to the class distribution of its examples. To measure how impure each branch is, impurity
measures are often employed.

1 Email: laber@inf.puc-rio.br
2 Email: lucas.murtinho@gmail.com

https://doi.org/10.1016/j.entcs.2019.08.050
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).

An impurity measure maps a vector u = (u1,..., ud), counting how many exam- ples of each class we have in a node (branch), into a non-negative scalar 3 . Arguably, one of the most classical impurity measures is the Gini impurity iGini , which mea- sures the probability of misclassification when an object is assigned to class i with probability  ui  . It is defined as:


d
iGini (u)= 
i=1
ui  1 −  ui	.

The Gini impurity is used in the CART package for classification and regression decision trees [3]. It is therefore important to understand the complexity of the problem of finding optimal partitions w.r.t. Gini impurity, and also to build heuris- tics and algorithms for this task.
Problem Definition. For a vector v where all components are non-negative, the weighted Gini impurity Gini (v) is defined as Gini(v)= v  1 · iGini (v). Let A be a nominal attribute that may take n possible values a1,..., an. The k-ary Partition with Minimum Weighted Gini Problem (k-PMWGP ) can be described abstractly as follows. We are given a collection of n non-null vectors V ⊂ Zd, where the i-th component of the j-th vector counts the number of examples in class i for which the attribute A has value aj. The goal is to find a partition P of V into k groups so as to minimize the sum of the weighted Gini impurities


k
Gini (P)= 
i=1

Gini
  vΣ∈Vi

v .	(1)

A problem that is equivalent to the above one from the perspective of optimality (but different from the perspective of approximation) is finding the partition P of
V into k groups that minimizes
Gini (P) −	Gini (v).	(2)
v∈V
Using concavity properties of Gini, one can prove that the above expression is always non-negative. An α-approximation with respect to goal (2) implies an α- approximation with respect to goal (1), but the converse is not necessarily true, so that approximations with respect to goal (2) are stronger [7].
In order to properly explain our work we shall recall the definition of the classic k-means clustering. In the geometric k-means problem we are given a set of vectors V ⊂ Rd and the goal is to find a partition P of V into k groups V1,..., Vk and a set of k centers c1,..., ck in Rd such that


k
CostKM (P)= 
i=1 v∈Vi

 v − ci  2



3 In the original definition an impurity measure maps a vector of probabilities into a non-negative scalar.

is minimized.
It is well known that if U is a set of vectors then the vector c for which

Σv∈U
 (v − c)  2 is minimum is the centroid of U , that is, c = (Σ

v∈U
v)/|U|.

Our Results. We show that PMWGP is APX-hard for arbitrary k and admits a randomized PTAS when the number k of clusters is fixed. These results significantly improve the current knowledge on the problem.
The key idea to obtain these results is to explore connections between PMWGP and the k-means clustering problem. In fact, the hardness of PMWGP relies on a reduction from the vertex cover problem in triangle-free graphs to the k-means clustering problem due to Awasthi et al. [2]. Furthermore, our randomized PTAS for PMWGP is a simple adaptation of the randomized PTAS for the k-means problem presented by Kumar et al. [9] and further explored by Ackermann et al. [1].
Related Work. There have been theoretical investigations on methods to compute the best split efficiently for impurity measures such as the Gini impurity. For d = k = 2, Breiman [3] presented a simple algorithm that finds the best binary partition in O(n log n) time for impurity measures in a certain class that includes Gini. The correctness of this algorithm relies on a theorem, also proved in [3], which is generalized for arbitrary d and k in Chou [5], Burshtein et al. [4], and Coppersmith et al. [7]. Basically, these theorems provide necessary conditions for partitions with minimum impurity and can be used to restrict the set of partitions that need to be considered. A connection between Gini and the squared l2 distance employed by k-means, that we explore here, is mentioned in the appendix of Chou [5].
The results presented here belong to the research project of the first author and his collaborators, whose goal is to obtain a better understanding, from the perspective of approximation algorithms, of clustering based on impurity mea- sures/Bregman divergences. In [10], new splitting procedures that provably achieve near-optimal impurity for binary partitions based on a family of measures that include the Gini impurity as well as the Entropy impurity are presented. In [6], ap- proximation algorithms for the same family of measures in the more general problem of k-ary partitions are given.
For the Euclidean k-means problem a vast literature is available and the problem is well understood from the perspective of approximation algorithms in the sense that the gap between the best available approximation factors and the thresholds given by the hardness results is small (see [2] and the references therein).
Paper Organization. In Section 2, we present the connections between the Parti- tion with Minimum Weighted Gini Problem (PMWGP ) and the k-means clustering problem. In addition, we use these connections to prove the hardness of PMWGP . Section 3 analyzes an algorithm that, for a fixed k and with constant probability, provides a (1 + ϵ)-approximation for PMWGP in polynomial time. We discuss the results and present further research directions in the conclusions section.

Connections between Gini minimization and k-means clustering
In this section we argue that the following connections between k-PMWGP and the
k-means problem hold:
Proposition 2.1 Let V be an instance of k-means in which all vectors have the same l1 norm. If P is an optimal partition for instance V of k-means, then P is also an optimal partition for instance V of k-PMWGP.
Proposition 2.2 There exists a pseudo-polynomial time reduction from k-
PMWGP to the geometric k-means problem.
The key observation for establishing both propositions is the following lemma.
Lemma 2.3 Let X be a set of n vectors, all of them with l1 norm equal to L. Then,

Gini Σ v − Σ Gini (v)= 1 × Σ

 v − c 2  ,

v∈X
v∈X
v∈X

where c is the centroid of the set of vectors in X.
Proof. Let u = Σv∈X v and d be the dimension of the vectors in X. We have that

Gini (u) −
vΣ∈X

Gini (v)=  u 1
Σi=1
  ui
  1 −  ui



—	v  1
v∈X	i=1
  vi
1 −	vi
v 1

Σ	(ui)2 
Σ Σ 
(vi)2 



On the other hand,
=
i=1
ui − L × n


d
—
i=1 v∈X
vi −	L	.

vΣ∈X 
2	2
2
i=1 v∈X

Thus, it suffices to show that, for any i,


(ui)2
ui − L × n −


vi −
v∈X

(vi)2


L

= 1
L
v∈X

(vi − ci)2 

.	(3)

The left side of (3) is equal to


(ui)2
Σv∈X (vi)2

1	 Σ





2	(ui)2 




Moreover, the right side of (3) is equal to
1 ×  Σ (v )2 − 2c Σ v + Σ (c )2 
		



=  ×
L
v∈X

(vi)2
(ui)2
— 2	+ n
n
(ui)2

n2
=  ×
L
v∈X

(vi)2 −
(ui)2
,
n

which establishes the lemma.	2
Proposition 2.1 is a direct consequence of Lemma 2.3, since it implies that, for all k-partitions P of V ,

1
Gini (P)=	· Cost KM
L

(P) +	Gini (v).
v∈V

With regard to Proposition 2.2, let V be an instance of k-PMWGP and let V j be the instance of k-means obtained from V as follows: for each vector v ∈ V , we add to the instance set V j exactly  v  1 copies of the vector vj = v/  v  1. Using Lemma 2.3 and also the fact that in any optimal solution for k-means identical vectors are in the same partition, we conclude that the optimal value of V and V j differ by exactly  v∈V Gini (v). Note that instance V j is obtained from V in
pseudo-polynomial time.
From Proposition 2.1 and the hardness of geometric k-means established in [2] we obtain:
Theorem 2.4 The Partition with Minimum Weighted Gini Problem (PMWGP) is NP-complete with respect to goal (1) and APX-hard with respect to goal (2).
Proof. Theorem 1.1 of [2] states that there is a constant ϵ such that it is NP-hard to approximate the k-means problem to a factor better than (1 + ϵ). In the instance used to prove the theorem in [2], all vectors have l1 norm of 2; from our Lemma 2.3, it follows that the goal (2) and the objective function for k-means differ by a factor of exactly 2. The NP-completeness of goal (1) follows because goals (1) and (2) differ by an additive constant.	2
Approximating the optimal Gini partition
The reduction given by Proposition 2.2 allows us to obtain new algorithms for k- PMWGP with provable approximation factors. As an example, we discuss how to obtain a randomized PTAS for k-PMWGP with respect to the objective function (2) when k is fixed. To do so, we run over instance V j the randomized PTAS for the k- means problem proposed by Kumar et al. [9]. Algorithm 1 (Cluster) is a slightly modified version of the algorithm as presented in Figure 1 of [1].
In what follows we explain how Algorithm 1 works and how we adapt it to our purposes. However, we do not detail the analysis of its approximation, since it is not necessary to establish our result. We refer the reader to [1] for a complete

presentation of the PTAS proposed in [9]. This presentation includes a simplified proof of its approximation factor as well as a discussion of how it generalizes to other objective functions.

Algorithm 1 Cluster(R, l, C˜)
Input:
R: set of remaining input points l: number of medians to be found C˜: set of medians already found Procedure:
1: if l =0 then return C˜
2: else
3:	if l ≥ |R| then return C˜ ∪ R
4:	else
5:	C(c˜) = {}
6:	/* sampling phase */
7:	sample a multiset S of size  2  from R
Σ		αγδ  

10:	C(c˜) → C(c˜) ∪ Cluster(R, l − 1, C˜ ∪ {c˜})
11:	/* pruning phase */
12:	let N  be the set with the  1 |R| minimal points p	∈	R  w.r.t.

min
˜ ||p − c˜||2

c˜∈C	2
13:	C(c˜) → C(c˜) ∪ Cluster(R \ N, l, C˜)
14:	return C ∈ C(c˜) with minimum cost

Cluster takes as input the set R of remaining objects to be clustered; the number l of medians yet to be found; and the set C˜ of medians already found. (Initially, R = V , l = k and C˜ = ∅.) In addition, it employs 3 parameters: γ, which is used to control the approximation factor; δ, which is used to control the probability of returning a good cluster in terms of approximation; and α, a positive constant that impacts both the running time and the approximation factor. Cluster returns the set C˜, containing k medians. The points of V can then be clustered according to their closest points in the set C˜.
If there are no medians to be found (l = 0), then the algorithm returns the set C˜. In another base case, when l ≥ |R|, each of the points in R becomes a new median. Otherwise, the algorithm considers strategies (i) and (ii), presented below, to cluster the points in R, and returns the best clustering among those built by
these strategies.
The algorithm builds a set C containing the centroids of all subsets of S with size 1/γδ, where S is a random multiset of R with size 2/αγδ. Each centroid c˜ ∈ C is added separately to C˜, and the algorithm performs |C| recursive calls with parameters (R, l − 1, C˜ ∪ c˜). This strategy corresponds to lines 7-10.

The algorithm builds a set N containing the |R|/2 points in R that are closest to the medians in C˜. Then, the procedure is recursively called with parameters (R \ N, l, C˜). This strategy corresponds to lines 12-13.
Therefore, we can define the number of calls made by Cluster as:


T (|R|, l)= ,
,

1	if l =0 or |R|≤ l
c × T (|R|,l − 1) + T (|R|/2, l) + 1	otherwise

(4)

where c is the cardinality of the set C. An important fact is that c is constant with
respect to |V | and k, since it only depends on γ, δ and α. In fact, c < 2|S| =2 2  .
Theorem 2.8 of [1] shows that Cluster executes at most n2O(k/γδ·log(1/(γδα)))
arithmetic operations. In addition, Theorem 2.5 of [1] shows that, for α <  1 ,


Pr[Cost(C˜) ≤ (1 + 8αk2)(1 + γ)OPT ] ≥
 1 − δ k


,	(5)

where Cost(C˜) is the cost of a clustering induced by the points in C˜ and OPT is the cost of the optimal solution.


Algorithm 1 for the minimization of Gini. By applying
Cluster
to the

instance V j, obtained via Proposition 2.2, we find with probability at least ((1 − δ)/5)k a partition whose Gini is not larger than (1 + ϵ) times the Gini of an optimal partition, where ϵ = 8αk2(1 + γ). Since the size of V j is pseudopolynomial on the size of V , Cluster(V j, k, {}) would have an exponential running time w.r.t. n = |V | in the worst case. This would be the case, however, if V j were represented as a simple list of vectors; we prove below that, with a more compact representation, the running time of Cluster(V j, k, {}) remains polynomial on n.
The first step is to build V j from V in polynomial time. We do this by keeping each distinct vector vj = v/  v  1 and its multiplicity ||v||1 rather than all the v∈V  v  1 vectors of instance V j. Next, we need to establish an upper bound for the running time of each call to Cluster, which we do in Lemma 3.1 below.
Finally, in Lemma 3.2, we prove that the total number of recursive calls performed by Cluster(V j, k, {}) is polynomial on n.
Lemma 3.1 Given a ﬁxed k, a single call to Cluster(V j, l, C˜) performs O(n lg n)
operations, where n = |V |.
Proof. Let W = |V j| = v∈V ||v||1. In the base case when l = 0, the algorithm returns the set of medians found, at constant cost. In the base case when W ≤ l, the algorithm assigns the W remaining points as medians; this would take O(W ) operations, but we are keeping track of the n different vectors in V j and their multiplicities, so assigning them as medians is O(n).
It remains to evaluate the complexity of calls when no base case has been reached. The only steps in Cluster that depend on the size of the input are steps 7 and 12,

and we analyze them for input V j as follows:
Step 7 samples a constant number of vectors from V j. Since the W vectors in V j are being represented as n vectors along with their multiplicities, we can sample these n vectors in proportion to their quantities, and no additional cost is incurred.
Step 12 computes the W/2 closest vectors to a given set of medians C˜. This is achieved in [1] by finding the median element of V j according to their distances to the medians, taking O(W ) operations. Instead, we can order the n unique vectors in V j according to their minimal distance to C˜, and then select the
W/2 closest vectors by checking the multiplicities of the vectors in V j. This will take O(n lg n) operations.
Therefore, the number of operations of any call of Cluster with V j as input is
O(n lg n).	2

Lemma 3.2 For the recurrence presented in (4), T (W, k) ≤ ck+1(lg ((k + 1)W ))k −
 2 
ck, where c is a constant smaller than 2  .

Proof. For the base cases mentioned above:
k = 0: T (W, 0) = 1 ≤ c(lg W )0 − c0 which holds as long as c ≥ 2;
k ≥ W ≥ 1: T (W, k) = 1 ≤ ck+1(lg ((k + 1)W ))k − ck which holds because
 1 +1 ≤ 2 ≤ c ≤ c(lg ((k + 1)W ))k.
Let W > k ≥ 1 and assume that the lemma holds for all Wj, kj with Wj < W or
kj < k. By induction, we can use recurrence (4) to show that:



T (W, k) ≤ c(ck(lg (kW ))k—1 − ck—1)+ ck+1
(k + 1)W	k
lg
2
— ck +1 

= ck+1  lg (kW ) k—1 +  lg (k + 1)W k  − 2ck + 1.	(6)


We must therefore show that (6) ≤ ck+1(lg ((k + 1)W ))k − ck. Since 1 ≤ ck if
c ≥ 1, it suffices to prove
 lg (kW ) k—1 +  lg (k + 1)W k ≤  lg ((k + 1)W ) k—1 +  lg (k + 1)W k
2	2
≤ (lg ((k + 1)W ))k


which we can do by showing that



(k+1)W  k
2
1+ (lg ((k + 1)W ))k—1 ≤ lg ((k + 1)W )	=⇒
(k+1)W  k
2
(lg ((k + 1)W ))k—1 ≤ lg
(k + 1)W 2

(k+1)W k—1 2
(lg ((k + 1)W ))k—1 ≤ 1.
(7)
Since (7) is always true, the lemma holds.	2
Theorem 3.3 For any ϵ, δ > 0 and for a ﬁxed k, Cluster provides, in polynomial time and with probability ≥ 1 − δ, a (1 + ϵ)-approximation to k−PMWGP.
Proof. Directly from Lemmas 3.1 and 3.2: since, for a fixed k, there are at most O(nk) calls to Algorithm 1, each performing at most O(n lg n) operations, the overall running time of Cluster(V j, k, {}) is O(nk+1 lg n).	2

Conclusion
We show in this paper how the connections between minimizing the Gini impu- rity of a partition and the geometric k-means problem provide new tools to better understand and work with the former problem. In particular, these connections allow us to establish hardness results for PMWGP and to find a polynomial-time, probabilistic (1 + ϵ)-approximation algorithm for the problem when k is fixed.
One natural question is whether other algorithms can be adapted to the PMWGP via the connection presented here. Since the size of a k-means instance built from a PMWGP instance can be pseudopolynomial on the size of the PMWGP instance, it may be the case that some algorithms do not remain polynomial when used through this connection, or that some known results for the k-means problem do not hold for the PMWGP . As an example, the algorithm of [8], a constant- approximation algorithm for the k-means problem, relies on the existence of a set of ϵ-centroids of cardinality O(n). It is not yet clear whether the pseudopolynomial transformation presented above would allow the PMWGP to be (approximately) solved in polynomial time by this algorithm.

References
Ackermann, M. R., J. Bl¨omer and C. Sohler, Clustering for metric and nonmetric distance measures, ACM Trans. Algorithms 6 (2010), pp. 59:1–59:26.
URL http://doi.acm.org/10.1145/1824777.1824779

Awasthi, P., M. Charikar, R. Krishnaswamy and A. K. Sinop, The hardness of approximation of Euclidean k-means, in: L. Arge and J. Pach, editors, 31st International Symposium on Computational Geometry (SoCG 2015), Leibniz International Proceedings in Informatics (LIPIcs) 34 (2015), pp. 754–
767.
URL http://drops.dagstuhl.de/opus/volltexte/2015/5117

Breiman, L., “Classification and regression trees,” Routledge, 2017. URL https://www.taylorfrancis.com/books/9781351460491


Burshtein, D., V. Della Pietra, D. Kanevsky, A. Nadas et al., Minimum impurity partitions, The Annals of Statistics 20 (1992), pp. 1637–1646.
URL https://projecteuclid.org/euclid.aos/1176348789

Chou, P. A., Optimal partitioning for classification and regression trees, IEEE Transactions on Pattern Analysis & Machine Intelligence (1991), pp. 340–354.
URL https://www.computer.org/csdl/trans/tp/1991/04/i0340.pdf

Cicalese, F. and E. Laber, Approximation algorithms for clustering via weighted impurity measures, arXiv preprint arXiv:1807.05241 (2018).
URL https://arxiv.org/abs/1807.05241

Coppersmith, D., S. J. Hong and J. R. Hosking, Partitioning nominal attributes in decision trees, Data Mining and Knowledge Discovery 3 (1999), pp. 197–217.
URL https://link.springer.com/article/10.1023/A:1009869804967

Kanungo, T., D. M. Mount, N. S. Netanyahu, C. D. Piatko, R. Silverman and A. Y. Wu, A local search approximation algorithm for k-means clustering, Computational Geometry 28 (2004), pp. 89–112.
URL https://www.sciencedirect.com/science/article/pii/S0925772104000215

Kumar, A., Y. Sabharwal and S. Sen, A simple linear time (1+є)-approximation algorithm for k-means clustering in any dimensions, in: Foundations of Computer Science, 2004. Proceedings. 45th Annual IEEE Symposium on, IEEE, 2004, pp. 454–462.
URL https://ieeexplore.ieee.org/abstract/document/1366265

Laber, E. S., M. Molinaro and F. A. M. Pereira, Binary partitions with approximate minimum impurity, in: International Conference on Machine Learning, 2018, pp. 2860–2868.
URL http://proceedings.mlr.press/v80/laber18a.html
