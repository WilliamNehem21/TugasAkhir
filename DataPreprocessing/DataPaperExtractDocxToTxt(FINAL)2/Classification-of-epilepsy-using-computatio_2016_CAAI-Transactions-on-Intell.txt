Available online at www.sciencedirect.com
ScienceDirect


CAAI Transactions on Intelligence Technology 1 (2016) 137e149
http://www.journals.elsevier.com/caai-transactions-on-intelligence-technology/
Original Article
Classification of epilepsy using computational intelligence techniques
Khurram I. Qazi a,*, H.K. Lam a, Bo Xiao a, Gaoxiang Ouyang b, Xunhe Yin c
a Department of Informatics, King's College London, Strand, London, WC2R 2LS, United Kingdom
b State Key Laboratory of Cognitive Neuroscience and Learning, School of Brain and Cognitive Sciences, Beijing Normal University, No.19, XinJieKoWai St.,
HaiDian District, Beijing, 100875, PR China
c School of Electronic and Information Engineering, Beijing Jiaotong University, Beijing, PR China
Available online 13 October 2016

Abstract
This paper deals with a real-life application of epilepsy classification, where three phases of absence seizure, namely pre-seizure, seizure and seizure-free, are classified using real clinical data. Artificial neural network (ANN) and support vector machines (SVMs) combined with su- pervised learning algorithms, and k-means clustering (k-MC) combined with unsupervised techniques are employed to classify the three seizure phases. Different techniques to combine binary SVMs, namely One Vs One (OvO), One Vs All (OvA) and Binary Decision Tree (BDT), are employed for multiclass classification. Comparisons are performed with two traditional classification methods, namely, k-Nearest Neighbour (k- NN) and Naive Bayes classifier. It is concluded that SVM-based classifiers outperform the traditional ones in terms of recognition accuracy and robustness property when the original clinical data is distorted with noise. Furthermore, SVM-based classifier with OvO provides the highest recognition accuracy, whereas ANN-based classifier overtakes by demonstrating maximum accuracy in the presence of noise.
Copyright © 2016, Chongqing University of Technology. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Keywords: Absence seizure; Discrete wavelet transform; Epilepsy classification; Feature extraction; k-means clustering; k-nearest neighbours; Naive Bayes; Neural networks; Support vector machines




Introduction
Epilepsy is a neurological condition such that it affects brain and the nervous system. It is a very commonly known neurological disorder and approximately 1% of general pop- ulation is affected [1]. Only in the UK, around 1 in 100, more than half a million people suffer from epilepsy. There can be many causes of epilepsy and sometimes it is not possible to identify them. In the domain of epilepsy, seizure is referred to as an epileptic seizure and brain is the source. During an epileptic seizure normal functioning of the brain is disturbed for that certain time period, causing disruption on signalling mechanism between brain and other parts of the body. These

* Corresponding author.
E-mail addresses: khurram_ishtiaq.qazi@kcl.ac.uk (K.I. Qazi), hak-keung. lam@kcl.ac.uk (H.K. Lam), bo.xiao@kcl.ac.uk (B. Xiao), ouyang@bnu.edu. cn (G. Ouyang), xhyin@bjtu.edu.cn (X. Yin).
Peer review under responsibility of Chongqing University of Technology.
seizures can put epilepsy patients at higher risk for injuries including fractures, falls, burns and submersion injuries, which are very common in children [2]. These injuries happen because seizure can happen anytime and anywhere without prior warning and the sufferer would continue his or her ac- tivity with an unconscious mind. If a system can effectively predict the pre-seizure phase (the transition time of the brain towards developing seizure), it could then generate an early warning alarm so that precautions can be taken by the sufferer. Absence seizure is one from many forms of generalized epileptic seizures in which larger part of the brain is disturbed. These seizures are very short and sometime may go un-notice. The patient seems confused and may not remember the seizure afterwards. The complex spike-and-wave patterns generated by the brain during these seizures can be recorded on the electroencephalogram (EEG) and a neurologist can identify the three absence seizure phases namely seizure-free, pre- seizure and seizure [3,4]. To automate this process, EEG data is converted into a digital format and fed into a computerized


http://dx.doi.org/10.1016/j.trit.2016.08.001
2468-2322/Copyright © 2016, Chongqing University of Technology. Production and hosting by Elsevier B.V. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



seizure detection (classification) system, which can automat- ically recognize the input pattern. The two core modules of this classification system are: feature extraction and design of a classifier using these features. A feature extraction method extracts the most discriminative information from the EEG recordings, which means an ideal feature can have the prop- erty of differentiating among three phases of absence seizure.
In literature, there is a long list of methods that can be used to extract features from the EEG signals. These methods include Fourier Transforms (FT); good for analysing station- ary signals, and Time Frequency Distribution (TFD); provides a tool for examining continuous segments of EEG signals. However, EEG signals are non-stationary in nature and con- ventional frequency analysis methods may not capture the full details of the brain signals. Lyapunov exponents; discussed the detection and prediction of epileptic seizure in Refs. [5], analysis of correlation structure [6], and high order spectral analysis (HOS) [7] are the examples of non-linear methods for EEG signal analysis in the domain of epilepsy. Advances in wavelet theory has also made it a very suitable for bio-medical signal processing. It has a built-in advantage of capturing repeated and irregular patterns. It can also deal with the analysis of transient and sudden signal changes [8,9]. This is possible because this technique provides variable window size, narrow at high and wide at low frequency levels. Furthermore [8], has discussed several different methods for EEG signal analysis and concluded that Wavelet Transform (WT) has more advantages over other methods. The next step in designing the classification system is to combine these extracted features with an appropriate learning method to design a classifier. These methods can be divided into tradi- tional classification methods and modern learning algorithms also known as computationally intelligent algorithms or ma- chine learning algorithms. Bayesian methods based on statis- tical theory, k-nearest neighbour (k-NN) and decision trees based on logical branching, are considered to be in the cate- gory of traditional classifiers. On the other hand, Support Vector Machine (SVM), Artificial Neural Network (ANN), k- Means Clustering (k-MC) and Self-organising Maps (SOMs) exhibit intelligent behaviour by learning the complex and non- linear patterns hidden inside the input feature vector, are considered to be computationally intelligent approaches. Below is the brief introduction about all the methods used in this research.
Classification systems based on Bayesian statistical theory have been widely and successfully used commercially [10]. This method gives a way to represent sensory evidence; fea- tures extracted from the raw data, and prior information about the problem in hand that is collected from domain knowledge. Considering the equal prior probabilities for all classes and hence, ignoring the hassle of obtaining the domain knowledge, the analysis becomes very straight forward. Although, it is a powerful and simple rule to handle and implement, yet esti- mating posterior probabilities from the data is a non-trivial task [11] and the distribution of data may not be uniform. k- NN rule as the name implies, classify an observation by giving it a label after probing the labels on all the k-NNs and making
decision based on majority voting. Usually, Euclidean distance is used to measure the distances between the neighbouring instances. This algorithm mostly provides an acceptable per- formance in many applications [10] such as visual category recognition [12] and is also very easy to implement. However, k-NN algorithm suffers due to large memory requirements and also there is no logical way to choose the best k value, which would affect the classification problem and may not yield very good results.
Inspired by the human brain functioning and architecture, McCulloch and Pitts in 1940s presented a logical threshold unit (LTU) [13]. This basic idea of LTU has been generalized in many ways since then and it is the building block for modern ANNs. In 1962 a single layer perceptron neural network was introduced by Rosenblatt [14] by extending the idea of LTU along with a trainable network with adaptive elements [15]. Despite the huge success of this neural network model, it was only limited to solve linearly separable prob- lems. A traditional feed-forward neural network [13] has three types of layers, i.e., input, hidden and output layers. Each layer consists of nodes connected in a layer-to-layer manner. The feed-forward neural networks can learn any smooth non-linear functions in a compact domain to any degree of accuracy and are considered to be universal approximators. They have many applications such as classification, prediction, clustering and approximation [13]. The drawback of this technique is that it requires a lot of parameters to be tuned for training the neural network. Also, there is no set of rules for finding the number of hidden layers and neurons in these layers. Furthermore, ordinary neural networks suffer from the “over-fitting” and “local optimization” problems [16].
The SVM method maps the non-linear and inseparable data from an input space into a higher dimensional feature space where the data would then be linearly separable [17]. This task is accomplished by utilising the concept of separating hyper planes [17]. Instead of computing a mapping function, the use of kernel function saves the computational demand especially for feature mapping function of higher dimensional space. The SVM algorithm aims to maximize the margin (the region separating the support vectors on either side of the hyperplane) and tries to find an optimal hyperplane. Hence, also called the maximal margin classifier. Although, the training parameters for SVM technique are very few, it can still be a computa- tionally time consuming and highly complex. Nonetheless, SVM has a good generalization ability, solution to the over- fitting problem and also performs well in a high dimensional feature space.
k-MC is an unsupervized learning technique that clusters samples based on similar properties. Whereas, k represents the number of clusters or classes. Within a cluster the samples are similar but different from the samples grouped in other clus- ters. This is an iterative process in which samples are grouped together according to their closet mean and cluster positions are adjusted until these positions do not change for some it- erations. The advantage of this technique is that it does not require labels with the input examples. The convergence is also faster if k is small. However, if clusters are of different



sizes, densities and not globular then the results are poor. The results may also vary depending on the initial location of the cluster centres.
From the above discussion, we can conclude that the advantage of using machine learning approaches over the traditional classification methods is that the classification system does not need to know much about the input data in the beginning but the characteristics of the input data are learnt by the learning algorithm. Once the classification system has been built, replacing a learning algorithm with any other would not require any changes in the system. Considering this advanta- geous factor, classifiers such as ANN and SVM have been widely used in designing the epilepsy detection systems [3,9,6,18]. Also, features that are highly discriminative not only yields better recognition accuracy but also speeds up the process. Recently [3], has shown a very good recognition accuracy of seizure phases by extracting features in the time domain as well as in the frequency domain using FT.
In this paper, we first present a different strategy for extracting features from the EEG signals. In the time domain local features, based on windowing filtered mechanism, and global features, based on the entire signal length, are calcu- lated. The signal is then decomposed into different levels using Discrete Wavelet Transform (DWT) and features at each level are calculated. In the next phase, we compare different ma- chine learning algorithms in terms of their recognition accu- racy based on the extracted features. Traditional classification methods provide an acceptable performance in many appli- cations and can be used as a benchmark for other techniques [10]. Comparisons have also been performed between both types of classification methods, traditional and intelligent al- gorithms. The robustness property has also been tested using noise-contaminated data.
The organisation of this paper is as follows. Section 2 ex- plains the mechanism of digital data acquisition from epilepsy patients and also discusses the process of feature extraction method for designing the epilepsy classification system. It then explains how the three computational intelligence techniques; SVM, ANN and k-MC work. Section 3 presents classification results that include design of classifiers using SVM, ANN and k-MC and evaluation of these classifiers against two traditional classification methods, NBC and k-NN. The paper ends with concluding remarks in Section 4.

Epilepsy classification system
Fig. 1 shows the design cycle of epilepsy classification system used in this research. This is an iterative process in which the system is updated based on the output produced by the learning algorithms. The details of this design scheme are discussed in the following sections.

Data acquisition

Using 10-20 international standard electrode placement system, with all 19 electrodes (FP1, FP2, F3, F4, C3, C4 P3, P4, O1, O2, F7, F8, T3, T4, T5, T6, Fz, Cz, and Pz) the data
has been collected from the epilepsy patients suffering from the absence seizure. These recordings were taken in the Peking University People's Hospital, China, and the patients (6 males
and 4 females), aged 8e21 years old, have agreed to use this
data for research and publishing results [3]. To record the neural activity, EEG data was samples at 256 Hz, filtered from
0.5 to 35 Hz bands using Neurofile NT digital video EEG system [3]. Extracted data has been divided into three phases of absence seizure; seizure-free, pre-seizure and seizure, by an epilepsy neurologist. These phases are separated based on the criteria of 1) the interval between the seizure-free phase and beginning point of seizures is greater than 15 s, 2) the interval is 0 and 2 s prior to seizure onset, and 3) the interval is the first 2 s of the absence seizure [3]. Each dataset has 110 samples and each sample size is 19 512. Fig. 2 shows the example of 19-channel EEG recordings of these three seizure phases. From the figure, seizure-free and seizure phases are quite obvious and easier to classify. However, the brain shifts from seizure-free to an absence seizure (pre-seizure phase) looks almost the same and is hard to distinguish. The generalized spike-wave discharges with a repetition rate of 3 Hz are typically associated with clinical absence seizures. More de- tails about the data collection can be located in Refs. [19e21].

Feature extraction

The purpose of this stage is to eliminate the redundancy in the EEG signals by selecting the discriminative features from the raw data. The process also helps in reducing the size of the input feature vector. In the case of EEG recordings, we have 19 channels from the 19 EEG electrodes (sensors) as shown in Fig. 2. The research in absence epilepsy has demonstrated that not all 19 channels are of the same importance. Successful experiments in Ref. [3] has also established that only 10 electrodes (FP1, FP2, F3, F4, C3, C4, F7, F8, T3 and T4) out of total 19 have the properties that can help in classifying the three seizure phases. A feature vector was then formed based on these 10 selected channels.
To capture the brain functioning over a period of time, local and global values of the same features were calculated from the EEG signals in the time domain. Global values were calculated based on the entire signal length and local features were extracted using a windowing function. The importance of the features in the time domain is evident from Fig. 2, in which there are no sudden or abrupt changes in the brain recordings during the seizure-free and pre-seizure phases. Such activities are very well captured by the time domain analysis. Energy (E), range (R), standard deviation (SD), sum of absolute values (SAV), mean absolute values (MAV) and variance (Var) of each selected channel were extracted in the time domain to form the part of the feature vector.
DWT decomposes the signal into several levels and each level represents a particular coarseness of the signal. At each level, the signal is passed through the high pass filter (HPF), which acts as the mother wavelet, and the low pass filter (LPF) that acts as a mirror version of the corresponding HPF. The output of each level is the downsized signal by a factor of 2.



Fig. 1. Design cycle of epilepsy classification system.

Fig. 2. EEG recordings. Seizure-Free (A), Pre-Seizure (B) and Seizure (C) phases.


Approximation (A1), consists of low frequency components, and detail (D1), consists of high frequency components, are the coefficients produced by LPF and HPF respectively. For the next level of decomposition, A1 is further decomposed and the whole process is repeated until the desired level of decomposition is achieved. For one dimensional signal x t the continuous wavelet transform (CWT) is defined by Morlet- Grossman [22]:
∞
process continues until the desired level is reached. Fig. 3 depicts the DWT decomposition scheme of a signal of length n and Y2 shows that the signal is downsized by a factor of 2 at each level.
We have used Daubechies D4 wavelets, which is most widely used DWT for EEG signal analysis. D4 uses four scaling and wavelet coefficients [9]. The scaling coefficients (LPF) are:

 1 
W(a; b)= 
—∞
x(t)j*(t — b) dt	(1)

where j is the analysing wavelet, a is the scalar parameter, and b is the position parameter. The CWT is converted into discrete wavelet by using binary scale factors. Hence, from (1) [23]:


W j; b
=  1  Z
x t j*(t — b) dt	(2)

(	)	,2ﬃﬃﬃjﬃ	( )	a


Which means A1 and D1 are the first split of signal x t . Then from using A1 the next split is A2 and D2 and the


Fig. 3. DWT decomposition scheme.

1	,3
= 4,ﬃ2ﬃ
3	,3
1 = 4,ﬃ2ﬃ
3	,3
2 = 4,ﬃ2ﬃ
1	,3
3 = 4,ﬃ2ﬃ

the features vector. After the learning (training) process has finished any new data sample is classified based on the

And the wavelet coefficients (HPF) are:
g0 = h3; g1 = —h2; g2 = h1; g3 = —h0
EEG signal has been decomposed into 5-levels and at each level energy, range and standard deviation has been calculated. In addition to these features, waveform length (WL), average amplitude change (AAC) and difference absolute standard deviation (DASD) [24] have also been computed at each level of decomposition to form the rest of the feature vector. Combining all the features from time domain as well as from the decomposed signal resulted in a large feature vector, which is considered to be computationally expensive in the classifi- cation process and can also generate poor recognition accu- racy. Fig. 4 depicts the whole process of feature extraction discussed above. As from Section 2.1, the individual electrode
trained classifier. This phase is called classification phase.
According to the learning procedure, these techniques can be divided into supervised and unsupervised learning. In su- pervised learning, the algorithm takes example inputs (training dataset) and their corresponding output (class la- bels). It then learns a general rule of mapping inputs to outputs. SVM and ANN follow this strategy. In an unsuper- vised learning mechanism, no output is attached with the input examples. The data is grouped and clustered based on their natural or similar characteristics. k-MC is an unsuper- vised learning algorithm. In this research, we have explored SVM, ANN and k-MC.

Support vector machine
A binary class classification problem y(i)2[+1; —1] with a

provides 512 brain readings (values) and only 10 channels are selected to build the feature vector. Hence, form each elec- trode, 192 local values using a window size of 16 and 6 global
training set	X(i) N
i=1
(class labels) y(i) N
i=1
2঩d , and the corresponding outputs can be stated as:

feature values are extracted. The values of individual electrode is then decomposed to 5-levels. Dj/DL represent the detail values and AL represents approximate values as shown in
WT X(i) + w0
≥ 1  ci : y(i) = 1	(3)

Fig. 3. Where j  1/L and L  5. Features are extracted at each level giving 36 values per electrode. Principal component analysis (PCA) has been used to reduce the dimension (d ) of the feature vector. A 40 dimensional feature vector is decided
WT X

(i)
+ w0 ≤ —1 ci : y

(i)
= —1	(4)

to be used as an input to design the classifier and to perform the classification experiments.

Computational intelligence techniques

Intelligent techniques take feature vector as an input and use a learning strategy these algorithms learn the patterns in
Where W2 d is the weight vector and w0 is the bias. How- ever, not all the classification problems are linearly separable. Such problems are solved using the soft-margin method and the classifier is known as soft-margin classifier. Maximising
the margin and minimising the number of misclassified points, we can formulate the constraint optimization problem for the soft-margin method as:




Fig. 4. Feature extraction process.




min
W;w0 ;z
J(W; z)= 
1	2	N
2||W|| + C

z(i)

subject to : y(i) WT X(i) + w0 ≥ 1 — z(i)
z(i) ≥ 0 and i = 1; 2; ….; N
(5)



Where z is a slack variable, which represents the upper bound training error and C > 0 trades off margin size and training error. Replacing C with 0 in (5) becomes the linearly separable
case. Non-linear Support Vector Classifiers (SVCs) use the kernel trick, which maps the original feature space to another higher dimensional space and through this mapping process the classification task may become easier. In this study we use the following three kernel functions [25].


Fig. 5. Three-class SVM-BDT.


chooses final class label with majority votes. This whole process is summarized in Fig. 6.
C-binary SVM classifiers are needed for C-class problem when using OvA method. Therefore, 3 binary classifiers for the problem in hand. Three-class problem is again divided into

Linear : K
i	i '	i  i '
j  j
j=1
		Xd	!q
(6)
two-class classification problem and the training procedure is shown in Fig. 7, where class 1 is seizure-free, class 2 is pre- seizure and class 3 is seizure phase. A new sample X(*) is passed through all three SVMs and the SVM having highest




RadialBasis : K





X(i); X(i)' 



= exp
j
j=1

d
— g
j



 x(i)x(i)

' 2!

Artificial Neural Networks
A fully-connected three-layer feed-forward neural network

j  j
j=1

(8)
(FFNN) is shown in Fig. 8. Each layer is connected to its
previous and the next layer. These connections are associated with connection weights. Hidden layers also contain an array

where K •; • represents a kernel function, d is the dimension of the feature vector, q is the degree of the poly- nomial and g sets the influence boundary for the radial basis
of artificial neurons, called processing units and an activation function, f • , for each neuron. A general feed-forward operation for classification purposes can be written as [16]:

kernel function. SVM is a binary classifier that can only handle two-class classification problems. To overcome this shortcoming different strategies are used to combine the bi-

g  X(i) ≡ z(i) = f

nH


p=1


wkp f
d
w
j=1
x(i) + w !
+ wk0!

(9)

nary SVM classifiers to tackle multiclass classification
problems. As in our case, we are required to classify three phases of absence seizure. We have compared three different methods to combine binary SVM classifiers, which are Bi- nary Decision Tree (BDT), One Vs One (OvO), and One Vs All (OvA).
BDT method requires C 1 classifiers for C-classes. Converting a three-class classification problem into a two- class classification problem, SVM1 is trained on samples from seizure phase (class 3) with labels, 1, and samples from seizure-free and pre-seizure phases (classes 1&2) combined together as one class with labels, 1. SVM2 is then trained on samples from seizure-free phase (class 1) with labels, 1, and pre-seizure phase (class 2) samples with labels, 1. A test sample X(*) is processed through both the SVMs and to obtain
the category of this new sample the results of both SVMs are
then combined as shown in Fig. 5. The classification accuracy depends on the SVMs in the upper levels.
C(C—1) binary SVM classifiers are required for C-classes using OvO method. Thus, for three-class problem, it requires 3 binary SVMs. SVM1 is trained on samples from class 1 (seizure-free) and class 2 (pre-seizure), SVM2 on samples from class 2 and class 3 (seizure), and SVM3 on samples from class 3 and class 1. For a new observation (sample) X(*), it
where the subscript j indexes units in the input layer and p indexes units in the hidden layer. wpj denotes the input-to- hidden layer weights at the hidden unit p. The subscript k indexes in the output layer and nH denotes the number of hidden units. d represents the dimension of the ith input sample. The advantage of using FFNN compared with SVM is that for C-classes (outputs), the network can learn C
discriminant functions zk and a training input X(i) is assigned to the function z(i) that has the maximum value from all other functions. Usually, these functions are learnt through a well-
known backpropagation algorithm. The goal of this algo- rithm is to find a set of weight values for all the connections, such that it can minimize the error between the actual and desired output. A new sample X(*) is classified using the trained FFNN.
To achieve the maximum recognition accuracy of the three seizure phases, we have tried different number of hidden layers, hidden nodes and various combination of activation functions in this study. The best recognition accuracy was achieved by using 4 hidden-layer FFNN. The first hidden layer has 22 neurons and uses radial basis activation function. The second hidden layer has an array of 8 neurons and each node uses tangent sigmoid activation function. The third and fourth hidden layers contain 10 and 4 neurons respectively, and both


 
Fig. 6. Three-class SVM-OvO.

Fig. 7. Three-class SVM-OvA.

Table 1
Class labelling scheme for FFNN.

XC	X	 2









Fig. 8. Three-layer feed-forward fully connected neural network.

layers are using logarithmic tangent sigmoid transfer function. The output layer is consisting of three neurons. Both input and output layers are using linear activation function. The FFNN takes the samples of size d as an input along with the corre- sponding class labels and then produces the output of size 3. The class labelling scheme has been shown in Table 1. The training parameters and procedure for this 4 hidden-layer FFNN architecture have been provided in the results section.

k-means clustering
The problem of clustering can be defined as the sum of square Euclidean distances of each cluster from its mean mk:
This criterion defines clusters as their mean vectors mk in
the sense that it minimizes the sum of the squared lengths of the error, X(i) mk. Where C represents the total number of clusters (classes), X(i) is the ith sample, and Dk is the kth cluster. Hence, a partition is optimal if it minimizes Je and also called minimum variance partition. In our case, the data was
clustered into three groups (seizure-free, pre-seizure and seizure phases) and a new sample was assigned to the cluster that has minimum distance from these three clusters.
It is worth mentioning that after the training process, all the parameters and weights of the classifiers are fixed values and can be used in the real-life applications. The computational cost for the classifiers to give out the results are all very low and can be ignored in most of the cases.

Classification results
From Section 2.1, each dataset (seizure-free, pre-seizure and seizure) has 110 samples. To design classifiers using extracted features from the EEG signals as discussed in



Section 2.2, and to perform the classification experiments, each dataset is divided into 22 testing samples and 88 training samples. The training dataset is used to train the classifier. After the training process, the classifier is tested using the testing dataset to evaluate its recognition accuracy for any new pattern (sample). The percentage of recognition accuracy for both training and testing datasets are calculated separately based on correctly classified samples within each individual dataset.

Classifiers

To perform the experiments with SVM-based classifiers, a soft-margin classifier has been used in all the multiclass SVM classifiers. When polynomial kernel function (7) is used, we choose q  5. When radial basis kernel function (8) is used, we choose g 0.2. These values are used in all experiments which are achieved by trial and error approach for the best performance.
The FFNN architecture discussed in Section 2.3.2 was trained with training samples along with their corresponding class labels for 30 epochs and with initial learning rate of 10—4 using Levenberg-Marquardt backpropagation algorithm. Mean squared error criteria was used to calculate the error between the outputs produced by the network and the actual class labels. For each successful step, the learning rate was decreased by 10—1 and for every failure, it was incremented by 5. The network takes 40 dimensional feature vector as an input, and produces the output of size 3. 1; 0; 0 represents an observation from class seizure-free, 0; 1; 0 corresponds to a sample from pre-seizure phase and 0; 0; 1 classifies a seizure phase.
For k-MC, 100 iterations were performed to determine the cluster centres. However, clustering is a random process, which starts with random cluster centres. During these itera- tions, if the clusters did not change their positions, new clus- ters centres were determined to avoid the local minima problem. This process replicated itself 5 times.

Evaluation

To evaluate the success of our feature extraction scheme and to equate the recognition accuracy between learning algorithms and traditional classification methods, we have also selected to perform experiments with naive Bayes and k-nn classification methods. To estimate the data distribution for naive Bayes classifier, we have employed two ways to estimate the input data distribution. The one uses the normal Gaussian function (Naive Bayes with Gaussian function NBN) and the other uses the kernel smoothing function (Naive Bayes with kernel smoothing function NBK). k-NN has been tested for four different values of k (i.e., 1, 3, 5 and 7). Class labels 1, 2, and 3 have been used for seizure-free, pre-seizure and seizure sam- ples respectively, for both the traditional classification methods. The recognition accuracy (classification performance) of all the methods are evaluated. Firstly, the experiments were per- formed using the original dataset, without any noise. In the
EEG process the noise however is inevitable due to certain uncertainties in the measurement process, such as, signal conversion, filtering, amplifying and environment conditions. To reflect these factors present in the real-life patient's clinical
data, how the underlying seizure recognition system would
behave in the presence of noise and also to investigate the robustness property of each algorithm, the data has been contaminated with different levels (0.05, 0.1, 0.2, 0.5 and 1.0) of random noise.
The recognition accuracy of all the methods are presented in Tables 2e7, where SVM-OvA represents SVM classifier with one vs all strategy, SVM-OvO is SVM classifier with one vs one method and SVM-BDT shows the results obtained using SVM classifier with binary decision tree method. Linear, polynomial and radial basis kernel functions show the results against each individual kernel method discussed in Section
2.3.1. NN is the neural network classifier consisting of four hidden layer architecture, NBC is naive Bayes classification method, which uses two different density estimation methods, normal (NBN) and kernel smoothing function (NBK). k-NN is shown with different values of k (i.e., 1, 3, 5 and 7). And finally, k-MC shows the k-means clustering results.
In Table 2, the recognition accuracy (%) of both training and testing datasets is shown where “Average” represents the average accuracy for all three classes (seizure phases). It can be seen that the highest average testing accuracy is 93.9394% which is achieved by SVM-OvO using polynomial kernel function and the corresponding average training accuracy is 99.6970%. Whereas, 1-NN has obtained the best average training recognition accuracy of 100%. On the other hand, the lowest average testing accuracy is 66.6667% which is ach- ieved by k-MC. Talking about successfully predicting the pre- seizure phase, SVM-OvO with polynomial kernel function is again leading with 90.9091% testing recognition accuracy. Also, traditional classification methods are not able to predict the pre-seizure phase with good recognition accuracy. NBC with kernel function has improved the average testing accu- racy from 69.1358% to 72.8395%. However, NBC provides a poor performance in predicting pre-seizure phase.
The fact that the real-life data will not be smooth and it will contain noise due to certain factors mentioned above. Tables 3e7 summarizes the results of testing data classification when original testing dataset is contaminated with different levels of noise. Addition of noise is a random process. Thus to collect the statistical information, each experiment was repeated 10-times on the test dataset only. These experiments were performed without training the classifier on the noisy dataset. “Worst”, “average”, “best” and “std” (standard devi- ation) show overall values for all the three seizure phases. The worst individual is the lowest individual accuracy obtained after these experiments.
Generally, the recognition accuracy is decreasing when the noise level is increasing. The recognition accuracy of SVM with polynomial kernel function has badly degraded in all the multiclass SVM methods. However, SVM with linear kernel function has shown good robustness property in the presence of different noise levels. Among all the classification methods,


Table 2
Summary of training and testing recognition accuracy for EEG signals with original datasets.
Classifier	Recognition accuracy (%). Noise level = 0	





Table 3
Summary of testing recognition accuracy for EEG signals under testing dataset subject to noise level of 0.05.
Classifier	Recognition accuracy (%). Noise level = 0.05		 Overall (seizure-free, pre-seizure, seizure)	Worst individual


Table 4
Summary of testing recognition accuracy for EEG signals under testing dataset subject to noise level of 0.1.
Classifier	Recognition accuracy (%). Noise level = 0.1		 Overall (seizure-free, pre-seizure, seizure)	Worst individual
































Table 5
Summary of testing recognition accuracy for EEG signals under testing dataset subject to noise level of 0.2.
Classifier	Recognition accuracy (%). Noise level = 0.2		 Overall (seizure-free, pre-seizure, seizure)	Worst individual


Table 6
Summary of testing recognition accuracy for EEG signals under testing dataset subject to noise level of 0.5.
Classifier	Recognition accuracy (%). Noise level = 0.5		 Overall (seizure-free, pre-seizure, seizure)	Worst individual
































Table 7
Summary of testing recognition accuracy for EEG signals under testing dataset subject to noise level of 1.
Classifier	Recognition accuracy (%). Noise level = 1		 Overall (seizure-free, pre-seizure, seizure)	Worst individual



the NN classifier has shown the maximum recognition accu- racy of 85.8943% at highest noise level of 1. From the dis- cussion and the results presented, it is fair to conclude that NN has a good generalization ability which is not very much degraded under the increasing levels of noise.

Conclusion
This paper has presented different supervised (SVM and NN) and unsupervised (k-MC) learning algorithms for classi- fying epilepsy seizure phases. Computationally intelligent techniques NN and SVM have proved to be very good in recognising and classifying the complex and complicated patterns in the input data (EEG signals). The performance of these algorithms has been compared with two traditional classification methods, NBC and k-NN. It can be concluded that NN and SVM have demonstrated the best recognition accuracy compared with traditional classification methods and an unsupervised learning algorithm, k-MC. Furthermore, NN and SVM both showed robustness property by maintaining best results even when the input data is contaminated with different noise levels. In addition, the feature extraction method is also introduced in the paper, which is able to gain rich information of the signal at a moderate dimension of the feature vector. Also, the NN method outperforms the SVM methods for some of the noisy dataset cases. In future, more classification methods based on neural networks such as self- organising maps and deep learning architecture will be explored and try to find the method, which is best at both noisy case and noise free case.

Acknowledgment
The work described in this paper was partly supported by King's College London, China Scholar Council, National Natural Science Foundation of China (No. 61172022) and
Foreign Experts Scheme of China (No. GDW20151100010).

References
P. Kwan, T.J. Brodie, Neurother 6 (2006) 397e406.
E.C. Wirrell, Epilepsia 47 (1) (2006) 79e86.
H.K. Lam, Udeme Ekong, Bo Xiao, Gaoxiang Ouyang, Hongbin Liu,
K.Y. Chan, Sai Ho Ling, Neurocomputing 149 (Part C) (2015) 1177e1187.
U. Ekong, H.K. Lam, B. Xiao, G. Ouyang, H. Liu, K.Y. Chan, S.H. Ling, Neurocomputing 199 (2016) 66e76.
S. Osowski, B. Swiderski, A. Cichocki, A. Rysz, COMPEL Int. J. Comput. Math. Electr. Electron. Eng. Emerald 26 (5) (2007) 1276e1287.
James R. Williamson, Daniel W. Bliss, and David W. Browne, Epileptic seizure prediction using the spatiotemporal correlation structure of intracranial EEG, IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), (2011) pp. 665e668.
K.C. Chua, V. Chandran, U.R. Acharya, C.M. Lim, J. Med. Syst. 35 (6) (2011) 1563e1571. Springer.
Amjad S. Al-Fahoum, Ausilah A. Al-Fraihat, ISRN Neurosci. 2014 (2014) 7. Hindawi Publishing Corporation, Article ID 730218.
Ibrahim Omerhodzic, Samir Avdakovic, Amir Nuhanovic, Kemal Dizdarevic, Kresimir Rotim, Energy distribution of EEG signal components by wavelet transform, in: Dumitru Baleanu (Ed.), Wavelet transforms and their recent applications in Biology and Geoscience, 2012, p. 310. InTech ISBN: 978-953-51-0212-0.
Anil K. Jain, Robert P.W. Dulin, Jianchang Mao, IEEE Tran Pattern Anal Mach. Intell. 22 (1) (2000) 4e37.
G.P. Zhang, IEEE Trans. Syst. Man Cybern. Soc. Appl. Rev. 30 (4) (2000) 451e462.
H. Zhang, A.C. Berg, M. Maire, J. Malik, IEEE Comput. Soc. Conf. Comput. Vis. Pattern Recognit. 2 (2006) 2126e2136.
Anil K. Jain, Jianchang Mao, K.M. Mohiuddin, IEEE Comput. Sci. Eng. 29 (3) (1996) 31e44.
G.L. Marcialis, F. Roli, Pattern Recognit. Lett. 26 (12) (2005) 1830e1839.
F. Ham, I. Kostanic, Principles of Neurocomputing for Science and Engineering, McGraw-Hill, New York, 2001.
Richard O. Duda, Peter E. Hart, David G. Stork, Pattern Classification, second ed, John Wiley and Sons Inc, U.K, 2006, p. 680. ISBN: 978-81- 265-116-7.
S. Yaman, J. Pelecanos, W. Zhu, Unifying PLDA and polynomial kernel SVMS, in: Proceedings of the IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), IEEE, 2013,
pp. 7698e7701.
Abdulhamit Subasi, Ahmet Alkan, Etem Koklukaya, M. Kemal Kiymik, Neural Netw. 18 (7) (2005) 985e997.
M. Thulasidas, C. Guan, J. Wu, IEEE Trans. Neural Syst. Rehabil. Eng. 14 (1) (2006) 24e29.
T.C. Ferree, P. Luu, G.S. Russell, D.M. Tucker, Clin. Neurophysiol. 112
(3) (2001) 536e544.
O'Keefe, M.L. Recce, Hippocampus 3 (3) (1993) 317e330.
S. Sanei, J.A. Chambers, EEG Signal Processing, John Wiley and Sons
Ltd, 2007, p. 312. ISBN: 978-0-470-02581-9.
X. Zhang, W. Diao, Z. Cheng, Wavelet Transform and Singular Value Decomposition of EEG Signal for Pattern Recognition of Complicated Hand Activities, Springer, 2007, pp. 294e303 vol. 4561 of the series lecture notes in computer science.
Angkoon Phinyomark, Pornchai Phukpattaranont, Chusak Limsakul, Expert Syst. Appl. 39 (2012) 7420e7431.
Gareth James, Daniela Witten, Trevor Hastie, Robert Tibshirani, An Introduction to Statistical Learning with Application in R, Springer Science and Business Media, New York, 2013, p. 440. ISBN: 978-1- 4614-7138-7.





Bo Xiao received bachelor and master (Hons.) degree from Chongqing University, China, in 2010 and 2013, respectively. He is currently working towards his
Ph.D. degree in King's College London. His current research interests include computational intelligence,
T-S fuzzy model based fuzzy control, polynomial fuzzy model based fuzzy control and interval type-2 fuzzy logic.



H. K. Lam received the B.Eng. (Hons.) and Ph.D. de- grees from the Department of Electronic and Informa- tion Engineering, The Hong Kong Polytechnic University, in 1995 and 2000, respectively. During the
period of 2000 and 2005, he worked with the Department of Electronic and Information Engineering at The Hong Kong Polytechnic University as Post-Doctoral Fellow and Research Fellow respectively. He joined as a Lecturer at King's College London in 2005 and currently
a Reader. His current research interests include intelli-
gent control systems and computational intelligence.
Gaoxiang Ouyang received the B.S. degree in auto- mation and the M.S. degree in control theory and control engineering both from the Yanshan University, Hebei, China, in 2002 and 2004, respectively, and the
Ph.D. degree from the Department of Manufacturing Engineering, City University of Hong Kong, in 2010. He currently serves as an Associate Professor in the School of Brain and Cognitive Sciences, Beijing Normal University, Beijing, China. His research in- terests include biosignal analysis, neural engineering, and dynamics system.



Xunhe Yin received the bachelor and doctoral degree from Harbin Science and Technology University and Harbin Institute of Technology, in 1989 and 2000,
respectively. Currently, he works in Beijing Jiaotong University, Beijing, China as Professor. His current research interests include networked control systems; communication, control, and security in smart grid; control and security of cyber-physical system; communication and control technologies in smart traffic systems; intelligent control theory with appli- cations for communication, networks and other areas.
He has published more than 50 journals and conference papers.
