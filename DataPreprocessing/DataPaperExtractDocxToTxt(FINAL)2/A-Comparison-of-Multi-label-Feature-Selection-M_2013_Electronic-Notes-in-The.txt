Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 292 (2013) 135–151
www.elsevier.com/locate/entcs
A Comparison of
Multi-label Feature Selection Methods using the Problem Transformation Approach
Newton Spolaˆor1,a,2 Everton Alvares Chermana,3 Maria Carolina Monarda,4 Huei Diana Leeb,5
a Laboratory of Computational Intelligence Institute of Mathematics and Computer Science University of Sa˜o Paulo
13560-970 S˜ao Carlos, SP, Brazil
b Laboratory of Bioinformatics Western Paran´a State University 85867-900 Foz do Igua¸cu, PR, Brazil

Abstract
Feature selection is an important task in machine learning, which can effectively reduce the dataset dimen- sionality by removing irrelevant and/or redundant features. Although a large body of research deals with feature selection in single-label data, in which measures have been proposed to filter out irrelevant features, this is not the case for multi-label data. This work proposes multi-label feature selection methods which use the filter approach. To this end, two standard multi-label feature selection approaches, which transform the multi-label data into single-label data, are used. Besides these two problem transformation approaches, we use ReliefF and Information Gain to measure the goodness of features. This gives rise to four multi-label feature selection methods. A thorough experimental evaluation of these methods was carried out on 10 benchmark datasets. Results show that ReliefF is able to select fewer features without diminishing the quality of the classifiers constructed using the features selected.
Keywords: Multi-label learning, Feature Ranking, ReliefF, Information Gain


Introduction
Feature Selection (FS) plays an important role in machine learning and data mining, and it is often applied as a data pre-processing step. FS aims to find a small number

1 This research was supported by the Brazilian Research Council FAPESP. The authors would like to thank the anonymous referees for their insightful comments on this paper. We would also like to thank Victor Augusto Moraes Carvalho and Antonio Rafael Sabino Parmezan for their help in additional analysis.
2 Email: newtonspolaor@gmail.com
3 Email: echerman@icmc.usp.br
4 Email: mcmonard@icmc.usp.br
5 Email: hueidianalee@gmail.com

1571-0661 © 2013 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2013.02.010

of features that describes the dataset as well as the original set of features does [14], providing support to tackle the “curse of dimensionality ” problem when learning from high-dimensional data. Feature selection can effectively reduce data dimen- sionality by removing irrelevant and/or redundant features, speeding up learning algorithms and sometimes improving their performance. In fact, various studies show that features can be removed without performance deterioration [22,7,31,24]. Feature selection algorithms evaluate the goodness of features in two main ways: individual evaluation and subset evaluation. On the one hand, individual evaluation is computationally less expensive, as this approach assesses individual features and assigns them weights (ranks) according to their degree of class prediction. To this end, several feature importance measures have been proposed. Nevertheless, the individual feature evaluation is incapable of detecting redundant features as they are likely to have similar rankings. On the other hand, the subset evaluation ap- proach can handle both, feature relevance and feature redundancy. However, unlike individual evaluation, in this approach the evaluation measures are defined against
a subset of features, thus showing a high computational cost.
For single-label learning, where each example (or instance) in the dataset is associated with only one class, feature selection has been studied for many years [31]. However, few results in feature selection on multi-label learning have been reported. Unlike single-label learning, each example in multi-label learning is associated with a subset of labels, i.e., each example can simultaneously belong to multiple classes. In addition, these labels are usually correlated. Multi-label learning is an emerging research topic due to the increasing number of applications where exam- ples are annotated using more than one class, such as bioinformatics [9], emotion
analysis [1], semantic annotation of media [29,2] and text mining [3].
This work proposes and experimentally evaluates four multi-label feature se- lection methods, which use the filter approach. In this approach, the goodness of a feature is evaluated irrespective of any particular classifier. We use the stan- dard multi-label feature selection approach, which consists of first transforming the multi-label data into single-label, which is then used to select features.
We propose the use of ReliefF (RF) and Information Gain (IG) as feature evaluation measures for each label, and the use of two problem transformation approaches [25], Binary Relevance (BR) and Label Powerset (LP ), to previously transform the multi-label data into single-label data.
The Binary Relevance approach transforms the multi-label dataset into many single-label datasets, one for each individual label in the multi-labels. One disadvan- tage of this transformation, is that it does not take into account label dependence, an important aspect in multi-label learning [4]. After this transformation, the con- tribution of each feature according to each individual label in the multi-labels is measured and the average of the score of all features across all labels is considered. Finally, features with an average score greater than a threshold are selected.
The Label Powerset approach directly transforms the multi-label dataset into one single-label dataset by considering each different combination of labels in the training set as a distinct class value of that single-label dataset, in which any single-

label feature selection method can be directly applied. Moreover, this approach implicitly takes into account label dependence.
The combination of the proposed feature evaluation measures and the prob- lem transformation approaches gives rise to the four multi-label feature selection methods proposed in this work: RF-BR, RF-LP , IG-BR and IG-LP .
It should be observed that the first method, RF-BR, was initially proposed in [20], although it was evaluated in few datasets. The evaluation of RF-BR using more datasets was carried out in [21], where it was experimentally compared with another feature selection method which directly measures the feature goodness in the multi-label dataset. Besides these two pieces of work, we are not aware that ReliefF has been used for multi-label feature selection.
The four methods were experimentally evaluated in 10 benchmark datasets. Results suggest that ReliefF using the BR and LP approaches, i.e., the methods RF-BR and RF-LP , are able to select less number of features which describe well the datasets.
The rest of this work is organized as follows: Section 2 briefly presents multi- label learning and Section 3 addresses feature selection for multi-label learning, as well as related work. The filter methods proposed are described in Section 4 and their experimental evaluation in Section 5. Section 6 concludes and highlights future work.
Multi-label Learning
This section first presents basic concepts and terminology of multi-label learning, followed by the description of two multi-label problem transformation methods, as well as the multi-label BRkNN [23] algorithm and the multi-label classifier evalua- tion measures used in this work.
Basic Terminology and Concepts
Let D be a dataset composed of N examples Ei = (xi, Yi), i = 1..N . Each exam- ple Ei is associated with a feature vector xi = (xi1, xi2,..., xiM ) described by M features Xj, j = 1..M , and a subset of labels Yi ⊆ L, where L = {y1, y2,... yq} is the set of q labels. Table 1 shows this representation. In this scenario, the multi- label classification task consists in generating a classifier H which, given an unseen instance E = (x, ?), is capable of accurately predicting its subset of labels Y , i.e., H(E) → Y .
Multi-label learning methods can be organized into two main categories: al- gorithm adaptation and problem transformation [25]. The first one consists of methods which extend specific learning algorithms in order to handle multi-label data directly. The BRkNN algorithm used in this work is in this category. The second category is algorithm independent, allowing the use of any state of the art single-label learning algorithm to carry out multi-label learning. It consists of meth- ods which transform the multi-label classification problem into either several binary classification problems, such as the Binary Relevance approach, or one multi-class

Table 1 Multi-label data.


classification problem, such as the Label Powerset approach. Recall that single-label learning is called multi-class classification whenever there are more than two class values, and it is called binary classification when the class values are Yes/No. Both approaches, BR and LP , are used in this work and are described next.
Binary Relevance
This approach decomposes the multi-label learning task into q independent binary classification problems, one for each label in L. In other words, the multi-label dataset D is first decomposed into q binary datasets Dyj , j = 1..q which are used to construct q independent binary classifiers. In each binary classification problem, examples associated with the corresponding label are regarded as positive and the other examples are regarded as negative. Finally, to classify a new multi-label instance BR outputs the aggregation of the labels positively predicted by the q independent binary classifiers. As BR scales linearly with size q of the label set L, it is appropriate for a not very large q. However, it suffers from the deficiency that correlation among the labels is not taken into account.
Label Powerset
This approach transforms the multi-label learning task into a multi-class learning task. To this end, LP considers each unique combination of labels in a multi-label dataset as one class value of the correspondent multi-class dataset. In other words, each Ei = (xi, Yi), i = 1..N , is transformed into Ei = (xi, li) where li is the atomic label representing a distinct label subset. In this way, unlike BR, LP takes into account correlation among the labels. However, as the number of class values of the correspondent multi-class dataset is given by the number of distinct label subsets in D, the main drawback of this approach is that some class values in the multi- class dataset may be associated with a very small number of instances, making the multi-class dataset unbalanced.
BRk NN
BRkNN is an adaptation of the lazy k Nearest Neighbor (kNN ) algorithm to clas- sify multi-label examples proposed in [23]. Despite the similarities between the al-

gorithms, BRkNN is much faster than kNN applied according to the BR approach, since only one search for the k nearest neighbors is performed by BRkNN .
To improve the predictive performance and to tackle directly the multi-label problem, the extensions BRkNN-a and BRkNN-b were proposed in [23]. Both ex- tensions are based on a label confidence score, which is estimated for each label from the percentage of the k nearest neighbors, containing this label. BRkNN-a classifies an unseen example E using the labels with a confidence score greater than 0.5, i.e., labels included in at least half of the k nearest neighbors of E. If no label satisfies this condition, it outputs the label with the greatest confidence score. On the other hand, BRkNN-b classifies E with the [s] (nearest integer of s) labels which have the greatest confidence score, where s is the average size of the label sets of the k nearest neighbors of E. In this work, we use the BRkNN-b extension.
Lazy algorithms are useful in the evaluation of feature selection methods, since the classifiers built by lazy algorithms are usually susceptible to irrelevant features.

Evaluation Measures
Unlike single-label classification where the classification of a new instance has only two possible outcomes, correct or incorrect, multi-label classification should also take into account partially correct classification. To this end, some measures, called example-based, were specifically defined for multi-label task, while others, called labeled-base, are adaptations from the single-label classification problem. A com- plete discussion on the performance measures for multi-label classification tasks is out of the scope of this work, and can be found in [25]. In what follows, we briefly describe the six multi-label evaluation measures used in this work.
Four of them, Hamming Loss, Subset Accuracy, F-Measure and Accuracy, de- fined by Equations 1 to 4, are example-based measures, where Δ represents the symmetric difference between two sets; Yi is the set of true labels and Zi is the set of predicted labels; I(true) = 1 and I(false) = 0.

1
Hamming Loss(H, D)= 
N
N
N
|YiΔZi| .	(1)
|L|
i=1

1
Subset Accuracy(H, D)= 
N
Σ I(Zi
i=1 N
= Yi).	(2)

1
F -Measure(H, D)= 
N
Σ 2|Yi ∩ Zi| .	(3)
i=1 |Zi| + |Yi|
N

1
Accuracy(H, D)= 
N
Σ |Yi ∩ Zi| .	(4)
i=1 |Yi ∪ Zi|

The remaining two measures, Macro F-Measure (Fa) and Micro F-Measure
(Fb), defined by Equations 5 and 6 respectively, are label-based measures, where

TPy , FPy , TNy
and FNy
represent, respectively, the number of true/false posi-

i	i	i	i
tives/negatives for a label yj from the set of labels L.

F (H, D)= 1 Σ
j=1

2TPy
2TPy
+ FPy

+ FNy

.	(5)

Fb(H, D)= 2 Σq	T
2 Σq
+ Σq

TPyj F

+ Σq	F

.	(6)

All these performance measures have values in the interval [0..1]. For Hamming Loss, the smaller the value, the better the multi-label classifier performance is, while for the other measures, greater values indicate better performance.
Feature Selection
Feature  selection  searches  the  feature  space  X   =   {X1, X2,
..., XM } in order to find a good subset of features Xj ⊆ X which describes the dataset as well as the original set of features X does. This section briefly describes basic feature selection approaches and concepts, as well as related work on FS to support multi-label classification.

Basic Concepts
Considering the interaction with the learning algorithm, there are three feature selection approaches: filter, wrapper and embedded [14].
The filter approach filters out irrelevant features independently of the learning algorithm. It only uses general characteristics of the dataset to select some features and exclude others. Thus, unlike wrappers explained next, filters may not choose the best features for specific learning algorithms. In addition, filters have the advantage of being fast and simple to implement. Moreover, the filter approach is the one more frequently used in research papers related to multi-label feature selection [22].
The wrapper approach requires a specific learning algorithm to evaluate and to determine which features are selected. Although it tends to find features better suited for the specific learning algorithm, it has a high computational cost since it has to call the learning algorithm for each feature set considered.
The embedded approach is the one used by some specific learning algorithms which incorporate feature selection as part of the training process, such as decision trees, to decide in each stage the feature that has the best ability to discriminate among classes.
Feature selection algorithms based on the filter and embedded approaches may return either a subset of selected features or the weights (measuring feature impor- tance) of all features.
Several feature importance measures have been proposed in the literature to evaluate the goodness of features for classification, such as the Fisher score, Chi- square, ReliefF, Gini Index, Information Gain, CFS [31] and Rough Set [19], to name a few.
The measures related to ReliefF and Information Gain used in this work and described in Section 4, for example, enable the search for features which provide a

better separability among classes and a reduction in the uncertainty, respectively. Despite the evaluation of each feature separately, the measure related to the ReliefF algorithm takes into account the effect of interacting features [6].
Related work
Feature selection has been an active research topic in supervised, semi-supervised and non-supervised machine learning, with a large number of related publications and comprehensive surveys [12,15,31]. However, most of the research related to supervised feature selection has been mainly to support single-label classification, and few results on multi-label classification have been reported. This was confirmed by a systematic review process related to multi-label feature selection we carried out in [22]. Despite the growing interest on this research topic in recent years, less than 60 related papers were found by the systematic review process. Some of these papers are addressed next.
In [18,30] the wrapper approach is directly addressed in multi-label data using evaluation measures and a metaheuristic to search for the best feature subset, while embedded feature selection in decision tree classifiers are proposed in [5,10].
However, most papers propose a previous transformation of multi-label data to single-label data, i.e., to multi-class data or binary data using respectively the Label Powerset or the Binary Relevance approach. Whenever the BR approach is used, features are independently selected in each binary data and the results are combined using, for example, an averaging approach.
After problem transformation, the filter approach is usually applied to the single- label data for which many methods have been proposed. To this end, importance measures which do not consider interaction among the features, such as Information Gain [3,27,7] and Chi-square [24], have been the most frequently used. On the other hand, in [20,21] we propose the use of ReliefF, which takes into account feature interaction.
Currently, methods which perform feature selection considering label correlation have also been proposed. The Chi-square measure is applied according to the LP approach in [24]. In [30] an evaluation measure which concerns the ranking quality between output labels is used. The Mutual Information measure is applied in [8] ac- cording to a modified LP approach [16], which also considers label dependence. The Symmetrical Uncertainty measure is extended in [13] to find relationships between all pairs of features and labels. Furthermore, in [11] it is proposed to simultaneously do feature selection and learn the labels correlation during label ranking.
Multi-label FS methods Proposed
In this work, we propose four feature selection methods which use ReliefF and Information Gain as feature importance measures. As both importance measures are originally defined for single-label data, the problem transformation approaches BR and LP are used to transform the multi-label data to single-label data, more specifically, into binary data or multi-class data respectively. Recall that the BR

approach does not take into account the correlation among the labels, while the LP
approach does. The four methods proposed are:
RF-BR: ReliefF based on the BR approach;
RF-LP : ReliefF based on the LP approach;
IG-BR: Information Gain based on the BR approach; and
IG-LP : Information Gain based on the LP approach.
The first one, RF-BR, was initially proposed in [20], where it was evaluated on few multi-label datasets. In [21] RF-BR was evaluated using more datasets and was compared with another method, which uses the new Information Gain measure for multi-label data defined in [5]. This new measure was applied directly in the multi-label dataset as a feature importance measure. Besides these two pieces of work, it is not of our knowledge the use of ReliefF for multi-label feature selection. ReliefF measures the quality of attributes of single-label data. The main advan- tage of ReliefF over other strictly univariate measures is that it takes into account the effect of interacting features. The basic idea of ReliefF is to reward an attribute for having different values on a pair of similar examples from different classes, and penalize it for having different values on examples from the same class [6,17]. For each feature, ReliefF outputs a value w, ranging from -1 to 1 with large positive w
assigned to important features.
Information Gain is a measure of dependence between one feature and the class label often used in papers related to multi-label feature selection [22,5].
The single-label IG of a feature Xj, j = 1..M , calculates the difference between the entropy 6 of the dataset D and the weighted sum of the entropy of each subset Dv ⊆ D, where Dv is composed by the examples where Xj has the value v. There- fore, if Xj has 10 distinct values in D, the weighted sum would be applied to 10 different Dv datasets. The IG measure is defined by Equation 7.

IG (D, Xj)= entropy (D) − Σ
v∈Xj
|Dv|entropy (D ) .	(7)
|D|	v

A high IG value for feature Xj indicates strong dependence between Xj and the class label.
RF-BR and IG-BR initially transform the multi-label dataset into q binary datasets. Afterwards, RF-BR using ReliefF and IG-BR using IG in the conventional way evaluate the set of features {X1, X2, ...XM } on each of the q binary datasets. The q measure values of each feature Xj,j = 1..M are then averaged, and the ones with average values greater than or equal to a ReliefF threshold or to an IG threshold respectively, are the features selected. As both methods use the standard multi-label filter approach, which considers each label separately, label correlation is not considered by these two methods.
RF-LP and IG-LP , on the other hand, use the feature importance measure directly calculated from the multi-class dataset, which was generated using the Label

6 The uncertainty inherent to the data.

Powerset approach. Thus, label correlation is considered by these two methods.

Experimental Evaluation
The four feature selection methods proposed in this work were implemented us- ing Mulan 7 [26], a package of Java classes for multi-label classification based on Weka 8 [28]. All the reported results were obtained by Mulan using 10-fold cross- validation with paired folds.

Datasets and Setup
The experiments were carried out using 10 benchmark multi-label datasets obtained from the Mulan’s repository 9 .
Table 2 shows, for each dataset, the number of examples (N ); the number of features (M ), where d indicates that the feature values are discrete and n indicates that the feature values are numeric; the number of labels (|L|); the Label Cardinality (LC), which is the average number of single-labels associated with each example defined by Equation 8; the Label Density (LD), which is the normalized cardinality defined by Equation 9; and the number of Distinct Combinations (DC) of labels.

1
LC(D)= 
|D|
1
LD(D)= 
|D|
|D|
|Yi|.	(8)
i=1
|D|
|Yi| .	(9)
|L|
i=1


Table 2
Description of the datasets used in the experiments


The specific versions of the BR and LP problem transformation approaches used in this work, as well as the algorithm BRkNN-b described in Section 2.4, are the ones available in Mulan. BRkNN-b was executed with k=5. Weka provides the implementation of ReliefF and Information Gain, which are used by the proposed

7 http://mulan.sourceforge.net
8 http://www.cs.waikato.ac.nz/ml/weka/
9 http://mulan.sourceforge.net/datasets.html

methods as feature importance measures, and were executed with default parame- ters. The threshold value for both measures was set as 0.01, which can be considered a conservative threshold.
Initially, for each dataset, the classifier constructed by BRkNN-b using all fea- tures was evaluated using the multi-label measures described in Section 2.5. These results were used as a baseline to evaluate the goodness of the feature selection meth- ods proposed. For each dataset, the four feature selection methods were executed and the classifier constructed with the selected features was evaluated.
Furthermore, for each dataset D, the Feature Reduction measure defined by Equation 10 evaluates the average reduction of features obtained by each feature selection method.

Feature Reduction(D, Xj) = 100 −
100 × |Xj|


M
.	(10)

were Xj ⊆ X is the subset of features selected from a dataset D with M examples.

Results and Discussion
Table 3 shows, for each dataset, the average Feature Reduction and its standard deviation in brackets. The five cases where all features showed a lower quality than the threshold used by the feature selection methods, are denoted by −.
Table 3
Average Feature Reduction and standard deviation.


As can be observed, aside from these 5 cases, which were all obtained by In- formation Gain as a feature importance measure, the average Feature Reduction shows a high variation. It goes from 0.00% (all features were considered important by the feature selection method) up to 99.55% for dataset 6-enron (only 0.45% of the features were considered important by the IG-BR method). Moreover, for some datasets such as 6-enron, there is a high Feature Reduction variation, going from 99.55% for IG-BR, down to 0.00% for IG-LP .
Next, for each dataset, and only using the features selected by each of the four feature selection methods, the correspondent BRkNN-b classifier was constructed and evaluated.
Table 4 shows the average multi-label evaluation measures (Section 2.5) and the correspondent standard deviation of these classifiers, as well as their correspondent baselines, given by the classifier constructed using all features. Considering the

standard deviation, light gray cells highlight the measures which showed a degra- dation compared to the correspondent baseline measure. As before, cases where all features showed a lower quality than the threshold used by the feature selection methods are denoted by −.
Table 4
Average evaluation measures (and standard deviation) using the BRkNN-b classifier.

BRkNN-b

Hamming-Loss	Subset-Accuracy

From the total of 210 performance measure values tabulated in Table 4 (4 FS methods × 6 performance measures × 10 datasets − 30 empty subset of features), only 20 of them (less than 10%), show a degradation compared to the correspondent baseline measure. This can be considered a very good result. In fact, the majority of these 20 cases refers to the Hamming Loss and Subset Accuracy measures (6 cases each). Recall that Hamming Loss, defined by Equation 1, is the relative frequency of correct labels not predicted and correct labels predicted. Subset Accuracy, defined by Equation 2, is a very strict evaluation measure as it requires the exact match of the predicted and the true multi-label to maximize its value.

To support the experimental comparison, spider graphs based on all the perfor- mance measures used in this work, where greater values indicate better performance, were generated using the R framework 10 . For each dataset, except dataset 2-cal500, which shows similar results across the performance measures, Figure 1 shows the performance of the four feature selection methods proposed, as well as their baseline (dashed line). Thus, better results are the ones plotted far away from the center and the dashed line.


bibtex
F−Measure
corel16k001
F−Measure
corel5k
F−Measure

  

Micro−averaged F−Measure
Micro−averaged F−Measure
Micro−averaged F−Measure




IG−LP	RF−BR	RF−LP	Baseline

IG−LP	RF−BR	RF−LP	Baseline

IG−LP	RF−BR	RF−LP	Baseline



emotions
F−Measure
enron
F−Measure
genbase
F−Measure

  

Micro−averaged F−Measure
Micro−averaged F−Measure
Micro−averaged F−Measure




IG−BR IG−LP

RF−BR RF−LP

Baseline

IG−BR IG−LP

RF−BR RF−LP

Baseline

IG−BR IG−LP

RF−BR RF−LP

Baseline


medical
F−Measure

scene
F−Measure

yeast
F−Measure

  

Micro−averaged F−Measure
Micro−averaged F−Measure
Micro−averaged F−Measure




IG−BR IG−LP

RF−BR RF−LP

Baseline

IG−BR IG−LP

RF−BR RF−LP

Baseline


IG−BR	RF−BR	RF−LP	Baseline

Fig. 1. Graphical evaluation of the datasets using BRkNN-b as multi-label learning algorithm.

As can be observed, RF-LP shows in general good performance, followed by RF-BR and IG-LP . On the other hand, IG-BR is the one that failed most (4 cases) in finding important features, followed by IG-LP (1 case). Furthermore, the few measure values worse than the baseline are concentrated in three datasets: 3- corel16k001, 4-corel5k and 6-enron. Observe that these three datasets have a high number of different combinations of labels in common.

10 http://www.r-project.org

In general, the experimental results suggest a relative superiority of the methods which use ReliefF as importance measure, compared with the ones that use Infor- mation Gain. This could be due to the fact that ReliefF considers the interaction among features. Moreover, there is little difference between the measures obtained by the feature selection methods built using the LP or BR approaches and the same importance measure, i.e., ReliefF or Information Gain. However, it was expected that methods using the LP approach would show better results than the ones using the BR approach, as LP takes into account label interaction. Indeed, it is expected that methods that take the interaction among labels into consideration should lead to better results [4].
Nevertheless, there are two aspects that should be jointly considered when fea- ture selection methods are evaluated: the reduction in the number of features versus the performance measure values of the classifier generated with the fea- tures selected. This sort of evaluation is better carried out by a graphical anal- ysis. In what follows, this analysis is illustrated in datasets 8-medical and 1- bibtex.  Graphs for all the datasets used in this work can be found at http:
//www.labic.icmc.usp.br/pub/mcmonard/ExperimentalResultsCLEI2012.pdf
The following figures show in the x-axis the values of the performance measure, as well as their correspondent baseline. The y-axis shows the percentage of selected features, which is given by
Selected Features =1 − Feature Reduction where Feature Reduction is defined by Equation 10.
Note that for Hamming Loss results nearer to the left-hand bottom corner of the figure are the best, as they showa low Hamming Loss value obtained with less features. On the other hand, for the other performance measures the best results are the ones nearer to the right-hand bottom corner, as they show a high measure value obtained with less features.
Figures 2 and 3 show the results for Hamming Loss. It can be observed that for dataset 8-medical — Figure 2 — the best results are obtained by IG-BR, followed by RF-LP and RF-BR. For dataset 1-bibtex — Figure 3 — the best results are obtained by RF-BR followed by RF-LP .
Figures 4 and 5 show the results for F-measure. For dataset 8-medical — Fig- ure 4 — the best results are obtained by RF-BR, followed by RF-LP . Observe that although IG-BR uses less features, its F-measure value degrades. For dataset 1-bibtex — Figure 5 — the best results are obtained by RF-BR.
For both datasets, similar results are obtained by the other measures. In con- clusion, this kind of analysis allowed us to identify the best choice among several feature selection methods for a given dataset.
Conclusion
This work proposes and analyses four feature selection methods for multi-label learning, which use the filter approach to select features. To this end, two standard







1.0




0.8




0.6




0.4




0.2




0.0

0.0	0.2	0.4	0.6	0.8	1.0

Hamming Loss

Fig. 2. 8-medical - Hamming Loss





Baseline
1.0




0.8




0.6




0.4




0.2




0.0

0.0	0.2	0.4	0.6	0.8	1.0

Hamming Loss

Fig. 3. 1-bibtex - Hamming Loss
multi-label feature selection approaches are used. The first one transforms the multi-label data to single-label data using the multi-label Binary Relevance problem transformation approach. Afterwards, the contribution of each feature according to each individual label in the multi-labels is measured and the average of the score of







1.0




0.8




0.6




0.4




0.2




0.0

0.0	0.2	0.4	0.6	0.8	1.0

F−measure

Fig. 4. 8-medical - F-measure





Baseline
1.0




0.8




0.6




0.4




0.2




0.0

0.0	0.2	0.4	0.6	0.8	1.0

F−measure

Fig. 5. 1-bibtex - F-measure
all features across all labels is considered. Finally, features with an average score greater than a threshold are selected. The second method transforms the multi- label data into single-label data using the Label Powerset problem transformation approach, in which any single-label feature selection method can directly be applied.

To measure the goodness of the features we use ReliefF and Information Gain, giving rise to the four feature selection methods: RF-BR, RF-LP , IG-BR and IG-LP .
The methods were experimentally evaluated using 10 multi-label benchmark datasets. To this end, we use the BRkNN-b multi-label learning algorithm, using the selected features to construct the classifiers for each dataset. In addition, the classifiers constructed by BRkNN-b using all features are considered as baseline for each dataset.
Results show that the methods which use ReliefF as a feature evaluation mea- sure, select more often smaller number of features than the ones that use Information Gain, with no degradation on the correspondent classifiers. This could be due to the fact that ReliefF considers interactions among features.
Although BR and LP are classic problem transformation methods in multi- label learning, and RF and IG are classic measures in feature selection, to the best of our knowledge they have not been combined before as proposed in this work. Therefore, the proposed multi-label feature selection methods could be useful for future comparisons with novel FS methods to support multi-label learning.
As future work, we plan to broaden the experimental evaluation of ReliefF using synthetic datasets. Furthermore, the analysis of potential ReliefF extensions for feature selection in order to tackle the multi-label feature selection problem directly, i.e., without any problem transformation, will also be considered.

References
Bhowmick, P. K., A. Basu, P. Mitra and A. Prasad, Multi-label text classification approach for sentence level news emotion analysis, in: International Conference on Pattern Recognition and Machine Intelligence, 2009, pp. 261–266.
Boutell, M. R., J. Luo, X. Shen and C. M. Brown, Learning multi-label scene classification, Pattern Recognition 37 (2004), pp. 1757–1771.
Chen, W., J. Yan, B. Zhang, Z. Chen and Q. Yang, Document transformation for multi-label feature selection in text categorization, in: IEEE International Conference on Data Mining, 2007, pp. 451–456.
Cherman, E. A., J. Metz and M. C. Monard, Incorporating label dependency into the binary relevance framework for multi-label classification, Expert Systems with Applications 39 (2012), pp. 1647–1655.
Clare, A. and R. D. King, Knowledge discovery in multi-label phenotype data, in: Proceedings of the 5th European Conference on Principles of Data Mining and Knowledge Discovery (2001), pp. 42–53.
Demˇsar, J., Algorithms for subsetting attribute values with Relief, Machine Learning 78 (2010), pp. 421– 428.
Dendamrongvit, S., P. Vateekul and M. Kubat, Irrelevant attributes and imbalanced classes in multi- label text-categorization domains., Intelligent Data Analysis 15 (2011), pp. 843–859.
Doquire, G. and M. Verleysen, Feature selection for multi-label classification problems, in: J. Cabestany,
I. Rojas and G. Joya, editors, Advances in Computational Intelligence, Springer-Verlag/Heidelberg, 2011 pp. 9–16.
Elisseef, A. and J. Weston, Kernel methods for multi-labelled classification and categorical regression problems, Advances in Neural Information Processing Systems 14 (2002), pp. 681–687.
Esuli, A., T. Fagni and F. Sebastiani, Boosting multi-label hierarchical text categorization, Information Retrieval 11 (2008), pp. 287–313.
Gu, Q., Z. Li and J. Han, Correlated multi-label feature selection, in: ACM International Conference on Information and Knowledge Management, 2011, pp. 1087–1096.

Guyon, I. and A. Elisseeff, An introduction to variable and feature selection, Journal of Machine Learning Research 3 (2003), pp. 1157–1182.
Lastra, G., O. Luaces, J. R. Quevedo and A. Bahamonde, Graphical feature selection for multilabel classification tasks, in: International conference on Advances in intelligent data analysis, 2011, pp. 246–257.
Liu, H. and H. Motoda, “Computational Methods of Feature Selection,” Chapman & Hall/CRC, 2008.
Liu, H. and L. Yu, Toward integrating feature selection algorithms for classification and clustering, IEEE Transactions on Knowledge and Data Engineering 17 (2005), pp. 491–502.
Read, J., A pruned problem transformation method for multi-label classification, in: New Zealand Computer Science Research Student Conference, 2008, pp. 143–150.
Robnik-Sikonja, M. and I. Kononenko, Theoretical and empirical analysis of ReliefF and RReliefF, Machine Learning 53 (2003), pp. 23–69.
Shao, H., G. Li, G. Liu and Y. Wang, Symptom selection for multi-label data of inquiry diagnosis in traditional chinese medicine, Science China Information Sciences (2012), pp. 1–13.
Slezak, D. and W. Ziarko, Attribute reduction in the bayesian version of variable precision rough set model, Electronic Notes in Theoretical Computer Science 82 (2003), pp. 263–273.
Spolaˆor, N., E. A. Cherman and M. C. Monard, Using ReliefF for multi-label feature selection (in portuguese), in: Conferencia Latinoamericana de Inform´atica, 2011, pp. 960–975.
Spolaˆor, N., E. A. Cherman, M. C. Monard and H. D. Lee, Filter approach feature selection methods to support multi-label learning based on ReliefF and Information Gain (in print), in: Brazilian Symposium on Artificial Intelligence, 2012, pp. 1–10.
Spolaˆor, N., M. C. Monard and H. D. Lee, A systematic review to identify feature selection publications in multi-labeled data, ICMC Technical Report No 374. 31 pg. (2012), University of S˜ao Paulo.
Spyromitros, E., G. Tsoumakas and I. Vlahavas, An empirical study of lazy multilabel classification algorithms, in: Hellenic conference on Artificial Intelligence (2008), pp. 401–406.
Trohidis, K., G. Tsoumakas, G. Kalliris and I. Vlahavas, Multi-label classification of music into emotions, in: International Conference on Music Information Retrieval, 2008, pp. 1–6.
Tsoumakas, G., I. Katakis and I. Vlahavas, Mining multi-label data, Data Mining and Knowledge Discovery Handbook (2009), pp. 1–19.
Tsoumakas, G., E. Spyromitros-Xioufis, J. Vilcek and I. Vlahavas, Mulan: A java library for multi-label learning, Journal of Machine Learning Research 12 (2011), pp. 2411–2414.
Wei, Q., Z. Yang, Z. Junping and Y. Wang, Semi-supervised multi-label learning algorithm using dependency among labels, in: International Conference on Machine Learning and Computing, 2009,
pp. 112–116.
Witten, I. H. and E. Frank, “Data Mining: Practical Machine Learning Tools and Techniques,” Morgan Kaufmann, 2011.
Worring, M., C. Snoek, O. de Rooij, G. Nguyen and A. Smeulders, The mediamill semantic video search engine, in: IEEE International Conference on Acoustics, Speech and Signal Processing, 2007,
pp. 1213–1216.
Zhang, M.-L., J. M. Pen˜a and V. Robles, Feature selection for multi-label Na¨ıve Bayes classification, Information Sciences 179 (2009), pp. 3218–3229.
Zhao, Z., F. Morstatter, S. Sharma, S. Alelyani, A. Anand and H. Liu, Advancing feature selection research - ASU feature selection repository, Technical Report (2011), Arizona State University.
