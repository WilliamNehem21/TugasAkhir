

Electronic Notes in Theoretical Computer Science 260 (2010) 91–108
www.elsevier.com/locate/entcs

Correct Execution of Reconfiguration for Stateful Components 
Moritz Hammer
Ludwig-Maximilians-Universita¨t Mu¨nchen
hammer@ifi.lmu.de

Alexander Knapp
Universit¨at Augsburg
knapp@informatik.uni-augsburg.de


Abstract
In component-based software engineering, reconfiguration describes structural changes to the architecture of a component system. For stateful components, not only structural but also behavioural aspects have to be taken into account in reconfiguration. We present a procedure to conduct reconfiguration in systems of concurrent, stateful components that interferes as little as possible with unchanged subsystems. Reconfig- uration is described by a plan for adding, deleting and reconnecting components. A plan is executed by a sequence of simple, local steps, which are suitable for implementation in a programming language. We prove that plan execution is indistinguishable from atomic reconfiguration and use this fact for state-space reduction for verifying properties by model checking.
Keywords: Reconfiguration, stateful components, model checking


Introduction
Software components have been proposed as a useful level of system partitioning ever since it became evident that such a partitioning would be necessary to under- stand and maintain larger software systems [11]. They are especially appealing if the configuration of components is done independently from the definition of the components themselves. It thus becomes possible to reason about the design of a software system on a level higher than actual code. It also becomes easier to reason about a redesign of the system at runtime, a process called reconfiguration [8].
We investigate a framework for reconfiguration of stateful components. To de- scribe reconfiguration, we employ plans, which are put into effect by a sequence

٨ This work has been partially sponsored by the project Sensoria IST-2005-016004.

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.12.033

of configuration rewriting steps. These steps are fine-grained enough to be imple- mented in a common programming language without having to block independent components.
Avoiding any unnecessary blocking is an important aspect of reconfiguration, since quite often, only a small part of the component system is modified, and blocking of the entire, possibly distributed system is infeasible. At the same time, executing reconfiguration in a highly concurrent scenario is hard, as it must not interfere with the normal component behaviour. Reconfiguration should hence be “perceived as atomic”, which describes that any component not directly involved in the reconfiguration should either detect that it is operating in the old or in the new
configuration, but not in any intermediate state. Combining both the “minimal invasiveness” and the “perceived atomicity” criteria are requirements for a lock-free algorithm [7].
In our case, reconfiguration is further complicated by the fact that the state of old components needs to be transferred to new components, which cannot be done automatically in every situation [21]. Like all other state-aware reconfigura- tion frameworks we know of, we pass this problem on to the user, who is required to explicitly describe how the state is to be transferred. This puts the user into an in- convenient situation: depending on the cause of the reconfiguration, state transferal can become arbitrarily complex (maybe a faulty component needs to be replaced, and its state has already become corrupted). At the same time, reconfiguration might occur only very infrequently, and thus be difficult to test.
We hence employ formal techniques to facilitate the task of defining a reconfigu- ration, and subject the plan rewriting rules to model checking. Here, the perceived atomicity becomes important since treating reconfiguration as a single step is much less costly than to have it distributed over a series of steps, with the part of the system uninfluenced by reconfiguration executing interleaving steps.
The remainder of this paper is structured as follows: Related work is discussed in Sect. 2. In Sect. 3 we illustrate our idea of reconfiguring stateful components by means of a fault-tolerance example. In Sect. 4 we give a formal account of our framework for stateful components. Reconfiguration is described in Sect. 5, introducing a notion of reconfiguration plan and how such a plan is executed by simple, local steps. In particular, we prove that the execution of a so-called shallow reconfiguration plan is not observable by components not mentioned in the plan. We apply this result to the verification of the fault-tolerant components example in Sect. 6. In Sect. 7 we close by drawing conclusions and an outlook to future work.

Related Work
Many component frameworks supporting reconfiguration have been developed, for overviews see [22,13,4]. Those that consider the component state and its retainment during reconfiguration are primarily concerned with the technical process and the algorithms; theoretical considerations, like [17,21], have only been concerned with particular aspects of stateful reconfiguration. We are not aware of frameworks with

a comprehensive formal basis that consider local component states.
We give a brief overview of state-aware reconfiguration frameworks by their way
of transferring the state. Argus [3] allows for the transferal of state with explicit
queries to accessor functions. One of the most often cited works is Polylith [8],
which requires components to implement encode and decode methods. The state can then be obtained from an old component and injected into a new one. Lim [12] uses similar state accessor methods, with a very broad definition of state.  The
work is otherwise similar to ours in that it proposes the employment of schedules
to achieve successful reconfiguration. Bidan et al. [2] and Tewksbury et al. [20] both enhance Corba with the ability to do stateful dynamic updates (the practical example of [23] also uses Corba). The former requires state accessor methods in reconfigurable components, the latter work tries to transfer the state with a 1:1 map- ping of variables, with a fallback to a user-defined mapping. Wegdam [23] employs a state translator, which is a function that is described as application dependent;
this is very comparable to our approach. He uses a notion of “mutually consistent states”, which provides an interesting criterion for verifying the correctness of state- considering reconfiguration, yet his work is not backed by a formal framework. In
Casa [15], state transferal is triggered by the reconfiguration handler. They con-
sider a very broad definition of components, however the notion of state remains quite abstract. Rasche and Polze [17] also employ a heuristic approach towards state transferal. Using an object-graph-traversal algorithm [18] and a mapping, they re- alize a semi-automatic, direct approach. For consistent state update, they rely on
the well-known formally underpinned concept of quiescence [10]. Vandewoude [21]
focuses on the transferal of state for component updates. His work is placed in the context of hot code updates, which gives some constraints on the data that need to be considered; a semi-automatic approach is presented that employs a series of
strategies to copy the state. Quiescence is weakened to tranquility for obtaining safe
reconfiguration states. CoBRA [9] uses the memento pattern to transfer the state, which amounts to encoding it into a special object and injecting it into the target component.
Most of these frameworks support interleaved reconfiguration, except [20], which prepares the reconfiguration concurrently, but performs the critical step of rewiring in an atomic switch-over. The other frameworks all use local blocking of the compo- nents that are to be removed, but a formally justified atomicity criterion is usually not given.
Model checking of component systems is done by the Vecors platform [1] and for the SOFA component framework [16], but although the latter supports recon- figuration, these two features have not been combined.

Example: A Stateful, Fault-Tolerant Component
We illustrate our approach to reconfiguration of stateful components by means of a small fault-tolerance example that nevertheless shows the capability of the method as well as the problems involved. Consider a component that stores data, maybe













The original system	(b) Reconfiguration plan description
Fig. 1. Fault tolerance components
an accounting system. We know that this component fails sometimes, and if it does, its state gets corrupted (i.e., we cannot make any safe assumption about it). We assume, however, that the component is deterministic, and that an error is communicated back to the callee immediately (e.g., in an actual implementation, by throwing an exception).
The basic idea (see Fig. 1(a)) is to maintain two copies of the unreliable com- ponent (Target1 and Target2), and to issue calls (from Client) to both of them (by Distributor); the second copy only receives the call after the call to the first copy has been completed successfully. Hence, any call issued to the second copy will successfully complete, so the state remains intact. Upon detecting a failure of the first copy, reconfiguration is employed to substitute the storage subsystem by an- other component (NewTarget, see Fig. 1(b)), which might be a different, maybe less efficient implementation, or some stub that takes care of a graceful and recoverable system shutdown.
Reconfiguration is done in several, local steps: First, the NewTarget component is instantiated, and the client’s connection to Distributor is reconnected to it. Thus, any message sent by Client during the remainder of the reconfiguration will be stored in component NewTarget. By initialising NewTarget in an inactive state, messages are not yet consumed. The reconfiguration then proceeds to update NewTarget with the data of Target2 and possibly pending messages of Distributor. Thus, the state is retained, and no messages are lost. Finally, NewTarget is activated and the now disconnected components are terminated.
The pseudo-code for the example is given in Fig. 2. S and V are the sets of component states and parameters. It is important to fix the set Em ⊆ V such that the Unsafe components (Target1, Target2) are fully deterministic: the second copy must not fail if the first one succeeded for a given parameter.
Component Framework
We describe a formal framework for concurrent, message-passing components with local state; it is inspired by the many already existing component frameworks (see [11] for an overview). The framework tries to restrict itself to just the ele- ments required to illustrate our idea of reconfiguration of stateful components. In particular, component behaviour is described by a simple process language which

focuses on local state manipulation and message passing. These processes could also be expressed in other process languages like the π-calculus [14], but the issues of state and message queues would have to be encoded.
We assume a set C of component identiﬁers, a set M of method names, that are implemented by components, a set R of rˆole names, through which components access other components, and a set V of values which are communicated between components. Interfaces, comprised in a set I , are finite subsets of method names. The local component states are drawn from a set S together with state ma- nipulating functions: upd : S × S → S for updating a state with another state; prm : S × M × V → S for storing a parameter value for a method name; and
ret : S × V → S for storing a returned value.
Definition 4.1 (Component process term) The implementation of a method (name) in a component is described by a component process term in the set P deﬁned by the following grammar:
P ∈ P ::= call(r, m, v).P | send(r, m, v).P | return(v).P
| set(σ).P | choose((Σj.Pj)j∈J ) | success | fail

with r ∈ R, m ∈ M, v ∈ V , σ ∈ S , Σj ⊆ S , and J an index set.
In a component process term, call(r, m, v) represents a synchronous invocation of method m on the roˆle r with parameter value v, where synchronous means that the


method Distributor::sync(prm) ret ← call(first, sync, prm) if (ret = err ) then fail
else call(second, sync, prm)
return ret; success fi

method Unsafe::sync(prm)
if (prm ∈ Esync)
then state ← err ; return err
else state ← susync(state, prm)
return sr (state, prm) fi success

method Safe::sync(prm) state ← susync(state, prm)
return sr (state, prm); success
method Distributor::async(prm) ret ← call(first, async, prm) if (ret = err ) then fail
else call(second, async, prm)
success fi

method Unsafe::async(prm)
if (prm ∈ Easync)
then state ← err ; return err
else state ← suasync(state, prm)
return ∗ fi success
method Safe::async(prm) state ← suasync(state, prm) success

where, for m ∈ {sync, async}, Em ⊆ V are sets of error-inducing parameter values and sum : S ×V → S are functions that produce the updated state of a component as the effect of method m, and sr : S × V → V is a function yielding the result of sync.
Fig. 2. Behaviour of the fault tolerance example

caller becomes blocked and waits for an answer by a return(v') with value v' from the callee. In contrast, send(r, m, v) asynchronously sends method name m with parameter value v to rˆole name r and does not wait for a result. The action set(σ) updates the local component state with σ, the action choose((Σj.Pj)j∈J ) selects non-deterministically some j ∈ J such that the current local component state is in Σj and proceeds with Pj. Finally, success and fail represent the successful or abnormal termination of executing a method.
Example 4.2 The pseudo-code used in Fig. 2 can be translated to component process terms in a straight-forward manner:  We define the set of data values
V to include ∗ as a “don’t care” and err as an error element.  We define
the set of local component states S as S × V × V with S a set, again with
{∗, err} ⊆ S, and we let state, prm and ret mean the first, second, and third part of a local component state.  Accordingly, the state manipulating functions

are: upd ((s, vprm, vret), (s', v'
'
ret
)) = (s', vprm, vret), prm((s, vprm, v), m, v'	) = 

(s, v'	, v) and ret ((s, vprm, v), v') = (s, vprm, v').	Then, we can translate, e.g.,
return ret resp. call(r, m, prm) to
choose(({(s, vprm, vret) | s ∈ S, vprm ∈ V }.return(vret))vret∈V ) resp. choose(({(s, vprm, vret) | s ∈ S, vret ∈ V }.call(r, m, vprm))vprm∈V ) . 
Definition 4.3 (Component) A component c is a tuple (id (c), IP (c), IR(c), μ(c), ι(c))
with id (c) ∈ C the component identiﬁer, IP (c) ⊆ I a finite set of provided in- terfaces, IR(c) : R ~ I a partial function with finite domain identifying required interfaces by roˆle names, μ(c) : IP (c) → P a method environment assigning an implementation to each method the component provides, and ι(c) ∈ S the initial state. The set of required roˆle names dom(IR(c)) is denoted by R(c).
Definition 4.4 (Component configuration) A component conﬁguration c˜ of a single component c is of the form
id (c),f (c˜), γ(c˜), e(c˜), σ(c˜),P (c˜), π(c˜)
where f (c˜) indicates whether the component is running (f (c˜) = r) or blocked (f (c˜) = 
b); γ(c˜) : R ~ C is a wiring for the required interfaces of c, telling which roˆle name points to which component; e(c˜) either contains a method call of the form c'.r'.m(v) with c' ∈ C , r' ∈ R, m ∈ M and v ∈ V , which c currently executes, or is empty (⊥); σ(c˜) ∈ S is a component state; P (c˜) ∈ P is a component process term; and π(c˜) ∈ (C × R × M × V )∗ represents the message queue, i.e., a sequence of messages, where we write :: for concatenation and ε for the empty sequence. For ease of reading, we write cf for id (c),f (c˜), and we group e(c˜) and σ(c˜) to form
⟨e(c˜), σ(c˜)⟩.
We say that γ(c˜) : R ~ C is well-connected if for all roˆles r defined in γ(c˜) the types of the provided and required interfaces match, i.e., IR(c)(r) ∈ IP (γ(c˜)(r));

cr , γ1, call(r, m, v).P  c2, π2 → cb, γ1,P  c2, π2 :: c1.r.m(v)	(CALL)
1	1
if c2 = γ1(r)
cr , γ1, send(r, m, v).P  c2, π2 → cr , γ1,P  c2, π2 :: c1.r.m(v)	(SEND)
1	1
if c2 = γ1(r)
cr , ⟨c2.r.m(v')⟩, return(v).P  cb, ⟨σ⟩→ cr , ⟨⊥⟩,P  cr , ⟨ret (σ, v)⟩	(RETURN)
1	2	1	2
cr, ⟨σ⟩, set(σ').P → cr, ⟨upd (σ, σ')⟩,P	(SET)
cr, ⟨σ⟩, choose((Σj.Pj)j∈J ) → cr, ⟨σ⟩, Pj	(CHOOSE) if j ∈ J and σ ∈ Σj
cr, ⟨σ⟩, success, c'.r.m(v) :: π → cr, ⟨c'.r.m(v), prm(σ, m, v)⟩, μ(c)(m),π	(DEQ)

Table 1 Configuration transition rules

and we say that γ is completely connected if dom(γ(c˜)) = R(c).
Example 4.5 For the fault-tolerance example, let the interface I provided by the unreliable component consist of the methods sync for synchronous and async for asynchronous invocations; and let the four components involved in the initial com- ponent graph of Fig. 1(a) be given by
(Client, {}, {store '→ I}, μClient, ιClient),
(Distributor, {I}, {first '→ I, second '→ I}, μDistributor, ιDistributor),
(Target1, {I}, {}, μUnsafe, ιTarget) and (Target2, {I}, {}, μUnsafe, ιTarget).
with arbitrary initial states ιClient, ιDistributor, and ιTarget. For the method environ- ment μClient we only assume that it keeps sending an arbitrary stream of sync and async messages to Distributor. The method environments μDistributor and μUnsafe are specified in the upper part of Fig. 2.
Definition 4.6 (Configuration) A conﬁguration C˜ of a finite set of components
C = {c1,..., cn} is a map {c1 '→ c˜1,..., cn '→ c˜n} from components to component
configurations; we write c˜1  ···  c˜n for such a map.
How configurations may change into other configurations is described by conﬁg- uration transition rules which can be applied to a configuration C˜ to yield another configuration C˜'. When defining such a rule, we abbreviate all occurring configura- tions to focus on relevant parts, in particular those that are changed. For example, the rule
cr, ⟨σ⟩, set(σ').P → cr, ⟨upd (σ, σ')⟩,P 
when applied to a configuration c˜1  ···  c˜n with c1 = c spells out to


cr, γ, ⟨e, σ⟩, set(σ').P, π
 c˜2  ··· 
c˜n → cr, γ, ⟨e, upd (σ, σ')⟩, P,π 
c˜2  ··· 
c˜n .


The configuration transition rules of our component framework are listed in Tab. 1

and reflect the informal semantics of the process actions by homonymous rules. In particular, CALL and SEND treat synchronous and asynchronous invocations differ- ently by blocking resp. continuing the caller. Although, in principle, we do not distinguish asynchronous and synchronous methods in interfaces, we assume that a method knows how it is going to be called: if it is invoked by call, it eventually needs to process a return subterm in order to unblock the caller, and if it is invoked by send, it should not contain return subterms. By DEQ a component can start processing of a new message from its queue. Note that DEQ applies only to running components and therefore prohibits circular synchronous calls: if processing a syn- chronous call from component c to component c' requires a synchronous call from c' to c, the system deadlocks. We deliberately accept this, as it preserves an important invariant: during processing a method, the state is not externally modified.

Reconfiguration
Reconfiguration is the process of changing the component graph [24]. This involves the removal and addition of components (i.e., nodes of the graph) and rewiring the components (i.e., adding and removing edges of the graph). We will, however, consider a less generic approach only: During reconfiguration, a set of components is removed and a set of components is added; only the connections of a retained component to a removed component are rewired to a new component. More complex scenarios (like a retained component becoming rewired to another retained one) can be reduced to such a simple scenario by removing and re-adding one of the retained components — for a practical implementation, it is straightforward to identify and treat such “delete and add again” scenarios. As mentioned before, the challenge of reconfiguration of stateful components is given by the need to retain the state, which consists of the data state σ ∈ S and the message queue π in our concrete component framework.
We will first introduce reconfiguration plans that describe the effect of a recon- figuration. We then introduce a set of fine-grained rules and a plan implementation that describes how these rules are applied to conduct a reconfiguration. We proceed to show that, for plans that redistribute messages in canonical way, plan implemen- tation is perceived as atomic, while stopping only a minimal number of components.

Reconﬁguration Plans
A reconfiguration of a configuration C˜ with C = dom(C˜) is described by a plan
Δ = (A, R, α, ρ, δ, ς)

with A being a set of components to be added where A ∩ C = ∅; R ⊆ C the set of components to be removed; and α, ρ, δ and ς functions that describe how components are connected (α for components in A, ρ for components in C \ R) and how the state is preserved (δ handling the messages and ς the data state).
Hence, α : A → (R → (C \ R) ∪ A) describes the connections of the new

components in A, which may be connected to both other new components and components that already exist, but do not get removed. We require α(c) to be well-connected for all c ∈ A. The partial function ρ : C \ R × R ~ A describes the rewiring of connections that get discarded because the target component is removed. Thus, ρ needs to be defined for (c, r) ∈ C \ R × R iff γ(c˜)(r) ∈ R. We require that γ(c˜)[r '→ ρ(c, r)], i.e., the connections of c with r pointing to ρ(c, r), is well-connected for c for all r for which ρ(c, r) is defined. Of course, if γ(c˜) is completely connected, then so is γ(c˜)[r '→ ρ(c, r)].
δ and ς are two functions that are used for defining how the state of an old com- ponent should be preserved. The partial function δ : R×(C×R) ~ A describes mes- sage retainment, i.e., the components that should process previously unprocessed messages of components in R. It is required that messages are moved to components that actually implement the required interface, that is IR(c)(r) ∈ IP (δ(c', (c, r))) for all (c', (c, r)) ∈ dom(δ). The function ς : (R → S ) × A → S describes the state of the new components, which is calculated from the state of the old components. This is a very general notion which subsumes more concrete, technical approaches of data state retainment [21,18,9].
Example 5.1 In the fault-tolerance-example, when the component Distributor has moved to fail, a reconfiguration has to be launched. The plan to be employed is illus- trated in Fig. 1(b) using a single push-out graph transformation rule as used in [19]; the dashed line is an “update edge” indicating state retrieval. In our notation, this plan Δ = (A, R, α, ρ, δ, ς) is represented by
A = {(NewTarget, {I}, {}, μSafe, ιTarget)} ,
R = {Distributor, Target1, Target2} ,	α = {} ,
ρ = {Client, store '→ NewTarget} ,	ς = {(r, NewTarget) '→ r(Target2)} . δ = {(Distributor, (Client, store)) '→ NewTarget}
where the method environment μSafe is specified in the lower part of Fig. 2.
Reconfiguration may only commence if all the components of R are not executing a method, i.e., their process terms are either success or fail. This is similar to the quiescent states of [10], which, roughly speaking, represent a situation where no communication is interrupted by a reconfiguration.
When a plan Δ is applied to a configuration C˜, the component set C is parti- tioned in three sets: the set R of components that get removed, a set W = dom(ρ) of components that need to have a roˆle rewired (by definition, W is disjoint to R), and the set C \ (R∪ dom(ρ)) of components that are not modified at all; after recon- figuration, the set A is added. Hence we can describe the effect of reconfiguration by the rule

R˜	W˜
→ Δ(W˜ )
A˜Δ	(RECONF)

with all components in R not performing a method, i.e., R˜ = cr , P1  ...  cr , Pn

with
success fail
1	n
and Δ(

Pi ∈ {	,	}
W ) describing the effect of the rewiring as defined

by ρ and A˜Δ consisting of new components, which are initialised using α, δ and ς.
In particular, if W˜ = cn+1, γn+1  ...  cn+m, γn+m, then Δ(W˜ ) = cn+1, γ'	 ... 
cn+m, γ'	with



'
n+i
(r) =	ρ(cn+i, r),	if ρ(cn+i, r) is defined
γn+i(r),	otherwise.


The configuration of new components A˜Δ is
A˜Δ = {c '→ cr, α(c), ⟨⊥,ς(σR, c)⟩, success, πc | c ∈ A} .

Herein, σR is the function capturing the states of components in R, i.e., for c' ∈ R and thus c˜' being part of R˜, we have σR(c') = σ(c˜'). The message queue πc is a linearisation of the parallelisation (or shuffling) of message sequences copied by δ: Let me = e(c˜t) if P (c˜t) = fail and me = ε otherwise; and let π|ϕ denote the sequence of messages of a queue π which are in ϕ. If (ct, (cs, ri)) ∈ δ−1(c), then me :: π(c˜t)|ϕ is a subsequence of πc, for ϕ ≡ {cs.ri.m(v) | m ∈ M ∧ v ∈ V }, and πc consists exactly of these subsequences. Note that the order of the subsequences is unspecified, which makes plan application nondeterministic. me is the message that produced a fail, and needs to be processed by the substituting component again.
The RECONF rule performs the entire task of reconfiguration at once, hence providing atomicity. We will now refine the rules into a sequence of much finer rules (which can rightfully be assumed to be atomic) and proceed to show that these rules can be applied in a way that is indistinguishable from the effect of the RECONF rule.

Rules for Reconﬁguration
In order to build fine-grained rules, we extend the set of component running states to {n, i, r, b, s, c}. The intended state machine for a component can be seen in Fig. 3. The new states have the following semantics:
n: A newly initialised component that needs to be connected to other components.
i: Once connected, a new component is put into this state, which is used for retrieving data and messages from old components, thus initialising the new com- ponent.











Fig. 3. State machine of a component, with rules to reach new states

C˜ → C˜
 cn, γ⊥, ⟨ι(c)⟩, success,ε	if dom(γ⊥) = ∅	(RCADD)

cn, γ1  c2 → cn, γ1[r '→ c2]  c2	if γ1[r '→ c2] is well-connected	(RCWIRE)
1	1
cn,γ → ci,γ	if γ is completely connected	(RCINIT)
ci → cr	(RCSTART)
cr, success → cs, success	(RCSTOPS)
cr, ⟨c2.r.m(v)⟩, fail,π → cs, fail, c2.r.m(v) :: π	(RCSTOPF)
cs , γ1  cf2 , γ2  ...  cfn , γn → cc, γ1  cf2 , γ2  ...  cfn , γn	(RCCOPY)
1	2	n	1	2	n

if ∀2 ≤ i ≤ n. c1 ∈ ran(γi) → fi ∈ {s, c}
C˜	cc → C˜


(RCKILL)

c1, γ1  cs  cn → c1, γ1[r '→ c3]  cs  cn
(RCREWIRE)

2	3	2	3
for r ∈ R with γ1(r) = c2 if γ1[r '→ c3] is well-connected
ci, ⟨σ⟩ cc, ⟨σ1⟩ ...  cc , ⟨σn⟩	(RCSTATE)
1	n
→ ci, ⟨ς((c1 '→ σ1,..., cn '→ σn), c)⟩ cc, ⟨σ1⟩ ...  cc , ⟨σn⟩
1	n
ci , π1  cc , π2  c3 → ci , π2|ϕ :: π1  cc , π2|¬ϕ  c3	(RCGETMSG)
1	2	1	2
for r ∈ R(c3) and ϕ ≡ {c3.r.m(v) | m ∈ M,v ∈ V }

Table 2 Reconfiguration transition rules


s: Once a component is scheduled for removal, it is put into this state; it remains there until it has become entirely unconnected.
c: Now that we are assured that no more messages are put into the queue from the outside (all components still connected to this component are in an s or c state), this state is taken, which allows the copying of parts of the message queue and the querying of the component state.
The fine-grained reconfiguration rules are shown in Tab. 2. There are two sets of rules: Rules that change the state of components to be added and to be removed, and rules that modify the components’ connections and their data state. The former set consists of RCADD (adding a new component), RCINIT, RCSTART, RCSTOPS and RCSTOPF, RCCOPY, which can be applied to a stopped component once it is safe for having its state copied to other components, i.e., no active component’s roˆle points to the stopped component anymore and finally RCKILL, used to dispose of a component. (Note that RCCOPY needs to consider the entire configuration.)
The set of rules to change the state consists of RCWIRE to connect the rˆoles of a recently added component to other components; RCREWIRE to reconnect active components’ rˆoles such that they point to new instead of stopped components; RCSTATE to take the states of the stopped components and combine them to a state for a new component; and RCGETMSG to transport residual messages of stopped to new components. RCSTATE is an abstraction of a sub-protocol that is performed to have the new component query the state of the old component; a description of

this process, which requires further component states and special restrictions on the process terms to avoid side-effects, exceeds the scope of this paper.
RCSTOPF is used to handle a component configuration with a fail process term; only this rule can advance such a configuration. Hence, fail is used to trigger a reconfiguration, which needs to follow a plan which disposes of the failed compo- nent. The method that failed must not be lost; maybe some other component is waiting for the return of the method, which would result in a deadlock. Thus, the method that produced the failure is prepended to the message queue, so that during reconfiguration, it can be moved to a new component which is capable of handling it properly.

Reconﬁguration Plan Implementation
Given a plan Δ = (A, R, α, ρ, δ, ς), we can implement Δ using a sequence of rule applications, which we call actions. We write RULE(p1 : z1,..., pn : zn) for an appli- cation of RULE, where the free variables p1,..., pn are instantiated with z1,..., zn. We omit the pi if the instantiation is clear from the context. For non-reconfiguration rules, only the acting component is free, except CHOOSE, where the chosen j ∈ J is also free (since the choice might be nondeterministic).
for each c ∈ R, RCSTOPS(c) or RCSTOPF(c) is used to stop the component. Note that this requires each c ∈ R to eventually stop processing the current method. This may lead to deadlocks (if a component about to be reconfigured is blocked, but the target has already been stopped, with the synchronous method still in its queue); avoiding them is the responsibility of the plan deviser.
for each c ∈ A, we use RCADD(c) to instantiate the component.
for each c ∈ A and each r ∈ R(c), we use RCWIRE(c, α(c)(r)), and for each (c', r) ∈ dom(ρ), we use RCREWIRE(c1 : c', c3 : ρ(c', r)). We then use, for each c ∈ A, RCINIT(c). This step connects the new and disconnects the old components.
then, we use RCCOPY(c) for each c ∈ R. For each (c', (c, r)) ∈ dom(δ), we use RCGETMSG(c1 : δ(c', (c, r)), c2 : c', c3 : c, r : r), thus copying all messages sent to c' over the rˆole r from component c to a new component.
for each c ∈ A, we use RCSTATE(c, c1,..., cn) for {c1,..., cn} = R.
now, for each c ∈ R, RCKILL(c) is used to remove the component, and, for each
c ∈ A, RCSTART(c) is used to start the components.

Shallow Reconﬁguration Plans
The generic definition of δ is done to allow for arbitrary message retainment. A reconfiguration may require to remove a large set of components, some of which are only connected from components that also get removed. For those components (e.g., component O2 in Fig. 4), a canonical message redistribution cannot be given; the unprocessed messages might have to be transferred to a new component that acts as a replacement. Such “deep” reconfiguration, however, is a complicated case that can

be expected to be of little interest. Most of the time, a single layer of components needs to retain messages, and the replacement can be found out from ρ. We thus define a shallow reconﬁguration plan as Δs = (A, R, α, ρ, ς) which translates to a plan Δ = (A, R, α, ρ, δρ,ς) with

δρ(c', (c, r)) =	ρ(c, r),	if c ∈ C \ R
undefined,	if c ∈ R.
Hence, messages are moved “with the rewiring”, which, due to its well-connectedness also ensures that the messages can indeed be processed. However, there is a prob- lem involved if a new component gets connected to by more than one component; the order of the messages moved to the new component cannot be determined de- terministically. Fig. 4 illustrates this problem: N becomes a replacement for both O1 and O3, so it receives the messages sent by C, but the order is arbitrary. In order to avoid this situation, for shallow plans we assume that |ρ−1(c)| ≤ 1 for all c ∈ A, i.e., a new component is only pointed to by one old component at most. This avoids the nondeterminism introduced by the arbitrary order in which RCGETMSG rules are executed.
Example 5.2 For the message retainment function δ and the rewiring function ρ of the plan of the fault-tolerance example δ = δρ holds and thus this plan can be represented by a shallow plan.

Interleaved Execution of Shallow Reconﬁguration Plans
A plan execution for a plan Δ is a sequence of steps that are conducted following the rules of a plan implementation of Δ. An interleaved plan execution is a sequence of steps that are taken according to both normal and reconfiguration rules, with the steps conducted by reconfiguration rules following the plan implementation of Δ. Such an interleaved plan execution is a trace of a component system that gets reconfigured by Δ, while the components not in A ∪ R continue their execution.
Example 5.3 During the interleaved plan execution of the fault-tolerance example, the client may continue to issue asynchronous messages to the component connected










ρ(C, r2)



Fig. 4. A reconfiguration scenario illustrating the problems with message retainment: Preserving message order (O1,O3) and finding a destination (O2). Non-solid arrows represent the functions ρ and δ of a plan Δ= ({N }, {O1, O2, O3}, {}, ρ, δ,ς).

to its store rˆole. While reconfiguring, not only the untainted state of Target2 is copied to NewTarget, but also those messages that have not yet been processed by Distributor are moved to NewTarget (due to using a shallow plan, they follow the rewiring), including those issued by Client in the time period between starting the reconfiguration and rewiring the store roˆle. Furthermore, this also contains the message that triggered the error in Target1; the application of RCSTOPF re-enqueues this message such that it is not lost.
An important property of our approach is to ensure that the reconfiguration remains local; i.e., only a part of the component system is concerned. Thus, while a plan execution may be mixed with arbitrary steps of other components, these other components do not observe the reconfiguration until it is completely finished. As mentioned in the introduction, this “hot reconfiguration” requires some careful treatment of the components’ states. For shallow plans, messages are transported in accordance to the rewiring of the retained components. Since this puts the messages sent before and after the application of RCGETMSG to the same component, we can show that reconfiguration of shallow plans is indeed observed as atomic.
In more detail, we prove that an interleaved plan execution of a shallow plan Δs can be simulated by another interleaved plan execution of Δs in which all re- configuration actions are grouped together and thus could be performed in a single
atomic step. To this end, we define (N˜, p, q) as a triple with N˜ being a configura-
tion in which all c ∈ dom(N˜) either are running or blocked; p a sequence of planned reconfiguration actions for Δs in the order given by the execution of Δs; and q a sequence of non-reconfiguration actions. We say that a component configuration C˜ is simulated by a triple (N˜, p, q), written as C˜ “ (N˜, p, q), if there exist C˜(1) and C˜(0) such that

C˜ = C˜(0) q	˜(1)  p	˜ ;
note that if C˜(1) and C˜(0) exist, they are uniquely determined. The regrouping of reconfiguration actions in a simulating execution is afforded by a transition system (N˜, p, q) =a⇒ (N˜', p', q') on triples defined by
(N˜, p, q) =a⇒ (N˜', p, q)	if a is a non-reconfiguration action
and p contains only RCSTOP actions and N˜ −a→ N˜',
(N˜, p, q) =a⇒ (N˜, p :: a, q)	if a is a reconfiguration action,
(N˜, p, q) =a⇒ (N˜, p, q :: a)	if a is a non-reconfiguration action
and p contains an action other than RCSTOP.
Proposition 5.4 Let Δs = (A, R, α, ρ, ς) be a shallow reconﬁguration plan, and let

the sequence ˜
a0	˜	a1
an−1  ˜
be an interleaved plan execution of Δ . Then

C0 −→ C1 −→ ... −→
there is a sequence (N˜ ,p ,q ) =a0
Cn
a=n⇒−1 (N˜
,p ,q ) such that C˜
“ (N˜
s
,p ,q )

0	0  0
⇒ ... 
n	n n
k	k	k	k

for all 0 ≤ k ≤ n with N˜0 = C˜0, p0 = ε and q0 = ε.
Proof. First of all, C˜0 “ (C˜0, ε, ε) = (N˜0, p0, q0). Let the claim hold up to some

0 ≤ k < n. In order to show that there is an (N˜
,p	,q 
) with (N˜
,p ,q ) =ak

k+1
k+1  k+1
k	k	k	⇒

(N˜k+1, pk+1, qk+1) and C˜k+1 “ (N˜k+1, pk+1, qk+1) we proceed by a case distinction on the action ak:
If ak is a non-reconfiguration action and pk contains an action other than
RCSTOP, we may trivially choose (N˜k, pk, qk :: ak).
If ak is a non-reconfiguration action, but pk only contains RCSTOP actions, we
have to provide N˜k+1, C˜(1) , and C˜(0) such that the following diagram commutes:


C˜k =
ak


˜(0)	qk k
ak
k+1

˜(1)
k
k+1

pk
 Nk
ak

C˜k+1
= ˜(0)	qk
k+1
˜(1)	pk k+1
N˜k+1

But as pk only contains RCSTOP actions, qk is ε; the component conducting ak is not a parameter of an action in pk and hence remains unaffected by pk. Thus N˜k+1

and C˜(0)
˜(1)
k+1
can be defined as the result of applying ak to N˜k and pk to N˜k+1.

If ak is a reconfiguration action, we have to provide C˜(1)
and C˜(0)
such that

the following diagram commutes:


C˜k =
ak
˜(0)	qk k
ak
˜(1)	pk k
ak
N˜k

C˜k+1
= ˜(0)	qk
k+1
˜(1)
k+1

All actions in qk have been invoked for a component in state r or b. Hence all reconfiguration actions applying to a single component only (except RCSTOP) apply to some other state (n, i, s or c). In particular, the rules RCADD, RCINIT, RCSTART and RCKILL are independent of qk. Rules RCWIRE and RCCOPY do not modify the other components, and do not rely on parts of the component configuration that can be changed by normal rules. For RCSTOPS and RCSTOPF, we have that qk = ε, since a plan is executed and we would only have actions in qk once all c ∈ R are stopped. RCSTATE also only uses components in a reconfiguration state and only uses the state part of their configuration that is not changed by actions in qk. This leaves two reconfiguration actions that actually interfere with actions in qk:
ak ≡ RCREWIRE(c1, c3, r): Any message sending from c1 over rˆole r in qk will be executed as a message sending to c3. Using δρ, this is where the messages sent over this roˆle are prepended to, and since |ρ−1(c3)|≤ 1 the order is kept the same.
ak ≡ RCGETMSG(c1, c2, c3, r): As the messages are prepended to the queue of c1 and c1 is in state i and has not yet dequeued any message, the copying does not interfere with any sending action in qk. If c2 is the target of a sending action in qk, ρ will point c3.r to c1, which is where the message would have been copied to according to δρ. Component c3 is not modified.

Thus we can choose C˜(1) to be the result of applying ak to C˜(1) and C˜(0)
as the

k+1	k	k+1
result of applying qk to C˜(0) .	 
This proposition asserts us that plan execution is “perceived as atomic”, mean- ing that it is indistinguishable from an atomic reconfiguration, i.e., from a trace first evolving to N˜n by normal actions, then applying the reconfiguration actions of pn, followed by further normal actions of qn. Hence, we do not have to stop compo- nents that are not scheduled to be stopped in order to reconfigure the system. As discussed before, this is a vital property for the applicability of reconfiguration in large component systems.

Verifying Properties in the Presence of Reconfigura- tion
Verification of properties of component systems that can be reconfigured needs to take into account all possible reconfiguration sequences. For non-atomic reconfigu- ration, the number of states grows considerably. Due to Prop. 5.4, we are allowed to check only atomic reconfigurations, and be asserted that any interleaved plan ex- ecution behaves likewise. To formally show this, we consider state formulas, which are sets of configurations. For a configuration C˜ and a state formula ϕ, we write C˜ |= ϕ for C˜ ∈ ϕ. We extend this notion to a transition system T by T |= ϕ iff C˜ |= ϕ for every reachable state C˜ of T .
Corollary 6.1 Let Ta be the transition system of a component system that can get reconﬁgured by (atomic) executions of a shallow plan Δs, and let Ti be the transi- tion system of the same component system that can get reconﬁgured by interleaved executions of Δs. If Ta |= ϕ for a state property ϕ, then Ti |= ϕ.
Proof. Assume that Ti |= ϕ. Then there is a (rooted) path C˜1 ... C˜n of Ti such

2e+07

3e+06

1.5e+07


2e+06
1e+07


1e+06

5e+06





0	0
0	1	2	3	4	5	6
Concurrent processes
Fig. 5. Atomic vs. interleaved reconfiguration in Maude. 6 concurrent processes for interleaved reconfigu- ration exceeded the main memory of 16GB.

that C˜n |= ϕ. Since Ta and Ti differ only in their reconfiguration, C˜1 ... C˜n is an interleaved plan execution of Δs. By Prop. 5.4, we can produce a sequence C˜' ... C˜'
1	n
with C˜' = C˜n, hence Ta |= ϕ.	 
Doing an atomic step instead of a series of fine-grained steps reduces the state- space, as the combinations of the intermediate reconfiguration states with the states of independent, concurrent components are not produced.
We have implemented the interleaved reconfiguration plan execution and the rules for normal component term execution in Maude [6], and used its search capa- bilities to verify that the client of the fault-tolerance example cannot observe the reconfiguration, if the new store component provides the same return values as the old ones. Atomic reconfiguration is derived from the interleaved reconfiguration by setting a flag that prohibits the execution of normal rules during a reconfiguration. Fig. 5 shows the size of the state space and the time required to produce it (by searching for an unreachable state, like the client being passed a corrupt value) for atomic and interleaved reconfiguration.

Conclusions
We have introduced a framework for concurrent, stateful components that com- municate by means of synchronous and asynchronous messages, and a procedure to reconfigure such a stateful system according to a plan such that states and messages are retained. An important property of this procedure is its minimal invasiveness: only those components that need to be removed are actively stopped in the course of reconfiguration execution.
The component framework presented is a precise description of an implementa- tion on top of Java. We found that the assumptions made in the model — e.g., the absence of shared data, in particular the restrictions on synchronous call-backs,
— are crucial to handling state-retaining reconfiguration in a real programming language. The locality of the reconfiguration greatly facilitates the planning of reconfiguration, which is already hard enough — it requires careful planning to en- sure that a plan can indeed be executed (i.e., all components in R eventually reach r, success).
However, the proposed component model is but one of many possible, useful models. We expect our results to be applicable to other, more complicated models as well. For example, a model might choose to issue time-stamps to the messages;
this would allow to lift the |ρ−1(c)| ≤ 1 property for shallow reconfiguration plans.
Due to the use of reconfiguration plans, we expect our approach to integrate well with reconfiguration planning methods like [5]. Integrating these techniques offers a strong and practicable approach towards the reconfiguration of stateful components.

References
T. Barros, A. Cansado, E. Madelaine, and M. Rivera. Model-checking distributed components: The Vercors platform. Electron. Notes Theor. Comput. Sci., 182:3–16, 2007.

C. Bidan, V. Issarny, T. Saridakis, and A. Zarras. A dynamic reconfiguration service for CORBA. In
International Conference on Configurable Distributed Systems. IEEE Computer Society, 1998.
T. Bloom. Dynamic module replacement in a distributed programming system. Technical Report MIT/LCS/TR-303, Massachusetts Institute of Technology, 1983.
J. S. Bradbury, J. R. Cordy, J. Dingel, and M. Wermelinger. A survey of self-management in dynamic software architecture specifications. In Workshop on Self-managed Systems, pages 28–33. ACM, 2004.
R. Bruni, A. L. Lafuente, and U. Montanari. Hierarchical design rewriting with Maude. In International Workshop on Rewriting Logic and its Applications, Electronic Notes in Theoretical Computer Science. Elsevier, 2008. to appear.
M. Clavel, F. Dur´an, S. Eker, P. Lincoln, N. Mart´ı-Oliet, J. Meseguer, and C. Talcott. All About Maude
- A High-Performance Logical Framework: How to Specify, Program, and Verify Systems in Rewriting Logic, volume 4350 of Lecture Notes in Computer Science. Springer-Verlag, 2007.
M. P. Herlihy and J. M. Wing. Linearizability: a correctness condition for concurrent objects. ACM Transactions on Programming Languages and Systems (TOPLAS), 12(3):463–492, 1990.
C. R. Hofmeister. Dynamic Reconfiguration of Distributed Applications. PhD thesis, Computer Science Department, University of Maryland, 1993.
F. Irmert, T. Fischer, and K. Meyer-Wegener. Runtime adaptation in a service-oriented component model. In M. Litoiu and H. Giese, editors, Software Engineering for Adaptive and Self-Managing Systems, pages 97–104. ACM, 2008.
J. Kramer and J. Magee. The evolving philosophers problem: Dynamic change management. IEEE Transactions on Software Engineering, 16(11):1293–1306, 1990.
K.-K. Lau and Z. Wang. A Survey of Software Component Models (2nd ed.). Technical Report CSPP- 38, School of Computer Science, The University of Manchester, 2006.
A. S. Lim. Abstraction and composition techniques for reconfiguration of large-scale complex applications. In International Conference on Configurable Distributed Systems, page 186. IEEE Computer Society, 1996.
P. K. McKinley, S. M. Sadjadi, E. P. Kasten, and B. H. C. Cheng. A taxonomy of compositional adaptation. Technical Report MSU-CSE-04-17, Department of Computer Science, Michigan State University, 2004.
R. Milner. Communicating and Mobile Systems: The Pi-Calculus. Cambridge Univ. Press, 1999.
A. Mukhija and M. Glinz. Runtime adaptation of applications through dynamic recomposition of components. In M. Beigl and P. Lukowicz, editors, Architecture of Computing Systems, volume 3432 of Lecture Notes in Computer Science, pages 124–138. Springer, 2005.
P. Parizek, F. Plasil, and J. Kofron. Model checking of software components: Combining Java PathFinder and behavior protocol model checker. In Proc. 30th IEEE/NASA Software Engineering Wsh. (SEW’06), pages 133–141. IEEE Computer Society, 2006.
A. Rasche and A. Polze. ReDAC dynamic reconfiguration of distributed component-based applications with cyclic dependencies. In 11th IEEE International Symposium on Object-Oriented Real-Time Distributed Computing, pages 322–330. IEEE Computer Society, 2008.
A. Rasche and W. Schult. Dynamic updates of graphical components in the .NET framework. In
Workshop on Selbstorganisierende, Adaptive, Kontextsensitive Verteilte Systeme, 2007.
A. Rensink. The GROOVE simulator: A tool for state space generation. In J. Pfalz, M. Nagl, and
B. B¨ohlen, editors, Applications of Graph Transformations with Industrial Relevance, volume 3062 of
Lecture Notes in Computer Science, pages 479–485. Springer-Verlag, 2004.
L. A. Tewksbury, L. E. Moser, and P. M. Melliar-Smith. Live upgrade techniques for CORBA applications. In Third International Working Conference on New Developments in Distributed Applications and Interoperable Systems, pages 257–272. Kluwer, B.V., 2001.
Y. Vandewoude. Dynamically updating component-oriented systems. PhD thesis, Katholieke Universiteit Leuven, 2007.
Y. Vandewoude and Y. Berbers. An overview and assessment of dynamic update methods for component-oriented embedded systems. In H. R. Arabnia and Y. Mun, editors, International Conference on Software Engineering Research and Practice, pages 521–527. CSREA Press, 2002.
M. Wegdam. Dynamic Reconfiguration and Load Distribution in Component Middleware. PhD thesis, University of Twente, Enschede, The Netherlands, 2003.
M. Wermelinger and J. L. Fiadeiro. Algebraic Software Architecture Reconfiguration. ACM SIGSOFT Softw. Notes, 24(6):393–409, 1999.
