Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 347 (2019) 303–324
www.elsevier.com/locate/entcs


A Denotational Semantics for Low-Level Probabilistic Programs
with Nondeterminism
Di Wang,a  Jan Hoffmanna and  Thomas Repsb,c
a Carnegie Mellon University
b University of Wisconsin
c GrammaTech, Inc.

Abstract
Probabilistic programming is an increasingly popular formalism for modeling randomness and uncertainty. Designing semantic models for probabilistic programs has been extensively studied, but is technically challenging. Particular complications arise when trying to account for (i) unstructured control-flow, a natural feature in low-level imperative programs; (ii) general recursion, an extensively used programming paradigm; and (iii) nondeterminism, which is often used to represent adversarial actions in probabilistic models, and to support refinement-based development. This paper presents a denotational-semantics framework that supports the three features mentioned above, while allowing nondeterminism to be handled in different ways. To support both probabilistic choice and nondeterministic choice, the semantics is given over control-flow hyper -graphs. The semantics follows an algebraic approach: it can be instantiated in different ways as long as certain algebraic properties hold. In particular, the semantics can be instantiated to support nondeterminism
among either program states or state transformers. We develop a new formalization of nondeterminism based on powerdomains over sub-probability kernels. Semantic objects in the powerdomain enjoy a notion we call generalized convexity, which is a generalization of convexity. As an application, the paper sketches an algebraic framework for static analysis of probabilistic programs, which has been proposed in a companion paper.
Keywords: Probabilistic programming, denotational semantics, control-flow hyper-graphs, nondeterminism, powerdomains

Introduction
Probabilistic programming provides a powerful framework for implementing ran- domized algorithms [2], cryptographic protocols [3], cognitive models [31], and machine-learning algorithms [29]. One important focus of recent studies on proba- bilistic programming is to reason rigorously about probabilistic programs and systems. The first step in such works is to provide a suitable formal semantics for probabilistic programs.
Despite the fact that lots of existing work focuses on high-level probabilistic programs, e.g., lambda calculus [8], higher-order functions [32,20], and recursive

https://doi.org/10.1016/j.entcs.2019.09.016
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

types [63], we observe that low -level features could arise naturally. For example, when developing a compiler for a probabilistic programming language [26,56], we need a semantics for the imperative target language to prove compiler correctness. There have been studies on denotational semantics for well-structured imperative programs [43,44,47,48,62,34,38,55,7], as well as operational semantics for control- flow graphs (CFGs) based on Markov chains (MCs) and Markov decision processes (MDPs) ([25,14,15]). On the one hand, we prefer CFGs as program representations because they enable rich low-level features such as unstructured flows, e.g., those introduced by break and continue. On the other hand, from the perspective of rigorous reasoning, a denotational semantics (i) abstracts from details about program executions and focuses on program eﬀects, and (ii) is compositional in the sense that the semantics of a program fragment is established from the semantics of the fragment’s proper constituents.
Therefore, in this paper, we devise a denotational semantics for low-level proba- bilistic programs. Our work makes three main contributions:
We use hyper-graphs as the representation for low-level probabilistic programs with unstructured control-flow, general recursion, and nondeterminism.
	We develop a domain-theoretic characterization of a new model of nondetermin- ism for probabilistic programming, which involves nondeterminacy among state transformers, opposed to a common model that involves nondeterminacy among program states.
We devise an algebraic framework for denotational semantics. The advantage of having a framework is that it can be instantiated with different models of nondeterminism. We show how to instantiate the framework using two different approaches to formalizing nondeterminism in Ex. 5.2. We also show that for programs without procedure calls and nondeterminism, the resulting denotational semantics is equivalent to a distribution-based operational semantics (§5.2).
We define the denotational semantics directly as an interpretation of the control- flow hyper-graphs (CFHGs) of low-level probabilistic programs, introduced in §2. Hyper-graphs consist of hyper-edges, each of which connects one source node and possibly several destination nodes. For example, probabilistic choices are represented by weighted hyper-edges with two destinations. Nondeterminism is then represented by multiple hyper-edges starting in the same node. The interpretation of hyper-edges is also different from standard edges. If the CFHG were treated as a standard graph, the subpaths from each successor of a branching node would be analyzed independently. In contrast, our hyper-graph approach interprets a probabilistic-choice hyper-edge with probability p as a function λa.λb.a p⊕ b, where p⊕ is an operation that weights the subpaths through the two successors by p and 1 − p. In other words, we do not reason about subpaths starting from a node individually, instead we analyze these subpaths jointly as a probability distribution. If a node has two outgoing probabilistic-choice hyper-edges, it represents two “worlds” of subpaths, each of which carries a probability distribution with respect to the probabilistic choice made in this “world.”

Some high-level decision choices about nondeterminism arise when we are devel- oping the low-level semantics. Nondeterminism itself is an important feature from two perspectives: (i) it arises naturally from probabilistic models, such as the agent for an MDP [6], or the unknown input distribution for modeling fault tolerance [40], and (ii) it is required by the common paradigm of abstraction and refinement 1 on programs [19,48]. While nondeterminism has been well studied for standard programming languages, the combination of probabilities and nondeterminism turns out to be tricky. One substantial question is when the nondeterminism is resolved. A well-studied model for nondeterminism in probabilistic programming is to resolve program inputs prior to nondeterminism [18,50,47,51,48,62]. This model follows a commonplace principle of semantics research that represents a nondeterministic function as a set-valued function that maps an input to a collection of possible outputs, i.e., an element in X → ℘(X), where X is a program state space and ℘(·) is the powerset operator. However, it is sometimes desirable to resolve nondeterminism prior to program inputs, i.e., a nondeterministic program should represent a collection of elements in ℘(X → X). For example, one may want to show for every refined version of a nondeterministic program with each nondeterministic choice replaced by a conditional, its behavior on all inputs are indistinguishable. We call the common model nondeterminism-last and the other nondeterminism-first. In §4, we present a domain-theoretic study of nondeterminism-first. Technically, we propose a notion of generalized convexity (g-convexity, for short), which expresses that a set of state transformers is stable under refinements (while standard convexity describes that a set of states is stable under refinements), as well as devise a g-convex powerdomain that characterizes expressible semantic objects.
To achieve our ultimate goal of developing a denotational semantics, instead of restricting ourselves to one specific model for nondeterminism, we propose a general algebraic denotational semantics in §5, which can be instantiated with different treatments of nondeterminism. The semantics is algebraic in the sense that it performs reasoning in some space of program states and state transformers, while the transformers should obey some algebraic laws. For instance, the program command skip should be interpreted as the identity element for sequencing in an algebra of program-state transformers. In addition, the algebraic approach is a good fit for static analysis of probabilistic programs. In §6, we sketch a static-analysis framework proposed in a companion paper [64], as an application of the denotational semantics. The algebraic approach we take in this paper is challenging in the setting of probabilistic programming. In contrast, for standard, non-probabilistic programming languages, it is almost trivial to derive a low-level denotational semantics once one has a semantics for well-structured programs at hand. The trick is to first define the semantic operations as a Kleene algebra [41,16,42,45], which admits an extend operation, used for sequencing, a combine operation, used for branching, and a closure operation, used for looping; then extract from the CFG a regular expression that captures all execution paths by Tarjan’s path-expression algorithm [61]; and

1 Abstraction enables reasoning about a program through its high-level specifications, and refinement allows stepwise software development, where programs are “refined” from specifications to low-level implementations.

if	then	if prob(1/2) then t :=0 else t :=1 fi else	if prob(1/3) then t :=0 else t :=1 fi	fi
Fig. 1. A nondeterministic, probabilistic program
finally use the Kleene algebra to reinterpret the regular expression to obtain the semantics for the CFG. However, this approach fails when both probabilities and nondeterminism come into the picture. Consider the probabilistic program with a nondeterministic choice  in Fig. 1. The program is intended to draw a random value t from either a fair coin flip or a biased one. If one adopts the path-expression approach, one ends up with a regular expression that describes a single collection of four program executions: (i) t :=0 with probability 1/2, (ii) t :=1 with probability 1/2,
(iii) t :=0 with probability 1/3, and (iv) t :=1 with probability 2/3. The collection does not describe the intended meaning, and does not even form a well-defined probability distribution—all the probabilities sum up to 2 instead of 1. Intuitively, the path- expression approach fails for probabilistic programs because it can only express the semantics as a collection of executions with probabilities, whereas probabilistic programs actually specify collections of distributions over executions.
Although the denotational semantics proposed in this paper supports interesting features including unstructured control-flow, general recursion, and nondeterminism, there are some other important features that the semantics does not support yet, such as continuous distributions and higher-order functions. We discuss those missing features in §7, and leave them for future work.

An Operational Semantics for Low-Level Probabilistic Programs
In this section, we sketch an operational semantics for an imperative, single-procedure, deterministic, 2 probabilistic programming language, following the approach of Borgström et al.’s distribution-based semantics [8]. We use the operational semantics to (i) illustrate how to model executions of probabilistic programs operationally, and
(ii) justify the development of a denotational semantics in later sections.
A Hyper-Graph Program Model
We define the operational semantics on CFHGs of programs. We adopt a com- mon approach for standard CFGs in which the nodes represent program loca- tions, and edges labeled with instructions describe transitions among program locations (e.g., [24,54,46]). Instead of standard directed graphs, we make use of hyper-graphs [27].
Definition 2.1 A hyper-graph H is a quadruple ⟨V, E, ventry, vexit⟩, where V is a finite set of nodes, E is a set of hyper-edges, ventry ∈ V is a distinguished entry

2 The term “deterministic” is used in the sense “not nondeterministic.”

node, and vexit ∈ V is a distinguished exit node. A hyper-edge is an ordered pair
⟨x, Y ⟩, where x ∈ V is a node and Y ⊆ V is an ordered, non-empty set of nodes. For a hyper-edge e = ⟨x, Y ⟩ in E, we use src(e) to denote x and Dst(e) to denote
Y . Following the terminology from graphs, we say that e is an outgoing edge of x and an incoming edge of each of the nodes y ∈ Y . We assume ventry does not have incoming edges, and vexit has no outgoing edges.
Definition 2.2 A probabilistic program contains a finite set of procedures {Hi}1≤i≤n, where each procedure Hi = ⟨Vi, Ei, ventry, vexit⟩ is a control-flow hyper-graph (CFHG)
i	i
in which each node except vexit has at least one outgoing hyper-edge, and vexit has

no outgoing hyper-edge. Define V d=ef S
i
Vi. To assign meanings to probabilistic

programs modulo data actions Act and deterministic conditions Cond that can be probabilistic, we associate with each hyper-edge e ∈ E = 1≤i≤n Ei a control-flow action Ctrl(e) that has one of the following three forms:
Ctrl ::= seq[act], where act ∈ Act | cond[ϕ], where ϕ ∈ Cond
|  call[i → j], where 1 ≤ i, j ≤ n
where the number of destination nodes |Dst(e)| of a hyper-edge e is 1 if Ctrl(e) is
seq[act] or call[i → j], and 2 otherwise.
Example 2.3 Fig. 2(b) shows the CFHG of the program in Fig. 2(a), where v0 is the entry and v4 is the exit. The hyper-edge ⟨v2, {v3}⟩ is associated with a sequencing action seq[n := n + 1], while ⟨v1, {v2, v4}⟩ is assigned a deterministic-choice action cond[prob(0.5) ∧ prob(0.5)], i.e., an event where two coin flips both show heads.
Note that break, continue (and also goto) are not data actions, and are encoded directly as edges in CFHGs in a standard way. The grammar below defines data actions Act and deterministic conditions Cond that could be used for an arithmetic program, where p ∈ [0, 1], c ∈ Q, a, b ∈ Z, and n ∈ N.
Act ::= x := e | x ∼ D | observe(ϕ) | skip ϕ ∈ Cond ::= Т | ¬ϕ | ϕ1 ∧ ϕ2 | e1 ≤ e2 | prob(p)
e ∈ Exp ::= x | c | e1 + e2 | e1 × e2	D ∈ Dist ::= Binomial(n, p) | Uniform(a, b) | Geometric(p) | ··· 

Dist stands for a collection of discrete probability distributions. For example, Binomial(n, p) with n ∈ N and p ∈ [0, 1] describes the distribution of the number of successes in n independent experiments, each of which succeeds with probability p; Uniform(a, b) represents a discrete uniform distribution on [a, b] ∩ Z.

A Distribution-Based Small-Step Operational Semantics
The next step is to define a semantics based on CFHGs. We adopt Borgström et al.’s distribution-based small-step operational semantics for lambda calculus [8] to our hyper-graph setting, while we suppress the features of multiple procedures and nondeterminism for now.
Three components are used to define the semantics:
A program state space Ω, e.g., for arithmetic programs, we can define Ω d=ef Var -
Q, i.e., a set of finite partial maps from program variables to their values.

n := 0;
while prob(0.5) ∧ prob(0.5) do
n := n + 1;
if n ≥ 10 then break else continue
od
(a)


false



(b)
Fig. 2. (a) An example of probabilistic programs; (b) The corresponding CFHG

A function Jact) from program states to (sub-probability) distributions over program states for each data action act. A distribution is a function Δ:Ω → [0, 1] such that ω∈Ω Δ(ω) ≤ 1. Intuitively, Jact) (ω)(ωj) is the probability that the action act, starting in state ω ∈ Ω, halts in a state ωj ∈ Ω [44].
A [0, 1]-valued function Jϕ) from program states for each deterministic condition
ϕ. Intuitively, Jϕ) (ω) is the probability that the condition ϕ holds in state ω ∈ Ω.
The point distribution δ(ω) is defined as λωj.[ω = ωj] where [ψ] is an Iverson bracket that evaluates to 1 if ψ is true and 0 otherwise. If Δ is a distribution and r ∈ [0, 1], we write r · Δ for the distribution λω.r · Δ(ω). If Δ1, Δ2 are distributions and r1, r2 ∈ [0, 1] satisfy r1 + r2 ≤ 1, we write r1 · Δ1 + r2 · Δ2 for the distribution λω.r1 · Δ1(ω)+ r2 · Δ(ω).
Fig. 3 shows interpretation of the data actions and deterministic conditions given in §2.1, where ω(e) evaluates expression e in state ω, [x '→ v]ω updates x in ω with v, and ΔD : Q → [0, 1] is the probability mass function of the distribution D. If ϕ does not contain any probabilistic choices prob(p), then Jϕ) (ω) is either 0 or 1. Intuitively, Jϕ) (ω) is the probability that ϕ is true in the state ω, w.r.t. a probability space specified by all the prob(p)’s in ϕ. Then the probability of ϕ1 ∧ ϕ2 is defined as the product of the individual probabilities of ϕ1 and ϕ2, because ϕ1 and ϕ2 are interpreted w.r.t. probabilistic choices in ϕ1 and ϕ2, respectively, and these two sets of choices are disjoint, thus independent.
Suppose that P = ⟨V, E, ventry, vexit⟩ is a single-procedure deterministic program. Therefore, each node in P except vexit is associated with exactly one hyper-edge. The program configurations T = V × Ω are pairs of the form ⟨v, ω⟩, where v ∈ V is a



Fig. 3. Interpretation of data actions and deterministic conditions
node in the CFHG, and ω ∈ Ω is a program state.
We define one-step evaluation as a relation ⟨v, ω⟩ −→ Δ between configurations
⟨v, ω⟩ and distributions Δ on configurations, as shown in Fig. 4.

⟨v, ω⟩ −→ λ⟨v′, ω′⟩.[v′ = u] · Jact) (ω)(ω′)	where e = ⟨v, {u}⟩ ∈ E, Ctrl(e)= seq[act]
⟨v, ω⟩ −→ Jϕ) (ω) · δ(⟨u1, ω⟩)+ (1 − Jϕ) (ω)) · δ(⟨u2, ω⟩)  where e = ⟨v, {u1, u2}⟩ ∈ E, Ctrl(e)= cond[ϕ]

Fig. 4. One-step evaluation relation

Example 2.4 For the program in Fig. 2, some one-step evaluations are ⟨v0, {n '→
233}⟩ −→ δ(⟨v1, {n '→ 0}⟩), ⟨v1, {n '→ 1}⟩ −→ 0.25 · δ(⟨v2, {n '→ 1}⟩)+ 0.75 ·
δ(⟨v4, {n '→ 1}⟩), and ⟨v3,n '→ 9⟩ −→ δ(⟨v1, {n '→ 9}⟩).
We now define step-indexed evaluation as the family of n-indexed relations
⟨v, ω⟩ −→n Δ between configurations ⟨v, ω⟩ and distributions Δ on program states inductively, as shown in Fig. 5.

⟨v, ω⟩ −→0 λω′.0
⟨vexit, ω⟩ −→n δ(ω)	if n > 0
⟨v, ω⟩ −→n+1	Σ	Δ(τ ) · Δ′	where⟨v, ω⟩ −→ Δ and τ −→n Δ′ for any τ ∈ supp(Δ)
τ∈supp(Δ)

Fig. 5. Step-indexed evaluation relation

Example 2.5 For the program in Fig. 2, some step-indexed evaluations are ⟨v4, {n '→
10}⟩ −→1 δ({n '→ 10}), ⟨v1, {n '→ 0}⟩ −→2 0.75·δ({n '→ 0}), and ⟨v1, {n '→ 0}⟩ −→5
0.75 · δ({n '→ 0})+ 0.1875 · δ({n '→ 1}).

def
For the program P = ⟨V, E, ventry, vexit⟩, we define its semantics JP )os (ω) =
supn∈N {Δ | ⟨ventry, ω⟩ −→n Δ} .
Example 2.6 For the program P in Fig. 2, JP )os (ω) for any initial state ω with
n ∈ dom(ω) is given by Σ9	(0.75 × 0.25k) · δ([n '→ k]ω)+ 0.00000095367431640625 ·
δ([n '→ 10]ω).
Why is a Denotational Semantics Desirable?
We have already shown how probabilistic programs execute operationally. As men- tioned in §1, we are instead interested in developing a denotational semantics, which concentrates on the eﬀects of programs and abstracts from how the program executes. This characterization of denotational semantics is indeed beneficial for rigorous

reasoning about programs, such as static analysis and model checking, because one usually only cares whether programs satisfy certain properties, e.g., if they terminate on all possible inputs. Even better, a denotational semantics is often compositional —that is, the property of a whole program can be established from properties of its proper constituents. In other words, one could develop local —and thus scalable—reasoning techniques based on a denotational semantics. In contrast, the operational semantics in §2.2 is not compositional—it takes into account the whole program P to define JP )os.
Another benefit of a denotational semantics is that it is often easier to extend
than an operational one. In the rest of this section, we briefly compare the complexity of adding procedure calls and nondeterminism to an operational semantics versus a denotational semantics. To support multiple procedures and procedure calls in the semantics proposed in §2.2, one needs to introduce a notion of stacks to keep track of procedure calls, as in [22,23,55]. Then the program configurations become triples of call stacks, control-flow-graph nodes, and program states. As a consequence, the one-step and step-indexed evaluation relations in Figs. 4 and 5 would become more complex. However, such an extension is almost trivial for a denotational semantics. Suppose we are able to compose semantic objects, e.g, JC1; C2)ds = JC2)ds ◦ JC1)ds,
where C1, C2 are program fragments, ◦ denotes a composition operation, and JC)ds
gives the denotation of C. If C1 is indeed a procedure call call Q where Q is a procedure, because we can obtain the denotation JQ)ds of Q, we can interpret Jcall Q; C2)ds merely as JC2)ds ◦ JQ)ds. By this means we do not need to reason about stacks explicitly.
Another important programming feature is nondeterminism. For operational semantics of probabilistic programs, nondeterminism is often formalized using the notion of a scheduler, which resolves a nondeterministic choice from the computation that leads up to it (e.g., [25,14,15]). When the scheduler is fixed, a program can be executed deterministically (as shown in §2.2). To reason about nondeterministic programs with respect to an operational semantics, one needs to take all possible schedulers into consideration. However, if one only cares about the effects of a program, it is possible to sidestep these schedulers by switching to a denotational semantics. For example, let C1, C2 be two program fragments and JC1)ds , JC2)ds be
their denotations, which should be maps from initial states to a collection of possible
final states. Then the denotation Jif  then C1 else C2 fi)ds of a nondeterministic- choice between C1 and C2 could be something like λω. JC1)ds (ω) ∪ JC2)ds (ω). Note that this approach does not need to consider schedulers explicitly.

A Summary of Existing Domain-Theoretic Develop- ments
Our development of models for nondeterminism makes great use of existing domain- theoretic studies of powerdomains, thus in this section, we present a brief summary of them. We review some standard notions from domain theory [33,1,49], as well as some results on probabilistic powerdomains [36,35] and nondeterministic powerdo-

mains [18,50,47,51,48,62].
Background from Domain Theory
Let P be a nonempty set with a partial order ±, i.e., a poset. The lower closure of a subset A is defined as ↓A d=ef {x ∈ P | ∃a ∈ A : x ± a}. The upper closure of a subset A is defined as ↑A d=ef {x ∈ P | ∃a ∈ A : a ± x}. A subset A satisfying ↓A = A is called a lower set. A subset A satisfying ↑A = A is called an upper set. If all
elements of P are above a single element x ∈ P , then x is called the least element, denoted commonly by ⊥. A function f : P → Q between two posets P and Q is monotone if for all x, y ∈ P such that x ± y, we have f (x) ± f (y). A subset A of P is directed if it is nonempty and each pair of elements in A has an upper bound in
A. If A is totally ordered and isomorphic to natural numbers, then A is called an
ω-chain. If a directed set A has a supremum, then it is denoted by  ↑ A.
A poset D is called directed complete or a dcpo if each directed subset A of D has a supremum ↑ A in D. A function f : D → E between two dcpos D and E is Scott- continuous if it is monotone and preserves directed suprema, i.e., f ( ↑ A)= ↑ f (A) for all directed subsets A of D.
Let D be a dcpo. For two elements x, y of D, we say that x approximates y, denoted by x  y, if for all directed subsets A of D, we have y ± .↑ A implies x ± a
∃a ∈ A : a  x}. The dcpo D is called continuous if there exists a subset B of D

such that for every element x of D, the set The set B is called a basis of D.
x ∩ B is directed and x = .↑(
x ∩ B).

Let D be a dcpo. A subset A is Scott-closed if A is a lower set and is closed under directed suprema. The complement D \ A of a Scott-closed subset A is called Scott-open. These Scott-open subsets form the Scott-topology on D. The closure of a subset A is the smallest Scott-closed set containing A as a subset, denoted by A.
Let X be a topological space whose open sets are denoted by O(X). A cover C of a subset A of X is a collection of subsets whose union contains A as a subset. A sub-cover of C is a subset of C that still covers A. The cover C is called an open-cover if each of its members is an open set. A subset A is compact if every open-cover of A contains a finite sub-cover. A subset A is saturated if A is an intersection of its neighborhoods. The saturation of a subset A is the intersection of its neighborhoods. In dcpo’s equipped with the Scott-topology, saturated sets are precisely the upper sets, and the saturation of a subset A is given by ↑A. The Lawson-topology on a dcpo D is generated by Scott-open sets and sets of the form D \ ↑x. A lens is a nonempty subset that is the intersection of a Scott-closed subset and a Scott-compact saturated subset. Lenses are always Lawson-closed sets. A continuous dcpo D is called coherent if the intersection of any two Scott-compact saturated subsets is also Scott-compact. The Lawson-topology on a coherent dcpo is compact.
We are going to use the following theorems in our technical development.
Proposition 3.1 (Kleene fixed-point theorem) Suppose ⟨D, ±⟩ is a dcpo with a least element ⊥, and let f : D → D be a Scott-continuous function. Then f has a

least fixed point which is the supremum of the ascending Kleene chain of f (i.e., the
ω-chain ⊥± f (⊥) ± f (f (⊥)) ± · · ·± fn(⊥) ± · · · ), denoted by lfp± f.
Proposition 3.2 (Cor. of [33, Hofmann-Mislove theorem]) Let X be a sober space, i.e., a T0-space where every nonempty closed set is either the closure of a point or the union of two proper closed subsets. The intersection of a filtered family
{Ai}i∈£ (i.e., the intersection of any two subsets is in the family) of nonempty compact saturated subsets is compact and nonempty. If such a filtered intersection is contained in an open set U, then Ai ⊆ U for some i ∈ I. Specifically, continuous dcpos equipped with the Scott-topology and coherent dcpos equipped with the Lawson-topology are sober.

Probabilistic Powerdomains
Jones et al.’s pioneer work on probabilistic powerdomains [36,35] extends the complete partially ordered sets, which are pervasively used in computer science, to model probabilistic computations. Let X be a nonempty countable set. The set of all distributions on X is denoted by D(X), i.e., a probabilistic powerdomain over X. Recall that a distribution on X is a function Δ: X → [0, 1] such that  x∈X Δ(x) ≤ 1, and the point distribution δ(x) for some x ∈ X is defined as λxj.[x = xj]. Distributions

are ordered pointwise, i.e., Δ1 ±D
Δ2 d=ef ∀x ∈ X : Δ1(x) ≤ Δ2(x). We define the

probabilistic-choice of distributions Δ1, Δ2 with respect to a weight p ∈ [0, 1], written
Δ1 p⊕ Δ2, as p · Δ1 + (1 − p) · Δ2.
The following theorems provide a characterization of the probabilistic powerdo- mains.
Proposition 3.3 ([36,35,47,62]) The poset ⟨D(X), ±D⟩ forms a coherent dcpo
with a countable basis {Σn	ri · δ(xi) | n ∈ N ∧ ri ∈ Q+ ∧ Σn	ri ≤ 1 ∧ xi ∈ X}. It
Proposition 3.4 ([35,62]) Every function f : X → D(X) can be lifted to a unique Scott-continuous linear (in the sense that it preserves probabilistic-choice) map f : D(X) → D(X).

Nondeterministic Powerdomains
When nondeterminism comes into the picture, as we discussed in §1, existing studies usually resolve program inputs prior to nondeterminism [37,18,50,47,51,48,62]. In
§1, we call such a model nondeterminism-last, which interprets nondeterministic functions as maps from inputs to sets of outputs. Let X be a nonempty countable set. A subset A of D(X) is called convex if for all Δ1, Δ2 ∈ A and all p ∈ [0, 1], we have Δ1 p⊕ Δ2 ∈ A. The convex hull of an arbitrary subset A is the smallest convex set containing A as a subset, denoted by conv(A). The convexity condition ensures that from the perspective of programming, nondeterministic choices can always be refined by probabilistic choices. The convex powerdomain PD(X) over the probabilistic powerdomain D(X) is then defined as convex lenses in D(X) with

the Egli-Milner order A ± B d=ef A ⊆ ↓B ∧ ↑A ⊇ B.
The following theorems provide a characterization of the convex powerdomains.
Proposition 3.5 ([47,62]) The poset ⟨PD(X), ±P ⟩ forms a coherent dcpo. It ad- mits a least element ⊥	d=ef {⊥ }. For r1, r2 ∈ [0, 1] satisfying r1 + r2 ≤ 1, we
define r1 · A + r2 · B d=ef C ∩ ↑C where C is {r1 · Δ1 + r2 · Δ2 | Δ1 ∈ A ∧ Δ2 ∈ B}. Then the probabilistic-choice operation is lifted to a Scott-continuous operation as
def
A p⊕P B = p · A + (1 − p) · B. Moreover, it carries a Scott-continuous semilattice
def

operation, called formal union, defined as A −∪−P B = C ∩ ↑C where C is conv(A ∪ B).
Intuitively, the formal union operation stands for nondeterministic choices.
Proposition 3.6 ([62]) Every function g : X → PD(X) can be lifted to a unique Scott-continuous linear (in the sense that it preserves lifted probabilistic-choice) map g : PD(X) → PD(X) preserving formal unions.
Example 3.7 Consider the following program P where can be refined by any deterministic condition involving the program variable t:
if   then t := t +1 else t := t − 1 fi
and we want to assign a semantic object to it from X → PD(X), where the state space X = Q represents the value of t. Fix an input t ∈ Q. The data actions t := t +1 and t := t − 1 then take the input to singletons {δ(t + 1)} and {δ(t − 1)}, respectively, in the powerdomain PD(Q). Thus the nondeterministic-choice is interpreted as
{δ(t + 1)} −∪−P {δ(t − 1)}, which is {r · δ(t + 1)+ (1 − r) · δ(t − 1) | r ∈ [0, 1]}, for a given t ∈ Q.

Nondeterminism-First
In this section, we develop a new model of nondeterminism—the nondeterminism- first approach, which resolves nondeterministic choices prior to program inputs—in a domain-theoretic way. This model is inspired by reasoning about a program’s behavior on different inputs (as mentioned in §1), which requires nondeterministic functions to be treated as a family of transformers (i.e., an element of ℘(X → X)) instead of a set-valued map (i.e., an element of X → ℘(X)). As will be shown in this section, with nondeterminism-first, t := t +1 and t := t − 1 are assigned semantic objects {λt.δ(t + 1)} and {λt.δ(t − 1)}, respectively.
We first introduce kernels, then propose a new notion of generalized convexity
(g-convexity, for short), and finally develop a powerdomain for nondeterminism-first.
A Powerdomain for Sub-Probability Kernels
Let X be a nonempty countable set. A function κ : X → D(X) is called a (sub- probability) kernel. Intuitively, a kernel maps an input state to a distribution over output states. The set of all such kernels is denoted by K(X) d=ef X → D(X). Kernels
are ordered pointwise, i.e., κ1 ±K κ2 d=ef ∀x ∈ X : κ1(x) ±D κ2(x).

Theorem 4.1 The poset ⟨K(X), ± ⟩ forms a coherent dcpo, with ⊥	d=ef λx.⊥	as its least element.
Let W(X) d=ef X → [0, 1] be the set of functions from X to the interval [0, 1].

We denote the pointwise comparison by ≤˙
and the constant function by r˙
for

any r ∈ [0, 1]. If κ is a kernel and φ ∈ W(X), we write φ · κ for the kernel λx.φ(x) · κ(x). If κ1, κ2 are kernels and φ1, φ2 ∈ W(X) such that φ1 + φ2≤˙ 1˙, we write φ1 · κ1 + φ2 · κ2 for the kernel λx.φ1(x) · κ1(x)+ φ2(x) · κ2(x). More generally,
if {κi}i∈N+ is a sequence of kernels, and {φi}i∈N+ is a sequence of functions in W(X)

such that Σ∞
φi≤˙ 1˙, we write Σ∞
φi · κi for the kernel .†
n i=1
φi · κi. Then

we define conditional-choice of kernels κ1, κ2 conditioning on a function φ ∈ W(X)
as κ1 3 κ2 d=ef φ · κ1 + (1˙ − φ) · κ2. We define the composition of kernels κ1, κ2 as
κ ⊗ κ d=ef λx.λxjj. Σ	κ (x)(xj) · κ (xj)(xjj).
Lemma 4.2	(i) The conditional-choice operation φ3 is Scott-continuous for all
φ ∈ W(X).
(ii) The composition operation ⊗ is Scott-continuous.
Generalized Convexity
As shown in §3.3, nondeterminism-last is captured by convex sets of distributions. However, a more complicated notion of convexity is needed to develop nondeterminism- first semantics over kernels. Let X be a nonempty countable set. Every semantic object should be closed under the conditional-choice φ3 for every function φ ∈ W(X).

Recall that the definition κ1
3 κ2 d=ef φ · κ1 + (1˙ − φ) · κ2 is similar to a convex

combination, except that the coefficients might not only be constants, but can also depend on the state. We formalize the idea by defining a notion of g-convexity.
Definition 4.3 A subset A of K(X) is called g-convex, if for all sequences {κi}i∈N+ ⊆
A and {φi}i∈N+ ⊆ W(X) such that Σ∞  φi = 1˙, then Σ∞  φi · κi is contained in A.
We now show that some domain-theoretic operations preserve g-convexity.
Lemma 4.4 Let A be a g-convex subset of K(X). Then
The saturation ↑A and the lower closure ↓A are g-convex.

The closure A is g-convex.
The g-convex hull of a subset A of K(X) is the smallest g-convex set containing A as a subset, denoted by gconv(A). Intuitively, gconv(A) enriches A to become a reasonable semantic object that is closed under arbitrary conditional-choice.
Following are some properties of the gconv(·) operator.
Lemma 4.5 Suppose that A and B are g-convex subsets of K(X). Then {κ φ3 ρ |
κ ∈ A ∧ ρ ∈ B} is g-convex for all functions φ ∈ W(X).
Corollary 4.6 If A and B are g-convex, then gconv(A ∪ B) is given by {κ1 φ3 κ2 |
κ1 ∈ A ∧ κ2 ∈ B ∧ φ ∈ W(X)}.

Proof. It is straightforward to show that gconv(A ∪ B) is a superset of {κ1 φ3 κ2 | κ1 ∈ A ∧ κ2 ∈ B ∧ φ ∈ W(X)}. Then it suffices to show this set is indeed g-convex. We conclude the proof by Lem. 4.5.	 
For a finite subset F of K(X), as an immediate corollary of Cor. 4.6, by a simple
induction we know that gconv(F )= {	κ∈F φκ · κ | {φκ}κ∈F ⊆ W(X) ∧	κ∈F φκ = 1˙}.
Lemma 4.7 For an arbitrary A ⊆ K(X), we have

∞
gconv(A)= 
i=1
φi · κi | {κi}i∈N+ ⊆ A ∧ {φi}i∈N+ ⊆ W(X) ∧
Σi=1
φi = 1˙	.


	
Lemma 4.8   (i) For an arbitrary A ⊆ K(X), we have gconv(A)= gconv A .
(ii) If {Ai}i∈£ is a directed collection of Scott-closed subsets of K(X) ordered by set inclusion, then gconv ( Ai)=  gconv(Ai).
Lemma 4.9 Let A and B be Scott-compact g-convex subsets of K(X).  Then
gconv(A ∪ B) is also Scott-compact.
We now turn to discuss some separation properties for g-convexity.
Lemma 4.10  (i) If A ⊆ K(X) is g-convex, then for all x, {κ(x) | κ ∈ A} is convex.
If A ⊆ K(X) is Scott-compact, then for all x, {κ(x) | κ ∈ A} is Scott-compact.
If A ⊆ K(X) is Scott-closed, then for all x, {κ(x) | κ ∈ A} is Scott-closed.
Lemma 4.11 Let us consider subsets of K(X). Suppose that K is a Scott-compact g-convex set and A is a nonempty Scott-closed g-convex set that is disjoint from K. Then they can be separated by a g-convex Scott-open set, i.e., there is a g-convex Scott-open set V including K and disjoint from A.
Lemma 4.12 If K ⊆ K(X) is nonempty and Scott-compact, then gconv(K) is Scott-compact.
A g-convex Powerdomain for Nondeterminism-First
From the literature, a Plotkin powertheory [1] is defined by one binary operation −∪−, called formal union, and the following laws: (i) A −∪− B = B −∪− A, (ii) (A −∪− B) −∪− C = A −∪− (B −∪− C), and (iii) A −∪− A = A, for all objects A, B, C in the powerdomain. Intuitively, the formal union −∪− represents nondeterministic-choice. Moreover, the formal union induces a semilattice ordering: A ≤ B if A −∪− B = B. The semilattice ordering is usually not interesting from the perspective of domain theory, however, it is instrumental to describe the relation between conditional-choice and nondeterministic- choice—A φ3 B ≤ A −∪− B for all semantic objects A, B—a nondeterministic-choice should abstract an arbitrary (possibly probabilistic) conditional-choice.
Let X be a nonempty countable set. As nondeterminism-first interprets programs as collections of input-output transformers, we hope to develop a powerdomain on

K(X), i.e., kernels on X. To achieve this goal, we need to (i) identify a collection of well-formed semantic objects in ℘(K(X)), which admits a formal-union operation described above, (ii) lift conditional-choice φ3 and composition ⊗ on kernels to the powerdomain properly, and (iii) prove the powerdomain is a dcpo and the operations are Scott-continuous.
Inspired by studies on convex powerdomains [1,47,62], we start with the following collection

GK(X) d=ef {S ⊆ K(X) | S a nonempty g-convex lens}
to be the set of all g-convex lenses of K(X) ordered by Egli-Miler order A ±G


B d=ef

A ⊆ ↓B ∧ ↑A ⊇ B. We call GK(X) a g-convex powerdomain over kernels on X.
The following theorem establishes a characterization of g-convex powerdomains.
Theorem 4.13 ⟨GK(X), ± ⟩ forms a dcpo, with a least element ⊥ d=ef {⊥ }.
We now lift conditional-choice φ3 (where φ ∈ W(X)) and composition ⊗ for kernels to the powerdomain GK(X) as follows.

A φ3G
B d=ef {a
φ3 b | a ∈ A ∧ b ∈ B}∩ ↑{a
φ3 b | a ∈ A ∧ b ∈ B}



A ⊗ B d=ef gconv({a ⊗ b | a ∈ A ∧ b ∈ B}) ∩ ↑gconv({a ⊗ b | a ∈ A ∧ b ∈ B})
The operations construct nonempty g-convex lenses by Lemmas 4.4 and 4.12. As conditional-choice and composition operations are Scott-continuous on kernels, the lifted operations are also Scott-continuous in the powerdomain.
Lemma 4.14 The operations φ3G and ⊗G are Scott-continuous for all φ ∈ W(X).
Finally, we define a formal union operation −∪−G as in Prop. 3.5 to interpret
def
nondeterministic-choice as A −∪−G B = C ∩ ↑C where C is gconv(A ∪ B).
Lemma 4.15 The formal union −∪−G is a Scott-continuous semilattice operation on
GK(X).
Example 4.16 Recall the probabilistic program P in Ex. 3.7:
if   then t := t +1 else t := t − 1 fi
the state space X is Q, and we want to show that for any probabilistic refinement Pr of
P (i.e.,  is refined by prob(r)), for input values t1, t2 of t, we have Et′ ~Δ ,t′ ~Δ [tj −
1	1 2	2 1
tj ]= t1 − t2, where the program Pr ends up with a distribution Δ1 starting with
t = t1 and Δ2 with t = t2.
With the g-convex powerdomain GK(X) for nondeterminism-first, t := t +1 and t := t − 1 are assigned semantic objects {λt.δ(t + 1)} and {λt.δ(t − 1)}, respectively. Thus the nondeterministic-choice is interpreted as a subset of {λt.δ(t+1)}−∪−G{λt.δ(t− 1)}, which is {κr | r ∈ [0, 1]}, where κr = λt.r · δ(t + 1)+ (1 − r) · δ(t − 1) is the kernel for the deterministic refinement Pr of P . Therefore for every r ∈ [0, 1], we
have Et′ ~Δ ,t′ ~Δ [tj − tj ]= Et′ ~κ (t ),t′ ~κ (t )[tj ] − Et′ ~κ (t ),t′ ~κ (t )[tj ]= (r(t1 +

1	1 2	2  1	2
1	r  1  2
r  2	1
1	r  1  2
r  2	2

1) + (1 − r)(t1 − 1)) − (r(t2 + 1)+ (1 − r)(t2 − 1)) = t1 − t2.
In contrast, if we started with the convex powerdomain PD(X) reviewed in
§3.3 for nondeterminism-last, we would obtain the semantic object λt.{r · δ(t + 1)+ 

(1 − r) · δ(t − 1) | r ∈ [0, 1]} for the program P , as shown in Ex. 3.7. Now the refinements of P include some κ such that κ(t1) = 0.5 · δ(t1 + 1)+ 0.5 · δ(t1 − 1) and κ(t2)= 0.3 · δ(t2 + 1)+ 0.7 · δ(t2 − 1), thus we are not able to prove the claim E[tj − tj ]= t1 − t2.
1	2
An Algebraic Denotational Semantics
The operational semantics described in §2.2 presents a reasonable model for eval- uating single-procedure probabilistic programs without nondeterminism. In this section, we develop a general denotational semantics for CFHGs (introduced in
§2.1) of multi-procedure probabilistic programs with nondeterminism. The seman- tics is algebraic in the sense that it could be instantiated with different concrete models of nondeterminism, e.g., nondeterminism-last reviewed in §3.3, as well as nondeterminism-first developed in §4.3. We will show the denotational semantics is equivalent to the operational semantics in §2.2 if we suppress procedure calls and nondeterminism in the programming model.

A Fixpoint Semantics based on Markov Algebras
The algebraic denotational semantics is obtained by composing Ctrl(e) operations along hyper-edges. The semantics of programs is determined by an interpretation, which consists of two parts: (i) a semantic algebra, which defines a set of possible program meanings, and which is equipped with sequencing, conditional-choice, and nondeterministic-choice operators to compose these meanings, and (ii) a semantic function, which assigns a meaning to each data action act ∈ Act. The semantic algebras that we use are Markov algebras introduced in [64]:
Definition 5.1 A Markov algebra (MA) over a set Cond of deterministic conditions is a 7-tuple M = ⟨M, ±M , ⊗M , φ3M , −∪−M , ⊥M , 1M ⟩, where ⟨M, ±M ⟩ forms a dcpo with ⊥M as its least element; ⟨M, ⊗M , 1M ⟩ forms a monoid (i.e., ⊗M is an asso-
ciative binary operator with 1M as its identity element); ϕ3M is a binary operator parametrized by a condition ϕ ∈ Cond; −∪−M is idempotent, commutative, associative and for all a, b ∈ M and ϕ ∈ Cond we have a ϕ3M b ≤M a −∪−M b where ≤M is the semilattice ordering induced by −∪−M (i.e., a ≤M b if a −∪−M b = b); and ⊗M , ϕ3M , −∪−M are Scott-continuous.
Example 5.2 Let Ω be a nonempty countable set of program states and Cond be a set of deterministic conditions, the definition and meaning of which are given in §2.1 and §2.2.
The  convex  powerdomain  PD(Ω)  admits  an  MA  ⟨Ω   →
PD(Ω), ±˙ P , ⊗P , ϕ3P , −∪−˙ P , ⊥˙ P , 1P ⟩, where ±˙ P , −∪−˙ P , ⊥˙ P	are pointwise ex-

tensions of ±P , −∪−P , ⊥P , defined in §3.3, and g ⊗P
h d=ef ^h^ ◦ g where ^h^ is given

by Prop. 3.6, g
ϕ3P
h d=ef λω.g(ω)
Jϕ)(ω)⊕P
h(ω), as well as 1P
d=ef λω.{δ(ω)}.

The	g-convex	powerdomain	GK(Ω)	admits	an	MA	⟨GK(Ω), ±G

, ⊗G, ϕ3 , −∪−G, ⊥G, 1G⟩, where ±G, ⊗G, ϕ3 , −∪−G, ⊥G come from §4.3, 3
and 1 d=ef {λω.δ(ω)}.
Definition 5.3 An interpretation is a pair I = ⟨M, J·)I ⟩, where M is an MA and J·)I : Act → M. We call M the semantic algebra of the interpretation and J·)I the semantic function.
Example 5.4 We can lift the interpretation of data actions defined in Fig. 3 to seman- tic functions with respect to convex or g-convex powerdomains—P = ⟨PD(Ω), J·)P⟩ with Jact)P d=ef λω.{Jact) (ω)} and G = ⟨GK(Ω), J·)G ⟩ with Jact)G d=ef {Jact)}.
Given a probabilistic program P	=	{Hi}1≤i≤n  where each Hi	=
⟨Vi, Ei, ventry, vexit⟩ is a CFHG, and an interpretation I = ⟨M, J·)I ⟩, we define
i	i
I [P ] to be the interpretation of the probabilistic program, as the least fixpoint of
the function FP , which is defined as


λS.λv.
⎧⎨ −−
 C^trl(e)(S(u1), ··· , S(uk)) | e = ⟨v, {u1, ··· , uk}⟩ ∈ E, v /= vexit for all i

1M	otherwise
where C^trl(e) for different kinds of control-flow actions is defined as follows:

s^eq[act](S1) d=ef Jact)I ⊗
S1, co^nd[ϕ](S1, S2) d=ef S1
ϕ3M
S2,
cal^l[i → j](S1) d=ef S(ventry) ⊗
S1.

The least fixpoint of FP exists by Prop. 3.1 as well as the following lemma. Hence

the semantics of the procedure Hi
is given by JHi)ds
d=ef (lfp±˙ M F
⊥M
)(ventry).

Lemma 5.5 The function FP is Scott-continuous on the dcpo ⟨V → M, ±˙ M ⟩ with
⊥˙	d=ef λv.⊥	as the least element, where ±˙	is the pointwise extension of ± .
Proof. Appeal to the Scott-continuity of the operations ⊗M , ϕ3M , and −∪−M .	 
An Equivalence Result
To justify the denotational semantics proposed in §5.1, we go back to the restricted programming language used to define the operational semantics in §2.2. If we suppress the features of multi-procedure and nondeterminism, we should end up with a semantics that is equivalent to the operational semantics J·)os.
Lemma 5.6 Let P = ⟨V, E, ventry, vexit⟩ be a deterministic single-procedure proba- bilistic program.
If we interpret P  using P  = ⟨PD(Ω), J·)P⟩, we will have JP )ds  =
λω.{JP )os (ω)}.
If we interpret P using G = ⟨GK(Ω), J·)G ⟩, we will have JP )ds = {JP )os}.

Proof. Recall the definition JP ) d=ef λω. sup
n∈N
{Δ | ⟨ventry, ω⟩ −→n Δ}.	On

the other hand,	the fixpoint (lfp±˙ M F
M
)(ventry)  is  actually  obtained  by



3 The conditional-choice is actually interpreted as Jϕ)3G in the powerdomain.

.†	Fn(⊥˙ M )(ventry) by Prop. 3.1. The proof proceeds by induction on n.	 


Application:	Static Analysis for Probabilistic Pro- grams with Nondeterminism
A lot of recent studies on probabilistic programming focus on rigorous reasoning about probabilistic programs (e.g., [52,53,39,4,17,11,57,12,10,34,28,13,38,55,9,5]). In this section, we discuss an application of the new denotational semantics as the concrete semantics of a static-analysis framework for probabilistic programs. More details about the static analysis and its soundness proof can be found in a companion paper [64].

Definition 6.1 A pre-Markov algebra (PMA) over a set Cond of deterministic
conditions is a 7-tuple M  = ⟨M, ±M , ⊗M , ϕ3  , −∪−M , ⊥M , 1M ⟩, which is essentially
an MA, except that ⟨M, ±M ⟩ forms a complete lattice, and ⊗M , ϕ3M , and −∪−M are only required to be monotone.

Intuitively, PMAs specify abstract semantics used in static analyses. We can define interpretations with respect to PMAs in the same way, except that we obtain the least fixpoint I [P ] of the function FP by the Knaster-Tarski theorem, given a probabilistic program P and an interpretation I = ⟨M , J·)I ⟩.

Definition 6.2 A probabilistic over-abstraction (resp., under-abstraction) from an MA C (i.e., a concrete semantics such as PD(Ω) and GK(Ω)) to a PMA Y is a concretization mapping, γ : Y → C, such that
⊥C ±C γ(⊥Y ) (resp., γ(⊥Y ) ±C ⊥C),
1C ±C γ(1Y ) (resp., γ(1Y ) ±C 1C),
for all Q1, Q2 ∈ Y , γ(Q1) ⊗C γ(Q2) ±C γ(Q1 ⊗Y Q2) (resp., γ(Q1 ⊗Y Q2) ±C
γ(Q1) ⊗C γ(Q2)),
for all Q1, Q2 ∈ Y , γ(Q1) ϕ3C γ(Q2) ±C γ(Q1 ϕ3Y Q2) (resp., γ(Q1 ϕ3Y Q2) ±C
γ(Q1) ϕ3C γ(Q2)), and
for all Q1, Q2 ∈ Y , γ(Q1) −∪−C γ(Q2) ±C γ(Q1 −∪−Y Q2), (resp., γ(Q1 −∪−Y Q2) ±C
γ(Q1) −∪−C γ(Q2)).

A probabilistic abstraction leads to a sound analysis:

Theorem 6.3 Let C and Y be interpretations over an MA C and a PMA Y; let γ be a probabilistic over-abstraction (resp., under-abstraction) from C to Y; and let P be an arbitrary program. If for all data actions act, Jact)C ±C γ(Jact)Y ) (resp., γ(Jact)Y ) ±C Jact)C ), then we have C [P ]±˙ C γ˙ (Y [P ]) (resp., γ˙ (Y [P ])±˙ CC [P ]).

Discussion
Continuous Distributions
One of the most important features of probabilistic programming is continuous probability distributions over real numbers, such as Gaussian distributions. Notions from measure theory, such as measures and kernels, are extensively used to model continuous distributions in probabilistic programming. Kozen studied the relation between deterministic probabilistic programs and continuous distributions via a metric on measures [43]. Many approaches use probability kernels [44,58], sub-probability kernels [8], and s-finite kernels [59,7]. A different approach uses measurable functions A → D(R≥0 × B) where D(S) stands for the set of all probability measures on S [60]. For higher-order languages, Jones and Plotkin [35,36] have developed a probabilistic powerdomain that consists of continuous evaluations, which are a reformulation of distributions in domain theory, on a state space. They show that the powerdomain can be used to solve recursive domain equations. Smolka et al. [58] study the semantics of probabilistic networks. Ehrhard et al. [20] provide a Cartesian-closed category on stable and measurable maps between cones, and use it to give a semantics for probabilistic PCF.
However, those measure-theoretic developments do not work properly when nondeterminism comes into the picture. To overcome this challenge, people have been adapting domain-theoretic results. McIver and Morgan build a Plotkin-style powerdomain over probability distributions on a discrete state space [47,48]. Mislove et al. [50,51] study powerdomain constructions for probabilistic CSP. Tix et al. [62] generalize McIver and Morgan’s results to continuous state spaces, and construct three powerdomains for the extended probabilistic powerdomains. Although there has been a lot of work on this direction, one has to keep in mind that the domain- theoretic notion of “continuous” distributions is different from the notion in measure theory—instead, the domain-theoretic studies are focused on computable distributions. In other words, real numbers are realized by some computable models, such as partial reals [21]. These models would become unsatisfactory when one wants to observe a random value drawn from a continuous distribution, e.g., the meaning of x := Normal(0, 1); if x = 0 then ··· fi is not expressible. We leave the semantic development of combining nondeterminism and continuous distributions (from a measure-theoretic perspective) for future work.
Higher-Order Functions
In functional programming, higher-order functions are functions that can take func- tions as arguments, as well as return a function as a result. Some probabilistic programming languages, such as Church [30], are indeed functional programming languages and can express higher-order functions. While operational models for prob- abilistic functional programming have been proposed [8], developing a denotational semantics for higher-order probabilistic programming has been an open problem for years.
The major challenge is to propose a Cartesian-closed category for semantic objects

of probabilistic programming. Intuitively, the Cartesian-closure property ensures that if type A and type B are two objects in the category, then the function space BA (i.e., an object for the arrow type A → B) is also contained in the category. The category of measures is clearly not Cartesian-closed; a lot of probabilistic powerdomains also do not admit a Cartesian-closed category [37]. Recently, Heunen et al. [32] propose quasi-Borel measures for higher-order functions in probabilistic programming. The measure-theoretic approach is further extended by Vákár et al. [63] to support recursive types. However, it is unclear how to model nondeterminism in the framework of quasi-Borel measures. We leave the combination of nondeterminism and higher-order functions for future work.

Conclusion
We have developed a framework for denotational semantics of low-level probabilistic programs with unstructured control-flow, general recursion, and nondeterminism, represented by control-flow hyper-graphs. The semantics is algebraic and it can be instantiated with different models of nondeterminism. We have demonstrated two instantiations with nondeterminism-first and nondeterminism-last, respectively. We have proposed a powerdomain for nondeterminism-first that consists of collections of kernels and enjoys generalized convexity. As an application, we have reviewed a static-analysis framework for probabilistic programs, which has been proposed in a companion paper.
In the future, we plan to combine continuous distributions and higher-order functions with nondeterminism in our semantics framework. We will also work on models of nondeterminism, especially nondeterminism-first, and investigate its connection with relational reasoning. Another research direction is to develop more formal reasoning techniques based on the denotational semantics.

Acknowledgement
This work was supported, in part, by a gift from Rajiv and Ritu Batra; by AFRL under DARPA MUSE award FA8750-14-2-0270, DARPA STAC award FA8750-15-C-0082
and DARPA AA award FA8750-18-C-0092; by ONR under grant N00014-17-1-2889; by NSF under SaTC award 1801369, SHF grant 1812876, and CAREER award 1845514; and by the UW-Madison OVRGE with funding from WARF.

References
Abramsky, S. and A. Jung, Domain Theory, in: Handbook of Logic in Computer Science, Oxford University Press Oxford, UK, 1994 .
	Barthe, G., T. Espitau, M. Gaboardi, B. Grégoire, J. Hsu and P.-Y. Strub, A Program Logic for Probabilistic Programs, Available on: https://justinh.su/files/papers/ellora.pdf (2016).
	Barthe, G., B. Grégoire and S. Zanella Béguelin, Formal Certiﬁcation of Code-based Cryptographic Proofs, in: Princ. of Prog. Lang. (POPL’09), 2009.

	Barthe, G., B. Köpf, F. Olmedo and S. Zanella Béguelin, Probabilistic Relational Reasoning for Diflerential Privacy, in: Princ. of Prog. Lang. (POPL’12), 2012.
	Batz, K., B. L. Kaminski, J.-P. Katoen and C. Matheja, How long, O Bayesian network, will I sample thee?, in: European Symp. on Programming (ESOP’18), 2018.
Bellman, R., A Markovian Decision Process, Indiana Univ. Math. J. 6 (1957).
	Bichsel, B., T. Gehr and M. Vechev, Fine-grained Semantics for Probabilistic Programs, in: European Symp. on Programming (ESOP’18), 2018.
	Borgström, J., U. D. Lago, A. D. Gordon and M. Szymczak, A Lambda-Calculus Foundation for Universal Probabilistic Programming, in: Int. Conf. on Functional Programming (ICFP’16), 2016.
	Bouissou, O., E. Goubault, S. Putot, A. Chakarov and S. Sankaranarayanan, Uncertainty Propagation Using Probabilistic Aﬃne Forms and Concentration of Measure Inequalities, in: Tools and Algs. for the Construct. and Anal. of Syst. (TACAS’16), 2016.
	Brázdil, T., S. Kiefer, A. Kučera and I. H. Vařeková, Runtime Analysis of Probabilistic Programs with Unbounded Recursion, J. Comput. Syst. Sci. 81 (2015).
	Chakarov, A. and S. Sankaranarayanan, Probabilistic Program Analysis with Martingales, in: Computer Aided Verif. (CAV’13), 2013.
	Chakarov, A. and S. Sankaranarayanan, Expectation Invariants for Probabilistic Program Loops as Fixed Points, in: Static Analysis Symp. (SAS’14), 2014.
	Chatterjee, K., H. Fu and A. K. Goharshady, Termination Analysis of Probabilistic Programs Through Positivstellensatz’s, in: Computer Aided Verif. (CAV’16), 2016.
	Chatterjee, K., H. Fu, P. Novotný and R. Hasheminezhad, Algorithmic Analysis of Qualitative and Quantitative Termination Problems for Aﬃne Probabilistic Programs, in: Princ. of Prog. Lang. (POPL’16), 2016.
	Chatterjee, K., P. Novotný and Ð. Žikelić, Stochastic Invariants for Probabilistic Termination, in: Princ. of Prog. Lang. (POPL’17), 2017.
Conway, J. H., “ Regular algebra and finite machines,” London: Chapman and Hall, 1971.
	Cousot, P. and M. Monerau, Probabilistic Abstract Interpretation, in: European Symp. on Programming (ESOP’12), 2012.
den Hartog, J. I. and E. P. de Vink, Mixing Up Nondeterminism and Probability: a preliminary report, Electr. Notes Theor. Comp. Sci. 22 (1999).
Dijkstra, E. W., “ A Discipline of Programming,” Prentice-Hall, 1997.
	Ehrhard, T., M. Pagani and C. Tasson, Measurable Cones and Stable, Measurable Functions, in: Princ. of Prog. Lang. (POPL’18), 2018.
Escardó, M. H., PCF extended with real numbers, Theor. Comp. Sci. 162 (1996).
	Etessami, K. and M. Yannakakis, Recursive Markov Chains, Stochastic Grammars, and Monotone Systems of Nonlinear Equations, in: Symp. on Theor. Aspects of Comp. Sci. (STACS’05), 2005.
Etessami, K. and M. Yannakakis, Recursive Markov Decision Processes and Recursive Stochastic Games,
J. ACM 62 (2015).
	Farzan, A. and Z. Kincaid, Compositional Recurrence Analysis, in: Formal Methods in Compiter-Aided Design (FMCAD’15), 2015.
	Ferrer Fioriti, L. M. and H. Hermanns, Probabilistic Termination: Soundness, Completeness, and Compositionality, in: Princ. of Prog. Lang. (POPL’15), 2015.
	Franke, B., M. O’Boyle, J. Thomson and G. Fursin, Probabilistic Source-Level Optimisation of Embedded Programs, in: Lang., Comp., and Tools for Embeded Syst. (LCTES’05), 2005.
Gallo, G., G. Longo, S. Pallottino and S. Nguyen, Directed Hypergraphs and Applications, Disc. Appl. Math. 42 (1993).

Gehr, T., S. Misailovic and M. Vechev, PSI: Exact Symbolic Inference for Probabilistic Programs, in:
Computer Aided Verif. (CAV’16), 2016.
Ghahramani, Z., Probabilistic machine learning and artiﬁcial intelligence, Nature (2015).
	Goodman, N. D., V. K. Mansinghka, D. M. Roy and J. B. Tenenbaum, Church: a language for generative models, in: Uncertainty in Artif. Intelligence, 2008.
	Gordon, A. D., T. A. Henzinger, A. V. Nori and S. K. Rajamani, Probabilistic Programming, in: Future of Softw. Eng. (FOSE’14), 2014.
	Heunen, C., O. Kammar, S. Staton and H. Yang, A Convenient Category for Higher-Order Probability Theory, in: Logic in Computer Science (LICS’17), 2017.
	Hofmann, K. H. and M. Mislove, Local compactness and continuous lattices, in: Continuous Lattices, 1981.
	Jansen, N., B. L. Kaminski, J.-P. Katoen, F. Olmedo, F. Gretz and A. K. McIver, Conditioning in Probabilistic Programming, Electr. Notes Theor. Comp. Sci. 319 (2015).
Jones, C., “ Probabilistic Non-determinism,” Ph.D. thesis, University of Edinburgh (1989).
	Jones, C. and G. Plotkin, A Probabilistic Powerdomain of Evaluations, in: Logic in Computer Science (LICS’89), 1989.
Jung, A. and R. Tix, The Troublesome Probabilistic Powerdomain, Electr. Notes Theor. Comp. Sci. 13
(1998).
	Kaminski, B. L., J.-P. Katoen, C. Matheja and F. Olmedo, Weakest Precondition Reasoning for Expected Run—Times of Probabilistic Programs, in: European Symp. on Programming (ESOP’16), 2016.
	Katoen, J.-P., A. K. McIver, L. A. Meinicke and C. C. Morgan, Linear-Invariant Generation for Probabilistic Programs: Automated Support for Proof-Based Methods, in: Static Analysis Symp. (SAS’10), 2010.
	Kattenbelt, M., M. Kwiatkowska, G. Norman and D. Parker, Abstraction Reﬁnement for Probabilistic Software, in: Verif., Model Checking, and Abs. Interp. (VMCAI’09), 2009.
Kleene, S. C., Representation of Events in Nerve Nets and Finite Automata, Available on https:
//www.rand.org/pubs/research_memoranda/RM704.html  (1951).
Kozen, D., On induction vs. *-continuity, in: Workshop on Logic of Programs, 1981.
Kozen, D., Semantics of Probabilistic Programs, J. Comput. Syst. Sci. 22 (1981).
Kozen, D., A Probabilistic PDL, J. Comput. Syst. Sci. 30 (1985).
Kozen, D., A Completeness Theorem for Kleene Algebras and the Algebra of Regular Events, J. Information and Computation 110 (1991).
	Lal, A., T. Touili, N. Kidd and T. Reps, Interprocedural Analysis of Concurrent Programs Under a Context Bound, in: Tools and Algs. for the Construct. and Anal. of Syst. (TACAS’08), 2008.
McIver, A. K. and C. C. Morgan, Partial correctness for probabilistic demonic programs, Theor. Comp. Sci. 266 (2001).
McIver, A. K. and C. C. Morgan, “ Abstraction, Refinement and Proof for Probabilistic Systems,” Springer Science+Business Media, Inc., 2005.
Mislove, M., Topology, domain theory and theoretical computer science, Topology and its Applications
89 (1998).
	Mislove, M., Nondeterminism and Probabilistic Choice: Obeying the Laws, in: Concurrency Theory, 2000.
	Mislove, M., J. Ouaknine and J. Worrell, Axioms for Probability and Nondeterminism, Electr. Notes Theor. Comp. Sci. 96 (2004).
	Monniaux, D., Abstract Interpretation of Probabilistic Semantics, in: Static Analysis Symp. (SAS’00), 2000.

	Monniaux, D., Abstract Interpretation of Programs as Markov Decision Processes, in: Static Analysis Symp. (SAS’03), 2003.
	Müller-Olm, M. and H. Seidl, Precise Interprocedural Analysis through Linear Algebra, in: Princ. of Prog. Lang. (POPL’04), 2004.
	Olmedo, F., B. L. Kaminski, J.-P. Katoen and C. Matheja, Reasoning about Recursive Probabilistic Programs, in: Logic in Computer Science (LICS’16), 2016.
	Paige, B. and F. Wood, A Compilation Target for Probabilistic Programming Languages, in: Int. Conf. on Machine Learning (ICML’14), 2014.
	Sankaranarayanan, S., A. Chakarov and S. Gulwani, Static Analysis for Probabilistic Programs: Inferring Whole Program Properties from Finitely Many Paths, in: Prog. Lang. Design and Impl. (PLDI’13), 2013.
	Smolka, S., P. Kumar, N. Foster, D. Kozen and A. Silva, Cantor meets Scott: Semantic Foundations for Probabilistic Networks, in: Princ. of Prog. Lang. (POPL’17), 2017.
	Staton, S., Commutative Semantics for Probabilistic Programming, in: European Symp. on Programming (ESOP’17), 2017.
	Staton, S., H. Yang, C. Heunen and O. Kammar, Semantics for probabilistic programming: higher-order functions, continuous distributions, and soft constraints, in: Logic in Computer Science (LICS’16), 2016.
Tarjan, R. E., A Uniﬁed Approach to Path Problems, J. ACM 28 (1981).
Tix, R., K. Keimel and G. Plotkin, Semantic Domains for Combining Probability and Non-Determinism, Electr. Notes Theor. Comp. Sci. 222 (2009).
Vákár, M., O. Kammar and S. Staton, A Domain Theory for Statistical Probabilistic Programming, in:
Princ. of Prog. Lang. (POPL’19), 2019.
	Wang, D., J. Hoffmann and T. Reps, PMAF: An Algebraic Framework for Static Analysis of Probabilistic Programs, in: Prog. Lang. Design and Impl. (PLDI’18), 2018.
