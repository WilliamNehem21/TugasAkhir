	Electronic Notes in Theoretical Computer Science 201 (2008) 223–253	
www.elsevier.com/locate/entcs


Power Aware System Refinement
Johanna Tuominen 1,2 , Tomi Westerlund 2 and Juha Plosila 3
1 Turku Centre for Computer Science, Finland
2 Dept, of Information Technology, University of Turku, Finland
3 Academy of Finland, Research Council for Natural Sciences and Engineering

Abstract
We propose a formal, power aware refinement of systems. The proposed approach lays its foun- dation to the traditional refinement calculus of Action Systems and its direct extension, time wise refinement method. The adaptation provides well-founded mathematical basis for the systems modeled with the Timed Action Systems formalism. In the refinement of an abstract system into more concrete one a designer must that show conditions of both functional and temporal properties, and furthermore, power related issues are satisfied.
Keywords: refinement, time, power, Timed Action Systems

Introduction
Advances in technology over recent years have considerably decreased both physical and functional size of single chip systems. Moreover, with the advent of wireless and mobile high performance computing platforms, and the limited operational life time of batteries, the low-power designs are required. To estimate the power consumption, there is a trade-off between the accuracy and the abstraction level of detail which the system is analyzed. The more detailed the description, the more accurate the simulation will be, but on the other hand the more time consuming it will be. Moreover, a designer wants to make decisions as early as possible in the design flow to avoid costly design backtracking.
Formal methods provide an environment to design, analyze, and verify digital hardware with the benefits of rigorous mathematical basis. The base formalism in our study is Action Systems [5], henceforward conventional Ac-


1571-0661© 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.02.022

tion Systems, which is based on an extended version of Dijkstra’s language of guarded commands [14]. We continue to base our research on the conventional Action Systems formalism and its timed extension [22]. The timing informa- tion allows the modeling and analysis of both logical and temporal aspects of both synchronous [18] and asynchronous [17] systems, and, furthermore, time is an essential concept for modeling power consumption.
In this study, we introduce a power modeling methodology in the Timed Action Systems context. It allows us to estimate energy and power consump- tion in a formal, abstract models, and to steer the development towards power- efficient implementations. Furthermore, we extend the refinement calculus for Timed Action Systems by introducing performance related constraints to it. The final goal in out research is to have modeling framework for SoC/NoC (System-on-chip/Network-on-Chip) systems in which, within one formalism, a system can be modeled from a specification down to an implementation model. The development of the systems starts from the high level system model S whose functional and temporal characteristics are given in the spec- ification. The refinement relations between system models can be defined so that the total correctness is preserved: if S ± S' and pSq then pS'q will also hold. After several successive refinement steps (S ± S' ... ± Sn ) we obtain an implementable specification [3]. Thus, we have developed, in a stepwise manner, an implementation of the original specification. The purpose is to be able to model SoC/NoC designs within one formalism from a specification down to an implementable model by taking into account the correctness of both the temporal and functional characteristics, and, furthermore, the power modeling issues.



Timed Action Systems

In this section we shortly review both the conventional and timed actions that form the formal basis for out power analysis.



Conventional Actions
An action A is defined (for example) by:





A ::= abort	(abortion, non-termination)
| skip	(empty statement)
| x := x'.R	(non-deterministic assignment)
| x := e	((multiple) assignment)
| {p}	(assertion statement)
| [p]	(assumption statement)
| p → A	(guarded action)
| A1 ▢ A2	(non-deterministic choice)
| A1 ; A2	(sequential composition)
| A1 L A2	(prioritized composition)
| do A od	(iterative composition)


where A and Ai, i = 1, 2, are actions; x is a variable or a list of variables; e is an expression or a list of expressions; and p and R are predicates (Boolean conditions). The variables which are assigned within the action A are called the write variables of A, denoted by wA. The other variables present in the action A are called the read variables of A, denoted by rA. The write and
read variables form together the access set vA of A: vA =^ wA ∪ rA.
A quantiﬁed composition of actions is defined by: [• 1 ≤ i ≤ n : Ai], and it
is defined by: A1 • ... • An, where the bullet • denotes any of the composition operators, and n is the number of actions. Furthermore, a substitution oper- ation within an action Ai, denoted by A[e'/e], where e refers to an element such as variables and predicates of the original action Ai and e' denotes the new element, which replaces e in Ai. The same notation is applied to action systems as well.
A prioritized (’  ’) composition [19] is a composition in which the execu- tion order of enabled actions is prioritized. We have: A  B = A ▢ ¬gA → B, where the highest priority belongs to the leftmost action in the composition; thus, the leftmost enabled action is always chosen for execution.
Body P of the procedure p : p(in x; out y) : P , is in general any atomic action A, possibly with some auxiliary local variables w initialized to w0 every time the procedure is called. The action A accesses the global and local variables g and l of the host/enclosing system and the formal parameters x and y. Hence, the body P can be generally defined by: P [var v; init w := w0; A(g, l, w, x, y)]. If there are no local variables, the begin-end brackets [ ] can be removed together: [A(g, l, x, y]= A(g, l, x, y). If there are neither local variables nor parameters, the action only accesses the global and local variables of the host system, then the procedure p can be written as: proc p : A(g, l).

Semantics of actions
An action is considered to be atomic, which means that only the initial and final states are observed by the system. Therefore, when action is selected for execution, it is completed without any interference from other actions. The actions are defined using weakest precondition for predicate transformers [14]. The weakest precondition for action A to establish the post condition q is defined for example: wp(abort, q) = f alse, wp(skip, q) = q and wp((A ▢ B), q) = wp(A, q) ∧ wp(B, q). The guard gA of an action A is defined by

gA =^ ¬wp(A, false). Considering a guarded action A =^
P → B we have

that gA =^ P ∧ gB. An action A is said to be enabled in some state, if its
guard is true (T ) in that state, otherwise disabled. The action A is said to be
always enabled, if wp(A, false)= f alse (that is, the guard gA is invariantly true: gA = true). Furthermore, if wp(A, true) = true holds, the action A is said to be always terminating. The body sA of the action A is defined
by: sA =^ A ▢ ¬gA → abort. Moreover, for assert statement, we have that
wp({p}, q)= (p ∧ q) and for the assumption statement we have: wp([p], q)= 
p → q.
Timed Action System
Above we introduced conventional Action Systems, which is the formal basis of our timed notation. In conventional Action Systems computation does not take time, a reaction is instantaneous – and therefore atomic in any possible sense. Atomicity means that only the pre- and post-states of actions are observable, and when they are chosen for execution they cannot be interrupted by external counterparts. In modeling SoC/NoC systems it is important to know the time consumed by actions, because the operation speed is determined by the delay of those actions, and, furthermore, time is an essential element in power modeling. It should be observed that the complexity of a timed action is not restricted, and thus the operation time is not bounded either. Let us next introduce the form of a timed action system [22].
A timed action system A has a form:
sys	A	(	imp pI ; exp pE ; )( u; ) ::
|[		delay		dAi, dp; constraint	{A}; var	l;
private proc	p dp)(x): (P );
public proc	pE dpE)(x): (PE );
action	Ai dAi) : (aAi); init g, l : g0, l0;
exec	do ˆ ▢ 1 ≤ i ≤ n : Ai˜ od  ]|

where we can identify three main sections: (1) interface, (2) declaration, and
(3) iteration:
The interface part declares those variable, u, that are visible outside the action system boundaries, and thus accessible by another systems. It also introduces communication procedures that are either imported or exported by the system. The exported procedures are introduced by the system itself and the imported ones are introduced by some other systems. If a timed action system does not have any interface variables or procedures, it is a closed system, otherwise it is an open system.
The declaration part declares the building blocks of the systems. In the delay clause it defines the delays of the timed actions, which are associ- ated with a timed action using delay brackets  ). The most commonly used delays are deterministic and bounded, non-deterministic delays de- noted by dA and dA : [dAmin, dAmax], respectively. The former defines a precise delay, whereas the latter one a delay whose value is chosen non- deterministically between the given interval. The time domain is dense and continuous T = R≥0, because it is a natural model for systems operating over continuous time. In the constraint clause is defined the functional and non-functional requirements of the system, and in the var clause is defined the local variables l. Then in the private and public procedure clauses is defined local p and exported pE procedures. The private (local) procedures are used only within the system, whereas the public ones are used in communication with other systems. In the action clause is given the definitions of the action Ai that perform operations (aAi) on local and global variables. The start time of a timed action is denoted by A.st and the finish time by A.f t whose separation in time is dA. The last item is the init clause that set the system in a safe state from which the system may commence its operation.
Finally, the iteration section, the exec do-od loop of the system, contains the composition of the actions defined in the declarative part. The compu- tation of a timed action system is started in an initialization in which the variables (both global and local ones) are set to predefined values. In the it- eration part, the actions are selected for execution based on the composition and enabledness of the timed actions. If there are no such timed actions, the timed action system is considered temporary delayed. The computation resumes to execution when some other timed action system enables one of the action via interface variables or procedure calls.

Parallel Composition of Action Systems
Consider two action systems A and B whose local variables are distinct, lA ∩ lB = ∅, and communication variables are a set uA ∩ uB. We require that the initializations of the communication variables uA ∩ uB are consistent with each other, so that the initial values are equivalent: ∀v ∈ uA ∩ uB.(v0A = v0B), where v0A ∈ u0A and v0B ∈ u0B. The parallel composition of A and B, denoted A  B is defined to be another action system whose global and local identifiers (procedures, variables, actions) consist of the identifiers of the component systems and whose exec-clause has the form: exec do A ▢ B od, where A and B are the actions of the systems A and B, respectively. The constituent systems communicate via their shared interface procedures. The definition of the parallel composition is used inversely in system derivation to decompose a system description into a composition of smaller separate systems or internal subsystems.

Procedure based communication
The procedure based communication [17] uses remote procedures to model communication channels between action systems. Consider the timed ac- tion systems Snd and Rec whose internal activities are denoted with actions S = (S1; call p(ls); S2) and R = (R1; await p; R2), where p is an interface pro- cedure defined in and exported by the receiver (Rec), and imported and called by the sender (Snd), with the variables ls (sender’s local variables) as actual parameters. The body P of p can be any atomic action writing onto variables lr (receiver’s local variables). The exec-clause is of the composed system Snd  Rec has the form: do S ▢ R od. The construct S ▢ R, where S calls p (call command) and R awaits such a call (await command), is regarded as a single atomic action SR, defined by: SR = (S1; R1; P [ls/x]; R2; S2). Hence, commu- nication is based on sharing an action in which data is atomically passed from Snd to Rec by executing the body P of the procedure p hiding the details of the communication into the procedure call.

Performance modeling
In this section, we present a performance modeling framework for hardware systems by introducing how to model a size of an action system (timed or untimed one). Henceforward, we denote the size as area complexity. The purpose is to develop a framework that can be used to identify which one of the two arbitrary actions is larger, and therefore to make difference in energy consumption between these two actions. Furthermore, the energy of actions

enable

y	x




Substitution opera- tion x := y
Guarded non-deterministic as- signment g → x := x′.Q

Fig. 1. Illustrations of assignments.
is adopted to model power consumption of the formal system description. When reasoning power consumption, the Timed Action Systems formalism is assumed.
Area complexity
The preliminary constraint in our modeling approach is that we consider the non-deterministic assignment as a base action in our complexity model. The non-deterministic assignment, also know as a specification statement, is the generalization of the assignment operation. In other words, it can be used to describe any operations targeted to variables in action context. Furthermore, by adopting the guard g, we obtain a guarded non-deterministic assignment, which is executed when its guard evaluates to true. In addition, an action is considered atomic, which means that only its initial and final states are observable. That is, we utilize the information received from the set of read (input) and set of write (output) variables. We start the analysis by giving some illustrations for the internal structure of an action. In Fig. 1 (a) we have a substitution operation and in Fig 1 (b) a guarded non-deterministic assignment. Observe that, the non-deterministic assignment can be illustrated as depicted in Fig. 1 (b) by excluding the guard g.
Widths of types
Consider an (atomic) action A. We assume that each variable v ∈ vA has a width wv which defines the number of bits needed to present the variable. For example, consider an integer type of variable v, which is defined by the interval (1..m), where m ∈ R+. The width is calculated by: 2wv = m ⇒ wv = [log2(m)|, (m > 1). In this paper, we consider only fixed point numbers, and, furthermore, constants are considered as variables in the modeling process.
Complexity of a substitution operation
Let an action A be a substitution A = x := v. Its write set is wA = {x}, and it is utilized to model the area complexity. If x and v are single variables of the

same type then the area complexity of A is the width wx of the variable x. If x and v are lists of variables then the complexity is defined by the sum of the widths of the variables x. The area complexity of the substitution operation

is defined by: C(wA) =^
x∈wA


Complexity of assignment statements
The assignment statements often contains operations that are Boolean func- tions at lower abstraction levels, such as, addition and multiplication. These operations are combinatorial logic at lower abstraction levels, and therefore, we cannot illustrate assignments with the chain of registers as we did with sub- stitution operations. That is, to model the structure of combinatorial logic requires, that the depth of the resulting circuit is taken into account in the area complexity model.
The idea behind the area complexity modeling of assignments arises from the graphical modeling of Boolean functions. Boolean functions can be rep- resented as a directed acyclic graph where the size of the function can be evaluated by calculating the number of nodes needed to present the function. These graphs are often denoted by Binary decision diagrams (BDD) and de- scribed, for instance, in [2], [9], [10]. In BDD’s each node has at least the following properties: index if it is a non-terminal node, value 0 or 1 if it is a terminal node. Furthermore, each non-terminal node has two children [2]. There are several extensions for the basic BDD’s. For instance, in Boolean Ex- pression Diagrams [1] there are three node types instead of the two presented above: a terminal node has a value 0 or 1, a variable node has a Boolean variable and operator node has a Boolean operator. Observe that the vari- able and operator nodes correspond the non-terminal nodes in the basic BDD description. However, these graph based presentations are not directly appli- cable to our approach because we do not have a exact Boolean function that could be use to generate the graph. In other words, our approach has a higher
'	'
abstraction level. For instance, for an action A =^ x := x .(x = a + b), which
adds together two variables a and b, we do not have a the detailed description
of its implementation. That is we do not know, for instance, whether it is implemented using full adders or carry look ahead adders.
Another approach to model the size of the Boolean functions is to assume that each Boolean expression with m arguments has a size 2m [9]. This would be more suitable approach for us, since we know the number of the input arguments for each action, that is the sum of the widths of the variables in the read set. However, the size expression grows exponentially, which gives rather pessimistic results when the value of m increases. For instance, if the number of input arguments is 64 (m = 64) then the size estimate would be

(a(0..2), b(0..2))


(a0, b0)
(a1, b1)
(a2, b2)


0	1	0	1	0	1
Fig. 2. Exemplification of the graph presentation.

(approximately) 1.84 · 1019.
We adopt some properties from both above mentioned approaches to our area complexity model. Consider an action A = g → x := x'.Q. Its area com- plexity modeling is divided into three parts: substitution, guard, and predicate. Recall that the complexity of the substitution is evaluated from the write set wA as discussed in Sect. 3.1.2. To be able to calculate guard and predicate complexities, the read set is further divided into two subset: the set of guard variables rg that appear in the guard and the set of predicate variables rQ that appear in the predicate (rA = rg ∪ rQ). The area complexity is modeled for both sets (rg, rQ) separately.
Let us first define the area complexity for the set rQ (rQ  /= ∅):
sW	eL
C(rQ) =^ [	| · 2 , where the eL denotes the number of elements in the
set rQ and it is defined by: eL =^ |rQ|, where |rQ| denotes the cardinality of
the set rQ, the sum of the widths (sW ) of the variables v in the set rQ is
sW =^ Σ	wv. In a similar manner, the complexity for the set rg can be
modeled by substituting the rg to rQ in the definition above. First we discuss
and compare the presented model to the size estimation methods presented above, and then we illustrate our model with an example.
The size estimation according to the input arguments has a drawback that its result increases exponentially. That is, it gives rather pessimistic results with larger number of input arguments, comparing to the size results obtained with BDD’s [1]. Therefore, we adopted properties from the graph based approach presented with BDD’s [9], [10]. More precisely, the first term in the complexity clause ([sW |), divide the sum of the widths (number of input arguments) into smaller groups. These groups resemble non-terminal nodes of BDD graph and each these groups have possible output values {0, 1} which correspond to terminal nodes. In other words, we estimate the area complexity of the predicate or guard by using the variable widths in the group as input arguments, and then multiply that area complexity with the number of the groups [sW |, where the result of the division is rounded up. The rounding occur especially when the widths of the input variables in the set are not equal.


Example 3.1 Let us assume that the predicate Q is defined by Q =^
(x' =

a + b), and furthermore, we assume that the variables a and b are of the same type and the widths of a and b are wa = wb = 3. We start by calculating the sum of the widths sW for the set rQ. That is, the sum of the widths is sW = 3 + 3 = 6, which corresponds with the number of the input arguments in the Boolean expression. The sum of the widths is divided into groups by dividing the sum of widths with the cardinality of the set: sW/|rQ| = 3. The graph idea behind this group division is illustrated in Fig. 3.1.3. Each of these groups has two possible output values {0, 1}, and therefore, the area complexity is evaluated for each group separately, and we have: C(rQ) = 22 · 6/3 = 8, where the first term represents the area complexity of single group, and the second term the number of these groups.
End of example.
The area complexity for the guarded non-deterministic assignment A = g →
x := x'.Q is defined by:

C(A)= C(rg)+ C(rQ)+ C(wA)	(complexity of assignment) (1)

where C(rQ) denotes the area complexity of the predicate Q, C(rg) denotes the area complexity of the guard g, and C(wA) denotes the area complexity of the substitution operation. Observe that in the non-deterministic assignment the set rg is empty (rg = ∅), and therefore the complexity of the guard is evaluated to zero. Therefore, we need to adjust the predicates area complexity model by introducing a complexity factor Ck (Ck ∈ R+).  Thus, the area
eL Ck
complexity of the set rQ is C(rQ) =^ bg · (2  )	, where, in general, it is
assumed that Ck = 1. Consider the following example.
Example 3.2 Let A = x := x.'Q, where the predicate is defined by Q = (x' = a + b). Assume that x, a and b are variables of the same type and the widths of a and b are wa and wb and the width of x is wx. Let us further assume that the widths of the variables a and b are equal wa = wb = m (m ∈ R+) and the width of the variable x is wx = m + 1. The complexity for the set rQ when the predicate performs an addition operation is discussed in Example 3.1 with read variable widths wa = wb = 3. In this example, both of the input variable have a width m, and therefore, we have: C(rQ)= (4m)Ck , where the value of Ck is one (Ck = 1). The complexity of A is C(A)= (m + 1) + 4m = 5m + 1, where the (m+1) is the area complexity of the substitution (C(wA)). Next, we assume that the predicate Q is a multiplication and the widths of the variables a and b are wa = wb = m, and the width of the variable x is wx = 2m. The complexity of the action A is C(A) = (4m)Ck + 2m. For multiplication, we can, for instance, assume that the complexity factor is two (Ck = 2). That is, we have, C(A)= (4m)2 + 2m = 16m2 + 2m. Notice that by adopting the complexity factor of Ck = 2, the result is close to school book multiplication

[12]. (That is, if the complexity of addition is m, then the complexity of the multiplication would be around m2).
End of example.
In this paper we will assume Ck = 1 for addition and subtraction and Ck =2 for multiplication and division. However, in general the value assigned to the factor can be any positive real number.
In general, the result of the area complexity modeling for substitution op- erations is the sum of the widths of the variables in the action. However, assignment statements often contains operations that are combinatorial logic at lower abstraction levels, and therefore we adopted the graph based pro- cedure. In both cases, the result is related to the BDD’s size evaluation. Although our model operates at higher abstraction level. The accuracy of the complexity model presented above was analyzed using Ordered Binary De- cision Diagrams (OBDD) library BuDDy [8], which is freely down-loadable from the Internet. OBDD is a BDD, where certain reduction and ordering rules are emphasized, see [9]. The advantage of OBBD is that it is canonical (unique) for a particular functionality.
Example 3.3 We briefly analyzed the accuracy of the addition and multi- plication operations using the BuDDy library. The OBBD was generated for addition operation described using full adders, and for basic binary multipli- cation using block multiplier structure. Both of these operators were analyzed using widths from 16 bits to 64 bits. The node counts of OBDD were com- pared with the width count of our area complexity model. The accuracy of our presentation was in both cases approximately 83%. This result is quite good for such a high abstraction level estimation, comparing to the amount of the information we have from the actions versus the fully defined Boolean functions, which are inputs for BuDDy.
End of example.
Complexity of systems
Area complexity (CAEnv) defines the area of the parallel system A  E nv. We form a set SA Env, which contains the timed actions A, and Env of the system
A  E nv. It is easy to see that SAEnv =^ SA ∪ SEnv, where the sets SA and
SEnv are defined by SA =^ {A|A ∈ SA} and SEnv =^ {Env|Env ∈ SEnv}. The
area complexity of the system is the sum of the complexities of timed actions
in the set SA Env and defined by:
CA Env =b CA + CEnv = XA∈SA C(A)+ XEnv∈SEnv C(Env)	(area complexity) (2)


where	Σ
C(A)	is	the	area	complexity	of	the	system	A,	and

Σ	A∈SA
Env∈SEnv C(E nv) is the area complexity of the system E nv. Consider the
following example.
Example 3.4 Consider a timed action system A which reads data from its local input buffer, performs an addition operation, and stores the result to its local output buffer. The system is modeled as a closed system, which does not interact with its environment. the system is defined by:
sys	A	(	) ::
|[
delay	dOp; dAdd;
var	ibuf, obuf : set of data; d1 , d2,r : data;
private proc	Add dAdd)(in x1, x2 : data; out r : data): r := r'.(r' = x1 + x2);
action	Op dOp) : d1, d2 := d' , d' .(d' , d' ∈ ibuf ); Add(d1, d2 , r); obuf := obuf ∪ {r};
1  2	1  2
init	ibuf, obuf, d1, d2,r := ibuf 0, obuf 0, d10, d20, r0;
exec	do Op od
]|
where the functionality of the system is placed into procedure Add, which is called by the timed action Op, which is executed by the system A forever, because its guard gOp is invariantly true.
To model the area complexity for the system A, we describe the read and write sets of timed action Op. The read set is rOp = {x1, x2} and the write set is wOp = {r, d1, d2}. Observe that the procedure Add is included within the timed action that calls it (x1, x2 in the read set are the read variables of the procedure). The area complexity modeling is divided into two parts: (1) substitution from input buffer ibuf to data variables d1, d2; substitution from result variable r to output buffer obuf ; and (2) non-deterministic assignment (in the procedure Add). We start from the non-deterministic assignment (2), by extracting the sets rg and rQ from the read set rOp. The set rg is empty (rg = ∅), and therefore only the set rQ is considered in the area complexity modeling. We have: rQ = {x1, x2}. The area complexity of the set is C(rQ)= 4m, described more detail in Examples 3.1 and 3.2. The area complexity of the procedure is C(Add) = 4m + (m + 1) = 5m + 1, where the m +1 is the area complexity of the substitution in the procedure. The complexity of the substitution operations (1) of the timed action Op is C(wA)= wd1 + wd2 + wr, where the width of the variables d1, d2 is m, whereas the substitution of the result variable r to the output buffer has the width m+1. The area complexity of the timed action Op is then: C(Op)= C(Add)+ C(wA)= (5m +1)+(3m + 1) = 8m + 2.
The area complexity of the system A requires that the local buffers are

modeled as well. We have C(ibuf )= |ibuf |·m, where the cardinality of the set describes the number of elements in the set, and m is the width of each element. In a similar manner we analyze the output buffer C(obuf )= |obuf |· (m + 1). Thus, the area complexity of the system A is CA = C(ibuf )+C(obuf )+C(Op).
Power model
The power consumption is defined by:	P (system) =^  Pdyn(system) +
Pstat(system), where the first term describes the dynamic power consump-
tion of the system, and the second term the static power consumption of the system. Dynamic power consumption is related to system operation. That is, dynamic power consumption can be reduced using different design methods such as self-timed design, see for example [16]. However, the static power consumption is caused by the leakage current Ileak, which is the combination of the sub threshold leakage (a weak inversion current across the device) and the gate-oxide leakage (a tunneling current through the gate oxide insulation) [15]. Detailed analysis of the leakage current can be found, for instance, in [15], [11]. In general, both the sub threshold and the oxide leakage depends on the total gate width or more approximately the gate count, temperature (sub threshold leakage), supply voltage, and oxide thickness (oxide leakage). The static power consumption can no longer be ignored, since the power con- sumption of chip leakage is approaching the dynamic power consumption, and the projected increases in off-state sub threshold leakage show it exceeding total dynamic power consumption as the technology drops below the 65 nm feature size [15]. Emerging techniques to moderate the gate-oxide tunneling effect could bring gate leakage under control by 2010. Let us first discuss the definition of the dynamic power consumption of an action including its energy model, and then the static power loss caused by an action.
Dynamic power
The energy dissipated by the timed action A every time it is executed is denoted by E(A)(∈ R+) and defined by:

E(A) =b α · C(A) · E1	(energy of A) (3)

where C(A) is the area complexity of the timed action A, α is a switching activity parameter, and E1 is the energy of an unit action A1.  The unit
action is defined by: A1 =^ x := x'.(x' = ¬y), where x and y are Boolean
variables. The action A1 models an inverter, whose energy E1 in CMOS can

be calculated by: E1 = 1 · CL · V 2
, where VDD is the applied supply voltage,

2	DD
and CL is the output capacitance of a unit size inverter driving another unit
size inverter in a given technology [13]. The switching activity parameter





E(A)






A
A.st



t	A.ft

time

Fig. 3. Energy model for timed action A
denotes the amount of bits that change state during execution of an action. We assume that 50 % of the bits in action changes their state (α = 0.5).
As shown in Fig. 3, the energy starts from zero when the action A starts operation at the time A.st and reaches the full value EA when A finishes operation at the time A.f t. This simple linear energy model indicates that the power consumption of the action A is considered constant during the action operation period [A.st, A.f t], given by the slope of the energy line depicted in Fig. 3. The linear energy model also means, that the energy dissipated by A during the time period [A.st, t], denoted by E∗t, where t ∈ [A.st, A.f t], is

given by : E∗t  =^  k∗tEA, where the fraction k∗t is defined as k∗t  =^
t−A.st ,

A	A
where Δ(A) =^ A.f t − A.st.
A	A	Δ(A)

Similarly, the amount of energy dissipated by A during the time period

[t, A.f t], where t ∈ [A.st, A.f t], is denoted by Et∗ and defined by Et∗ =^
kt∗EA

,where kt∗ =^
A.ft−t . Naturally, k∗t + kt∗ = 1 holds for the fractions k∗t and

kt∗, and E∗t
+ Et∗
= EA.  As an example, if the action A is interrupted

A	A	A
(killed) prematurely by some other action at the time t ∈ [A.st, A.f t], the
amount of energy the action A dissipated before the interruption is equal to E∗t. Thus, the dynamic power consumption of an arbitrary action A during single execution is:



Pdyn
(A) =b
E(A)
Δ(A)
(dynamic power) (4)



where the execution time of the action A is Δ(A)= A.f t − A.st.
Static power consumption
Unlike dynamic power consumption, the static power consumption is not ac- tivity based. As stated above, it is a certain percent from the total power consumption of the system. To model the static power consumption of an ac- tion A, we adopt the unit action A1. The static power consumption in CMOS
for the unit actions is calculated by P 1	= VDD · Ileak. Observe that, at this
abstraction level, we do not have any information either from supply voltage

or from the leakage current. Therefore, the static power consumption of unit

action A1 is P 1
Pstat(A) =b C(A) · P 1
and an arbitrary action A causes a static power loss Pstat(A):
(static power) (5)



where the C(A) is the area complexity of the action A. That is, the area in- formation is utilized to evaluate the difference in the static power loss between actions.
Average power consumption
To model and analyze the power consumption of a digital system, we consider here a finite period of time T ∈ T, defined by: T = [T.st, T.ft], where T.st, and T.ft denote the start and finish times of the period, such that (T.st, T.ft ∈ T) ∧ (0 ≤ T.st < T.ft). The difference between finish and start times is
denoted by: Δ(T ) =^ T.ft − T.st. During the observation period T , a timed
action system A executes a set SA(T ) of timed actions. We have:



SA(T )
= {A|(A ∈ SA
∧ (∃t : T.st < t < T.ft : gA(t))}
(set of timed actions) (6)



where gA(t) refers to the guard of an action A at a given time t. Hence, the set SA(T ) contains all the actions that are enabled at some point during the observation period T . This includes actions that are either started or finished, or both started and finished, within the observation period T , as well as those actions that are started before and finished after the observation period T . The order of events in the set SA(T ) is determined by the temporal relations of the involved actions [22].
Consider the set SAEnv(T ) which contains those actions of the composite system A  E nv that are enabled and either partly or completely executed within the observation period T , based on the definition given in (6). Observe that, the set SAEnv, contains all the timed actions of composite system A  E nv whereas the presented set SAEnv(T ) contains only those timed actions that are enabled during the observation period T . Let us assume that actions A ∈ SA(T ) and Env ∈ SEnv(T ) are repeated nA and nEnv times (nA, nEnv ≥ 1), respectively, during the observation period T . Then the average power consumption of A  E nv during T can be defined by:

P A Env(T ) =b
P A (T )+ P Env(T )	(average power (A  E nv))



where the average power consumption P A (T ) of A and P Env(T ) of E nv are

calculated by: P A
(T ) =^
EA(T ) + P A
, and P Env(T ) =^
EEnv (T ) + P Env, where

Pstat(A) and Pstat(E nv) are the static power consumption of the systems A and
E nv, respectively. The observation period is not included into static power,

because it is not dependent on the amount of enabled actions during the ob- servation period. That is, it is certain percent of the total power consumption of the system, whenever the system is enabled. The total energies EA(T ) and EEnv(T ) of the systems A and E nv during T are defined by:


EA(T ) =b XA∈S  (T ) X	k EA	(energy (A))
nA  j


EEnv (T ) =bXEnv∈S
(T )XnEnv kj
EEnv	(energy (E nv))




If SA(T ) or SEnv(T ) is empty, then the energy EA(T ) or EEnv(T ), respectively,
is considered zero. In the above definitions, kj and kj	are fractions indi-
cating the portion of an action A or Env which resides inside the observation period T in the jth execution of A or Env. Such a fraction can be generally
j	Δj (X)	j
defined for an action X ∈ {A, Env} as: kX  =^  T	 , where ΔT (X) denotes
the time the action X spends within the observation period T in its jth
execution during T . Thus, we have that 0 < Δj (X) ≤ Δ(X). If the action X is executed only once during T , i.e., nX = 1, we write simply ΔT (X) and the corresponding fraction is denoted by kX.


Example 3.5 Consider the timed action system defined in Example 3.4. To
model its average power, we define an observation period T =^ [T.st, T.ft],
where the start time of the period is assumed to be zero (T.st = 0) and the
finish time T.ft denotes the time when the system is performed one oper- ation cycle T.ft = Op.ft. Naturally, the observation period is selected by the designer, but for simplicity, we illustrate the power analysis with single operation cycle. The average power consumption for the system is defined by

P A (T ) = EA(T ) + P A
, where the E(A) is the energy consumption of the

avg
ΔT	stat	Σ	1

system defined by: EA =
Op∈S(A) α · (E
· (C(Op)+ C(obuf )+ C(ibuf ))),

where the complexities of the action Op and buffers ibuf and obuf are mul-
tiplied by the unit energy E1. The area complexity modeling for system A is discussed more detailed in Example 3.2.


Temporal power modeling
Consider the set SAEnv(T ), where the observation period T is divided into time segments Ti (1 ≤ i ≤ m), which satisfy the following condition:


`Ti ⊆ T ´ ∧ `i ≥ 1´	(p1)
∧ `∀t : Ti.st < t < Ti.ft : (∀A : A ∈ SAEnv(T ): t /∈ {A.st, A.ft})´	(p2)
∧ “∃m : m ≥ i :
`i =1 ⇒ Ti.st = T.st´
∧ `1 ≤ i < m ⇒ Ti.ft = Ti+1.st´
∧ `∃A : A ∈ SAEnv(T ): (Ti.ft ∈ {A.st, A.ft})´	(p3)
∧ `i = m ⇒ Ti.ft = T.ft´”

Hence, the start time of a time segment Ti is either the start time T.st of the observation period T or the start or finish time of an action A ∈ SAEnv(T ) (p1,p3 ). Analogously, the finish time of a time segment Ti is either the finish time T.ft of the whole observation period T or the start or finish time of an action A ∈ SAEnv(T ). Furthermore, the finish time of a segment Ti is the start time of the next segment Ti+1 for 1 ≤ i < m and m > 1. No action A ∈ SAEnv(T ) is started or finished inside a time segment Ti, i.e. when Ti.st < t
< Ti.f t (p2 ).
From the definition of the time segment Ti follows that the set SAEnv(T ) of actions executed during the observation period T is the union of the subsets
SA Env(Ti) for 1 ≤ i ≤ m: SA Env(T ) =   m  SA Env(Ti).  Moreover, with
the whole observation period T , we have for the time segments Ti (⊆ T ) that SAEnv(Ti)= SA(Ti)∪SEnv(Ti). Notice that each timed action can be executed only once within the time segment due to the definition of Ti.
The temporal power consumption during a time segment Ti, denoted by
P AEnv(Ti), is then defined as:


P AEnv (Ti)

A
tmp
(Ti)+ P Env(Ti)	(temporal power)



The subsystem-specific power consumptions P A (Ti) and P Env(Ti) are given

by: P A
(Ti) =^
EA(Ti) + P A
and P Env(Ti) =^
EEnv (Ti) + P Env where Δ(Ti) de-

notes the time segment (Δ(Ti) =^  Ti.f t − Ti.st), and EA(Ti) and EEnv(Ti) are
the total energies of the actions in the sets SA(Ti) and SEnv(Ti), respectively.
The static power consumption of the systems A and E nv are P A	and P Env,
respectively. If the set SA(Ti) or SEnv(Ti) is empty, then the energy EA(Ti) or
EEnv(Ti), respectively, is considered zero. Otherwise we have that:
X
EEnv(Ti) =b XEnv∈SEnv(Ti )kEnv(Ti)EEnv	(energy (E nv))

where kA(Ti) and kEnv(Ti) are fractions indicating the portion of an action A
or Env which occurs inside the time segment Ti.

Modeling and Constraining the Behavior of the Sys- tems

Constraints

A constraint is an expression, a Boolean condition B, according to which the timed actions are obliged to operate. In a hardware systems, the violation of the constraints indicates a useless, unpredictable computation, and therefore it is modeled either as an assert statement or as an assumption statement. When a constraint is modeled as a assert statement, it behaves as a skip statement if it holds (B ≡ true), but otherwise it behaves as an abort statement (B ≡ f alse). In other words, if constraints are satisfied, they operate as empty statements that do not change the state at all. On the other hand, if a constraint is not satisfied, it is a never terminating statement, which does not establish any post condition causing an abnormal termination of the system. This model reflects the definition of a hard constraint whose adherence is mandatory and a violation terminates the program. In a text, we denote a constraint by {B}, where B defines the Boolean condition according to which the involved timed actions are obligate to operate. We start by shortly reviewing the time related constraints, defined in [23], which are divided into hard and relative constraints. Relative constraints define the order in which timed actions are allowed to be executed. Then, we introduce new performance related constraints.



Deadline and relative constraints
A deadline defines the maximum time that a timed action is allowed to con- sume in its operation. Applying the time constraint definitions given above,
we define [23]: {A, d} =^ {Δ(A) ≤ d}, where A defines those timed actions
and their relations whose operation time is bounded and Δ(A) is a time ex-
pression that evaluates to a time value (T). Time expressions are composed of mathematical operations, e.g. + and −.
Relative constraints use relative timing of operations to define the order in which the operations much be executed with respect to each other in the time domain. The relative constraints are defined using relative orderings defined below. With the can be directly used in constraints to restrict the temporal behavior of timed actions. The relative constraints are, for example:




A starts before B =b A.st < B.st	(starts before) (7)
A ends after B =b B.ft < A.ft	(ends after)	(8)
A precedes B =b A.ft < B.st	(precedes)	(9)
A meets B =b A.ft = B.st	(meets)	(10)
A starts with B =b A.st = B.st	(starts with) (11)
A ends with B =b A.ft = B.ft	(ends with)	(12)

where the Ai.st and Ai.f t are the start and finish times of a timed action, respectively.

Performance related constraints
Let us next introduce performance related constraints: area, average power and temporal power, where the two latter ones are defined as a hard constraint and the former using the assumption based constraint. The assumption based constraint, denoted by [B], where B defines the Boolean condition, do not terminate the system. That is, it gives the designer information about the system, which can be used in the design process. The area constraint defines the maximum area complexity for a timed action system. For the following system A  E nv, we have: [A  E nv, l] = [CAEnv ≤ l], where l defines the maximum area complexity allowed for the system.
The average power constraint defines the maximum average power that the system is allowed to consume during the observation period T . The power related constraint is evaluated after timing related constraints and the area constraint is verified. That is, we need both area and time information to

validate the power consumption. We have: {A	E nv, p
avg
} =^
{P AEnv(T ) ≤

pavg} where the pavg is the limit value for average power during the observation
period T . Observe that, functionality inside the observation period in not altered during refinement steps. This restrictions follows from the deadline constraint, we cannot exceed the observation period if the deadline constraint holds for all timed actions in the system.
The temporal power constraint is defines the maximum peak power that the system is not allowed to exceed. This constraint is evaluated af- ter timing and area related constraints are validated. We have: {A
A E nv
E nv, ppeak} =^ {∀i.P	(Ti) ≤ ppeak}, where the ppeak is the limit value for
temporal power for each time segment Ti in the observation period T . That
is, we set a peak power value ppeak that is not allowed to exceed in any seg- ment Ti (∈ T ). The power constraints are defined as hard constraint, because exceeding the assigned power limit can impose serious problems to a system.

Refinement of Systems
Action Systems are intended to be developed in a stepwise manner within the reﬁnement calculus framework [3]. The refinement calculus guarantees the correctness of each performed transformation step. In this section, we concentrate on applying the refinement calculus framework for power model- ing. We start by describing the basic concepts of the refinement calculus and derivation of conventional Action Systems.
An (atomic) action A is said to be (correctly) reﬁned by action C, de- noted A ≤ C, if ∀Q.(wp(A, Q) ⇒ wp(C, Q)) holds. This is equivalent to the condition

∀P, Q((P A Q) ⇒ (P C Q))	(total correctness property)

which means that the concrete action C preserves every total correctness prop- erty of the abstract action A.
Data reﬁnement of conventional actions
Let us assume an (atomic) action A on the variables a and u is refined by concrete action C on the variables c and u using an abstraction invariant R(a, c, u), which is a Boolean relation between the abstract variables a and the concrete variables c. Then the abstract action A is data-reﬁned by the concrete action C, denoted by A ≤R C, if the following condition holds.

∀Q(R ∧ wp(A, Q)) ⇒ wp(C, ∃a.R ∧ Q)	(condition of the data reﬁnement)

The predicate ∃a.R∧Q is a Boolean condition on the system variables u and c. The above definition of data-refinement can be written in terms of the guards gA, gC, and the bodies sA, sC of the actions A and C as follows:

R ∧ gC ⇒ gA	(body)
∀Q.(R ∧ gC ∧ wp(sA, Q) ⇒ wp(sC, ∃a.R ∧ Q))	(guard)

The data refinement A ≤R C replaces a with c preserving the variables u.
Reﬁnement of Timed Action Systems
Rather than going into the details of refinement of parallel and reactive sys- tems [4], we review here a commonly used method to prove the refinement step. The presented data refinement method has been used to prove a cor- rectness of superposition refinement of action systems [6] as well as the trace refinement of action systems. In the trace refinement of an action system the trace is preserved, or the sequence of global states (observable behavior), of

the system in question. A fundamental study of the trace refinement can be found in [7]. In the superposition refinement the behavior of a system model is enhanced by adding new functionality into the model while preserving the old one. The method is also extended to prove the correctness of data refinement of action systems with remote procedures in [20].
Below we describe how to establish the refinement of action systems. We will first describe the refinement formally and thereafter give an informal de- scription of the required conditions.
Reﬁnement Rule
Consider action systems operating in an arbitrary environment E nv of form (simplified model): E nv(u) :: |[action E; init u := u0; exec do E od]|:
sys	C	(	u; ) ::

sys	A	(	u; ) ::
|[
constraint	{A};
delay	dA;
var	a;
action			A dA) : (aA); init	g, a : g0, a0; exec		do A od
]|
|[
constraint	{C}; delay		dC; dX; var	c;
action	C dC) : (cC);
X dX) : (cX);
init	g, c : g0, c0;
exec	do C ▢ X od
]|

where the constraints of a timed action system, say A, are denoted by: {A} = cnst1 ∧ ... ∧ cnstn, where cnsti, 1 ≤ i ≤ n, are the constraints of the timed action system A.
An abstract action A on the variables a (local variables) and u (global variables) is refined by the concrete action C on the variables c and u using an abstraction invariant R(a, c, u), which is a Boolean relation between the abstract variables a and the concrete variables c. The abstract system A is correctly refined by the concrete system C, denoted A ± C, if there exists an invariant R(a, c, u) (an abstraction relation) on state variables, if the following conditions are satisfied:

R(a0, c0 , u0 )= true	(Initialization)	(i)
A ≤R C	(Main action)	(ii)
skip ≤R X	(Auxiliary action)	(iii)
R ∧ gA ⇒ gC ∨ gX	(Continuation condition) (iv)
R ⇒ wp(do X od, true)	(Internal convergence)	(v)
R ∧ wp(E, true) ⇒ wp(E, R)	(Non-interference)	(vi)
R ∧ {C} ⇒ {A}	(Timed behavior)	(vii)
R ∧ [C] ∨ R ∧ {C} ⇒ [A] ∨ {A}	(Performance related)	(viii)


The first condition says that the initialization of the systems A and C es- tablish the abstraction relation R.
The second condition requires the abstract action A to be data-refined by the concrete action C using R.
The third condition, in turn, indicates that the auxiliary action X is ob- tained by data-refining a skip action. This basically means that X behaves like skip action with respect to the global variables u which are not allowed to be changed in the refinement.
The fourth condition requires that whenever the action A of the abstract system A is enabled, assuming the abstraction relation R holds, there must be a enabled action in the concrete system C as well.
The fifth condition states that if R holds, the execution of the auxiliary action X, taken separately, must terminate at some point.
The sixth condition guarantees that the interleaved execution of E actions preservers the abstraction relation R
The seventh condition requires that all the time constraints are met in the concrete timed action system C.
The eight condition requires that the area constraint is verified in the con- crete timed action system C and the power constraints are met in the con- crete timed action system C.
The conditions (i)-(v) guarantee, in terms of global and local variables, that the behavior of an abstract system is preserved in the concrete one. That is, the action system is executed in isolation, as a closed system. Therefore, the five first requirements and the seventh (vii) are sufficient in proving the trace refinement of the timed action system. However, when the system is operat- ing in a parallel composition with other action systems the non-interference condition (vi) must be validated, too.
The functionality of the timed action is refined using data refinement of conventional actions. The correctness of the data refinement of timed actions is proved by the condition A ≤R C ⇒ A dP ) ≤R C dP ). The proof of this condition is shown in [23]. The performance related constraints (viii) are evaluated after the time related constraint are verified. The area constraint is evaluated first because it provides necessary information to the verification of the power constraint. Moreover, the power constraint is evaluated after the defined observation period is completed.

Reﬁnement of the constraints
We illustrate the refinement of constraints through several examples.
Example 5.1 The operation of the timed action system A presented in Ex- ample 3.4 was not restricted by any constraints. We start by defining time when the result of the computation of the timed action Op must be written onto the output buffer. The deadline is of form (Δ(Op[Add]) ≤ D), where Op[Add] denotes the fact that Op calls a procedure in its body and D is the given deadline (D ∈ T).
In Example 3.5, the average power of the system A was modeled during observation period T : T = [0, Op.ft]. Before we can constrain the average power of the system, we set the area constraint: (CA ≤ l), where l is the maximum allowed area complexity for the system. Notice that the value of l remains through refinement steps performed, whereas the system complexity increased/decreases. The value of l is set by the designer, and in this example, we assume that it is twice the area complexity of the original system l = 2 · CA(orig). By adopting the limit value l in area constrain, we constraint the
average power consumption of the system A by: P A (T ) ≤ pavg, where the
pavg is the average power limit constraining the system and it is defined by

pavg
E1·l
Δ(T )
1
stat
· l, where l is the limit value of the area constraint and

Δ(T ) is the difference between start and finish times of observation period T

1
stat
is the static power consumption of an unit action.

End of example.
Example 5.2 Consider the timed action system A, described in Example
The timed action Op reads data from the input buffer, performs the computation and writes the result onto the output buffer. To separate the read
and write operations, the timed action Op : dOp) : d1, d2 := d' , d' .(d' , d' ∈
1	2	1	2
ibuf ); Add(d1, d2, r); obuf := obuf ∪ {r}; is divided into two timed actions
Ro(read operate) and W (write): Ro dRo) : ¬b → d1, d2 := d' , d' .(d' , d' ∈
1	2	1	2
ibuf ); Add(d1, d2, r); b, s := T, r; and W dW ) : b → obuf, b := obuf ∪ {s},F ;,
whose delays are Δ(Ro) and Δ(W ) defined such that Δ(Ro)+Δ(W )= Δ(Op). Notice that only the delay Δ(Op) of timed action Op is reallocated between the new timed actions. The delay of the procedure is not altered, it is included into the delay of the timed action that calls it. The variable b (b ∈ {T, F }) sequences the operation between the new timed actions. After the refinement we have a timed action system A' whose behavior is given by: exec do Ro ▢ W od. Showing the correctness of this refinement we need to prove that the conditions (i)-(viii) of timed refinement are satisfied. We have:
Initialization. The initializations of the timed action systems A and A'
do not contradict.

Main action. Our goal is to prove that Op ≤I W , where I is an invariant of form I =^ b ⇒ r = s. We have:
Body:
I ∧ gR ∧ wp(sOp, Q) ⇒ wp(sR, I ∧ Q)
⇔ {weakest precondition of sA and sCp } I ∧ b ∧
Q ⇒ I[F/b] ∧ Q[r/s]

Guard:
I ∧ gR ⇒ gW
⇔ I ∧ b ⇒ T
⇔ T
⇔ {the invariant I = b ⇒ r = s}
(b ⇒ r = s) ∧ b ∧ Q ⇒ (b ⇒ r = s)[F/b] ∧ Q[r/s]
⇔ { logic }
(b ⇒ r = s) ∧ b ∧ Q ⇒ T ∧ Q[r/s]
⇔ {logic }
b ∧ (r = s) ∧ Q ⇒ Q[r/s]
⇐ b ∧ Q ⇒ Q[r/s] ∧ r = s
⇔ b ∧ Q ⇒ Q
⇔ T

Auxiliary action. Because the auxiliary action Ro does not modify any
interface variables, it behaves like a skip with respect to this kind of variables.
Continuation condition. There is always either of the new timed actions
Ro or W enabled when the original timed action Op is enabled.
Internal convergence. Holds trivially as the new auxiliary action disables itself.
Non-interference. Holds trivially as the new system is a closed action system.
Timing behavior. The tenability of the constraint (Δ(Op[Add]) ≤ D), defined in Example 5.1, must be validated to show correctness of this re- quirement. In verification, we use a computation path [23], which defines a path through the system. The computation path in the new system, based on the functional behavior, the following: CP (Ro '→ W ), where the timed action Ro is performed before the execution of W . The compu- tation path delay is Δ(∗Ro '→ W ∗)= Δ(Ro)+ Δ(W ), where ∗ shows by its position, is the delay of the timed action included in the calculation
or not: for the former action we have: ∗Ro =^  ”Ro included ”, Ro∗ =^  ”
Ro excluded ”; and for the latter action the other way around:∗W =^ ”W
excluded ”, W ∗ =^  ”W included ”. By calculating the computation path
delay and comparing the result to the delay of the original timed action
Op, we are able to show that the timing requirement is satisfied. We have: Δ(∗Ro '→ W ∗) = Δ(Ro[Add]+ W ) = Δ(Ro[Add]) + Δ(W ) = Δ(Ro)+ 
Δ(Add)+ Δ(W ). Based on the requirement: Δ(Ro)+ Δ(W ) = Δ(Op)

and the fact that Δ(Op[Add]) = Δ(Op)+ Δ(Add) we may conclude that the deadline is satisfied, and thus the requirement is satisfied as well.
Performance related. The area complexity constraint, described in Ex- ample 5.1, is validated by defining the area complexity of the system A': CA' = C(Ro)+ C(W )+ C(ibuf )+ C(obuf ), where the area complexity of the input and output buffers is defined in Example 3.4. That is, the area complexity of the buffers (ibuf, obuf ) is not altered during refinement. To evaluate the effect of the refinement step it is enough to compare the area complexity of the timed action Op from the system A with the com- plexities of the timed actions Ro and W from the system A'. We have: C(Op) = 8m + 2, whereas C(Ro) = 8m +4 and C(W ) = m + 3. We conclude that, the area complexity of the system is increased by m +5 nodes during the refinement step A ± A'. However, the area constraint set for the system, in the Example 5.1 is satisfied, because the area com- plexity of the new system is less than twice the area complexity of the old system A. Thus, the area constraint is updated by: (CA' ≤ l).
The average power constraint verification is divided into two phases. First, we verify the observation period from the power equation. The deadline of the action Op is satisfied by the new actions Ro and W as stated in (vii), and, furthermore, the delay of the timed action Op is reallocated between the two new timed actions Ro and W , we can adopt the same observation period T as defined in Example 5.1. In other words, the denominator of the dynamic power clause does not change. Second, we revise the area constraint, which has effect on both dynamic and static power consumption. The new system A' satisfies the area constraint, and therefore, we may conclude that the average power constraint is satisfied

as well. We have: (P A'
≤ pavg).

Thus, we have performed the refinement A ± A'.

sys	A'	(	) ::
|[
delay	dRo, dW, dAdd;
constraint	cnstD : (Δ∗Ro '→ W ∗) ≤ D;
cnstA : (CA' ≤ l);
'
cnstP : (P A ≤ pavg );
var	ibuf, obuf : set of data; d1 , d2,r : data;
private proc	Add dAdd)(in x1, x2 : data; out r : data): r := r'.(r' = x1 + x2);
action	Ro dRo) : ¬b → d1, d2 := d' , d' .(d' , d' ∈ ibuf ); Add(d1, d2 , r); b, s := T, r;
1  2	1  2
W dW ) : b → obuf, b := obuf ∪ {s},F ;
init	b, d1 , d2, r, ibuf, obuf := F, d10, d20, r0 , ibuf 0, obuf 0;
exec	do Ro ▢ W od
]|
To have detailed information on power consumption of the two new timed actions, we set the temporal power constraint for the system A'. The order of events in the observation period T is defined using relative constraints, we have: Ro meets W . According to this, we divide the observation period into two time segments, and constrain the peak power consumption for both
segments (T ,T ). We have: (P A' (T ) ≤ p	) for time segment T , and
1	2	tmp	1	peak	1

(P A' (T ) ≤ p
) for time segment T . The time segments are defined by

tmp	2
peak	2

T1 = [Ro.st, Ro.ft] and T2 = [W.st, W.f t]. The peak power limit is a power
limit set by the designer, and therefore in this example we assume that it is twice the average power constraint (ppeak =2 · pavg ).

Example 5.3 The communication for the system A' is implemented using procedure based communication. That is, we include a sender and a re- ceiver actions to implement the communication between the system A' and its environment E nv. The behavior of the environment is given as (sim- plified model): exec do Snd ▢  E ▢ Rec od, where the timed actions Snd and Rec implements the procedure based communication with the sys- tem A'' and E describes timed actions in the environment. The environ- ment interacts with the system A'' only through the new interface pro- cedures. That is, it does not have access to the inner operation of the system A''. To accomplish this behavior, we introduce a receiver action: Rec dRec) : ¬id → await n; id := T , where n is the communication procedure of form: n dn) : (in y : data): ibuf := ibuf ∪ y. Furthermore, we introduce a sender action: Snd dSnd) : od → call p(obuf ); od := F , where the communi- cation procedure p is defined by the environment. The existing timed actions Ro and W are slightly updated, and we have timed actions Ro2 and W2:
Ro2 dRo2) : ¬b ∧ id → d1, d2 := d' , d' .(d' , d' ∈ ibuf ); Add(d1, d2, r); b, s :=
1	2	1	2

T, r and W2 dW2) : b → obuf, b, id, od := obuf ∪ {s}, F, F,T . The de- lays for these four actions Snd, Ro2, W2 and Rec are defined such that Δ(Snd)+ Δ(Ro2)+ Δ(W2)+ Δ(Rec) = Δ(Ro)+ Δ(W ). After refinement, we have a parallel system composition A''  E nv, where the behavior of the system A'' is given: exec do Snd ▢ Ro2 ▢ W2 ▢ Rec od. Showing the correctness of this refinement we need to prove that the conditions (i)-(viii) of timed refinement are satisfied. We have:
Initialization. The initializations of the timed action systems A' and A''
do not contradict.
Main action.	Our goal is to prove that Ro ≤I Snd, where I is an invariant of form I =^ n(y) ⇒ r = s. We have (n(y) is n in the proof):
Body:
I ∧ gSnd ∧ wp(sn, Q) ⇒ wp(sSnd, I ∧ Q)
⇔ {weakest precondition of sn and sSnd } I ∧ n ∧
Q ⇒ I[F/n] ∧ Q[r/s]

Guard:
I ∧ gSnd ⇒ n
⇔ I ∧ od ⇒ T
⇔ T
⇔ {the invariant I = n ⇒ r = s}
(n ⇒ r = s) ∧ n ∧ Q ⇒ (n ⇒ r = s)[F/n] ∧ Q[r/s]
⇔ { logic }
(n ⇒ r = s) ∧ n ∧ Q ⇒ T ∧ Q[r/s]
⇔ {logic }
n ∧ (r = s) ∧ Q ⇒ Q[r/s]
⇐ n ∧ Q ⇒ Q[r/s] ∧ r = s
⇔ n ∧ Q ⇒ Q
⇔ T

Auxiliary action. Because the auxiliary actions Rec and Ro2 do not modify any interface variables, they behave like a skip with respect to this kind of variables.
Continuation condition. There is always either of the timed actions Rec, Ro2, W2 or Snd enabled when the original timed action Op is enabled.
Internal convergence. Holds trivially as the new auxiliary action disables itself.
Non-interference. Holds because the environment E nv interacts with the system only through the new interface procedures and has no effect on its inner operation.
Timing behavior. From the timing point of view, we have to show that the constraint cnstD is satisfied by the new system. This can be proved by showing that the duration that it takes for data item to traverse through the new system does not violate the constraint. Therefore, we

need to determine the computation path for the sent items. The com- putation path is: (∗Rec '→ Snd∗) = Rec; n; Ro2; W2; Snd and its delay is (Δ(∗Rec '→ Snd∗)) = Δ(Rec)+ Δ(Ro2)+ Δ(W2)+ Δ(Snd). Thus, we may conclude that the new system adheres the constraint of the orig- inal timed action system, because we required in the refinement that Δ(Rec)+ Δ(Ro2)+ Δ(W2)+ Δ(Snd)= Δ(∗Ro '→ W ∗).
Performance related. The area complexity constraint is validated by defining the area complexity of the system A : CA'' = C(Snd)+S(Rec)+ C(Ro2)+ C(W2)+ C(ibuf )+ C(obuf ). The effect of the refinement step A' ⊆ A'' is m + 6 to the area complexity and all together the area com- plexity is increased by the amount of m + 11 during these two refinement steps (A⊆A'⊆A''). Thus, we may conclude that the area constraint is satisfied and we have: (CA'' ≤ l).
The average power constraint is verified in a similar manner as in Ex- ample 5.2. The verification of the time constraint in (vii) stated that
the new system adheres the deadline constraint from the timed action system A'. Therefore, we may conclude that the observation period T is not altered. Furthermore, the area constraint was satisfied for the system A'', and therefore, we may conclude that the average power constraint is

satisfied as well, we update the power constraint: (P A'' ≤ p
avg ).

To verify the temporal power constraints, we define the order of events during the observation period T using temporal relations, and we have the following execution sequence: Rec; Ro2; W2; Snd. That is, after the Rec action is finished, the read operate Ro2 is enabled and so on. As a result, we have four time segments each of which contains the opera- tion of a one timed action according to the execution sequence presented above (the first time segment contains the operation of the timed ac- tion Snd, the second contains the operation of the timed action Ro2 and so on.). The time segments are defined by: T1 = [Snd.st, Snd.ft], T2 = [Ro2.st, Ro2.f t], T3 = [W2.st, W2.f t], and T4 = [Rec.st, Rec.ft]. Furthermore, we have that ΔT1 + ΔT2 + ΔT3 + ΔT4 = Δ(∗Rec '→ Snd∗), where Δ(∗Rec '→ Snd∗) is the computation path delay of a one operation cycle. To verify the temporal power we have to show that the limit value ppeak is not exceeded in any of these segments.
The area complexities for the timed actions Ro2 and W2 are C(Ro2)= 
8m+5 and C(W2)= m+5, respectively. The communication actions Snd and Rec are evaluated as follows: C(Snd)= 2m + 2, which includes the area complexity of the communication procedure n. The area complexity of the receiver action Rec is C(Rec)= m + 2, where the procedure call is modeled as a substitution operation. Observe that the area complexity of

exported procedure p would be modeled in the environment, but since the environment is excluded we can leave it out from this discussion. We can see that the area complexities of the communication actions are signifi- cantly smaller than the actions Ro2 and W2. Furthermore, by comparing the area complexities of timed actions Ro and W , defined in Example 5.2, we see that there is no significant increase. Therefore, the effect of area complexity is small to the temporal power. The delays of the time segments are smaller as they were in Example 5.2, which increases the temporal power consumption. However, we may conclude that the peak power consumption limit ppeak is not exceeded (in other words, the power consumption in each time segment is smaller than four times the average power limit pavg. This is due to the moderate increase in area, and due to the sequential operation of the actions (see the above definitions of the actions). That is, even though the duration of time segments are smaller, the operation of the action is divided equally for the whole time segment. Thus, we may conclude that the temporal power constraint is satisfied
for the time segments T , T , T and T . We have: (P A'' (T ) ≤ p	),

1	2	3	4
tmp	1
peak

(P A'' (T ) ≤ p
), (P A'' (T ) ≤ p
), and (P A'' (T ) ≤ p	)

tmp	2
peak
tmp	3
peak
tmp	4
peak

Thus, we have performed trace refinement A' ± A''.
sys	A''	(	imp p; exp n; ) ::
|[
delay	dRec, dSnd, dn, dRo2 , dW2, dAdd;
constraint	cnstOp : (Δ∗Snd '→ Rec∗) ≤ D; cnstA : (CA'' ≤ l);
''

cnstP : (P A
''
cnstP : (P A
''
cnstP : (P A
''
cnstP : (P A
''
cnstP : (P A
≤ pavg );
(T1) ≤ ppeak);
(T2) ≤ ppeak);
(T3) ≤ ppeak);
(T4) ≤ ppeak);

var	ibuf, obuf : set of data;
b, id, od : bool; d1 , d2, r, s; data;
public proc	n : dn) : (val y : data): ibuf := ibuf ∪ y;
private proc	Add dAdd)(in x1, x2 : data; out r : data): r := r'.(r' = x1 + x2);
action	Rec dRec) : ¬id → await n; id := T ;
Ro2 dRo2) : ¬b ∧ id → d1 , d2 := d' , d' .(d' , d' ∈ ibuf ); Add(d1, d2, r); b, s := T, r;
1  2	1  2
W2 dW2) : b → obuf, b, id, od := obuf ∪ {s}, F, F,T ; Snd dSnd) : od → call p(obuf ); od := F ;
init	id, od, b, s, ibuf, obuf, d1, d2 := T, F, F, d0, ibuf 0, obuf 0, d10, d20;
exec	do Rec ▢ Ro2 ▢ W2 ▢ Snd od
]|

Conclusion
In this paper, we have proposed a method to develop, in a stepwise manner, an abstract system towards more concrete one. One of the advantages of the defined refinement rules is that it is a clear extension to existing refinement rules of both conventional Actions Systems and Timed Actions Systems, and therefore easily adopted. We illustrated using simple examples, how the area and power related refinement rules are step wisely validated into a form where the operation is sequenced by a new auxiliary Boolean variable.
In this paper, we did not considered the scalability of our power modeling approach, as the scope was to introduce this property into the time domain. However, the introduced power modeling approach and the refinement rules is an important step towards that direction, as we are now able to start the development of a system from an abstract specification and refine it towards a more concrete one in a stepwise manner preserving both the temporal and functional characteristics, and furthermore, the power related issues.

References
H. R. Andersen and H. Hulgaard. Boolean Expression Diagrams, in Proc. IEEE Logic in Computer Science, Poland, 1997.
S. B. Akers. Binary Decision Diagrams, IEEE Transaction on Computers, Vol C-27, No.6, August 1978, pp. 509-516.
R. J. Back and J. von Wright, Reﬁnement Calculus: A Systematic Introduction, Springer- Verlag, 1998.
R. J. Back. Reﬁnement Calculus, part II: Parallel and Reactive Programs, in Proc. on Stepwise refinement of distributed systems: models, formalisms, correctness, vol. 430, 1990, pp. 67-93.
R. J. R. Back and K. Sere. From Modular Systems to Action Systems, in Proc. of Formal Methods Europe’ 94, Spain, October 1994. Lecture notes on computer science, Springer-Verlag.
R. J. Back and K. Sere. Superposition Reﬁnement of Reactive Systems, in Formal Aspects of Computing, vol. 8, no. 3, 1996, pp. 324-346.
R. J. Back and J. von Wright. Trace Reﬁnement of Action Systems, in International Conference of Concurrent Theory, 1994, pp. 367-384.
BuDDy BDD package by J. Nielsen. http://sourceforge.net/projects/buddy/.
R. E. Bryant. Graph.Based Algorithms for Boolean Function Manipulation, IEEE Transaction on Computer, Vol C-35, No. 8, August 1986, pp. 677-691.
R. E. Bryant. On the Complexity of VLSI Implementations and Graph Presentations of Boolean Functions with Application to Integer Multiplication., IEEE Transaction on Computers, Vol. 40, No. 2, February 1991, pp.205-213.
A. Chandrakasan, W. Bowhill, and F. Fox. Design of High-Performance Microprocessor Circuits, IEEE press, 2001.
T. H. Cormen, C. E. Leiserson, R. L. Rivest, C. Stein. Introduction to algorithms - second edition, MIT Press, Cambridge, 2001.

W. J. Dally, and J. W. Poulton. Digital systems engineering, Cambridge University Press, 1998.
E. W. Dijkstra. A discipline of programming, Prentice-Hall International, 1976.
N. S. Kim, T. Austin, D. Blaauw, T. Mudge, K. Flautner, J. S. Hu, M. J. Irwin, M. Kandemir and V. Narayanan. Leakage Current: Moore’s Law Meets Static Power, in Computer (IEEE Computer Society), vol. 36, Issue 12, pp. 68-75.
P. Liljeberg, J Tuominen, S. Tuuna, J Plosila, and J. Isoaho. Self-Timed Methodology and Techniques for Noise Reduction in NoC Interconnects, Chapter 11 in Interconnect-Centric Design for Advanced SoC and NoC, Kluwer Academic Publishers, Boston, July 2004, pp. 285- 313, ISBN 1-4020-7835-8.
J. Plosila, P. Liljeberg, and J. Isoaho Modeling and reﬁnement of an on-chip communication architecture, in Formal Methods and Software Engineering: 7th International Conference on Formal Engineering Methods, pp. 219-234.
T. Seceleanu. Systematic Design of Synchronous Digital Circuits, Ph.D Thesis, Turku Centre of Computer Science, 2001.
E. Sekerinski, and K. Sere. A theory of prioritizing composition, in The Computer Journal, vol. 39, no.8, pp. 701-712, The BCS, Oxford University Press, 1996.
K. Sere and and Marina Wald´en. Data Reﬁnement of Remote Procedures, in Formal Aspect of Computing, vol. 12, no. 4, 2000, pp. 278-297.
J. Tuominen, T. S¨antti, and J. Plosila. Towards a formal power estimation framework for hardware systems, in Proc. of International Symposium of System of Chip, Tampere, Finland, Nov. 2005.
T. Westerlund, and J. Plosila. Time aware modeling and analysis of multiclocked VLSI systems, LNCS, vol 4260, pp. 737-756, in Proc. of 8th International Conference on Formal Engineering Methods, Macau SAR, China, November 2006.
T. Westerlund, and J. Plosila. Time Aware System Reﬁnement, In REFINE workshop 2006, Macau SAR, China, Nov. 2006,
