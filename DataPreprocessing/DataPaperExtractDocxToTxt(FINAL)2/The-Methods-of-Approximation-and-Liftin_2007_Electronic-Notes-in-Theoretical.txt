	Electronic Notes in Theoretical Computer Science 167 (2007) 387–423	
www.elsevier.com/locate/entcs

The Methods of Approximation and Lifting in Real Computation
Manuel L. Campagnolo 1
D.M./I.S.A.
Lisbon University of Technology and SQIG/IT Lisbon, Portugal
Kerry Ojakian 2
Department of Mathematics SQIG - IT and IST, Portugal Lisbon, Portugal

Abstract
The basic motivation behind this work is to tie together various computational complexity classes, whether over different domains such as the naturals or the reals, or whether defined in different manners, via function algebras (Real Recursive Functions) or via Turing Machines (Computable Analysis). We provide general tools for investigating these issues, using a technique we call the method of approximation. We give the general development of this method, and apply it to obtain 2 theorems. First we connect the discrete operation of linear recursion (basically equivalent to the combination of bounded sums and bounded products) to linear differential equations, thus providing an alternative proof of the result from Campagnolo, Moore and Costa [3]. Secondly, we extend this to prove a result similar to that of Bournez and Hainry [1], providing a function algebra for the real functions computable in elementary time. Their proof involves simulating the operation of a Turing Machine using a function algebra. We avoid this simulation, using a technique we call “lifting,” which allows us to lift the classic result regarding the Kalmar elementary computable functions to a result on the reals. While we do not claim that our result is necessarily an improvement (perhaps just different), we do want to make the point that our two techniques appear readily applicable to other problems of this sort.
Keywords: Computable Analysis, Real Recursive Functions, Elementary Computable.


1 Email: mlc@math.isa.utl.pt
2 Email: ojakian@math.ist.utl.pt





1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2006.09.013

Introduction
We will study classes of functions with respect to their computational com- plexity, showing connections between different models of computation. The classic case is when the classes of functions have as their domain and range the natural numbers, N, using something like Turing Machines to specify which functions are in the class. More recent work has extended computational com- plexity to classes of functions over the real numbers, R. In the classic case, there is one agreed upon concept of computation and computational complex- ity with different models yielding the same set of functions. This is not the case for the analogous work over the reals. We will concentrate on two models of computation over the reals, “The Real Recursive Functions” and “Com- putable Analysis.” The former originated with Moore [6] and the latter with Grzegorczyk [4]. In Computable Analysis, Turing Machines are used to char- acterize various classes of functions over the reals, with the idea being that a real function is computable by a Turing Machine if it can be approximated to the appropriate level of precision from approximations to the input of the function. In the case of Real Recursive Functions, classes of functions are defined using function algebras in which the discrete operations of recursion are replaced by operations which find solutions to differential equations. Our goal is to study connections between these three different kinds of function classes, the classic ones over the naturals, the ones arising from Computable Analysis, and the ones arising from Real Recursive Functions.
There have been a number of results tieing together these three different models of computation. Campagnolo, Moore and Costa [3] describe a class of real functions (they call L) that use linear differential equations in place of discrete recursion; they show that the “discrete part” (definition 4.1) of L is exactly the usual Kalmar elementary computable functions on the naturals. Building on this, Bournez and Hainry [1] show that L extended by a certain limit operation is the class of C2 elementary computable functions on R. In section 4, we provide an alternative proof of the result of [3]. In section 5 we prove a variation of the result from [1]; in particular, we extend L by a differ- ent limit operation and show that this is exactly the elementary computable functions on R.
The novelty we bring to these problems are two new techniques, which we call “the method of approximation” and “lifting.” The first technique is used throughout the paper. The basic idea of this technique is to define a general kind of approximation relation that can hold between two classes of functions F and H. Roughly speaking we will say that H approximates F, if for any required precision, any function of F can be approximated to that precision with a function from H; this will be written, roughly, as F ≤

H, and will in fact be a transitive relation under the right conditions. Our approach to both the theorems of this paper is to first show that two classes of functions approximate each other, and then derive the desired equality from the sufficiently close approximation. The approximation inclusions in our proofs proceed by induction on the construction of the function algebra. Due to the transitive property of the approximation relation, to show one class approximates another we can break down the proof into a series of natural tasks.
The second technique, “lifting,” is the main tool used in section 5 to pro- vide a function algebra for the elementary computable functions on R. The idea is to begin with a known complexity result on N, such as the fact that the elementary time functions defined via Turing Machines are exactly the functions in the function algebra FA[+,  . , U, 0, 1; comp,  ,  ], and lift this to a result on R. The lifting can be seen as a 3 step process. First we lift the result on N to an analogous result on the rationals, Q, where the model treats the rationals as pairs of natural numbers. The second step (the most involved one) is to lift this to a result on Q, where the rationals are given by oracle approximations (i.e. exactly the Computable Analysis model restricted to Q). The third step is to lift this to R by applying limits. In the work of [1], the proof involves coming up with a new Turing Machine simulation of the class of elementary computable functions on R; we manage to avoid using a new Turing Machine simulation, by re-using the classic result (which of course involves a Turing Machine simulation) and lifting this to R.
We would like to claim that the advantages of these techniques are twofold. First, they provide a different approach to some of these problems, which seems to facilitate thinking about these problems, especially when dealing with function algebras. Second, the techniques appear to be more amenable to generalization and wider application than some of the earlier approaches. We claim this based on other work in progress, and based on the character of the development. A number of lemmas are general, not specific to the elementary computable functions. Furthermore, it seems that a number of lemmas stated for the elementary computable functions, could be stated in a more general way. The wider vision for this approach is a collection of general tools with broad application. We present the beginning of such a development.

Approximation
To develop formally the definition of approximation we will need to be able to talk about functions and their arguments in a precise way. If a function f (x1,... , xk) is defined on exactly Xk and takes values in X, we say it is

an X−function, or equivalently, that it has universe X; we do not consider vector valued functions. We always assume the universe is a subset of R, and in fact the only particular cases we consider in this paper will be N, Q, and
R. To refer to function arguments precisely we use the notion of “variables.”
Definition 2.1 (Variables)
Let the set V = {vi | i ∈ N} be called variables. If we refer to a “set of variables,” we always mean a ﬁnite subset of V.
Suppose X ⊆ R is some set. For a set of variables ν ⊂ V, a function from
ν to X is called an assignment in X.
If we write μ; ν we mean that sets of variables μ and ν are disjoint (while writing μ, ν is neutral on this point).
Suppose ν = {vi1 ,... , vik }, where i1 < ... < ik, and x ∈ X is a length k sequence (i.e. x = x1,... , xk and all xj ∈ X). By ν → x, we mean the assignment which maps vij to xj.
Definition 2.2 A function with variables from Xk to X is a ﬁnite set of variables ν, together with a rule which takes an assignment ν → a (a ∈ Xk) as input and outputs an element of X. If f is the name of a function with variables, we may write f (ν) in order to display its variables; in this case, those are all the associated variables. We denote the value of f (ν) at some assignment ν → a by f (ν → a), or simply f (a) if the variables and assignment are clear from context.
Now we want to define a kind of substitution operation on variables. For example, if f (x, y) = xy we could substitute a for x and b for y to obtain f (a, b)= ab; we could also substitute z for both x and y obtaining f (z)= z2. Thus the operation can be used to change the names of variables, or effect a genuine change in the function (note that though formally we defined the variables V = {v1, v2,.. .}, we will in fact freely use any lower case letters for variables).
Definition 2.3 Given a function on variables f (u1,... , uk) and a ﬁnite list (possibly with repetitions) of variables v1,... , vk let g = sub(f ; v1,... , vk) be the function with variables ν = {v1,... , vk} (i.e. the set of variables making up the list) such that for any assignment ρ : ν → x, the value of g(ρ)= f (ρ∗), where ρ∗(ui)= ρ(vi).
We use the following convention, throughout the paper.
Remark 2.4 When we specify a set of functions with some arbitrary vari- ables, we then assume it is closed under any application of sub. If we specify a set of functions without reference to variables we can always think of it as a

set of functions with variables by arbitrarily assigning variables to arguments of functions (distinct variables for distinct arguments) and closing under the sub operation.
For the approximation relation we will use functions to translate between different universes.
Definition 2.5 Suppose X and Y are sets of real numbers. An interpreta- tion from X to Y is a (possibly partial) injection from X to Y .
Notice that an interpretation is a function of one argument; if ω is an interpre- tation and we write ω((a1,... , ak)), we mean (ω(a1),... , ω(ak)). For u ∈ X, we call ω(u) ∈ Y the code of (or the interpretation of) u, and conversely,
for v ∈ Y , we say that v codes ω−1(v) ∈ X. We always use the symbol “ω” for a generic interpretation, sometimes using it without mentioning that it is an interpretation, and not specifying its domain and range when clear from context.
Before defining the relation A ≤E,[ω] B, we give some intuition. First we point out that E is a set of functions with universe R, A and B are sets of functions with universes A and B respectively, where A, B ⊆ R, and ω : A → B is an interpretation. The relation says that for any f ∈A and any desired precision, indicated by a function ε ∈ E, there is a function h ∈ B, such that h approximates f with precision ε, under the interpretation ω. The latter condition concerning the interpretation means that if we have a ∈ A and we
want to use it in B, then we really use ω(a), and if we have b ∈ B which we want to use in A, we use ω−1(b). We now make this precise, starting with the relation on 2 functions (example 2.11 follows the definitions).
Definition 2.6 Suppose μ and ν are disjoint sets of variables, and A, B ⊆ R. Suppose f (μ) is a function with variables, on universe A, and h(μ; ν) is a function with variables, on universe B. Suppose ε(μ; ν) is a function with variables, on universe R. Suppose ω : A → B is an interpretation. By
f ≤ε,[ω] h,

we mean that for all a, b ∈ Domain(ω), h(μ → ω(a),ν → ω(b)) is in the domain of ω−1, and the following holds:

|f (μ → a) − ω−1 ◦ h(μ → ω(a); ν → ω(b))|≤ ε(μ → a; ν → b).
Definition 2.7 Let A, B, and E be classes of functions with variables with universes A, B, and R, respectively, such that A, B ⊆ R. Suppose ω : A → B is an interpretation.
We write

A ≤E ,[ω] B
to mean that for any f (μ) ∈A and ε(μ) ∈ E , there is h(μ) ∈ B, such that
f ≤ε,[ω] h.
We write
A ≤E ,[ω] B
to mean that for any f (μ) ∈ A and ε(μ; ν) ∈ E , there is h(μ; ν) ∈ B, such that f ≤ε,[ω] h.
Note that definition of approximation states that it needs to work for any precision ε ∈ E; in many applications it would suffice to just have one ε ∈ E, yet it appears easier to inductively prove approximations for the stronger notion we use. Consider some useful conventions regarding the approximation notation.
Remark 2.8 Suppose A and B are sets of functions on universes A and B
respectively. Consider A ≤E,[ω] B.
If ω is missing we mean for ω = idA∩B (the identity function on A ∩ B).
When it is clear that something is an interpretation, we may omit the square brackets.
If E is missing, we assume E = {0} (i.e. the “approximation” must have no error).
If we leave out “+” or “−”, we mean “−”.
If we use “+/−” in a statement we mean that it holds for “+” substituted everywhere for “+/−”, or for “−” substituted everywhere for “+/−”.
The same conventions apply to the case where A and B are each replaced by single functions. We now work out an example which we will in fact use later (in lemma 4.9); we will use the following basic functions.
Definition 2.9 We deﬁne some functions on universe N.
pair(a, b)= (1/2)(a + b + 1)(a + b)+ a (a bijection from N × N to N)
parity(n)= ⎧⎨ 0, if n even;
gcd(a, b)= the greatest common divisor of a and b (note that gcd(a, b)=0 
only if a or b is zero)
code(a, b, s)= 2pair( 	a	, 	b	 )+ s, where we take “x/0” to be 0 (code
gcd(a,b) gcd(a,b)
is motivated in the example)
We introduce the interpretation we will work with throughout this paper.

Definition 2.10
When we say that a rational is presented in (signed) lowest terms we mean that it is given to us as (−1)k(a/b), where either a = b = k = 0, or a, b ∈ N, k ∈ {0, 1}, with a, b > 0 and a and b relatively prime.
We deﬁne an interpretation λ : Q → N. For any rational (−1)ka/b presented in lowest terms, let λ((−1)ka/b) = 2pair(a, b) + k (which = code(a, b, k)).
ρ1 and ρ2 are the unique functions from N to N such that for any ra- tional (−1)ka/b presented in lowest terms, ρ1(λ((−1)ka/b)) = a, and ρ2(λ((−1)ka/b)) = b.
Example 2.11 Consider the function mult(x, y) = xy, on Q. Suppose we want a function mult∗(n, m) on N, such that it interprets mult (via λ), i.e.
mult ≤λ mult∗, which by our convention means mult ≤{0},[λ] mult∗. Given 2
rationals presented in lowest terms as x = (−1)k(p/q), and y = (−1)c(a/b),
their product is arrived at by multiplying the tops of the fractions together, dividing by the product of the bottoms, and taking account of the sign, to attain (−1)k+cpa/qb, where the presentation may no longer be in lowest terms. To interpret this we carry out the same kind of procedure, but on the natural numbers n and m which code rationals. Thus the top should be ρ1(n)ρ1(m), the bottom should be ρ2(n)ρ2(m), and the sign should be s(n, m) = delta(n)delta(m)parity(parity(n) + parity(m)), where delta(x) = 0 if x = 0 and 1 otherwise. To create the proper code, we need to put the fraction in lowest terms, which just means dividing the top and bottom by their greatest common divisor. The function code is defined to make this coding more convenient.  So finally, we end up with mult∗(n, m) =
code(ρ1(n)ρ1(m), ρ2(n)ρ2(m), s(n, m)). We can now check that the interpre-
tation is correct, which in this case amounts to showing:
(*)λ(mult(x, y)) = mult∗(λ(x), λ(y)), for x, y ∈ Q.
Consider x and y as above, and we then have that ρ1(λ(x))	=	p, ρ1(λ(y)) = a, ρ2(λ(x)) = q, ρ2(λ(y)) = b, parity(x) = k, and parity(y) =
Thus the left side of (*) is λ((−1)k+cpa/qb), and the right side is code(pa, qb, delta(λ(x))delta(λ(y))parity(k + c)). These are equal by the defi- nitions.
We now come to some definitions that for the purpose of this paper we could avoid. However, they are essential for showing at least a bit of how these techniques could become more general. We will define the concepts of “bounding class” and “error class.” Intuitively, a class of functions is a

bounding class if it can be used to measure the growth rate of some other class of functions. A class of functions is an error class, if it can be used to measure the error when one class of functions approximates another.
Definition 2.12 A class of functions B is a bounding class if it has the following properties:
Its universe is R.
There is an f ∈B such that f ≥ 1.
f ∈B implies the value of f is always > 0.
For f (x; t) ∈ B, f (x; t)= f (x; −t), for any variable t.
f ∈ B implies f is increasing. Furthermore, for f (μ; t), where t is any variable of f, f converges to inﬁnity in the strong sense that for any positive N ∈ R, there is a positive M ∈ R such that for any μ → x ∈ R, we have f (μ → x; M) > N.
If β(ν) is in B and γ are variables disjoint from ν, then there is β∗(ν; γ)
in B such that β(ν) ≤ β∗(ν; γ).
If f, g ∈ B, then there are h1, h2, h3 ∈B such that f + g ≤ h1, f ∗ g ≤ h2
and f ◦ g ≤ h3.
Definition 2.13 A class of functions E is an error class if it has the fol- lowing properties:
Its universe is R.
f ∈E implies the value of f is ≥ 0.
For f (x; t) ∈ E , f (x; t)= f (x; −t).
f ∈ E implies f is decreasing. Furthermore, for f (μ; t), f converges to zero in the strong sense that for any positive ϵ ∈ R, there is a positive M ∈ R such that for any μ → x ∈ R, f (μ → x; M) ≤ ϵ.
If β(ν) is in E and γ are variables disjoint from ν, then there is β∗(ν; γ)
in E such that β(ν) ≥ β∗(ν; γ).
If f ∈ E , then there is f ∗ ∈E such that f ∗ ≤ (1/2)f.
We always use E to denote a generic error class, thus we do not always mention this. We relate these kinds of classes by taking the reciprocal.
Definition 2.14 For a set of functions F , 1/F = {1/f | f ∈ F}.
Proposition 2.15 If B is a bounding class then 1/B is an error class.
Proof. We can check that 1/B satisfies the 6 defining properties. For example, consider the last property. Suppose we have 1/f ∈ 1/B, and we need f ∗ ∈B such that (1/f ∗) ≤ (1/2)(1/f ). Since f ∈ B, and B is a bounding class, there

is f ∗ ∈B such that f ∗ ≥ f + f = 2f , which has the desired property. 
Some examples of bounding classes are the following; TW is the only one we will use in this paper.
Definition 2.16
Let P be {a(|x1| + 1)b ... (|xn| + 1)b | n ∈ N, a,b ∈ Q, a, b > 0}.
Let TW be {2···2p | p ∈ P}, that is the functions which consist of a tower
of powers of 2 with a function like a polynomial at the top.
Proposition 2.17 P and TW are bounding classes.
Thus 1/P and 1/TW are error classes. We will also be interested in another error class defined using iterated logs; we actually iterate a modification of log2 so the functions are defined on all of R.
Definition 2.18
Let lg(y)= ⎧⎨ log2 y, if y ≥ 2;
let IL be {a · lg ◦ ... ◦ lg(p) | p ∈ P,a ∈ Q,a > 0}
Proposition 2.19 1/IL is an error class.
Note that IL is not a bounding class (though it would be if we removed condition f ∗ g ≤ h2 from the last line in the definition of bounding class).
Now we justify the approximation notation by showing it is a partial order under the right conditions, that is it satisfies transitivity; when we reference “transitivity” in this paper we mean some application of the following lemma.

Lemma 2.20 (Transitivity) Suppose A, B, and C are classes of functions on universes A, B, and C, respectively, and ω : B → C is an interpretation.

If A ≤E
E
+/−
C then A ≤E,idA∩B∩C C

If A ≤E B ≤E	C then A ≤E,idA∩B∩C C
If Domain(ω) ⊆ A and A ≤E B ≤[ω] C then A ≤E,ω C
Proof.
Let f (μ) ∈ A, α(μ; ν) ∈ E and we need h(μ; ν) ∈ C such that |f (x) − h(x; y)|≤ α(x; y) for x; y ∈ A ∩ B ∩ C. Since E is an error class there is α∗(μ; ν) ∈E such that α∗(μ; ν) ≤ (1/2)α(μ; ν). Let g(μ; ν) ∈B such that
|f (x) − g(x; y)|≤ α∗(x; y) for all x; y ∈ A ∩ B. Let h(μ; ν) ∈C such that
|g(x; y) − h(x; y)| ≤ α∗(x; y) for x; y ∈ B ∩ C. Thus |f (x) − h(x; y)| ≤ 


 	 	 
α∗(x; y)+ α∗(x; y) ≤ α(x; y), as required. Note that we need “+” for the first approximation, but “+” or “−” works for the second one.
The proof is very similar to the previous one.
Let f (μ) ∈ A and α(μ; ν) ∈ E and we need h(μ; ν) ∈ C such that
|f (x) − ω−1 ◦ h(ω(x); ω(y))| ≤ α(x, y) for all x; y ∈ Domain(ω). Let g(μ; ν) ∈ B such that |f (x) − g(x; y)| ≤ α(x; y) for all x; y ∈ A ∩ B. Let h(μ; ν) ∈ C such that |g(x; y) − ω−1 ◦ h(ω(x); ω(y))| ≤ 0 for all x; y ∈ Domain(ω). Thus |f (x) − ω−1 ◦ h(ω(x); ω(y))| ≤ α(x; y) for all x; y ∈ A ∩ B ∩ Domain(ω), which is enough since Domain(ω) ⊆ A, B.
Note that the condition Domain(ω) ⊆ A also ensures that A ≤E,ω C
makes sense.

A useful shorthand is the following “approximate equality.”

Definition 2.21 We write A ≈E
B to mean that both A ≤E
B and

E
+/−
A hold.

Note that with the definition of approximation (with its particular quantifiers)

it is important to read the definition in the right order. We use B ≥E
A as

another way to write A ≤E	B.
Another important kind of relationship between classes of functions will
be that of one class dominating another.
Definition 2.22 Suppose A and B are classes of functions on the same uni- verse X. We write A≤B if for every function f (x) ∈A there is a function h(x) ∈B such that |f (x)|≤ h(x) for all x ∈ X.
Again, note that due to quantifiers in the definition, the order in which we read the expression is important; by writing B≥ A we mean that A≤ B.

Function Algebras And Operations
We will use function algebras to define most of our classes of functions. They are defined by giving some basic functions and closing the class under opera- tions on functions.
Definition 3.1 (Operations) An operation on functions (or operation for short) is a function which takes as input some functions with variables (and possibly some variables), and outputs a single function with variables. An operation has universe F (a set of functions) if it is deﬁned on functions from F and returns a function in F (for any F that we consider, there is always an associated X ⊆ R such that all functions in F have universe X). If

F is all the functions with universe X ⊆ R, we say the operation has universe
X.
For example,	we could define an operation called “bounded sum,” (f (x; y); y; z), with Σuniverse N, which takes one function and two variables
	
Definition 3.2 (Function Algebras) Suppose B is a set of functions (called basic functions), and O is a set of operations. Then FA[B; O] is called a function algebra, and it denotes the smallest set of functions containing B and closed under the operations in O. For ease of readability, we often list the elements of B or O simply as a list separated by commas.
An example of a function algebra we will use is the elementary computable functions defined via bounded sums and bound products. Let be the op- eration on universe N which takes a function f (x¯; y) and returns g(x¯, z) =

z
y=0
f (x¯; y).	Let comp be the operation which takes some functions and

composes them. We define the basic functions for this class.
Definition 3.3 Let basicN  be the following functions with universe N:
+,  . , P, 0, 1, where P is the set of all projection functions on N and  . is the usual cut-off subtraction, deﬁned by
x  . y =	x − y	if x ≥ y .
0	otherwise
Thus FA[basicN; comp,  ,  ] is the elementary computable functions.
Definition 3.4 Let the function algebra FA[basicN; comp, , ] be abbrevi- ated by FAN.
Recall that by convention all sets of functions (including ones defined via function algebras) are implicitly functions with variables, closed under sub (notice that sub is in fact an operation, so the convention means that it is included in all function algebras as one of its operations). Notice that in a function algebra, there could be 2 distinct ways to construct the same function. This highlights the syntactic side of a function algebra, which will become an issue in the section 5.
Definition 3.5 Given a function algebra F , and f ∈ F, bya construction tree of f we mean a tree which describes a construction of f in the function algebra. The leaves of this tree are labeled by various basic functions in the algebra, and internal nodes are labeled by operations in the algebra. Thus, we can think of the tree as specifying how to build a function, starting with the leaves and moving up the tree. Each node then can then be seen as specifying

a syntactic term, as well as a corresponding function. For the tree to be associated to f, means that f is the function associated with the root of the tree.
We now develop a useful notion of one operation approximating another. The rough idea is that one operation approximates another one if by begin- ning with functions which approximate each other, applying the operations maintains this approximation.
Definition 3.6 Suppose opA  and opB  are operations of the same arity

k > 0 with universes A and B respectively, we say opA
E ,[ω]
+/−
opB
if for

any f1,... , fk ∈ A and any ε ∈ E whose variables contain all those of

opA(f1,... , fk), there are ε1,... , εk ∈ E , such that for any f ∗,... ,f ∗
∈ B,

1	k
if fi ≤εi,[ω] f ∗ (i = 1 ... k) then op (f1,... , fk) ≤ε,[ω] op (f ∗,... ,f ∗).
+/−	i	A	+/−	B	1	k
The notational conventions for approximation (remark 2.8) continue to apply for approximation with operations; recall that by convention we can choose “+” throughout or “−” throughout in the above definition. Considering the above definition, it is conceivable that we wind up considering f ≤ω f ∗, where
domain(ω) /⊆ domain(f ) or domain(f ∗) /⊆ range(ω). This would raise some
issues for the definitions, so we simply rule this out by convention when dealing
with operation approximation.
To make the definition more concrete consider a “interpreting” composition (it will be used later). Supposing F is a class of functions, by compF we mean the operation of composing functions from F; if in place of F, we have a set
A ⊆ R, we mean that F includes all functions with universe A.
Proposition 3.7 Suppose X, Y ⊆ R and ω : X → Y is an interpretation. Then compX ≤[ω] compY .

Proof. Suppose f (μ; t) and g(γ) are functions on universe X and fω(μ; t) and gω(γ) are functions on universe Y such that f ≤ω fω and g ≤ω gω. We need to show that f (μ; g(γ)) ≤ω fω(μ; gω(γ)). Fix any assignments μ → a; γ → b ∈ X, and the following calculation proves this:

ω−1 ◦ fω(μ → ω(a); gω(γ → ω(b))) = ω−1 ◦ fω(μ → ω(a); ω ◦ g(γ → b))
= ω−1 ◦ ω ◦ f (μ → a; g(γ → b))
= f (μ → a; g(γ → b))
The first equality follows by g ≤ω gω and the second by f ≤ω fω.	 

Definition 3.8 For sets of operations OA and OB, we write OA ≤E,[ω] OB if for every opA ∈ OA, there exists a opB ∈ OB, such that opA ≤E,[ω] opB.

Given a function algebra, we can also think of it as specifying operations. For example, for a function f (x; y) ∈ FAN, we could create the function
g(x; u; z) = u + Σz	f (x; y). We can think of this as an operation which
takes any function f with universe N as input and outputs the function g.
Definition 3.9 Given a set of functions B on universe X ⊆ R, and operations O on universe X, we let OP[B; O] be the following set of operations on universe X:
Include “function variables” along with the basic functions B, and consider the function algebra deﬁned by closing under the operations O. The result- ing “functions” which have at least one function variable can be seen as operations in which any function (with universe X) can be substituted for a function variable.
The following is an easy but repeatedly used lemma.
Lemma 3.10 Suppose B1 and B2 are classes of functions and O1 and O2 are sets of operations whose universes include B1 and B2, respectively. If B1 ≤E,[ω]
FA[B2; O2] and O1 ≤E,[ω] OP[B2; O2] then FA[B1; O1] ≤E,[ω] FA[B2; O2].
+/−	+/−
Proof. We show inductively on FA[B1; O1] that FA[B1; O1] ≤E,[ω] FA[B2; O2]. For the basic functions B1 we are given that fact. Now consider any op ∈ O1 of arity k and any f1,... , fk ∈ FA[B1; O1]. Let h = op(f1,... , fk) ∈ FA[B1; O1].  Given any α ∈ E whose variables contain those of h, we

need h∗ ∈ FA[B2; O2] such that h ≤α,[ω]
h∗.	Since op ∈ O1, we have

op∗ ∈ OP[B2; O2] such that op ≤α,[ω] op∗, meaning that we have α1,... , αk ∈E 

such that for any f ∗,... ,f ∗ such that f
≤α1 ;[ω] f ∗,... ,f 
≤αk ;[ω] f ∗, we have

1	k	1
+/−	1
k	+/−	k

op(f1,... , fk) ≤α,[ω] op∗(f ∗,... ,f ∗). Inductively we have such f ∗,... ,f ∗, so
+/−	1	k	1	k
we let h∗ = op∗(f ∗,... ,f ∗).	 
1	k
The previous lemma demonstrates the utility of approximating an oper- ation. The straightforward approach to showing that some function algebra contains another (or approximates another) is to work inductively on the par- ticular function algebra in question. For another related claim, the same pro- cess is carried out, starting from scratch. With the concept of approximating an operation we can show once and for all the resources needed to approx- imate an operation and then this fact can be re-used in different contexts. This technical point fits in with our vision of trying to develop a collection of generally applicable tools, within the context of the method of approximation. We will now show how composition can be approximated in a general way (in this paper it will be used for two special cases). We introduce some

terminology in order to make the claim.
Definition 3.11 |¯b − a¯| abbreviates |b1 − a1| + ... + |bn − an|.
We define a modification of the Lipshitz condition.
Definition 3.12
Let f be a function on n arguments, and L a function on 2n arguments. f is L−lipshitz if the universe of L contains that of f and |f (¯b) − f (a¯)| ≤ L(¯b; a¯)|¯b − a¯| for all a¯ and ¯b in the universe of f.
Supposing F and L are classes of functions, we say F is L−lipshitz if for every f ∈F there is an L ∈L such that f is L−lipshitz.
At first the next lemma may seem to say that as the bounds get worse, the approximation gets better. However, note that for two bounding classes, say

P and T W, op
≤1/T W op
is not a stronger claim than op
≤1/P op
, since

A	+	B	A	+	B
in the latter approximation, the functions to which the operations are applied
are only within 1/P accuracy.

Lemma 3.13 Suppose B is a bounding class and F is some class of functions which is B−Lipshitz, closed under composition, and satisﬁes F ≤ B. Then
1/B
compF ≤	OP[sub, comp].

Proof. Suppose f (u), g(x) ∈F (one variable for simplicity) and r(x; y) ∈ B. We need α1,α2 ∈ B such that if f ≤1/α1 f ∗ and g ≤1/α2 g∗, then for h(x) = f (g(x)), we can construct h∗ from f ∗, g∗, comp, and sub, such that h ≤1/r h∗. Let r∗(z; y) be r(x; y) with a new variable z substituted for x.	Let s(u; y; z)  ∈  B such that s(u; y; z)  ≥  r∗(z; y);  note that s(u; y; z) and
α1(u; y; z)= 2s(u; y; z) are in B by the properties of bounding classes.
Now we describe α2.	Using our assumptions on F, let L(b; a) be the B−Lipshitz function for f and let bg be a function in B such that |g(x)| ≤ bg(x). Let p(x; y¯) = 1 + 2r(x; y¯)L(bg(x); bg(x) + 1). By the properties of bounding classes, there is α2(x; y) ∈B such that |p|≤ α2.

Now suppose f ∗(u; y; z) is such that f ≤1/α1 f ∗ and g∗(x; y¯) is such that g ≤1/α2 g∗. Let h∗(x; y¯)= f ∗(g∗(x; y¯); y¯; x). Note that h∗ is a result of comp and sub used on f ∗ and g∗. Note that f ∗ has access to the approximation g∗ and all the variables in question; this is a reason we need arbitrarily long lists of parameters. Now we show h ≤1/r h∗. We start with:
|h(x) − h∗(x; y¯)|≤ |f (g(x)) − f (g∗(x; y¯))| + |f (g∗(x; y¯)) − f ∗(g∗(x; y¯); y¯; x)|.
We look at the above two terms. Consider the first one.

|f (g(x)) − f (g∗(x; y¯))|≤ L(g(x); g∗(x; y¯)) |g(x) − g∗(x; y¯)|
≤ L(g(x); g(x)+ 1) |g(x) − g∗(x; y¯)|
1

≤ L(g(x); g(x)+ 1)
≤ 1/2r(x; y¯)


2r(x; y¯)L(bg(x); bg(x)+ 1)

For the second inequality note that g∗ is within at least 1 of g(x) for all x; y by the definition of p(x; y); thus in particular g∗(x; y¯) ≤ g(x)+ 1. We use throughout, the fact that functions in B are increasing. Consider the second term.

|f (g∗(x; y¯)) − f ∗(g∗(x; y¯); y¯; x)| ≤ 1/α1(x; y¯; x) ≤ 1/2r(x; y), by definition of α1.
Thus |h(x) − h∗(x, y¯)|≤ 1/2r + 1/2r = 1/r.	 

Linear Recursion versus Linear Differential Equa- tions
In this section we apply the ideas of approximation to reprove a result from [3], which says that the “discrete part” of a set of R−functions (whose essential operation is linear differential equations) is exactly the elementary computable functions (which can be defined with linear recursion as its essential opera- tion).
Definition 4.1 (from [3]) Suppose F is a class of functions on R. By the discrete part of F , denoted dp(F ) we mean the following class of functions over universe N: First take all the functions in F whose values are in N on domain N; then restrict these functions just to domain N.
The key analog operation on R is the operation of obtaining a solution to a linear differential equation; for k ∈ N by Ck we mean the k−times continuously differentiable functions on R.
Definition 4.2 LI is the operation which takes any C2 functions with TW bounds g1(x¯),... , gn(x¯), s11(x¯, y),..., snn(x¯, y) and returns h1(x¯, y) where we have the following deﬁning equations:
h1(x¯, 0) = g1(x¯)
.
hn(x¯, 0) = gn(x¯)
 ∂  (h1(x¯, y)) = s11(x¯, y)h1(x¯, y)+ ... + s1n(x¯, y)hn(x¯, y)
.

 ∂  (hn(x¯, y)) = sn1(x¯, y)h1(x¯, y)+ ... + snn(x¯, y)hn(x¯, y)
Definition 4.3 Let basicR be the following functions with universe R: 0, 1, −1, π, P, θ3, where P is the set of all projection functions on R (note that independent of the universe, we use the same notation for projection functions), π is the famous constant, and for any k ∈ N (k > 0), θk(x) = 
0,  x < 0; , a Ck−1 version of the discontinuous function which indicates
xk, x ≥ 0.
whether a number is to the left or right of zero.
The function algebra on the reals that we will now be concerned with is:
FA[basicR; comp, LI].
Note that the restriction in LI to C2 functions with TW bounds has no effect on this class, but is used in approximating LI in lemma 5.22. We use the following notation from earlier papers.
Definition 4.4 Let L abbreviate the function algebra FA[basicR; comp, LI]. The goal we are now aiming for is theorem 4.25:
dp(L)= FAN.
The proof in [3] proceeds by showing the two inclusions. The inclusion “⊇” is proved inductively on the construction of the functions in FAN, using the operations of L at each step. The inclusion “⊆” is again proved by induction, this time on the functions in L, but rather than using the operations of FAN at each step, a Turing Machine is constructed, and it is shown how in elementary time an appropriately close approximation can be carried out; of course this relies on the well-known fact that the function algebra FAN corresponds to elementary time. We will give an alternative proof of this inclusion in which we do not use this fact or use any Turing Machines; the proof proceeds naturally using the operations of the function algebra itself.
If one were to begin thinking about a proof along these lines, an apparent problem presents itself. A function f ∈ dp(L) is in there due to some associ- ated construction tree (recall definition 3.5). While f (the function associated with the root of the construction tree) is required to have natural number values on natural number inputs, there is no such constraint on the functions associated with other nodes in the construction tree (they maybe real valued). To inductively show that f is in FAN, requires that we deal with these non-root nodes in FAN; however, it is unclear how to deal with real number values in FAN. The way we get around this issue is to introduce an intermediary function

algebra with universe Q. This function algebra will naturally approximate L (corollary 4.24). Then we can naturally interpret this function algebra on Q into FAN (see corollary 4.12). The theorem then follows by the transitivity of the approximation relation. At the end of this section we discuss a number of advantages of this approach.
The main operations of the function algebra on Q will be a kind of bounded sum (line ) and bounded product (line ) on the rationals. They are de- fined so that they preserve continuous functions when applied to continuous functions. This property is important for the next section, and while not important for this section, presents little complication for it. We call the operation (on f (x; y)) a line sum because for a fixed x ∈ Q, the plot of g(x; z) = line (f, y, z) will look like this: For each integer n, g(x; n) has some value in Q (namely f (x; 0)+ f (x; 1) + ... + f (x; n)), and the rest of g is described by connecting successive values on integers by straight lines. Prod-
ucts are similar. Note that the operations will be deﬁned for negative rationals
due to our convention that Σz	f (x; y) or  z	f (x; y) will be taken to be
zero for integers z < 0.
Definition 4.5 We deﬁne operations line	and line	with universe Q. Sup- pose f (x¯; y) is a function on universe Q.
line Σ(f, y, z)= g, where Σ	Σ

g(x¯; z)= (1 + [z♩− z)
line  (f, y, z)= h, where 


[z♩
y=0


f (x¯; y)+ (z − [z♩)

[z|
y=0

f (x¯; y).


We have the following basic functions.
Definition 4.6 Let basicQ be the following functions with universe Q: 0, 1,
−1, P, ∗, +, div, θ1, where P is the set of projection functions, θ1 is understood as a function with universe Q (though it was originally deﬁned for R), and
div(x)=	1/x, if x ≥ 1;
1,	else.
Our function algebra of interest is then:
FA[basicQ; comp, line Σ, line  ].

Notice that all the functions in this class are continuous; in the next section we will define an extension of this function algebra which contains discontinuous functions; this motivates the following abbreviation (in the next section we will define FAQ(disctn)).

Definition 4.7 Let the function algebra FA[basicQ; comp, line , line ] be abbreviated by FAQ(ctn).
If it appears to you that the basic functions are redundant, you are probably correct. We should be able to derive ∗ and + in the class, as is typically done for these functions in FAN. However, for us the classes on the rationals are merely a means to an end, so we include possible redundancy to simplify the technical development. We define some functions contained in FAQ(ctn).
Definition 4.8 The following are some continuous functions with universe
Q.
|x| = the absolute value of x.
,⎪⎪⎨ 0, if x ≤ 0;

⎪⎪, 1, if x ≥ 1.
⎪⎪⎨ x,	if 0 ≤ x < 1;
1,	if |x|≥ 1.
They are all in FAQ(ctn), because
|x| = θ1(x)+ θ1(−x), and
sgn(x)= θ1(x) − θ1(x − 1), and
δ(x)= sgn(x)+ sgn(−x).
Our goal now is to show that FAQ(ctn) can be interpreted in FAN and that it can approximate L. In example 2.11, we in fact showed how multiplication in basicQ could be interpreted (via λ) in FAN; the other functions of basicQ can be handled similarly, thus we have the following lemma (note that FAN us a strong class and we will frequently use the fact that it contains many typical functions).
Lemma 4.9 basicQ ≤λ FAN
Lemma 4.10 line	≤λ OP[basicN; comp,	,	]
Proof. Let f (ν; y) be a function with universe Q, and we assume we have an interpretation (via λ) fλ(ν; y), meaning that for any assignment ν → b; y → a we have f (ν → b; y → a) = λ−1 ◦ fλ(ν → λ(b); y → λ(a)). We need an interpretation of line (f ) using fλ. Recall that line  is defined


similar and we can put them together easily. Our goal is hλ(ν; z) such that h ≤λ hλ. We let down(x) = [x♩, x ∈ Q. We have downλ ∈ FAN, such that down ≤λ downλ. The rest of the interpretation is like that for multiplication in example 2.11. To find the bounded product, we find what the top and bottom of the resulting fraction should be, along with its sign and put this together properly. In the following development, we assume that variable z (which indicates the range of the product) will code a positive rational, since we can easily design a function with cases depending on the sign of z. In the following 3 functions (top, bottom, and s), we will want to range over fλ(ν; y) for y = λ(0), λ(1),..., λ([c♩), where c is the value z will be assigned to. To do this, note that ρ1(downλ(λ(c))) = [c♩; this motivates the range of the products/sums below to being ρ1(downλ(z)). To range over λ(y) as y = 0, 1,... , [c♩ we will use the fact that a non-negative integer y presented as a fraction in lowest terms is of the form (−1)0(y/1) and so we code it as code(y, 1, 0).

Let s(ν; z)= parity(	ρ1 (downλ(z)) parity(fλ(ν; code(y, 1, 0))))
Then hλ(ν; z)= code(top(ν; z), bottom(ν; z), s(ν; z)).	 
The proof for sums is similar, though finding the “top” is a bit more technically involved.
Lemma 4.11 line	≤λ OP[basicN; comp,	,	]
Corollary 4.12 FAQ(ctn) ≤λ FAN
Proof. By lemma 3.10, it suffices to show that basicQ ≤λ FAN, and that FAN interprets the 3 operations in the rational class. The last two propositions showed that both line sums and products can be interpreted. Proposition 3.7 shows that composition can be interpreted.	 
Now we develop the approximation of L by FAQ(ctn). Approximating the basic functions is relatively straightforward, as is the following bound.
Proposition 4.13 TW ≤ FAQ(ctn).
Lemma 4.14 basicR ≤1/TW FAQ(ctn)
Proof. Except for θ3 and the constant π, all the functions and constants of basicR are extensions of something in basicQ and so we approximate them with zero error on Q. We can approximate θ3 with zero error since θ3 =

θ1 ∗ θ1 ∗ θ1. For π we carry out a sufficiently long Taylor series approximation, which is simulated using line and other simple functions from FAQ(ctn); notice the importance of div. The necessary length of the series will be a function from T W, which we can dominate in FAQ(ctn), by proposition 4.13. 
Linear recursion will be a useful tool for capturing Euler’s Method. We begin with the definition on N.
Definition 4.15 LR is the operation which takes any functions on universe N, g1(x¯),... , gn(x¯), s11(x¯, y),..., snn(x¯, y), t1(x¯, y),..., tn(x¯, y) and returns h1(x¯, y), where we have the following equations:

h1(x¯, 0) = g1(x¯)
.
hn(x¯, 0) = gn(x¯)

h1(x¯, y + 1) = s1n(x¯, y)h1(x¯, y)+ ... + s1n(x¯, y)hn(x¯, y)+ t1(x¯, y)
.
hn(x¯, y + 1) = sn1(x¯, y)h1(x¯, y)+ ... + snn(x¯, y)hn(x¯, y)+ tn(x¯, y)

Proposition 4.16 LR ≤ OP[basicN; comp,	,	]
We define a version of linear recursion for the rationals, which (as with line Σ
and line	) yields continuous functions when it begins with continuous func-
tions.
Definition 4.17 lineLR is the operation with universe Q which takes some input functions f1, f2,... and returns:
h(x; y)= (1 + [y♩− y)LR(f1, f2,... , [y♩)+ (y − [y♩)LR(f1, f2,... , [y|);
where y is the recursion variable, and we take the value of the function returned by LR to be 0 if the recursion parameter ([y♩ or [y|) is less than zero.
We state a lemma that results from “lifting” proposition 4.16 to the rationals. The proof will follow from some later involved work (the proof appears after corollary 5.19).
Lemma 4.18 lineLR ≤ OP[basicQ; comp; line  , line  ]
Definition 4.19 Given a class of differentiable R−functions F , let F' =
{f ' | f ∈ F} and F'' = {f '' | f ∈ F}, where by f ' and f '' we mean that
f is differentiated with respect to any one variable.

The following is proved inductively (details appear in [3], propositions 4.3 and 4.4).
Proposition 4.20 ([3]) The functions in L are C2 and L, L', L'' ≤T W.
Lemma 4.21 LI ≤1/TW OP[basicQ; comp, line  , line  ]
Proof. We use Euler’s method to approximate the application of any linear differential equation. We will use the operation lineLR in a direct manner to write down the approximating Euler equations (recall that by lemma 4.18 we can freely use lineLR). The basic idea is straightforward; to approximate an application of LI to some accuracy in 1/TW we choose sufficiently accurate approximations to the functions that LI is applied to and we choose a suffi- ciently large number of intervals in TW for the Euler approximation. The point is that the error function with its exponential is easy to overcome with functions from T W.
Suppose we define h(x) from a system of linear differential equations, where the function F describes the differential equation, that is h' = F (x, h); we just display the variable x, the one with respect to which we differentiate. We want to describe a Q−function h∗ that approximates this h to some precision within 1/TW (we really mean to accuracy 1/α for some α ∈T W, but for ease of ex- position, in this proof, we will avoid working out the bounds exactly, referring to 1/TW and TW a bit informally in this way). We start with Q−functions which approximate within 1/TW the functions defining the system of linear
differential equations, that is, we have F ∗ which approximates F to within 1/T W. Approximation h∗ will use F ∗ to simulate Euler’s method on the in- terval [0, x], dividing the interval up into some number of subintervals n, given by the points 0 = x0 < x1 < ... < xn = x, where each interval [xi, xi+1] is of length δ = x/n. We use hi to denote the approximation of h(xi). With lineLR it is straightforward to write down the Euler equations: hi+1 = hi+δF ∗(xi, hi); note that it is a linear recursion because F ∗ is and because the form of the Euler equations is. Note that in writing down these equations we have the needed functions at hand: ∗, +, div (div is used to find δ).
To check that the error really is bounded by 1/T W, we follow the standard error analysis for Euler’s method. Let ei = hi − h(xi), the (global) error after i steps. We can expand h(xi+1)= h(xi)+ δF (xi, h(xi)) + δτ , where τ is a bound on the (local) error at any step of Euler’s method. We can bound τ by δd, where d is a bound on the second derivative of h on the entire interval [0, x]; by proposition 4.20 we have a TW bound on d; note that the bound holds it on the entire interval because functions in bounding classes are increasing.
Suppose that our approximate differential equation F ∗ is within r precision to F , where r is in 1/T W. Because F describes a linear differential equation,

we can factor out of the equation a function to arrive at a Lipshitz function L with a TW bound on it; that is |F (x, h1) − F (x, h2)| ≤ L|h1 − h2|, for L ≤T W. Now we can calculate an error recurrence:
|ei+1| = |hi+1 − h(xi+1)|
= |(hi + δF ∗(xi, hi)) − (h(xi)+ δh'(xi)+ δτ )|
≤ |ei| + δ|F (xi, hi) − h'(xi)| + δ|τ | + δr
≤ |ei| + δL|hi − h(xi)| + δ|τ | + δr
= |ei|(1 + δL)+ δ|τ | + δr
We solve the recurrence to arrive at:
|ei|≤ exL(|e0| + δd + r).
We now see that we can make this error less than 1/TW because we are given L and d with their TW bounds, but we can obtain arbitrarily good 1/TW bounds on r, δ, and |e0|.	 
We will use the Lipshitz property to approximate composition.
Proposition 4.22 If F is a class of functions such that F' ≤ B, where B is a bounding class, then F is B−Lipshitz.
Proof. Let h(x1,... , xn) be in F. To find a Lipshitz function, consider:
|h(¯b) − h(a¯)|≤	|h(b1, b2,... , bn) − h(a1, b2,... , bn)|
+ |h(a1, b2,... , bn) − h(a1, a2, b3 ... , bn)|
.
+ |h(a1,... , an−1, bn) − h(a1,... , an)|
Consider the first term |h(b1, b2,... , bn) − h(a1, b2,... , bn)|. Consider the func- tion  ∂  h(x1, x2,... , xn) ∈ F' and let β(x1,... , xn) ∈B such that β dominates it. Let L1(b; a) = β(|a1| + |b1|, b2,... , bn), which is dominated by a func- tion in B (which for convenience we also call L1). Since β is increasing and
|a1|, |b1| ≤ |a1| + |b1|, L1 dominates the derivative  ∂  h(x1, b2,... , bn) for all
x1 on the interval between a1 and b1, and so we have:
|h(b1, b2,... , bn) − h(a1, b2,... , bn )|≤ 
|(h(a1, b2,... , bn)+|b1 − a1|L1(b; a)) − h(a1, b2,... , bn)| = L1(b; a)|b1 − a1|.

We obtain L2(b; a),... , Ln(b; a) for all the terms and we bound the sum by something in B, yielding our Lipshitz function.	 
Using proposition 4.22 and proposition 4.20, the following is immediate.
Corollary 4.23 The functions in L are T W−Lipshitz.
Corollary 4.24 L ≤1/TW FAQ(ctn)

Proof. By lemma 3.10, it suffices to show that basicR ≤1/TW FAQ(ctn), and that FAQ(ctn) approximates the 2 operations in the real class. The last propo- sition shows that LI can be approximated. Lemma 3.13 shows that composition
can be approximated (we set B = TW and F = L in that lemma, and note that by proposition 4.20 and corollary 4.23 we satisfy the conditions of that lemma).	 
Now we finish the alternative proof of [3], lemma 4.8 (note that we are only interested in an alternative proof of one of the directions in the below equality).
Theorem 4.25 dp(L)= FAN
Proof.
⊇: This direction is carried out inductively in [3] and we make no modifi- cation to the existing proof.
⊆: By corollaries 4.24 and 4.12, respectively, we have:
L ≤1/TW FAQ(ctn) ≤λ FAN.
By transitivity we have L ≤1/T W,λ FAN. Let f (x) ∈ dp(L), and take any α(x; y) ∈ 1/T W, so we have f ∗(u; v) ∈ FAN such that f ≤α,λ f ∗. By fixing y to a large enough number, we have α(x; y) ≤ 1/3 for all x and can obtain h(u) ∈ FAN such that for x ∈ N, |f (x) − λ−1 ◦ h(λ(x))| ≤ 1/3. Since f (x) ∈ N for x ∈ N, f (x)= nearest(h(λ(x))) ∈ FAN, where nearest(a) returns the closest natural to the rational coded by a. Note that we use the fact that both nearest and λ|N are in FAN.

We point out here that most of the work of this section is needed for the theorem of the next section. We re-use exactly corollary 4.24 in the next section; in the next section we need to prove the opposite approximation in corollary 5.24 and both approximations are used to obtain corollary 5.25. Thus given that we want the result of the next section, the only extra work in this section is the relatively straightforward work with the interpretation.

Connection to Computable Analysis via Lifting
We will use standard notions from Computable Analysis following the develop- ment in Ker-I Ko [5]. For the most part he restricts his attention to functions defined on a finite interval, while we consider functions defined on all of R. Thus in this work, the main difference is that a number of notions will de- pend on both the input value to the function, as well as the usual accuracy

parameter. We will be concerned with the elementary computable functions over various universes. By E(R) we mean the total R−functions f (x) which can be computed to accuracy 1/n in time t(x; n), where t ∈ T W. The real input x is given by an oracle which gives x to any demanded precision as a dyadic rational (the set of dyadic rationals is denoted D). Note that we use the approximation of the form 1/n rather than 1/2n, since for elementary computable functions such distinctions have no effect. We will be relating such classes for N, Q, and R. For R, we always use the typical model above, which we can think of as a kind of “approximation model.” For N, we compute exactly since we know the input exactly, thus on N we are using a “discrete model;” we let E(N) be the usual elementary computable functions on N. For Q, we have two options. We can use the approximation model used for R, where it just happens that for a function f (x), x and f (x) are always in Q; we call this class of functions apxE(Q). An alternative is to use a discrete model for Q (which we will call disE(Q)), for which the following definitions of a kind of denominator and numerator function will be convenient.
Definition 5.1
Let D(0) = N(0) = 0.
For a rational (−1)ka/b presented in lowest terms, let D((−1)ka/b) = (−1)kb and N((−1)ka/b)= (−1)ka.
A Q−function f (x) is in disE(Q) if there is an elementary time Turing Ma- chine on N that computes it in the following sense: On input x ∈ Q the machine is given the triple (|N(x)|, |D(x)|, s(x)), where s(x) is the sign of x, and we must compute the triple (|N(f (x))|, |D(f (x))|, s(f (x))); note that the time allowed depends on the length of the representation of x as a triple of natural numbers (for a sequence of numbers x we use a sequence of triples). Note that apxE(Q) contains only continuous functions, while disE(Q) con- tains discontinuous functions. The general approach of this section is to lift complexity results from N to ones on R. To do this we will see that E(N) and disE(Q) are easily related and that apxE(Q) and E(R) are closely connected. The main work will be in providing a useful connection between apxE(Q) and disE(Q).
We introduce the technique of lifting in this section and apply it to prove
a result that is similar to that of Bournez and Hainry [1]. One of their main claims is that:
E(R)= L∗ (for C2 functions), where the latter class is L with a certain limit operation added.

They prove both inclusions, where E(R) ⊆ L∗ is the more involved one, done by showing how to simulate Turing Machines in L∗. In our result we will use a different limit operation.
Definition 5.2 Suppose E is a class of error functions. E −LIM is the oper- ation which takes a function ε(ν; t) ∈E and any function f (ν; t) and returns F (ν)= limt→∞f (ν; t) if the limit exists and F ≤ε f.
For a class of functions F, we write F(E −LIM) to indicate the class F closed under the operation E −LIM. We will prove the following (in theorem 5.26):
E(R)= L(1/IL−LIM).

The proof avoids the use of a Turing Machine simulation, and instead proceeds by lifting the existing result on the naturals (of course the original result on the naturals involves a Turing Machine simulation, but the point is that we do not carry out another simulation on the reals as is done in [1]). It is arguable as to which proof is simpler or which result is better, but this work offers another perspective and it does seem that these tools should be more generally applicable.
To relate disE(Q) and apxE(Q) we will use modulus functions. These functions enforce a strong notion of continuity. It is well-known that the functions of Computable Analysis are continuous on their domain, but they have a stronger property of having modulus functions, which roughly means that their continuity is witnessed by modulus functions. We modify the usual notion to allow the input x to function f (x) to also be input to the modulus function (in addition to the usual accuracy parameter).
Definition 5.3
Suppose f (x) and m(x; z) are functions in which the universe of f is con- tained in the universe of m. Then m is a modulus for f if :
For all x¯ and y¯ in the universe of f, and z /=0 in the universe of m,
|x¯ − y¯|≤ m(x¯; z) implies |f (x¯) − f (y¯)|≤ 1/z.

The class of functions M is a modulus for the class of functions F if for any f ∈ F, there is m ∈M such that m is a modulus for f.
The following proposition is similar to corollary 2.20 from [5], which is an analogous statement for the real polynomial time functions on a bounded interval.
Proposition 5.4 apxE(Q) and E(R) both have a 1/T W− modulus.

We will need some technical lemmas. The following extends a function on domain Z to a well-behaved continuous function on domain R.
Definition 5.5 Suppose f (x1,... , xk) is deﬁned on Z (taking on values in
Let fˆ be deﬁned on R as follows:
fˆ(x1,... , xk)= 
f ([x1♩,... , [xk♩)([x1♩ +1 − x1) ... ([xk♩ +1 − xk)
+ f ([x1|, [x2♩,... , [xk♩)(x1 − [x1♩)([x2♩ +1 − x2) ... ([xk♩ +1 − xk)
.
+ f ([x1|,... , [xk|)(x1 − [x1♩) ... (xk − [xk♩),
where the intention is to range over all 2k combinations of [·♩ and [·| applied to the xi; corresponding to whether [x♩ or [x| is applied, we multiply f by ([x♩ +1 − x) or (x − [x♩), respectively.
Proposition 5.6 Suppose f is a function with domain Z.
For x ∈ Z, fˆ(x)= f (x).
fˆ is continuous.
min(f ([x1♩,... , [xk♩),... ,f ([x1|,... , [xk|)) ≤ fˆ, and
fˆ ≤ max(f ([x1♩,... , [xk♩),... ,f ([x1|,... , [xk|)), where the minimum and maximum are taken over all 2k combinations.
The following lemma makes the basic connection between the two models of computation on the rationals, the approximation model and discrete model.

Lemma 5.7 apxE(Q) ≈1/TW {f ∈ disE(Q) | f has a modulus in 1/T W}.
Proof. We prove the two approximate inclusions.
(⊇) We prove the stronger claim of containment. Let f (x) ∈ disE(Q) (we consider just one variable for simplicity), with a modulus m(x; z) ∈ 1/T W, and we show that f ∈ apxE(Q). Let M be the Turing Machine which computes f where the input x ∈ Q is given as a triple of natural numbers. We design a Turing Machine N to put f in apxE(Q). N has an oracle for x and an accuracy input z.	First compute m(x∗; z), where x∗ is a number such that x ≤ x∗ (easily obtained by querying for close enough approximation to x and then adding one). Put m(x∗; z) on the query tape to get some y such that |x − y| ≤ m(x∗; z) ≤ m(x; z) (the last inequality holds since functions in error classes decrease). Also note that y is a dyadic of length ≤ m(x∗; z) (by usual definitions in Computable Analysis, see [5], definition 2.1, requirement “prec(φ(n)) = n”) and so for y = p/q, we have p, q ≤ m(x∗; z). Now we simply run M on (p, q), thus outputing exactly f (y)

(we ignore the sign of y for simplicity), and due to the modulus condition we know that |f (x) − f (y)| ≤ 1/z. This is within TW time in x and z because the length of the p and q are larger by at most a function in TW and M ’s running time is bounded by a function in T W.
(≤1/TW ) Let f (x) ∈ apxE(Q), and let α(x; y) ∈ T W, and we need f ∗(x; y) ∈ disE(Q) such that f ≤1/α f ∗ and f ∗ has a modulus in 1/T W. Let M be the Turing Machine that computes f in the Computable Analysis sense of approximation. Thus M has an oracle tape which gives approxima- tions of x, and an input tape where the reciprocal of the desired accuracy is input. We will design a Turing Machine N which takes x; y ∈ Q as input (as exact pairs of naturals); f ∗(x; y) will be the function computed by N. To obtain the condition f ≤1/α f ∗ alone would be straightforward. We could define N in terms of M, by inputing the the desired accuracy, [α(x; y)|, to the machine M, and use x as the oracle to M. This is roughly how N will in fact be defined, but guaranteeing the modulus condition will require some care and is the reason for complicating the definition of N. For ease of exposition, suppose the inputs (to machine N ) x, y are both of length 1, so we write them as x and y. Let t(x; n) ∈T W be the time bound on machine
M. First we will want to compute functions that dominate α, t ∈T W, and
do so continuously; to approximate them directly would lead to problems because they can take on irrational values, so that approximations would not be continuous. Thus, we consider their values on domain N (which are in N), and take the linearized versions of definition 5.5 and proposition 5.6. Since functions in TW are concave down, α ≤ αˆ and t ≤ tˆ. We can calcu-
late these functions exactly given sufficient time in T W. We also calculate τ (x; y)= t(x;1 + αˆ(x; y)). Now we will define a function h(u1; u2; u3) on N, taking on values in Q:
Take the output of running M(u3 /u2 )(u1), which means that we use u3/u2
as the oracle, and use u1 as the accuracy input. When we say to use u3/u2 as the oracle we mean that we consider the binary expansion of u3/u2 and whenever some accuracy is asked of the oracle, exactly enough bits of this expansion are given.
Consider the continuous function hˆ(u1; u2; u3) obtained from h, as indicated
in definition 5.5. We define N(x; y)= hˆ(1 + αˆ(x; y); τ (x; y); xτ (x; y)). It is
continuous because it is the result of composing continuous functions with a continuous function. Furthermore this function has a 1/TW modulus because the only functions we apply to the inputs are bounded by T W. It is left to check that it operates as required, that is that |f (x) − N(x; y)|≤ 1/α(x; y). By proposition 5.6, it suffices to note that all of the 23 versions of h on Z are within 1/α(x; y) of f (x). This is true because h runs M,

and the following two points hold concerning this run. First, M is given a sufficiently large accuracy parameter 1 + αˆ, i.e. both [1+ αˆ♩ and [1+ αˆ| are at least as big as α. Second, M uses an oracle for x that is good enough, meaning that whenever M asks for x to some accuracy, it gets something that is that accurate. Since the running time for M is bounded by τ , it can ask for an x of accuracy at best 1/τ . By the the definition of h, we picked a number as the oracle that was this close to x (i.e. [xτ ♩/[τ | is always close enough to x, as are the 3 other combinations applying [·| or [·♩; in fact we should choose instead of τ something slightly larger).

We now develop some results concerning the limit operation. The following claim is similar to corollary 2.21 from [5].
Proposition 5.8 E(R) is closed under 1/IL−LIM.
Proof. The idea behind the proof is that we can start with functions that are 1/IL close, and make them close to within 1/n by composing a IL function with a function from TW (i.e. functions from TW grow just fast enough to bring the slow growing functions from IL up to the speed of the identity function). Suppose f (μ; t) ∈ E(R) and let M be a machine that computes it. Suppose F (μ)= limt→∞f (μ, t), with |F (μ) − f (μ, t)|≤ 1/α(μ; t) for some
α(μ; t) ∈ IL. To show F is in the class, first note that by the completeness
of R, the limit is in R (i.e. this argument would break down for apxE(Q) since this class requires function values to be in Q). We need a machine N that runs in TW time, such that for any assignment μ → x and any oracle for μ → x, Nx(n) converges to F (μ → x) at rate 1/n.
We define a machine N as follows:
Write down a large enough number h(x; n) so that α(x; h(x; n)) > 2n; since
α ∈ IL, there is such an h ∈T W, so we have time to write it down.
Approximate f (x; h(x; n)) with machine M to accuracy 1/2n.
The run time is within bounds since we can write down h, and M runs in TW 
time. Consider the approximation accuracy:
|F (x) − Nx(n)|≤ |F (x) − f (x; h(x; n))| + |f (x; h(x; n)) − M(x,h(x;n))(2n)|
≤ 1/2n + 1/2n
= 1/n


Proposition 5.9 If A ≤E B, then A(E −LIM) ≤ B(E −LIM).

Proof. We proceed inductively on the number of applications of E −LIM. For the base case, we show A ≤ B(E−LIM). Consider f (μ) ∈ A, and we need g(μ) ∈ B(E −LIM) such that f (μ) ≤ g(μ). Take h(μ; t) ∈ B such that f (μ) ≤ε(μ;t) h(μ; t) for some ε(μ; t) ∈ E. By definition |f (μ)−h(μ; t)|≤ ε(μ; t). Since ε is an error function, ε(μ; t) → 0 as t → ∞, so limt→∞h(μ; t) = f (μ).
Thus we let g(μ)= limt→∞h(μ; t) ∈ B(IL−LIM).
Now, suppose inductively that we know f (μ; t) ≤ g(μ; t), where f ∈ A(E −LIM) and g ∈ B(E −LIM); thus f (μ; t) = g(μ; t) on the intersection of their domains. Suppose ε(μ; t) ∈E and suppose F (μ)= limt→∞f (μ; t) with F ≤ε f . We need G(μ) ∈ B(E−LIM) such that F (μ) ≤ G(μ). To obtain this, just let G(μ)= limt→∞g(μ; t), and since f (μ; t)= g(μ; t) and F ≤ε f , we have G ≤ε g, so G ∈ B(E−LIM).	 
Lemma 5.10 Suppose F is a class of continuous functions with universe R. If apxE(Q) ≈1/IL F then E(R)= F(1/IL−LIM).
Proof. We prove both inclusions.
⊆ We start with apxE(Q) ≤1/IL F.	We want to show that E(R) ≤1/IL
apxE(Q), so for f (x) ∈ E(R), and α(x; y) ∈ IL we need h(x; y) ∈ apxE(Q)
such that f ≤1/α h. We start with a machine M for f and basically re-use the machine N from the proof of lemma 5.7. A difference is that rather than having x and y exactly, now N queries for an appropriate accuracy, and then runs using these answers. Note that h is automatically continuous, but what is required now is that there is some accuracy input, say r, and N must get within 1/r of the correct output. This is taken care of by the fact that the machine N we borrowed from the other lemma had a 1/TW modulus, meaning that by taking inputs for x; y close enough, we get as close as we like to the desired output.
By transitivity, E(R) ≤1/IL F; in fact transitivity only gives this for val- ues in Q, due to the intermediary class apxE(Q), but since F consists of continuous functions we obtain the approximation for all of R. By propo- sition 5.9 we obtain: E(R)(1/IL−LIM) ≤ F(1/IL−LIM). By proposi- tion 5.8 we change the left side in the previous line to finish this inclusion: E(R) ≤ F(1/IL−LIM).
⊇ We start with F ≤1/IL apxE(Q). Since F contains only continuous func- tions, we obtain F ≤1/IL E(R). In a manner similar to the previous inclu- sion, we apply propositions 5.9 and 5.8 to complete this inclusion.
 
Thus our ultimate goal now is to show that apxE(Q) ≈1/IL L, and then
the theorem follows by lemma 5.10.

The next step will be to introduce a function algebra on Q which will yield the same functions as disE(Q). It is defined by simply adding D to the basic functions of FAQ(ctn) (we indicate this addition to the basic functions by placing D after the existing basic functions with a comma separating them):
FA[basicQ, D; comp, line Σ, line  ].
This function algebra contains discontinuous functions, and so we name it as follows.
Definition 5.11 The function algebra FA[basicQ, D; comp, line  , line  ] will be abbreviated by FAQ(disctn).
The following simple proposition is surprisingly useful. It says, in words, that any elementary computable function on N has an extension in FAQ(ctn). Thus we can be quite flexible in coming up with functions in this rational class as long as we do not care how it operates off of N; in fact, we will also reference this lemma for functions on Z, since in FAQ(ctn) we can code an integer easily as a natural, perform the function in N, and convert back to Z. Note that it is not possible to code Q into N within FAQ(ctn) (i.e. λ /∈ FAQ(ctn)), since this would require a discontinuous function (the discontinuous class FAQ(disctn) can do this).
Proposition 5.12 FAN ≤ FAQ(ctn)
Proof. Immediate from lemma 3.10, since we can approximate the basic func- tions, composition, sum and product. For the basic functions, note that that we can obtain cut-off subtraction by: x  . y = sgn(x − y)(x − y). The others are easier to deal with.	 
We will sometimes quote proposition 5.12 with FAQ(disctn) in place of
FAQ(ctn).
Lemma 5.13 disE(Q)= FAQ(disctn).
Proof. We show 2 inclusions. For both, we use the following well-known characterization of elementary time:
(*) E(N)= FAN
(⊆) Immediately from the definitions we obtain disE(Q) ≤λ E(N). By *, we have E(N) ≤ FAN. By proposition 5.12 we have FAN ≤ FAQ(disctn). By transitivity, we have disE(Q) ≤λ FAQ(disctn). Using D and N, we can put λ and λ−1 in FAQ(disctn), and thus we obtain disE(Q) ≤ FAQ(disctn), because for any f (x) ∈ disE(Q), we have fλ ∈ FAQ(disctn) such that f (x)= λ−1 ◦ fλ(λ(x)), and λ−1 ◦ fλ(λ(x)) ∈ FAQ(disctn), by closure under composition.

(⊇) We can strengthen corollary 4.12 to FAQ(disctn) ≤λ FAN, simply by noting that D ≤λ FAN. By *, FAN ≤ E(N). Immediate from the definitions we have E(N) ≤ disE(Q). By transitivity, FAQ(disctn) ≤λ disE(Q). We have λ, λ−1 ∈ disE(Q), so by the same reasoning as in the previous inclusion we have FAQ(disctn) ≤ disE(Q).

We will connect FAQ(ctn) and FAQ(disctn)via modulus functions in lemma 5.18. We first develop a number of ideas used in that proof. The following is proved inductively on the function algebra.
Proposition 5.14 FAQ(ctn) has a 1/TW modulus.
The next important technical lemma relates to the syntactic structure of the function algebra (i.e. the construction trees, recall definition 3.5) and is sensitive to the exact definition of the function algebra (i.e. other function algebras which yield the same functions in the end, might have the wrong syntactic property).
Lemma 5.15 For every f (x) ∈ FAQ(disctn), there is a construction tree for
f in which D is only applied to variables.
Proof. It suffices to show we can push D into any of the basic functions and past any of the operations. By proposition 5.12, we can easily extend a number of functions from N to functions in FAQ(ctn), only caring how these functions behave on N. In particular we have extensions of the gcd function, and a division function [·/·♩ which defines x/0 = 0. Also recall the functions δ and sgn in FAQ(ctn) (definition 4.8). We have N, because N(x) = |x|D(x) (the absolute value is in FAQ(ctn)). A more significant function is the full,
discontinuous “sign function” on Q (as opposed to the continuous function

sgn), sign(x)= , 0, if x ≤ 0;
, 1, if x > 0.

We can define this in FA

Q(disctn) in such a way

that D is only applied to variables, as follows: sign(x) = sgn(θ1(D(x))). We show how to push past the basic functions:
D(θ1(x)) = θ1(D(x))

D(div(x)) = , x, if x ≥ 1;
, 1, if x < 1.

=1 + θ1(x − 1)

​

D(xy)=		D(x)D(y)	
gcd(D(x)D(y), N(x)N(y))

We can check this by letting x = p/q and y = a/b and checking the equation: D((p/q)(a/b)) = D(pa/bq)= bq/gcd(bq, pa). Furthermore, the equation works for either x or y equals 0 (recall the definition of [·/·♩ mentioned above), and the sign matches (i.e. the sign of D(xy) is the same as the sign of D(x)D(y) and we will assume that gcd is defined so that it is always non-negative).
For + we have cases on whether or not x or y is 0, and use a function
s(x; y), built up from sign, to make the sign correct:

s(x;y)D(x)D(y) gcd(D(x)D(y),N(x)D(y)+N(y)D(x))
 , if x, y /= 0;

D(x + y)= ⎨⎪ D(x),	if y = 0;

Note that branching on the 3 cases can be carried out with sign (in fact even sgn could be employed using the fact that D(x)= 0 ⇐⇒ x = 0).
Now consider how we can push into the operation line  . Recalling the def-

[z♩)  [z| f (y; x)). In FAQ(disctn), the products up to [z♩ and [z| can both be
written as legitimate function on their own (this is not the case of FAQ(ctn)). We can push D into multiplication, addition, and [·♩ (the latter uses reasoning
similar to the above functions), so we are just left with the product operation itself. Note that

[z♩
D(

f (y; x)) =
[z♩
y=0
D(f (y; x))
.


y=0

gcd(  [z♩
D(f (y; x)),  [z♩
N(f (y; x)))

The case for summation is similar to products.	 
Another important step will be to consider two kinds of variables in func- tions, those whose values have an effect on the function value (dependent) and those that don’t (independent). For example (1−1)x+ y has x as independent and y as dependent.
Definition 5.16 Suppose f (x; y) is a function with all its variables displayed. We say that x are independent relative to f if for any assignment to the variables y in the universe of f, the value of f is ﬁxed (i.e. all assignments to x in the universe give the same value once y is ﬁxed).
As defined, it is different to say that each of the variables of x are indepen- dent, versus saying that the entire set x is independent. However, due to the following proposition, we need not worry about this distinction.

Proposition 5.17 If both x and y are independent relative to f (x; y; z), then
x ∪ y are independent relative to f (x; y; z).
Lemma 5.18 FAQ(ctn)= {f ∈ FAQ(disctn) | f has 1/ 7У modulus}
Proof. We prove the two inclusions.
⊆: Immediate by lemma 5.14.
≥: Let f (x) ∈ FAQ(disctn), with all its variables displayed. We now show that either f is discontinuous (so need not be considered) or continuous and in FAQ(ctn). We assume that all the variables of f are dependent (i.e. not independent), since for the independent variables, we can do the following: Fix them in any manner, and then consider where in the construction tree of f they are used; these parts are simply fixed rationals that do not need D. Also, by lemma 5.15, we assume that D is only applied to variables, so we only need to consider the following two cases, depending on whether or not D is applied to one of the the variables in x = x1,... , xk.
Some variables of x have D applied to them: In this case we show f is discontinuous. We will use a technical claim.
Claim  6x ∈ Q 6ϵ > 0 Em ∈ N 6q ≥ m (q prime) Ez ∈ (x — ϵ, x +
ϵ) |D(z)| = q.
The proof of the claim is as follows:
Choose m such that 1/m < ϵ. Then for any q ≥ m there is p ∈ Z such that |p/q — x|≤ 1/q < ϵ. For q prime, |D(p/q)| = q.
We continue with this case. Assume variable x1 has D applied to it, so we can write f as g(D(x1); x2; ... ; xk), for some function g. Now we obtain that for some way of fixing x2,... , xk, g(D(x1); x2; ... ; xk) is discontinu- ous in x1. To show this assume otherwise. Since variable x1 is dependent, there is some way of fixing x2,... , xk, so that we have v1 < v2 ∈ Q satis- fying g(D(v1); x2; ... ; xk) /= g(D(v2); x2; ... ; xk); from now on we leave off x2,... , xk for ease of readability. By the assumption of continuity, we can find u1 /= u2 such that g(D(u1)) /= g(D(u2)), and such that u1 and u2 are either both positive or both negative. By the assumption of continuity, g(D(u)) is continuous at u1 and u2. To be continuous at u1 means that nearby rationals are mapped close to g(D(u1)). By the above claim we can pick nearby rationals with prime denominators in order to obtain that for primes p, as p → ∞, either g(p) → g(D(u1)) or g(—p) → g(D(u1)); the sign depends on whether u1 is positive or negative (recall that the denominator function carries the sign with it). Continuity at u2 requires the same kind of convergence to g(D(u2)) (note that the sign of u2 is the same as u1 so the primes in the converging sequence really have the same sign), which is impossible since g(D(u1)) /= g(D(u2)).

Otherwise: In this case we show f is in FAQ(ctn).
For this case, all occurrences of D must have their variables bound by a sum or product. Sums or products only range over natural numbers, so we can deal with them easily. For all x ∈ N, D(x) = 1, except for D(0) = 0. Thus we simply replace occurrences of D(x) by δ(x). Therefore f is in FAQ(ctn).

The following is a main corollary of the previous development.
Corollary 5.19 apxE(Q) ≈1/TW FAQ(ctn)
Proof. By proposition 5.7, apxE(Q) is equal to disE(Q), restricted to 1/7 У—modulus functions. By lemma 5.13, we can replace disE(Q) by FAQ(disctn). Then we apply lemma 5.18 to get the result.	 
We return to the promised missing proof of lemma 4.18:
lineLR ≤ OP[basicQ; comp; line Σ, line  ].

Proof. We lift the result from the naturals to the rationals and then use the relationship between the continuous and discontinuous versions. First we can easily lift the result on the naturals to the discontinuous class of functions on the rationals, that is, we have:
(*) FA[basicQ, D; comp, lineLR]= FAQ(disctn).
The left side of *, restricted to 1/7 У—modulus is FA[basicQ; comp, lineLR], by a similar argument to that in lemma 5.18. The right side of *, restricted to 1/7 У—modulus functions is FAQ(ctn), by exactly lemma 5.18. The miss- ing technical detail is to extend lemma 5.15 so that D can be pushed past the lineLR operation, and then note that lemma 5.18 works the same with lineLR because it recurses on natural numbers, as do  and . Thus, FA[basicQ; comp, lineLR]= FAQ(ctn), yielding our result.	 
Now we want to show that the real class is strong enough to approximate the rational one. We can show that a number of useful functions are this class (most of the following is shown in [3]).
Proposition 5.20 ([3]) L contains: ∗, +, sin, cos (on R).
Lemma 5.21 basicQ ≤1/TW L
Proof. Except for θ1 and div, every function in basicQ has an extension of it in basicR and thus is approximated exactly. For θ1 we use LI to approximate it by defining a function φ(x) with slope 0 up to 0 — ϵ and slope 1 after 0 + ϵ;

we switch in a smooth manner between these slopes and make ϵ as small as required with an argument similar to the switching carried out below in the proof of lemma 5.22. For div we use the fact that 1/x can be approximated (for x ≥ 1) because 1−e−tx ∈L (observed in [1]) and we can take t large. For the non-differentiable place, at x = 1, we again switch smoothly between the
different slopes.	 
Lemma 5.22 line  ≤1/TW OP[basicR; comp; LI]
Proof. Assuming we can approximate f (x) (we leave out other variables for ease of exposition), we need to show that we can approximate g(y) =
line  z	f (x).	Recall that using techniques based on continuous time
“clocks”, in [3], with LI, they define a pair of simulating functions y1(τ, t) and y2(τ, t) such that for all n ∈ N, |y1(n, n)—g(n)|≤ exp(—β(n))2n(n+1)βn+1(n), where β is some function in L. It is clear that by choosing a faster growing β in the class, the error |y1(n, n) — g(n)| can be made as small as any demanded accuracy in 1/7 У.
We will define a function close to g with the differential equation z'(t) = s(t), where s(t) will give the approximate slope of g, i.e. for x ∈ [n, n + 1], we want s(x) to be approximately g(n + 1) — g(n). Using the construction in [3] (lemma 4.7), we can – adjusting the initial conditions for the linear
differential equations – define two copies of the simulation functions (y1, y2), we denote by (y3, y4) and (y5, y6), such that y5(t)= y3(t + 1) = y1(t + 2) and y6(t) = y4(t + 1) = y2(t + 2). 3 Hence, Y1 = y6(t) — y4(t) is constant and approximates g(n + 1) — g(n) when t ∈ [n, n + 1] and Y2 = y3(t) — y1(t) is also constant and approximates g(n + 1) — g(n) when t ∈ [n + 1,n + 1].
The idea is to define s such that s(t) switches (in a continuous and even
Ck manner) from Y1 on [n, n + 1 ] to Y2 on [n + 1 ,n + 1] for all n ∈ N. This
2	2
can be done simply by defining s(t)= c(t)Y1(t) — (1 — c(t))Y2(t) where c(t) is
a function in the class that alternates between 1 and 0. More precisely, one can define with sin and θk a function c such that c(t) = σ(M(t)θk(sin 2πt)), where σ in the class is an increasing step function satisfying σ(t)= 0 for t < 0 and σ(t)= 1 for t > 1 (its behavior in between is not important since we can choose M large enough to account for it). Therefore, c(t) grows from 0 to 1 on [n, n + ε] for some ε that depends on M. Then, c(t) is1 on [n + ε, n + 1 — ε],
decreases back to 0 on [n+ 1 —ε, n+ 1] and c(t)= 0 on [n+ 1 , n+1]. Adjusting
2	2	2
M, we can make ε as small as we want.
From the smoothness of Y1 and Y2, one can guarantee that s(t) is going to quickly and smoothly alternate from Y1 and Y2 and that z is going to be an approximation of g, which can be made as tight as required within 1/7 У,

3 To simplify the notation, we drop the argument τ from yi.

because we can choose β and M in 7У (since the class dominates 7У since it can exponentiate and is closed under composition).	 
The following lemma again uses a clock argument, as in the previous proof.

Lemma 5.23 line	≤1/TW OP[basicR; comp; LI]
Thus, the following corollary follows by using lemma 3.10 and the above approximations following the form of the proof in corollary 4.24 (notice that we use lemma 3.13 again).
Corollary 5.24  FAQ(ctn) ≤1/TW L
By putting together previous claims using transitivity , we have a goal we set out for.
Corollary 5.25 apxE(Q) ≈1/TW L
Proof. By corollary 5.24 and corollary 5.19 we have: apxE(Q) ≤1/TW L. From lemma 4.24 and corollary 5.19 we have: L ≤1/TW apxE(Q). Thus we have the claim.	 
By lemma 5.10 and corollary 5.25 we have the theorem.
Theorem 5.26 E(R)= L(1/IL—LIM)
Note that corollary 5.25 is stronger than what is needed, but it is more natural to prove this strengthening. Using it and previous lemmas we could in fact show:
E(R)= L(1/IL—LIM)= L(1/7 У—LIM).

Conclusion
We have introduced two techniques, lifting and the method of approximation, and have applied them to obtain two theorems. An informal claim of this work is that these techniques are general and should be applicable to other com- plexity classes and results. This claim is supported by other work in progress (which is perhaps not so convincing to the reader) and by the character of many of the claims which did not depend on the fact that we were working with the elementary computable functions in this paper. So of course further work is to apply these techniques more broadly. In particular we have work in progress relating to the class #P . Furthermore it seems that it should be relatively straightforward to apply these techniques to the classes stronger than the elementary computable functions, in particular, to the Grzegorczyk hierarchy up to the primitive recursive functions and recursive functions (such

connections have been made in terms of discrete part in [3], and in terms of Computable Analysis in [1] and [2]). More ambitious goals include results of this kind for the weaker complexity classes such as the polynomial time functions.

Acknowledgement
This work was partially supported by Funda¸c˜ao paraa Ciˆencia e a Tecnologia
and EU FEDER POCTI/POCI, namely, via CLC, project ConTComp POCTI
/ MAT / 45978 / 2002, and grant SFRH / BPD / 16936 / 2004.

References
Bournez, O. and E. Hainry, Elementarily computable functions over the real numbers and R- sub-recursive functions, Theoretical Computer Science 348 (2005), pp. 130–147.
Bournez, O. and E. Hainry, Recursive analysis characterized as a class of real recursive functions
(2006), to appear.
Campagnolo, M. L., C. Moore and J. F. Costa, An analog characterization of the Grzegorczyk hierarchy, Journal of Complexity 18 (2002), pp. 977–100.
Grzegorczyk, A., Computable functionals, Fund. Math. 42 (1955), pp. 168–202.
Ko, K.-I., “Complexity Theory of Real Functions,” Birkhau¨ser, 1991.
Moore, C., Recursion theory on the reals and continuous-time computation, Theoretical Computer Science 162 (1996), pp. 23–44.
