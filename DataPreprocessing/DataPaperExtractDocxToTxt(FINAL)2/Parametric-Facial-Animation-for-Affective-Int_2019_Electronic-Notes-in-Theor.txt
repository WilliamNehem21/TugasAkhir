Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 343 (2019) 73–88
www.elsevier.com/locate/entcs

Parametric Facial Animation for Affective Interaction
Workflow for Avatar Retargeting
Juan Sebastian Vargas Molano a,1, Gloria Mercedes D´ıaz b,2
and Wilson J. Sarmiento a,3
a Multimedia Research Group Military Nueva Granada University Bogot´a D.C., Colombia.
b Grupo de Investigaci´on Autom´atica, Electronica y Ciencias Computacionales Instituto Tecnol´ogico Metropolitano
Medell´ın, Colombia.


Abstract
Virtual avatars/characters are essential in the interaction with virtual environments and 3D video games. While these avatars can be controlled by a bot or a human, they need a suitable model that allows emotion representation regardless of what or who is controlling them, since it increases the interaction and immersion. Provide facial expressions to any avatar require an animation process to emulate a particular gesture or emotion before to include it in a virtual environment. This process is specific to every single facial expression required in the avatar. This paper presents an initial approximation to contribute to the solve this problem with a workflow to apply a well-known parameterized face model called Candide. The proposal aims to adapt Candide into any avatar previously modeled through automatic morphological adjustment and texture mapping. This work includes the application of this workflow into six characters licensed under the Creative Commons copyright. We also present examples of facial emotion animation, as well as subjective assessment by five experts in audiovisual and animation production.
Keywords: Animation, Avatar, Candide, Emotion.


Introduction
In a Virtual Environment (VE), the avatars/characters are the main way of user interaction. In a strict definition, an Avatar, also known as embodiment, is a visual representation of an user [11]; while a character, or non-playable character (NPC), is a representation of an artificial agent [27]. In this work, the word avatar is used

1 email:u1201559@unimilitar.edu.co
2 email:gloriadiaz@itm.edu.co
3 email:wilson.sarmiento@unimilitar.edu.co

https://doi.org/10.1016/j.entcs.2019.04.011
1571-0661/© 2019 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

for referring to these two concepts in a general way. The potential of an avatar as an element of interaction is that it becomes a virtual projection of a human in VE. Thus, an ideal avatar must be prepared to support natural interaction and communication, adding human behaviour, including emotional behaviour [3]. But in raw level, an avatar is only a 3D mesh, i.e., a set of vertices correctly linked. Then, the traditional way of representing any behaviour in avatars is through an animation process, which requires that an animator (a specialist in 3D digital arts) builds, by hand, a consistent visual movement that represents such high-level behaviour, for example, an emotion [5].
The usual method to animate an avatar is known as rigging, it is composed of two stages. In the first one, known as rig building, a hierarchical structure of bones (skeleton) with physical constraints is created, and methods of direct and inverse kinematics for these bones are applied [16]. For each bone, it is necessary to perform a process known as weight paint, in which the animator assigns a set of vertices of the avatar geometry for manipulation. In essence, the geometry of the 3D models is painted over, and this paint acts as an influence for the bones to move a set of vertices. Then, bones move the selected vertices depending on the influence area set by the weight paint. Since a bone acts as a parent of these vertices, it moves its assigned geometry and this transformation is broadcasted to all sub-bones on the hierarchical chain [13]. The second stage starts when the rig is completed. The animator may move each bone of the rig freely, as manipulating a doll, since bones move vertices of the mesh. This stage is the animation itself; it is when the animator gives life to the avatar applying correct movements in time, position and velocity [24]. An advantage of this method is that the animation is independent of the rig, in the way that it is possible to get many animations with the same rig [19] or skeleton. Another advantage is that the process of building a body rig of a human avatar is well-known and documented, it is even possible to affirm that there are some de facto standards for it. However, there is not a typical or usual way to create a face rig; each animator defines how it should be done himself.
Another method to animate an avatar is known as blend shapes or morph targets. In this approach, an animator deforms (morphs) the geometry of the mesh itself, vertex by vertex, by hand, from an initial to a final state. The animation runs interpolating intermediate positions of vertexes [23]. The main advantage of this method is that the animator is free to morph the geometry as he/she sees fit without restrictions. In other words, he is not limited by the structure of a rig. For this reason, some animators prefer this method to generate facial animations, though it is more computationally expensive than rigging.
In any case, the animation process is very time consuming, it requires a trained digital 3D artist (animator), it is also necessary to use professional 3D software, and the result is highly dependant on the skills and experience of the animator [21]. Also, if it is necessary to animate more than one avatar, this process gets even more complicated, since every character must be animated independently; therefore, the amount of time it would take to finish the task increases with the number of avatars and emotions required. Besides, these animations, once exported to a game

engine [28], are fixed and non-modifiable. Thus, to modify one specific animation, it is necessary to go back to the professional 3D animation software and ask the animator to redo the animation.
This work aims to tackle this problem by proposing a solution to retargeting any avatar’s face, with an emotion-ready face without animator requirement. The core of the proposal is a workflow that replaces the original face of an avatar with a parametric face that doesn’t need a trained 3D artist to be animated. Facial ani- mations work automatically at runtime and these animations can also be modified or created without the need of an external 3D software. The proposal is based in the Candide model, a parameterized 3D mesh of a face specifically developed for coding of human faces [20,1]. This mask implements a set of Action Units (AU) that are designed to reproduce facial micro-expressions, which correctly coupled may represent emotions in 3D face mesh [20,1].
The rest of this paper is organized as follows, Section 2 presents selected previous works that had contributed to deal with the problem that motivated this work. Section 3 explains the proposed workflow, describing in detail the whole pipeline that must be followed to achieve the expected result. Section 4 shows the implementation results of the proposal in six avatars with different characteristics and geometrical configurations, it includes a subjective evaluation of several experts in the field of digital arts. Finally, section 5 contains the finals remarks, open problems, and perspectives of this work.

Related Work and Background
Candide was introduced initially in 1987 by Rydfalk [20]. An update to this model was proposed in 2001 by Ahlberg [1]. Candide is a parametric computer graphical model of human face that was originally proposed to perform face coding in the mpeg-4 standard. The model was inspired by the work of Ekman, who identified a set of micro facial expressions that are performed by any person as natural expres- sion of some emotions [7,8]. In this way, the Candide Model implements a set of action units (AU), each one aiming to copy a Ekman micro expression. The current model leaves out 10 micro expression be due to restrictions in the polygons resolu- tion of the model[1]. An advantage of Candide model is in its inspiration, because the Ekman work shows that the set of micro expressions is an indicator of human emotions [7,8]. For this reason, a subset of Candide AUs may reproduce a specific emotion.
Additionally, to generate a suitable emotional behavior in an avatar is a desir- able goal of VE. Maybe, Fabri et al. research [10] was the first to introduce the term emotional avatars. They underline how important accurate representation of emo- tion is in virtual environments when nonverbal forms of communication are present. However, Benford the al. had previously indicated the relevance of provide suitable channels of non-verbal and affective communication in an avatar, as a requirement to generate co-presence in a collaborative virtual environment [4].
From these [4,10], and also this work [6,12] conceptual guidelines have emerged

to create some proposals to generate the ability to expel emotions in an avatar. Lukasz Zalewski’s et al. [29] creates a 3D shape based on segments of a photograph, generating both a simple 3D geometry and a texture to be mapped on this geometry, but, this 3D representation is generic and not linked to any avatar whatsoever. Zhiyong Wu et al. [26] create Facial Expressions using MPEG-4 FAP Features in a Three-dimensional Avatar. While it still uses human performance, in the way of a video recording, it is also able to recognize six different expression and show them on a 3D avatar. However, all these expressions were created before hand and as such, are static and non modifiable. Also, they are mapped on top of one avatar only. Thibaut Weise’s et al. [25] show is an interesting approach to facial animation, but requires an actor to provide the facial performance for the character, as well as Ming Ouhyoung’s work [17] which maps the actor’s performance utilizing the depth and feature extraction.
Robin Palmberg, Christopher Peters and Adam Qureshi, explained on their paper [18], published in 2017, that there is indeed a very strong impact on the valence of facial expressions on the perception of each of the individuals that took part on their experiment.
Also, it is crucial to understand why portrayal of emotions are important in virtual avatars. Marc Fabri’s et al. research [9] uses emotionally expressive avatars in order to do therapeutic intervention for people with autism. These people re- sponded more positively to avatars who expressed their emotions in a human way. Laramie D. Taylor et al. also explain on their work [22] that there is a preference for people to respond to messages and questions if there was an avatar associated with said message, and if the avatar showed empathy, people were more likely to give a response. Likewise, on a paper done by Russell Beale and Chris Creed et al. [3], we find big supporting evidence claiming that emotional agents can indeed affect users. They found that the use of emotional avatars as opposed to the unemotional avatars, did indeed enhance perceptions of a given system, they also found that subjects became more involved into a given task when emotional avatars were used. Finally, there have been different methods to achieve to emotional avatars. Ba- sori et al. [2] shows the importance of haptic vibration to support emotional ex- pression of avatar in virtual reality systems, claiming that actual implementations still lacked interactivity and immersiveness. While not really a implementation that used animation per se, they aimed to create a realistic avatar based on haptic tac- tile frequency using hardware. They conclude that they are able to increase the
sensation of emotion by the means of haptic vibration.
On a similar account, Kim et al. [14] proposes a method to create emotional avatars. They use image processing techniques to extract information out of pictures of humans denoting any kind of emotion, and are able to then map that emotion on top of an avatar in a VE, so a more lively avatar was produced. This approach is novel, but requieres the human to properly depict an emotion, as well as an adequate picture that allows for image processing.
On the same subject of signal processing, Hiroki Mori et al. [15] propose a frame- work for generating facial expressions from emotional states in daily conversations.

This framework allows the avatar to show the emotion of a speaker based on the synthesis and signal processing of the person’s speech. While this work is more more focused in speech, the results show that the speech synthesis useful not only for making virtual characters in various applications more expressive, but also for fundamental psychological studies related to face, speech, and emotion.
Understanding this, we know that emotional avatars are indeed helpful to en- hance the experience of users interacting with a given environment, and it is also clear that the state of the art solutions have indeed been able to solve the problem of facial animation itself, but are often cost intensive or require a trained actor to do the facial animation for the avatar. Also, these solutions are often static and can be applied only to a select amount of avatars that fit very specific characteristics in their design.
This work aims to make a facial animation system that can be applied to any kind of humanoid avatar and in multiple ones at once, also being able to automat- ically replace the original’s face, which allows the creation of facial animation and expressions without the need of an actor or a video recording, and also one that is able to be modifiable on the fly by the addition or alteration of parameters, not needing an external 3D package to do so.

Fig. 1. Proposed workflow to avatar retargeting, to enable it to emotional facial animation. Five steps compose up the proposal, i.e., 1) avatar verification, 2) avatar rebake, 3) morphological fitting, 4) face swapping and 5) emotional animation. See that the input is a standard avatar, and output is an avatar with emotion facial expressions capabilities.


Method
As mentioned above, this work shows the first version of a workflow to provide abilities of emotional facial expressions to a previously designed avatar. Figure 1 depicts in a flowchart the sequence of steps that make up the proposed workflow, where the input is a standard avatar (without facial expression capabilities), and the output is an avatar ready to convey emotions through facial expressions. The first step is to verify some mandatory criteria on the avatar. The second step is

conditional, in some cases if the avatar doesn’t meet all verification criteria it is possible to carry out a process of avatar texture rebake. The next step (third) is to do a morphological fitting between Candide model and original face of the avatar. In the fourth step, the avatar face is swapped by Candide face model. Then, the avatar is ready, and the VE may play any emotional face animation by the avatar. The following subsections describe in detail each step of the proposed workflow.

Avatar Veriﬁcation
The starting point of the process is getting an avatar and verify if it is or not it is suitable for further treatment. After having obtained an avatar, an acceptance checklist must be taken into account in order to establish whether or not the avatar must be worked on. We call this checklist: Avatar Veriﬁcation Checklist, which is step 1 in Figure 1. Decision trees must then be followed depending if the character checks certain parameters in the list. Different actions must be taken depending on what properties the avatar checks. This Avatar Veriﬁcation Checklist includes:
The avatar has a humanoid face.
The avatar has a clean 3D mesh face.
The avatar has a suitable textures map.
The avatar has more than one texture map.
The following three items will explain in detail what must be done when items in the checklist are not satisfied.

The avatar has a humanoid face
This criterion is related to face anatomy of the avatar; it is necessary for the avatar’s face to have human anatomy. This criterion seems trivial (two eyes, one mouth, one nose, chin, front, cheeks), however in some cases by artistic concepts, an avatar may have a human aspect, but some anatomy features are not human, e.g., big and circular eyes, nose absence, spherical nose, among others, as well as anthropomor- phic animals or things. Figure 2 shows some examples of avatars that don’t satisfy this criterion. If this criterion is not satisfied, it is not possible to work with this 3D avatar, and the process must be aborted. In other words, it is a condition not rectifiable.

Fig. 2. Example of Avatars that don’t work with out model. Generally speaking, human avatars, as long as they don’t have exaggerated features, are the only ones that work.

The avatar has a clean 3D mesh face
Avatar retargeting process works over the part of the mesh that corresponds to the face, for this reason, it is necessary to verify that avatar face/head accessories, e.g., glasses, piercings or hats, of the avatar are independent 3D meshes that not will obstruct the process. If the avatar face is not a clean 3D mesh (without accessories), it is not possible to continue the process.

The avatar has a suitable texture map
Generally, a 3D artist allows a professional 3D software to generate the textures map automatically (known in the lingo of production of 3D content as UV map) that will be used in the avatar while rendered. The problem with this practice is that the resulted texture map is incomprehensible, this fact makes it difficult, in some cases impossible, to do any post-processing on the image texture, and the proposed workflow for avatar retargeting is basically a post-processing method. For this reason, it is necessary to verify that the avatar’s face is completely laid flat and without obstruction over the textures map. In Figure 3 we have a clear example where you can see how an avatar’s texture needs rebaking, since it is incomprehensible.



Fig. 3. An avatar with its UV map that was made by the original artist. This map is incomprehensible and needs to be rebaked.


The avatar has only one texture map
A 3D artist often packs an avatar with more than one textures map to maintain resolution, but this can be circumvented if the textures map are packed on one image of high resolution that kept desired quality. In order to remove the hassle of working with multiple texture images for the character, it is necessary to have an avatar that only has one texture image assigned. In the case of these happen also is necessary a rebake process to get a unique textures map.

Avatar Textures Map Rebake
In the case that the avatar has textures map problems previously identified (Step 1), it is possible to emend this failure with a texture map rebake process. For the avatar retargeting workflow to work correctly, an appropriate texture map must be visually comprehensible, so that the avatar face is completely laid flat and without obstruction over it, and it is also necessary that textures map is over a unique image. If the texture is not in this way, we suggest an textures map cutting method that is displayed in the results section of our paper. This can be done in any 3D package to prepare the avatar, and seams must be applied to the mesh where the green lines are presented in Figure 6, in order to have a correct textures map layout for the avatar.

Morphological Fitting
In order to fit the mask to the size of the avatar’s face, it is necessary to take some measurements on both the Candide mask and the avatar’s face in order to match scales in the X, Y and Z axis. Figure 4 explains what measurements have to be made.

(a)	(b)	(c)	(d)	(e)	(f)
Fig. 4. Side view of the Candide mask (a) (b), with 5 of the measurements that must be made. A = Forehead length. B = NoseTip length, C = Top of head to center of eyes. D = Center of eyes to bottom of nose. E = Middle of mouth to bottom of head. The front view (c) has the other 3 measurements that must be made. F = Forehead Width. H = Eye Separation distance. G =Jaw Width. (d) (e) and (f) depict the very same measurements but on the Avatar of choice.

After having made the measurements for both, an scale factor can be created for the Candide mask in order to scale the mask properly with
ScaleFactor = MeasurementOf Avatar/MeasurementOf Candide
and it must be applied on X, Y and Z respectively. Thus, the Candide mask can be scaled on all three axes with the scale factor to match the Avatar’s face, so that width, height and depth of both faces are the same.

Face swapping
In the engine of choice, it is necessary to select the vertices that correspond to the avatar’s face and push them back, to make room for the Mask. Figure 5 depicts this step. After, the Mask’s nose must be placed where the avatar’s nose used to

be. Due the measurements taken in the step before, the mask will fit as the new face of the avatar.

(b)
Fig. 5. Side view of the avatar’s face being replaced. Note how in (a) the vertices that correspond to the face are pulled back, in order to make room for the candide masks in (b).

In order to texture it, in the engine, the textures map of Candide must be ma- nipulated to place its textures map on top of the face of the character in the image. Translation, rotation, and scaling should be applied in textures map coordinates in order to achieve the effect. Once the textures map of Candide are on top of the face, the mask will resemble the original avatar’s face. On Figure 7, (c) shows exactly how the Candide textures map should be positioned on top of the Avatar’s Texture.

Emotional Animation
The parametric animation system utilizes the following parametric equation 1,
−→	−→	−→
L = (1 − t) ∗ A + t ∗ B	(1)
−→	−→

where
A and B are 3D vectors that describe the initial coordinate and desti-

nation coordinate for each vertex of the face, respectively. With this, we are able to move a vertex through 3D space from and to where it is necessary, by modifying the parameter t. Expressions are then generated by moving different groups of ver- tices at the same time to different positions. These instructions can be read from an XML file and as such, adding or modifying expressions on the fly is possible, since the XML file can contain which vertices need to be moved to create a facial expression, as well as where they need to be moved to in 3D space. Simply adding or modifying this XML file will make animations modifiable.
Results
The implementation of our method was realized in the Unity game engine, with six avatars with different characteristics, geometrical configurations and texture mapping. These characters were acquired from the website Blendswap.com , which is a free 3D model repository that operates under the Creative Commons licence. The textures map modification developed the specialized 3D software. Figure 6, Figure 7, Figure 8, and Figure 9 show the results of the implementation, and also depict the results of correct texture mapping by our suggested method, as well as

the correct placement of the seams in a humanoid avatar. The seams must be placed where the green lines are in Figure 6.


Fig. 6. Simple Avatar mapped with a color grid texture. (a) Represents the completed texture map after having applied the suggested method. Note how the face (d) in the image is non obstructed. (b) and (c) depict how to properly create the seams of the character, showing that such seams must be placed where the green lines are in (b) and (c).













(c)

Fig. 7. Results of our implementation. (a) shows an avatar before treatment. (b) shows the comparison between the original avatar textures map and the created by the proposed method. (c) depicts the textures map mapping that was done on the Candide mask.

Depiction of textures map treatment is visible on Figure 7. An avatar’s UV map with the Candide mask is also shown as well, with the Candide textures map on top of the face. Candide’s textures map have been transformed inside the unity engine to properly translate and scale the textures map in place.


Fig. 8. Results of the Candide Face Replacement with parametric animation. The emotions of Happiness, Sadness and Anger are represented. No professional animator was present to develop the animations and the system was tested with a range of different avatars. Note how, independently of the avatar, it is possible to generate facial expressions with our system. Depicted here we have each emotion with a parameter t of 0.33, 0.66 and 1 respectively.

Additionally, the Candide mask is also fully animated. The Figure 8 also rep- resents the emotions of Happiness, Sadness and Anger for each avatar with the Candide mask on, as well a just the Candide mask rendered by wireframes, depict- ing how it really looks like.
Six Avatars where used to test our workflow on. Figure 9 shows the comparison on these avatars between their original faces and their faces replaced with Candide. To further verify our results, we asked six experts to evaluate our Candide mask replacement, generating a questionnaire for them to solve so that feedback from professionals in the area is accounted for, and we are able to quantify how much difference our method makes to the avatar’s face. Each professional saw a compar- ison between the the original avatar’s face and the same avatar with the Candide mask on it, same as in Figure 9 . The questionnaire had six affirmations for each avatar, asking about general differences, polygon differences, texture differences, expression differences and feature differences. These affirmations are graded by a number, where 1 meant strongly disagree and 5 meant strongly agree, depending of what the expert thought. If they wished, there was also an option for them to leave
comments on each category. Table 1 show the issues of questionnaire.
The expert’s opinion was then collected with the help of the questionnaire and the results can be seen in Table 2 and Table 3. There are six different rows denoting the six experts that took the questionnaire, each one of them answering the questions


Table 1
Affirmations that the experts where shown in the questionnaire. Each one of them related to a category. The experts responded with strongly disagree (graded with 1), disagree (graded with 2), borderline agree (graded with 3), agree (graded with 4), and strongly agree (graded with 5).

on the 5 categories. Note that the lower the score, the better a certain category was actually graded.

Fig. 9. Results of the Candide Face Replacement on all six avatars. On each column, we have a rendition of the avatar in front view, side view and 3/4 view.
The results of the questionnaire depict that the texture mapping difference is the best graded, while the general and feature difference is the worst graded. The experts were also able to distinguish some differences in the polygon count as well


Table 2
Results of the questionnaire. The worst graded category was General, while the best graded Category was Texture. In this figure, it is possible to see exactly what each expert thinks of our implementation in each gradeable category.

Table 3
Results of the questionnaire. This table shows the worst grade, best grade and average grade for each category.

as the expression of the faces, claiming that the biggest differences are found on the nose and the chin of the avatars.
As a side note, in very few instances the replies went over borderline agree, and the highest average for all categories also barely surpasses borderline agree, which means the graphical difference that exists between the original characters and our method isn’t significant.
Conclusions and Further Work
In this work, a workflow for generating avatars able to reproduce emotional expres- sions in runtime was proposed. It is based on the retargeting of a predesigned avatar by replacement of its original face with the parametric mask (Candide) , which avoid performing 3D animation processes every time. Candide model was here used because it is inspired by the Ekaman emotional model, which makes possible to generate many facial expressions, from a simple wink to more complex expressions such as happiness, sadness and anger. The proposed workflow was implemented to represent emotional states on facial expressions into 6 available avatars licensed under the Creative Commons Copyright.
One of the most challenging task of the proposed framework was the anatomic

modification of the avatar face, which is produced by facial replacement. To evaluate the effect of these modifications, a group of six digital artists develops a subjective comparison of each original and retargeting avatar. Details about the way in which each avatar was generated were hidden to the experts, in order to avoid any bias. The experts indicated that they observed differences between the two avatars, but in all cases, these variation were considered weak (less than 3 in a scale of 1 to 5), which allows to conclude that the retargeting process generates an avatar very similar to the original.
It was interesting that all digital artists thought that the anatomical face vari- ations of the avatars were due to a change in the number of face polygons and none of them involved a face replacement. This allows also to conclude that the proposed process generates avatars similar to those obtained with a conventional 3D animation process.
Regarding the anatomical characteristics, the experts observed greater differ- ences in the nose and chin, which could due to the sharp-featured face of the Candide mask, which demonstrates that in a further work it is to apply 3D mesh deforma- tion models to the Candide mask to minimize the anatomical differences between the final and the original avatar. Additionally, it is also necessary to automate the Morphological Fitting process, which is being done manually.
Finally, it should be considered that this work used the latest available version of Candide (2001), which was designed to do facial coding and not animation. Even so, the results obtained are promising and therefore the need to propose an update to the Candide model oriented to the animation of 3D avatars is evident.
Acknowledgment
This work was funded by Vicerrector´ıa de Investigaciones of the Universidad Militar Nueva Granada, research project ID INV-ING-2643.

References
Ahlberg, J., CANDIDE-3– An Updated Parameterised Face, Technical report, Image Coding Group Dept. of Electrical Engineering, Linko¨ping University (2001).
URL  http://www.bk.isy.liu.se/publications/LiTH-ISY-R-2326.pdf

Basori, A. H., A. Bade, M. S. Sunar, D. Daman and N. Saari, Haptic Vibration for Emotional Expression of Avatar to Enhance the Realism of Virtual Reality, in: 2009 International Conference on Computer Technology and Development (2009), pp. 416–420.
URL http://ieeexplore.ieee.org/document/5360181/

Beale, R. and C. Creed, Affective interaction: How emotional agents affect users, International Journal of Human Computer Studies 67 (2009), pp. 755–776.
URL https://dx.doi.org/10.1016/j.ijhcs.2009.05.001

Benford, S., J. Bowers, Fahl´e, L. E. N, C. Greenhalgh and D. Snowdon, Embodiments, avatars, clones and agents for multi-user, multi-sensory virtual worlds, Multimedia Systems 5 (1997), pp. 93–104.
URL https://doi.org/10.1007/s005300050045

Chan, C. S. and F. S. Tsai, Computer Animation of Facial Emotions, in: 2010 International Conference on Cyberworlds (2010), pp. 425–429.
URL https://dx.doi.org/10.1109/CW.2010.49


Churchill, E. F. and D. Snowdon, Collaborative virtual environments: An introductory review of issues and systems, Virtual Reality 3 (1998), pp. 3–15.
URL https://doi.org/10.1007/BF01434994

Ekman, P. and W. V. Friesen, Measuring facial movement, Environmental Psychology and Nonverbal Behavior 1 (1976), pp. 56–75.
URL https://doi.org/10.1007/BF01115465

Ekman, P., F. Wallace, W. V. Freisen and S. Ancoli, Facial Signs Of Emotional Experience, Journal of Personality and Social Psychology 6 (1980), pp. 1125–1134.
URL https://doi.org/10.1037/h0077722

Fabri, M., S. Y. A. Elzouki and D. Moore, Emotionally Expressive Avatars for Chatting, Learning and Therapeutic Intervention, in: Human-Computer Interaction. HCI Intelligent Multimodal Interaction Environments, Springer Berlin Heidelberg, Berlin, Heidelberg, 2007 pp. 275–285.
URL  https://dx.doi.org/10.1007/978-3-540-73110-8_29

Fabri, M., D. J. Moore and D. J. Hobbs, The Emotional Avatar: Non-verbal Communication Between Inhabitants of Collaborative Virtual Environments, in: A. Braffort, R. Gherbi, S. Gibet, D. Teil and
J. Richardson, editors, Lecture Notes in Computer Science (including subseries Lecture Notes in
Artificial Intelligence and Lecture Notes in Bioinformatics), LNCS 1739, Springer Berlin / Heidelberg, 1999 pp. 269–273.
URL  https://dx.doi.org/10.1007/3-540-46616-9_24

Gazzard, A., The avatar and the player: Understanding the relationship beyond the screen, Proceedings of the 2009 Conference in Games and Virtual Worlds for Serious Applications, VS-GAMES 2009 (2009),
pp. 190–193.
URL  https://dx.doi.org/10.1109/VS-GAMES.2009.11

Guye-Vuill`eme, A., T. K. Capin, I. S. Pandzic, N. M. Thalmann and D. Thalmann, Nonverbal communication interface for collaborative virtual environments, Virtual Reality 4 (1999), pp. 49–59.
Jin Ok Kim, Bum Ro Lee and Chin Hyun Chung, On the inductive inverse kinematics algorithm for an articulated body, , vol.5 (2002), p. 5.
URL http://ieeexplore.ieee.org/document/1176418/

Kim, M.-S. and K.-S. Hong, An avatar expression method using biological signals of face-images, in: 2016 International Conference on Information and Communication Technology Convergence (ICTC) (2016), pp. 1101–1103.
URL http://ieeexplore.ieee.org/document/7763378/

Mori, H., K. Oshima and M. Nakamura, Generating avatar’s facial expressions from emotional states in daily conversation, in: 2011 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP) (2011), pp. 5720–5723.
URL http://ieeexplore.ieee.org/document/5947659/

Ortiz, A., D. Oyarzun, I. Aizpurua and J. Posada, Three-dimensional whole body of virtual character animation for its behavior in a virtual environment using H-Anim and inverse kinematics, in: Proceedings of Computer Graphics International Conference, CGI (2004), pp. 307–310.
URL https://dx.doi.org/10.1109/CGI.2004.1309226

Ouhyoung, M., H.-S. Lin, Y.-T. Wu, Y.-S. Cheng and D. Seifert, Unconventional approaches for facial animation and tracking, SIGGRAPH Asia 2012 Technical Briefs on - SA ’12 (2012), pp. 1–4.
URL https://dx.doi.org/10.1145/2407746.2407770

Palmberg, R., C. Peters and A. Qureshi, When facial expressions dominate emotion perception in groups of virtual characters, 2017 9th International Conference on Virtual Worlds and Games for Serious Applications, VS-Games 2017 - Proceedings (2017), pp. 157–160.
URL  https://dx.doi.org/10.1109/VS-GAMES.2017.8056588

Pranatio, G. and R. Kosala, A comparative study of skeletal and keyframe animations in a multiplayer online game, Proceedings - 2010 2nd International Conference on Advances in Computing, Control and Telecommunication Technologies, ACT 2010 (2010), pp. 143–145.
URL https://dx.doi.org/10.1109/ACT.2010.54

Rydfalk, M., CANDIDE, a parameterized face, Technical report, Image Coding Group Dept. of Electrical Engineering, Link¨oping University (1987).
Takala, T. M., M. Makarainen and P. Hamalainen, Immersive 3D modeling with Blender and off-the- shelf hardware, in: IEEE Symposium on 3D User Interface 2013, 3DUI 2013 - Proceedings (2013), pp. 191–192.
URL https://dx.doi.org/10.1109/3DUI.2013.6550243


Taylor, L. D., Avatars and Emotional Engagement in Asynchronous Online Communication, Cyberpsychology, Behavior, and Social Networking 14 (2011), pp. 207–212.
URL https://dx.doi.org/10.1089/cyber.2010.0083
Thobie, S.-A., An advanced interpolation for synthetical animation, , 3 (1994), pp. 562–566.
URL https://dx.doi.org/10.1109/ICIP.1994.413743

Togawa, H. and M. Okuda, Position-based keyframe selection for human motion animation, Proceedings of the International Conference on Parallel and Distributed Systems - ICPADS 2 (2005),
pp. 182–185.
URL https://dx.doi.org/10.1109/ICPADS.2005.239

Weise, T., S. Bouaziz, H. Li and M. Pauly, Realtime performance-based facial animation, ACM Transactions on Graphics 30 (2011), p. 1.
URL https://dx.doi.org/10.1145/2010324.1964972

Wu, Z., S. Zhang, L. Cai and H. M. Meng, Real-time Synthesis of Chinese Visual Speech and Facial Expressions using MPEG-4 FAP Features in a Three-dimensional Avatar, in: Ninth International Conference on Spoken Language Processing, Figure 1, 2006, pp. 1802–1805.
YeiBeech Jang, W. K., An exploratory study on avatar-self similarity, mastery experience and self- efficacy in games, in: 2010 The 12th International Conference on Advanced Communication Technology (ICACT) (2010).
URL https://ieeexplore.ieee.org/document/5440353

Ying, G., L. Xuqing, W. Xiuliang, F. Yi and G. Shuxia, Design and realization of 3D character animation engine, Proceedings of 2009 2nd IEEE International Conference on Broadband Network and Multimedia Technology, IEEE IC-BNMT2009 (2009), pp. 524–528.
URL https://dx.doi.org/10.1109/ICBNMT.2009.5347860

Zalewski, L. and S. Gong, Synthesis and recognition of facial expressions in virtual 3D views, Proceedings - Sixth IEEE International Conference on Automatic Face and Gesture Recognition (2004),
pp. 493–498.
URL https://dx.doi.org/10.1109/AFGR.2004.1301581
