Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 288 (2012) 61–74
www.elsevier.com/locate/entcs

Stratified Static Analysis Based on Variable Dependencies
David Monniaux2
CNRS / VERIMAG
Grenoble, France 1
Julien Le Guen2
VERIMAG & STMicroelectronics Grenoble, France 1

Abstract
In static analysis by abstract interpretation, one often uses widening operators in order to enforce conver- gence within finite time to an inductive invariant. Certain widening operators, including the classical one over finite polyhedra, exhibit an unintuitive behavior: analyzing the program over a subset of its variables may lead a more precise result than analyzing the original program! In this article, we present simple workarounds for such behavior.
Keywords: Static analysis, abstract interpretation, polyhedral domain, widening operator.


Introduction
During experiments, we found examples over which classical polyhedral analysis [8], even with alternative widenings [1], would fail to discover some simple program invariants, which could sometimes even be discovered by interval analysis. This would even happen on simple loops, e.g. for( int i=0; i<N; i++), if the loop contained a nested loop not touching i: the analysis would not discover i ≥ 0! It is counter- intuitive that difficulties in analyzing the behavior of the program on other variables should lead to imprecise results for i.
In some of these examples, such as this simple loop, the lost invariants could be easily recovered by syntactic pattern-matching, but such techniques are brittle.

1 VERIMAG  is  a  joint  laboratory  of  CNRS  and  Universit´e  Joseph  Fourier.	Emails:
David.Monniaux@imag.fr and Julien.LeGuen@imag.fr
2 This work was partially supported by ANR project “ASOPT”.

1571-0661 © 2012 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2012.10.008

We therefore searched for techniques inspired by our intuition that poor results on certain variables should not impact variables not depending on them.
Generalities and Notations
We consider the strongest invariant of a loop (or, more generally, of a program), defined as the least fixed point lfp Ψ of a monotone operator Ψ over sets of program states [6]. For instance, in program 2, the strongest invariant of the loop is the least fixed point in (P(Z × Z), ⊆) of the operator
(1)	Ψ(X)= {(1, 0)}∪ {(i + 1,j + i) | (i, j) ∈ X Λ i ≤ 5}
Explicit-state model-checking computes such invariants as explicitly represented
sets of states (that is, for each state there exists some little data structure). Implicit- state model checking uses compact representations of such sets, such as binary decision diagrams, and computes the least solution of Ψ(X) = X by finding the limit of the ascending sequence X0 = ∅, Xn+1 = Ψ(Xn); for systems with at most n states, this limit is reached within at most n iterations. For infinite state systems such as software programs 3 such an approach is infeasible, because (a) the sets of states Xi may be large (or even infinite, if infinite nondeterminism is used) (b) the sequence may not converge within a finite number of iterations.
Abstract interpretation [5,6] solves point (a) by replacing arbitrary sets of states by over-approximations; for instance, a set of points in Zn or Qn may be replaced by an enclosing convex polyhedron [8,11,13]. A given analysis thus restricts itself to a given abstract domain of sets of states; in this article, we focus, as an example, on the domain of polyhedra, but there exist many other abstract domains, for numerical
[15] or non-numerical states. The operator Ψ on concrete states is replaced by an abstract operator Ψ , satisfying a soundness condition Ψ(X ) ⊆ Ψ (X ) for all X . 4
Problem (b), that is, failure for the sequence X	= Ψ (X ) to become station-
ary, remains if the abstract domains contains infinite strictly ascending sequences; 5 this is for instance the case of the domain of convex polyhedra. Some form of con- vergence acceleration is thus needed. Starting with u = ∅, upwards iterations with

widening [5, 6] compute 6
(2)

u	= u o(u  H Ψ (u ))

n+1	n	n	n
x H y is such that x, y ⊆ x H y (in the case of polyhedra, H is generally taken to be the convex hull), and o is a widening operator, such that for all x ⊆ y, y ⊆ xoy
(soundness property), and any sequence of the form u	= u ov , where v  is
n+1	n	n	n
any other sequence, is stationary: after a certain N , it is constant (termination

3 One of the authors once heard the remark that a program without dynamic allocation or recursion was just a finite-state automaton, thus all properties are decidable, including halting. For the purpose of practical analysis, except for very small and simple programs, such state spaces are so large that they should be treated as infinite.
4 Some presentations of abstract interpretation distinguish the abstract element X] from the set of states
γ(X]) that it represents. In this article, we chose not to, in order to simplify notations.
5 Again, for practical purposes, it suffices that there exist exceedingly long finite ascending sequences for analysis to become unfeasible.
6 Following the usage in APRON [14], our definition of uov assumes that u ⊆ v; if this is not the case, use
uo(u H v) instead.

property). Then, Ψ(u  ) ⊆ Ψ (u  ) ⊆ u  o(u  H Ψ (u  )) = u  , thus Ψ(u  ) ⊆ u  ,
N	N	N	N	N	N	N	N

which means that u 
invariant is included.
is an inductive invariant of the program, in which the strongest

Once an inductive invariant u 
is obtained, it may be refined by narrowing iter-

ations, which in practice generally consist in computing Ψ k becomes stationary or k exceeds a preset limit.
(u ) until the sequence

Widening operators have various unpleasant properties. The best known is that they bring imprecision: the result of widening/narrowing iterations may be strictly larger than the least element of the abstract domain that is an inductive invariant, let alone an invariant (in Sec. 5 we shall list some alternative approaches that do not suffer from this inconvenience, at the expense of generality). The contribution of this article is a generic method to reduce some of the imprecision induced by widening.
Motivating Example
Classical polyhedral analysis [8], 7 when applied to Listing 1, discovers that i ≥ 1 Λ i ≤ 5 is an invariant at the head of the loop. Yet, running the same analysis on Listing 2 yields i ≤ 5 but not i ≥ 1.
Listing 1: Loop until 5
int  i =1; 
while  (i <=5)  {
i = i +1; 
}

Listing 2: j = i(i + 1)/2
int  i =1 ,  j =0;	n4
while  (i <=5)  {
j=j+ i ; i = i +1; 
}
This example is not fortuitous: it models how to address consecutive lines of a matrix in lower triangular packed storage mode. In that memory-effective approach, the matrix is stored in memory as a unidimensional array, each line next to the preceding one, and line number i only uses i positions in the array: j is the index of the start of the line in the array.
Program 1 is an abstraction of Program 2: each execution of the latter maps to an execution of the former. Yet, the analysis of the former produces a more precise loop invariant than the analysis of the latter. This is an example of the non- monotonicity of analyzes using widenings, a long-known phenomenon [7, ex. 11]: a more precise abstraction may ultimately lead to less precision in the final analysis result.

7 One may try examples on B. Jeannet’s online Interproc analyzer at http://pop-art.inrialpes.fr/ interproc/interprocweb.cgi


Table 1
Comparison of classic static analysis (upward iterations with widening o followed by descending iterations) and stratified static analysis on Program 2. Classic analysis loses the constraint i ≥ 1 and finds T in 5 iterations. The upper bound i ≤ 5 is found with one narrowing iteration. Stratified analysis on the stratum consisting of variable i first finds 1 ≤ i ≤ 5. Then, it analyzes stratum i, j and intersect with
result of stratum i. A fixed point is found after 4 iterations (u] , last line). The table also shows the polyhedra found after two narrowing iterations. The resulting polyhedron, even without narrowing iterations, is much more precise than the one found by classic analysis.

Analysis of Program 2 with the basic upwards iteration and widening scheme (widening at every iteration) [6], using the standard widening on polyhedra, 8 yields the successive polyhedra
i =1 Λ j =0 
−i + j ≥ −1 Λi ≥ 1: draw a line through the first two reachable states and obtain a polyhedron in (i, j) generated by vertex (1, 0) and ray (1, 1);
−i + j ≥ −1 Λ 7i − 4j ≥ 7: polyhedron in (i, j) generated by vertex (1, 0) and rays (1, 1) and (4, 7).
So far, so good: such polyhedra still imply i ≥ 1. At the next iteration, however, this constraint is lost and one gets the polyhedron −i + j ≥ −1, and finally T, the whole plane. The constraint i ≤ 5 is recovered by one step of downwards iteration. Analysis with the improved widening proposed by Bagnara et al. [1], as implemented in the Parma Polyhedra Library, yields a different iteration sequence, but still reaches T at the end.
If one runs a polyhedral analysis on Program 1, one gets the inductive invariant 1 ≤ i ≤ 5, which is also valid for Program 2. Intersecting this invariant with the output of the widening in the analysis of Program 2 yields a reasonably precise polyhedron (Table 1).

8 The standard widening on polyhedra P1oSP2, in intuitive terms, suppresses from P2 constraints not present in P1. In reality, its correct definition contains subtleties regarding polyhedra of dimension less than the dimension of the space, and the original definition [8] had to be corrected [11]. [1] recalls the corrected definition.

Thus, the basic idea of our method: run preliminary analyzes over abstractions of the program obtained by removing some of the variables, in order to refine the analysis of the complete program. In order to further convey our intuition, let us remark that Prog. 2 is the result of loop fusion over the following program :
for ( i =1;  i <=5;  i ++)  t [ i ] = i ;
for ( i =1;  i <=5;  i ++)  j  +=  t [ i ] ;
Normal forward polyhedral analysis on this program will find good invariants for both loops. In particular, the second loop may not perturb analysis of the first loop. It seems reasonable that the same applies to the code after loop fusion.
The same code could have been the result of the compilation into C of a data-flow program (e.g. Simulink or Lustre) consisting in a ramp generator and an integrator:

Again, it seems natural that the analysis of the integrator should not hamper the analysis of the ramp.

Stratified Analysis
We have investigated two approaches. In stratiﬁed analysis, we successively perform several static analyzes by abstract interpretation, the results from each analysis being used to refine the following ones. In stratiﬁed widening, a single analysis pass is performed, but with a widening improving on and derived from the traditional widening on polyhedra.

Dependency Strata
We consider a set S of subsets of the set of variables V of the program, such that V ∈ S; we order it by inclusion. An immediate predecessor of S ∈ S, denoted by Sj ≺ S, is Sj such that Sj Ç S and there is no Sjj such that Sj Ç Sjj Ç S.
In practice, if we have a relationship v1 → v2 meaning “v1 flows into v2 through some computation” or “v2 depends on v1”, then the elements of S are, in addition to V itself, subsets S of V closed by: if v ∈ S and vj → v, then vj ∈ S. One way to construct such subsets is to compute for each variable v the set S(v) = {vj | vj → v}, and add this set to S unless it is already present. For better efficiency, one computes the strongly connected components of →, and takes S(v) for one v in each component.
Note that → needs not be the semantics dependency relation, which takes into account both data and control dependencies. In intuitive (and imprecise) terms, a variable x is said to be data-dependent on a variable y if x is assigned to by an expression where y appears; a variable x is said to be control-dependent on a variable y if x is assigned in a program branch executed or not executed according to the value of y. Collecting all program elements on which a variable depends, through data or control dependencies, is known as slicing [27]. If → takes into

account all dependencies, then S(v) is the slice of variables on which v depends.
A helpful intuition of our method is that it performs analyzes on program slices of increasing size; but this is somewhat misleading, because we do not make any assumption on → and thus it does not necessarily reflect all dependencies. In par- ticular, ignoring control dependencies, compared conventional slicing, may produce simpler slices, of a more manageable size — X. Rival, when developing the Astr´ee static analyzer, observed that, for many variables, the slice corresponded to approx- imately 80% of the code, thus slicing did not significantly simplify the program [20].


Informal Deﬁnition
Let S be a subset of the variables in program P . We note P|S the program P where all references to variables outside S have been replaced by nondet() nondeterministic choices.

Program P
int  i =1 ,  j =0; 
while  (i <=5)  {
j=j+ i ;
if  ( j % 2 == 0 )  i = i +1; 
}
P|S for S = {i}
int  i =1; 
while  (i <=5)  {
if  ( nondet ( ) )  i = i + 1 ;
}

For any program P , let C(P ) be its collecting semantics: the set of reachable states of P . In order to simplify notations, for S ⊆ Sj, we identify sets of states referring to the variables in S with their completion by all values for variables in Sj \ S. For any S, P|S is a safe abstraction of P : C(P ) ⊆ C(P|S ). More generally, if S ⊆ Sj, C(P|S′ ) ⊆ C(P|S ).
For any program P , let A(P ) be the result of static analysis of P . Correctness of the analysis means C(P ) ⊆ A(P ). Let A(P, K) be the result of the static analysis of P where the semantics of P is restricted to states in K: in other words, all states outside of K are removed from the transition relation. For any K ⊇ C(P ), C(P ) ⊆ A(P, K).
For each S ∈ S, we compute the intermediate analysis result R(S) after all
R(Sj), Sj ≺ S, have been computed, as follows:


(3)

R(S)= A	P|S,
S′≺S
R(Sj)!

Remark that in this formula, we could have made Sj to range over all predecessors without changing the result; however, this would have been less efficient.
By induction on the length of the ≺-chains, for all S, R(S) ⊇ C(P|S ). At the end, R(V) ⊇ C(P ) is a correct analysis result for the whole program; in fact, any R(S) ⊇ C(P ), so one can stop the analysis at any step, for instance because of a time limit.
This is the analysis performed in §1.2, with S = {{i}, {i, j }}.

Formal Deﬁnitions and Variants
Let S ∈ S. We assume that the result R(Sj) of the analysis for all Sj ≺ S has already been computed. Let K = S′≺S R(Sj); we assume that lfp Ψ ⊆ R(Sj) for all Sj ≺ S and thus that lfp Ψ ⊆ K .
The analysis described at Eqn. 3 is defined by the sequence:
u	= u o(u  H (Ψ (u  ∩ K ) ∩ K ))
n+1	n	n	n

We compute the limit R(S)= u 
of that stationary sequence, and output u 
∩K .

Let us note Ψ|A(X)= Ψ(X ∩ A) ∩ A. In other words, Ψ|A is Ψ with everything outside of A being discarded. The following lemma means that we do not change the strongest invariant by throwing out unreachable states in the definition of the semantics, which is intuitive.
Lemma 2.1 lfp Ψ= lfp Ψ|A for any A ⊇ lfp Ψ.
Proof. lfp Ψ|A is the limit of the ascending sequence defined by X0 = ∅, Xn+1 = Ψ|A(Xn), lfp Ψ that of Y0 = ∅, Yn+1 = Ψ(Yn). By induction, for all n, Xn = Yn. 2

Corollary 2.2 u  , and thus u 
∩ K , includes lfp Ψ, that is, the reachable states.

N	N

Proof Because y ⊆ xoy and y ⊆ x H y for all x, y, Ψ (u 
∩ K ) ∩ K ⊆ u 
and

thus Ψ
] (u )= Ψ(u  ∩ K ) ∩ K ⊆ u . Thus, lfp Ψ ] ⊆ u . The result follows

K	N	N	N	K	N
from the lemma.
We conclude that, by induction over ≺, for all S, lfp Ψ ⊆ R(S).
We shall now describe a subtly different iteration scheme, which supposes some additional properties of o:
Definition 2.3 We say that o satisfies the “up to” termination condition if for
any fixed K , any u  ⊆ K , any sequence v  ⊆ K the sequence defined by u	=

0
(u ov ) ∩ K is stationary if u  ⊆ v 
n
for all n.
n+1

n	n	n	n
This property ensures the correctness of widening “up to” [13], a well-known improvement to widening, and is true of the standard widening on polyhedra as well as Bagnara et al.’s improved widening [1, p. 53]. Using the same notations and hypotheses as above, we use this iteration:
u	= (u o(u  H (Ψ (u ) ∩ K ))) ∩ K 
n+1	n	n	n

Again, once we get a stationary value u 
in this sequence, then it is such that

lfp Ψ ⊆ u  :
Lemma 2.4 If u 


⊆ u 

in Eqn. 5, u 

includes lfp Ψ, the set of reachable states.

Proof Ψ(u  ) ∩ K  ⊆ Ψ (u  ) ∩ K  ⊆ u	⊆ u  , from the correctness of Ψ .
N	N	N +1	N
Furthermore, by construction, u  ⊆ K , thus Ψ(u )∩K =Ψ ] (u ). Ψ ] (u ) ⊆
N	N	|K	N	|K	N
u  , thus lfp Ψ|K] ⊆ u  . The result follows from Lem. 2.1.
N	N

Stratified Widenings
An alternative to the method described in the preceding section, which runs suc- cessive analyzes of increasing precision, is to run a single analysis over a reduced product [5] of polyhedral domains, but with a special widening operator. We shall provide two options for that operator.

Widening with or without Reduction
We distinguish the internal state (PS)S∈E of the iteration sequence from the set of states represented, as in [17]. The various abstract operations will therefore continue operating on polyhedra as usual: only the widening operator is replaced.
Our widening operators will take a tuple (PS)S∈E as a first argument and single polyhedron Q as a second argument. A tuple (PS)S∈E represents the polyhedron
γ ((PS)S∈E )=	PS;
S∈E
the tuples are ordered point-wise, (PS)S∈E ± (QS)S∈E if and only if for all S, (PS) ⊆ (QS).
We note πS(P ) the projection of polyhedron P onto the variables in S. If S ⊆ Sj, a polyhedron on the variables in S shall be also considered as a polyhedron on the variables in Sj by keeping the same constraints. This means, in particular, that P ⊆ πS(P ) for any P and S.
The first widening operator is very simple:
(PS)S∈E o1Q = (PSoπS(Q))S∈E
where o is any widening on polyhedra. This widening converges because each coordinate converges, since o is a widening. It is obvious that, if (PS)S∈E is the resulting limit, then γ ((PS)S∈E ) is an inductive invariant.
The second widening applies internal reductions. (RS)S∈E denotes (PS)S∈E o2 (QS)S∈E . We compute the RS in ascending order with respect to ≺, with the convention that the intersection of zero polyhedra is the full polyhedron:

​
RS = (PSoπS(Q)) ∩
S′≺S
RS′

Theorem 3.1 Assuming that o is a widening satisfying the “up to” termination condition (Def. 2.3), o2 is a widening.
Proof Let u(n+1) = u(n)o2v(n) be a sequence, with u(n) ± v(n) for all n; each element u(n) consists in u(n) for S ∈ S. We prove that for all S ∈ S the sequence
u(n) is stationary, by induction over ≺.
For S with no predecessor, (u(n)) is of the form u(n+1) = u(n)ov(n), and the

S
result follows from o being a widening.
S	S	S

Consider now the property satisfied for all Sj ≺ S. For all Sj ≺ S, (u(n)) is stationary; thus there is a N such that for n ≥ N , all (u(n)) for Sj ≺ S are constant.


S′≺S
u(n) is thus constant for n ≥ N . The results follows from o being a widening

satisfying our additional property.
Instead of polyhedra, one may use other abstract domains fitted with an op- eration H such that a ∩ b ⊆ a H b for all a, b. Let us however note that o1 and o2 yield the same results as the ordinary widening o if applied to domains, such as difference bound matrices or octagons [15] where o and projection commute: πS(P )oπS(Q)= πS(P oQ), and therefore that they bring no improvement for such domains: the PS are just projections of PU . More precisely:
Lemma 3.2 Assume that πS(P )oπS(Q)= πS(P oQ) for all P and Q. Any itera- tion sequence of the form P (n+1) = P (n)oQ(n) then satisﬁes, for all n and S ∈ S, P (n) = πS(P (n)), assuming this equality holds for n = 0.
S	U
Proof Regarding o1: by induction over n, for any S, P (n+1) = P (n)oπS(Q(n))= 
S	S
πS(P (n))oπS(Q)= πS(P (n)oQ)= πS(P (n+1)).
U	U	U
Regarding o2: by induction over n, then by induction over S with respect to
>: (P (n)oπS(Q(n))) ∩  ′	P (n+1) = (πS(P (n))oπS(Q(n))) ∩   ′	πs′ (P	)= 
πS(P (n)oπS(Q(n)))∩  ′	πs′ (P (n+1)) = πS(P (n+1))∩  ′	πs′ (P	) = πS(P	),

Generalized Reduction Leads to Nontermination
Communicating information between several abstract domains used at the same time is sometimes referred to as a closure or reduction operation. Our o2 operation includes a partial closure, with information flowing from a to b if a ≺ b, but not the reverse. One could wonder about applying reductions in all directions. Unfor- tunately, we would lose the termination property of widening, as demonstrated by the following example. 9
Listing 3: Alternating increments
int  i =0 ,  j =0; 
while  ( true )  {
if  ( i <= j )  i ++;  else  j ++;
}
This loop has different behaviors on odd and even iterations: at iteration 2n, i = n and j = n; at iteration 2n + 1, i = n + 1 and j = n. The results of a static analysis with polyhedra on (i, j), and unions instead of widenings, are, in constraint

form: P 
: P Λ i ≤ n and P 
: P Λ j ≤ n, P denoting i ≥ j Λ i ≤ j +1 Λ j ≥ 0

(we identify P with the conjunctions of the constraints that define it). If for the iteration n = 4 we use widening, 10 we instead obtain P = P , which is an inductive
invariant.

9 The fact that widenings followed by reductions with cycles (reduce a using b, then reduce b using a) may not ensure termination is already known. For instance, closure in difference-bound matrices and octagons breaks termination. [15, example 3.7.3, p. 85]
10 Applying unions at n first iterations and then applying widening is a standard technique known as delayed widening.

We have established that this program poses no challenge to “classical” polyhe- dral analysis. The same is true if we apply one of the analyzes of Sec. 2 or one of the widenings of Sec. 3.1. Let us now see what happens if we modify the o2 operator of Sec. 3.1 by allowing reductions not following ≺.
Instead of the definition given at Eq. 8, we instead initialize all RS to PSoπS(Q), then apply some replacements, or reductions, of the form:

(9)
RS := RS ∩
S′/=S
πS(RS′ )

If we reach a fixed point for this replacement system, using the terminology from octagons [15], we say that we have applied the closure operation.
Let us first remark that γ ((RS)S∈E ) is left unchanged any number of such re- ductions:
Lemma 3.3 Let (Rj )S∈E be the same as (RS)S∈E except that
Rj	= RS ∩  ′	πS (RS′ ). Then, γ ((Rj )S∈E )= γ ((RS)S∈E ).

Proof γ ((Rj )S∈E )= 

S∈E
j = γ ((RS)S∈E ) ∩ 

S′/=S0
πS0 (RS′ )= γ ((RS)S∈E ) ∩


S′∈E
πS(RS′ ). Since RS′ ⊆ πS (RS′ ) for any Sj, 

S′∈E
πS(RS′ ) ⊇ 

S′∈E
RS′ =

γ ((Rj )S∈E ). The result follows.
Because γ ((RS)S∈E ) does not change, after the reductions, γ ((RS)S∈E ) is still the same as γ(P oQ). Our new “widening” thus verifies the soundness property (see Sec. 2.3); the problem is that it does not verify the termination property!
Let us have S = {{i}, {j}, {i, j}}; instead of P{i}, P{j} and P{i,j} we shall re- spectively note I , J and P . At iteration n, we shall therefore have a polyhedron I  on {i} (thus, an interval) and one polyhedron J on {j} in addition to the poly-
n	n
hedron P  on {i, j}. If using unions instead of widenings, we have I	= [0, n],

 
2n+1
= [0,n + 1], J 
= [0, n] and J 
= [0, n]. Consider now using widening at

the iteration n = 4. I = I = [0, 2], but J = [0, +∞).
4	3	4
Let us now apply the closure operation: we replace P = P by its intersection
with I and obtain P Λi ≤ 2; then we replace J by its intersection with the updated
4	4
P and obtain [0, 2]. At the next iteration, with the roles of I and J reversed, we
obtain I = [0, 3], J = [0, 2] after closure, and then I = [0, 3], J = [0, 3].
5	5	6	6
The iterations with widening followed by closure behave, on I  and J , like
those with unions — and they do not converge within ﬁnite time. Observe that this happens because we alternatively reduce I → P → J and J → P → I , whereas the definitions of Sec. 3.1 only allow I → P and J → P .
Experimental Results
The stratified analysis presented in section 2, in both variants (Eqn. 4 and Eqn. 5), was evaluated against the classical analysis described by Eqn. 2 on a set of bench- marks used by STMicroelectronics in the development cycle of its compilers, in addition to a few specific examples such as the one from Sec. 1.2.

LAO Kernels is a set of benchmarks internally used for the evaluation of com- pilers code generators and optimizations. It is mainly composed of small computa- tional kernels representative of the target applications of STMicroelectronics (audio and video stream processing, embedded device control), associated with a testing harness to be able to run them on the target processor. It contains 63 functions, of which 49 contain at least one loop. Loops have to exhibit some properties, like a non-linear relation between variables in the loop scope, in order to benefit from this method. Stratified analysis finds a more precise invariant for 5 of these functions.
Among these 5 functions, discrete cosine transform has three nested loops. The intuition of why stratified analysis performs better is it obtains an invariant for the indices affected by the outer loop before attempting to analyze the inner loop, thus preventing imprecisions during the inner loop analysis to affect the invariant on the outer loop indices.
The dependency relation used to create the strata is based on a modified dataflow graph; strongly connected components (SCC) are reduced to super-nodes, while keeping the existing dependency relations. Initial strata stem from the root nodes of this SCC dependency graph, additional ones are created by following the de- pendency relations until one stratum encompasses all variables in the dependency graph. In the while loop of the listing 2, the variable j depends from i; the SCC nodes simply consist of {i} and {j}, and the analysis creates two strata {i} and
{i, j}.
The two variants of stratified analysis described by Eqn. 4 and Eqn. 5 find the same results, and in all cases find invariants equal to or stronger than those obtained by the classical analysis. Bagnara et al.’s alternate widening [1] yields iteration sequences different from those obtained by the classical widening, but ultimately finds the same invariant; thus, our approach improves on theirs on this benchmark set.
Table 2 shows the number of variables in the outermost stratum, along with the number of strata considered by the analysis and its overhead with respect to the standard analysis using only the classic widening. Some programs exhibit a large number of strata, impacting the cost of the analysis. It is possible to run the expensive stratified analysis after a first cheaper standard analysis, while focusing on certain loop nests (those reaching T for instance).


Table 2
Number of variable in the last stratum, number of strata and overhead of stratified analysis for programs that benefit from this method. The baseline for overhead measures is the classic analysis using bare widenings, without delay or widening-up-to).

We rely on the APRON numerical abstract domain library 11 [14] for all ab- stract domain computations. APRON implements, among other domains, convex polyhedra with the classical widening, with linearization of nonlinear expressions following Min´e’s approach [16]. In addition, in order to compare with Bagnara et al.’s alternate widening, we used the Parma Polyhedra Library 12 [2] (with the classical widening, the PPL produces exactly the same results as APRON up to equivalence of constraints, thus providing a means to test for possible bugs in the polyhedral computations).

Related Work
It has long been recognized that analysis using polyhedra over all variables in a program, or even all variables in a single function, is unfeasible because of the high complexity of polyhedral operations in higher dimensions. This is also true of weaker domains such as octagons. For this reason, the Astr´ee analyzer uses relational do- mains only on “packs” of variables [3, 4]: for instance, if we have four variables a, b, c, d and two packs {a, b} and {b, c, d}, the analysis will track relationships be- tween a, b and b, c, d separately: no direct relation will be established between a and d.
A related approach is factoring of polyhedra [12]: when a polyhedron P is a Cartesian product P1 × ... × Pn of polyhedra in lower dimension, with respectively vi vertices (or, more generally, generators), it is often advantageous to keep this product representation as much as possible instead of considering it as a polyhe- dron of i vi vertices, because of algorithms that need to work on the generator representation. An alternative is to dispense totally with the generator representa- tion [22, 23].
The literature on slicing is abundant, since the early 1980s [27]. Syntactic slicing extracts all program statements, variables etc. that affect the value of variable v, or, rather, a safe superset thereof. The resulting slice is executable, which is interesting for testing or debugging methods, but less so for abstract interpretation; this is why we may use lax dependency relations (Sec. 2.1), since we in effect replace any unknown dependency by nondeterministic choice. Semantic slicing relaxes the requirement that the resulting program be a syntactic subset of the original program [26]. X. Rival considers a form of abstract semantic slicing [19, 20], where program executions are restricted to those affecting the reachability of undesirable program states (alarms); in contrast, our method does not suppose we have a set of properties (absence of alarms) to prove.
The design of widening operators is surprisingly difficult. The original widening operator on polyhedra [8] was sensitive to syntax: different ways of representing the same polyhedron in constraint form yielded different widened polyhedra; this problem was later fixed [11]. Because the result of iterations with widening is non-

11 http://apron.cri.ensmp.fr/library/
12 http://www.cs.unipr.it/ppl/

monotonic, precision is highly heuristic: in particular, replacing a widening operator by one producing smaller polyhedra at each iteration does not necessarily translate in a smaller invariant in the end [1, p. 42].
Despite this caveat, many widening operators have been proposed for convex polyhedra [1, p. 30] [22]. Many are variants on the classical widening: some apply union in lieu of the classical widening in a way that does not preclude termination [1]; the “up to” widening, also known as widening with thresholds or limited widening [13], extracts possibly relevant constraints from the program and keeps in P oQ the constraints from that set satisfied by both P and Q; a related idea is widening with landmarks, which uses estimates of the number of supplementary iterations necessary to enable a currently disabled transition [24]; widening with a care set uses a proof goal and counterexamples in order to guide the widening [25]. Our approach is largely orthogonal to these, and in fact can be combined with them.
In the recent years, there has been much interest in techniques for inferring in- variants without doing conventional Kleene iterations. Policy iteration (also called strategy iteration; the technique is inspired by game theory) exists in two flavors. Descending policy iteration [9] solves a descending sequence of least fixed points of simpler operators; these least fixed points may be solved approximately using widenings, thus this technique is orthogonal to ours. In contrast, ascending policy iteration [10] and other techniques based on constraint programming [21] or quan- tifier elimination [18] provide some optimality guarantees, but impose restrictions on the kind of program instructions supported. Such restrictions may be lifted by abstracting program operations into the supported subset [15], which may in turn entail an outer loop with widenings.
We finally note that nothing in our approach is specific to polyhedra, or even to numerical domains.

Conclusion
Following our intuition that failure to analyze well parts of a program should not negatively influence precision on other parts not depending on them, we proposed four analysis schemes: two proceed by analyzes of restrictions of the program code to variable subsets, the other ones use alternative widening operators. Though we focused on improving the classical polyhedral analysis, two of our methods apply to any abstract domain, and the two other ones make a reasonable assumption on the underlying abstract domain and its widening operator.

References
Bagnara, R., P. M. Hill, E. Ricci and E. Zaffanella, Precise widening operators for convex polyhedra, Science of Computer Programming 58 (2005), pp. 28–56.
Bagnara, R., P. M. Hill and E. Zaffanella, The Parma Polyhedra Library: Toward a complete set of numerical abstractions for the analysis and verification of hardware and software systems, Science of Computer Programming 72 (2008), pp. 3–21.


Blanchet, B., P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min´e, D. Monniaux and X. Rival, Design and implementation of a special-purpose static program analyzer for safety-critical real-time embedded software, in: T. Æ. Mogensen, D. A. Schmidt and I. H. Sudborough, editors, The Essence of Computation: Complexity, Analysis, Transformation, number 2566 in LNCS, Springer, 2002 pp. 85–108.
Blanchet, B., P. Cousot, R. Cousot, J. Feret, L. Mauborgne, A. Min´e, D. Monniaux and X. Rival, A static analyzer for large safety-critical software, in: PLDI, ACM, 2003, pp. 196–207.
Cortesi, A. and M. Zanioli, Widening and narrowing operators for abstract interpretation, Computer Languages, Systems & Structures 37 (2011), pp. 24–42.
Cousot, P. and R. Cousot, Abstract interpretation frameworks, J. of Logic and Computation (1992),
pp. 511–547.
Cousot, P. and R. Cousot, Comparing the Galois connection and widening/narrowing approaches to abstract interpretation, in: PLILP, LNCS 631 (1992), pp. 269–295.
Cousot, P. and N. Halbwachs, Automatic discovery of linear restraints among variables of a program, in: Principles of Programming Languages (POPL) (1978), pp. 84–96.
Gaubert, S., E. Goubault, A. Taly and S. Zennou, Static analysis by policy iteration on relational domains, in: R. de Nicola, editor, Programming Languages and Systems (ESOP), LNCS 4421 (2007),
pp. 237–252.
Gawlitza, T. and H. Seidl, Precise fixpoint computation through strategy iteration, in: R. de Nicola, editor, Programming Languages and Systems (ESOP), LNCS 4421 (2007), pp. 300–315.
Halbwachs, N., “D´etermination automatique de relations lin´eaires v´erifi´ees par les variables d’un programme,” Ph.D. thesis, Universit´e scientifique et m´edicale de Grenoble (1979).
Halbwachs, N., D. Merchat and L. Gonnord, Some ways to reduce the space dimension in polyhedra computations, Formal Methods in System Design 29 (2006), pp. 79–95.
Halbwachs, N., Y.-E. Proy and P. Roumanoff, Verification of real-time systems using linear relation analysis, Formal Methods in System Design 11 (1997), pp. 157–185.
Jeannet, B. and A. Min´e, APRON: A library of numerical abstract domains for static analysis, in:
A. Bouajjani and O. Maler, editors, CAV, LNCS 5643 (2009), pp. 661–667.
Min´e, A., “Domaines num´eriques abstraits faiblement relationnels,” Ph.D. thesis, E´cole polytechnique (2004).
Min´e, A., Symbolic methods to enhance the precision of numerical abstract domains, in: VMCAI, LNCS
3855 (2006), pp. 348–363.
Monniaux, D., A minimalistic look at widening operators, Higher order and symbolic computation 22
(2009), pp. 145–154.
Monniaux, D., Automatic modular abstractions for template numerical constraints, Logical Methods in Computer Science (2010).
Rival, X., “Traces Abstraction in Static Analysis and Program Transformation,” Ph.D. thesis, E´cole polytechnique (2005).
Rival, X., Understanding the origin of alarms in Astr´ee, in: C. Hankin and I. Siveroni, editors, SAS, LNCS 3672 (2005), pp. 303–319.
Sankaranarayanan, S., “Mathematical Analysis of Programs,” Ph.D. thesis, Stanford University (2005).
Simon, A. and L. Chen, Simple and precise widenings for H-polyhedra, in: K. Ueda, editor, APLAS, LNCS 6461 (2010), pp. 139–155.
Simon, A. and A. King, Exploiting sparsity in polyhedral analysis, in: C. Hankin and I. Siveroni, editors,
SAS, LNCS 3672 (2005), pp. 336–351.
Simon, A. and A. King, Widening polyhedra with landmarks, in: APLAS (Programming languages and systems), LNCS 4279 (2006), pp. 166–182.
Wang, C., Z. Yang, A. Gupta and F. Ivan¸ci´ı, Using counterexamples for improving the precision of reachability computation with polyhedra, in: CAV (Computer aided verification), LNCS 4590 (2007),
pp. 352–365.
Ward, M. and H. Zedan, Slicing as a program transformation, ACM Trans. Program. Lang. Syst. 29
(2007).
Weiser, M., Program slicing, IEEE Trans. Software Eng. 10 (1984), pp. 352–357.
