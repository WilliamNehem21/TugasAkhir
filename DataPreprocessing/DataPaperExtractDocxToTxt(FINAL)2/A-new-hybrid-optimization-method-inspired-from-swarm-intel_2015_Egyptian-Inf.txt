
FULL-LENGTH ARTICLE
A new hybrid optimization method inspired from swarm intelligence: Fuzzy adaptive swallow swarm optimization algorithm (FASSO)

Mehdi Neshat a,*, Ghodrat Sepidname b,1

a Department of Computer Science, College of Software Engineering, Shirvan Branch, Islamic Azad University, Shirvan, No. 6,
Amam Khomini 78, Mashhad, Iran
b Department of Computer Science, College of Hardware Engineering, Shirvan Branch, Islamic Azad University, Shirvan, No. 67, Kooh Sangi 34, Mashhad, Iran

Received 26 May 2014; revised 5 April 2015; accepted 25 July 2015
Available online 21 November 2015

Abstract In this article, the objective was to present effective and optimal strategies aimed at improving the Swallow Swarm Optimization (SSO) method. The SSO is one of the best optimiza- tion methods based on swarm intelligence which is inspired by the intelligent behaviors of swallows. It has been able to offer a relatively strong method for solving optimization problems. However, despite its many advantages, the SSO suffers from two shortcomings. Firstly, particles movement speed is not controlled satisfactorily during the search due to the lack of an inertia weight. Secondly, the variables of the acceleration coefficient are not able to strike a balance between the local and the global searches because they are not sufficiently flexible in complex environments. Therefore, the SSO algorithm does not provide adequate results when it searches in functions such as the Step or Quadric function. Hence, the fuzzy adaptive Swallow Swarm Optimization (FASSO) method was introduced to deal with these problems. Meanwhile, results enjoy high accuracy which are obtained by using an adaptive inertia weight and through combining two fuzzy logic systems to accurately calculate the acceleration coefficients. High speed of convergence, avoidance from falling into local extremum, and high level of error tolerance are the advantages of proposed method.


* Corresponding  author.  Tel.:  +98  5118533114;  fax:  +98
5118526851.
E-mail addresses: neshat@ieee.org (M. Neshat), Sepidname@iau- shirvan.ac.ir (G. Sepidname).
1 Tel.:+98 5856243901; fax:+98 5856243902.
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
http://dx.doi.org/10.1016/j.eij.2015.07.003
1110-8665 © 2015 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

340	M. Neshat, G. Sepidname

The FASSO was compared with eleven of the best PSO methods and SSO in 18 benchmark func- tions. Finally, significant results were obtained.
© 2015 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).



Introduction

In the engineering world, one of the most significant issues is optimization and designing optimum systems. It is crystal clear that one of the best solutions for dealing with the problem is Swarm Optimization. These methods are inspired from some specific animals which live together as a colony or group. Swarm Intelligence (SI) algorithms have been ordinarily com- prised of an uncomplicated agent’s population or particles which interact concerning a specific location with one another and with their environments. The phenomenon is shown natu- ral world regularly, especially biological agents. The systems follow completely simple rules and structures, notwithstanding that there is not centralized supervision structure commanding how sole factor should act, not general, and to a special degree stochastic, reciprocal activities between like agents bring about the impression of ‘‘intelligent” global management, unfamiliar to the single agents. As a result, swarm intelligence is not an obvious explanation and some experts present different defini- tions for instance SI would be a multi-agent environment which has self-organizing behavior that shows striking smart conduct [1]. The recent decade, there are many examples of SI include ant colony optimization [2–4], Artificial Bee Colony (ABC) [37], Termite Colony Optimization (TCO) [38], bird flocking [5–7], animal herding [8–10], bacterial growth and for- aging [11], fish schooling [12–14] and Glowworm Swarm Algo- rithm [15–18], Swallow Swarm Optimization (SSO) [19]. Furthermore, there are some articles about PSO which are included by adaptive inertia weight [43–45] and fuzzy adaptive acceleration and their results are so considerable [39–41].
In this paper, it is presented a novel algorithm called Fuzzy Adaptive Swallow Swarm Optimization (FASSO) for the simultaneous computation of multimodal functions. This new method assesses and evaluates Swallow Swarm Optimiza- tion [19] (SSO) with realizing its weak points, trying to imple-
research which shows efficiency of SSO is a combination of PSO with it [42].

Swallow Swarm Optimization (SSO)

The SSO algorithm inspired by the collective movement of swal- lows and the interaction between flock members has attained good results. This algorithm has presented a metaheuristic method based on the special properties of swallows, including fast flight, hunting skills, and intelligent social relations. At a glance, the algorithm is similar to PSO but it has unique charac- teristics which cannot be found in similar algorithms, including the use of three types of particles: Explorer Particles (ei), Aim- less Particles (oi), and Leader Particles (li), each of which has certain responsibilities in the group. The ei particles are respon- sible for searching the problem space. They perform this search- ing behavior under the influence of a number of parameters [19]:

Position of the local leader.
Position of the global leader.
The best individual experience along the path.
The previous path.

The particles use the following equations for searching and continuing the path:
VHLi+1 = VHLi + aHLrand()(ebest — ei)+ bHLrand()(HLi — ei)  (1)
Eq. (1) shows the velocity vector variable in the path of the global leader.
aHL = {if (ei = 0||ebest = 0)— — > 1.5}	(2)
Eqs. (2) and (3) calculate the acceleration coefficient vari- able (aHL) which directly impacts individual experiences of each particle.

ment the new strategy and improve it. Swallows have high swarm intelligence; in addition, their flying speed is high, so they are able to fly long distances in order to immigrate from one point to another and also they fly in great colonies. Flying collectively they misguide hunters in perilous positions. Swal- low swarm life has many special features which are more con-
if (e < e	)&&(e < HL ) → rand().ei
aHL = <if (ei < ebest)&&(ei > HLi) → 2rand()·ebest
: if (ei > ebest)	→   ebest	
ei, ebest–0
ei–0

(3)

fusing and bewildering in contrast with fish schools and ant colonies. Therefore, it has been appointed as the subject of research and algorithm simulation.
bHL = {if (ei = 0||ebest = 0)—— > 1.5}	(4)
8> if (ei < ebest)&&(ei < HLi) → rand()·ei	ei, HLi–0



tion is about benchmark function and finally, the fifth section is about experimental results examining the diverse kinds of PSO and then comparing them to SSO and proposed method.
>: if (ei > ebest)	→	HLi

(5)

Methods

The Swallow swarm algorithm is a new swarm optimization method which was introduced by Neshat et al [19]. Another
Eqs. (4) and (5) calculate the acceleration coefficient vari- able (bHL) which directly impacts the collective experiences of each particle. In fact, these two acceleration coefficients are quantified considering the position of each particle in rela- tion to the best individual experience and the global leader [19].

Fuzzy adaptive swallow swarm optimization algorithm	341
The oi particles have a completely random behavior in the environment and move through the space without attaining a specific purpose and share the results with other flock mem- bers. As a matter of fact, these particles increase the chance of finding the particles which have not been explored by the ei particles. Also, if other particles get stuck in an optimum local point, there is hope that these particles save them. These particles use the following Eq. (6) for random movements [19]:
"	rand(min, max)#
In SSO algorithm there are two types of leaders: the local leader and the global leader. The particles are divided into groups. The particles in each group are mostly similar. Then, the best particle in each group is selected and is called the local leader. Next, the best particle among the local leaders is chosen and is called the global leader. The particles change their direc- tion and converge according to the position of these particles.

Flowchart of SSO

Programming of the SSO algorithm is some complex, so its flowchart can improve a deeper understanding of its opera- tions. In this research, MATLAB software is used for above simulation. We can see clearly the flowchart of SSO algorithm in Fig. 1.

Proposal method

The SSO algorithm enjoys many advantages compared to the PSO and fish swarm optimization (FSO), but it has disadvan- tages as well, which are observed when it is tested and evalu- ated in various environments. One of its main problems is related to controlling the speed of the particles and their con- vergence so that sometimes particles pass by an optimum point due to their high speed and do not meet it. This problem causes the occurrence of premature convergence. Using an adaptive inertia weight can be a good help solving this problem. Another problem associated with the SSO is the lack of a proper balance between local and global searches which leads to its poor performance in some environments. A combination of fuzzy inference systems was used in the FASSO to reduce the mentioned problems and to obtain greater flexibility in the acceleration coefficients. These points are discussed thor- oughly in the following sections of the article.


Adaptive inertia weight
The movement of particles in the SSO depends on several parameters: the environment related to the problem, the best individual experience of each particle, the position of each par- ticle in local groups, and the study of the best position of each particle among all of the groups. These parameters determine the interaction of each particle with other particles. After find- ing a suitable position, particles tend to converge around it. Here, one of the most important parameters that determine the speed of convergence is the inertia weight.
According to the research carried out in this regard [21–24], the inertia weight plays the main role in determining the effi- ciency of swarm optimization methods and this coefficient








Figure 1	The flowchart of SSO algorithm.


should have particular features. For example, the speed of the particles should decrease with time so that the environment surrounding the problem is searched more accurately (the local

342	M. Neshat, G. Sepidname


search is performed more accurately). Furthermore, perfor- mance will improve if this declining trend is parabola [25]. In the proposed algorithm, an adaptive inertia weight is used that possesses both of the above-mentioned features. Eq. (7) shows this inertia coefficient.
2
—   t 
The h parameter (Eq. (8)) shows the difference between the best individual experience of each particle and its present posi- tion. In fact, the magnitude of this parameter has a significant effect on the local search process. The w parameter (Eq. (9)) indicates the difference between the position of the best leader among all of the groups and the current position of the parti- cle, and based on this difference we can determine how success-

Wt+1
= {(Wt — W
min
)· [(t
max
— t)/t
max
] + Wmin
} · e
(tmax)
(7)
ful the particle has been in its global search and find out whether it has followed a correct path so far. The third param-

Fuzzy acceleration coefficients
Acceleration coefficients have a central role in controlling the establishment of equilibrium between local and global searches. Parameters aHL, aLL illustrate the ‘‘self-cognition” that pulls the particle to its own historical best position, assist-
ing explores local recesses and preserving the variety of the swarm. Parameters bHL, bLL represent the ‘‘social influence” that shoves the swarm to converge to the current globally best region, improving with fast convergence.
The fuzzy system with the w, d and the h inputs was used to calculate bHL and aHL parameters, and these inputs were cal- culated by employing the following equations:
h = ebest — ei	(8)
w = HLi — ei	(9)
d = HLi — LLi	(10)
eter d (Eq. (10)) represents the difference between the best lea- der among the whole population and the local leader, compares the behavior of the current local group with that of all of the particles, and determines the spatial position of this local group. Of course, all three parameters were used, after fuzzy inferences were made, to determine the acceleration coefficients bHL and aHL. Fuzzy membership functions of all these three inputs are shown below:
The fuzzy system with h, d and s inputs was employed to calculate acceleration coefficients bHL and aHL bHL aHL. Two of the three inputs have equations similar to fuzzy system 1, but the s input is calculated using Eq. (11).
s = LLi — ei	(11)
The input parameter s stands for the difference between the position of the local leader and the current position of the par- ticle and expresses the position of the particle among its group (how close it is to the leader of the group and whether the local search should be improved or not).


















Figure 2	Fuzzy membership functions for the input h.


















Figure 3	Fuzzy membership functions for the input w.

Fuzzy adaptive swallow swarm optimization algorithm	343
















Figure 4	Fuzzy membership functions for the input s.


As triangular and trapezoidal membership functions have the simplest calculations for fuzzification, they were selected to use in the proposed method; thus, its time complexity is improved. The domains of these functions depend on the type
must maintain it. However, in the high state the particles must perform a better global search to reach better conditions in the colony. Eq. (13) was used for the fuzzification of the medium state.

of the benchmark function, so the values in the figures are fac-


8> x—0.35
0.35 < x < 0.5




their fuzzification equations are explained in the following sec-
0.9—x
0.4
0.5 < x < 0.9

tions (see Figs. 2–7).
This membership function has the three states of low, med- ium, and high which show differences between the current sit- uation of each particle and its best individual experience. In the low state, the particle has not made good progress with regard to its past situation and must pay closer attention to the global search in order to improve the conditions. The local and the global searches of the particles will be in equilibrium in the medium state. In the high state, the particles must spend more time on the local search and on the search for the local leader.
The input s represents the difference between the current position of the particle and that of a local leader. The fuzzy membership function has the four different states of very low, low, medium, and high. In the very low and low states, there is very little distance between the particle and the local leader and hence, the colony enjoys a good position and should focus more on the global search. Therefore, in the high state, particles should carry out better local searches so that the colony can attain more ideal conditions. Eq. (14) was used for the fuzzification of the medium state.

x—0.1
< 0.2
0.1 < x < 0.3
8 x—0.35
0.35 < x < 0.5

lmedium(h)= 
1	0.3 < x < 0.6
(12)
>< 0.15

>: 0.8—x
0.6 < x < 0.8
lmedium (s)=	1	0.5 < x < 0.6
>
(14)



position of the particle and that of a global leader. The fuzzy membership function has the four different states of very low, low, medium, and high. In the low and very low states, there is a short distance between the particle and the global lea- der and, hence, in general, the colony has a good position and
The d input shows the difference between a global leader and a local one. This difference expresses the general situation of a local colony and indicates whether it is in a good position in relation to a global optimum. This fuzzy membership func- tion has the three different states of low, medium, and high. In


















Figure 5	Fuzzy membership functions for the input d.




344	M. Neshat, G. Sepidname

















Figure 6	Fuzzy membership functions for the output aHL.


















Figure 7	Fuzzy membership functions for the output bHL.


the low state, the local leader has a good strategy, the group is in a suitable position, and particles should continue their local search, so in the high state, the group has not been successful in its search and it is better for the local leader to appear.
Each of the two systems f1 and f2 has two outputs. These outputs are acceleration coefficients aHL, bHL, aLL, bLL. In the sections below, fuzzy membership functions aHL and bHL are presented (Figs. 6 and 7).
Mamdani’s fuzzy inference was used for both fuzzy sys- tems. The set of fuzzy rules for fuzzy system 1 is as follows (see Table 1):
The first fuzzy rule was designed for creating a balance between local and global searches. In this state, chances of par- ticles to find a local or a global optimum are the same. In the both second and third fuzzy rules, particles perform the local search with greater care and rely more on their individual expe- riences. This search may result in not only finding a good local optimum, but also selecting an appropriate local leader. The forth and fifth Fuzzy rules are related to the situation in which the coefficient of the local search declines, the coefficient of the global search rises, and the particles converge around a global optimum. These rules are also known as the convergence rules and it causes a rise to the speed of convergence as well. The sixth and seventh rules help particles to escape from a situation of premature convergence and from a local optimum as well.
The fuzzy rules in the second fuzzy system are also like those in the first fuzzy system; however, there is a great



difference for instance in the second fuzzy system, an attempt is made to find a local optimum point and to converge around that point. As a matter of fact, in this system, particles con- verge around a local leader, and this local leader will converge toward a global leader. Of course, in the SSO algorithm, we never forget the importance of oi, because not only do these particles give the group the chance of finding unknown areas through performing random and irregular movements, but also allow particles to escape from local optimum points.

Pseudo-code FASSO
The algorithm of proposed method is shown for better under- standing and clarifying.
Fuzzy Adaptive Swallow Swarm Optimization Algorithm:

Fuzzy adaptive swallow swarm optimization algorithm	345




































Benchmark functions

To show the performance of the proposed algorithm and to compare it to some different PSOs and FSOs 18 benchmarks have been attended. Each of these functions tests has special conditions and features; thus, feebleness points of the opti- mization methods will be clear.

Experiments

Benchmark functions which have various features were used to evaluate the proposed algorithm all the better and show its weak and strong points. The efficiency of this method (the FASSO) was compared with that of 11 other methods, from PSO to SSO. Table 2 lists the features of all these methods.
This is done to assess the significance of the FASSO among other current methods. To do it, these methods should be tested in same software and hardware. Applied software is MATLAB version 7.0.4 (R14) service pack2. Current hard- ware is Celeron 2.26-GHz CPU, 256-MB memory, and Win- dows XP2 operating system. The number of particles is 20 and the number of iterations is 1000. Table 3 shows the perfor- mance of the methods of Table 2 and the FASSO method.
As can be seen clearly in Table 3, the proposed method per- formed better than SSO in all benchmark functions except for the Shifted Rosenbrock’s Function. The use of an adaptive inertia weight coefficient, together with the two fuzzy control systems  for  better  determination  of  the  acceleration



































coefficients improved the efficiency of the SSO algorithm. Interesting results are observed in the above-mentioned table if it is examined more deeply. Firstly, the FASSO method exhibited more intelligent behaviors than all other methods in seven of the benchmark functions (Rosenbrock, Schwefel’s P2.22, Quadric, Quadric noise, Perm, Rotated Rastrigin, Rotated Griewank) and had a very good performance so that the results obtained from that were more optimal than those of the other methods. Secondly, the proposed method yielded rel- atively good results in five of the benchmark functions (Sphere, Rastrigin, Schwefel, N-Rastrigin and Shift Rastrigin). These results were equal to the best results obtained from using the other methods. Thirdly, in six of the benchmark functions used, the FASSO algorithm could not find the best answers, but the answers found were close to the best ones obtained from using the other methods.
According to Fig. 8, the proposed method enjoyed a better efficiency than the other ones. For example, with respect to the Sphere Function, the APSO was the best method after 100 iter- ations and enjoyed a greater speed of convergence as well. However, after 250 iterations, the proposed method could rapidly follow an optimal path, yield excellent results, and con- verge around the absolute optimal point at the 500th iteration. Concerning the Ackley function, the FASSO could achieve the best results in fewer than 250 iterations and reached better results than the SSO in a close competition; however, it, unfor- tunately, fell into a local optimal point and the particles con- verged around that point. At the 650th iteration, particles could escape from that point with the help of randomly

346	M. Neshat, G. Sepidname






v = 0.729, P ci = 4.1







Fuzzy adaptive swallow swarm optimization algorithm	347






















































Figure 8	Convergence performance of the eleven different PSOs, SSO and FASSO on the 6 test functions. (a) Sphere. (b) Ackley. (c) Griewank. (d) N_Rastrigin. (e) Quadric noise. (f) Quadric.


moving particles and reached more optimal points at a remarkable speed, but this behavior was not consistent and the previous tragedy happened again and the APSO and OLPSO-L performed better than the proposed method. As for the Quadric and N-Rastrigin functions, the proposed method exhibited an intelligent behavior and, through prevent- ing from falling into local points, could successfully find the absolute optimal point.
One of the most complicated functions used for testing optimization methods is the Rotate functions four of which were employed in this research [20]. As shown in Table 3 and in Fig. 9, the proposed method showed good behaviors in the two functions of Rotated Rastrigin and Rotated Grie- wank and achieved the best results. The results it obtained in the other two Rotate functions were close to the best ones achieved.

348	M. Neshat, G. Sepidname






















































Figure 9	Convergence performance of the eleven different PSOs, SSO and FASSO on the 6 test functions. (g) Rastrigin. (h) Rosenbrock.
(k) Schwefel’s P2.22. (i) Schwefel. (m) Rotated Rastrigin. (n) Rotated Griewank.


Conclusion

After investigating and analyzing the SSO, the conclusion was drawn that this method does not behave well in some of the functions. The main reasons for this include the lack of an inertia weight to control the speeds of particles so that particles sometimes pass by the optimal points. Moreover, the accelera- tion coefficients do not reflex appropriately to the environ- ment; in other words, they lack the required flexibility and hence, no suitable balance is established between the local
and global searches. Two suggestions were put forward to solve these shortcomings. The first is to introduce an adaptive inertia weight that decreases parabolically with time. The sec- ond is to use fuzzy control in optimizing the acceleration coef- ficients (and here, the two fuzzy inference systems played the central role). The proposed method had a high speed of con- vergence, could avoid premature convergence around local optimal points, and had substantial flexibility in complex envi- ronments. It cannot be claimed that the FASSO is the best optimization method; however, it is hoped that this method

Fuzzy adaptive swallow swarm optimization algorithm	349


will be improved with the help of other researchers and experts.

References

Beni G, Wang J. Swarm intelligence in cellular robotic systems. In: Proceed. NATO advanced workshop on robots and biological systems, Tuscany, Italy, June 26–30; 1989.
Dorigo M, Maniezzo V, Colorni A. The ant system: optimization by a colony of cooperating agents. IEEE Trans Syst Man Cybern Part B 1996;26(1):29–41.
Dorigo M, Gambardella LM. Ant colony system: a cooperative learning approach to the traveling salesman problem. IEEE Trans Evol Comput 1997;1(1):53–66.
Dorigo M, Stu¨tzle T. Ant colony optimization. Cambridge: MIT Press; 2004.
Kennedy J, Eberhart RC. Particle swarm optimization. In: Proceedings of the IEEE international conference on neural networks. Piscataway: IEEE Press; 1995. p. 42–8.
Clerc M. Particle swarm optimization. London: ISTE Ltd.; 2007.
Poli R, Kennedy J, Blackwell T. Particle swarm optimization: an overview. Swarm Intell 2007;1(1):33–57.
Wang B, Jin XP, Cheng B. Lion pride optimizer: an optimization algorithm inspired by lion pride behavior, Berlin Heidelberg; Science China Press and Springer Verlog.
He S, Wu QH, Saunders JR. A novel group search optimizer inspired by animal behavior ecology. In: Proc. IEEE congr. evol. comput.. Vancouver, BC: Sheraton Vancouver Wall Center; 2006.
p. 1272–8.
He S, Wu QH, Saunders JR. Group search optimizer: an optimization algorithm inspired by animal searching behavior. IEEE Trans Evolutionary Comput 2009;13(5).
Passino KM. Biomimicry of bacterial foraging for distributed optimization and control. IEEE Control Syst Mag 2002:52–67.
Li XL. A new intelligent optimization-artificial fish swarm algorithm. PhD thesis. China: Zhejiang University; 2003.
Jiang MY, Yuan DF. Artificial fish swarm algorithm and its applications. In: Proc. of the international conference on sensing, computing and automation, (ICSCA’2006). Chongqing, China; 8– 11 May 2006, p. 1782–7.
Xiao JM, Zheng XM, Wang XH. A modified artificial fish-swarm algorithm. In: Proc. of the IEEE 6th world congress on intelligent control and automation, (WCICA’2006). Dalian, China; 21–23 June 2006. p. 3456–60.
Krishnanand KN, Ghose D. Detection of multiple source locations using a glowworm metaphor with applications to collective robotics. In: Proceedings of IEEE swarm intelligence symposium. Piscataway: IEEE Press; 2005. p. 84–91.
Krishnanand KN, Ghose D. Glowworm swarm based optimiza- tion algorithm for multimodal functions with collective robotics applications. Multiagent Grid Syst 2006;2(3):209–22.
Krishnanand KN, Ghose D. Theoretical foundations for multiple rendezvous of glowworm inspired mobile agents with variable local-decision domains. In: Proceedings of American control conference. Piscataway: IEEE Press; 2006. p. 3588–93.
Krishnanand KN, Ghose D. Glowworm swarm optimization for simultaneous capture of multiple local optima of multimodal functions. Swarm Intelligence 2009;3:87–124. http://dx.doi.org/ 10.1007/s11721-008-0021-5.
Neshat Mehdi, Sepidnam Ghodrat, Sargolzaei Mehdi. Swallow swarm optimization algorithm: a new method to optimization. Neural Comput Appl 2013;23(2):429–54.
Salomon R. Reevaluating genetic algorithm performance under coordinate rotation of benchmark functions. BioSystems 1996;39:263–78.

Feng CS, Cong S, Feng XY. A new adaptive inertia weight strategy in particle swarm optimization. In: IEEE Congress on evolutionary computation, CEC 2007; 2007. p. 4186–90.
Malik RF, Rahman TA, Hashim SZM, Ngah R. New particle swarm optimizer with sigmoid increasing inertia weight. Int J Comput Sci Security (IJCSS) 2007;1(2):35.
Xin J, Chen G, Hai Y. A particle swarm optimizer with multistage linearly-decreasing inertia weight. Computational sciences and optimization 2009. CSO 2009. International joint conference on, vol. 1. IEEE; 2009. p. 505–8.
Li HR, Gao YL. Particle swarm optimization algorithm with exponent decreasing inertia weight and stochastic mutation. 2009 Second international conference on information and computing science. IEEE; 2009. p. 66–9.
Bansal JC, Singh PK, Saraswat M, Verma A, Jadon SS, Abraham
A. Inertia weight strategies in particle swarm optimization, In: IEEE third world congress on nature and biologically inspired computing (NaBIC); 2011. p. 633–40.
Shi Y, Eberhart RC. A modified particle swarm optimizer. In: Proc. IEEE world congr. comput. intell.; 1998. p. 69–73.
Kennedy J, Mendes R. Population structure and particle swarm performance, In: Proc. IEEE congr. evol. comput.; Honolulu, HI; 2002. p. 1671–6.
Kennedy J, Mendes R. Neighborhood topologies in fully informed and best-of-neighborhood particle swarms. IEEE Trans Syst, Man, Cyber – Part C: Appl Rev 2006;36(4):515–9.
Mendes R, Kennedy J, Neves J. The fully informed particle swarm: simpler, maybe better. IEEE Trans Evolution Comput 2004;8(3):204–10.
Ratnaweera A, Halgamuge S, Watson H. Self-organizing hierar- chical particle swarm optimizer with time-varying acceleration coefficients. IEEE Trans Evolution Comput 2004;8(3):240–55.
Liang JJ, Suganthan PN. Dynamic multi-swarm particle swarm optimizer with local search. In: Proc. IEEE congr. evol. comput.; 2005. p. 522–8.
Liang JJ, Qin AK, Suganthan PN, Baskar S. Comprehensive learning particle swarm optimizer for global optimization of multimodal functions. IEEE Trans Evolution Comput 2006;10 ():281–95.
Ho S-Y, Lin H-S, Liauh W-H, Ho S-J. OPSO: orthogonal particle swarm optimization and its application to task assignment problems. IEEE Trans Syst, Man, Cybernetics – Part A 2008;38 ():288–98.
Zhan Zhi-Hui, Zhang Jun, Li Yun, Hung Chung Henry Shu. Adaptive particle swarm optimization. IEEE Trans Syst, Man, Cyber, Part B: Cyber 2009;39(6):1362–81.
Zhan Zhi-Hui, Zhang Jun, Li Yun, Shi Yu-Hui. Orthogonal learning particle swarm optimization, IEEE Trans Evolution Comput, 99. p. 1. ISSN 1089–778X.
Karaboga D. An idea based on honey bee swarm for numerical optimization. Technical report. Computer engineering depart- ment. Engineering faculty. Erciyes University; 2005.
Hedayatzadeh R, Akhavan Salmassi F, Keshtgari M. Termite colony optimization: a novel approach for optimizing continuous problems. IEEE ICEE 2010.
Nickabadi A, Ebadzadeh MM, Safabakhsh R. A novel particle swarm optimization algorithm with adaptive inertia weight. Appl Soft Comput 2011;11(4):3658–70.
Bansal JC, Singh PK, Saraswat M, Verma A, Jadon SS, Abraham
A. Inertia weight strategies in particle swarm optimization. In: IEEE 2011 third world congress on nature and biologically inspired computing, vol. 640; 2011. p. 633.
Liliana Dewi Yanti, Widyanto M Rahmat. Particle swarm optimization with fuzzy adaptive acceleration for human object detection. Int J Video Image Process Network Sec 2011;11(1):11.

350	M. Neshat, G. Sepidname


Kaveh A, Bakhshpoori T, Afshari E. An efficient hybrid particle swarm and swallow swarm optimization algorithm. Comput Struct 2014:40–59, <http://www.sciencedirect.com/science/arti- cle/pii/S0045794914001564>, alikaveh@iust.ac.ir.
Neshat Mehdi. FAIPSO: fuzzy adaptive informed particle swarm optimization. Neural Comput Appl 2013;23(1):95–116.
Neshat Mehdi, Rezaei Masoud. AIPSO: adaptive informed particle swarm optimization. In: Intelligent systems (IS), 2010 5th IEEE international conference. p. 438–43.
Neshat M, Sargolzaei M, Masoumi A, Najaran A. A new kind of PSO: predator particle swarm optimization. Int J Smart Sens Intell Syst 2012;5:521–39.
