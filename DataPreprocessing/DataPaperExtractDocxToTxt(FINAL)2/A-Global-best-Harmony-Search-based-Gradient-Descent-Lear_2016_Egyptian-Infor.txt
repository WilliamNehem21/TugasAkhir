
FULL-LENGTH ARTICLE
A Global-best Harmony Search based Gradient Descent Learning FLANN (GbHS-GDL-FLANN) for data classification

Bighnaraj Naik *, Janmenjoy Nayak, Himansu Sekhar Behera

Department of Computer Science Engineering & Information Technology, Veer Surendra Sai University of Technology, Burla 768018, Odisha, India

Received 4 December 2014; revised 17 June 2015; accepted 5 September 2015
Available online 23 October 2015

Abstract While dealing with real world data for classification using ANNs, it is often difficult to determine the optimal ANN classification model with fast convergence. Also, it is laborious to adjust the set of weights of ANNs by using appropriate learning algorithm to obtain better classification accuracy. In this paper, a variant of Harmony Search (HS), called Global-best Harmony Search along with Gradient Descent Learning is used with Functional Link Artificial Neural Network (FLANN) for classification task in data mining. The Global-best Harmony Search (GbHS) uses the concepts of Particle Swarm Optimization from Swarm Intelligence to improve the qualities of harmonies. The problem solving strategies of Global-best Harmony Search along with searching capabilities of Gradient Descent Search are used to obtain optimal set of weight for FLANN. The proposed method (GbHS-GDL-FLANN) is implemented in MATLAB and compared with other alternatives (FLANN, GA based FLANN, PSO based FLANN, HS based FLANN, Improved HS based FLANN, Self Adaptive HS based FLANN, MLP, SVM and FSN). The GbHS-GDL-FLANN is tested on benchmark datasets from UCI Machine Learning repository by using 5-fold cross validation technique. The proposed method is analyzed under null-hypothesis by using Friedman Test, Holm and Hochberg Procedure and Post-Hoc ANOVA Statistical Analysis (Tukey Test & Dunnett Test) for statistical analysis and validity of results. Simulation results reveal that the performance of the proposed GbHS-GDL-FLANN is better and statistically significant from other alternatives.
© 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information,
Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.
org/licenses/by-nc-nd/4.0/).




* Corresponding author. Tel.: +91 9439152272.
E-mail addresses: mailtobnaik@gmail.com (B. Naik), mailforjnayak@ gmail.com (J. Nayak), mailtohsbehera@gmail.com (H.S. Behera).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.



Introduction

Data Analysis is an analytical process of examining data to discover useful information and draw conclusions which help in decision making. It integrates diversified techniques under Statistic, Engineering and Science. Since 1990, data are being collected in numerous speed and in large volume in the area

http://dx.doi.org/10.1016/j.eij.2015.09.001
1110-8665 © 2015	Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



of web, business management, e-commerce, remote sensors, microarrays gene expression, scientific simulations, production control and engineering design, transactions, stocks and bioin- formatics, etc. These explosive growths of data collection and the need of automated extraction of novel, valid, unknown and potentially useful information from the data in large databases gave birth to many data analysis methodology, which includes Data Mining and Business Intelligence.
Data mining is the process of identifying novel, understand- able and previously unknown patterns in data which helps in decision making. Most tricky and challenging decision making processes in day to day human life is classification, which helps to make decision from past experience. In data mining, the Classification is defined as a variety of data analysis process that can be used to assign important classes to unknown patterns. Classification task predicts definite class labels and constructs a model based on the training dataset which is used to classify anonymous patterns.
In the recent years, many classification tasks have been proposed in emerging areas of science and engineering which includes document classification [1–3], Sentiment classification [4–7], Fault classification [8–11], Text classification [12–14], Image classification [15–18] and Gene Expression classification and Bio Medical Data classification [19–23] and others [24–30], which have given new shape, motivation and direction to application of the classification task in data mining.
Although a number of traditional classification methods are proposed by many researchers [31–35], first time, Zhang et al. [34] realized that artificial neural network models are alternative to various conventional classification methods which are based on statistics. The artificial neural networks (ANNs) are capable of generating complex mapping between input and the output space; thus, they can form arbitrarily complex nonlinear decision boundaries. Along the way, there are already several artificial neural networks, each utilizing a different form of learning or hybridization. As compared to higher order neural network, the classical neural networks (Example: MLP) are suffering from slow convergence and unable to automatically decide the optimal model for classifi- cation. In the last few years, to overcome the limitations of conventional ANNs, some researchers have focused on higher order neural network (HONN) models [36,37] for better performance.

Literature survey

In this paper, it is an attempt to design higher order neural network model with competitive learning based on new meta-heuristic optimization algorithm for classification of benchmark datasets from the well known machine learning data repository.
Prior to this, a Chebyshev Functional Link Artificial Neural Network model (Chebyshev-FLANN) with Chebyshev polynomial functional expansion for prediction of financial indices is proposed by Patra et al. [38]. The performance of FLANN and chFLANN is found nearly equivalent and train- ing time for FLANN and chFLANN is noticed as almost half of the MLP. Among MLP, FLANN and Chebyshev-FLANN, the chFLANN is found best among these three. Also it is observed that FLANN and chFLANN are efficient and have less complex architecture as compared to MLP.
Misra and Dehuri [39] have proposed a classification method by using FLANN and simulation results show that proposed FLANN model is capable to handle linearly non-separable classes by increasing the dimension of input space through func- tional expansion. The execution time and accuracy of this model is found to be better than the other alternatives.
A hybrid functional link artificial neural network (HFLANN) based on genetic algorithm (GA) for optimal input feature selection by using functionally expanded selected features is proposed by Dehuri et al. [40] which address nonlin- ear nature of classification problems. Through experimental results, the HFLANN is proven to be better in optimal set feature selection as compared to RBFN and FLANN with back propagation learning.
A comprehensive survey on FLANN is made and an efficient PSO based back propagation learning is proposed by Dehuri and Cho [41]. In this paper, the basic concept of FLANN, associated basis functions, learning schemes and development of FLANNs over time are discussed. Also the authors have used PSO based back propagation learning scheme on Chebyshev-FLANN for classification and the pro- posed method is proved to be better as compared to FLANN by testing with benchmark datasets.
An efficient FLANN for stock price prediction of the clos- ing price of US stocks is suggested by Patra et al. [42] and found to be better in performance in terms of more accurate predictions of stock. In this paper, a FLANN with trigonometric functional expansion (Trigonometric-FLANN) is used and shown to be better result as compared to MLP-based prediction model.
A FLANN based prediction model for prediction of causing genes in gene diseases is proposed by Sun et al. [43]. In this study, three classifiers (i.e. MLP, SVM, FLANN) have been implemented and compared. The performance of the FLANN classifier is found to be better over MLP and SVM. For better prediction of the stock market indices, Chakravarty and Das [44] have proposed a Functional Link Neural Fuzzy (FLNF) Model and compared with FLANN based prediction model in terms of root mean square error. The simulation results show that the FLNF performs better over FLANN. Also the authors have addressed the issue of falling in local minima in case of back propagation learning
by employing Particle Swarm Optimization.
A classification method based on FLANN is achieved by Majhi et al. [45] for classification of online Indian customer behavior and the proposed FLANN model found to be superior in classification accuracy than other statistical approach (discriminant analysis). Also authors have suggested to use psychographic and cultural information for further improvement of the proposed method.
An accurate hybrid FLANN classifier (HFLNN) is pro- posed by Dehuri and Cho [46] by selecting an optimal subset of favorable input features. This is achieved by eliminating fea- tures with fewer or no predictive information. The proposed method is found to be better as compared to FLANN and RBFN.
Forecasting of stock exchange rates is achieved with Genetic algorithm (GA) based FLANN model by Nayak et al. [47] and proposed method is compared with MLP, GA based MLP and GA based FLANN models. The authors have claimed that the FLANN-GA is found better in almost all cases.



Bebarta et al. [48] have implemented few variants of FLANN model (Power FLANN, Legendre FLANN, Cheby- shev FLANN and Laguerre FLANN) for forecasting stock price index and performances are measured in terms of stan- dard deviation error, squared error, etc. All the four proposed methods are implemented and found to be simple and efficient to predict the various Indian stock data.
A Bat inspired optimization based FLANN classification method is proposed by Mishra et al. [49]. The method is com- pared with FLANN and hybrid PSO based FLANN classifica- tion method. In this paper, bat algorithm is used to adjust the weights of the FLANN efficiently which results in high accu- racy for classification. The simulation results show that the proposed method outperforms FLANN and hybrid PSO based FLANN classifiers.
Various dimension reduction strategies are projected by Mahapatra et al. [50] for the Chebyshev FLANN classifier and have been used for cancer classification. The basic idea
used in this paper is to perform PCA, FA, DFT and DCT tech- niques to reduce dimension of the data and then Chebyshev FLANN classifier is applied for better classification. It is observed that the combination of DCT feature reduction tech- nique along with Chebyshev FLANN classifiers outperforms other possible alternatives.
Mishra et al. [51] have developed MLP, FLANN and PSO- FLANN classification models for classification of biomedical data. In this paper, to extract important input features, an effi- cient dynamic classifier fusion (DCF) is proposed along with principal component analysis (PCA) scheme. After extraction of optimal input features, LMS classifier is performed along with PSO based Back propagation learning algorithm. Although MLP is a traditional ANN, surprisingly, in this study, PSO based Back propagation learning-MLP is found to be better as compared to FLANN and PSO-FLANN.
An Improved PSO (IPSO) based FLANN classifier (IPSO-FLANN) is proposed by Dehuri et al. [52] and





compared with MLP, support vector machine (SVM), RBFN, FLANN with gradient descent learning and Fuzzy Swarm Net (FSN) model. Initially, IPSO is used to optimize the weight value of Functional link ANN and finally, functionally expanded (using trigonometric basis functions) input patterns are supplied to FLANN for classification. The proposed method is found to be simple and better as compared to MLP, SVM, FLANN with gradient decent learning and FSN. Mili and Hamdi [53] have developed a good number of FLANN based classifier such as PSO based FLANN, GA based FLANN and Differential Evolution (DE) based FLANN for classification task. These classifiers are compared and tested with various expansion functions. In their study, the authors have concluded that the proposed methods are performing better in terms of accuracy and convergence as
compared to traditional FLANN.
An efficient classification method based on FLANN and a hybrid learning scheme based on PSO and GA have been proposed by Naik et al. [54] and it is found to be relatively better in performance as compared to other alternatives. The PSO, GA and the gradient descent search are used iteratively to adjust the parameters of FLANN until the error is less than the required value, which helps the FLANN model to get better classification accuracy.
Naik et al. [55] have designed a Honey Bee Mating Optimization (HBMO) based learning scheme for FLANN classifier and compared with FLANN, GA based FLANN and PSO based FLANN classifiers. The proposed method mimics the iterative mating process of honey bees and strate- gies to select eligible drones for mating process, for selection of best weights for FLANN classifiers.
Along with these applications, many recent applications of FLANN model with various hybrid learning schemes from the period 2000–2015 are listed in Table 1.
Table 1 represents various recent applications of FLANN models with varieties of hybrid learning methods to solve real life applications.

Background study of the proposed work

From all the FLANN models discussed in literature survey (Table 1), few of them (Table 2) implement some form of
learning methods which learns from past data in Classification tasks in Data mining. Almost all the higher order ANNs (HONNs) including functional link higher order ANN (FLANN) are sensitive to random initialization of weight and rely on the learning algorithm adopted. Although a selection of efficient learning algorithm for HONNs helps to improve the performance, initialization of weights with optimized weights rather than random weights also plays important roles in efficiency of HONNs.
In related works (Table 2), it is noticed that, all most all the previously published works have addressed the issue of random initialization of weight in FLANN by using various optimiza- tion algorithms such as Genetic Algorithm (GA) [64,65], Parti- cle Swarm Optimization (PSO) [66], and Honey-Bee Mating Optimization (HBMO) [67,68]. In these papers, various opti- mization algorithms (GA, PSO, Improved PSO, HMBO, etc.) are used to select the best set of weight for FLANN models for various nonlinear data classification. Although it is reported that these optimization techniques are successfully used in FLANN models for improved models such as GA based FLANN (GA-FLANN) [40], PSO based FLANN (PSO-FLANN) [41], IPSO based FLANN (IPSO-FLANN)
[52], HS based FLANN (HS-FLANN) [63] and HBMO based FLANN [55] (HBMO-FLANN), the major negative aspects of these implementations are the requirement of various complicated mathematical operators such as (i) Mutation and Crossover operator in GA in GA-FLANN, (ii) Position and Velocity calculation in PSO in PSO-FLANN and IPSO-FLANN and (iii) Crossover and Mutation in HBMO in HBMO-FLANN. The performance of these models depends upon the way of implementation of these mathematical opera- tions (such as selection of crossover operation, mutation oper- ation and mutation rate) and any changes in these factors may lead to increase in time and space complexity of the algorithm. Considering these, some new variants of Harmony Search
[69] are used in FLANN learning model with Gradient Des- cent learning scheme for classification. Many researchers are attracted toward the study of harmony search and its applica- tions due to the fact that, HS algorithms have few mathemat- ical requirements as compared to earlier meta-heuristic optimization algorithms and can be easily used for optimiza- tion problems. We have surveyed about 170 published papers




Survey on Various Application of Harmony Search Algorithm: 2004 - 2015

Engineering Cross Application
Others

Power and Energy Water System Management
Medical Robotics
Control
0	20	40	60

Figure 1	Various contributions on applications of harmony search algorithms.


on application of harmony Search algorithms till the year 2015 in the scientific databases of Elsevier, IEEE and Springer.
It is found that, various papers have been published in the area of different application of HS (Fig. 1) which includes
engineering (32.353%), water system management (4.118%),
medical (2.353%), robotics (1.765%), control (1.176%), power and energy (12.353%), cross application (22.941%) and others (22.941%). Starting from the development of HS, it has been a keen interest among the diversified researchers and has been used in various real life applications [70–239] (Table 3).
Inspired from successful applications of harmony Search algorithms, in this paper, an attempt has been made to address the intricacy in adjusting the set of weights of the FLANN model by using appropriate learning algorithm. Here the prob- lem solving approach of the Global-best Harmony Search along with learning ability of the Gradient Descent Learning (GDL) is used to obtain the optimal set of weight of FLANN model. The objective is to design an Ease-of-use FLANN model with Global-best Harmony Search technique which requires very few mathematical operation as compared to other meta-heuristics.
In this paper, an attempted has been made to design a FLANN model with hybrid Global-best Harmony Search (GbHS) and Gradient descent search based learning method for classification. The performance in terms of classification accuracy of the proposed method is compared with some of the existing popular methods such as MLP, SVM, and FSN and found that the results are exceeding over others.



Table 3  Various applications of harmony search algorithms.
References	Application area

Lee and Geem [70], Lee et al. [71], Saka [72,73], Zarei et al. [74], Kaveh and Talatahari [75], Fesanghary et al. [76], Fesanghary [77], Kaveh and Shakouri [78], Khazali et al. [79], Parizad et al. [80], Wei et al. [81], Verma et al. [82], Barzegari et al. [83], Nezhad et al. [84], Zhang and Hanzo [85], Gao et al. [86], Jafarpour and Khayyambashi [87], Sarvari and Zamanifar [88], Yadav et al. [89], Erdal et al. [90], Srinivasa et al. [91], Kudikala et al. [92], Mehdizadeh et al. [93], Kermani et al. [94], Gao et al. [95], Bekda and Nigdeli [96], Harrou and Zeblah [97], Del Ser et al. [98], Fesanghary et al. [99], Kaveh and Ahangaran [100], Shariatkhah et al. [101], Degertekin [102], Askarzadeh and Rezazadeh [103], Landa-Torres et al. [104– 107], Manjarres et al. [108,109], Gil-Lopez et al. [110], Del Ser et al. [111], Manjarres et al. [112], Yoo et al. [113], Huang et al. [114], Niu et al. [115], Askarzadeh and Masoud [116], Li et al. [117], Akin and Saka [118], Wang et al. [119], Zhai et al. [120], George et al. [121], Ouyang et al. [122], Wang et al. [123], Tarkeshwar et al. [124]
Engineering

Geem [125], Ayvaz [126,127], Geem [128], Geem et al. [129], Ayvaz [130], Cisty [131]	Water/Ground Water System Management
Panchal [132,133], Gandhi et al. [134], Landa-Torres et al. [135]	Medical
Tangpattanakul et al. [136], Yazdi et al. [137], Xu et al. [138]	Robotics
Coelho et al. [139], Das Sharma et al. [140]	Control

Vasebi et al. [141], Coelho and Mariani [142], Ceylan and Ceylan [143], Geem [144], Sinsupan et al. [145], Gao et al. [146], Ceylan et al. [147], Coelho et al. [148], Sui et al. [149], Sivasubramani and Swarup [150], Geem [151], Khorram and Jaberipour [152], Pandi and Panigrahi [153], Sivasubramani and Swarup [154], Chatterjee et al. [155], Afshari et al. [156], Sirjani et al. [157], Sirjani and Mohamed [158], Sirjani et al. [159], Javaheri and Goldoost-Soloot [160], Mukherjee [161]
Geem [162], Alexandre et al. [163], Geem [164], Wang et al. [165], Diao [166], Cobos et al. [167], Sarvari et al. [168], Hoang et al. [169], Alia et al. [170], Mandava et al. [171], Forsati and Mahdavi [172], Kaizhou et al. [173], Gao et al. [174], Han et al. [175], Yadav et al. [176], Wang et al. [177], Ayachi et al. [178], Ramos et al. [179], Navi et al. [180], Chandran and Nazeer [181], Ahmed et al. [182], Yusof et al. [183], Ko and Sim [184], Li et al. [185], Pan et al. [186,187], Ren et al. [188], Fu and Zhang [189], Jing et al. [190], Peiying et al. [191], Diao and Shen [192], Krishnaveni and Arumugam [193], Ezhilarasi and Swarup [194], Li et al. [195], Hua et al. [196], Ahmad et al. [197], Habib et al. [198], Salcedo-Sanz et al. [199], Gao et al. [200]
Geem [201], Geem and Choi [202], Geem and Williams [203], Fourie et al. [204], Mun and Geem [205,206], Coelho and Bernert [207], Ma et al. [208], Zou et al. [209,210], Fourie et al. [211], Mohsen et al. [212], Cheng and Yong [213], Bo et al. [214], Kattan et al. [215], Wong and Guo [216], Jaberipour and Khorram [217], Wang et al. [218], Huang et al. [219], Zou et al. [220], Kayhan et al. [221], Wang et al. [222], Kulluk et al. [223], Kattan and Abdullah [224], Alsewari and Zamli [225], Taleizadeh et al. [226], Landa-Torres et al. [227], Kulluk et al. [228], Salcedo-Sanz et al. [229], Garcı´ a-Torres et al. [230], Plasencia et al. [231], Turky et al. [232], Valian et al. [233], Yuan et al. [234], Kong et al. [235,236], Go¨ kc¸e and Ayvaz [237], Gupta and Jain [238], Salman et al. [239]
Power and Energy


Cross- Application




Others



The remaining part of this paper is organized as follows: Preliminaries in Section 4, proposed method in Section 5, experimental setup in Section 6, simulation results and perfor- mance comparisons in Section 7, proof of statistical signifi- cance in Section 8, conclusion in Section 9 and references.

Preliminaries

Functional link artificial neural network architecture

The Functional Link Artificial Neural Network (FLANN)
[240] is a class of Higher Order Neural Networks that make use of higher combination of its inputs [241,242] and has been successfully used in many applications such as pattern recogni- tion [243,244], classification [245–247], channel equalization
[248], system identification [249–253] and prediction [254]. Even if it has a single-layer network, still it is capable to handle nonlinear separable classification task as compared to MLP.
In FLANN, the dimension of input pattern increases artifi- cially through the functional expansion and then the extended and transformed input data are used to train the feed forward network. During functional expansion, various mathematical functions, such as sine, cosine, and log, are used to transform an original input pattern to its extended version. The number of input terms during functional expansion depends upon the number of attribute of an input pattern. The basic structure of FLANN is depicted in Fig. 2.
erated by using Eq. (1), where xi(j) stands for jth attribute The functionally expanded values for dataset x can be gen- of       order       m     ×     n. value of ith pattern and ‘x’ is a dataset in a form of matrix
u(xi(j)) = {xi(j); cos Pxi(j); sin Pxi(j); cos 2Pxi(j);
sin 2Pxi(j) ... cos nPxi(j); sin nPxi(j)}	(1)
erated for an input attribute value xi(j) of a pattern xi, intern, Total 2n + 1 number of functionally expanded values are gen-



























































Weight Updation





Figure 2	Functional link artificial neural network architecture.

(n * (2n + 1)) number of expanded values are generated for a single input pattern xi. In Eq. (1), value for i and j can be ran- ged from i = 1, 2 .. . m and j = 1, 2 ... n, where m and n are number of input pattern and number of attribute values of each input pattern respectively except class level (probably last
column of dataset x). Hence, the complete functionally expanded values for dataset x is represented using Eq. (2).
u = {{u(x (1)); u(x (2)) .. . u(x (n))}T;
{u(x2(1)); u(x2(2)) ... u(x2(n))}T ... 
{u(x (1)); u(x (2)) .. . u(x (n))}T}	(2)
The weights of FLANN are set randomly prior to the above
functionally expanded values ‘/’ are the input to FLANN clas- sifier. Total n * (2n + 1) number of weights are set for each individual pattern, as each input pattern is transformed to n * (2n + 1) number of functionally expanded values. Random initialization of weight-set for each individual pattern can be visualized as Eq. (3).
Wi = {wi;1; wi;2; ... wi;2n+1};  for i = 1; 2 ... n	(3)
where Wi is the weight vector initialized randomly for a single
input pattern. Hence, initialization of set of weight for input patterns of dataset ‘x’ can be viewed as a weight vector
W = {W ; W .. . W }T, where W is the set of weight for ith pattern in the dataset x. The dataset ‘x’ is supplied to FLANN
in terms of functionally expanded values ‘/’ and the net output is obtained as follows.
First, values of S is calculated as S = uXW = {s1; s2 ... sm}.
Then, the net output Y is computed as Y = f (S) =
{f (s1);f (s2)...f (sm)} = {y1;y2 ...ym } = {tanh(s1); tanh(s2)
...tanh(sm)}. Here tanh is used as activation function and net output yi is for input pattern xi.

Based on net output yi and given target value ti, error of FLANN is calculated and a suitable learning method is adopted to adjust weight values of FLANN.

Gradient descent learning scheme

Gradient descent learning is the most commonly used training methods in which weights are changed in such a way that network error is declined as rapidly as possible. The learning of FLANN model using Gradient descent method with error of the network is described below.
Error of kth input pattern is generated as e(k) = Y (k) — t(k)
2
k
2
for k = 1, 2 ... m, where m is the number of input pattern in a dataset.
Then,  weight  factor  of  ‘DW ’  can  be  computed  as
 PL 2×l×u ×d 

Finally, weight updation is done as wnew = w + DW where w = (w1; w2 ... wL×(2n+1)) and DW = (DW 1; DW 2 .. . DW L×(2n+1)).
Basically, a better learning algorithm helps the ANN model for fast convergence. Further, a use of competitive optimiza- tion technique can, not only improve the convergence of a learning algorithm, but also enhance accuracy of an ANN based classifier. In the next subsection, a new meta-heuristic optimization technique, known as Harmony Search technique and its variants have been described.

Variants of harmony search

The Harmony Search (HS) [69] is a meta-heuristic algorithm inspired by musical process of searching for a perfect shape of harmony. The algorithm is based on natural musical processes in which a musician searches for a better state of harmony by tuning pitch of each musical instrument, such as jazz improvisa- tion. The music improvisation by pitch adjustment in the Harmony Search is analogous to local and global search process to find better solution in any optimization techniques.

Harmony search
This section contains brief review on working procedure of the harmony search algorithm. In general, basic steps of harmony search can be expressed as follows:


Basically, the harmony memory (HM) is a group of pre- defined number of solution vectors similar to a population of particle in PSO or chromosome in GA. Initially HM is initial- ized with random solution vectors and gradually, solution vectors in HM are improved by using Step-3 of harmony search procedure known as HM improvisation step. This step is entirely controlled by the parameters: Harmony Memory Consideration Rate (HMCR), Pitch Adjustment Rate (PAR) and Bandwidth (bw).
In HS, the HMCR controls the balance between exploration and exploitation and it is set between 0 and 1. The searching procedure behaves as purely random search, if the HMCR is set to 0 and a value 1 for HMCR specifies 100% of previous solution vectors from HM are taken into consideration for next




Where  u = (u1; u2 .. . uL ),   e = (e1; e2 .. . eL)  and d = (d1; d2 ... dL) are the vector which represent sets of functional expansion, set of error and set of error
term respectively where L is the number of input patterns.
harmony from outside the HM. In this way, HMCR keeps
the balance between exploration and exploitation. Another parameter PAR determines the rate of adjustment of solution vectors based on the bandwidth (bw) which is usually a variable, and behaves as step size.



The HMCR and PAR determine Memory Consideration Probability (MCP), Pitch Adjustment Probability (PAP) and Random Probability (RP) as follows:
MCP = HMCR * (1 — PAR)* 100
PAP = HMCR * PAR * 100
of bw with iteration is inspired from the strategy of decreasing the learning rate of neural networks dynamically [256].
Unlike HS, the bw and PAR are not fixed and this value changes according to HS iterations which is achieved by using Eqs. (5) and (6).

RP = 100 — MCP — PAP
Basically, Improvisation of HM is governed by these parame-
bw(iter)= bwmax × exp
ln  bwmin
max × iter
N
(5)

ters (MCP, PAP, and RP).
MCP = 0.9 * (1 — 0.45) * 100 = 49.5	and	PAP = 0.9 * Example:  If  HMCR = 0.99  and  PAR = 0.45  then
0.45 * 100 = 40.5 and RP = 100 — 49.5 — 40.5 = 10. Which
In Eq. (5), bw(iter) is the bandwidth in particular iteration
‘iter’, bwmin and bwmax are the minimum and maximum band-
width respectively and N is the number of solution vector in the population.

means, during harmony improvisation phase (Step-3), 49.5%
of solution vectors are migrated (without any changes) from
PAR(iter)= PAR

min
+ PARmax — PARmin × iter	(6)
N

previous harmony memory (HM) to New Harmony Memory
(NHM), 40.5% of solution vectors are gone through pitch adjustment and then included into NHM and 10% of solution vectors are gone through modification by adding randomly generated values with existing solution vector in HM.
In HS, the bw and PAR are fixed and pitch adjustment is done according to Eq. (4).
HMi(t + 1)
HMi(t + 1)= HMj(t)— rand(1)* bw if rand(1) < 0.5
i	j
(4)
In Eq. (4), HMi(t + 1) is the next ith harmony at time t + 1 and HMj(t) is the jth randomly selected harmony for pitch adjustment at time t.
In recent years, many Harmony Search variants (Fig. 3) have been proposed by the researchers by incorporating some modifications to the original HS algorithm [69]. Further, these variants are some modifications of three major variations of HS and those are Improved HS, Global-best HS and Self Adaptive HS. These variants have some common steps and are different in strategies of solving optimization problem. Overall strategies and steps involved with these variants of Harmony Search have been demonstrated in Fig. 3.

Improved harmony search
The Improved Harmony Search (IHS) [255] is an initial variant of HS, which employs a novel strategy for generation of new solution vectors that not only enhances accuracy but also improves the convergence rate of basic HS algorithm. The authors have claimed the better performance of IHS over HS by eliminating constant parameters (bw, PAR) in HS algorithm and incorporating dynamically changes in PAR and bw with iteration number.
The IHS is free from the fixed values of PAR and bw in the HS algorithm by decreasing bw and increasing PAR with an iteration number and found considerable influence on the quality of solutions. The mechanism of dynamically decreasing
In Eq. (6), PAR(iter) is the pitch adjustment rate in particular iteration ‘iter’, PARmin and PARmax are the minimum and
maximum pitch adjustment rate and N is the number of solu- tion vector in the population.

Global-best Harmony Search
Inspired from successful use of PSO in numerous applications, Omran and Mahdavi [257] have developed Global best Har- mony Search (GbHS), which borrowed the concepts from PSO to enhance its performance of HS optimization. Instead


employ the small constant PAR which may prevent overshoot- ing and oscillation that normally occurs in IHS.
In GbHS, it eliminates the difficulties of selecting appropri- ate bandwidth (bw) by directly adopting the current best pitch (Global best) from the harmony memory and adjusting other solution vectors to improve their qualities in the HM without pitch adjustment step. This process of HM improvisation is analogous to selection of local best (LBest) and global best (GBest) particle (In PSO) from population based on which, changing of position of particles is obtained. The performance of GbHS is found to be significantly better than HS and IHS in terms of quality of solution and convergence rate.

Self adaptive harmony search
In, SAHS [258], the pitch adjustment step in IHS has been modified to incorporate better utilization of its own experi- ences, by updating the new harmony according to the maxi- mum and minimum values in the HM. Here, the objective is to simplify pitch adjustment step by introducing a new strategy of adjusting new harmony by using maximum and minimum values in HM encountered so far, thereby eliminating bw altogether from HS procedure.
Like IHS, in SAHS, the bw and PAR change with HS iter- ations. The SAHS is different from IHS in pitch adjustment mechanism as illustrated in Eq. (7).
Let minHM and maxHM denote the lowest and the highest values of the ith variable in the HM respectively and then har- mony in HM is further adjusted by the following Equations:





HMi(t + 1)= 
HM (t + 1)= HM (t)+ max (HM) — HM (t) × rand(1) if rand(1) < 0.5 HM (t + 1)= HM (t)— HM (t)— min (HM) × rand(1)	if rand(1) > 0.5
(7)



Steps Involves in Harmony Search Variants






Figure 3	Harmony search variants.



where HMi(t + 1) is the next ith harmony at time t + 1, HMj(t) is the jth randomly selected harmony for pitch adjust- ment at time t, min(HM) and max(HM) are the minimum and
maximum values of entire harmony memory (HM) and rand
(1) is a uniform number in the [0, 1] range without 1.

Proposed method

In this section, we have considered four FLANN classifiers with Gradient descent learning based on four variants of Harmony Search algorithm. In this paper, a deep experimental analysis on Harmony Search algorithm and its different vari- ants (i.e. Improved HS, Global-best HS and Self Adaptive HS) has been done and an attempt has been made to use the problem solving strategies of these variants to improve perfor- mance of FLANN classifiers. Here the objective is to select the best set of weight (Weight-set) from a set of randomly selected
weight-sets (Population) for FLANN model for classification task. This paper mainly focused on Global-best HS based Gradient Descent Learning-FLANN model (GbHS-GDL- FLANN) for classification and the objective is to investigate the performances of Global-best HS (GbHS) to enhance clas- sification accuracy of FLANN classifier as compared to basic HS (HS), Improved HS (IHS) and Self Adaptive HS (SAHS). Also, the performance of GbHS-GDL-FLANN is compared with other meta-heuristic algorithm (GA based FLANN and PSO based FLANN) to get generalized performance. The pseudo codes developed during implementation of proposed GbHS based Gradient descent learning FLANN (GbHS- GDL-FLANN) are presented in Section 5.1. The simulation results and the comparisons of performance of these hybrid FLANN classifiers (FLANN, GA-GDL-FLANN, PSO-GDL-FLANN, HS-GDL-FLANN, IHS-GDL-FLANN, GbHS-GDL-FLANN and SAHS-GDL-FLANN), MLP, SVM
and FSN are discussed in Section 7.



Global-best Harmony Search based Gradient Descent Learning-FLANN (GbHS-GDL-FLANN)

Initially (Fig. 4), the population of weight-sets (HM) is randomly initialized. Each weight-set is a possible candidate set of weight of FLANN for classification of the dataset. Each
individual weight-set in HM can be defined as follows:

Wi = wi;1; wi;2 .. . wm×n×(2×k+1)	(8) In Eq. (8), the (2 × k + 1) is the number of functionally expanded values for a single value in input pattern (for a cho-
sen value of k), n is the number of values (features) in a single input pattern and m is the number of patterns in the dataset.
The set of weight-sets in the HM (population) is represented as Eq. (9).
HM = (W1; W2 ... Wm)	(9)
The objective of this study is to improve the quality of weight-
sets by using Global-best HS and to find the best weight-set from the population (HM). The problem solving strategies of Global-best HS are used here to improve the qualities of harmonies in harmony memory (HM) and the complete flow of execution can be realized by using Fig. 4 and pseudo codes (Algorithms 1–4). Initially, the harmony memory (HM) is initialized with ‘n’ numbers of weight-sets for FLANN. Each weight-set Wi is set to FLANN and the FLANN model is trained with a particular dataset. Based on output of the


HM
Figure 4	Overview of proposed scheme.



FLANN and given target value, error of the network is obtained. For a specific dataset, the root mean square error (RMSE) (Eq. (10)) for each Weight-set Wi is computed by using output of the FLANN (Algorithm 4) and given target value. Based on RMSEs, fitness of the weight-sets is computed by using Eq. (11).
The Root Mean Square Error (RMSE) of predicted output values y^i of a target variable yi is computed for n different pre- dictions as follows:
sﬃPﬃﬃﬃﬃﬃnﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ2ﬃﬃ

		

After improvisation of HM by using Glabal-best Harmony Search optimization, the HM is updated by based on comparison of fitness of weight-sets in HM and NHM. If the fitness of ith weight-set in HM is less than fitness of ith weight-set in NHM, then HM(i,:) will be replaced by NHM (i,:) else HM(i,:) serves as new harmony for next iteration. The pseudo codes for HM updation procedure are represented in Algorithm 3. These processes are continued iteratively until maximum iteration is reached or increase in fitness of weight-sets in HM in subsequent iteration is not significant.




FWi = 1/RMSEi	(11)
In Eq. (11), Wi is the ith weight-sets in the population, RMSEi
is the root mean square error of ith weight-set and FWi is the fitness of ith weight-set Wi.
After evaluation of fitness values for each weight-set in HM, the HM goes through HM improvisation process based on Global-best Harmony Search (GbHS). During this, the parameters: HMS (Harmony Memory Size), HMCR (Har- mony Memory Consideration Rate), PAR (Pitch Adjustment Rate) and bw (Bandwidth) are set and based on which MCP (Memory Consideration Probability), PAP (Pitch Adjustment Probability) & RP (Random Probability) are computed (Algo- rithm 1). Basically, the Harmony Search procedure is governed by these parameters.
Algorithm 2 represents pseudo-codes for Harmony Memory improvisation in which, initially, among all weight-sets (har- monies) in HM, some are randomly selected with a probability of MCP (Memory Consideration Probability) and included into New Harmony Memory (NHM). Here the objective is to migrate some weight-sets (harmonies) from HM into NHM without any changes on them, which serve as new harmonies. For the improvement of weight-sets through pitch adjustment, some weight-sets are selected randomly from HM with a prob- ability of PAP and then they are adjusted based on the variable distance bandwidth (bw) which is similar to the local search method with a step size bw. Similarly, with a probability of Random Probability (RP), some weight-sets are selected ran- domly and added to NHM by suitably adding or subtracting a random value on it. Although Global-best Harmony Search is suggested to bypass the pitch adjustment step, better result also can be obtained through pitch adjustment of harmonies.
After the generation of harmonies NHM from HM through Harmonic Memory Consideration, Pitch Adjustment and Random Selection phases with probabilities of MCP, PAP and RP respectively, all the harmonies in NHM are treated as local best particles (LBest) from which the harmony with best fitness is chosen as global best particle (GBest). Here, the population of harmonies in HM is analogous to popula- tion of particles in PSO. The next velocities (Vnew) of har- monies (particles) is computed by using V (Initial Velocity), LBest and Gbest from Eq. (12). After obtaining next velocity Vnew, the next position of harmonies in NHM is computed from Eq. (13) (Algorithm 2).
Vi(t + 1)= Vi(t)+ c1 * rand(1)* (lbesti — Xi(t))
+ c2 * rand(1)* (gbesti — Xi(t))	(12)
Xi(t + 1)= Xi(t)+ Vi(t + 1)	(13)
Algorithm	1. Global-best-Harmony-Search-GDL-FLANN (GbHS-GDL-FLANN) Procedure



Algorithm 2. ImprovizationOfHarmonyMemory Procedure





Algorithm 3. UpdateHarmonyMemory Procedure


Algorithm 4. fitfromtrain Procedure





Experimental setup

In this section, the environment for simulation, the dataset used for training & testing phase and the parameter setting for proposed methods during simulation are presented.
All the classification methods (FLANN, GA-FLANN, PSO-FLANN, HS-FLANN,  IHS-FLANN,  SAHS-FLANN
and GbHS-FLANN) are implemented in Matlab (Version 9.0) in a system with Window XP operating system. After obtaining the results of simulation, statistical analysis has been carried out using SPSS statistical tool (Version 16.0).
The benchmark datasets (Table 4) used for classification are originated from UCI machine learning repository [259] and processed by KEEL software [260].
Table 4 represents the list of benchmark datasets which is used to evaluate the models. All the datasets are presented
Pitch	Adjustment	Rate	(PAR):	PARmin = 0.01, PARmax = 0.9
Bandwidth (bw): bwmin = 0.0001, bwmax = 	1	

Global-best Harmony Search parameter

Harmony Memory Size (HMS): 40
Harmony Memory Consideration Rate (HMCR): 0.9 Pitch Adjustment Rate (PAR): 0.3
Bandwidth (bw): 0.0001

Self adaptive harmony search parameter

Harmony Memory Size (HMS): 40
Harmony Memory Consideration Rate (HMCR): 0.9

along with their number of patterns, number of attributes
(without class attribute) and number of classes.
The detail descriptions about all these dataset can be obtained at ‘http://archive.ics.uci.edu/ml/’ and ‘http://keel.es/’.

Parameters setting used for simulation
Pitch	Adjustment	Rate	(PAR):	PAR PARmax = 0.9


Results and comparisons

min
= 0.01,



FLANN parameter
During the learning of the FLANN model, the gradient des- cent learning method is used by setting ‘l’ to 0.13. The value of ‘l’ is obtained by testing the models in the range 0–3. Each value in the input pattern is expanded to 11 number of func-
In this section, the classification accuracies (Eq. (14)) obtained from various methods for all benchmark datasets with their comparison results are represented. These classification accuracies (Tables 6–8) are observed individually for training and testing phase.

tionally expanded input values by setting n = 5. (As FLANN model suggests to generate 2n + 1 number of functionally expanded input values for a single value in the input pattern.)
i=1 n
Classification accuracy =	n
i=1
m
j=1;
i=j
j=1 m
CMi;j CMi;j
× 100	(14)

Harmony search parameter

Harmony Memory Size (HMS): 40
Harmony Memory Consideration Rate (HMCR): 0.9 Pitch Adjustment Rate (PAR): 0.3
Bandwidth (bw): 0.0001


Improved harmony search parameter

Harmony Memory Size (HMS): 40
Harmony Memory Consideration Rate (HMCR): 0.9
In Eq. (14), the CM is the confusion matrix which represents
number of well classified and miss classified pattern after classification operation.
Here n and m are no. of row and no. of column of CM respectively and they are supposed to be equal (i.e. n = m).

Cross validation

The Cross-Validation [261] is a statistical method to estimate generalized performance of the learned model from data which compare learning algorithms by dividing data into two seg- ments: training set & testing set, which are used to train and evaluate the model respectively. In k-fold cross-validation [262], the data are partitioned into k equally or nearly equally sized fragments on which training and validation are per- formed such that, in each test different fold of the data is used for training and validation.
In this paper, all the datasets used for classification are pre- pared for cross validation by using 5-folds cross validation technique. During the preparation of datasets for 5-fold cross validation, 5 pairs of dataset sample are created and each pair contains datasets for training and testing phase.
Table 5 represents 5-fold cross validated Newthyroid data- set in which dataset is divided into 5 pair datasets. Each pair contains dataset for training and testing which are used to train and test the models respectively.
For example (Table 5), the ‘newthyroid-5-1tra.dat’ and ‘n ewthyroid-5-1tst.dat’ data are a pair of datasets sample of New Thyroid dataset which is used for training and testing phase for a single run respectively. As 5-fold cross validation



is employed, the New Thyroid datasets contains 5 such pair of dataset sample for training and testing the algorithms.
The 5-fold cross validated dataset for NEW THYROID dataset is presented in Table 5. All other datasets are prepared for 5-fold cross validation in the same fashion and collected from KEEL Dataset Repository [260]. The average classifica- tion accuracies on 5-fold cross validation dataset during train- ing and testing phase are listed in Tables 6–8. In Tables 6–8, the average of classification accuracies of algorithms on ‘new thyroid-5-1tra.dat’, ‘newthyroid-5-2tra.dat’, ‘newthyroid-5-3tr a.dat’, ‘newthyroid-5-4tra.dat’ and ‘newthyroid-5-5tra.dat’ is posted as the classification accuracy in training phase for New Thyroid dataset. Similarly, the average of classification accuracies of algorithms on ‘newthyroid-5-1tst.dat’, ‘newthyr oid-5-2tst.dat’, ‘newthyroid-5-3tst.dat’, ‘newthyroid-5-4tst.da t’ and ‘newthyroid-5-5tst.dat’ is posted as the classification accuracy in testing phase.
Table 6 describes the comparison of classification accura- cies of FLANN, GA based FLANN (GA-FLANN), PSO
based FLANN (PSO-FLANN) and HS based FLANN (HS- FLANN) classifiers and Table 7 represents comparison of other 4 classifiers: HS based FLANN (HS-FLANN), Improved HS based FLANN (IHS-FLANN), Self-Adaptive HS based FLANN (SAHS-FLANN) and Global-best HS based FLANN (GbHS-FLANN), which are based of variants of Harmony Search technique.
After comparison of proposed method with hybrid models (Tables 6 and 7), we have made some comparison with other similar approaches in the same area. The projected method (GbHS-FLANN) is compared with Multi-Layer Perceptron [52], Support Vector Machine [52] and Fuzzy System Nets [52]. Table 8 represents the average classification accuracies of the GbHS-FLANN, MLP, SVM and FSN for both training and testing phase. The average of training and testing accura- cies on the datasets are listed in Table 9. The overall statistic on performance of all the methods in this study is shown in Fig. 5. From the simulation results (Table 9), it clearly indi- cates that the proposed GbHS-FLANN outperforms over the other results in all the tested datasets.
In this study, the performance of GA, PSO, HS, IHS, SAHS and GbHS is analyzed in order to know the improve- ment of harmonies (weight-sets) in the population by these algorithms in different generation. The changes in fitness of weight-sets in different generations are observed in all the 11 number of datasets and Figs. 6–16 demonstrate the improve- ments of fitness of weight-sets in the population.

Proof of statistical significance

In this section, the statistical comparison of classifiers over multiple datasets [263] is presented to argue the projected method is statistically better and significantly different from













Average Classiﬁcation Accuracies in %
87.75 %  87.91 % 88.032 % 88.53 %














1	2	3	4	5	6	7	8	9	10




other alternative classifiers by using Friedman test [264,265]. List of datasets on which these tests have been carried out and the assigned ranks to each of the considered methods is presented in Table 10.

Friedman test

The Friedman test is a nonparametric statistical method which computes average ranks of algorithms (Eq. (15)) and compares
Figure 5 Comparisons of results of proposed method with all related work.


them. In Eq. (15), r j is the rank of the jth of k number of clas- sifiers on ith of N number of datasets.
In Table 10, all the classification models are ranked based on their performance on datasets. Each classifier is assigned with a rank which is mentioned with brackets. The models with lowest and highest rank are considered to be models hav- ing best and worst performance respectively.

72	B. Naik et al.






























Figure 6	Improvements in fitness of population in different iterations observed in MONK2 dataset.





























Figure 7	Improvements in fitness of population in different iterations observed in IRIS dataset.

In Table 10, the ranks of each classifier on various datasets	{R1 = 7, R2 = 5.91, R3 = 4.636, R4 = 4.364, R5 = 2.636,

are shown in brackets. Based on r j, the average ranks of seven
R6 = 2.273, R7
= 1.182}

classifier is found from Eq. (15).
R =  1 Xrj
(15)
The X2 value is computed from the average rank Rj of each



of X2 as 61.232. From the value of X2 , the Friedman
The average ranks for all classifiers are found as follows:	statistics FF is computed by Eq. (17) and found as 128.42281.

A Global-best Harmony Search based Gradient Descent Learning FLANN	73





























Figure 8	Improvements in fitness of population in different iterations observed in HEART dataset.






























Figure 9	Improvements in fitness of population in different iterations observed in HAYESROTH dataset.


The Friedman statistic is distributed according to X2
with
(7 — 1) * (11 — 1) = 60	degrees	of	freedom,	a	crucial

(k — 1) degree of freedom under the null-hypothesis (H0)
from FF with (k — 1) and (k — 1) * (N — 1) degree of and the critical value of the F-distribution can be obtained number of datasets, FF = 128.42281 with 7 — 1 = 6 and freedom. In our case, for the 7 number of classifiers and 11
value = 3.12 is obtained from suitably selecting a = 0.01.
Density plot for degree of freedom (6, 60) is obtained and displayed in Fig. 17.
The null-hypothesis is clearly rejected as critical value 3.12 is less than FF statistic 128.42281.

74	B. Naik et al.






























Figure 10	Improvements in fitness of population in different iterations observed in WINE dataset.






























Figure 11	Improvements in fitness of population in different iterations observed in IONOSPHERE dataset.

H0: All the classifier has same rank, hence they are



FF = (N — 1)X2 , N(K — 1) — X2	(17)

X2 = (12N/k(k + 1))
k(k + 1)2
j
j
(16)
After the rejection of null-hypothesis from Friedman test, in
order to evaluate performance by pairwise comparison of pro- posed classifier with another classifier based on z-score value

A Global-best Harmony Search based Gradient Descent Learning FLANN	75






























Figure 12	Improvements in fitness of population in different iterations observed in HEPATITIS dataset.

































Figure 13	Improvements in fitness of population in different iterations observed in PIMA dataset.

76	B. Naik et al.





























Figure 14	Improvements in fitness of population in different iterations observed in NEW THYROID dataset.

































Figure 15	Improvements in fitness of population in different iterations observed in BUPA dataset.

A Global-best Harmony Search based Gradient Descent Learning FLANN	77



























Figure 16	Improvements in fitness of population in different iterations observed in DERMATOLOGY dataset.






















Figure 17	Density plot.








and p-value, the post-hoc test has been carried out by using the Holm procedure [263,266,267].

Holm and Hochberg procedure

is used to compare classifiers with their p-value and a/(k — i). In this section, the Holm [268] and Hochberg [269] procedure During this test, the z-value is obtained from Eq. (18) and
based on z-value, p-value is computed from the table of the normal distribution.
	 ,rkﬃﬃﬃ(ﬃﬃkﬃﬃﬃﬃ+ﬃﬃﬃﬃ1ﬃﬃﬃ)ﬃ
The ANOVA [272] is the general statistical technique for testing the differences between more than two related perfor- mances of the classifiers measured on the same datasets for training and testing. During ANOVA test, the null-hypothesis is to be considered is that: ‘‘all classifiers are same in perfor- mances and differences in performances are simply random”. In ANOVA test, total variability in classifier’s performances is investigated and classified into three categories: between- classifiers variability, between the datasets variability and between-error variability. It divides the total variation into the variability between the classifiers, variability between the datasets  and  the  residual  (error)  variability.  The  null-

z = Ri — Rj	6N
(18)
hypothesis can be rejected if and only if, the between-
classifiers variability is larger than the between-error variability. In this paper, the statistics on all classifier’s performance is

where z is the z-score value, k is the number of classifiers, N is
the number of datasets and Ri and Rj are average rank of ith and jth classifier respectively.
z-value, p-value and a/(k — i), where ‘i’ is the classifier’s number. Table 11 presents comparison of All 7 classifier based on In the Holm [268] and Hochberg [269] procedure, the
corresponding value of a/(k — i). In Table 11, all classifiers null-hypothesis (H0) is rejected if pi – value is less than the and a/(k — i) values. For example, while comparing between are compared with proposed method with respect to pi – value than a/(k — i) value 0.000089. Hence the null-hypothesis is GbHS-FLANN and PSO-FLANN, the pi – value 3.74974 is less rejected in this case.
a/(k — i), it was observed that, in almost all the cases pi – values is less than a/(k — i) values. Hence, it is clear that the By using the Holm test, when we compared the pi – value with
null-hypothesis is rejected. Thus, the proposed classifier
‘GbHS-FLANN’ is statistically better and significantly different from other classifiers (except IHS-FLANN and SAHS-FLANN) in performance on cross validated data and outperforms other classifiers. In a more close observation, while comparison with IHS-FLANN and SAHS-FLANN, the GbHS-FLANN is found better than IHS-FLANN and SAHS-FLANN in performance but it is not much significantly different.

Post-Hoc ANOVA Statistical Analysis (Tukey Test & Dunnett Test)

After the rejection of the null-hypothesis from Friedman test in Section 8.1 and Holm procedure in Section 8.2, in this section, the Post-Hoc ANOVA Statistical Analysis has been carried out by using Tukey Test [270] & Dunnett Test [271] to get generalized statistic on the performance of all classifiers.
computed under post-hoc-ANOVA test by using SPSS (Version: 16.0) statistical tool. All the methods are executed for 10 numbers of runs on each dataset. The test has been carried out with 90% confidence interval, 0.1 significant level and linear polynomial contrast. To get the differences between the performances of classifiers, we have used post-hoc ANOVA test by using mostly used Tukey test and Dunnett test. The Tukey test is carried out for comparisons of perfor- mance of all classifiers with each other and the Dunnett test for comparisons of all classifiers with base classifier (proposed classifier). The results from Tukey test and Dunnett test are presented in Tables 12 and 13 respectively.
In Tukey test (Table 12), all the methods are compared pairwise with respect to mean difference, standard error and level of significance. The null-hypothesis is rejected if the between-classifiers variability is larger than the between-error variability. For example, while comparing the proposed method (GbHS-FLANN) with PSO-FLANN, we noticed that, the between-classifiers variability (1.04641) is larger than the between-error variability (1.02109). Hence, the null- hypothesis is rejected in this case. According to this observa- tion, we found the rejection of null-hypothesis in all most all cases (4 out of 6).
In Dunnett test (Table 13), only the proposed method is compared with other alternative methods with respect to mean difference, standard error and level of significance. The criteria for the rejection of null-hypothesis are same as Tukey test. For example, while comparing the proposed method (GbHS-FLANN) with GA-FLANN, we notice that, between-classifiers variability (2.02909) is larger than the between-error variability (1.02109). Hence, the null- hypothesis is rejected in this case. The rejection of null- hypothesis is noticed in all most all cases.











As a conclusion of these tests, we noticed that, the mean differences (between-classifiers variability) among classifiers are larger than the standard errors (between-error variability) (except between GbHS-FLANN & IHS-FLANN and GbHS- FLANN & SAHS-FLANN) (Table 12). Also in Dunnett test (Table 13), while comparing GbHS-FLANN with other classi- fiers, we observed same as that of Tukey test. In both Tukey test and Dunnett test, the rejection of null-hypothesis holds for all most all classifier (out of 6 classifiers, rejection of null-hypothesis holds for 4 classifiers). Hence, as a whole, the null-hypothesis can be rejected.

Conclusion

From multiple comparison of classifiers by using Tukey test and Dunnett test (Tables 12 and 13), and rejection of the null-hypothesis of post-hoc test, clearly the proposed method is found significantly better and different from other methods. This is because, in all most all the cases, we noticed that, the mean differences (between-classifiers variability) among classi- fiers are larger than the standard errors (between-error vari- ability). In Friedman test, the null-hypothesis is rejected as the critical value of the F-distribution is found less than FF statistic, which proves the proposed classifier is statistically sig- nificant from other classifiers. After the rejection of the null- hypothesis in Friedman test, all classifiers are compared pair-
wise in terms of the z-values, p-values and a/(k — i) from the
ANOVA post-hoc test by using the Holm procedure (Table 11).
than a/(k — i) values thereby rejection of null-hypothesis. We observed that, in all most all the cases, p-values are less
From rigorous test under well known statistical methods
(Friedman test, Post-hoc test by Holm and Hochberg proce- dure, Tukey test and Dunnett test), we claim the proposed GbHS-FLANN classifier is better and outperforms other alter- natives (FLANN, GA-FLANN, PSO-FLANN, HS-FLANN,
IHS-FLANN, SAHS-FLANN). Also it can be computed with a low cost due to less complex architecture of FLANN and Global-best HS requires less mathematical computation and is free from complicated operators (like crossover in GA) and parameters (like c1, c2 in PSO). The future work is com- prised of integration of other improved variants of HS with other higher order neural network in diverse applications of data mining.

Acknowledgment

This work is supported by Department of Science & Technology (DST), Ministry of Science & Technology, New Delhi, Govt. of India, under grants No. DST/INSPIRE Fellowship/2013/585.

References

Mladeni D, Brank J, Grobelnik M. Document classification. Encyclopedia Mach Learn 2010:289–93.
Macioek P, Dobrowolski G. Using shallow semantic analysis and graph modelling for document classification. Int J Data Min, Model Manage 2013;5(2):123–37.
Yang P, Gao W, Tan Q, Wong K. A link-bridged topic model for cross-domain document classification. Inform Process Man- age 2013;49(6):1181–93.
Zhang Z, Ye Q, Li Y, Law R. Sentiment classification of online Cantonese reviews by supervised machine learning approaches. Int J Web Eng Technol 2009;5(4):382–97.
Yin P, Wang H, Zheng L. Sentiment classification of Chinese online reviews: analyzing and improving supervised machine learning. Int J Web Eng Technol 2012;7(4):381–98.
Hao Z, Cheng J, Cai R, Wen W, Wang L. Chinese sentiment classification based on the sentiment drop point. Emerg Intell Comput Technol Appl, Commun Comput Inform Sci 2013;375:55–60.
Hajmohammadi MS, Ibrahim R, Selamat A. Bi-view semi- supervised active learning for cross-lingual sentiment classifica- tion. Inform Process Manage 2014;50(5):718–32.
Upendar J, Gupta CP, Singh GK. Modified PSO and wavelet transform-based fault classification on transmission systems. Int J Bio-Inspired Comput 2010;2(6):395–403.
Bhalja B, Maheshwari RP. A new fault detection, classification and location scheme for transmission line. Int J Power Energy Convers 2011;2(4):353–64.
Yu F, Zhi-song Z, Xiao-ping W. Research on model of circuit fault classification based on rough sets and SVM. Adv Comput Sci Inform Eng, Adv Intell Soft Comput 2012;168:433–9.
He Z, Lin S, Deng Y, Li X, Qian Q. A rough membership neural network approach for fault classification in transmission lines. Int J Electr Power Energy Syst 2014;61(October):429–39.
Joachims T. Text classification. In: Learning to classify text using support vector machines. The Springer international series in engineering and computer science, vol. 668; 2002. p. 7–33.
Wajeed MA, Adilakshmi T. Supervised and semi-supervised learning in text classification using enhanced KNN algorithm: a comparative study of supervised and semi-supervised classifica- tion in text categorisation. Int J Intell Syst Technol Appl 2012;11 (3/4):179–95.
Uysal AK, Gunal S. Text classification using genetic algorithm oriented latent semantic features. Expert Syst Appl 2014; 41(13):5938–47.
Tolambiya A, Venkataraman S, Kalra PK. Content-based image classification with wavelet relevance vector machines. Soft Comput 2010;14(2):137.
Hiremath PS, Bannigidad P. Identification and classification of cocci bacterial cells in digital microscopic images. Int J Comput Biol Drug Des 2011;4(3):262–73.
Sriramkumar D, Malmathanraj R, Mohan R, Umamaheswari S. Mammogram tumour classification using modified segmentation techniques. Int J Biomed Eng Technol 2013;13(3):218–39.
Mei K, Dong P, Lei H, Fan J. A distributed approach for large- scale classifier training and image classification. Neurocomputing 2014;144:304–17.
Kim K, Cho S. DNA gene expression classification with ensemble classifiers optimized by speciated genetic algorithm. Pattern Recogn Mach Intell, Lect Notes Comput Sci 2005;3776:649–53.
Kianmehr K, Alshalalfa M, Alhajj R. Fuzzy clustering-based discretization for gene expression classification. Knowl Inform Syst 2010;24(3):441–65.
Lee S, Lee E, Lee KH, Lee D. Predicting disease phenotypes based on the molecular networks with condition-responsive correlation. Int J Data Min Bioinform 2011;5(2):131–42.
Keedwell Ed, Narayanan A. Gene expression rule discovery and multi-objective ROC analysis using a neural-genetic hybrid. Int J Data Min Bioinform 2013;7(4):376–96.
Gillies CE, Siadat M, Patel NV, Wilson GD. A simulation to analyze feature selection methods utilizing gene ontology for gene expression classification. J Biomed Inform 2013; 46(6):1044–59.
Andreopoulou Z, Tsekouropoulos G, Koutroumanidis T, Vla- chopoulou M, Manos B. Typology for e-business activities in the agricultural sector. Int J Bus Inform Syst 2008;3(3):231–51.



Sarkar BK, Sana SS, Chaudhuri K. Accuracy-based learning classification system. Int J Inform Decis Sci 2010;2(1):68–86.
Valavanis IK, Spyrou GM, Nikita KS. A comparative study of multi-classification methods for protein fold recognition. Int J Comput Intell Bioinform Syst Biol 2010;1(3):332–46.
Solesvik MZ. Collaborative knowledge management: case stud- ies from ship design. Int J Bus Inform Syst 2011;8(2):131–45.
Kumar P, Varma KI, Sureka A. Fuzzy based clustering algorithm for privacy preserving data mining. Int J Bus Inform Syst 2011;7(1):27–40.
Mulay P, Kulkarni PA. Knowledge augmentation via incremen- tal clustering: new technology for effective knowledge manage- ment. Int J Bus Inform Syst 2013;12(1):68–87.
Lai DTC, Garibaldi JM. A preliminary study on automatic breast cancer data classification using semi-supervised fuzzy c- means. Int J Biomed Eng Technol 2013;13(4):303–22.
Quinlan JR. C4.5: programs for machine learning. San Francisco (CA, USA): Morgan Kaufman Publishers Inc.; 1993.
Yung Y, Shaw MJ. Introduction to fuzzy decision tree. Fuzzy Net Syst 1995;69(1):125–39.
Hamamoto Y, Uchimura S, Tomita S. A bootstrap technique for nearest neighbour classifier design. IEEE Trans Pattern Anal Mach Intell 1997;19(1):73–9.
Zhang GP. Neural networks for classification: a survey. IEEE Trans Syst Man Cybernet, Part C: Appl Rev 2000;30(4):451–62.
Yager RR. An extension of the naive Bayesian classifier. Inform Sci 2006;176(5):577–88.
Redding N, Kowalczyk A, Downs T. Constructive high-order network algorithm that is polynomial time. Neural Networks 1993;6:997–1010.
Goel A, Saxena S, Bhanot S. Modified functional link artificial neural network. Int J Electr Comput Eng 2006;1(1):22–30.
Patra JC, Lim W, Meher P, Ang E. Financial prediction of major indices using computational efficient artificial neural networks. In: IEEE international joint conference on neural networks, Canada; July 16–21, 2006. p. 2114–20.
Mishra BB, Dehuri S. Functional link artificial neural network for classification task in data mining. J Comput Sci 2007; 3(12):948–55.
Dehuri S, Mishra BB, Cho S. Genetic feature selection for optimal functional link artificial neural network in classification. Proceedings of the 9th International Conference on Intelligent Data Engineering and Automated Learning. Berlin Heidelberg: Springer-Verlag; 2008.
Dehuri S, Cho S. A comprehensive survey on functional link neural networks and an adaptive PSO-BP learning for CFLNN. Neural Comput Appl 2009;19(2):187–205.
Patra JC, Lim W, Thanh N, Meher P. Computationally efficient FLANN-based intelligent stock price prediction system. In: IEEE proceedings of international joint conference on neural networks, Atlanta, Georgia, USA; June 14–19; 2009. p. 2431–8.
Sun J, Patra J, Lim W, Li Y. Functional link artificial neural network-based disease gene prediction. In: IEEE proceedings of international joint conference on neural networks, Atlanta, Georgia, USA; June 14–19; 2009. p. 3003–10.
Chakravarty S, Dash PK. Forecasting stock market indices using hybrid network. In: IEEE world congress on nature & biolog- ically inspired computing; 2009. p. 1225–30.
Majhi R, Pandu S, Panda B, Majhi B, Panda G. Classification of consumer behavior using functional link artificial neural net- work. In: IEEE international conference on advances in computer engineering; 2010. p. 323–5.
Dehuri S, Cho S-B. Evolutionarily optimized features in func- tional link neural network for classification. Expert Syst Appl 2010;37:4379–91.
Nayak SC, Mishra BB, Behera B. Index prediction with neuro- genetic hybrid network: a comparative analysis of performance.
In: IEEE international conference on computing, communica- tion and applications (ICCCA); 2012. p. 1–6.
Bebarta DK, Rout AK, Biswal B, Das PK. Forecasting and classification of indian stocks using different polynomial func- tional link artificial neural networks. In: India conference (INDICON); 2012. p. 178–82.
Mishra S, Shaw K, Mishra D. A new meta-heuristic bat inspired classification approach for microarray data. C3IT, Proc Technol 2012;4:802–6.
Mahapatra R, Shaw K, Mishra D. Reduced feature based efficient cancer classification using single layer neural network. In: 2nd international conference on communication, computing & security. Proc Technol, vol. 6; 2012. p. 180–7.
Mishra S, Shaw K, Mishra D, Patnaik S. An enhanced classifier fusion model for classifying biomedical data. Int J Comput Vision Robot 2012;3(1/2):129–37.
Dehuri S, Roy R, Cho S, Ghosh A. An improved swarm optimized functional link artificial neural network (ISO- FLANN) for classification. J Syst Softw 2012:1333–45.
Mili F, Hamdi H. A comparative study of expansion functions for evolutionary hybrid functional link artificial neural networks for data mining and classification. In: International conference on computer applications technology (ICCAT); 2013. p. 1–8.
Naik B, Nayak J, Behera HS. A novel FLANN with a hybrid PSO and GA based gradient descent learning for classification. In: Proc of the 3rd int conf on front of intell comput (FICTA). Adv Intell Syst Comput, vol. 327; 2015. p. 745–54. http://dx.doi. org/10.1007/978-3-319-11933-5_84.
Naik B, Nayak J, Behera HS. A honey bee mating optimization based gradient descent learning – FLANN (HBMO-GDL- FLANN) for classification. In: Proceedings of the 49th annual convention of the computer society of India CSI – emerging ICT for bridging the future. Adv Intell Syst Comput, vol. 338; 2015.
p. 211–20. http://dx.doi.org/10.1007/978-3-319-13731-5_24.
Sicuranza GL, Carini A. A generalized FLANN filter for nonlinear active noise control. IEEE Trans Audio, Speech, Lang Process 2011;19(8):2412–7.
George NV, Panda G. A particle-swarm-optimization-based decentralized nonlinear active noise control system. IEEE Trans Instrum Meas 2012;61(12):3378–86.
Parija S, Sahu PK, Nanda SK, Singh SS. A functional link artificial neural network for location management in cellular network. In: International conference on information communi- cation and embedded systems (ICICES); 2013. p. 1160–4.
Sicuranza GL, Carini A. On the BIBO stability condition of adaptive recursive FLANN filters with application to nonlinear active noise control. IEEE Trans Audio, Speech, Lang Process 2012;20(1):234–45.
Ali HH, Haweel MT. Legendre based equalization for nonlinear wireless communication channels. In: International electronics, communications and photonics conference (SIECPC), Saudi; 2013. p. 1–4.
Durga Ganesh Reddy AV, Tarun Varma L. Wind power forecasting without using historical data. In: International conference on advances in electrical engineering (ICAEE); 2014. p. 1–3.
Cui M, Liu H, Li Z, Tang Y, Guan X. Identification of Hammerstein model using functional link artificial neural network. Neurocomputing 2014;142:419–28.
Naik B, Nayak J, Behera HS, Abraham A. A harmony search based gradient descent learning-FLANN (HS-GDL-FLANN) for classification. In: Computational intelligence in data mining, vol. 2. India: Springer; 2015. p. 525–39.
Holland JH. Genetic algorithms. Sci Am 1992:66–72.
Goldberg DE. Genetic algorithms in search, optimization, and machine learning. Reading, MA: Addison-Wesley; 1989, ISBN: 0201157675.



Kennedy J, Eberhart R. Particle swarm optimization. In: Proceedings of the 1995 IEEE international conference on neural networks, issue no. 4; 1995. p. 1942–8.
Abbass HA. A monogenous MBO approach to satisfiability. In: International conference on computational intelligence for modelling control and automation, CIMCA 2001, Las Vegas, NV, USA; 2001.
Abbass HA. Marriage in honey-bee optimization (MBO): a haplometrosis polygynous swarming approach. In: The congress on evolutionary computation (CEC 2001), Seoul, Korea; 2001. p. 207–14.
Geem ZW, Kim JH, Loganathan GV. A new heuristic opti- mization algorithm: harmony search. Simulation 2001;76:60–70.
Lee KS, Geem ZW. A new structural optimization method based on the harmony search algorithm. Comput Struct 2004;82(9–
):781–98.
Lee KS, Geem ZW, Lee SH. The harmony search heuristic for discrete structural optimization. Eng Optim 2005;37:663–84.
Saka MP. Optimum design of steel skeleton structures. Stud Comput Intell 2009;191:87–112.
Saka MP. Optimum design of steels way frames to BS5950 using harmony search algorithm. J Constr Steel Res 2009;65(1):36–43.
Zarei O, Fesanghary M, Farshi B, JaliliSaffar R, Razfar MR. Optimization of multi-pass face-milling via harmony search algorithm. J Mater Process Technol 2009;209:2386–92.
Kaveh A, Talatahari S. Particle swarm optimizer, ant colony strategy and harmony search scheme hybridized for optimization of truss structures. Comput Struct 2009;87:267–83.
Fesanghary M, Damangir E, Soleimani I. Design optimization of shell and tube heat exchangers using global sensitivity analysis and harmony search algorithm. Appl Therm Eng 2009; 29(5):1026–31.
Fesanghary M. Harmony search applications in mechanical, chemical and electrical engineering. Stud Comput Intell 2009;191.
Kaveh A, Shakouri A. Cost optimization of a composite floor system using an improved harmony search algorithm. J Constr Steel Res 2010;66(5):664–9.
Khazali AH, Parizad A, Kalantar M. Optimal reactive/voltage control by an improved harmony search algorithm. In: Canadian conference on electrical and computer engineering; 2010. p. 1–6.
Parizad A, Khazali AH, Kalantar M. Sitting and sizing of distributed generation through harmony search algorithm for improving voltage profile and reduction of THD and losses. In: Canadian conference on electrical and computer engineering; 2010. p. 1–7.
Wei L, Guo W, Wen F, Ledwich G, Liao Z, Xin J. Waveform matching approach for fault diagnosis of a high-voltage trans- mission line employing harmony search algorithm. IEE Proc Gener Transm Distrib 2010;4(7):801–9.
Verma A, Panigrahi BK, Bijwe PR. Harmony search algorithm for transmission network expansion planning. IEE Proc Gener Transm Distrib 2010;4(6):663–73.
Barzegari M, Bathaee SMT, Mohsen A. Optimal coordination of directional over current relays using harmony search algorithm. In: International conference on environment and electrical engineering; 2010. p. 321–4.
Nezhad SE, Kamali HJ, Moghaddam ME. Solving K-coverage problem in wireless sensor networks using improved harmony search. In: International conference on broadband, wireless computing, communication and applications; 2010. p. 49–55.
Zhang R, Hanzo L. Iterative multiuser detection and channel decoding for DS-CDMA using harmony search. IEEE Signal Process Lett 2010;16(10):917–20.
Gao L, Zou D, GeY, Jin W. Solving pressure vessel design problems by an effective global harmony search algorithm. In: Chinese control and decision conference, China; 2010. p. 4031–5.
Jafarpour N, Khayyambashi MR. QoS-aware selection of web service composition based on harmony search algorithm. In: 12th international conference on advanced communication technology, vol. 2; 2010. p. 1345–50.
Sarvari H, Zamanifar K. A self-adaptive harmony search algorithm for engineering and reliability problems. In: Second international conference on computational intelligence, mod- elling and simulation; 2010. p. 59–64.
Yadav P, Kumar R, Panda SK, Chang CS. Improved harmony search algorithm based optimal design of the brushless DC wheel motor. In: IEEE international conference on sustainable energy technologies; 2010. p. 1–6.
Erdal F, Dogan E, Saka MP. Optimum design of cellular beams using harmony search and particle swarm optimizers. J Constr Steel Res 2011;67(2):237–47.
Srinivasa RR, Narasimham SVL, Raju MR, Rao AS. Optimal network reconfiguration of large-scale distribution system using harmony search algorithm. IEEE Trans Power Syst 2011; 26(3):1080–8.
Kudikala S, Sabat SL, Udgata SK. Performance study of harmony search algorithm for analog circuit sizing. In: Interna- tional symposium on electronic system design; 2011. p. 12–7.
Mehdizadeh A, Horestani AK, Al-Sarawi SF, Abbott D. An efficient 60 GHz resonator using harmony search. In: IEEE recent advances in intelligent computational systems; 2011. p. 369–72.
Kermani EM, Salehinejad H, Talebi S. PAP reduction of OFDM signals using harmony search algorithm. In: International conference on telecommunications; 2011. p. 90–4.
Gao J, Wang J, Wang B, Song X. APAPR reduction algorithm based on harmony research for OFDM systems. Proc Eng 2011;15:2665–9.
Bekda G, Nigdeli SM. Estimating optimum parameters of tuned mass dampers using harmony search. Eng Struct 2011; 33(9):2716–23.
Harrou F, Zeblah A. An efficient harmony search optimization for maintenance planning to the telecommunication systems. In: Mansour Nashat, editor. Search algorithms and applications; 2011. ISBN: 978-953-307-156-5.
Del Ser J, Bilbao MN, Gil-Lopez S, Matinmikko M, Salcedo- Sanz S. Iterative power and subcarrier allocation in rate- constrained OFDMA downlink systems based on harmony search heuristics. Eng Appl Artif Intell 2011;24(5):748–56.
Fesanghary M, Asadib S, Geem ZW. Design of low-emission and energy efficient residential buildings using a multi-objective optimization algorithm. Build Environ 2012;49:245–50.
Kaveh A, Ahangaran M. Discrete cost optimization of compos- ite floor system using social harmony search model. Appl Soft Comput 2012;12(1):372–81.
Shariatkhah MH, Haghifam MR, Salehi J, Moser A. Duration based reconfiguration of electric distribution networks using dynamic programming and harmony search algorithm. Int J Electr Power Energy Syst 2012;41(1):1–10.
Degertekin SO. Improved harmony search algorithms for sizing optimization of truss structures. Comput Struct 2012:229–41.
Askarzadeh A, Rezazadeh A. An innovative global harmony search algorithm for parameter identification of a PEM fuel cell model. IEEE Trans Ind Electron 2012;59(9):3473–80.
Landa-Torres I, Manjarres D, Gil-Lopez S, DelSer J, Salcedo- Sanz S. A preliminary approach to near optimal multi-hop capacitated network design using grouping-dandelion encoded heuristics. In: IEEE international workshop on computer-aided modeling analysis and design of communication links and networks; 2012a.
Landa-Torres I, Gil-Lopez S, DelSer J, Salcedo-Sanz S, Man- jarres D, Portilla-Figueras JA. Efficient city wide planning of open wifi access networks using novel grouping harmony search heuristics. Eng Appl Artif Intell 2012;26(3):1124–30.



Landa-Torres I, DelSer J, Salcedo-Sanz S, Gil-Lopez S, Portilla- Figueras JA, Alonso-Garrido O. A comparative study of two hybrid grouping evolutionary techniques for the capacitated P- median problem. Comput Oper Res 2012;39(9):2214–22.
Landa-Torres I, Gil-Lopez S, Salcedo-Sanz S, DelSer J, Portilla- Figueras JA. A novel grouping harmony search algorithm for the multiple-type access node location problem. Expert Syst Appl 2012;39(5):5262–70.
Manjarres D, DelSer J, Gil-Lopez S, Vecchio M, Landa-Torres I, Lopez-Valcarce R. A novel heuristic approach for distance and connectivity based multi hop node localization in wireless sensor networks. Appl Soft Comput 2012:1–12.
Manjarres D, Landa-Torres I, Gil-Lopez S, DelSer J, Salcedo- Sanz S. A heuristically-driven multi-criteria tool for the design of efficient open WiFi access networks. In: IEEE international workshop on computer-aided modeling analysis and design of communication links and networks (CAMAD); 2012b.
Gil-Lopez S, DelSer J, Salcedo-Sanz S, Perez-Bellido AM, Cabero JM, Portilla-Figueras JA. A hybrid harmony search algorithm for the spread spectrum radar poly phase codes design problem. Expert Syst Appl 2012;39(12):11089–93.
Del Ser J, Matinmikko M, Gil-Lopez S, Mustonen M. Central- ized and distributed spectrum channel assignment in cognitive wireless networks: a harmony search approach. Appl Soft Comput 2012;12(2):921–30.
Manjarres D, DelSer J, Gil-Lopez S, Vecchio M, Landa-Torres I, Salcedo-Sanz S, et al. On the design of a novel two-objective harmony search approach for distance-and connectivity-based localization in wireless sensor networks. Eng Appl Artif Intell 2013;26(2):669–76.
Yoo Do Guen, Kim Joong Hoon, Geem Zong Woo. Overview of harmony search algorithm and its applications in civil engineer- ing. Evol Intel 2014;7(1):3–16.
Huang Yin-Fu et al. Music genre classification based on local feature selection using a self-adaptive harmony search algorithm. Data Knowl Eng 2014;92:60–76.
Niu Qun et al. A hybrid harmony search with arithmetic crossover operation for economic dispatch. Int J Electr Power Energy Syst 2014;62:237–57.
Askarzadeh Alireza, Zebarjadi Masoud. Wind power modeling using harmony search with a novel parameter setting approach. J Wind Eng Ind Aerodyn 2014;135:70–5.
Li Yazhi, Li Xiaoping, Gupta Jatinder ND. Solving the multi-objective flowline manufacturing cell scheduling problem by hybrid harmony search. Expert Syst Appl 2015;42(3): 1409–17.
Akin A, Saka MP. Harmony search algorithm based optimum detailed design of reinforced concrete plane frames subject to ACI 318-05 provisions. Comput Struct 2015;147:79–95.
Wang Xiaolei, Gao Xiao-Zhi, Zenger Kai. The harmony search in context with other nature inspired computational algorithms. An introduction to harmony search optimization method. Springer International Publishing; 2015.
Zhai Junchang, Gao Liqun, Li Steven. Robust pole assignment in a specified union region using harmony search algorithm. Neurocomputing 2015.
George James T, Elias Elizabeth. Design of multiplier–less continuously variable bandwidth sharp FIR filters using mod- ified harmony search algorithm. Int J Inform Technol Manage 2015;14(1):5–25.
Ouyang Hai-bin et al. Improved novel global harmony search with a new relaxation method for reliability optimization problems. Inform Sci 2015;305:14–55.
Wang Youwei et al. Novel feature selection method based on harmony search for email classification. Knowl-Based Syst 2015;73:311–23.
Tarkeshwar Mahto, Mukherjee Vivekananda. Quasi-opposi- tional harmony search algorithm and fuzzy logic controller for
load frequency stabilisation of an isolated hybrid power system. IET Gener, Transm Distrib 2015;9(5):427–44.
Geem ZW. Optimal cost design of water distribution networks using harmony search. Eng Optim 2006;38:259–80.
Ayvaz MT. Application of harmony search algorithm to the solution of ground water management models. Adv Water Resour 2008;32(6):916–24.
Ayvaz MT. Identification of ground water parameter structure using harmony search algorithm. Stud Comput Intell 2009;191:129–40.
Geem ZW, Tseng CL, Williams JC. Harmony search algorithms for water and environmental systems. Stud Comput Intell 2009;191:113–27.
Geem ZW. Harmony search optimisation to the pump-included water distribution network design. Civ Eng Environ Syst 2009; 26(3):211–21.
Ayvaz MT. Solution of ground water management problems using harmony search algorithm. Stud Comput Intell 2010;270:111–22.
Cisty M. Application of the harmony search optimization in irrigation. Stud Comput Intell 2010;270:123–34.
Panchal A. Harmony search optimization for HDR prostate brachy therapy. Rosalind Franklin University of Medicine and Science; 2008.
Panchal A. Harmony search in the rapeutic medical physics. Stud Comput Intell 2009;191:189–203.
Gandhi TK, Chakraborty P, Roy GG, Panigrahi BK. Discrete harmony search based expert model for epileptic seizure detec- tion in electroencephalography. Expert Syst Appl 2012; 39(4):4055–63.
Landa-Torres I, Manjarres D, Salcedo-Sanz S, DelSer J, Gil- Lopez S. A multi-objective grouping harmony search algorithm for the optimal distribution of 24-hour medical emergency units. Expert Syst Appl 2012;40(6):2343–9.
Tangpattanakul P, Meesomboon A, Artrit P. Optimal trajectory of robot manipulator using harmony search algorithms. Stud Comput Intell 2010;270:23–36.
Yazdi E, Azizi V, Haghighat AT. A new biped locomotion involving arms swing based on neural network with harmony search optimizer. In: IEEE international conference on automa- tion and logistics; 2011. p. 18–23.
Xu H, Gao XZ, Wang T, Xue K. Harmony search optimization algorithm: application to a reconfigurable mobile robot proto- type. Stud Comput Intell 2011;270:11–22.
Coelho LS, Diego L, Bernert A. A harmony search approach using exponential probability distribution applied to fuzzy logic control optimization. Stud Comput Intell 2010;270:77–88.
Das Sharma K, Chatterjee A, Rakshit A. Design of a hybrid stable adaptive fuzzy controller employing Lyapunov theory and harmony search algorithm. IEEE Trans Contr Syst Tech 2010;18:1440–7.
Vasebi A, Fesanghary M, Bathaeea SMT. Combined heat and power economic dispatch by harmony search algorithm. Int J Electr Power Energy Syst 2007;29(10):713–9.
Coelho LS, Mariani VC. An improved harmony search algo- rithm for power economic load dispatch. Energy Convers Manage 2009;50(10):2522–6.
Ceylan H, Ceylan H. Harmony search algorithm for transport energy demand modeling. Stud Comput Intell 2009;191:163–72.
Geem ZW. Population variance harmony search algorithm to solve optimal power flow with non-smooth cost function. In: Recent advances in harmony search algorithm; 2010a. p. 65–75.
Sinsupan N, Leeton U, Kulworawanichpong T. Application of harmony search to optimal power flow problems. In: Interna- tional conference on advances in energy engineering; 2010. p. 219–22.
Gao XZ, Jokinen T, Wang XL, Ovaska SJ, Arkkio A. A new harmony search method in optimal wind generator design. In:



XIX international conference on electrical machines; 2010.
p. 1–6.
Ceylan O, Dag H, Ozdemir A. Harmony search method based parallel contingency analysis. In: International conference on power system technology; 2010. p. 1–6.
Coelho LS, Bernert LA, Mariani VC. Chaotic differential harmony search algorithm applied to power economic dispatch of generators with multiple fuel options. In: IEEE congress on evolutionary computation; 2010b. p. 1–5.
Sui J, Yang L, Zhu Z, Hua Z. Mine ventilation optimization design based on improved harmony search. In: International conference on information engineering, vol. 1; 2010. p. 67–70.
Sivasubramani S, Swarup KS. Environmental/economic dispatch using multi-objective harmony search algorithm. Electr Power Syst Res 2011;81(9):1778–85.
Geem ZW. Discussion on combined heat and power economic dispatch by harmony search algorithm. Int J Electr Power Energy Syst 2011;33(7):1348.
Khorram E, Jaberipour M. Harmony search algorithm for solving combined heat and power economic dispatch problems. Energy Convers Manage 2011;52(2):1550–4.
Pandi VR, Panigrahi BK. Dynamic economic load dispatch using hybrid swarm intelligence based harmony search algo- rithm. Expert Syst Appl 2011;38(7):8509–14.
Sivasubramani S, Swarup KS. Multi-objective harmony search algorithm for optimal power flow problem. Int J Electr Power Energy Syst 2011;33:745–52.
Chatterjee A, Ghoshal SP, Mukherjee V. Solution of combined economic and emission dispatch problems of power systems by an apposition based harmony search algorithm. Int J Electr Power Energy Syst 2011;39(1):9–20.
Afshari S, Aminshahidy B, Pishvaie MR. Application of an improved harmony search algorithm in well placement opti- mization using stream line simulation. J Petrol Sci Eng 2011;78 (3–4):664–78.
Sirjani R, Mohamed A, Shareef H. An improved harmony search algorithm for optimal capacitor placement in radial distribution systems. In: 5th international power engineering and optimization conference; 2011. p. 323–8.
Sirjani R, Mohamed A. Improved harmony search algorithm for optimal placement and sizing of static var compensators in power systems. In: First international conference on informatics and computational intelligence; 2011. p. 295–300.
Sirjani R, Mohamed A, Shareef H. Optimal allocation of shunt var compensators in power systems using a novel global harmony search algorithm. Int J Electr Power Energy Syst 2012;43(1):562–72.
Javaheri H, Goldoost-Soloot R. Locating and sizing of series facts devices using line out age sensitivity factors and harmony search algorithm. In: 2nd international conference on advances in energy engineering, vol. 14; 2012. p. 1445–50.
Mukherjee V. A novel quasi-oppositional harmony search algorithm and fuzzy logic controller for frequency stabilization of an isolated hybrid power system. Int J Electr Power Energy Syst 2015;66:247–61.
Geem ZW. Optimal scheduling of multiple dam system using harmony search algorithm. In: Lecture notes in computer science, vol. 4507; 2007. p. 316–23.
Alexandre E, Cuadra L, Gil-Pita R. Sound classification in hearing aids by the harmony search algorithm. Stud Comput Intell 2009;191:173–88.
Geem ZW. Multi objective optimization of time-cost trade-off using harmony search. J Constr Eng Manage 2010;136(6): 711–6.
Wang L, Pan QK, Tasgetiren MF. Minimizing the total flow time in a flow shop with blocking by using hybrid harmony search algorithms. Expert Syst Appl 2010;37(12):7929–36.
Diao R. Two new approaches to feature selection with harmony search. In: IEEE international conference on fuzzy systems, Aberystwyth, UK; 2010. p. 1–7.
Cobos C, Andrade J, Constain W, Mendoza M, Leon E. Web document clustering based on global-best harmony search, K- means, frequent term sets and Bayesian information criterion. In: IEEE congress on evolutionary computation; 2010. p. 1–8.
Sarvari H, Khairdoost N, Fetanat A. Harmony search algorithm for simultaneous clustering and feature selection. In: Interna- tional conference of soft computing and pattern recognition; 2010. p. 202–7.
Hoang DC, Yadav P, Kumar R, Panda SK. A robust harmony search algorithm based clustering protocol for wireless sensor networks. In: IEEE international conference on communications workshops, Singapore; 2010. p. 1–5.
Alia OM, Mandava R, Aziz ME. A hybrid harmony search algorithm to MRI brain segmentation. In: IEEE international conference on cognitive informatics, Malaysia; 2010. p. 712–21.
Mandava R, Alia OM, Wei BC, Ramachandram D, Aziz ME, Shuaib IL. Osteosarcoma segmentation in MRI using dynamic harmony search based clustering. In: International conference of soft computing and pattern recognition, Malaysia; 2010.
p. 423–9.
Forsati R, Mahdavi M. Web text mining using harmony search. Stud Comput Intell 2010;270:51–64.
Kaizhou G, Quanke P, Junqing L, Yongzheng H. A novel grouping harmony search algorithm for the no-wait flow shop scheduling problems with total flow time criteria. In: Interna- tional symposium on computer communication control and automation, vol. 1; 2010. p. 77–80.
Gao KZ, Li H, Pan QK, Li JQ, Liang JJ. Hybrid heuristics based on harmony search to minimize total flow time in no-wait flow shop. In: Chinese control and decision conference, China; 2010a.
p. 1184–8.
Han YY, Pan QK, Liang JJ, Li J. A hybrid discrete harmony search algorithm for blocking flow shop scheduling. In: IEEE international conference on bio-inspired computing: theories and applications; 2010. p. 435–8.
Yadav P, Kumar R, Panda SK, Chang CS. An improved harmony search algorithm for optimal scheduling of the diesel generators in oil rig platforms. Energy Convers Manage 2011; 52(2):893–902.
Wang L, Pan QK, Tasgetiren MF. A hybrid harmony search algorithm for the blocking permutation flow shop scheduling problem. Comput Ind Eng 2011;61(1):76–83.
Ayachi I, Kammarti R, Ksouri M, Borne P. Application of harmony search to container storage location. In: IEEE inter- national conference on systems, man, and cybernetics, France; 2011. p. 1556–61.
Ramos CCO, Souza AN, Chiachia G, Falca˜ o AX, Papa JP. A novel algorithm for feature selection using harmony search and its application for non-technical losses detection. Comput Electr Eng 2011;37(6):886–94.
Navi SP, Asgarian E, Moeinzadeh H, Chahkandi V. Using harmony search for solving a typical bioinformatics problem. In: International conference on informatics and computational intelligence; 2011. p. 18–20.
Chandran LP, Nazeer KAA. An improved clustering algorithm based on K-means and harmony search optimization. In: IEEE recent advances in intelligent computational systems; 2011. p. 447–50.
Ahmed AM, Bakar AA, Hamdan AR. Harmony search algorithm for optimal word size in symbolic time series repre- sentation. In: Conference on data mining and optimization, Malaysia; 2011. p. 57–62.
Yusof UK, Budiarto R, Deris S. Harmony search algorithm for flexible manufacturing system (FMS) machine loading problem.



In: Conference on data mining and optimization, Malaysia; 2011. p. 26–31.
Ko KE, Sim KB. An EEG signals classification system using optimized adaptive neuro-fuzzy inference model based on harmony search algorithm. In: International conference on control, automation and systems, Korea; 2011. p. 1457–61.
Li J, Pan Q, Xie S, Gao K, Wang Y. An effective discrete harmony search for solving bi-criteria FJSP. In: Chinese control and decision conference, China; 2011. p. 3625–9.
Pan QK, Wang L, Gao L. A chaotic harmony search algorithm for the flow shop scheduling problem with limited buffers. Appl Soft Comput 2011;11(8):5270–80.
Pan QK, Suganthan PN, Liang JJ, Tasgetiren MF. A local-best harmony search algorithm with dynamic sub-harmony memories for lot-streaming flow shop scheduling problem. Expert Syst Appl 2011;38(4):3252–9.
Ren WJ, Duan JH, Zhang FR, Han HY, Zhang M. Harmony search algorithms for bi-criteria no-idle permutation flow shop scheduling problem. In: Chinese control and decision conference, China; 2011. p. 2513–7.
Fu F, Zhang C. A modified harmony search for multi-mode resource constrained project scheduling problem. In: Interna- tional symposium on computational intelligence and design, China, vol. 1; 2011. p. 181–4.
Jing C, Guang-Liang L, Ran L. Discrete harmony search algorithm for identical parallel machine scheduling problem. In: Chinese control and decision conference; 2011.
p. 5457–61.
Peiying H, Dandan W, Xiaoping L. An improved harmony search algorithm for blocking job shop to minimize make span. In: International conference on computer supported cooperative work in design; 2012. p. 763–8.
Diao R, Shen Q. Feature selection with harmony search. IEEE Trans Syst Man Cybern Part B: Cybern 2012:1–15.
Krishnaveni V, Arumugam G. A novel enhanced bio-inspired harmony search algorithm for clustering. In: International conference on recent advances in computing and software systems; 2012. p. 7–12.
Ezhilarasi GA, Swarup KS. Network partitioning using har- mony search and equivalencing for distributed computing. J Parallel Distrib Comput 2012;72(8):936–43.
Li Y, Chen J, Liu R, Wu J. A spectral clustering-based adaptive hybrid multi-objective harmony search algorithm for community detection. In: IEEE congress on evolutionary computation, Brisbane; 2012. p. 1–8.
Hua J, Yun B, Zheng L, Yanxiu L. A hybrid algorithm of harmony search and simulated annealing for multiprocessor task scheduling. In: International conference on systems and infor- matics; 2012. p. 718–20.
Ahmad I, Mohammad GH, Salman AA, Hamdan SA. Broadcast scheduling in packet radio networks using harmony search algorithm. Expert Syst Appl 2012;39:1526–35.
Habib S, Rahmatia A, Hajipoura V, Niakib STA. A soft computing pareto based meta-heuristic algorithm for a multi- objective multi-server facility location problem. Appl Soft Comput 2013;13(4):1728–40.
Salcedo-Sanz S et al. A coral reefs optimization algorithm with harmony search operators for accurate wind speed prediction. Renew Energy 2015;75:93–101.
Gao Kai Zhou et al. An effective discrete harmony search algorithm for flexible job shop scheduling problem with fuzzy processing time. Int J Prod Res ahead-of-print 2015:1–16.
Geem ZW. Harmony search algorithm for solving sudoku. In: Knowledge-based intelligent information and engineering sys- tems, vol. 4692; 2007. p. 371–8.
Geem ZW, Choi JY. Music composition using harmony search algorithm. In: Lecture notes in computer science, vol. 4448; 2007.
p. 593–600.
Geem ZW, Williams JC. Ecological optimization using harmony search. In: Proceedings of American conference on applied mathematics, Cambridge; 2008.
Fourie J, Mills S, Green R. Visual tracking using harmony search. In: International conference on image and vision com- puting, New Zealand; 2008. p. 1–6.
Mun S, Geem ZW. Determination of viscoelastic and damage properties of hot mix asphalt concrete using a harmony search algorithm. Mech Mater 2009;41(3):339–53.
Mun S, Geem ZW. Determination of individual sound power levels of noise sources using a harmony search algorithm. Int J Ind Ergon 2009;39(2):366–70.
Coelho LS, Bernert LA. An improved harmony search algorithm for synchronization of discrete-time chaotic systems. Chaos Soliton Fract 2009;41:2526–32.
Ma J, Liu J, Ren Z. Parameter estimation of poisson mixture with automated model selection through BYY harmony learn- ing. Pattern Recogn 2009;42(11):2659–70.
Zou D, Ge Y, Gao L, Wu P. A novel global harmony search algorithm for chemical equation balancing. In: International conference on computer design and applications, China, vol. 2; 2010a. p. 1–5.
Zou D, Gao L, Li S, Wu J, Wang X. A novel global harmony search algorithm for task assignment problem. J Syst Softw 2010;83(10):1678–88.
Fourie J, Green R, Mills S. An accurate harmony search based algorithm for the blind deconvolution of binary images. In: International conference on audio language and image process- ing, New Zealand; 2010. p. 1117–22.
Mohsen AM, Khader AT, Ramachandram D. An optimization algorithm based on harmony search for RNA secondary structure prediction. Stud Comput Intell 2010;270:163–74.
Cheng P, Yong W. A hybrid simplex harmony search algorithm and its application to model reduction of linear systems. In: Chinese control conference, China; 2010. p. 5272–5.
Bo G, Huang M, Ip WH, Wang X. The application of harmony search in fourth-party logistics routing problems. Stud Comput Intell 2010;270:135–45.
Kattan A, Abdullah R, Salam RA. Harmony search based supervised training of artificial neural networks. In: International conference on intelligent systems, modelling and simulation, Malaysia; 2010. p. 105–10.
Wong WK, Guo ZX. A hybrid intelligent model for medium- term sales forecasting in fashion retail supply chains using extreme learning machine and harmony search algorithm. Int J Prod Econ 2010;128(2):614–24.
Jaberipour M, Khorram E. Solving the sum-of-ratios problems by a harmony search algorithm. J Comput Appl Math 2010; 234(3):733–42.
Wang L, Pan QK, Tasgetiren MF. Harmony filter: a robust visual tracking system using the improved harmony search algorithm. Image Vis Comput 2010;28(12):1702–16.
Huang M, Bo G, Wang X, Ip WH. The optimization of routing in fourth-party logistics with soft time windows using harmony search. In: Sixth international conference on natural computa- tion, vol. 8; 2010. p. 4344–8.
Zou D, Gao L, Li S, Wu J. Solving 0–1 Knapsack problem by a novel global harmony search algorithm. Appl Soft Comput 2011;11(2):1556–64.
Kayhan AH, Korkmaz KA, Irfanoglu A. Selecting and scaling real ground motion records using harmony search algorithm. Soil Dyn Earthq Eng 2011;31(7):941–53.
Wang FR, Wang W, Yang HQ, Zuo FC. A novel binary harmony search algorithm for intelligent test-sheet composition. In: International conference on electrical and control engineer- ing; 2011b. p. 6213–6.
Kulluk S, Ozbakir L, Baykasoglu A. Self-adaptive global best harmony search algorithm for training neural networks. In:



World conference on information technology, vol. 3 ;2011. p. 282–6.
Kattan A, Abdullah R. An enhanced parallel and distributed implementation of the harmony search based supervised training of artificial neural networks. In: International conference on computational intelligence, communication systems and net- works; 2011. p. 275–80.
Alsewari AA, Zamli KZ. Interaction test data generation using harmony search algorithm. In: IEEE symposium on industrial electronics and applications; 2011. p. 559–64.
Taleizadeh AA, Niaki STA, Seyedjavadi SMH. Multi-product multi-chance-constraint stochastic inventory control problem with dynamic demand and partial back-ordering: a harmony search algorithm. J Manuf Syst 2012;31(2):204–13.
	Landa-Torres I, Ortiz-Garcia EG, Salcedo-Sanz S, Segovia MJ, Gil-Lopez S, Miranda M, et al. Evaluating the internationaliza- tion success of companies using a hybrid grouping harmony search—extreme learning machine approach. IEEE J Sel Top Signal Process 2012;6(4):388–98.
Kulluk S, Ozbakir L, Baykasoglu A. Training neural networks with harmony search algorithms for classification problems. Eng Appl Artif Intell 2012;25(1):11–9.
Salcedo-Sanz S, Manjarres D, Pastor-Sanchez A, DelSer J, Portilla-Figueras JA, Gil-Lopez S. One-way urban traffic recon- figuration using a multi-objective harmony search approach. Expert Syst Appl 2013;40(9):3341–50.
Garcı´ a-Torres Jose´ M et al. A case study of innovative population-based algorithms in 3D modeling: artificial bee colony, biogeography-based optimization, harmony search. Expert Syst Appl 2014;41(4):1750–62.
Plasencia Manuel et al. Geothermal model calibration using a global minimization algorithm based on finding saddle points and minima of the objective function. Comput Geosci 2014;65:110–7.
Turky Ayad Mashaan, Abdullah Salwani. A multi-population harmony search algorithm with external archive for dynamic optimization problems. Inform Sci 2014;272:84–95.
Valian Ehsan, Tavakoli Saeed, Mohanna Shahram. An intelli- gent global harmony search approach to continuous optimiza- tion problems. Appl Math Comput 2014;232:670–84.
Yuan Xiaofang et al. Hybrid parallel chaos optimization algorithm with harmony search algorithm. Appl Soft Comput 2014;17:12–22.
Kong Xiangyong et al. Solving large-scale multidimensional knapsack problems with a new binary harmony search algo- rithm. Comput Oper Res 2015;63:7.
Kong Xiangyong et al. A simplified binary harmony search algorithm for large scale 0–1 knapsack problems. Expert Syst Appl 2015;42(12):5337–55.
Go¨ kc¸e S erife, Tamer Ayvaz M. Evaluation of harmony search and differential evolution optimization algorithms on solving the booster station optimization problems in water distribution networks. Recent advances in swarm intelligence and evolution- ary computation. Springer International Publishing; 2015.
Gupta Chhavi, Jain Sanjeev. New approach for function optimization: amended harmony search. Advances in intelligent informatics. Springer International Publishing; 2015.
Salman Ayed A, Omran Mahamed G, Ahmad Imtiaz. Adaptive probabilistic harmony search for binary optimization problems. Memetic Comput 2015:1–26.
Pao YH. Adaptive pattern recognition and neural networks. Addison-Wesley Pub; 1989.
Pao YH, Takefuji Y. Functional-link net computing: theory, system architecture, and functionalities. Computer 1992;25: 76–9.
Patra JC, Kot AC. Nonlinear dynamic system identification using Chebyshev functional link artificial neural networks. IEEE Trans Syst, Man, Cybernet, Part B: Cybernet 2002;32:505–11.
Klaseen M, Pao YH. The functional link net in structural pattern recognition. In: TENCON 90. IEEE region 10 conference on computer and communication systems, vol. 2; 1990. p. 567–71.
Park GH, Pao YH. Unconstrained word-based approach for off- line script recognition using density-based random-vector func- tional-link net. Neurocomputing 2000;31:45–65.
Liu LM, Manry MT, Amar F, Dawson MS, Fung AK. Image classification in remote sensing using functional link neural networks. In: Proceedings of the IEEE southwest symposium on image analysis and interpretation; 1994. p. 54–8.
Raghu PP, Poongodi R, Yegnanarayana B. A combined neural network approach for texture classification. Neural Networks 1995;8(6):975–87.
Abu-Mahfouz I-A. A comparative study of three artificial neural networks for the detection and classification of gear faults. Int J Gen Syst 2005;34:261–77.
Patra JC, Pal NR. A functional link artificial neural network for adaptive channel equalization. Signal Process 1995;43:181–95.
Teeter J, Mo-Yuen C. Application of functional link neural network to HVAC thermal dynamic system identification. IEEE Trans Ind Electron 1998;45:170–6.
Abbas HM. System identification using optimally designed functional link networks via a fast orthogonal search technique. J Comput 2009;4(2):147–53.
Nanda SJ, Panda G, Majhi B, Tah P. Improved identification of nonlinear MIMO plants using new hybrid FLANN-AIS model. In: IEEE international conference on advance computing, 2009, IACC 2009; 2009. p. 141–6.
Patra JC, Bornand C. Nonlinear dynamic system identification using Legendre neural network. In: The 2010 international joint conference on neural networks (IJCNN); 2010. p. 1–7.
Emrani S, Salehizadeh SMA, Dirafzoon A, Menhaj MB. Individual particle optimized functional link neural network for real time identification of nonlinear dynamic systems. In: 5th IEEE conference on industrial electronics and applications (ICIEA); 2010. p. 35–40.
Majhi R, Panda G, Sahoo G. Development and performance evaluation of FLANN based model for forecasting of stock markets. Expert Syst Appl 2009;36:6800–8.
Mahdavi M, Fesanghary M, Damangir E. An improved harmony search algorithm for solving optimization problems. Appl Math Comput 2007;188:1567–79.
Haykin S. Neural networks: a comprehensive foundation. 3rd ed. Upper Saddle River (NJ, USA): Prentice-Hall, Inc.; 2007.
Omran MG, Mahdavi M. Global-best harmony search. Appl Math Comput 2008;198(2):643–56.
Wang C-M, Huang Y-F. Self-adaptive harmony search algo- rithm for optimization. Expert Syst Appl 2010;37:2826–37.
Bache K, Lichman M. UCI machine learning repository. Irvine (CA): University of California, School of Information and Computer Science; 2013, <http://archive.ics.uci.edu/ml>.
Alcala´ -Fdez J, Fernandez A, Luengo J, Derrac J, Garcı´ a S, Sa´ nchez L, et al. KEEL data-mining software tool: data set repository, integration of algorithms and experimental analysis framework. J Multiple-Valued Logic Soft Comput 2011;17(2–3): 255–87.
Larson S. The shrinkage of the coefficient of multiple correla- tion. J Educ Psychol 1931;22:45–55.
Mosteller F, Turkey JW. Data analysis, including statistics. In: Handbook of social psychology. Reading (MA): Addison- Wesley; 1968.
Demsar J. Statistical comparisons of classifiers over multiple data sets. J Mach Learn Res 2006;7:1–30.
Friedman M. The use of ranks to avoid the assumption of normality implicit in the analysis of variance. J Am Stat Assoc 1937;32:675–701.
Friedman MA. comparison of alternative tests of significance for the problem of m rankings. Ann Math Stat 1940;11:86–92.



Luengo J, Garcia S, Herrera F. A study on the use of statistical tests for experimentation with neural networks: analysis of parametric test conditions and non-parametric tests. Expert Syst Appl 2009;36:7798–808.
Garcia S, Fernandez A, Luengo J, Herrera F. Advanced nonparametric tests for multiple comparisons in the design of experiments in computational intelligence and data mining: experimental analysis of power. Inform Sci 2010;180:2044–64.
Holm. A simple sequentially rejective multiple test procedure. Scand J Stat 1979;6:65–70.
Hochberg Y. A sharper Bonferroni procedure for multiple tests of significance. Biometrika 1988;75:800–3.
Tukey JW. Comparing individual means in the analysis of variance. Biometrics 1949;5:99–114.
Dunnett CW. A multiple comparison procedure for comparing several treatments with a control. J Am Stat Assoc 1980;50:1096–121.
Fisher RA. Statistical methods and scientific inference. 2nd ed. University of Michigan (New York): Hafner Publishing Co.; 1959.
