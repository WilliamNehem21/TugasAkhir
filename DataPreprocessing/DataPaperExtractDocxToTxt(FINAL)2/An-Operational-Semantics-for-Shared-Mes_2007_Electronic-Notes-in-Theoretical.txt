Electronic Notes in Theoretical Computer Science 192 (2007) 77–92	
www.elsevier.com/locate/entcs

An Operational Semantics for Shared Messaging Communication
Astrid Kiehn
Department of Computer Science and Engineering Indian Institute of Technology Delhi
New Delhi 110016, India

Abstract
Shared Messaging Communication (SMC) has been introduced in [9] as a model of communication which reduces communication costs (both in terms of communication latency and memory usage) by allowing tasks to communicate data through special shared memory regions. Sending a reference to an otherwise inaccessible memory regions rather than the data itself, the model combines the advantages of message
passing and shared memories. Experimental results have shown that SMC in case of large data payloads clearly outperforms the classical message passing.
In this paper we give a formal operational semantics to SMC exhibiting unambiguously the effect of executing an SMC command on local and shared memories. Based on this semantics we show that any program using message passing can be proved to be weakly bisimilar to one based on SMC and that with respect to communication costs the latter is amortised cheaper, [7].
Keywords: Shared messaging communication, operational semantics, cost evaluation, message-passing.

Introduction
To achieve high performance, modern computer applications are executed on net- works of (multi)processors. Those with a high data rate like digital signal processing are most efficiently implemented on micro-architectures employing shared memory as a means of interprocess communication. However, shared memory programming has to deal explicitly with correct data access and data integrity and the negligence of these cause faulty computations. Architectures based on message-passing pre- vent such errors by barring shared address space and by their clear separation of computation and communication. But message-passing has the drawback of high data latency and redundancy of data transfer.
Shared messaging communication (SMC) aims at combining the advantages of message passing while utilizing the availability of shared memory to reduce the cost of communication. In an SMC-architecture communication is performed via references (called tokens) to shared memory which are provided by the network and which ensure mutually exclusive access. To communicate, a process asks for a

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.08.021

token (by get unused memory), writes its data to the granted memory region and then sends the token to the target node (send token) by message-passing. On the receiving side, the process receives the token (receive token), reads or writes on the assigned memory region and then either sends the token to another node or releases it (usage over). In general, when the volume of data transferred is very high, compared to the expense of granting, releasing, sending or receiving a token, the overall performance of shared-messaging communication can be significantly better than direct data communication.
The SMC-model of communication 1 has been introduced in [9]. It describes the semantics of the SMC communication primitives by the pre- and postconditions in Figure 1 and reports experimental results showing that SMC outperforms message- passing if large data items are to be communicated.
SMC supports a system level design where certain features of a class of imple- mentations has been abstracted to a level which is amenable to a formal verification (cf. [6]). Employing SMC one goes beyond the purely qualitative behaviour de- scription. A current program using SMC as model of communication does not only specify the (qualitative) computation with respect to functionality but also makes (quantitative) assumptions by taking the decision that the underlying mode of data transfer should rely on shared memory.
The contribution of this paper is a formal operational semantics for a class of concurrent programs of which the set of constituting sequential components is partitioned into regions and where communication within a region is performed via SMC. The partitioning into regions mirrors the (quantitative) assumption of an expected efficient implementation. The use of SMC as a model of communication guarantees that conceptually it can be considered as message passing (which is formally proved in this paper).
The semantics is based on the clear separation between local and shared memory and shows how tokens are administered and dealt with when sent to or received by other processes of the same region. Based on this semantics we are able to prove the claim made in [9] that the new SMC model of computation does not introduce non- determinism due to the shared memory region allocation by the underlying network. In particular, we show that any program based on message-passing can be rewritten to one based on SMC such that the two programs are observation equivalent ([8]). Moreover, we show that if communication costs are taken into account the message- passing program is (amortised) more expensive than the SMC-based counterpart underpinning the experimental results of [9]. The latter comparison is based on the notion of amortised bisimilarity introduced in [7]. Other bisimulation-based behavioural preorders like the efficiency preorder of [10] and [2] turned out not to be capable of expressing this relation adequately.




1 The variant for broadcasting given in [9] is not dealt with in this paper.


Fig. 1. Semantics of SMC-commands as given in [9].
Shared Messaging Communication (SMC)
An SMC-program consists of a finite set of sequential programs partitioned into so-called regions. The processes of a region – which in analogy to networks we call nodes – communicate with each other using the SMC communication primitives. Intuitively, a region comprises those nodes which in the final mapping to a network should be located such that they can efficiently communicate via shared memories. We right here make the assumption that we deal with one universal region, only, to keep the notational overhead as small as possible. The general case will be discussed in the conclusions.
To describe the behaviour of a single node we use the core commands of an imperative language and intersperse them with the SMC communication primitives. So an SMC-program (assuming one global region) is a set of sequential programs p1,... , pn derived from the following grammar:

pi :== skip | l := a | pi; pi | if b then pi else pi | while b do pi |
gumi(t, k) | uoi(t, k) | stj(t, k) | rti (t, k) | cpsi(t, a) | csmi(t, l)
i	j

Each node operates on its private memory space while interprocess commu- nication is based on SMC which affects the shared memory of the entire region. Accordingly, the operational semantics is given in two layers. First we define the semantics of single nodes and based on that the semantics of regions is given.
The Operational Semantics of Nodes
The semantics of the commands in the first line of the grammar for SMC-programs is completely standard (see e.g. [11]) and we assume familiarity with its presentation by a set of rules. In notation we use a for an arithmetic expression, b for a boolean

expression and l ∈ Loci for a location. As in our context the computation of the values of these expressions is of no interest, we adopt a big step semantics here. We assume a relation ~ which evaluates an arithmetic expression a to a (non- specified) value v and a boolean expression b to true or false wrt a given assignment of locations σ: a, σ ~ v and b, σ ~ true or b, σ ~ false .
The state of a node is given by a pair ⟨pi, σi⟩ where pi is the program that remains to be executed and σi is a mapping providing the actual values of locations. We give a small step semantics here to clearly demonstrate the changes of local and shared memory with a computation step. To simplify the behaviour analysis later on we decorate transitions with the assignments or evaluation that is performed with an execution step.




skip
⟨skip, σi⟩ −→ σi
a, σi ~ v

⟨l := a, σi⟩ l—:=→v σi{l := v}



α	'	'	α	'
⟨pi, σi⟩ −→ ⟨pi, σi⟩	⟨pi, σi⟩ −→ σi
α
⟨pi; qi, σi⟩ −α→ ⟨p' ; qi, σ'⟩	⟨pi; qi, σi⟩ −→ ⟨qi, σ'⟩
i	i	i




⟨if b then p
b, σi ~ true
else q ,σ ⟩ b=−t→rue ⟨p ,σ ⟩
b, σi ~ false
b=false

i	i	i
i	i	⟨if b then pi else qi, σi⟩
−→ ⟨qi, σi⟩



b, σi ~ true

⟨while b do p ,σ ⟩ b=−t→rue ⟨p ; while b do p ,σ ⟩
b, σi ~ false

b=false

i	i	i
i	i	⟨while b then pi, σi⟩
−→ σi

Token handles form a third syntactic category and are ranged over by t. By Thdlsi we denote the token handles of node πi. The SMC-commands intuitively have the following meaning where we assume the view of a node i:
gumi(t)	give unused memory (to be bound to t) uoi(t)	usage over (of the memory given by t) stj(t)	send token to node j
rti (t)	receive token from node j
cpsi(t, a)	compose token:
write the value of a to the memory specified by t
csmi(t, l)	consume token:
write the contents of the memory specified by t to location l.



GUMi
⟨gumi(t), σi⟩

gumi(m)
−→

σi{t := m}
(σi(t)= ⊥)



UOi
⟨uoi(t), σi⟩

uoi(m)
−→ σi{t := ⊥}
(σi(t)= m)



STj

⟨stj(t), σi⟩

stj(m)
—i → σi{t := ⊥}
(σi(t)= m)



RTi

⟨rti(t), σi⟩

rti(m)
−→ σi{t := m}
(σi(t)= ⊥)



CPSi
a, σi ~ v
cps (m,v)
(σi(t)= m)

i
⟨cpsi(t, a), σi⟩	−→	σi


CSMi
⟨csmi(t, l), σi⟩

csmi(m,v,l)
−→	σi{l := v}
(σi(t)= m)


Fig. 2. Node Rules
Formally, a node is a pair 2 πi = ⟨pi, σi⟩ where
σi : (Loci ∪ Thdlsi ) → (Values ∪ {⊥} ∪ SM )
σi(Loci) ⊆ Values ∪ {⊥} and σi(Thdlsi ) ⊆ SM ∪ {⊥}. The symbol ⊥ denotes undefinedness of a local location or an SMC location.
As we have left expressions and locations unspecified one may also assume con- current assignments and by this we may assume non-scalar data structures. This modelling admittedly dilutes a crucial property of SMC, namely, that not all the data written to the shared memory by a source process will not necessarily be ac- cessed by the target process (by csm). However, as the emphasis of our modelling is the administration of tokens we do not give a refined model here. The model as described in [9] assumes tokens of a predefined size (which we assume to be one).
The state of the shared memory is given by σ. It is not residing at a particular node but somewhere in the region. It will only be visible when we describe the semantics of an entire region. The remaining rules required to describe the semantics of nodes are the rules for SMC-commands, Figure 2.
Some comments on the rules:
Rule CPSi: With the execution of cpsi(t, a) the shared memory cell m bound to t

2 If a program has completely been executed the semantics simply yield a state σi. We identify this with the pair ⟨nil, σi⟩ to ensure that we can always assume the result has two components.

will be updated to σi(a) but since m is not local to π this updating is visible only in the context of its region.
Rules U Oi, STj: The effect of executing either uoi(t) or stj(t) is the same, locally.
i	i
Rule CSMi: The side condition of this rule expresses that a token handle has been
assigned a shared memory location.
Regions
A region consists of a set of nodes {π1,... , πn} which share a local memory SM . For each (unidirectional) channel from a node i to a node j of the region there is

a channel variable cj
in C = {cj
| i, j ∈ {1,... , n},i /= j}. For simplicity, again,

we assume that the channel capacity is one token item only and we do not consider channels leading outside the region. Channels also act as buffers (the token-buffers mentioned in Figure 1); if the capacity is bounded the data rate of the sending process can be forced to respect the processing time of the receiver. This is a characterizing feature of blocking networks. Assuming capacity 1 the channel can simply be described as a variable which eases the overall needed notation.
Formally, a region R is given by  R = ⟨(π1  ···  πn), σ⟩ where σ provides the current state of SM and the channel variables: σ : SM ∪ C → Values ∪ {⊥} ∪
{available }∪ SM ∪ {empty } satisfying σ(SM ) ⊆ Values ∪ {⊥} ∪ {available } and
σ(C) ⊆ SM ∪ {empty }. The set of all state mappings σ is given by ΣSM .
The operational semantics is such that a memory cell m is given the value available by σ if m is not bound to a token handle or contained in one of the channels. For a given region with n nodes, the state mapping σ and its local counterparts σi should be consistent for all nodes i, j, i /= j:
σi(t)= m implies σ(m) /= available
σ(cj)= m implies σi(t) /= m /= σj(t') and σ(m) /= available
σi(t) /= ⊥ /= σj(t') implies σi(t) /= σj(t')
where t and t' are any token handles, m ∈ SM .
Consistency expresses the key property of SMC: at any state of the execution of a program a token can be bound at most to one token handle, property (c), and if it is residing in a channel, (b), it is not bound to any handle neither it is available. Obviously, initial programs – which satisfy σi(t)= ⊥ and σ(cj)= empty for all token handles and channels – are consistent, and we will prove later that consistency is preserved by the operational semantics. The transition rules for SMC-commands for regions are given in Figure 3.
Some comments on the rules: Let σ' denote the updated state mapping. In R — GU Mi tokem m is granted to node πi which changes its status from being available to ⊥. In R — U Oi token m is released thus σ'(m)= available . In R — STj

the token is sent to node πj i.e. to the conecting channel cj
provided the latter

is empty. As we assumed channel capacity 1, non-emptyness coincides with that the channel is full. With R — CPSi the updating of m to σi(a) due to the local


π gumi(m)  '

R — GUM
	i	—→	πi	
( σ(m)= )

i
⟨(···	π
·· ·), σ⟩ gumi(m) ⟨(···	π'
·· ·), σ{m := ⊥}⟩
available

i	—→	i

uoi(m) '

R — UOi
πi
uoi(m)
—→ πi
'

⟨(···  πi  ·· ·), σ⟩

πi
—→ ⟨(···  πi  ·· ·), σ{m := available }⟩
stj(m)
—i → π'

R — STj
i
stj(m)	j
(σ(cj )= empty )

⟨(···	πi	·· ·), σ⟩ —i → ⟨(···	π'	·· ·), σ{c
i
:= m}⟩


R — RTj

πj
rtj(m)
rtj(m)
—i → π'

(σ(cj )= m)

⟨(···  πj  ·· ·), σ⟩
—i → ⟨(···	π'
·· ·), σ{cj := empty }⟩

π cpsi(m,v)  '
i	—→	πi

R — CPSi
⟨(···  πi  ·· ·), σ⟩
cps (m,v)
—→	⟨(···	πi
·· ·), σ{m := v}⟩

π csmi(m,v,l)  '

R — CSMi
i	—→	πi
csmi(m,v,l)	'
(σ(m)= v)

⟨(···  πi  ·· ·), σ⟩	—→	⟨(···  πi  ·· ·), σ⟩

Fig. 3. Region Rules

execution of cpsi(t, a) is implemented. Finally, R — CSMi dscribes the access to contents of token m; it does not change the state mapping.
Let PROGSMC denote the set of all shared messaging communication programs (which may be in their initial state or have been executed for some while). Formally, PROGSMC is the least set containing the initial state programs which is closed under transitions. The initial state of a program is given by σ(cj)= empty , σi(l)= ⊥ and
σ(m)= available for all all cj, l, m.
Proposition 2.1 (Well-Deﬁnedness of the Operational Semantics)
All programs in PROGSMC are consistent.
The conditions on SMC-commands in Figure 1 are satisﬁed by the operational semantics.
Proof. of (i) by induction on the length of the transition sequence establishing membership in PROGSMC .
(ii) is easily verified by inspecting the respective transition rules.	 
Note that the given conditions on channels might seem stronger than required in Figure 1, however, this is only due to the fact that we assumed channels of capacity one.

Moving from Message Passing to SMC
For an application with a high data rate it is feasible to employ shared messaging communication in its final implementation. In this section we formally prove that any program using message-passing as communication mechanism can be rewrit- ten as an SMC-program such that the two programs are weakly bisimilar (up to some renaming of communication actions) which shows, in particular, that the (non)deterministic structure of the MP-program is preserved by the translation. We first give a formal description of the message passing model and then relate it in terms of bisimilarity to the SMC-presentation.
The Message-Passing Model
The syntax of programs using message-passing is a straightforward variation of that of SMC-programs:

pi := skip | l := a | if b then pi else pi | while b do pi | pi; pi | smj(a) | rmi (l)
i	j
The intuitive meaning of MP-commands is
smj(a)	send message: send the value of a to node j,
rmi (l)	read/save message from node j at location l .
As before, a region consists of a set of nodes {π1,... , πn} and C = {cj | i, j ∈
{1,... , n},i /= j} is the set of (unidirectional) channels between nodes. We adopt the same simplicifations as for SMC by assuming channel capacity one, no channels leading outside a region and one universal region, only. The current states of the channels are given by mapping σ : C → Values ∪ {⊥} ∪ {empty }.
To describe the operational semantics the rules for SMC-commands are replaced by rules for message passing for nodes and regions, respectively.


SMj
a, σi ~ v
j

RMi	i

i
⟨smj(a), σi⟩
smi (v)
—→ σi
j
⟨rmi(l), σi⟩
rmj(v,l)
—→ σi{l := v}




R — SMj
smj(v)
πi —→i	'

(σ(cj )= empty )

i
⟨(···  πi  ·· ·), σ⟩
smj(v)
—i→ ⟨(···	π'
i
·· ·), σ{cj := v}⟩


R — RMj

πj
rmj(v,l)
rmj(v,l)
—i→  π'
j


v=σ(cj ),
/=empty

⟨(···  πj  ·· ·), σ⟩
—i→  ⟨(···	π'
·· ·), σ{ci
:= empty }⟩

In accordance with the definitions for SMC the set PROGMP denotes the least set containing the initial state MP-programs which is closed under transitions. Ini- tially, σi(cj)= empty for all j.
Weak Bisimilarity of MP- and SMC-programs
The translation of PROGMP into PROGSMC is straightforward. We replace each command
smj(a)	by	gumi(t); cpsi(t, a); stj(t) and
i	i
rmj(l)	by	rtj(t); csmj(t, l); uoj(t)
i	i
where for each node just one token handle is used. This substitution is described by the mapping T which we later on generalize to a relation between states of MP- and SMC-regions. As stated above, an MP-program will be related to their SMC- counterpart by weak bisimilarity. So far we have only considered so-called strong transitions opposed to weak transitions which are transitions preceded and followed by an arbitrary number of internal actions. In our setting internal actions are those concerning the administration of token handles, only. Let Act denote the set of all transition labels occurring in Section 1 and
Act adm = {gumi(t), stj(t), rtj(t), uoj(t) | i, j ≤ n, i /= j},
i	i
Act SMC = Act adm ∪ {cpsi(m, a), csmi(v, l) | i ≤ n, m, a, v, l arbitrary} For β ∈ Act ∪ Act SMC — Act adm weak transitions are given by

p =⇒ p'
if p ⇒—→⇒ p'
where

p ⇒ p' if p →∗ p' and
p → p' if there is α ∈ Act adm such that p —α→ p'.
For the message-passing model, we define
Act MP = {smj(v), rmj(v, l) | i, j ≤ n, i /= j, v, l arbitrary}
i	i
The substitution T described above is extended to relation T in Figure 4. It associates MP-programs with computationally equivalent programs based on SMC. The relation is not one-to-one as the memory cell for a particular token handle is chosen arbitrarily. Note, that by T all m ∈ SM which are in use (that is σ˜(m) /= available ) represent a message in one of the channels. This is due to the

fact that smj
and rmj
are atomic actions. States corresponding to the “halfway

execution” of such an action on the SMC side will be added later.
Restricted to initial MP-programs, T is a function and we use T(Πmp) to denote the unique SMC-program.	The main result of this section is that an




where
⟨(π1,  ...  πn), σ⟩	T	⟨(T (π1),  ...  T (πn)), σ˜⟩

πi = ⟨pi, σi⟩, T (πi)= ⟨T (pi),T (σi)⟩,
T (σi)(l)= σi(l) for l ∈ Loci and T (σi)(t)= ⊥,
⎧⎨ empty	if σ(cj)= empty
⎩ some m such that σ˜(m)= σ(cj ) otherwise
σ˜, T (σ1),... ,T (σn) are consistent
and |{m | σ˜(m) /= available }| = |{cj | σ(cj) /= empty }|.
i	i

Fig. 4. relation T ⊆ PROG MP × PROGSMC

initial program Πmp ∈ PROGMP is weakly bisimilar to T (Πmp). Actually, we establish a stronger result namely that Πmp and T (Πmp) are in the efficiency preorder-relation ≤, [2] (adapted to our setting). Intuitively, this means – apart from their weak bisimilarity – that the message-passing model is more efficient than the SMC-model while the experimental results suggest the converse. However, this is not a contradiction as we neglected the cost associated with sending big data items over channels.

Proposition 3.1 (Efficiency relation between MP- and SMC-programs) Let ΠMP ∈ PROGMP, ΠSMC ∈ PROGSMC .
Then ΠMP ≤ ΠSMC, ΠMP is more efficient than  ΠSMC, if there is a relation
R ⊆ PROGMP × PROGSMC such that for all (Πmp, Πsmc) ∈ R


if Π
γ	'  ,γ ∈ Act then ∃Π'	:Π 
γ	'	and (Π'
, Π'
) ∈ R,

mp —→ Πmp
smj (v)
smc
smc =⇒ Πsmc
cps (m,v)
mp	smc

if Πmp
if Πmp
—i→ Π'
rmj(v,l)
—i→ Π'
then ∃Π'
then ∃Π'
,m : Πsmc
,m : Πsmc
==i =⇒ Π'
csm (m,v,l)
===⇒ Π
and (Π'
and (Π'
'
smc
, Π'
) ∈ R,
) ∈ R,

mp	smc
smc
mp	smc

if Πsmc —→ Π'
then (Πmp, Π'
) ∈ R,

if Π
γ	'	,γ ∈ Act then ∃Π'	:Π	γ
'	and (Π'
, Π'
) ∈ R.

smc —→ Πsmc
cps (m,v)
mp	mp —→ Πmp
smj (v)
mp	smc

if Πsmc
i	'
smc
csmi(m,v,l)
then ∃Π'
: Πmp
—i→ Π'
rmj(v,l)
and (Π'
'
smc
) ∈ R.

if Πsmc
'
smc
then ∃Π'
: Πmp
—i→ Π'
and (Π'
'
smc
) ∈ R.


and (ΠMP , ΠSMC) is contained in R.
Proof. follows directly from the definition of the efficiency preorder given in [2] where we match sm-actions with cps -actions and, respectively, rm -actions with

csm-actions. Actually, what we consider here is an instantiation of a ρ-σ-preorder as defined in [1].	 
By a tedious case analysis which is outlined in the next section one can establish the preorder result for a MP-program and its representation as an SMC-program. As, finally, our aim is to prove that the SMC-program is more cost-efficient than the original MP-program, and the efficiency preorder suggests the opposite result, we do not phrase it as a theorem. However, the following straightforward corollary shows the correctness of our translation.
Theorem 3.2 Let Π= ⟨(p1  ...  pn), σ⟩ be a message passing program and SM a shared memory of size at least n. Then Π is weakly bisimilar to its SMC-translation T(Π) where internal transitions are only those concerning the administration of token handles.
Proof of the Theorem
We define a relation R which satisfies the seven conditions of Proposition 3.1. This proves Πmp ≤ Πsmc. For principal reasons, every pair of processes which satisfy the efficiency preorder ≤ is weakly bisimilar, see [2]. So this proves the theorem. For R we verify case (ii) in detail. From a respective analysis for the other items of Proposition 3.1 it follows Πmp ≤ Πsmc.
We define R to be the least set containing T which is closed under the following conditions:
If (Πmp , Πsmc) ∈ R and Πmp = ⟨(...  ⟨smj(a); pi, σi⟩ .. .), σ⟩,
Πsmc = ⟨(...  ⟨T (smj(a)); T (pi),T (σi)⟩ .. .), σ˜⟩
then
(Πmp,⟨(...  ⟨cpsi(t, a); stj(t); T (pi),T (σi){t := m}⟩  .. .), σ˜{m := ⊥}⟩) ∈ R,
'  ,⟨(...  ⟨stj(t); T (pi),T (σi){t := m}⟩  .. .), σ˜{m := σi(a)}⟩)	∈ R

where	Π'	= ⟨(...  ⟨pi, σi⟩ .. .), σ⟩ and σ˜(m)= available .
If (Πmp , Πsmc) ∈ R and Πmp = ⟨(...  ⟨rmj(l); pj, σj⟩ .. .), σ⟩,
Πsmc	= ⟨(...  ⟨T (rmj(l)); T (pj),T (σj )⟩ .. .), σ˜⟩
then
(Πmp,⟨(...  ⟨csmj(t, l); uoj(t); T (pj ),T (σj){t := m}⟩  .. .), σ˜{cj := empty }⟩) ∈ R,
'  ,⟨(...  ⟨uoj(t); T (pj),T (σj){l := σ(m)}⟩  .. .), σ˜{cj := empty }⟩)	∈ R


where	Π'
= ⟨(...  ⟨pj, σj⟩ .. .), σ⟩ and σ˜(cj)= m.

Lemma 3.3 Let ⟨pi, σi⟩ be an MP-node such that ⟨pi, σi⟩
smj (v)
—i→ ⟨p', σ'⟩. Then there

i	i
are processes q1, q2, q3 and local state mapping σ1, σ2 and σ3 such that
i	i	i	i	i	i


⟨T (pi),T (σi)⟩
gumi(m)
—→
⟨q1, σ1⟩
cps (m,v)
—→	⟨q , σi⟩
stj(m)
—i → ⟨q3, σ3⟩

i	i	i	i	i
and q3 = T (p') and σ3 = T (σi)= T (σ') and m is some shared memory cell.
i	i	i	i
Proof. We show the lemma for pi = smj(a). The general case is easily derived.

As ⟨smj(a), σi⟩
smj (v)
—i→ σi we have σi(a)= v.

Now, for the three transitions
⟨gum (t); cps (t, a); stj(t),T (σ )⟩ gumi(m) ⟨cps (t, a); stj(t), σ1⟩

i	i	i
i	—→	i	i	i

⟨cps (t, a); stj(t), σ1⟩ cpsi(m,v) ⟨stj(t), σ2⟩

i	i	i
j
—→	i	i
stj(m)

⟨st (t), σ2⟩	—i →	σ3.
i	i	i
it is easily proved by inspecting the corresponding transition rules that σ1 =
T (σi){t := m}, σ2 = σ1 and σ3 = σ2{t := ⊥} = T (σi){t := ⊥} = T (σi).	 
i	i	i	i
We show next, that this local transition sequence is also possible in the context of a region provided there is a free shared memory cell available.
Lemma 3.4 Let Πmp = ⟨(...  πi  .. .), σ⟩ ∈ PROGMP and (Πmp, Πsmc) ∈ T. If


⟨(...  πi  .. .), σ⟩
smj (v)
—i→ ⟨(...	π'
.. .), σ'⟩

and m ∈ SM is such that σ(m)= available then
Πsmc = ⟨(T (π1)  ...  T (πi)  ... ...  T (πn)), σ˜⟩
gumi(m)	1	1	1
—→	⟨(...  ⟨qi , σi ⟩,  .. .),σ ⟩
cpsi(m,v)	2	2	2
—→	⟨(...  ⟨qi , σi ⟩,  .. .),σ ⟩
stj(m)
—i → ⟨(...	⟨q3, σ3⟩,	.. .), σ3⟩
i	i
where the qj and σj are given by the previous lemma and σ3 = σ˜{m := v, cj :=
i	i	i
m} and T (σ1),... ,T (σn) and σ3 are consistent.
Proof. The first transition is possible by the choice of m and rule R — GUMi. Thus, σ1 = σ˜{m := ⊥}. The second transition follows from the previous lemma and rule R — CPSi as σ1(t)= m. So, σ2 = σ1{m := v}. The last transition can be deduced
with rule R — STj as its side condition is satisfied for σ2 because σ(cj )= empty –
i	i
otherwise the local smj(v)-transition had been impossible – and the definition of
T. So, σ3 = σ˜{m := v, cj := m}.	 
Lemma 3.5 Let Π ∈ PROGSMC such that each node uses at most one token han- dle. If Π has n nodes and global state mapping σ and n nodes then n ≥ |{m ∈ SM |

σ(m) /= available }|.
Proof. by induction on the length of the transition sequence establishing member- ship in PROGSMC . The crucial rules are R — GUMi and R — UOi in conjunction with the underlying node transitions.	 
Proposition 3.6 Let Πmp = ⟨(...  πi  .. .), σ⟩ ∈ PROGMP and (Πmp, Πsmp) ∈
T. Let, |SM | ≥ n.

If	⟨(...  πi  .. .), σ⟩
smj (v)
—i→ ⟨(...	π'
.. .), σ'⟩

`
Π	= ⟨(...	T (π )	.. .), σ˜⟩	cpsi(m,v)
Π˛'¸	x
⟨(...	T (π')	.. .), σ˜'⟩

then
smc	i
===⇒
`	Πs˛m¸c'	x

and	(Π'
˜'  ) ∈ T where
σ˜' = σ˜{m := v, cj := m}.

Proof. By Lemma 3.5 and Lemma 3.4.	 

Cost Analysis
The discussion in the previous section has shown that a comparison based on match- ing actions, only, does not capture the different performance of systems but rather suggests a relation which on intuitive grounds one would reject. However, if one assigns costs to actions and matches actions while keeping track of the current cost balance, the quantitative performance can also be observed. To this end, amortised bisimilarity has been introduced in [7]. In short, it combines bisimilarity with an quantitative cost evaluation.
The main idea is to consider actions together with their costs and to modify bisimulation equivalence in such a way that actions are matched with ”function- ally equivalent” actions. The difference in their costs adds to the credit which is accumulated during the mutual simulation. For a system p to be considered less ex- pensive than another system q, the amortised bisimulation containing (p, q) should have nonnegative credit everywhere. We are interested in applications where some additinal internal activity – like the administration of tokens in SMC – increases the performance of the system in some way. It is weak amortised bisimilarity that is of interest in this setting as the internal activity corresponds to invisible τ -actions. So we allow a visible action to be simulated by an action which is preceded and followed by a sequence of costly actions which are functionally equivalent to τ . Functional equivalence is given by relation ρ. In general, it is only a small set of actions for which ρ does not reduce to identity and costs are considered different from 0. A

uav
weak transition is a sequence p =⇒ p
where a is a visible action and u and v are

actions which are functionally equivalent to τ , that is u = b1 ... bn, v = d1 ... dm and biρτ , djρτ for all i, j. In short, uρε and vρε. Every action a is equipped with a nonnegative cost ca ∈ N. For words u = u1 ... un we have cu = cu1 + ··· + cun .
Definition 4.1 Let ⟨P, Aτ , —→⟩ be a labelled transition system with transition labels in Aτ = A∪ {τ }, ρ ⊆ Aτ × Aτ and every action a ∈ Aτ carries costs ca (by

definition cτ = 0). A family (Ri)i∈N of binary relations over P is a weak amortised
ρ-bisimulation, if for all i ∈ N, (p, q) ∈ Ri:

a	'	'
uˆbv	'	'	'

p —→ p
implies ∃q , b, u, v [aρb, ερ uv, q =⇒ q
and (p ,q ) ∈ Ri+c ˆ −ca ],

q —b→ q' implies ∃p', a, u, v [aρb, uvρ ε, p =uaˆ⇒v p' and (p', q') ∈ Ri+c −c	],
where a, b ∈ Aτ and u, v ∈ A∗ and aˆ = a if a /= τ and τˆ = ε. Process p is (weakly) amortised cheaper (more cost efficient) than q up to credit i, p ≺ρq, if (p, q) ∈ Ri
for some weak amortised ρ-bisimulation (Ri)i∈N.


The costly actions in our applications which are functionally equivalent to τ are those in Act adm. They carry cost 1. Actions of type cps and csm are mapped to sm and rm, respectively, but to the former we assign cost 0 as they are local read and write operations opposed to the latter channel operations which we give cost 2. One should note here that due to the simplification in the formal modelling of SMC the real benefit of SMC gets diluted. However, the reader should readily see that if large data packets are sent over the channels then the credit in the amortised bisimulation will increase significantly. We also neglect that in case of very small data items the cost of token administration might exceed the cost of simply sending the data by message passing. Naturally, the overall speedup also depends on the ratio of computation to communication. To give some real data, for the application of a JPEG encoder implemented with SMC the overall speedup has been shown
1.77 over the message-passing model, see [9].
The transition system we consider is the disjoint union of the transition systems induced by the operational semantics of MP- and SMC-programs. We define an amortised bisimulation which for each pair (Πsmc, Πmp) ∈ Ri satisfies


if Πsmc  α	'	,α ∈ Act
, then (Π'
, Πmp) ∈ Ri−1,

—→ Πsmc
adm
smc

γ
if Π	'	,γ ∈ Act then there exist Π'	:
smc —→ Πsmc	mp
Π	'	'	'
mp —→ Πmp and (Πsmc, Πmp) ∈ Ri.

if Π
cpsi(m,v) Π'
then there exist Π'
, j,v :

smc
—→
Πmp
smc
smj (v)
—i→ Π'

and (Π'
mp
'  ) ∈ Ri+2.

if Π
csmi(m,v,l)	'	'

smc
—→	Πsmc then there exist Πmp, j, v,l :
rmj(v,l)

Πmp
—i→ Π'
and (Π'
'  ) ∈ Ri+2.

γ
if Π	'  ,γ ∈ Act then there exist u ∈ Act ∗
, Π'	:

mp —→ Πmp
uγ
Π
adm
'	'
smc

smc =⇒ Πsmc and (Πsmc, Πmp) ∈ Ri−cu,
smj (v)

if Πmp
—i→ Π'
then there exist u, w ∈ Act ∗
'
smc
,m :

Π	u cpsi(m,v) w	'	'	'

smc =⇒ —→
=⇒ Πsmc and (Πsmc, Πmp) ∈ Ri+2−cuw ,


if Πmp
rmj(v,l)
—i→ Π'
then there exist u, w ∈ Act ∗

'
smc

,m :

Π	u csmi(m,v,l) w	'	'	'

smc =⇒	—→
=⇒ Πsmc and (Πsmc, Πmp) ∈ Ri+2−cuw .


Note, that (i) – (iv) and (v) – (vii) directly correspond to conditions (iv) – (vii) and
(i) – (iii) given in Proposition 3.1. The two groups are swapped here as we consider a reverse ordering. Since all actions in Act adm are mapped to τ by ρ it suffices to verify conditions (i) – (vii) to prove that (Ri)i∈N is an amortised bisimulation. Using relation R defined in the last section we set up the family (Ri)i∈N:
(Πsmc, Πmp) ∈ Ri if and only if (Πmp, Πsmc) ∈ R and i ≥ D(Πsmc)
where D(Πsmc) is defined as D(Πsmc)= Σn	D(pi) and

⎧⎪ 0 if p cpsi(m,v) or p csmi(m,v)

for some m, v

—→	i	—→
⎨
		
⎪⎩ 1 otherwise
D(pi) gives the maximal credit demand that is necessary to perform the initial token administration. It is easily verified that conditions (i) – (vii) indeed hold. As every initial program Π with n nodes has maximal demand n in its SMC version the following proposition is straightforward.
Theorem 4.2 If Πmp is an initial MP-program with n sequential components and
Πsmc is its SMC-presentation that is Πsmc = T (Πmp) then Πsmc is amortised cheaper
than Πmp with credit n, |SM | ≥ n: T (Π) ≺ρ Π where ρ is deﬁned as after Def. 4.1.
Conclusions
We have given an operational semantics for concurrent programs whose sequential processes are partionened into regions to facilitate an efficient implementation. The processes forming a region are assumed to communicate via shared memory in the final implementation. We have restricted the formal model to one region, only, but the set-up is easily generalized. In that case the whole system is described

by ⟨ ⟨(π1	...	π1 ), σ1⟩,... , ⟨(πm	...	πm ), σm⟩,C ⟩ where each ⟨(πi
... 

1	n1	1	nm	1
i ), σi⟩ is a region as described in Section 2. Mapping C provides the current state
for each inter-region channel. The semantics of inter-region channels is defined as in the section on message-passing and the channel access operations are sm and rm.
SMC-programs support a system design methodology – called orthogonalization of concerns in [6] – which separates functional correctness from other, quantitative aspects. A program using message-passing as model of communication (only) can be proved functionally correct independently of any concern with respect to an efficient transfer of data. Once this has been established the question of performance can be tackled. We have shown in this paper that one may move from message-passing to

shared messaging communication without loosing functional correctness but with higher efficiency if the volume of data transferred is high. SMC may be seen as a mechanism efficiently implementing message-passing for high data-rate applications and as such it goes in line with e.g. FLASH architecture [3], ARACHNE protocol [4] or efficient implementations of value passing languages [5]. However, SMC provides a new communication model transperant to the programmer and this supports portability and reusability of software within a certain class of micro-architectures.

References
S. Arun-Kumar. On bisimilarities induced by relations on actions. In Conference on Software Engineering and Formal Methods. IEEE, 2006.
S. Arun-Kumar and M. Hennessy. An efficiency preorder for processes. Acta Informatica, 1(29):737– 760, 1992.
Kushin et. al. The Stanford FLASH multiprocessor. In Proceedings of the 21st International Symposium on Computer Architecture, 1994.
K. G. W. Goossens. A protocol and memory manager for on-chip communication. In IEEE International Symposium on Circuits and Systems, 2001.
A. Ing´olfsd´ottir and R. Pugliese. Towards verified lazy implementation of concurrent value-passing languages. In W. Brauer, editor, Net Theory and Application, volume 27 of Electronic Notes in Theoretical Computer Science. Elsevier, 1999.
K. Keutzer, S. Malik, R. Newton, J. M. Rabaey, and A. Sangiovanni-Vincentelli. System-level design: orthogonalization of concerns and platform-based design. IEEE Transactions on Computer-Aided Design of Integrated Circuits and Systems, 19(12):1523–1543, 2000.
A. Kiehn and S. Arun-Kumar. Amortised bisimulations. In Proceedings of FORTE 2005, number 3731 in Lecture Notes in Computer Science, pages 320–334. Springer–Verlag, 2005.
R. Milner. Communication and Concurrency. Prentice-Hall, 1989.
Satya Kiran M.N.V., Jayram M.N., Pradeep Rao, and S.K. Nandy. A complexity effective communication model for behavioral modeling of signal processing applications. In Proceedings of 40th Design Automation Conference, 2003.
F. Moller and C. Tofts. Relating processes with respect to speed. In Proceedings of CONCUR 91, number 527 in Lecture Notes in Computer Science, pages 424–438. Springer–Verlag, 1991.
G. Winskel. The Formal Semantics of Programming Languages. MIT-Press, 1993.
