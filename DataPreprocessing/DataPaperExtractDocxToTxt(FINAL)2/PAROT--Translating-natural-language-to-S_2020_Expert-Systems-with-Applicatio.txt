

		


PAROT: Translating natural language to SPARQL
Peter Ochieng
Taita Taveta university, Nairobi, Kenya


a r t i c l e	i n f o	a b s t r a c t

	

Article history:
Received 13 April 2019
Revised 16 February 2020
Accepted 16 February 2020
Available online 21 February 2020

Keywords:
SPARQL
Natural language processing Ontologies
Query
This paper provides a dependency based framework for converting natural language to SPARQL. We present a tool known as PAROT (which echos answers from ontologies) which is able to handle user’s queries that contain compound sentences, negation, scalar adjectives and numbered list. PAROT employs a number of dependency based heuristics to convert user’s queries to user’s triples. The user’s triples are then processed by the lexicon into ontology triples. It is these ontology triples that are used to construct SPARQL queries. From the experiments conducted, PAROT provides state of the art results.
© 2020 The Author. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)





Introduction

In our bid to develop an ontology based chatbot, we envision developing a tool that would allow users to use their natural lan- guage (NL) and have a near natural conversation with a tool which fetches facts (answers) contained in an ontology based knowledge base (KB). This requires us to employ a plugin tool that trans- lates the user’s statements written in NL to SPARQL query lan- guage (SPARQL Working Group, 2013), a W3C recommended lan- guage for querying ontologies. We experimented with various NL to SPARQL tools such as AquaLog (Lopez, Pasin, Motta, Hall, & Keynes, 2005), CASIA@12 (He, Zhang, Liu, & Zhao, 2014), Querix (Kaufmann, Bernstein, & Zumstein, 2006), AutoSPARQL (Lehmann & Bühmann, 2011), K-Extractor (Tatu, Balakrishna, Werner, Erekhin- skaya, & Moldovan, 2016), SPARK (Ferré, 2017) that currently exist in literature in order to select the best tool. The best tool was to be selected based on its precision and recall value (i.e. its ability to fetch correct and all require answers). However, we realized that despite the tools converting a number of user’s queries to the cor- rect SPARQL queries, the tools’ precision and recall values drasti- cally dropped for queries which contained:
Opposing scalar adjectives such as in the query which is the longestand shortestriver that traverses Mississippi ?, (Zhao, Zou, Wang, Yu, & Hu, 2017) estimates that 12% of the total errors in queries generated by their gAnswer tool is due to the fact that it does not support superlatives and comparatives in its imple- mentation. This underscores the importance of handling scalar adjectives.

E-mail address: onexpeters@gmail.com
Negation such as which rivers donotflow through Alaska ? or which river neitherflows through Alaska nor Mississippi ?.
Numbered list such as list fiverivers that flow through Alaska ?
Compound sentences which female actor played in Casablanca and is married to a writer born in Rome ?. (Zhao et al., 2017) es- timates that 9% of the total errors in queries generated by their gAnswer tool is due to the fact that it does not handle queries with unions or filters.
In addition to these weaknesses, most of the state of art tools use techniques that are not able to capture the entire vocabulary of the underlying knowledge base i.e. they don’t generalize the entire knowledge base adequately. This affects the word disambiguation process hence reducing their precision and recall values.
This research addresses the above mentioned key challenges by introducing the following key concepts:
Design a lexicon that that is able:
To fully represent the vocabulary of the underlying knowl- edge base therefore helping in resolving word ambiguities that exist in the user’s query.
To tag adjective entities in the knowledge base with their positive and negative scalars. Through this we are able to resolve the problem of opposing scalar adjectives when con- verting NL to SPARQL.
We develop a number of high coverage syntactic heuristics which can convert different scenarios of possible questions to correct SPARQL queries.
From the evaluation, the developed technique outperforms gAn- swer (Zhao et al., 2017), which was the top performing tool in QALD-9 challenge (Usbeck, Gusmita, Saleem, & Ngomo, 2018a).


https://doi.org/10.1016/j.eswax.2020.100024
2590-1885/© 2020 The Author. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)

This is due its ability to effectively disambiguate words and its high coverage of user questions.

Literature review

In this section, we review state of the art applications that con- verts NL to SPARQL as well as those that convert NL to SQL. We highlight the key techniques used by a tool and discuss whether it can handle the challenges discussed in Section 1. AquaLog (Lopez et al., 2005) is an ontology independent question answer- ing system for the Semantic Web. It is composed of a linguis- tic component to map user query to query triples. It is these query triples that are further processed into an ontology compli- ant triples from where answers are derived. The linguistic compo- nent is composed of the GATE infrastructure (Cunningham, May- nard, Bontcheva, & Tablan, 2001) and resources to annotate the user query. The annotations in the user query include verbs, nouns, tokens etc. The component also employs JAPE grammars which ex- pand annotations embedded by the GATE by identifying terms, re- lations, question indicators (which/who/when, etc.) and patterns or types of questions. AquaLog does not contain components to deal with scalar adjectives, numbered list and compound sen- tences. CASIA@12 (He et al., 2014) is a question answering sys- tem over linked data. After generating a number of possible phrase to semantic item mappings, it then uses Markov logic network (MLN) for disambiguation and finally form a SPARQL query. CA- SIA@12 does not handle scalar adjectives, negation, numbered list and compound sentences. DEANNA (Yahya, Berberich, & Elbassuoni, 2012) also translates a question in NL into a structured query. The key element of DEANNA is the use integer linear program (ILP) to solve the disambiguation of terms to semantic items. Querix (Kaufmann et al., 2006) is a pattern matching ontology indepen- dent natural language interface (NLI). Querix uses the Stanford parser to syntactically analyze the input query. From the syntax tree the query analyzer extracts the sequence of the key word cat- egories such as Noun (N), Verb (V), Preposition (P), Wh-Word (Q), and Conjunction (C). Based on the generated word categories a query skeleton is generated. WordNet is used to supply all syn- onyms to the verbs and nouns in the query. It then matches the skeleton with triples in the ontology. In Querix ambiguities are not resolved automatically rather users are asked for clarifications in a pop-up dialog menu window to disambiguate. Queries has a dis- advantage that it only allows users to write queries starting with which, what, how many, how much, give me or does hence can- not handle questions starting with terms such as “List”. It also does not handle negation, scalar adjectives and extensively relies on WordNet which makes query generation process slow. PANTO (Wang, Xiong, Zhou, & Yu, 2007) utilizes Stanford parser (Klein & Manning, 2003) to generate a parse tree from the user sub- mitted query. It then extracts nominal phrase constituents in the parse trees. The nominal phrases in the parse trees are extracted as pairs to form an inter-mediate representation called QueryTriples. It the utilizes the knowledge in the ontology, to map QueryTriples to OntoTriples which are represented with entities in the ontol- ogy. Finally, together with targets and modifiers extracted from the parse trees, OntoTriples are interpreted as SPARQL. PANTO can handle conjunctions / disjunctions, negation, comparatives and su- perlatives. It however cannot handle opposing scalar adjectives and numbered list. AutoSPARQL (Lehmann & Bühmann, 2011) uses su- pervised machine learning to generate a SPARQL query based on positive i.e. resources which should be in the result set of the SPARQL query, and negative examples, i.e. resources which should not be in the result set of the query. The user can either start with a question as in other QA systems or by directly searching for a relevant resource. He or she can then select an appropriate result, which becomes the first positive example. After that, he is asked a
series of questions on whether a resource should also be contained in the result set. These questions are answered by a yes or a no. This feedback allows the supervised learning method to gradually learn which query to generate. AutoSPARQL faces the challenge of portability to a different Knowledge Base (KB) (Sander, Waltinger, Roshchin, & Runkler, 2014). The effort of learning the positive and negative examples also increases drastically with the size of the KB (Sander et al., 2014). SPARK (Ferré, 2017) is another tool for processing NL keyword to SPARQL. Its output is a ranked list of SPARQL queries. Its key steps include: term mapping, construc- tion of the query graph and query ranking. Ranking of query ap- plies a probabilistic model based on the Bayesian Theorem. Its key challenge involves choosing an option out of the ranked query list since this requires an expert in SPARQL who has knowledge on the underlying KB (Sander et al., 2014). Exploiting the recent success of deep learning, a number of studies introduce deep learning neu- ral network based method to convert NL to structured query lan- guages. Research in (Hao et al., 2017) applies a bidirectional long short term memory(LSTM) (Hochreiter & Schmidhuber, 1997) net- work to convert NL to SPARQL. Bidirectional LSTM is employed to capture the context of a word in relation to both the words be- fore and after it. Their technique exploits the top key words in the submitted user question to extract candidate answers from the knowledge base. The words are then linked to the correct answer tokens by learning their relatedness. WDAqua (Usbeck, Gusmita, Saleem, & Ngomo, 2018b) generates SPARQL query from natural language by employing rule-based combinatorial approach to gen- erate leveraging the semantics encoded in the underlying knowl- edge base. Other state of the art tools for converting NL to SPARQL include TeBaQA (Usbeck et al., 2018b), Elon (Usbeck et al., 2018b) and QASystem (Usbeck et al., 2018b). A complete review of NL to SPARQL tools is discussed in (Bouziane, Bouchiha, Doumi, & Malki, 2015), (Cimiano & Bielefeld, 2011) and (Diefenbach, Both, Singh, & Maret, 2018).
To convert NL to SQL, research in (Iyer, Konstas, Cheung, Krish-
namurthy, & Zettlemoyer, 2017), employs a bidirectional LSTM to generate the best SQL query from a given NL. It applies encoder- decoder model proposed in (Ahmad & Hunt, 2015). In the de- coder, the conditional probability distribution of the SQL token is predicted based on the previous combination of SQL token em- beddings. They also incorporates human feedback to improve the learning process. Other studies that have exploited neural net- works to convert NL to structured language include (Cai et al., 2018), (Yu et al., 2018) and (Gur, Yavuz, Su, & Yan, 2018). One ma- jor challenge with Neural Network based method is that they try to represent the whole knowledge base using a few training exam- ples. This becomes a challenge when the model meets new vocab- ulary that it had not seen in the training data hence affecting the prediction of neural network based. Table 1 gives a summary of an evaluation of selected state of the art tools on whether they can handle negation, opposing scalar adjectives, compound sentences and the key disambiguation technique a tool applies.


PAROT architecture

Step 1: identifying targets

Given a user query submitted in natural language, the first task is to identify targets words from the query. A target (or projec- tion) word is a variable that will be placed directly after SELECT key word in a SPARQL query. To help in identifying target words in a user submitted query, we use a typed dependency parser such as Stanford typed dependency parser (Marneffe & Manning, 2015). The dependency parser provides a simple description of grammat- ical relationships that exists between the words in the user sub-

Table 1
Evaluation of selected NL to structured language tools.


	


Fig. 1. Showing dependency.


mitted query. To extract the target words in the parsed query, we categorize the queries into two categories i.e.
The Wh (WRB, WP, WDT) queries.
The non WH queries.

Targets in Wh based queries
This category is composed of queries which start with Wh (i.e. what, when, where, who, whom, which, whose, why, and how). To identify target words in this category of queries, we apply two key set of rules in equation 1 and 2.
∀wx, wy.(nsubj(wx, wy) ⇒ Target(wy))	(1a)

∀wx, wy, wz.(nsubj(wx, wy) ∧ conj(wy, wz)
⇒ Target(wy) ∧ Target(wz))	(1b)

∀wx, wy.(nsubjpass(wx, wy) ⇒ Target(wy))	(2a)

∀wx, wy, wz.(nsubjpass(wx, wy) ∧ conj(wy, wz)
⇒ Target(wy) ∧ Target(wz))	(2b)
Here, dep(x, y) is a dependency that exists between words x and
y. The words of a sentence compose the constants of a domain over which the functions operate. The rules in (1a) and (1b) apply in a non relation query (see Section 3.2.1 for definition of a rela- tional and non-relational query). They basically identify the nomi- nal subjects in the user submitted query and flags them as targets. Eq. (1a) applies for a query where there is no conjunct relation between the head subject of the query and any other nominal. For example, when a user submits a query such as What is the area of the most populated state ?, using Stanford dependency parser, the dependency diagram in Fig. 1 is generated.
The grounded version of formula (1a) is shown below.
nsubj(is, area) ⇒ Target(area)
Fig. 2. Showing dependency.


The dependency nsubj(is, area) holds between the words is and area. Therefore, the nominal area is selected as the target of the query. If a user submits a query such as What is the area and pop- ulation of the most populated state ?, Stanford dependency viewer generates dependency shown in Fig. 2.
Since in this query the head subject area is connected to an- other nominal by a coordinating conjunction “and”, the formula in Eq. (1b) is applied.
nsubj(is, area) ∧ conj(area, population)
⇒ Target(area) ∧ Target(population)
Both the nominals area and population are selected as targets. This rule also captures scenarios where more than one nominal is con- nected to the head subject by a coordinating conjunction, such as and, or and “,” such as What is the population, area and capital of the most populated state?
The rules in Eq. (2a) and (2b) are applicable in a relational based query. They flag subjects word in a query by identifying the passive nominal subjects in the user submitted query. Eq. (2a) is applicable where the head subject is not connected to any other nominal via a conjunction. For example in the query Which Ger- man actor was killed in a road crash ?, the nominal actor is selected as the target word.
nsubjpass(killed, actor) ⇒ Target(actor)
The rule in Eq. (2b) captures scenarios where one or more nomi- nals are connected to the head subject by a coordinating conjunc- tion, such as and, or and “,” such as in the query Which German actor and musician were killed in a road crash?. The nominals actor and musician are picked as target words.
nsubjpass(killed, actor) ∧ conj(actor, musician)
⇒ Target(actor) ∧ Target(musician)

Targets in non-WH queries
To flag out target words in a non-Wh queries, we use the func- tions in equation 3 and 4. Eq. (3a) and (3b) applies in a non-Wh




Fig. 3. Showing dependency.


query that the direct object is not connected to a preposition such as Give me all the rivers that traverse Mississippi(see dependency di- agram in Fig. 3). The rule in Eq. (3b) applies where the direct ob- ject is connected to one or more nouns via a conjuction (e.g. Give me all rivers and lakes that traverse Mississippi)
∀wx, wy.(dob j(wx, wy ) ⇒ T arget(wy ))	(3a)

∀wx, wy, wz.(dob j(wx, wy ∧ conj(wy, wz )
⇒ T arget(wy ) ∧ T arget(wz ))	(3b)

∀wx, wy.(pob j(wx, wy ) ⇒ T arget(wy ))	(4a)

∀wx, wy, wz.(pob j(wx, wy ∧ conj(wy, wz )
⇒ T arget(wy ) ∧ T arget(wz ))	(4b)
Equation (4a) and (4b) applies in a non-Wh query that the head of a noun phrase folows a preposition. Equation (4a) applies in query such as In which country does the Nile start?. Here, the head noun country is not connected to any noun via a conjuction. Equa- tion (4b) applies to a query where the head noun is connected to one or more nouns via a conjuction such as in the query In which country and continent does the Nile start?. The head noun must be connected to a preposition.

Step 2: identifying user triple pattern

SPARQL query is composed of a set of triple patterns known as graphs patterns. The graphs patterns are placed directly after the WHERE key word or after the target variables in the SPARQL query. The triple patterns are of the form of < subject > < predi- cate > < object > where the subject, predicate and object may be variables (SPARQL Working Group, 2013). The idea therefore in this section is to process a user submitted query to identify potential triples that will be used to construct the SPARQL graphs. Triples identified from the user query are referred here as user triples. To identify user triples from the submitted query, we categorize it into either:
Relational phrase based query.
Non-relational phrase based query.

Identifying user triple pattern in a relation based user query
Relation based user query is a query which contains at least a relational phrase linking two nominals. A relation phrase can be a transitive verb (e.g. in the query “which rivers traverse Alaska?’’, traverse is a relational phrase) or intransitive verb followed with prepositional complement (e.g. “which river flows through Alaska?’’, flows through is a relational phrase). Therefore, a relational phrase linking two nominals may be a verb, a verb followed directly by a preposition or a verb followed by nouns, adjectives, or adverbs ending in a preposition (Fader, Soderland, & Etzioni, 2011).
To identify user triples in a relation based query, we apply Algorithm 1.


Algorithm 1 Extracting user triples from a relation based user query.

Input Sentence S=(w1 , w2 ··· wn).
Output UserTriples.
Given a sentence S = {w1, w2, ..., wn}
Check if S is compound i.e CheckCompound(S). (equation 5)).
if CheckCompound(S)=true. then
Break(S) = (s1 (cc)s2 )
else
UserTriple = GenerateTriple(S).
end if
return UserTriples


Fig. 4. POS tag patters to flag compound sentences in a relation based queries.


The algorithm accepts a user submitted query (sentence) which is composed of a number of words as its input. It then evaluates if a sentence is compound using the function checkCompound(S). The function checkCompound(S) applies a number of syntactic con- straints to categorize a sentence as compound or not. The syntactic constraints are shown in Fig. 4. A compound sentence should be composed of the following sequence:
A verb followed by a preposition followed by a noun, conjuction and a verb (e.g. Which female actor played in Casablanca and is marriedto writer born in Rome)
verb followed by a noun followed by a conjuction and a noun(e.g. Which river traverses Mississippi or Alaska)
Noun followed by a conjuction followed by a noun and a verb(e.g. Which rivers and lakes traverse Alaska
An adverb, superlative followed by a conjuction followed by an adverb, superlative followed by a verb(e.g. Which is the least and most populated state in America)
Using Stanford dependency parser, we translate the syntanctic constrains in Fig. 4 into a set of formulas shown in equation 5.
∀we, wh, wi, wj , wk.(prep(we, wh ) ∧ pob j(wh, wi )
∧cc(we, wj ) ∧ conj(we, wk )
⇒ Compound(wa, wb, ··· , wn ))	(5a)
∀we, wh, wi, wj .(dob j(we, wh ) ∧ cc(wh, wi ) ∧ conj(wh, wj )
⇒ Compound(wa, wb, ··· , wn ))	(5b)
∀we, wh, wi, wj .(nsubj(we, wh ) ∧ cc(wh, wi ) ∧ conj(wh, wj )
⇒ Compound(wa, wb, ··· , wn )	(5c)
∀we, wh, wi, wj .(advmod(we, wh ) ∧ cc(wh, wi ) ∧ conj(wh, wj )
⇒ Compound(wa, wb, ··· , wn )	(5d) Consider the sentence, S=which rivers traverse Mississipi or Alaska. Applying rule (5b), the sentence is categorized as com-
pound as shown below.
dob j(traverse, Mississippi) ∧ cc(Mississippi, or) ∧
conj(Mississippi, Alaska) ⇒ Compound(S)
A sentence S that is categorized as compound has to broken into two simple sentences (s1 and s2). The sentences are joined by

a conjuction that was linking them in the user query i.e. (s1(cc)s2). The function Break(S) which is iterative applies rule in Eq. 6 to ex- tract two simple sentences from the compound sentence S.
Break(S) ≡ s1 (cc)s2	(6)
where
s1 = wa, wb, ··· , wis2 = wa, wb, ··· , we−1, wj+1 ··· wn	(7a)

s1 = wa, wb, ··· , whs2 = wa, wb, ··· , wh−1 , wj	(7b)

s1 = wa, ··· , wh, ··· , wj+1 ··· wns2 = wa, wi+1, ··· , wj , ··· , wn
(7c)

s1 = wa, ··· , wh, wj+1 ··· wns2 = wa, ··· , wh−1 , wj ··· , wn	(7d)
The rule in Eq. (7a) applies to a compound sentence identi- fied by the rule in Eq. (5a). Likewise, (7b) applies to (5b),(7c) to (5c) and (7d) applies to (5d). Here, we assume the last word of a sentence is wn. Applying rule (7b), the compound sentence: S=which rivers traverse Mississipi and Alaska is broken down into two simple sentences i.e.
s1=Which rivers traverse Mississippi. s2=Which rivers traverse Alaska.
cc = and
Finally, the algorithm identifies triples through the function GenerateTriples. For each simple sentence si, GenerateTriple func- tion, identifies user triples in it by extracting two subsequent head nouns and connects them using a relational phrase that links them. For instance, in s1 above we have the nouns rivers and Mississippi as two subsequent head nouns and traverse is the relational phrase linking them, therefore, GenerateTriple will generate a single triple from s1 i.e. {rivers traverse Mississippi}. Likewise, in s2 a single user triple {rivers traverse Alaska} will be generated.
Once the user triples have been established, we generate all triple arrangements that predict possible ways in which concepts in the user triples may be modeled in the underlying ontology. For example, the user triple {rivers traverse Mississippi} predicts that the undelying ontology for instance has modeled the concepts river and Mississippi as { river : flows_through Mississippi}. However, the undelying ontology may have modeled the concepts as { Mississippi
:hasRiver river}. To capture both these possibilities, for each user triple we extract, we create a second one where the concepts in the subject and object position are interchanged. Therefore, for the user triple {rivers traverse Mississippi} we create another {Misssippi traverse river} where the concepts Mississippi and rivers are inter- changed. In this example we generate the following user triples.
{rivertraverseMississippiORMississippitraverseriver}
{rivertraverseAlaskaORAlaskatraverseriver}.
The correct arrangement of concepts as modeled in the ontol- ogy will be resolved by the lexicon.
Consider the user query Which female actor played in Casablanca and is married to a writer born in Rome ?, based on the rule in Eq. (5a), the query is marked as a compound sentence. The query is therefore broken into two simple sentences i.e.
s1= Which female actor played in Casablanca
s2=Which female actor is married to a writer born in Rome. cc = And
When GenerateTriple function is applied to both s1 and s2, the following user triples are generated.
GenerateTriple(s1)= {actor played_in Casablanca} GenerateTriple(s2)={actor married_to writer,writer born_in Rome}
The user triples are then expanded to predict possible positions in the undelying ontologies.
{(actor played_in Casablanca, Casablanca played_in actor) }
{(actor married_to writer, writer married_to actor)(writer born_in Rome, Rome born_in writer)}
The final user triples generated from the query is shown in Listing 1.
The correct position of the concepts as modeled in the un- delying ontology will be resolved by the lexicon as discussed in Section 3.5. There are two special cases where GenerateTriple ap- plies the rules in equation 8. The rules basically apply in scenarios where it is not explicit which nouns a verb is linking. The rules are applied;
When a query starts with Who e.g Who killed Ceasar ?
When a query contains a verb that does not lie between any two nouns (i.e. when a verb is at the end of a sentence such as in the query In which continent does the Nile traverse ?.
Rule (8a) applies for the first scenario while (8b) for the second.

∀we, wh, wi.(nsubj(we, wh ) ∧ dob j(we, wi )
⇒ T riple(?wh, we, wi )) ∨ T riple(wi, we, ?wh ))	(8a)

∀we, wh, wi, wj .(pob j(we, wh ) ∧ prep(w j , we ) ∧ nsubj(wj , wi )
⇒ T riple(wi, wj , wh ) ∨ T riple(wh, wj , wi )	(8b)
Consider the query Who killed Ceasar ?, applying the rule in Eq. (8a),
nsubj(killed, who) ∧ dobj(killed, Ceasar)
⇒ T riple(?who, killed, Ceasar)
∨Triple(Ceaser, killed, ?who)
the user triple {?who killed Ceasar} or {Ceaser killed ?Who} is gen- erated. Again consider the user query In which continent does the Nile traverse ?. The dependency diagram is shown in Fig. 5. Apply- ing the rule in Eq. (8b),
pob j(In, continent) ∧ prep(traverse, In) ∧ nsubj(traverse, Nile)
⇒ T riple(Nile, traverse, continent)
∨Triple(Continent, traverse, Nile)

Identifying user query triple pattern in non-relation based user query
A non-relational query is a query (sentence) which has no rela- tional phrase linking any of its nominals. For instance, the sentence What is the area of the most populated state? is a non-relation based query. To identify triples that exist in this category of queries, we use Algorithm 2.

Algorithm 2 Triple extraction Algorithm in a non-relational query.


Input SentenceS = (w1 , w2 ··· wn ).
Output UserTriples.
1: Given a sentence S = ({w1, w2, ..., wn})
2: Check if it is compound i.e CheckCompound2(S) (equation 9,10 or 11)).
3: if CheckCompound2(S) == true then
4:   (s1, s2 ) = Break2(S)
5: else
6:	UserTriples = GenerateTriple2(S).
7: end if
8: return UserTriples


The function CheckCompound2 establishes whether a non- relation query is compound or not based on two key rules devel- oped. The rules in Eq. 9 and 10 applies for Wh and non-Wh based queries respectively. The rule in Eq. 11 applies for a a non rela- tional query that contains the pattern JSS → CC → JJS → N i.e.

{
(actor played_in Casablanca) OR (Casablanca played_in actor)
AND
(actor married_to writer) OR (writer married_to actor) (writer born_in Rome) or (Rome born_in writer)
}

Listing 1. Sample user triples.

S2=What is the area of the most populated state
Finally, user triples are identified through the function Gener- ateTriples2. To identify triples, GenerateTriples2 exploits the pres- ence of;
Preposition (father of Tom or mountain in Germany).
Genitive’s construction (Tom’s father).







Fig. 5. Showing dependency.






Listing 2. Sample user triples.

an adjective, superlative followed by a conjuction followed by an adjective, superlative followed by a noun(e.g. Which is the longest and shortest river in America).
The preposition “of” signals that a given noun posess a speci- fied property e.g. in the query What is the area of the most pop- ulated state suggests that the noun state has a property area. The preposition in is a signal that a given object belong to a noun e.g. What is the highest mountain in Germany depicts that Germany has an object of the type mountain. Therefore based on the type of preposition used, GenerateTriples2 uses two key set of rules to extract triples from a user submitted query. When using a prepo- sition to extract triples, the general syntactic constraint in non- relation based query is N → IN → (A∗) → N i.e. a noun followed by a preposition followed by a noun. Sometimes a determinant and an adjective may exists as depicted by A∗.
The rules in equations 13 apply for a query where of preposition is used. Eq. (13a) and (13b) are exploited for Wh and non-Wh based queries respectively.
∀wu, wx, wy, wz.(nsubj(wu, wx ) ∧ prep(wx, wy )
∧ pob j(wy, wz ) ∧ (wy = “of rr )

∀wd, wf , wg, wk.(nsubj(wd, wf ) ∧ cc(wf , wg ) ∧ conj(wf , wk )
⇒ Compound(wa, wb, ··· , wn )	(9)

∀wd, wf , wg, wk.(dob j(wd, wf ) ∧ cc(wf , wg ) ∧ conj(wf , wk )
⇒ Compound(wa, wb, ··· , wn )	(10)
∀we, wh, wi, wj .(amod(we, wh ) ∧ cc(wh, wi ) ∧ conj(wh, wj )
⇒ Compound(wa, wb, ··· , wn )	(11)
A compound sentence is broken into two simple sentences, s1, s2 as shown in the rule in Eq. 12. where
Break2(S) ≡ s1 (cc)s2	(12)
Where in a compound sentence identified by rule 9 and 10
s1 = wa, wb, ··· , wf , wk+1 , ··· , wn
s2 = wa, wb, ··· , wf −1 , wg+1 ··· wn while in a compound sen- tence identified by rule 11
s1 = wa, ··· , wh, wj+1 ··· wn
s2 = wa, ··· , wh−1 , wj ··· , wn
Consider the user query S=What is the population and area of the most populated state ?. The dependency diagram is shown in Fig. 2. Applying Eq. 10,
nsub j(is, population) ∧ cc(population, and) ∧ conj(population, area)
⇒ Compound(S)

Since the sentence is compound, the Break2(S) function is ap- plied to break it into two simple sentences s1 and s2 i.e.
⇒ T riple(wz, (wx_wy ), ?k))	(13a)

∀wu, wx, wy, wz.(dob j(wu, wx ) ∧ prep(wx, wy )
∧ pob j(wy, wz ) ∧ (wy = “of rr )
⇒ T riple(wz, (wx_wy ), ?k))	(13b)
Hence, applying GenerateTriples2 (rule (13a)) to s1 and s2
(nsubj(is, population) ∧ prep(population, of )
∧ pob j(of, state) ∧ (of = “of rr )
⇒ T riple(state, (population_of ), ?k))

(nsubj(is, area) ∧ prep(area, of ) ∧ pob j(of, state) ∧ (of = “of rr )
⇒ T riple(state, (area_of ), ?k))
GenerateTriples2(s1)={State population_of ?x} GenerateTriples2(s2)={State area_of ?x}
UserTriples={State population_of ?x AND State area_of ?y}
For a case where the in preposition is used, Eq. (14a) and (14b) applies for Wh and non-Wh based questions respectively.
∀wu, wx, wy, wz.(nsubj(wu, wx ) ∧ prep(wx, wy )
∧ pob j(wy, wz ) ∧ (wy = “inrr )
⇒ T riple(wz, ?k, wx )) ∨ T riple(wx, ?k, wz ))	(14a)

∀wu, wx, wy, wz.(dob j(wu, wx ) ∧ prep(wx, wy )
∧ pob j(w , w ) ∧ (in = “inrr )

s1 =What is the population of the most populated state	y	z

cc= And
⇒ T riple(wz, ?k, wx ) ∨ T riple(wx, ?k, wz ))	(14b)


















Fig. 6. Dependency diagram.


Consider the user query Which is the highest mountain in Ger- many ?, applying the rule in Eq. (15a),
nsubj(is, mountain) ∧ prep(mountain, in) ∧ pob j(in, Germany) ∧ (wy = “inrr )
⇒ Triple(Germany, ?k, mountain) ∨ T riple(mountain, ?k, Germany))

The user triple{Germany ?k Mountain} or {Mountain ?k Germany}
is extracted.
Here, we try to preempt all the possible the arrangements of concepts in the underlying ontology. The term Germany could be modeled in the ontology to occupy the subject position such as in the triple {Germany :hasMountain Adelegg} or it can be mod- eled to occupy the object position such as {Adelegg :belongTo Ger- many}. The exact triple to be selected will be determined by the lexicon. When it comes to genitive’s complement the rule in equa- tion 15 is applied. The syntactic constraint applied is N → POS → N
i.e. it involves two nouns, the head and the dependent (or mod- ifier noun)(e.g German’s flag). The dependent noun modifies the head by expressing some property of it. The rule in (15a) is ap- plicable in a non-Wh query while (15b) applies in a Wh based queries.
wu, wx, wy, wz.(dob j(wu, wx ) ∧ poss(wx, wy ) ∧ possessive(wy, wz )
⇒ T riple(wy, ?k, wx )) ∨ T riple(wx, ?k, wy ))	(15a)

wu, wx, wy, wz.(nsubj(wu, wx ) ∧ poss(wx, wy ) ∧ possessive(wy, wz )
⇒ T riple(wy, ?k, wx )) ∨ T riple(wx, ?k, wy ))	(15b)
Fig. 7. POS tag patterns for scalar adjective.






Listing 3. User triples.




Listing 4. User triples.




Listing 5. User triples.


An adverb, superlative followed by a verb followed by a noun (e.g., most populated state)
A scalar adjective is a signal that a given noun posses a prop- erty indicated by the adjective. For example the combination longest river is an indication that the noun river has a property length. We therefore translate the syntactic contraints in Fig. 7 to the rules in Eq. 16 and 17. The rules are used to create triples from a query that contains a scalar adjective.
∀wu, wx, wy.(amod(wx, wu ) ∧ (wu ≡ JJS)
⇒ T riple(wx, root(wu ), ?k))	(16)

∀w , w , w .(advmod(w , w ) ∧ amod(w , w )

Consider the query What is Angela’s birth name?(see depen-
x	y
u	y	x

dency in Fig. 6), applying the rule in Eq. (15b),
nsubj(is, name) ∧ poss(name, Markel) ∧ possessive(Markel,r s)
⇒ Triple(Markel, ?k, name) ∨ T riple(name, ?k, Markel))
The triple {Markel, ?k name}∨ {name,?k, Markel} is generated.

Step 3: handling adjectives

To handle adjectives, we categorize them into two groups
Scalar adjectives.
Non-scalar adjectives.

Scalar adjectives
Scalar adjectives are those which communicate the idea of scale. In OWL ontologies, scalar adjectives can be mapped to an owl:DatatypeProperty. To flag out scalar adjectives, we use two two key syntactic constraints
The syntactic constraint requires that a scalar adjective matches any of the the POS tag pattern shown in Fig. 7. The pattern limits a scalar adjective to be
1. An adjective, superlative followed by a noun (e.g., longest river).
⇒ T riple(wy, root(wx ), ?k))	(17)
Consider the query Which is the longest river in America ?. Ap- plying the rule in Eq. 16, the user triple in Listing 4 is generated.
amod(river, longest)∧(longest ≡ JJS)⇒Triple(river, root(longest),
?k))
Consider the query Which is the most populated state in America
?. Applying the rule in Eq. 18, the user triple in Listing 5 is gener- ated.
advmod(populated, most) ∧ amod(state, populated)
⇒ T riple(state, root(populated), ?k))

Non-scalar adjectives
These are adjectives that do not communicate the idea of scale. For a non-scalar adjective, we restrict that a syntactic constraint must match the POS tag patterns shown in Fig. 8. The pattern lim- its a non-scalar adjective to be an adjective followed directly by another adjective then a noun (e.g. female Russian astronaut) or an adjective followed directly by a noun (e.g., German chemist).
The POS tag pattern is translated into the rule in Eqs. (18) and
(19) respectively. The functions try to preempt all the possible modeling of the concepts’ positions in the underlying ontology. Consider the statement German fighter. An ontology can model

the Nobel prize ?, since it is a relation based query, it is first pro- cessed based on the discussion in 3.3.1, the GenerateTriple(s) will extract the triples {(Chemist won Nobel)or (Nobel won Chemist)}. Adding this to the triple generated based on adjective, the final user triples for the query is shown in Listing 6.


Listing 6. User triples.


this in two possible ways i.e fighter :hasNationality German or Ger- many :hasFighter fighter hence the word fighter can be modeled in the subject or object position. When extracting the user triples in Eqs. (18) and (19), we extract the two possibilities.
∀wu, wx, wy.(amod(wx, wu ) ∧ amod(wx, wy )
Lexicon

The words in user triples need to be mapped to the entities in the underlying ontology i.e. the user triples need to speak the lan- guage of the ontology. For instance, the terms in the user triple
{State population_of ?x} in Listing 3 need to be mapped to the terms {State :hasPopulation ?x} in the ontology in Listing 7. For this purpose a lexicon is developed.

⇒ (T riple(wx, ?k, wu ) ∧ T riple(wx, ?k, wy ))∨
(T riple(wu, ?k, wx ) ∧ T riple(wy, ?k, wx ))
∀wu, wx.(amod(wx, wu )
(18)
The lexicon helps in mapping user terms to the entities in the
ontology. A lexicon records information specific to individual enti- ties (classes, predicates and individuals) contained in the ontology. We believe that a good lexicon should be rich enough to :

⇒ T riple(wx, ?k, wu )) ∨ T riple(wu, ?k, wx ))	(19)
Applying the rule in Eq. 19 to the user query Which German chemist won the Nobel prize ?
amod(Chemist, German)⇒ Triple(Chemist ?k German)∨ Triple(German ?k Chemist)
Likewise applying the rule in Eq. 18 to the user query Which female German chemist won the Nobel prize
amod(Chemist, German) ∧ amod( female, German) ⇒ Triple(Chemist ?k German) ∧ Triple(Chemist ?k female)∨ Triple(German ?k Chemist) ∧ Triple(female ?k German)
The user triples of a user query generated based on adjectives are added to the existing user triples generated in the previous step. For example in the query Which female German chemist won
Disambiguate words such as Mississippi river and Mississippi state.
Identify positive and negative scalar adjectives in a user triple.
Map words in a way that minimizes the search space.
Resolve the exact position of a concept as modeled in the un- derlying ontology.
To develop this kind of a lexicon, we adopted lemon (Lexi- cal Model for Ontologies) (McCrae, Spohr, & Cimiano, 2011) which is a model for lexicons that are machine readable. It allows in- formation to be represented relative to the underlying ontology. Lemon was a natural choice since it is RDF based and uses the principles of Linked Data. It can also be extended easily to cap- ture the information needed. To reduce the work of generat- ing the lexicon manually, we adopted the technique proposed in


 



















Listing 7. Sample ontology.


 























Listing 7. Continued





Fig. 8. POS tag patterns for non-scalar adjective.


(Walter, Unger, & Cimiano, 2013). We exploited the technique to generate the lexicon in lemon model semi-automatically. We de- signed the lexicon such that it preserved the structure of the un- derlying ontology. For each lexical entry in the lexicon we specify the following information:
LexicalEntry. This is a given ontology entity (class, property or individual) extracted from the underlying ontology e.g. the en- tity River in Listing 7.
partOfSpeech. This entry specifies the part of speech to which the word in the LexicalEntry belongs to.
canonicalForm. This is the lemma of the word in the LexicalEn- try. To extract a canonical for of a verb, its lemma is its in- finitive form or its present tense (e.g. the canonical form of the verb married is marry). The canonical form a noun is the noun’s singular form (e.g. the canonical form of the noun Rivers is River). For the adjectives it is the positive (i.e., non-negative, non-graded) form (e.g. high).
type. This is the rdf:type of the LexicalEntry.
OntotripleCategory. This indicates the position of the LexicalEntry
in the ontology triple i.e. is it a subject, predicate or an object.
If the LexicalEntry is a subject, then the its associated predicate and object must be specified. Likewise, if it is an object then its asscotiated subject and predicate should be indicated and finally if the lexical item is predicate then its associated subject and object should be indicated.
positive and negative entries. These entries are included for a LexicalEntry which is a domain of a owl:DatatypeProperty where the asscociated range is a non-negative integer. They give the positive and negative scalar adjectives associated with the Lexi- calEntry. For example in Listing 7, the entity State is the domain of the property hasPopulation and its range is a non-negative in- teger. Therefore its positive and negative scalar adjectives must be indicated. The positive scalar adjectives asscociated with the word State w are Most Populated and Most Inhabited while neg- ative include Least populated and Least Inhabited. This entries are helpful when processing sentences that contain scalar ad- jectives such as ”Which is the most populated state in USA, the term most populated will be mapped to a positive scalar adjec- tive in the lexicon.
A sample lexicon extracted from the ontology in Listing 7 is shown in Listing 8.
Step 4: converting user triples to ontology triples

The user triples need to be converted to ontology triples using the lexicon. To do this, terms in the user triples need to be mapped to the terms in the underlying ontology. To map a user triple to


	     



   	 
   
Listing 9. SPARQL query structure.
  




    	
    

 	  

  	
Listing 10. SPARQL construction.


terms of the user triple (x⟩(y⟩(z⟩) are mapped position by po- sition to the lexicon terms (x_lexicon⟩(y_lexicon⟩(z_lexicon⟩). The second option ((z⟩(y⟩(x⟩) is hence dropped. If they don’t match the lexicon is queried to fetch x_lexicon that occupies the < ob- ject > position. We then map the  < subject > of x_lexicon to subject in the user triple i.e. < subject > of x_lexicon is compared with (z⟩. If they match then the terms of the user triple (z⟩(y⟩(x⟩) are mapped position by position to the lexicon terms (z_lexicon⟩(y_lexicon⟩(x_lexicon⟩). The first option ((x⟩(y⟩(z⟩ is hence dropped. The same technique is applied for the various forms of user triples generated.


Listing 8. Sample lexicon created from the ontology in Listing 7.


an ontology triple, the position a term occupies in the user triple should match the position of the term it is mapped to in the on- tology triple i.e. a term in the user triple that occupies the subject position must be mapped to a term in the ontology triple that oc- cupies subject position. By doing this, we narrow the search space hence reducing the mapping time significantly. A user triple can be in any of this forms
((x⟩(y⟩(z⟩)
((?x⟩(y⟩(z⟩)
((x⟩(?y⟩(z⟩)
((x⟩(y⟩(?z⟩)
Given a user triple in the form ((x⟩(y⟩(z⟩) or ((z⟩(y⟩(x⟩) e.g. (ac- tor played_in Casablanca) or (Casablanca played_in actor) the lexi- con has to perform two key tasks.
When a user triple presents two options, it should select the correct triple that reflects the modeling in the underlying on- tology.
Map user terms to ontology terms(i.e. bridge the gap between user terms and ontology terms).
Using string metric (Stoilos, Stamou, & Kollias, 2005) and back- ground knowlege Wordnet, the user entity x from the user triple ((x⟩(y⟩(z⟩) or ((z⟩(y⟩(x⟩) is mapped to the lexical entry x_lexicon in the lexicon. Once the term x is mapped to a term x_lexicon in the lexicon, the lexicon is queried to fetch all lexical entries that con- tain the term x_lexicon. We then select all x_lexicon where < on- totripleCategory > is the subject i.e. we select all x_lexicon that occupy the subject position. We then map the < object > of x_lexicon to the < object > of x in the user triple i.e. < ob- ject > of x_lexicon is compared with (z⟩. If they match then the
Query construction

After converting user triples into ontology triples, the next stage is to generate a SPARQL query. The general syntax of the SPARQL query is shown in Listing 9.
The targets are the words identified in step 1 (section 3.1). For instance, consider the sentence what is the area and population of the most populated state, from the rule in Eq. (1b), the words area and population will be identified as the targets. Therefore the intial SPARQL query generated will be
SELECT ?area ?population
{
}
After identifying targets, the next step is to identify the user triples as discussed in step 2 (section 3.2). In the sentence what is the area and population of the most populated state, since this query is a non-relation based query, it will be processed based on the discussion in Section 3.2.2. The user triple is shown in Listing 3. After generating user triples, the next step is to convert the user triples into ontology triples by the use of the lexicon as discussed in Section 3.5. The triples that will be generated in our running example will be:
For each unique noun X that appears in the subject posi- tion of the ontology triple, we add the triple < ?x >  < rdf: type >  < X > and replace the all the subsequent noun X in the ontology triples with < ?x > . Therefore, the triple ontology triples in Listing 10 is expanded to Listing 11.

Query ﬁlters

Query filters are the additional information contained in the user submitted query that helps to further narrow down the re- sults to meet a user’s required answer. Here, we handle,


  	   	







  


Listing 11. SPARQL construction.







Listing 12. SPARQL construction.








Listing 13. SPARQL construction.








Listing 14. SPARQL construction.



Logical operators (e.g. Which river flows through Alaska or Mis- sissippi ?).
Adjectives (e.g Which is the longest river in USA ?).
Negation (e.g Which river does not flow through Mississippi ?).
Numbers (e.g.Which are the four longest rivers in USA ?).


Logical operators

In SPARQL query, the conjunction(AND) operator need no spe- cial handling. Therefore, for instance in Listing 11, we just drop the AND operator resulting in the query in Listing 12.
In case of an OR operator, SPARQL provides a number of options on how to process it (SPARQL Working Group, 2013). In our case we use the UNION i.e. each identified OR operator is replaced with
Listing 15. SPARQL template for positive scalar adjective.






Listing 16. SPARQL template for negative scalar adjective.


the key word UNION. For instance the query in Listing 13 will be transformed into 14.

Adjectives

The non-scalar adjectives need no special handling apart from those discussed in Section 3.3.2. However, scalar adjectives help to further narrow down the query hence need addition processing on top of those discussed in section 3.4.1. The syntactic constrains of scalar adjectives are defined in section 3.3.1. To process the scalar adjectives, we execute three steps
Identify a scalar adjective.
Categorize it as either positive or negative scalar adjective.
Map the adjective to a SPARQL query statement.
For the first step, the scalar adjectives are identified using the syntactic constraints defined in Section 3.3.1. The scalar adjectives can either be JSS → NN or RBS → VBN → NN. If it is JSS → NN, the Lexical entry of NN is queried in the lexicon to ascertain whether the JSS matches any of its (positive⟩ or (negative⟩ tags. For example in the query Which is the longest river in USA ?, the scalar adjective is longest river. The lexical entry of the noun river is queried in the lexicon to ascertain whether the term longest matches its (positive⟩ or (negative⟩ tag. If the scalar adjective is RBS → VBN → NN, the lexical entry of NN is searched to ascertain whether, RBS → VBN matches any of its (positive⟩ or (negative⟩ tags. For example, in the query What is the area and population of the populated state
?, the scalar adjective is most populated state. The lexical entry of the noun state is queried in the lexicon to ascertain whether the term most populated matches its (positive⟩ or (negative⟩ tag.
If the scalar is mapped to a positive tag, the adjective is mapped to the SPAQRL statement shown in Listing 15.
In case of a negative tag, the scalar adjective is mapped to the SPARQL template in Listing 16.
Therefore the final SPARQL query for the sentence What is the area and population of the populated state ? is shown in Listing 17. If a user query contains both the negative and positive scalar adjectives such as in the sentence S=Which is the longest and short- est river in America ?, using rule in Eq. 11, the query will be catego- rized as a compound query. Therefore, the compound sentence has to be broken into two sentences, where each contains an opposing scalar adjective. For example in this query, using the rule in Eq. 12,
we split the sentence S into
s1=Which is the longest river in America. s2=Which is the shortest river in America. cc = and


  






Listing 17. SPARQL query.







Listing 18. SPARQL query.















  







  




Each sentence is then processed independently as discussed in the previous sections to generate a SPARQL query for each sen- tence. For instance, s1=Which is the longest river in America is pro- cessed to generate the query in Listing 18.
while s2=Which is the shortest river in America is processed to generate the query in Listing 19.

Numbers

Numbered scalar adjectives
Numbered scalar adjective is a signal that a user wants a spec- ified list of items. The syntactic constraints used to flag these type of adjectives are
CC → JJS → NN (e.g. four longest rivers)
CC → RBS → VBN → NN (e.g. four most populated states)
If the scalar adjective is mapped to a positive tag of a noun (NN), the numbered adjective is mapped to the SPAQRL query shown in Listing 20
In case it is mapped to a negative tag of a noun (NN), the scalar adjective is mapped to the query in Listing 21.






































Listing 19. SPARQL query.


   		 





Listing 20. Positive numbered adjective template.


    





Listing 21. Negative numbered adjective template.






Listing 22. Numbered list template.


Numbered list
Sometimes a user query may want to get a list of items without specifying any condition such as List four rivers in USA or Which four rivers flow through USA. To identify a numbered list, we use two rules
∀w1, w2, w3.(num(w1, w2 ) ∩ nsub j(w3, w1 ) ⇒ NumberedList(w1 ))
(20)

∀w1, w2, w3.(num(w1, w2 ) ∧ dob j(w3, w1 ) ⇒ NumberedList(w1 ))
(21)

(num(rivers, f our ) ∧ nsubj( flow, rivers) ⇒ NumberedList( f our)
(22)

(num(rivers, f our ) ∧ dob j(List, rivers) ⇒ NumberedList( f our)
(23)
The NumberedList is mapped to a SPARQL query in Listing 22.

Negation
Negation is a reversal of some truth. Currently, we handle two types of negation i.e. not and neither. To recognize negation not we use the rule in Eq. 24 while to recognize neither we use the rule in Eq. 25.
∀wu, wx, wy.(neg(wx, wu ∧ nsubj(wx, wy ) ⇒ Negation(wu )	(24)
For example, Which river does not traverse Alaska or Mississippi ?
neg(not, traverse) ∧ nsubj(traverse, river) ⇒ Negation(not)

wu, wx, wy.(advmod(wx, wu ) ∧ nsubj(wx, wy ) ⇒ Negation(wu )
(25)

Listing 23. Initial listing for negation example.











Listing 24. Initial listing for negation example.


For example the query Which river neither traverses Alaska nor Mississippi ?
advmod(nor, traverses) ∧ nsubj(traverses, river) ⇒ Negation(nor)
To process the negation, we first remove the negation part and extract triples contained in the positive query. For example, in the query Which river does not traverse Alaska or Mississippi ? its pos- itive form is Which river traverses Alaska or Mississippi ?. The user query is then processed normally to generate initial SPARQL query as ahown in Listing 23.
To handle the negation part of the user query, we extract the target words (subjects) in it using the functions in discussed in Section 3.1. For each noun identified, using the lexicon, it is mapped to a corresponding term in the underlying ontology. We then extract its most general triple in the ontology. From this gen- eral triple we MINUS the triples generated by the positive form of the query. For instance the noun river is selected as subject of the query Which river traverses Alaska or Mississippi ? (see Eq. (1a)). After mapping the term river to River in the underlying ontology, we then extract the triple representing the general type of River which is of the form  < ?x >  < rdf: type >  < X >  i.e.
< ?river >  < rdf: type >  < River > . From this triple we MI-
NUS the triple in Listing 23 to generate Listing 24.

Evaluation

To gauge the performance of PAROT, we evaluated it on both simple and complex questions. Simple questions are questions that can be solved using only one triple pattern (Bordes, Usunier, Chopra, & Weston, 2015) while complex questions are questions which the intended SPARQL query consist more than one triple pattern (Trivedi, Maheshwari, Dubey, & Lehmann, 2017). By using these two categories of datasets, we sought to evaluate if the ex- tra capabilities of PAROT gives it any performance advantage over tools that don’t incorporate techniques such as negation, adjective and compound sentence handling capabilities.

Table 2
PAROT vs gAnswer in Macro results.


Table 3
PAROT vs gAnswer in Macro results.


Table 4
PAROT vs gAnswer in Macro results.



Dataset

For complex questions, we used two datasets, the first dataset was from the 9th challenge on question answering over linked data (QALD-9)1. We specifically evaluated the PAROT system on the test dataset. The questions contained in the dataset are of different complexity, including questions with counts, superlatives compar- atives and temporal aggregators. The second type of dataset used was that provided by Mooney2 which has been used previously by PANTO for similar evaluation. We specifically used the dataset that is composed of geography data in the United States. The dataset is accompanied with 880 queries where each query has its ex- pected response in prolog format. From this dataset, we converted the prolog format into OWL ontology. We then selected queries that were compound in nature and contained negation. To evaluate the performance of PAROT on simple questions, we used 200 ques- tions and their corresponding answers from the dataset proposed by (Bordes et al., 2015).

Evalutation metrics

To evaluate the PAROT system, we replicated the metrics used in QALD-9 challenge. Specifically, we used the following parame- ters
Number correct answers generated for question q
precision =
Number of total answers generated for question q
(26)

Number correct answers generated for question q
recall =
Number of gold standard answers provided for question q
(27)
If for a given question q the gold answerset is empty but the system generates an answer, the precision, recall and F-measure values is set to 0.
If for a given question q the system generates an empty answer set while the gold answerset is not empty the precision is set to 1 while the recall and F-measure values are set to 0.
We then computed the macro and micro F-measure of PAROT over all test questions. To compute micro-F-measure, we summed up all true and false positives and negatives and calculated the pre- cision, recall and F-measure at the end. For the macro-measures, we calculated precision, recall and F-measure per question and averaged the values at the end. The results were compared with those of gAnswer tool (Zhao et al., 2017), which was the top per- forming tool in QALD-9 challenge (Usbeck et al., 2018a).

Results and discussion

Simple questions results
From the results in Tables 2 and 3 gAnswer performs slightly better in terms of precision in both micro and macro-precision. However, when it comes to recall PAROT performs slightly bet- ter in both Micro and micro-recall. From the F-measure values the tools are almost equal. From these results when dealing with sim- ple questions, the development of PAROT may not be justified even though it is able to fetch more correct answers as compared to gAnswer as depicted by its relatively higher recall value.

Complex questions results

Results for QALD9 dataset
When dealing with complex questions, the strength of PAROT is evident. In QALD9 dataset, the PAROT outperforms gAnswer in both macro F-measure and micro F-measure as shown in Tables 4 and 5. This is attributed to its ability to handle a num-

2×recall× precision
F − measure =
recall + precision
We also adopted the following rules
(28)
ber of variety of questions i.e. the wide coverage of the syntac- tic based heuristics. PAROT performs 18% better that gAnswer in this task. Its high coverage is depicted by its comparatively higher recall value. Its high precision shows that the heuristics are able

1. If for a given question q the gold answerset is empty and the system also generates an empty answerset, the precision, recall and F-measure values is set to 1.

1 https://github.com/ag-sc/QALD/tree/master/9/data.
2  ftp://ftp.cs.utexas.edu/pub/mooney/nl-ilp-data/geosystem/geobase.
to resolve user questions into correct SPARQL queries. However, an optimum performance of PAROT was inhibited by its inability to answer questions that start with When. It also can only partially handle aggregation. When it comes to query processing time qAn- swer has a significant lower response time as compared to PAROT. The significantly slow query response time of PAROT is attributed to its elaborate query analysis and categorization step which takes

Table 5
PAROT vs gAnswer in Macro results.


Table 6
PAROT vs gAnswer in Macro results.


Table 7
PAROT vs gAnswer in Micro results.



a significant amount of time as compared to query conversion to SPARQL and answer retrieval steps. The query processing time does not include the time for loading ontology and parsing it to create the lexicon.

Results of geoquery dataset
In Geoquery dataset which we tailored to contain mostly nega- tion and compound queries, PAROT still outperforms gAnswer both in macro and micro F-measure as shown in table Tables 6 and 7. It achives a macro F-measure of 87.55% as compared to gAnswer’s 67.76%. When it comes to micro F-measure, PAROT achieves an F- measure of 88.31% as compared to qAnswer’s 67.60%. This is an indication that PAROT is significantly eﬃcient when handling com- pound and negation based questions. Some of the wrong answers generated by PAROT were attributed to wrong dependency rela- tionships generated by Stanford dependency parser.

Weaknesses of PAROT
From the evaluation of PAROT, we noted that it had the follow- ing weaknesses which we seek to address as an ongoing work;
Its query analysis and categorization step is significantly slow.
It still cannot handle questions that start with When.
It still has a low precision and recall when processing aggrega- tion based questions.
The lexicon generation step is still slow hence not scalable to large ontologies.

Conclusion

PAROT is a NL to SPARQL tool. It has the ability to handle com- pound, negation, numbered list and scalar adjective based ques- tions in addition to other questions. PAROT adopts an approach that generates the most likely triple from a user query. The triple is then validated by the lexicon. It relies on dependency parser to process user’s queries to user triples. The user triples are then con- verted to ontology triples by the lexicon. The triples generated by the lexicon is what is used to construct SPARQL query that fetches the answers from the underlying ontology. Based on the results discussed in Section 6, PAROT is highly successful in converting NL to SPARQL. It can therefore be used in a system that anticipates to bridge the gap between users and ontologies. As an ongoing work
Source code for PAROT implementation is released at3.

Authorship Statement

All persons who meet authorship criteria are listed as authors, and all authors certify that they have participated suﬃciently in the work to take public responsibility for the content, including participation in the concept, design, analysis, writing, or revision of the manuscript. Furthermore, each author certifies that this ma- terial or similar material has not been and will not be submitted to or published in any other publication before its appearance in the Hong Kong Journal of Occupational Therapy.

Authorship contributions

Please indicate the specific contributions made by each author (list the authorsinitials followed by their surnames, e.g., Y.L. Che- ung). The name of each author must appear at least once in each of the three categories below.

Declaration of Competing Interest

I confirm that the paper: PAROT: Translating Natural Language to SPARQL has no conflict of interest.

Acknowlgedgments

All persons who have made substantial contributions to the work reported in the manuscript (e.g., technical help, writing and editing assistance, general support), but who do not meet the cri- teria for authorship, are named in the Acknowledgements and have given us their written permission to be named. If we have not in- cluded an Acknowledgements, then that indicates that we have not received substantial contributions from non-authors.
References

Ahmad, S., & Hunt, B. J. (2015). Effective approaches to attention-based neu- ral machine translation. In In proceedings of the 2015 conference on empir- ical meth- ods in natural language processingn (pp. 1412–1421). doi:10.1007/ 978-3-319-28308-1_29. September.
Bordes, A., Usunier, N., Chopra, S., & Weston, J. (2015). Large-scale simple question answering with memory networks.
Bouziane, A., Bouchiha, D., Doumi, N., & Malki, M. (2015). Question answering systems: survey and trends. Procedia Computer Science, 73(Awict), 366–375. doi:10.1016/j.procs.2015.12.005.

we seek to use PAROT to develop an ontology based chatbot for		

diagnosing chicken diseases.
3 https://github.com/onexpeters/PAROT.

Cai, R., Xu, B., Zhang, Z., Yang, X., Li, Z., & Liang, Z. (2018). An encoder-decoder framework translating natural language to database queries. IJCAI International Joint Conference on Artificial Intelligence, 2018-July, 3977–3983.
Cimiano, P., & Bielefeld, U. (2011). Is question answering fit for the semantic web ?: A survey.. Semantic Web, 2(2), 125–155.
Cunningham, H., Maynard, D., Bontcheva, K., & Tablan, V. (2001). Gate. Proceedings of the 40th annual meeting on Association for Computational Linguistics - ACL ’02,
168. doi:10.3115/1073083.1073112.
Damljanovic, D., Agatonovic, M., & Cunningham, H. (2011). FREyA : An interactive way of querying linked. Eswc, 125–138.
Diefenbach, D., Both, A., Singh, K., & Maret, P. (2018). Towards a question answering system over the semantic web. Semantic Web, 0(0), 1–15.
Fader, A., Soderland, S., & Etzioni, O. (2011). Identifying relations for open infor- mation extraction. In Proceedings of the 2011 conference on empirical methods in natural language processing (emnlp), Edinburgh, UK (pp. 1535–1545). doi:10.1016/ j.jcrysgro.2013.04.025.
Ferré, S. (2017). Sparklis: An expressive query builder for SPARQL endpoints with guidance in natural language. Semantic Web, 8(3), 405–418. doi:10.3233/ SW-150208.
Gur, I., Yavuz, S., Su, Y., & Yan, X. (2018). DialSQL: Dialogue based structured query generation. Proceedings of the 56th Annual Meeting of the Association for Compu- tational Linguistics, 1339–1349.
Hao, Y., Zhang, Y., Liu, K., He, S., Liu, Z., Wu, H., et al. (2017). An end-to-end model for question answering over knowledge base with cross-attention combining global knowledge. In In proceedings of the 55th annual meeting of the associa- tion for computational linguistics (pp. 221–231). doi:10.18653/v1/p17-1021.
He, S., Zhang, Y., Liu, K., & Zhao, J. (2014). CASIA@V2: A MLN-based question an- swering system over linked data. In Clef (pp. 1249–1259). 61272332.
Hochreiter, S., & Schmidhuber, J. (1997). Long short-term memory. Neural Computa- tion, 9(8), 1–32.
Iyer, S., Konstas, I., Cheung, A., Krishnamurthy, J., & Zettlemoyer, L. (2017). Learning a neural semantic parser from user feedback. In Proceedings of the 55th annual meeting of the association for computational linguistics.
Kaufmann, E., Bernstein, A., & Zumstein, R. (2006). Querix: A natural language in- terface to query ontologies based on clarification dialogs. In In proceedings of the 5th international semantic web conference (iswc 2006), Athens, GA (pp. 5–6). November.
Klein, D., & Manning, C. D. (2003). Accurate unlexicalized parsing (1, pp. 423–430). doi:10.3115/1075096.1075150.
Lehmann, J., & Bühmann, L. (2011). AutoSPARQL: Let users query your knowledge base. Lecture Notes in Computer Science (including subseries Lecture Notes in Arti- ficial Intelligence and Lecture Notes in Bioinformatics), 6643 LNCS(PART 1), 63–79. doi:10.1007/978- 3- 642-21034- 1_5.
Lopez, V., Pasin, M., Motta, E., Hall, W., & Keynes, M. (2005). AquaLog : An ontology- portable question answering system for the semantic web (pp. 546–562). doi:10. 1007/11431053_37.
Marneffe, M. c. D., & Manning, C. D. (2015). Stanford typed dependencies manual.
20090110 Httpnlp Stanford, 40, 1–22. doi: 10.1.1.180.3691.
McCrae, J., Spohr, D., & Cimiano, P. (2011). Linking lexical resources and ontologies on the semantic web with lemon. In Lecture notes in computer science (including subseries lecture notes in artificial intelligence and lecture notes in bioinformatics): 6643 LNCS (pp. 245–259). doi:10.1007/978- 3- 642-21034- 1_17.
Sander, M., Waltinger, U., Roshchin, M., & Runkler, T. (2014). Ontology-based transla- tion of natural language queries to SPARQL. 2014 AAAI Fall Symposium Ontology- Based, 42–48. doi:10.1016/j.ijmedinf.2012.01.005.
SPARQL Working Group (2013). SPARQL Query Language for RDF. http://www.w3. org/TR/rdf-sparql-query/.
Stoilos, G., Stamou, G., & Kollias, S. (2005). A string metric for ontology align- ment. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 3729 LNCS, 624–637. doi:10.1007/11574620_45.
Tatu, M., Balakrishna, M., Werner, S., Erekhinskaya, T., & Moldovan, D. (2016). Auto- matic extraction of actionable knowledge (pp. 396–399). doi:10.1109/ICSC.2016.29. Trivedi, P., Maheshwari, G., Dubey, M., & Lehmann, J. (2017). LC-QuAD: A corpus for complex question answering over knowledge graphs. Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes
in Bioinformatics), 10588 LNCS, 210–218. doi:10.1007/978- 3- 319-68204- 4_22.
Usbeck, R., Gusmita, R. H., Saleem, M., & Ngomo, A. C. N. (2018a). 9th challenge on question answering over linked data (QALD-9). CEUR Workshop Proceedings, 2241, 58–64. doi:10.1007/978- 3- 319-69146- 6_6.
Usbeck, R., Gusmita, R. H., Saleem, M., & Ngomo, A. C. N. (2018b). 9th challenge on question answering over linked data (QALD-9). CEUR Workshop Proceedings, 2241, 58–64. doi:10.1007/978- 3- 319-69146- 6_6.
Walter, S., Unger, C., & Cimiano, P. (2013). A corpus-based approach for the induc- tion of ontology lexica. Lecture Notes in Computer Science (including subseries Lec- ture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 7934 LNCS, 102–113. doi:10.1007/978- 3- 642-38824- 8_9.
Wang, C., Xiong, M., Zhou, Q., & Yu, Y. (2007). PANTO: A portable natural language interface to ontologies. The Semantic Web: Research and Applications, 473–487. doi:10.1007/978- 3- 540- 72667- 8_34.
Xu, X., Liu, C., & Song, D. (2017). SQLNet: Generating structured queries from natural language without reinforcement learning. . CoRR, 1–13.
Yahya, M., Berberich, K., & Elbassuoni, S. (2012). Natural language questions for the web of data. In Emnlp, Deanna12 (pp. 379–390). July.
Yu, T., Yasunaga, M., Yang, K., Zhang, R., Wang, D., Li, Z., et al. (2018). SyntaxSQL- Net: Syntax tree networks for complex and Cross-DomainText-to-SQL task. In In proceedings of the 2018 conference on empirical methods in natural language processing (pp. 1653–1663).
Zhao, D., Zou, L., Wang, H., Yu, J. X., & Hu, S. (2017). Answering natural language questions by subgraph matching over knowledge graphs. IEEE Transactions on Knowledge and Data Engineering, 30(5), 824–837. doi:10.1109/tkde.2017.2766634.
