	Electronic Notes in Theoretical Computer Science 91 (2004) 56–95	
www.elsevier.com/locate/entcs




Generating Nearly Sorted Sequences - The use of measures of disorder
Vladimir Estivill-Castro1
School of Computing and Information Technology Griffith University
Brisbane, Australia

Abstract
There have been several formal proposals for a function that evaluates disorder in a sequence. We show here that definitions that allow equivalence to an operational formulation allow for the construction of an algorithm for pseudo-random generation of nearly sorted sequences. As there is interest in comparing performance of algorithms on nearly sorted sequences during experimental evaluations of their implementation, our methods here provide the pathway for establishing the benchmarks datasets to compare such algorithms.
Keywords: Sorting Algorithms, Statistical Correlation of Rankings, Graph Isomorphism, Measures of Disorder, Generation of pseudo-random permutations.


Introduction
A measure of disorder estimates the amount of existing disorder in a sequence, and usually gives an approximation to the minimum number of operations of a specific (and sometimes obscure) type required to sort the sequence. For example, the number of inversions in a sequence X = ⟨x1,... , xn⟩, denoted by Inv(X) and defined by Inv(X) = {(i, j) | i < j, xi > xj} (where the items in X belong to some total order and  S  denotes the cardinality of a set S). Alternatively, the number of inversions is the minimum number of exchanges of adjacent elements required to sort X or the number of exchanges

1 Email: v.estivill-castro@griffith.edu.au



1571-0661 © 2004 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2003.12.006

performed by Bubble Sort. Interest in measures of disorder is motivated by several applications:
Measures of disorder formalize the notion that not all sequences of length n require Ω(n log n) comparisons to sort them. The adaptive behavior of sorting algorithms is explained with respect to a measure of disorder. For example, Inv has been used to describe the behavior of Straight Insertion Sort and has been used in the analysis of several comparison-based sorting algorithms [25,28,34,38,46,47,54]. Work on Mergesort by Burge [6] sug- gested
“A measure of the disorder existing in the data is defined as the minimum amount of work required to sort the data into complete order.”
and Runs counts the number of boundaries between runs. These bound- aries are the so-called “step-downs” [34, page 161], where a smaller element follows a larger one.
Researchers have tried to demonstrate the practicality of adaptive sort- ing by testing implementation on nearly sorted sequences [16,39,49,53]. This search for adaptive sorting algorithms that are practical demands the gener- ation of nearly-sorted sequences with a clear understanding if they favor one type of disorder over another. This ‘type of disorder’ essentially is quantified by the measure of disorders used. Experimental results are usually criticized because the generation of nearly sorted sequences seems to favor the sort- ing algorithm that is to be used. The approach taken by Levcopoulos and Peterson [39] and Moffat et al [49] is as follows.
Definition 1.1 Let N<N the set of all finite sequences of natural numbers and let f, g : N<N → ঩ be two measures of disorder. We say that f is algorithmically finer than g (denoted f ≤alg g) if and only if any f - optimal algorithm is also g-optimal 2 . Accordingly, we say f and g are algorithmically equivalent (denoted f =alg g) if and only if f ≤alg g and g ≤alg f .
In Statistics, measures of disarray or right invariant metrics are used to obtain coefficients of correlation for rank correlation methods. These co- efficients of correlation are used to test the significance of observed rank correlations. For example, Inv appears in the definition of Kendall’s τ [31], the most popular coefficient of correlation. However, the distribution of disorder values must be known, and sometimes it is mathematically hard
2  An algorithm is g-optimal if for all X it sorts X in O(  X  + log  below′(g(X), X  , g)  )
where below′(k, n, g)); = {π ∈ S  | g(⟨π⟩) ≤ k}, where S denotes the group of permutations
n
of {1, 2,..., n}. Moreover, the identity permutation in Sn is denoted by id. The product of two permutations π, σ ∈ Sn is denoted by π · σ and is defined by π · σ(i) = π(σ(i)). If π ∈ Sn, then ⟨π⟩ denotes the sequence ⟨π(1), π(2),... , π(n)⟩.

to obtain a result in this direction. However, pseudo-random generation can obtain tables of values that reflect the properties and parameters of the distribution.
A benchmark problem arises from the use of graphs in pattern matching. Graphs are combinatorial objects that have been widely used in applica- tions where structured objects emerge in a natural way. Remarkably, in the pattern-matching arena, modeling with graphs has been fruitfully used to match objects [59,61,48]. Thus, the interest in finding efficient algorithms to deal with the Graph Isomorphism (GI) problem, although its precise com- putational complexity remains unknown [48]. It requires finding a bijection of the vertices so that the edge structure is the same. Labeling the ver- tices from the same set corresponds to finding a permutation π. However, it is not a minimization problem. Nevertheless, in practice a close vari- ant is a hard minimization problem. In real world applications of pattern matching, the existence of noise, distortion, uncertainty or measurement er- rors, together with weights associated to nodes and edges, translates the GI problem into its inexact version: the inexact Graph Isomorphism (iGI) or Error-Correcting Graph Isomorphism (ECGI) [59]. To define this problem we first need the notion of attributed graph [59].
Definition 1.2 [AG] An attributed graph is a 4-tuple Ga = (V, E, α, β) where V /= ∅ is a finite set of vertices; E ⊂ V × V , is a set of distinct ordered pairs (edges) of distinct elements in V ; α : V → ঩, is a function called the vertex interpreter; and β : E → ঩, is a function called the edge interpreter.
Definition 1.3 [ECGI] Given two AGs Ga = (V (Ga), E(Ga), αG, βG) and Ha = (V (Ha), E(Ha), αH, βH), with | V (G) |=| V (H) |, the Error-Correcting Graph Isomorphism problem is to find a permutation π : V (Ga) → V (Ha) so that some metric of total dissimilarity between the graph Ha and the graph G'a = (π[V (Ga)], π[E(Ga)], αG' , βG' ) is minimized.
Once again, to evaluate algorithms one needs to generate instances of the error correcting graph isomorphism that are ‘easier’ because the attributed graphs are not to far apart. Generation with respect to a measure of disorder would typify in what sense the instances are ‘easy’.
The clustering of a data array [44] has two important practical applica- tions: the design of distributed database systems [51] and the design of web sites [62]. These are instances of hard problems (sometimes identified as belonging to the class NP -Hard) where the answer is a permutation (an element in Sn). Links exist, for example, between McCormick et al. [44] clustering problem and the Traveling Salesman Problem [35]. It is interest-

ing then to consider if these problems are solvable in polynomial time if we know that we have a current solution not further away that a parameter k from the optimal solution. In particular, in the context of parameterized complexity [15].
Other applications of measures of disorder include the study of error-sensitivity of sorting algorithms [29,26] and the study of the behavior of external sort- ing algorithms on nearly sorted sequences [18]. Right invariant metrics have applications in cryptography where they are used to build tests for random permutations [56].
In order for a function to evaluate disorder, its value on a sequence X must depend only on the relative order of the elements in X and not on their particular values. Moreover, it should be minimized when there is no disorder. We formalize these ideas in the following definition.
Definition 1.4 Let |X| denote the length of a sequence X and let M :
N<N → ঩. We say that M is a measure of disorder, or mod, if,
X sorted implies M (X)= min|Y |=|X|{M (Y )}, and
if X = ⟨x1, x2,... , xn⟩, Y = ⟨y1, y2,... , yn⟩ and (xi ≤ xj if and only if
yi ≤ yj), for all i, j ∈ {1, 2,... , n}, implies M (X)= M (Y ).
In this paper we study the set of measures of disorder and how can we ob- tain operational definitions so we can generate pseudo-random nearly sorted sequences. Section 2 presents right invariant metrics and measures of disor- der appearing in the literature. There are many ways to evaluate disorder; however, what constitutes a nearly sorted sequence is intuitively clear.
Mannila observed that Definition 1.4 is too general and, by requiring ad- ditional properties, he defined measures of presortedness.
Definition 1.5 Letting M : N<N → N be some function, we say that M is a measure of presortedness if:
If X is in ascending order, then M (X)= 0.
If X = ⟨x1, x2,... , xn⟩, Y = ⟨y1, y2,... , yn⟩ and xi ≤ xj if and only if
yi ≤ yj, for all i, j ∈ {1, 2,... , n}, then M (X)= M (Y ).
If Y is a subsequence of X, then M (Y ) ≤ M (X).
If X ≤ Y , then M (XY ) ≤ M (X)+ M (Y ).
For all x in N , M (⟨x⟩X) ≤ |X| + M (X).
Estivill-Castro, Mannila and Wood examined how well Definition 1.5 re- flects intuition and showed that this model was incomplete [17]. Estivill- Castro, Mannila and Wood [17] discussed several important subsets of the set

of measures of disorder. Here we show that there are measures of disorder in the literature that do not satisfy Definition 1.5, but for these measures, we can use the proposal by [17] to obtain an equivalent measure of presortedness. However, Definition 1.5 is still unsatisfactory because there are measures of presortedness whose behavior seems to contradict intuition. Despite these problems, Definition 1.5 was a step in the right direction. The tools we use are the necessary and sufficient conditions for a measure of disorder to be extensible to a right invariant metric and the concept of a normal measure of disorder. These measures are precisely those that can be used as right in- variant metrics. We use the characterization [17] of those ri-metrics that are measures of presortedness. These ri-metrics are called regular.
Here, we summarize results to show that normal measures of disorder, up to ranking, are measures defined in terms of sets of sorting operations. This result is used in Section 5 as the basis for pseudo-randomly generating nearly sorted files.

Evaluating disorder
The distance between permutations is particularly useful in statistics for rank correlation methods. The intuition behind it is simple. If we obtain two permutations π, σ in Sn from two independent uniform generators, we expect the distance between π and σ to be close to the average of all possible distances in Sn. However, if the distance between π and σ is small it is reasonable to suspect that there is correlation. Under the hypothesis that the generators are independent, once π has been chosen, σ can be any permutation in Sn with equal probability. A measure of distance from any permutation to the identity is a random variable and its moments (where it is assumed that each permutation has equal probability) are computed. In order to apply this information to π and σ, we must be able to shift π or σ to the identity; that is, we must be able to relabel the data. This brings us to the concept of right invariant metrics.
Diaconis and Graham [11] introduced right invariant metrics (ri-metrics) on permutations to evaluate the distance between two permutations. Statis- ticians normalize these metrics to obtain non-parametric measures of associ- ation that have the properties of a rank correlation coefficient [31, page 4]. Kendall’s τ , the most popular coefficient of correlation, is defined as τ = 1 − 4Inv(σ, π)/n(n − 1), where Inv(π, σ) is the minimum number of pairwise adjacent transpositions required to bring ⟨π−1⟩ into the order ⟨σ−1⟩. Fligner and Verducci [21] use ri-metrics to generalize Mallow’s [41] ranking models. They have studied ranking models based on Cayley’s measure and the Ham-

ming distance. Cayley’s measure is denoted by Exc, and Exc(π, σ) is defined as the minimum number of exchanges required to bring ⟨π⟩ into the order ⟨σ⟩. The Hamming distance between permutation π and σ is the number of posi- tions where the sequences ⟨π⟩ and ⟨σ⟩ differ and it is denoted by Ham(π, σ).

Definition 2.1 A collection of functions {dn : Sn × Sn → ঩}n∈N is a right invariant metric, or ri-metric, if, for all n ∈ N , for all π, σ ∈ Sn,
dn(π, σ) ≥ 0,
dn(π, σ) = 0 if and only if π = σ,
dn(π, σ)= dn(σ, π),
dn(σ, π) ≤ dn(σ, τ )+ dn(τ, π), for all τ ∈ Sn,
dn(σ, π)= dn(σ · τ, π · τ ), for all τ ∈ Sn.
A collection of functions {dn : Sn × Sn → ঩}n∈N is a right invariant pseudo- metric if there is a constant c > 0 such that, for all n ∈ N , dn satisfies 1, 2, 3, and 5 above and
dn(σ, π) ≤ c[dn(σ, τ )+ dn(τ, π)], for all π, σ, τ ∈ Sn.
We will omit the subscript of dn when this is clear from the context. Some examples of ri-metrics are immediately obtained from well known vector met- rics.
 π, σ  p = (Σn	|π(i) − σ(i)|p)1/p, p ≥ 1. (p = 1 is the metric associated
with Spearman’s footrule [57].)
 σ, π  2 = √Σn	|π(i) − σ(i)|2 is the metric associated with Spearman’s
coefficient of correlation ρ =1 − 6 π, σ 2/(n3 − n).
 π, σ  ∞ = max1≤i≤n |π(i) − σ(i)|.
M01(π, σ) = 0 if π = σ, and M01(π, σ) = 1 otherwise, (the discrete metric).
More ri-metrics are obtained from other measures of disarray appearing in the statistical literature. For example, Gordon [22] implicitly defined
Grp(π, σ)= n − {i | π · σ−1(j) < π · σ−1(k) whenever 1 ≤ j < i ≤ k ≤ n}.
From the computational point of view, we can use a metric d in S|X| to measure the disorder in X.
Definition 2.2 Given a sequence X of distinct elements from a total order, X defines a permutation P erm[X] in Sn by P erm[X](i) is the final position of xi when X is sorted. Let d be a metric in S|X|, and define Md by Md(X)= 

d(id, P erm[X]).
And by definition we obtain:
Lemma 2.3 If d is a metric, then Md is a measure of disorder.
Moreover, some of the ri-metrics introduced above give the corresponding measure appearing in the computer science literature. For example, Inv(X)= Inv(id, P erm[X]) and this is the number of inversions in X and Exc(X) = Exc(id, P erm[X]) is the minimum number of exchanges required to sort X. Because of this, we abbreviate d(id, π) by d(π).
There have been other motivations for studying different ways to evalu- ate disorder. Knuth suggested two measures related to Runs, namely Read, the number of readings 3 in a sequence [34, Sec. 5.1.3 Ex. 20], and LRuns, the number of long runs 4 [34, Sec. 5.1.3 Ex. 23]. Burge [6] introduced two measures of presortedness inspired by the number of operations required to merge ascending runs. Let X = ⟨x1,... , xn⟩ and suppose Runs(X) = k
k+1
i=1
then TW eight1(X) is the weight of the lightest binary tree with k +1 leaves and corresponding weights li. (The weight of an internal node is the sum
of the weights of its two children, and the weight of the tree is the sum of the weights of the internal nodes.) To define the second measure we de- note by Ts the minimum weight binary tree with s leaves each of weight 1.
Then, TW eight2(X) = Σd	W eight(T˜ ), where d is the number of runs in
li
Reverse(X) with lengths l˜1,... , ˜ld. Rem(X) stands for the smallest number
of element that must be removed from X to produce a sorted sequence. Note that Rem(X)= |X|− Las(X) where Las(X) is the length of the largest as- cending subsequence of X. This is a very popular measure of disorder, since apparently signals out the items that need to be repositioned. One measure of local disorder is Dis that is defined as the largest distance determined by an inversion. Related to Dis is M ax defined as the largest distance an element must travel to reach its sorted position.
Skiena proposed a measure, named Enc(X), defined as the number of sorted lists constructed by Melsort [55] when sorting X. Recall that S  de- notes the cardinality of a set S Katajainen, Levcopoulos and Petersson [30,36],

3 A sequence is said to require k readings if we must scan it at least k times from left to right in order to read off its elements in nondecreasing order.
4 The long runs of a sequence are obtained by placing vertical lines just before a segment fails to be monotonic; long runs are either increasing or decreasing, depending on the order of their first two elements, so the length of each long run (except possibly the last) is at least 2.



defined


|X|
Osc(X)= 
i=1


|X|
 cross(xi) 	and	Dst(X)= 
i=1


 rcross(xi)  ,

where cross(xi) = {j | 1 ≤ j < |X|, xj+1 < xi < xj} and rcross(xi) =
{j | i < j < |X|, xj+1 < xi < xj}. They studied Heapsort and Local Insertion Sort with respect to these measures. Recently, other measures related to adaptive sorting algorithms have been introduced by Carlsson, Levcopoulos and Petersson [7,37].
The above examples illustrate that disorder can be measured in many ways and although measures differ on the sequences for which they provide small values, what constitutes nearly sorted sequences remains intuitively clear. A sequence is nearly sorted if it requires few operations to sort it or it was created from a sorted sequence by a few perturbations. Measures of disorder are a fundamental model but little can be said about them because of their generality. We would like additional properties that bring measures closer to our intuition about disorder. Definition 1.5 captures some important ideas for making measures correspond to our intuition.

Consequences of the Axioms
Although the disorder in a sequence can be evaluated in many ways, Man- nila [43] proposed the first set of properties that disorder evaluators should satisfy (see Definition 1.5.) We show that if Mannila’s properties are used as an axiomatic definition, the resulting mathematical model seems incomplete. Two types of difficulties are discussed. First, there are measures of disorder that do not qualify as measures of presortedness and intuitively they should. Next, we present EncR, a function that satisfies Definition 1.5; but it is not (intuitively) a measure of presortedness. This suggests that the formal defini- tion does not correspond entirely to our intuition. Despite these drawbacks, the conditions in Definition 1.5 are sufficient to prove properties and general theorems about measures of presortedness.
The diﬃculties with measures of presortedness
Naturally we would like to know if the ri-metrics appearing in statistics lead to measures of disorder that qualify as measures of presortedness (Definition 1.5). The first step in this direction is provided by the following result.
Theorem 3.1 The ri-metrics   ∞, Inv, Exc, M01, Grp, and Ham lead to measures of disorder that qualify as measures of presortedness.

Again, this justifies the use of Inv to denote both the measure of presort- edness and the ri-metric. Also, instead of writing M p =  id, P erm[X]  p we simplify the notation and, for a sequence X, we use  X  p to denote  id, P erm[X]  p. Note that, for p ≥ 1,  p does not satisfy axiom 5 in Definition 1.5, since for example, if n > 1, X = ⟨1, 2,... , n⟩, and x = n + 1, then
n
 ⟨x⟩X  p = (np +	1p)1/p > n = |X| +  X  p.
i=1
The following result shows how close these metrics are to measures of presort- edness. It requires an illustrative but laborious proof (see appendix).
Theorem 3.2  X  p = id, P erm[X]  p satisﬁes Deﬁnition 1.5 except for ax- iom 5.
Theorem 3.2 has provided us with an infinite set of measures of disorder; however, these functions do not necessarily qualify as measures of presorted- ness. Although a function that intuitively evaluates disorder does not qualify as a measure of presortedness, it may be possible to find an equivalent func- tion that qualifies as a measure of presortedness. For example, observe that Runs is actually defined as the number of step-downs and not as the number Read of ascending runs. This scaling is required so that Runs is zero on a sorted sequence (it satisfies axiom 1 in Definition 1.5). Can we use Read to estimate disorder? Clearly, it makes sense, Runs and Read are algorithmically equivalent (see Definition 1.1).
Similarly, Definition 1.5 requires the co-domain of a measure of presort- edness to be a set of non-negative integers; however, functions such as   p are real-valued. A real-valued function M satisfying axioms 1 and 2 in Defi- nition 1.5 has a finite image since the domain S|X| is finite; thus, we may use ranking to obtain an algorithmically equivalent measure rkM defined by
rkM (X)= {M (⟨π⟩) | π ∈ S|X| and M (⟨π⟩) < M (X)}.
The function rkM scales the measure M to nonnegative integers preserving the property that it evaluates to zero on sorted sequences.
Thus, a measure of presortedness will be a representative of a class of disor- der evaluators; namely, all the measures of disorder algorithmically equivalent to it. Although scaling and ranking make Definition 1.5 more flexible, there are other problems.
There are many measures of disorder in the literature that do not qualify as measures of presortedness for which algorithmically equivalent measures of presortedness are hard to find.

We would like the space of measures of presortedness to have some natu- ral structure. For example, since disorder can be evaluated in many ways, given two measures M1 and M2 of presortedness, M (X)= M1(X)+ M2(X) is intuitively a measure of presortedness that incorporates both types of evaluation. However, M (X) may fail to qualify as a measure of presorted- ness. An easy example is to take M1 = M2 = Inv.
There are functions that qualify as measures of presortedness but have an unexpected behavior that contradicts intuition.
Let 5a be the following axiom: “there is a constant c > 0 such that M (⟨x⟩X) ≤ c|X| + M (X)”. Note that if we replace axiom 5 in Definition 1.5 by axiom 5a not only would   p qualify as a measure of presortedness, for all p ≥ 1, but we obtain the following result (see Appendix for proof).
Theorem 3.3 Any linear combination M = Σr	liMi, with nonnegative
constants li, of measures of disorder Mi that satisfy conditions 3 and 4 in Def- inition 1.5 and axiom 5a is a measure of disorder that satisﬁes conditions 3 and 4 in Deﬁnition 1.5 and axiom 5a.
We now present an example of a function that qualifies as a measure of presortedness but, intuitively, it does not evaluate disorder. Consider the fol- lowing property:
Prefix Monotonicity:  If X ≤ Z, Y  ≤ Z and M (X) ≤ M (Y ), then
M (XZ) ≤ M (Y Z).
We claim that it is intuitively natural to require a measure of presortedness to satisfy this property. To support this claim, we first show that twelve important measures have the prefix monotonicity property.
Theorem 3.4 The functions Dis, Inv, Rem, Exc, Runs,   p,   ∞, MGrp, Dst, Osc, M0, M01, satisfy the preﬁx monotonicity property.
Note that if X ≤ Y , then
Inv(XY )= Inv(X)+ Inv(Y ),









and
Rem(XY )= Rem(X)+ Rem(Y ),
Exc(XY )= Exc(X)+ Exc(Y ),
Runs(XY )= Runs(X)+ Runs(Y ),  XY ∞ =  X  ∞ +  Y ∞, MGrp(XY )= MGrp(X)+ MGrp(Y ),

M0(XY )= M0(X)+ M0(Y ).

Thus, for Inv, Rem, Exc, Runs,  ∞, MGrp and M0 the proof follows from the following result.
Lemma 3.5 Let axiom 4a be “if X ≤ Y , then M (XY ) = M (X)+ M (Y ).” If a function M satisﬁes axioms 1,2 and 4 in Deﬁnition 1.5 and, additionally, axiom 4a, then M satisﬁes the preﬁx monotonicity property.
Proof. Axiom 4 implies that if X ≤ Z, then M (XZ) ≤ M (X)+ M (Z); thus, X ≤ Z, Y ≤ Z, and M (X) ≤ M (Y ) imply that M (XZ) ≤ M (X)+ M (Z) ≤ M (Y )+ M (Z)= M (Y Z).	 
To prove Theorem 3.4 for Dis note that
Dis(XZ)= max(Dis(X), Dis(Z))
≤ max(Dis(Y ), Dis(Z)) = Dis(Y Z).
The reader may verify that M01 satisfies the monotonicity axiom, but
M01(⟨2, 1⟩⟨4, 3⟩) < M01(⟨2, 1⟩)+ M01(⟨4, 3⟩).

Therefore, for a monotonic measure of presortedness M , X ≤ Y does not necessarily imply that M (XY )= M (X)+ M (Y ).
The prefix monotonicity property is intuitively desirable. Suppose we re- place a prefix with something that has less disorder. Moreover, suppose that all the elements in the prefix and in the replacement prefix are less than ev- ery other element in the sequence. If a measure does not satisfy the prefix monotonicity property, then the modified sequence has more disorder than the original sequence. It is displeasing that locally sorting a prefix, that is in sorted order with the remainder of the sequence, introduces disorder.
In order to present an example of a function that qualifies as a measure of presortedness but does not satisfy the monotonicity axiom we describe the measure Enc introduced by Skiena [55]. Given a sequence X, Enc(X) is the number of “dequeues” in the “encroaching set” of X. The encroaching set of a sequence X = ⟨x1,... , xn⟩ is defined by the following procedure: We say that xi fits a dequeue D if xi can be added to the front or the end of D so as to maintain D in sorted order. Repeatedly insert xi into the first dequeue that it fits. Create a new dequeue if xi does not fit in any existing dequeue. An example will make this process clear. Consider the sequence X = ⟨4, 6, 5, 2, 9, 1, 3, 8, 0, 7⟩. Initially, D1 consists of 4, and the second element fits at the end of D1. 5 is between 4 and 6, so 5 is added to an empty D2. The next three elements all fit in D1 and are placed there. 3 does not fit in D1 but it fits at the front of D2. Similarly, 8 fits at the end of D2. 0 fits in

D1, but the last element requires a new dequeue. The final encroaching set is
{D1 = [0, 1, 2, 4, 6, 9],	D2 = [3, 5, 8],	D3 = [7]}.
Thus Enc(⟨4, 6, 5, 2, 9, 1, 3, 8, 0, 7⟩) = 3. Enc does not satisfy Definition 1.5 even if it is scaled. That is, Enc−1(X)= Enc(X) − 1 does not satisfy axiom 4, since for X = ⟨1, 5, 2, 4, 3⟩ and Y = ⟨10, 9, 8, 7, 6⟩, Enc−1(X)= 2, Enc−1(Y )= 0 and Enc−1(XY ) = 3. However, if we define a new measure


MEnc(X)= 

Enc(⟨xk,... , xn⟩) otherwise, where
⟨x1,... , xk−1⟩ is sorted and xk−1 > xk

we obtain a measure of presortedness that is algorithmically equivalent to
Enc.
We are now ready to define our counterexample. For a sequence X =
⟨x1, x2,... , xn⟩ define
EncR(X)=	0	if X is sorted
Enc(Reverse(PX )) otherwise
where PX = ⟨x1,... , xk⟩, xk−1 > xk and SX = ⟨xk,... , xn⟩ is sorted. In other words, if X is not sorted, EncR(X) is computed by taking a prefix from X that contains the last step-down in X. If X is sorted, we let PX = ⟨⟩. Then, Enc is computed on the reverse of PX.
Theorem 3.6 EncR is a measure of presortedness.
See appendix for proof. Although EncR is a measure of presortedness, EncR does not satisfy the prefix monotonicity property. Let X = ⟨5, 4, 3, 2⟩, Y = ⟨3, 4, 5, 2, 1⟩, and Z = ⟨7, 9, 10, 8, 11, 6⟩. Thus EncR(X)= 1, EncR(Y )= 
2, and EncR(Z) = 3.  EncR(XZ) = 4 and X < Z, but EncR(Y Z) = 3 and Y < Z. Therefore, EncR demonstrates that there are measures of presortedness with displeasing properties.
Monotonic measures
Mannila [42] was the first to define a measure of presortedness.
“[the properties of Definition 1.5] are general conditions which any measure of presortedness should satisfy.”
But, when discussing future work, he stated that
“The properties (1)-(5) proposed for a measure are not strong. In trying to

strengthen the results of Section 5, additional properties could be useful.”
The following property, which implies the prefix monotonicity property, is intuitively natural.
Definition 3.7 Let M : N<N → ঩ be a measure of disorder; we say that M is a monotonic measure of disorder if W ≤ X ≤ Z, W ≤ Y ≤ Z and M (X) ≤ M (Y ) imply that M (WXZ) ≤ M (WY Z).
Mannila [42] used two approaches to justify the conditions for a measure of presortedness. He considered a concrete approach, where disorder is quantified by the number of operations of a given type which are needed to sort the input, and an information-theoretic approach, where disorder is quantified by how much information of the form xi < xj must be collected to identify a sequence.
Using the same concrete approach we observe that if WY Z is a sequence where W < Y and Y < Z, then the elements in Y are in their correct positions with respect to the elements in W and Z. If we rearrange the elements of Y and obtain X with M (X) < M (Y ); then, according to M , there is less disorder in X than in Y . That is, less operations are needed to sort X than to sort Y . Thus, sorting WXZ needs no more operations than sorting WY Z.
Using the information theoretic approach, if W < X, W < Y , X < Z, Y < Z and no more comparisons are needed to identify X than to identify Y , then to identify WXZ requires comparisons to identify W , X and Z. But the number of comparisons needed cannot be more than those required to identify WY Z under W < Y < Z.
Note that monotonicity is not implied by the conditions in Definition 1.5, since monotonicity implies the prefix monotonicity property. By verifying Definition 3.7 directly, we can prove the following result.
Corollary 3.8 Inv, Runs, Rem, Exc, Dis, m0, m10, MEnc[k,A,D] , MGrp,   p
and  ∞ are monotonic measures of disorder.
Although the conditions for measures of presortedness are incomplete, we demonstrate that it is a step in the right direction. We claim that measures of presortedness provide a constructive mathematical model and are closer to intuition than measures of disorder. Mannila’s goal was to obtain conditions for the existence of M -optimal algorithms for a measure M . We are able to establish some general results on the behavior of measures of presortedness.
Theorem 3.9 Let M be a measure of presortedness, then the sequences X =
⟨n, 1,... ,n − 1⟩ and X = ⟨2, 1, 4, 3, 6, 5,.. .⟩ have at most a linear amount of disorder; that is, M (X) is O(n).

Proof. Let X = ⟨n, 1,... ,n − 1⟩. Then M (X) ≤ |X| + M (⟨1,... ,n − 1⟩) by axiom 1.5-5. From axiom 1.5-1, M (⟨1,... ,n − 1⟩) = 0; therefore, M (X) ≤
|X|. Now, let X = ⟨2, 1, 4, 3, 6, 5,.. .⟩. By axioms 1.5-2 and 1.5-4,
M (X) ≤ M (⟨2, 1⟩)+ M (⟨4, 3, 6, 5,.. .⟩).
An inductive argument gives M (X) ≤ |X|M (⟨2, 1⟩)/2= O(|X|).	 
Theorem 3.10 For any measure of presortedness M and any sequence X, the value M (X) is O(|X|2).
Proof. Using axiom 1.5-5 repeatedly we obtain M (⟨x1,... , xn⟩) ≤  |X| i =
O(|X|2).	 
The following result shows that, if M is a measure of presortedness such that M (X) = 0 implies X is sorted, then Wn = {π ∈ Sn | rkM (⟨π⟩) = 1} is always a set of generators of Sn.
Lemma 3.11 Let M be a measure of presortedness such that M (X) = 0 implies X is sorted and let Wn = {π ∈ Sn | rkM (⟨π⟩) = 1}. If π ∈ Sn is a transposition of adjacent elements, then π ∈ Wn.
Proof. Let ⟨π⟩ = ⟨1,... ,i − 1,i + 1, i,i + 2,... , n⟩. M (⟨π⟩) /=0 since ⟨π⟩ is not sorted. M (⟨π⟩) ≥ M (⟨i + 1, i⟩) = M (⟨2, 1⟩) by axioms 2 and 3, Defini- tion 1.5. Now, M (⟨π⟩) ≤ M (⟨1,... ,i − 1⟩)+ M (⟨i + 1, i⟩)+ M (⟨i + 1,... , n⟩) by axiom 4. Therefore, M (⟨π⟩)= M (⟨2, 1⟩). Since, for any unsorted sequence X, M (X) ≥ M (⟨2, 1⟩) by axioms 2 and 3, we conclude that rkM (⟨π⟩)= 1. 
Conversely, if π ∈ Wn implies that π is a transposition of adjacent elements, then rkM = Inv and M is algorithmically equivalent to Inv.

The relation with ri-metrics
In this section, we revise the relationship between ri-metrics and measures of disorder [17]. This reviews the necessary and sufficient conditions for a measure of disorder to be extended to a ri-metric. If a measure of disorder can be extended to a ri-metric, it is called normal [17]. Examples of normal measures are M01, Inv, MGrp, Exc, Ham, and Rem. We also recall the necessary and sufficient conditions for a ri-metric to be used as a measure of presortedness. This results allow a method to naturally construct normal measures; namely, providing sets of sorting transformations. Conversely, if we are given a normal measure, we can almost always identify a set of operations that defines the measure up to ranking.

The necessary and sufficient conditions for a measure to be extended to a
ri-metric are a consequence of the following two technical results [17].
Lemma 4.1 If d is a ri-metric, then d(id, π)= d(id, π−1); and d(id, π · σ) ≤
d(id, π)+ d(id, σ).
Recall that if π ∈ Sn, then ⟨π⟩ denotes the sequence ⟨π(1),... , π(n)⟩. We abbreviate d(id, π) by d(π).
Lemma 4.2 If M is a measure of disorder such that
M (X)= 0 implies X is sorted, and
there are constants a, b ≥ 0, such that, for all n ∈ N and for all π, σ ∈ Sn, we have M (⟨π · σ⟩) ≤ a M (⟨π⟩)+ b M (⟨σ⟩),

then

dM (π, σ)= 
M (⟨π · σ−1⟩)+ M (⟨σ · π−1⟩)
a + b

is a ri-pseudo-metric.
In the next definition we describe those measures that are extensible to ri-metrics.
Definition 4.3 Let M be a measure of disorder. We say that M is normal
if,
M (X) = 0 implies X is sorted,
for all n ∈ N , and for all π ∈ Sn, M (⟨π⟩)= M (⟨π−1⟩), and
for all n ∈ N , and for all π, σ ∈ Sn, M (⟨σ · π⟩) ≤ M (⟨σ⟩)+ M (⟨π⟩).
Normal measures are well-behaved measures in the following sense. If we are told that there is no disorder in a sequence, then it is because the sequence is sorted. By applying a permutation σ to a sorted sequence and then applying another permutation τ , we can produce only as much disorder as the disorder produced by each of the permutations σ and τ . Since we need apply only π−1 to sort a permutation π, and we need apply only π to sort π−1, the disorder in π should be the same as the disorder in π−1.
Theorem 4.4 Let M be a mod. Let dM (π, σ) = (M (⟨π · σ−1⟩)+ M (⟨σ · π−1⟩))/2. The function dM is a ri-metric such that dM (id, π)= M (⟨π⟩) if and only if M is normal.
Examples of this result are Exc, MGrp, Ham, Inv, M01, and Rem [17]. Moreover, we have dRem(id, σ)= Rem(⟨σ⟩) and this corresponds to a ri-metric implicitly defined by Gordon [23].

Theorem 4.5 Any linear combination, with positive coeﬃcients, of normal measures is a normal measure.
Proof. Let Mi be normal measures of disorder for i = 1,... , k, and c1, c2,... , ck
be positive constants. We will prove that M (X)= Σk	ciMi(X) isa normal
mod. First, if X is sorted; then, Mi(X)= 0, for i = 1,... , k; thus M (X)= 0.
Now, if M (X) = 0, since c1 > 0, M1(X) = 0; thus, X is sorted. Second,

M (⟨π⟩)= Σk
k i=1
k
ciMi(⟨π−1⟩)= M (⟨π−1⟩). Finally,

M (⟨π · σ⟩)=	ciMi(⟨π · σ⟩)
i=1 k
=	ci [Mi(⟨π⟩)+ Mi(⟨σ⟩)]
i=1
= M (⟨π⟩)+ M (⟨σ⟩)
as required	 
Applying Lemma 3.11 to normal measures gives the following corollary.
Corollary 4.6 Let M be a normal measure of presortedness and let
Wn = {π ∈ Sn | rkM (⟨π⟩)= 1}.
If π ∈ Sn is a transposition of adjacent elements, then π ∈ Wn.
In fact, for normal measures of presortedness, we obtain the following stronger result than Theorem 3.10.
Theorem 4.7 If M is a normal measure of presortedness, then there is a
K > 0 such that, for all X ∈ N<N , M (X) ≤ K · Inv(X).
Proof. By Corollary 4.6, write P erm[X] as the product of Inv(X) transpo- sitions of adjacent elements. Then M (X) ≤ M (⟨2, 1⟩)Inv(X) as required. 
This implies, for example, that any sorting algorithm that is adaptive with respect to M (the smaller the value of M , the less time is spent by the sorting algorithm) is also adaptive with respect to Inv. Although Inv plays an important role among ri-metrics and measures of presortedness, its role is not yet fully understood.

Random generation of nearly sorted sequences
Imagine you would like to test the ability of a Rubik Cube sorter to adapt the number of operations performed to the difficulty of the input configuration.

In fact, you want to test the sorter in different configurations that are no more than k turns away from Start (the sorted position) and see if the sorter performs a number of operations proportional to k. How would you shuffle the cube randomly, so that you have some control of the disorder and the distribution of the generated configurations?
We propose the following strategy. Given the cube at Start, for i = 1 to k, choose with equal probability either to make one of the allowable turns in the cube or not to make a turn. That is, if there are r allowable turns, then the probability of not making a turn is 1/(1 + r). Clearly, the result is not more than k turns away from Start and, in the case k = 1, it produces a configuration of the cube that is less than one turn away from Start with equal probability.
In this section we show that this strategy is applicable to normal measures. For any normal measure M there are sets of sorting operations Wn = {π ∈ Sn | rkM (⟨π⟩) = 1} such that the minimum number of sorting operations in W|X| required to sort X equals the minimum number of sorting operations in W|X| required to construct X from the sorted sequence containing the elements in X [17]; that is, MW (X)= MW (X), where
MW (X)= min{k | π1,... , πk ∈ W|X| and (... (Xπ1 )π2 .. .)πk is sorted}, and
MW (X)= min{k | π1,... , πk ∈ W|X| and P erm[X]= π1 · π2 · ... · πk}.
In fact, normal measures provide Sn with a regular graph structure.
Definition 5.1 Let M be a normal measure of disorder. We define the sorting graph of Sn with respect to M (denoted by SGM (n)) as follows. Sn is the set of vertices of SGM (n) and two nodes π,σ ∈ Sn are adjacent if and only if dMW (π, σ) = 1. (See Lemma 4.2 and Theorem 4.4).
For example, Knuth [34, Page 13, Figure 1] gives SGInv(4).
Theorem 5.2 For any n ∈ N and any normal measure M, SGM (n) is a regular graph of degree  Wn  = {π ∈ Sn | rkM (⟨π⟩) = 1} = {π ∈ Sn | MW (⟨π⟩)= 1}.
Proof. Let σ ∈ Sn be any vertex in SGM (n), Γ(σ) be the set of vertices adjacent to σ, and Wn = {π ∈ Sn | MW (⟨π⟩) = 1}. We define a bijection ψ from Wn onto Γ(σ) by ψ(τ )= τ · σ.
Now, since M is normal, dMW is a ri-metric and by Theorem 4.4, dMW (τ ·
σ, σ)= dMW (τ, id)= MW (⟨τ ⟩)= 1; thus, ψ(τ )= τ · σ ∈ Γ(σ).

Suppose τ1, τ2 ∈ Wn and ψ(τ1)= ψ(τ2). Then, τ1 · σ = τ2 · σ or τ1 = τ2 and we conclude that ψ is injective.
To prove ψ is onto, let π ∈ Γ(σ); we must exhibit a τ  ∈ Wn such that
ψ(τ ) = π. Let τ = π · σ−1; then, ψ(τ ) = (π · σ−1) · σ = π and MW (⟨τ ⟩) = 

MW (⟨π · σ−1⟩). Since M is normal, MW is normal and dM
is a ri-metric;

therefore, MW (⟨π · σ−1⟩)= dM  (id, π · σ−1)= dM  (σ, π)= 1, since π ∈ Γ(σ).
Thus, τ ∈ Wn and the proof is complete.	 

Theorem 5.2 proves that for a normal measure we obtain the structure of a “vertex-transitive graph” [2]. Since the set Wn of generators may be redundant, methods to efficiently construct “a strong generating sets” [3] may be very useful.
Let M be a measure of disorder. Informally, we say that a pseudo-random generator of nearly sorted sequences (PRG) with respect to M is an algo- rithm that given nonnegative integers k and n uses a pseudo-random number generator to produce a permutation π ∈ Sn with M (⟨π⟩) ≤ k. A uniform pseudo-random generator can be constructed as follows. Given k and n, list all the permutations in below'(k, n, M ) = {π ∈ Sn | M (⟨π⟩) ≤ k} in some canonical order and use a uniform pseudo-random number generator to gener- ate the index of one of them. This algorithm, however, takes exponential time for interesting measures; therefore, it is impractical. Also, below'(k, n, M ) is not a subgroup, so direct application of the methods in [3] is not possible.
Let M be an integer-valued normal measure of presortedness (if M is not integer-valued, we use rkM which has the same below sets). Let Wn = {π ∈ Sn | MW (⟨π⟩) = 1}. Any PRG must produce id ∈ Sn for k = 0. For k = 1, any PRG must produce a permutation in Wn ∪ {id}; moreover, for k = 1, a uniform PRG should choose π ∈ Wn ∪ {id} with equal probability. We generalize these observations to give a procedure to generate pseudo-random nearly sorted sequences with respect to a normal measure of disorder. We assume we have a procedure Select(M, n) that generates a permutation τ from Wn ∪ {id} randomly and uniformly. Note that this is equivalent to uniformly choosing a permutation in below(1, n, MW ) an thus, the implementation of Select is a much simpler problem that we will discuss later on. We give the procedure Generate in Figure 1. Let k and n be nonnegative integers. We initialize π0 to be id ∈ Sn. Procedure Select chooses k permutations τi from Wn ∪ {id} uniformly; that is, the probability of selecting τi ∈ Wn ∪ {id} is 1/(1 +  Wn  ). We then form the product of πk−1 with the permutations τi to give the permutation πk. Our method for generating pseudo-random nearly sorted sequences has three fundamental features.
First fundamental feature: Generate distribution on equal(z, n, M )= {π ∈

procedure Generate(M, n, k);
initialize π ← id ∈ Sn; for i := 1 to k do begin
τ ← Select(M, n); π ← π · τ
end
Fig. 1. Generate returns the product of k permutations τi with M (⟨τi⟩) ≤ 1.
Sn | M (⟨π⟩)= z} (the equal sets with respect to MW ) is uniform.
Second fundamental feature: Every permutation in below'(k, n, MW ), which we recall it is defined as below'(k, n, MW )= {π ∈ Sn | MW (⟨π⟩) ≤ k}, can be generated.
Third fundamental feature: Generate is practical for small k and large n. The following theorem supports the first two fundamental features. We denote
conditional probability of event A given event B by Pr[A|B], and we recall that
it is defined by Pr[A|B] = P [AB]/P [B] if P [B] > 0 and is left undefined if
P [B]= 0.
Theorem 5.3 Let M be an integer-valued normal measure of disorder and let k and n be nonnegative integers. If Pr[Select(M, n)= τ ]= 1/(1 +  Wn  ), for each τ ∈ Wn ∪ {id}, then
for any nonnegative integer s ≤ k, let
ps,k = Pr[ Generate(M, n, k)= τ | M (⟨Generate(M, n, k)⟩)= s ],

then


and



ps,k
=  1/  equal(s, n, MW )  if MW (⟨τ ⟩)= s
 0	otherwise

Pr[Generate(M, n, k)= τ ] > 0, for each τ ∈ below'(k, n, MW ). To prove the theorem we require two technical lemmas.
Lemma 5.4 Let n be an integer and M be an integer-valued normal measure of disorder; then, for all σ, τ ∈ Sn such that MW (⟨σ⟩)= s = MW (⟨τ ⟩),
 Γ(σ) ∩ equal(s − 1, n, MW )  = Γ(τ ) ∩ equal(s − 1, n, MW )  ,
 Γ(σ) ∩ equal(s, n, MW )  = Γ(τ ) ∩ equal(s, n, MW )  , and
 Γ(σ) ∩ equal(s + 1, n, MW )  = Γ(τ ) ∩ equal(s + 1, n, MW )  .

Proof. By induction on s.
Basis: If s = 0, then MW (⟨σ⟩) = 0 = MW (⟨τ ⟩) implies that σ = id = τ.
Thus, 1, 2 and 3 follow trivially.
Induction step: Let s ≥ 1. To prove 1 use the induction hypothesis for 2. To prove 2 the induction hypothesis for 3. Now,





and
1
[Γ(σ) ∩ equal(s + i, n, MW )] = Γ(σ)
i=−1

1



i= −1
[Γ(τ ) ∩ equal(s + i, n, MW )] = Γ(τ ).

Since both of the unions above are disjoint unions and the graph SGM (n) is regular, Γ(σ)  = Γ(τ )  . This, together with 1 and 2, implies 3.	 
Lemma 5.5 Let n be an integer and M be an integer-valued normal measure of disorder. For all τ, σ ∈ Sn, such that MW (⟨τ ⟩)= MW (⟨σ⟩) we have
Pr[Generate(M, n, k)= τ ]= Pr[Generate(M, n, k)= σ],
for all k ≥ 0.
Proof. We prove the lemma by induction on r = MW (⟨τ ⟩)= MW (⟨σ⟩).
Basis: If r = 0, then MW (⟨τ ⟩)= 0MW (⟨σ⟩) implies that τ = id = σ and
Pr[Generate(M, n, k)= τ ]= Pr[Generate(M, n, k)= σ], for all k ≥ 0.
Induction step: Let r ≥ 1 and τ, σ ∈ Sn be such that r = MW (⟨τ ⟩) =
MW (⟨σ⟩). We prove that
Pr[Generate(M, n, k)= τ ]= Pr[Generate(M, n, k)= σ],
for all k ≥ 0, from which the claim follows directly.
Let {τ (<),τ (<),... ,τ (<)} = equal(r−1, n,M )∩Γ(τ ); that is, {τ (<),... ,τ (<)}
1	2	l	1	l
are the nodes in SGM (n) that are adjacent to τ and have less disorder than
τ . Similarly, let {τ (=),... ,τ (=)} = equal(r, n, M ) ∩ Γ(τ ) be the nodes with
1	m
equal disorder and finally let {τ (>),... ,τ (>)} = equal(r + 1, n,M ) ∩ Γ(τ ) be
1	p
the nodes with greater disorder. Then,
Pr[Generate(M, n, k)= τ ]= 



 Γ(τ ) ∩ equal(r − 1, n, MW )
i
i=1
m

+	1	 Σ Pr[Generate(M, n, k − 1) = τ (=)]






 Γ(τ ) ∩ equal(r + 1, n, MW )  Similarly, by Lemma 5.4, we have
Pr[Generate(M, n, k)= σ]= 
i
i=1



l

		1		Pr[Generate(M, n, k − 1) = σi(<)] Γ(σ) ∩ equal(r − 1, n, MW )
i=1
m
+		1		Pr[Generate(M, n, k − 1) = σi(=)] Γ(σ) ∩ equal(r, n, MW )
i=1
r
+		1		Pr[Generate(M, n, k − 1) = σi(>)] Γ(σ) ∩ equal(r + 1, n, MW )
i=1
The induction hypothesis implies that
Pr[Generate(M, n, k − 1) = τ i(<)]= Pr[Generate(M, n, k − 1) = σi(<)], Pr[Generate(M, n, k − 1) = τ i(=)]= Pr[Generate(M, n, k − 1) = σi(=)]

and

Pr[Generate(M, n, k − 1) = τ i(>)]= Pr[Generate(M, n, k − 1) = σi(>)].

Thus, Lemma 5.4 prove Equation (1).	 
Proof of Theorem 5.3: To prove the first claim let k ≥ 0 and 0 ≤ s ≤ k. We use the definition of conditional probability to show that if τ, σ ∈ equal(s, n, MW ), then
Pr[ Generate(M, n, k)= τ | MW (⟨Generate(M, n, k)⟩)= s ]
= Pr[ Generate(M, n, k)= σ | MW (⟨Generate(M, n, k)⟩)= s ],
from which the first claim in follows immediately. Using Lemma 5.5 we obtain the following derivation.
Pr[ Generate(M, n, k)= τ | MW (⟨Generate(M, n, k)⟩)= s ]= 
= Pr[Generate(M, n, k)= τ and MW (⟨Generate(M, n, k)⟩)= s]
Pr[MW (⟨Generate(M, n, k)⟩)= s]
= Pr[Generate(M, n, k)= τ and MW (⟨τ ⟩)= s]
Pr[MW (⟨Generate(M, n, k)⟩)= s]
= Pr[Generate(M, n, k)= σ and MW (⟨σ⟩)= s]
Pr[MW (⟨Generate(M, n, k)⟩)= s]
= Pr[ Generate(M, n, k)= σ | MW (⟨Generate(M, n, k)⟩)= s ]
The second claim follows from the fact that every τ in below'(n, k, MW ) can be factored into the product of no more than k permutations in Wn. Thus, every τ can be represented as the product of k permutations in Wn ∪{id} and,

thus, Pr[Generate(M.n, k)= τ ] > 0.	 

Theorem 5.3 proves that Generate maximizes the entropy [47, Page 174] of the distributions on all equal sets. Therefore, at least on these sets, Generate does not disclose information that characterizes the generated permutation. Note that Generate does not produce uniformly probable permutations on the below sets. The uniformity is restricted to the equal sets and that is all. Procedure Generate should not be used in simulations to corroborate theoretical results for uniform distributions on other sets that are not equal sets. However, we will prove three additional features of procedure Generate that in some sense show that Generate is the best possible.
First additional feature: For any n, and any normal measure M of presort- edness, the distribution of the permutations produced by Generate tends to the uniform distribution on Sn when k tends to infinity. This is a natural and expected property.
Second additional feature: Generate can be generalized to obtain pseudo- random permutations near a given permutation and not only near the iden- tity. Again, this is natural since the normality of the measure gives Sn a regular structure.
Third additional feature: Any other generator that uses the sorting (shuf- fling) operations defined by the measure and with the properties listed above is equivalent to Generate. Thus, Generate is in some sense best possible. This means for example, that we cannot shuffle Rubik’s cube (without vio- lating the restrictions imposed by the mechanics of the cube) in any other way and obtain a distribution with the properties claimed.
The stochastic process defined by Generate is a Markov chain [52, Chapter 4]. SGM (n) is finite; thus, we assume we have an enumeration of the nodes in SGM (n) given by Sn = {π1, π2,... , πn!}. Let Pij denote the probability that the process will, at node πi, next make a transition to node πj. Thus, we have Pij ≥ 0, for i, j ≥ 1, and
n!
Pij = 1,	for i = 1,... , n!.
j=1
Moreover, let r = Wn  be the degree of SGM (n). Then, Pii = 1/(1 + r), for all i, Pij = 1/(r + 1) for r values of j with j /= i, and Pij = 0 for all other values of j.

Let Ps
denote the s-step transition probability; that is, the probability

that the process will move from πi to πj in s transitions. Recall that node πi

procedure Gen-Generate(σ, M, n, k);
initialize π ← σ ∈ Sn; for i := 1 to k do begin
τ ← Select(M, n); π ← π · τ
end
Fig. 2. Gen-Generate returns the product of σ with k permutations τi with M (⟨τi⟩) ≤ 1
in a Markov chain is said to have period d if Ps = 0 whenever s is not divisible by d. Since Pii = 1/(1+r) for all i, all nodes are aperiodic. Furthermore, since we have a finite state Markov chain, all states are recurrent 5 and all recurrent states are positive recurrent 6 . This means that we have an irreducible ergodic Markov chain and we can apply Theorem 4.1 from Ross [52, page 145].
Theorem 5.6 For an irreducible ergodic Markov chain, lims→∞ Ps exists and is independent of i. Furthermore, letting Pj = lims→∞ Ps , for j > 0, the
unique nonnegative solution of Pj = Σn! PjPij, for j > 0 and Σn! Pj = 1,
is Pj.
We observe that Pj = 1/n!, for j = 1,... , n!, is a solution to the system defined above and we obtain the following theorem.
Theorem 5.7 Generate(M, n, k) produces a distribution of the permutations that converges to the uniform distribution on Sn as k goes to inﬁnity.
In fact, Generate can be parametrized to allow generation of permutations close to a permutation σ. Instead of initializing π0 to id ∈ Sn we initialize π0 to σ as shown in Figure 2. The properties outlined in Theorem 5.3 and Theorem 5.7 can be extended for the generalized version of Generate.
Theorem 5.8 Let M be an integer-valued normal measure of presortedness, let k and n be nonnegative integers, and let σ ∈ Sn. If Pr[Select(M, n)= τ ]= 1/(1 +  Wn  ), for all τ ∈ Wn ∪ {id}, then
For all τ1, τ2 such that dMW (σ, τ1)= dMW (σ, τ2) ≤ k,
Pr[Gen-Generate(σ, n, M, k)= τ1)] = Pr[Gen-Generate(σ, n, M, k)= τ2].

5 A state s is recurrent if the probability that, starting in state i, the process will ever reenter state s is 1.
6 A recurrent state s is positive recurrent if, starting in s, the expected time until the process returns to state s is finite. Positive recurrent aperiodic states are called ergodic.

For all τ such that dMW (τ, σ) ≤ k, Pr[Gen-Generate(σ, n, M, k) = τ ] >
0.
For all τ ∈ Sn, limk→∞ Pr[Gen-Generate(σ, n, M, k)= τ ]= 1/n!
For all τ1, τ2 such that dMW (σ, τ1) ≤ 1 and dMW (σ, τ2) ≤ 1
Pr[Gen-Generate(σ, n, M, k)= τ1]= Pr[Gen-Generate(σ, n, M, k)= τ2]
Moreover, we prove that Gen-Generate is the only Markov chain with the properties described in Theorem 5.8.
Theorem 5.9 Let M be a integer-valued measure of presortedness, A be any Markov chain describing a random walk with states in Sn, and Pij be the probability of A changing from node πi to πj. If A is such that

dMW
(πi, πv) = dMW
(πi, πu) implies Pk
= Pk , for all k; (that is, A is

uniform on the equal sets);
lims→∞ Ps = 1/n!; (that is, A converges to the uniform distribution);
πi, πj ∈ Sn and MW (πi, πj) = k implies Ps = 0, for s < k; (that is, if
two items are at distance k it is impossible to move from one to the other in less that k steps);
then, there is p > 0 such that Pii = p, for i = 1,... , n!, and if i /= j,



Pij
=  (1 — p)/r if πi is adjacent to πj in SGM (n)
 0	otherwise

Proof. Let πu, πv be adjacent to πi in SGM (n); then, dMW (πi, πu) = 1 =
dMW (πi, πv). Therefore, by hypothesis 1, Piu = Piv.
Now, if πi is not adjacent to πj, then dMW (πi, πj) > 1; thus, Pij = 0. To conclude the proof, observe that A must be ergodic and time reversible, which implies that A is symmetric.	 
If we require A to maximize the entropy in the below'(1, n,M ) sets, then
A must be equal to the Gen-Generate.
Corollary 5.10 Let A is as in Theorem 5.9. Suppose for πi, πj, πu ∈ Sn, dMW (πi, πu) ≤ 1 and also dMW (πj, πu) ≤ 1 implies Piu = Pju (it is uniform at each node), then Pii = 1/(1 + r) and Pij = 1/(1 + r) if and only if i /= j and πi is adjacent to πj in SGM (n).
The rate of convergence to the uniform distribution of Markov chains that represent shuffling processes has recently received attention in the statisti- cal literature [1,13,12]. Theorem 5.7 guaranties convergence. In fact, since

Generate defines a random walk on a finite group 7 (Sn) and, by Theorem 5.3, the corresponding probability distribution is not concentrated on a subgroup 8 or a translate 9 of a subgroup. Therefore, we can apply the results of Aldous and Diaconis [1] to strengthen Theorem 5.7.
Theorem 5.11 The distribution of permutations produced by Generate con- verges to the uniform distribution at a geometric rate.
However, for each measure M of disorder, a more precise description of the rate of convergence to the uniform distribution by the shuffling process defined by Generate may be achieved. For example,
for Exc, if k is larger than 1 n log n, then Generate(Exc,n,k) is very close to uniform and if k is less than 1 n log n, then Generate(Exc,n,k) is very
far from uniform [13].
Let σi be a permutation that has at most one nontrivial cycle 10 and this nontrivial cycle is a cyclic shift of ⟨1,... , i⟩; that is, σi = (1 ... i)(i + 1) ... (n). Let Wn = {σi | i = 2,... , n}. Then, Generate(MW , n, k) is very close to uniform if k > n log n and very far from uniform if k < n log n [1].
For many other normal measures, a precise description of the rate of con- vergence is not known. We have modified the proof of the second exam- ple above to show that there is a c > 0 such that if k > cn log n, then Generate(Rem, n, k) is very close to uniform.
We are left with describing how Select(M, n) uniformly chooses a permu- tation τ such that M (⟨τ ⟩) ≤ 1. For Inv, the problem is not difficult. We uniformly choose a number t in {1, 2,... , n}. If t = n we set τ = id. Other- wise, we set τ = (t t+1); that is, τ is the permutation that swaps the t-th and (t + 1)-th elements. Furthermore, in an implementation of Generate, rather than building τ and applying it to πi we swap the t-th and (t + 1)-th elements of the array representing πi to obtain πi+1. Thus, π ← π · Select(M, n) takes

7 A group (G, ·) consists of a set G and a binary operation · : G × G → G, such that:
the operation · is associative,
there is an identity element e such that, for all a ∈ G, a · e = a = e · a, and
for all a ∈ G, there is an inverse a−1 ∈ G such that a · a−1 = e = a−1 · a.
8 A subgroup S of a group (G, ·) is a subset of G such that (S, ·) is a group.
9 Let (G, ·) be a group and S ⊆ G a subgroup. We say that Sa = {s · a|s ∈ S} and
aS = {a · a|s ∈ S} are translates of S.
10 A cycle (i0, i2,... ir−1) ina permutation π means π(ij)= j(i+1)mod(r). Every permutation can be represented as a product of its cycles. A cycle is a permutation that leaves all other points fixed.

constant time. A similar strategy gives Select(Exc, n).
For Rem, choose t ∈ {1, 2,... , n} uniformly and then select one of the n — 1 gaps in ⟨1, 2,...,t — 1,t + 1,... , n⟩ in which to insert t and produce a sequence with Rem(X) ≤ 1. Since there are permutations that can be ob- tained in two different ways, in order for Select(Rem, n) to be uniform, the gaps should be selected as follows.
Case t = 1. Choose the first gap with probability p1 = 	1	.
		|X|+(|X|−2)(|X|−1)
Choose the gap (2, 3) with probability p2 =	|X|	 .	For i =
2(|X|+(|X|−2)(|X|−1))
3,... , n, choose the gap after i with probability pi = 1−p1−p2 .
|X|−2
Case t = n. Choose the last gap with probability p	=	1	.
|X|+(|X|−2)(|X|−1)
Choose the gap (|X|, |X|— 1) with probability p	= 	|X|	 .
2(|X|+(|X|−2)(|X|−1))

For i = 1,... , |X|— 2, choose the gap before i with probability pi
= 1−pn−pn−1 .
|X|−2

Case 1 < t and t < n.	Choose the gap (t — 1,t + 1) with probability
pc = 	1	. Choose the gap (t — 2,t — 1) and (t + 1,t + 2) with
|X|+(|X|−2)(|X|−1)
probability ps = 	1	.  Choose all other gaps with probability
|X|+(|X|−2)(|X|−1)
pi = 1−pc−2ps .
|X|−3
The design of procedure Select(M ax, n) demands more effort. Let En =  below'(n, 1,Max)  denote the number of permutations π in Sn such that M ax(⟨τ ⟩) ≤ 1. Clearly E0 = 0, E1 = 1 and E2 = 2. Now, consider π ∈ below'(n, 1,Max) with π(1) = 1. Thus,
1 ≥ M ax(⟨π⟩)= M ax(⟨π(1)⟩⟨π(2),... , π(n)⟩)
= M ax(⟨π(2),..., π(n)⟩).
Therefore, P erm[⟨π(2),... , π(n)⟩] ∈ below'(n — 1, 1,Max).	If π(1) = 2,
then π(2) = 1 and 1 ≥ M ax(⟨π⟩) = M ax(⟨π(1), π(2)⟩⟨π(3),... , π(n)⟩) = max{1,M ax(⟨π(3),... , π(n)⟩)}. Thus, P erm[⟨π(3),... , π(n)⟩] ∈ below'(n — 2, 1,Max). We conclude that En = En−1 + En−2 for n ≥ 3. It is not hard to see that for n ≥ 1, En = Fn+1 where Fn is the n-th Fibonacci number. Thus,
En = √1 (φn+1 — φˆn+1) for n ≥ 1, and we have proved the following theorem.
Theorem 5.12


lim
n→∞
 {π ∈ Sn | M ax(⟨π⟩) ≤ 1}    {π ∈ Sn−1 | M ax(⟨π⟩) ≤ 1}

= φ,

where φ is the golden ratio.
Using this result we code Select(M ax, n) for selecting uniformly from below'(1,Max, n) as shown in Figure 3. We assume that random uniformly generates a real number in [0, 1).
Other methods have been proposed to generate pseudo-random nearly

procedure Select(M ax, n); initialize π ← id ∈ Sn; i ← 1; while i < n do
if random ≤ 1/φ then
π ← π · (i i + 1); i ← i + 2;
else i ← i + 1;
Fig. 3. The implementation of Select(Max, n) allows generation of permutations with small amount of local disorder.



sorted sequences. To carry out their experiments, Cook and Kim [8] designed an ad hoc algorithm to generate sequences X with small Rem(X). Their method has several drawbacks. It only works when Rem(X) is much smaller than |X| and, if successful, the set of elements that must be removed from X to obtain a sorted subsequence is unique. This makes the method biased for sequences with a single large sorted subsequence and a small subsequence of elements out of order. Furthermore, no properties about the distribution corresponding to this method are known; thus, simulation results are difficult to analyze.
The method to generate permutations of order n uniformly [33, pages 139-140] is as follows. Let L = ⟨r1 < ... < rn⟩ be a sorted sequence of n distinct elements. Repeatedly, select an element uniformly from L, remove it from L and place it in the output stream. Oommen and Ng [50] presented a
generalization of this method. They consider a control vector S = [s1,... , sn]
with Σn	si = 1, si ≥ 0 and a conditional control vector S|L = [s' ,... , s' ] as
the vector of normalized probabilities of S, where the elements still in L are the only ones with nonzero probability. That is,


i	, s / Σ
s otherwise.

Their method works as follows. Repeatedly, choose an element ri from L based on the distribution S|L, remove it from L and place it in the output stream. Oommen and Ng use a parameter ρ ∈ [0, 1] to specify the n values in the control vector S. They call ρ the degree of uniformity. When ρ = 0, the identity permutation is always generated. When ρ = 1, S = [1/n,... , 1/n] and the generated permutation is uniformly distributed. As ρ approaches 0, nearly sorted permutations become more likely.
Oommen and Ng propose two strategies to relate ρ to the control vector.

The geometric progression relates ρ and S by


si =
, 1−ρn
if i =1 

ρsi−1 for i = 2,... , n.
and the arithmetic progression defines si by
n + (n — i + 1)(1 — ρ)
si =  2	 .
n [2n +1 — ρ(n + 1)]
Unfortunately, Oommen and Ng consider n to be small when n is 2, 3 and 4, and n to be large when n is between 5 and 15. We are interested in values of n between 10,000 and 1,000,000. We have implemented both the geometric progression method and the arithmetic progression method. For the geometric method, note that, si → 0 geometrically as i → ∞. Thus, for i > 24, the value of si is much smaller, than s1 and s2. The number of random bits returned by a call to the pseudo-random generator is insufficient to separate si and si+1, for i > 24. Several tricks can be tried to keep the number of calls to the random number generator linear; however, there are still numerical problems involved. We consider this method to be impractical for n > 24.
The arithmetic progression suffers from a different problem. Note that,
n	2(1 — ρ)(n — i + 1)
si = n2 + (1 — ρ)n(n + 1) + n2 + (1 — ρ)n(n + 1);
thus, si depends linearly on i and only on the second term while the denom- inator is quadratic in n. This means that when n is large the si are almost equal; therefore, the method produces permutations almost uniformly for al- most all ρ. In fact, because of numerical precision, the implementation of this method cannot be distinguished from an uniform generator.

Additional remarks
We have discussed four related concepts: measures of disorder, measures of presortedness, normality, and regularity. We have shown that the most inter- esting and widely used measures of disorder are normal and regular measures of presortedness or at least they are algorithmically equivalent to such mea- sures. Moreover, we have shown that for such measures it is possible to obtain nearly sorted sequences efficiently. These nearly-sorted sequences are neces- sary to generate benchmark test-sets for a series of important computational problems beyond sorting. For example, for the Error Correcting Graph Iso- morphism.
Moreover, it seems that for problems like the Clustering of a Data Ar- ray [44] or the Longest Hamiltonian Path Problem (where the answer is a

permutation that minimizes/maximizes a criteria that can be evaluated in polynomial time), if we are given a current solution π and a normal measure of presortedness M plus the additional information that for the optimal solu- tion σ we have M (π, σ) ≤ k, then Theorem 4.7 implies that this version of the problem is Fix-Parameter tractable [15] (we simply need to search the space of all permutations at distance k or less from π which has size a polynomial in k and the operators can be identified as Wn).
The connections between mods and ri-metrics leave several open questions. Researchers have attempted to compare ri-metrics by establishing inequalities between them [11]. Note that Inv is defined from a set of operations that are exactly all transpositions of adjacent elements. Lemma 3.11 shows that Inv is the normal measure (ri-metric) most sensitive to disorder. The popularity of Kendall’s τ is due to the fact that Inv is asymptotically normally distributed with known mean and variance for each n. Our results show that normal measures (ri-metrics and coefficients of correlation) can be constructed in a similar way, we only need to provide the sets Wn of sorting operations to obtain the ri-metric dMW .
In order to use these ri-metrics in statistical applications, the characteris- tics of the distributions must be described either analytically or by tabulation of their values. Analytical results can be difficult, as suggested by Ulam’s problem (computing the limiting behavior of the expected value of Rem(X)). Stanley [58] listed Ulam’s problem as one of several open problems in enu- merative combinatorics and Knuth [34, Section 5.1.4,Problem 28] ranked the problem as M47. In fact, several researchers using many different mathemat- ical techniques have contributed to solve the problem. Baer and Brock [4] computed extensive tab√les and conjectured that E[Las(X)] = 2 |X|; thus,
Baer and Brock (up to |X| = 36) to |X| = 75. Dixon [14] proved that
‘... the probability that the length of the longest monotonic subsequence in a sequence X1, X2,... , Xn of independent random variables with a common continuous distribution lies in the range (e−1n1/2, en1/2) tends to 1 as n →
∞.’
This shows that the asymptotic expected value of Lax(X) is Θ(|X|1/2). Del Junco and Steele [10] discuss the problem once more in 1979. By then, Ham- mersley [27] had proved, via an ingenious use of the planar Poisson process, that lim|X|→∞ |X|1/2E[Las(X)] = c where c is a constant and the convergence is in probability. Hammersley [27] gave bounds on c which were improved by Kingman [32]. Then, Logan and Shepp [40] used calculus of variations, Hilbert transform and Fourier transform to prove c ≥ 2. Vershik and Kerov[7] [60] used similar methods to prove c ≤ 2. Dallal and Hartigan [9] have performed

Monte Carlo simulations using 10,000 random permutation with length in the range 20 to 400. Their results showed that there is a simple and reasonably accurate relationship between |X| and the variance of Rem(X). Their em- pirical evidence indicated that the distribution of Rem(X) is asymptotically normal. Much of the recent work on this has been summarized by Baik, De- ift and Johansson [5] who elaborated on the Tracy-Widom distribution and the Gaussian Unitary Ensemble of random matrix theory. In fact, more ac- curate estimates of the constants involved in the limit for the distribution of E[Rem(X)] are now known. The mathematical effort and the sophistication of the techniques for this problem are remarkable.
Since analytical results on the distribution of measures are hard, it is de- sirable, at least, to characterize those ri-metrics that decompose into a sum of independent uniform distributions or other well known distributions. If we want to test correlation or agreement of more than two rankings (because the objects are ranked independently by boards of judges), the corresponding techniques must be developed [19,20,21]. Random generation with bounded disorder may provide the answer for tabulating values of the distributions.
Requiring that a measure used for describing the behavior of a sorting algorithm qualify as a measure of presortedness may seem restrictive. How- ever, in understanding a measure, it is helpful to describe its distribution over S|X| and its relationship to other measures; it is also helpful to analyze its properties, and in particular, to verify if it is normal or regular.
Finally, the arguments here are somewhat biased in that we do not neces- sarily consider a sequence in descending order to be sorted. 11 The elements in a sequence in descending order are very far from being randomly shuffled; however, some work must be done to obtain the sequence in ascending order. Although Mannila has already argued that ⟨2, 1⟩ should be considered to be a sequence with disorder, we argue that ascending and descending order cannot be considered equivalent. From the point of view of statistics, a ranking in ascending order cannot be considered the same as a ranking in descending order. From the point of view of sorting, the equivalence of ascending order with descending order implies that the operation of reversing a sequence has no cost. From the theoretical point of view, for any sequence X, we can ex- ecute two copies of an M -optimal algorithm in parallel, with inputs X and Reverse(X), terminating as soon as either one terminates. This gives an al- gorithm that sorts X in ascending or descending order, whichever was easier according to M , with a constant delay factor.

11 The reader familiar with Kolmogorov Complexity may attempt to define a measure of disorder that takes this into account, however, several technical difficulties must be resolved.

References
Aldous D. and P. Diaconis, Shuffling cards and stopping times. American Mathematical Monthly, 93:333–348, 1986.
Babai, L., Local expansion of vertex-transitive graphs and random generation in finite groups. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 164–174, New Orleans, LA, 1991. SIGACT: Special Interest Group on Algorithms and Computation Theory, ACM Press.
Babai, L., G. Cooperman, L. Finkelstein, E. Luks, and A. Seress, Fast Monte Carlo algorithms for permutation groups. In Proceedings of the twenty-third annual ACM symposium on Theory of computing, pages 90–100, New Orleans, LA, 1991. SIGACT: Special Interest Group on Algorithms and Computation Theory, ACM Press.
Baer, R. M. and P. Brock, Natural sorting permutation spaces. Mathematics of Computation, 22:385–410, 1968.
Baik, J., P. Deift and K. Johansson, On the distribution of the length of the longest increasing subsequence of random permutations. Journal of the American Mathematical Society 12:(4) 1119-1178, 1999.
Burge, W. H., Sorting, trees and measures of order. Information and Control, 1:181–197, 1958.
Carlsson, S., C. Levcopoulos, and O. Petersson, Sublinear merging and natural merge sort. Technical report, Department of Computer Science, Lund University, 1989.
Cook, C. R. and D. J. Kim, Best sorting algorithms for nearly sorted lists. Communications of the ACM, 23:620–624, 1980.
Dallal, G. E. and J. A. Hartigan, Note on a test of monotone association insensitive to outliers. Journal of the American Statistical Association, 75(371):722–725, 1980.
Del Junco, A. and M. Steele, Hammersley’s law for the Van der Corput sequence:an instance of probability theory for pseudorandom numbers. The Annals of Probability, 7(2):267–275, 1979.
Diaconis, P, and R. L. Graham, Spearman’s footrule as a measure of disarray. J. Royal Statistical Soc. Series B, 32(2):262–268, 1977.
Diaconis, P., R. L. Graham and J. A. Morrison, Asymptotic analysis of a random walk on a hypercube with many dimensions. Random Structures and Algorithms, 1(1):51–72, 1990.
Diaconis, P. and M. Shahshahani, Generating a random permutation with random transpositions. Zeitschrift fu¨r Wahrscheinlichkeitstheorie und verwandte Gebiete, 57:159–179, 1981.
Dixon, J. D., Monotonic subsequences in random sequences. Discrete Mathematics, 12:139– 142, 1975.
Downey, R. G. and M. R. Fellows, Parameterized Complexity. Monographs in Computer Science. Springer-Verlag, 1999.
Elmasry, A., Three sorting algorithms using priority queues. In Proceedings of 14th International Symposium on Algorithms and Computation (ISAAC 2003), Kyoto, Japan, 2004. Springer-Verlag Lecture Notes in Computer Science. to appear.
Estivill-Castro, V., H. Mannila and D. Wood, Right invariant metrics and measures of presortedness. Discrete Applied Mathematics, 42:1–16, 1993.
Estivill-Castro, V. and D. Wood, The performance of replacement selection on nearly sorted sequences. Technical Report in preparation, Department of Computer Science, University of Waterloo, 1991.


Feigin, P. D. and M. Alvo, Intergroup diversity on concordance for ranking data: An approach via metrics for permutations. The Annals of Statistics, 14:691–707, 1986.
Feigin, P. D. and A. Cohen, On a model for concordance between judges. J. Royal Statistical Soc. Series B, 40:203–213, 1978.
Fligner, M. A. and J. S. Verducci, Distance based ranking models. J. Royal Statistical Soc. Series B, 48:359–369, 1986.
Gordon, A. D., Another measure of the agreement between rankings. Biometrika, 66(2):327– 332, 1979.
Gordon, A. D., A measure of the agreement between rankings. Biometrika, 66(1):7–15, 1979.
Graham, R. L., D. E. Knuth and O. Patashnik, Concrete Mathematics. Addison-Wesley Publishing Co., Reading, MA, 1989.
Guibas, L. J., E. M. McCreight and M. F. Plass, A new representation of linear lists. In The Proceedings of the 9th ACM Annual Symposium on Theory of Computing, pages 49–60, 1977.
Hadjicostas, P. and K. B. Lakshmanan, Bubblesort with erroneous comparisons. www.acs.brockport.edu/~klakshma/bio/research.html, 2003.
Hammersley, J. M., A few seedlings of research. Proc. Sixth Berckley Symp. Math. Statis. Probability, 1:345–394, 1972. Univ. of California Press.
Hertel, S., Smoothsort’s behavior on presorted sequences. Information Processing Letters, 16:165–170, 1983.
Islam, T. and K. B. Lakshman, On the error-sensitivity of sort algorithms. In S. G. Akl,
F. Fiala, and W. W. Koezkodaj, editors, Proceedings of the International Conference On Computing and Information, pages 81–85, Toronto, May 1990. Canadian Schoolar’s Press. Advances in Computing and Information.
Katajainen, J.,C. Levcopoulos and O. Petersson, Local insertion sort revisited. In Proc. Optimal Algorithms, pages 239–253. Springer-Verlag Lecture Notes in Computer Science 401, 1989.
Kendall, M. G., Rank Correlation Methods. Griffin, London, 4th edition, 1970.
Kingman, J. F. C., Subadditive ergodic theory. Annals of Probability, 1:883–899, 1973.
Knuth, D. E., The Art of Computer Programming, Vol.2: Seminumerical Algorithms. Addison-Wesley Publishing Co., Reading, MA, 1973.
Knuth, D. E., The Art of Computer Programming, Vol.3: Sorting and Searching. Addison- Wesley Publishing Co., Reading, MA, 1973.
Lenstra, J. K., Clustering a data array and the traveling-salesman problem. Operations Research, 22:413–414, 1974.
Levcopoulos, C., and O. Petersson, Heapsort — adapted for presorted files. In F. Dehne,
J.R. Sack, and N. Santoro, editors, Proceedings of the Workshop on Algorithms and Data Structures, pages 499–509. Springer-Verlag Lecture Notes in Computer Science 382, 1989.
Levcopoulos, C. and O. Petersson, Sorting shuffled monotone sequences. Technical report, Department of Computer Science, Lund University, Box 118, S-2100 Lund, Sweden, 1989.
Levcopoulos, C. and O. Petersson, Splitsort—an adaptive sorting algorithm. In B. Rovan, editor, Mathematical Foundations of Computer Science, pages 416–422. Springer-Verlag Lecture Notes in Computer Science 452, 1990.
Levcopoulos, C. and O. Petersson, Adaptive Heapsort. Journal of Algorithms, 14:395–413, 1993.
B.F. Logan and L.A. Shepp. A variational problem for random Young tableaux. Advances in Mathematics, 26:206–22, 1977.


Mallows, C. L., Non-null ranking models. Biometrika, 44:114–130, 1957.
Mannila, H., Instance Complexity for Sorting and NP-Complete Problems. PhD thesis, University of Helsinki, Department of Computer Science, 1985.
Mannila, H., Measures of presortedness and optimal sorting algorithms. IEEE Transactions on Computers, C-34:318–325, 1985.
McCormick, Jr., H. T., P. J. Schweitzer and T. W. White, Problem decomposition and data reorganization by a clustering technique. Operations Research, 20:993–1009, 1972.
McKay, J., The largest degrees of irreducible characters of the symmetric group. Mathematics of Computation, 30(135):624–635, 1976.
Mehlhorn, K., Sorting presorted files. Proceedings of the 4th GI Conference on Theory of Computer Science, Springer-Verlag Lecture Notes in Computer Science 67:199–212, 1979.
Mehlhorn, K., Data Structures and Algorithms, Vol 1: Sorting and Searching. EATCS Monographs on Theoretical Computer Science. Springer-Verlag, Berlin/Heidelberg, 1984.
Messmer, B. T. and H. Bunke, A decision tree approach to graph and subgraph isomorphism detection. Pattern Recognition, pages 1979–1998, 1999.
Moffat, A. Eddy and O. Pettersson, Splaysort. fast, versatile, practical. Software – Practice and Experience, 26(7):781–797, July 1996.
Oommen, B. J., and D. T. Ng, On generating random permutations with arbitrary distributions. The Computer Journal, 33(4):368–374, 1990.
O¨ zsu, M. T., and P. Valduriez, Principles of Distributed Database Systems. Prentice Hall, 2nd edition, 1999.
Ross, S. M., Introduction to Probability Models. Academic Press, Inc., Orlando, Florida, 3rd edition, 1985.
Roura, S. Improving Mergesort for linked lists. In J. Nesetril, editor, Algorithms - ESA ’99, 7th Annual European Symposium, volume 1643, pages 267–276, Prague, Czech Republic, July 16-18 1999. Lecture Notes in Computer Science.
Sedgewick, R., Quicksort. Garland Publishing Inc., New York and London, 1980.
Skiena, S. S., Encroaching lists as a measure of presortedness. BIT, 28:755–784, 1988.
Sloane, N. J. A., Encrypting by random rotations. In T. Beth, editor, Proceedings of Cryptography, Burg Feuerstein 82, pages 71–128. Springer-Verlag Lecture Notes in Computer Science 149, 1983.
Spearman, C., The proof and measurement of association between two things. American Journal of Psychology, 15(1):72–101, 1904.
Stanley, R. P., On planar partitions: Part II. Studies in Applied Math., 50:259–279, 1971.
Tsai, W. H. and K.-S. Fu, Error-correcting isomorphisms of attributed relational graphs for pattern analysis. IEEE Transactions on Systems, Man and Cybernetics, 9(12):757–768, December 1979.
Vershik, A. M. and S. V. Kerov, Asymptotics of the Placherel measure of the symmetric group and the limiting form of Young tables. Soviet Math. Dokl., 18:527–531, 1977.
Wang, Y.-K., K.-C. Fan and J.-T. Horng, Genetic-based search for error-correcting graph isomorphism. IEEE Transactions on Systems, Man and Cybernetics, Part B: Cybernetics, 27(4):588–597, August 1997.
Xiao, J., Y. Zhang, X. Jia and T. Li, Measuring similarity of interests for clustering web- users. In Australian Computer Science Communications. Database Technologies. Proceedings of the 12th Australasian Database Conference ADC 2001, volume 23-2, pages 107–114. IEEE Computer Society, 2001.

Appendix
We present here laborious proofs for completeness so referees can review them. We hope that placing them here facilitates the flow of the main ideas. For Theorem 3.2.
Proof. It is not difficult to verify axioms 1, 2 and 4. To verify axiom 3, we show that if Y is a subsequence of X obtained by throwing away one element, then Y p ≤ X  p, for all p ≥ 1. The general case follows by induction.
Let p ≥ 1,p ∈ ঩. Let X = ⟨x1, x2,... , xn⟩, and let Y be obtained from X by deleting xi, thus Y = ⟨x1, x2,... , xi−1, xi+1,... xn⟩. Assume P erm[X](i) > i (the case P erm[X](i) < i is symmetric, and if P erm[X](i)= i then  Y  p ≤
X  p trivially).
Let m = |P erm[X](i)—i| and consider (  X  p)p = Σn	|P erm[X](j)—j|p.

Since



and
y = , xj	if j ≤ i — 1
, xj+1 if j ≥ i

, P erm[X](j)	if j < i and P erm[X](j) < i 
P erm[Y ](j)=	P erm[X](j + 1)	if j ≥ i and P erm[X](j) < i P erm[X](j) — 1	if j < i and P erm[X](j) ≥ i
P erm[X](j + 1) — 1 if j ≥ i and P erm[X](j) ≥ i
the terms |P erm[X](j)—j| with j /= i in (  X p)p are in a one to one correspon-
p	p	n 1	p j=1
and each P erm[Y ](j) can be rewritten by P erm[X](j) with j /= i. A careful
case analysis shows that the corresponding term to |P erm[Y ](j) — j| is always larger unless i ≤ j < Perm[Y ](j) < Perm[X](i) and in this case, it increases by one. More precisely, using the correspondence between terms and Iverson’s notation [24, Chapter 2] (for a predicate P , [P ]= 1 if P is true and [P ] = 0 if P is false), we write
i−1
AX =	|P erm[X](j) — j|p[P erm[X](j) < Perm[X](i)],
j=1 i−1
BX =	|P erm[X](j) — j|p[P erm[X](j) > Perm[X](i)],
j=1





CX1




CX2
n
=	|P erm[X](j) — j|p[P erm[X](j) < Perm[X](i)]
j=i+1
[j ≤ P erm[X](j)],
n
=	|P erm[X](j) — j|p[P erm[X](j) < Perm[X](i)]
j=i+1
[j > Perm[X](j)],
n

DX =	|P erm[X](j) — j|p[P erm[X](j) > Perm[X](i)],
j=i+1 i−1
AY =	|P erm[Y ](j) — j|p[P erm[X](j) < Perm[X](i)],
j=1 i−1
BY =	|P erm[Y ](j) — j|p[P erm[X](j) < Perm[X](i)],
j=1 n−1

CY1




CY2
=	|P erm[Y ](j) — j|p[P erm[X](j + 1) < Perm[X](i)]
j=i
[j +1 ≤ P erm[X](j + 1)],
n−1
=	|P erm[Y ](j) — j|p[P erm[X](j + 1) < Perm[X](i)]
j=i
[j +1 > Perm[X](j + 1)],
n−1

DY =	|P erm[Y ](j) — j|p[P erm[X](j + 1) > Perm[X](i)].
j=i

Therefore,



and

(  X  p)p = AX + BX + CX + CX + DX + mp,
(  Y p)p = AY + BY + CY + CY + DY

AY = AX,	BY ≤ BX,	CY2 ≤ CX2 and	DY = DX.

Thus, in order to prove that (  Y p)p ≤ (  X  p)p it is sufficient to prove that CY — CX ≤ mp. Let J = {j ∈ [1, n] | i < j ≤ P erm[X](j) < Perm[X](i)}, and R = {P erm[X](j) — j  | j ∈ J }. The cardinality of R is less than P erm[X](i) — i; that is,  R  ≤ m — 1 and if r ∈ R then, |r| ≤ m — 2. If we

denote R = {r1, r2,... , rt} then, CX1
t
k=1
rp and CY
t
k=1
(rk + 1)p.

For w ≥ 0, f (w)= wp is convex and monotonically increasing, thus 0 ≤ u < v

implies f (v) — f (u) ≤ f '(v)(v — u). Therefore,

t	t


CY1
— CX1
= Σ[(rk + 1)p — rp] ≤ Σ p(rk + 1)p−1.

k=1	k=1
Case p ≥ 2: Under the conditions of this problem, it can be shown (see below) that


thus
max
t


k=1
mp
(rk + 1)p−1	≤		, p

mp


as required.
CY1
— CX1 ≤ p p
= mp,

Case 1 ≤ p ≤ 2: Under the conditions of this problem, it can be shown (see below) that


max
t


k=1
(rk + 1)p−1
≤	max
t∈{1,2,...,m−1}
t(m — t)p−1.

Let g(t) = t(m — t)p−1, t ∈ [1,m — 1]. Setting the derivative of g(t) equal to zero and solving for t, shows that g(t) is maximized when t = m/p. This implies that

CY1
— CX1
≤ p	max
t∈{1,2,...,m−1}
t(m — t)p−1

≤ p(m/p)(m — m/p)p−1 = mp( p — 1 )p−1 ≤ mp
p
as required.	 
To prove (2) and (3) we translate the problem of bounding max(Σt	(rk +
p−1) into the problem of bounding the weight of the matchings in a weighted bipartite graph. We introduce the corresponding graph in Definition 7.1 and the required results in Lemmas 7.2 and 7.3.
Definition 7.1 Let q ≥ 0,q ∈ ঩, m ≥ 2 and let Gq(V, E) be the weighted bi- partite graph given by: V = V1∪V2, |V1| = |V2| = m—1, V1 = {u1, u2,... , um−1}, V2 = {v1, v2,... , vm−1} and (ui, vj) ∈ E if and only if i ≤ j and, in that case, the weight of the edge is c[(ui, vj)] = (j — i + 1)q.
Now, let uk correspond to i + k and vk correspond to i + k, for k = 1, 2,... ,m — 1. The set R defines a matching in Gp−1 by: if P erm[X](j) — j ∈
R, then include the edge (uj−i, vP erm[X](j)−j) in the matching. Now, Σt	(rk +
p−1 is the weight of the matching.

To obtain (2) apply Lemma 7.2 with q = p — 1. For (3) use Lemma 7.3.
Lemma 7.2 Let q ≥ 1, q ∈ ঩, and Gq as in Deﬁnition 7.1.
A matching of maximum weight is given by:
M = {(u1, vm−1), (u2, vm−2),... , (u[m/2♩, vm−[m/2♩)}.
The cost of the maximum weight matching is no greater than mq+1 .
Proof. We prove the lemma by induction on m.
Basis: For m = 2 the graph has only one edge, this edge is a matching of maximum weight. For m = 3 the graph has four non-empty matchings:
M	weight
{(u1, v1)}	1q
{(u2, v2)}	1q
{(u1, v1), (u2, v2)} 1q + 1q
{(u1, v2)}	2q
Clearly the claim holds.
Induction Step:  Assume the claim is true for k, 2 ≤ k < m. We will show that any matching M of maximum weight can be transformed, without reducing the weight, into a matching M ' containing the edge (u1, vm−1). Thus M ' is a matching of maximum weight. To obtain a maximum weight matching in the form claimed by the lemma, we observe that M ' — {(u1, vm−1)} is a matching of the subgraph G'q obtained from Gq by removing the vertices u1, v1, um−1, vm−1. But (u1, vm−1) is the largest edge in Gq thus applying the induction hypothesis to G'q proves the result.
It only remains to prove that (u1, vm−1) belongs to a matching of maximum weight. Let M be a matching of maximum weight that does not include the edge (u1, vm−1). Let us be such that (us, vm−1) ∈ M , 1 < s ≤ m — 1. (If no edge in M is adjacent to vm−1, then M ∪ {(um−1, vm−1)} will be a matching of larger weight since in Gq, um−1 is only adjacent to vm−1.) Let vr be such that (u1, vr) ∈ M , 1 ≤ r < m — 1. (Again, if no edge in M is adjacent to u1, then M ∪ {(u1, v1)} will be a matching of larger weight since in Gq, v1 is only adjacent to u1.)
Case 1: r < s. Let M ' = M ∪ {(u1, vm−1)}— {(us, vm−1), (u1, vr)}. Then, clearly M ' is a matching and
W (M ')= W (M )+ (m — 1)q — (m — 1 — s + 1)q — (r — 1+ 1)q
= W (M )+ (m — 1)q — ((m — s)q + rq).

Since for q ≥ 1, a, b ≥ 0 we have aq + bq ≤ (a + b)q, we obtain (m — s)q + rq ≤ (m — s)q + (s — 1)q ≤ (m — 1)q
and W (M ') ≥ W (M ) as claimed.
Case 2: r ≥ s. Let M ' = M ∪ {(u1, vm−1), (us, vr)}— {(us, vm−1), (u1, vr)}. Then, clearly M ' is a matching and
W (M ')= W (M )+ (m — 1)q + (r — s + 1)q — ((m — s)q + rq).
Since for q ≥ 1, 0 ≤ c ≤ a, b we have aq + bq ≤ cq + [a + (b — c)]q, we obtain (m—s)q +rq ≤ (r —s+1)q +[(m—s)+(r —(r —s+1))]q = (r —s+1)q +(m—1)q and W (M ') ≥ W (M ) as claimed.
Now, let M be a maximum weight matching.
[m/2♩
W (M )=	(m — i — i + 1)q
i=1

[m/2♩
=
(m — 2i + 1)q ≤
m−1

iq ≤
m−1
(x + 1)qdx =
mq+1 q +1 

i=1
i=0	0


Lemma 7.3 Let 0 ≤ q < 1,q ∈ ঩, and Gq be as in Deﬁnition 7.1. Then,
there is r ≥ 1 such that a matching of maximum weight is given by:
M = {(u1, vm−r), (u2, vm+1−r),... , (ur, vm−1)}
and,
there is r ≥ 1 such that r ≤ m — 1 and the cost of the maximum weight matching is no greater than than r(m — r)q.
We leave the proof of this last lemma to the reader. For Theorem 3.3.
Proof. Let M (X) = Σr	liMi(X), where li are non-negative constants li,
and Mi are measures of disorder. By definition, M depends only on the relative order of the elements in X and since, for all i, Mi(X) is minimized when X is sorted, and li are nonnegative, M (X) is minimized when X is sorted. Therefore, M is a mod.
If Y is a subsequence of X, then Mi(Y ) ≤ Mi(X), for all i; this gives
M (Y )= Σr	liMi(Y ) ≤ Σr	liMi(X)= M (X).

Now, because for the Mi’s satisfy condition 3 Y ≤ X, implies that Mi(Y X) ≤
Mi(Y )+Mi(X), for all i. Therefore, M (Y X)= Σr	liMi(Y X) ≤ Σr	li[Mi(Y )+

Σr	Σr i=1
i=1



r
M (⟨x⟩X)=	liMi(⟨x⟩X)
i=1 r
≤	li[ci|X| + Mi(X)]
i=1

r
=
i=1

lici
|X| + M (X).



To prove EncR is a measure of presortedness we first prove the following lemma.
Lemma 7.4 If Y is a subsequence of X, then Enc(Y ) ≤ Enc(X).
Proof. Assume Y is a subsequence of X = ⟨x1,... , xn⟩ obtained by deleting xi from X. The general case follows by induction. Let D(xi) denote the dequeue containing xi in the encroaching set for X. For x1,... , xi−1, the encroaching sets of Y and X are the same, and elements in xi+1,... , xn that are blocked by xi end up in dequeues no older than those obtained when building the encroaching set for Y . Thus, no more dequeues are required for Y than for X.	 
Proof. The definition of EncR(X) depends on only the relative order of el- ements in X and, clearly, if X is sorted, then EncR(X) = 0; thus, we verify axioms 3, 4 and 5 directly.
We verify axiom 3 in Definition 1.5; that is, if Y is a subsequence of X, then EncR(Y ) ≤ EncR(X). Suppose Y is a subsequence of X. If Y is sorted, then EncR(Y )=0 ≤ EncR(X). If Y is not sorted, then PY is a subsequence of PX and by Lemma 7.4
Enc(Reverse(PY )) ≤ Enc(Reverse(PX)).
We now prove axiom 4, namely, if X ≤ Y , then EncR(XY ) ≤ EncR(X)+ 
EncR(Y ). Let X ≤ Y . If Y is sorted, then PX = PXY and
EncR(XY )= Enc(Reverse(PX)) = EncR(X) ≤ EncR(X)+ EncR(Y ).

Assume Y is not sorted. The encroaching set for Enc(XY ) is built by con- structing the encroaching set of Reverse(PY ) and then adding to it the ele- ments in Reverse(X). Therefore, X ≤ Y implies SX ≤ PY , and the elements of the sorted sequence SX end up in the first dequeue. Thus, there is a sub- sequence B of PX such that
EncR(XY )= Enc(Reverse(B)) + Enc(Reverse(PY )).
By Lemma 7.4, Enc(Reverse(B)) ≤ Enc(Reverse(PX )); thus,
EncR(XY ) ≤ Enc(Reverse(PX)) + EncR(Y )
= EncR(X)+ EncR(Y ).
We now check axiom 5, namely, EncR(⟨x⟩X) ≤ |X| + EncR(X). We analyze two cases.
Case X = ⟨⟩. This implies EncR(⟨x⟩X)= 0= |X| + EncR(X), as required.
Case X /= ⟨⟩. There are three subcases.
Subcase |X| = 1. This implies EncR(⟨x⟩X) ≤ 1 ≤ |X| + EncR(X); Subcase X is sorted and |X| > 1. This implies Enc(⟨x⟩X) ≤ 2 ≤ |X| + EncR(X);
Subcase X is not sorted. This implies P⟨x⟩X = ⟨x⟩PX. Thus,
Enc(Reverse(P⟨x⟩X )) ≤ Enc(Reverse(PX)) + 1
≤ EncR(X)+ |X|.
This completes the proof.	 
