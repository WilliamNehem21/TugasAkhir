Electronic Notes in Theoretical Computer Science 208 (2008) 57–75	
www.elsevier.com/locate/entcs

Formal Modelling of Salience and Cognitive Load 
R. Rukˇse˙nasa,1 , J. Backb,2 , P. Curzona,3 , A. Blandfordb,4
a Dept. of Computer Science, Queen Mary, University of London, London, UK
b University College London Interaction Centre, London, UK

Abstract
Well-designed interfaces use procedural and sensory cues to increase the salience of appropriate actions and intentions. However, empirical studies suggest that cognitive load can influence the strength of procedural and sensory cues. We formalise the relationship between salience and cognitive load revealed by empirical data. We add these rules to our abstract cognitive architecture developed for the verification of usability properties. The interface of a fire engine dispatch task used in the empirical studies is then formally verified to assess the salience and load rules. Finally, we discuss how the formal modelling and verification suggests further refinements of the rules derived from the informal analysis of empirical data.
Keywords: human error, formal verification, salience, cognitive load, model checking.


Introduction
The correctness of interactive systems depends on the behaviour of both human and computer actors. Human behaviour cannot be fully captured by a formal model. However, it is a reasonable, and useful, approximation to assume that humans behave “rationally”: entering interactions with goals and domain knowledge likely to help them achieve their goals. If problems are discovered resulting from rational behaviour then such problems are liable to be systematic and deserve attention in the design. Whole classes of persistent, systematic user errors may occur due to modelable cognitive causes [16,11]. Often opportunities for making such errors can be reduced with good design [6]. A methodology for highlighting those designs that

٨ This research is funded by EPSRC grants GR/S67494/01 and GR/S67500/01.
1 Email: rimvydas@dcs.qmul.ac.uk
2 Email: j.back@ucl.ac.uk
3 Email: pc@dcs.qmul.ac.uk
4 Email: a.blandford@ucl.ac.uk

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.03.107

allow users to make systematic errors, even when behaving in a rational way, is important. It will allow such designs to be significantly improved.
Well-designed interfaces increase the sensory salience of signals that are used to cue actions that are frequently forgotten or are performed in the wrong sequence. Chung and Byrne [8] found that a sensory signal has to be highly visually salient in order to ensure that a task-critical step, buried deep down in the task structure, is not forgotten. The specificity of the cue is important. Making a target action more salient by moving it to an area of the interface which is likely to be the focus of attention is often insufficient. The cue has to be indicative of the type of action required – e.g., a red flashing arrow pointing at the button to be clicked. Low level perceptual studies [7] have shown that an individual is unable to process visually salient features if cognitive control functions are not available to maintain the active goal. This suggests that sensory cues are not always noticed under high workload scenarios.
Non-sensory cues, known as procedural cues (internal to the cognitive system), can be used to retrieve previously formulated intentions (expert procedural knowl- edge) enabling the next procedural step to be performed. Remembering that, and so doing, after performing x always do y if z is true is an example of following a procedural cue rule. Sensory cues (external to the cognitive system) can also be used to retrieve intentions (expert procedural knowledge). For example, if sensory cue p is attended to then it may indicate that q should be the next step if r is true. People make slip errors frequently, but do not make them every time. Empiri- cal studies [1] suggest that cognitive load can influence the frequency of errors by affecting the strength of procedural and sensory cues. In this paper, we formalise the relationship between salience and cognitive load revealed by the informal anal- ysis of empirical data. We then incorporate these rules into our abstract cognitive architecture [9,17] developed earlier from abstract cognitive principles, such as a user entering an interaction with knowledge of the task and its subsidiary goals, and choosing non-deterministically between appropriate actions. This extension re- fines non-determinism by introducing a hierarchy of choices governed by the salience (strength) of procedural and sensory cues, and the level of cognitive load imposed
by the task performed.
As an assessment step for our extension, we formally model the fire engine dis- patch task used in the empirical studies [1]. One reason for doing this is to check whether the systematic errors identified during the experiments can also be de- tected by the formal verification of the same task, thus indicating that our extended cognitive architecture generates behaviours corresponding to those of real people. Another reason is that possible mismatches between the two sets of behaviours can suggest new empirical studies leading to refinements of our salience and load rules and their formalisation within the cognitive architecture.

Contribution
Summarising, the main contribution of this paper is the following:
An investigation into the formal modelling of salience and cognitive load.

A formalisation of the connection between salience and cognitive load revealed by empirical studies.
An extension of our verification framework involving salience and load rules, and a hierarchy of salience levels.
A formal modelling, as an assessment step, of the task used earlier in our empirical studies.

Related work
There is little related work on salience and cognitive load. Cartwright-Finch and Lavie [7] developed a theory that a high extraneous load only reduces perception of a sensory cue when cognitive control functions are not available to maintain the active goal. More generally, work on human error has shown that the provision of visual cues can strengthen procedural cueing providing they manage to capture attention [8].
Duke et al. [10], and Bowman and Faconti [4] use Interactive Cognitive Sub- systems (ICS) [3] as the underlying model of human information processing. Their models deal with information flow between the different cognitive subsystems and constraints on the associated transformation processes. As a result, the above work focusses on reasoning about multi-modal interfaces and analyses whether interfaces based on several simultaneous modes of interaction are compatible with the capa- bilities of human cognition.
In the related area of safety-critical systems, Rushby et al. [19] focus on mode errors and the ability of pilots to track mode changes. They formalise plausible mental models of systems and analyse them using the Murφ verification tool. The mental models though are essentially abstracted system models; they do not rely upon structure provided by cognitive principles.

Cognitive Architecture
Our cognitive architecture is a higher-order logic formalisation of abstract principles of cognition and specifies a form of cognitively plausible behaviour [5]. The archi- tecture specifies possible user behaviour (traces of actions) that can be justified in terms of specific results from the cognitive sciences. Real users can act outside this behaviour of course, about which the architecture says nothing. However, be- haviour defined by the architecture can be regarded as potentially systematic, and so erroneous behaviour is similarly systematic in the design. The predictive power of the architecture is bounded by the situations where people act according to the principles specified. The architecture allows one to investigate what happens if a person acts in such plausible ways. The behaviour defined is neither “correct” nor “incorrect”. It could be either depending on the environment and task in question. We do not attempt to model the underlying neural architecture nor the higher- level cognitive architecture such as information processing. Instead our model is an abstract specification, intended for ease of reasoning.

Cognitive Principles
In the formal user model, we rely upon abstract cognitive principles that give a knowledge level description in the terms of Newell [15]. Their focus is on the internal goals and knowledge of a user. These principles are briefly discussed below. Their formalisation in SAL is described in Sections 2.2 and 3.

Non-determinism
In any situation, any one of several cognitively plausible behaviours might be taken. It cannot be assumed that any specific plausible behaviour will be the one that a person will follow where there are alternatives.

Relevance
Presented with several options, a person chooses one that seems relevant to the task goals. For example, if the user goal is to get cash from an ATM, it would be cognitively implausible to choose the option allowing one to change a PIN. A person could of course press the wrong button by accident. Such classes of error are beyond the scope of our approach, focussing as it does on systematic slips.

Salience
Even though user choices are non-deterministic, they are affected by the salience of possible actions. For example, taking money released by a cash-point is a more salient, and thus much more likely, action to take than to terminate the interaction by walking away from the machine without cash. In general, salience could be affected by several factors such as the sensory (visual) salience of an action, its procedural cueing as a part of a learned task, and the cognitive load imposed by the complexity of the task performed.

Mental versus physical actions
There is a delay between the moment a person mentally commits to taking an action (either due to the internal goals or as a response to the interface prompts) and the moment when the corresponding physical action is taken. To capture the consequences of this delay, each physical action modelled is associated with an internal mental action that commits to taking it. Once a signal has been sent from the brain to the motor system to take an action, it cannot be revoked after a certain point even if the person becomes aware that it is wrong before the action is taken. To reflect this, we assume that a physical action immediately follows the committing action.

Pre-determined goals
A user enters an interaction with knowledge of the task and, in particular, task dependent sub-goals that must be discharged. These sub-goals might concern infor- mation that must be communicated to the device or items (such as bank cards) that must be inserted into the device. Given the opportunity, people may attempt to

Table 1
A fragment of the SAL language

Notation	Meaning

x:T	x has type T
λ(x:T):e	a function of x with the value e
x' = e	an update: the new value of x is that of e
{x:T | p(x)}	a subset of T such that the predicate p(x) holds
a[i]	the i-th element of the array a
r.x	the field x of the record r
r WITH .x := e the record r with its field x updated by e g → upd    if g is true then update according to upd c [] d	non-deterministic choice between c and d
[](i:T): ci   non-deterministic choice between ci with i in range T



discharge such goals, even when the device is prompting for a different action. Such pre-determined goals represent a partial plan that has arisen from knowledge of the task in hand, independent of the environment in which that task is performed. No fixed order other than a goal hierarchy is assumed over how pre-determined goals will be discharged.

Reactive behaviour
Users may react to an external stimulus, doing the action suggested by the stimulus. For example, if a flashing light comes on a user might, if the light is noticed, react by inserting coins in an adjacent slot.

Voluntary task completion
A person may decide to terminate the interaction. As soon as the main task goal has been achieved, users intermittently, but persistently, terminate interactions [6], even if subsidiary tasks generated in achieving the main goal have not been completed. A cash-point example is a person walking away with the cash but leaving the card. Users also may terminate interactions when the signals from the device or environment suggest that task continuation is impossible due to some fault. For example, if the cash-point signals that the inserted card is invalid (and therefore retained), a person is likely to walk away and try to contact their bank.

Forced task termination
If there is no apparent action that a person can take that will help to complete the task then the person is forced to terminate the interaction. For example, if, on a ticket machine, the user wishes to buy a weekly season ticket, but the options pre- sented include nothing about season tickets, then the person will give up, assuming the goal is not achievable.

TRANSITION
[](g:GoalRange,slc:Salience): CommitAction:
NOT(comm) ∧
finished = notf ∧
(HighestSalience(slc, g, status, goals, ...)
∨
HighSalience(slc, g, status, goals, ...) ∧
NOT(∃h : HighestSalience(LowSLC, h, status, goals, ...)	→
∨
LowSalience(slc, g, status, goals, ...) ∧
NOT(∃h : (HighSalience(LowSLC, h, status, goals, ...) ∨
HighestSalience(LowSLC, h, status, goals, ...))) ∧
(g /= ExitGoal ∨ MayExit)



commit'[act(Goals[g].subgoals)] =
committed;
status' = status
WITH .trace[g] := TRUE WITH .last := g
WITH .length :=
status.length + 1

[]
[](a:ActionRange):  PerformAction:
commit'[a]= ready;

commit[a]= committed	→
[]
ExitTask:
Transition(a)

goals[TopGoal].achieved(in, mem) ∧ BrokenState(in, mem, env) ∧ NOT(comm) ∧
finished = notf
[]
Abort:
NOT(ExistsSalient(...)) ∧

→	finished' = ok



finished' = IF Wait(in, mem)

NOT(goals[TopGoal].achieved(in, mem)) ∧	→
NOT(comm) ∧
finished = notf
[]
Idle:
finished = notf	→
THEN notf
ELSE abort ENDIF


Fig. 1. Cognitive architecture in SAL (simplified)

Cognitive Architecture in SAL
We have formalised the cognitive principles within the SAL environment [14]. It provides a higher-order specification language and tools for analysing state ma- chines specified as parametrised modules and composed either synchronously or asynchronously. The SAL notation we use here is given in Table 1. We also use the usual notation for the conjunction, disjunction and set membership operators. A simplified version of the SAL specification of a transition relation that defines our user model is given in Fig. 1, where predicates in italic are shorthands explained later on. Below, whilst explaining this specification (SAL module User), we also discuss how it reflects our cognitive principles.



Guarded commands
SAL specifications are transition systems. Non-determinism is represented by the non-deterministic choice, [], between the named guarded commands (i.e. transitions). For example, CommitAction in Fig. 1 is the name of a family of transitions indexed by g. Each guarded command in the specification describes an action that a user could plausibly take. The pairs CommitAction – PerformAction of the corresponding transitions reflect the connection between the physical and men- tal actions. The first of the pair models committing to a goal, the second actually taking the corresponding action (see below).

Goals structure
The main concept in our cognitive architecture is that of user goals. 5 User goals are organised as a hierarchical (tree like) goal–subgoals structure. The nodes of this tree are either compound or atomic:
atomic Goals at the bottom of the structure (tree leaves) are atomic: they consist of (map to) an action, for example, a device action.
compound All other goals are compound: they are modelled as a set of task subgoals.
In this paper, we consider an essentially flat goal structure with the top goal con- sisting of atomic subgoals only. We will explore the potential for using hierarchical goal structures in subsequent work.
In SAL, user goals are modelled as an array, Goals, which is a parameter of the
User module. Each element g in Goals is a record with the following fields:
guard A predicate, denoted grd, that specifies when the goal g is enabled, for example, due to the relevant device prompts.
choice A predicate (choice strategy), denoted choice, that models a high-level ordering of goals by specifying when the goal g can be chosen. An example of the choice strategy is: “choose only if g has not been chosen before.”
achieved A predicate, denoted achieved, that specifies the main task goal when
g is the top goal, not used for atomic goals.
salience A value, denoted slc, that specifies the sensory salience of g.
cueing A function, denoted cue, that for each goal h returns the strength of g as a procedural cue for h.
load A value, denoted load, that specifies the intrinsic load associated with the execution of g.
subgoals A data structure, denoted subgoals, that specifies the subgoals of the goal. It takes the form comp(gls) when the goal consists of a set of subgoals gls. If the goal is atomic, its subgoals are represented by a reference, denoted atom(act) to an action in the array Actions (see below).

Goal execution
To see how the execution of an atomic goal is modelled in SAL consider the guarded command PerformAction for doing a user action that has been previously committed to:

commit[a]= committed	→
commit'[a]= ready;
Transition(a)

The left-hand side of → is the guard of this command. It says that the rule will only activate if the associated action has already been committed to, as indicated by the element a of the local variable array commit holding value committed. If the

5 Note that we are omitting from the description of the goal structure some aspects related to the relevance and timing of goals. They are not used in the work described here; for the omitted detail see [18].

rule is then non-deterministically chosen to fire, this value is changed to ready to indicate there are now no commitments to physical actions outstanding and the user model can select another goal. Finally, Transition(a) represents the state updates associated with this particular action a.
The state space of the user model consists of three parts: input variable in, output variable out, and global variable (memory) mem; the environment is modelled by a global variable, env. All of these are specified using type variables and are instantiated for each concrete interactive system. The state updates associated with an atomic goal are specified as an action. The latter is modelled as a record with the fields tout, tmem and tenv; the array Actions is a collection of all user actions. The three fields are relations from old to new states that describe how two components of the user model state (outputs out and memory mem) and environment env are updated by executing this action. These relations, provided when the generic user model is instantiated, are used to specify Transition(a) as follows:
out' ∈ {x:Out | Actions[a].tout(in,out,mem)(x)};
mem' ∈ {x:Memory | Actions[a].tmem(in,mem,out')(x)};
env' ∈ {x:Env | Actions[a].tenv(in,mem,env)(x) ∧ possessions}
Since we are modelling the cognitive aspects of user actions, all three state up- dates depend on the initial values of inputs (perceptions) and memory. In addition, each update depends on the old value of the component updated. The memory update also depends on the new value (out') of the outputs, since we usually as- sume the user remembers the actions just taken. The update of env must also satisfy a generic relation, possessions. It specifies universal physical constraints on possessions and their value, linking the events of taking and giving up a possession item with the corresponding increase or decrease in the number (counter) of items possessed. For example, it specifies that if an item is not given up then the user still has it. The counters of possession items are modelled as environment components. PerformAction is enabled by executing the guarded command for selecting an atomic goal, CommitAction, which switches the commit flag for some action a to committed thus committing to this action (enabling PerformAction). A goal g may be selected only when one of the disjuncts specifying its salience level (see Section 3) is true. The last conjunct in the guard of CommitAction distinguishes the cases when the selected goal is ExitGoal or not. ExitGoal (given as a parameter of the User module) represents such options as “cancel” or “exit”, available in some form in most interactive systems. We omit the definition of M ayExit from Fig. 1 here,
since it is irrelevant for this paper.
When an atomic goal g is selected, the user model commits to the corresponding action act(Goals[g].subgoals). The record status keeps track of a history of selected goals. Thus, the element g of the array status.trace is set to true to indicate that the goal g has been selected, status.last records g as the last goal selected, and the counter of selected goals, status.length, is increased.

Task completion
In the user model, we consider two ways of terminating an interaction. Voluntary completion (finished is set to ok) can occur when the main task goal, as the user perceives it, has been achieved (see the ExitTask command). Forced termination (finished is set to abort) models random user behaviour (see the Abort com- mand). Since the choice between enabled guarded commands is non-deterministic, the ExitTask action may still not be taken. Also, it is only possible when there are no earlier commitments to other actions.
In the guarded command Abort, the condition of forced termination (no enabled salient actions) is expressed as the negation of the predicate ExistsSalient (it states that there exists a goal for which one of the predicates HighestSalience, HighSalience or LowSalience is true). Note that, in such a case, a possible action that a person could take is to wait. The user model will only do so given some cognitively plausible reason such as a displayed “please wait” message. The waiting conditions are represented in the model by predicate parameter Wait. If Wait is false, finished is set to abort to model a user giving up and terminating the task.

Salience and Load Rules
In this section, we discuss the connection between the salience of cues and cognitive load observed in empirical studies and expressed as salience and load rules. We then formalise these rules within our verification framework and incorporate them into our cognitive architecture.

Cognitive load
Slip errors are made frequently, but they are not made every time. The frequency of these errors is determined by causal factors internal (goals) and external to the cognitive system. After formulating goals, new information may interfere with the ability to retain previous formulations. We designed an experimental paradigm [2] that manipulated the availability (and awareness) of both procedural and sensory cues that were needed to overcome performing erroneous “springs to mind” actions. Our hypothesis was that slip errors were more likely when the salience of cues was not sufficient to actively influence attentional control. If processes are directed by a passive (off-line) attentional control system then errors associated with performing “springs to mind” actions are more likely. A simulation of a ‘Fire Engine Dispatch Centre’ was developed. The overall objective was to send navigational information to fire engines enabling the fastest possible incident response times. Training trials were used to ensure that participants became familiar with the sequence of actions required. Cognitive load was manipulated by the complexity of routes imposed and the quantity of task irrelevant information displayed.
We found that the difficulty associated with performing a proceduralized task significantly influences the likelihood of making a slip error. The inherent difficulty of the task at hand can be referred to as intrinsic cognitive load. Our experiments have shown that this load can influence the strength of procedural cues used to

perform future task critical actions (background intrinsic cognitive load). Another load type, known as extraneous load, has been shown to influence the awareness individuals have of sensory cues when intrinsic load is high. Extraneous load is imposed by information that does not contribute directly to the performance of a specific goal. Activities such as attempting to find relevant information on the device display (visual search) or manipulating the user interface in an attempt to find relevant information (interactive search), that do not foster the process of performing a goal can be classified as extraneous. Our findings suggest that sensory cues will only be low in overall salience when both the intrinsic and extraneous load imposed on the individual is high. These findings are compatible with Cartwright- Finch and Lavie’s [7] theory that a high extraneous load only reduces perception of a sensory cue (or distractor) when cognitive control functions are not available to maintain the active goal.
The informal analysis [2] of our empirical data suggested the following connec- tions between salience and cognitive load:
sensory When both the intrinsic and extraneous load is high, the salience of sen- sory cues may be reduced.
procedural High intrinsic load reduces the salience of procedural cues. Next we formalise these connections within our verification framework.

Formalisation
In our formalisation, salience can take one of the following three values: HighSLC, LowSLC and NoSLC, whereas both the intrinsic and extraneous load can be either HighLD or LowLD. First, we had to capture the meaning of “reduced salience” in the above rules. We decided to interpret this as salience going from high to low. Then the sensory salience rule is expressed as follows:
if default = HighSLC ∧ intr = HighLD ∧ extr = HighLD
(1)	then sensory = HighSLC ∨ sensory = LowSLC
else sensory = default
Here, intr and extr represent the intrinsic and extraneous load, respectively. The variable default denotes the salience of a sensory cue without taking into account the cognitive load experienced, whereas sensory denotes the actual sensory salience of that cue. Note that our formalisation is non-deterministic, i.e., we assume that a sensory cue can be salient (and thus be noticed by people) even under the high cog- nitive load condition. This reflects the modality may in the corresponding informal rule.
In the cognitive architecture, we need a predicate that specifies when the sensory salience of a goal is high. Thus, rule (1) is translated into the following definition of SensSalient:
SensSalient(arb,g,status,goals)(inp,mem,env) =
IF goals[g].slc(inp,mem,env) = HighSLC ∧
status.intrinsic = HighLD ∧ extraneous = HighLD


THEN arb = HighSLC
ELSE goals[g].slc(inp,mem,env) = HighSLC ENDIF
Here, goals[g].slc(inp,mem,env) is the default salience (as determined in any specific case by HCI experts) of the goal g. The parameter arb represents a possible value of the actual sensory salience. This value is chosen non-deterministically as an index of the guarded command CommitAction (see Fig. 1).
The procedural salience rule is formally expressed as follows:

(2)
if default = HighSLC ∧ intr = HighLD then procedural = LowSLC
else procedural = default

In the cognitive architecture, this is translated into two predicates, ProcHigh and
ProcLow, that specify when the procedural salience is high and low, respectively:
ProcHigh(g,status,goals)(inp,mem,env) = goals[status.last].cue(g)(inp,mem,env) = HighSLC ∧ status.intrinsic = LowLD
ProcLow(g,status,goals)(inp,mem,env) = goals[status.last].cue(g)(inp,mem,env) = LowSLC ∨ goals[status.last].cue(g)(inp,mem,env) = HighSLC ∧
status.intrinsic = HighLD





Hierarchy of choices
Next we discuss how the sensory and procedural salience influence the choice of goals in our cognitive architecture. Recall that the underlying choice principle is non-determinism – any “enabled” goal can be chosen for execution. The addition of salience refines the notion of enabledness by introducing a hierarchy of choices into our cognitive architecture. We started with a version that included a two- level hierarchy, high salience and low salience. A goal was defined to have the high salience, if either of the predicates SensSalient or ProcHigh was true, otherwise its salience was defined as low. We also assumed that high salience goals have priority over low salience ones. However, with this version of the architecture, our verification efforts described in Section 5 produced errors (and the corresponding behaviours of the model) not observed during our empirical studies. The analysis of the counter examples suggested a refinement of the two-level hierarchy which yielded a new version specified in Fig. 2.
The new version consists of three levels of salience. Assuming the choice strategy and the guard for an atomic goal is true, its salience belongs to the highest level, if
(i) its procedural salience is high, or (ii) its procedural salience is low, and sensory salience is high (see Fig. 2). It belongs to the middle level (high salience), if it is not procedurally cued, but its sensory salience is high. Such a goal is only chosen, if there are no goals in the highest level. Finally, the lowest level includes all the remaining atomic goals whose choice strategy and guard are true.

HighestSalience(arb,g,status,goals)(inp,mem,env) = atom?(goals[g].subgoals) ∧ goals[g].grd(inp,mem,env) ∧ goals[g].choice(g,s) ∧ (ProcHigh(g,status,goals)(inp,mem,env) ∨ ProcLow(g,status,goals)(inp,mem,env) ∧
SensSalient(arb,g,status,goals)(inp,mem,env))

HighSalience(arb,g,status,goals)(inp,mem,env) = atom?(goals[g].subgoals) ∧ goals[g].grd(inp,mem,env) ∧ goals[g].choice(g,s) ∧ goals[status.last].cue(g)(inp,mem,env) = NoSLC ∧
SensSalient(arb,g,status,goals)(inp,mem,env)

LowSalience(arb,g,status,goals)(inp,mem,env) = atom?(goals[g].subgoals) ∧ goals[g].grd(inp,mem,env) ∧ goals[g].choice(g,s)

Fig. 2. Levels of salience

Fig. 3. ‘Fire Engine Dispatch’ interface

Fire Engine Dispatch Task
In this section, we describe the task we chose to model for the assessment of our development of the cognitive architecture.
The overall objective of the task was to send navigational information to fire engines enabling the fastest possible incident response times using the interface

shown in Fig. 3. When commencing the task an individual has to decide which call to prioritize before clicking on the ‘Start next call’ button. Choosing call priority involves clicking on the radio button that is located alongside the required call ID (see the bottom right part of Fig. 3). However, call priority is actually set only when the ‘Confirm priority change’ button is clicked. Clicking on this button updates the visual confirmation of the selected call, located at the top of the priority selection window (ID 4 in Fig. 3). The selected call is then processed by clicking on the ‘Start next call’ button.
The second part of this task is to construct the optimal route and send the necessary information to fire engines. This is done using the bottom left part of the interface from Fig. 3 which is displayed only when a call has been processed. At this point, the location of the nearest fire engine and the location of the incident are displayed as waypoints on the map in the upper part of the interface. Depending on the availability of GPS signals, there are two options for constructing route information: automatic and manual. The most appropriate automatically generated route could only be used when GPS signals are being received by the fire engine attending the incident. When GPS signals cannot be relied upon, a route must be constructed based on waypoint information in the local area. The indicator located above the telephone image informs which option must be used. The leftmost drop-down menu supports manual route construction by allowing the user to select waypoints and add them to the route by clicking on the ‘Add’ button. The selected waypoints are then displayed in the text box below. One of the automatically generated routes can be selected by clicking on the menu just above the ‘Add’ button. Selecting the wrong route construction method is regarded as a mode error.
The constructed route is sent by clicking the ‘Get/Send route information’ but- ton, thus finishing the task. However, before this step is taken, a fire engine des- ignated as the backup unit must be selected. This selection involves clicking the radio button alongside one of the units in the centrally located menu. Again the backup unit is only set once the ‘Route complete’ button has been clicked.

Task Verification
In this section, we instantiate our generic architecture, thus deriving a user model for the ‘Fire Engine Dispatch Task’. This model is then used for the verification of correctness properties for the interface described in the previous section. 6
In this paper, we consider one usability property. It aims to ensure that, in any possible system behaviour, the user’s main goal of interaction (as they perceive it) is eventually achieved. This is written in SAL as the following LTL assertion (here F means “eventually”):
F(Perceived(inp, mem, env))

6 The complete SAL sources for this example, including the SAL specification of the interface, are available at http://www.dcs.qmul.ac.uk/research/imc/hum/examples/fmis07.zip .

The main purpose of our verification is to find out how close, with respect to the errors detected, its results are to the data of our empirical studies. Since the actual experiments essentially consisted of two subtasks, we split the task (and its user model) into two parts: setting call priority (top goal PriorityGoal) and sending route information (top goal RouteGoal).
Call Priority
We assume that the user model for this (sub)task includes three atomic goals: SelectPriorityGoal, ConfirmPriorityGoal and StartCallGoal. As an example, SelectPriorityGoal is the following record:
choice := NotYetDischarged
grd := λ(inp,mem,env): inp.PrioritySelection slc := λ(inp,mem,env): HighSLC
cue := λ(g):λ(inp,mem,env):
IF g = ConfirmPriorityGoal THEN HighSLC ELSE NoSLC ENDIF subgoals := atom(SelectPriority)
Thus, this goal may be selected only if the priority selection menu is displayed. The choice strategy NotYetDischarged is a pre-defined predicate that allows one to choose a goal only when it has not been chosen before. We assume that the sensory salience of this goal is high, since the visual attention, at this point in task execution, should be in the correct area. The salience of this goal as a procedural cue for the call confirmation action is high, but it should not cue other actions. The corresponding action SelectPriority is defined as follows:
tout := λ(inp,out0,mem):λ(out):
out = Default WITH .PrioritySelected:= TRUE tmem := λ(inp,mem0,out):λ(mem):
mem = mem0 WITH .PrioritySelected:= TRUE
Here Default is a record with all its fields set to false thus asserting that nothing else is done.
The definitions for the other two goals are similar. Their sensory salience is assumed to be high. ConfirmPriorityGoal serves as a procedural cue of high salience for the goal StartCallGoal, whereas the latter being the last step in the procedure does not cue other actions. Finally, the top goal PriorityGoal for the call priority subtask is defined as follows:
load := λ(inp,mem,env): LowLD
achieved := λ(inp,mem,env): inp.WaitMsg subgoals :=
comp({SelectPriorityGoal, ConfirmPriorityGoal, StartCallGoal})
It includes all three atomic goals as its subgoals. Since setting call priority is a cognitively simple procedure, we assume that the intrinsic load for this task is low. Finally, the component achieved defines the perceived goal of the task. The latter is regarded as achieved when a wait message is displayed by the interface. This only

happens once the processing of the selected call has been started. Note that the interface specification ensures that this state can never be reached, if the priority setting procedure was not properly (as described in Section 4) executed.
The user model derived by the above instantiation of the generic architecture is parametric with respect to the extraneous load. This parameter can be manipulated in different verification runs, similarly as was done in the empirical studies.

Verification
For both extraneous load conditions, verification of property (3) fails. The counter examples indicate that the user model starts by immediately executing StartCallGoal. This corresponds to the empirical studies which found this initial- isation error to be systematic. Its main cognitive cause is that the required action of choosing priority is the first one in the procedure, and thus is not procedurally cued. This suggests that the initialisation error is unlikely to be eliminated by increasing the sensory salience of the relevant menu options. On the other hand, “sequencing” the interface so that the ‘Start next call’ button becomes available only when call priority has been set should eliminate this error. Verification of the task with a modified interface confirms this.
Next we check whether the task goal is achieved assuming the first step was taken correctly. This is expressed as a slightly modified property (3):
X(commit[SelectPriorityAction]=committed) ⇒ F(Perceived(inp,mem,env))
Here X is the LTL operator “next”. The property states that the task goal is eventually achieved, if the first thing that the user model does is committing to SelectPriorityAction. Verification of the modified usability property is successful for both extraneous load conditions. Again, this corresponds to the results of our empirical studies which found that the action of confirming priority is almost never omitted presumably due to its high procedural cueing by the previous action of selecting priority. Next we consider the second subtask.

Sending Route Information
We assume that the user model for this (sub)task includes the following atomic goals: ObserveModeGoal, GPSGoal, ManualGoal, ClickAddGoal, SelectBackupGoal, ConfirmRouteGoal and SendRouteGoal. For the goal of observing the mode in- dicator (ObserveModeGoal), the essential components are specified as follows:
grd := λ(inp,mem,env): inp.ModeDisplayed /= NoMode slc := λ(inp,mem,env): LowSLC
cue := λ(g):λ(inp,mem,env):
IF g = GPSGoal V g = ManualGoal THEN HighSLC ELSE NoSLC ENDIF
Here ModeDisplayed denotes the required mode for route construction. It can take one of the following three values: ModeGPS, ModeManual and NoMode. This goal can only be selected when the mode indicator is displayed and shows the required mode. The sensory salience of ObserveModeGoal is low, since the mode indica-

tor is displayed in a different area from the route construction menus. Observing the mode indicator is a highly salient procedural cue for both automatic and man- ual route construction goals, ModeGPS and ManualGoal, respectively. Finally, the memory update for ObserveModeAction specifies that the indicated mode of route construction is stored in memory:
tmem := λ(inp,mem0,out):λ(mem):
mem = mem0 WITH .mode := inp.ModeDisplayed
Similarly, the essential components for SelectBackupGoal are defined as fol- lows:
grd := λ(inp,mem,env): inp.BackupSelection slc := λ(inp,mem,env): HighSLC
cue := λ(g):λ(inp,mem,env):
IF g = ConfirmRouteGoal THEN HighSLC ELSE NoSLC ENDIF
Note that selecting a backup unit is a highly salient procedural cue for clicking the ‘Route complete’ button (ConfirmRouteGoal).
Finally, for the top goal RouteGoal of the subtask of sending route we have the following definitions:
load := intrinsic
achieved := λ(inp,mem,env): inp.SuccessMsg cue := λ(g):λ(inp,mem,env):
IF g = ObserveModeGoal THEN LowSLC ELSE NoSLC ENDIF subgoals :=
comp({ObserveModeGoal, GPSGoal, ManualGoal, ClickAddGoal,
SelectBackupGoal, ConfirmRouteGoal, SendRouteGoal})
The variable intrinsic is a parameter of our user model. It denotes the intrinsic load associated with the route construction procedure and depends on the construc- tion method required. As previously, the model is also parametric with respect to the extraneous load. The perceived task goal is to send route information. Achieving this goal is indicated by a success message displayed by the interface. RouteGoal is a lowly salient procedural cue for observing the mode indicator (ObserveModeGoal). Note that the latter is actually cued by the action of starting a new call, which ends the first subtask. Since the whole dispatch task is split into two subtasks, the pro- cedural cue for ObserveModeGoal is specified to be the top goal RouteGoal of the second subtask. We assume that the salience of this cueing is low, since the mode indicator is displayed after some delay during which a person is likely to be engaged in a complex process relevant to the route identification and construction.

Verification
As mentioned in Section 3, initially we used a hierarchy with two salience levels. For this version of the cognitive architecture, verification of property (3) produced erroneous behaviours that were not observed in the empirical studies. Namely, after selecting the backup unit, the user model was executing SendRouteAction

instead of ConfirmRouteAction which is procedurally cued with high salience by SelectBackupAction. This lead to the introduction of additional salience level, prioritising the goals with high procedural salience (e.g., ConfirmRouteAction) over those whose sensory salience is high (e.g., SendRouteAction). The modified version of the cognitive architecture was used for further verification.
To simplify the analysis of the counter examples produced, we introduced two additional correctness properties:
G(mem.RouteConstructed ⇒ (mem.mode /= NoMode))
G((out.ConfirmRoute V out.SendRoute) ⇒ mem.BackupSelected)
The first one is relevant to the mode error and states that the memory component RouteConstructed (updated by GPSAction and ClickAddAction) can only be true, if the user model attended to the mode indicator and stored its value in mem.mode. The second property states that the buttons ‘Confirm Route’ and ‘Get/Send Route’ can only be clicked, if a backup unit has been selected (indicated by the memory component BackupSelected which is updated by SelectBackupGoal. It is relevant in the situations when the action of selecting backup is omitted (termination error). We start with property (4). Its verification fails for all load conditions: the user model omits the action of attending to the mode indicator. These results are inconsistent with respect to the empirical studies which found that only both the in- trinsic and extraneous load being high leads to the systematic mode error. However, these inconsistencies are false positives. Our verification did not miss erroneous be- haviours. Rather, it indicated problems that were not seen in specific experiment when this task was performed by humans. This suggests that our salience and load rules and/or the hierarchy of salience levels need further refinements. We intend to
explore this in future work.
Next we verify property (5) relevant to the termination error. In this case, the correlation with the empirical data is closer. For all load conditions but intrinsic being high and extraneous being low, verification of (5) yields the same results as the empirical studies. In the case when both the intrinsic and the extraneous load is high the omission of selecting backup is observed. When the intrinsic load is low verification of (5) is successful. This corresponds to low error rates for these load conditions in our empirical studies (non-systematic error). On the other hand, the single mismatch occurring when the intrinsic load is high and the extraneous load is low is potentially more serious than previous false positives. In this case, verification is successful, even though our empirical data indicates a systematic error.
One possible explanation for this mismatch is that our judgement about salience values for some goals was inappropriate. In fact, verification yields the termination error when, for SelectBackupGoal, the salience of procedural cueing is set to high and the sensory salience is set to low (in the original specification, these values were low and high, respectively). Furthermore, the new value for procedural cueing could be reasonably argued for, though admittedly the new value for the sensory salience is probably more difficult to defend. Another possible explanation is that our rules are simply too coarse. Whichever the case may be further experiments are necessary.

Conclusion
In this paper, we added to our cognitive architecture the concepts of procedural and sensory salience. We formalised the connection between both salience types and cognitive load imposed by the complexity of the task performed. We then refined the underlying principle of non-deterministic choice of goals by introducing a hierarchy of choices governed by the salience of goals. Verification attempts using the new version of the cognitive architecture suggested further refinements to the hierarchy of salience levels.
As an assessment step for these developments, we undertook the formal mod- elling of the fire engine dispatch task used in our empirical studies. Our goal was to check the consistency between the behaviours generated by our cognitive architec- ture and those exhibited by human participants of our experiments. We found that, with respect to the initialisation error, verification yielded results that are consistent with the human behaviour observed during the experimental studies. For the mode error, our verification produced false positives in some cases. On the other hand, in the single case when the intrinsic load is high and the extraneous load is low, verification missed the termination error found to be systematic in the experiments. The inconsistencies shown in the results of the formal verification and the exper- iment in no way reduce the significance of our work. Rather they warrant further refinements of our formal rules and new empirical studies. In fact, this provides a good example of the cyclic nature of our research methodology. The results of our original experiment have led us to informal salience and cognitive load rules. Since those results have been obtained by manipulation of factors (cognitive and perceptual load) that are known to apply to other domains [20,12,13], we expect our rules to be generic. Their formalisation and the subsequent formal verification raise some questions and suggest new experimental hypotheses. These hypotheses need to be tested in future experiments that will allow us further refine our rules
and again assess them in formal verification.

References
Back, J., A. Blandford and P. Curzon, Slip errors and cue salience, “Proc. ECCE 2007: Invent! Explore!”
Back, J., A. Blandford, P. Curzon and R. Rukˇse˙nas, Explaining mode and omission errors: a load model, Submitted for journal publication.
Barnard, P. J., and J. May, Interactions with advanced graphical interfaces and the deployment of latent human knowledge, “Design, Specification and Verification of Interactive Systems: DSV-IS’95,” Springer-Verlag, 1995, 15–49.
Bowman, H., and G. Faconti, Analysing cognitive behaviour using LOTOS and Mexitl, Formal Aspects of Computing 11 (1999), 132–159.
Butterworth, R., A. Blandford and D. Duke, Demonstrating the cognitive plausibility of interactive systems, Formal Aspects of Computing 12 (2000), 237–259.
Byrne, M. D., and S. Bovair, A working memory model of a common procedural error, Cognitive Science 21(1) (1997), 31–61.
Cartwright-Finch, U., and N. Lavie, The role of perceptual load in inattentional blindness, Cognition
102(3) (2007), 321–340.


Chung, P., and M. D. Byrne, Visual cues to reduce errors in a routine procedural task, “Proc. 26th Ann. Conf. of the Cognitive Science Society, Hillsdale, NJ,” Lawrence Erlbaum Associates, 2004.
Curzon, P., and A. E. Blandford, Detecting multiple classes of user errors, “Proc. EHCI 2001,” vol. 2254 of LNCS, Springer-Verlag, 2001, 57–71.
Duke, D. J., P. J. Barnard, D. A. Duce and J. May, Syndetic modelling, Human-Computer Interaction
13(4) (1998), 337–394.
Gray, W., The nature and processing of errors in interactive behavior, Cognitive Science 24(2) (2000), 205–248.
Hale, A., and I. Glendon, “Individual Behaviour in the Control of Danger,” Industrial Safety Series – Volume 2, Elsevier, 1987.
Leveson, N. G., and C. S. Turner, An Investigation of the Therac-25 Accidents, IEEE Computer 26(7) (1993), 18–41.
de Moura, L., S. Owre, H. Ruess, J. Rushby, N. Shankar, M. Sorea and A. Tiwari, SAL 2, “Computer Aided Verification: CAV 2004,” vol. 3114 of LNCS, Springer-Verlag, 2004, 496–500.
Newell, A., “Unified Theories of Cognition,” Harvard University Press, 1990.
Reason, J.: “Human Error,” Cambridge University Press, 1990.
Rukˇse˙nas, R., P. Curzon, J. Back and A. Blandford, Formal modelling of cognitive interpretation, “Proc. DSVIS 2006,” vol. 4323 of LNCS, Springer-Verlag, 2007, 123–136.
Rukˇse˙nas, R., P. Curzon, A. Blandford and J. Back, Combining human error verification and timing analysis, To appear in “Proc. EIS 2007,” LNCS, Springer-Verlag.
Rushby, J., Analyzing cockpit interfaces using formal methods, Electronic Notes in Theoretical Computer Science 43 (2001).
Wickens, C., Multiple resources and performance prediction, Theoretical Issues in Ergonomic Science
3(2) (2002), 159–177.
