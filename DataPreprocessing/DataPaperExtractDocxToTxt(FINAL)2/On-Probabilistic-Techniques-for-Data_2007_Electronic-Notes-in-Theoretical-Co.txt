Electronic Notes in Theoretical Computer Science 190 (2007) 59–77	
www.elsevier.com/locate/entcs

On Probabilistic Techniques for Data Flow Analysis
Alessandra Di Pierro1	Chris Hankin2	Herbert Wiklicky3
Department of Computing Imperial College London 180 Queen’s Gate
London SW7 2AZ United Kingdom

Abstract
We present a semantics-based technique for analysing probabilistic properties of imperative programs. This consists in a probabilistic version of classical data flow analysis. We apply this technique to pWhile programs, i.e programs written in a probabilistic version of a simple While language. As a first step we introduce a syntax based definition of a linear operator semantics (LOS) which is equivalent to the standard structural operational semantics of While. The LOS of a pWhile program can be seen as the generator of a Discrete Time Markov Chain and plays a similar role as a collecting or trace semantics for classical While. Probabilistic Abstract Interpretation techniques are then employed in order to define data flow analyses for properties like Parity and Live Variables.
Keywords: Probabilistic Programs, Linear Operators Semantics, Probabilistic Abstract Interpretation, Data Flow Analysis


Introduction
Typically a classical Data Flow Analysis (DFA) considers all the paths in a control- flow graph and computes safe solutions according to a ‘meet-over-all-paths’ ap- proach, i.e. all paths contribute equally to the solution independently on how fea- sible or likely to be executed they are. However, in practice even moderately large programs execute only a small part of the billions of paths that are possible for the program to execute. For a given analysis, the specific nature of the problem under consideration may help to reduce the number of paths. Moreover, it is now widely recognised that considering probabilistic information allows one to obtain more pre- cise and practically useful results from a static analysis. Statistical or other types

1 Email: adip@doc.ic.ac.uk
2 Email: clh@doc.ic.ac.uk
3 Email: herbert@doc.ic.ac.uk

1571-0661 © 2007 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.07.005

of information can be encoded in the form of probabilities in the program semantics and used to weight the execution paths according to their likelihood to actually be executed. Following this approach, in this paper we define probabilistic DFA whose solutions are not safe in the classical sense but ‘close’ enough to the concrete behaviour of the program. This is similar to the approach in [7,8], where proba- bilistic techniques are used for the construction of static analyses of approximate properties. Considering approximate rather than absolute properties corresponds to asserting the property up to a given quantifiable error margin rather than as a Boolean relation. The significance of this approach is that it allows in general for more realistic analyses. As an example, a security property such as confinement is never satisfied by real systems in its absolute formulation asserting whether the system is confined or not.
We show how a classical DFA can be turned into a probabilistic one via the use of an appropriate collecting semantics. This is defined as a linear operator on a linear space representing the operational configurations (current state and instruction to be executed) of a given probabilistic program. We will show that the use of tensor product is essential in the construction of the semantics in order to obtain a correct result of the analysis. This is demonstrated via the application of our probabilistic technique to two classical DFA examples, namely Parity Analysis and Live Variables Analysis. These examples also show how probabilistic analysis can result in more practically useful information compared to the standard classical techniques.
Probabilistic While
The language we consider is a simple classical While language extended with a probabilistic choice statement. A different but equivalent approach to defining prob- abilistic languages is the use of random assignment as in e.g. [14]. We choose the first approach because it is more suited as a base for the treatment we present in this paper. The same approach is also adopted in [6] where a Hoare-style proof system is introduced for reasoning about probabilistic sequential programs.
Syntax
The syntax of the language, which we call pWhile, is given by:
S ::= skip | stop | x ← a | S1; S2 | choose p1 : S1 or p2 : S2 |
if b then S1 else S2 end if | while b do S end while
In the probabilistic choice construct we assume that pi are some constants rep- resenting the probability that statement Si is executed; they define a probability distribution, i.e. p1 + p2 = 1. Arithmetic expressions a follow the usual syntax:
a ::= n | x | a1 ⊙ a2
where n represents a constant value, x a program variable (for which we use typeface characters) and ‘⊙’ represents one of the usual arithmetic operations, like ‘+’, ‘−’, or ‘×’. We also need Boolean expressions which have the following form:










Table 1
A Probabilistic Transition System for pWhile

b ::= true | false | ¬b | b1 ∨ b2 | b1 ∧ b2 | a1 <> a2
where ‘<>’ denotes one of the standard comparison operators for arithmetic expres- sions like: <, ≤, =, ≥, >.

Operational Semantics
In this section we define a small step operational semantics for pWhile in the usual SOS style [17]. This corresponds to a probabilistic transition system [13] where states encode information on both the memory state (values of the program’s vari- ables) and the instruction to be executed. The transition relation is probabilistic in the sense that the configuration reached in one step from another configuration is associated with a number expressing the probability of actually performing that transition; moreover, all probabilities associated to the outgoing transitions of a configuration form a probability distribution (i.e. they sum up to one). In other words, we define the operational semantics of our language according to the gen- erative model of probability [20] and the resulting execution process is a Discrete Time Markov Chain, [19,1].

Conﬁgurations and Transitions
We define a state as a mapping from variables to values:
σ ∈ State = Var → Value,



Table 2 Semantics of Expressions

The value of arithmetic and Boolean expressions is given by the functions [.]]a and [.]]b defined in Table 2. We will omit the index from the function [. ] when it is clear from the context.
A conﬁguration is a pair ⟨S, σ⟩ consisting of a statement S and a state σ ∈ State. The probabilistic transition relation −→ p is defined by the rules in Table 1. These reflect the typical semantics of a classical while language but for the presence of a probability labelling each transition. Final states are represented via a looping on configurations ⟨stop, σ⟩. This allows us to model a computation as a Markov chain as it guarantees that the matrix representing the execution of a pWhile program is a stochastic matrix (cf. Section 3.1).
Abstract Syntax
Following the approach in [16], in order to determine the control flow (graph) of a program, regardless of the concrete information on the actual states in which each statement is executed, we associate a label from a set Lab to each assignment statement, to the skip statement and to each test that appears in conditional and loop statements. We call the resulting program a labelled program and define its syntax, which we call abstract syntax, as follows:
S ::= [skip]l | [stop]l | [x ← a]l | S1; S2 | [choose]l p1 : S1 or p2 : S2
| if [b]l then S1 else S2 end if | while [b]l do S end while
We will refer to the elementary labelled instructions as commands and denote by Cmd the set of all commands with typical elements c1, c2,.. ., corresponding to statements of the form [skip]l or [b]l, etc. We will assume a unique labelling of commands, i.e. we will use l interchangeably with c, where c is the elementary instruction with label l; this is sometimes called the label consistency property [16]. The control flow FS of a program S is then defined as a set of pairs (l, l') or ⟨c1, c2⟩
which record the fact that it is possible that after executing c1 the next command
executed could be c2 (abstracting from the concrete outcomes of tests).
In principle, it is possible to reconstruct the syntax of the original program S from the pairs ⟨l, l'⟩ in FS: we only have to indicate which branch is taken when a test succeeds or fails; and for probabilistic choices we need to record the transition probabilities. If we do this the control flow FS of S, which is sometimes also referred to as its abstract syntax [5], can be taken as an alternative description of the syntax of the program. In this paper, we will use the terms control flow and abstract syntax

interchangeably to refer to a labelled program.

Probabilistic Data Flow Analysis
Classically, Data Flow Analysis is based on a graph representation of a program usually defined via a collecting semantics. For our probabilistic language we will define such a collecting semantics as a linear operator (more precisely a Markov chain); this represents the states and the program statements in a way that the probabilistic information can be appropriately derived in the final result of the analysis. We will consider here the more common equational approach to DFA and introduce a probabilistic technique based on this approach and the linear operator semantics in Section 3.1.
The probabilistic DFA we present is intra-procedural (we do not deal with func- tions or procedures). We will demonstrate our technique by presenting two classical examples of a forward and a backward analysis, the former being specifically a Parity Analysis and the latter Live Variables Analysis.

Collecting Semantics
A probabilistic transition system has a straightforward representation as a matrix,
i.e. a linear operator on the vector space over configurations. In general, for any set X we can define the vector space V(X) over X as the space of formal linear combinations of elements in X with coefficients in a field W. We can represent the elements in V(X) as vectors with coefficients in W indexed by X:
V(X)= { (vx)x∈X | vx ∈ W}.
The field W is typically taken to be the set of complex numbers C. In our case it simply corresponds to [0, 1] ⊆ R ⊆ C, as we are interested in representing probability distributions. The support set X corresponds to the set of configurations Conf.
A convenient way to represent the vector space on configurations is via the tensor product operation. The tensor product is a useful tool when it comes to describing composite systems, such as in the theory of stochastic systems, performance anal- ysis, quantum physics, etc. Its properties allow us to elegantly represent the vector
space over a Cartesian product V(X × Y ) as V(X) ⊗ V(Y ). These properties and an abstract definition can be found e.g. in [18].
Concretely, we can define the tensor or Kronecker product of two finite dimen- sional matrices as follows. Given an n × m matrix A and and a k × l matrix B then A ⊗ B is the nk × ml matrix:


A ⊗ B =	.
⎝
. . .
.
.	⎟⎠

a1mB ... anmB
Based on the abstract syntax or control flow of a program S we define the col- lecting semantics of S as a linear operator on V(State × Cmd) = V(Value) ⊗



Table 3 Collecting Semantics

V(Cmd) = V(Value1) ⊗ ... ⊗ V(Valuen) ⊗ V(Cmd). Here we assume an enu- meration of the program variables x1,... , xn and denote by Valuei the set of values for xi. Thus, we have that V(Value) = V(Value1 × ... × Valuen) = V(Value1) ⊗ ... ⊗ V(Valuen).
For all possible control flow steps in FS, i.e. pairs of commands ⟨c1, c2⟩ in a program S we will describe the possible changes to the variables and then sum up these effects together with the actual transfer of control from one statement to another. In other words, we define the collecting semantics ßSJ of a program S in
terms of the linear operators T(⟨c1, c2⟩), which for every pair ⟨c1, c2⟩ ∈ FS transform a vector configuration →x ∈ V(State×Cmd) into the vector configuration describing the effect of the control flow step from c1 to c2. Formally:

ßSJ =
⟨c1,c2⟩∈FS
T(⟨c1, c2⟩)

where T can be expressed via the tensor product:
T(⟨c1, c2⟩)= M(⟨c1, c2⟩) ⊗ N(⟨c1, c2⟩)
with M(⟨c1, c2⟩) an operator on V(State) describing the possible state changes, and N(⟨c1, c2⟩) an operator on V(Cmd) describing the control flow. For all possible commands c1, c2 ∈ Cmd we define T(⟨c1, c2⟩) in Table 3, where we use the control and state operators explained below.
Note that the last rule in Table 3 which expresses the looping on the final configuration guarantees that ßSJ is a stochastic matrix, and that the collecting semantics of a program is indeed a Markov chain.

Control Operators
In the deterministic case, i.e. the case where for a command c1 there is only one command c2 with ⟨c1, c2⟩ ∈ FS, the control flow part N is very simple: it is given by a so called matrix unit Ec1,c2 containing a single non-zero entry in row ci and column cj, namely


N(⟨c ,c ⟩)= (E
)	= ,⎨ 1 for i = c1 ∧ j = c2

This is the case for [skip]l, [stop]l and [x ← a]l. This kind of control transition applies in particular to the case of the control flow between the statements in a sequential composition.
In the case of the random choice we have to construct the control transfer oper- ator as a linear combination between two deterministic transitions, from the choice point to the initial label of each of the branches. This is effectively achieved by defining
N(⟨c, ck⟩)i,j = pk · Ec,ck
where c is the choice command and ck the first command in the kth branch.
For tests c = [b]l in while’s and if’s we also have to construct a linear combi- nation, however in this case we have to filter in each case those transitions which actually happen when the test succeeds from those when the test does not succeed. Thus, we define the control flow at a test point by
T(⟨c, ct⟩) = B(b) · (I ⊗ Ec,ct )
T(⟨c, cf ⟩) = (I − B(b)) · (I ⊗ Ec,cf )
where c is the test command [b]l and ct and cf the successors in case the test succeeds or fails, respectively. In this definition the operator I is the identity while the test operator B is defined by:
D(evk ) 
B(b)=	Σ	 
n
b(v1,...,vn)=true	k=1
where D(x) denotes the diagonal matrix with diagonal vector x and ei the unit- vectors:
(e )= ,⎨ 1 if i = j
0 otherwise.
Thus, D(ei) = Ei,i, and B contains a ‘1’ in the diagonal if for some values of the variables the test b succeeds.
Example 3.1 Suppose that we have two variables x1 and x2 which can both have values between 0 and 4. Then
[x1 < 2]l
is represented by a 25 × 25 matrix given as the tensor product:

1 0 0 0 0 
0 1 0 0 0 
⎛ 1 0 0 0 0 ⎞
⎜	⎟




0 0 0 0 0 
0 0 0 0 0 
⎜⎜⎝ 0 0 0 1 0 ⎟⎟⎠



which expresses the fact that the value of x2 does not contribute to the result of the test.

State Operators
For all commands c except for the assignment [x ← a]l the variables will not change, i.e. we will have M = I, i.e. the identity on the vector space representing the
value of the program variables. In the case of the assignment we have for constants


M([xi
i−1
← n]l1 , c2)=	I
k=1
⎛⎝Σj
Ejn⎞⎠ ⊗
n
I	,
k=i+1

and for general expressions a involving (other) variables

M([xi ← a]l1 , c2)=	Σ
Av ,...,v '→v,...,v


where Av1 ,...,vi'→v,...,vn is a matrix with a single non-zero entry defined as:

i−1

k=1

D(evk ) 
⊗ ⎛⎝Σ

Evi,v ⎞⎠ ⊗
n

k=i+1

D(evk )  .

In other words, if a certain combination of values vi for the variables in a pro- duces a certain result v, then the corresponding matrix Av1 ,...,vi'→v,...,vn indicating this single update contributes to the sum.
Example 3.2 Consider two variables x1 and x2 which both can take values in
{0, 1, 2}. Then the update
[x1 ← x1 + 1]l
is represented by a 9 × 9 matrix, which is the tensor product of two 3 × 3 matrices, namely:
⎛ 0 1 0 ⎞	⎛ 1 0 0 ⎞




while

is represented by
⎝ 0 0 0 ⎠

[x2 ← 1]l
⎛ 1 0 0 ⎞
⎝ 0 0 1 ⎠




⎛ 0 1 0 ⎞



⎝ 0 0 1 ⎠	⎝ 0 1 0 ⎠
In the following example we show how to construct the full collecting semantics for a simple pWhile program S.
Example 3.3 Consider the program S defined by:



if [x == 0]a then
[x ← 0]b;
else
[x ← 1]c; end if ; [stop]d




If we enumerate the four relevant program points by a, b, c and d and assume that x can have only two values, namely 0 and 1 then the whole program is represented by (where Id denotes the d × d identity matrix):
ßSJ = ⎛⎛ 1 0 ⎞ ⊗ I ⎞ · (I ⊗ E )+ 

⎝⎝ 0 0 ⎠
4⎠	2	ab

+ ⎛⎛ 0 0 ⎞ ⊗ I ⎞ · (I ⊗ E  )+ 

⎝⎝ 0 1 ⎠
4⎠	2	ac

+ ⎛⎛ 1 0 ⎞ ⊗ E
+ ⎛⎛ 0 1 ⎞ ⊗ E
bd⎞⎠ +
⎞ +

⎝⎝ 0 1 ⎠
cd⎠

+ ⎛⎛ 1 0 ⎞ ⊗ E
dd⎞⎠ =

⎛⎜	⎛ 1 0 0 0 ⎞⎞ ⎛
⎛ 0 1 0 0 ⎞⎞

⎜⎝⎝ 0 0 ⎠

⎛⎜

0 0 1 0
⎝ 0 0 0 1 ⎠⎠
⎛ 1 0 0 0 ⎞⎞
⎜⎝⎝ 0 1 ⎠

⎛

0 0 0 0
⎝ 0 0 0 0 ⎠⎠
⎛ 0 0 1 0 ⎞⎞

⎜⎝⎝ 0 1 ⎠

0 0 1 0
⎝ 0 0 0 1 ⎠⎠
⎜⎝⎝ 0 1 ⎠

0 0 0 0
⎝ 0 0 0 0 ⎠⎠

⎛⎜	⎛ 0 0 0 0 ⎞⎞	⎛
⎛ 0 0 0 0 ⎞⎞



⎜⎝⎝ 1 0 ⎠

⎛⎜

0 0 0 0
⎝ 0 0 0 0 ⎠⎠
⎛ 0 0 0 0 ⎞⎞
⎜⎝⎝ 0 1 ⎠

0 0 0 1 
⎝ 0 0 0 0 ⎠⎠

⎜⎝⎝ 0 1 ⎠

⎛⎜

0 0 0 0
⎝ 0 0 0 1 ⎠⎠
⎛ 0 1 0 0 ⎞⎞	⎛




⎛ 0 0 1 0 ⎞⎞

⎜⎝⎝ 0 0 ⎠

⎛⎜

0 0 0 0
⎝ 0 0 0 0 ⎠⎠
⎛ 0 0 0 0 ⎞⎞
⎜⎝⎝ 0 1 ⎠

⎛

0 0 0 0 
⎝ 0 0 0 0 ⎠⎠
⎛ 0 0 0 0 ⎞⎞

⎜⎝⎝ 1 0 ⎠

⎛⎜

0 0 0 0
⎝ 0 0 0 0 ⎠⎠
⎛ 0 0 0 0 ⎞⎞
⎜⎝⎝ 0 1 ⎠

0 0 0 1 
⎝ 0 0 0 0 ⎠⎠

⎜⎝⎝ 0 1 ⎠

0 0 0 0
⎝ 0 0 0 1 ⎠⎠

The collecting semantics results therefore in the following stochastic matrix:






ßSJ =
0 1 0 0 0 0 0 0 
0 0 0 1 0 0 0 0 
0 0 0 0 0 0 0 1 
0 0 0 1 0 0 0 0 
0 0 0 0 0 0 1 0 
0 0 0 1 0 0 0 0 
0 0 0 0 0 0 0 1 
⎝ 0 0 0 0 0 0 0 1 ⎠
... ⟨x = 0, [x == 0]a⟩
...  ⟨x = 0, [x ← 0]b⟩
...  ⟨x = 0, [x ← 1]c⟩
...  ⟨x = 0, [stop]d⟩
... ⟨x = 1, [x == 0]a⟩
...  ⟨x = 1, [x ← 0]b⟩
...  ⟨x = 1, [x ← 1]c⟩
...  ⟨x = 1, [stop]d⟩

Probabilistic Abstract Interpretation
In order to construct abstract DFA equations for a pWhile program out of the col- lecting semantics we need a way to abstract the semantics. A quantitative approach for obtaining DFA equations is provided by our framework of Probabilistic Abstract Interpretation (PAI) [10,11]. This is a general framework for the probabilistic anal- ysis of (probabilistic) programs which recasts classical Abstract Interpretation [3,4] in a vector space setting. The basic idea is to define abstractions as some kinds of linear operators on some appropriate vector spaces. More precisely, domains (both abstract and concrete) are Hilbert spaces that is vector spaces with inner product. The crucial notion of concretisation is expressed in this framework via a notion of generalised inverse of a linear operator, and in particular the notion of Moore- Penrose pseudo-inverse (MP). The properties of this kind of linear inverse makes the pair of a linear operator and its MP into an adjunction similar to the classical notion of Galois connection.
Let C and D be two Hilbert spaces and A : C '→ D a bounded linear map between them. A bounded linear map A† = G : D '→ C is the Moore-Penrose pseudo-inverse of A iff
A ◦ G = PA and G ◦ A = PG


where PA and PG denote orthogonal projections (i.e. P∗
= PA = P2
and P∗ =

PG = P2 where .∗ denotes the adjoint [18, Ch 10]) onto the ranges of A and G.
A probabilistic abstract interpretation is defined by a pair of linear maps, A :
C '→ D and G : D '→ C, between the concrete domain C and the abstract domain
D, such that G is the Moore-Penrose pseudo-inverse of A, and vice versa.
As shown in [10] any classical abstract interpretation can be seen as a prob- abilistic abstract interpretation by lifting the classical domains into probabilistic domains (Hilbert spaces). Another approach to the analysis of probabilistic pro- grams applies classical Abstract Interpretation techniques to probabilistic semantics [15]. This approach corresponds to a special case of classical abstract interpretation and it therefore results in safe, i.e. worst case analyses. As shown in [9], the more general PAI approach allows us to construct additionally statistical information which is more in the spirit of an average case analysis.
Let T be a linear operator on some Hilbert space V representing the probabilistic semantics of the concrete program, and let A : V '→ W be a linear abstraction function with G = A†. An abstract semantics can then be defined on W as:
T# = GTA.

This so-called induced semantics is guaranteed to be the best correct approximation of the concrete semantics for classical abstractions based on Galois connections. In the linear space based setting of PAI where the order of the classical domains is replaced by some notion of metric distance, the induced abstract semantics is the closest one to the concrete semantics [10].

Parity Analysis
The first example of an analysis based on the LOS of pWhile concerns the parity of variables. Although, it is arguable whether this analysis by itself is of any practical use it is certainly a quite useful example in order to illustrate the basic methodology. The purpose of the Parity Analysis is to determine at every program point whether a variable is even or odd. It is a simple forward analysis where the concrete
value of variables is abstracted to just their parity information.
We will consider two simple programs which compute the factorial and twice the factorial of n which is left in m when the programs terminate.



[m ← 1]1;
while [n > 1]2 do
[m ← m × n]3; [n ← n − 1]4
end while
[stop]5
[m ← 2]1;
while [n > 1]2 do
[m ← m × n]3; [n ← n − 1]4
end while
[stop]5



Without going into too many details, it should be obvious that a classical Parity Analysis should detect that the parity of m = 2×n! at the end of the second program is always even even when the original value (and parity) of n is “unknown”. A safe classical analysis of the first program will however fail in the sense that it will return
“unknown” or T for the parity of m at the end of the program. This is in some sense rather unsatisfactory as it is obvious that m = n! is “nearly always” even. Only in the “rare” case that the initial n is less than or equal to 1 will m be odd at the end of the program. The purpose of a probabilistic program analysis is a formal
derivation of this intuition about the parity of m when the program terminates.
The idea is to (re)construct the abstract LOS of a program in the same way as the concrete LOS with the only difference that in the abstract case (some of the) operators which implement tests and assignments are replaced by their abstract versions. The general scheme for this program is given by:
T = Mi ⊗ I ⊗ E1,2
+ TT(I ⊗ I ⊗ E2,3)
+ T⊥(I ⊗ I ⊗ E2,5)
+ A ⊗ E3,4
+ I ⊗ N ⊗ E4,2
+ I ⊗ I ⊗ E5,5
where the first factor, i.e. first variable, represents m and the second one n. By a slight abuse of notation we use the same symbol I for identities of different di- mensions. The last component in each contribution Eij is a 5 × 5 matrix which represents the control-flow steps.
The first term specifies the dynamics at program point 1: m is initialised using an operator Mi with i = 1, 2 – depending which of the two programs we consider –

of the form:


0 1 0 0 ... 
0 1 0 0 ... 
M1 =
0 1 0 0 ... 



0 0 1 0 ... 
0 0 1 0 ... 
M2 =
0 0 1 0 ... 


  	  

which implement a change of m’s value to 1 and 2, respectively. The variable n stays unchanged, represented by the identity I and control is transfered from statement 1 to statement 2.
The next two terms deal with the test [n > 1]2: The actual effect is just a control transfer with no changes to the two variables. However, depending on the test’s result we get different control-flow transfers. The test operators are realised as:

⎛ 0 0 0 0 ... ⎞
0 0 0 0 ... 
⎜	⎟
⎛ 1 0 0 0 ... ⎞
0 1 0 0 ... 
⎜	⎟



0 0 0 1 ... 
⎝	⎠
0 0 0 0 ... 
⎝	⎠



TT “filters” all those configurations where n is 2, 3,... while T⊥ “blocks” all con- figurations except where n is equal to 0 or 1. In both cases the value of m and the current position in the program are irrelevant; this is indicated by the identity operators used to define the test operators.
The fourth term represents the most complicated arithmetic operation namely the statement [m ← m × n]3. The changes to m and n are rather complicated so that A, the operator describing the effects on m and n, is given as an “entangled” operator of the form:

A =	((I ⊗ Ej,i·j ⊗ I)(Ti ⊗ Tj ⊗ I))
i,j

with Tk = Ek,k for all k. This can be interpreted as follows: For all pairs (m, n) we first test whether m and n have certain values i and j, respectively (ignoring the current statement label), then we perform the update by leaving n (and the current statement label) unchanged but changing the value of m from j to its new value i · j.
The penultimate term in the definition of T results in the decrement of n via

the (shift) operator:

⎛ 0 0 0 0 ... ⎞
1 0 0 0 ... 
N =	0 1 0 0 ... 
0 0 1 0 ... 
⎝	⎠


and the last term just specifies the terminal configuration [stop]5 as an infinite loop which leave m and n unchanged.

Concrete Semantics
Both programs have five program points, which means that the control transition matrices are 5×5 matrices. The state of the two variables are in principle represented by infinite dimensional vectors. However, if we fix a maximal input size n ≤ N then we can also restrict the maximum size of m. For this finite approximation we need a vector in V({0, 1,... ,N !}) ⊗ V({0, 1,... ,N }) ⊗ V({1,... , 5}), i.e. a 5 · N · N ! dimensional vector. The concrete (finite) semantics is thus represented by a matrix of the same dimensions. Though sparse, it is for N = 10 already an about 18 million × 18 million matrix.

Abstract Semantics
In order to obtain an abstract semantics we can abstract both variables, i.e. “execute” the program with m and n being classified as even or odd. Alternatively, we can abstract only m and use the concrete values for the control variable n (the while-loop here is actually a for-loop). In both cases the dimension of T is drastically reduced. It is 2 · 2 · 5 = 20 in the first case, and 2 · N · 5= 10N in the
second case – for N = 10 we get a 100 × 100 matrix to represent T# instead of the 18 million in the concrete case.
To be more precise: We construct the abstract semantics T# simply as
T# = M# ⊗ I ⊗ E1,2
+ T#(I ⊗ I ⊗ E2,3)
+ T#(I ⊗ I ⊗ E2,5)
+ A# ⊗ E3,4
+ I ⊗ N# ⊗ E4,2
+ I ⊗ I ⊗ E5,5
i.e. exactly in the same way as the concrete one. The difference is exhibited by only few of the operators (and the fact that the identities have a much reduced dimension). The control-flow steps are not changed at all as we are interested here only in a data-flow analysis. Each of the operators is constructed using the parity

abstraction operator P:

PT =	0 1 0 1 0 ... 
1 0 1 0 1 ... 

and its Moore-Penrose Pseudo Inverse P†. For example: M#
= P†MP, A# =

(P† ⊗ P†)A(P ⊗ P), etc. We thus get (if we abstract both variables):
M# = ⎛ 0 1 ⎞	M# = ⎛ 1 0 ⎞	N# = ⎛ 0 1 ⎞	A# = ⎛ 0 1 ⎞
1	⎝ 0 1 ⎠	2	⎝ 1 0 ⎠	⎝ 1 0 ⎠	⎝ 1 0 ⎠
The abstract test operators T# and T# depend on the finite approximation we
consider. If we do not abstract the control variable n we need a more complicated

A# but the test operators T#
and T#
as well as N# are the same as for the

concrete semantics.

Numerical Results
It is rather straightforward to implement the finite approximations of the con- crete as well as the abstract semantics using a numerical program like octave [12]. As to be expected the size, even using sparse matrix representations, is prohibitively large; while for N = 5 things still work out it is effectively impossible to deal with case that N = 10. The abstract semantics creates no such problem.
If we abstract m to even and odd but leave n a concrete control variable, we can “execute” the (abstract) program for example in an initial configuration where n is represented by a uniform distribution over all possible input values for n < N and m has a 50 : 50 chance of being even and odd. Depending on the cut-off point N we obtain for N = 10 a 20%, for N = 100 a 2%, and for N = 1000 a 0.2% chance that m is odd at the end of the program. As expected, this reflects exactly the fact that m “most likely” will be even in the end, the more so the larger the maximal input for n is.

Live Variable Analysis
A more interesting static program analysis is the classical Live Variable (LV) Analysis, see e.g. [16]. It is an example of a backward analysis, i.e. the control flow has to be reversed. In order to do this we use the transposes of the original control-transfer matrices.
The property we are interested in is whether a variable at every program point is live or dead (at the exit of a program point), i.e. if there is a possible use for a variable at a later stage of the program’s execution. If, for example, there is an assignment to a program variable which is never used then we can eliminate this
assignment, i.e. perform a dead code elimination. The aim of a probabilistic LV
Analysis is to provide estimates for the probabilities that a certain variable is later
used. A possible application could be to support caching, variables which are more

likely to be used could be kept in a ‘faster’ cache, while variables which have a very low probability of being live would be kept in a ‘slower’ cache or the main memory. The idea is, as before, to replace the operators which define the concrete se- mantics (describing the precise changes of the values of variables) with operators which only record whether a variable is used (in a test b or an expression a on the right hand side of an assignment) or being redefined (it appears on the left hand
side of an assignment). This is essentially what also happens in the classical LV
Analysis: if x is on the right hand side of an assignment it gets “killed”, if it is a
(free) variable in a or b then it becomes alive (gets “generated”). The semantics of all other statements does not change the “liveliness” of a variable.
Intra-Statement Updates
The information we record about every variable is just if it is alive or not, i.e. we need to consider for every variable an abstract state in V({live, dead}). The update can be specified in a very similar way as in the case of the classical analysis (following e.g. [16]), where the local transfer function is usually defined by means
of two auxiliary functions kill and gen returning for each label the set of variables
which are dead and alive, respectively. We can define kill and gen operators as:

0 1 
⎝ 0 1 ⎠
G = ⎛ 1 0 ⎞ .

By using these operators we can represent tests and assignments by means of the following abstract operators:
ß[b]lJ = V(b) ⊗ NF	and
⎛ i−1		⎞

ß[xi := a] J = ⎝(

j=1
I ⊗ K ⊗

j=i+1
I) · V(a)⎠ ⊗ N ,

where NF is the transposed of the control-flow matrix, and V(e) identifies the (free) variables in a Boolean or arithmetic expression e:

n
V(e)= 
j=1
V (e) with V (e)= , G if xi ∈ FV (e)
, I otherwise.

Note, that we could obtain these operators by abstracting the concrete ones but for brevity’s sake we based our definitions directly on the classical analysis (as presented in [16]).
Inter-Statement Updates
The problem which we still have to resolve is what happens at “confluence points”,
i.e. when several (backward) control-flows come together as at a test in an if- or while-statement. To do this we need additional information about the branching probabilities. This could be obtained (i) experimentally from profiling or (ii) from the concrete semantics, by not abstracting at least those variables which determine

the control-flow (as in the previous example) or (iii) via another abstract seman- tics which estimates these probabilities. In the following we will sketch the basic elements of an analysis which is based on the last option.
To illustrate the basic aims and structure of a probabilistic version of the LV Anal-
ysis consider the following two programs:



[skip]1
if [odd(y)]2 then
[x ← 1]3
else
[y ← 1]4
end if
[y ← x]5
[y ← 2 × x]1
if [odd(y)]2 then
[x ← 1]3
else
[y ← 1]4
end if
[y ← x]5

The classical LV Analysis of both programs, starting with LVexit(5) = ∅, results, among other things, in the following description of the live variables at the beginning of the test in the second statement:
LVentry(2) = {x, y}.
This indicates the fact that both variables x and y might be alive at the test point. This is a conservative result. It is possible that in a concrete execution x is actually not alive: although it will be needed at label 5, it will be ‘killed’ (by redefining its value) if the test results fails, i.e. if y is even. In the first program, we can’t make too many assumptions about the probability that the test succeeds, but a closer inspection of the second program easily confirms that indeed y is always even, although the lack of knowledge about x makes it impossible to know the concrete value of y.
A reasonable result of a probabilistic LV analysis we are aiming for could be:


for the first program and
LVentry
1
(2) = {⟨x, ⟩, ⟨y, 1⟩}
2

LVentry(2) = {⟨y, 1⟩}
for the second one. In the first case we would expect that a good estimate for the branching probabilities is given by 50 : 50 chance that the test succeeds or fails. This is a consequence that (without any further assumptions) the chances that y is even or odd are the same. In the second program we can guarantee via a reasonable parity analysis that y is always even, despite the fact that the exact value of y is completely unknown.
As with all probabilistic versions of classical analyses we could (re)construct the classical result from the probabilistic version by just recording the possibilities instead of probabilities. For this we only have to define a forgetful map from V(X) to P(X) which just considers the support of a probability distribution, i.e. those elements in X which are associated with a non-zero probability.

Estimating the Branching Probabilities
A formal estimation of the branching probabilities for if-statements (and similarly for while-loops) follows the following scheme: Perform a first phase analysis (e.g. a parity analysis) to determine the branching probabilities, then use these estimates to perform the actual analysis. This means in effect that we replace tests b with
probabilistic choices where the choice probabilities pT and p⊥ are determined by a first phase analysis.

[skip]1 [choose]2
pT : [x ← 1]3
or
p⊥ : [y ← 1]4 [y ← x]5
[y ← 2 × x]1
[choose]2
pT : [x ← 1]3
or
p⊥ : [y ← 1]4 [y ← x]5


As can be seen in this example our approach makes it necessary that “abstract” programs are eventually probabilistic ones, even when the “concrete” programs were deterministic.
Once we have determined the choice probabilities pT and p⊥ the actual LV
Analysis is performed in essentially the same way as in the classical case, with
the exception that we have to deal with probability distributions over variables (indicating the chances that a variable is live at a certain program point) instead of sets of potentially live variables. In order to combine information about live variables at “confluence” points we then use weighted sums or linear combinations
utilising the choice probabilities pT and p⊥ instead of set union as in the classical case.

Conclusions
In this paper we presented the basic elements of syntax-based probabilistic data- flow analyses constructed via Probabilistic Abstract Interpretation which replaces the standard Cousot & Cousot approach for a possibilistic analysis.
We illustrated this framework using a small imperative language with a proba- bilistic choice construct, namely pWhile. For this language we presented a collect- ing semantics which essentially constructs the generator of a Discrete Time Markov Chain implementing the operational semantics of a pWhile program. This col- lecting semantics was defined “compositionally” using linear combinations (sums) and the tensor product in order to represent states and state transformations. Ex- ploiting this structure we could then define an abstract semantics as the basis of our analyses. This kind of “compositional” analysis was possible because sum and tensor product distribute over probabilistic abstractions.
While it is straightforward to lift analyses like the Parity Analysis to a proba- bilistic version, other analyses like the Live Variable Analysis require a two-phase analysis where we first estimate the branching probabilities before the actual anal- ysis takes place. Further work will look into the issue of optimal abstractions, i.e.

ways to construct abstractions of in particular control variables in order to obtain optimal estimates for these branching probabilities. Alternatively, for sub-optimal abstractions, we are also interested in determining the error margins of these esti- mates. Finally, we plan to extend this approach also to higher order, e.g. functional, languages and to implement a prototype of an automatic analyser.

References
Bause, F. and P. Kritzinger, “Stochastic Petri Nets – An Introduction to the Theory,” Vieweg Verlag, 2002, second edition.
Bergstra, J., A. Ponse and S. Smolka, editors, “Handbook of Process Algebra,” Elsevier Science, Amsterdam, 2001.
Cousot, P. and R. Cousot, Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints, in: Proceedings of POPL’77, Los Angeles, 1977, pp. 238–252.
Cousot, P. and R. Cousot, Systematic Design of Program Analysis Frameworks, in: Proceedings of POPL’79, San Antonio, Texas, 1979, pp. 269–282.
Cousot, P. and R. Cousot, Systematic design of program transformation frameworks by abstract interpretation, in: Proceedings of POPL’02 (2002), pp. 178–190.
den Hartog, J. and E. de Vink, Verifying probabilistic programs using a Hoare-like logic, International Journal of Foundations of Computer Science 13 (2002), pp. 315–340.
Di Pierro, A., C. Hankin and H. Wiklicky, Quantitative relations and approximate process equivalences, in: R. Amadio and D. Lugiez, editors, Proceedings of CONCUR’03, Lecture Notes in Computer Science 2761 (2003), pp. 508–522.
Di Pierro, A., C. Hankin and H. Wiklicky, Measuring the confinement of probabilistic systems, Theoretical Computer Science 340 (2005), pp. 3–56.
Di Pierro, A., C. Hankin and H. Wiklicky, Abstract interpretation for worst and average case analysis, in: T. Reps, M. Sagiv and J. Bauer, editors, Program Analysis and Compilation, Theory and Practice: Essays dedicated to Reinhard Wilhelm, LNCS 4444, Springer-Verlag, 2007 pp. 160–174.
Di Pierro, A. and H. Wiklicky, Concurrent Constraint Programming: Towards Probabilistic Abstract Interpretation, in: Proceedings of PPDP’00 (2000), pp. 127–138.
Di Pierro, A. and H. Wiklicky, Measuring the precision of abstract interpretations, in: Proceedings of LOPSTR’00, Lecture Notes in Computer Science 2042 (2001), pp. 147–164.
Eaton, J., “Gnu Octave Manual,” www.octave.org, 2002.
Jonsson, B., W. Yi and K. Larsen, “Probabilistic Extensions of Process Algebras,” Elsevier Science, Amsterdam, 2001 pp. 685–710, see [2].
Kozen, D., Semantics for probabilistic programs, Journal of Computer and System Sciences 22 (1981),
pp. 328–350.
Monniaux, D., Abstract interpretation of probabilistic semantics, in: Proceedings of SAS’00, Lecture Notes in Computer Science 1824 (2000), pp. 322–339.
Nielson, F., H. R. Nielson and C. Hankin, “Principles of Program Analysis,” Springer Verlag, Berlin – Heidelberg, 1999.
Plotkin, G., A structural approach to operational semantics, Journal of Logic and Algebraic Programming 60-61 (2004), pp. 17–139.
Roman, S., “Advanced Linear Algebra,” Graduate Texts in Mathematics 135, Springer Verlag, 2005, second edition.
Tijms, H., “Stochastic Models – An Algorithmic Approach,” John Wiley & Sons, Chichester, 1994.
van Glabbeek, R., S. Smolka and B. Steffen, Reactive, generative and stratified models of probabilistic processes, Information and Computation 121 (1995), pp. 59–80.
