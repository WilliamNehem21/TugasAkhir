EURO Journal on Computational Optimization 9 (2021) 100014

		




Twenty years of continuous multiobjective optimization in the twenty-first century
Gabriele Eichfelder∗
Institute of Mathematics, TU Ilmenau, Po 10 05 65, 98684 Ilmenau, Germany


a r t i c l e	i n f o	a b s t r a c t

	

Keywords:
multiobjective optimization scalarization
non-convex optimization bilevel optimization robust optimization
The survey highlights some of the research topics which have attracted attention in the last two decades within the area of mathematical optimization of multiple objective functions. We give insights into topics where a huge progress can be seen within the last years. We give short introductions to the specific sub-fields as well as some selected references for further reading. Primarily, the survey covers the progress in the development of algorithms. In particular, we discuss publicly available solvers and approaches for new problem classes such as non-convex and mixed integer problems. Moreover, bilevel optimization problems and the handling of uncertainties by robust approaches and their relation to set optimization are presented. In addition, we discuss why numerical approaches which do not use scalarization techniques are of interest.





Introduction and basic recent literature

Multiobjective optimization, i.e., the optimization of multiple objec- tives at the same time, has been an active area of research since the early works of Edgeworth and Pareto. Edgeworth introduced an opti- mality notion for such problems in his book in 1881 (Edgeworth, 1881) and Pareto in his book on political economy in 1906 (Pareto, 1971). In mathematics, this branch of optimization started with a paper by Kuhn and Tucker (Kuhn and Tucker, 1951). Since about the end of the 1960s, research has intensively been made in this area. For a short historical overview see Eichfelder and Jahn (2016).
optimization: if one collects multiple, for instance 𝑚, scalar-valued ob- Multiobjective optimization can be seen as a special case of vector
jective functions in a vector, then one can consider the objective function of the optimization problem to be a vector-valued map. The image space
is then the 𝑚-dimensional linear space of real numbers. Moreover, one
needs a partial ordering in the image space which is often assumed to be
the componentwise —called natural— ordering. If one allows more gen- eral linear spaces as image space of the vector-valued objective function of the optimization problem, then the name vector optimization is more suitable for these problems. For a good introduction to optimization with vector-valued maps as well as many results on a characterization of the optimal solutions of such problems we refer to the book by Jahn (2011). In this paper, we focus on multiobjective optimization problems, i.e., problems with a finite dimensional image space. Moreover, we concen- trate on continuous problems (including mixed integer problems). Also in the field of discrete multiobjective optimization many important new

∗ Corresponding author.
E-mail address: gabriele.eichfelder@tu-ilmenau.de
results, algorithms, and applications have been proposed in the last two decades, but we do not try to cover those. We give examples of topics of interest and progress in the field of continuous multiobjective optimiza- tion in the last twenty years. We select some topics, which can, of course, only be a personal view on this field. The literature on multiobjective optimization is exploding, and such a survey cannot cover all aspects of interest. Instead, we hope that this selection raises the reader’s inter- est to explore the presented or other modern aspects of multiobjective optimization further.
Before we start to discuss some specific topics, we would like to highlight some of the most fundamental books which have appeared in the last twenty years on the topic. We already mentioned the book by Jahn (2011) with the first edition from 2004 and the second edition from 2011. The book discusses vector optimization in a general setting, and applications to vector approximation, cooperative game theory, and multiobjective optimization are described. The theory is extended in the second edition by a chapter on set optimization, i.e., on optimization problems with a set-valued objective function, which will be a topic of our Section 8. Also in a general setting, Göpfert, Riahi, Tammer and Zǎlinescu study, in their book (Göpfert et al., 2003) from 2003, solution concepts, optimality conditions, scalarizations, duality, and applications of vector optimization problems. A book authored by Ehrgott (2005), which is written in the final dimensional setting and is a perfect intro- duction to the topic, appeared in 2000 and has now its second edition in 2005. The book provides the necessary mathematical foundation of multiobjective optimization and covers topics such as the most famous scalarization techniques or approaches for linear multiobjective prob- lems. In addition, combinatorial problems as the knapsack problem are


https://doi.org/10.1016/j.ejco.2021.100014
Received 16 December 2020; Received in revised form 20 August 2021; Accepted 11 October 2021
2192-4406/© 2021 The Author. Published by Elsevier Ltd on behalf of Association of European Operational Research Societies (EURO). This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/)



discussed. More recently, for linear multiobjective optimization prob- lems there is a book by Luc, 2016 . Another book written in the finite dimensional setting, but which focuses on nonlinear problems and addi- tionally covers interactive methods for decision making in multiobjec- tive optimization, is due to Miettinen, 1999.
In the last two decades, next to the above introductory books, sev- eral valuable books on more specific topics in vector optimization ap- peared. Many of those are part of the book series entitled Vector Op- timization with the series editor Jahn, which started in 2008 with the first book about adaptive scalarization methods (Eichfelder, 2008). The series contains publications in various sub-fields of optimization with vector-valued objective functions. So far, ten books have been published
handling uncertainties. Finally, we conclude the survey in Section 8 with some comments on other ordering structures, which appeared more fre- quently in the literature in the last decade, and on developments in set optimization.

Basic notation

In the following, we briefly define the basic concepts, mainly for having a unified notation. For a more thorough introduction we refer to one of the books mentioned in the previous section. In multiobjective optimization one studies optimization problems formally defined by
min 𝑓 (𝑥) = (𝑓 (𝑥), … , 𝑓 (𝑥))⊤

new solution concept for vector optimization problems is introduced which allows to define an appropriate concept of the infimum with a high degree of analogy to corresponding results in single-objective (i.e., scalar-valued) optimization.
Moreover, we would like to specifically mention the book on re- cent developments in vector optimization which is edited by Ansari and Yao (2012) and which contains a wide range of topics such as duality results, new ordering structures, specific classes of multiobjective opti- mization problems such as fractional problems, and vector variational principles.
Another collection of recent results on multiobjective optimization, and most of all the related field of multicriteria decision making, is Greco et al. (2016) which is edited by Greco, Ehrgott and Figueira and appeared in 2016 (after a first edition in 2005). The two volumes in- clude surveys on the foundations and on many areas of current research as well as new applications. The main focus of the book is on multicrite- ria decision making, as the title says, which covers the aspect of finally
subject to the constraint
𝑥 ∈ 𝑀, 
with a vector-valued objective function 𝑓 ∶ ℝ𝑛 → ℝ𝑚 (𝑚, 𝑛 ∈ ℕ, 𝑚 ≥ 2) and a nonempty set of feasible points 𝑀 ⊆ ℝ𝑛.
The most common optimality notion is the one of eﬃciency, also called Pareto optimality or Edgeworth-Pareto optimality.
Definition 2.1. A point 𝑥̄ ∈ 𝑀 is called an eﬃcient solution or eﬃcient point or eﬃcient for (MOP), and 𝑓 (𝑥̄) is called a nondominated point, if there is no 𝑥 ∈ 𝑀 with
𝑓𝑖 (𝑥)	≤	𝑓𝑖 (𝑥̄)	for all 𝑖 ∈ {1, … , 𝑚} and
𝑓𝑗 (𝑥)	<	𝑓𝑗 (𝑥̄)	for at least one 𝑗 ∈ {1, … , 𝑚}.
By using a set notation one obtains that 𝑥̄ ∈ 𝑀 is eﬃcient for (MOP)
if and only if
{𝑓 (𝑥̄)} − ℝ𝑚 ∩ 𝑓 (𝑀 ) = {𝑓 (𝑥̄)},
with  𝑓 (𝑀 ) = {𝑓 (𝑥) ∈ ℝ𝑚 ∣ 𝑥 ∈ 𝑀 }  and  ℝ𝑚 = {𝑦 ∈ ℝ𝑚 ∣ 𝑦 ≥ 0,  𝑖 =

selecting one of the optimal solutions of a multiobjective optimization	1
+	𝑖

problem in a decision making process. Such a selection is for instance reached by interactive methods or preference modeling. However, de- cision making is not the topic of this survey. Instead, we aim at finding all optimal solutions of the multiobjective optimization problem, which are, from a mathematical point of view, non-comparable.
The book Branke et al., 2008 edited by Branke, Deb, Miettinen, and Slowinski from 2008 covers interactive methods and evolutionary al- gorithms to solve multiobjective optimization problems. In the area of evolutionary approaches an intense progress has be seen since the fun- damental book by Deb (2001), but these approaches will also not be covered within this survey.
Finally, we would like to point out that since 2004 there has been a series of Dagstuhl meetings, with the first meeting organized by Branke, Deb, Miettinen, and Steuer, which bring together people from multi- criteria decision making, multiobjective optimization, and evolutionary algorithms. Until now, seven meetings have taken place which all cov- ered topics of recent research interest as Scalability in Multiobjective Optimization (Dag, 0000), Personalized Multiobjective Optimization, or Hybrid and Robust Approaches. A link to the websites of the ear- lier meetings can be found on the website of the most recent meeting (Dag, 0000).
In Section 2, we briefly recall the basic concepts of multiobjective optimization for clarifying our notation. In Section 3, we present some useful and publicly available solvers for certain classes of multiobjective optimization problems. Scalarization is a widely used tool in multiob- jective optimization and we recall the basics in Section 4. We discuss why the study of non-scalarization based methods is of interest. An im- portant class of multiobjective optimization problems for which non- scalarization based methods might be more suitable are those of non- convex problems, which includes, in particular, mixed integer multiob- jective optimization. These problem classes are the topic of Section 5. A further challenging problem class is bilevel optimization. We discuss the developments in this field in Section 6. These problems have a re- lation to set-valued optimization. This is also the case for the problems discussed in Section 7: robust multiobjective optimization problems for
, … , 𝑚}. The set of all eﬃcient solutions is named eﬃcient set, and the set of all nondominated points is the nondominated set for (MOP) denoted by  ⊆ 𝑓 (𝑀 ).
For an arbitrary set 𝑆 ⊆ ℝ𝑚, we say that a point 𝑦1 ∈ 𝑆 dominates
𝑦2 ∈ 𝑆, and that 𝑦2 is dominated by 𝑦1, if 𝑦1 ≠ 𝑦2 and 𝑦1 ≤ 𝑦2 for all

𝑖 ∈ {1, … , 𝑚}. The set of all not dominated points within 𝑆 is called the set of nondominated points or the nondominated set of 𝑆.
When one speaks of solving the problem (MOP) one typically aims
points, i.e., the nondominated set for (MOP). If the set 𝑀 is not finite, to find all eﬃcient solutions, i.e., the eﬃcient set, or all nondominated
in general an infinite number of eﬃcient solutions exist. Especially for nonlinear problems, in general not all eﬃcient points can be calculated with numerical methods. In that case one aims to find an approxima- tion (or representation) of the eﬃcient (or nondominated) set or to find a covering (or enclosure). For a definition and a classification of such concepts we refer to the survey Ruzika and Wiecek, 2005. Within such methods, often ineﬃcient solutions are generated which are just weakly eﬃcient solutions. For that reason we briefly recall their definition.
Definition 2.2. A point 𝑥̄ ∈ 𝑀 is called a weakly eﬃcient point or weakly eﬃcient solution or weakly eﬃcient for (MOP), and 𝑓 (𝑥̄) is called a weakly nondominated point, if there is no 𝑥 ∈ 𝑀 with
𝑓𝑖 (𝑥)  <  𝑓𝑖 (𝑥̄)   for all 𝑖 ∈ {1, … , 𝑚}.
By using a set notation, one obtains that 𝑥̄ ∈ 𝑀 is weakly eﬃcient for (MOP) if and only if
{𝑓 (𝑥̄)} − int(ℝ𝑚) ∩ 𝑓 (𝑀 ) = ∅.
With int(ℝ𝑚) = {𝑦 ∈ ℝ𝑚 ∣ 𝑦𝑖 > 0, 𝑖 = 1, … , 𝑚} we denote the interior of the cone ℝ𝑚. Any eﬃcient point for (MOP) is weakly eﬃcient for (MOP).
From an application point of view one would in general only be inter- ested in a weakly eﬃcient point which is at the same time eﬃcient, as otherwise one could improve the value of one objective function without deteriorating the others. Still, this concept is important for characteriz- ing the output of numerical algorithms or for theoretical results such as optimality conditions.






Fig. 1. The nondominated set of a triobjective optimization problem as a re- sult of a run of Bensolve for a multiobjective optimization problem with three objective functions (provided by D. Dörfler, FSU Jena).


Publicly available solvers
A huge progress can be seen in solving problems of the form (MOP) numerically, and many algorithms for specific classes of problems as
non-convex problems are discussed in the literature. We will look into details of some of them later. However, only for a few classes of prob- lems, implementations of solvers exist which are freely available. The International Society on Multiple Criteria Decision Making (MCDM) lists (MCD, 0000) several freely available numerical algorithms for decision making and for multiobjective optimization. In the following we present some of them.
Linear multiobjective optimization problems with many vari- ables and several objective functions can be handled with Bensolve (Bensolve). That is, problems of the form
(𝑐1)⊤𝑥
min	⋮
𝑥∈ℝ𝑛  𝑐𝑚 )⊤𝑥
s.t.	𝑏1	≤	𝐵𝑥	≤	𝑏2
𝓁𝑏	≤	𝑥	≤	𝑢𝑏
with  appropriate  vectors  𝑏1, 𝑏2,  a  matrix  𝐵,  and  vectors
𝑐1, … , 𝑐𝑚, 𝓁𝑏, 𝑢𝑏 ∈ ℝ𝑛 can be solved, i.e., all nondominated points can be calculated. For two vectors 𝑤, 𝑧 ∈ ℝ𝑘, the inequality 𝑤 ≤ 𝑧 has to be read componentwise as 𝑤𝑖 ≤ 𝑧𝑖 , 𝑖 = 1, … , 𝑘. The algorithm is
based on what is known as Bensons algorithm and its extensions, see Löhne and Weißing, 2017. According to Dörfler et al. (2021) some of the basic ideas can be traced back to earlier works in different areas of research, as in global optimization or in convex optimization. The algorithm available on Bensolve was first implemented in Matlab, and
problems with 𝑚 = 2 and 𝑚 = 3, visualizations of the nondominated set from version 2.0.0 it is written in the C programming language. For
are generated, see Fig. 1 for such an output.
For this class of problems another solver is given on GitHub by Csirmaz (2021); Cr2 (0000). The solver also makes use of Benson’s basic idea and the author states that the algorithm is specifically appropriate for problems with ten or more objective functions.
With FLO (FLO, 0000) there is a Matlab based implementation by Günther of a deterministic solver for planar multiobjective location problems, based on, among others, Alzorba et al. (2015) by Alzorba, Günther, Popovici, and Tammer. One of the problem classes which can
which consist in minimizing the distances between a new facility 𝑥 ∈ ℝ2 be solved with FLO are multiobjective single-facility location problems and all given attraction facilities 𝑎1, … , 𝑎𝑚 ∈ ℝ2 simultaneously. For the Manhattan metric one obtains a problem formulation with 𝑚 objective
functions of the type
but other metrics can also be used. The solver determines all nondomi- nated points.
For more general constrained nonlinear multiobjective optimization problems with a small number of objective functions, approximations
of the set of nondominated points for (MOP) can be generated with the
which is based on the algorithm from Eichfelder, 2008. For 𝑚 = 2, nearly Matlab based implementation ASMO (ASM, 0000) provided by Warnow,
equidistant approximations of the set of nondominated points can be generated for smooth nonlinear problems, i.e., for problems with smooth objective and constraint functions. The method uses sensitivity informa- tion and a scalarization approach. As the scalarization problems have to be solved to global optimality, and as a local solver is used within the implementation, the algorithm is in particular suited for convex opti- mization problems.
Another solver for constrained nonlinear multiobjective optimiza- tion problems named MOSQP is provided by Vaz based on his joint work with Fliege (Fliege and Vaz, 2016). The idea behind the algorithm is the extension of single-objective sequential quadratic programming to the multiobjective setting. The extension is done in such a way that not just a single eﬃcient solution is calculated but an approximation of the non- dominated set. A Matlab based implementation is provided (MOS, 0000) together with results of comparisons on benchmark problems with, for instance, the evolutionary algorithm NSGA-II. One can contact the au- thors for sets of benchmark problems.
MOSQP makes use of derivative information of the objective func- tions and constraints to build quadratic models. Another well-known solver for constrained nonlinear multiobjective optimization problems, named Direct MultiSearch (DMS), aims to avoid exactly this. An al- gorithmic implementation with Matlab can be obtained by sending an email (DMS, 0000) and is freely available for academic use. The algo- rithm is based on Custodio et al. (2011) by Custodio, Madeira, Vaz and Vicente, which extends the idea of multisearch developed for single- objective optimization to multiobjective optimization.
In addition, there is a solver for linear or integer linear (but not mixed integer) multiobjective optimization named PolySCIP which is available on Pol (0000) by Schenker and Strunk, with a last update from 2017. The implementation is based on Borndörfer et al. (2016) by Borndör- fer, Schenker, Skutella, and Strunk. PolySCIP only finds the so-called supported solutions. Supported solutions are those which can be found by the weighted sum scalarization, see Subsection 5.3. For non-convex problems as integer problems, the supported solutions in general only form a subset of the set of eﬃcient solutions.
A solver for determining an enclosure of the nondominated set of a multiobjective mixed integer convex optimization problem based on a branch-and-bound method can be downloaded from MOM (0000) and is based on De Santis et al., 2020 by De Santis, Eichfelder, Niebling and Rocktäschel. We discuss this problem class and the approach from De Santis et al., 2020 in more detail in Section 5. Moreover, in Section 6 we present multiobjective bilevel optimization problems, for which an implementation of the algorithm from Eichfelder (2010), which determines an approximation of the nondominated set, can be downloaded from MOB (0000).
Finally, for strictly convex quadratic multiobjective problems with linear constraints, reformulations as parametric linear complemen- tarity problems can be used, for which at least rudimentary imple- mentations exist as detailed by Jayasekara, Adelgren and Wiecek in Jayasekara et al. (2020).

Direct approaches avoiding scalarization

A widely used approach to solve multiobjective optimization prob- lems are scalarizations. Scalarization means that one formulates a parameter-dependent single-objective optimization problem to the mul- tiobjective optimization problem. A huge progress can be seen for scalar- ization approaches in the last decades, for instance in procedures for

𝑓 (𝑥) = 𝑥
– 𝑎𝑖 + 𝑥 − 𝑎𝑖 ,	𝑖 = 1, … , 𝑚,
choosing the parameters for those, and in extending the techniques for

𝑖	| 1
1|  | 2	2|



a smart parameter control to problems with three or more objective functions, see for instance the recent publications Burachik et al., 2014; 2017; 2021 by Burachik, Kaya and Rizvi.
However, in the last decade an increasing interest in numerical meth- ods without such scalarizations can be observed. While some of these methods still use scalarizations on subproblems, as for instance for find- ing new candidate points for weakly nondominated points in subregions of the image space, they avoid to first scalarize the overall problem and then to apply standard solvers from single-objective optimization. In this section, we discuss such approaches and collect arguments against a scalarization. But first we recall the basic ideas of scalarization appo- raches.

Basics on scalarizations

objective optimization problem 𝑃 (𝜔) for a set of parameters Ω to the For a scalarization one formulates a parameter-dependent single- multiobjective optimization problem (MOP) and then aims
to obtain an eﬃcient solution for (MOP) as optimal solution of 𝑃 (𝜔)
for any choice of the parameter 𝜔 ∈ Ω, and
to find all eﬃcient solutions for (MOP) as optimal solutions of 𝑃 (𝜔)
by varying the parameter 𝜔 ∈ Ω appropriately.
Well known scalarization approaches are the weighted sum or the
𝜀-constraint scalarization. For the weighted sum method one solves
∑𝑚
The above mentioned gap for the weighted sum method between (i) and (ii) already shows a drawback of scalarization approaches. Never- theless, they are widely used in multiobjective optimization. The advan-
tage is that for the scalarization, i.e., for the problem 𝑃 (𝜔), one can use
all theoretical results and numerical algorithms which have been devel-
of (MOP) are linear and the set 𝑀 is defined by linear constraints and oped for the single-objective case. For instance, if the objective functions some of the variables 𝑥𝑗 , 𝑗 ∈ 𝐽 ⊆ {1, … , 𝑛} are additionally assumed to
be integer, then the weighted sum approach reduces the problem to a linear mixed integer single-objective optimization problem for which powerful solvers are available. As a drawback, one has to check care- fully whether the optimal solutions of the family of scalarization prob-
lems {𝑃 (𝜔) ∣ 𝜔 ∈ Ω} indeed represent the eﬃcient set. For instance, in
the mixed-integer case a weighted sum approach is not suitable due
to the intrinsic non-convexity of the problem. With the weighted sum method one is only able to find the supported solutions, which is s topic of Subsection 5.3.
Only in some rare cases one is satisfied with finding only one eﬃ- cient solution, i.e., with aim (i) from above. An example are problems which are so challenging that one has to accept the fact that already calculating just one eﬃcient solution has to be enough. Such a situa- tion occurs if multiple expensive simulation based objective functions appear, cf. Prinz et al., 2021 by Prinz, Thomann, Eichfelder, Boeck and Schumacher.
However, even then, when solving just one scalarization problem, just by choosing a certain scalarization it might happen that one gets unexpected or even unwanted eﬃcient solutions. For instance, the
weighted sum scalarization applied to a non-convex multiobjective op-

min
𝑥∈𝑀
𝑖
𝑤𝑖 𝑓𝑖 (𝑥),	𝑃WS (𝑤)
timization problem might deliver even for equal weights 𝑤𝑖 just an
individual minimizer, i.e., some 𝑥 ∈ argmin{𝑓 (𝑥) ∣ 𝑥 ∈ 𝑀 } for some

with weight set Ω = 𝑊 1 ∶= {𝑤 ∈ ℝ𝑚 ∣ ∑𝑚 𝑤 = 1} or Ω = 𝑊 2 ∶= {𝑤 ∈
𝑖
1} and
( ) = .

int(ℝ𝑚) ∣ ∑𝑚 𝑤 = 1}. The weighted-sum scalarization is named linear
+  ‖  2

scalarization, as the linear function 𝑦 ↦ 𝑤⊤𝑦 is applied to the objective
function vector 𝑓 (𝑥).
For 𝑤 ∈ 𝑊 2, any optimal solution 𝑥̄ of (𝑃WS (𝑤)) is eﬃcient for
(MOP). If 𝑤 ∈ 𝑊 1, an optimal solution of (𝑃WS (𝑤)) is only guaranteed to
(1, 0)⊤ and (0, 1)⊤ can be found, no matter which weights are chosen.
Obtaining one of these individual minimizers might be unexpected or
even not wanted, as one might believe that one gets, for equal weights
𝑤 , a solution with a balanced trade-off between the objective functions.

be weakly eﬃcient for (MOP). For these results see, for instance, Chap-
hold for (𝑃WS (𝑤)) for convex problems, i.e., if 𝑓𝑖 , 𝑖 = 1, … , 𝑚 are convex ter 3 in Ehrgott (2005). On the other hand, aim (ii) is known to only functions and the set 𝑀 is convex. Under these convexity assumptions one can find all eﬃcient solutions of (MOP) by allowing 𝑤 ∈ 𝑊 1, but
𝑖

Here, such a balanced solution would be 𝑥 = (1∕ 2, 1∕ 2)⊤. For other nonlinear scalarizations a careful choice of the parameter 𝜔 ∈ Ω can
be important. Otherwise, the scalarization problem might not be solv- able despite the original multiobjective optimization problem having eﬃcient solutions. An example for a scalarization problem which might

not if one restricts the parameter set to 𝑊 2, cf. Ehrgott (2005). Thus
there is a gap between the necessary and the suﬃcient conditions even
in the convex case.
Another well-known scalarization is the 𝜀-constraint scalarization
min {𝑓𝑚 (𝑥) ∣ 𝑓𝑖 (𝑥) ≤ 𝜀𝑖 , 𝑖 = 1, … , 𝑚 − 1, 𝑥 ∈ 𝑀 }	𝑃𝐶 (𝜀
not be solvable despite there exists an eﬃcient solution is (𝑃𝐶
a parameter 𝜀1 with 𝜀1 < inf 𝑥∈𝑀 𝑓1 (𝑥).
Directly extending single-objective methods
(𝜀)) with

with upper bounds 𝜀 ∈ Ω ⊆ ℝ𝑚−1. We pick here the objective function
𝑓𝑚 to be minimized without loss of generality. The 𝜀-constraint scalar-
ization belongs to the class of nonlinear scalarizations as it is derived
ternatively, a nonlinear separation functional on the set 𝑓 (𝑀 ). For more from solving an approximation problem with a parametric norm, or, al-
scalarization, a diﬃcult question is how to restrict the parameter set Ω, details we refer to Jahn (2011); Tammer and Weidner (2020). For this especially for the case 𝑚 ≥ 3. Moreover, any optimal solution of (𝑃𝐶 (𝜀)) is only guaranteed to be weakly eﬃcient for (MOP) unless it is unique,
Ehrgott (2005).
Many more of such scalarizations are known, and many of them can be seen as a special case of minimizing the very general Tammer-Weidner-functional, which is again related to the scalariaz- tion by Pascoletti and Serafini, c.f. Pascoletti and Serafini (1984). See Eichfelder (2009) for such relations and the recent paper (Bouza et al., 2019) by Bouza, Quintana and Tammer on a unified characterization of nonlinear scalarization functionals. Just recently, an extensive book on the Tammer-Weidner-functional appeared which presents all its useful properties and application fields, see (Tammer, Weidner, 2020) .
Thus, even if one is satisfied with only one eﬃcient solution, scalar- izations might not be the best choice. Another argument against using scalarizations is that the scalarization might destroy the structure of the optimization problem. For instance, in Thomann and Eichfelder (2019), the authors study unconstrained multiobjective optimization problems
which are called heterogeneous: one of the objective functions, e.g. 𝑓1, is
assumed to be an expensive function, i.e., each function evaluation 𝑓1(𝑥)
objective functions 𝑓2, … , 𝑓𝑚 are analytically given and easy to evalu- is very time consuming, for instance a simulation run, while the other
ate. Then one should of course make use of the heterogeneous struc- ture within an algorithm. But the weighted sum scalarization would not allow that and one would end up with only one expensive objec-
tive function 𝑤⊤𝑓 (⋅). Similarly, the 𝜀-constraint scalarization adds con-
straints and thus destroys the simplicity of having an unconstrained opti-
mization problem. Thus, in Thomann and Eichfelder (2019) the authors develop a tailored trust-region method to make use of the heterogeneity. Non-scalarization-based approaches might also have advantages for problems without such a heterogeneous structure but with purely ex- pensive functions. In Ryu and Kim (2014), and in Prinz et al., 2021, the authors work with model functions for expensive objective functions




Prinz et al., 2021 guarantees to find, from some starting point 𝑥0, an and with model-based trust-region methods. The approach proposed in eﬃcient solution 𝑥̄ for (MOP), or at least a point satisfying the neces-
this point 𝑥̄ satisfies 𝑓𝑖 (𝑥̄) ≤ 𝑓𝑖 (𝑥0) for all 𝑖 = 1, … , 𝑚. A good starting sary optimality conditions for that, see Definition 4.1. What is more,
point, i.e., a meaningful guess for a good solution, is often available in applications. The important aspect is that it is guaranteed that in each
successful iteration the new iterate 𝑥𝑘+1 satisfies 𝑓𝑖 (𝑥𝑘+1) < 𝑓𝑖 (𝑥𝑘) for
all 𝑖 = 1, … , 𝑚. As a consequence, even in case the algorithm has to be
stopped during the run as the evaluations of the functions are too costly,
one has already a guaranteed improvement.
Many well-known techniques from single-objective optimization as descent methods, Newton’s method, trust-region methods, and SQP methods for constrained optimization problems have successfully been extended to solve multiobjective optimization problems without an in- termediate scalarization step. For instance, one of the earlier papers is on a steepest descent method for multiobjective optimization, see Drummond and Svaiter (2005). A year later, Fliege has proposed an interior point method based approach, see Fliege (2006). Fliege, Drum- mond and Svaiter have extended Newton’smethod in Fliege et al. (2009).
Lemma 4.2. Let 𝑓𝑖 ∶ ℝ𝑛 → ℝ, 𝑖 = 1, … , 𝑚, be continuously diﬀerentiable functions and 𝑀 = ℝ𝑛. For the function 𝜔 ∶ ℝ𝑛 → ℝ defined by
𝜔(𝑥) ∶= − min  max ∇𝑓𝑖 (𝑥)⊤𝑑	(1)
𝑑 2 ≤1 𝑖=1,…,𝑚
the following statements hold.
The map 𝑥 ↦ 𝜔(𝑥) is continuous.
It holds 𝜔(𝑥) ≥ 0 for all 𝑥 ∈ ℝ𝑛.
A point 𝑥 ∈ ℝ𝑛 is Pareto critical for (MOP) if and only if it holds 𝜔(𝑥) = 0
Recall that a descent direction for (MOP) at a point 𝑥 is a vector
𝑑𝜔 ∈ ℝ𝑛 such that there exists a scalar 𝑡0 > 0 with 𝑓𝑖 (𝑥 + 𝑡 𝑑𝜔 ) < 𝑓𝑖 (𝑥) for all 𝑡 ∈ 0, 𝑡0 and for all 𝑖 ∈ {1, … , 𝑚}. A suﬃcient condition for such a descent direction is ∇𝑓𝑖 (𝑥)⊤𝑑𝜔 < 0 for all 𝑖 = 1, … , 𝑚, cf. (Thomann and Eichfelder, 2019, Lemma 2.5). If 𝑥 ∈ ℝ𝑛 and 𝑑𝜔 ∈ ℝ𝑛 denotes a solution of the optimization problem in (1) defining 𝜔(𝑥) with 𝜔(𝑥) > 0, then 𝑑𝜔 is a descent direction for (MOP) at the point 𝑥. As a consequence, the point 𝑥 is not Pareto critical for (MOP).
Similarly, for the Newton method, Fliege, Drummond and Svaiter propose in Fliege et al. (2009) to solve the problem

Moreover, we have already mentioned in Section 3 an extension of se-
min
max  ∇𝑓 (𝑥)⊤𝑑 + 1 𝑑⊤∇2 𝑓 (𝑥)𝑑

quential quadratic programming , see Fliege and Vaz (2016). In addi-
𝑑∈ℝ𝑛
𝑖=1,…,𝑚	𝑖
2	𝑖

tion, just recently a descent method for non-smooth but Lipschitz mul-
tiobjective optimization problems has been proposed by Gebken and Peitz (2021), which shows the ongoing interest in such methods.
As the basic techniques are all methods from convex, i.e., non-global single-objective optimization, which only guarantee to find locally op- timal solutions, the extensions to multiple objectives only allow to find
a point 𝑥̄ ∈ ℝ𝑛 is called a locally (weakly) eﬃcient solution for (MOP) if locally eﬃcient solutions — unless the problems are convex. Recall that there exists a neighborhood 𝑈 ⊆ ℝ𝑛 of 𝑥̄ such that 𝑥̄ is (weakly) eﬃcient for (MOP) restricted to 𝑀 ∩ 𝑈 . In Section 5 we discuss methods which
guarantee to find globally eﬃcient solutions for non-convex problems.



Descent directions and proximity measures

Most of the extensions above (except the SQP extension) are for un- constrained optimization problems. In single-objective optimization one aims to find a stationary point, i.e., a point at which the gradient van- ishes. The well-known concept of stationarity from single-objective op- timization transfers as defined next, cf. Smale (1975). The transferred stationarity concept is used in a work on steepest descent methods for multiobjective optimization by Fliege and Svaiter (2000).

Definition 4.1. Let 𝑓𝑖 ∶ ℝ𝑛 → ℝ, 𝑖 = 1, … , 𝑚, be continuously differen- tiable and 𝑀 = ℝ𝑛. A point 𝑥̄ ∈ ℝ𝑛 is called Pareto critical for (MOP) if for every 𝑑 ∈ ℝ𝑛 there exists an index 𝑗 ∈ {1, 2, … , 𝑚} such that
∇𝑓𝑗 (𝑥̄)⊤𝑑 ≥ 0 holds.
Obviously, for 𝑚 = 1 the definition reduces to ∇𝑓 (𝑥̄) = 0. Pareto crit-
example, Fliege and Svaiter (2000): If 𝑥̄ ∈ ℝ𝑛 is locally weakly eﬃcient icality is indeed a necessary condition for local weak eﬃciency, see, for for (MOP), then it is Pareto critical for (MOP).
However, in contrast to the single-objective case, even if each 𝑓𝑖 ∶
ℝ𝑛 → ℝ, 𝑖 = 1, … , 𝑚 is convex, Pareto criticality does not imply eﬃ- ciency for (MOP) with 𝑀 = ℝ𝑛. To see this take the simple example
from the work by Fliege, Drummond and Svaiter, Fliege et al. (2009):
𝑓 ∶ ℝ → ℝ2 with 𝑓 (𝑥) = (1, 𝑥)⊤. Then all 𝑥 ∈ ℝ are Pareto critical but no
𝑥 is eﬃcient. Instead one needs strict convexity of all functions 𝑓𝑖 such that Pareto criticality implies eﬃciency for (MOP).
To prove convergence results and to measure somehow the distance
for obtaining a direction for calculating the next iterate. Here, ∇2 𝑓𝑖 (𝑥)
denotes the Hessian of 𝑓𝑖 in 𝑥. For any 𝑥 ∈ ℝ𝑛, the optimal solution 𝑑 is
a Newton direction.
In algorithms for single-objective unconstrained optimization, the norm of the gradient of the objective function at some iteration point
𝑥𝑘 is often used as stopping criterion for the algorithm. That is, if the
norm of the gradient at some 𝑥𝑘 is less than some positive constant 𝜀, the iteration point 𝑥𝑘 is accepted as ”almost stationary”. On the other hand,
from having found a stationary point. Similarly, the value 𝜔(𝑥) for some large values of the norm of the gradient are interpreted as being far away
𝑥 ∈ ℝ𝑛 as defined in Lemma 4.2 can be interpreted as a measure for how close some point 𝑥 is to Pareto criticality. The function 𝜔(𝑥) from
Lemma 4.2 is only suitable for unconstrained optimization problems. Similar concepts for constrained optimization problems are discussed in the literature. They are for instance used by Deb, Abouhawwash and Dutta in Deb et al. (2015) for evolutionary algorithms as an additional selection or stopping criterion: based on the selection criterion, candi- dates from the current set of iterates (named population) are selected for proceeding to the next iteration. An important property of such func-
tions as 𝜔 from Lemma 4.2 is that they are continuous, as discussed in
Deb et al. (2015). We recall the definition of a proximity measure ac-
cording to Eichfelder and Warnow (2021b):

Definition 4.3. A function 𝜔 ∶ ℝ𝑛 → ℝ is called a proximity measure for (MOP) if for every eﬃcient solution 𝑥̄ ∈ 𝑀 for (MOP) in which the Abadie constraint qualification (CQ) holds and every sequence (𝑥𝑘)ℕ ⊆ ℝ𝑛 with lim𝑘→∞ 𝑥𝑘 = 𝑥̄ the following three properties are satisfied:
(PM1) 𝜔(𝑥) ≥ 0 for all 𝑥 ∈ ℝ𝑛,
(PM2) 𝜔(𝑥̄) = 0,
(PM3) lim 𝜔(𝑥𝑘) = 𝜔(𝑥̄).
𝑘→∞
Next to proposals for proximity measures based on a scalarize-first approach, measures which directly use the multiobjective Karush-Kuhn- Tucker (KKT) optimality conditions for constrained multiobjective op-
sume in the following that the feasible set of (MOP) is nonempty and timization problems have been developed. For presenting those, we as-
defined by
𝑀 ∶= {𝑥 ∈ ℝ𝑛 ∣ 𝑔𝑗 (𝑥) ≤ 0, 𝑗 = 1, … , 𝑝}	(2)
with 𝑔𝑗 ∶ ℝ𝑛 → ℝ, 𝑗 = 1, … , 𝑝. We give KKT conditions using the Abadie

to Pareto criticality, similar to ‖
‖2 in the single-objective case, the
CQ. Such necessary optimality conditions can be found in the basic lit-





Theorem 4.4. Let 𝑓𝑖 ∶ ℝ𝑛 → ℝ and 𝑔𝑗 ∶ ℝ𝑛 → ℝ be continuously diﬀeren- tiable functions for all 𝑖 = 1, … , 𝑚 and all 𝑗 = 1, … , 𝑝 and let 𝑀 be given as in (2). Let 𝑥̄ ∈ 𝑀 be a weakly efficient solution for (MOP) and let the Abadie
CQ hold at 𝑥̄. Then there exist Lagrange multipliers 𝜂̄ ∈ ℝ𝑚 and 𝜆̄ ∈ ℝ𝑝 such
of all boxes is halved in each iteration. See Fig. 6 for such enclosures. We discuss concepts of box-coverages and enclosures in the next section on non-convex problems.

+
that (𝑥̄, 𝜂̄, 𝜆̄) is a KKT point, i.e., such that it holds
+
Non-convex multiobjective optimization

𝑚
𝑖=1
𝜂̄𝑖 ∇𝑓𝑖 (𝑥̄) +
𝑝
𝑗=1
𝜆̄𝑗 ∇𝑔𝑗 (𝑥̄) = 0,
Solving non-convex single-objective optimization problems to global optimality or at least with some guaranteed proximity to the optimal

𝑔(𝑥̄) ≤ 0,
𝑚
𝑖=1
𝜆̄⊤𝑔(𝑥̄) = 0,
𝜂̄𝑖 = 1.
value is known to be a challenge in mathematical optimization. This challenge of finding globally optimal solutions transfers to non-convex multiobjective optimization. The source of non-convexity can be that

Note that no convexity assumption is required for the optimality con- ditions in Theorem 4.4. They have nothing to do with a weighted sum scalarization. In the following, we present a proximity measure, intro- duced in Eichfelder and Warnow (2021b), which is easy to calculate as it requires to solve only a linear optimization problem.
Theorem 4.5. Let 𝑓𝑖 ∶ ℝ𝑛 → ℝ and 𝑔𝑗 ∶ ℝ𝑛 → ℝ be continuously diﬀeren- tiable functions for all 𝑖 = 1, … , 𝑚 and all 𝑗 = 1, … , 𝑝 and let 𝑀 be given as in (2). The function 𝜔𝑠 ∶ ℝ𝑛 → ℝ defined by
either one or several of the objective functions are not convex, or that the feasible set is not convex. A reason for the latter can be that some of the variables are required to take only integer values. Then one speaks of mixed integer multiobjective optimization.
For non-convex problems a possible approach are scalarization methods, see, for instance, the method by Burachik, Kaya and Rizvi, Burachik et al. (2021). However, then the parameter dependent single- objective subproblems are non-convex and have to be solved to global optimality. Solving single-objective subproblems to global optimality

⎧	‖ ∑𝑚
𝜂 ∇𝑓 (𝑥) +
∑𝑝
𝜆 ∇𝑔 (𝑥)
≤ 𝜀,⎫
has to be done iteratively for many choices of the parameters of the

𝑖	𝑖
𝑖=1
𝑝
𝑗	𝑗
𝑗=1
𝑚
scalarization. Let us assume that the single-objective subproblems are
solved by a branch-and-bound method, which is often used for non-

𝜔𝑠 (𝑥) ∶= min ⎪⎨𝜀 ∈ ℝ+ |	∑ 𝜆𝑗 𝑔𝑗 (𝑥) ≥ −𝜀, ∑ 𝜂𝑖 = 1,	⎪⎬
convex optimization. Then, the steps of the branch-and-bound method

𝑗=1
⎪
𝑖=1
⎪
have to be repeated for each single-objective subproblem, despite the

𝑔𝑗 (𝑥) ≤ 𝜀 for all 𝑗 ∈ {1, … , 𝑝},
⎪⎩	𝜂 ∈ ℝ𝑚, 𝜆 ∈ ℝ𝑝	⎪⎭
fact that they are typically quite similar. Instead, a better approach

for all 𝑥 ∈ ℝ𝑛 is a proximity measure. Moreover, for any 𝑥 ∈ ℝ𝑛 it holds
𝜔𝑠 (𝑥) = 0 if and only if there exist 𝜂 ∈ ℝ𝑚 and 𝜆 ∈ ℝ𝑝 such that (𝑥, 𝜂, 𝜆) is
tiobjective optimization problem as a whole and without the detour of

a KKT point.
+	+	scalarization. We concentrate on such direct methods in this section, as
they have experienced a significant progress in the last two decades.

Proximity measures can be used within numerical procedures to ob- tain a termination criterion, or, for evolutionary algorithms, as a selec- tion criterion. Note that these proximity measures are based on local
For the following, we concentrate on multiobjective optimization problems of the form
min 𝑓 (𝑥) = (𝑓1 (𝑥), … , 𝑓 (𝑥))⊤

should either be used for convex optimization problems, or it should be taken into account that they do not measure a proximity to a globally
s.t. 𝑔𝑗 (𝑥) ≤ 0, 𝑗 = 1, … , 𝑝,
𝑥𝑙 ∈ ℤ, 𝑙 ∈ 𝐵
(MOP-NC)

eﬃcient solution.

4.4. Guarantee of quality

Numerical algorithms in multiobjective optimization in general aim to find a representation or approximation of the eﬃcient or the nondom- inated set. Representations and approximations often consist of a finite number of points, which are either a subset of the nondominated set, i.e., representations, or are close to the nondominated set, i.e., approxi- mations. Moreover, approximations can be based on linear or quadratic functions, and can be a part of a sandwich-type approximation which constructs inner and outer approximations for the nondominated set. An example for that are enclosures as discussed in Subsection 5.4. A classifi- cation of such concepts can be found in Ruzika and Wiecek, 2005. There are non-scalarization based solution approaches which allow to guaran- tee a certain quality of the generated approximation, like an enclosure with a certain width, after a finite number of iterations.
In the next section on non-convex multiobjective optimization we give more details on such enclosures. Some of the mentioned proce- dures give further examples for advantages of non-scalarization based methods. Also for convex multiobjective optimization such enclosure- producing methods exist, as Benson’s algorithm extended to convex problems by Ehrgott, Shao and Schöbel in Ehrgott et al. (2011), and by Löhne, Rudloff and Ulus in Löhne et al. (2014). The method deliv- ers polyhedral inner and outer approximations of the nondominated set.
an enclosure which consists of the union of 𝑚-dimensional intervals, The box coverage algorithm in Eichfelder and Warnow (2020) produces
i.e., of boxes. The authors show that this enclosure contains the non- dominated set. Moreover, it can be guaranteed that the enclosure has a certain width after a finite number of iterations, and that a side length
with objective functions 𝑓𝑖 ∶ ℝ𝑛 → ℝ, 𝑖 = 1, … , 𝑚, constraint functions
𝑔𝑗 ∶ ℝ𝑛 → ℝ, 𝑗 = 1, … , 𝑝, and an index set 𝐵 ⊆ {1, … , 𝑛}. As before, let
𝑀 denote the feasible set, which is assumed to be nonempty. If 𝐵 ≠ ∅
or if one of the functions 𝑓𝑖 or 𝑔𝑗 is non-convex, then (𝑀𝑂𝑃 − 𝑁 𝐶) is a
non-convex optimization problem.
tiobjective optimization is used for problems of this type with 𝐵 ≠ ∅ In the literature, one often finds that the notion mixed integer mul-
present approaches for 𝐵 ≠ ∅ with linear but also with nonlinear func- for aﬃne-linear objective and constraint functions. In this section, we tions 𝑓𝑖 and 𝑔𝑗 . Moreover, we discuss problems without an integrality constraint, i.e., with 𝐵 = ∅, but with non-convex objective functions. For a recent book on non-convex multiobjective optimization with 𝐵 = ∅
with 𝐵 = {1, … , 𝑛}, i.e. purely integer problems, are not in the focus of see Pardalos et al. (2017) by Pardalos, Žilinskas and Žilinskas. Problems
this paper.

Diﬀerences to the single-objective case

First, we point out some differences one can observe between the single-objective and the multiobjective case.
only one integer variable, e.g., 𝐵 = {1}, and that 𝑥1 is binary, i.e., it can For a start, we assume that all functions are convex and that there is only take the values 𝑥1 = 0 or 𝑥1 = 1. For 𝑚 = 1 and convex functions 𝑓𝑖 and 𝑔𝑗 one has to solve only two convex single-objective optimization
problems, one for each integer assignment, i.e., one for each fixing of the integer variable. The smaller optimal value of the two problems is equal to the optimal value of the mixed integer optimization problem.
For 𝑚 ≥ 2, i.e., for two or more objective functions, already this sim-
values for 𝑥1 would mean to determine the whole nondominated set of ple setting is much more challenging. Solving the problems with fixed




	


nary variable 𝑥1. The curves show the nondominated sets of the subproblems Fig. 2. Illustration of convex mixed integer biobjective optimization with a bi-
obtained by fixing the binary variable.


a convex multiobjective optimization problem, which is in general in- finite. Then, after computing two sets of nondominated points one has to compare them and to determine the “smallest” values. Situations as illustrated with Fig. 2 can occur. In Fig. 2(a) large parts of the individual
nondominated sets for a fixing of 𝑥1 = 0 or 𝑥1 = 1 have been calculated
which finally do not contribute to the overall nondominated set. On the
other hand, this illustration shows that several integer assignments can contribute to the overall solution set, which is not typical in mixed in- teger single-objective optimization. In Fig. 2(b) it is even worse and no
eﬃcient solution to 𝑥1 = 1 contributes to the overall eﬃcient set at all.
For a similar discussion see the recent paper (Cabrera-Guerrero et al.,
2021) by Cabrera-Guerrero, Ehrgott, Mason, and Raith. The authors pro- pose an algorithm for biobjective mixed-binary problems which makes use of the idea of comparing nondominated sets to integer assignments by using bounds for the sets.
Thus, problems which are known to be simple to solve in the single-
To give another example, solving (𝑀𝑂𝑃 − 𝑁 𝐶) without constraints, objective setting might no longer be simple in the multiobjective setting.
𝑚 = 1, and a strictly convex quadratic objective function can be done analytically. But solving (𝑀𝑂𝑃 − 𝑁 𝐶) with 𝑚 = 2 and strictly convex
quadratic objective functions is in general already NP-hard. The fact that solving these problems can no longer be done analytically has an influence on the development of solution methods for (mixed) integer convex quadratic multiobjective optimization problems as in De and Eichfelder (2021).
It is well known that, unless the feasible set of the multiobjective optimization problem is convex and the objective functions are strictly convex, cf. Burachik et al. (2017) by Burachik, Kaya and Rizvi, there can be weakly eﬃcient solutions which are not eﬃcient. From an ap- plication point of view, these points are often not of interest. However, scalarizations can often only guarantee to find weakly eﬃcient solu- tions. Strictly convex objective functions are not suﬃcient to guarantee that any weakly eﬃcient solution is eﬃcient. For instance, take an in-
with 𝑛 = 1 and 𝐵 = {1} and with 𝑓1 (𝑥) = (𝑥 − 1.5)2 and 𝑓2 (𝑥) = (𝑥 − 1)2 . teger biobjective optimization problem without inequality constraints Then 𝑥 = 1 and 𝑥 = 2 are weakly eﬃcient but only 𝑥 = 1 is eﬃcient, as
𝑓 (1) = (0.25, 0)⊤ and 𝑓 (2) = (0.25, 1)⊤.
Next to the above listed examples, also other ideas from the single-
objective setting cannot be transferred to problems with multiple ob- jectives: it was shown by Eichfelder and Groetzner (2021), after a first examination by Bai and Guo (2014), that lifting procedures which use completely positive relaxations for mixed binary non-convex quadratic problems fail for two or more objective functions in the sense that they can only find supported eﬃcient solutions.
Decision space and criterion space methods

For non-convex optimization problems of the type (𝑀𝑂𝑃 − 𝑁 𝐶) with 𝐵 ≠ ∅, i.e. for (mixed) integer problems, the existing algorithms
are mainly for aﬃne-linear objective and constraint functions, see for instance the review in Belotti et al. (2016) by Belotti, Soylu and Wiecek
for biobjective problems. Two of the few exceptions are the proce- dure by de Santis, Eichfelder, Niebling and Rocktäschel (De Santis et al., 2020), for multiobjective mixed integer convex optimization prob- lems, and by Cabrera-Guerrero, Ehrgott, Mason and Raith (Cabrera- Guerrero et al. (2021)) for biobjective mixed binary convex optimiza- tion problems. The known algorithms for multiobjective mixed integer problems can be divided into two main classes: decision space search
algorithms, i.e., approaches that work in the space of feasible points ℝ𝑛,
space    of    objective    function    values    ℝ𝑚. and criterion space search algorithms, i.e., methods that work in the
Among the decision space search algorithms, the method proposed by Mavrotas and Diakoulaki (1998) is the first branch-and-bound algo- rithm for solving mixed binary linear multiobjective problems. Also the method presented recently in De Santis et al., 2020 for solving convex mixed integer multiobjective problems is such a decision space method. An implementation of the algorithm from De Santis et al., 2020 is pro- vided in MOM (0000). The drawback of such methods is that in general the dimension of the decision space is much larger than the dimension of the criterion space. On the other hand, the decision space allows a more direct treatment of the integer variables and to transfer some of the techniques from single-objective optimization more directly.
Criterion space search algorithms find nondominated points by solving a sequence of single-objective optimization problems, for in- stance on the search zones as explained below. Once a nondomi- nated point is computed, dominated parts of the criterion space are removed and the algorithm continues the search for new nondomi- nated points. Criterion space search algorithms for linear integer biob- jective and triobjective problems are given by Boland, Charkhgard and Savelsbergh in Boland et al. (2015a, 2016) and for linear mixed inte- ger problems in Boland et al. (2015b), and more recently by Perini, Boland, Pecin and Savelsbergh in Perini et al. (2020). The method from Cabrera-Guerrero et al. (2021) for mixed integer convex biobjective optimization problems is also a procedure with bounds in the image space. And recently, Eichfelder and Warnow propose in Eichfelder and Warnow, 2021a a criterion space search algorithm for mixed integer convex multiobjective optimization problems.
In Stidsen et al. (2018), Stidsen and Andersen give a good overview on the literature and a discussion on the two different classes, decision space and criterion space methods. Moreover, the authors promote the use a hybrid approach and propose such an algorithm for linear mixed integer biobjective optimization problems.

Supported solutions

In this subsection, we would like to point out that some methods
supported solutions. A feasible point 𝑥̄ ∈ 𝑀 of the problem (𝑀𝑂𝑃 − for non-convex multiobjective optimization only generate what is called
𝑁𝐶) is called supported if there exist weights 𝑤 ∈ ℝ𝑚 with  𝑚 𝑤𝑖 = 1
such that for every 𝑥 ∈ 𝑀 it holds that 𝑤⊤𝑓 (𝑥̄) ≤ 𝑤⊤𝑓 (𝑥), i.e., 𝑥̄ is an
optimal solution of the weighted sum scalarization
min{𝑤⊤𝑓 (𝑥) ∣ 𝑥 ∈ 𝑀 }.
For instance, in Bai and Guo (2014) only supported solutions are gen- erated by a relaxation of the original optimization problem with the cone
of doubly nonnegative matrices for specific problems (𝑀𝑂𝑃 − 𝑁 𝐶)
with 𝐵 = ∅. This is examined in more detail for 𝐵 = ∅ and for 𝐵 ≠ ∅
in Eichfelder and Groetzner (2021).
purely integer variables, i.e., 𝐵 = {1, … , 𝑛} (or purely continuous, i.e., The method named PolySCIP, Pol (0000), is for linear problems with
𝐵 = ∅). The method finds all nondominated points for two and three objective functions. For 𝑚 ≥ 4 the method is only able to find those non-
dominated points which are supported points. Przybylski, Gandibleux and Ehrgott examine in Przybylski et al. (2010) linear integer multi- objective optimization problems, but only a subset of the supported solutions are calculated, the so called extreme supported solutions. Özpeynirci and Köksalan present in Özpeynirci and Köksalan (2010) an














Fig. 3. Example for a set G𝑃𝑁𝑆 , cf. Niebling and Eichfelder (2019).


exact algorithm to find all extreme supported solutions for a linear mixed integer multiobjective optimization problem. A more recent work on this topic by Przybylski, Klamroth and Lacour contains a good liter- ature review on such approaches, cf. Przybylski et al., 2019.

Enclosure of the nondominated set
Numerical methods in non-convex single-objective optimization of- ten generate converging sequences of lower and upper bounds on the
Fig. 4. Set G𝑃𝑁𝑆 marked with dots and local upper bound set 𝑈 (G𝑃𝑁𝑆 ) marked
with crosses, cf. Klamroth et al. (2015); Niebling and Eichfelder (2019).
the set 𝑆 = 𝑆(G) with
𝑚
+
𝑦∈G
it holds
∀𝑧 ∈ 𝑆(G) ∃𝑝 ∈ 𝑈 (G) ∶ 𝑧 < 𝑝
∀𝑧 ∈ 𝑍 ⧵ 𝑆(G) ∀𝑢 ∈ 𝑈 (G) ∶ 𝑧 ≮ 𝑢 and
∀𝑢1, 𝑢2 ∈ 𝑈 (G) ∶ 𝑢1 ≰ 𝑢2 or 𝑢1 = 𝑢2.
The set 𝑆(G) in the definition above is called search region. For the
search zones 𝐶(𝑢) defined by 𝐶(𝑢) ∶= {𝑢} − int(ℝ𝑚) ∩ 𝑍 for 𝑢 ∈ 𝑈 (G),

optimal value. In single-objective optimization, the largest lower bound and the smallest upper bound are unique. As a consequence, the distance between the bounds, which are scalars, can easily be calculated as their difference. In multiobjective optimization this is no longer possible. Up- per bounds are often derived from images of feasible points under the
it holds
𝑆(G) =
𝑢∈𝑈 (G)
and
+
𝐶(𝑢) = (𝑈 (G) − int(ℝ𝑚)) ∩ 𝑍

objective function. For 𝑚 ≥ 2 these are vectors in a partially ordered
space and no longer a unique smallest upper bound might exist. Thus
 ⊆ G
𝑃𝑁𝑆
∪ 𝑆(G
𝑃𝑁𝑆
) ⊆ 𝑈 (G
𝑃𝑁𝑆
) − ℝ𝑚.

one collects these images of feasible points under the objective function and determines the nondominated points among them. In the follow- ing, this set of points will be denoted the provisionally nondominated
set G𝑃𝑁𝑆 , as they are not yet nondominated for the multiobjective op-
timization problem but only nondominated among the points found so far. The set G𝑃𝑁𝑆 is also called incumbent set in some of the literature on (mixed) integer multiobjective optimization. An illustration is given
in Fig. 3.
Hence, any feasible point 𝑥 of (𝑀𝑂𝑃 − 𝑁 𝐶) gives with 𝑓 (𝑥) an upper bound for the nondominated points for (𝑀𝑂𝑃 − 𝑁 𝐶): It gives an upper point in the sense that there is no eﬃcient point 𝑥̄ for (𝑀𝑂𝑃 − 𝑁 𝐶) with
𝑓 (𝑥̄) ∈ {𝑓 (𝑥)} + ℝ𝑚 ⧵ {0}. Thus, if G𝑃𝑁𝑆 denotes a finite set of points
𝑓 (𝑥) with 𝑥 feasible for (𝑀𝑂𝑃 − 𝑁 𝐶), then the nondominated set  of
𝑓 (𝑀 ) is a subset of
For an illustration of a local upper bound set see Fig. 4. There, (𝑧1, 𝑧2)
is related to the box 𝑍̂ .
While the local upper bound set is easy to compute for 𝑚 = 2, as can
be seen in Fig. 4, it gets much more complicated in higher dimensions.
But the authors in Klamroth et al. (2015), and more recently Dächert, Klamroth and Lacour in Dächert et al. (2017), give algorithms which al- low a fast computation. The local upper bounds can be used for a calcu- lation of the hypervolume indicator, see Lacour, Klamroth and Fonseca, Lacour et al. (2017). The hypervolume indicator is used for comparing the quality of discrete approximations or representations of the non- dominated set. It measures the volume of the part of the criterion space dominated by the points of the approximation and bounded by some reference point.
The local upper bounds can well be used within algorithms. For in-

G𝑃𝑁𝑆 ∪
(ℝ𝑚 ⧵
𝑦∈⋃G𝑃𝑁𝑆
({𝑦} + ℝ𝑚
)).	(3)
stance, the search zones are used for improving the set G𝑃𝑁𝑆 by solving
a scalarization within a search zone. A further example is the algorithm
from Niebling and Eichfelder (2019) which is for problems of the type
(𝑀𝑂𝑃 − 𝑁 𝐶) with 𝐵 = ∅, convex constraints, and non-convex objective

For an illustration of a set G𝑃𝑁𝑆 for a problem (𝑀𝑂𝑃 − 𝑁 𝐶) with 𝑚 = 2,
𝐵 = ∅ and 𝑓𝑖 non-convex see the dots in Fig. 3. The set from (3) is marked
in Fig. 4 in gray.
By using the concept of local upper bounds from Klamroth, Lacour, and Vanderpooten, Klamroth et al. (2015), the second set in the union in
functions. The algorithm generates, for a predefined 𝜀 > 0, a list G𝑃𝑁𝑆 such that for the related set of local upper bounds 𝑈 = 𝑈 (G𝑃𝑁𝑆 ) it holds that all nondominated points for (𝑀𝑂𝑃 − 𝑁 𝐶) are contained in the set
𝑇 ∶= ( ∪ {𝑝} − (ℝ)𝑚) ⧵ ( ∪ {𝑝 − 𝜀 𝑒} − int((ℝ)𝑚)).	(4)

(3) (restricted to a superset 𝑍̂ of 𝑓 (𝑀 )) can be expressed with the help
of a finite set 𝑈 as 𝑈 − int(ℝ𝑚). The usefulness of such lower and upper
𝑝∈𝑈
+	𝑝∈𝑈	2	+

+
bound sets for multiobjective combinatorial optimization problems has
already been recognized earlier by Ehrgott and Gandibleux (2007). For the definition below based on Klamroth et al. (2015), a set G is called
stable if for any two points 𝑦1, 𝑦2 ∈ G it either holds 𝑦1 = 𝑦2 or 𝑦1 Ą 𝑦2.
In (4), the vector 𝑒 denotes the vector of ones (also known as all-ones vector). An illustration of such a set 𝑇 is shown in Fig. 5.
the decision space. For applying the algorithm, a box 𝑋0 has to be known The algorithm is a branch-and-bound method with subdivisions in

For G we will later take the set G
𝑃𝑁𝑆
. Moreover, a box 𝑍̂ is required
with 𝑀 ⊆ 𝑋0. The basic idea of the algorithm goes back to a paper

with 𝑓 (𝑀 ) ⊆ int(𝑍̂ ). We denote its interior by 𝑍, i.e., 𝑍 ∶= int(𝑍̂ ).
Definition 5.1. Let G be a finite and stable set of points in 𝑍. A finite set 𝑈 (G) ⊆ 𝑍̂ is called a local upper bound set with respect to G if for
from Fernández and Tóth, Fernández and Tóth (2009), in which interval arithmetic and lower bounds by ideal points are used. The ideal point of
(𝑀𝑂𝑃 − 𝑁 𝐶) is the point 𝑎 ∈ ℝ𝑚 with 𝑎𝑖 ∶= min𝑥∈𝑀 𝑓𝑖 (𝑥), 𝑖 = 1, … , 𝑚.
In Niebling and Eichfelder (2019), lower bounds generated with the help






















Fig. 5. Set 𝑇 from (4) depending on 𝜀 , cf. Niebling and Eichfelder (2019).

2

of convex relaxations and outer approximations are used. Those deliver lower bounds in such a way that the above property in (4) is guaran- teed after a finite number of iterations. The local upper bounds are used
is called approximation or enclosure of the nondominated set  given
𝐿 and 𝑈 .
In Eichfelder et al. (2021a) it is proposed to use for 𝑈 the set of local upper bounds to a stable set G = G𝑃𝑁𝑆 of images of feasible points. The lower bounds are generated by ideal points, for which relaxations of 𝑓𝑖 and of 𝑀 as well as subproblems, e.g. over 𝑀 ∩ 𝑋𝑘 with some subbox
𝑋𝑘, are used.
To measure the width of this enclosure, similar to the difference be-
objective global optimization, the following is used: The width 𝑤(𝐿, 𝑈 ) tween the upper and the lower bound on the optimal value in single-
is defined as the optimal value of the problem
max{ (𝑦 + 𝑡𝑒) − 𝑦  ∕ 𝑚 ∣ 𝑡 ≥ 0, 𝑦, 𝑦 + 𝑡𝑒 ∈ 𝐴(𝐿, 𝑈 )}.
𝑦,𝑡
The authors in Eichfelder et al. (2021a) show that the width 𝑤(𝐿, 𝑈 ) of
𝐴(𝐿, 𝑈 ) coincides with
max{𝑠(𝓁, 𝑢) ∣ (𝓁, 𝑢) ∈ 𝐿 × 𝑈, 𝓁 ≤ 𝑢}
𝓁,𝑢
where the length of a shortest edge 𝑠(𝓁, 𝑢) of a box [𝓁, 𝑢] is used which
is defined by

in a similar way for convex mixed integer multiobjective optimization
𝑠(𝓁, 𝑢) ∶=  min
𝑗
(𝑢𝑗 − 𝓁𝑗 ).

within a decision space branch-and-bound algorithm in De Santis et al., 2020.
For non-convex single-objective optimization the generated lower bounds are used in general more directly than in Niebling and Eich- felder (2019) where they are only used for discarding tests. Using the lower bounds as in the single-objective setting was transferred recently to the multiobjective setting by Eichfelder, Kirst, Meng and Stein in Eichfelder et al. (2021a). As a result, the authors obtain a box-based coverage of the nondominated set, called enclosure in Eichfelder et al. (2021a). Recall that we denote the nondominated set
of 𝑓 (𝑀 ) by  . We use for a box with bounds 𝓁, 𝑢 ∈ ℝ𝑚 the notation
[𝓁, 𝑢] ∶= 𝑦 ∈ (ℝ)𝑚 ∣ 𝓁𝑖 ≤ 𝑦𝑖 ≤ 𝑢𝑖 forall𝑖 = 1, … , 𝑚 .
Definition 5.2. Let 𝐿, 𝑈 ⊆ ℝ𝑚 be two finite sets with
 ⊆ 𝐿 + ℝ𝑚 and  ⊆ 𝑈 − ℝ𝑚.
=1,…,𝑚
Moreover, the authors in Eichfelder et al. (2021a) show that if
𝑤(𝐿, 𝑈 ) < 𝜀 holds for some 𝜀 > 0, then all points from the set 𝐴(𝐿, 𝑈 ) ∩
𝑓 (𝑀 ) are at least 𝜀-nondominated for (𝑀𝑂𝑃 − 𝑁𝐶).
For 𝜀 > 0, a point 𝑥̄ ∈ 𝑀 is called 𝜀-eﬃcient for (𝑀𝑂𝑃 − 𝑁 𝐶) if there exists no 𝑥 ∈ 𝑀 such that 𝑓 (𝑥) ≤ 𝑓 (𝑥̄) − 𝜀𝑒 and 𝑓 (𝑥) ≠ 𝑓 (𝑥̄) − 𝜀𝑒 hold. In that case the vector 𝑓 (𝑥̄) is called 𝜀-nondominated for (𝑀𝑂𝑃 − 𝑁 𝐶).
the specific choice 𝜀𝑒 ∈ ℝ𝑚. In Fig. 6 we illustrate the concept of an For a source of this definition see, for instance, Loridan (1984) with
enclosure for a convex biobjective and for a convex triobjective problem, respectively.

Multiobjective bilevel optimization

Bilevel optimization is another class of problems that, already in the single-objective case, are very challenging from both the theoretical and

+	+	numerical point of view. In bilevel optimization two optimization prob-

Then 𝐿 is called lower bound set, 𝑈 is called upper bound set, and the set 𝐴 which is given as
𝑚	𝑚
+	+
𝓁∈𝐿 𝑢∈𝑈, 
𝓁≤𝑢
lems appear, one on the so-called upper level and one on the lower level. The optimization variable of the upper level is a parameter for the lower-level problem, and the optimal solutions of the lower-level problem influence the objective function value on the upper level. For a good introduction to the topic we refer to the book by Dempe, 2012. Due




Fig. 6. Enclosure for a (a) biobjective and a (b) triobjective problem.



to several applications, for instance in the energy market, this problem class is of recent interest.
A simply structured bilevel optimization problem with just one ob- jective function on each level and without further constraints is the fol- lowing,
tinen and Wiecek, Ruuska et al. (2012), this topic is thoroughly exam- ined together with an excellent literature survey on other contributions to this topic. It is still an open research question whether these refor- mulations can be used for developing numerical algorithms for bilevel problems with more than one or two variables on the upper level.

min
𝑥 ∈ℝ𝑛𝑢 𝑓𝑢 (𝑥𝓁 , 𝑥𝑢 )
(5)
If one allows multiple objectives, a more general lower-level problem
than the one in (5) might read as

s.t.	𝑥𝓁 ∈ argmin𝑧∈ℝ𝑛𝓁 {𝑓𝓁 (𝑧, 𝑥𝑢 )},

𝑛	𝑛
min {𝑓 (𝑥, 𝑦) ∣ (𝑥, 𝑦) ∈ 𝐺}	(6)

where 𝑓𝑢 , 𝑓𝓁 ∶ ℝ 𝓁 × ℝ 𝑢 → ℝ (𝑛𝑙 , 𝑛𝑢 ∈ ℕ). Here, we assume that the
lower-level problem has a minimal solution for any 𝑥𝑢 ∈ ℝ𝑛𝑢 . Already in
this simple setting, one of the major diﬃculties of this problem class can
then the bilevel problem is not well defined. For some 𝑥𝑢 from the upper appear: if the lower-level problem has a non-unique optimal solution, level and a set of optimal solutions 𝑋𝓁 (𝑥𝑢 ) to that 𝑥𝑢 for the lower level, it is not clear for which 𝑥𝓁 ∈ 𝑋𝓁 (𝑥𝑢 ) the upper-level objective function
𝑓𝑢 should be evaluated. In approaches for bilevel optimization one often
which is for instance the case for strictly convex functions 𝑓𝓁 (⋅, 𝑥𝑢 ). If just assumes that the lower-level problem has a unique optimal solution,
there is a multiobjective optimization problem with two or more objec- tive functions on the lower level, this can no longer be assumed, since these problems have in general an infinite set of eﬃcient solutions, even in case all functions are strictly convex. Already in this simple setting, one of the major diﬃculties of this problem class can appear: if the lower-level problem has a non-unique optimal solution, then the bilevel
problem is not well defined. For some 𝑥𝑢 from the upper level and a set
of optimal solutions 𝑋𝓁 (𝑥𝑢 ) to that 𝑥𝑢 for the lower level, it is not clear for which 𝑥𝓁 ∈ 𝑋𝓁 (𝑥𝑢 ) the upper-level objective function 𝑓𝑢 should be
evaluated. In approaches for bilevel optimization one often just assumes that the lower-level problem has a unique optimal solution, which is for
instance the case for strictly convex functions 𝑓𝓁 (⋅, 𝑥𝑢 ). If there is a mul-
tiobjective optimization problem with two or more objective functions
on the lower level, this can no longer be assumed, since these prob- lems have in general an infinite set of eﬃcient solutions, even in case all functions are strictly convex. Already in this simple setting, one of the major diﬃculties of this problem class can appear: if the lower-level problem has a non-unique optimal solution, then the bilevel problem is
not well defined. For some 𝑥𝑢 from the upper level and a set of optimal
solutions 𝑋𝓁 (𝑥𝑢 ) to that 𝑥𝑢 for the lower level, it is not clear for which
𝑥𝓁 ∈ 𝑋𝓁 (𝑥𝑢 ) the upper-level objective function 𝑓𝑢 should be evaluated.
In approaches for bilevel optimization one often just assumes that the
the case for strictly convex functions 𝑓𝓁 (⋅, 𝑥𝑢 ). If there is a multiobjec- lower-level problem has a unique optimal solution, which is for instance
tive optimization problem with two or more objective functions on the lower level, this can no longer be assumed, since these problems have in general an infinite set of eﬃcient solutions, even in case all functions are strictly convex.
𝑥∈ℝ 1
with the parameter 𝑦 ∈ ℝ𝑛2 from the upper level, a vector-valued func- tion 𝑓 ∶ ℝ𝑛1 × ℝ𝑛2 → ℝ𝑚1 , and a nonempty feasible set 𝐺 ⊆ ℝ𝑛1 × ℝ𝑛2 (𝑛1 , 𝑛2 , 𝑚1 ∈ ℕ). For the upper-level variable 𝑦 ∈ ℝ𝑛2 we define
Ψ(𝑦) ∶= argmin𝑥 {𝑓 (𝑥, 𝑦) ∣ (𝑥, 𝑦) ∈ 𝐺} ⊆ ℝ𝑛1 .
For 𝑓 vector-valued, it has to be clarified what a minimal solution is, i.e., how the set Ψ(𝑦) is defined: whether it is the set of eﬃcient solutions,
or the set of weakly eﬃcient solutions, or something different (as the set of properly eﬃcient solutions which is a subset of the set of eﬃcient solutions). For the following, we define them to be the eﬃcient solutions of (6).
The optimization problem on the upper level might then be given
as
min {𝐹 (𝑥, 𝑦) ∣ 𝑥 ∈ Ψ(𝑦), 𝑦 ∈ 𝐺̃},
𝑦∈ℝ𝑛2
with a vector-valued function 𝐹 ∶ ℝ𝑛1 × ℝ𝑛2 → ℝ𝑚2 , 𝑚2 ∈ ℕ, and a com- pact set 𝐺̃ ⊆ ℝ𝑛2 . As discussed above, the upper-level problem is not
well-defined without saying which of the eﬃcient solutions of the lower- level problem should be chosen for the evaluation of the upper level function. An often chosen approach is the optimistic approach, i.e., one solves
min  {𝐹 (𝑥, 𝑦) ∣ 𝑥 ∈ Ψ(𝑦), 𝑦 ∈ 𝐺̃}.
𝑥∈ℝ 1 , 𝑦∈ℝ 2
Another possible approach for a non-singleton set Ψ(𝑦) is to study all
eﬃcient solutions of (6) at the same time and thus the set-valued map
𝑦 ↦ {𝐹 (𝑥, 𝑦) ∣ 𝑥 ∈ Ψ(𝑦)} ⊆ ℝ𝑚2 .
Examinations on such an approach can be found in a recent PhD the- sis by Pilecka, 2016. The use of set relations, which compare sets as a whole, then allows to study optimal solutions of a bilevel problem with- out explicitly choosing one of the lower level eﬃcient solutions. Possible set relations are, for instance, the lower-less order relation defined by
𝐴 ≼𝓁 𝐵	∶⇔	𝐵 ⊆ 𝐴 + ℝ𝑚2
⇔	∀𝑏 ∈ 𝐵 ∃𝑎 ∈ 𝐴 ∶ 𝑎𝑖 ≤ 𝑏𝑖 ∀𝑖 ∈ {1, … , 𝑚2 }
for two nonempty sets 𝐴, 𝐵 ⊆ ℝ𝑚2 . Another widely used set relation is
called the upper-less order relation defined by

If the lower-level problem has non-unique optimal solutions, then a
is not only minimized with respect to 𝑥𝑢 but also with respect to 𝑥𝓁 . common approach is the optimistic approach: the upper level function
𝐴 ≼𝑢
𝐵	∶⇔	𝐴 ⊆ 𝐵 − ℝ𝑚2
⇔	∀𝑎 ∈ 𝐴 ∃𝑏 ∈ 𝐵 ∶ 𝑎𝑖 ≤ 𝑏𝑖 ∀𝑖 ∈ {1, … , 𝑚2 }
(7)

Next to the optimistic approach, pessimistic formulations are discussed in the literature for which the upper level function is maximized w.r.t.
𝑥𝓁 .
Before we discuss the optimistic approach for the multiobjective set-
ting, we would like to mention that bilevel optimization problems have already raised the interest in the multiobjective community with respect to another research question: Is it possible to reformulate a bilevel opti- mization problem equivalently as a biobjective optimization problem? It can easily be seen that the biobjective optimization problem with ob-
jective functions 𝑓𝑢 and 𝑓𝓁 does not fulfill that. However, there are more
complex reformulations, which require for instance the study of multi-
defined   by   𝐹    ∶   ℝ𝑛𝓁 +𝑛𝑢     →   ℝ𝑛𝑢 +3 objective optimization problems with a vector-valued objective function
for two nonempty sets 𝐴, 𝐵 ⊆ ℝ𝑚2 . For an illustration of both concepts
see Fig. 7. For more details on set optimization and on solution concepts
based on such set relations we refer to the book by Tammer, Khan and Zǎlinescu, Khan et al. (2015). The set relations then allow to model optimistic or pessimistic situations as well, see Pilecka, 2016.
Solving bilevel optimization problems numerically is known to be a challenge, but this is even more the case for multiple objectives on the
lower level. To see this, observe that in the single-objective case (𝑚1 = 1)
the lower level problem can be reformulated by using the minimal value
function
𝑤(𝑦) ∶= min{𝑓 (𝑥, 𝑦) ∣ (𝑥, 𝑦) ∈ 𝐺}
𝑥
and by replacing the upper level constraint 𝑥 ∈ Ψ(𝑦) by

𝐹 (𝑥 , 𝑥 ) = (𝑥 , 𝑓 (𝑥 , 𝑥 ), 𝑓 (𝑥 , 𝑥 ), ∇
𝑓 (𝑥 , 𝑥 )  )
𝑓 (𝑥, 𝑦) ≤ 𝑤(𝑦),	(𝑥, 𝑦) ∈ 𝐺.	(8)

𝓁  𝑢	𝑢  𝑢  𝓁  𝑢	𝓁  𝓁  𝑢  ‖ 𝑥𝓁 𝓁  𝓁  𝑢 ‖2

and a specific partial ordering. A first discussion of this topic is due to (Fliege, Vicente, 2006) from 2006. In a recent survey by Ruuska, Miet-
function 𝑓 on the lower level maps to ℝ𝑚1 with 𝑚1 ≥ 2, then the values Thus, smoothness of this minimal value function is important. If the




	
Fig. 7. Lower-less (left) and upper-less (right) order relation for two sets 𝐴, 𝐵 ⊆
ℝ2 and ordering cone 𝐶 ∶= ℝ2 .


of the minimal value function 𝑤(𝑦) are sets of nondominated points (or of weakly nondominated points, dependent on the choice of Ψ(𝑦)), and
(8) has to be replaced by the more complicated formulation
𝑓 (𝑥, 𝑦) ∈ 𝑤(𝑦) − ℝ𝑚,	(𝑥, 𝑦) ∈ 𝐺.
As multiobjective bilevel problems are important within many ap- plication areas, many researchers have tried to contribute to solvers for these problems. For linear multiobjective bilevel problems procedures have already been proposed more than twenty years ago, see for instance the paper by (Nishizaki, Sakawa, 1999) .
Much fewer papers are dealing with nonlinear multiobjective bilevel
problems. A first interactive method has been proposed in 1997 by
with objective function 𝑓 ∶ ℝ𝑛 × ℝ𝑘 → ℝ𝑚 and nonempty feasible set
𝑀 (𝜉) ⊆ ℝ𝑛 for each scenario 𝜉 ∈ T . The uncertain optimization prob- lem P(T ) is given as the family (P(𝜉), 𝜉 ∈ T ). For being feasible one typically requires that some 𝑥 ∈ ℝ𝑛 is feasible for all scenarios 𝜉 ∈ T ,
i.e., one requires
𝑥 ∈ 𝑀 ∶=   𝑀 (𝜉).	(9)
𝜉∈T
Then, for 𝑚 = 1, following the classical approach of minmax-
robustness, one has to solve the optimization problem
min max𝑓 (𝑥, 𝜉).
𝑥∈𝑀 𝜉∈T

extended to meaningful definitions for problems with 𝑚 ≥ 2 objective In the last ten years, many authors have examined how this can be
functions. One can observe an increasing interest in the definition of suitable optimality notions and solution approaches.

Objective-wise worst case

A first approach to extend minmax-robustness is due to Kuroiwa and Lee (2012). Doolittle, Kerivin and Wiecek, Doolittle et al. (2018), fol- low a similar approach. They replace each objective function of the un- certain multiobjective optimization problem by the worst case of the respective objective function values, i.e., they study the deterministic multiobjective optimization problem
max 𝑓 (𝑥, 𝜉)

Shi and Xia (1997). Other contributions which appeared in the last ten
min 𝑓 OW(𝑥) with 𝑓 OW(𝑥) ∶=
⎛⎜ 𝜉∈T 1
⎞⎟,	(10)

years use scalarizations, as by Sinha, Malo and Deb in Sinha et al. (2018).
However, in the case the scalarization does not characterize the set of (weakly) eﬃcient solutions of the lower level problem exactly, this
𝑥∈𝑀 T
T
max
𝜉∈T
𝑓𝑚 (𝑥, 𝜉
)⎟⎠

might be a problem and imply a relaxation or restriction of the over- all problem. What is more, as discussed by Dempe and Mehlitz (2020), it can be a delicate issue if just locally optimal solutions of the scalar- ization are investigated. Further approaches are the characterization of the upper-level feasible set as solution set of specific multiobjective opti- mization problems, as proposed by Eichfelder (2010); Eichfelder, 2008. Bonnel and Morgan consider in Bonnel and Morgan (2006) a semivecto- rial bilevel optimization problem and propose a solution method based on a penalty approach. In Gebhardt and Jahn (2009), Gebhardt and Jahn propose to use a multiobjective search algorithm with a subdivi- sion technique. Approaches for solving multiobjective bilevel optimiza- tion problems by using evolutionary algorithms in combination with local solvers can be found, for instance, in Deb and Sinha (2010). In 2020, a survey on multiobjective bilevel optimization by Eichfelder ap- peared which summarizes the progress in numerical solvers in more de- tail, see Eichfelder (2020). Moreover, just recently, an implementation of the algorithm from Eichfelder (2010) was made publicly available MOB (0000).
However, all existing approaches are limited in the sense that they only apply to very specific problem classes and can, for instance, only be applied for a small number of objective functions on the levels and/or a small number of variables, specifically on the upper level. Thus more research is needed.

Robustness in uncertain multiobjective optimization

In many real-world applications there are uncertainties in the opti- mization problems under examination which should be taken into ac- count when solving the problem. One way to deal with uncertainties is robust optimization. Then, the aim is to find solutions which remain feasible and of good quality for all possible scenarios, i.e., for all possi-
by some uncertainty set T ⊆ ℝ𝑘. For each parameter 𝜉 ∈ T , called sce-
nario, one studies the parametric optimization problem
min 𝑓 (𝑥, 𝜉)	P(𝑥)
where 𝑂𝑊 stands for objective-wise. Then, the robust (weakly) eﬃcient
solutions for P(T ) w.r.t. the objective-wise approach are the (weakly)
eﬃcient solutions for (10).
A similar concept was studied by Fliege and Werner (2013) in the
applying first an 𝜀-constraint scalarization to the problems (P(𝑥)). Then context of portfolio selection. The above reformulation is motivated by
one obtains a formulation as
min {𝑓𝑚 (𝑥, 𝜉) ∣ 𝑓𝑖 (𝑥, 𝜉) ≤ 𝜀𝑖 , ∀ 𝑖 = 1, … , 𝑚 − 1},
𝑥 𝑀 𝜉
for some 𝜀 ∈ ℝ𝑚−1. As a next step, when applying the same idea as stated in (9) for feasibility, a point 𝑥 is feasible for all 𝜉 ∈ T if it holds 𝑥 ∈ 𝑀
and
max 𝑓𝑖 (𝑥, 𝜉) ≤ 𝜀𝑖  for all 𝑖 = 1, … , 𝑚 − 1.
𝜉 T
The minmax-robustness approach for this single-objective optimization problem results in
min max 𝑓 (𝑥, 𝜉)
𝑥∈𝑀 𝜉∈T
s.t. max 𝑓𝑖 (𝑥, 𝜉) ≤ 𝜀𝑖 ,  for all 𝑖 = 1, … , 𝑚 − 1.
𝜉 T
The above problem is exactly an 𝜀-constraint scalarization of (10).
Next to this motivation, a further advantage of the formulation in
(10) is that the problems are in general easier to solve than those be- ing a result from the approach in the next subsection. We illustrate the concept using the objective-wise worst case with an example:
Example 7.1. Let 𝑚 = 2, 𝑛 = 𝑘 = 1 and assume we have as uncertainty set the interval T = [0, 1]. Moreover, assume that there are only two fea- sible points, i.e., 𝑀 = {𝑥1, 𝑥2} ⊆ ℝ and that 𝑓 ∶ 𝑀 × T → ℝ2 is defined
by
𝑓 (𝑥1, 𝜉) = 𝜉(5) + (1 − 𝜉)(2) and 𝑓 (𝑥2, 𝜉) = (4) for all 𝜉 ∈ T .
Then for the objective function in (10) we have
𝑓 𝑂𝑊 (𝑥1) = (5),   𝑓 𝑂𝑊 (𝑥2) = (4),



and thus 𝑥2 is the only eﬃcient solution for (10) and thus the only robust eﬃcient solution w.r.t. the objective-wise worst case approach for P(T ).

has only an impact for 𝑥1. For 𝑥1 we have for many scenarios 𝜉 smaller In the above example we have a specific situation: the uncertainty objective function values for each objective function than for 𝑥2. For the remaining scenarios 𝜉 we have that 𝑓 (𝑥1, 𝜉) and 𝑓 (𝑥2, 𝜉) are not
comparable w.r.t. the componentwise ordering. We never have that
𝑓 (𝑥2, 𝜉) ≤ 𝑓 (𝑥1, 𝜉), but still 𝑥1 is not a robust eﬃcient solution w.r.t. the
objective-wise worst case approach.
The reason for that is that with this approach we do not model that one scenario will finally happen for the overall problem. Instead we
model that one scenario 𝜉1 can lead to the worst case of objective func-
tion 𝑓1 and another 𝜉2 to the worst case for objective function 𝑓2 . In
valued function 𝑓 directly and to keep the interwoven structure of the practice, one might instead want to model the worst-case for the vector- objective functions 𝑓𝑖 , 𝑖 = 1, … , 𝑚, connected by the parameter 𝜉.
one has the componentwise ordering in the image space, i.e., with 𝑦 ≤ 𝑧 Moreover, such an objective-wise approach can only be studied if if and only if 𝑦𝑖 ≤ 𝑧𝑖 for all 𝑖 = 1, … , 𝑚. The objective-wise approach can-
not be transferred to more general ordering structures as those briefly mentioned in Section 8. Thus, other robustness notions are discussed in the literature which use a set based approach, see the next subsection.

Set-valued based worst case

use  of  the  set-valued  function  𝑓T   ∶  ℝ𝑛  →  ℝ𝑚, We call the approach in this section set-valued approach as it makes
𝑓T (𝑥) ∶= {𝑓 (𝑥, 𝜉) ∈ ℝ𝑚 ∣ 𝜉 ∈ T }.
Ehrgott et al. (2014) the following optimality notions for P(T ): Based on this function, Ehrgott, Ide and Schöbel define in
Definition 7.2. A point 𝑥̄ ∈ 𝑀 with 𝑀 as in (9) is denoted
robust weakly eﬃcient (rwe) for P(T ) if there is no 𝑥 ∈ 𝑀 ⧵ {𝑥̄}
such that
𝑓T (𝑥) ⊆ 𝑓T (𝑥̄) − int(ℝ𝑚).
robust eﬃcient (re) for P(T ) if there is no 𝑥 ∈ 𝑀 ⧵ {𝑥̄} such that
𝑓T (𝑥) ⊆ 𝑓T (𝑥̄) − (ℝ𝑚 ⧵ {0}).
robust strictly eﬃcient (rse) for P(T ) if there is no 𝑥 ∈ 𝑀 ⧵ {𝑥̄} such
that
order relation defined in (7), and variants of it, and sets are often not comparable.
Robust multiobjective optimization with the set-valued approach has an obvious relation to set optimization, which is widely used in the lit- erature for further theoretical examinations and for first numerical ap- proaches. For instance, this is studied in the specific case of decision un- certainty by Eichfelder, Krüger and Schöbel in Eichfelder et al. (2017),
i.e., for the case that one has for the problem (P(𝑥)) objective functions
of the form 𝑓𝑖 (𝑥, 𝜉) = 𝑓̃ (𝑥 + 𝜉), with functions 𝑓̃ ∶ ℝ𝑛 → ℝ, 𝑖 = 1, … , 𝑚
and T ⊆ ℝ𝑛. The uncertainty is thus in the realization of the variable 𝑥
and we obtain the set-valued function
𝑓T (𝑥) = {𝑓̃(𝑥 + 𝜉) ∈ ℝ𝑚 ∣ 𝜉 ∈ T }.
For this type of problem, numerical approaches based on com- paring the sets with a decision space branch-and-bound procedure have been proposed by Eichfelder, Niebling and Rocktäschel, see Eichfelder et al. (2020).
Multiobjective optimization problems with decision uncertainty arise in applications whenever exactly computed optimal solutions can- not be realized in practice with such an exactness. To give an exam- ple, for a horticulture problem with economic and environmental goals uncertainties in the mixing process have been modeled and robust solu- tions have been calculated by Krüger, Castellani, Geldermann and Schö- bel in Krüger et al. (2018).
The relation of robust optimization for uncertain multiobjective op- timization to set optimization is intensively studied by Klamroth, Köbis, Schöbel, and Tammer in Klamroth et al. (2017). As various set relations exist, one can derive various notions of robustness when replacing the upper-less order relation by one of those. A recent survey by Ide and Schöbel (2016) gives a good overview on the variety of concepts used in robust uncertain multiobjective optimization. For a more recent pa- per with a survey and with another new concept called multi-scenario eﬃciency see Botte and Schöbel (2019).
In some specific situations one has that the uncertainties of the individual objective functions are independent of each other, called
objective-wise uncertainty. Then one studies instead of (P(𝑥)) a multi-
objective optimization problem with objective function 𝑓 ∶ ℝ𝑛 × ℝ𝑘 →
ℝ𝑚,
𝑓1 (𝑥, 𝜉1 )
𝑓 (𝑥, 𝜉) =
⎜	⎟




Note that with (7) the condition in (11) can equivalently be written
with 𝜉𝑖 ∈ T𝑖 ⊆ ℝ𝑘𝑖 , 𝑖 = 1, … , 𝑚, ∑𝑚  𝑘𝑖 = 𝑘, and T = T1 ×…× T𝑚 , see

for instance (Ehrgott et al., 2014, Def. 5.1) by Ehrgott, Ide and Schö-
as	bel. According to their Theorem 4.4, if max𝜉 ∈T 𝑓𝑖 (𝑥, 𝜉𝑖 ) exists for all
𝑖  𝑖

𝑓T (𝑥) ≼𝑢 𝑓T (𝑥̄).
In the following we use the abbreviations in the brackets in Definition 7.2. It holds that rse implies re which again implies rwe for
P(T ). The authors in Ehrgott et al. (2014) also compare their approach
their Theorem 4.11, in the case max𝜉∈T 𝑓𝑖 (𝑥, 𝜉) exists for all 𝑖 = 1, … , 𝑚 with the objective-wise worst case approach from above. According to and all 𝑥 ∈ 𝑀 , then any weakly eﬃcient solution for (10) is rwe for P(T ).

lem in Example 7.1. The point 𝑥2 is eﬃcient and hence weakly eﬃcient Example 7.3. We determine the rwe, re and rse solutions to the prob- for (10). Thus, by the above cited theorem it is rwe for P(T ). By apply- ing the definition above it is easy to verify that 𝑥2 is rse and re for P(T ), and 𝑥1 is rse, re and thus rwe for P(T ).
The example shows that we can find more robust eﬃcient solutions
are rse and re for P(T ) as now sets are compared by using the upper-less with the set-valued approach. Moreover, in general many feasible points
𝑖 = 1, … , 𝑚 and all 𝑥 ∈ 𝑀 , then a point 𝑥̄ is an eﬃcient solution for (10) if and only if it is re for P(T ).
all possible scenarios one might decide for a solution, as for 𝑥2 in Robust solutions are in some sense expensive: for being robust to
would have been possible (for instance for 𝜉 = 0.5, 𝑥1 would have been Example 7.1, despite for many scenarios much better function values
the better choice). Consequently, the arising gaps of the objective func- tion values of the robust solutions and of the solutions for the indi- vidual scenarios are of interest and subject to research, see, for in- stance, Krüger, Schöbel and Wiecek (Krüger et al., 2017). Similarly, in Groetzner and Werner (2021), the authors aim to minimize the regret of not having chosen the optimal decision. In their paper (Groetzner and Werner, 2021) they study the objective-wise worst case approach. The extensions from the concepts of the single-objective case to the multi- objective case are often not straightforward and finally complicated to calculate numerically. Thus, more progress has to be made before ro- bust solutions can be calculated numerically, and with an appropriate trade-off between robustness and a possible regret.



Further topics such as extension of concept

The sections in this survey intend to give the basic ideas of some of the fields in continuous multiobjective optimization which have seen a huge progress or which have raised ongoing interest in the last twenty years. Of course, there are many more topics for which important and new results have been gained and where the area of multiobjective op- timization is developed further.
To list some of the further topics, there is the study of multiobjec- tive optimal control problems and how these can be solved by means of model predictive control, see Grüne and Stieler (2019). A reason for the increasing interest in this type of problem is that there are im- portant applications as in the energy market, see Sauerteig and Worth- mann (2020). Also for multiobjective optimal control problems scalar- izations are a possible approach, as examined for instance by Kaya and Maurer in Kaya and Maurer (2014) and by Logist, Houska, Diehl and Van Impe in Logist et al. (2010).
A further topic of ongoing interest is to try to include more informa- tion on the application problem in the formulation of the multiobjective optimization problem by introducing more general ordering structures than partial orderings. So far in this paper, for defining an eﬃcient so-
lution for (MOP) we use the so called natural ordering, i.e., the compo-
nentwise ordering in the criterion space: for 𝑦, 𝑧 ∈ ℝ𝑚 we use
𝑦 ≤ 𝑧 ⇔ 𝑦𝑖 ≤ 𝑧𝑖 for all 𝑖 = 1, … , 𝑚.
The above concept implies that a feasible point 𝑥 is preferred to some feasible point 𝑤 if all objective functions are satisfied at least equally well, i.e., 𝑓𝑖 (𝑥) ≤ 𝑓𝑖 (𝑤), 𝑖 = 1, … , 𝑚, and at least one has a smaller value, i.e., 𝑓 (𝑥) ≠ 𝑓 (𝑤). As a consequence, it holds for an eﬃcient solutions
that there is no feasible point such that one can improve the value of any of the objective functions without deteriorating at least one of the others.
However, one can take additional knowledge on the preferences on a possible trade-off between the objective functions into account which has already been done in vector optimization for a long time. For in- stance, one might not be willing to accept a slight improvement for one function value if this implies a huge worsening of another objec- tive function value. Then such information on the preferences for an allowed trade-off can be modeled mathematically by an ordering cone, see Hunt and Wiecek (2003). In that case, one needs to define eﬃciency by using a more general (antisymmetric) partial ordering ≤. We use here the notion of a partial ordering in a linear space according to the book Jahn (2011). Any such antisymmetric partial ordering can be rep-
resented by a pointed convex cone 𝐾 ⊆ ℝ𝑚, then called ordering cone,
by
𝑦 ≤𝐾 𝑧	⇔	𝑧 − 𝑦 ∈ 𝐾.
Recall that a nonempty set 𝐾 ⊆ ℝ𝑚 is a convex cone if 𝜆(𝑥 + 𝑦) ∈ 𝐾 for all 𝜆 ≥ 0, 𝑥, 𝑦 ∈ 𝐾 , and 𝐾 is a pointed convex cone if additionally 𝐾 ∩ (−𝐾) = {0}. Then a point 𝑥̄ ∈ 𝑀 is an eﬃcient solution for min𝑥∈𝑀 𝑓 (𝑥)
w.r.t. a partial ordering defined by 𝐾 if
({𝑓 (𝑥̄)} − 𝐾 ) ∩ 𝑓 (𝑀 ) = {𝑓 (𝑥̄)}.
the cone ℝ𝑚. Such a modeling with more general ordering cones is a Thus one can model problems with more general ordering cones than
topic of Wiecek (2007), in which Wiecek also discusses the concept of variable domination which goes back to Yu (1974). Variable ordering structures can be defined in several ways. For instance, for a set-valued
map Q ∶ ℝ𝑚 → 2ℝ𝑚 with Q(𝑦) a pointed convex cone for all 𝑦 ∈ ℝ𝑚 one
can define for 𝑦, 𝑧 ∈ ℝ𝑚
𝑦 ≤Q 𝑧	∶⇔	𝑧 − 𝑦 ∈ Q(𝑦)	⇔	𝑧 ∈ {𝑦} + Q(𝑦).
The binary relation ≤Q is in general reflexive, but not transitive, not antisymmetric, and not compatible with addition or nonnegative scalar multiplication, see Lemma 1.10 in the book Eichfelder (2014b) on this topic. Such non-transitive binary relations are important for modeling
so far ℝ𝑚 or 𝐾, depends on 𝑦, i.e., it is Q(𝑦). For instance, in intensity applications when the set of less preferred directions in the image space,
modulated radiotherapy a slight worsening of the dose value delivered to one organ, in the case one is still much below some bound, can be accepted for a significant improvement in another organ. But there are regions of values for the dose where a slight worsening of the value would no longer be acceptable as it would imply a strong impact on that organ, cf. Eichfelder (2014b). For modeling such a situation, bounded
set Q(𝑦), instead of cones, are required. In the literature, a huge progress
on theoretical results for variable ordering structures can be seen. First
numerical algorithms, as in Hirsch et al. (2011) by Hirsch, Shukla and Schmeck and in Eichfelder (2014a) by Eichfelder, have already been proposed.
optimization was extended already from its first days from ℝ𝑚 as crite- Next to extensions to more general ordering concepts, multiobjective rion space to general linear spaces 𝑌 as image space, see for instance
the book Jahn (2011). Then another issue can be that many theoretical results use the topological interior of the ordering cone. For instance, above we define the weakly eﬃcient elements by using the interior of
ℝ𝑚. But in many topological linear spaces ordering cones are of interest
which have an empty interior, then called non-solid. Theoretical results
for such problems can be found, for instance, in the paper Durea et al., 2010 by Durea, Dutta and Tammer.
lems with objective map 𝐹 ∶ 𝑀 → 2𝑌 for some feasible set 𝑀 and a In addition, in the last twenty years the extension to set-valued prob- linear space 𝑌 have become more important, i.e., the extension to opti-
mization problems
min 𝐹 (𝑥).	(SOP)
𝑥∈𝑀
Section 6 and Section 7 give examples where such set-valued optimiza- tion problems, called set optimization problems, play an important role.
A first optimality notion for (𝑆𝑂𝑃 ) is obtained by directly transfer-
tor approach. We demonstrate this here for 𝑌 = ℝ𝑚 and ordering cone ring the concepts from multiobjective optimization. It is called the vec- ℝ𝑚. For that, define a set 𝑆 by
𝑆 =	𝐹 (𝑥)
𝑥∈𝑀
and determine a nondominated element 𝑦̄ of 𝑆, i.e., 𝑦̄ ∈ 𝑆 with
{𝑦̄} − ℝ𝑚 ∩ 𝑆 = {𝑦̄}. Then any pair (𝑥̄, 𝑦̄) with 𝑦̄ ∈ 𝐹 (𝑥̄), 𝑥̄ ∈ 𝑀 , is called a minimizer for (𝑆𝑂𝑃 ), cf. (Jahn, 2011, Definition 14.2). We il-
lustrate this concept with an example.
Example 8.1. Let 𝑀 = {1, 2, 3} and 𝐹 ∶ 𝑀 → 2ℝ2 be defined by
𝐹 (1)	=	conv{(1, 1)⊤, (0.5, 3)⊤},
𝐹 (2)	=	conv{(1, 1)⊤, (3, 2)⊤},
𝐹 (3)	=	conv{(1, 1)⊤, (3, 0.5)⊤}.
Then for 𝑦̄ ∶= (1, 1)⊤ all pairs (𝑥, 𝑦̄) with 𝑥 ∈ 𝑀 , and all pairs (3, 𝑦) with
𝑦 ∈ 𝐹 (3), are minimizers for (𝑆𝑂𝑃 ).

less order relation which allow to compare the sets 𝐹 (𝑥) for 𝑥 ∈ 𝑀 as However, as discussed in Section 6, there are concepts as the lower- a whole, and not represented by one element 𝑦̄. Then one can define that 𝑥̄ ∈ 𝑀 is a minimal solution for (𝑆𝑂𝑃 ) w.r.t. the lower-less order
relation if it holds
𝑥 ∈ 𝑀, 𝐹 (𝑥) ≼𝓁 𝐹 (𝑥̄) ⇒ 𝐹 (𝑥̄) ≼𝓁 𝐹 (𝑥),
cf. (Jahn, 2011, Definition 14.5). The above approach is called the set approach. We illustrate it on the set optimization problem from Example 8.1.
Example 8.2. We consider again the set 𝑀 and the set-valued map 𝐹 from Example 8.1. The points 𝑥 = 1 and 𝑥 = 3 are minimal solutions for (𝑆𝑂𝑃 ) w.r.t. the lower-less order relation, but not 𝑥 = 2, as 𝐹 (1) ≼𝓁 𝐹 (2). but F2not ≼𝓁 𝐹 1.



Set-optimization with the set approach has been in the focus of the research in set optimization for about twenty years as it is con- sidered to be more practically relevant than the vector approach. Also for this set approach for set optimization, variable ordering structures have been examined, see for instance Köbis, 2017 and Eichfelder and Pilecka, 2016. Moreover, scalarizations for set optimization problems have been proposed, even if the ordering cones have an empty in- terior, see, for instance, Gutiérrez, Jiménez, Miglierina and Molho, Gutiérrez et al. (2015). In addition, relations between the optimal so- lution sets of (finite-dimensional) multiobjective optimization problems and set-optimization problems w.r.t. the set approach have been ex- plored by Eichfelder and Rocktäschel (2021) based on support function- als for convex-valued maps, and for nonconvex problems by Eichfelder, Quintana and Rocktäschel in Eichfelder et al., 2021b.
To sum up this survey, a huge progress can be seen in multiobjec- tive optimization, and many new approaches, fields, and applications have been explored in the last twenty years. However, there are still many open research questions, and significant improvements in solu- tion procedures for many problem classes are required. We suggest that it might be a promising approach to develop algorithms which are not based on a scalarization. Moreover, it is important that the developed algorithms are provided to the public such that multiobjective optimiza- tion techniques are used more widely for solving application problems in practice.
Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgements

The author thanks the two anonymous referees for their very care- ful reading and their helpful comments on the first version of this manuscript. Moreover, the author thanks Dr. Tobias Gerlach, Dr. Ernest Quintana, and M.Sc. Leo Warnow for their valuable comments on an ear- lier version of this manuscript. The research did not receive any specific grant from funding agencies in the public, commercial, or not-for-profit sectors.
References

Alzorba, S., Günther, C., Popovici, N., Tammer, C., 2015. A special class of extended mul- ticriteria location problems. Optimization 64 (5), 1305–1320.
Ansari, Q.H., Yao, J.-C., 2012. Recent Developments in Vector Optimization. Springer. Bai, Y., Guo, C., 2014. Doubly nonnegative relaxation method for solving multiple objec-
tive quadratic programming problems. J. Ind. Manag. Optim. 10 (2), 543–556.
Belotti, P., Soylu, B., Wiecek, M.M., 2016. Fathoming rules for biobjective mixed integer linear programs: Review and extensions. Discrete Optimization 22, 341–363.
Boland, N., Charkhgard, H., Savelsbergh, M., 2015. A criterion space search algorithm for biobjective integer programming: The balanced box method. INFORMS J. Comput. 27 (4), 735–754.
Boland, N., Charkhgard, H., Savelsbergh, M., 2015. A criterion space search algorithm for biobjective mixed integer programming: The triangle splitting method. INFORMS J. Comput. 27 (4), 597–808.
Boland, N., Charkhgard, H., Savelsbergh, M., 2016. The l-shape search method for triob- jective integer programming. Math. Program. Comput. 8 (2), 217–251.
Bonnel, H., Morgan, J., 2006. Semivectorial bilevel optimization problem: Penalty ap- proach. J. Optim. Theory Appl. 131 (3), 365–382.
Borndörfer, R., Schenker, S., Skutella, M., Strunk, T., 2016. PolySCIP. mathematical soft- ware. In: Greuel, G.-M., Koch, T., Paule, P., Sommese, A. (Eds.), roceedings of ICMS 2016, Vol. 9725, pp. 259–264. Lecture Notes in Computer Science
Botte, M., Schöbel, A., 2019. Dominance for multi-objective robust optimization concepts.
Eur. J. Oper. Res. 273 (2), 430–440.
Bouza, G., Quintana, E., Tammer, C., 2019. A unified characterization of nonlinear scalar- izing functionals in optimization. Vietnam Journal of Mathematics 47, 683–713.
Branke, J., Deb, K., Miettinen, K., Slowinski, R., 2008. Multiobjective Optimization. Inter- active and Evolutionary Approaches. Springer.
Burachik, R.S., Kaya, C.Y., Rizvi, M.M., 2014. A new scalarization technique to approxi- mate pareto fronts of problems with disconnected feasible sets. J. Optim. Theory Appl. 162 (2), 428–446.
Burachik, R.S., Kaya, C.Y., Rizvi, M.M., 2017. A new scalarization technique and new algorithms to generate pareto fronts. SIAM J. Optim. 27 (2), 1010–1034.
Burachik, R.S., Kaya, C.Y., Rizvi, M.M., 2021. Algorithms for generating pareto fronts of multi-objective integer and mixed-integer programming problems. Engineering Opti- mization doi:10.1080/0305215X.2021.1939695.
Cabrera-Guerrero, G., Ehrgott, M., Mason, A.J., Raith, A., 2021. Biobjective opti- misation over a set of convex sub-problems. Annals of Operations Research doi:10.1007/s10479-020-03910-3.
Csirmaz, L., 2021. Inner approximation algorithm for solving linear multiobjective opti- mization problems. Optimization 70 (7), 1487–1511.
Custodio, A., Madeira, J.F.A., Vaz, A.I.F., Vicente, L.N., 2011. Direct multisearch for mul- tiobjective optimization. SIAM J. Optim. 21 (3), 1109–1140.
Dächert, K., Klamroth, K., Lacour, R., Vanderpooten, D., 2017. Eﬃcient computation of the search region in multi-objective optimization. Eur. J. Oper. Res. 260, 841–855.
De Santis, M., Eichfelder, G., 2021. A decision space algorithm for multiobjective convex quadratic integer optimization. Computers & Operations Research 134, 105396.
De Santis, M., Eichfelder, G., Niebling, J., Rocktäschel, S., 2020. Solving multiobjective mixed integer convex optimization problems. SIAM J. Optim. 30 (4), 3122–3145.
Deb, K., 2001. Multi-Objective Optimization using Evolutionary Algorithms. Wiley.
Deb, K., Abouhawwash, M., Dutta, J., 2015. An optimality theory based proximity measure for evolutionary multi-objective and many-objective optimization. Lecture Notes in Computer Science, 9019. Springer.
Deb, K., Sinha, A., 2010. An eﬃcient and accurate solution methodology for bilevel multi-
-objective programming problems using a hybrid evolutionary-local-search algorithm. Evolutionary Computation 18 (3), 403–449.
Dempe, S., 2012. Foundations of bilevel programming. Kluwer.
Dempe, S., Mehlitz, P., 2020. Semivectorial bilevel programming versus scalar bilevel programming. Optimization 69 (4), 657–679.
Doolittle, E.K., Kerivin, H.L.M., Wiecek, M.M., 2018. Robust multiobjective optimization with application to internet routing. Ann Oper Res 271, 487–525.
Dörfler, D., Löhne, A., Schneider, C., Weißing, B., 2021. A benson-type algorithm for bounded convex vector optimization problems with vertex selection. Optimization Methods and Software doi:10.1080/10556788.2021.1880579.
Drummond, L., Svaiter, B., 2005. A steepest descent method for vector optimization. J. Comput. Appl. Math. 175, 395–414.
Durea, M., Dutta, J., Tammer, C., 2010. Lagrange multipliers for 𝜀–pareto solutions in
vector optimization with non-solid cones in banach spaces. J. Optim. Theory Appl. 145, 196–211.
Edgeworth, F. Y., 1881. Mathematical psychics. Kegan Paul, London. Ehrgott, M., 2005. Multicriteria Optimization. Springer.
Ehrgott, M., Gandibleux, X., 2007. Bound sets for biobjective combinatorial optimization problems. Computers & Operations Research 34 (9), 2674–2694.
Ehrgott, M., Ide, J., Schöbel, A., 2014. Minmax robustness for multi-objective optimization problems. Eur. J. Oper. Res. 239, 17–31.
Ehrgott, M., Shao, L., Schöbel, A., 2011. An approximation algorithm for convex multi-
-objective programming problems. J. Global Optim. 50, 397–416.
Eichfelder, G., 2008. Adaptive Scalarization Methods in Multiobjective Optimization.
Springer.
Eichfelder, G., 2009. Scalarizations for adaptively solving multi-objective optimization problems. Computational Optimization and Applications 44 (2), 249–273.
Eichfelder, G., 2010. Multiobjective bilevel optimization. Mathematical Programming, Ser. A 123 (2), 419–449.
Eichfelder, G., 2014. Numerical procedures in multiobjective optimization with variable ordering structures. J. Optim. Theory Appl. 162 (2), 489–514.
Eichfelder, G., 2014. Variable Ordering Structures in Vector Optimization. Springer.
Eichfelder, G.Dempe, S., Zemkoho, A. (Eds.), 2020. Methods for multiobjective bilevel optimization. Chapter in Bilevel Optimization — Advances and Next Challenges.
Eichfelder, G., Groetzner, P., 2021. A note on completely positive relaxations of quadratic problems in a multiobjective framework. Optimization Online, http://www.optimization-online.org/DB_HTML/2021/01/8230.html.
Eichfelder, G., Jahn, J., 2016. Vector and set optimization. In: Greco, S. (Ed.), Multiple Cri- teria Decision Analysis. State of the Art Surveys. Springer, Heidelberg, pp. 695–737.
Eichfelder, G., Kirst, P., Meng, L., Stein, O., 2021. A general branch-and-bound framework for continuous global multiobjective optimization. J. Global Optim. 80, 195–227.
Eichfelder, G., Krüger, C., Schöbel, A., 2017. Decision uncertainty in multiobjective opti- mization. J. Global Optim. 69 (2), 485–510.
Eichfelder, G., Niebling, J., Rocktäschel, S., 2020. An algorithmic approach to multiobjec- tive optimization with decision uncertainty. J. Global Optim. 77, 3–25.
Eichfelder, G., Pilecka, M., 2016. Set approach for set optimization with variable ordering structures part II: Scalarization approaches. J. Optim. Theory Appl. 171 (3), 947–963. Eichfelder,  G.,  Quintana,  E.,  Rocktäschel,  S.,  2021b.  A  vectorization scheme  for  nonconvex  set  optimization  problems.  Optimization  Online,
http://www.optimization-online.org/DB_HTML/2021/07/8510.html.
Eichfelder, G., Rocktäschel, S., 2021. Solving set-valued optimization problems using a multiobjective approach. Optimization Online. http://www.optimization-online.org/DB_HTML/2021/02/8243.html.
Eichfelder, G., Warnow, L., 2020. An approximation algorithm for multi- objective optimization problems using a box-coverage. Optimization Online. http://www.optimization-online.org/DB_HTML/2020/10/8079.html.
Eichfelder, G., Warnow, L., 2021a. A hybrid patch decomposition approach to compute an enclosure for multi-objective mixed-integer convex optimization problems. Optimiza- tion Online, http://www.optimization-online.org/DB_HTML/2021/08/8541.html.
Eichfelder, G., Warnow, L., 2021. Proximity measures based on KKT points for constrained multi-objective optimization problems. J. Global Optim. 80, 63–86.
Fernández, J., Tóth, B., 2009. Obtaining the eﬃcient set of nonlinear biobjective optimiza- tion problems via interval branch-and-bound methods. Computational Optimization and Applications 42, 393–419.



Fliege, J., 2006. An eﬃcient interior-point method for convex multicriteria optimization problems. Mathematics of Operations Research 31 (4), 825–845.
Fliege, J., Drummond, M.G., Svaiter, B.F., 2009. Newton’s method for multicriteria opti- mization. SIAM J. Optim. 20 (2), 602–626.
Fliege, J., Svaiter, B., 2000. Steepest descent methods for multicriteria optimization. Math.
Methods of Oper. Res. 51, 479–494.
Fliege, J., Vaz, I.A., 2016. A method for constrained multiobjective optimization based on SQP techniques. SIAM J. Optim. 26 (4), 2091–2119.
Fliege, J., Vicente, L.N., 2006. Multicriteria approach to bilevel optimization. J. Optim.
Theory Appl. 131 (2), 209–225.
Fliege, J., Werner, R., 2013. Robust multiobjective optimization and applications in port- folio optimization. Eur. J. Oper. Res. 234, 422–433.
Gebhardt, E., Jahn, J., 2009. Global solver for nonlinear bilevel vector optimization prob- lems. Pac. J. Optim. 5 (3), 387–401.
Gebken, B., Peitz, S., 2021. An eﬃcient descent method for locally lipschitz multiobjective optimization problems. J. Optim. Theory Appl. 188, 696–723.
Göpfert, A., Riahi, H., Tammer, C., Zalinescu, C., 2003. Variational methods in partially ordered spaces. CMS Books in Mathematics.
Greco, S., Ehrgott, M., Figueira, J.R., 2016. Multiple Criteria Decision Analysis. State of the Art Surveys (2nd Edition). Springer.
Groetzner, P., Werner, R., 2021. Multiobjective optimization under uncer- tainty: A multiobjective robust (relative) regret approach. Eur. J. Oper. Res. doi:10.1016/j.ejor.2021.03.068.
Grüne, L., Stieler, M., 2019. Multiobjective model predictive control for stabilizing cost criteria. Discrete & Continuous Dynamical Systems - B 24 (8), 3905–3928.
Gutiérrez, C., Jiménez, B., Miglierina, E., Molho, E., 2015. Scalarization in set optimization with solid and nonsolid ordering cones. J. Global Optim. 61, 525–552.
Hirsch, C., Shukla, P.K., Schmeck, H., 2011. Variable preference modeling using multi-ob- jective evolutionary algorithms. In: Takahashi, R.H.C. (Ed.), Evolutionary Multi-Crite- rion Optimization 6th International Conference. Lecture Notes in Computer Science, Vol. 6576. Springer.
Hunt, B.J., Wiecek, M.M.Tanino, T., Tanaka, T., Inuiguchi, M. (Eds.), 2003. Cones to aid decision making in multicriteria programming. Multi-Objective Programming and Goal- Programming. T. Tanino, T. Tanaka, and M. Inuiguchi (eds.) 153–158.
Ide, J., Schöbel, A., 2016. Robustness for uncertain multi-objective optimization: a survey and analysis of different concepts. OR Spectrum 38, 235–271.
Jahn, J., 2011. Vector optimization: Theory. Applications, and Extensions, 2nd edn.
Springer, Heidelberg.
Jayasekara, P.L.W., Adelgren, N., Wiecek, M.M., 2020. On convex multiobjective programs with application to portfolio optimization. J. of Multi-Criteria Decision Analysis 27 (3-4), 189–202.
Kaya, C.Y., Maurer, H., 2014. A numerical method for nonconvex multi-objective optimal control problems. Computational Optimization and Applications 57, 685–702.
Khan, A., Tammer, C., Zǎlinescu, C., 2015. Set-valued Optimization — an Introduction with Applications. Springer, Heidelberg.
Klamroth, K., Köbis, E., Schöbel, A., Tammer, C., 2017. A unified approach to uncertain optimization. Eur. J. Oper. Res. 260, 403–420.
Klamroth, K., Lacour, R., Vanderpooten, D., 2015. On the representation of the search region in multi-objective optimization. Eur. J. Oper. Res. 245, 767–778.
Köbis, E., 2017. Set optimization by means of variable order relations. Optimization 66 (12), 1991–2005.
Krüger, C., Castellani, F., Geldermann, J., Schöbel, A., 2018. Peat and pots: An application of robust multiobjective optimization to a mixing problem in agriculture. Computers and Electronics in Agriculture 154, 265–275.
Krüger, C., Schöbel, A., Wiecek, M.M., 2017. The robustness gap for uncertain multiobjec- tive optimization. Preprint-Serie des Instituts für Numerische und Angewandte Math- ematik. Universität Göttingen
Kuhn, H.W., Tucker, A.W., 1951. Nonlinear programming. In: Neyman, J. (Ed.), Proceed- ings of the 2nd Berkeley Symposium on Mathematical Statistics and Probability. Uni- versity of California Press, Berkeley, pp. 481–492.
Kuroiwa, D., Lee, G.M., 2012. On robust multiobjective optimization. Vietnam Journal of Mathematics 40, 305–317.
Lacour, R., Klamroth, K., Fonseca, C.M., 2017. A box decomposition algorithm to compute the hypervolume indicator. Computers & Operations Research 79, 347–360.
Logist, F., Houska, B., Diehl, M., Impe, J.V., 2010. Fast pareto set generation for nonlin- ear optimal control problems with multiple objectives. Struct. Multidisc. Optim. 42, 591–603.
Löhne, A., 2011. Vector Optimization with Infimum and Supremum. Springer. In the series Vector Optimization
Löhne, A., Rudloff, B., Ulus, F., 2014. Primal and dual approximation algorithms for con- vex vector optimization problems. J. Global Optim. 60, 713–736.
http://www.bensolve.org
Löhne, A., Weißing, B., 2017. The vector linear program solver bensolve – notes on theo- retical background. Eur. J. Oper. Res. 260 (3), 807–813.
Loridan, P., 1984. 𝜖-solutions in vector minimization problems. J. Optim. Theory Appl.
43, 265–276.
Luc, D. T., 2016. Multiobjective Linear Programming. Springer.
Mavrotas, G., Diakoulaki, D., 1998. A branch and bound algorithm for mixed zero-one multiple objective linear programming. Eur. J. Oper. Res. 107 (3), 530–541.
Miettinen, K., 1999. Nonlinear multiobjective optimization. Kluwer.
Niebling, J., Eichfelder, G., 2019. A branch-and-bound based algorithm for nonconvex optimization problems. SIAM J. Optim. 29, 794–821.
Nishizaki, I., Sakawa, M., 1999. Stackelberg solutions to multiobjective two-level linear programming problems. J. Optim. Theory Appl. 103 (1), 161–182.
Özpeynirci, O., Köksalan, M., 2010. An exact algorithm for finding extreme supported nondominated points of multiobjective mixed integer programs. Managament Science 56 (12), 2302–2315.
Pardalos, P., Žilinskas, A., Žilinskas, J., 2017. Non-Convex Multi-Objective Optimization.
Springer.
Pascoletti, A., Serafini, P., 1984. Scalarizing vector optimization problems. J. Optim. The- ory Appl. 42 (4), 499–524.
Perini, T., Boland, N., Pecin, D., Savelsbergh, M., 2020. A criterion space method for biobjective mixed integer programming: The boxed line method. INFORMS J. Comput. 31 (1), 16–39.
Pareto, V., 1971. Manuale di Economia Politica Societa Editrice Libraria, Milano, 1906. English Translation: Pareto, V.: Manual of Political Economy, translated by Schwier,
A. S.. Augustus 1170 M.Kelley Publishers, New York
Pilecka, M., 2016. Set-valued optimization and its application to bilevel optimization.
Technische Universität Bergakademie Freiberg. PhD thesis.
Prinz, S., Thomann, J., Eichfelder, G., Boeck, T., Schumacher, J., 2021. Expensive multi-
-objective optimization of electromagnetic mixing in a liquid metal. Optimization and Engineering 22, 1065–1089.
Przybylski, A., Gandibleux, X., Ehrgott, M., 2010. A recursive algorithm for finding all non- dominated extreme points in the outcome set of a multiobjective integer programme. INFORMS J. Comput. 22, 371–386.
Przybylski, A., Klamroth, K., Lacour, R., 2019. A simple and eﬃcient dichotomic search algorithm for multi-objective mixed integer linear programs. ArXiv:1911.08937.
Ruuska, S., Miettinen, K., Wiecek, M.M., 2012. Connections between single-level and bilevel multiobjective optimization. J. Optim. Theory Appl. 153 (1), 60–74.
Ruzika, S., Wiecek, M.M., 2005. Approximation methods in multiobjective programming.
J. Optim. Theory Appl. 126, 473–501.
Ryu, J.-H., Kim, S., 2014. A derivative-free trust-region method for biobjective optimiza- tion. SIAM J. Optim. 24, 334–362.
Sauerteig, P., Worthmann, K., 2020. Towards multi-objective optimization and control of smart grids. Optimal Control - Applications and Methods (OCAM); special issue MPC for Energy Systems: Economic and Distributed Approaches 41, 128–145.
Shi, X., Xia, H., 1997. Interactive bilevel multi-objective decision making. J. Oper. Res.
Soc. 48 (9), 943–949.
Sinha, A., Malo, P., Deb, K., 2018. A review on bilevel optimization: From classical to evolutionary approaches and applications. IEEE Transactions on Evolutionary Com- putation 22 (2), 276–295.
Smale, S., 1975. Global analysis and economics: Pareto optimum and a generalization of morse theory. Synthese 31, 345–358.
Stidsen, T., Andersen, K.A., 2018. A hybrid approach for biobjective optimization. Discrete Optimization 28, 89–114.
Tammer, C., Weidner, P., 2020. Scalarization and separation by translation invariant func- tions: with applications in optimization. Nonlinear Functional Analysis, and Mathe- matical Economics. Springer.
Thomann, J., Eichfelder, G., 2019. A trust-region algorithm for heterogeneous multiob- jective optimization. SIAM J. Optim. 29 (2), 1017–1047.
Wiecek, M., 2007. Advances in cone-based preference modeling for decision making with multiple criteria. Decis. Mak. Manuf. Serv. 1, 153–173.
Yu, P.L., 1974. Cone convexity, cone extreme points and nondominated solutions in deci- sion problems wit multiobjectives. J. Optim. Theory Appl. 14, 319–377.
https://github.com/GEichfelder/ASMO (accessed May 17 2021)
https://www.mcdmsociety.org/content/software-related-mcdm-0(accessed  May  17
2021).
https://www.dagstuhl.de/de/programm/kalender/semhp/?semnr=20031(accessed May http://www.norg.uminho.pt/aivaz/MOSQP/(accessed May 17 2020)
17 2021).
https://github.com/lcsirmaz/inner(accessed May 17 2021).
http://www.mat.uc.pt/dms/(accessed May 17 2021),
https://github.com/GEichfelder/MOBO(accessed May 17 2021).
https://github.com/mariannadesantis/MOMIX(accessed May 17 2021).
https://project-flo.de/ (accessed May 17 2021),
https://www.scipopt.org/doc-3.2.0/applications/MultiObjective/(accessed	May	17
2021).
