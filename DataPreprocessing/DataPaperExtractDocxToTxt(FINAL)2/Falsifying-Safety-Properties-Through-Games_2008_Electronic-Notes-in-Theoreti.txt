

Electronic Notes in Theoretical Computer Science 223 (2008) 71–86
www.elsevier.com/locate/entcs

Falsifying Safety Properties Through Games on Over-approximating Models
Nathaniel Charlton1 and Michael Huth2
Department of Computing Imperial College London

Abstract
Abstractions of programs are traditionally over-approximations and have proved to be useful for the ver- ification of safety properties. They are presently perceived as being useless for the falsification of safety properties, i.e. showing that program execution definitely reaches a “bad” state. Alternative techniques, such as the computation of under-approximating must transitions, have addressed this shortcoming in the past. We show that over-approximating models can indeed falsify safety properties by relying on and ex- ploiting the seriality and partial determinism of programs: programs don’t just stop for no reason, and most program statements have deterministic semantics. Our method is based on solving a two-person attractor game derived from over-approximating models and makes no assumptions about the abstraction domain used. An example demonstrates the successful use of our approach, and highlights the role played by seri- ality and our handling of nondeterminism. Finally, we show that our method can encode must transitions, if supplied, by a simple modification of the ownership of nodes in the attractor game derived from the over-approximating model.
Keywords: software verification, games, falsification


Introduction
For over thirty years, over-approximating models have been used for verifying safety properties of programs. Intuitively an over-approximating model has all the be- haviours of the original program, and possibly many more; this is expressed by conditions such as trace-inclusion and simulation. Verification of safety properties is based on the following observation: because an over-approximating model has at least all the behaviours of the original program, any “bad” behaviours (i.e. those that violate the desired safety property) present in the program are also present in the model. Therefore, if the model contains no bad behaviours, the program does not either. Systems such as SLAM [1], BLAST [11] and our own system HECTOR

1 Email: nathaniel.charlton@imperial.ac.uk
2 Email: michael.huth@imperial.ac.uk

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.12.032

[5] have been developed which automatically extract over-approximating models from programs using abstraction.
However, the above scheme does not allow the falsiﬁcation of safety properties,
because bad behaviours found in the model need not be present in the original program – they may be “artifacts” introduced by the over-approximation process, and therefore “not feasible” in the original program. Approaches to the falsification of safety properties have focused on showing that abstract counterexamples are indeed feasible, for example by:
searching for a corresponding concrete counterexample (e.g. [15]),
proving the feasibility of the abstract counterexample path by satisfiability checking (e.g. [1]), or
adding under-approximation or calculation of “must-transitions” to the model (e.g. [9,10]).
Here we present a method of falsifying safety properties which uses only over- approximating models. In particular, our method doesn’t perform any of the tasks (i), (ii), (iii) above. Instead, our method is based on playing a two-player game over the transition graph of the over-approximating model, and exploits two properties of programs: seriality (execution of a program does not just “stop” for no reason) and what we call partial determinism. By this we mean that most program statements are deterministic, so any nondeterminism in the program or its instrumentation is confined to a small number of identifiable locations.
The rest of the paper is structured as follows: Section 2 sets out the notions of programs, models and safety properties we use. Section 3 shows how to use two- player games to falsify safety properties without performing any of the usual tasks listed above. Section 4 presents an illuminating example of falsification. Section 5 shows how our approach supports the easy incorporation of must information when it is available. Section 6 concludes and discusses related work.

Background
Programs and their semantics.
We begin by setting out the kinds of programs, models and safety properties we deal with in this paper. It will be seen that our setup is very general.
In this paper, we work with programs expressed as control flow graphs (CFGs). Figure 1 shows two simple such programs, drawn with unfilled nodes. Formally, each control flow graph is encoded by giving a set of locations Locs, which includes
an element start, and a function E : Locs → Edges mapping each location l to the
single (hyper)edge leaving it. The allowed forms of edges are:
Conditional (hyper)edges: if(Φ) : l1 : l2 . These transfer control to location l1 if the condition Φ holds, and to l2 otherwise.
Edges for ordinary statements: f : l . These execute the statement f and transfer control to location l.

Choice (hyper)edges: choice : l1 : l2 . These represent nondeterministic choice, and (in a sense) transfer control to both l1 and to l2.
(Here Φ is a guard condition, and Φ) ⊆ State denotes the set of states in which Φ holds. In this paper we assume nothing about guard conditions Φ or −).)
To give semantics to our programs, we assume a set State of program states, including an initial state sinit in which execution begins. We also assume that for each ordinary statement f, an associated transfer function f : State → State is given.
Note that we work at a high level of abstraction, assuming nothing about the nature of the state space State and the transfer functions f , so that for instance our results apply equally to languages with heaps as to those without.
As is customary, we put a transition system semantics onto programs. A pro- gram’s transition system has state space Locs × State, whose elements we call con- figurations. The transition relation −→ ⊆ (Locs × State) × (Locs × State) is given by the following (named) rules:
ord



choice-1 choice-2 if-true if-false
E(l) = f : l'


(l, s) −→ (l',f (s))
E(l) = choice : l1 : l2 (l, s) −→ (l1, s)
E(l) = choice : l1 : l2 (l, s) −→ (l2, s)
E(l) = if(Φ) : l1 : l2	s ∈ Φ)
(l, s) −→ (l1, s)
E(l) = if(Φ) : l1 : l2	s ∈/ Φ)
(l, s) −→ (l2, s)

We say that a configuration (l, s) is reachable if there exists a sequence (l1, s1) −→

··· →−
(lk, sk) such that (l1, s1) = (start, sinit) and (lk, sk) = (l, s).

We stated in the introduction that our development will depend on seriality and partial determinism, so we establish a lemma for these, which looks fairly innocuous but will be crucial later.
Lemma 2.1 Seriality and partial determinism of −→. The concrete transition relation −→ is
serial, i.e. for all conﬁgurations (l, s), there exists a conﬁguration (l', s') such that (l, s) −→ (l', s').
partially deterministic, i.e. for all conﬁgurations (l, s) with E(l) not of form
choice : l1 : l2 , if (l, s) −→ (l', s') and (l, s) −→ (l'', s'') then (l', s') = (l'', s'').
Proof. Consider an arbitrary configuration (l, s). Then E(l) has three possible forms:

E(l) = f : l1.  For seriality, put (l', s') := (l1,f (s)) and the ord rule gives (l, s) −→ (l', s'). For partial determinism, simply note that no other rule is applicable.
E(l) = if(Φ) : l1 : l2.  Either s ∈  Φ) or s ∈/  Φ).  If s ∈  Φ) then for
seriality, put (l', s') := (l1, s) and the if-true rule gives (l, s) −→ (l', s'); for
partial determinism note that no other rule is applicable.  If s ∈/  Φ) then
for seriality, put (l', s') := (l2, s) and the if-false rule gives (l, s) −→ (l', s'); for
partial determinism note that no other rule is applicable.
E(l) = choice : l1 : l2. For seriality, put (l', s') := (l1, s) and the choice-1 rule gives (l, s) −→ (l', s'). For partial determinism there is nothing to check.



Abstraction domains.
Our abstract models of programs will be built from abstraction domains. An abstraction domain in this paper will consist of a set A of abstract values, and a concretisation function γ : A → P(State) which gives meaning to the abstract values. This very general formulation is all that is needed in this paper, and so our results apply to arbitrary abstraction domains, though in practice an abstraction domain comes with more components, such as abstract successor functions (see e.g. [14,7]). Our development here also applies to the analysis modules presented in [4,3].


Model checking queries.
In this paper, we will consider a particular type of safety property: our queries will be expressed by giving a set B ⊆ Locs × State of “bad” configurations, and asking whether any of the bad configurations are reachable in the program. If no b ∈ B is reachable, then the safety property represented by B is true (which we will abbreviate to “B is true”). On the other hand if some b ∈ B is reachable, then the safety property represented by B is false (which we will abbreviate to “B is false”). We will not go into the issue of how one abstractly represents such a set B.
Definition 2.2 Abstract (over-approximating) models. An abstract model M = (N, −a−b→s ) for a given program, and built using the abstraction domain (A, γ), consists of a set N ⊆ Locs ×A of abstract nodes, and an abstract transition relation

−a−b→s
⊆ N × N , satisfying the following healthiness conditions:

H1 If (l, a) −a−b→s
H2 If (l, a) −a−b→s
(l', a') then E(l) must be some edge with l' as a target. (l', a') and E(l) is a choice edge, then a' = a.

H1 ensures that transitions in the model only occur between locations that are connected by CFG edges, so that the transition structure of the model falls into line with the structure of the CFG. H2 ensures that choice edges are treated simply as junctions, by not allowing the abstract value to change across a choice edge. (No such restriction is made for conditionals because we want to allow abstract values




Fig. 1. The unfilled nodes, and their associated transitions, show the control flow graphs of two simple pro- grams. The attached filled nodes, and their associated (green) transitions, depict sound default augmented models for the programs, built using sign analysis. Our method establishes that both programs reach their error states, unlike a conventional treatment of must transitions e.g. [9].

to become more precise across a conditional edge, as a result of incorporating the information that the guard holds (respectively does not hold).)	 
Figures 1 and 2 (page 10) are examples of such abstract models, drawn using the filled nodes (ignoring the P/F annotations for now). These models are built using a sign analysis, an abstraction domain which tracks only the sign of each variable,
i.e. whether it is negative, zero or positive, and discards all other information.
Definition 2.3 Soundness of abstract models. An abstract model M as above is said to be sound if the following, standard simulation-type, condition holds:
S1 Let (l, a) ∈ N and (l, s) be such that s ∈ γ(a). Let (l, s) −→ (l', s'). Then there

exists (l', a') ∈ N such that (l, a) −a−b→s
(l', a') and s' ∈ γ(a').	 

It can be seen that the models in Figure 1 are sound. Verification of safety properties, in the standard way described in the introduction, can be performed on the basis of S1; we will not dwell on this as we concentrate on falsification here.

Lemma 2.4 Seriality of
−a−b→s . The abstract transition relation −a−b→s
is serial in

sound models, in the sense that for all nodes n = (l, a) ∈ N, provided γ(a) is

nonempty there exists a node n' such that n −a−b→s
n'.

Proof. Let s ∈ γ(a). By Lemma 2.1 (seriality) there exists (l', s') such that (l, s) −→
(l', s'). Applying S1 completes the proof.	 

Our games on over-approximating models
We now show how to use two-player attractor games to falsify safety properties. The intuition of what follows is that, given a safety property B, we are going to play a

two-person game, where the positions are the nodes n ∈ N of an over-approximating model.
We call the players F and P: player F is trying to Falsify (i.e. reach) B, and player P is trying to Prevent this from happening. A play in this game consists of a sequence of moves which generate a sequence of nodes. Plays may start at any position n ∈ N . A move at position n in a play means choosing n' such that

n −a−b→s
n'; node n' becomes the new position of that play.

An extra function ρ determines which player is to move at each position. We say that player F wins from position n if he can ensure that all plays beginning in position n reach a position m = (l, a) where for all s ∈ γ(a), we have (l, s) ∈ B, i.e. all concrete configurations represented by m are “bad”. In other words, player F wins all those positions n from which he can force all plays into “bad” positions.
Definition 3.1 Augmented models An augmented model M + = (N, −a−b→s , ρ) consists of an abstract model (N, −a−b→s ) along with a function ρ : N → {F, P}. 
Of course, we cannot just use any old partition of the nodes among the two players. The following definition sets out what we require from such a partition. (In A1 the quantifier pattern is ∀∃, and in A2 it is ∃∃, which is reminiscent of the relations R∀∃ and R∃∃ from [8]. In the proof of Theorem 3.3, A1 will give us exactly what we need at F’s nodes, and A2 will give us just what we need at P’s nodes.)
Definition 3.2 Soundness of augmented models. An augmented model M + = (N, −a−b→s , ρ) is said to be sound if (N, −a−b→s ) is sound and the following hold:

A1 Let ρ((l, a)) = F and s ∈ γ(a). Then for all (l', a') such that (l, a) −a−b→s
there exists s' ∈ γ(a') such that (l, s) −→ (l', s').
(l', a'),

A2 Let ρ((l, a)) = P and s ∈ γ(a). Then there exists (l', a') and s' ∈ γ(a') such

that (l, a) −a−b→s
(l', a') and (l, s) −→ (l', s').	 

The models in Figure 1 and Figure 2 (page 10) are sound augmented models, with the ρ function depicted with P/F annotations at each abstract node.
To perform falsification from a node n, we demonstrate that player F wins the attractor game (as described above) from that node. To that end we use a judgement Hn which means that the abstract node (or game position) n ∈ N is “Hopeless” with respect to the set B of bad configurations, i.e. that once execution reaches n there is no hope for avoiding forever the set of bad configurations B. In terms of the attractor game, the intent of this judgement is to express that player F wins position n. The named derivation rules for this judgement are as follows.
h-all-bad
{(l, s) | s ∈ γ(a)}⊆ B
H(l, a)
h-P-move

ρ(n) = P	∀n' ∈ N, if n −a−b→s
Hn
n' then Hn'

h-F-move


ρ(n) = F	∃n' ∈ N such that n −a−b→s
Hn


n' and Hn'

The following theorem shows that what we are calling “hopeless” nodes really do inevitably lead to a bad configuration. Its subsequent corollary justifies falsification using sound augmented models and the judgement H.
Theorem 3.3 Let M + be a sound augmented model, and B a safety property. Let n = (l, a) ∈ N and s ∈ γ(a). If Hn then there exists (l1, s1) −→ · · · →− (lk, sk), with (l1, s1) = (l, s) and (lk, sk) ∈ B.
Proof. We proceed by structural induction on the derivation of Hn.
Base case: The base case is when Hn is derived by a single application of a rule. This can only be the h-all-bad rule. (At first glance it appears there is a possibility of using h-P-move if n has no successors, but since s ∈ γ(a), Lemma 2.4 shows this is impossible.) From the premises of h-all-bad and s ∈ γ(a), we have (l, s) ∈ B. Taking the one-element sequence (l, s) we are done.
Inductive case for h-P-move: From the premises of h-P-move we have ρ(n) = 

P . Applying A2, we see that there exists (l', a') such that (l, a) −a−b→s
(l', a'), and

there exists s' ∈ γ(a') such that (l, s) −→ (l', s'). Now, also from the premises of h-P-move, we see that H(l', a'). Applying the induction hypothesis and renumber- ing, we obtain a sequence (l2, s2),..., (lk, sk) such that (l2, s2) −→ · · · →− (lk, sk), with (l2, s2) = (l', s') and (lk, sk) ∈ B. Set (l1, s1) := (l, s) and we are done.
Inductive case for h-F-move: By the premises of h-F-move there exists n' =

(l', a') such that n
−a−b→s
n' and Hn'.  Also from the premises of h-F-move we

have ρ(n) = F . This means we can apply A1 to obtain an s' ∈ γ(a') such that (l, s) −→ (l, s'). Applying the induction hypothesis to n' = (l', s') and renumbering, there exists a sequence (l2, s2),..., (lk, sk) such that (l2, s2) −→ · · · →− (lk, sk), with (l2, s2) = (l', s') and (lk, sk) ∈ B. Set (l1, s1) := (l, s) and we are done.
 
Corollary 3.4 Falsification with sound augmented models. Let M + be a sound augmented model, and let B be a safety property. Let (start, a) ∈ N such that sinit ∈ γ(a). If H(start, a) then B is false.	 
The above is all well and good, but how do we obtain a sound augmented model for a program? Below we present a way to turn any sound over-approximating model (supporting verification only) into a sound augmented model (supporting falsification also). The construction is very easy, and simply assigns all abstract nodes corresponding to choice edges to player F, and all other nodes to player P.
Augmented models constructed in this way capture precisely the “must informa- tion” implicitly present in the original over-approximating model: at choice nodes all transitions are taken, and at other nodes some transition must be taken, as ex- ecution cannot simply stop. Such models allow, as we shall see, the falsification of

some safety properties, by means of solving an attractor game.
Definition 3.5 Let M be a sound model for a program. We define the default augmented model for M to be M + = (M, ρ) where

 F	if E(l) has the form choice : l' : l''

For example, the augmented models in Figures 1 and 2 are all default augmented models.	 
Theorem 3.6 Let M be a sound model. Then the default augmented model M + is a sound augmented model.
Proof.
Proof of A1: Let ρ((l, a)) = F and s ∈ γ(a).  Let also (l', a') be such that
(l, a) −a−b→s (l', a'). Since ρ((l, a)) = F , edge E(l) has form choice : l1 : l2 . By H1,
either l1 = l' or l2 = l'. If l1 = l' then the choice-1 rule gives us (l, s) −→ (l', s); on
the other hand if l2 = l' then the choice-2 rule provides the same conclusion. By H2, we have a' = a, whence s ∈ γ(a'). Putting s' := s we have found, as required, s' ∈ γ(a') such that (l, s) −→ (l', s').
Proof of A2: Let ρ((l, a)) = P and s ∈ γ(a). By Lemma 2.1 (seriality), there exists a configuration (l', s') such that (l, s) −→ (l', s'). By S1 (soundness) there

exists (l', a') ∈ N such that (l, a) −a−b→s
(l', a') and s' ∈ γ(a').



It is in the A2 part of the preceding proof that seriality played its key part. Theorem 3.3 can also be used to perform “conditional” falsifications, of the form “If execution ever reaches node n, then it will proceed to a configuration in B”.
To decide whether Hx for a particular node x (typically start), we simply apply the three rules for H over and over again, discovering more and more nodes n for which Hn, until either we have shown Hx, or no more applications of the rules are possible. This can be viewed as computing, in the underlying two-person game, as much of the attractor as is needed to determine whether it includes x.
Example 3.7 Using the augmented model in Figure 1 (left) we can prove that the program reaches the error state, i.e. we can falsify the safety property given by bad configurations B := {ERROR}× State. We begin by using the h-all-bad rule to establish H(ERROR, [x : zero]) and H(ERROR, [x : pos]). This reflects the fact that if execution reaches these nodes then clearly the safety property has been broken. Now we consider the node (1, [x : pos]), which is a node of player P. We have shown H for each of its −a−b→s -successors, so we can use the h-P-move rule to
get H(1, [x : pos]). This reflects the fact that although the abstraction used (here: sign analysis) cannot tell which way execution goes from the node (1, [x : pos]), it must go somewhere, and wherever it goes, the safety property will be broken. One

further application of h-P-move gives us H(start, zero), whence, by Corollary 3.4,
B is false, that is, execution reaches the error state.	 

The model in Figure 1 (right) allows a similar proof that ERROR is reached, but shows that uncertainty over which path execution takes through the program can be dealt with, as well as uncertainty over the values of the program’s variables.


In-depth example
Example 4.1 The program in Figure 2 generates an arbitrary natural number n and then computes n − 1 in y and the integer square root of n in x, that is, finds an integer value for x such that x2 ≤ n < (x + 1)2. The program is instrumented with a conditional which checks that the correct square root has been calculated, and transitions to the ERROR state if not. However, we have introduced a “mistake”: the guard for the square root computation part is the negation of what it should be.
Figure 2 includes a default augmented model constructed by a simple sign analy- sis, and this is enough to prove the program faulty. Describing the proof in terms of the game, Player F (the Falsifier) is in charge of the choice of which natural number n is generated. If player F plays so as to force a positive n to be generated, this wins the game; player P still has some choice of moves, because the sign analysis could not determine whether y becomes zero or positive, but whichever of these is taken, execution ends up at the ERROR node.	 

The preceding example only works because we distinguish F and P nodes, and use different rules for them; if P controlled the choice of n, he could force n = 0 and then ERROR is not reached. The example also illustrates the style in which we intend to deal with nondeterminism, which is needed to ensure that the program is tested over all inputs. Instead of using atomic nondeterministic statements such as havoc (e.g. [13]), we propose to encode them using small control flow graphs consisting of choice edges and deterministic statements, and then analyse these with over-approximation in the same way as the rest of the program. The game structure will take care of making sure that all the possible choices are explored.
When we perform verifications/falsifications in HECTOR [5], to which we have added an implementation of this approach, we put each piece of generating code into a procedure, which we call a generator procedure, which helps structure the instru- mentation process. Implementation issues, along with the treatment of procedures, are discussed in the upcoming thesis [6].
We have also used generator procedures with linked data structures, for example to generate all possible linked lists, which we use with models we build from a shape analysis (see [5]). We intend also to experiment with modelling nondeterministic memory allocation in this way.




Fig. 2. An example of a faulty program, and a default augmented model which proves that the error state is reached, thanks to our differing treatment of F and P nodes. See Example 4.1 for details.

Incorporation of must information
The PhD thesis [8] proposes the use of mixed transition systems (MTSs) as models which can both verify and falsify properties of programs. This is achieved by using two transition relations: a “may” transition relation, which over-approximates and

is like our −a−b→s , and a “must” transition relation −m−u−→st
which under-approximates.

In this section we show that our augmented models can neatly capture all the must information that is present in a MTS, while:
keeping the same node structure,
remaining sound for both verification and falsification, and
still only needing one transition relation.
This is achieved essentially by changing the player in charge of particular nodes, and works because of the way we have carefully isolated nondeterminism into the choice statement, which is used to build generator procedures. We begin by defining MTSs.
Definition 5.1 Mixed Transition Systems (MTSs). A Mixed Transition Sys- tem M† = (N, −a−b→s , −m−u−→st ) for a given program consists of an abstract model

M = (N, −a−b→s ) along with another transition relation −m−u−→st
the following healthiness conditions:
⊆ N × N satisfying

M1 If (l, a) −m−u−→st
M2 If (l, a) −m−u−→st
(l', a') then E(l) must be some edge with l' as a target.
(l', a') and E(l) is a choice edge, then a' = a.	 

We sometimes write M† = (M, −m−u−→st ) to emphasise that the MTS is obtained

from an abstract model M by adding −m−u−→st
transitions to it. As in [8], our definition

relaxes the requirement in [12] that all must transitions are also may transitions, but it also adds M1 and M2 as natural constraints for our program abstractions.
Definition 5.2 Soundness of MTSs. An MTS M† as above is said to be sound
if M is sound and the following condition holds:

S2 Let (l, a) ∈ N and (l, s) be such that s ∈ γ(a). Let (l, a) −m−u−→st
(l', a'). Then

there exists s' such that (l, s) −→ (l', s') and s' ∈ γ(a').	 
The next theorem, which we establish via a lemma, shows how MTSs can be used to falsify safety properties.
Lemma 5.3 Let M† be a sound MTS. Let there exist a sequence n1,..., nk ∈ N

(where each ni is (li, ai)) such that n1 −m−u−→st
n2 −m−u−→st
· · · −m−u−→st
nk. Let s ∈ γ(a1).

Then there exist s1,..., sk ∈ State such that s1 = s, sk ∈ γ(ak) and (l1, s1) −→ ... −→
(lk, sk).
Proof. We proceed by induction on k. The base case when k = 1 is trivial. For the inductive case, k > 1, let there exist a sequence n1,..., nk ∈ N (where each ni

is (li, ai)) such that n1 −m−u−→st
n2 −m−u−→st
· · · −m−u−→st
nk. Let s ∈ γ(a1).

Applying the induction hypothesis to the prefix n1,..., nk−1,  there exist
s1,..., sk−1 such that s1 = s, sk−1 ∈ γ(ak−1) and (l1, s1) −→ ... −→ (lk−1, sk−1).

Applying S2 to the transition nk−1
−m−u−→st
nk, there exists (lk, s') such that

(lk−1, sk−1) −→ (lk, s') and s' ∈ γ(ak). Putting sk := s' we are done.	 
Theorem 5.4 Falsification with MTSs. Let M† = (N, −a−b→s , −m−u−→st ) be a sound MTS for a program P. Consider a safety property expressed by a set B of bad conﬁgurations. To falsify B it is sufficient to ﬁnd a sequence n1,..., nk ∈ N with

n1 −m−u−→st
n2 −m−u−→st
· · · −m−u−→st nk

n1 = (start, a1) with sinit ∈ γ(a1), and
nk = (lk, ak), with {lk}× γ(ak) ⊆ B.
Proof. Let M† = (N, −a−b→s , −m−u−→st ) be a sound MTS for a program P . Let B be a set of bad configurations. Suppose there exists a sequence n1,..., nk ∈ N satisfying conditions (i), (ii), and (iii) of Theorem 5.4.
Due to (i) and (ii), we can apply the previous lemma (Lemma 5.3) to get a sequence of states sinit, s2,..., sk ∈ State such that (start, sinit) −→ (l2, s2) −→ ... −→ (lk, sk) and sk ∈ γ(ak). From (iii) we have (lk, sk) ∈ B, i.e. we have found an execution sequence starting at (start, sinit) and leading to the bad state (lk, sk). 
Next, as promised, we show how to simply construct an augmented model which neatly captures all the must information from an MTS, while keeping the same node structure as the underlying over-approximating model, remaining sound for both verification and falsification, and still only needing one transition relation.

The construction is simple, differing from the default augmented model in that, at any abstract node which has a must transition leaving it, we put player F in

charge, and replace the outgoing −a−b→s
edges with the provided outgoing −m−u−→st
edges.

This is sound due to the partial determinism of the concrete semantics −→.
Definition 5.5 Given an MTS M† = (M, −m−u−→st ) for an abstract model M  =

(N, −a−b→s ), we define the augmented model incorporating −m−u−→st
for M to be



M +[−m−u−→st ] = ( 
abs +	)


where (listing cases in order of priority):
N, −−→  ,ρ 

(l, a) abs + (l', a')	⇔	(l', a') ∈ T (l, a)



T (l, a) :=
{(l', a') | (l, a) −a−b→s
{(l', a') | (l, a) −m−u−→st
⎪⎩{(l', a') | (l, a) −a−b→s
(l', a')}	if E(l) has the form choice : l1 : l2
(l', a')}	if there exists n' ∈ N such that n −m−u−→st n'
(l', a')}	otherwise




ρ(l, a) :=
F	if E(l) has the form choice : l1 : l2
F	if there exists n' ∈ N such that n −m−u−→st n'
⎪⎩P	otherwise


The following theorem shows that, after incorporating must information, the augmented model is still sound for both verification and falsification.
Theorem 5.6 Let M† = (M, −m−u−→st ) be a sound MTS. Then M +[−m−u−→st ] is a sound augmented model.
Proof.

+
Proof of H1 and H2: First we must check that −−→
satisfies the healthiness

conditions H1 and H2; this comes easily from the corresponding conditions H1 and H2 for −a−b→s , and M1 and M2 for −m−u−→st .

Proof of S1: Now we must check that (
+
N, −−→ ) is actually a sound model,w

i.e. that it satisfies S1. (This part of the proof depends on the determinism of ordinary program statements.)
Let (l, a) ∈ N and (l, s) be such that s ∈ γ(a). Let (l, s) −→ (l', s'). If E(l) has the form choice : l1 : l2 or has no must transitions leaving it, then (l, s) has the same successors under abs + as it does under −a−b→s , and the conclusion follows from the S1 property of (N, −a−b→s ).
So suppose E(l) doesn’t have the form  choice : l1 : l2 , and there exists

n'' = (l'', a'') ∈ N such that n
−m−u−→st
n''.	By definition of abs + we have

abs +	''

''	''	''

''	''

n −−→	n . By S2, there exists s
such that (l, s) −→ (l ,s ) and s
∈ γ(a ). By

Lemma 2.1 (partial determinism), l' = l'' and s' = s''. Hence, putting a' := a'', we have found as required (l', a') ∈ N such that (l, a) abs + (l', a') and s' ∈ γ(a').
Proof of A1: Let ρ((l, a)) = F and s ∈ γ(a). Let (l', a') be such that (l, a) −a−b→s +
(l', a'). There are two situations in which we can have ρ((l, a)) = F .
The first situation is when E(l) has the form choice : l1 : l2 . By H1, either l1 = l' or l2 = l'. If l1 = l' then the choice-1 rule gives us (l, s) −→ (l', s); on the other hand if l2 = l' then the choice-2 rule provides the same conclusion. By H2, we have a' = a, whence s ∈ γ(a'). Putting s' := s we have found, as required, s' ∈ γ(a') such that (l, s) −→ (l', s').
The second situation is when E(l) doesn’t have the form  choice : l1 : l2 , and
+
there exists n'' = (l'', a'') ∈ N such that n −m−u−→st n''. By definition of abs	we

have (l, a) −m−u−→st
(l', a') (because here we chose the must edges). By S2, there

exists s' such that (l, s) −→ (l', s') and s' ∈ γ(a') and we are done.
Proof of A2: Let ρ((l, a)) = P and s ∈ γ(a). By Lemma 2.1 (seriality), there exists a configuration (l', s') such that (l, s) −→ (l', s'). By S1 (applied to (N, −a−b→s )),

there exists (l', a') ∈ N such that (l, a) −a−b→s
(l', a') and s' ∈ γ(a').  To finish,

note that by definition of abs + and the fact that (l, a)
−a−b→s
(l', a'), we have

(l, a) abs + (l', a') (because here we have chosen the ordinary abstract edges).

The next theorem, proved with the aid of the following lemma, confirms that, as promised, M +[−m−u−→st ] really does capture all the must information from the MTS (M, −m−u−→st ).
Lemma 5.7 Let M† = (N, −a−b→s , −m−u−→st ) be a sound MTS. Let B ⊆ Locs × State and let there exist a sequence n1,..., nk ∈ N (where each ni is (li, ai)) such that

n1 −m−u−→st
n2 −m−u−→st
··· −m−u−→st
nk and γ(a1) is nonempty. Let {lk}× γ(ak) ⊆ B. Then

Hn1 with respect to M +[−m−u−→st ] (the augmented model incorporating −m−u−→st ).

Proof. Write M +[−m−u−→st ] as ( 
abs +
). We proceed by induction on k. For the

base case, when k = 1, the condition {lk}× γ(ak) ⊆ B is exactly what is needed to invoke the h-all-bad rule to obtain Hn1.
For the inductive case, k > 1, let n1,..., nk ∈ N (where each ni is (li, ai))

be such that n1
nonempty.
−m−u−→st
n2 −m−u−→st
· · · −m−u−→st
nk and {lk} × γ(ak) ⊆ B and γ(a1) is

The set γ(a1) is nonempty so there exists s1 ∈ γ(a1). By S2, there exists some s2 in γ(a2) such that (l1, s1) −→ (l2, s2). Therefore γ(a2) is also nonempty, and we can apply the induction hypothesis to the suffix n2,..., nk ∈ N , obtaining Hn2. Because there is a must transition leaving n1 (i.e. the one to n2) it follows from the definition of M +[−m−u−→st ] that ρ(n) = F . We can use the h-F-move rule to complete


the proof if we can show
abs +
n1 −−→
n2.

There are two cases to check.
In the first case, E(l1) has form	choice : l' : l'' .  It follows from M2 that
a1 = a2.  From (l1, s1) −→ (l2, s2), using S1, there exists (l2, a') ∈ N such that

(l1, a1) −a−b→s
(l2, a'). By H2, a' = a1, and we already know a1 = a2. Thus we have

(l ,a ) −a−b→s
(l ,a ), i.e. n
−a−b→s
n . Finally, in the definition of abs + we choose

1	1	2	2	1	2
−−→

the ordinary abstract edges at n1, so we have
abs +
n1 −−→
n2.


abs +

In the second case, E(l1) has some other form, and in the definition of −−→

we choose the must edges at n1.  Thus the required n1
abs +
−−→	n2
follows from

n1 −m−u−→st
n2.


Theorem 5.8 Let P be a program and M† = (M, −m−u−→st ) a sound MTS for P. Consider any safety property expressed by a set B of bad conﬁgurations. If B is falsiﬁed by (M, −m−u−→st ) (using Corollary 5.4) then B is also falsiﬁed by the augmented model M +[−m−u−→st ] (using Theorem 3.3).
Proof. This follows easily from the previous lemma (Lemma 5.7); just note that the premises of Theorem 5.4 demand that sinit ∈ γ(a1), and this is what assures the nonemptiness of γ(a1) needed to invoke the lemma.	 
Informally, this theorem says that, for the kinds of safety properties and pro- grams considered in this paper, it is always worth transforming an MTS into one of our attractors game before using it for falsification: the transformation is easy to perform and the resulting game falsifies at the very least the same set of properties as the original MTS, and possibly more.
At first glance the reader may find Theorem 5.8 odd; since we have isolated nondeterminism into choice statements, one might think that no useful must infor- mation can exist for the other statement forms. Such an assessment is incorrect, however: consider an abstract node (l, a), where E(l) is an ordinary (and thus deter-
ministic) statement, with two must-successors (l', a1) and (l', a2). If we have neither
γ(a1) ⊆ γ(a2) nor γ(a1) ⊇ γ(a2) then the must edges are providing useful informa- tion. Even assuming perfect automated reasoning about the abstraction domain, it is not in general possible to replace (l', a1) and (l', a2) with a single successor without losing information, because the abstraction domain used need not contain any a' such that γ(a') = γ(a1) ∩ γ(a2).
We end this section with an unresolved question. When incorporating must information, we have shown that we obtain all the falsification power of the MTS from which the must edges come; we also know that the augmented model remains sound for both verification and falsification. But it remains to be seen what happens to the verification and falsification power of the default augmented model when additional must edges are incorporated; we can contrive situations where this power increases (as we would hope), decreases or remains unchanged, but do not have a

feeling for what will most frequently happen in practice.

Conclusions and related work
In this paper, we used a two-player attractor game to show that models which only over-approximate can nevertheless be used to falsify safety properties, that is, with- out using any under-approximation, feasibility checking or concrete counterexample search. To make this work, we focused on two properties of programs that are not accounted for in a conventional treatment of must transitions (e.g. [9]) namely seri- ality and partial determinism. Through Example 4.1 we demonstrated how and why our method works. Finally, we showed that if some must transitions are available, they can be incorporated into our approach very easily. We proved that by doing this, we obtain in a simple way all the falsification power of the must transition approach, and yet our models remain sound for both verification and falsification, retain the same node structure and still require only a single transition relation.


Related work
The present paper explores what generalised model checking [2], which effectively “case splits” on unknown propositions, means in the particular context of checking safety properties of programs. The existing works closest to ours, as far as we are aware, are [10] and [16], which also build models which can both verify and falsify properties.
In [10], which is specific to predicate abstraction domains, seriality is exploited but only for conditional statements (as in Figure 1 (right)), and not for ordinary
statements (needed for Figure 1 (left)). For ordinary statements, [10] uses must transitions to weaker tri-vector states. The “must hyper-transitions” used in [16] also capture seriality, though this is not the motivation given in [16] for introducing
them; rather, they are proposed as a way to make abstraction refinement monotonic. Both [10] and [16] require the use of two separate transition relations, whereas we need only one. Here we handle only safety properties expressed by giving a set of bad configurations, whereas [10,16] handle the much more expressive temporal logic CTL, and additionally address automatic abstraction refinement which we do not. We emphasise the expected role of generator procedures in our approach,
rather than atomic statements such as havoc, in producing more falsifications. Our
method subsumes the “choose-free-paths” technique from [15].

Acknowledgement
We thank Nir Piterman for his comments on a draft version of this paper. This research was in part supported by the UK EPSRC projects Efficient Speciﬁcation Pattern Library for Model Validation (EP/D50595X/1) and Complete and Efficient Checks for Branching-Time Abstractions (EP/E028985/1).

References
Ball, T. and S. K. Rajamani, “Automatically validating temporal safety properties of interfaces,” in:
Proc. of the 8th SPIN, LNCS 2057 (2001), pp. 103–122, Springer Verlag.
Bruns, G. and P. Godefroid, “Generalized model checking: Reasoning about partial state spaces,” in:
Proc. of the 11th CONCUR, LNCS 1877 (2000), pp. 168–182, Springer Verlag.
Charlton, N., “Program verification with interacting analysis plugins,” Formal Aspects of Computing (2006), Springer Verlag, DOI: 10.1007/s00165-007-0029-4.
Charlton, N., “Verification of Java programs with interacting analysis plugins,” Electronic Notes in Theoretical Computer Science 145, Proceedings of the 5th International Workshop on Automated Verification of Critical Systems (AVoCS 2005) (2006), pp. 131–150, Elsevier.
Charlton, N. and M. Huth, “HECTOR: software model checking with cooperating analysis plugins,” in: Proc. of the 19th CAV, LNCS 4590 (2007), pp. 168–172, Springer Verlag.
Charlton, N., “Cooperatively combining program verifiers: foundations and tool support,” Ph.D. thesis draft, University of London, Imperial College of Science, Technology and Medicine (July 2008)
Cousot, P. and R. Cousot, Abstract interpretation: a unified lattice model for static analysis of programs by construction or approximation of fixpoints, in: Conference Record of the 4th POPL (1977), pp. 238– 252, ACM Press.
Dams, D., “Abstract Interpretation and Partition Refinement for Model Checking,” Ph.D. thesis, Eindhoven University of Technology (1996).
Godefroid, P., M. Huth and R. Jagadeesan, “Abstraction-based model checking using modal transition systems,” in: Proc. of the 12th CONCUR, LNCS 2154 (2001), pp. 426–440, Springer Verlag.
Gurfinkel, A. and M. Chechik, “Why waste a perfectly good abstraction?,” in: Proc. of the 12th TACAS, LNCS 3920 (2006), pp. 212–226, Springer Verlag.
Henzinger, T. A., R. Jhala, R. Majumdar and G. Sutre, “Lazy abstraction,” in: Proc. of the 29th POPL
(2002), pp. 58–70, ACM Press.
Larsen, K. G. and B. Thomsen, “A modal process logic,” in: Proc. of the 3rd LICS (1988), pp. 203–210, IEEE Computer Society Press.
Leino, K. R. M. and F. Logozzo, “Loop invariants on demand,” in: Proc. of the 3rd Asian Symp. APLAS, LNCS 3780 (2005), pp. 119–134, Springer Verlag.
Nielson, F., H. R. Nielson and C. Hankin, “Principles of Program Analysis,” Springer-Verlag New York, Inc., Secaucus, NJ, USA, 1999.
Pasareanu, C. S., M. B. Dwyer and W. Visser, “Finding feasible counter-examples when model checking abstracted Java programs,” in: Proc. of the 7th TACAS, LNCS 2031 (2001), pp. 284–298, Springer Verlag.
Shoham, S. and O. Grumberg, “Monotonic abstraction-refinement for CTL,” in: Proc. of the 10th TACAS, LNCS 2988 (2004), pp. 546–560, Springer Verlag.
