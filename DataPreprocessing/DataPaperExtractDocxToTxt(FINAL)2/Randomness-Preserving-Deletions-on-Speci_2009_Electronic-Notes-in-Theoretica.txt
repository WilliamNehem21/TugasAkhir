

Electronic Notes in Theoretical Computer Science 225 (2009) 99–113
www.elsevier.com/locate/entcs

Randomness Preserving Deletions on Special Binary Search Trees
Michaela Heyer1 ,2
Department of Computer Science National University of Ireland Cork
Ireland

Abstract
Deletions in binary search trees are difficult to analyse as they are not randomness preserving. We will present a new kind of tree which differs slightly from the standard binary search tree. It will be referred to as an ordered binary search tree as it stores a history element in its nodes, which provides information about the order in which the nodes were inserted. Using this extra information it is possible to design a new randomness preserving and order preserving deletion algorithm.
Keywords: Binary Search Trees, Randomness Preservation, Knott’s Paradox


Introduction
Average-case analysis is the most challenging aspect of algorithm analysis and it is complicated further by the lack of randomness preservation as many have pointed out in the past ([5,6,11,1,10]). But what exactly do we mean by randomness preser- vation? In the context of binary search trees (BSTs) it is defined as follows: an operation is considered to be randomness preserving if, when applied to a random tree it will produce a random tree, i.e each possible tree structure is equally likely to arise. While BST insertions possess this special property, deletions in BSTs are not randomness preserving ([5,6,11,1]), a fact which was first discovered by Gary
D. Knott ([4]). When an algorithm is randomness preserving the general consensus is that average-case time analysis is feasible. That this is indeed the case has been formally demonstrated in [10] where Schellekens introduces a new programming language MOQA for which all programs are guaranteed to be randomness preserv-

1 Partially supported by the Centre for Efficiency Oriented Languages, Cork, Ireland
2 Email: mh4@cs.ucc.ie

1571-0661/© 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.12.069

ing and as a result give rise to recurrence equations for the average-case time in a (semi-) automated fashion.
Binary search trees and in particular the deletion on BSTs have been an active topic of research for a long time. One of the main researchers in this area was T.N. Hibbard and when he started his work in this field in 1962 he believed that BST deletions could be considered to be randomness preserving ([2]). It was only a few years later in 1975 that he was contradicted by Knott, who came up with what is now known as ‘Knott’s paradox’ ([4]). Knott was the first to discover the pitfall with deletions in BSTs: they appear to be randomness preserving at first sight, as any number of deletions indeed preserve the property. However, once we have a number of deletions followed by a number of insertions, followed by more deletions the preservation of randomness is not necessarily guaranteed any longer (see [3,6]). Since this discovery the question of the existence of a truly randomness preserving deletion algorithm for BSTs has been posed by Knuth ([6]) and many have tried to find an answer. Seidel and Aragon in particular claim to be the first ones to have discovered such an algorithm, which when applied to their special tree, the randomized search treap, preserves randomness ([12]). Treaps are very similar to BSTs but they store a priority value in addition to the key value. The items are arranged in the tree with the keys adhering to the BST property and the priorities adhering to heap-order, i.e. the priority of any given node in the tree is smaller than or equal to that of its parent. For a randomized treap it is assumed that all priorities are independent, identically distributed random variables through which the randomness is introduced. The insertion algorithm is quite similar to the normal BST insertion algorithm but it requires a left/right rotation to ensure the heap- order priority of the nodes. Deleting an element is basically a backwards version of the insertion and both algorithms take O(log n) as do all update operations for this data structure. Even though treaps provide good performance, they do not actually answer Knuth’s long standing question, as randomness is created on each insertion rather than truly preserved.
We will here present another variant of BST, the ordered binary search tree
(OBST) and provide an insertion and deletion algorithm with guaranteed logarith- mic performance. This new variant is quite similar to treaps, in that it stores two kinds of information at each node with one set being heap-ordered and the other set being ordered adhering to the BST property. However instead of using random variables as the set of priorities we simply time stamp each element on insertion and store this timestamp as the node’s history value. This has the obvious disadvantage that the original input has to be randomized unlike treaps, where the randomness is introduced automatically as part of the insertion process. However randomizing the input is easily done and does not affect the overall performance. There is no need for rotations as part of the insertion algorithm in OBSTs, as the heap order property is automatically achieved by way of assigning the history values. The main advantages of OBSTs over treaps and other existing BST variants are a) all OBST operations, including the deletion, are truly randomness preserving and b) the deletion in OBSTs is also order preserving. By order preservation we mean

that deleting an element will create the same tree as would have been produced had the element deleted never been inserted. This property has several advantages: for example, we can predict the shape of a tree by examining the input sequence and the operations that are to be performed on it. As well as that we can easily determine an element’s relative time of insertion from its position in the tree. This can be useful when analysing the order and kind of operations performed on the data structure. As mentioned above, OBSTs form the first data structure which provides a truly randomness preserving deletion algorithm, whereas randomness creating deletion algorithms have been described before ([7], [9]).
The remainder of this paper is organised as follows: in section 2 we describe this new kind of tree and provide an explanation of the deletion algorithm as well as its pseudo code. In section 3 we compare the probabilities of the different tree structures after a normal deletion with those using the new deletion algorithm on an example of a tree of size three. We then move on to provide a general proof of randomness preservation by proving that there is a bijection between the set of OBSTs and the set of permutations which respects the corresponding deletion and insertion operations. The average-case analysis of the algorithms can be found in section 4 where we show that both the insertion and the deletion algorithm have an expected performance of O(log n). Finally we provide a conclusion and some ideas for future work in section 5.


The Ordered Binary Search Tree
Introduction
We introduce a new notion of BST which stores information about the order in which the elements were inserted. From here on we will refer to these trees as ordered binary search trees (OBSTs). A similar notion is mentioned in [8] where Mishna describes the transformation between BSTs and heap ordered trees in order to create a bijection between permutations and BSTs. Any normal BST will create the same shape for the two distinct permutations (yx z) and (yz x). In an OBST we will store an item at each node consisting of the usual key and its position in the permutation thus yielding two distinct trees. This difference between BSTs and OBSTs is depicted in figure 1. Any OBST is therefore uniquely defined by the elements it contains and the order in which they were inserted unlike ordinary BSTs which are defined only by the elements they contain. When classifying the OBSTs, the underlying permutation will be taken into account, meaning that the two OBSTs depicted in figure 1 are classified as two different trees even if their shape is the same. In traditional BSTs it suffices to look at the shape alone when classifying the trees, as the deletion algorithm is based purely on this structural information. For our new kind of tree however, the deletion algorithm will use history information as well as structural information, which is the reason that this distinction is required.



y

x	z
Normal BST
(y,1)

(x,2)	(z,3)
OBST for (y x z)
(y,1)

(x,3)	(z,2)
OBST for (y z x)


Fig. 1. The difference between OBSTs and ordinary BSTs

The Algorithm
To find/insert an element in an OBST, we proceed the same as with normal BSTs [11,5]. For simplicity we will assume that all keys are distinct.
The deletion operation uses information about the structure of the tree as well as information about the history of the tree which is in contrast to BSTs which rely on structural information only. Deleting an element from an OBST follows algorithm 1 and takes O(log n). Given a random tree as input, this operation will produce a random tree also, which is due to the fact that the tree is re-structured in such a way as to re-establish the heap order of the history values as well as the BST order of the key values. An item is deleted by replacing it with the smallest history value and then recursively re-inserting the remaining subtree at the correct historic level of the tree. The deletion process is shown in figure 2(a) to 4(a). As we are guaranteed that each new item has a different history value, there is no need to relabel the existing values after a deletion. This is a very important observation as relabeling would cause the algorithm to run in linear time, thus making it unacceptable in practice.

Randomness Preservation
In [3], Jonassen and Knuth show the deletion paradox on the example of BSTs of size 2 and 3 and we would ask the reader to familiarise himself with this example, as we will follow it very closely to show that our new BST variant solves the paradox. Knuth begins by defining the different shapes possible for BSTs of size 2 and 3. So let us consider the possible OBSTs of size 2 and 3 as depicted in figure 5. It can be easily seen that any list of integers with the same inter-relationship will yield one of the generic structures depicted and we can therefore restrict ourselves to values between 1 and n. There are two different trees of size 2, namely F and G and six different trees of size 3, which are S1, S2, S3, S4, S5, S6. OBSTs are defined by the elements they contain and the order of those elements hence S3 and S4 have to be considered 2 different trees. For normal BSTs we also have two trees of size 2, F and G but only five trees of size 3 which we will refer to as A, B, C, D and E and which are depicted in figure 6. The OBST trees of size 3 map to ordinary BSTs of size 3 as follows:
S1 → A S2 → B S3 → C

Algorithm 1 The Deletion in OBST Delete( T, k )
x ← Find( T, k )
if ( x has no children ) remove x
else if ( x has one child c ) replace( x, c )
else
if ( x.leftChild.getHistory < x.rightChild.getHistory ) l ← x.leftChild
if (l.rightChild.isExternal) replace( x, l )
else
s ← l.rightChild replace( x, l )
ReInsert( l.rightChild, left, s )
else
//equivalently ( change right to left and v.v. )

Algorithm 2 ReInsert in OBST ReInsert( correctPos, direction, subtree ) testPos ← correctPos.parent
while ( correctPos.getHistory < subtree.getHistory && !testPos.isExternal ) set Flag
if ( direction = left )
correctPos ← correctPos.leftChild else
correctPost ← correctPos.rightChild testPos ← correctPos
if ( Flag is not set ) newSubtree ← correctPos replace( correctPos, subtree )
ReInsert( subtree, - direction, newSubtree ) else
if ( !correctPos.isExternal ) replace( correctPos, subtree )
ReInsert( subtree, - direction, correctPos ) else
replace( correctPos, subtree )


S4 → C S5 → D S6 → E

It is easy to see that we do not have a bijection, as there are two OBSTs which map to only one BST. However we do have a bijection between permutations and OBSTs which will help us in achieving randomness preservation.
We will now look at the deletion process on an OBST of size 3: there are three possible labels that can be deleted: x, y or z. If we use the deletion algorithm described in algorithm 1, then we get the following distribution of 2-element trees:





j,1
Delete j




g,3



f,6	i,7






l,4

p,2
Move up the node with the lower history value i.e. (p,2)



q,5





k,9	n,10	t,8





OBST from permutation (j p g l q f i t k n)


p,2


Insert subtree after this node



g,3




l > g



q,5



f,6	i,7	t,8












The tree before ReInsert and its separate subtree
Fig. 2.


	

The tree after the first call to ReInsert with its separate subtree





The tree after the second call to ReInsert with its separate subtree
Fig. 3.






Tree after deletion of j
Fig. 4.

(z,1)	(z,1)

	

(y,1)

(x,2)
F
(x,1)

(y,2)
G
(y,2)

(x,3)
S 1
(x,2)

(y,3) S 2



(y,1)

(x,2)	(z,3)
(y,1)

(x,3)	(z,2)
(x,1)





(z,2)
(x,1)





(y,2)


	



S 3	S 4
(y,3)
S 5
(z,3)
S 6


Fig. 5. OBST of size 2 and 3
The overall distribution of F and G is  9 = 1 , which is the same for deletion
on normal BSTs and is as we would expect. If we distinguish between the different F and G structures according to the elements they contain however, then we get


z	z	y	x

y	x	x	z	z

x	y	y
A	B	C	D

x

y

z
E

Fig. 6. BST of size 3
different values for deletion in OBSTs and deletion in normal BSTs:
Now assume a random insertion into these trees. The newly inserted element can have the following inter-relationship to the elements that were originally in the tree, before the first deletion took place: a) be smaller than all of them, b) be bigger than one of the items but smaller than the other two, c) be bigger than two of the items but smaller than the other one, d) be bigger than all of them. For example the structure F(x,z) yields the structure S1 in case a), the structure S2 in cases b) and c) and the structure S3 in case d). If we do the same for all 2-element structures and then compute the probabilities we get the following values for OBSTs:
P(S1) = 3+3+6 = 1 , P(S2) = 3+6+3 = 1 , P(S3) = 6+3+3 = 1 ,

72	6
72	6
72	6

P(S4) = 3+3+6 = 1 , P(S5) = 3+6+3 = 1 , P(S6) = 6+3+3 = 1 .

72	6
72	6
72	6

In comparison, the values of the five possible shapes for normal BSTs are as
follows:
P(A) = 11 , P(B) = 13 , P(C) = 25 , P(D) = 11 , P(E) = 12 .
72	72	72	72	72
If we now perform an additional random deletion and compute the probabilities
we see that structures F and G still arise with a probability of 1 each, in comparison to normal BSTs where this final deletion causes structure F to arise with probability
109 > 1 .
216	2
From the above results it seems that this new deletion is randomness preserving.
However we have only looked at trees of a very small size and cannot infer from these examples that it will hold for all trees. To prove that it does, we will first prove the correctness of the operations described in section 2.2. In the following we will show

that if we are given an OBST which was created from a specific permutation, then inserting an element into this OBST is equivalent to adding a new element to the permutation and then using this new permutation to create the OBST. Similarly deleting an element from the OBST is equivalent to removing the element from the permutation and then creating the OBST from the modified permutation. In particular we will show the following three things:
there is a one-to-one mapping between permutations and OBSTs
a random insertion preserves this mapping
a random deletion also preserves this mapping. We will use the following notation.
Let P = (e1 e2 ... en) denote a permutation of the set X of keys e1 to en and let T = [e1, e2 ... en] denote the OBST which was created by inserting the keys e1 to en in exactly that order.
Lemma 3.1


Proof. First we show
P = (e1e2 ... en) ⇔ T = [e1, e2 ... en]


P = (e1 e2 ... en) ⇒ T = [e1, e2 ... en]

T = [e1, e2 ... en] is defined above to be the OBST created by inserting the elements
e1 to en in that order, so the equation is true by definition.
Now let us assume that
P = (e1e2 ... en) ⇐ T = [e1, e2 ... en]
does not hold, i.e. we have two different permutations for one OBST. This would mean that we have at least one key at a different position which would result in a node with a different key-history value pair, hence a different tree. So we have found a contradiction and thus we know that
P = (e1e2 ... en) ⇐ T = [e1, e2 ... en]


Lemma 3.2
P = (e1e2 ... enen+1) ⇔ Insert en+1 into T = [e1, e2 ... en]
Proof. To show P = (e1 e2 ... enen+1) ⇒ Insert en+1 into T = [e1, e2 ... en] con- sider the permutation P = (e1 e2 ... enen+1). We have already shown in a) that this creates exactly one OBST which is: T = [e1, e2 ... en, en+1]. This in turn describes the OBST which contains the elements e1 to en+1 and as en+1 is the element with the highest subscript, it is the node in T with the highest history value, i.e. the node that was inserted last. No changes have been made to the position or history values of any other nodes.

Now consider the initial situation Insert en+1 into T = [e1, e2 ... en], i.e. the case P = (e1 e2 ... enen+1) ⇐ Insert en+1 into T = [e1, e2 ... en]. Can we show that the resulting tree could have only been created by P = (e1 e2 ... enen+1)? From
a) we know that T = [e1, e2 ... en] could have only been created by P = (e1e2 ... en) and we also know that our insertion algorithm will give en+1 the highest history value and add it at its correct position in T without actually modifying the rest of the tree structure. Therefore the only permutation that this tree could have been created from is the permutation (e1e2 ... en) plus en+1 added to the end of it which is equal to P = (e1 e2 ... enen+1).

Lemma 3.3
P = (e1 e2 ... ej−1 ej+1 ... en) ⇔ Delete ej from T = [e1, e2 ... en]
Proof. First we will look at P = (e1 e2 ... ej−1 ej+1 ... en) ⇒ Delete ej from T = [e1, e2 ... en].
We already know from a) that the permutation P = (e1 e2 ... ej−1 ej+1 ... en) creates exactly one OBST, which is T = [e1, e2,... ej−1, ej+1 ... en]. Is this the same tree as [e1, e2 ... en] with ej deleted? For two OBSTs to be the same, all nodes have to have the same key value, same history value and same position in the tree, where the position is characterized by a) the level and b) the left/right relationships. No key or history values are being modified as part of the deletion, so we do not have to worry about that aspect. It is easy to see that the position in the tree is certainly the same for all nodes up to ej−1, as the deletion only modifies the part of the tree which contains nodes with history values greater than that of the deleted node. Both trees do not contain ej, so we still have a match. What about the nodes ej+1 to en? The deletion may move these nodes, but if it does it takes care of two things: moving them to the correct level according to its history value and following the proper key comparison while moving them down the tree. Thus the new position of those nodes is the same as if they had been inserted after ej−1, which proves that the two trees are the same.
To prove P = (e1 e2 ... ej−1 ej+1 ... en) ⇐ Delete ej from T = [e1, e2 ... en] we need to show that the deletion algorithm does not violate any of the following two properties: the relationship between the element ej and any of its predecessors ei, where 1 ≤ i < j and the original relative ordering of the remaining elements. When deleting an element from an OBST, the algorithm does not modify the relative ordering between the elements. The structure of T is modified but the subtrees are reattached to the tree in such a manner as to preserve the original left and right subtree relationships. Consider an arbitrary element ex in the tree: after the deletion of ej the position of ex might be different. However if ex was in the left (right) subtree of any ej, where 1 ≤ j < x, then it will still be there afterwards. Therefore the new tree could have only been created from the permutation P = (e1 e2 ... ej−1 ej+1 ... en).	 
Lemmas 1-3 immediately imply the following Theorem:

Theorem 3.4 There is a bijection between the set of OBSTs and the set of permu- tations (of the same size) which respects deletion and insertion.
Proof. The Theorem is an immediate corollary from the Lemmas.	 
Corollary 3.5 Deletions and Insertions on Ordered binary search trees are ran- domness preserving.
Proof. Permutations are randomness preserving ([11]) and we have proved that there exists a bijection between ordered binary search trees and permutations. 

Average-Case Analysis
As both the insertion and the deletion algorithm are randomness preserving it is very easy to compute the expected values for internal path-length and height. The internal path-length is equivalent to the construction cost of a random BST and is described by the following recurrence which can be found in [11]:

(1)
C  = n	1+ 1
n	n
Σ

1≤k≤n
(Ck−1
+ Cn−k
) for n > 0 with C0 =0 

We can solve this recurrence using generating functions yielding the following for- mula for the expected internal path-length:
2(n + 1)(Hn+1 − 1) − 2n	Σ
k=1 k
know that the sum of depths of all nodes in the tree yields the internal path-length:
Internal path-length = Σ di
i=1
Now we can easily calculate the expected depth of an item from (2) and (3):


Expected depth: D
= 2H
Hn+1 − 1 − 4

We will also require a value for the expected height Heighte in a BST. An approx- imation for this value can be found in [11]:
Expected height: Heighte = c log n, where c ≈ 4.31107
These are the main values required to analyse both algorithms.
Insertion: To insert a new item into the tree, we essentially perform a search for the correct external node and it is not hard to see that the expected cost is directly related to the number of nodes that are visited along this search. In fact, the insertion of a new node is equal to the expected depth plus one to reach the external node.


I = D
+1 = 2H
Hn+1 − 1 − 3

The approximation for the harmonic number Hn is given in [11] as: Hn ≈ log n +
.57721 ·· ·. Using this approximate and the fact that Hn+1 goes to zero as n becomes

large, we can see that the average cost for insertion is O(log n) which is as we would expect for random BSTs.
Deletion: To analyse the deletion algorithm, we divide it into two main parts. In the first part we perform a search for the node to be deleted, let us refer to the cost for this as Se. The second part of the algorithm is concerned with re-structuring the tree to make sure it is still an OBST. The cost for this part will be referred to as Re. Finding the cost Se involves arguments similar to the ones used for the insertion algorithm. Again, the cost is directly related to the number of nodes visited along our search path, only this time we are looking for an internal node rather than an external node. Therefore we can simply take the value for the expected depth of a node: Se = De.
Finding the value for Re is slightly more complex. The cost for re-structuring is essentially nothing but a recursive call to ReInsert, so the cost can be expressed by the following recurrence:
Rn = In + Rs, with 0 < s < n 
The cost of ReInsert on a tree with n nodes is made up of the cost of inserting a
tree of arbitrary size into a tree of size n plus the cost of calling ReInsert on a smaller tree Tsub with only s nodes. We stop when we reach an external node, i.e. when s = 0. Inserting a tree is not very different from inserting a single node: we consider only the root of the tree and try and find the correct position in the tree, insert the root and through its children pointers, the rest of the tree gets automatically inserted. As part of ReInsert we do not necessarily need to find an external node to insert the tree and so we use the cost of finding a node in a tree of size n:


I = D
= 2H
Hn+1 − 1 − 4

Now we need to get a handle on the average size s of Tsub. We already know the expected depth of an element in the tree and we also know the expected height Heighte of the tree. Therefore we can compute the expected height h of Tsub as follows:
h = Heighte − De
Filling the values from (5) and (4) into (8) we get:


h = c log n − 2H

n+1
2 Hn+1 − 1 +4 
n

Now we can fill in the approximation for the harmonic number to give:

h = c log n − 2 log (n + 1)+ 2c1
2  log (n + 1)+ c1 − 1 +4 
n

At this point the equation looks like it is not going to be of any help, as it is far too complicated to plug into the recurrence. However, for this analysis we are not interested in constant factors, so we can ignore them. Also we are only interested in the upper bound, so using O() notation we get the following:





This simplifies to:
 log n h = O( log n) − O( log n)+ O(	n	)


h = O(
log n n	)

We also know that for the average height of Tsub the normal formula applies, so:
h
h = c log s ⇔ s = 2 c
We fill in the value we got for h in (9) into (10) yielding the following for the cardinality s of Tsub:
s = √n n
Now we can finally plug the values into (7):

Rn = O( log n)+ O(R √n n)
After the first recursive step the recurrence will look as follows:


O( log n)+ O( log
√n n)+ R √n 

It is easy to see that as this recurrence unfolds further, all additional terms are bounded by O(log n) and we can now use this value in our original equation for the average cost of deletion:

Dele = Se + Re = O( log n)+ O( log n)= O( log n)
Thus we get a logarithmic average running time for the deletion algorithm.

Conclusion and Future Work
We have presented a new kind of BST, which provides an answer to Knuth’s open question regarding the existence of a randomness preserving deletion algorithm for BSTs. The insertion and deletion algorithms are very straightforward, easy to implement and have an expected performance of O(log n). The deletion algorithm is the first ever to be randomness preserving. On top of that it is also order preserving which can be very useful in practice. Clearly OBSTs are very usable as they have good average-case performance and the only extra information needed is the history value in the form of an integer.
To further investigate the usability of these trees, one might be interested to show how the actual tree structure and the corresponding internal path-length change after multiple insertions and deletions. It would also be of interest to compare ex- perimentally the (average-, best- and worst-case) performance of the OBST deletion with the deletion of other random BSTs to see which runs faster on average.

References
S. Edelkamp. Weak-heapsort, ein schnelles sortierverfahren. Diplomarbeit, Universit¨at Dortmund, 1996.
T. N. Hibbard. Some combinatorial properties of certain trees with applications to searching and sorting. Journal of the ACM, 9, 1962.
A. T. Jonassen and D.Knuth. A trivial algorithm whose analysis isn’t. J.Comput. Syst. Sci.Journal of the ACM, 16(3):301–322, 1978.
G. D. Knott. Deletion in Binary Storage Trees. PhD thesis, Stanford University, 1975.
D. Knuth. Sorting and Searching - Volume 3 of The Art Of Computer Programming. Reading, Massachusetts: Addison-Wesley, 1973.
D. Knuth. Deletions that preserve randomness. IEEE Trans. Software Engineering, 3:351–359, 1977.
C. Martinez and S.Roura. Randomized binary search trees. Journal of the ACM, 45(2), 1998.
M. Mishna. Attribute grammars and automatic complexity analysis. INRIA, 2000.
W. Pugh. Skip lists: A probabilistic alternative to balanced trees. Communications of the ACM, 33(6):668–676, 1990.
M. P. Schellekens. A Modular Calculus for the Average Cost of Data Structuring, Efficiency-Oriented Programming in MOQA. Springer, to appear.
R. Sedgewick and P. Flajolet. An Introduction to the Analysis of Algorithms. Addison-Wesley, 1996.
R. Seidel and C. Aragon. Randomized search trees. Algorithmica, 16:464–497, 1996.
