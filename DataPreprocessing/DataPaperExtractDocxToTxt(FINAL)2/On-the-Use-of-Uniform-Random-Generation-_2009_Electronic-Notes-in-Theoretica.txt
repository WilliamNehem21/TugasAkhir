

Electronic Notes in Theoretical Computer Science 253 (2009) 37–51
www.elsevier.com/locate/entcs

On the Use of Uniform Random Generation of Automata for Testing
Fr´ed´eric Dadeau1	Jocelyn Levrey3
LIFC - INRIA CASSIS Project, 16 route de Gray, 25030 Besanc¸on, FRANCE
Pierre-Cyrille H´eam2
LSV - ENS de Cachan,CNRS UMR 8643, 61, avenue du Pr´esident Wilson, 94235 Cachan cedex, FRANCE
LIFC - INRIA CASSIS Project, 16 route de Gray, 25030 Besanc¸on, FRANCE

Abstract
Developing efficient and automatic testing techniques is one of the major challenges facing software vali- dation community. In this paper, we show how a uniform random generation process of finite automata, developed in a recent work by Bassino and Nicaud, is relevant for many faces of automatic testing. The main contribution is to show how to combine two major testing approaches: model-based testing and ran- dom testing. This leads to a new testing technique successfully experimented on a realistic case study. We also illustrate how the power of random testing, applied on a Chinese Postman Problem implementation, points out an error in a well-known algorithm. Finally, we provide some statistics on model-based testing algorithms.
Keywords: Random generation, automata, model-based testing, Chinese Postman algorithm


Introduction
Motivations and Contributions
Producing secure, safe and bug-free programs is one of most challenging problem of modern computer science. In this context, two complementary approaches ad- dress this problem: verification and testing. On one hand, verification techniques mathematically prove that a code or a model of an application is safe. However, complexity bound makes verification difficult to apply on large-sized systems. On the other hand, testing techniques do not provide any proof but are relevant, in

1 Email: frederic.dadeau@lifc.univ-fcomte.fr
2 Email: heam@lsv.ens-cachan.fr , pierre-cyrille.heam@lifc.univ-fcomte.fr
3 Email: jlevrey@chu-besancon.fr

1571-0661 © 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.09.050

practice, in order to produce high quality software. Last years, many works have been done in order to upgrade hand-made (or experience-based) testing techniques to scientific based frameworks.
Since every configuration of a software can not be practically explored, one of the key problem for a validation engineer is to choose a relevant test suite while controlling the number of tests. The crucial question raised is then: “what means relevant?”. A frequent answer, in the literature and in practice, is to consider a test suite as relevant if it fulfils some well-known coverage criteria; for instance, a code coverage criterion, that is satisfied if all the lines of the codes are executed at least once when running the tests. It is important to point out that coverage criteria can be applied on the code (white box or structural testing) or on a model of the implementation (black box or functional testing [6]). Since there are many ways to fulfil coverage criteria [21], other criteria can be taken into account, for example based either on computing minimal/maximal length test suites, or on selecting boundary or random values for the test data.
This paper is dedicated to show how a recent result by Bassino and Nicaud [5] may be successfully exploited for several testing techniques. More precisely, this work describes the uniform random generation of automata. Thus, we propose to employ this technique in order to:
explore (in Sect. 2.1) how to combine random generation of finite automata with model based testing using FSM coverage criteria. A challenge of model-based testing is to define good test generation functions. In this direction, several cov- ering criteria have been defined for finite state machines: test suite has either to cover each state, or each transition, or each pair of successive transitions, each loopfree-path, each k-path, etc. It is known that the simplest one each state does not provides a relevant test suite. Moreover, the each transition transition is quite good in practice. However other criteria provide very large test suites, even on simple examples. Our goal is to provide a parametric criteria based on random generation that improve the each transition criterion without resulting in an uncontrollable blow-up of the test suite size. This is done using random generation of finite automata. To the best of our knowledge, it is the first work making such a combination. Our technique was applied (Sect. 2.2) on a non- trivial example of an electronic purse model and implementation. This technique turns out to be very efficient, in particular it pointed out two non-conformances between the model and the implementation that were not discovered by other testing techniques. This part is the main contribution of our paper.
show (in Sect. 3) how it can be used in a purely random testing approach, for generating test data. In this context, we report on a bug on a widely-used Chi- nese Postman Problem implementation (ranking second when googling “Chinese Postman Problem”). Of course, programs taking a labeled graph as input are not so common. However, many applications deal with complex data structures, sim- ilar to labeled graphs, and we think that the random generation of finite graphs can be applied in many cases.

provide (in Section 4) some statistics on test suites generated in a pure model based testing approach based on FSM coverage. Such statistics may be relevant in order to help the validation engineer to choose among different existing testing techniques. We think such statistics can be also useful in order to have a better understanding of coverage criteria.

Notations
Following definitions are provided in order to prove that our approach fulfils the each transtition criterion (or some others) and in order to provide formal and rigourous constructions.
We define in this section useful notations. We assume the reader to be familiar with common definitions of alphabet, letter, word, etc. A ﬁnite automaton A is a quintuplet (Q, Σ, Δ,I,F ), where Q a finite set whose elements are called states, Σ is a finite alphabet (i.e. a finite set of symbols), I ⊆ Q is the set of initial states, F ⊆ Q is the set of ﬁnal states and Δ ⊆ Q × Σ × Q is the set of transitions. A finite automaton is deterministic if for every state p, there exists at most one transition of the form (p, a, q) and if I is a singleton. A finite automaton is complete if for every state p there exists at least one one transition of the form (p, a, q).
A path π in a finite automaton A = (Q, Σ, Δ,I,F ) is a a finite sequence (p1, a1, q1), ... , (pn, an, qn) of transitions such that for every 1 ≤ i ≤ n−1, qi = pi+1. The transition (pi, ai, qi) is denoted π(i). Such a path is called a path from p1 to qn. A path π meets a state p if there exists i such that either π(i) = (p, a, q) or π(i)= (q, a, p). The finite word a1 ... an is called the label of π. The path from an initial state to a final state is called a successful path. The set of labels of successful path is the language accepted by A. A state p is accessible if there exists a path from an initial state to p. A transition (p, a, q) is accessible if p is accessible. A path is accessible if its first transitions is accessible. A simple loop is a path π from a state p to p such that for every i, j, π(i) /= π(j).
Given two finite automata A1 = (Q1, Σ, Δ1, I1, F1) and A2 = (Q2, Σ, Δ2, I2, F2), the automaton A1 ⊗ A2 is the automaton (Q1 × Q2, Σ, Δ, I1 × I2, F1 × Q2) where Δ = {((p1, p2), a, (q1, q2)) | (p1, a, q1) ∈ Δ1 and (p2, a, q2) ∈ Δ2}. Notice that this definition only differs from the definition on classical product of finite automata for final states. If t = ((p1, p2), a, (q1, q2)) ∈ Δ, we denote by ProjA(t) the transition (p1, a, q1) of A. This projection can naturally be extended to paths of A1 ⊗ A2 by ProjA(t1,... , tn)= ProjA(t1),... , ProjA(tn) which is trivially a path of A.
Consider the following illustrating toy example with automata A and B:

a


A
b
b
a

The product A ⊗ B is the following automaton (we only represent accessible states).

a


a
A⊗B 
Path π = ((0, 3), a, (1, 3)), ((1, 3), b, (0, 4)), ((0, 4), a, (1, 3)), ((1, 3), b, (0, 4)) is a
path of A⊗ B. And we have ProjA(π)= (0, a, 1)(1, b, 0), (0, a, 1), (1, a, 0).
A coverage criterion is a function that maps each automaton and each set of accessible paths of this automaton to 0 or 1. The state coverage criterion ϕs is defined by: ϕs(A, Π) = 1 if and only if for every accessible state q of A there exists a path π in Π meeting q. The transition coverage criterion ϕt is defined by: ϕt(A, Π) = 1 if and only if for every accessible transition (p, a, q) there exists a path π in Π such that π(i) = (p, a, q). The consecutive transition coverage criterion ϕct is defined by: ϕct(A, Π) = 1 if and only if for every pair of accessible transitions of the form (p, a, q), (q, b, r) there exists a path π in Π such that π(i) = (p, a, q) and π(i + 1) = (q, b, r). The simple loop transition coverage criterion ϕsl is defined by: ϕsl(A, Π) = 1 if and only if for every accessible simple loop πloop, there exists a path π in Π and paths π1, π2 in A such that π = π1, πloop, π2.

Combining Random Testing and Model Based Test- ing
In this section, we assume the tested implementation is modeled by a finite automa- ton A, which is a frequently used abstraction of a real system. We first present a generic approach to combine model-based and random testing and we then expose experimental results.
Automata are randomly generated using the REGAL tool [3], according to the following principle. Given a finite alphabet and a number of states n, REGAL gen- erates an accessible deterministic automaton with n states on the given alphabet. Generation is done up-to isomorphism, i.e. the tool generates with a same proba- bility a representant of isomorph equivalence classes. This can be done using the result of [5] based on combinatorial analytic techniques [11].

Testing Process
The main purpose is to generate a test suite that fulfils a given coverage criteria while integrating a random process. Given a testing procedure and a model A to be tested, the idea is to randomly generate an automaton B, to compute a direct- product, like A⊗ B, and then to apply the testing procedure to this product.
The basic idea of this procedure is to guide the test generation by a coverage criterion, but also by the expected number of tests, that a validation engineer may require. Indeed, it is a current practice, especially in the industry, to consider that the more tests are run on the system, the more confidence we may have in it. This approach is thus dedicated to drive the test generation so as to obtain, from a

given automaton and a given coverage criterion implemented into an test generation algorithm, a given number of tests. This approach reuses the results of previous section on the statistics of test suites. Notice that this approach can not be used to reduce a test suite; its goal is to increase the number of tests that would have been obtained using the selected test generation algorithm on the original automaton.
Proposition 2.1 Let x ∈ {s, t, ct}. If B is a complete accessible ﬁnite automaton and if ϕx(A ⊗ B, Π) = 1, then ϕx(A, ProjA(Π)) = 1. If B is a complete strongly connected ﬁnite automaton and if ϕsl(A⊗ B, Π) = 1, then ϕsl(A, ProjA(Π)) = 1.
Proof We give here the proof for the ϕt criterion. Others proofs are similar.
Assume that ϕt(A ⊗ B, Π) = 1. Let (p, a, q) be an accessible transition of A. By definition, there exists a path π1 in A from an initial state to q. Since B is complete and accessible, there exists a path π2 in B from an initial state of B with the same label as π1. Therefore, by a direct induction, there exists a path π in A ⊗ B from an initial state of A⊗ B and ending in state of the form (p, r) where r is the last state of π2. Since B is complete, there exists in B a transition of the form (r, a, s). Consequently, transition ((p, r), a, (q, s)) is an accessible transition of A⊗ B. By hypotheses, there exists a path π' in Π and a positive integer i such that π'(i)= ((p, r), a, (q, s)). To finish, it suffices to note that ProjA(π')(i)= (p, a, q). It follows that ϕt(A, ProjA(Π)) = 1	 
For the ϕsl criterion, notice that the REGAL tool [3] makes it possible to perform a uniform random generation of strongly connected deterministic automata.
Now, one can consider the paths on automata A and B given in Sect. 1.2.
π1 = ((0, 3), a, (1, 3)), ((1, 3), b, (0, 4)), ((0, 4), a, (1, 3)), ((1, 3), b, (0, 4)) and
π2 = ((0, 3), a, (1, 3)), ((1, 3), a, (2, 4)), ((2, 4), a, (0, 3)).
One has ϕt(A⊗ B, {π1, π2})= 1. Now ProjA(π1)= (0, a, 1)(1, b, 0), (0, a, 1), (1, a, 0)
and ProjA(π2)= (0, a, 1)(1, a, 2), (2, a, 0). One can easily check that
ϕt(A, ProjA({π1, π2})) = 1.

Experimentation on the Demoney Case Study
Demoney, a Demonstrative Electronic Purse, is a specification of electronic purse that has been developed by Trusted Logics [19]. Even though Demoney is meant to be used for research purposes and does not aim at being embedded on a smart card, it recreates all the mechanisms of a smart card application, and thus, it is considered as a realistic case study. This electronic purse is protected by two pin codes, one for the user and one for the bank. As for every smart cards, it has a life cycle, that begins with a personalization phase, in which the different parameters of the card are being set (maximal amount on the card, maximal debit authorized, pin and bank codes). Then the card reaches the use phase in which different operations can be performed, such as crediting or debiting. The credit operation requires an authentification of the user, though the verification of his pin code. If he fails to authentificate (a given number of times) the card becomes blocked. It is then up

to the bank to authentify in order to change the pin/unblock the card. If the bank also fails to authenticate, the card is definitively dead.
From previous research and teaching experiments we had designed a formal model of the system, written as a B abstract machine [2] anda Java implementation, along with a number of mutants of this implementation. Each mutant is a variation of the original implementation, in which a mistake has been introduced on purpose. This technique is used to evaluate the quality of a test suite: the more mutants are killed, the more efficient is the test suite.
The experiment we designed is summarized in Fig. 1. We manually designed an automaton representing the Demoney specification. This automaton is displayed in Appendix B. We used an abstraction of the states, for which we abstracted the value of the balance of the purse to two possible values: 0 and > 0. The transitions are labeled by the different “behaviors” of the operations. A behavior is a subpart of an operation (for example, operation VERIFY PIN(type,value) contains 4 behaviors depending on the input values of the parameters and the expected output of the command: correct –or incorrect– verification of a user –or bank– pin). Then, we ask the algorithm to generate a given number of tests, that will thus consist of sequences of operation behaviors. In order to generate full operation calls with instantiated values of parameters, we replay these sequences on a B symbolic animator [7]. We then only have to reuse our existing material to produce the JUnit tests cases that can be applied on the Java implementation and the mutants. Finally, for each final state of the automaton, we add a reset transition, that is a transition labeled by a reserved character indicating that the system has to be re-initialized.
When running the tests, we are looking for a non-conformance between the results obtained by the implementation, and the expected results given by the model. This conformance relationship is based on observing the outputs of the different commands, that are supposed to return a status code indicating if the command succeeded or failed, and why, depending on the value of the error code. A test fails if the codes do not correspond at a given step of the execution of the test.
The results of this experiment is given in Tab. 1. This table displays the following informations: the list of test suites that we have generated (col. test suite), the number of tests for each suite (col. # tests), the average length of the tests (col. av. length), the maximal length of a test (col. max. length), the computation time (col. time), and the number and percentage of mutants killed.
The ChinesePostman lines shows the results obtained by applying the Chinese Postman algorithm on the automaton of Demoney (with initially 2 final states).

Figure 1. Process of the experiment


Table 1
Results on the Automata Augmentation

The ChineseAug2F 10L (with L ∈ a..e) lines show the result on 5 runs of the automaton augmentation algorithm in order to reach 10 tests when applying the Chinese Postman algorithm. The goal of this set of test sequences is to see if the randomly generated automaton for the product may have an influence on the resulting tests. Globally, we notice that their average and maximal length may vary but this has no serious influence on the efficiency of the test cases. Generally, the ChineseAugNF S lines represent the application of the automaton augmentation on the Demoney automaton having N final states, and aiming at producing S tests. Finally, the last line, LTG, compares the results obtained by a commercial fully-automated test generation tool, Leirios Test Generator from the Smartesting company 4 . This tool generates tests from a B model and using model structural coverage criteria.
These results shows that the average length of the tests suites is relatively similar for a given number of final states. The computation time decreases with the increase of the number of final states. Intuitively, this is due to the fact that adding final states add backward transitions that simplify the search for the optimal path in the Chinese Postman algorithm. However, the resulting test cases are longer, and

4 http://www.smartesting.com , formerly Leirios Technologies

more efficient as the number of final states decreases, even with a small number of tests. In general, the maximal length of the tests sequences increases with the number of tests. We can notice that the efficiency of the test suite is related to the length of the test cases. A deeper study of the results (not represented here) shows that the longest test cases find the largest number of errors. In addition, notice that before starting the mutational analysis, we found two non-conformances on the original versions of our B model and implementation, whereas these were supposed to conform to each other, according to extensive test campaigns that had been performed before (notably using LTG).
The conclusion on this case study let us think that the technique of augmenting the test suite can be efficient in practice, for a given test generation algorithm. But a deeper study needs to be done on other case studies.

Random Testing of a Chinese Postman Algorithm
The main idea of random testing is that randomness is not influenced by the tester. In this context, a crucial issue is to perform uniform generation, i.e. every element has the same chance to be selected by the algorithm. Otherwise, selected values are related to the chosen algorithms, what is precisely opposed to the main idea. The result presented in [5] is therefore interesting, opening many possibilities to test algorithms that manipulate labeled graphs (e.g. the traveling salesman algorithm).
As an example, we use random generation of finite automata in order to test a well-known freely downloadable 5 implementation [25] of the Chinese Postman Problem [20]: given a labeled graph G, the question is to find a path in G of smallest length using all transitions.
We randomly generate strongly connected finite automata and we ask the tested program to provide a minimal path starting from the initial state and using all transitions. Generating 30 deterministic automata with 8 states on a 20 letters alphabet, we point out an automaton making the program fail (a smaller automaton causing the program failure and obtained with a larger test suite is provided in appendix A). It is not the purpose of the paper to discuss why there is a problem in the code, just notice that it is the indexing of an array out of its bounds. We fixed this bug and we did several random tests on both the implementations, that did not reveal other errors on the original program nor side-effects introduced when fixing the bug.

5 http://web4.cs.ucl.ac.uk/uclic/harold/cpp/

Table 2
Test results for the two versions of the algorithm (bugged, fixed)

Table 2 shows several experiments. Column states shows the size of generated automata, Column alphabet the size of the alphabet, Column tests the number of randomly generated automata, Column (1) fails the number of tests making the initial program failed, Column (2) fails the number of tests making the new imple- mentation fail, Column (2) better the number of test providing a better (smaller) path with the new implementation and Column (1) better the number of test pro- viding a better (smaller) path with the old implementation.
As we see, random generation does not provide any test that makes the new implementation fail. Moreover, several times, the new implementation provides better results than the old implementation (whose result is therefore not optimal and thus false).

Statistics for Model-Based Testing
Motivations
The goal of this section is to show how random generation of finite automata can provide fruitful statistics in a model-based testing context. However a not diffi- cult (but quite long) work has to be done in order to compute and compare these statistics.
The testing phase is a crucial point for industrial development. For this purpose many works have been done in order to automatically achieve this point. Since testing process may be long and expensive, it is useful to provide some help to the validation engineer to choose a testing technique. For instance, the time spent on test execution directly depends on the number and the sizes of the test sequences. We propose in this section to employ random automata generation for building statistics for evaluating the result of different test generation algorithms.

Examples of Statistics
For computing above statistics, we proceeded as follows: we uniformly generate 30 deterministic complete automata, whose states are both accessible and co-accessible, with n states on a 10-letters alphabet. Then, we compute a test suite using a depth-first search algorithm (DFS) that covers all states. It is reported on Table 3 by the DFS method. We count the number of tests, that is the number of leafs of the covering tree. We also compute the average length of tests (the length of a test is the length of the path from the root to the corresponding leaf in the covering tree). Finally, we compute the maximal length of obtained tests and the length standard deviation. The same work is done using the covering criterion all transitions obtained by the Chinese Postman Problem (CPP), reported on Table 3 by the CPP method. As mentioned before, for each final state of each automaton, we add a reset transition, that is a transition labeled by a reserved character indicating that the system has to be re-initialized.
Table 3 provides some experimental results that are incomplete, but illustrate what kind of statistics can be done. Many experimental work is left to be done


Table 3 Statistical Results

in order to obtain experimental laws. One can give here some first interesting experimental results. First, one can notice that for the CPP methods, the number of tests is about the half of the number of states. This observation is still right if we vary the alphabet size. However, a deeper investigation shows that with the CPP method, the number of tests is approximately equal to the number of final states of the generated automaton. Obviously, there is a connection between the average number of final states of randomly generated automaton and the size of the automaton.
Notice that statistics on non-complete deterministic automata may also be com- puted using the work [4].

The AUTIST Tool
Statistics presented above were obtained using an automatic tool called AUtomaton TestIng STatistics. This C++ based tool can be downloaded at: http://lifc.univ-fcomte.fr/home/fdadeau/tools/#AUTIST
For instance, the following command line (on a linux computer)
$ autist -n9 -s8 -k4 -v -g
computes 9 automata (-n9) of size 8 (-s8) on a 4 letters alphabet (-k4). Flag -v prints the transition table of all generated automata. Flag -g computes a .dot file encoding a graphical representation of the automaton. Each .dot file is translated into a .jpeg file. This conversion may require a very long time, so flag -g has to be used carefully and is reserved to small automata.
In order to compute statistics, it is also possible to use the autist-grid com- mand. For instance, command
$ autist-grid -n30 -mins8 -maxs24 -k4
computes statistics for automata from sizes 8 to 24, each on 30 4-letters automata. The AUTIST tool has to be improved in order to handle other testing algorithms

and to compare them to each other.

Conclusion and Related Works
We have presented in this paper the use of uniform random generation of automata, as a basis for test generation related works.
First, we have explored an original combination of random and model based testing, through a technique that makes it possible to augment the size of a test suite. Second, we have illustrated how random testing can be employed to detect bugs, even on a well-known and widely-spread algorithm. Third and finally, we have provided some experimental data and statistics on several test generation algorithms based on automata.
For the future, we plan to experiment on large-scale examples. In this paper, we have limited the statistical study to 2 test generation algorithms. We think it would be fruitful to improve the statistical study by analysing other FSM-based test generation algorithms. We also intend to compare our combination approach with other automated testing techniques. Another prospect would be to extend our approach to take properties, expressed as labeled transitions (equivalent to logical formulae, regular expressions, etc.) into account. Finally, it would be also interesting to explore our approach in a probabilistic covering criteria way.
Related Works
The work proposed in this paper is based on a random approach [9,16]. Even if such an approach is usually presented as one of the poorest way of generating data, it has been experienced as an efficient way for finding errors, confidence into the software [15]. Random testing can be employed for generating test data, such as in DART [13]. The DART approach consists in combining static and dynamic program analysis in order to test software.
Random testing is also used to generate complete test sequences, as in the Jartege tool [22]. Similarily, in [23], the authors expose how to test object-oriented programs by generating sequences of method calls using random generation.
A recent work [14] provides an approach combining random-testing and model- checking. The work [24] proposes to use random walk for testing concurent systems. Based on partial order reduction, this work aims to reduce the size of the test suite by removing equivalent traces.
Recently, random path generation has been explored in several testing works. In [10], Dwyer and al. expose how to include a random process in Depth-First Search algorithms in order to get better test suites. The work [12] show how perform random walks in a model is deeply influenced by the topology of the model. To tackle this problem, an algorithm to generate uniformly a path of a given length is provided and successfully experimented on very large models.
We are in the context of testing from finite state machines [18]. This consits in describing the system from a labeled transition system on which different algorithms may be used to extract the test cases. This is the principle of SpecExplorer [8] that uses the Chinese Postman algorithm [25] to generate test sequences. Other

approaches, such as TGV [17], consider a product of the initial transition system with a test purpose, also expressed by a labeled transition system, that guides the executions of the system.
The test generation technique that we propose differs by proposing a test se- quence length-guided approach. It can be seen as the automated test generation of test purposes for the TGV tool, motivated by the goal of providing a user-defined number of tests (increasing the number of tests that would have been obtained) when the test generation process is applied. To the best of our knowledge, this approach has never been targeted before.

References
29th International Conference on Software Engineering (ICSE 2007), Minneapolis, MN, USA, May 20-26, 2007. IEEE Computer Society, 2007.
J.R. Abrial. The B-Book. Cambridge University Press, 1996.
F. Bassino, J. David, and C. Nicaud. REGAL: a library to randomly and exhaustively generate automata. In Jan Holub and Jan Zˇdˇ´arek, editors, 12th International Conference on Implementation and Application of Automata (CIAA’07), volume 4783 of LNCS, pages 303–305, Prague, Czech Republic, July 2007. Springer-Verlag.
F. Bassino, J. David, and C. Nicaud. Random generation of possibly incomplete deterministic automata. In GASCOM’ 08, pages 31–40, 2008.
F. Bassino and C. Nicaud. Enumeration and random generation of accessible automata. Theorerical Computer Science., 381(1-3):86–104, 2007.
B. Beizer. Black-Box Testing: Techniques for Functional Testing of Software and Systems. John Wiley & Sons, New York, USA, 1995.
F. Bouquet, B. Legeard, and F. Peureux. CLPS-B: A constraint solver to animate a B specification.
International Journal on Software Tools for Technology Transfer, STTT, 6(2):143–157, August 2004.
C. Campbell, W. Grieskamp, L. Nachmanson, W. Schulte, N. Tillmann, and M. Veanes. Testing concurrent object-oriented systems with spec explorer. In J. Fitzgerald, I. J. Hayes, and A. Tarlecki, editors, International Symposium of Formal Methods Europe (FM’05), volume 3582 of Lecture Notes in Computer Science, pages 542–547, Newcastle, UK, July 18-22 2005. Springer.
J.W. Duran and S. Ntafos. A report on random testing. In ICSE ’81: Proceedings of the 5th international conference on Software engineering, pages 179–183, Piscataway, NJ, USA, 1981. IEEE Press.
Matthew B. Dwyer, Sebastian G. Elbaum, Suzette Person, and Rahul Purandare. Parallel randomized state-space search. In ICSE [1], pages 3–12.
Philippe Flajolet, Paul Zimmermann, and Bernard Van Cutsem. A calculus for the random generation of labelled combinatorial structures. Theor. Comput. Sci., 132(2):1–35, 1994.
Marie-Claude Gaudel, Alain Denise, Sandrine-Dominique Gouraud, Richard Lassaigne, Johan Oudinet, and Sylvain Peyronnet. Coverage-biased random exploration of models. Electr. Notes Theor. Comput. Sci., 220(1):3–14, 2008.
P. Godefroid, N. Klarlund, and K. Sen.  Dart: directed automated random testing.  In PLDI’05: Proceedings of the 2005 ACM SIGPLAN conference on Programming language design and implementation, pages 213–223, New York, NY, USA, 2005. ACM.
A. Groce and R. Joshi. Random testing and model checking: building a common framework for nondeterministic exploration. In WODA ’08: Proceedings of the 2008 international workshop on dynamic analysis, pages 22–28, New York, NY, USA, 2008. ACM.
D. Hamlet and R. Taylor. Partition testing does not inspire confidence (program testing). IEEE Trans. Softw. Eng., 16(12):1402–1411, 1990.
Richard Hamlet. Random testing. In Encyclopedia of Software Engineering, pages 970–978. Wiley, 1994.


C. Jard and T. J´eron. Tgv: theory, principles and algorithms, a tool for the automatic synthesis of conformance test cases for non-deterministic reactive systems. Software Tools for Technology Transfer (STTT), 6, October 2004.
D. Lee and M. Yannakakis. Principles and methods of testing finite state machines - a survey. In
Proceedings of the IEEE, pages 1090–1123, 1996.
R. Marlet and D. Le Metayer. Security properties and java card specificities to be studied in the secsafe project, 2001.
Kwan Mei-Ko. Graphic programming using odd or even points. Chinese Math., 1:273–277, 1962.
A.J. Offutt, Y. Xiong, and S. Liu. Criteria for Generating Specification-Based Tests. In 5th International Conference on Engineering of Complex Computer Systems (ICECCS ’99), pages 119–, Las Vegas, NV, USA, Oct 1999. IEEE Computer Society.
Catherine Oriat. Jartege: A tool for random generation of unit tests for java classes. In R. Reussner,
J. Mayer, J.A. Stafford, S. Overhage, S. Becker, and P.J. Schroeder, editors, QoSA/SOQUA, volume 3712 of Lecture Notes in Computer Science, pages 242–256. Springer, 2005.
Carlos Pacheco, Shuvendu K. Lahiri, Michael D. Ernst, and Thomas Ball. Feedback-directed random test generation. In ICSE [1], pages 75–84.
Koushik Sen. Effective random testing of concurrent programs. In R. E. Kurt Stirewalt, Alexander Egyed, and Bernd Fischer, editors, ASE, pages 323–332. ACM, 2007.
Harold W. Thimbleby. The directed chinese postman problem. Software Practice and Experience, 33(11):1081–1096, 2003.

Automaton to find the Bug in the Chinese Postman






The Java program performing the erroneous run can be downloaded at:
http://lifc.univ-fcomte.fr/home/fdadeau/tools/ChinesePostmanBug.zip

Automaton of the Demoney Case Study



VERIFY_PIN(holder,_) → OK
VERIFY_PIN(bank,_) → OK
VERIFY_PIN(holder,_) → KO
VERIFY_PIN(bank,_) → KO
INITIALIZE_TRANSACTION(credit,_) → OK
INITIALIZE_TRANSACTION(debit,_) → OK
COMMIT_TRANSACTION()
PIN_CHANGE_UNBLOCK(_)
