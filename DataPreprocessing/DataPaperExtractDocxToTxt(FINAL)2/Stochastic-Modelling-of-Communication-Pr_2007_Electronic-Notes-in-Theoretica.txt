	Electronic Notes in Theoretical Computer Science 190 (2007) 129–145	
www.elsevier.com/locate/entcs

Stochastic Modelling of Communication Protocols from Source Code
Michael J. A. Smith1 ,2
Laboratory for Foundations of Computer Science University of Edinburgh
Edinburgh, United Kingdom

Abstract
A major development in qualitative model checking was the jump to verifying properties of source code directly, rather than requiring a separately specified model. We describe and motivate similar extensions to quantitative/performance analyses, with particular emphasis on communication protocols. The central aim is to extract a stochastic model (in the PEPA language) from such source code.
We construct a model compositionally, so that each function in the system corresponds to a sequential PEPA process. Such a process is derived by abstract interpretation over the state machine of a function, using interval abstraction to represent linear expressions of integer variables. We illustrate this by an analysis of a simple protocol.
Keywords: Performance modelling, Stochastic process algebra, Static analysis, Communication protocols


Introduction
Communication protocols are notoriously difficult to get right. Not only do the usual challenges of distributed and concurrent programming apply, but they provide a service that other applications depend upon. Thus the performance of a protocol is critical to its success. For example, if a routing protocol fails to react quickly to changes in topology, the network can be brought to a standstill. Similarly, a reliable transport-layer protocol must be able to maintain a reasonable throughput, even when the network is congested. Because of this, it is vital to understand the performance characteristics of these protocols.
There are currently two approaches to analysing such performance properties; either we dynamically take measurements from the real system, or we build an ab- stract model, which can then be analysed. The former includes techniques such

1 This work was funded partly by the Engineering and Physical Sciences Research Council, and partly by a Microsoft Research European Scholarship
2 Email:M.J.A.Smith@sms.ed.ac.uk

1571-0661 © 2007 Published by Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.08.013

as code profiling and operational analysis (as applied to the measurements taken), which can give accurate figures if we have access to the deployed system. The latter includes simulation and mathematical modelling (at various degrees of ab- straction), which are much more useful for explaining the behaviour of the system, and predicting its behaviour before deployment, but require a separate model to be developed. Simulations are often complicated, and may contain bugs. On the other hand, mathematical modelling is beyond the skill of the typical programmer, and also prone to mistakes.
Stochastic extensions to existing formalisms in concurrency theory, such as pro- cess algebra, have considerably mitigated this last problem. In particular, the Per- formance Evaluation Process Algebra (PEPA) [10] is a high-level and compositional language, in which models describe continuous time Markov chains (CTMCs). This is arguably more intuitive, and less prone to error, than working directly with these mathematical structures.
Despite these advances, performance models are still very much removed from implementations. Work has been done to derive PEPA models from UML [3], but this is at a higher level than the implementation itself. In most cases, the model is validated empirically, by comparing its predictions to measurements taken from the real system (and refining the model if necessary). However, when the source code of the system is available, we can obtain a much more definite handle on what it means for a model to be correct. In this paper we present the first steps towards solving this problem, by describing an abstraction to a performance model, directly from source code.
In the world of qualitative model checking, where we are concerned with just the possibility of certain behaviours, this step has already been taken. SLAM [2] and Blast [9] both use predicate abstraction and counter-example guided reﬁnement to verify such properties directly on real code, written in C. However, we cannot simply apply the same approach in a quantitative setting, since we do not have a well-defined notion of counter-example. Indeed, the problem is made much more difficult since we need to determine the probability of control-flow decisions, given some abstract environment of the program’s variables. The search space of such abstractions is simply too large to explore by a sequence of refinements, and so we must avoid initially over-abstracting the program.
The benefits of such a technique for model extraction are numerous, and can be applied to more general distributed systems (web services being a prime example), rather than just communication protocols. Our main motivations are as follows. Firstly, we want to encourage wider application of performance analysis techniques, by providing a tool that non-specialists can use. Microsoft◯R ’s Static Driver Veri- fier (SDV) [1] is a good example of how theory can be successfully applied in this way. Secondly, we want to allow non-functional testing to take place throughout the development cycle, rather than just at the end. We can do this by enabling performance evaluation of partially completed code. Finally, we wish to allow devel- opers to verify that a protocol (or more general distributed system) satisfies some performance contract, or service-level agreement (SLA). The work in [7] takes some

steps towards this, but at a more abstract level, in the context of web services.
In this paper, we begin by introducing the structure of the protocols we will be analysing, and the language of the source code we consider, in Section 2. We then briefly introduce the PEPA language in Section 3. In the following two sections (4 and 5 respectively), we describe how to construct a PEPA model first at the structural level (i.e. how to build a model of the system from models of the functions) and then at the functional level (i.e. how to build a model of a function from its source code). To illustrate this, we analyse a simple transport protocol in Section 6. We conclude with some comments on future work in Section 7.


Communication Protocols and Source Code
In this paper, we will limit our analysis to that of end-to-end communication proto- cols. In other words, we will not consider hop-by-hop protocols, such as those used for routing, since representing the topology of such systems leads to an unmanage- able state space. We do, however, wish to deal with real protocols, and so we need to analyse real-world languages. In this case, that means C.
There exist a number of tools for handling C, such as CCured [18], which together with the C Intermediate Language (CIL) [17] provides a cleaner (and type-safe) framework for analysis. However, even with the aid of these tools, the analysis of arbitrary programs is uncomputable (if we wish to retain some notion of the error involved). Fortunately, most protocols do not exhibit complex looping or recursive behaviour, so we can justifiably consider only a subset of the language.
Let us take a subset of C, with only integer variables, boolean variables and enumeration types. In addition, we impose the following restrictions:
No pointers. We intend to relax this in future work, but that is beyond the scope of this paper.
No recursion. In general this is beyond our ability to model in a Markovian setting, due to the memoryless property of states. Note that we can model tail recursion, if the analog of condition 4 below holds, but since this is equivalent to iteration (it does not require a stack), we will not discuss this further.
We allow only linear conditions of the form Σn	aixi {<, ≤, =, ≥, >} c, where
ai and c are integer constants.
Loop variables must be memoryless with respect to previous iterations of the loop, or else vary linearly with time. In other words, on each iteration, a loop variable must either be set independently of its previous value, or incre- mented/decremented each time by a constant.
The restriction on conditions is quite a strict one. In particular, we can see that pro- cedures like exponential backoff do not satisfy this. We expect that this restriction can be relaxed somewhat (for example, allowing bit-shifting operations to access flags in a field of a packet), but that is the subject of future work.

The PEPA Language

The target of our abstraction is a PEPA model. In PEPA, a system is a set of concurrent components, which are capable of performing activities. An activity a ∈ Act is a pair (α, r), where α ∈ A is its action type, and r ∈ R+ ∪ {T} is the rate of the activity. This rate parameterises an exponential distribution, and if unspecified (denoted T), the activity is said to be passive. This requires another component in cooperation to actively drive the rate of this action. PEPA terms have the following syntax:


P := (α, r).P | P1 + P2 | P1 D  P2 | P/L | A


We briefly describe these combinators as follows. For more detail, see [10].
Preﬁx ((α, r).P ): the component can carry out an activity of type α at rate r to become the component P .
Choice (P1 + P2): the system may behave either as component P1 or P2. The cur- rent activities of both components are enabled, and the first activity to complete determines which component proceeds. The other component is discarded.
Cooperation (P1 D P2): the components P1 and P2 synchronise over the cooper- ation set L. For activities whose type is not in L, the two components proceed independently. Otherwise, they must perform the activity together, at the rate of the slowest component. At most one of the components may be passive with respect to this action type.
Hiding (P/L): the component behaves as P , except that activities whose type is in L are hidden, and appear externally as the unknown type τ .
Constant (A = P ): the name A is assigned to component P .
The operational semantics of PEPA defines a labelled multi-transition system, which induces a derivation graph for a given component. Since the duration of a transi- tion in this graph is given by an exponentially distributed random variable, this corresponds to a CTMC.
There are many alternative formalisms that could be applied in this setting, but a Markovian approach has the advantage that it can be solved analytically. We choose CTMCs over discrete-time Markov chains, since we our execution model is at a higher level than that of a clocked processor. As we shall see, the states in the model correspond to basic blocks in the code, each having a different duration that is not deterministic (the processor may be pre-empted, etc.). Hence a continuous-time setting where we can attribute a rate seems most appropriate. Note that all choices in the model take some amount of time, since some condition must be evaluated, so we do not need zero-duration activities.




Fig. 1. The structure of an end-to-end protocol
Structural Modelling
Assuming that we can model the behaviour of a function, what does a model of the system look like? For an end-to-end protocol, we have two clients, A and B, which communicate over a network. The operation of the protocol is driven by events, which fall into three categories – user interactions (i.e. telling the protocol to do something), receiving packets over the network, and timeouts. This is shown schematically in Figure 1, and leads to the following general form of the PEPA system equation, where the action sets U , T , R and S define the interfaces between the components:


(User D  ClientA D  Timer )	D 
Network	D 
(User D  ClientB D  Timer )

UA	TA
SAB ∪RBA
SBA∪RAB	UB	TB

Furthermore, a client X will have the following form at the top-level:


def
ClientX =
i
(recv i, T).RecvX i +
(usercall j, T).UserCallX j + (timeout , T).TimeoutX
j

where i ranges over the abstract environment space of packets (i.e. it encodes an abstracted version of the packet contents), and j over that of the user interface (i.e. the API calls, and corresponding arguments, that the user can make). The states RecvX , UserCallX , and TimeoutX correspond to models of the corresponding functions on the client, whose construction is described in Section 5. Note that this implies a single-threaded client, since only one function can be called at a time. We can model multi-threaded clients by composing the functions in parallel.
The network can be modelled in a number of ways, depending on the properties required. When the model of the system is constructed, we expect the user to choose the network from a library of components, so that they do not need to write the PEPA process themselves. For example, a half-duplex network with probability p of packet loss would look like the following:

def
Network =
i
def X



(A send i, T).NetworkABi +


(B send j , T).NetworkBAj
j

def
NetworkBAj =
i
(A recv j, (1 — p).rnetwork ).Network + (τ, p.rnetwork ).Network

where i and j range of the abstract environment of packets sent by clients A and
B respectively.


		


Fig. 2. The three ways of modelling a function call

Here, we encode the passing of both arguments and return values (when calling a function) and the contents of packets (when communication across the network) by an interface of actions. We will consider this interface in more detail in the next section, but first we must discuss how a function call can be modelled. When we abstract a function, we will reduce each sequential block to a single transition in the model; namely a single action, with a single rate. Given this, there are three fundamental approaches to modelling a function call, as illustrated in Figure 2:
Abstract the call to a single transition to the result state. This assumes that the function executes at a fixed rate, irrespective of the input, but this is often good enough for our purposes, and simplifies the model considerably.
Explicitly embed a model of the function. This is more general, and is ap- propriate when the function has a more complex behaviour that we wish to capture. The disadvantage is in remembering which environment we were in before calling the function, so that we can recover the correct state afterwards, which leads to an exponential blowup of state. We assume that such a call is synchronous.
Model the function as a separate process running in parallel, which synchronises over call and return actions. This separates the functionality of one function from another, at the expense of an exponential blowup of state when we do a Markovian analysis. We will use this abstraction when two components are communicating over the network, and the call is assumed to be asynchronous.
We rely on user annotations in the source code to tell us how to analyse each function. In the first case, the user must also provide a summary of the function’s behaviour (i.e. how it affects the environment of the caller), so that we can model the system without having to analyse every function (e.g. libraries, system calls, etc.). The user does not write in PEPA directly; instead using a simple but higher- level syntax. Note that the second and third cases are essentially equivalent, except for whether the call is synchronous or asynchronous.
After deriving models for all the functions in the system, we identify a set of ‘top-level’ functions; namely those that are invoked externally (by a network event, timeout, or user call). These fit into the component ClientX described previously, which in turn forms part of the system equation. Other than having the user pick

out which are the top-level functions, and how the network and users behave, the system equation can be constructed automatically – the synchronisation sets are just the interfaces we compute in the next section.

Functional Modelling
Up to now, we have looked only at how to compose a model of the system from its sequential components. These sequential components correspond to functions in the source code, and we will now describe their abstraction. There are two key ideas involved in this – that of abstracting the control-flow of the program, and that of abstracting the environment of its variables.
The steps that we will take can be summarised in four steps. Firstly, we convert the program to an abstract syntax, in Static Single Assignment (SSA) form. From here, we derive the control-flow state space. We define this as the fixed point of a reduction →f , but it can be viewed intuitively in terms of paths on the control-flow graph. Thirdly, we determine the data environment space. We abstract arithmetic expressions to intervals over the integers, for which considerations of independence are of vital importance. Finally, the PEPA model can be built. This involves deter- mining the probability of moving from one state to another, which is a conditional probability on the data environments of reachable states (in the control-flow).

Abstract Syntax and SSA
In order to proceed, we first convert the function (written into the subset of C that we defined) into an abstract syntax that will be easier to analyse. A function definition has the form f (X1,... , Xn) := C, where the body of the function is a command C, defined as follows:
C	:=	skip | return E | X := E | X := g(X1,... , Xn)
|	C1 ; C2 | (if B then C1 else C2); Φ | while Φ ; B do C
Here, X are variables (which we limit to integers and booleans), f, g are functions, E are arithmetic expressions, B are boolean expressions, and Φ are sequences of φ-functions, which will shortly be defined. E and B must be linear, so that any expression E can be written in the form i aiXi, with constants ai. Since we can convert a C function to this form, we will adopt this syntax from now on.
To simplify the analysis of variable dependencies, we convert the function into SSA form [20]. This ensures that each variable is only assigned to (statically) once, so we need not worry about a variable name being reused later, in an independent context. This transformation alters the control-flow graph in two ways. First, at each node in the control-flow graph with more than one incoming edge (a join node), and for each set of conflicting definitions of the same variable, we insert a φ-function, φ(d1,... , dn), such that φ(d1,... , dn)= di if the node is reached via the ith in-edge. We then rename all variables so that each is only statically assigned to once.

In our abstract language, each node will have at most two in-edges (nested if- statements have separate join nodes), so the arity of all φ functions will be two. If Φ is a sequence of n φ-functions, X1 := φ(Y1, Z1); ... ; Xn:= φ(Yn, Zn), we define the following projections:
ΦL = X1 := Y1 ; ... ; Xn:= Yn
ΦR = X1 := Z1 ; ... ; Xn:= Zn

Control-Flow State Derivation
We can now define a state in our abstract system as a 4-tuple, ⟨L, C, P, U ⟩, consisting of a label L, a command C, a predicate P and an update U . The predicate is a boolean expression on the program variables that is valid on entering the state. The update is a partial finite map from variables to expressions, indicating the change of state that takes place at that node. The command is the remainder of the program, to be executed after leaving the state.
To allow us to represent a function by these states, we introduce two more atomic commands; goto and call. The first of these specifies a set of labels, L1,... , Ln, which determine the set of reachable states that may follow. The second encodes the assignment of a variable X to a function call g, followed by a continuation L. The syntax of commands is extended as follows:
C	:=	goto{L1,... , Ln} | call(X, g, L)(E1,... , En)
Finally, we need a notion of environment variable. There is no need for every variable to be represented in the abstract environment, as some will be uniquely determined by the others. If this is the case, we can eliminate the variable by substituting for its definition, hence it will never need to appear in an update U . Informally, a variable is an environment variable if it is an input to the function, the return value of a function call, or if its definition reaches over the backward branch of a loop. Formally, we define a function I : C → X → X, which maps each environment variable onto an equivalence class:
I(skip)	= {}
I(return E)	= {}
I(X := E)	= {}
I(X := g(E1,... , En))	= {X '→ X}
I(X1 := φ(X2, X3))	= {X1 '→ X1, X2 '→ X1, X3 '→ X1}
I((if B then C1 else C2); Φ) = I(C1) ∪ I(C2) I(while B do Φ ; C)	= I(Φ) ∪ I(C)
I(C1 ; C2)	= I(C1) ∪ I(C2)
I(f (X1,... , Xn) := C)	= {X1 '→ X1,... , Xn '→ Xn}∪ I(C)
If X ∈ dom(I(C)) then X is an environment variable in C.  Furthermore, if
I(C)(X)= I(C)(Y ), then X and Y are the same environment variable. To simplify

notation, if we have a function f defined as f (X1,... , Xn) := C, then we define Vf to be f(f (X1,... , Xn):=C). In other words, Vf determines the environment variables of the function f .
We can now define the state space of an abstract function as the fixed point of a reduction, →f . This reduction takes a 4-tuple, which represents some state in the function’s execution, and partially evaluates the command. In general, this partial evaluation leads to a set of possible states, because the control-flow decisions are not completely determined statically. We define →f as follows, where L' and L''
are fresh labels:

⟨L, C, P, U ⟩	→f {⟨L, C, P, U ⟩} if C is atomic (call, goto or return)
⟨L, (skip) ; C, P, U ⟩	→f {⟨L, C, P, U ⟩}
⟨L, (return E); C, P, U ⟩	→f {⟨L, return E, P, U ⟩}
⟨L, (X := E); C, P, U ⟩	→f {⟨L, C{E/X}, P, U ⟩} if X ∈/ dom(Vf )
⟨L, (X := E); C, P, U ⟩	→f	L, C, P, U {E/Vf (X)}	if X ∈ dom(Vf )
⟨L, call(X, g, L')(E1,. .. , En), P, U ⟩ ,

⟨L, (X := g(E1,.. ., En)) ; C, P, U ⟩	→f
⟨L', C, T, {}⟩

⟨L, (if B then C else C ); Φ ; C , P, U ⟩ →
( ⟨L, C1 ; ΦL ; C3, P ∧ B, U ⟩ , )
  


⟨L, (if B then C1
else C2) ; Φ, P, U ⟩	→f
⟨L, C1, P ∧ B, U ⟩ ,
⟨L, C2, P ∧ ¬B, U ⟩
8>< ⟨L, goto{L', L''}, P, U ⟩ ,

9>=

⟨L, (while B do Φ ; C1) ; C2, P, U ⟩	→f
⟨L', C1 ; goto{L', L''},P ∧ B, {}⟩ ,
>: ⟨L'', C2, ¬B, {}⟩	>;

Note that we require that a function ends in a return instruction, so all other commands must be followed by another. The only exception to this is a conditional statement, which may appear as the final command if both branches terminate in a return instruction, hence the two cases above.
Informally, this reduction is amalgamating sequential states in the concrete control-flow graph, and expanding out conditional statements, so that each ab- stract state represents a path between two interaction points – namely calling or returning from a function, or reentering a loop. Importantly, in this abstraction, we only have one set of states for the body of a loop. Hence we can only model a loop if the probability of reentering it has a trivial dependency with respect to time (as we will see later).
To compute the fixed point of this reduction, we firstly define ⇒f as a reduction over sets of states:
t1 →f T1	...	tn →f Tn
{t1,... , tn} ⇒f T1 ∪ ... ∪ Tn
Now, the abstract state space £(f ) of a function f is defined as follows:
£(f )= T iff {⟨0, C, T, {}⟩} ⇒∗ T Λ 6T '.T ⇒∗ T ' ⇒ T ' = T
f	f
This fixed-point can be shown to exist by induction on the structure of C, under the assumption that C is well-formed; namely, 6T. {⟨0, C, T, {}⟩} ⇒∗ T ⇒ ET '.T ⇒f
T '. We will hereafter refer to £(f ) as the control-flow states of f .

Data Environments
As it stands, £(f ) is not sufficient to model the state space of the function. This is because the predicate at each state is the weakest condition that must hold, irrespective of how we arrived there. We therefore lose all memory of any stronger conditions that hold due to the particular path that was taken; namely the values of variables that do not affect the local control-flow decision. However, they may well affect future control-flow decisions, or need to be communicated (e.g. as a return value).
A further problem with these states is the difficulty in relating the predicates
P to one another. To connect the states together probabilistically, we need to

determine probabilities of the form Pr(P '
| P ), where P '
is the predicate P ' with

its variables updated by U . Essentially, this is the probability of one set of linear
constraints holding, given another, which can be difficult to determine. We therefore need to consider a data abstraction, in addition to £(f ).
Let Pf be the set of all predicates P in £(f ), expressed in the following normal form, where ❽ ∈ {<, ≤, =, ≥, >}:
  ,⎝  Σ aijkxijk  ❽ cij Λ  pj ⎞⎠

where aijk, cij are rational constants, xijk are integer variables, and pj are boolean variables (atoms). Now, let Ef be the set of all expressions k akxk in Pf , repre- sented in vector form, a · x, where x is the image of Vf , expressed as a vector, and a is a column vector of integers. If there are N unique such expressions, and M variables (i.e. x has dimensionality M × 1), then the M × N matrix A is defined by taking its rows to be the vectors a.
To determine the data environment, we need to find a basis for these vectors. Given our assumption that the environment variables have no hidden dependencies between one another (i.e. all dependencies are from conditions within the function in question), the independence of two expressions (in the absence of any other information) corresponds to orthogonality between their a-vectors. Hence, we can determine an optimal basis using Principle Components Analysis (PCA) [12]. This equates to performing singular value decomposition of the matrix A into UΣVT . Here, U is an M × M orthogonal matrix (i.e. its columns form an orthonormal basis), which can be viewed as a linear map from the target basis into the original. Hence UT is a linear map into the target basis:
A' = UT A = UT UΣVT = ΣVT
Hence A' is an M × N matrix whose columns represent the same expressions as in
A, but in the new basis U.
For each basis vector ui in U, we associate two sets of rationals, sU and sL called
i	i
the upper and lower closed splittings of ui. These splittings define a finite set of intervals, which will form the abstract data environment of the expression that ui

denotes. For example, if this expression is x − y, and the sets sU and sL are {0, 10}
i	i
and {10, 11} respectively, then the abstract environment records only whether the
value of x − y lies in the interval (−∞, 0], (0, 10), [10, 10], (10, 11) or [11, ∞).
More formally, an open interval (x, x) over the rationals denotes the set {q ∈
Q | x < q < x}, where x ∈ Q ∪ {−∞, ∞}. Similarly, a closed interval [x, x] denotes
{q ∈ Q | x ≤ q ≤ x}, for x ∈ Q, and the definitions of [x, x) and (x, x] are as expected. An interval space is a set I of intervals, such that  i ∈ I = Q and
define all the closed interval ends (for the upper and lower bounds on the interval
respectively), causing the open ends to follow uniquely.
To determine the splittings for each basis, we consider the expressions in e ∈ A'.
For all e of the form a'ui, then for all conditions e ❽ c, we examine the normalised
condition  e ❽'  c . If ❽' ∈ {<, ≥}, we add  c to sL, and if ❽' ∈ {>, ≤}, we add

a'	a'
a'	i

c to sU . Otherwise, for ❽' = ‘=’, we add  c to both sL and sU . These splittings
a'	i	a'	i	i
define the top-level abstract data environment. All remaining conditions are on
expressions of the form i a'ui. These define the remainder of the abstract data environment.
A state E in the abstract data environment completely determines the truth of all conditions in the function. We write E = ET Λ ES, where ET is the top-level environment, defining a box in the vector space that U determines. ES can be seen as a conjunction of the remaining predicates and their negations; namely a set of linear constraints that must hold within ET . We denote the abstract data environment space of a function f by E(f ).
PEPA Model Construction
We can now bring together the control-flow and data abstractions to build a PEPA model. In this model, each state identifies a (path, environment) pair. The rates depend on both the expected duration of a path (which is determined by basic block profiling), and the probability of moving from one environment to another. We describe this process more precisely as follows.
Recall that £(f ) is the set of all control-flow states, and E(f ) the set of all data environments, for the function f . Now, for S = ⟨L, C, P, U ⟩ ∈ £(f ) we will denote the projection operations by SL ... SU respectively. The states of the PEPA model are denoted State i,j, where i and j range over the control-flow and data states respectively, and are indices into the sets £(f ) and E(f ) respectively (under some ordering). For conciseness, we will herein refer to just £ and E, in relation to the function f .
In constructing the model, we need to determine a rate for each state transition. This is partly determined statically, by the probability of moving from one state to another (which we will show below how to calculate), but also dynamically by the average time taken to execute the instructions corresponding to that state. We can measure this execution time empirically using basic block profiling – namely, we profile the sequential instructions by running them multiple times, and averaging the duration with respect to a control. For a state S, we will denote the inverse of

this duration (i.e. its rate) by SR
In general, the probabilities we must calculate are of the form Pr(P | E), where P is a boolean expression containing linear conditions, and E is an environment. We can simplify matters by splitting the environments E into their two components, ET and ES, allowing us to compute the following:

Pr(P | ET

Λ ES
Pr(P Λ ES | ET )
)=	Pr(E  | E )

The top-level environment, ET , defines a rectilinear volume in n-dimensional space (where n is the number of basis vectors). Hence we are computing the probability of a set of linear conditions holding in this volume. To compute these probabilities generally, we apply a dart-throwing Monte Carlo method. The basic approach here is to take a sample of points from the environment defined by ES, and evaluate the compound condition (left hand side of the conditional probability) for each of them. The proportion of points for which the condition evaluates to true gives an estimate of the probability of the condition being true over the population.
We define a function reach(S) as the reachable control-flow states from S:
reach (⟨L, goto{L1,... , Ln}, P,U ⟩)	= {S|SL ∈ {L1 ... Ln}}
reach (⟨L, call(X, g, L)(E1,... , En), P,U ⟩) = {S|SL = L}
reach (⟨L, return E, P, U ⟩)	= {S|SL = 0}
When we define the PEPA processes for the states State i,j, we have three dif- ferent cases to consider. For S = £i, we look at the type of command, SC. For a goto instruction, the state change is entirely internal, and so the action will be of type τ . For call and return instructions, however, we are interacting with another function, and so need a public interface of actions. When the function gets called, it must enter an initial control-flow state. Since it relies on the arguments it was called with to determine this, we must encode the control-flow state into a set of call actions. Similarly, the return value is passed by a set of return actions. The function begins and ends in a special state Init, where it passively waits to be called. This is shown diagrammatically in Figure 3, where the black state is Init. Since it is the caller that must determine what environment to instantiate the function with, it needs to know what information the function requires. The interface f(f ) of a function f is a set of pairs, (α, P ), where α is an action and P is the predicate held by the action. In other words, if a function is called using α, P is the initial environment that will hold. Hence we define:
f(f )= {(call f,i,P )|SL =0 Λ SP = P Λ S = £(f )i}
When we return from a function, it is the caller that needs to specify the abstract en- vironment of the return value. We represent this calling context as a triple (R, l,X) of an interval space R, an index l into that space, and a variable X to update. R = Ri is the environment from which the function was called, and the callee eval- uates the probability of moving to other states in R when it returns. Note that




Fig. 3. The structure of a function interface
only certain components of the environment space will be modified by the function call; namely those relating the return value to other environment variables. Hence we can optimise the implementation by only considering certain states.
In general, it follows that calculating the return value of a function depends on both the state of the function and that of the caller. In other words, we need to represent the calling environment in some way to generate a model of the function. There are two ways of doing this:
Generate a separate model of the function for each calling context. This cor- responds to embedding the model of the function into that of the caller.
Use functional rates [11]. Here we only have one model of the function, in parallel with that of the caller, and the rates are expressed as a function of the state of the caller. To model this, we introduce an additional component, recording the calling context, which can be passively set by the caller, and queried by the function. We can think of this component as an oracle, allowing the correct probabilities of environment change to be represented in the model.
To simply the presentation that follows, we will make no assumption about which approach is taken; only that the calling context is available to the function.
We will now define the PEPA model of f . Firstly, the initial state of the function is defined as:

Init =	Σ	Σ	(call f,i, Pr(E'
| SP ).T).State i',j'

S'∈S|S' =0 E'⇒S' ∈E
L	P
where S' = £i' and E' = Sj'. Note that the probability is not of the form Pr(P | E), but we can calculate it in exactly the same way by splitting the predicate into an orthogonal component PT , and the remaining conditions PS, as we did for environ- ments in Section 5.3.
For all S = £i such that SC is a goto instruction:
State i,j =	Σ	Σ	(τ, Pr(E {SU (x)/x} | E).SR).State i',j'
S'∈reach (S) E'⇒S' ∈E

where S = £i, E = Sj, S' = £i' and E' = Sj' .
The states where SC is a ‘call(X, g, L)(E1,... , En)’ instruction are defined as:

def Σ	'



CallState
i,j
def
(α,P )∈I(g)
def Σ|E|
(α, Pr(P {E1/arg 1
... En
/arg
'
n} | E').r).ReturnState
'
i,j


where S = £i, E = Sj, E' = Sj' , and r is the rate of calling the function (determined empirically). Intuitively, CallState corresponds to invoking the function, after which we enter ReturnState where we wait for it to return.
Finally, when SC is a ‘return ER’ instruction, under the calling context (R, l,X) we define:


State

i,j
|R|
d=ef	(return
k=0
k, Pr(Rk

{ER

/X} | Rl

Λ E{SU

(x)/x}).SR

).Init

where S = £i and E = Sj.
Finally, having derived a model for each function in the system, we can compose them as described in Section 4.


Stenning’s Protocol – An Example
To illustrate the approach we have described, let us examine a simple protocol. Stenning’s protocol [13] provides reliable end-to-end delivery of packets in the sim- plest possible way. A source and a sink each locally maintain a sequence number for the connection. When the source sends a packet, it attaches its current sequence number to it. When the sink receives the packet, it checks whether the sequence number matches what it was expecting, and if so increments its sequence number and sends an acknowledgement to the source. The source will send the next packet if it receives a correct acknowledgement; otherwise, after a timeout, it will retransmit the last packet.
We implemented this protocol as a network simulator ns-2 [14] agent. For the purposes of this paper, we will consider an abstract version of the receive function of the source, ignoring the payload of the protocol, and some other specifics of the simulator. Here, pseq is the packet sequence number, and sseq is that of the source:
source recv(pseq) :=
if (pseq == sseq) then (sseq := sseq + 1) else skip;  := source send(sseq);
return 1;




Fig. 4. Utilisation and throughput of Stenning’s protocol, as the timeout varies
Following the algorithm in Section 5.2, the control flow states are as follows:

⟨0, call( , source send, 1)(sseq), pseq = sseq, {sseq '→ sseq + 1}⟩ ,
⟨0, call( , source send, 2)(sseq), pseq /= sseq, {}⟩ ,
⎪⎩ ⟨1, return 1, T, {}⟩ ,  ⟨2, return 1, T, {}⟩	⎪⎭

There is only one expression occurring in the above predicates, namely sseq−pseq. This has the interval space I = {(−∞, 0), [0, 0], (0, ∞)}, since the only comparison on this expression is to the constant zero. In this example, the data environment is trivial (in that there are no non-orthogonal components), and so we can proceed directly to construct the model. We will assume a calling context (R, l,X). Naming the control-flow states A to D, and for empirically-derived rates rcall , r1 and r2 we reach:


Init StateA
def
= (call source recv,1, T).StateA + (call source recv,2, T).StateB
def
= (τ, r1).CallStateA

CallStateA	def P
(α, Pr(P {sseq/arg } | sseq — pseq ∈ (0, ∞)).r
).ReturnStateA


ReturnStateA
=
def
=
def
(α,P )∈I(source send)	1
i(return source send,i, T).StateC
call

StateB	= (τ, r1).CallStateB

CallStateB	def P
(α, Pr(P {sseq/arg } | sseq — pseq ∈ [0, 0]).r
).ReturnStateB

ReturnStateB
=
def
=
(α,P )∈I(source send)	1
i(return source send,i, T).StateD
call

StateC	def P|R| (return
, Pr(Y {1/X} | Y Λ sseq — pseq ∈ (0, ∞)).r ).Init


StateD	def P|R| (return
, Pr(Y {1/X} | Y Λ sseq — pseq ∈ [0, 0]).r ).Init

Note that the probability calculations in CallStateA and CallStateB are functional rates, since they depend upon the state of the sink. Due to space considerations, we cannot describe this process in detail, but it is essentially the same as that used for transmitting the return value of a function call.
Constructing a similar model for the sink, and composing these together with a network and timer as described in Section 4, we can build a complete model of the system. Some results from the analysis (setting all rates to 1) are shown in Figure 4. The second graph shows how the timeout rate affects the throughput; if we timeout too slowly, we have to wait a long time after a packet was lost to get the retransmission, but if we timeout too quickly, we send too many unnecessary retransmissions, therefore wasting bandwidth.

Conclusions
In this paper, we presented an abstract interpretation from source code to a perfor- mance model. There is still much work to be done in formalising this with respect to the errors involved, and it seems that a proper formulation in the context of abstract interpretation [5] would be appropriate. By this, we refer to the static component of the abstraction, which maps to a domain of labelled probabilistic automata. In- corporating the empirically derived rates, SR, takes us to a PEPA model. Note that this is different to the probabilistic abstract interpretation of [19,16], which is denotational rather than state-based.
Furthermore, we have not considered the size of the models that can be gener- ated. Whilst we cannot avoid the state space explosion problem if we remain in a Markovian setting, there are many existing simplification techniques [4,6,15] to re- duce the size of the models. One promising approach that we are currently pursuing is the use of stochastic bounds [8] at the level of a PEPA model, to compositionally construct upper and lower lumpable bounding matrices for the model.
The ultimate aim of this work is to produce a tool for semi-automatic derivation of performance models from real code. We did not mention any implementation details in this paper, but at the time of writing, we are completing a prototype implementation of the abstractions described, in the context of network simulator ns-2 [14] agents. This will allow us to validate the models we generate against simulation results, and is the first step towards a tool that can deal with ‘native’ protocols written in C.
Whilst this work is still in its early stages, the abstraction techniques we have considered seem to be feasible, and future work looks to be promising. This is certainly a tool that is needed, and would be widely appreciated by both the software engineering and performance evaluation communities. Although there are many challenges yet to be faced, we have taken the first few steps, and look forward to continuing along this path.

References
T. Ball, E. Bounimova, B. Cook, V. Levin, J. Lichtenberg, C. McGarvey, B. Ondrusek, S. K. Rajamani, and A. Ustuner. Thorough static analysis of device drivers. In EuroSys’06: European Systems Conference, 2006.
T. Ball and S. K. Rajamani. The SLAM toolkit. In Conference on Computer Aided Verification, 2001.

C. Canevet, S. Gilmore, J. Hillston, M. Prowse, and P. Stevens. Performance modelling with UML and stochastic process algebras. IEE Proceedings: Computers and Digital Techniques, 150(2):107–120, March 2003.
W. Cao and W. J. Stewart. Iterative aggregation/disaggregation techniques for nearly uncoupled Markov chains. Journal of the ACM, 32(3):702–719, July 1985.

P. Cousot and R. Cousot. Abstract interpretation: A unified lattice model for static analysis of programs by construction or approximation of fixpoints. In POPL, pages 238–252, 1977.
T. Dayar and W. J. Stewart. Quasi lumpability, lower-bounding coupling matrices, and nearly completely decomposable markov chains. SIAM J. Matrix Anal. Appl., 18(2):482–498, 1997.


V. Firus, S. Becker, and J. Happe. Parametric performance contracts for QML-specified software components. In Proceedings of Formal Foundations of Embedded Software and Component-Based Software Architectures, Electronic Notes in Theoretical Computer Science. ETAPS 2005, 2005.
J. M. Fourneau, M. Lecoz, and F. Quessette. Algorithms for an irreducible and lumpable strong stochastic bound. Linear Algebra and its Applications, 386:167–185, 2004.
T. A. Henzinger, R. Jhala, R. Majumdar, and G. Sutre. Software verification with Blast. In Proceedings of the Tenth International Workshop on Model Checking of Software (SPIN), Lecture Notes in Computer Science 2648, pages 235–239. Springer-Verlag, 2003.
J. Hillston. A Compositional Approach to Performance Modelling. Cambridge University Press, 1996.
J. Hillston and L. Kloul. Formal techniques for performance analysis: Blending SAN and PEPA. Formal Aspects of Computing, 2006.
I. T. Jolliffe. Principle Component Analysis. Springer Series in Statistics. Springer, 2002.
N. A. Lynch. Distributed Algorithms. Morgan Kaufmann Publishers Inc., San Francisco, CA, USA, 1996.
S. McCanne and S. Floyd. The Network Simulator, ns-2. http://www.isi.edu/nsnam/ns.
V. Mertsiotakis. Approximate Analysis Methods for Stochastic Process Algebras. PhD thesis, Institut fu¨r Mathematische Maschinen und Datenverarbeitung, 1998.
D. Monniaux. Abstract interpretation of probabilistic semantics. In Seventh International Static Analysis Symposium (SAS’00), number 1824 in Lecture Notes in Computer Science, pages 322–339. Springer Verlag, 2000. Extended version on the author’s web site.
G. C. Necula, S. McPeak, S. P. Rahul, and W. Weimer. CIL: Intermediate language and tools for analysis and transformation of C programs. In Proceedings of Conference on Compiler Construction, pages 213–228, 2002.
G. C. Necula, S. McPeak, and W. Weimer. CCured: Type-safe retrofitting of legacy code. In Proceedings of the Principles of Programming Languages, 2002.
A. Di Pierro and H. Wiklicky. Probabilistic abstract interpretation and statistical testing. In PAPM- PROBMIV, pages 211–212, 2002.
B. K. Rosen, M. N. Wegman, and F. K. Zadeck. Global value numbers and redundant computations. In Proceedings of 15th ACM Symposium on Principles of Programming Languages, pages 12–27. ACM Press, 1988.
