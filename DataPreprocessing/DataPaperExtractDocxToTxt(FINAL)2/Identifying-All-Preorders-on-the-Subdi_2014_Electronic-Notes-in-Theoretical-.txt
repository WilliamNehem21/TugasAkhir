Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 308 (2014) 309–327
www.elsevier.com/locate/entcs

Identifying All Preorders on the Subdistribution Monad
Tetsuya Sato1
Research Institute for Mathematical Sciences Kyoto University
Kyoto, Japan

Abstract
The countable valuation monad, the countable distribution monad, and the countable subdistribution monad are often used in the coalgebraic treatment of discrete probabilistic transition systems. We identify preorders on them using a technique based on the preorder TT-lifting and elementary facts about pre- orders on real intervals preserved by convex combinations. We show that there are exactly 15, 5, and 41 preorders on the countable valuation monad, the countable distribution monad, and the countable subdis- tribution monad respectively. We also give concrete definitions of these preorders. By applying Hesselink and Thijs’s/ Hughes and Jacobs’s construction to some preorder on the countable subdistribution monad, we obtain probabilistic bisimulation between Markov chains ignoring states with deadlocks.
Keywords: coalgebras, preorders, monads, probabilistic transition systems, probabilistic bisimulation


1	Introduction
We completely identify preorders on the countable valuation monad V, the count- able distribution monad D=1, and the countable subdistribution monad D on Set respectively. We list the main results of this paper:
There are exactly 15 preorders on the monad V, and they are generated from 4 preorders ±0, ±1, ±2, and ±3 (Section 4).
There are exactly 5 preorders on the monad D=1, and they are generated from the equality EqQ=1 and the support-inclusion ±s (Section 5).
There are exactly 41 preorders on the monad D, and they are generated from 5 preorders ±r, ±s, ±d, ±m, and ±M (Section 6).
To identify preorders on V, it is enough to analyse preorders at the singleton type. To identify preorders on D and D=1, it is enough to analyse preorders at

1  Email:satoutet@kurims.kyoto-u.ac.jp

http://dx.doi.org/10.1016/j.entcs.2014.10.017
1571-0661/© 2014 Elsevier B.V. All rights reserved.

the Boolean type.
Our task is identifying the class Pre(T ) of preorders on a monad T (T = V, D=1, D). We focus on the component ±I of each ± ∈ Pre(T ) at a set I. The component ±I is a preorder on TI that satisfies congruence and substitutivity. We denote by CSPre(T, I) the set of such preorders on TI. We introduce the mapping (−)I : Pre(T ) → CSPre(T, I) that extracts components at I from preorders on T . We calculate preorders on T from CSPre(T, I) by the left adjoint ⟨−⟩I and the right adjoint [−]I of the mapping (−)I , and we analyse the sandwiching situation
⟨≤⟩I Œ±Œ [≤]I for each ≤∈ CSPre(T, I), where Œ is the component-wise inclusion order for preorders on T .
We identify Pre(V), Pre(D=1), and Pre(D) as the following steps:
We identify the sets CSPre(V, 1), CSPre(D=1, 2), and CSPre(D, 1). Then, the class Pre(V) is identified by applying [8, Lemma 7].
We calculate the mappings ⟨−⟩I and [−]I for (T, I) = (D=1, 2) and (T, I) = (D, 1). We then identify Pre(D=1) by proving ⟨−⟩2 = [−]2. To finish identifying Pre(D), we analyse the remaining preorders ± ∈ Pre(D) such that ⟨±1⟩1 Œ
± Œ [±1]1 by using preorders on D=1.
In [8], Katsumata and the author developed a method to identity preorders on monads, but it is not applied well to the monads V, D=1, and D. In this paper, we introduce the following new ideas to identify Pre(V), Pre(D=1), and Pre(D): in (i) of the above steps, we use Lemma 1.1 to identify congruent and substitutive preorders on the inﬁnite sets V1, D=12, and D1. In (ii), we introduce the left adjoint
⟨−⟩I of the mapping (−)I , and we use the sandwiching situation ⟨≤⟩I Œ ± Œ [≤]I to identify Pre(D=1) and Pre(D).
This work is motivated by a mathematical interest. The author has not found interesting applications of the main results of this work yet, but at least, we have the following contribution: By applying preorders on D to methods in [5,7,8], we discuss coalgebraic simulations between probabilistic transition systems, and obtain probabilistic bisimulations ignoring states with deadlocks between Markov chains (Section 7).

Background
Preorders on monads are equivalent to pointwise preorder enrichments on their Kleisli categories. A suitable partial order on a monad gives a coalgebraic trace se- mantics [4] and forward/backward simulations between coalgebras [3]. In the studies [5,7], simulations between coalgebras are given from preorders on coalgebra functors systematically. Many of them involve preorders on monads (e.g. the inclusion order P(A × −)).
In the study [10], precongruences on a typed language with nondeterminism (or) and a divergent term are determined completely, and they are almost equivalent to preorders on the composite monad PL of the powerset monad P and the monad L given by L = 1 + Id [8]. From this point of view, in other words, our work is seen as

the variant of [10] for probabilistic languages: behavioural precongruences on the language with subprobabilistic choice i∈I pi(−i) and the probabilistic conditional expression for the ground type X correspond to congruent substitutive preorders on DX.

Preliminaries
Throughout this paper, we work on the category Set of sets and functions. For a monad (T, η, μ) on Set and a function f : X → TY , the Kleisli Lifting f : TX → TY of f is the composition f = μ ◦ T (f ).
For each set X, we denote by TX the trivial relation X ×X on X, and denote by EqX the equality/diagonal relation on X. We denote by Rop the opposite relation of R.
We will use the complete semiring ([0, ∞], +, ·, 0, 1) for the countable valuation monad; it has arbitrary summations, and an infinite sum is the least upper bound with respect to the standard order ≤ of all finite partial sums [2, Volume A, pp. 124–125, denoted by R+].
The following lemma is crucial to analyse preorders.
Lemma 1.1 Let 0 < N < ∞. If ≤ is a preorder on the interval [0,N ] that is preserved by convex combinations; in other words, the preorder ≤ satisﬁes
(p1 ≤ q1 ∧ p2 ≤ q2 ∧ t ∈ [0, 1]) =⇒ tp1 + (1 − t)p2 ≤ tq1 + (1 − t)q2
then p ≤ q for some 0 < p < q < N implies r ≤ s for each 0 < r < s < N.
Proof. Suppose p ≤ q and 0 < p < q < N . First, we construct the monotone decreasing sequence {an}n∈N and the monotone increasing sequence {bn}n∈N such that limn→∞ an = 0, limn→∞ bn = N , and 0 < an < bn < N and an ≤ bn for each n ∈ N.
Let α = p/q. We define the sequence {an}n∈N by an = αnp = pn+1/qn. Since 0 < α < 1, the sequence {an}n∈N is monotone decreasing, and it converges to 0. Since ≤ is preserved by convex combinations, and 0 ≤ 0 holds from the reflexivity of ≤, for each n ∈ N we obtain
an+1 = αn+1p + (1 − αn+1) · 0 ≤ αn+1q + (1 − αn+1) · 0= an.

Let β = (N − q)/(N − p). We define the sequence {bn}n∈N by bn = βnp + (1 − βn)N . Since 0 < β < 1, p < N , and q < N , the sequence {bn}n∈N is monotone increasing, and it converges to N . Since N ≤ N holds, and ≤ is preserved by convex combinations, for each n ∈ N we obtain
bn = βnp + (1 − βn)N ≤ βnq + (1 − βn)N = bn+1.

Since a0 = p = b0, we obtain 0 < an < bn < N and an ≤ bn for each n ∈ N.

Next, we suppose 0 < r < s < N . There is m ∈ N such that am < r < s < bm
since limn→∞ an = 0 and limn→∞ bn = N . Let


s − r
γ =
bm − am
,	c =	rbm − sam	.
(r − s)+ (bm − am)

It is obvious that 0 < γ < 1 and 0 < c hold. We prove c < N as follows:
N ((r − s)+ (bm − am)) − (rbm − sam)= (N − s)(N − am) − (N − bm)(N − r) > 0. Since c ≤ c and am ≤ bm hold, and ≤ is preserved by convex combinations,
r = γam + (1 − γ)c ≤ γbm + (1 − γ)c = s.



Fig. 1. The picture of proof of Lemma 1.1 (in the case of m = 2)


Monads for Probabilistic Branching
We first introduce some notations: the sum d[U ] of d : X → [0, ∞] over U ⊆ X is defined to be	x∈U d(x). The support of d : X → [0, ∞] is defined by supp(d) = 
{ x ∈ X | d(x) /=0 }. The zero distribution 0 is defined by 0(x) = 0. The Dirac
distribution δx is defined by δx(x)=1 and δx(y)=0 (x /= y).
Next, we define the three monads V, D, and D=1 on Set as follows:

Definition 2.1 • We denote by (V, ηV , μV ) the countable valuation monad
that is defined as follows:	the functor part V  is defined by VX	=
{ d : X → [0, ∞] | ω ≥ |supp(d)|} for each set X and Vf (d)(y) =	x∈f−1(y) d(x)
for each f : X → Y and y ∈ Y . The unit and multiplication are defined by
ηV (x)= δx and (μV (ξ))(x)= Σ	ξ(d) · d(x) (x ∈ X).
The countable subdistribution monad (D, ηQ, μQ) is defined as follows: for each set X, DX = { d : X → [0, 1] | d[X] ≤ 1 }, and the unit and the multiplication are inherited from the countable valuation monad.
The countable distribution monad (D=1, ηQ=1 , μQ=1 ) is defined as follows: for each set X, D=1X = { d : X → [0, 1] | d[X]=1 }, and the unit and the multiplication are inherited from the subdistribution monad.
We remark that the condition ω ≥ |supp(d)| is automatically obtained from
d[X]=1 (d[X] ≤ 1) in the definitions of the (sub)distribution monad. The probabilistic branching is characterised coalgebraically by D:
A Markov chain is characterised as ξ1 : X → DX.
A probabilistic transition system is characterised as ξ2 : X → D(A × X).
A Segala automaton [11] is characterised as ξ3 : X → PD(1 + A × X).
Since DX ∼= D=1(1 + X), we obtain the notion of deadlocks in the probabilistic branching. For example, a Markov chain ξ : X → DX has a deadlock at a state x ∈ X when ξ(x)[X] < 1. For further examples, see [12].
The Class of Preorders on a Monad
We introduce some results of [8], which we use to identify preorders on monads. We fix a monad (T, η, μ) on Set. We denote it by T for simplicity.
We define the congruence and substitutivity of preorders on TI and preorders on the monad T , the latter of which correspond bijectively to pointwise preorder enrichments of the Kleisli category SetT of T .
Definition 3.1 Let I be a set, and let ≤ be a preorder on TI. (i) We call ≤ congruent if (∀j ∈ J.f (j) ≤ g(j)) =⇒ (∀x ∈ TJ.f (x) ≤ g (x)) for each set J and functions f, g : J → TI. (ii) We call ≤ substitutive if f is a monotone function on (TI, ≤) for each f : I → TI.
We write (CSPre(T, I), ⊆) for the set of congruent and substitutive preorders on TI, ordered by inclusions. It is closed under opposites and intersections, and it has the greatest and least preorders TTI and EqTI respectively.
Definition 3.2 ([8, Definition 3]) A preorder ± on a monad T is an assignment of a preorder ±I on TI to each set I such that (i) each ±I is congruent, and (ii) for each f : J → TI, f is a monotone function from (T J, ±J ) to (TI, ±I ) (we also call this property substitutivity ).
For example, the assignment ± that is defined by A±XB ⇐⇒ A ⊆ B is indeed

a preorder on the powerset monad P.
We write (Pre(T ), Œ) for the class of preorders on T , ordered by the partial order Œ defined by ± Œ ±j ⇐d⇒ef ∀I.±I ⊆ ±j . It is closed under these opposites and

intersections, which are defined by (±op)X = (±X )op and ( 
λ∈Λ
±λ)X = 
λ∈Λ
±λ ,

and it has the least and greatest preorders: the equality EqT defined by EqT =

EqTX and the trivial preorder TT defined by TT
= TTX .

For each preorder ± on T , we call ±I the evaluation at I of ±. The eval- uation mapping (−)I : ± '→ ±I is a monotone mapping from (Pre(T ), Œ) to (CSPre(T, I), ⊆). It has both the right and left adjoints.

(CSPre(,T,,I), ⊆)

⟨−⟩I E (−)I E [−]I
J	J 
(Pre(T ), Œ)

Fig. 2. Right and left adjoints of the evaluation mapping (−)I : ± '→ ±I

The right adjoint [−]I of the evaluation mapping (−)I is defined by
x [≤]I y ⇐⇒ ∀f : X → TI.f (x) ≤ f (y).

The mapping [−]I is monotone, and it preserves opposites and intersections. We remark that it preserves the empty-intersection, that is, [TTI ]I = TT .
Proposition 3.3 ([8, Theorem 3]) For each I, (−)I E [−]I and [−]I = Id.
Hence, the preorder [≤]I on T is the greatest one whose evaluation at I equals
≤ for each ≤∈ CSPre(T, I).
The left adjoint ⟨−⟩I of the evaluation mapping (−)I is defined by
⟨≤⟩I =   {± ∈ Pre(T ) | ±I = ≤} .

The preorder ⟨≤⟩I on T is the least one whose evaluation at I equals ≤ for each
≤ ∈ CSPre(T, I) since Pre(T ) is closed under intersections, and [≤]I = ≤ holds. By using this, we easily obtain that the mapping ⟨−⟩I is monotone, that it preserves opposites, and that the adjunction ⟨−⟩I E (−)I holds.
Lemma 3.4 Let ≤ ∈ CSPre(T, I). If [≤]I = ⟨≤⟩I then the preorder [≤]I the
unique preorder whose evaluation at I equals ≤.
We here introduce the opposite-intersection operators on Pre(T ) and
CSPre(T, I). The one on CSPre(T, I) is given as follows:
CCSPre(T,I)(K)=   L ∩   M op L, M ⊆ K , where K ⊆ CSPre(T, I)

The opposite-intersection closure operator on Pre(T ) is given in a similar way as the above (we denote it by C∩,op	). We often write C∩,op for simplicity.
Main Results
Theorem 3.5 Preorders on V, D=1, and D are identiﬁed as follows:
Pre(V)= C∩,op {±0, ±1, ±2, ±3} ∼= CSPre(V, 1) ∼= 15 where


d1 ±0
d1 ±1
d1 ±2
d1 ±3
d2 ⇐d⇒ef	supp(d1) ⊆ supp(d2)
d2 ⇐d⇒ef	∀x ∈ X.(d1(x) ≤ d2(x))
d2 ⇐d⇒ef	∀x ∈ X.(d1(x)= d2(x) ∨ d2(x)= ∞)
d2 ⇐d⇒ef	∀x ∈ X.(d1(x) ≤ d2(x) ∧ (d1(x)=0 =⇒ d2(x) ∈ {∞, 0})).

Pre(D=1)= C∩,op {±s, EqQ=1 } ∼= CSPre(D=1, 2) ∼= 5 where
d1 ±s d2 ⇐d⇒ef	supp(d1) ⊆ supp(d2).
Pre(D)= C∩,op {±r, ±s, ±d, ±m, ±M } ∼= CSPre(D, 2) =∼ 41 where


d1 ±r d1 ±s d1 ±d
d2 ⇐d⇒ef	∀x ∈ X.d1(x) ≤ d2(x), d2 ⇐d⇒ef	supp(d1) ⊆ supp(d2),
d2 ⇐d⇒ef	(d1[X]=1 =⇒ d2[supp(d1)]= 1),

d1 ±m d2 ⇐d⇒ef	(d1[X]=1 =⇒ d2 = d1),
d1 ±M d2 ⇐d⇒ef	(d1[X]=1 =⇒ (d2[X]=1 ∧ supp(d1)= supp(d2))). We prove (i), (ii), and (iii) of Theorem 3.5 in Section 4, 5, and 6.
Preorders on the Countable Valuation Monad
Preorders on a semiring-valued ﬁnite multiset monad are pointwise [8, Lemma 7 and Theorem 8]. The following lemma holds by applying this fact to the monad V with a slight change of cardinality of supports to countable.
Lemma 4.1 Each ± ∈ Pre(V) satisﬁes d1 ±X d2 ⇐⇒ ∀x ∈ X.d1(x) ±1 d2(x), where 1= {∗}. Moreover, CSPre(V, 1) ∼= Pre(V).
Hence, it suffices to identify CSPre(V, 1) to identify Pre(V). We regard V1 as [0, ∞] by the correspondence between each d ∈ V1 and the value d(∗) ∈ [0, ∞]. For each ≤∈ CSPre(V, 1), the substitutivity of ≤ is equivalent to
(p ≤ q ∧ t ∈ [0, ∞]) =⇒ tp ≤ tq,
and the congruence of ≤ is equivalent to
∀i ∈ I.(pi ≤ qi ∧ ti ∈ [0, ∞]) =⇒ Σ piti ≤ Σ qiti.


Hence, each ≤∈ CSPre(V, 1) is preserved by convex combinations.
We partition the set V1 × V1	∼=	[0, ∞] × [0, ∞]  into  EqV1,	R0	=
{ (0, q) | q ∈ (0, ∞) }, R1 = { (p, q) | 0 < p < q < ∞ }, R2 = {(0, ∞)}, R3 =
{ (p, ∞) | p ∈ (0, ∞) }, R4 = R0op, R5 = R1op, R6 = R2op, and R7 = R3op.


R2	R3
EqV 1







Fig. 3. The partitions EqV1, R0, R1,..., R7 of V 1 × V1

By using Lemma 1.1, we obtain Lemma 4.2 and 4.3.
Lemma 4.2 Let ≤∈ CSPre(V, 1). We obtain the following properties:
p ≤∞ for some 0 < p < ∞ if and only if r ≤∞ for all 0 < r ≤ ∞. This is equivalent to R3 ∩≤ /= ∅ =⇒ R3 ⊆ ≤.
0 ≤∞ if and only if r ≤ s for all 0 ≤ r ≤ ∞.
This is equivalent to R2 ∩≤ /= ∅ =⇒ R2 ∪ R3 ⊆ ≤.
p ≤ q for some 0 < p < q < ∞ if and only if r ≤ s for all 0 < r < s ≤ ∞. This is equivalent to R1 ∩≤ /= ∅ =⇒ R1 ∪ R3 ⊆ ≤.
0 ≤ q for some 0 < q < ∞ if and only if r ≤ s for all 0 ≤ r < s ≤ ∞. This is equivalent to R0 ∩≤ /= ∅ =⇒ R0 ∪ R1 ∪ R2 ∪ R3 ⊆ ≤.
Lemma 4.3 Let ≤ ∈ CSPre(V, 1). We obtain ≤ = EqV 1 ∪	i∈I Ri where I =
{ i ∈ {0, 1,..., 7}| Ri ∩≤ /= ∅ }.
We prepare the following congruent substitutive preorders on V1:
p ≤0 q ⇐d⇒ef	(p > 0 =⇒ q > 0)
p ≤1 q ⇐d⇒ef	(p ≤ q)
p ≤2 q ⇐d⇒ef	(p = q) ∨ (q = ∞)
p ≤3 q ⇐d⇒ef	(p ≤ q) ∧ (p =0 =⇒ q ∈ {∞, 0})
Proposition 4.4 We obtain CSPre(V, 1) = C∩,op ≤0, ≤1, ≤2, ≤3} ∼= 15.
Proof (Sketch). Let ≤ ∈ CSPre(V, 1). We define R(p0, p1,..., p7) = EqV 1 ∪
{ Ri | pi = true }. By Lemma 4.3, we obtain ≤ = R(p0, p1,..., p7) where pi ⇐⇒ Ri ∩≤ /= ∅ (i ∈ {0, 1,..., 7}). From Lemma 4.2 and the transitivity of ≤, the octuple (p0, p1,..., p7) should satisfy the following formula:
P = (p0 =⇒ p1 ∧ p2) ∧ (p1 ∨ p2 =⇒ p3)
∧ (p3 ∧ p7 =⇒ p1 ∧ p5) ∧ (p2 ∧ p7 =⇒ p0) ∧ (p3 ∧ p6 =⇒ p4)
∧ (p4 =⇒ p5 ∧ p6) ∧ (p5 ∨ p6 =⇒ p7).

We remark that the last 2 clauses of P are given by applying the opposite order ≤op to Lemma 4.2. It is easy to check that there are exactly 15 satisfying assignments of P and that the following inclusion holds:
15 ∼= { R(p0, p1,..., p7) | (p0, p1,..., p7) satisfies P }⊆ C∩,op {≤0, ≤1, ≤2, ≤3}.
Since	CSPre(V, 1)	⊆	{ R(p0, p1,..., p7) | (p0, p1,..., p7) satisfies P }	and
≤0, ≤1, ≤2, ≤3 ∈ CSPre(V, 1), we conclude this proposition.	 
Theorem 4.5 (Theorem 3.5(i)) Let ±i be the pointwise ordering generated from
≤i (i ∈ {0, 1, 2, 3}). We obtain Pre(V)= C∩,op ±0, ±1, ±2, ±3} ∼= 15.
Proof. It is proved immediately from Lemma 4.1 and Proposition 4.4.	 

Preorders on the Distribution Monad
First, we identify CSPre(D=1, 2) where 2 = {0, 1}. We regard D=12 as [0, 1] by the correspondence between each d = d(0)δ0 + (1 − d(0))δ1 ∈ D=12 and the value d(0) ∈ [0, 1]. For each ≤∈ CSPre(D=1, 2), the substitutivity of ≤ is equivalent to
p ≤ q =⇒ ∀t, u ∈ [0, 1].((t − u)p + u ≤ (t − u)q + u),
and the congruence of ≤ is equivalent to
(∀i ∈ I.(pi ≤ qi) ∧ Σ ti = 1) =⇒ Σ piti ≤ Σ qiti.

Hence, each ≤∈ CSPre(V, 1) is preserved by convex combinations.
We partition the set D=12×D=12 ∼= [0, 1]×[0, 1] into EqQ=12, R0 = { (0, 1), (1, 0) }, R1 = { (p, q) | p ∈ {0, 1}, 0 < q < 1 }, R2 = { (p, q) | p, q ∈ (0, 1),p /= q }, and R3 = R1op.







0

Fig. 4. The partitions EqD=1 2, R0, R1, R2, and R3 of Q=12 × Q=12
By using Lemma 1.1, we obtain Lemma 5.1 and 5.2.
Lemma 5.1 Let ≤∈ CSPre(D=1, 2). We obtain the following properties:
p ≤ q for some 0 < p < q < 1 if and only if r ≤ s for all r, s ∈ (0, 1). This is equivalent to R2 ∩≤ /= ∅ =⇒ R2 ⊆ ≤.
0 ≤ q for some 0 < q < 1 if and only if r ≤ s for all (r, s) ∈ [0, 1] × (0, 1). This is equivalent to R1 ∩≤ /= ∅ =⇒ R1 ∪ R2 ⊆ ≤.

0 ≤ 1 if and only if r ≤ s for all r, s ∈ [0, 1].
This is equivalent to R0 ∩≤ /= ∅ =⇒ R0 ∪ R1 ∪ R2 ∪ R3 ⊆ ≤.
Lemma 5.2 Let ≤ ∈ CSPre(D=1, 2). We obtain ≤ = EqQ=12 ∪	i∈I Ri where
I = { i ∈ {0, 1, 2, 3}| Ri ∩≤ /= ∅ }.
Proposition 5.3 . We have the following identiﬁcation:


CSPre(D=1, 2) = C∩,op {≤ , EqQ=1
2} = {TQ=1
2, EqQ=1
2, ≤s, ≤sop, ≤s ∩ ≤sop} =∼ 5,

where p ≤s q ⇐d⇒ef  (p /= q) =⇒ (0 < q < 1).
Proof (Sketch). Analogous to Lemma 4.4, by Lemma 5.1 and 5.2 and the transi- tivity of ≤, for each ≤∈ CSPre(D=1, 2), there is a quadruple (p0, p1, p2, p3) of truth values which satisfies the following formula:
P = (p0 =⇒ p1 ∧ p2 ∧ p3) ∧ (p1 =⇒ p2) ∧ (p1 ∧ p3 =⇒ p0) ∧ (p3 =⇒ p2)
and the union R(p0, p1, p2, p3)= EqQ=12 ∪ { Ri | pi = true } is equal to the given preorder ≤. It is easy to check that there are exactly 5 satisfying assignments (p0, p1, p2, p3) of P and that the following inclusion holds:


5 ∼= { R(p0, p1, p2, p3) | (p0, p1, p2, p3) satisfies P }⊆ C∩,op {≤s, EqQ
2}.

Since	CSPre(D=1, 2)	⊆	{ R(p0, p1, p2, p3) | (p0, p1, p2, p3) satisfies P }	and

≤s, EqQ
2 ∈ CSPre(D=1, 2), we conclude this proposition.	 

Next, we calculate the mapping [−]2 : CSPre(D=1, 2) → Pre(D=1). Since it

preserves intersections and opposites, and CSPre(D=1, 2) = C∩,op {≤s, EqQ
2}, it

suffices to identify the preorders [EqQ=1
Proposition 5.4 The preorders [EqQ
2]2 and [≤s]2 (we denote it by ±s).
2]2 and ±s are identiﬁed as follows:

d1 [EqQ=1
2]2
=1
d2 ⇐⇒ d1 = d2.

d1 ±s
d2 ⇐⇒ supp(d1) ⊆ supp(d2).

Next, we calculate the mapping ⟨−⟩2 : CSPre(D=1, 2) → Pre(D=1).
Lemma 5.5 Let ≤ ∈ CSPre(D=1, 2) and α ∈ [0, 1]. If d1, d2 ∈ D=1X satisfy the following condition: for each y ∈ X such that d1(y) > d2(y),

 α + (1 − α) d2(y)  δ
+ (1 − α) 1 − d2(y)  δ

≤ d2(y) δ
+  1 − d2(y)  δ

d1(y)	0
d1(y)	1
d1(y) 0
d1(y)	1



then (αd1 + (1 − α)d2) ⟨≤⟩2
d2 holds.

Proof. Let Y = { x ∈ X | d1(x) > d2(x) }. We assume d1 /= d2 without loss of generality. This implies Y /= ∅, X \ Y /= ∅, and	d (x) − d (x) > 0. We obtain
Σx∈X\Y d2(x) − d1(x)= Σx∈Y d1(x) − d2(x) since d1[X]= d2[X]= 1.

Hence, the following distribution d3 ∈ D=1X is well-defined:

1
d = Σ


Σ (d (x) − d (x))δ .


From the assumption of this lemma, for each y ∈ Y we obtain

 α + (1 − α) d2(y)  δ
+ (1 − α) 1 − d2(y)  δ

≤ d2(y) δ
+ 1 − d2(y)  δ .

d1(y)	0
d1(y)	1
d1(y) 0
d1(y)	1



We denote by cy and cJ
the left-hand and right-hand side of the above inequality

respectively for each y ∈ Y . We define the mapping fy : 2 → D=1X by fy(0) = δy and fy(1)= d3 for each y ∈ Y .
From the substitutivity of ⟨≤⟩2, we obtain f (c ) ⟨≤⟩2 f (cJ ) for each y ∈ Y .
y  y	X  y  y

We define e
= f (c ) and eJ
= f (cJ ) for each y ∈ Y . They are calculated as

y	y y
y	y  y

e =  α + (1 − α) d2(y)  δ + (1 − α) 1 − d2(y)  d ,
y	d1(y)	y	d1(y)	3

eJ = d2(y) δ
+ 1 − d2(y)  d .

y	d1(y) y	d1(y)	3


We define g, gJ : X → D=1X by g(y) = ey and gJ(y) = eJ g(x)= gJ(x)= δx for each x ∈ X \ Y . We obtain g(x) ⟨≤⟩2
for each y ∈ Y , and
gJ(x) for each x ∈ X.

From the the congruence of ⟨≤⟩2, we obtain g (d1) ⟨≤⟩2 gJ (d1).
We obtain g (d1)= αd1 + (1 − α)d2 by

g (d1)= Σ d1(y)ey + Σ
d1(x)δx

y∈Y	x∈X\Y
= Σ(αd1(y)+ (1 − α)d2(y))δy + (1 − α) Σ(d1(y) − d2(y))d3 + Σ d1(x)δx

y∈Y
y∈Y
x∈X\Y

= Σ(αd1(y)+ (1 − α)d2(y))δy + (1 − α) Σ (d2(x) − d1(x))δx + Σ d1(x)δx

y∈Y
= αd1 + (1 − α)d2.
x∈X\Y
x∈X\Y

Similarly (apply α = 0 to the above calculation), we obtain gJ (d1)= d2. Therefore,
we conclude (αd1 + (1 − α)d2)= g (d1) ⟨≤⟩2 gJ (d1)= d2.	 
Proposition 5.6 The mapping ⟨−⟩2 equals the mapping [−]2.
Proof (Sketch). We prove the case ≤ = ≤s ∩ ≤sop, and omit the other cases. (Case: ≤ = ≤s ∩ ≤sop) Suppose d1[≤]2 d2. By Lemma 5.4, it is equivalent to supp(d1) = supp(d2). This implies for each y ∈ X such that d1(y) > d2(y),

  d1(y)+ d2(y) δ

+ d1(y) − d2(y) δ

  ≤  d2(y) δ

+ d1(y) − d2(y) δ  .

2d1(y)	0
2d1(y)	1
d1(y) 0
d1(y)	1

By Lemma 5.5 with α = 1/2, we obtain (d1 + d2)/2 ⟨≤⟩2
d2. Similarly, we also have

d1 ⟨≤⟩2
(d1 + d2)/2. Thus, d1 ⟨≤⟩2
d2. Therefore, [≤]2 = ⟨≤⟩2 holds.	 

Theorem 5.7 (Theorem 3.5(ii)) We obtain the following identiﬁcation:
Pre(D=1)= C∩,op {±s, EqQ=1 } = {TQ=1 , EqQ=1 , ±s, ±sop, ±s ∩ ±sop} ∼= 5.
Proof. It is proved from Lemma 3.4, Proposition 5.4, 5.3, and 5.6.	 

Preorders on the Subdistribution Monad
First, we identify CSPre(D, 1). We regard D1 as [0, 1] by the correspondence between each d ∈ D1 and the value d(∗) ∈ [0, 1]. For each ≤ ∈ CSPre(D, 1), the substitutivity of ≤ is equivalent to
(p ≤ q ∧ t ∈ [0, 1]) =⇒ tp ≤ tq,
and the congruence of ≤ is equivalent to
∀i ∈ I.(pi ≤ qi) ∧ Σ ti ≤ 1 =⇒ Σ piti ≤ Σ qiti.

Hence, each ≤∈ CSPre(D, 1) is preserved by convex combinations.
We partition the set D1 × D1 ∼= [0, 1] × [0, 1] into EqQ1, R0 = {(0, 1)}, R1 =
{ (0, q) | 0 < q < 1 }, R2 = { (p, 1) | 0 < p < 1 }, R3 = { (p, q) | 0 < p < q < 1 },
R4 = R0op, R5 = R1op, R6 = R2op, and R7 = R3op.

Fig. 5. The partition EqD1, R0, R1,..., R7 of Q1 × Q1
By using Lemma 1.1, we obtain Lemma 6.1 and 6.2.
Lemma 6.1 Let ≤∈ CSPre(D, 1). We obtain the following properties:
p ≤ q for some 0 < p < q < 1 if and only if r ≤ s for all 0 < r < s < 1. This is equivalent to R3 ∩≤ /= ∅ =⇒ R3 ⊆ ≤.
0 ≤ q for some 0 < q < 1 if and only if r ≤ s for all 0 ≤ r < s < 1. This is equivalent to R1 ∩≤ /= ∅ =⇒ R1 ∪ R3 ⊆ ≤.
p ≤ 1 for some 0 < p < 1 if and only if r ≤ s for all 0 < r < s ≤ 1. This is equivalent to R2 ∩≤ /= ∅ =⇒ R2 ∪ R3 ⊆ ≤.
0 ≤ 1 if and only if r ≤ s for all 0 ≤ r < s ≤ 1.
This is equivalent to R0 ∩≤ /= ∅ =⇒ R0 ∪ R1 ∪ R2 ∪ R3 ⊆ ≤.

Lemma 6.2 Let ≤ ∈ CSPre(D, 1). We obtain ≤ = EqQ1 ∪	i∈I Ri where I =
{ i ∈ {0, 1,..., 7}| Ri ∩≤ /= ∅ }.
We prepare the following congruent substitutive preorders on D1:
p ≤r q ⇐d⇒ef	p ≤ q
p ≤s q ⇐d⇒ef	p > 0 =⇒ q > 0
p ≤d q ⇐d⇒ef	p =1 =⇒ q =1 
The superscripts r, s, and d stand for real values, supports, and deadlocks of dis- tributions respectively. We let ≤sd= ≤s ∩ ≤d for simplicity.
Proposition 6.3 We obtain CSPre(D, 1) = C∩,op {≤r, ≤s, ≤d} ∼= 25.
Proof (Sketch). Analogous to Lemma 4.4, by Lemma 6.1 and 6.2 and the transi- tivity of ≤, for each ≤∈ CSPre(D, 1), there is an octuple (p0, p1,..., p7) of truth values which satisfies the formula P = Pj ∧ P jj where
Pj = (p0 ⇐⇒ (p1 ∧ p2)) ∧ ((p1 ∨ p2) =⇒ p3), P jj = (p4 ⇐⇒ (p5 ∧ p6)) ∧ ((p5 ∨ p6) =⇒ p7)
and the union R(p0, p1,..., p7) = EqQ1 ∪  { Ri | pi = true } is equal to the given preorder ≤. It is easy to check that there are 25 satisfying assignments (p0, p1,..., p7) of P and that the following inclusion holds:
25 ∼= { R(p0, p1,..., p7) | (p0, p1,..., p7) satisfies P }⊆ C∩,op {≤r, ≤s, ≤d}.
Since	CSPre(D, 1)	⊆	{ R(p0, p1,..., p7) | (p0, p1,..., p7) satisfies P }	and
≤r, ≤s, ≤d ∈ CSPre(D, 1), we conclude this proposition.	 
Next, we calculate the mapping [−]1 : CSPre(D, 1) → Pre(D). Since it pre- serves intersections and opposites, and CSPre(D, 1) = C∩,op {≤r, ≤s, ≤d} holds, it suffices to identify the preorders [≤r]1, [≤s]1, and [≤d]1 (e.g. [≤d ∩ ≤sop]1 = [≤d]1 ∩ [≤s]1op). Let ±r= [≤r]1, ±s= [≤s]1, and ±d= [≤d]1.
Proposition 6.4 The preorders ±r, ±s, and ±d are identiﬁed as follows:

d1 ±r
d1 ±s
d1 ±d
d2 ⇐⇒ ∀x ∈ X.d1(x) ≤ d2(x). d2 ⇐⇒ supp(d1) ⊆ supp(d2).
d2 ⇐⇒ (d1[X]=1 =⇒ d2[supp(d1)] = 1).

Next, we calculate the mapping ⟨−⟩1 : CSPre(D, 1) → Pre(D). Generally speaking, ⟨−⟩I : CSPre(T, I) → Pre(T ) needs not preserve intersections, but the mapping ⟨−⟩1 : CSPre(D, 1) → Pre(D) preserves intersections.
Proposition 6.5 The mapping ⟨−⟩1 satisﬁes the following:
The mapping ⟨−⟩1 preserves intersections and opposites.

⟨≤r⟩1 = ±r, ⟨≤s⟩1 = ±s, and  ≤d 1 = ±m where ±m is deﬁned by
d1 ±m d2 ⇐d⇒ef	(d1[X]=1 =⇒ d1 = d2).

By Proposition 6.3 and 6.5, we obtain that the preorder ⟨≤⟩1 is identified com- pletely for each ≤∈ CSPre(D, 1) (e.g. ≤d ∩ ≤sop 1 = ±m ∩ ±sop).
The following lemma and is crucial to identify the mapping ⟨−⟩1.
Lemma 6.6 Let ≤∈ CSPre(D, 1). If d1, d2 ∈ DX satisfy the condition:
∀x ∈ supp(d ).  1+ d1[X] ≤ (1 + d1[X]) min(d1, d2)(x)	(1)

1	2	2
d1(x)



then we obtain d1 ⟨≤⟩1
min(d1, d2).

Here, min(d1, d2) ∈ DX is defined by min(d1, d2)(x)= min(d1(x), d2(x)).
Proof. We may assume d1 /= 0 since min(d1, d2)= 0 whenever d1 = 0. We recall
⟨≤⟩1 = ≤. From the substitutivity of ⟨≤⟩1, for each x ∈ supp(d1),


1+ d1[X] δ
⟨≤⟩1
(1 + d1[X]) min(d1, d2)(x) δ .

2	x	X	2	d1(x)	x
We define the functions f, g : X → DX as follows: for each x ∈ supp(d1),


f (x)= 1+ d1[X] δ
and g(x)= (1 + d1[X]) min(d1, d2)(x) δ

2	x	2	d1(x)	x


and f (x) = g(x) = 0 for each x ∈ X \ supp(d1). It is obvious f (x) ⟨≤⟩1
g(x) for

each x ∈ X. From the congruence of ⟨≤⟩1, we obtain
d = f 	2	 d  ⟨≤⟩1 g 	2	 d  = min(d ,d ).
1	1+ d1[X] 1	X	1+ d1[X] 1	1	2
We remark that 2d1/(1 + d1[X]) ∈ DX because 2d1[X]/(1 + d1[X]) ≤ 1.	 
Proof of Proposition 6.5 (Sketch). First, we prove ±m ∈ Pre(D). Since ±m =
≤d, the image of the mapping (−)1 under C∩,op {±r, ±s, ±m} is CSPre(D, 1). Next,

we check d1 ⟨±1⟩1
min(d1, d2) ⟨±1⟩1
d2 for each d1 ±X d2 by applying Lemma 6.6

for each ±∈ C∩,op {±r, ±s, ±m}.
For instance, we check the following case.
(case: ± = ±m) We have ±1 = ≤d.  Suppose d1 ±X d2, that is, d1[X] =
1 =⇒ d1 = d2. Thus, we may assume d1[X] < 1. This implies (1 + d1[X])/2 < 1,

and hence 1+d1[X] ≤d (1+d2[X]) min(d1,d2)(x)
for each x ∈ supp(d1). By Lemma 6.6,

2	2	d1(x)

d  ≤d 1 min(d ,d ). Next, we have (1+d2[X]) min(d1,d2)(x)
≤d 1+d2[X]
for each x ∈

supp(d ) since min(d ,d ) ≤ d . By Lemma 6.6, min(d ,d ) ≤d 1 d .	 

We see that the mappings [−]1 and ⟨−⟩1 coincide on the subset C∩,op {≤r, ≤s} of CSPre(D, 1), and that they differ from each other. Hence, there is a preorder ± on D such that ⟨≤⟩1 Œ ± Œ [≤]1, and hence ±1 = ≤. The following proposition tells
that there are exactly 4 such preorders.
Proposition 6.7 Let ≤ ∈ CSPre(D, 1). If ≤ is one of ≤d, ≤dop, ≤d ∩ ≤sop, and
≤dop ∩ ≤s,a preorder ±∈ Pre(D) such that ⟨≤⟩1 Œ±Œ [≤]1 is determined uniquely
/	/
as follows:
≤ = ≤d =⇒ ± = ±M ,	≤ = ≤dop =⇒ ± = ±M op,
≤ = ≤d ∩ ≤sop =⇒ ± = ±M ∩ ±sop,	≤ = ≤dop ∩ ≤s =⇒ ± = ±M op ∩ ±s,
where, the preorder ±M ∈ Pre(D) is deﬁned by
d1 ±X d2 ⇐d⇒ef	(d1[X]=1 =⇒ (d2[X]=1 ∧ supp(d1) = supp(d2))).
Otherwise, a preorder ±∈ Pre(D) such that ⟨≤⟩1 Œ ± Œ [≤]1 does not exist.
/	/
To prove this proposition, we introduce the following restriction mapping C. Let τ : D=1 ⇒D be the natural transformation defined by τX (d)= d for each d ∈ D=1X. For each ±∈ Pre(D), we define its restriction C(±) by
C(±)X = { (d1, d2) ∈ D=1X × D=1X | τX (d1) ±X τX (d2) } .
The following lemma shows that the restriction C(−) is a monotone mapping from (Pre(D), Œ) to (Pre(D=1), Œ) since the monotonicity of C is obvious.
Lemma 6.8 For each ±∈ Pre(D), C(±) is indeed a preorder on D=1.
Lemma 6.9 Let ≤∈ CSPre(D, 1) and ±∈ Pre(D) with ⟨≤⟩1 Œ ± Œ [≤]1.
/	/
(d1[X] < 1 ∨ d2[X] < 1) =⇒ (d1 [≤]1 d2 ⇐⇒ d1 ±X d2 ⇐⇒ d1⟨≤⟩1 d2)
C(⟨≤⟩1) Œ C(±) Œ C([≤]1)
/	/
Proof. (proof of (i)) We first prove d1 [≤]1 d2 ⇐⇒ d1 ⟨≤⟩1 d2 whenever d1[X] < 1
X	X
or d2[X] < 1. Suppose a pair d1[≤]1 d2 such that d1[X] < 1 or d2[X] < 1. Since
the mappings ⟨−⟩1, [−]1, and C(−) preserve intersections and opposites, it suffices

to check d1 ⟨≤⟩1
d2 in the following 3 cases:

(case: ≤ = ≤r) Since ⟨≤r⟩1 = [≤r]1, it is obvious that d1 ⟨≤⟩1
(case: ≤ = ≤s) Since ⟨≤s⟩1 = [≤s]1, it is obvious that d1 ⟨≤⟩1
d2.
d2.

(case: ≤ = ≤d) Suppose d1 [≤d]1 d2, that is, d2[X] < 1 =⇒ d1[X] < 1. Since

d [X] < 1 or d [X] < 1, we obtain d [X] < 1. This implies d
 ≤d 1 d .

Since ⟨≤⟩1 Œ ± Œ [≤]1, for each ≤∈ CSPre(D, 1), we conclude


(d1[X] < 1 or d2[X] < 1) =⇒ (d1 [≤]1
d2 ⇐⇒ d1 ±X d2 ⇐⇒ d1 ⟨≤⟩1
d2).

(proof of (ii)) From (i) of this lemma, ⟨≤⟩1 Œ ± Œ [≤]1 implies the following:
/	/

d1[X]= d2[X]=1 and (d1, d2) ∈/ ⟨≤⟩1
holds for some X and d1 ±X d2.

d3[Y ]= d4[Y ]=1 and (d3, d4) ∈/±Y holds for some Y and d3 [≤]1
d4.

The former implies C(⟨≤⟩1)X ŒC(±)X because there is dj ∈ D=1X such that τ (dj)= d for each d ∈ DX such that d[X] = 1, and the latter implies C(±)Y Œ C([≤]1)Y similarly. These imply C(⟨≤⟩1) Œ C(±) Œ C([≤]1).	 
/	/
Hence, each preorder ± ∈ Pre(D) such that ⟨≤⟩1 Œ ± Œ [≤]1 is determined by
/	/
preorders on D=1 between C(⟨≤⟩1) and C([≤]1) and the preorder [≤]1. Then, we obtain the preorder ±M , which is the unique preorder between ±m and ±d. It is easy to check ±M is indeed a preorder on D.
Proof of Proposition 6.7 (Sketch). In the first 4 cases, C(⟨≤⟩1) = EqQ=1 and C([≤]1) ∈ {±s, ±sop}. Thus, C(±) = ±s ∩ ±sop by Lemma 6.9 (ii). Hence, the preorder ± is determined uniquely by Lemma 6.9 (i). Otherwise, ⟨≤⟩1 Œ ± Œ [≤]1
/	/
contradicts Lemma 6.9 (ii) since C([≤]1)= ±s ∩±sop or C([≤]1)= C(⟨≤⟩1) holds. 
We have finished identifying Pre(D).
Theorem 6.10 (Theorem 3.5(iii)) The set Pre(D) is identiﬁed as Table 1 be- low. Moreover, we obtain Pre(D)= C∩,op {±r, ±s, ±d, ±m, ±M } ∼= 41.

Table 1
The table of CSPre(Q, 1) and Pre(Q) (we omit opposite preorders)

Proof. It is proved immediately from Proposition 6.3, 6.4, 6.5, and 6.7.	 
The next lemma tells that CSPre(D, 2) is enough to identify Pre(D).
Theorem 6.11 We obtain Pre(D) ∼= CSPre(D, 2).
Proof (Sketch). By Lemma 3.3 and [8, Lemma 3], it suffices to check ±2 /= ±j
whenever both ± /= ±j and ±1 = ±j1 hold. This is straightforward.	 

For each ≤∈ CSPre(D, 1), possible preorders on D whose evaluation at 1 equal
≤ are show in Table 1.
In fact, Pre(D) is the opposite-closure of the collection of all preorders on
D in the right column of Table 1.	To check that the opposite-closure equals

r	s	d	m	M
Q	r	r op	M
M op
d	dop

C∩,op {± , ± , ± , ± , ±
}, we remark Eq
= ± ∩±	, ±	∩± 
= ± ∩±	,

±s ∩ ±M = ±s ∩ ±d, ±r Œ ±s ∩ ±m, and ±m Œ ±M Œ ±d.
/	/	/
Thus, Table 1 shows that there are exactly 7 equivalence relations on D: TQ, EqQ, ±s ∩ ±sop, ±m ∩ ±mop, ±d ∩ ±dop, ±m ∩ ±mop ∩ ±s ∩ ±sop, and ±d ∩ ±dop ∩
±s ∩ ±sop. There are exactly 9 partial orders on D: EqQ, ±r, ±r ∩ ±sop, ±r ∩ ±dop,
±r ∩ ±sop ∩ ±dop, and their opposite partial orders.
Remark 6.12 In the paper [13], Sokolova and Woracek proved that there are ex- actly 5 congruences on the convex algebra [0, 1] ∼= D1. This fact corresponds to that there are exactly 5 equivalence relations in CSPre(D, 1), namely EqQ1, TQ1,
≤s ∩ ≤sop, ≤d ∩ ≤dop, and ≤s ∩ ≤sop ∩ ≤d ∩ ≤dop.
Congruent and substitutive preorders on DX are equivalent to precongruences on the convex algebra DX over |X|-dementional vector space with an orthonormal basis { ex | x ∈ X }. The precongruence needs to be closed under linear operators T such that ∀x ∈ X.((  Tex  1 ≤ 1) ∧ (0 ≤ T ex)), where   x∈X pxex 1 =  x∈X |px|
(1-norm).  Thus generally speaking, there are more congruences on the convex
algebra DX ⊆ [0, 1]X than equivalence relations in CSPre(D,X) when 2 ≤ |X|. When X ∼= 1, the closedness under operators is implied by the closedness under convex combinations, and hence congruences on [0, 1] equal equivalence relations in CSPre(D, 1).
Coalgebraic Simulations between Markov Chains
Simulations between coalgebras are defined coalgebraically by using relational lift- ings of coalgebra functors. In this section, we focus on simulations between Markov chains (i.e. D-coalgebras). We focus on the relational liftings of D that are constructed from preorders on D by the method in [5,7]. For a given preorder
±∈ Pre(D), we construct the relational lifting D(±) of D by


D(±)(R)= ±
◦{ (Dπ (d), Dπ (d)) ∈ DX × DY | d ∈ D(R) }◦± 

X	1	2	Y
where π1 : R → X and π2 : R → Y are projections from a relation R ⊆ X × Y .
We apply the preorders EqQ, ±r, ±s, ±s ∩ ±sop, ±m, ±M , and ±d on D to the construction D(—). The first four cases are seen in earlier studies.
D(EqD )-simulation, that is, D-bisimulation in [1, Section 3] is a coalgebraic for- mulation of probabilistic bisimulation [9]. This fact is shown in [1].
The study [3] shows that D(±r )-simulations coincide Jonsson-Larsen simulations over Markov chains.
It is easy to see that a relation R is a D(±s)-simulation between Markov chains (X, ξ) and (Y, ξj) if and only if it is a simulation between two P-colagebras

(X, supp ◦ ξ) and (Y, supp ◦ ξj) in the standard sense. We call D(±s)-simulations
support-simulations. See also [7, Example 4.5(4)].

Analogous to D
(±s)
-simulations, D
(±s∩±sop)
-simulations are obtained from bisim-

ulation between two P-colagebras. See also [7, Example 6.4]. We call D(±s∩±sop)- simulations support-bisimulations.
When we apply the the remaining three preorders ±m, ±M , and ±d on D to the construction D(—), we obtain the notion of probabilistic bisimulations, support- bisimulations, and reverse support-simulations ignoring states with deadlocks be- tween Markov chains.
For two Markov chains (X, ξ) and (Y, ξj), a relation R ⊆ X × Y is:
(±m)
a D	-simulation if and only if



(x, y) ∈ R ∧ ξ(x)[X]=1 =⇒ (ξ(x), ξj(y)) ∈ D
(EqD)
(R).

This is seen as a probabilistic bisimulation ignoring states with deadlocks.
(±M )
a D	-simulation if and only if
(x, y) ∈ R ∧ ξ(x)[X]=1 =⇒ (ξ(x), ξj(y)) ∈ D(±s∩±sop)(R). This is seen as a support-bisimulation ignoring states with deadlocks.
(±d)
a D	-simulation if and only if
(x, y) ∈ R ∧ ξ(x)[X]=1 =⇒ (ξ(x), ξj(y)) ∈ D(±sop)(R).
This is seen as a reverse support simulation ignoring states with deadlocks.
We give an example of D(±m)-simulation. We consider two Markov chains (X, ξ) and (Y, ξj) and their start states x ∈ X and y ∈ Y as Fig. 6. The dashed arrows are a D(±m)-simulation R between x and y. First, since the state x has a deadlock, the states x and y are assumed to be probabilistic bisimilar unconditionally. Next, since transitions started from the state xj has no deadlock, the state yj must be

probabilistic bisimilar to the state xj
in the sense of D
(EqD)
-(bi)similarity.




1

4
x′	y′	•

•	•	•	•
1	1	1	1

(±m )

Fig. 6. A Q	-simulation between Markov chains (X, ξ) and (Y, ξ )


Future Work
We have the following future work at this time:

We expect to analyse preorders on other monads. For example, the convex module monad CM [6,14] that captures discrete probabilistic branching combined with nondeterminism.
We expect to obtain preorders on the composite monad ST of monads S and T
by using a distributive law δ : TS ⇒ ST from preorders on S and T .
Acknowledgement
The author thanks to Shin-ya Katsumata for valuable comments and stimulating discussions, Paul Blain Levy for constructive suggestions, and Naohiko Hoshino, Ichiro Hasuo, and Corina Cˆırstea for helpful comments.

References
E.P de Vink and J.J.M.M Rutten. Bisimulation for probabilistic transition systems: a coalgebraic approach. Theoretical Computer Science, 221(1 - 2):271 – 293, 1999.
Samuel Eilenberg. Automata, languages, and machines. Pure and Applied Mathematics. Elsevier Science, 1974.
Ichiro Hasuo. Generic forward and backward simulations ii: Probabilistic simulation. In Paul Gastin and Fran¸cois Laroussinie, editors, CONCUR 2010 - Concurrency Theory, volume 6269 of Lecture Notes in Computer Science, pages 447–461. Springer Berlin Heidelberg, 2010.
Ichiro Hasuo, Bart Jacobs, and Ana Sokolova. Generic trace semantics via coinduction. Logical Methods in Computer Science, 3(4), 2007.
Wim H. Hesselink and Albert Thijs. Fixpoint semantics and simulation. Theor. Comp. Sci, 238:200–0, 2000.
Bart Jacobs. Coalgebraic trace semantics for combined possibilitistic and probabilistic systems, 2008. Proceedings of the Ninth Workshop on Coalgebraic Methods in Computer Science (CMCS 2008).
Bart Jacobs and Jesse Hughes. Simulations in coalgebra. Electronic Notes in Theoretical Computer Science, 82(1):128–149, 2003. CMCS’03, Coalgebraic Methods in Computer Science (Satellite Event for ETAPS 2003).
Shin-ya Katsumata and Tetsuya Sato. Preorders on monads and coalgebraic simulations. In Frank Pfenning, editor, Foundations of Software Science and Computation Structures, volume 7794 of Lecture Notes in Computer Science, pages 145–160. Springer Berlin Heidelberg, 2013.
Kim Guldstrand Larsen and Arne Skou. Bisimulation through probabilistic testing. Information and Computation, 94(1):1–28, 1991.
Paul Blain Levy. Boolean precongruences, 2009. Manuscript.
Roberto Segala. A compositional trace-based semantics for probabilistic automata. In Insup Lee and Scott Smolka, editors, CONCUR ’95: Concurrency Theory, volume 962 of Lecture Notes in Computer Science, pages 234 – 248. Springer Berlin / Heidelberg, 1995. 10.1007/3-540-60218-6 17.
Ana Sokolova. Probabilistic systems coalgebraically: A survey. Theor. Comput. Sci., 412(38):5095– 5110, 2011.
Ana Sokolova and Harald Woracek. Congruences of convex algebras. Technical report, Vienna University of Technology, 2012. Submitted, ASC Report 39.
Daniele Varacca and Glynn Winskel. Distributing probability over non-determinism. Mathematical Structures in Computer Science, 16(1):87–113, 2006.
