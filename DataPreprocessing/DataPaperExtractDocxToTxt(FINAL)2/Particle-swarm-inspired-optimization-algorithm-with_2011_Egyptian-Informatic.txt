
ORIGINAL ARTICLE

Particle swarm inspired optimization algorithm without velocity equation
Mahmoud Mostafa El-Sherbiny

Dept. of Quantitative Analysis, Faculty of Business Administration, King Saud University, Saudi Arabia

Received 7 September 2010; accepted 5 January 2011
Available online 31 March 2011

Abstract This paper introduces Particle Swarm Without Velocity equation optimization algorithm (PSWV) that significantly reduces the number of iterations required to reach good solutions for optimization problems. PSWV algorithm uses a set of particles as in particle swarm optimization algorithm but a different mechanism for finding the next position for each particle is used in order to reach a good solution in a minimum number of iterations. In PSWV algorithm, the new position of each particle is determined directly from the result of linear combination between its own best position and the swarm best position without using velocity equation. The results of PSWV algo- rithm and the results of different variations of particle swarm optimizer are experimentally com- pared. The performance of PSWV algorithm and the solution quality prove that PSWV is highly competitive and can be considered as a viable alternative to solve optimization problems.
© 2011 Faculty of Computers and Information, Cairo University.
Production and hosting by Elsevier B.V. All rights reserved.



Introduction

Particle Swarm Optimization (PSO) is a new evolutionary com- putation technique motivated from the simulation of social
behavior and originally designed and developed by Eberhart
and Kennedy [1–3]. The population in the PSO is called a swarm and each individual is called a particle [4]. It is inspired by the behavior of bird flocking and fish schooling. A large number of birds or fish flock synchronously, change direction

		suddenly, and scatter and regroup together. Each particle ben-

E-mail address: msherbiny@ksu.edu.sa

1110-8665 © 2011 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.

Peer review under responsibility of Faculty of Computers and Information, Cairo University.
doi:10.1016/j.eij.2011.02.004

efits from the experience of its own and that of the other mem- bers of the swarm during the search for food.
PSO algorithm has a number of desirable properties, includ- ing simplicity of implementation, scalability in dimension, and good empirical performance. So, it is an attractive choice for solving nonlinear programming problems. PSO algorithm had been applied to solve capacitor placement problem [5], short- term load forecasting [6], soft-sensor [7], to estimate the voltage stability of the electric power distribution systems [8,9] to direct the orbits of discrete chaotic dynamical systems towards desired target region [10] and to solve the permutation flowshop sequencing problem [11].

PSO has been successfully used as an alternative to other evolutionary algorithms in the optimization of D-dimen- sional real functions. Particles move in a coordinated way through the D-dimensional search space towards the opti- mum function. Their movement is influenced not only by
cient is to prevent the velocity to grow out of bounds, with the advantage that, theoretically, velocity clamping is no long- er required. As a result of this study, the velocity Eq. (1) changes to Eq. (3).
vid(t + 1)= v(vid(t)+ b1r1(p (t)— xid(t)) + b2r2(p (t)— xid(t)))

each particle’s own previous experience, but also by a social compulsion to move towards the best position found by its
id	id
(3)

neighbors. To implement these behaviors, each particle is de- fined by its position and velocity in the search space. In each iteration, changes resulting from both influences in the par- ticle’s trajectory are made to its velocity. The particle’s posi- tion is then updated accordingly to the calculated velocity. PSO, its main variants and the structural model behind it are extensively discussed in [12]. Some work is done that al- ters basic particle motion with some success, such as by El_Sherbiny [13,14], Li-Yong Wan and Wei Li, [15], Hui Wang et al. [16], Jianhua Liu et al. [17], and Yi Jiang and Qingling Yue [18] . But, the possibility for improvement in this area is still open.
This paper aims to introduce a Particle Swarm Without Velocity equation optimization algorithm (PSWV) and to dis- cuss the results of experimentally comparing the performance of its versions with the standard particle swarm optimizer (PSO) [19,13,14].
The rest of the paper is organized as follows: in Section 2, the PSO algorithm is described. In Sections 3 and 4 PSWV algorithm and its convergence study are exposed. Test func- tions and test conditions are presented in Section 5. In Section 6, optimization test experiments are illustrated. In Section 7, experimental results are reported, and are discussed in Section
8. Finally, conclusion is reported in Section 9.

Particle swarm optimization

Let us assume an n-dimensional search space, S ffin and de- notes the size of the swarm population. Each particle represents a candidate solution and has the following attributes: (a) its
current position in the search space xi = (xi1, xi2, .. ., xin) ∈ S,
(b) its current own best position pi = (pi1, pi2, .. ., pin), (c) the global best position, pg = (pg1, pg2, .. ., pgn), i.e., the position of the best particle that gives the best fitness in the entire pop- ulation; and (d) its current velocity i = (i1, i2, .. ., in), which rep-
resents its position change. For each particle in the swarm in the next iteration step, the velocity is updated using (1), hence the position is updated using Eq. (2).
vid(t + 1)= xvid(t)+ b1r1(pid (t)— xid(t))
+ b2r2(pid (t)— xid(t))	(1)
xid(t + 1)= xid(t)+ vid(t + 1)	(2)
The original PSO formulas developed by Kennedy and Eber- hart [3] were combined by Shi and Eberhart [23] with the intro- duction of an inertia parameter, x, that was shown empirically to improve the overall performance of PSO. Clerc and Ken- nedy provided a theoretical analysis of particle trajectories to ensure convergence to a stable point [12],
b r P (t)+ b r P 
Several interesting variations of the PSO algorithm have re- cently been proposed by researchers in [13–18]. Many of these PSO improvements are essentially extrinsic to the par- ticle dynamics at the heart of the PSO algorithm. One of these variations is the combined particle swarm optimization algorithm (CPSO) that presented by El_Sherbiny [13]. In the CPSO algorithm the particle updates its velocity according to Eq. (4) instead of Eq. (2), where (Xg) is the global best position, (X2g) is the previous global best position and R1, R2 ∈ U[0, 1].

Vi(t)= aVi(t — 1)+ b1r1(Xpi — Xi(t)) + b1r1 (R1Xg + R2X2g)— Xi(t)
(4)
Another variation of PSO is the Modified Particle Swarm Optimizer (MPSO) [14]. In the MPSO algorithm, the particle updates its velocity according to Eq. (5) instead of Eq. (4) where constriction coefficient v is presented.
Vi(t)= v(Vi(t — 1)+ b1r1(Xpi — Xi(t)) + b2r2((R1Xg + R2X2g)— Xi(t)))
(5)

PSWV algorithm

The Particle Swarm without velocity (PSWV) equation optimi- zation Algorithm is built based on the random linear combina- tion between the local best position and the global best position, which is represented by Eq. (6). The particles fly be- tween its own previous best position and the global best posi- tion. It means that the new position of each particle is allocated in the area between its own current local best and the current global best.
xi(t + 1)= c1r1pi(t)+ c2r2pg(t)	(6)
where r1 and r2 are defined as the combination weights, r1, r2 ∈ U[0, 1], and b1, b2 ∈ [0, 1] are defined as the own and the social attraction coefficients. pi(t) is the particle best position and pg(t) is the global best position at generation t.
In the PSWV algorithm, we do not need the velocity equation, which implies that Eqs. (1) and (2) be replaced with Eq. (6). The main steps of PSWV algorithm is illustrated in Fig. 1.

Convergence analysis of PSWV algorithm

For simplicity Eq. (6) can be rewritten as follows:
xi(t + 1)= k1Pi(t)+ k2Pg	(7)

v = 1 1  i
2 2  g

b1r1 + b2r2
The main result of this work is the introduction of the constric- tion coefficient and different classes of constriction models. The objective of this theoretically derived constriction coeffi-
where c1r1 = k1 and c2r2 = k2. Consider that k1 + k2 = 1, Eq.
(7) can be translated into Eq. (8)

xi(t + 1)= kPi(t)+ (1 — k)Pg	(8)

We will calculate the limits of xi(t + 1) at t tend to infinity as follows:
lim xi(t + 1)= lim (ktxi(0)+ (1 — kt)Pg)
t→+∞	t→+∞
lim xi(t + 1)= lim ktxi(0)+ lim (1 — kt)Pg

t→+∞
t→+∞
t→+∞

lim xi(t + 1)= lim ktxi(0)+ lim (—ktPg)+ lim Pg

t→+∞
t→+∞
t→+∞
t→+∞

	t lim xi(t + 1)= Pg	(13)
From Eq. (13) we can conclude that xi(t + 1) will converge to
Pg.

Test functions and test conditions






Figure 1	Main steps of PSWV algorithm.



For simplicity we can consider that Pi(t)= xi(t). So, in the ini- tial equation conditions Pi(0) = xi(0) that means that at the start iteration Eq. (8) would take the following form (Eq. (9)).
xi(1)= kxi(0)+ (1 — k)Pg	(9)
at t = 1 Eq. (9) will be
xi(2)= kxi(1)+ (1 — k)Pg	(10)
Substituting xi(1) from (8) and Eq. (10) yield Eq. (11) as follows:
xi(2)= k[kxi(0)+ (1 — k)Pg]+ (1 — k)Pg
x (2)= k x (0)+ k(1 — k)P + (1 — k)P
In order to know how competitive the PSWV algorithm is, we decided to compare its two versions (PSWV1 and PSWV2) against many versions of particle swarm algorithms as follows: Two versions of PSO algorithm (PSO1 and PSO2) that is rep- resented in [19], two versions of Modified PSO algorithm (MPSO1 and MPSO2) that is represented in [13], and two ver- sions of Combined PSO algorithm (CPSO1 and CPSO2) that is represented in [14]. Five benchmarking functions were selected to examine the performance of such Algorithms. The consid- ered benchmark functions were used in [20,21,19]. The bench- mark functions and its dimensions are illustrated in Table 1, while, its admissible range of the variable (x), the goal values, and the optimal solution are summarized in Table 2.
Two parameter sets (Eqs. (1)–(3)) v = a and b = b1 = b2 were selected to be used in the test based on the suggestions in other literature where these values have been found, empir- ically, to provide good performance [11,12,21], and used in testing the PSO by Trelea [19].
Parameter set 1 (a = 0.6 and b = 1.7) was selected by the author in the algorithm convergence domain after a large num- ber of simulation experiments [21]. c1 and c2 in Eq. (3) are set

xi(2)= k2xi(0)+ (k + 1)(1 — k)Pg	(11)
Also, at t = 2 Eq. (8) will be
xi(3)= k3xi(0)+ (k2 + k + 1)(1 — k)Pg
So, recurrence relation can be obtained as follows:
xi(t + 1)= ktxi(0)+ (kt—1 + kt—2 + ... + k + 1)(1 — k)Pgxi(t + 1)
= ktx (0)+ (1 — k ) (1 — k)P
Parameter set 2 (a = 0.729 and b = 1.494) was recom- mended by Clerc [22] and also tested in [21] giving the best re- sults published so far known to the author. c1 and c2 in Eq. (4) are set to be 1/1.494 for PSWV2 algorithm.
A more detailed study of convergence characteristics for different values of these parameters exists in [19].

Optimization test experiments

(1 — k)

xi(t + 1)= ktxi(0)+ (1 — kt)Pg	(12)
where 0 6 k 6 1
In order to test the performance of the PSWV and the other algorithms [13,14,19] two sets of experiments were used with















the above mentioned test conditions and the two parameter sets.
In the first set of experiments, the maximum iteration num- ber was fixed to 2000. Each optimization experiment was run 20 times with random initial values of x and in the range [xmin, xmax] indicated in Table 1. Population sizes of N = 15, 30 and 60 particles were tested. Average number, success rate of required iterations, and expected number of function evalu- ations, for each test function are calculated and presented in Tables 3–7.
In the second set of experiments, each optimization experi- ment was run 20 times for 1000 iterations with population sizes of N = 30 particles. Averages of the best values in each itera- tion were calculated and plotted in Figs. 2–6.

The experimental results

This section compares the various algorithms to determine their relative rankings using both robustness and convergence speed as criteria. A ‘‘robust’’ algorithm is one that manages to reach the goal consistently (during all runs) in the performed experiments [19]. Tables 3–7 present the following informa- tion: Average number of iterations required to reach a func- tion value below the goal, ‘‘success rate’’, and expected number of function evaluations. The ‘‘success rate’’ column lists the number of runs (out of 20) that managed to attain a function value below the goal in less than 2000 iterations, while the ‘‘Ex. # of Fn. Evaluation’’ column presents the expected number of function evaluations needed on average to reach
the goal, calculated only for the succeeded runs using the fol- lowing formula.
Ex. # of Fn. evaluation = (average number of iterations)
× (number of particles in the swarm)
/(success rate)
Tables 3 and 4 show that while SPOS1 algorithm with 15 par- ticles failed to reach the goal during some runs for solving the Sphere (F0) and Rosenbrock (F1) functions, the other algo- rithms succeeded to reach the goal during all runs. This means the number of particles affects the convergence of the standard PSO algorithm for such type of problems while such effect is not with the PSWV algorithm. PSWV1 and PSWV1 algo- rithms succeeded to reach the goal in a few numbers of itera- tions than the remaining algorithms.
Also, as illustrated in Fig. 2, the two versions of PSWV algorithm (PSWV1 and PSWV2) are the candidates to reach the optimal solution in a few numbers of iterations than the remaining algorithms. This means the convergence speed of PSWV is faster than the other algorithms and it is more robust than the others.
While all the algorithms have no difficulties in reaching the goal of Rosenbrock function (F1), the PSO1 algorithm with 15 particles had difficulties for solving such function. Also, the ex- pected number of function evaluation needed for the two ver- sions of PSWV algorithm is less than the expected number of function evaluation needed for the other algorithms. This means the speed of PSWV algorithm is faster than the other















1.E+06

1.E+04

1.E+02

1.E+00

1.E-02

1.E-04

1.E-06
F0

0	100	200	300	400	500	600	700	800	900	1000
Number of Iterations

Figure 2	Average best fitness curves for Sphere function (F0).




1.E+09


1.E+07


1.E+05


1.E+03


1.E+01


1.E-01
F1


0	100	200	300	400	500	600	700	800	900  1000
Number of Iterations

Figure 3	Average best fitness curves for Rosenbrock function
(F1).




1.E+03

1.E+02

1.E+01

1.E+00

1.E-01

1.E-02

1.E-03

1.E-04

1.E-05
F2


0	100	200	300	400	500	600	700	800	900  1000
Number of Iterations

Figure 4	Average best fitness curves of functions F0, F1, and F2.







algorithms tested in this paper in searching for the solution of such kind of problems (see Table 4).
Also, Fig. 3 illustrates that the PSWV1, PSWV2 reached values below the function goal in different numbers of itera-
tions and below that value reached by the remaining algo- rithms. This means that solution quality of PSWV algorithm is better than that of other algorithms for such kind of problems.
Table 5 shows that the two versions of PSWV algorithm perform admirably on the Rastrigin function (F2), while the two versions of SPO algorithm are less robust on the same function for such type of problems where they had some diffi- culties in reaching the goal in some runs.

Table 5 and Fig. 4 illustrate that the PSWV1 and PSWV1 algorithms are doing very well on such problem type, deliver- ing the best overall performance for the Rastrigin function (F2), where they reached the goal in approximately less than 10 iterations and they are candidates to reach the optimal solu- tion in approximately less than 100 iterations. While all ver- sions of CPSO and MPSO algorithms are candidates to reach it in approximately less than 200, 400 iterations PSO1 and PSO2 are stacked at a solution value far from the optimal solution by 10e + 02. This means that the PSWV algorithm is superior to all algorithms as shown in Fig. 4.
Griewank’s function (F3) proves to be hard to solve with the two versions of PSO [19] algorithm where it had some dif- ficulties in reaching its goal. While PSO had some difficulties in reaching its goal the other algorithms consistently reached the goal in all runs as can be seen in Table 6.
Fig. 5 illustrates that the versions of PSWV, MPSO, and CPSO algorithms are candidates to reach the optimal solution of Griewank function (F3) while both versions of PSO did not reach the goal during some runs.
Concerning the Schaffer’s function (F6), Table 6 illustrates that while the two versions of PSWV, MPSO, and CPSO algo- rithms reached the goal in all runs, the two versions of stan- dard PSO algorithm have some difficulties in reaching the goal. Fig. 6 illustrates that: using the average of 30 runs for 1000 iterations of each algorithm, the versions of PSWV algorithm reached a solution value (10—3) below the goal in different number of iterations and be candidates to reach the goal while

F3
1.E+03

1.E+02

1.E+01

1.E+00

1.E-01

1.E-02

1.E-03

1.E-04
0	100	200	300	400	500	600	700	800	900	1000
Number of Iterations

Figure 5	Average best fitness curves of functions F3 and F6.


F6
the two versions of SPO algorithm are stacked at value 10—2 greater than the goal. PSWV1 reached a value below the goal in 13 iterations on average while the two versions of PSO needed more than 200 iterations and the four versions of MPSO and CPSO algorithms come in between them.

8. Discussion

Overall, as far as robustness is concerned, the PSWV algorithm appears to be the winner, since its two versions (PSWV1 and PSWV2) achieved perfect scores in most test cases as repre- sented in boldface (see Tables 3–7). As a result, the PSO [19] must be executed several times to ensure good results, whereas one run of PSWV and MPSO [13] are usually sufficient.
Note that, there is a little difference between the perfor- mance of the algorithms with parameter set1 and with param- eter set 2 where all the algorithms’ convergence under study with parameter set1 are faster than algorithms’ conversance with parameter set 2 (see Tables 3–7). Regarding convergence speed, PSWV1 is always the fastest followed by PSWV2, whereas the PSO1 and PSO2 are always the slowest. Especially on the all functions, PSWV1 has a very fast convergence (2–5 times faster than PSO). This may be of practical relevance for some real-world problems where the evaluation is computa- tionally expensive and the search space is relatively simple and of low dimensionality. Overall, PSWV is clearly the best performing algorithms under study. It finds the lowest fitness value for most of the problems, see Figs. 2–6.
Looking at the number of function evaluations, the PSWV was in the lead, followed by the MPSO [13] algorithm and PSO
[19] algorithm comes in the last as shown in Tables 3–7. Considering the above mentioned point that the two ver-
sions of PSWV have no difficulty in reaching the goal and all its solutions are below their corresponding goals more than other algorithms under study can conclude that PSWV is more superior and robust. So, we can consider it as the best alterna- tive algorithm for solving optimization problems.

9. Conclusion

This paper has proposed a new particle swarm- inspired opti- mization algorithm (PSWV). In this algorithm the new posi- tion of each particle is calculating directly from the combination of its own best position and the global best posi- tion. The implementation of this idea is simple, based on stor- ing the previous positions. The PSWV algorithm outperforms
all the algorithms under study on many benchmark functions,

1.E+00

1.E-01

1.E-02

1.E-03

1.E-04

1.E-05

1.E-06


0	100	200	300	400	500	600	700	800	900	1000
Number of Iterations
being less susceptible to premature convergence, and less likely to be stuck in local optima.
In this study, the PSWV has shown its worth on tested problems, and it outperformed MPSO [13], CPSO [14], and PSO [19] algorithms on all the numerical benchmark problems as well. Among the tested algorithms, the PSWV can rightfully be regarded as an excellent first choice, when faced with a new optimization problem to solve.
To conclude, the performance of PSWV is outstanding in comparison to MPSO [13], CPSO [14], and PSO [19] algo- rithms. It is simple, robust, converges fast, and finds the opti- mum in almost every run. In addition, it has few parameters to set, and the same settings can be used for many different

Figure 6	Average best fitness curves for Schaffer’s function F6.
problems.

Future work includes further experimentation with param- eters of PSWV, testing the new algorithm on other benchmark problems, and evaluating its performance relative to evolution- ary algorithms.

Acknowledgement

The author is grateful for the valuable comments and sugges- tions made by the reviewers, which helped in improving the contents of the paper.

References

Eberhart R, Kennedy J. A modified optimizer using particle swarm theory. In: Proc. 6th Int. Symp. Micro Machine and Human Science (Nagoya, Japan); 1995. p. 39–43.
Eberhart RC, Kennedy J. A new optimizer using particles swarm theory. In: Proc. sixth international symposium on micro machine and human science (Nagoya, Japan). IEEE Service Center: Piscataway, NJ; 1995. p. 39–43.
Kennedy J, Eberhart RC. Particle swarm optimization. In: Proc. IEEE international conference on neural networks (Perth, Aus- tralia), vol. IV. IEEE Service Center: Piscataway, NJ; 1995. p. 1942–8.
Kennedy J. The particle swarm: social adaptation of knowledge. In: Proc. IEEE international conference on evolutionary compu- tation (Indianapolis, Indiana). IEEE Service Center: Piscataway, NJ; 1997, p. 303–8.
Naing win O. A comparison study on particle swarm and evolutionary particle swarm optimization using capacitor place- ment problem. 2nd IEEE international conf. on power and energy, Dec. 2008; 2008. p. 1249–2.
Wang B, Tai Neng-ling, Zhai Hai-qing, Ye Jian, Zhuc Jia-dong, Qic Liang-bo. A new ARMAX model based on evolutionary algorithm and particle swarm optimization for short-term load forecasting. Electr Power Syst Res 2008;78:1679–85.
Wang Hui, Qian Feng. An improved particle swarm optimizer with behavior-distance models and its application in soft-sensor. 7th World Congress Intell Control Automation 2008:4473–8.
Shigenori N, Takamu G, Toshiku Y, Yoshikazu F. A hybrid particle swarm optimization for distribution state estimation. IEEE Trans Power Syst 2003;18:60–8.
EL-Dib Amgad A, Youssef Hosam M, EL-Metwally MM, Osman Z. Maximum loadability of power systems using hybrid
particle swarm optimization. Electr Power Syst Res 2006;76: 485–92.
Liu Bo, Wang Ling, Jin Yi-Hui, Tang Fang, Huang De-Xian. Directing orbits of chaotic systems by particle swarm optimiza- tion. Chaos, Solitons Fractals 2006;29:454–61.
Tasgetiren M, Liang Yun-Chia, Sevkli Mehmet, Gencyilmaz Gunes. A particle swarm optimization algorithm for makespan and total flowtime minimization in the permutation flowshop sequencing problem. Eur J Oper Res 2007;1930–47.
Clerc M, Kennedy J. The particle swarm: explosion, stability, and convergence in a multi-dimensional complex space. IEEE Trans Evol Comput 2002;6:58–73.
El_Sherbiny MM. A combined particle swarm optimization algorithm based on the previous global best and the global best positions. Int J Comput Inf (IJCI) 2007;1:13–26.
El-Sherbiny MM. A modified algorithm for particle swarm optimization with constriction coefficient. Int J Comp Inf (IJCI) 2009;2:17–30.
Wan Li-Yong, Li Wei. An improved particle swarm optimization algorithm with rank-based selection. In: Proceedings of the seventh international conference on machine learning and cyber- netics, Kunming, 12–15 July 2008; 2008. p. 4090–5.
Wang Hui, Liu Yong, Wu Zhijian, Sun Hui, Zeng Sanyou, Kang Lishan. An improved Particle Swarm Optimization with adaptive jumps. IEEE World Congress Comput Intell 2008:392–7.
Liu Jianhua, Fan Xiaoping, Qu Zhihua. An improved particle swarm optimization with mutation based on similarity. Third Int Conf Nat Comput 2007:824–8.
Jiang Yi, Yue Qingling. An improved particle swarm optimization with new select mechanism. Int Workshop Knowledge Discov Data Mining 2008:383–6.
Trelea Ioan Cristian. The particle swarm optimization algorithm: convergence analysis and parameter selection. Inf Process Lett 2003;85:317–25.
Gazi Veysel, Passino Kevin M. Stability analysis of social foraging swarms. IEEE Trans Syst Man Cybern B Cybern 2004;34(1): 539–57.
Boeringer Daniel W, Werner Douglas H. Particle swarm optimi- zation versus genetic algorithms for phased array synthesis. IEEE Trans Antenna Propagation 2004;52(3):771–9.
Clerc M. The Swarm And The Queen: Towards A Deterministic And Adaptive Particle Swarm Optimization. Proc ICEC, Wash- ington, DC 1999:1951–7.
Shi Y, Eberhart R. A combined particle swarm optimizer. Proc IEEE World Congress Comput Intell 1998:69–73.
