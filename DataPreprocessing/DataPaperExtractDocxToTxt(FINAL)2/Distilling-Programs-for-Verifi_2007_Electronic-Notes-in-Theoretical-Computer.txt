Electronic Notes in Theoretical Computer Science 190 (2007) 17–32	
www.elsevier.com/locate/entcs

Distilling Programs for Verification
G.W. Hamilton 1 ,2
School of Computing Dublin City University Dublin, IRELAND

Abstract
In this paper, we show how our program transformation algorithm called distillation can not only be used for the optimisation of programs, but can also be used to facilitate program verification. Using the distillation algorithm, programs are transformed into a specialised form in which functions are tail recursive, and very few intermediate structures are created. We then show how properties of this specialised form of program can be easily verified by the application of inductive proof rules. We therefore argue that the distillation algorithm is an ideal candidate for inclusion within compilers as it facilitates the two goals of program optimization and verification.
Keywords: transformation, optimization, proof, verification


Introduction
In 2004, the UK Computing Research Committee initiated a number of ‘Grand Challenges’ aimed at stimulating long term research in key areas of computing science. The sixth challenge which was identified was that of dependable systems evolution, which was inspired by the idea of a verifying compiler [8], which is a compiler that guarantees the correctness of a program before running it.
In this paper we present a program transformation algorithm called distillation which we argue provides a major step towards the dream of a verifying compiler. The distillation algorithm [5] was originally devised with the goal of eliminating in- termediate data structures from functional programs. A number of program trans- formation techniques have been proposed which can eliminate some of these inter- mediate data structures; for example partial evaluation [9], deforestation [25] and supercompilation [22]. Although supercompilation is strictly more powerful than both partial evaluation and deforestation, Sørensen has shown that supercompila- tion (and hence also partial evaluation and deforestation) can only produce a linear

1 Email: hamilton@computing.dcu.ie
2 Fax: +353 1 7005442

1571-0661 © 2007 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2007.09.005

speedup in programs [19]. Distillation, however, can produce a superlinear speedup in programs.
Example 1.1 Consider the program shown in Figure 1.
rev xs
where
rev = λxs. case xs of
[]	⇒ []
| x : xs ⇒ app (rev xs ) [x ]
app = λxs.λys. case xs of
[]	⇒ ys
| x : xs ⇒ x : (app xs ys)
Fig. 1. Example Program

This program reverses the list xs, but in terms of time and space usage, it is quadratic in the length of the list xs. Applying the distillation algorithm to this program, we obtain the program shown in Figure 2, which is linear in the length of the list xs.
rev xs
where
rev = λxs.rev ' xs []
rev ' = λxs.λys. case xs of
[]	⇒ ys
| x : xs ⇒ rev ' xs (x : ys)
Fig. 2. Example Program Transformed


The programs resulting from distillation are in a specialised form in which func- tions are tail recursive and very few intermediate structures are created. We show that this specialised form is very amenable to the automatic verification of prop- erties of programs through the application of inductive proof rules. We therefore argue that the distillation algorithm is an ideal candidate for inclusion within a compiler as it enables both powerful optimization and program verification.
The remainder of this paper is structured as follows. In Section 2, we define the higher-order language on which the described transformation and verification are performed. In Section 3, we give an overview of the distillation algorithm. In

Section 4, we show how the programs resulting from distillation can be verified. Section 5 considers related work and concludes.

Language
In this section, we describe the language which will be used throughout this paper.

Definition 2.1 [Language] The language for which the described transformations are to be performed is a simple higher-order functional language as shown in Figure 3.

|	c e1 ... en	Constructor Application
|	f	User-Defined Function
|	λv.e	λ-Abstraction
|	e0 e1	Application
|	case e0 of p1 ⇒ e1 |··· | pk ⇒ ek	Case Expression
p	::=	c v1 ... vn	Pattern
Fig. 3. Language Grammar

Programs in the language consist of an expression to evaluate and a set of func- tion definitions. The intended operational semantics of the language is normal order reduction. It is assumed that the language is typed using the Hindley- Milner polymorphic typing system (so erroneous terms such as (c e1 ... en ) e and case (λv.e) of p1 ⇒ e1 |··· | pk ⇒ ek cannot occur). The variables in the patterns
of case expressions and the arguments of λ-abstractions are bound; all other vari-
ables are free. We use fv(e) to denote the free variables of expression e. We require that each function has exactly one definition and that all variables within a defi- nition are bound. We write e ≡ e' if e and e' differ only in the names of bound variables.
Each constructor has a fixed arity; for example Nil has arity 0 and Cons has arity 2. We allow the usual notation [] for Nil , x : xs for Cons x xs and [x1,... , xn] for Cons x1 ... (Cons xn Nil ). We also allow the notation 0 for Zero,1 for Succ Zero and n +1 for Succ n.
Within the expression case e0 of p1 ⇒ e1 |··· | pk ⇒ ek , e0 is called the selector, and e1 ... ek are called the branches. The patterns in case expressions may not be nested. Methods to transform case expressions with nested patterns to ones without nested patterns are described in [1,24]. No variables may appear more
than once within a pattern. We assume that the patterns in a case expression are

non-overlapping and exhaustive.

Distillation
In this section, we give an overview of the distillation algorithm; full details of the algorithm can be found in [5]. The distillation algorithm is a significant advance over the supercompilation algorithm. Using the supercompilation algorithm, it is only possible to obtain a linear improvement in the run-time performance of programs; with distillation it is possible to produce a superlinear improvement.
We define the rules for distillation by identifying the next reducible expression (redex) within some context. An expression which cannot be broken down into a redex and a context is called an observable. These are defined as follows.
Definition 3.1 [Redexes, Contexts and Observables] Redexes, contexts and ob- servables are defined by the grammar shown in Figure 4, where red ranges over redexes, con ranges over contexts and obs ranges over observables.

Fig. 4. Grammar of Redexes, Contexts and Observables
The expression con ⟨e⟩ denotes the result of replacing the ‘hole’ ⟨⟩ in con by e.
Lemma 3.2 (Unique Decomposition Property) For every expression e, either
e is an observable or there is a unique context con and redex e' s.t. e = con ⟨e'⟩.
Definition 3.3 [Normal Order Reduction] The core set of transformation rules for distillation are the normal order reduction rules shown in Figure 5 which define the map N from expressions to ordered sequences of expressions [e1,... , en]. We use the notation e{v1 := e1,... , vn := en} to represent the simultaneous substitution of the sub-expressions e1,... , en for the free occurrences of variables v1,... , vn, respectively, within e. The function unfold unfolds the function in the redex of its argument expression as follows:


N [[con⟨case (v e1 ... en ) of p1 ⇒ e' {v ' := v e1 .. . en } |·· ·| pk ⇒ e' {v ' := v e1 ... en }⟩]]
1	k
= [v e1 .. . en , con ⟨e' {v ' := p1 }⟩, . .., con ⟨e' {v ' := pk }⟩]
1	k
N [[con⟨case (c e1 .. . en ) of p1 ⇒ e' | ··· | pk ⇒ e' ⟩]]
1	k
= [con⟨ei {v1 := e1 ,.. ., vn := en }⟩] where pi = c v1 ... vn

Fig. 5. Normal Order Reduction Rules for Disitllation
unfold (con ⟨f ⟩)= con ⟨e⟩ where f is defined by f = e
The above reduction rules are mutually exclusive and exhaustive by the unique decomposition property. The rules simply perform normal order reduction, with information propagation within case expressions giving the assumed outcome of the test (this is called uniﬁcation-based information propagation in [20]).
Definition 3.4 [Process Trees] A process tree is a directed acyclic graph where each node is labelled with an expression, and all edges leaving a node are ordered. One node is chosen as the root, which is labelled with the original expression to be transformed. Within a process tree t, for any node α, t(α) denotes the label of α, anc(t, α) denotes the set of ancestors of α in t, and t{α := t'} denotes the tree
obtained by replacing the subtree with root α in t by the tree t'. Finally, the tree
e → t1,... , tn is the tree with root labelled e and n children which are the subtrees
t1,... , tn respectively.
A process tree is constructed from an expression e using the following rule:
T [[e]] = e → T [[e1 ]],... , T [[en]] where N [[e]] = [e1,... , en]
Definition 3.5 [Partial Process Trees] A partial process tree is a process tree which may contain repeat nodes. A repeat node has a dashed edge to an ancestor within the process tree.
Definition 3.6 [Instance] An expression e is an instance of expression e', denoted by e' ≤· e, if there is a substitution θ such that e'θ ≡ e.
Repeat nodes correspond to a fold step during transformation. When a term is encountered which is an instance of an ancestor term within the process tree, a repeat node is created. This matching ancestor is called a function node.
Thus, if the current expression is e, and there is an ancestor node α within the process tree labelled with e' where e is an instance of e', then a dashed edge e −−· α is created within the process tree, representing the occurrence of a repeat node. As any infinite sequence of transformation steps must involve the unfolding of a function, we only check for the occurrence of a repeat node when the redex of the current expression is a function.
Example 3.7 Consider the program shown in Figure 6.
Transformation of this program produces the partial process tree given in Figure

app (app xs ys) zs
where
app = λxs.λys. case xs of
[]	⇒ ys
| x : xs ⇒ x : (app xs ys)



7 3 .
Fig. 6. Example Program



Fig. 7. Example Partial Process Tree



Definition 3.8 [Residual Program Construction] A residual program can be con- structed from the partial process tree resulting from supercompilation using the rules C as shown in Figure 8.
The residual program constructed from the partial process tree in Figure 7 is shown in Figure 9
If the transformation rules presented so far were left unsupervised, non-termination could arise, even in the presence of folding. This non-termination will always in- volve encountering expressions which are embeddings of previously encountered ex- pressions. We therefore allow transformation to continue until an embedding of a previously encountered term is encountered within the current one, at which point generalization is performed to ensure termination of the transformation process.
The form of embedding which we use to guide generalization is known as home- omorphic embedding. The homeomorphic embedding relation was derived from re- sults by Higman [7] and Kruskal [11] and was defined within term rewriting systems
[4] for detecting the possible divergence of the term rewriting process. Variants of this relation have been used to ensure termination within supercompilation [20],

3 This process tree, and later ones presented in this paper, have been simplified for ease of presentation.

C[[(v e1 ... en ) → t1 ,... , tn ]]	=	v (C[[t1 ]]) ... (C[[tn ]])
C[[(c e1 ... en ) → t1 ,... , tn ]]	=	c (C[[t1 ]]) ... (C[[tn ]])

C[[β = (con ⟨f ⟩) −−· α]]	=	f ' e1... en
where β ≡ α{v1 := e1,... , vn := en} C[[(con ⟨(λv.e0 ) e1 ⟩) → t ]]	=	C[[t ]]
C[[(con ⟨case (c e1 ... en) of p1 ⇒ e' |··· | pk ⇒ e' ⟩) → t ]] = C[[t ]]
C[[(con ⟨case (v e1 ... en ) of p1 ⇒ e1 |··· | pn ⇒ en⟩) → t0 ,... , tn ]]
= case (C[[t0 ]]) of p1 ⇒ C[[t1 ]] |··· | pn ⇒ C[[tn ]]
Fig. 8. Rules For Constructing Residual Programs

letrec
f0 = λxs.λys.λzs. case xs of
[]	⇒ letrec
f1 = λys.λzs. case ys of
[]	⇒ zs
| y ' : ys' ⇒ y ' : (f1 ys' zs)
in f1 ys zs
| x ' : xs' ⇒ x ' : (f0 xs' ys zs)
in f0 xs ys zs

Fig. 9. Constructed Residual Program

partial evaluation [14] and partial deduction [2,12]. It can be shown that the home- omorphic embedding relation Ð is a well-quasi-order, which is defined as follows.

Definition 3.9 [Well-Quasi Order] A well-quasi order on a set S is a reflexive, transitive relation ≤S such that for any infinite sequence s1, s2,... of elements from S there are numbers i, j with i < j and si ≤S sj.	 

This ensures that in any infinite sequence of expressions e0, e1,... there definitely exists some i < j where ei Ð ej, so an embedding must eventually be encountered and transformation will not continue indefinitely. If ei Ð ej then all of the sub- expressions of ei are present in ej embedded in extra sub-expressions. This is defined more formally as follows.
Definition 3.10 [Homeomorphic Embedding Relation] Variable    Diving	Coupling


x Ð y
e Ð ei for some i e Ð φ(e1,... , en)
	ei Ð e' for all i
φ(e1,... , en) Ð φ(e' ,... , e' )

1	n
This embedding relation is extended slightly to be able to handle constructs such as λ-abstraction and case which may contain bound variables. In these instances, the corresponding binders within the two expressions must also match up. 
Example 3.11 Some examples of the homeomorphic embedding relation are as follows.
f1 x Ð f2 (f1 y)	5.	f (g x) Ø f y 
f1 x Ð f1 (f2 y)	6.	f (g x) Ø g y 
f1 (f3 x) Ð f1 (f2 (f3 y))	7.	f (g x) Ø g (f y)
f1(x, x) Ð f1(f2 y, f2 y)	8.	f (g x) Ø f (h y)	 

In distillation, generalization is performed when an expression is encountered which is an embedding of a previously encountered expression. To represent the re- sult of generalization, we introduce a let construct of the form let v1 = e1,.. .,vn = en in e0 into our language. This represents the extraction of the expressions e1,... , en, which will be transformed separately. The normal-order reduction for the let construct is as follows:
U [[let v1 = e1 ,... , vn = en in e0 ]] = [e0,... , en]
The rule for constructing a residual program from a sub-tree which contains a let
construct in the redex of the root node is as follows:
C[[(let v1 = e1 ,... , vn = en in e0 ) → t0 ,... , tn ]] =
let v1 = C[[t1 ]],... , vn = C[[tn ]] in C[[t0 ]]
If an expression e is encountered which is an embedding of a previously encountered expression e', generalization is also performed. This generalization of e and e' is the most speciﬁc generalization, denoted by e H e', as defined in term algebra [4]. When an expression is generalized, sub-expressions within it are replaced with
variables, which implies a loss of knowledge about the expression. The most specific generalization therefore entails the least possible loss of knowledge.
Definition 3.12 [Generalization] A generalization of expressions e and e' is a triple (eg, θ, θ') where θ and θ' are substitutions such that egθ ≡ e and egθ' ≡ e'.	 

Definition 3.13 [Most Specific Generalization] A most specific generalization of expressions e and e' is a generalization (eg, θ, θ') such that for every other general-
ization (e' , θ'', θ''') of e and e', eg is an instance of e' . The most specific general-
g	g
ization, denoted by e H e', of two expressions e and e' is computed by exhaustively applying the following rewrite rules to the initial triple (v, {v := e}, {v := e'}).
(e, {v := φ(e1,... , en)}∪ θ, {v := φ(e' ,... , e' )}∪ θ')
1	n
⇓
(e{v := φ(v1,... , vn)}, {v1 := e1,... , vn := en}∪ θ, {v1 := e' ,... , vn := e' }∪ θ')
1	n
(e, {v1 := e', v2 := e'}∪ θ, {v1 := e'', v2 := e''}∪ θ')
⇓
(e{v1 := v2}, {v2 := e'}∪ θ, {v2 := e''}∪ θ')	 
The first of these rewrite rules is for the case where both expressions have the same functor at the outermost level. In this case, this is made the outermost func- tor of the resulting generalized expression, and this functor is removed from each of the two expressions. The second rule identifies common sub-expressions within an expression. The results of applying this most specific generalization to items 1-4 in Example 3.11 are as follows:
(v, {v := f1 x}, {v := f2 (f1 y)})
(f1 v, {v := x}, {v := f2 y})
(f1 v, {v := f3 x}, {v := f2 (f3 y)})
(f1(v, v), {v := x}, {v := f2 y})
When we encounter an expression e which is an embedding of a previously encoun- tered expression e', we calculate the most specific generalization of e and e'. If the redex of this most specific generalization is a variable, then the partial process subtree rooted at e is replaced by the result of transforming the generalized form of e. Otherwise, the partial process subtree rooted at e' is replaced by the result of transforming the generalized form of e'. The generalized forms of these expressions are constructed using the abstract operation.
Definition 3.14 [Abstract Operation]
abstract(e, e') = let v1 = e1,... , vn = en in eg
where e H e' = (eg, {v1 := e1,... , vn := en}, θ)	 
Many of the sub-terms which are extracted by generalization may actually be in- termediate, but will not be removed if they are permanently extracted. We there- fore futher transform these generalized terms to remove these possibly intermediate structures. Thus, if a node within the partial process tree is labelled with a term which has been generalized, we replace this node with a new one which has the pro- gram constructed from this node as its label. This new node is then further trans- formed. Generalizations which are performed on nodes labelled with constructed

programs are permanent and are not further transformed.
We now give a more formal definition of distillation. The rule for transforming a node β within a partially constructed tree t, where the label of β is an expression with a function in the redex position is as follows:
if ∃α ∈ anc(t, β).t(α) ≤· t(β)
then (t{β := t(β) −−· α}){α := T [[C[[α]]]]}
else if ∃α ∈ anc(t, β).t(α) Ð t(β)
then if t(α) H t(β)= con ⟨v ⟩
then t{β := T [[C[[T [[abstract (t (β), t (α))]]]]]]}
else t{α := T [[C[[T [[abstract (t (α), t (β))]]]]]]}
else t{β := t(β) → T [[unfold (t(β))]]}
The rule for transforming a node β within a partially constructed tree t, where the label of β is a constructed program is as follows:
if ∃α ∈ anc(t, β).t(α) ≤· t(β)
then t{β := t(β) −−· α}
else if ∃α ∈ anc(t, β).t(α) Ð t(β)
then t{α := T [[abstract(t(α), t(β))]]}
else t{β := t(β) → T [[unfold (t(β))]]}
Definition 3.15 [Distilled Form] The expressions resulting from distillation are in
distilled form dt which is defined as follows:
dt	::=	v dt1 ... dtn
|	c dt1 ... dtn
|	λv.dt
|	letrec f = λv1 ... vn.dt in f v ' ... v '
1	n
|	f dt1 ... dtn
|	case (v dt1 ... dtn ) of p1 ⇒ dt ' |··· | pk ⇒ dt '
|	let v = dt0 in dt1

Proofs of the correctness and termination of the distillation can be found in [5].

Verifying Distilled Programs
In this section, we show how programs can be verified using the distillation algo- rithm. In order to prove a property p of a program e0 where f1 = e1 ... fn = en , we apply the distillation algorithm to the program p e0 where f1 = e1 ... fn = en. The result of this transformation will be a boolean expression which is in distilled form. Inductive proof rules are then applied to this expression to verify it. The func- tions within the boolean expression are all potential inductive hypotheses. If one of

the parameters in a recursive call of one of these functions is decreasing, then this inductive hypothesis can be applied, and the value True returned. If, however, all of the parameters in a recursive call of one of these functions are non-decreasing, then the function is potentially non-terminating, so the undefined value ⊥ is returned.
Definition 4.1 [Decreasing Parameter] A parameter is decreasing from value e to value e', denoted by e' и e, if e' is a sub-component of e.
Definition 4.2 [Non-Decreasing Parameter] A parameter is non-decreasing from value e to value e', denoted by e ± e', if e и e' or e = e'.
The inductive proof rules are shown in Figure 10. Within these rules, φ contains the set of previously encountered function calls which are the potential inductive hypotheses.
P[[v ]] φ	=	False
P[[c]] φ	=	c
P[[let v1 = e1 ,... , vn = en in e0 ]] φ
=	(P[[e0 ]] φ) ∧ ... ∧ (P[[en ]] φ)
P[[case (v e1 ... en ) of p1 ⇒ e' |··· | pn ⇒ e' ]] φ
=	(P[[e' ]] φ) ∧ ... ∧ (P[[e' ]] φ)
1	n
P[[letrec f = λv1 ... vn .e0 in f v' ... v' ]] φ
1	n
=	P[[e0 [v' /v1,... , v' /vn]]] (φ ∪ {f v' ... v' })
1	n	1	n
⎧⎪ True,	if ∃(f e' ... e' ) ∈ φ.∃i ∈ {1 ... n}.ei и e'
⎪⎨	1	n	i





where
f	=	λv1 ... vn.e0
⎪⎪⎩
1	n	i
e,	otherwise

e	=	P[[e0 [e1/v1,... , en/vn]]] (φ ∪ {f e1... en})
Fig. 10. Inductive Proof Rules

These rules can be explained as follows. In rule (1), if we encounter a variable, the value False is returned, as this is one possible value of this variable (it must be a boolean). In rule (2), if we encounter a constructor, then we return the value of this constructor (again, this must be a boolean). In rule (3), if we encounter a let expression, then we need to apply the proof rules to all the sub-terms within this expression to show that they are all true. In rule (4), if we encounter a case expression, we need to apply the proof rules to all the branches of the case to show that they are all true. In rule (5), if we encounter a letrec expression, we add the

sort xs
where
sort	= λxs. case xs of
[]	⇒ []
| x : xs ⇒ insert x (sort xs )
insert = λy.λxs. case xs of
[]	⇒ [y ]
| x ' : xs' ⇒ case (less x ' y ) of
True ⇒ x ' : insert y xs'
| False ⇒ y : xs
less	= λx .λy. case y of
Zero	⇒ False
| Succ y ' ⇒ case x of
Zero	⇒ True
| Succ x ' ⇒ less x ' y '
Fig. 11. Program for Sorting Lists

function call to the set φ and further apply the proof rules to the unfolded function call. In rule (6), if we encounter a function call, then we look within φ for previous calls of this function. If one of the parameters in the current call is decreasing, then we apply the inductive hypothesis and return the value True. If all of the parameters in the current call are non-decreasing, then the function is potentially non-terminating so we return the value ⊥. Otherwise, we add the function call to
the set φ and further apply the proof rules to the unfolded function call. Note that
there are no rules for expressions of the form v e1 ... en, c e1 ... en or λv.e as the proof rules are only applied to expressions of boolean type.
It is possible that overgeneralization can occur, thus turning a theorem into a non-theorem (but not the converse). This means that our theorem prover may determine that a correct program is not actually correct. However, if our theorem prover determines that a program is correct, then this is definitely the case.
Example 4.3 Consider the program shown in Figure 11 for sorting lists of natural numbers.
If we want to verify this program, we need to show that the list resulting from the program is sorted, so we define a property to this effect as shown in Figure 12. This property is applied to the program for sorting lists to obtain the boolean expression sorted (sort xs). This expression is transformed by the distillation al-

sorted = λxs. case xs of
Nil	⇒ True
| Cons x xs ⇒ sorted ' x xs sorted ' = λx .λxs. case xs of
Nil	⇒ True
| Cons y ys ⇒ case (less x y ) of
True ⇒ sorted ' y ys
| False ⇒ False
Fig. 12. Required Property for List Sorting Program

case xs of
[]	⇒ True
| x : xs ⇒ letrec f0 = λxs. case xs of
[]	⇒ True
| x ' : xs' ⇒ f0 xs'
in f0 xs
Fig. 13. Resulting Program
gorithm into the program shown in Figure 13.
The verification of this program now proceeds as shown in Figure 14

Conclusion and Related Work
In this paper, we have presented a novel transformation algorithm called distil- lation, which can produce a superlinear speedup in programs. This represents a major advance over existing unfold/fold transformation techniques, which can only produce a linear improvement. We have shown that, not only is distillation useful for performing program optimization, it also facilitates the relatively straightfor- ward verification of the resulting programs. We therefore argue that the distillation algorithm is an ideal candidate for inclusion within a compiler as it enables both powerful optimization and program verification.
The distillation algorithm was largely inspired by supercompilation, which was originally formulated in the early seventies by Turchin and has been further devel- oped in the eighties [22]. The form of generalization used by Turchin is described in [23]. This involves looking at the call stack to detect recurrent patterns of function calls. Interest in supercompilation was revived in the nineties through the positive supercompiler [19], and the homeomorphic embedding relation was later proposed

P[[ case xs of
[]	⇒ True
| x : xs ⇒ letrec f0 = λxs. case xs of
[]	⇒ True
| x ' : xs' ⇒ f0 xs'

in f0 xs]] {}
=	(P[[True ]] {}) ∧ (P[[ letrec f0 = λxs. case xs of
[]	⇒ True

(by (4))



Fig. 14. Program Verification

to guide generalization and ensure termination of positive supercompilation [20]. Supercompilation has been used for the verification of infinite state systems [13], with some limited success.
The distillation algorithm would be of equivalent power to the supercompilation algorithm if the terms which are extracted on performing generalization were not substituted back in to the generalized term. This means that over-generalization would occur quite frequently when using supercompilation, thus greatly limiting its power. Also, in order to show that the program resulting from supercompilation terminates, Turchin requires that all functions are total, so the onus is on the user to show that this really is the case. In order to show that the program resulting from distillation terminates, we simply need to show it is infinitely progressing [3],

which is much easier to check automatically.
A number of papers illustrate the relationship between the unfold/fold transfor- mation technique and the proofs of program properties, both for functional and logic programs [10,15,16,18,17]. However, the folding mechanism which is used in this work is not as powerful as the folding mechanism presented here, so less program properties can be verified using these techniques.
There are a number of possible directions for further work. Firstly, although the distillation algorithm has already been implemented, it is intended to develop a re-implementation in its own input language which will allow the transformer to be self-applicable. Secondly, the distillation algorithm has been incorporated into an automatic inductive theorem prover called Poit´ın; some preliminary results of this are reported in [6] 4 . Finally, it is intended to incorporate the distillation algorithm into a full programming language; this will not only allow a lot of powerful optimizations to be performed on programs in the language, but will also allow the automatic verification of properties of these programs using our theorem prover.

References
Augustsson, L., Compiling Pattern Matching, in: Functional Programming Languages and Computer Architecture, 1985, pp. 368–381.
Bol, R., Loop Checking in Partial Deduction, Journal of Logic Programming 16 (1993), pp. 25–46.
Brotherston, J., Cyclic Proofs for First-Order Logic With Inductive Definitions, Lecture Notes in Computer Science 3702 (2005), pp. 78–92.
Dershowitz, N. and J.-P. Jouannaud, Rewrite Systems, in: J. van Leeuwen, editor, Handbook of Theoretical Computer Science, Elsevier, MIT Press, 1990 pp. 243–320.
Hamilton, G., Distillation: Extracting the Essence of Programs, in: Proceedings of the ACM SIGPLAN Symposium on Partial Evaluation and Semantics-Based Program Manipulation, 2007, pp. 61–70.
Hamilton, G. W., Poit´ın: Distilling Theorems From Conjectures, Electronic Notes in Theoretical Computer Science 151 (2006), pp. 143–160.
Higman, G., Ordering by Divisibility in Abstract Algebras, Proceedings of the London Mathemtical Society 2 (1952), pp. 326–336.
Hoare, C. A. R., The Verifying Compiler: A Grand Challenge for Computing Research, Journal of the ACM 50 (2003), pp. 63–69.
Jones, N., C. Gomard and P. Sestoft, “Partial Evaluation and Automatic Program Generation,” Prentice Hall International, 1993.
Kott, L., Unfold/Fold Transformations, in: M. Nivat and J. Reynolds, editors, Algebraic Methods in Semantics, CUP, 1985 pp. 412–433.
Kruskal, J., Well-Quasi Ordering, the Tree Theorem, and Vazsonyi’s Conjecture, Transactions of the American Mathematical Society 95 (1960), pp. 210–225.
Leuschel, M., On the Power of Homeomorphic Embedding for Online Termination, in: Proceedings of the International Static Analysis Symposium, 1998, pp. 230–245.
Lisitsa, A. and A. P. Nemytykh, Towards Verification via Supercompilation, in: Proceedings of the 29th Annual International Computer Software and Applications Conference, 2005, pp. 9–10.
Marlet, R., “Vers une Formalisation de l’E´valuation Partielle,” Ph.D. thesis, Universit´e de Nice - Sophia Antipolis (1994).

4 It has previously been shown in [21] how supercompilation can be used in inductive theorem proving.

Pettorossi, A. and M. Proietti, Synthesis and Transformation of Logic Programs Using Unfold/Fold Proofs, Journal of Logic Programming 41 (1999), pp. 197–230.
Pettorossi, A. and M. Proietti, Perfect Model Checking via Unfold/Fold Transformations, in:
Proceedings of the First International Conference on Computational Logic, 2000, pp. 613–628.
Pettorossi, A., M. Proietti and V. Senni, Proofs of Program Properties via Unfold/Fold Transformations of Constraint Logic Programs, in: Transformation Techniques in Software Engineering, 2005.
Roychoudhury, A., K. N. Kumar, C. R. Ramakrishnan, I. V. Ramakrishnan and S. A. Smolka, Verification of Parameterized Systems Using Logic Program Transformations, in: Proceedings of the 6th International Conference on Tools and Algorithms for Construction and Analysis of Systems, 2000,
pp. 172–187.
Sørensen, M. H., “Turchin’s Supercompiler Revisited,” Master’s thesis, Department of Computer Science, University of Copenhagen (1994), dIKU-rapport 94/17.
Sørensen, M. H. and R. Glu¨ck, An Algorithm of Generalization in Positive Supercompilation, Lecture Notes in Computer Science 787 (1994), pp. 335–351.
Turchin, V., The Use of Metasystem Transition in Theorem Proving and Program Optimization, Lecture Notes in Computer Science 85 (1980), pp. 645–657.
Turchin, V., The Concept of a Supercompiler, ACM Transactions on Programming Languages and Systems 8 (1986), pp. 90–121.
Turchin, V., The Algorithm of Generalization in the Supercompiler, in: Proceedings of the IFIP TC2 Workshop on Partial Evaluation and Mixed Computation, 1988, pp. 531–549.
Wadler, P., Efficient Compilation of Pattern Matching, in: S. P. Jones, editor, The Implementation of Functional Programming Languages, Prentice Hall, 1987 pp. 78–103.
Wadler, P., Deforestation: Transforming Programs to Eliminate Trees, in: European Symposium on Programming, 1988, pp. 344–358.
