Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 353 (2020) 77–105
www.elsevier.com/locate/entcs

Stochastic Bounds for the Max Flow in a Network with Discrete Random Capacities
L. Echabbia,1,4 J.M. Fourneaub,1,2,3,5 O. Gacemb,2 H. Lotfi a,1,6
N. Pekergin c,7
a INPT
Rabat, Morocco
b Univ. Paris-Saclay UVSQ, DAVID
Versailles, France
c LACL
Univ. Paris-Creteil Creteil, France

Abstract
We show how to obtain stochastic bounds for the strong stochastic ordering and the concave ordering of the maximal flow in a network where the capacities are non negative discrete random variables. While the deterministic problem is polynomial, the stochastic version with discrete random variables is NP-hard. The monotonicity of the Min-Cut problem for these stochastic orderings allows us to simplify the input distributions and obtain bounds on the results. Thus we obtain a tradeoff between the complexity of the computations and the precision of the bounds. We illustrate the approach with some examples.
Keywords: Maximal flow, Random capacity, Stochastic ordering, Increasing convex ordering


Introduction
Due to the large number of sensors available in smart cities, smart buildings and in transportation systems, we have a huge volume of data for our models. Quite

1 This project was financially partially supported by CAMPUS FRANCE (PHC TOUBKAL 2019 (French- Morocco bilateral program) Grant Number: 41562UA)
2 This work was supported by a public grant as part of the Investissement d’avenir project, reference ANR-11-LABX-0056-LMH, LabEx LMH, in a joint call with Gaspard Monge Program for optimization, operations research and their interactions with data sciences.
3 This work was partially supported by the Paris Ile-de-France Region through a DIM RFSI grant.
4 Email: loubnaechabbi@gmail.com
5 Email: Jean-Michel.Fourneau@uvsq.fr
6 Email: h.lotfi.hl.hl@gmail.com
7 Email: Nihal.Pekergin@u-pec.fr

https://doi.org/10.1016/j.entcs.2020.10.014
1571-0661/© 2020 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

often, these information change with time, due to noise, contention, incidents. They could not be seen as deterministic anymore and we have to deal with the apparent randomness of our measures. For instance the variation of the delay in a transporta- tion system is more much related to congestion than to noise in the measurement process.
Here we propose a method to deal with this randomness for a classical problem in operation research: the computation of the maximal flow. This problem is also important for the analysis of a network reliability. Indeed, it is well known that the two terminals reliability problem is a special case of the maximal flow problem with 0/1 capacity. Let G = (V, E) a capacited directed graph (or network) where the capacity of directed edge e is an integer (it could be 0 and in this case the edge is broken). The capacity of edge e is denoted as w(e). Let Nv be the number of edges. Here we assume that the capacities are random discrete variables.
Computing the maximal flow of such a capacited network is polynomial when the capacities are deterministic. Unfortunately, it is not true anymore when the edges are associated with random variables (see [3] for a survey on the complexity for various delays and flow problems for networks or graphs with random discrete costs or durations). The two terminals reliability problem is proved to be NP-Hard and the stochastic maximal flow is NP-Hard as well. As it is difficult to solve the problem, several approaches have been proposed to obtain approximations or bounds. We briefly reviewed some of these approaches and put more emphasis of the methods associated with stochastic ordering.
Note that it is still possible to solve the problem for small instances when the discrete variables take values in very small sets. It is sufficient to use the Total Probability Theorem after conditioning on the states of all the random variables. Let Xi be the discrete multivariate random vector associated with the distribution of the capacity. We assume that the support of Xi is finite. Let Si be the size of the support of Xi (i.e. the number of atoms in the distribution) and Pr(Xi = di) the probability that edge i has capacity di. We denote by Ω the Cartesian product of the support of the input distributions. Under the independence assumptions, the probability of (d1, ..., dk) is given by:
Nv
(1)	Pr(d1, ..., dNv)=	Pr(Xi = di).
i=1

And the size of Ω is QNv
Si.

Assume that MFlow(d1, .., dNv) is the maximal flow when the capacities are deterministic and equal to di for edge i. The total probability law gives a formal description of the distribution of the Maximal Flow:

Pr(MaxFlow = k)=	Σ
(d1,...,dNv)∈Ω
Pr(d1, ..., dNv )1MFlow(d1,..,dNv
)=k,

where 1X is the indicator function for condition X. As already mentioned, com- puting MFlow(d1, .., dNv) is a polynomial problem, and computing Pr(d1, ..., dNv) is easily done with Eq. 1. Therefore the hard part of the problem is the size of Ω. Note that, to avoid the confusion, MFlow will denote the deterministic problem

while MaxFlow will be the random variable associated with the maximal flow when the capacities are random.
Thus we advocate computing bounds with input distributions which have a smaller number of atoms. The number of atoms we want to keep is a parameter of the algorithms we have designed. Thus the number of atoms we keep in the bounds gives us a tradeoff between the complexity and the accuracy [2]. We use two key properties. First the strong stochastic ordering and the increasing concave ordering are used to design upper or lower bounding distributions of the capacity of the edges which have a smaller number of atoms. Second, we prove the monotonicity of the Max-Flow problem for the strong stochastic order and the increasing convex order. Due to the monotonicity, computing the max-flow for input bounds is easier because they have less atoms and provides a stochastic bound on the results.
Following Fulkerson’s approach of stochastic PERT networks [11], it is often suggested to use the expectations of the random variables as the inputs of a deter- ministic problem (assuming that the expectations are integers or fractions). Such an approach provides un upper bound for the expectation of the maximal flow (see section 2). Monte Carlo simulation was proposed and improved by Fishman to estimate the cumulative distribution of the maximal flow [7]. Sarangan et al. [15] had proposed an algorithm to compute the distribution of the minimum capacity (and thus MaxFlow). First they replace each random variable by its expectation. They obtain a minimal cut-set for the deterministic problem and they compute the distribution of the capacity of the chosen cut-set taking into account the input distributions. They approximate the value of the stochastic maximal flow by this distribution. Recently, Hastings had proposed in her PHD [13] a new method based on a symbolic description of paths or sets of edges (for instance cuts) and an auto- matic derivation of stochastic bounds (for large models) or exact results (for small or simple models). These bounds are based on associated random variables [6] and strong stochastic bounds. In an intuitive formulation, associated random variables are positively correlated and this is a natural property exhibited by paths or cut set (because they share edges). Associated random variables were previously used for the analysis of the completion time of a task graph or a PERT network [17]. The methodology proposed by Hastings can be used for many optimization problems on graphs associated with min, max and ”+” operators (for instance, Shortest Path, Completion Time and Maximal Flow).
The technical part of the paper is as follows. In section 2, we present a brief introduction to strong stochastic bounds and increasing convex stochastic bounds. We show how to build discrete distributions which are lower or upper bound in the sense of these ordering. We also present basic algorithms to change the size of the distributions while building a bound. These results have already been published in [2,4] and they are given here for the sake of readability. The methods allow to build many stochastic bounds for the max-flow with a low complexity. Therefore in Section 3, we show how to combine them to obtain a more accurate bound. Section 4 is devoted to the numerical results.

Strong Stochastic Bounds, Convex/Concave Stochas- tic Bounds
In the following, SX will denote the support of the distribution or the random variable. |S| will be the size of set S. δx will denote a Dirac distribution with an atom in x.
In [2,4] we have proposed to reduce the number of atoms while keeping some quantitative and qualitative information on the results. This is obtained through the use of stochastic orderings. We begin with the definition of the orders we will use in this paper (see [14] and [16] for more information).
Definition 2.1 [strong stochastic ordering] Let X and Y be two random variables,
X ≤st Y if for all increasing function Φ, E[φ(X)] ≤ E[φ(Y )] if the expectations exist.
The stochastic comparison of random variables also implies that a strict inequal- ity between their expectations as seen below.
Proposition 2.2 Let X and Y be two random variables, such that X ≤st Y . If
E[X]= E[Y ] then X =st Y .
We also use some orders associated with variability of the random variables to obtain tighter bounds. Let us first consider convex order which is defined as follows.
Definition 2.3 [stochastic convex ordering] Let X and Y be two random variables, X ≤cx Y if for all convex function φ, E[φ(X)] ≤ E[φ(Y )] if the expectations exist [12].
Here we will use the concave ordering which is easily derived from the convex ordering.
Definition 2.4 [stochastic concave ordering] Let X and Y be two random vari- ables, X ≤cv Y if Y ≤cx X
Definition 2.5 [increasing convex ordering] Let X and Y be two random vari- ables, X ≤icx Y if for all increasing convex function φ, E[φ(X)] ≤ E[φ(Y )] if the expectations exist.
Definition 2.6 [increasing concave ordering] Let X and Y be two random vari- ables, X ≤icv Y if for all increasing concave function φ, E[φ(X)] ≤ E[φ(Y )] if the expectations exist.
Corollary 2.7 Thus, we have:
X ≤cx X, X ≤cv X and X ≤st X.
If X ≤st Y , then for all k, E[Xk] ≤ E[Y k].
If X ≤cx Y , then for all k > 1, E[Xk] ≤ E[Y k]. Taking into account that
E[X]= E[Y ], we get V ar[X] ≤ V ar[Y ].
X ≤cx Y , if and only if E[X]= E[Y ] and X ≤icx Y .

If X ≤st Y then X ≤icv Y
If X ≤st Y then X ≤icx Y
If X ≤cv Y then X ≤icv Y
These orders differ considerably when we consider the expectations of the ran- dom variables. Thus, using the convex ordering instead of the strong stochastic ordering we keep constant the expectation and we hope that we only introduce a small bias when we deal with bounds instead of the measurements.
The maximal flow of a network is monotone related to various stochastic order- ings. Let us define first the generic Ψ−monotony.
Definition 2.8 [Ψ−Monotony] A function f is Ψ−monotone if for all X and Y
random variables such that X ≤ψ Y , then f (X) ≤ψ f (Y ).
Due to the definitions of the orderings we considered by set of functions, the following property holds:
Proposition 2.9 If f is increasing, then it is st − monotone. Similarly, f is in- creasing and concave then it is monotone for the increasing concave ordering.
In this paper, we will prove that the problems we consider are monotone for the strong ordering or monotone for the increasing concave ordering.
Proposition 2.10 The MaxFlow problem is monotone for the strong stochastic ordering.
Proof: As the maximal flow is equal to the minimal cut (the so called Max- Flow=Min Cut Theorem), we define the problem as follows.
MaxFlow = MinC Σ w(e)
e∈C
where C is a cut of network G and e is an arbitrary edge of C. Therefore as it is defined with the ”min” and ”+” operators which are increasing, the problem is monotone for the strong stochastic ordering .
Proposition 2.11 Similarly, the MaxFlow problem is monotone for the increasing concave ordering.
Proof: because they are all defined with the ”min” and ”+” operators which are increasing and concave.
Combining the Ψ−monotone property and the approach based on conditioning suggest the following method: algorithmically reduce the number of atoms in the distributions to get bounds on the inputs and obtain bounds of the outputs due to the monotone property. We have studied this approach for the strong stochastic ordering [2] and the convex/concave ordering [4]. Such an approach was shown to be valuable for network performance modeling [1], operation research [2], reliability modeling [10]. This approach will be detailed in Section 2.
Our approach relies on the following theorem:
Theorem 2.12 Let us consider a network with arbitrary random discrete capacity

Di for directed edge i. Let us consider some other distributions Li. If for all directed edge i, Li ≤cv Di, then
MAXFLOW (L1, L2, .., LNv) ≤icv MAXFLOW (D1, D2, .., DNv).
Proof: First Li ≤cv Di implies that Li ≤icv Di. Then, the inequality is a consequence of Prop. 2.11 on the monotonicity of the MaxFlow problem for the increasing concave ordering.
In the literature, a well-known approach consists in taking the expectation for all the random variables and solve the deterministic problem (assuming that the expectations are integers). Using this approach adds a systematic bias which must be known.
Theorem 2.13 If we replace each random variables by its expectation and com- pute the maximum flow, we obtain an upper bound of the expectation of the exact distribution.
E[MAXFLOW (D1, D2, .., DNv)] ≤ MAXFLOW (E[D1], E[D2], .., E[DNv]).
Proof: First we know from Property 2.15 that X ≤icv E(X) for all random variable X. As MaxFlow problem is defined with increasing and concave function, we have:
MaxFlow(X) ≤icv MaxFlow(E(X))
Taking the expectations of both random variables , we get (because of the definition of the increasing concave ordering)
E(MaxFlow(X)) ≤ E(MaxFlow(E(X))) As MaxFlow(E(X)) is deterministic, we have:
E(MaxFlow(E(X))) = MaxFlow(E(X)).
And finally,
E(MaxFlow(X)) ≤ MaxFlow(E(X)).
Note that the expectation of the distribution is an upper bound for the concave order. Thus one may expect that using Theorem 2.12 instead one may obtain more accurate concave bounds. The upper bound based on the expectation of all the distributions is the worst bound based on concave ordering of the inputs. Finally we add some well-known properties which will be useful to prove our algorithms. Their proofs and more results on these stochastic orderings can be found in the literature [14,16].
Proposition 2.14 (Stop Loss) Let X and Y be two random variables, X ≤cx Y
if and only if E[X]= E[Y ] and, for all d we have, E[(X − d)+] ≤ E[(Y − d)+].
Proposition 2.15 (Expectation) Let X be a random variable with ﬁnite expec- tation, then E[X] ≤cx X and X ≤cv E[X].
This property is very important as it provides the basic action to obtain upper bound for the concave ordering and lower bound for the convex ordering.

Proposition 2.16 (Mixing) Let X, Y and Θ three random variables such that
[X|Θ= a] ≤cx [Y |Θ= a] for all a in the support of Θ, then X ≤cx Y .
Basic algorithms for strong stochastic ordering
The main question is to build stochastic upper and lower bounding distributions for the input distributions. Let us assume that the size of an arbitrary initial distribution is N and that we want to obtain a bound of size K. We first propose some elementary actions which build upper or lower bounds for the strong stochastic ordering with one atom less. Applying these actions N − K times we will get distributions with size K.
Lemma 2.17 [Upper bounding distribution] We consider an arbitrary discrete dis- tribution (say D1). We consider two atoms a and b of D1 (without loss of generality we assume that a < b) deﬁned by their positive probabilities pa and pb. We consider discrete distribution D2 deﬁned as follows:
qi is the probability of atom i in D2
The atoms of D2 are the atoms of D1 except a
for all atoms i of D2 except b, qi = pi,
qb = pa + pb. Then, D1 ≤st D2.

Fig. 1. Fusion of two atoms for an upper bound for the strong order

Lemma 2.18 [Lower bounding distribution] We consider again an arbitrary dis- crete distribution (say D1) and two atoms arbitrary a and b of D1 (without loss of generality we assume that a < b) with by their positive probabilities pa and pb. We consider discrete distribution D3 deﬁned as follows:
qi is the probability of atom i in D3
The atoms of D3 are the atoms of D1 except b
for all atoms i of D2 except a, qi = pi,
qa = pa + pb. Then, D3 ≤st D1.

Fig. 2. Fusion of two atoms for a lower bound for the strong order

Example 2.19 Let D1 be a discrete distribution defined on H1= {1, 2, 4, 5, 8, 9}
with following probabilities [0.2, 0.1, 0.1, 0.2, 0.3, 0.1].	Then applying two times

Lemma 2.17 on atoms 2, 4 for the first iteration and atoms 5 and 9 for the sec- ond, we obtain a distribution D2 with 4 atoms: H2= {1, 4, 8, 9} with probability
[0.2, 0.2, 0.3, 0.3]. One can easily obtain E[D1] = 5.1 and E[D2] = 6.1. Both distributions are depicted in Fig. 3.

0	2	4	6	8	10

V1

Fig. 3. Upper bound for the strong order. The CDF of D1 is depicted in blue. Its upper bound in in red. Remark that an upper bound in strong ordering implies that the CDF of the upper bound is always below the initial distribution.

So we can obtain upper and lower bounds for the input distributions of our problem (i.e. the capacities) with a very low complexity. It remains to decide which atoms to combine to obtain an accurate bound and also which input distributions have to be replaced by a bound. The first problem has partially been solved in [2] for the strong stochastic order and in [4] for the convex and concave order. We do not address the last problem in this paper but we show in Section 3 how to improve bounds by combining them.
In [2] we have also found an algorithm which computes the optimal upper bounds in the following sense. For an arbitrary distribution D with size N and any positive increasing reward function r, we proved in [2] an algorithm to find the distributions D1 and D2 with size K < N such that
D1 ≤st D ≤st D2
D1 and D2 are optimal bounds according to the expectation of function r.
The optimality of D1 means that if we found a distribution D3 such that D3 ≤  D and Σi r(i)D1(i) ≤ Σi r(i)D3(i) ≤ Σi r(i)D(i), then D3 = D1 or D3 = D. The optimality of D2 is defined in a similar manner. Note that, as function r is increasing, D1 ≤st D implies that Σi r(i)D1(i) ≤ Σi r(i)D(i).
Basic algorithms for concave and convex orderings
We now prove some lemmas on the basic operations of fusion of atoms to obtain lower bounds (Lemma 2.20) and upper bounds (Lemma 2.21) for the convex or- dering. Note that we only present the simplest actions, one can find in [4] more complex algorithms to design bounds for this stochastic order.
Lemma 2.20 We consider an arbitrary discrete distribution (say D1) with two atoms a and b (without loss of generality we assume that a < b) deﬁned by the following positive probabilities pa and pb. We build distribution D2 as follows:
qi is the probability of atom i in D2

The atoms of D2 are the atoms of D1 except a and b which are omitted and M
which may be added.
for all atoms i of D2 except a, b and M, qi = pi,
qM = pa + pb + pM (pM is 0 when M is not an atom of D1). Then, D2 ≤cx D1. We also have D1 ≤cv D2.
Proof: it is a simple application of property 2.15 and 2.16 Note that M may already be an atom of D1. In that particular case, the number of atoms in D2 is reduced by 2. Otherwise, it is only reduced by 1.


Fig. 4. Fusion of two atoms for a lower bound for the convex order and an upper bound for the concave order

It is worthy to remark that a lower bound for the concave ordering of a distri- bution with more than 2 atoms has at least two atoms. Therefore when we design Icv bounds for the maximal flow, we must use strong stochastic bounds of the input distributions (due to item 5 of Corollary 2.7) if the complexity of using concave bounds of these distributions is too high.
Lemma 2.21 We consider an arbitrary discrete distribution (say D3) with at least three atoms a, b, c (without loss of generality we assume that a < b < c) deﬁned by the positive probabilities pa, pb and pc. Let us build D4 a new distribution with probabilities denoted as qi such that
The atoms of D4 are the atoms of D1 except b which is omitted,
for all atoms i of D2 except a and c, qi = pi,
qa and qc are deﬁned by


and
qa + qc = pa + pb + pc

aqa + cqc = apa + bpb + cpc.

Then, D4 is an upper bound for the convex stochastic ordering of D3: D3 ≤cx D4. We also have D4 ≤cv D3

Fig. 5. Upper bounding distribution.

See [4] for a proof. Remark that this operation is the discrete analog of the fusion operation studied by Eltan and Hill [5].
Example 2.22 Consider again D1 a discrete distribution defined on H1 =
{1, 2, 4, 5, 8, 9} with following probabilities [0.2, 0.1, 0.1, 0.2, 0.3, 0.1].	We apply Lemma 2.21 on atoms 1, 2, 5 for the first iteration.  We obtain a distribution

bf D2 with 5 atoms: H2= {1, 4, 5, 8, 9} with probability [0.275, 0.1, 0.225, 0.3, 0.1]. One can easily check that E[D1]= 5.1 = E[D2]. Now we apply again Lemma 2.21 to split atom 8 on atoms 5 and 9. We get support H3= {1, 4, 5, 9} and probabilities
[0.275, 0.1, 0.3, 0.325] All the distributions are depicted in Fig. 6.


	

0	2	4	6	8	10

Atoms
0	2	4	6	8	10

Atoms


Fig. 6. Upper bound for the concave order. The CDF of D1 is depicted in blue in both figures. The first upper bound in in red in the left figure. The second upper bound for the concave order in the green in the right figure. Remark that the CDF of the upper bounds in the concave order cross the CDF of the initial distribution.

We also have an optimal bound for the increasing concave upper bound which has been proved in [4] which is based on the basic actions we have presented. More precisely, for a arbitrary distribution D with N positive atoms and convex function r(x)= x2, we prove an algorithm to find D2 such that
D ≤cx D2
D2 has size K < N .
D2 is an optimal bound according to the expectation of function r(x)= x2 (i.e. the second moment).
Furthermore, we propose an algorithm to find a lower bound D1 with size K but we do not prove the optimality of our method. This method can be easily modified to deal with any arbitrary increasing convex function r.
When we deal with capacity in a maximal flow problem, one must take into account that the capacities must be an integer for the Ford and Fulkerson Algorithm to converge. Thus, when we add one new atom by the fusion operation detailed in Lemma 2.20, this atom is not an integer in general. Therefore we proceed as follows:
Let D be the initial distribution.
First, we compute the bounding distributions using the fusion operation and we do not care about the expectation being integer or not
We proceed until the distribution has fixed size K . Let D1 be this distribution. By construction we have: D ≤icv D1.
Finally, we modify all the atoms d of D1 to obtain a new distribution (say
D2).
If atom d of D1 is an integer, we keep it in D2
If atom d is not an integer, it is mapped to the next integer in D2. Several atoms may be merged in that operation.

Clearly, D1 ≤st D2.
Thus, D1 ≤icv D2 (see Corollary 2.7) and D ≤icv D2 by transitivity.
Example 2.23 Let  D1  be  a  discrete  distribution  defined  on  H1  =
{1, 2.5, 2.6, 3, 3.4, 5} with following probabilities [0.2, 0.1, 0.1, 0.2, 0.3, 0.1].  Then
atoms 1 and 5 are kept unchanged. Atoms 2.5 and 2.6 are merged with atom 3 and become atom 3 in the bound while atom 3.4 becomes atom 4 of D2. The support of D2 is {1, 3, 4, 5} and the distribution is [0.2, 0.4, 0.3, 0.1].
This example shows one of the property of the concave ordering.
Proposition 2.24 The extreme atoms of the initial distributions (say D) are kept in the upper concave bounds (i.e. they have a positive probability) computed by the splitting operation described in Lemma 2.21. Indeed at each step, we consider three ordered atoms and the atom in the middle is removed. Note that this property is also true when we compute the upper bounding distribution (for the increase concave ordering) of the maximal flow by the Total Probability method.
Combining Several Distributions
In the previous section we have found how to compute bounds for the input distri- butions. Let Ni be the size of the random variable describing capacity of edge i. We know how to bound these distributions with lower or upper bounds with size Ki. The question is to chose Ki for all directed edge i. Clearly, changing the Ki gives a new bounding distribution for the MaxFlow and a natural question is to combine all these bounds into a more accurate one. Of course, the way to combine these distributions depend on the ordering and so on the bounding algorithms.

Strong order
Thus the general problem is the following. Assume that we have obtained two upper bounds Y and Z of X for the strong stochastic ordering. How to compute a new distribution W such that: X ≤st W , W ≤st Y and W ≤st Z ? Clearly W will be more accurate than both Y and Z.
Remember that X ≤st Y implies that for all a in the support of the distributions Pr(Y ≤ a) ≤ Pr(X ≤ a). By assumptions we also have: Pr(Z ≤ a) ≤ Pr(X ≤ a). Therefore, for all a
max(Pr(Z ≤ a),Pr(Y ≤ a)) ≤ Pr(X ≤ a).
Let us define W by Pr(W ≤ a)= max(Pr(Z ≤ a),Pr(Y ≤ a)). Clearly W satisfies all the properties required. As we deal with discrete distributions, we only have to compute Pr(W ≤ a) for the support of SZ ∪ SY . And we have: SW ⊂ SZ ∪ SY . Algorithm 1 performs such a computation. It assumes that the distributions are represented as sorted lists.
In the code of this algorithm, CDF is a scalar to store the current value of the CDF up to the current atom Procedure Insert(W,a,f) performs an insertion in the

structure W (the output distribution) of an atom a with sum of probability f . This can be done with a constant complexity using sorted lists.

Algorithm 1 Algorithm to combine two upper bounds for the strong stochastic ordering.
Input: input distributions Z, and Y given by their atoms SY and SZ and their probability vectors PrZ[] and PrY []
Output: Output distribution W
1: CDFZ = 0. CDFY = 0. CDFW = 0.
2: for all atoms a in SY ∪ SZ do
3:	if (PrZ(a) > 0) then
4:	CDFZ += P rZ[a]
5:	end if
6:	if (PrY (a) > 0) then
7:	CDFY += P rY [a]
8:	end if
9:	f = max(CDFZ,CDFY )
10:	if (f > CDFW ) then
11:	CDFW = f
12:	Insert(W,a,f)
13:	end if
14: end for


Proposition 3.1 Consider two upper bounding distributions the size of which are N 1 and N 2. Algorithm 1 computes a combination of two upper bounding distribu- tions which is more accurate and it requires 0(N 1+N 2) steps if the distributions are stored as sorted lists. The size of the resulting distribution is smaller than N 1+ N 2 but this value is tight.
Note that it is possible to combine with the same approach two lower bounds for the strong stochastic ordering. Assume that we have obtained two lower bounds Y and Z of X for the strong stochastic ordering. We define W by its CDF Pr(W ≤ a) = min(Pr(Z ≤ a),Pr(Y ≤ a)). Then we have: Pr(X ≤ a) ≤ Pr(Y ≤ a) and Pr(X ≤ a) ≤ Pr(Z ≤ a) (by assumptions) and Pr(X ≤ a) ≤ Pr(W ≤ a) = min(Pr(Z ≤ a),Pr(Y ≤ a)). Again W is more accurate than both Y and Z.
Example 3.2 Let D1 be a discrete distribution defined on H1 = {1, 2, 4, 5, 8, 9}
with probabilities [0.1, 0.2, 0.1, 0.3, 0.2, 0.1] and D2 be another discrete distribu-
tion defined on H2= {1, 3, 4, 6, 7} with probabilities [0.2, 0.1, 0.1, 0.4, 0.2]. Assume that both distributions are upper bounds for the strong stochastic order of a dis- tribution D. Then one can combine distributions D1 and D2 to obtain a more accurate bound. Using Algorithm 1, we obtain a bounding distribution defined on H3= {1, 2, 4, 5, 6, 7} with probability vector [0.2, 0.1, 0.1, 0.3, 0.1, 0.2]. All three distributions are depicted in Fig. 7. Clearly the CDF of the new bound is the maximum of the two CDFs.


	

0	2	4	6	8	10

Atoms
0	2	4	6	8	10

Atoms


Fig. 7. Combining upper bounding distributions. On the left, two upper bounds. On the right, the new upper bound obtained by combining these two bounds.
Concave order
We show how we can combine two upper bounds to prove a more accurate upper bounding distribution. The method is based on the stop loss property. An equiva- lent formulation of this property for increasing concave ordering follows: X ≤icv Y if for all d, E[min(X, d)] ≥ E[min(Y, d)]. This property will be used to develop algorithms when the random variables are discrete.
Definition 3.3 We define the stop loss function of random variable (or distribu- tion) Y as follows:
SLY (y)= E[min(Y, y)].
Proposition 3.4 The Stop Loss function of Y is an affine function by intervals. The boundaries of the intervals are the atoms of distribution Y . Furthermore at an atom di, the difference of the slopes of two consecutive affine functions gives the probability of the atom.
Proof: Algebraic manipulation. Consider two consecutive atoms d1 and d2 of Y . We assume that d1 < y ≤ d2. By construction:
SLY (y)= E[min(Y, y)] = Σ min(i, y)Pr(i)= y ∗ Σ Pr(i)+ Σ i ∗ Pr(i)

i∈SY
i≥d2
i≤d1

When y ≤ d1 where d1 is the smallest atom of Y we have SLY (y) = y while SLY (y) = E[Y ] when y is larger than all the atoms of the distribution. Thus the result is proved for all the intervals. Clearly, the function is also continuous at the boundaries. Now let us compute the two curves crossing at d2. The first curve has
equation y ∗ Σi≥d2 Pr(i)+Σi≤d1 i∗Pr(i) while the second will be y ∗ Σi≥d3 Pr(i)+ 
Σi≤d2 i∗Pr(i), assuming that d3 is the next atom after d2. Therefore the difference
of the slopes is Pr(d2). It is also interesting to note that the slopes are decreasing when we progress along the intervals.
Example 3.5 Consider again D a discrete distribution defined on H1 = {1, 2, 4} with following probabilities [0.3, 0.3, 0.4]. The Stop Loss function of D1 is defined as follows
y ≤ 1, SLY (y) = E[min(Y, y)] = E[(y)] because min(Y, y) = y due to the as- sumption. Thus, SLY (y)= y.
1 < y ≤ 2, SLY (y)= E[min(Y, y)] = Pr(Y = 1) ∗ 1+ Pr(Y = 2) ∗ y + Pr(Y =

4) ∗ y = 0.7 ∗ y + 0.3.
2 < y ≤ 4, SLY (y)= E[min(Y, y)] = Pr(Y = 1) ∗ 1+ Pr(Y = 2) ∗ 2+ Pr(Y =
4) ∗ y = 0.4 ∗ y + 0.9.
y > 4, min(Y, y) = Y . Therefore SLY (y) = E[Y ] = 2.5. The function is repre- sented Fig. 8

y









x



Fig. 8. The stop loss function.

Proposition 3.6 Let Y and Z be two upper bounds of X for the increasing concave stochastic ordering. We deﬁne distribution W by its step function as follows:
SLW (y)= min(SLZ(y), SLY (y)).
Then W is an upper bound of X for the increasing concave ordering.
Proof: First, we have by assumption: for all y, SLX (y) ≤ SLZ(y) and SLX (y) ≤ SLY (y). Therefore SLX (y) ≤ min(SLZ(y), SLY (y)) = SLW (y). We will show in Prop. 3.7 that W is a proper distribution of probability. First we have to detail the construction of SLW .
We give an algorithm to compute SLW (y). Let us consider an arbitrary interval and its boundaries d1 and d2 which are consecutive atoms in SY ∪SZ. Without loss of generality we assume that d1 < d2. Between d1 and d2 the stop loss function of Z (i.e. SLZ(x)) is an affine function aZ(d1)x + bZ(d1). Similarly SLY (x) is an affine function aY (d1)x + bY (d1). Therefore for the interval (d1, d2] we have to compute the minimum of two affine functions. Two cases may occur:
The two functions cross in the interval. Thus the minimum is not an affine function on the interval. Instead it is an affine function between d1 and c and

Fig. 9. Affine functions f 1 and f 2 cross inside interval (d1, d2] at c

another affine function between c and d2 where c is the intersection of the two

affine functions f 1 and f 2. Thus we have to compute the intersection and the description of the minimum. Clearly, we have:
c = bY (d1) − bZ(d1)
aZ(d1) − aY (d1)
and the minimum is easily determined by the smallest function at d1.
Affine functions do not cross inside the interval. Therefore one of them is domi- nating the other on the interval. And the minimum is the affine function which is the dominated one.

Fig. 10. Affine functions f 1 and f 2 do not cross inside interval (d1, d2].

Now we have to show how to extract a distribution of probability from SLW (y).
The algorithm builds the Stop Loss function by an iteration on the successive intervals. In the following algorithm, the method ”Add(SL,d1,d2,f)” appends func- tion f inside interval (d1, d2] in the description of Stop Loss function SL.
Proposition 3.7 One can derive a probability distribution from SLW (y) = min(SLY (y), SLZ(y)). The support is included in the union of the support of Z and Y and the set of intersection nodes. The probability of an atom is obtained by the difference between the slopes of the function before and after the atom. Atoms with a null probability are removed from the support.
It is important to remark that we can not use this method to improve increas- ing concave lower bounds. Of course, one can build max(SLZ(y), SLY (y)) but it is easy to remark that the slopes of these functions may be increasing in some cases. And this property implies that we cannot extract a proper distribution from max(SLZ(y), SLY (y)).
Proposition 3.8 Consider two upper bounding distributions for the increasing con- cave ordering, the size of which are N 1 and N 2. Algorithm 2 computes a combina- tion of two upper bounding distributions for the increasing concave ordering, which is more accurate and it requires 0(N 1+ N 2) steps if the distributions are stored as sorted lists. The size of the resulting distribution may be as large as N 1+ N 2. Remark that two arrays of size N 1 or N 2 are sufficient to store one Stop Loss Function.
Example 3.9 Consider the following two distributions D1 with support H1 =
{1, 2, 4} and probabilities [0.3, 0.3, 0.4] and D2 with support H1 = {1, 3, 4} and
probabilities [0.4, 0.1, 0.5] (see Fig. 11). The Stop Loss functions intersect between 2 and 3, more precisely at 2.5. Therefore the bound obtained by combining D1



Algorithm 2 Algorithm to combine two upper bounds for the increasing concave stochastic ordering.
Input: input distributions Z, and Y Output: Output distribution W
1: Compute the Stop Loss functions for Z: fZ(x) for all intervals. 2: Compute the Stop Loss functions for Y : fY (x) for all intervals. 3: Init SLW = ∅
4: for all intervals (d1, d2] based on consecutive atoms d1, d2 in SY ∪ SZ do
5:	Let f 1 be the affine function associated with Z and f 2 be the affine function associated with Y .
6:	if f 1 and f 2 cross in c inside (d1, d2] then
7:	if f 1(d1) < f 2(d1) then
8:	Add(SLW , d1, c,f 1)
9:	Add(SLW , c, d2,f 2)
10:	else
11:	Add(SLW , d1, c,f 2)
12:	Add(SLW , c, d2,f 1)
13:	end if
14:	else
15:	if f 1(d1) < f 2(d1) then
16:	Add(SLW , d1, d2,f 1)
17:	else
18:	Add(SLW , d1, d2,f 2)
19:	end if
20:	end if
21:	Obtain W from its Stop Loss function SLW using the difference of the slopes.
22: end for

and D2 has a support included in {1, 2, 2.5, 3, 4}. We give in Table 1 the Stop Loss functions for both distributions.

Table 1
Stop Loss functions for Y , Z and W .

We now build the Stop Loss function SLW . From these function, it can be seen that the probability of atom 3 and atom 2 is zero and we remove them from the support. Finally, the support of the combined distribution is {1, 2.5, 4}, and the probability vector is [0.4, 0.2, 0.4].
Remark that the intersection of the two stop loss functions may not be an integer.

y









x



Fig. 11. The stop loss functions cross between 2 and 3.
Even if it is not a valid capacity, it improves the accuracy of the expectation of the maximal flow.
The algorithms we design to bound the input distributions can be used in many applications where measurements are available. They will be added in the next version of our performance evaluation tool (XBorne [9,8]).
Numerical Results
We study a small graph (depicted Fig. 12) to illustrate the approach and the al- gorithms. As the size of the graph and the distributions is not very large, we can compare the bounds with the exact results.

Fig. 12. The example graph.
The input distributions for the edge capacity are gathered in Table 2. The graph has 8 edges. Each input distribution has 8 atoms. Therefore the total prob- ability approach leads to the evaluation of 88 = 224 Maximal Flow problems with deterministic capacity.
In the following we present the results for several bounding strategies. They are grouped by the number of deterministic flows (i.e. Mflows) we have to solve. As expected this provides a tradeoff between the complexity of the computation (i.e. the number of deterministic maximal flows problems) and the accuracy of the bound. To compare bounds, we compute the expectation of the maximal flow for each strategy. The exact expectation of the maximal flow is 6.38. Indeed, as the complexity is not that large, we also solve the initial problem: we compute with the Total Probability approach the distribution of the maximal flow. The complete distribution is depicted in Fig. 13. We have drawn in separate pictures the tail and the head of the distribution to be clearer.


Table 2
The capacity distributions for the edges of the simple graph.


	

2	5	10	15	19

Atoms
20	21	22	23	24

Atoms


Fig. 13. Exact distribution of the maximal flow. Head on the left, tail on the right.
.

Bounds with 1 Mflow
Clearly, as the problem is monotone for the strong ordering, replacing each distri- bution by a Dirac on an atom equal to the maximum of the distribution will provide a deterministic problem the solution of which will be an upper bound of the distri- bution of the flow. Similarly, replacing all the input distributions by a Dirac on the smaller atom gives a lower bound.
Furthermore note that the approach based on the deterministic problem with capacity equal to the expectation of the random variables leads to a maximal flow which is an upper bound for the concave ordering. as shown by Theorem 2.13. All these bounds only require to solve 1 deterministic maximal flow problem. The expectations are reported in the next table. The computation time is smaller than 1ms.

Table 3
Comparisons for the expectation for the bounding distributions (strong ordering and increasing convex ordering).


The lower bound and the upper bound based on two atoms require 14 ms on the same ordinary laptop.

Bounds with 8 Mflows
We first present a very simple strategy for the strong stochastic bounds (both upper and lower bounds). All random variables (except one) are replaced by their extreme atom associated with a probability equal to 1. So we obtain 8 upper bounding distributions and 8 lower bounding ones which are reported in the following figures (we have omitted the upper bounds based on edges e2 and e6 because they consist in a δ24 distribution. Note that these bounds are very easily obtained as the number of cases is now the number of atoms in the input distribution for the capacity which has not been changed (8 atoms in this example). The lower bounding distributions are not represented: they are all equal to δ2.

		

14 15	19 20  22 23 24

Bound e0
17 18 19 20	24

Bound e1
13 14 15 16	19 20 21	24

Bound e3


Fig. 14. Upper bound for strong ordering. Left figure (only e0 is unchanged), center (only e1 is unchanged), figure on the right (only e3 is unchanged)


		

13 14 15 16	19 20 21	24

Bound e4
13 14 15 16	20 21 22  24

Bound e5
13 14 15 16 17  19	22  24

Bound e7


Fig. 15. Upper bound for strong ordering. Left figure (only e4 is unchanged), center (only e5 is unchanged), figure on the right (only e7 is unchanged)

The strategy for the upper bound in the increasing concave ordering is similar. All the random variables (except one) are replaced by a Dirac distribution located at their expectation. We also report the 5 bounding distributions, the remaining ones are all equal to δ10. The bounds using increasing concave ordering are clearly much more accurate than the bounds based on strong stochastic ordering.
To obtain lower bounds for increasing concave ordering, one must consider lower bounds of the inputs for this ordering. But the simplest lower bound for the concave ordering have two atoms. These bounds contain the smallest and the largest atom of the initial distribution and have the same expectation. For instance for the lower bound for the capacity of edge e0, we use a distribution with two atoms in 2 and 9 and a probability vector equal to [0.5, 0.5]. To keep the same strategy (all distribution except one aggregated into one single atom), one must use strong stochastic bounds and we get the same result. To obtain a distinct lower bound


		

7  8	10

Bound e0
6  7  8  9	11

Bound e3
6  7  8  9	12  13

Bound e4


Fig. 16. Upper bound for increasing concave ordering ordering. Left figure (only e0 is unchanged), center (only e3 is unchanged), figure on the right (only e4 is unchanged)


	

6  7  8  9  10

Bound e5
7  8  9  10

Bound e7


Fig. 17. Upper bound for increasing concave ordering ordering. Left (only e5 is unchanged), right (only e7 is unchanged)

for the increasing concave bound of the maximal flow, we need to compute 28 deterministic maximal flow problems (i.e. a concave bounds with two atoms per distribution and 8 edges). Therefore these bounds are reported in the next section with all the bounds using 256 computations of max flow.
For each method we have reported the more accurate result and the less accurate result.

Table 4
Expectations for St and Icv Bounds based on 8 deterministic flows.

It is clear that icv upper bounds are always better (for the expectations) than the st upper bounds.

Bounds with 28 Mflows
To obtain better bounds we investigate now two directions. First we use another constant aggregation scheme of the input distributions with more atoms. We bound each input distribution by a distribution with two atoms, according to the strong ordering or the convex ordering. Second, we use some properties of the graph to design an aggregation pattern which does not use the same number of atoms for each input distributions.

Constant Aggregation Scheme
As mentioned previously we build stochastic bounds with two atoms for each input distribution. We present two sets of bounds for each input distribution. Note that the results of these new schemes and the former ones cannot be compared as the inputs (i.e. the input bounds) are not comparable in general for the two schemes.
St Upper Bound:
Each input distribution is divided into two groups of consecutive atoms. For the strong upper bound, we aggregate all the atoms in a group into the largest atom. The probability of this remaining atom is the sum of the probabilities of the elements of the group. For the first scheme, the two groups have the same size (i.e. 4 atoms here). For the second scheme, the first group contains the two smallest atoms. The table contains the atoms and the probabilities for both schemes.

Table 5
St Upper Bounds for the Input Distributions with 2 atoms.


	

8	12	16 17	20	24

Atoms
4 5 6 7	14 15 16  18	24

Atoms


Fig. 18. St Upper Bound (first scheme on the left, second scheme on the right).


St Lower Bound
We use similar schemes to bound the input distributions. We divide into two groups the atoms and we keep the smallest atom to concentrate the probability of the elements of a group. The distributions are given in Table 6.
Icv Upper Bound
This is done with an iterative application of Lemma 2.20 for the upper bound. We chose the atoms to be merged such that the resulting atom is an integer. Note that the results of this new scheme (depicted in Fig. 20) and the former ones (in


Table 6
St lower Bounds for the Input Distributions with 2 atoms


	

2  3  4	7  8  9	11	14	18

Atoms
2 3 4	10	16	18

Atoms


Fig. 19. St Lower Bound (first scheme on the left, second scheme on the right)

Fig. 16 and Fig. 17) cannot be compared as the inputs (i.e. the input bounds) are not comparable for the two schemes. The upper bounding distributions of the inputs are given in Table 7.

Table 7
Icv upper bounds of the input distributions with two atoms.


	

2  3	5  6  7  8	10 11 12

Atoms
2 3 4	13	17	24

Atoms


Fig. 20. Icv Upper Bound (left), Icv Lower Bound (right).

Icv Lower bound:
Icv lower bounds are easily obtained by considering concave lower bounds for the input distributions. Such bounds on the input distributions with two atoms are unique. Indeed the two extreme atoms must be in the concave lower bound. For instance, the concave lower bound for the distribution of the capacity of edge e0 contains two atoms, 2 and 16 both with probability 0.5.

Table 8
Icv Lower bounds for the Input Distributions with 2 atoms.

As mentioned previously, the smallest and the largest atoms of the exact distri- bution 2 and 24 are also in the lower increasing concave bound (see the right part of Fig. 20. for the lower increasing concave bound and Fig. 13 for the exact result). To present a synthetic comparison of these results, we now give the expectations of the distributions we have computed in the following table.

Table 9
Expectations for the bounds based on 256 deterministic max flows

Clearly the bounds based on the increasing concave ordering are better than the bounds obtained by the the strong ordering and they are also more accurate than the approach based on the expectation of all the random variables modeling the inputs.

Aggregation based on the Minimal Cut
Following [15] and [13], an edge is important for the accuracy of the bound when it is in the cut-set. We now try a simple heuristic to decide which input distributions must be aggregated and which one must keep all its atoms. Note that all the previous bounds are based on a global aggregation pattern which is not related to the graph properties. Here we experiment with the following heuristic: keep more atoms for the input distributions associated with the edges of a minimal cut of the deterministic problem associated with the expected capacities (here they are e3 and e4). Of course, we keep the same global number of max flow computations (i.e. 256 in this section) to compare with the other analysis. We both present st-bounds and icv-bounds for three schemes.

We keep distributions associated to e3 or e4 unchanged. Some distributions are bounded by distributions with with two atoms and the last ones are bounded by Dirac distributions such as the number of deterministic problems is always 28. For the Dirac distributions, the atoms depend on the ordering and the bounds (extreme atoms for the st bounds or expectations for the Icv upper bound, Icv lower bounds are replaced by st lower bounds as usual when we deal with Dirac distributions). For the bounding distributions with two atoms, we use the first scheme for st-bounds (see Table 5 and 6) as it provides better results. For Icv Upper bounds, the input distributions with 4 atoms are taken from Table 10 and the distributions with two atoms in Table 7. In the following figures, the bounds will be named according to number of atoms for the edges.

Table 10
Lower and Upper Concave Bounds of the input distributions with 4 atoms.



First Scheme:
We keep two edges with initial distributions, one edge with a distribution with 4 atoms and the remaining ones bounded by a Dirac distribution.

	

13 14 15 16	19 20 21 22 23 24

Bound 1 : e0=8_e3=8_e5=4
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	24

Bound 3 : e3=8_e4=8_e5=4





	

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	24

Bound 4 : e3=8_e4=8_e6=4
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21 22  24

Bound 5 : e3=8_e4=8_e7=4


Fig. 21. St upper bounds first scheme.

We do not represent some lower bounding distributions for the strong stochastic ordering because they have one or two atoms.


	

2 3 4 5	8 9 10

Bound 1 : e0=8_e3=8_e5=4
2 3 

Bound 3 : e3=8_e4=8_e5=4


Fig. 22. St Lower bounds first scheme.



Fig. 23. Icv upper bounds first scheme.


Fig. 24. Icv Lower bounds first scheme. Only two distributions are depicted.

Table 11
Expectation of the Max flow for the bounds, Scheme 1.

Clearly, upper Icv bounds are better than strong stochastic bounds. For lower bounds, as Icv bounds of the input are often based on strong stochastic bounds we obtain quite similar results for the expectation of the distribution of the max flow.

Second Scheme
We now keep only one edge with its initial distribution. Two edges are associated with bounding distributions on 4 atoms while one edge is modeled by a distribution on 2 atoms and all the remaining have a Dirac distribution.

	

5 6 7 8	11 12 13 14 15 16	19 20 21  23 24

Bound 1 : e3=8_e5=4_e0=4_e4=2
5 6 7 8	11 12 13 14 15 16  18 19 20 21	24

Bound 2 : e4=8_e1=4_e7=4_e3=2


Fig. 25. St Upper bound, Scheme 2.

	

2  3  4  5  6	8  9  10  11

Bound 1 : e3=8_e5=4_e0=4_e4=2
2  3  4  5	7  8  9  10	12

Bound 2 : e4=8_e1=4_e7=4_e3=2


Fig. 26. St Lower bound, Scheme 2.

	

2  3  4  5  6  7  8  9  10 11 12 13 14 15 16

Bound 1 : e3=8e5=4e0=4e4=2
2  3  4  5	7  8  9  10 11	13 14 15 16 17

Bound 2 : e4=8e1=4e7=4e3=2


Fig. 27. Icv Upper bound, Scheme 2.

 	

2  3  4  5  6	8  9  10  11	13

Bound 1 : e3=8_e5=4_e0=4_e4=2
2  3  4  5	7  8  9  10  11	13

Bound 2 : e4=8e1=4e7=4e3=2


Fig. 28. Icv Lower bound, Scheme 2.


Table 12
Expectation of the Max flow for the bounds, Scheme 2.

Third Scheme
Finally, we keep both edges of the minimal cut (i.e. e3 and e4) with their initial input distributions. Two other edges have a bounding distribution with two atoms. The remaining edges, as usual, are associated with Dirac distributions. The figures are labelled with the name of the edges associated with distributions


	

2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	24

Atoms
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	24

Atoms



2 3 4 5 6 7 8 9 10 11 12 13 14 15 16 17 18 19 20 21	24

Atoms

Fig. 29. St Upper bounds: upper left (e0, e1), upper right (e0, e7), bottom (e5, e7), Third Scheme.

Lower bounding distributions (both st and icv) are not represented because they only have one or two atoms.

	

2  3  4  5  6  7  8  9 10 11 12 13

Atoms
2 3 4 5 6 7 8 9 10 11 12 13 14 15 16

Atoms



2 3 4 5 6 7 8 9 10 11 12 13 14 15

Atoms

Fig. 30. Icv Upper bounds: upper left (e0, e1), upper right (e0, e7), bottom (e5, e7), Third Scheme.


Table 13
Expectation of the Max flow for the bounds, Scheme 3.
Concluding Remarks
Our method allows us to obtain bounds very easily and with a tradeoff between accuracy and the computing efforts. Clearly, one can derive some conclusions from these first analysis:
St upper bounds are less accurate than Icv upper bounds with the same number of atoms in the input bounds.
Increasing the size of the input distributions increase the accuracy of the results.
Heuristic based on the minimal cut-set are not really better than the constant aggregation scheme for the upper bounds.
It is difficult to improve the lower bounds (both St and Icv). Concave bounds require an input distribution with at least two atoms leading to a problem with 28 deterministic flows to compute. Increasing the size of the input distributions does not help significantly. The initial St lower bound is 2 and the strategy with 8 deterministic flows leads to distributions which all have an expectation equal to 2.
We now want to develop new heuristics to chose the input variables which must be bounded and the number of remaining atoms in the input bounds. This also suggests to develop iterative techniques to improve the accuracy of the bounds by considering a sequence of bounds of the inputs for some important edges.

References
F. A¨ıt-Salaht, H. Castel-Taleb, J.-M. Fourneau, and N. Pekergin. Performance analysis of a queue by combining stochastic bounds, real traffic traces and histograms. Comput. J., 59(12):1817–1830, 2016.

F. Ait Salaht, J. Cohen, H. Castel Taleb, J. M. Fourneau, and N. Pekergin. Accuracy vs. complexity: the stochastic bound approach. In 11th International Workshop on Discrete Event Systems (WODES2012), number 8, Mexico, 2012.

M. Ball, C. Colbourn, and J. Provan. Network reliability. Handbooks in Operation Research and Management Science, 7:673–762, 1995.
J. Cohen, A. Fauquette, J.-M. Fourneau, G. C. Noukela, and N. Pekergin. Convex stochastic bounds and stochastic optimisation on graphs. Electr. Notes Theor. Comput. Sci., 337:23–44, 2018.
J. Elton and T. P. Hill. Fusions of a probability distribution. The Annals of Probability, 20(1):421–454, 1992.
J. D. Esary, F. Proschan, and D. W. Walkup. Association of random variables, with applications. Ann. Math. Statist., 38(5):1466–1474, 10 1967.
G. S. Fishman. Monte carlo estimation of the maximal flow distribution with discrete stochastic arc capacity levels. Naval Research Logistics (NRL), 36(6):829–849, 1989.


J.-M. Fourneau, Y. Ait El Mahjoub, F. Quessette, and D. Vekris. Xborne 2016: A brief introduction. In T. Czach´orski, E. Gelenbe, K. Grochla, and R. Lent, editors, Computer and Information Sciences - 31st International Symposium, ISCIS, volume 659 of Communications in Computer and Information Science, pages 134–141, Poland, 2016. Springer.
J.-M. Fourneau, M. Le Coz, N. Pekergin, and F. Quessette. An open tool to compute stochastic bounds on steady-state distributions and rewards. In 11th International Workshop on Modeling, Analysis, and Simulation of Computer and Telecommunication Systems (MASCOTS 2003), Orlando, FL. IEEE Computer Society, 2003.
J.-M. Fourneau and N. Pekergin. Dynamic fault trees with rejuvenation: Numerical analysis and stochastic bounds. Electronic Notes in Theoretical Computer Science, 327:27 – 47, 2016. The 8th International Workshop on Practical Application of Stochastic Modeling, PASM 2016.
D. R. Fulkerson. Expected critical paths in PERT networks. Operation Research, 10:808–817, 1962.
A. K. Gupta and M. A. S. Aziz.  Convex ordering of random variables and its applications in econometrics and actuarial science. EUROPEAN JOURNAL OF PURE AND APPLIED MATHEMATICS, 3(5):779–785, 2010.
K. Hastings. Algebraic Approaches to Stochastic Optimization. https://tigerprints.clemson.edu/ all_dissertations/935, PHD Clemson Univ., 2012.
A. Muller and D. Stoyan. Comparison Methods for Stochastic Models and Risks. Wiley, New York, NY, 2002.
V. Sarangan, D. Ghosh, and R. Acharya. State aggregation using network flows for stochastic networks. In Global Telecommunications Conference, 2002. GLOBECOM ’02. IEEE, volume 3, pages 2430–2434
vol.3, 2002.
M. Shaked and J. G. Shantikumar. Stochastic Orders and their Applications. Academic Press, San Diego, CA, 1994.
A. W. Shogan. Bounding distributions for a stochastic pert network. Networks, 7:359–381, 1977.
