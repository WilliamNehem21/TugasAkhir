	Electronic Notes in Theoretical Computer Science 207 (2008) 153–169	
www.elsevier.com/locate/entcs

Engineering of An Assertion-based PSLSimple-Verilog Dynamic Verifier by Alternating Automata
Naiyong Jin, Chengjie Shen, Jun Chen and Taoyong Ni1 ,2
Software Engineering Institute East China Normal University Shanghai, China

Abstract
Alternating Finite Automata (AFA) has linear space complexity in representing Linear-Time Temporal Logics. However, It is difficult to manipulate AFA in the run-time. In this paper, we focus on implementation methods to make alternating automata from static representation to run-time verification engines. 1) We have Directed Acyclic Graphs (DAG) represent all possible runs of a Local-variable-enhanced AFA (LAFA). The acceptance of universal choices is conditioned on successful synchronization of universal branches. 2)We encode states and local variables by symbolic approaches, and adopt historic trees in representing all possible parallel runs. The encoding enables multiple assignments to states and local variables in a configuration. By those methods, we are able to maintain the linear complexity of verification in both space and time.
Keywords: Assertion-based Verification, Automata Construction, Property Specification Language


Introduction
Assertion-based dynamic verifiers automatically pick up execution traces, which satisfy or violate certain property assertions, during the simulation of DUVs (De- sign Under Verification). The automation can significantly reduce the verification cost and promote the design quality [12]. Therefore, assertion-based verification is becoming a more and more important engineering practice.
PSL [3] is an industry standard specification language (IEEE-1850) for hard- ware and embedded system design. PSL has many features supporting simulation, including the directives for the assertions, the test range restrictions, and the func- tional coverages. The simple subset of PSL (PSLSimple) conforms to the notion of

1 This paper is supported by the ”Dengshan Project”(067062017) of the Science and Technology Commis- sion of Shanghai Municipality.
2 Email:{nyjin, cjshen, jchen, ytni}@sei.ecnu.edu.cn

1571-0661 © 2008 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2008.03.091




Fig. 1. FoCs Environment

Fig. 2. A PSL Formula Transformation Flow for Dynamic Verification

monotonic advancement of time, which in turn ensures that formulas within the subset can be simulated easily. Verilog-HDL (IEEE-1364) [1] is an implementation language popularly used in circuit design.
In this paper, we introduce a PSLSimple-Verilog dynamic verifer and discuss meth- ods for its implementation.

Related Work
In [2], Abarbanel et. al. presented the framework FoCs (Formal Checkers, Fig. 1) for generating property checkers from RCTL specifications. The checkers are integrated into DUVs. During simulation, the checkers monitor the execution of a design and identify violations of the property assertions. Following Abarbanel’s approach, Pidan et al. [20] proposed an optimized algorithm for dynamic verifiers of PSL formulas. Firstly, they transformed a PSL formula to a Non-deterministic Finite Automaton(NFA). Then they implemented the NFA with a Discrete Transi- tion System(DTS), which, in turn, will be translated into Verilog HDL codes. The transformation flow is illustrated in Fig. 2
Fisman et al. [4] defined the core logic of PSL as LTLWR, an extension of LTL with regular expressions. They proved that for every LTLWR formula f there exists a Non-deterministic Bu¨chi Automaton (NBA) whose size is doubly exponential in the size of f . As a NFA is the finite fragment of a NBA, the algorithm of Pidan has the same complexity as that of Fisman. The exponential increase of states comes from the intersection (length-matching conjunction) of regular expressions r1&&r2. Fisman [10] also defined a subset of PSL formulas, whose violations can be detected by linear automata on finite words. The subset did not include r1&&r2.
Alternating Finite Automata (AFA) [4] are exponentially more succinct than NFA in expressing temporal logic formulas. The size of resulting AFA is linear to that of f . Feikbeiner and Sipma [9] proposed three algorithms to check at run-time whether a reactive system satisfies a LTL specification by AFA. Those algorithms traversed an AFA in different ways: breadth-first search, width-first search and

backward search. Feikbeiner’s methods reflect the exponential time-complexity in handling AFA.
Fisman et al. also used AFA as an intermediate form in transforming LTLWR to NFA. One important reason that forced them to further transform AFA to NFA was due to the fact that
Alternation in general may lead to automata runs in which each branch is accepting, while at the same time, the simultaneously visited states may include accepting and some non-accepting states at all times (in Section 4 of [4] ) 
The statement conveys the idea that the traditional AFA do not have accept-
ing states for the length-matching construction r1&&r2. As a consequence, it is impossible to sequentially concatenate and fuse the AFA of r1&&r2 with others.
However, it is always enticing to use AFA as verification engines [14] [21]. In [16], we solved Fisman’s problem by enhancing AFA with local variables (LAFA) so that our automata constructions had accepting states for all PSLSimple’s regular expressions, and were able to distinguish different satisfaction strengths. Here, we focus on implementation methods to turn the LAFAs from representation to dynamic verification engines.

Contribution
In summary, we contribute to the literature in the following aspects.
We have Directed Acyclic Graphs (DAG) represent all possible runs of a LAFA. The acceptance of universal choices is conditioned on the successful synchro- nization of universal branches.
We encode states and local variables by symbolic approaches, and adopt his- toric trees in representing all possible parallel runs.
By those methods, we managed to avoid breadth-ward searching. Consequently, the complexity of our algorithm is linear in both space and time.

PSLSimple: A Subset of PSL for Dynamic Verification
PSL has four layers of language structures: Boolean, temporal, verification and modelling. The temporal layer is the heart of PSL. It is used to describe complex temporal relations between signals. The temporal layer of PSL supports regular expressions, linear temporal logic and branching temporal logic. Here, we work on formulas in regular expressions and linear temporal logic.
The simple subset of PSL (PSLSimple) is a subset that conforms to the notion of monotonic advancement of time, left to right through the property, which in turn ensures that properties within the subset can be simulated easily.
PSLSimple restricts operand types of temporal formulas. Let b range over Boolean expressions, r range over Sequential Extended Regular Expressions(SEREs), and f range over PSLSimple formulas.
The set of SEREs is defined recursively as follows:

Definition 2.1 (SEREs)
r ::= b	Boolean expression
| [∗0]	empty SERE
| r; r	sequential concatenation
| r&&r	length − matching conjunction
| r[∗]	repeating r for zero or more times
| r[∗k]	repeating r for k times
| r[∗n : m] repeating r for n to m times
In this definition, the length-matching conjunction operator && constructs a SERE in which two SEREs both hold at the current cycle, and furthermore both complete in the same cycle.
The set of PSLSimple formulas is defined recursively as follows:
Definition 2.2 (PSLSimple formulas)
f ::= r	SERE
| b ∨ f | b ∧ f	Boolean operations
| never r	negation
| X! f	strong next
| X f	weak next
| r |→ f	trigger
trigger
| f until! b | f until b	strong and weak until
| b until! b | b until b strong and weak overlapping until(release)
| eventually! r	strong eventually
| always f	always
| f abort b	abnormal termination on b
In the above definition,
The until! constructor is different from abort. For f abort b, it is unnecessary to verify f any more when b is asserted. But for f until! b, even if b is asserted already, one must keep on verifying the strong satisfaction of f on all words starting before the assertion of b.
eventually! r holds in the current cycle of a given path iff the SERE r does hold at the current cycle or at some future cycle during simulation.
Additional PSLSimple temporal operators are treated as syntactic sugars of the above

operators [11].
In dynamic verification, only behaviors with finite length are considered. PSL
( [3], Section 4.4.5) defines four levels of satisfaction.
Holds Strongly, in cases when no bad states have been seen, all future obli- gations have been met, and the formula will hold on any extension of the word.
Holds, in cases when no bad states have been seen, all future obligations have been met, and the formula may or may not hold on any given extension of the word.
Fails, in cases when a bad state has been seen, future obligations may or may not have been met, and the formula will not hold on any extension of the word.
Pending, in cases when no bad states have been seen, and future obligations have not been met. The formula may or may not hold on any extension of the word.
The precise semantics for PSLSimple is carefully studied in [8] [10] [11] [7] [16].

Runs of Alternating Automata: Trees or DAGs
Nowadays, more and more work suggests the AFA as engines for assertion-based ver- ification. AFA wins over Non-Deterministic Bu¨chi Automata in space complexity. The LTL to AFA conversion is linear space [13].
An alternating ﬁnite automaton on ﬁnite words is a tuple of A =< Σ, S, s0, ρ, F >, where Σ is the input letter, S is a finite set of states, s0 is the initial state, ρ : S ×Σ → 22S is a transition function, and F is a finite set of accepting states. The target of a transition is not a state of S, but subsets of S. A state may transit to multiple target sets to express non-deterministic. ρ(s, l) describes all possible configurations of states which A can activate when it is in state s and reads the letter l. For instance, a transition ρ(s, l) = {{s1, s2}, {s3, s4}} means that A accepts a letter l from state s, and it activates both s1 and s2, or both s3 and s4.
Traditionally, runs of AFAs are expressed in terms of trees [23] [17]. A finite tree is a finite non-empty set T ⊆ N∗ such that forall x · c ∈ T, with x ∈ N∗ and c ∈ N, we have x ∈ T. The elements of T are called nodes, and the empty word s is the root of
T. The level of a node x, denoted | x |, is its distance from the root s. Particularly,
| s |= 0. A run of A on a finite word w = l0 · l1. .¸. ln−1 is a S-labelled tree < Tr, r >, where Tr is a tree and r : T → S maps each node of T to a state in S. For a < Tr, r >, the followings hold:
r(s) = s0
Let x ∈ Tr with r(x) = s and ρ(s, l|x|) = Sj. There is a (possible empty) set SK = {s1,... , sk} such that there exists a Sy ⊆ SK with Sy ∈ Sj, and for all 1 ≤ c ≤ k, we have x · c ∈ Tr and r(x · c) = sc
A run tree r is accepting if all nodes at depth n are labelled by states in F. A word
W is accepted iff there is an accepting run on it.
Though AFAs are succinct in expressing LTL formulas, it is difficult to handle

tree-represented AFA at verification time. The difficulties lie in
AFAs do not constrain the breadth of a level. An active state will move to sets of target states whenever values of input variables satisfy corresponding letters. So with the verification process continuing, the memory cost grows without restrictions.
A tree is just one possible run of an AFA. One have to try breadth-first search or depth-first search in looking for an accepting run.
Kupferman and Vardi [18] [17] proposed to merge similar target states of tran- sitions into a single one. That results in representing runs of AFAs by Directed Acyclic Graphs (DAG). For two nodes x1 and x2, they are similar iff | x1 |=| x2 | and r(x1) = r(x2). Recently, the DAG approach [14] [5] is accepted in static verification (model checking) of LTL properties. The intuition is that the LTL formulas are equivalent to star-free words. For AFAs converted from LTL formulas, they do not have loops other than self loops. That feature implies that, during verification, one only needs to look in the future, but never the past. Hence, people call runs of traditional LTL-AFAs memoryless [17]. In other words, similar states correspond to same future mission: to accept the suffixes which satisfy a common property.
Kupferman represents a memoryless run < Tr, r > by a DAG Gr =< V, E >, where
V ⊆ S × N is such that < s, l >∈ V iff there exists x ∈ Tr with | x |= l and r(x) = s. For example, < s0, 0 > is the only vertex of Gr in S × {0}.
E ⊆  l≥0(S × {l}× (S × {l + 1})) is such that E(< s, l >, < sj, l + 1 >) iff there exists
x ∈ Tr with | x |= l, r(x) = s and r(x.c) = sj for some c ∈ N.
Conﬁgurations Ci ⊆ S are sets of active states, where i refers to the level of a DAG. It is easy to see that, by DAG, every configuration contains at most | S | states that are roots of different subtrees. A DAG is acceptable if there is Ci ⊆ F.
One shall note that the branches of AFA’s DAGs take resemble the require- ments of universal choices. A DAG is just a single path through the existential choices of an AFA. The time-complexity of static LTL verification by AFA is ex- ponential [22] [14]. For dynamic verification, the exponential time-complexity is not released. Feikbeiner and Sipma [9] tried breadth-first, depth-first and backward searchs in checking finite traces using AFA.

Local-variable-enhanced	Alternating	Finite	Au- tomata
In [16], we introduced the formalism of Local-variable-enhanced Alternating Finite Automata (LAFA). We use LAFAs to represent PSLSimpleformulas.
Definition 4.1 A LAFA is a tuple of A = (V, LV, ΣA, S, s0, ρA, F)
Where,
V is the set of variables updated by a DUV.

LV is the set of local variables of the automaton. V and LV satisfy the following conditions,
V ∩ LV = φ
Variables in V do not depend on variables in LV. Updates on LV will not influence variables of V.
ΣA = BoolV ∪ FOPLV is the letter set of A. We denote by FOPX the first order predicates over X. We distinguish trueV and trueLV . trueV stands for logic true over V and trueLV stands for logic true over LV.
S is the set of states of an automaton.
s0 is the initial state.
F is the set of states for strong acceptance.
In LAFA, a transition ρA is in type of S × ΣA × U × 2S, where
ΣA specifies the guarding conditions. A transition can take place only when sampled values of V and LV satisfy its guarding condition. The guarding con- dition is an expression of either BoolV or FOPLV. As we verify behaviors of designs against synchronous properties, values of variables V are sampled at clock events, such as the occurrences of positive edges and negative edges of clocks. We say a transition is external if its guarding condition is the con- junction of a BoolV expression and a clock expression. Otherwise, we call it an internal transition. The following definition gives the syntax of external conditions.

extcondition :: BoolV ∧ posedge(clk) | BoolV ∧ negedge(clk)

In the diagrams of our LAFA,
Solid arrow s represent external transitions.
Dotted arrow s represent internal transitions.
Dashed arrow s represent lines whose types are not important in the context. So a dashed arrow represents either an external or an internal transition.
U is a set of statements which update local variables whenever the transition takes place. For instance, {c1 := c1 + 1, c2 := a} states that c1 will increase by 1 and c2 will get the value of a. We do not allow a U to have multiple assignments which updates a common local variable .
The target of a transition is a subset of S. All elements of the subset shall be active after the transition. Thus, our LAFA maintains the universal choice. We realize the existential choices by means of non-deterministic transitions.
For a state with external transitions,  it has an internal transition (s, trueLV , us, {s}). The internal transition is triggered if the state has no more enabled internal transitions and it is still not the time for external transition scheduling. By the internal transition, a state waits in the current state for the next clock event. Thus, samplings on V are executed synchronously.
We adopt DAG in expressing the runs of LAFAs. To avoid search in breadth- ward, we try to have a DAG to represent all possible runs.

l F b denotes that the letter l satisfies the Boolean expression b. the Boolean satisfaction relation F⊆ Σ × BoolV behaves in the usual manner.
Definition 4.2 (Boolean Satisfaction)
For letter l e Σ, atomic proposition p e PV , and Boolean expressions b, b1, b2 e
BoolV , then 1. l F p e p e l
l F ¬b e l /F b
l F true ∧ l /F false
l F b1 ∧ b2 e f F b1 ∧ f F b2
Given a predicate g on local variables, g e FOPLV, C |= g denotes the satisfaction of g under the configuration C. That is there exists some states and local variables which make g true .
Definition 4.3 Let A = (V, LV, ΣA, S, s0, ρA, F) be a LAFA, runs of A over a word
w = w0w1w2 ... wk is a sequence of conﬁgurations Δ = C0C1 ... Cn, where
C0 = {s0}
If E sx e Ci, (sx, gx, ux, Sj ) e ρ, gx e FOLLV and gx ≠ trueLV , such that Ci |= g, then for all states s e Ci which has enabled internal transition (s, g, u, Sj), we have Sj ⊆ Ci+1
If given wi, E s e Ci, (s, g, u, Sj) e ρ, g e extcondition, and wi F g, then Sj ⊆ Ci+1
The second clause of Definition 4.3 will not trigger the self-loop transition (s, trueLV , us, S) provided that there are active states which have enabled internal guards other than trueLV . Meanwhile, the clause removes infinite loops of trueLV guarded transitions which are regarded as chaos [15] or live-lock [19].
In Definition 4.3, whenever a transition’s guarding condition holds, the transition shall take place. That amounts to an identical treatment towards both universal choices and existential choices. However, such a treatment will not impact the correctness of verifying PSLSimple properties. Because in PSLSimple, the acceptance of universal choices asks for synchronization on peer branches. As an example, for r1&&r2, if the branches of r1 and r2 do not synchronize on termination, then the conditions for checking the strong satisfaction of r1&&r2 will fail. And the strong- accepting instant of f until! b is at the moment when f is strongly satisfied by all runs started before the assertion of b. Therefore, we can conclude the violation of a PSLSimple formula only when all its possible runs ended without being accepted.
Owing to the non-determinism in PSLSimple formulas, usually, there is no sched- uled synchronization instant. For instance, to r1[∗3 : 4] &&r2[∗5 : 6], we do not know the exact value of m1 and m2, m1 e {3, 4} and m2 e {5, 6}, such that r1[∗m1] and r2[∗m2] are length-matching. So, we must have run-time monitors to
pick up runs which strongly satisfy a PSLSimple formula,
conclude the strong violation of a PSLSimple formula.



Fig. 3.

As illustrated in the dotted rectangle of Fig. 3.A, we represent a PSLSimple-Verilog dynamic verifier by
DV =< Af , Mo, M1,... Mf >, where
The Verilog DUVs update the variables in V.
The Af is the LAFA constructed with respect to formula f . Af changes its run-time states and local variables in LV on sampling the values of V.
Mf is the monitors of f . It accept or reject runs Af .
Mis are the monitors which watch on the runs of f ’s sub-formulas. In the bottom- up way, they report and propagate necessary information on which Mf depends in deciding the acceptance and rejection of f .
We define monitors in terms of deterministic finite automata, M  =< ΣM, Q, qi, ρM >, where
ΣM = FOPLV∪S is the letter of M.
Q is the set of states of a monitor. We denote by
qi the Idle state,
qp the Pending state, which indicates that the acceptance of a formula is still pending.
qs the Strong Satisfying state for indicating the strong satisfaction of a formula.
qv the Strong Violating state for indicating the strong violation of a formula.
Transitions of M are in type of Q × ΣM × U × Q, where
ΣM gives out the guarding conditions,
U is a set of updates on LV and S.
A monitor follows a general behavior template, as shown in Fig. 3.B.
By T01 and T02, a monitor stays in the idle state qi provided that the initial state is the only active state in the run time configuration C. T01 is an external transition triggered under clock events, T02 is the internal one.

T1 =df (qi, (C \ {s0}) ∩ S ≠ φ, φ, qp), which says that if a monitor is in idle state, and the run time configuration contains state other than initial states, then the monitor will move to the pending state qp.
T2 =df (qp, C = {s0}, φ, qi) returns a monitor to qi whenever C contains just initial states again.
Once the the conditions for strong accepting [16] hold, the transitions T4 and T5 move a monitor to the strong accepting state qs. T4 is triggered from the pending state. If the strong accepting holds at the first sampling, T5 is expected to take place.
Transitions T6 and T8 move a monitor to the strong violating state. We will give their definition in the next section.
By transitions T31 and T32, a monitor remains in the pending state if none guards of T4, T5, T6 and T8 holds. T31 is triggered on clock events, but T32 is an internal transition.
Once a monitor enters qs, it will stay in the qs by the transition T10 until the next clock event arrives. And then by the transition T7, it returns to qi. The guarding condition of T7 is trueLV∪S. The situation applies to qv as well. However, the self-loop transition is T11 and returning transition is T9.
Now, let us have an analysis on the time complexity of the verification process by our dynamic verifiers. Since we have DAGs represent all possible runs, we need not try all branches for searching strong satisfying or violating words. Therefore, the time complexity of our approach is linear to the depth of simulation. The good result comes from features of PSLSimple which emphasizes that
For strong accepting r1&&r2, words of r1 and r2 shall start and stop simulta- neously.
For strong accepting f until! b, all words started before the assertion of b shall strongly satisfy f .
These two conditions amount to require the existence of successful synchronization of all branches for accepting a universal choice. Without such a requirement, we are unable to try all running branches in parallel, and achieve the linear time complexity.

Data Types and Encodings of LAFA
In this section, we discuss the run-time techniques for manipulating LAFAs. Firstly, we adopt a symbolic method to encode states. For a state s e S, we use an one-bit

variable fs to flag s’s activeness in the current configuration, and f j
for the next

configuration. Given the current configuration Ci, we define
fs ≡ s e Ci	f j ≡ s e Ci+1	(1)
If s is in the target state set, command f js := 1 activates s in the next con- figuration. Command f js := 0 indicates the deactivation of s. However, such a




t0	t1

c lk

t2	t3	t4
r1 [ * 2 ]
r1	r1
r2	r2

[*1]; a[*3] 	

[*2]; a[*3]

[*3]; a[*3]
r2 [ * 2 ]
t0	t1	t2


A . W o rd s o f { [* 1 :7 ];a [* 3 ]}	B . P S L Sim ple fo rm ulas are no t m em o ryles s

Fig. 4.
command is redundant in our automata. Because, if one does not explicitly specify the command f j := 1, s will not be active in the next configuration. Actually, we do
not allow f j := 0 to avoid conflicts with other possible f j := 1.
s	s
The symbolic encoding also brings ease to represent values of local variables. Due to the non-determinism of PSLSimple, local variables may have multiple values in a configuration. For example, suppose there is a SERE {[∗1 : 7]; a[∗3]} which specifies that after 1 to 7 clock cycles, a shall hold for 3 cycles, then at time t3, the repetition counter c of a can be 0, 1, and 2, as illustrated in Fig. 4.A. To enable multiple assignments to a local variable, we denote by f ce to flag that in the current configuration, the value of variable c equals to e, that
f ce ≡ c == e	(2)
Same as the approach of state activation, to update a local variable, we only need to assert the new flags. For example, to increase the value of c from 3 to 4, the command is {f jc4 := 1}
Recalling that a LAFA transition is a tuple of ρ = (s, g, U, Sj), where U is the command updating local variables and Sj is the target set, we simplify the run-time mechanism on transitions by unifying the operations on states and local variables with the symbolic encoding.
We can not directly apply algorithms for DAGs of LTL-AFAs to ours. Because LTL is a star-free language. But, PSLSimpleis not star-free. For correct synchro- nization, branching runs shall remember the time at which they start and fork. Namely, the SERE {r1[∗1 : 2] && r2[∗1 : 2]}[∗2 : 3] specifies 2 or 3 times repetition of {r1[∗1 : 2]&&r2[∗1 : 2]}. As illustrated in Fig. 4.B, when its LAFA manages to synchronize on r1[∗1] &&r2[∗1] at t1, it will run for another r1[∗1 : 2]&&r2[∗1 : 2]. Yet, the LAFA may also choose not to synchronize on the first r1, and continue for r1[∗2]. At t2, when it reaches r1[∗2], it shall synchronize with the r2[∗2] branch started at t0, not the one forked at t1.
We accompany each state and local variable with a historical record h = t0t1 ... tn–1tn which logs that at time t0 the run started, at t1 there was a univer- sal branch, and the last universal branch took place at tn. Furthermore, we organize records with the same last branching time into a tree. For instance, in Fig. 5, HA records three branching runs, one started at t0, one at t1 and one at t5. All three




Fig. 5. A Tree of Branching Time





true LV
s0( r1)

{ f'_ s 0(r1).H.inc lud e(p us h(f_ s _ 0.H, t))
}
s	}
{ f'_ s 0(r2).H.inc lud e(p us h(f_ s _ 0.H, t))
}

true LV
s0( r2)




{ f'_ s s _ 2.H.inc lud e(f_ s s _ 2.H)
}

Fig. 6. LAFA of r1&&r2


runs forked again at t8.
Given a historical tree H,
function H.start returns all the starting time, they are the leaves of H. For the
HA in Fig. 5, HA.start = {t0, t1};
H.last refers to its root which gives the last branching time;
H.pop decomposes H by removing its root, and returns a set of sub-trees of H;
H.clear(t) removes branches which start at t;
Given a set of trees H, push(H, t) returns a new tree with t as the root and elements of H as subtrees. In Fig. 5, HC is the result of push({HA, HB}, t10).
For a tree set H, H.clear() empties all its elements. Likewise, H.start, H.last, H.pop, H.clear(t) work on all elements of H.
f s.H and f cv.H denote the historic trees of the state s and the local variable
c.
Now, we can give the condition for concluding the strong violation of a prop-
erty. Let H(C)V stand for the historic trees of states which do have enabled out-going transitions in the current configuration C. Let H(C) stand for the historical trees contained in states of C. Thus H(Cj) contains the historic trees of the next config- uration.
g6 =df H(C)V .start ≠ φ ∧ H(C)V .start ∩ H(Cj).start = φ

In this definition, H(C)V .start ≠ φ says that there are states all whose transitions fail to take place. H(C)V .start ∩ H(Cj).start = φ says that all triggered transitions do not start from HV . In other words, all runs starting from HV terminate after the current configuration. If g6 holds, the monitor shall record the starting and ending time of strong violation. That is
U6 =df {qj .b := vH.start, qj .e := t}
v	v
The monitor approach releases the obligation of LAFAs to maintain a state for strong violation. That removes a significant amount of LAFA transitions.
With above improvements, we modify the LAFA construction clauses proposed in [16]. Here, we give out the clause for r1 && r2, as illustrated in Fig.6
Given Ai = (V, LVi, Σ, Si, s0(ri), ρi, {ss(ri)}) are LAFAs of ri, then
LV(r1&&r2) = LV(r1) ∪ LV(r2)
S(r1&&r2) = S(r1) ∪ S(r2) ∪ {s0, sf } s0(r1&&r2) = s0
ρA(r1&&r2) = ρA(r1) ∪ ρA(r2)
∪{(s0, trueLV , ui, {s0(ri)}) | ui = {ls0(ri).H.include(push(ls0.H, t))}}
∪{(ss(r1), g, u, {sf }), (ss(r2), f ss1, φ, φ)}
F(r1 && r2) = {sf }
where, g = f ssr2 ∧ (f ssr1.H ∩ f ssr2.H ≠ φ)
u = { T := f ssr1.H ∩ f ssr2.H; f jsf := 1; f jsf .H.include(T.pop); } In the above clause, all target states of s0 inherit s0’s historic trees, and have a new universal branch time as the root of their historic trees. sf is the strong satisfaction state of r1&&r2. Before reaching sf , we shall synchronize on the strong satisfaction of both r1 and r2. For the transition from ss(r1), the guarding condition
f ssr2 ∧ (f ssr1.H ∩ f ssr2.H ≠ φ)
conveys the idea that for a successful synchronization, both ss(r1) and ss(r2) shall be active and both of them have common histories of universal choices. The update part activates sf and assigns the T.pop as histories to sf . T is a temporary variable. It is just the common histories of ss(r1) and ss(r2). T.pop removes the branching time which is pushed into historic tress on leaving s0. Both ss(r1) and ss(r2) are deactivated once the automata reaches sf .
By this example, we can also see the effect of the trueLV guarded self-loop transition. By the semantics of PSL [7], The length of a SERE is counted on clock events. Internal transitions within two external transitions do not take time. It may takes different internal transitions to reach ss(r1) and ss(r2). With the self-loop transition, ss(r1) will not miss the synchronization with ss(r2) only if ss(r2) can be active in the current clock cycle.



Fig. 7. Architecture of The PSLSimple Dynamic Verifier

...

Fig. 8. Sequence of Event Scheduling in VVP+
Development of The PSLSimple-Verilog Dynamic Veri- fiers
We develop the PSLSimple-Verilog Dynamic Verifier in aids of two GPL open source tools, the Icarus-Verilog [24] and the GTKWave [6].
Icarus-Verilog is a package of back-end tool kits for the Verilog HDL as described in the IEEE-1364 standard [1]. It includes a compiler iverilog,a simulator VVP, an XNF (Xilinx Netlist Format) generator and an EDIF FPGA netlist generator. For batch simulation, the compiler iverilog transforms Verilog code into some interme- diate assembly codes called vvp. The simulation engine VVP reads the vvp assembly codes and outputs the result in Value-Change-Dump (vcd) format. The GTKWave is a wave viewer. It interprets the vcd files and paints the signal values. The Icarus-Verilog/GTKWave combination makes a small but complete Verilog design environment. However, none of them supports assertion-based dynamic verification. We enhance Icarus-Verilog and GTKWave with new functionalities as plugins.
We illustrate the verification flow of PSLSimple-Verilog in Fig.7, where
aamake is a module newly developed by us. It parses and transforms PSLSimple properties to LAFAs. To utilize the existing run-time mechanism of VVP, we represent LAFAs in vvp-like assembly codes.
VVP+ introduces an extra event schedule phase into VVP. The new phase pro- cesses LAFA transitions. It is circled in dotted lines of Fig. 8.
The original implementation of VVP follows the standard scheduling seman- tics of Verilog HDL [1]. At each simulation cycle, VVP processes in order of



Fig. 9. A Snapshot of the PSLSimple Dynamic Verifier


Blocking Assignment events, Non-blocking Assignment events, and the last Read-Only-Sync events. The Blocking Assignment and the Non-Blocking As- signment update the variables in V. The updates may make some Boolean expressions of external transitions hold. For the conjunction of clock expres- sions in guarding conditions, the external transitions will be postponed until the occurrence of clock events. If some guarding conditions of internal tran- sitions hold after the execution of external transitions, then there will be a sequence of internal transitions. VVP+ returns to process verilog events when there is no more internal transitions other than the trueLV guarded self-loops.
GTKWave+ is developed on the GTKWave. GTKWave+ can graphically express the four states of a property monitor. They are Idle, Pending, Strong Satisfying and Strong Violating. When the state of a property turns from idle into pend- ing, its trace is raised and painted in white. The legend of Strong Satisfaction is a upward green triangle and the legend of Strong Violation is a downward red triangle. Fig. 9 is a snapshot of our PSLSimple-Verilog dynamic verifier.

Discussion and Future Work
In this paper, we have presented methods to make alternating automata from static representation to run-time verification engines.
We have Directed Acyclic Graphs (DAG) represent all possible runs of a LAFA. The acceptance of universal choices depends on successful synchronization of universal branches.
We encode states and local variables by symbolic approaches, and adopt his- toric trees in representing all possible parallel runs.

By those methods, we are able to maintain the linear complexity of alternating automata both in space and time.
We also have pointed out that the good result comes from features of PSLSimple which emphasizes that only by successful synchronization of all branches, can we accept universal choices. Without such a requirement, we can not achieve the linear time complexity.
We just finished the prototype of our PSLSimple-Verilog dynamic verifier. In the future, we will have quantitative experiments of our method and propose optimiza- tions.
In addition, we plan to extend our approach to specification assurance meth- ods. Conflicting properties will put verification effort into vain. For developing a specification-centric methodology, we must ensure the consistency of properties before handing them out. Besides these, automatic test generation from PSLSimpleis also an interesting topic to us.

References
IEEE standard verilog hardware description language, IEEE std 1364-2001.
Y. Abarbanel, I. Beer, L. Gluhovsky, S. Keidar, and Y. Wolfsthal. FoCs: Automatic generation of simulation checkers from formal specifications. In International Conference on Computer Aided Verification, LNCS-1855:538–542, 2000.
Accellera. Property Specification Language Reference Manual Rev 1.1, 2004.
Shoham Ben-David, Roderick Bloem, Dana Fisman, Andreas Griesmayer, and Ingo Pill Sitvanit Ruah. Automata construction algorithm optimized for psl. Technical Report Delivery 3.2/4, PROSYD, July 2005.
R. Bloem, A. Cimatti, I. Pill, M. Roveri, and S. Semprini. Symbolic implementation of alternating automata. In Proceedings of The 11th International Conference on Implementation and Application of Automata, LNCS 4094, pages 208–218, 2006.
Tony Bybell. Gtkwave. http://home.nc.rr.com/gtkwave/index.html.
Cindy Eisner, Dana Fisman, John Havlick, Y Lusting, A McIssac, and D Van Campenhout. Reasoning with temporal logic on truncted paths. In proceedings of 15th CAV, LNCS 2725:27–40, 2003.
Cindy Eisner, Dana Fisman, John Havlick, and Johan Mrtensson. The t, ± appraoch for truncated semantics. Technical report, Accellera, May 2006.
Bernd Feikbeiner and Henny Sipma. Checking finite traces using alternating automata. Formal Methods in System Design, Volume 24 ,Issue 2:101–127, 2004.
Dana Fisman. The subset of linear violation. Technical Report MCS05-07, Computer Science and Applied Mathematics, Weizmann Institute of Science, August 2005.
Dana Fisman, Cindy Eisner, and John Havlicek. Formal syntax and Semantics of PSL: Appendix B of Accellera’s Property Specification Language Reference Manual. Accellera, 1.1 edition, March 2004.
Harry Foster, Adam C. Krolnik, and David J. Lacey. Assertion-Based Design, Section 1.3.3. Kluwer Academic Publishers, 2nd edition, 2004.
P. Gastin and D. Oddoux. Fast LTL to Bu¨chi automata translation. In Proceedings of the 13th International Conference on Computer Aided Verification (CAV 2001), LNCS-2012:53–65, 2001.
Moritz Hammer. Linear Weak Alternating Automata and The Model Checking. PhD thesis, 2005.
C.A.R Hoare and Jifeng He. Unifying Theories of Programming. Series in Computer Science. Prentice- Hall Europe, 1st edition, 1998.
Naiyong Jin and Chengjie Shen. Dynamic verifying the properties of the simple subset of psl. In Proceedings of The first IEEE/IFIP Symposium on Theoretic Aspect on Software Engineering, pages 229–238, 2007.


Orna Kupferman and Moshe Y. Vardi. Weak alternating automata and tree automata emptiness. Proceedings of the Thirtieth Annual ACM Symposium on the Theory of Computing, pages 224–233, 1998.
Orna Kupferman and Moshe Y. Vardi. Weak alternating automata are not that weak. ACM Transactions on Computational Logic (TOCL), Volume 2, Issue 3:408–429, 2001.
Robin Milner. Communication and Concurrenvy. Prentice Hall, 1989.
Dmitry Pidan, Sharon Keidar-Barner, Mark Moulin, and Dana Fisman. Optimized algorithms for dynamic verification. Technical Report Delivery 3.1/1, PROSYD, March 2005.
Daniel Sheridan. Bounded model checking with snf, alternating automata, and bu¨chi automata. In Proceedings of the 2nd International Workshop on Bounded Model Checking (BMC 2004), ENTCS 119,ISSUE 2:83–101, 2004.
Moshe Y. Vardi. Alternating automata and program verification. In Computer Science Today, LNCS 1000, pages 471–485. 1995.
Moshe Y. Vardi. An automata-theoretic approach to linaer temporal logic. Logics for Concurrency : Structure versus Automata, LNCS-1043:238–266, 1995.
Stephen Williams. Icarus verilog. http://www.icarus.com/eda/verilog/index.html.
