Electronic Notes in Theoretical Computer Science 59 No. 4 (2001)
URL: http://www.elsevier.nl/locate/entcs/volume59.html 17 pages


Syntactic Theories in Practice

Olivier Danvy and Lasse R. Nielsen
BRICS 1
Department of Computer Science, University of Aarhus Building 540, Ny Munkegade, DK-8000 Aarhus C, Denmark.
E-mail: {danvy,lrn}@brics.dk


Abstract
The evaluation function of a syntactic theory is canonically defined as the transitive closure of (1) decomposing a program into an evaluation context and a redex, (2) contracting this redex, and (3) plugging the result in the context. Directly imple- menting this evaluation function therefore yields an interpreter with a worst-case overhead, for each step, that is linear in the size of the input program. We present sufficient conditions over a syntactic theory to circumvent this overhead, and illus- trate the method with an interpreter for the call-by-value λ-calculus and a transfor- mation into continuation-passing style (CPS). In particular, we mechanically change the time complexity of this CPS transformation from potentially quadratic to linear.
An extended version is available as the technical report BRICS RS-01-31 [4].


Introduction
A syntactic theory provides a uniform, concise, and elegant framework to specify a programming language and to reason about it [5]. We consider the issue of implementing the evaluation function of a syntactic theory in the form of an interpreter. Our emphasis, however, is not on automating this process, as in Xiao and Ariola’s SL project [10,11]. 2 Instead, we show how to circumvent the overhead entailed by a direct implementation of an evaluation function.
We first identify the overhead. Then, we list sufficient conditions to rephrase a syntactic theory so that implementing its evaluation function does not incur this overhead. The proof that these conditions are sufficient is constructive in that it indicates how to mechanically rephrase the syntactic

1 Basic Research in Computer Science (www.brics.dk), funded by the Danish National Research Foundation.
2 www.cs.uoregon.edu/~ariola/SL/
◯c 2001 Published by Elsevier Science B. V. Open access under CC BY-NC-ND license.


theory to circumvent the overhead. We consider two examples: a syntactic theory for the call-by-value λ-calculus and a transformation of λ-terms into continuation-passing style due to Sabry and Felleisen [8].

Refocusing in a syntactic theory
A syntactic theory is a small-step semantics where evaluation is defined as the transitive closure of single reductions, each performed by (1) decomposing a program into a context and a redex, (2) contracting the redex, and (3) plugging the result of contraction in the context. Most syntactic theories satisfy a unique decomposition property.
The interpreter for a syntactic theory corresponding to its evaluation func- tion naturally consists of a decompose-contract-plug loop. Often, the only viable implementation of decomposition is a depth-first search in the abstract syntax tree. The decompose step therefore introduces a significant overhead, proportional to the program size. Likewise, plugging can also take time linear in the program size, although it always takes at most as long as the following decomposition, if there is one, and as illustrated below.
For example, here is a syntactic theory of the call-by-value λ-calculus: e ∈ Λ	e ::= x | λx.e | e e 
v ∈ Values	v ::= x | λx.e
x ∈ Vars
E ∈ EvCont	E ::= [ ] | E e | v E r ∈ Redex		r ::= v v 
Plugging the hole of an evaluation context with an expression is defined as usual:
([ ])[e] = e
(E e')[e] = E[e] e'
(v E)[e] = v E[e] The only reduction rule is the following one:
E[(λx.e) v] → E[e[v/x]]
Decomposition induces a linear overhead and thus evaluation may take a quadratic time. Let us illustrate this complexity using Church numerals: [n| is the Church numeral for the number n, i.e., λs.λz. s(s(s(... (s z) .. .))).
`	˛¸	x
n
Example 2.1 We consider the term [n| (λx.x) v where v is any value. This term reduces in two steps to (λx.x)((λx.x)((λx.x)(... ((λx.x) v) .. .))). From
`	˛¸	x
n


then on, each decomposition into a context and a redex, always (λx.x) v, takes time proportional to the number of remaining applications. The total evaluation time is thus O(n2).	✷
Example 2.2 More generally, let us consider the term
(λx.x)((λx.x)((λx.x)(... ((λx.x) e) .. .)))
`	˛¸	x
n
where e reduces to v in m steps. The time complexity of reducing this term to a value is at least O(m × n + n2). Indeed, the factor n is attached to each of the m reduction steps and thus the time complexity for reducing e to v in this context is at least O(m×n), after which we are back at the previous example.
✷
We propose an alternative implementation of consecutive plug-and-decom- pose operations that avoids the overhead. In this alternative implementation, the composition of plug and decompose is replaced by a single function that we call refocus. This replacement is only possible if the syntactic theory satisfies some properties that essentially amount to the next redex occurring, in the depth-first traversal of decompose, later than any other expression that can occur in an evaluation context. We show that these properties hold if the syntactic theory is given in the “standard” way, i.e., by a context-free grammar of values and evaluation contexts, and if it satisfies a unique-decomposition property. We also show how to construct a refocus function that avoids the overhead.
Context-free syntactic theories (terms)
First, we can assume some properties of the grammar of the language. We are working with abstract syntax, i.e., a program is an abstract-syntax tree where each node is created by a production in the language grammar. Because the abstract syntax need not correspond to the concrete syntax, we can, without loss of generality, assume (1) that all productions are of the form
e ::= c(e1,... , en)
for some terminal symbol c and non-terminals e1,... , en, and (2) that there is only one production using c. We call the terminal symbols, c, constructors and the non-terminals, e, term identiﬁers. The set of terms generated from each non-terminal of the grammar is associated to this non-terminal, and the non-terminal is used to refer to any element of that set. When we refer to a generic production on the form e ::= c(e1,... , en), the set associated to e is called Exp and the ones ranged over by e1 through en are called Exp1 through Expn, respectively.
We assume that there is no trivial syntactic category, i.e., one where Expi contains only zero or one element. There are standard ways to transform any grammar into an equivalent grammar with no trivial syntactic categories [6, Section 4.4].

We also require the syntactic theory to be defined by context-free grammars of values, evaluation contexts, and redexes. (Xiao, Sabry, and Ariola also make this assumption for terms and evaluation contexts [11].)
Context-free syntactic theories (values)
If e ::= c(e1,... , en) is a production in the language grammar, a production for values is then of the form
v ::= c(x1,... , xn)
where the xi are either term identifiers, ei, or non-terminals that, like v, rep- resent values. We call such non-terminals value identiﬁers. The set of value terms for a value identifier vi is called Vali and is necessarily a subset of Expi.
Context-free syntactic theories (contexts)
Likewise, evaluation contexts are given by a grammar of the form:
E ::= [ ] | c(x1,... , xi−1, Ei, xi+1 ..., xn) | ... 
where again the xj’s are either value- or term identifiers, and Ei and E are non-terminals representing evaluation contexts. We call such non-terminals context identiﬁers. The terminal [ ] is called the hole of a context.
The binary operator ◦ constructs the composition of two contexts. It is defined inductively on the structure of its first argument as follows.
[] ◦ E2 = E2
c(e1,... , E,... , en) ◦ E2 = c(e1,... ,E ◦ E2,... , e)
Composition thus satisfies (E1 ◦ E2)[e]= E1[E2[e]].
Contexts with composition form a monoid where the empty context, [ ], is the unit, and all other evaluation contexts can be constructed by composing elementary contexts, i.e., contexts where the immediate sub-context is a hole, e.g., c(x1,... , xi−1, [ ], xi+1,... , xn).
Because composition is associative, we can define evaluation contexts cor- responding to composition on the left or on the right. For example, at the beginning of Section 2, we gave a traditional definition of EvCont where eval- uation contexts are created by composition on the left. We can also specify EvCont so that evaluation contexts are created by composition on the right:
E ∈ EvCont   E ::= [ ] | E[[ ] e] | E[v [ ]]
Plugging the hole of an evaluation context with an expression is then defined iteratively as follows:
([ ])[e] = e
(E[[ ] e'])[e] = E[e e']
(E[v [ ]])[e] = E[v e]

In the sense that an evaluation context represents a function from terms to terms, it is injective, i.e., if E[e] = E[e'] then e = e'. This injectivity can be proven by structural induction over E.
We define the depth of an evaluation context E as the number of produc- tions used to create it. We write it |E|, and define the depth function |·| inductively over the structure of evaluation contexts as follows.
|[ ]| = 0 
|c(e1,... , E,... , en)| = 1 + |E|
One can then easily show that |E1 ◦ E2| = |E1| + |E2| and |E| =0 ⇔ E =[ ]. We also define a partial ordering on evaluation contexts based on the com- position operation: E1 ≤ E2 if there exists an E' such that E2 = E1 ◦E'. This
relation is reflexive, anti-symmetric, and transitive (simple proof omitted).
If two evaluation contexts are both smaller than a third one, then the two are themselves related. This follows from the structure of the grammar of evaluation contexts, and therefore we can uniquely define the greatest lower bound of any set of contexts with respect to the ordering. We write E1 ◦ E2 for the binary greatest lower bound of E1 and E2.
If E1 is strictly smaller than E2, i.e., if E1 ≤ E2 and E1 /= E2, we use the traditional notation E1 < E2. This strict ordering is well-founded, since the
|· | function maps the evaluation contexts into the natural numbers ordered by size and it is monotone with respect to <.
Some syntactic categories contain only values, e.g., the syntactic categories of literals. We assume that there are no evaluation contexts for those syntactic categories, since such evaluation contexts could never occur in a decomposition into context and redex anyway, and as such they are irrelevant to the semantics of a language. Likewise, there is no reason to distinguish between values and expressions, so we only represent the syntactic category by the term identifier and never by the associated value identifier.

Context-free syntactic theories (redexes)
We require redexes to be defined by a context-free grammar using only con- structors, term identifiers, and value identifiers. More precisely, productions must be of the form r ::= c(x1,... , xn) where xi is either vi or ei. The set of redexes ranged over by ri is called Redexi. It is a subset of Expi. We require Redexi to be disjoint from the set of values, Vali.
Some syntactic theories have more than these groups of terms and include groups of terms that are considered errors or that are stuck. With an abuse of language, we group all these terms under the name “redex”. Indeed, we are primarily interested in the plugging and decomposition taking place between reductions, independently of what the contractions do, inasmuch as we can distinguish values from non-value expressions.

Properties of syntactic theories
We require one property of the syntactic theory, that of unique decomposition.
Definition 2.3 [Unique decomposition] A syntactic theory satisfies a unique- decomposition property if any term, e, can be uniquely decomposed into either a value, if e itself is a value, or an evaluation context E and a redex r such that e = E[r].	✷
Unique decomposition is so fundamental to syntactic theories for determin- istic languages that it is almost always the first property to be established. Its proof is often technically simple, but because of its many small cases, it tends to be tedious and error-prone. This state of affairs motivated Xiao, Sabry, and Ariola to develop an automated support for proving unique-decomposition properties [11].
If the syntactic theory satisfies a unique-decomposition property, then its redexes are exactly the non-value terms that can only be trivially decomposed. A decomposition, E[e] is trivial if either e is a value or E is the empty con- text. That redexes are exactly the non-value terms that can only be trivially decomposed follows from the following two inclusions.
Let r be a redex. If E[e] is a decomposition of r then either e is a value, and the decomposition is trivial, or e is not a value, and it can be decomposed uniquely into e = E'[r']. Then (E ◦ E')[r'] is a decomposition of r into a context and a redex. Since [ ][r] is also a decomposition into an evaluation context and a redex, and it is unique, we know that E ◦ E' =[ ] and r' = r. The only way E ◦ E' can be [ ] is if both E and E' are [ ], and so the decomposition was trivial.
Let e be a non-value term that can only be trivially decomposed. Then by unique decomposition e can be uniquely decomposed as E[r]. Since this decomposition must be trivial, and r is a redex and thus not a value, it follows that E = [ ] and thus e = r is itself a redex.
The following property of syntactic theories with unique decomposition allows us to show the correctness of the refocus function.
Definition 2.4 [Left-to-right reduction sequence] A syntactic theory is said to have a left-to-right reduction sequence if for each production e ::= c(e1,... , en) of the language grammar, there exists a number 0 ≤ m ≤ n, called the length of the reduction sequence of c, such that if e = c(e1,... , en) then the following two properties hold.
If all of e1 through em are values then e is either a value or a redex, depending only on c.
If 1 ≤ i < m is the first i such that ei is not a value, with a (unique) decomposition of ei = E[r], then the decomposition of e is
c(e1,... , ei−1, E, ei+1,... , en)[r].


Depending on whether e is a value or a redex in the first property, we say that
c constructs a value or c constructs a redex, respectively.
In words, the length of a reduction sequence is the number of immediate sub-expressions that must be reduced to produce a value or a redex.	✷
For simplicity, in the present version of this article, we only consider syntactic theories that have such a left-to-right reduction sequence. (In an extended version of this article [4], we consider arbitrary reduction sequences.)
In the remainder of this section, for ease of reference, we subscript the evaluation-context constructors by the index of the argument that is an eval- uation context. That is, E ::= ci(v1,... , vi−1, Ei, ei+1,... , en), since all evalu- ation contexts that are used must be of this form.
Construction of a refocus function
We now define a function, refocus, that is extensionally equivalent to the composition of the plug function and the decompose function. It uses a different representation of evaluation contexts—a stack of elementary contexts—that allows us to inexpensively compose an elementary context on the right of a context, i.e., to plug an elementary context in a context. Since the evaluation context is only ever accessed in plug and decompose, we are free to choose our own representation when implementing these functions.
The refocus function is defined with two mutually recursive functions. The first, refocus, takes an evaluation context—represented as a stack of elemen- tary evaluation contexts—and an expression. It then tries to find the redex in that expression by decomposing the expression. If it fails, then the expression must be a value, and it calls an auxiliary function, refocusaux , defined by cases on the top-most evaluation context on the stack.
For each e ::= c(e1,... , en) in the language grammar, there exists one corresponding rule in refocus.
If the length of the reduction sequence of c is 0, then we know that
c(e1,... , en) isa value ora redex.
If c constructs a value, then
refocus(c(e1,... , en), E)= refocusaux (E, c(e1,... , en)).
If instead c constructs a redex, then
refocus(c(e1,... , en), E)= (E, c(e1,... , en)) since we have found the decomposition.
If the length of the reduction sequence is non-zero, then the first expres- sion must be reduced, and we simply refocus on it:
refocus(c(e1,... , en), E)= refocus(e1,E ◦ c([ ], e2,... , en)) where we write E ◦ c(... , [ ],.. .) for “pushing” the elementary context,
c(... , [ ],.. .), on the “stack” of contexts, E.

Likewise, the refocusaux function is defined by cases on the evaluation context on top of the stack. Let us take, e.g., the evaluation context ci(v1,... , vi−1, [ ], ei+1,... , en) as the top-most elementary context on the stack.
If the length of the reduction sequence of c is i and c constructs a value, then
refocusaux (E ◦ ci(v1,... , vi−1, [ ], ei+1,... , en), vi)
= refocusaux (E, c(v1,... , vi−1, vi, ei+1,... , en))
The auxiliary function tries to find the next expression in the re- duction sequence by plugging the value given and picking the next subexpression in the reduction sequence. In this case there is no next sub-expression, so refocusaux iterates with the newly constructed value.
If the length of the reduction sequence of c is i, but c constructs a redex, then we have found a decomposition. Thus, the rule is:
refocusaux (E ◦ ci(v1,... , vi−1, [ ], ei+1,... , en), vi)
= (E, c(v1,... , vi−1, vi, ei+1,... , en)).
If the length of the reduction sequence of c is bigger than i, then we are not finished evaluating the c-expression, so we refocus on the next subexpression in the reduction sequence, and the rule is:
refocusaux (E ◦ ci(v1,... , vi−1, [ ], ei+1, ei+2,... , en), vi)
= refocus(ei+1,E ◦ ci+1(v1,... , vi−1, vi, [ ], ei+2,... , en)).
Finally there is one rule for the empty context.
refocusaux ([ ], v)= v
This base case accounts for the situation where the entire expression is a value, so the decomposition has to return a value rather than a pair of a context and a redex.
With one such rule for each construction in the language syntax and in the evaluation-context syntax, the refocus function terminates only when finding a decomposition into an evaluation context and a redex or if the entire program is found to be a value. An ordering argument using the reduction sequence shows that refocus visits expressions in an order corresponding to a depth- first post-order traversal of the syntax tree, skipping the branches that are not in position to be evaluated (such as the conditional branches of an “if” construct). As such, refocus necessarily terminates, and it does so exactly with a decomposition, which has to be unique. We define this ordering as follows.
Definition 2.5 [Ordering on decompositions of a term] Let e = E1[e1] =
E2[e2] be two decompositions of the same term. There are two cases where

(E1, e1) ± (E2, e2).
(E1, e1) ± (E2, e2) if E1 ≤ E2 (i.e., if E2 = E1 ◦ E' for some E'). Two equivalent ways of stating the requirement are that E1 ◦ E2 = E1 or that e2 is a subterm of e1. When E1 ≤ E2 we write E2 \ E1 for the E' satisfying E2 = E1 ◦ E'. This E' is in fact uniquely determined by E1 and E2.
If neither E1 < E2 nor E2 < E1 then E = E1 ◦ E2 is strictly smaller than both E1 and E2. Let us look at where the evaluation contexts differ; let
' = E1 \ E and E' = E2 \ E.
Since E' and E' are both non-empty and E' [e1]= E' [e2], there must be
1	2	1	2
some constructor c such that they are of the form E1\E = ci(e1,... , ei−1, Ei,
ei+1, ... , en) and E2 \ E = cj(e1,... , ej−1, Ej, ej+1,... , en). The i and j (which we will call the index of the sub-context) must be different, since otherwise E◦ (ci(v1,... , Ei,... , en)) would be a lower bound of both E1 and E2, contradicting that E is the greatest lower bound.
Then (E1, e1) ± (E2, e2) if i > j, i.e., if e1 is “later in the reduction sequence” than e2.
No decompositions are related by ± unless they satisfy one of these two cases.
✷
The relation ± is reflexive, symmetric, and transitive (proof omitted), i.e., it is an ordering relation. It is also total, i.e., for any two decompositions of the same term, E1[e1] = E2[e2], either (E1, e1) ± (E2, e2) or (E2, e2) ± (E1, e1). This follows from any two decompositions, E1[e1]= E2[e2], satisfying at least one of three cases:
E1 ◦E2 = E1, in which case E1 < E2 and thus (E1, e1) < (E2, e2) (by the first case of Definition 2.5),
E1 ◦E2 = E2, in which case E2 < E1 and thus (E2, e2) < (E1, e1) (by the first case of Definition 2.5), or
E1 ◦ E2 is strictly smaller than both E1 an E2, so by the second case of Definition 2.5, either (E1, e1) ± (E2, e2) or (E2, e2) ± (E1, e1).
We define the strict ordering, и, as (E1, e1) и (E2, e2) if (E1, e1) ± (E2, e2) and (E1, e1) /= (E2, e2). Then и is well-founded, since the number of decom- positions of a term is finite.
Now we are in position to prove that refocus terminates. The proof shows that consecutive calls to refocusaux happen on smaller and smaller decompo- sitions of the same term, so such calls must eventually terminate.
Lemma 2.6 (Totality) Any call to refocus terminates, i.e., refocus is total.
Proof. We prove that refocusaux is total and that it yields a decomposition of its argument into an evaluation context and a redex.
The proof is by well-founded induction on the arguments of refocusaux . There are four cases, one base case and one case for each possible behavior

of refocusaux on an argument (E ◦ ci(e1,... , [ ],... , en), v).
If the argument to refocusaux is an empty context and a value, then it stops immediately with that value as result.
If the reduction sequence of c has length m = i and c constructs a value, then
refocusaux (E ◦ ci(e1,... , [ ],... , en), v)
= refocusaux (E, c(e1,... , v,... , en)).
Since E < E ◦ ci(e1,... , [ ],... , en), the first case of the definition of ±
shows that
(E, c(e1,... , v,... , en)) и (E ◦ ci(e1,... , [ ],... , en), v).
If the reduction sequence of c has length m = i and c constructs redexes, then refocusaux stops with a decomposition into an evaluation context and a redex.
If the reduction sequence of c has length m > i then
refocusaux (E ◦ ci(e1,... , [ ], ei+1,... , en), v)
= refocus(ei+1,E ◦ c1(e1,... , v, [ ],... , en)).
NB: (E◦c1(e1,... , v, [ ],... , en), ei+1) и (E◦ci(e1,... , [ ], ei+1,... , en), v).
Here we need a sub-induction to show that refocus(ei+1,E ◦c(e1,... , v, [ ],... , en)) eventually either computes a value, or calls refocusaux with an argument that is smaller than (E ◦ ci(e1,... , [ ], ei+1,... , en), v).
The proof is by structural induction on ei+1. We show that for any ar- gument, (e, E), to refocus, either the above happens, or refocus calls itself with an argument that is still smaller (wrt. и) than (E ◦ci(e1,... , [ ], ei+1,
... , en), v) and structurally smaller than e, guaranteeing eventual termi- nation. The three cases are as follows.
Either refocus calls refocusaux with the same arguments that it received itself (in the opposite order). These arguments are smaller than (E ◦ ci(e1,... , [ ], ei+1,... , en), v) by induction hypothesis.
refocus terminates with a result that is an evaluation context and a redex.
refocus calls itself with an argument that is structurally smaller, and which is also a smaller decomposition than (E◦ci(e1,... , [ ], ei+1,... , en), v).
It suffices to show that if (E1, e1) и (E', e') and E1 /< E' (i.e., it is due to the second case in the definition of ±), then any other decomposition (E2, e2) with E1 < E2 also satisfies (E2, e2) и (E', e'). This follows directly from the definition, since E'◦E1 = E'◦E2, and thus E1\E'◦E1
< E2 \ E' ◦ E2.

The above induction argument proves that refocusaux is a total function, and another induction, similar to the sub-induction in Case 3, shows that refocus is total as well.	✷

The function refocus is total, and it can only terminate yielding either a value (from the decomposition [ ][v]) or a decomposition into an evaluation context and a redex (E[v1 v2]), both smaller decompositions of the original term. Since such decompositions are unique, refocus must compute the same function as the composition of plugging and decomposing.
We make only a short argument that using refocus leads to a more efficient implementation of finding the next redex than plugging and then decomposing using a depth-first search for the redex. The refocus function visits the nodes of the syntax tree in the same order as a recursive-descent decomposition. However, it does not start from scratch, but from the node that is about to be plugged. 3 In an implementation of a syntactic theory that uses refocus, the time taken to perform a reduction sequence like E[e] → E[e'] → E[e''] is thus independent of E. Using refocus thus makes it possible to avoid the overhead identified at the start of this section.


An interpreter for the call-by-value lambda-calculus
Let us get back to the call-by-value λ-calculus, as initially considered in the beginning of Section 2. First, we state the grammars of the language and of the syntactic theory, but this time in the format used in Section 2.1 and onwards.


e ∈ Exp
x ∈ Ide
e ::= var(x) | lam(x, e) | app(e, e)

v ∈ Val	v ::= var(x) | lam(x, e)
E ∈ EvCont	E ::= [ ] | app(E, e) | app(v, E)
r ∈ Redex	r ::= app(v, v)
Ide is a syntactic category of identifiers, containing only values, so there is no value- and evaluation-context definition for it.



3 In fact, as we observed elsewhere [3], the refocus function corresponds to a CPS imple- mentation of recursive descent with a first-order representation of the continuation: the evaluation context. Applying the auxiliary function to a context and a value corresponds to applying the continuation to that value.


The interpreter is defined as follows.


eval : Exp → Val + (EvCont × Redex)
eval (e) = eval'(decompose(e))

eval' : Val + (EvCont × Redex) → Val + (EvCont × Redex)
eval'(v) = v
eval'(E, app(lam(x, e), v)) = eval (plug(E, e[v/x]))
eval'(E, app(x, v2)) = (E, app(x, v2))

given
decompose : Exp → Val + (EvCont × Redex)
plug : EvCont × Exp → Exp


The interpreter takes one expression as argument, and attempts to repeatedly decompose, contract, and plug, until either a value or a stuck redex is reached, if any.
The syntactic theory given has a left-to-right reduction sequence, where the length of the reduction sequence of app is 2 and app constructs redexes. Therefore we define the refocus and refocusaux functions as follows.


refocus : (Exp × EvCont) → Val + (EvCont × Redex)
refocus(var(x), E) = refocusaux (E, var(x)) refocus(lam(x, e), E) = refocusaux (E, lam(x, e)) refocus(app(e1, e2), E) = refocus(e1,E ◦ (app([ ], e2))

refocusaux : EvCont × Val → Val + (EvCont × Redex)
refocusaux ([ ], v) = v
refocusaux (E ◦ (app([ ], e2)), v) = refocus(e2,E ◦ (app(v, [ ])))
refocusaux (E ◦ (app(v1, [ ])), v) = (E, app(v1, v))

The interpreter is then changed to using refocus instead of decompose and
plug.
eval : Exp → Val + (EvCont × Redex)
eval (e) = eval'(refocus(e, [ ]))

eval' : Val + (EvCont × Redex) → Val + (EvCont × Redex)
eval'(v) = v
eval'(E, app(lam(x, e), v)) = eval'(refocus(e[v/x], E))
eval'(E, app(x, v2)) = (E, app(x, v2)) NB: The last rule of eval' is used for stuck redexes.
To implement this interpreter, e.g., in ML, the grammars of the syntactic
theory can be expressed directly as ML data types. As for the refocus function, it can use a stack of elementary contexts and a push operation to efficiently implement the composition of an elementary context to the right of a context. It is however more convenient then to represent evaluation contexts “inside out”:
E ::= [ ] | E[[ ]e] | E[v[ ]].
Because it uses refocusing instead of consecutive plugging and decompos- ing, the resulting interpreter does not incur any overhead. For example (cf. Example 2.1 in Section 2), it evaluates a term such as [n| (λx.x) v in time O(n).
Sabry and Felleisen’s CPS transformation
In their work on reasoning about programs in continuation-passing style (CPS), Sabry and Felleisen designed a new CPS transformation [8, Definition 5]. This CPS transformation integrates a notion of generalized reduction and thus yields very compact CPS programs [2]. It is also unusual in the sense that it builds on the notion of a syntactic theory, rather than on operational seman- tics [1,7] or denotational semantics [9]. Therefore, and unlike all the other formalized CPS transformations we are aware of, it is not defined by struc- tural induction over its input. Instead, it is defined as the transitive closure of decomposing, performing an elementary CPS transformation, and plugging. Therefore, its direct implementation incurs the same overhead as considered in Section 2. In the rest of this section, we derive an implementation that operates in linear time over the source program.
The original specification is indexed with 0 (Section 4.1). We first make decomposition and plugging explicit, indexing this specification with 1 (Sec- tion 4.2). Then, we present a version that explicitly uses a refocus function, indexing this specification with 2 (Section 4.3). Using the construction of

Section 2, we then define an efficient version of the refocus function.
Terms, values, and contexts are defined as follows.
M ∈ Λ	M ::= V | M M 
V ∈ Values	V ::= x | λx.M x ∈ Vars
E ∈ EvCont	E ::= [ ] | E[V [ ]] | E[[ ] M ]

The original speciﬁcation
Definition 4.1 [Sabry and Felleisen, 1993] The following CPS transforma-
tion uses three mutually recursive functions: C0 to transform terms, Φ0 to
transform values, and K0 to transform evaluation contexts. Let k, ui ∈ Vars

be fresh. The functions C0
and K0
are parameterized over a variable k that

represents the current continuation.
C0 : Λ → Λ
C0[[V ]] = k Φ0[[V ]]
C0[[E[xV ]]] = x K0[[E]] Φ0[[V ]]
k	k
C0[[E[(λx.M ) V ]]] = (λx.C0[[E[M ]]]) Φ0[[V ]]
k	k
Φ0 : Values → Λ
Φ0[[x]] = x
Φ0[[λx.M ]] = λk.λx.C0[[M ]]
K0 : EvCont → Λ
K0[[[ ]]] = k
K0[[E[x [ ]] ] = x K0[[E]]
k	k
K0[[E[(λx.M )[ ]] ] = λx.C0[[E[M ]]]
k	k
K0[[E[[ ] M ]]] = λui.C0[[E[ui M ]]]
k	k
The CPS transformation of a complete program M is λk.C0[[M ]].	✷

Making decomposition and plugging explicit
We make decomposition and plugging explicit through the two following func- tions.
decompose :Λ → Values + EvCont × Λ
plug : EvCont × Λ → Λ
Therefore, instead of C[[E[M ]]], we explicitly write C◦ plug [[E, M ]].
The original CPS transformation can then be expressed as follows, using an auxiliary function C˜1.

C1 : Λ → Λ
C1[[M ]] = C˜1 ◦ decompose[[M ]]
k	k
C˜1 : Values + EvCont × Λ → Λ
C˜1[[V ]] = k Φ1[[V ]]
C˜1[[E, x V ]] = x K1[[E]] Φ1[[V ]]
k	k
C˜1[[E, (λx.M ) V ]] = (λx.C1 ◦ plug[[E, M ]]) Φ1[[V ]]
k	k
Φ1 : Values → Λ
Φ1[[x]] = x
Φ1[[λx.M ]] = λk.λx.C1[[M ]]
K1 : EvCont → Λ
K1[[[ ]]] = k
K1[[E[x [ ]]]] = x K1[[E]]
k	k
K1[[E[(λx.M ) [ ]]]] = λx.C1 ◦ plug[[E, M ]]
k	k
K1[[E[[ ] M ]]] = λui.C1 ◦ plug [[E, ui M ]]
k	k
The CPS transformation of a complete program M is λk.C1[[M ]].

Inlining C1
makes it clear that decompose is mostly called with the re-

sult of plug —in fact always, since M = plug[[[ ], M ]], initially and in the CPS transformation of a λ-abstraction. Therefore, we can re-express the CPS trans- formation using a refocusing function combining decompose and plug .
Refocusing
The refocusing function refocus is defined as the composition of decompose
and plug . Its type is thus as follows.
refocus : EvCont × Λ → Values + EvCont × Λ
The CPS transformation can then be expressed as follows.
C2 : Values + EvCont × Λ → Λ
C2[[V ]] = k Φ2[[V ]]
C2[[E, x V ]] = x K2[[E]] Φ2[[V ]]
k	k
C2[[E, (λx.M ) V ]] = (λx.C2 ◦ refocus[[E, M ]]) Φ2[[V ]]
k	k
Φ2 : Values → Λ
Φ2[[x]] = x
Φ2[[λx.M ]] = λk.λx.C2 ◦ refocus[[[ ], M ]]
K2 : EvCont → Λ
K2[[[ ]]] = k
K2[[E[x [ ]] ] = x K2[[E]]
k	k


K2[[E[(λx.M )[ ]] ] = λx.C2 ◦ refocus[[E, M ]]
k	k
K2[[E[[ ] M ]]] = λui.C2 ◦ refocus[[E, ui M ]]
k	k
The CPS transformation of a complete program M is λk.C2 ◦ refocus[[[ ], M ]].
We are now free to use any implementation of refocus that is extensionally equivalent to decompose ◦ plug .
Eﬃciency
The following ‘deforested’ implementation of refocus avoids redundant plug- ging and decomposition. We write it by following the guidelines of Section 2, inlining refocusaux .
refocus[[E, M0 M1]] = refocus[[E[[ ] M1], M0]]
refocus[[E[[ ] M1], V0]] = refocus[[E[V0 [ ]], M1]]
refocus[[E[V0 [ ]], V1]] = [[E, V0 V1]]
That this function is extensionally equivalent to decompose ◦ plug and more efficient follows from the more general proof in Section 2.6. Also, that this refocus function is similar to the one in Section 3 is unsurprising since it is based on the same syntactic theory of the λ-calculus.
With this definition of refocus, the CPS transformation (C2, Φ2, K2) pro-
k	k
duces the same compact terms as (C0, Φ0, K0), but it operates in linear time,
k	k
or more precisely, in one pass, over its input.
Conclusion and issues
We have presented a general result about syntactic theories with context-free grammars of values, evaluation contexts, and redexes, and with a unique- decomposition property. This result enables one to mechanically derive an interpreter that does not incur a linear-time overhead for each reduction step. We have illustrated the result with an interpreter for the call-by-value λ- calculus and by mechanically turning a quadratic-time program transforma- tion into a program transformation that operates in one pass. In both cases, contexts are best represented inside out.
Acknowledgments:
We are grateful to Daniel Damian, Bernd Grobauer, Fritz Henglein, and Julia Lawall for commenting the initial version of this article. This work is sup- ported by the ESPRIT Working Group APPSEM (http://www.md.chalmers.se/Cs/Research/Semantics/APPSEM/).

References
Olivier Danvy and Andrzej Filinski. Representing control, a study of the CPS transformation. Mathematical Structures in Computer Science, 2(4):361–391,


1992.
Olivier Danvy and Lasse R. Nielsen. CPS transformation of beta-redexes. In Amr Sabry, editor, Proceedings of the Third ACM SIGPLAN Workshop on Continuations, Technical report 545, Computer Science Department, Indiana University, pages 35–39, London, England, January 2001. Also available as the technical report BRICS RS-00-35.
Olivier Danvy and Lasse R. Nielsen. Defunctionalization at work. In Harald Søndergaard, editor, Proceedings of the Third International Conference on Principles and Practice of Declarative Programming, pages 162–174, Firenze, Italy, September 2001. ACM Press. Extended version available as the technical report BRICS RS-01-23.
Olivier Danvy and Lasse R. Nielsen. Syntactic theories in practice. Technical Report BRICS RS-01-31, DAIMI, Department of Computer Science, University of Aarhus, Aarhus, Denmark, July 2001. Extended version of an article to appear in the informal proceedings of the Second International Workshop on Rule-Based Programming (RULE 2001), Firenze, Italy, September 4, 2001.
Matthias Felleisen. The Calculi of λ-v-CS Conversion: A Syntactic Theory of Control and State in Imperative Higher-Order Programming Languages. PhD thesis, Department of Computer Science, Indiana University, Bloomington, Indiana, August 1987.
John E. Hopcroft and Jeffrey D. Ullman. Introduction to Automata Theory, Languages, and Computation. Addison-Wesley, 1979.
Gordon D. Plotkin. Call-by-name, call-by-value and the λ-calculus. Theoretical Computer Science, 1:125–159, 1975.
Amr Sabry and Matthias Felleisen. Reasoning about programs in continuation- passing style. Lisp and Symbolic Computation, 6(3/4):289–360, 1993.
Mitchell Wand. Correctness of procedure representations in higher-order assembly language. In Stephen Brookes, Michael Main, Austin Melton, Michael Mislove, and David Schmidt, editors, Proceedings of the 7th International Conference on Mathematical Foundations of Programming Semantics, number
598 in Lecture Notes in Computer Science, pages 294–311, Pittsburgh, Pennsylvania, March 1991. Springer-Verlag.
Yong Xiao, Zena M. Ariola, and Michel Mauny.	From syntactic theories to interpreters: A specification language and its compilation.	In Nachum Dershowitz and Claude Kirchner, editors, Informal proceedings of the First International Workshop on Rule-Based Programming (RULE 2000), Montr´eal, Canada, September 2000. Available online at http://www.loria.fr/~ckirchne/=rule2000/proceedings/.
Yong Xiao, Amr Sabry, and Zena M. Ariola. From syntactic theories to interpreters: Automating proofs of decomposition lemma. Higher-Order and Symbolic Computation, 14(4), 2001. To appear.
