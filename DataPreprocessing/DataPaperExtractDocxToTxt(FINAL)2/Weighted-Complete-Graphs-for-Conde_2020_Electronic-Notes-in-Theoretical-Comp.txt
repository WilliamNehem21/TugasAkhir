Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 354 (2020) 45–60
www.elsevier.com/locate/entcs

Weighted Complete Graphs for Condensing Data
A. Guzm´an-Ponce 1 J. Raymundo Marcial-Romero 2
R.M. Valdovinos-Rosas 3
Universidad Auton´oma del Estado de M´exico, Facultad de Ingenier´ıa Toluca, Estado de M´exico, M´exico
J.S. S´anchez-Garreta4
Institute of New Imaging Technologies, Department of Computer Languages and Systems Universitat Jaume I, Castell´o de la Plana, Spain

Abstract
In many real-world problems (such as industrial applications, chemistry models, social network analysis, among others), their solution can be obtained by transforming the problem in terms of vertices and edges, that is to say, using graph theory. Data Science applications are characterized by processing large volumes of data, in some cases, the data size can be higher than the resources for their processing, situation that
makes prohibitive to use the traditional methods. In this way, to develop solutions based on graphs for condensing data can be a good strategy for handling big datasets. In this paper we include two methods for condensing data based on graphs, the two proposals consider a weighted complete graph by acquiring an induced subgraph or a minimum spanning tree from the whole datasets. We conducted some experiments in order to validate our proposals, using 24 benchmark real-datasets for training the 1NN, C4.5, and SVM classifiers. The results prove that our methods condensed the datasets without reducing the performance of the classifier, in terms of geometric means and the Wilcoxon’s test.
Keywords: Weighted Graph, Induced Subgraph, Minimum Spanning Tree, Condensed data, Data Science.


Introduction
Nowadays a promised area for extract knowledge from data is called Data Science. Data Science is a multi-disciplinary approach for finding, extracting, and discov- ering patterns in data through several methods from data mining, deep learning,

1 Email: mailto:angelicagp1416@hotmail.com
2 Corresponding author Email: mailto:jrmarcialr@uaemex.mx
3 Email: mailto:rvaldovinosr@uaemex.mx
4 Email: mailto:sanchez@uji.es

https://doi.org/10.1016/j.entcs.2020.10.005
1571-0661/© 2020 The Author(s). Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

forecasting, machine learning, optimization, predictive analytics, and statistics, be- tween others [3]. The main problem in Data Science is the data volume, in some cases, it can be so great that is prohibitive to apply traditional statistics methods and even some machine learning or data mining algorithms. For this reason, it is necessary to have strategies for obtaining a representative subset of the data with which the classification or prediction algorithms do not affect their performance.
Graph theory is becoming a popular technique in other areas of science for giving solution in real-world problems. For example, in chemistry a graph can represent the topology of a molecule, in physics, a graph can be used for describing the grade of thermodynamic stability, on electrical engineering the graph theory can be applied over configuration of antennas and their frequencies, in urban planning can be used for programing bus paths or the traffic lights, and many others optimization problems can be solved by graph theory because it is a way of knowledge abstraction, thus allowing get reliable solutions [13,16,8,6].
In Data science are several approaches based on graphs, where the main idea is based on extracting knowledge from graph topologies. Newman et al. [11] pro- posed an approach for clustering communities through iterative remove edges from a graph, the algorithm gives more weight to the borderline edges between commu- nities than the edges inside it. Their results showed that it is possible to extract community structures from networks artificially generated using information from known communities.
Zhang and Hancock [18] proposed a graph-based method to feature selection. They used weight graphs, where a vertex represents a feature and their pairwise relationship is an edge, the weight of an edge was given by the degree of relevance between two features. Their proposal uses the multidimensional interaction infor- mation criterion for feature selection, this criterion detects the relationships between feature combinations with greater relevance. Therefore, the proposal gets the most information that a class has.
On the other hand, Maillo et al. [10] proposed a fuzzy kNN approach based on Hybrid Spill-Tree, where through a tree structure the nearest-neighbors are approx- imated. This method has two types of binary trees: a Metric-Tree and a Spill-Tree. The Metric-tree organizes the instances of a dataset in a spatial hierarchical manner, where its root vertex represents all instances, and each vertex represents an instance. An internal vertex is partitioned into two subsets, who are disjoint sets, i.e they do not have repeated instances. While, the Spill-Tree is similar to a Metric-Tree, with the difference that Spill-Tree enables repeated instances.
In this paper, we introduce two methods based on graph theory, that using a weighted complete graph for condensing data. The main contributions can be summarized as follows:
We propose the use of graph theory to obtain an induced subgraph, which allows getting the borderline of the classes, and build a Minimum Spanning Tree (MST) which include the core of the data.
Depending on the balance between the classes included in the data, the number

of representative instances that will be considered by the induced subgraph and the MSP is computed.
The experimentation shows that when the condensed datasets obtained with our methods, the classifier do not diminish their performance, in fact, it is increased.
The remainder of the paper is structured as follows. In Section 2 we review the main definitions used in the paper. Details about the two proposals are detailed in Section 3. While in Section 4 we summarized the time complexity required by the methods proposed. The experimental setup is reported in Section 5. After that, in Section 6 the experimental results are explained. Finally, in Section 7 the main concluding remarks and some open lines are exposed.

Preliminaries
Let G = (V, E) be an undirected simple graph (i.e loop-less and without parallel edges) consisting of a nonempty vertex set V and a set of edges E, where each edge is a non-order pair of vertices, denoted as {u, v}. Two vertices v and v are named adjacent if there is an edge {u, v}∈ E, joining them. A complete graph is a simple graph in which any two vertices are adjacent.
The neighbourhood of a vertex v in a graph G = (V, E) is N (v) = {∀u ∈ V |
{v, u} ∈ E}, i.e N (v) is the set of all vertices adjacent to v without itself and its closed neighbourhood when N (v) ∪ v, which is denoted as N [v]. Note that v is not in N (v), but it is in N [v].
A graph H is a subgraph of G if V (H) ⊆ V (G) and E(H) ⊆ E(G). Let X ∈ V (G) a set of vertices deleted, the induced subgraph is denoted by G − X; if Y = V \ X represents the set of vertices that remain undeleted, the induced subgraph is denotes by G[Y ] and referred to as subgraph of G induced by Y , where Y is the set of vertices of G and whose set of edges consist of all edges of G that have both ends in Y .
A path from a vertex v to a vertex u in a graph is a sequence of edges: v0v1, v1v2, ··· , vn−1vn, such that v = v0, vn = u, vk is adjacent to vk+1 and the length of the path is n. A simple path is a path such that v0, v1, ··· , vn−1 are all distinct. A cycle is just a nonempty path such that the first and last vertices are identical, and a simple cycle is a cycle in which no vertex is repeated, except the first and last vertices.
A connected graph is a graph G = (V, E) if every pair of vertex in G has a path between them. If the graph is not connected, each maximal connected piece is named a component.
A weighted graph Gw = (V, E), is a graph where each edge e ∈ E let there be associated a real number w(e), called its weight. The adjacency matrix of a weighted graph Gw is a V × V matrix MG = (wvu) where each element (vi, vj) contains the weight w(e) assigned to the edge e = vi, vj or 0 according to wheter the vertices vi and vj are adjacent or not in the graph.
If H is a subgraph such that H ⊂ Gw the weight w(H) of the H is the sum

of the weights  w(e) on its edges. A subgraph of a certain type with minimum (or maximum) weight, is a graph which path of minimum (or maximum) weights connecting two specified vertices u0 and v0. A Minimum Spanning Tree (MST) is an induced subgraph, where the set of edges connects all vertices, without any cycles and the sum of its edge weights are the minimal.
Proposed methods for condensing datasets
In this section, we present two methods based on graphs for condensing data. The proposals start by dividing a two-class dataset with n instances into two subsets denoted as C− and C+, instances from negative class and positive class respectively. The C+ commonly is the most important class to identify and it is less represented respect with another class or classes. Our proposals work only over the negative class, in order to diminish the cardinality of it until it can be similar to C+. For that, from the data included in C− we building of a weighted complete graph before generating an induced subgraph or a Minimum Spanning Tree, which aims purpose is obtaining a borderline or a core of negative class respectively.
The Algorithm 1 describes the general procedure. The proposals build a weighted complete graph (GraphProcedure in the Algorithm 1) to generate an in- duced subgraph (Section 3.1) or a Minimum Spanning Tree (Section 3.2). Note that, IRm is a desirable imbalance ratio, that is, the desirable ratio of the positive class size to the negative class size.

Algorithm 1 Condensed data
Require: DS = {p1, p2,..., pn} , IRm
Ensure: DS’ condensed data set
1: Split DS into two subsets C− and C+. 2: C′− → GraphProcedure(C−, IRm, C+)
3: DS′ = C+ ∪ C′−
Given a dataset DS formed by n-instances with m-features, each instance pn is a tuple (fn,1, fn,2,..., fn,m, ω), where, fm is the value of the m − th feature of a instance pn. This instance belongs to a class ω. Graphs are used to model practical problems and get optimal solutions, thus our proposals use a graph-based method to obtain a subset Cj−. We consider the set C− as a weighted complete graph denoted as Gw, and it is built as follows:
V (G)= {∀pi ∈ C− | i ∈ V (G)} the set of vertex.
E(G)= {{v, u}| v, u ∈ V (G)} the set of edges.
∀e = {v, u}∈ E(G), w(e)= d(pv, pu) where d(pv, pu) is the euclidean distance between v and u.

Induced Graph Under-Sampling (IG-US)
IG-US is a proposed method to get an induced subgraph, which aims is to keep borderline instances, i.e instances that are further away from each other. The IG-US proposal described in the Algorithm 2 condense the negative class through

generating an induced subgraph of Gw, which set of vertices will have size according to the equation 1. The size will be updated and stored in the variable Sample until the algorithm obtains an imbalance ratio (IR 5 ) equal to maxIR (lines 2-7 in Algorithm 2).

⎧⎪	|C−|	σ2Z2

if x =1 

f (x)= 
⎪⎨ e2(|C−|−1)+σ2Z2
(1)

⎪⎪⎩
f (x−1)	σ2Z2 e2(|C−|−1)+σ2Z2
otherwise

where Z = 1.96, σ = 0.5 and e = 0.05 for a confidence level of 95%.
Once the size of the negative class subset is obtained, the Algorithm 2 computes the adjacency matrix of Gw (line 9) by the distance between each pair of instances (vertices). The function GetMaximum ensures a set of edges (pairs of instances), which have a high distance according to the adjacency matrix (line 11). For each edge in this set, the vertices that have not been visited yet are added into C− to be marked as visited (lines 12-20). In the end, the set C− is the condensed dataset
from the negative class.

Algorithm 2 IG-US algorithm
Require: C−, Rmax, C+
Ensure: C− a condensed dataset
1: Build Gw = (V, E) a weighted graph with V = C−
2: Sample → |C−|
3: IR → Sample
|C  |
4: while IR > maxIR do
5:	Sample →  Sample  σ2 Z2 
e2 (|C−|−1)+σ2 Z2
6:	IR → Sample
|C  |
7: end while
8: C− → []
9: MG → adjacencyMatrix(Gw) 10: while |C−| < Sample do
11:	Maximum → GetMaximum(MG) 12:	for all {v, u} in Maximum do 13:		if v has not been visited then 14:			Mark v as visit
15:	C− → C− ∪ {v}
2	2
16:	end if
17:	if u has not been visited then
18:	Mark u as visited
19:	C− → C− ∪ {u}
2	2
20:	end if
21:	Remove {v, u} from MG
22:	end for
23: end while
24: return C−


Minimum Spaning Tree Under-Sampling (MIST-US)
We will denote the second proposal as MIST-US, this approach builds a minimum spanning tree from a weighted complete graph. In general, the Algorithm 3 finds a

5 represent the difference between the cardinality of C− and C+

representative core of the negative class, trough a MST. The goal of using MST is to build a subgraph whose sum of edge weights is as small as possible, therefore the dispersion of the condensed negative class is less.

Algorithm 3 MIST-US algorithm
Require: C−, IRm, C+
Ensure: C′− ⊂ C− a condensed dataset
1: Build Gw = (V, E) a complete graph from C−. 2: S → |C−|
3: IR →  S 
|C  |
4: C′− → []
5: MG →incidenceMatrix(Gw) 6: MST →GetMST(Gw, MG)
7: while IR > IRm do
	S  σ2 Z2	
e2 (|C−|−1)+σ2 Z2
9:	IR →  S 
|C  |
10: end while
11: for all {v, u} in E(MST ) do
12:	C′− ∪ u
13:	C′− ∪ v
14:	if |C′−| > S then
15:	return C′−
16:	end if
17: end for
In our proposal, firstly an incidence matrix MG is built before process theGetMST method which ensures a MST. GetMST requires the weight graph Gw and its incidence matrix to process the Prim’s [12] algorithm, described as follows:
Choose a vertex v, this is the start vertex.
Select the edge e = {v, u} with the lower weight in MG, mark v as visited. Now, the next vertex to be analyzed is u.
Repeat step 2 whenever the chosen edge links a vertex that has been visited and others that have not been visited, provided any cycles.
The MST will be building until all vertices have been visited.
Although by definition a MST contains all the vertices of a graph, in our pro- posal, the condensed negative class is built by only the first S-vertices from the edge set of the MST. MIST-US algorithm finishes when the size of Cj− is greater than
S. The S value is the representative amount of instances from negative class, and it is computed by equation 1 (lines 7-10).
Time complexity
Time complexity is estimated counting the number of operations performed by an algorithm [2], thus the worst-case is used to measure the time complexity. In our proposals, IG-US and MIST-US time complexity is described as follows:
IG-US algorithm has a time complexity of O(n2). In this case, the complexity is mostly governed by the following set of instruction:
Updating Sample and IR (lines 4-7) has a complexity of n.
Computing the adjacency matrix (line 9) has a complexity of n2.

Generating a majority class subset of size Sample (lines 10-23) has a complexity of n2, which comes from n iterations to compute the set Maximum with a complexity of n.
Thus, the time complexity of IG-US algorithm becomes O(n2) in the worst-case.
In similar way, MIST-US is computed on O(n2) too, because of:
The incidence matrix from Gw in the worst case over n2 is computed.
Get a Minimum Spanning Tree, based on Prim’s algorithm, in the worst case takes n2 because it is necessary to visit all edges with an endpoint that has not been visited in the incidence matrix MG with the lower weight.
Experimental set-up
In order to validate the methods proposed, we conducted an experimental study designed over 24 real-problem datasets with several imbalanced ratio (IR) i.e. IR >
9. The next sections describe the datasets used, the metrics employed for measured the methods performance and the classifiers used to validate our proposals.

Datasets
The experiments were carried out on 24 two-class data sets taken from the KEEL Data Set Repository (https://sci2s.ugr.es/keel/imbalanced.php#subA), each dataset has different distribution between the classes i.e IR. Table 1 summarizes the main characteristics of the datasets used in the experiments, the data sets are sorted on an increase in IR value. The IR is obtained by dividing the number of patterns included in C+ between the number of patterns of C−. As we can see in Table 1, the range of IR is around more than 9 to 82, which indicates a high imbalance ratio.
All datasets have been partitioned using 10-fold cross-validation in order to avoid biased results [5]. Each original data set was randomly divided into 10 stratified parts, for each fold, 9 blocks have been used as a training set, and the remaining portion was used as a test set. We evaluated our methods, comparing their benefit in contrast with the most popular methods for condensing data included in the literature (see Table 2).

Evaluation metrics
The performance of the condensing methods was tested with a Decision Tree classi- fier (C4.5), the 1-Nearest Neighbor classifier (1-NN) and a Support Vector Machine (SVM) classifier with all default values of the WEKA software. To evaluate the behavior of learning classifier, a confusion matrix like that in Table 3 (for a two- class problem) is usually employed [4]. From this, four simple measures can be directly obtained: TP and TN denote the number of positive and negative cases correctly classified, while FP and FN refer to the number of misclassified positive and negative examples, respectively.

Table 1
A brief summary of the basic characteristics of the database.

Data Set	Class distribution	#Instances	IR
1	yeast-0-5-6-7-9 vs 4	51 - 477	528	9.35
2	vowel0	90 - 898	988	9.98
3	glass-0-1-6 vs 2	17 - 175	192	10.29
4	glass2	17 - 197	214	11.59
5	shuttle-c0-vs-c4	123 - 1706	1829	13.87
6	yeast-1 vs 7	30 - 429	459	14.30
7	glass4	13 - 201	214	15.47
8	ecoli4	20 - 316	336	15.80
9	page-blocks-1-3 vs 4	28 - 444	472	15.86
10	glass-0-1-6 vs 5	9 - 175	184	19.44
11	shuttle-c2-vs-c4	6 - 123	129	20.50
12	yeast-1-4-5-8 vs 7	30 - 663	693	22.10
13	glass5	9 - 205	214	22.78
14	yeast-2 vs 8	20 - 462	482	23.10
15	flare-F	43 - 1023	1066	23.79
16	yeast4	51 - 1433	1484	28.10
17	yeast-1-2-8-9 vs 7	30 - 917	947	30.57
18	yeast5	44 - 1440	1484	32.73
19	ecoli-0-1-3-7 vs 2-6	7 - 274	281	39.14
20	abalone-17 vs 7-8-9-10	58 - 2280	2338	39.31
21	yeast6	35 - 1449	1484	41.40
22	shuttle-2 vs 5	49 - 3267	3316	66.67
kdd-buffer oflow vs back	30 - 2203	2233	73.43
poker-8-9 vs 5	25 - 2050	2075	82.00
Table 2 Condensing methods


Table 3 Confusion matrix

When the distribution between classes is not equal in the dataset, the most recommended measure is the geometric mean, because it tries to maximize the accuracy on each of the two classes while keeping these accuracies balanced [1]:

CNN
TL




RUS

1NN C4.5 SVM



EUS


Original
0  2  4  6  8  10

RUSBOOST


MIST-US

SBC
IG-US

Fig. 1. Average Rank
g = √a+ · a−, where a+ indicate the accuracy for the minority class (   T P	 ) and
a− is the majority class accuracy (  T N	 ).
Analysis of results
In this section, we analyze the results according two ways:
A comparison between the proposed method and the techniques described in the Table 2, using the classifiers 1NN, C4.5 and SVM .
Influence of the percentage reduction on the behavior of the classifiers 1NN, C4.5 and SVM when the condense datasets are used.
Comparison on Performance
The experimental results given in Figure 1 correspond to the average Fisher’s rank where the best condensing method is the one with the lowest value. These results are obtained by the overall geometric mean, which results are reported in the Ap- pendix A. Results with the original dataset i.e without reduction, were included for comparison purposes.
From the results reported in this section, some preliminary conclusions can be drawn. First, from results in Figure 1, independently the classifier used IG-US and MIST-US get the best performance according to the Fisher’s rank. When com- pare the behavior between the condensed methods, we can see that our methods outperform strategies that are based on neighborhood, such as CNN and TL. Nev- ertheless, some random methods can obtain competitive results, just as, RUS and RUSBOOTS, whose Fisher’s ranks in classifiers as C4.5 and SVM get similar results that ours.
Comparing both proposals (Table A.3), we can observe that IG-US is still su- perior to MIST-US over 1NN and C4.5 classifiers on 14 and 13 datasets per each classifier, while MIST-US has 6 and 8 datasets, respectively. Only in SVM classi- fier, MIST-US get better performances with 11 datasets compared to IG-US in 10

datasets.
Finally, we can conclude that get a core of the negative class as a condens- ing method has better performance (MIST-US) than keeps a borderline from the negative class (IG-US), because MIST-US keeps a representative subset of negative class which is far away from the borderline between classes. According to the results shown in this section, we consider necessary to make a statistical analysis, which is included in the next section.
Statistic signiﬁcant analysis
In order to complete the analysis we apply the Wilcoxon’s test with a level of confidence of 0.9 and 0.95, in order to identify the statistical significance between the results. Tables 4-6 summarize the Wilcoxon’s test, where the upper diagonal is part of the level significance α = 0.9, and the lower diagonal to α = 0.95. The symbol “•” denote that the method in the row improves the method of the column, otherwise, we used the symbol “◦”. The last two rows indicate the number of methods where the column method gets statistical significance compared with the rest of the methods according to the significance level.
Table 4
Summary of the Wilcoxon’s test using the 1NN classifier.

In order to conduct the analysis, we take as an example Table 5, focusing on the proposal IG-US, when we compare by pairs, with a significance level of 0.95 (row in bold), there are four methods in which IG-US improves to the other methods (Original, CNN, TL, and SBC). However, with a significance level of 0.90 (column in bold), there are six methods in which IG-US provides statistical significance over the methods: Original, RUS, CNN, TL, EUS, and SBC.
From results in Tables 4-6 it is possible to identify that methods which remove instances considered as redundant, for example, CNN, the performance is lower than the rest of the methods because in any classifier there are elements that improve the results obtained by CNN compared to other methods. Nonetheless, methods such as RUS, TL, RUSBOOST, and SBC show a behavior statistically significantly below our proposals over the 1NN classifier, but with SVM classifier the EUS method has a better statistical significant than IG-US, but MIST-US is significantly superior

Table 5
Summary of the Wilcoxon’s test using the C4.5 classifier.

Table 6
Summary of the Wilcoxon’s test using the SVM classifier.

overall methods. From these results, we can conclude that weighted complete graphs for condensing data are better than another method used in the experiments.

Influence of percentage reduction
An important aspect that determines if a condensed method is viable or not, is the percentage of IR obtained. Figure 2 plots the average percentage reduction per method, details of the reduction percentages per each method and dataset are in the Appendix B.
From Figure 2, we can remark that:
IB-US and MIST-US are the methods with the highest IR reduction, it is around 1.0.
The methods that not have a good percentage of reduction are TL and SBC. The first one removes instances according to achieve the form a link, so in most datasets, it is possible that in the negative class there are not instances that form a Tomek link. While the second method, cluster construction is probably the reason, due to the low density of the negative class.

Finally, IB-US and MIST-US have the best behavior in terms of geometric mean with datasets condensed with too low IR.

TL


EUS






RUSBOOST






SBC
CNN






RUS
0  20 40 60 80 100





MIST-US


IG-US

Fig. 2. Average percentage reduction

Concluding Remarks
In this work, we propose two new condensing methods called IG-US and MIST-US. The new methods consider a weighted complete graphs for obtaining two structures: the first one is the generation of an induced subgraph, which keeps the borderline instances from negative class, and the second one obtains a Minimum Spanning Tree constructed with the patterns included in the core of the negative class.
The experimental study was carried out using three supervised classifiers: 1NN, C4.5 and SVM, which allowed validate the effectiveness of the methods proposed.
We compare the performance of both proposals with some well-know condensed methods, finding that both methods obtain datasets with an IR very low with high quality, where the classifiers outperform their behavior respect than the obtained with the when other condensed methods are used. In addition, the computational cost of both proposals, in the worst-case, is computed on O(n2).
Finally, using the Wilcoxon test was possible to emphasize that the methods proposed obtain high condensed datasets without lost useful information, with dif- ference statistically significant and better behavior in terms of geometric mean, in comparison with othrers condensed methods widely used in the state-of-art.
The open lines pointing out to study the multi-class problems, to apply another classifiers, as well as to use datasets with greater size.

Acknowledgment
This work was partially supported by the Universitat Jaume I under grant [UJI-B2018-49], the Mexican CONACYT under scholarship [702275] an by the 5046/2020CIC UAEM proyect.

References
Cleofas-Sanchez, L., J. S´anchez, V. Garc´ıa and R. Valdovinos, Associative learning on imbalanced environments: An empirical study, Expert Systems with Applications 54 (2016), pp. 387–397.
Cormen, T. H., C. E. Leiserson, R. L. Rivest and C. Stein, “Introduction to algorithms,” MIT press, 2009.
Dhar, V., Data Science and Prediction, Commun. ACM 56 (2013), pp. 64–73.
Galar, M., A. Fernandez, E. Barrenechea, H. Bustince and F. Herrera, A Review on Ensembles for the Class Imbalance Problem: Bagging-, Boosting-, and Hybrid-Based Approaches, IEEE Transactions on Systems, Man, and Cybernetics, Part C (Applications and Reviews) 42 (2012), pp. 463–484.
Garc´ıa, V., A. I. Marqu´es and J. S. S´anchez, Exploring the synergetic effects of sample types on the performance of ensembles for credit risk and corporate bankruptcy prediction, Information Fusion 47 (2019), pp. 88–101.
Gonz´alez, A., E. Barra, A. Beghelli and A. Leiva, A sub-graph mapping-based algorithm for virtual network allocation over flexible grid networks, in: 2015 17th International Conference on Transparent Optical Networks (ICTON), 2015, pp. 1–4.
Hart, P., The Condensed Nearest Neighbor rule, IEEE Transactions on Information Theory 14 (1968),
pp. 515–516.
Hassani, L., M. R. Moosavi and P. Setoodeh, A graph based approach to analyse metabolic networks for strain engineering, in: 2019 27th Iranian Conference on Electrical Engineering (ICEE), 2019, pp. 1839–1843.
Liu, X., J. Wu and Z. Zhou, Exploratory Undersampling for Class-Imbalance Learning, IEEE Transactions on Systems, Man, and Cybernetics, Part B (Cybernetics) 39 (2009), pp. 539–550.
Maillo, J., J. Luengo, S. Garc´ıa, F. Herrera and I. Triguero, A preliminary study on hybrid spill-tree fuzzy k-nearest neighbors for big data classification, in: 2018 IEEE International Conference on Fuzzy Systems (FUZZ-IEEE), 2018, pp. 1–8.
Newman, M. E. J. and M. Girvan, Finding and evaluating community structure in networks, Phys. Rev. E 69 (2004), p. 026113.
URL https://link.aps.org/doi/10.1103/PhysRevE.69.026113
Prim, R. C., Shortest connection networks and some generalizations, The Bell System Technical Journal
36 (1957), pp. 1389–1401.
Samaddar, A., T. Goswami, S. Ghosh and S. Pal, An algorithm to input and store wider classes of chemical reactions for mining chemical graphs, in: 2015 IEEE International Advance Computing Conference (IACC), 2015, pp. 1082–1086.
Seiffert, C., T. M. Khoshgoftaar, J. Van Hulse and A. Napolitano, RUSBoost: A Hybrid Approach to Alleviating Class Imbalance, IEEE Transactions on Systems, Man, and Cybernetics - Part A: Systems and Humans 40 (2010), pp. 185–197.
Tomek, I., wo Modifications of CNN, IEEE Transactions on Systems, Man, and Cybernetics SMC-6
(1976), pp. 769–772.
Turvill, D., L. Barnby and A. Anjum, A conceptual framework for the use of graph representation within high energy physics analysis, in: 2018 18th IEEE/ACM International Symposium on Cluster, Cloud and Grid Computing (CCGRID), 2018, pp. 384–385.
Yen, S.-J. and Y.-S. Lee, Cluster-based under-sampling approaches for imbalanced data distributions, Expert Systems with Applications 36 (2009), pp. 5718–5727.
Zhang, Z. and E. R. Hancock, A graph-based approach to feature selection, in: X. Jiang, M. Ferrer and
A. Torsello, editors, Graph-Based Representations in Pattern Recognition (2011), pp. 205–214.

Classification results
Tables included in this appendix summarizes the performance classification with the 1-NN, C4.5 and SVM classifier over condensed datasets.
Table A.1
Geometric mean results obtained by 1-NN.

Table A.2
Geometric mean results obtained by c4.5.















Table A.3
Geometric mean results obtained by SVM.






Percentage of Reduction
Table B.1 summarizes the final percentage reduction after applying the condensing methods.
Table B.1
% Reduction.
