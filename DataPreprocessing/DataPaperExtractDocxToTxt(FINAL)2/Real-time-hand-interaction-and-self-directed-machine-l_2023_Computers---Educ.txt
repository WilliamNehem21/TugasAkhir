Computers & Education: X Reality 3 (2023) 100038








Real-time hand interaction and self-directed machine learning agents in	 immersive learning environments
Muhammad Zahid Iqbal *, Abraham G. Campbell
School of Computer Science, University College Dublin, Ireland



A R T I C L E I N F O 

Keywords:
Mixed reality Virtual reality Extended reality
Technology enhanced learning Immersive learning
XR
Learning technology
A B S T R A C T 

Integration of extended reality (XR) in education is becoming popular to transform the traditional classroom with immersive learning environments. The adoption of immersive learning is accelerating as an innovative approach for science and engineering subjects. With new powerful interaction techniques in XR and the latest de- velopments in artificial intelligence, interactive and self-directed learning are becoming important. However,
there is a lack of research exploring these emerging technologies research with kinesthetic learning or “hands- one learning" as a pedagogical approach using real-time hand interaction and agent-guided learning in
immersive environments. This paper proposes a novel approach that uses machine learning agents to facilitate interactive kinesthetic learning in science and engineering education through real-time hand interaction in the virtual world. To implement the following approach, this paper uses a chemistry-related case study and presents a usability evaluation conducted with 15 expert reviewers and 2 subject experts. NASA task load index is used for cognitive workload measurement, and the technology acceptance model is used for measuring perceived ease of use and perceived usefulness in the evaluations. The evaluation with expert reviewers proposed self-directed learning using trained agents can help in the end-user training in learning technical topics and controller-free hand interaction for kinesthetic tasks can improve hands-on learning motivation in virtual laboratories. This success points to a novel research area where agents embodied in an immersive environment using machine learning techniques can forge a new pedagogical approach where they can act as both teacher and assessor.





Introduction

Extended Reality (XR) is transforming learning technology by providing the ability to create more interactive learning content (Skult & Smed, 2020) and simulating immersive user experiences to understand complex technical concepts more practically (Dawley & Dede, 2014). The use of XR as learning technology, explained by Dengel (2022) as immersive learning, has increased quickly in the last couple of years due to the higher immersion capabilities. The adoption of Head-Mounted Displays (HMDs) and smart glasses is increasing with time due to improved interaction techniques, more accessibility, decreasing cost, and increasing portability. Following these developments in HMDs and computer graphics, immersive learning technology has gained enough potential to take technology-enhanced learning in resource-constrained environments closer to real-world settings (Gao et al., 2021). Among all other novel approaches in learning technologies, Immersive Learning Environments (ILE) are the most revolutionary interactive platforms (Freina & Ott, 2015), that can become more productive with self-guided
learning and hands-on learning capabilities. Self-guided learning which also known as self-directed learning (Abdullah, 2001) helps learners to take initiative and responsibility for planning, organizing, and executing their learning plans (Sandars & Walsh, 2016). In this type of learning, learners are not depending on direct supervision or guidance, rather they have control over what, how and when they learn. Immersive learning applications depend on different factors, including rendering quality, responsiveness, interactiveness, and user interfaces (Vlahovic et al., 2022). The latest hand interaction technologies can help to incorporate kinesthetic learning in STEM (Science, Technology, Engi- neering, and Mathematics) subjects because they provide hands-on experiential learning opportunities that complement the theoretical concepts taught in these disciplines (see Figs. 5 and 6).
Furthermore, Banjar and Campbell (2022) introduced the gamifica- tion approach in XR whichdemonstrated a motivation factor to enhance
students’ performance and learning outcomes. With the exponential growth in computational power and recent technological advancements
in artificial intelligence, intelligent agents are rapidly evolving. Stanney



* Corresponding author.
E-mail address: Muhammad-zahid.iqbal@ucdconnect.ie (M.Z. Iqbal).

https://doi.org/10.1016/j.cexr.2023.100038
Received 21 August 2022; Received in revised form 22 August 2023; Accepted 27 August 2023
Available online 7 September 2023
2949-6780/© 2023 The Authors. Published by Elsevier Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).



et al. (2022) highlighted the growing use of AI agents to implement as intelligent tutors to provide personalized guidance in the learning pro- cess. They can help in end-user training, analyzing learners’ behavior,
assessing performance, and adapting the new learning materials ac-
cording to individual needs. Interestingly, concepts such as real-time hand interaction for hands-on kinesthetic learning and agent-oriented approaches to support self-guided learning are still not widely investi- gated in immersive learning. This research has proposed higher user interaction with the latest real-time hand-interaction technologies and incorporating intelligent agents in immersive learning to make it more user-centered and self-guided. The principal contribution of this research is to implement the AGILEST AGents to facilitate Interactive kinesthetic LEarning in STEM education using a Touchless interaction approach with HMDs, using touchless hand interaction and machine learning agents.

Related work

From simply overlaying digital content on top of real environments to a completely virtual environment, XR has been successfully imple- mented and adopted at different levels (Papanastasiou et al., 2019). Recent advances in machine vision have taken hand tracking to a very advanced level which is like a realistic hand interaction (Hameed et al., 2021; Kang et al., 2020). Examples include GesturAR presented by Wang et al. (2021), which uses freehand gestures within its augmented reality (AR) authoring tool, allows end-users to use customized freehand out- puts for creating AR applications. Birt, James et al. (2018) used mixed reality (MR) on smartphones for experiential learning with a hybrid self-directed approach to successfully enhance engagement in medicine and health sciences. Similarly, Iqbal and Campbell (2021a, 2021b) re- ported a case study with leapmotion hand interaction and agents for learning PC assembly. Some post-COVID recommendations for using immersive learning environments suggested adoption for such modern approaches that create productivity in the learning process (Wang et al., 2022). XR in its different forms has been explored in healthcare and medical training as a virtual reality (VR) simulator (Dyulicheva et al., 2021), collaborative training for better visualization (Chheang et al., 2019) and interaction in anatomy learning (Zorzal et al., 2019), which shows the capabilities of this technology at a broader scope. Hu-Au and Okita (2021) reported a comparison of a VR chemistry laboratory and a traditional chemistry laboratory concluded participants in the VR lab have higher learning gain than the traditional.
MagicChem developed by Pan et al. (2022), is a MR system designed
to conduct chemistry experiments where users can learn with hand interaction-based basic chemical experiments presented higher user satisfaction and interaction compared to the traditional chemistry lab.
Ramírez, J. A´., & Bueno, A. M. V. (2020) conducted a fully Immersive VR
(IVR) experiment with Oculus Rift for learning organic chemistry, proved its capability of emotionally involving students with higher motivation and making the learning process more engaging for peda- gogical objectives. Based on the IVR practical experiments for chemistry learning, Miller et al. (2021) suggested that IVR can effectively reinforce learning and increase student success in formal classroom settings. Ac- cording to Araiza-Alba et al. (2021), using IVR to learn and practice problem-solving skills proved that immersive technology could engage interest, motivate the users and assist in cognitive processing. The exploration of using immersive technology for nursing students by Kim and Ahn (2021) found learning satisfaction as one of the most significant factors for adopting immersive technology. Lowe and Liu (2017) re- ported use of AR with see-through head-mounted displays (HMDs) for conventional experimental approach as a worthy tool for increasing learning gain in the chemistry experiments with illustrative scenarios. Another approach by Fujiwara et al. (2021), for collaborative chemistry learning using VR technology supported the evidence for distance learning approach with remote collaboration in VR spaces.
There are many research studies that have reported successful
hypotheses testing with different settings of immersive learning with different display and tracking devices (Beck, 2019; Hurrell & Baker, 2021; Jantakoon et al., 2019; Ummihusna & Zairul, 2022). An immer- sive learning approach adopted by Edwards et al. (2019) with haptic technology for learning chemistry called VR Multi-sensory Classroom (VRMC) achieved a higher engagement, motivation, and interest in a controlled learning environment. Furthermore, using machine learning in immersive learning environments for confidence estimation found that Long Short Term Memory (LSTM) model can help to understand
trainee’s learning status and increase learning performance (Tao et al., 2020). A recent study by Elme et al. (2022) presented evidence that
immersive VR is an effective learning technology for acquiring complex scientific concepts where formal learning approaches are not producing sufficient results. The use of immersive learning in environmental and sustainability education has found XR as affordable solution for enhancing the understanding of complex topics related to environment (Aguayo & Eames, 2023; MacDowell, 2023).
By incorporating artificial intelligence agents in immersive learning environments, Sharma et al. (2019) tested an emergency response sys- tem. By creating an experimental setup, this research proposed the use of immersive learning environment for training security agents for emer- gency response. Elor and Kurniawan (2020) used deep reinforcement learning in immersive VR and presented a novel game mechanic for exercise games that suggested human-level performance is possible with agents in immersive environment if we use right parameters. Jacobson et al. (2008) proposed the implementation of multi-user immersive learning experiences with intelligent agents to support the learning gain and engagements.
These approaches of using immersive technology in different display devices and diverse learning environments are gaining popularity in STEM education. It offers unique opportunities for learners to experi- ence real-world phenomena in safe and controlled environment. Although XR is well investigated in learning but not in the context of its future use in resource-constrained and remote environments, where there may be limitations in funding, access to physical resources, or qualified instructors. Secondly, new horizons in the intelligent agents are not well explored in the domain of XR for learning to make the immersive learning more independent and supportive for personal learning environments where students can learn without the support of an instructor. Considering exciting studies, current challenges and future emerging opportunities presented by Iqbal et al. (2022)., this research focuses on investigating the research gap of exploring real-time hand interaction for kinesthetic learning pedagogy and self-directed learning in immersive environment.

Research questions

RQ1: Can we use real-time hand interaction with virtual objects in immersive environments to help learners in kinesthetic learning for STEM subjects? This aims hands-on experiments in the resource- constrained environment where provision of physical material is not possible.
RQ2: How self-guided learning approach with intelligent agents can help in user training and independent learning in immersive learning environments? The aim is to incorporate agents which can help the learners in the independent environment where instructor is not available.

Immersive learning Environment with hand interaction &
ML- agents

The methodology plays a vital role in developing an innovative so- lution by providing a structured and systematic approach for designing, implementation, and evaluation. There are two major components of our proposed solution; real-time hand interaction in virtual environment and agent guided learning. The aim of using hand interaction in the



immersive environment is to facilitate the kinesthetic learning approach for science subjects in resource-constrained scenarios. The integration of machine learning agents aims to enhance the user training process and facilitate assessment process.
To test proposed with HMDs, a case study is developed for Oculus Quest 21 using controller-free hand interaction. The reason for prefer- ring Oculus Quest over other VR handsets was the standalone feature of Oculus (Hillmann & Hillmann, 2019), and hand tracking functionality (Bun´ et al., 2022) as a major requirement. This case study uses chemistry learning experiments with a resource-constrained learning scenario following the concept of virtual laboratories. Fig. 1 has explained the system architecture diagram which is providing an overview of learning flow.
Following the virtual chemistry lab concept by Qin et al. (2020), this study provides an immersive learning experience using touchless hand interaction technique (Iqbal & Campbell, 2021a, 2021b) and AI-guided approach to support learners. The learning flow adopted in the appli- cation (Fig. 1), consists of three different modules LEARN, TEST, and QUIZ. The first module uses machine learning agents to guide the user in learning chemical reaction with previous trained agent and gain knowledge to practice in the TEST module. After getting training with the LEARN module, the user moves to TEST module, where actual hands-on learning allows the user to perform hands-on or kinesthetic tasks for creating chemical reactions with virtual hand interaction. Finally, the last module is QUIZ, which is an MCQ-based assessment on learning from the previous two modules.

Training module - LEARN

Unity ML-Agents are used to implement machine learning agents (Juliani et al., 2018) in the LEARN module. These agents are using reinforcement learning, defined by Sutton and Barto (2018) as “a ma- chine learning training method award for desired behaviors and/or punish for undesired behaviors”. Unity ML-Agents is an open-source toolkit originally developed to integrate AI with machine learning in gaming. A reinforcement learning agent can perceive and interpret the environ- ment, take actions, and learn through errors (Noothigattu et al., 2019) as Fig. 2 explains. Unity ML-agents are developed with python APIs and can be integrated with the Unity applications after training the neural network (NN). This process consists of Integrate, Train, and Embed; further explained in detail in Fig. 3. This toolkit previously explored in the context of custom gameplay by Youssef et al. (2019), shooting game by Lai et al. (2019), e-sports by Li (2022), and other kinds of gaming environments. The use of agents in immersive technology, especially in the context of immersive learning, is not explored yet to a level where this toolkit can help to develop a learning environment for STEM sub- jects (see Fig. 4).
The training process of LEARN module consists of the following;

Collecting required data of reinforcement learning; states, actions, rewards
Training of reinforcement learning model
Inputting trained model back to the unity application

ML-agent trained for Grab, Move and Collide actions of user with observation collection function during kinesthetic tasks for performing chemical reactions in the virtual environment (as in Fig. 4). It considers each episode’s length and time consumed to perform a complete
episode. An agent training process consists of several episodes followed
by changing buffer size to gain better results.
Different buffer sizes are needed to conduct an efficient training session for a reasonable learning rate. These parameters include buffer size, maximum steps, batch size and normalization permissions inside
the Unity ml-agents. Agent learning is action based learning, learning through interaction with the cubic chemicals.

Kinesthetic learning module - TEST

The TEST module allows users to interact with 3D chemical elements and following the machine learning module, create chemical reactions in an immersive environment. In this module, reactions are followed by audio and vibration feedback to let the user know about chemical reactions.
This module is developed using the latest Interaction SDK of Oculus Quest, which allows controller-free hand interaction in virtual envi- ronments. Using deep learning technology, Unity ML-Agents SDK pro- vides a very realistic and complex AI environment for training neural network models.
In the TEST module, users can understand the gas, crystals, and liquid forms of different elements and molecules (see Fig. 5). This module allows users to move within the assigned tracking space and follow the complete hands-on learning approach.

Assessment module - QUIZ

After completing the TEST module and learning with hands-on practice, the user will enter to QUIZ module. The QUIZ module over- lays the TEST module to provide MCQs-based assessment quiz for users. On every right answer, user will get 10 score and negative 5 if the user selects the wrong choice. The QUIZ module is linked to a database where instructors can review students’ learning outcomes.
Evaluation plan

When adopting a new technology, evaluation and assessment are the most crucial components in immersive learning same as other learning applications. Ledo et al. (2018) clarified the importance of adopting the right evaluation approach and how using ineffective or imperfect ap- proaches in the learning systems can risk the successful acquisition of targeted learning goals and outcomes. It can also mislead the user experience with specific technologies creating negative perceptions. To achieve the proof-of-concept, this evaluation study aimed to investigate the effectiveness, interactiveness, and level of value creation in the learning process.
Arpaia et al. (2022) emphasize that usability evaluation of XR sys- tems allows the developers to assess different aspects of the system in a better way. In addition, the evaluation reveals how the users interact with the system and finds if the system easy to use, required meeting the proposed goals and investigating design issues (Dengel & Ma¨gdefrau, 2018). This blended learning approach using the components of Section 3 in a VR application is supposed to provide a self-directed learning experience and user confidence in the immersive environment.
This evaluation study investigated whether integrating hand inter- action in immersive environment and using machine learning for user training could improve the students’ concept learning in science subjects
with hands-on practice. This evaluation study uses the expert reviewers’
method to assess the usability of the proposed system.

Recruiting participants

For evaluating the prototype, an in-person evaluation was conducted during ACM International Conference on Interactive Media Experiences (IMX) 20222 and 16th EATEL Summer School on Technology Enhanced Learning 2022 (Learning, 1999). EATEL Summer School3 is an annual gathering of researchers in Technology Enhanced Learning (TEL) who






1 https://www.meta.com/gb/quest/products/quest-2/.
2 https://imx.acm.org/2022/.
3 https://ea-tel.eu/jtelss22.




Fig. 1. System architecture for AGILEST Approach for HMDs.


Fig. 2. Process of reinforcement learning in ML-agents.


are working on immersive learning, learning analytics, and learning intelligence.
ACM IMX is an international conference for researchers in interactive media experiences with major contributors from XR researchers. The main pre-requisite for participants participating in the research evalu- ation was a research-level experience in immersive technology, HCI, and technology-enhanced learning with peer reviewed publications. A direct outreach was performed based on identified research work in their contribution to the IMX and summer school. A total of 15 expert re- viewers (plus two subject experts presented in section 5.3) participated in the evaluation process, which included 6 subjects from summer school and 9 from the IMX conference.
Out of 6 in summer school, 2 were females and 4 males, while out of
9 in IMX conference, all 9 were males. All of the expert reviewers are at different levels of research with diverse age, starting from 23 to 38 as the maximum one (Fig. 6). Among these 15 participants, 6 participants who were from summer school had more background in technology- enhanced learning as general or learning pedagogy in XR. The 9 other participants who participated from the IMX conference had more experience in multimedia and interactive technologies for learning, which includes different types of immersive technologies.

Learning goals

In immersive learning, the primary learning goal is to support the learner with engaging and interactive experience to promote deep




Fig. 3. Steps of Training Machine Learning Agents & integrating in Mixed Reality application.





Fig. 4. Process of training the Neural Network (NN) of ML-Agent.

understanding on the content (Appelman, 2005), developing skill and knowledge retention (Webster, 2016). In the context of this research, the learning goal of the experiments is basic chemical reactions at the sec- ondary school level. The learning goal also includes evaluating the learning gain, cognitive load, and user confidence in the adopted approach.


Subject matter

Aside from the formal evaluation process by testing, we invited two
(2) secondary school level instructors to participate in the process of developing and testing learning flow, using hand interaction and introducing cubic elements in the virtual environment. The aim of involving the chemistry-related instructors was to get subject-related help for usability recommendations for the system and help with the
future work in TEL for STEM.

Procedure

Experimental procedures in immersive learning can have different procedures depending on the experiment’s goals. As a first step, getting informed consent is a crucial part in immersive learning same as any
other research involving human participants. After getting consent from participants, they were introduced to the technical aspects of the application and the learning goal of this research. Further, they were guided about the learning flow in the application before starting actual experiments with Oculus Quest. With hand-tracking functionality enabled, Oculus Quest 2 was used in the experimentation process with fully controller-free interaction. Every expert reviewer was asked to step into LEARN module after starting the experiment, where a pre-trained machine learning-based neural network can help to learn creating chemical reactions in the virtual environment (Fig. 8).
After learning through LEARN module, each participant switched to TEST module for practicing hands-on learning for creating chemical reactions practically. The tracking area, field view, and interaction space are set to a user sitting on a chair and can reach all elements at arm’s
length without the need to walk in the tracking area. By following
learning from the LEARN module, participants created all chemical re- actions using virtual hand interaction with 3D cubic elements.
After completing the both LEARN and TEST modules, each partici- pant was provided with a Google form based questionnaire based on two components;

NASA Task Load Index (NASA TLX)
Technology Acceptance Model (TAM) to measure Perceived useful- ness (PU) and Perceived ease-of-use (PEOU)




Fig. 5. (a) Start Menu and Navigating between different modules of the application; (b, c d) Interacting with chemical elements to grab and create reactions.





Fig. 6. Participants Age group.

NASA task load index (NASA TLX) is a tool used for conducting mental workload (MWL) assessment (Hart, 2006), its main objectives are listed in the Table 1. It is a widely used approach for subjective and
multi-dimensional assessment. Applying Technology Acceptance Model (TAM) in VR can help to get valuable insights about the factors which influence the acceptance of VR learning experiences by users (explained in Fig. 7).
By understanding the perceived usefulness, perceived ease of use, attitude toward using application, behavioral intention to use, and actual use of VR technologies, this evaluation has presented ways to optimize the proposed design of VR applications for improving user acceptance. Grani´c and Maranguni´c (2019) reported TAM is one of the most known research models to determine acceptance of information systems/information technology. According to Subramanian (1994) and Papakostas et al. (2021), perceived usefulness and perceived ease of use are fundamental determinants of TAM for user acceptance. When a user perceives a new technology as useful, they are likely to see value in adopting it due to its alignment with their needs and expectations (Lee & Coughlin, 2015).

Results

To measure the efficacy of immersive technology tools in education and the relevant learning outcomes, there are several factors those are investigated through this post-experiment questionnaire. The results of





Fig. 7. Technology Acceptance Model (TAM) used in the part of questionnaire.




Fig. 8. Learning with LEARN module using self-directed learning: Evaluations with young researchers at ACM International Conference on Interactive Media Ex- periences 2022.


Fig. 9. Results of NASA TLX on 5-Likert scale.



Table 1
Sub-dimensions of workload NASA Task Load Index.
the questionnaires are presented in Table 2, Table 3 and analysis done using R studio.

Sub-dimensions of workload
Explanation

Evaluation results

Mental Demand	Mentally demand needed for task completion. Physical Demand	Physical demand for task completion.
Temporal Demand	Time demand and pace of the task. Performance	Success in accomplishing the task.
Effort	How hard to accomplish your level of performance?
Frustration	How irritated, stressed, and annoyed felt to complete the task?

The evaluation results hold importance in research as critical nexus between hypothesis and conclusion, which means bridging the gap be- tween theory and empirical evidence based on the experiments (Fig. 10 shows the partipants during experiments).
The result of the NASA TLX in Table 2 and Fig. 9 shows the level of mental and physical workload required to complete the tasks in the



Table 2
Analysis based on data of NASA Task Load Index questionnaire’s responses with average, median, minimum, and maximum.
interaction with an average of 6.40 for effectiveness.
The use of machine learning agents is the most crucial component of this study as RQ 2 is entirely focusing on the success of the component.

Questions- NASA Task Load Index (TLX) 5-point Likert scale
How much mental and perceptual activity was required? (Low - High)
How much physical activity was required for performing hands-on tasks?
How much time pressure did you feel when performing tasks?
How successful were you in performing the task?
How hard did you have to work (mentally and physically) to accomplish your level of performance?
How irritated, stressed, and annoyed versus content, relaxed, and complacent did you feel during the task?



Table 3
Average	Median	Min	Max


1.8	2	1	4

1.6	2	1	3

1.93	2	1	3

3.93	4	3	5

1.86	2	1	3


1.73	2	1	3
With an average of score of 5.33, participants think machine learning can help in self-directed learning in immersive environments. This result has endorsed the previous studies where machine learning and artificial agents are proposed for immersive learning (Dyulicheva & Glazieva, 2022; Elor & Kurniawan, 2020).
This shows the potential of adding the power of intelligent agents in immersive learning environments. By leveraging the power of these agents, immersive learning can become more engaging, efficient, and productive. The visual presentation of these results is in Fig. 11. The average score of all other usability and ease of questions, including the nature of the experience, was high, showing a higher engagement and satisfaction, leading to a higher recommendation score of 6.33. Visual stimuli boost cognitive processing, visual information process faster in brain and more effective as compared to text-based information (Kuhail et al., 2022) (see Fig. 12).
Perceived usefulness and ease of use are key predictors to find if the users will adopt and use this learning technology with the proposed

Data of Perceived Usefulness and Ease of Use questionnaire responses with average, median, minimum, and maximum score.
approach. With higher scores, expert reviewers perceived that this
technology could be useful and easy to use, and it can be beneficial with

Questions - Perceived Usefulness and Ease of Use)
7-point Likert scale
Average	Median	Min	Max
the real end users. Due to immersive, more natural and intuitive inter- action methods in the virtual environment, this approach has shown higher user experience and increased engagement which is a major requirement for such solutions (M¨akinen et al., 2022). The higher pleasant score of the learning experiment proved as significant impact in the learners’ overall motivation and engagement.
The combination of TAM and NASA TLX results allows researchers to
gauge learners’ inclination to adopt immersive learning and holistically
evaluate the cognitive implications of using these technologies. These findings help to understand the interplay between user acceptance and mental demands, ultimately guiding the design process and imple- menting immersive learning environments that are user-friendly and effective.

Subjective questionnaire results










application is 1.8 and 1.6 respectively in average on a scale of 5. It shows interaction with the application and performing experiments in the virtual environment was not causing much workload for the partici- pants. If participants experiencing lower levels of time pressure when performing a task, they perform better to allocate cognitive resources, engage in problem-solving, and apply decisions effectively.
The time pressure of performing tasks and work needed to accom- plish performance is 1.93 and 1.86, respectively, which shows a minimal level on both of these parameters. As discussed by Jeffri and Rambli (2021), there is a positive correlation when mapping task performance against mental workload.
In getting success to perform the tasks successfully, the average score is 3.93, reasonably higher, which shows participants were very suc- cessful in performing the required tasks with hand interaction. Finally, the level of irritation, stress, and annoyance compared to the quality of content, level of relaxation, and self-satisfaction was low with an average score of 1.73 which shows participants were more relaxed and satisfied as compared to irritation.
Results based on TAM for Perceived Usefulness and Ease of Use questionnaire in Table 3 and Fig. 11 show use of hands-on “kinesthetic” learning in immersive environment increased the learning performance
with an average score of 6.07 on a scale of 7 and use of real-time hand
Along with the NASA TLX and TAM questionnaire, expert reviewers were asked to provide subjective feedback about usability and further recommendations. About hand interaction to create chemical reactions, participants responded “Hand interaction makes it more interactive as compared to other interaction approaches in XR", “easy to use and friendly" and “interactive, accessible and very responsive". This shows the realistic level of hand interaction technology in immersive environment. The results of NASA TLX and TAM proved that hand interactions in VR can potentially reduce cognitive load as compared to using traditional
controllers. As hands are the primary way of interaction with the real world, so hand interaction ability in VR enhances the feeling of realism. The use of hand interaction can help to manipulate virtual objects in VR environment with higher precision and accuracy which plays a crucial role in STEM related XR applications like training or VR laboratories. This way, we can reduce the workload for complex UI of button presses or controllers, which makes learning friendlier. A participant expressed his views about the adopted approach, “I like the adopted approach of kinesthetic learning". This is very similar to Hernandez, Jessa et al. (2020) study, where kinesthetic learners gained broader visuospatial under- standing in learning anatomy. This shows the potential of this proposed approach of intelligent agents and real-time interaction for future research.
About using machine learning agents for self-directed learning, the responses of the expert reviewers were; “it can help in personalization, more scalable and deliver accurate learning contents". According to Tapa- lova and Zhiyenbayeva (2022), personalized learning can help for training in virtual contexts, adaptation of learning content to personal needs, real-time feedback, improving the learning process and mental




Fig. 10. Hands-on learning with interaction hand: Evaluations with young researchers at ACM International Conference on Interactive Media Experiences 2022.


Fig. 11. Ratings for Perceived Usefulness and Ease of Use on 7-point Likert scale.


stimulations. Other participants said, “machine learning and intelligent agents can play a very big role in learning systems for training users", “I like the self-directed learning approach before going to hands-on learning" which is acceptance of the workflow designed in this methodology and “Yes, it can be improved more. It helps definitely before going to actual interaction-based experience". These responses show encouragement from expert reviewers for exploration on intelligent agents for immersive
learning. The intelligent agents in immersive technology are those fu- turistic components of XR research which will continue to advance and opening up new opportunities (Guan et al., 2023; Loureiro, 2023). Fig. 12 shows the main keywords found in the subjective responses of the expert reviewers.
Answering the question of the most interesting thing about experiment, expert reviewers responded “I liked the interaction and the immediate



Fig. 12. Word cloud for “Most interesting thing" from expert reviewers.


reaction" and “The visual effects that make for an immersive experience". Bowma & McMahan (2007) suggest that higher immersive quality in system can help to bring more psychological presence. “To see visual reactions like fire really enhances the experience" and “Simulated environ- ment is pleasant with attractive textures", inclusion of these above features certainly have a cost in terms of application speed. This shows that creating visual effects in chemicals to make them realistic and real-time hand interaction provides a pleasant experience in the immersive environment. An expert user appreciated the learning cycle, “learning flow of the app is interesting; taking the user from self-guided learning approach to hands-on learning and then integrated assessment". This can
help a learner to learn in a proper way where training occurs before practice and testing. Sharing experience about the use of ML-agents, an expert reviewer said “use of agent guided learning can reduce cognitive overload and such suitable learning concepts can improve the learning gain". One expert reviewer appreciated the use of the kinesthetic learning approach, “use of kinesthetic learning encourages users to engage with 3D material and increase the effectiveness", and another one acknowledged the approach, this approach combines great thoughts of hands-on learning, agent-led, and interactive learning. The best thing about real-time hand interaction is using immersive technology practice scenarios in a kinesthetic learning style which proves the hypothesis of this paper.
In expert reviewers’ recommendations, “increasing the overall visual quality (including special fusion effects) would improve the experience in general", “the organization of the cubes should be optimized. Even though the
Model explains in which "order" you must combine the elements, to give the user freedom of exploration regarding similar components, would be much appreciated and “adding more relevant case studies to engage user within the app". These recommendations encourage the advancement in the pro- totype for more engagement of users and scalability if the approach. One expert reviewer recommended using user collaboration in an immersive environment, “this approach can be enhanced to collaborative approach, adding multiple users in the learning environment", this is a great idea to develop a co-creation concept within this approach. de et al. (2020) has explored the collaborative learning aspects in immersive environments in great detail for CAVE, but it is definitely increasing opportunities in the HMDs based XR like Drey et al. (2022). Another similar response was about collaboration, “try to develop user collaboration for interaction to create co-learning activities". Collaboration in learning environment pro- motes social interaction where learners can exchange ideas and provide peer support (Jovanovi´c and Milosavljevi´c, 2022). When collaborating
with peers, students feel a sense of connection leading to increased participation and reduced feelings of isolation (Hu et al., 2022).
Moving towards the realism approach, a response was “You can use this approach with multi-sensory experiences which will allow the user
to play in the immersive environment with realism", this can be achieved by combining VR tools with wearable haptic devices like Sanfilippo et al. (2022) which is definitely the future of the XR and indeed it will add much higher value in the current system.

Design implications

The expert reviewers’ feedback about the ML-agent for increasing
the learning efficiency was positive as discussed in the section 6.1, which supported the hypothesis behind implementing the machine learning agents. Addressing the RQ1, this evaluation suggests that integration of kinesthetic or hands-on learning using real-time hand interaction technology can actively engage learners in technical topics by involving physical movement, manipulation of objects, and practical experimentation. When a learner actively involves in the learning pro- cess, it helps better understanding of abstract concepts by making them tangible and relatable. Furthermore, the realism approach by providing real-time hand interaction with learning material got positive reviews from reviewers. Recently, the development of cloud anchors (Antunes et al., 2019), spatial anchors (Delmerico, et al., 2022), and the increasing capability of recent Meta HMD Quest Pro has created many opportunities for collaboration in immersive learning experiences which will enhance the importance of kinesthetic learning in virtual collabo- ration (Jovanovi´c and Milosavljevi´c, 2022).
Addressing the RQ2, the integration of AI or machine learning agents in immersive learning has the potential to create personalized, interac- tive, and engaging learning environments. But it is a fact that there are lots of ethical issues (Mystakidis et al., 2021), data privacy concerns (Zallio & Clarkson, 2022), and need to balance automation and human interaction for a beneficial learning experience. The Unity ML-Agents Toolkit has developed over time and offers more stability with every new release. These new releases come up with additional features, and slowly it is becoming possible to implement complex learning systems within the Unity platform. Unity ML-agents can be used to implement gamification concepts in the learning experiences by creating game-based learning environments, which can make the learning pro- cess more engaging and interactive. According to the goal of RQ2, AI or



machine learning agents can work as virtual mentors or tutors (Patar- anutaporn et al., 2021) in immersive learning environments to empower independent learning. These agents can be developed to guide learners in learning or performing numerous tasks, simulations, or scenarios and providing real-time support. This approach can enhance the learning process by demonstrating proper techniques, correcting mistakes, and providing expert advice.

Limitations

Though the usability tests show results and participants’ acknowl- edgment of this approach with positive reviews and recommendations,
this study still has its limitations. The primary limitation of this research is the lack of extensive end-user experiments, which have a fundamental role in assessing the true effectiveness and practical implications of the proposal solution. As we have evaluated with expert reviewers, we still need to conduct assessments with secondary school students who are real end-users to explore more advanced usability factors. There is also a need to conduct control group experiments to draw a comparison with traditional learning. As this study will go through control group exper- iments, there will be different design, ethics and privacy related issues to focus in this proposed solution.

Conclusion & future work

In this tech-savvy generation, immersive learning has professed ad- vantages that encourage embracing technologies like XR as pedagogical tools in STEM education. Although the XR, as a learning technology has shown very positive and encouraging results (Tang et al., 2020; Yang & Goh, 2022), it can be challenging to see how these results could be scaled and repeated for different use cases. This study offers insight into how this could be possible by focusing on controller-free hand interaction for kinesthetic learning and using machine learning as self-guided learning using agents.
The results of the NASA Task Load Index and TAM for Perceived Usefulness and Ease of Use show alignment with the proposed approach. These results also suggest that XR has a strong ability to work as a bridge between hands-on learning practices and learning technical topics in the resource-constrained environments. In immersive learning environ- ments, the agency can play a supportive role in simulated experiences and can help in progress toward the metaverse (Iqbal & Campbell, 2022). As technology continues to evolve with time, we can expect more innovative and advanced uses of intelligent agents as learning technol- ogy in the near future. Knowing that the use of XR technologies is becoming a prominent trend for virtual laboratories for STEM subjects, this research will further enhance the capacity of XR with standalone devices, controller-free interaction, and a self-directed learning approach for personalized learning spaces. The findings from this eval- uation indicate that such immersive learning approaches can be incor- porated to increase the learning gain, equal access to learning resources, and add more value to existing XR approaches for user engagement. The approach is complementary to existing constructivist approaches, which summed up by Travers et al. (1993) as “an approach to learning that
holds that people actively construct or make their knowledge and that
reality is determined by the experiences of the learner". Further, with the recommendations and new developments in the multi-sensory haptic technology (Sanfilippo et al., 2022), this approach can further extend to realism with sensing gloves in future work. Finally, this work mirrors current developments in robotics, where NVIDIA uses ML agents trained in physical simulations to perform those actions in real life (Mako- viychuk, et al., 2021). AGILEST approach points to a fascinating and novel pedagogical approach where data can be used to train virtual agents to become our teachers and assessors. This area is not limited to the STEM case study presented in this paper, as it offers a world where virtual agents coexisting in our reality could fundamentally change immersive learning in future.
Ethics approval

Participants consent was taken from participants.

Statements on open data and ethics

The data used in this paper was obtained from publicly available open data sources and collected through experiments. All included studies have been appropriately cited and listed in the References sec- tion. For primary data collection through experiments, participants’
consents were taken. All included studies were assessed for compliance
with relevant ethical guidelines, and only studies that reported ethical approval and informed consent were considered for inclusion.

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgment

The paper’s authors would like to thank all the young researchers for
signing consents and participating in the expert reviews to help evaluate this study. Their experience, feedback, and recommendations will help to improve the system and new interventions in this research area.

Acronyms

XR	Extended Reality
AR	Augmented Reality
MR	Mixed Reality
VR	Virtual Reality
AGILEST AGents to facilitate Interactive kinesthetic LEarning in STEM education using a Touchless interaction
TAM	Technology Acceptance Model
STEM	Science, Technology, Engineering and Mathematics NASA	National Aeronautics and Space Administration TLX	Task Load Index
HMD	Head-mounted Display
IMX	International Conference on Interactive Media Experiences CAVE	Cave Automatic Virtual Environment

References

Aguayo, C., & Eames, C. (2023). Using mixed reality (XR) immersive learning to enhance environmental education. The Journal of Environmental Education, 54(1), 58–71.
Antunes, J. L., Bidarra, J., & Figueiredo, M. (2019). AR with cloud anchors: A way to
improve HCI and interactive art. International Journal of Creative Interfaces and Computer Graphics, 10(2), 29–40.
Appelman, R. (2005). Designing experiential modes: A key focus for immersive learning
environments. TechTrends, 49(3), 64–74.
Araiza-Alba, P., Keane, T., Chen, W. S., & Kaufman, J. (2021). Immersive virtual reality as a tool to learn problem-solving skills. Computers & Education, 164, Article 104121.
Arpaia, P., De Benedetto, E., De Paolis, L., D’Errico, G., Donato, N., & Duraccio, L. (2022). Performance and usability evaluation of an extended reality platform to
monitor patient’s health during surgical procedures. Sensors, 22(10), 3908.
de Back, T. T., Tinga, A. M., Nguyen, P., & Louwerse, M. M. (2020). Benefits of immersive
collaborative learning in CAVE-based virtual reality. International Journal of Educational Technology in Higher Education, 17, 1–18.
Banjar, A., & Campbell, A. G. (2022). Work-in-Progress—design of mixed reality
educational applications using enhanced ARCS+ G model for motivation. In 2022 8th international conference of the immersive learning research network (iLRN) (pp. 1–3). IEEE.
Beck, D. (2019). Augmented and virtual reality in education: Immersive learning research. Journal of Educational Computing Research, 57(7), 1619–1625.
Birt, J., Stromberga, Z., Cowling, M., & Moro, C. (2018). Mobile mixed reality for
experiential learning and simulation in medical and health sciences education.
Information, 9(2), 31.
Bowma & McMahan. (2007). Virtual reality: How much immersion is enough? Computer, 40(7), 36–43.



Bun´, P., Hus´ar, J., & Kaˇsˇcak, J. (2022). Hand tracking in extended reality educational
applications. In International scientific-technical conference MANUFACTURING (pp. 317–325). Cham: Springer International Publishing.
Chheang, V., Saalfeld, P., Huber, T., Huettl, F., Kneist, W., Preim, B., & Hansen, C.
(2019). Collaborative virtual reality for laparoscopic liver surgery training. In 2019 IEEE international conference on artificial intelligence and virtual reality (AIVR) (pp. 1–17). IEEE.
Dawley, L., & Dede, C. (2014). Situated learning in virtual worlds and immersive
simulations. Handbook of Research on Educational Communications and Technology, 723–734.
Delmerico, J., Poranne, R., Bogo, F., Oleynikova, H., Vollenweider, E., Coros, S., …
Pollefeys, M. (2022). Spatial computing and intuitive interaction: Bringing mixed reality and robotics together. IEEE Robotics and Automation Magazine, 29(1), 45–57.
Dengel, A. (2022). What is immersive learning?. In 2022 8th international conference of the
immersive learning research network (iLRN) (pp. 1–5). IEEE.
Dengel, A., & M¨agdefrau, J. (2018). Immersive learning explored: Subjective and
objective factors influencing learning outcomes in immersive educational virtual environments. In 2018 IEEE international conference on teaching, assessment, and learning for engineering (TALE) (pp. 608–615). IEEE.
Drey, T., Albus, P., der Kinderen, S., Milo, M., Segschneider, T., Chanzab, L., …
Rukzio, E. (2022). April). Towards collaborative learning in virtual reality: A comparison of co-located symmetric and asymmetric pair-learning. In Proceedings of
the 2022 CHI conference on human factors in computing systems (pp. 1–19).
Dyulicheva, Y. Y., Gaponov, D. A., Mladenovic, R., & Kosova, Y. A. (2021). The virtual reality simulator development for dental students training: A pilot study. In AREdu
(pp. 56–67).
Dyulicheva, Y. Y., & Glazieva, A. O. (2022). Game based learning with artificial
intelligence and immersive technologies: An overview. CEUR Workshop Proceedings, 3077, 146–159.
Edwards, B. I., Bielawski, K. S., Prada, R., & Cheok, A. D. (2019). Haptic virtual reality
and immersive learning for enhanced organic chemistry instruction. Virtual Reality, 23, 363–373.
Elme, L., Jørgensen, M. L., Dandanell, G., Mottelson, A., & Makransky, G. (2022).
Immersive virtual reality in STEM: Is IVR an effective learning medium and does
adding self-explanation after a lesson improve learning outcomes? Educational Technology Research & Development, 70(5), 1601–1626.
Elor, A., & Kurniawan, S. (2020). Deep reinforcement learning in immersive virtual
reality exergame for agent movement guidance. In 2020 IEEE 8th international conference on serious games and applications for health (SeGAH) (pp. 1–7). IEEE.
Freina, L., & Ott, M. (2015). A literature review on immersive virtual reality in education:
State of the art and perspectives. The international Scientific Conference Elearning and Software for Education, 1(133), 10–1007.
Fujiwara, H., Kano, T., & Akakura, T. (2021). Development of collaborative chemistry
experiment environment using VR. In International conference on human-computer interaction (pp. 14–26). Cham: Springer International Publishing.
Gao, H., Bozkir, E., Hasenbein, L., Hahn, J. U., Go¨llner, R., & Kasneci, E. (2021). Digital
transformations of classrooms in virtual reality. In Proceedings of the 2021 CHI conference on human factors in computing systems (pp. 1–10).
Grani´c, A., & Maranguni´c, N. (2019). Technology acceptance model in educational
context: A systematic literature review. British Journal of Educational Technology, 50
(5), 2572–2593.
Guan, J., Morris, A., & Irizawa, J. (2023). March). Cross-reality for extending the metaverse: Designing hyper-connected immersive environments with XRI. In 2023 IEEE conference on virtual reality and 3D user interfaces abstracts and workshops (VRW)
(pp. 305–311). IEEE.
Hameed, A., Perkis, A., & Mo¨ller, S. (2021). Evaluating hand-tracking interaction for
performing motor-tasks in vr learning environments. In 2021 13th international conference on quality of multimedia experience (QoMEX) (pp. 219–224). IEEE.
Hart, S. G. (2006). NASA-task load index (NASA-TLX); 20 years later. In Proceedings of the
human factors and ergonomics society annual meeting (Vol. 50, pp. 904–908). Sage CA: Los Angeles, CA: Sage publications. No. 9.
Hernandez, J. E., Vasan, N., Huff, S., & Melovitz-Vasan, C. (2020). Learning styles/ preferences among medical students: Kinesthetic learner’s multimodal approach to learning anatomy. Medical Science Educator, 30, 1633–1638.
Hillmann, C., & Hillmann, C. (2019). Comparing the gear vr, oculus go, and oculus quest.
Unreal for Mobile and Standalone VR: Create Professional VR Apps Without Coding, 141–167.
Hu-Au, E., & Okita, S. (2021). Exploring differences in student learning and behavior
between real-life and virtual reality chemistry laboratories. Journal of Science Education and Technology, 30, 862–876.
Hu, G., Mao, M., & Zhao, Y. (2022). Remote care and collaboration for empty nest family:
Smart home, digital twin and mixed reality. In 2022 8th international conference on virtual reality (ICVR) (pp. 126–134). IEEE.
Hurrell, C., & Baker, J. (2021). Immersive learning: Applications of virtual reality for
undergraduate education. College & Undergraduate Libraries, 27(2–4), 197–209.
Iqbal, M. Z., & Campbell, A. G. (2021a). Touchless hand interaction and machine learning agents to support kinesthetic learning in augmented reality. In Proceedings of the 52nd ACM technical symposium on computer science education (p. 1322), 1322.
Iqbal, M. Z., & Campbell, A. G. (2021b). From luxury to necessity: Progress of touchless interaction technology. Technology in Society, 67, Article 101796.
Iqbal, M. Z., & Campbell, A. G. (2022). Metaverse as tech for good: Current progress and emerging opportunities. Available at: SSRN 4150689.
Iqbal, M. Z., Mangina, E., & Campbell, A. G. (2022). Current challenges and future research directions in augmented reality for education. Multimodal Technologies and Interaction, 6(9), 75.
Jacobson, M. J., Miao, C., Kim, B., Shen, Z., & Chavez, M. (2008). Research into learning in an intelligent agent augmented multi-user virtual environment. In 2008 IEEE/ WIC/ACM international conference on web intelligence and intelligent agent technology
(Vol. 3, pp. 348–351). IEEE.
Jantakoon, T., Wannapiroon, P., & Nilsook, P. (2019). Virtual immersive learning environments (VILEs) based on digital storytelling to enhance deeper learning for
undergraduate students. Higher Education Studies, 9(1), 144–150.
Jeffri, N. F. S., & Rambli, D. R. A. (2021). A review of augmented reality systems and their effects on mental workload and task performance. Heliyon, 7(3).
Jovanovi´c, A., & Milosavljevi´c, A. (2022). VoRtex Metaverse platform for gamified
collaborative learning. Electronics, 11(3), 317.
Juliani, A., Berges, V. P., Teng, E., Cohen, A., Harper, J., Elion, C., … Lange, D. (2018).
Unity: A general platform for intelligent agents. arXiv preprint arXiv:1809.02627.
Kang, T., Chae, M., Seo, E., Kim, M., & Kim, J. (2020). DeepHandsVR: Hand interface using deep learning in immersive virtual reality. Electronics, 9(11), 1863.
Kim, Y. J., & Ahn, S. Y. (2021). Factors influencing nursing students’ immersive virtual reality media technology-based learning. Sensors, 21(23), 8088.
Kuhail, M. A., ElSayary, A., Farooq, S., & Alghamdi, A. (2022). Exploring immersive learning experiences: A survey. In Informatics (Vol. 9, p. 75). MDPI. No. 4.
Lai, J., Chen, X. L., & Zhang, X. Z. (2019). Training an agent for third-person shooter
game using unity ML-agents. In International conference on artificial intelligence and computing science (pp. 317–332). Hangzhou.
Learning, S. D. (1999). What IS self-directed learning? (Vol. 9). The Sourcebook for Self-
directed Learning.
Ledo, D., Houben, S., Vermeulen, J., Marquardt, N., Oehlberg, L., & Greenberg, S. (2018).
Evaluation strategies for HCI toolkit research. In Proceedings of the 2018 CHI conference on human factors in computing systems (pp. 1–17).
Lee, C., & Coughlin, J. F. (2015). Perspective: Older adults’ adoption of technology: An
integrated approach to identifying determinants and barriers. Journal of Product Innovation Management, 32(5), 747–759.
Li, H. (2022). Design and implement of soccer player AI training system using unity ML-
agents. In CIBDA 2022; 3rd international conference on computer information and big data applications (pp. 1–4). VDE.
Loureiro, S. M. C. (2023). Artificial intelligence and extended reality in luxury fashion
retail: Analysis and reflection. In Augmented reality and artificial intelligence: The fusion of advanced technologies (pp. 323–348). Cham: Springer Nature Switzerland.
Lowe, D., & Liu, Z. (2017). Potential for utilising head-mounted displays (HMDs) for
augmenting laboratories. In 2017 4th experiment@ international conference (pp. 165–170). IEEE. exp. at’17).
MacDowell, P. (2023). Teachers designing immersive learning experiences for
environmental and sustainability education. In Immersive education: Designing for learning (pp. 171–186). Cham: Springer International Publishing.
M¨akinen, H., Haavisto, E., Havola, S., & Koivisto, J. M. (2022). User experiences of
virtual reality technologies for healthcare in learning: An integrative review.
Behaviour & Information Technology, 41(1), 1–17.
Makoviychuk, V., Wawrzyniak, L., Guo, Y., Lu, M., Storey, K., Macklin, M., … State, G. (2021). Isaac gym: High performance gpu-based physics simulation for robot learning.
arXiv preprint arXiv:2108.10470.
Miller, M. D., Castillo, G., Medoff, N., & Hardy, A. (2021). Immersive VR for organic
chemistry: Impacts on performance and grades for first-generation and continuing- generation university students. Innovative Higher Education, 46, 565–589.
Mystakidis, S., Berki, E., & Valtanen, J. P. (2021). Deep and meaningful e-learning with
social virtual reality environments in higher education: A systematic literature review. Applied Sciences, 11(5), 2412.
Noothigattu, R., Bouneffouf, D., Mattei, N., Chandra, R., Madan, P., Varshney, K. R., …
Rossi, F. (2019). Teaching AI agents ethical values using reinforcement learning and policy orchestration. IBM Journal of Research and Development, 63(4/5), 2, 1.
Pan, Z., Luo, T., Zhang, M., Cai, N., Li, Y., Miao, J., … Lu, J. (2022). MagicChem: A MR system based on needs theory for chemical experiments. Virtual Reality, 26(1),
279–294.
Papakostas, C., Troussas, C., Krouska, A., & Sgouropoulou, C. (2021). Measuring user experience, usability and interactivity of a personalized mobile augmented reality training system. Sensors, 21(11), 3888.
Papanastasiou, G., Drigas, A., Skianis, C., Lytras, M., & Papanastasiou, E. (2019). Virtual and augmented reality effects on K-12, higher and tertiary education students’ twenty-first century skills. Virtual Reality, 23, 425–436.
Pataranutaporn, P., Danry, V., Leong, J., Punpongsanon, P., Novy, D., Maes, P., & Sra, M.
(2021). AI-generated characters for supporting personalized learning and well-being.
Nature Machine Intelligence, 3(12), 1013–1022.
Qin, T., Cook, M., & Courtney, M. (2020). Exploring chemistry with wireless, PC-less
portable virtual reality laboratories. Journal of Chemical Education, 98(2), 521–529. Ramírez, J.A´., & Bueno, A. M. V. (2020). Learning organic chemistry with virtual reality.
In 2020 IEEE international conference on engineering veracruz (ICEV) (pp. 1–4). IEEE. Sandars, J., & Walsh, K. (2016). Self-directed learning. Education for Primary Care, 27(2),
151–152.
Sanfilippo, F., Blazauskas, T., Salvietti, G., Ramos, I., Vert, S., Radianti, J., … Oliveira, D.
(2022). A perspective review on integrating VR/AR with haptics into STEM education for multi-sensory learning. Robotics, 11(2), 41.
Sharma, S., Devreaux, P., Sree, S., Scribner, D., Grynovicki, J., & Grazaitis, P. (2019). Artificial intelligence agents for crowd simulation in an immersive environment for emergency response. Electronic Imaging, 2019(2), 176, 1.
Skult, N., & Smed, J. (2020). Interactive storytelling in extended reality: Concepts for the design. Game user experience and player-centered design, 449–467.
Stanney, K. M., Archer, J., Skinner, A., Horner, C., Hughes, C., Brawand, N. P., …
Perez, R. S. (2022). Performance gains from adaptive eXtended Reality training



fueled by artificial intelligence. The Journal of Defense Modeling and Simulation, 19(2), 195–218.
Subramanian, G. H. (1994). A replication of perceived usefulness and perceived ease of
use measurement. Decision Sciences, 25(5-6), 863–874.
Sutton, R. S., & Barto, A. G. (2018). Reinforcement learning: An introduction. MIT press.
Tang, Y. M., Au, K. M., Lau, H. C., Ho, G. T., & Wu, C. H. (2020). Evaluating the
effectiveness of learning design with mixed reality (MR) in higher education. Virtual Reality, 24(4), 797–807.
Tao, Y., Coltey, E., Wang, T., Alonso, M., Shyu, M. L., Chen, S. C., … Vassigh, S. (2020).
Confidence estimation using machine learning in immersive learning environments. In 2020 IEEE conference on multimedia information processing and retrieval (MIPR) (pp.
247–252). IEEE.
Tapalova, O., & Zhiyenbayeva, N. (2022). Artificial intelligence in education: AIEd for personalised learning pathways. Electronic Journal of E-Learning, 20(5), 639–653.
Travers, J. F., Elliott, S. N., & Kratochwill, T. R. (1993). Educational psychology: Effective
teaching, effective learning. Brown & Benchmark/Wm. C. Brown Publ.
Ummihusna, A., & Zairul, M. (2022). Investigating immersive learning technology intervention in architecture education: A systematic literature review. Journal of Applied Research in Higher Education, 14(1), 264–281.
Vlahovic, S., Suznjevic, M., & Skorin-Kapov, L. (2022). A survey of challenges and methods for Quality of Experience assessment of interactive VR applications. Journal on Multimodal User Interfaces, 16(3), 257–291.
Wang, Y., Lee, L. H., Braud, T., & Hui, P. (2022). Re-Shaping post-COVID-19 teaching and learning: A blueprint of virtual-physical blended classrooms in the metaverse era. In 2022 IEEE 42nd international conference on distributed computing systems
workshops (ICDCSW) (pp. 241–247). IEEE.
Wang, T., Qian, X., He, F., Hu, X., Cao, Y., & Ramani, K. (2021). Gesturar: An authoring system for creating freehand interactive augmented reality applications. In The 34th
annual ACM symposium on user interface software and technology (pp. 552–567).
Webster, R. (2016). Declarative knowledge acquisition in immersive virtual learning environments. Interactive Learning Environments, 24(6), 1319–1333.
Yang, F., & Goh, Y. M. (2022). VR and MR technology for safety management education:
An authentic learning approach. Safety Science, 148, Article 105645.
Youssef, A. E., El Missiry, S., El-gaafary, I. N., ElMosalami, J. S., Awad, K. M., & Yasser, K. (2019). Building your kingdom imitation learning for a custom gameplay using unity ml-agents. In 2019 IEEE 10th annual information technology, electronics and mobile communication conference (IEMCON). IEEE, 0509-0514.
Zallio, M., & Clarkson, P. J. (2022). Designing the metaverse: A study on inclusion, diversity, equity, accessibility and safety for digital immersive environments. Telematics and Informatics, 75, Article 101909.
Zorzal, E. R., Sousa, M., Mendes, D., dos Anjos, R. K., Medeiros, D., Paulo, S. F., …
Lopes, D. S. (2019). Anatomy studio: A tool for virtual dissection through augmented 3D reconstruction. Computers & Graphics, 85, 74–84.
