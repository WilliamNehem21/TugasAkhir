Egyptian Informatics Journal 24 (2023) 277–290











Energy-aware intelligent scheduling for deadline-constrained workflows in sustainable cloud computing
Min Cao a, Yaoyu Li b,⇑, Xupeng Wen c, Yue Zhao b,⇑, Jianghan Zhu b
a Intelligent Manufacturing College, Zhanjiang University of Science and Technology, Zhanjiang 524094, PR China
b Science and Technology on Information Systems Engineering Laboratory, National University of Defense Technology, Changsha 410073, PR China
c School of Traffic and Transportation Engineering, Central South University, Changsha 410075, PR China



a r t i c l e  i n f o 


Article history:
Received 21 December 2022
Revised 19 March 2023
Accepted 3 April 2023
Available online 18 April 2023


Keywords:
Sustainable cloud computing Intelligent scheduling Energy-efficiency
Workflow
Dynamic voltage/frequency scaling
a b s t r a c t 

It is challenging to handle the non-linear power consumption model, complex workflow structures, and diverse user-defined deadlines for energy-efficient workflow scheduling in sustainable cloud computing. Although metaheuristics are very attractive to solve this problem, most of the existing work regards the problem as a black-box and ignores the use of domain knowledge. To make up for their shortcomings, this paper tailors an energy-aware intelligent scheduling algorithm (EIS) with three new mechanisms. First, we derive the optimal execution time that minimizes energy consumption for each task on a given resource. Second, based on the optimal execution time of each workflow task, the EIS distributes the workflow slack time (difference between its completion time and deadline) to reduce the voltages and frequencies of task executions for energy saving. Third, the EIS mines the idle time gaps caused by task precedence constraints to further reduce dynamic energy consumption whilst satisfying workflows’ deadline constraints. To measure the performance of the EIS, we conduct extensive comparison experi- ments based on actual workflow applications. The results demonstrate that the energy consumption of the EIS is much lower than that of the competitors under different deadlines, and has a faster descend rate with the evolution process.
© 2023 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intel-
ligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creative-
commons.org/licenses/by-nc-nd/4.0/).





Introduction

Context and issues

Cloud computing is an innovative development of distributed and utility computing [1,2]. It provides on-demand access to unlimited virtual resources such as network, storage, and comput- ing on a pay-as-you-go basis. This flexible and scalable resource delivery paradigm is attractive to a wide range of individuals and various organizations [3–5]. According to Gartner’s report, more than 60 percent of organizations resort to cloud services for

* Corresponding authors.
E-mail addresses: Garett_1984@hotmail.com (Y. Li), zhaoyue08a@nudt.edu.cn (Y. Zhao).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.

required resources, and cloud computing alone accounts for nearly 15 percent of global IT expenditure [6].
To match the prosperously growing demand for cloud services, mainstream cloud service providers, e.g., Google Cloud, Amazon EC2, and Alibaba Cloud, have built a large number of super-scale datacentres around the world. A cloud datacentre often houses thousands of heterogeneous servers, network devices sensors, cooling systems, and many other facilities [7]. It is a typical high power density infrastructure, consuming enormous amounts of electrical energy [8–10]. Statistics illustrate that the energy con- sumption of a medium-sized datacentre reaches the energy con- sumption of 25,000 households [11]. Such high energy consumption of cloud datacentres subsequently increases opera- tional costs, and leaves substantial carbon footprints that nega- tively pound the environmental sustainability [12]. Amazon’s evaluation of its datacentres reveals that 42% of the operation and maintenance costs are caused by energy consumption [13]. In cloud datacentres, the energy consumed by computing and cool- ing systems together accounts for 85% of total energy consumption [8]. Meanwhile, the energy consumption of the cooling system mainly stem from dissipating the heat emitted by the computing


https://doi.org/10.1016/j.eij.2023.04.002
1110-8665/© 2023 THE AUTHORS. Published by Elsevier BV on behalf of Faculty of Computers and Artificial Intelligence, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



system. Thus, developing application oriented energy-aware opti- mization technologies is of upmost importance to cloud datacen- tres toward reducing operational cost and maintaining service sustainability [14,12,15].
Over the past decade, the widespread implementation of Inter- net of Things has resulted in the rapid growth of data volume and data generation speed [8,16]. To process the huge amounts of data automatically and timely, workfow has become a powerful tool for cloud platforms [17]. A workfow refers to a set of data processing tasks together with dependencies, which fulfil data communica- tion for various scientifc and business applications. It is notewor- thy that the workflow scheduling component is a middleware layer of a cloud platform, and it directly determines its energy con- sumption, other performance metrics, and service experience of users. In general, workflow application oriented energy-aware scheduling in clouds involves mapping tasks to resources, arrang- ing tasks’ execution order, and assigning appropriate task execu- tion time, such optimizing energy consumption and satisfying complex constraints. It is a NP-complete problem, and has been researched extensively in the literature [18,19]. Most of the exist- ing literature on energy-aware workflow scheduling in clouds can be roughly divided into two categories: heuristics and metaheuristics.

Literature and motivations

Heuristic rule-based workflow scheduling approaches are typi- cally classified as: cluster-based, replication-based, and list-based scheduling heuristics. Cluster-based heuristics [20,21] are to group workflow tasks into multiple clusters according to certain criteria, and then map all tasks of the same cluster to the same resource. The replication-based algorithms [22,23] replicate the one task on multiple resources, hence decreasing data transmission over- head among workflow tasks and start time of tasks. Most heuristic approaches are based on the list strategy. They arrange all the workflow tasks based on certain priorities and then schedule tasks one by one onto suitable resources. For instance, Lee et al. [24] sug- gested two energy-conscious workflow scheduling methods using dynamic voltage scaling to balance makespan and energy con- sumption. Li et al. [25] designed a resource selection, task merging, and resouce reuse mechanism to reduce execution cost and energy consumption while meeting workflows’ deadlines. Safari et al. [26] integrated the dynamic voltage/frequency scaling technique into the list-based scheduling algorithm to minimize energy consump- tion while considering workflow deadlines. Qureshi et al. [27] sug- gested a profile-based energy-efficient approach to balance power usage, CPU utilization, and monetary cost. Rani et al. [28] proposed a power and temperature-aware scheduling algorithm to minimize the computing and cooling energy of executing workflow tasks along with meeting deadlines. Fan et al. [29] designed a hybrid workflow scheduling approach to optimize energy consumption and resource utilization while meeting the deadline and data- dependency constraints of workflows. As heuristics are often tai- lored for specific scenarios, their generalization capability is insuf- ficient, especially for dealing with complex structured workflows and nonlinear power consumption model.
Metaheuristics depending on stochastic search techniques are attractive for handling complex optimization problems. In recent years, many metaheuristic algorithms [30–32] have been designed to optimize the energy efficiency of workflow execution in cloud platforms. For instance, Gill et al. [12] suggested a cuckoo optimization-based cloud resource management approach to holistically cut down energy consumption and carbon footprints for cloud data centers, while satisfying service reliability. Tarafdar et al. [33] combined a heuristic search and positive feedback mech- anism into the ant colony optimization to improve energy-
efficiency and task schedulability. Malik et al. [34] employed parti- cle swarm optimization to iteratively optimize energy efficiency and resource utilization for virtualized data centers. Li et al. [35] improved the nondominated-sorting Owl optimization algorithm with a chaotic local search to balance the energy consumption, makespan, and cost, while meeting the pre-specified deadline and budget constraints. Qi et al. [36] formalized virtual machine scheduling in cloud platforms as a multi-objective optimization problem, and improved NSGA-II to balance consumption, down- time, and resource utilization. Hussain et al. [37] designed an energy-aware approach including new task sequencing, variable neighborhood search, and resource searching mechanism to sched- ule deadline-constrained workflows. Domanal et al. [38] hybri- dised ant colony optimization and particle swarm optimization for task scheduling and resource management. These metaheuris- tics follow a completely black-box approach and fail to make effec- tive use of domain knowledge, such as workflow structure, power consumption model, and dynamic voltage/frequency scaling tech- nique. Modern cloud processors using a multi-core architecture commonly integrate voltage regulators for each core [39], which enables per-core dynamic voltage/frequency scaling to dynami- cally adjust the supply voltages and operation frequencies by tak- ing actual workloads into account.
Meanwhile, there exists substantial work on intelligent algo- rithms to optimize the energy consumption of executing a set of dependent tasks in other fields [40–42]. For instance, Wang et al.
[43] developed a cooperative memetic algorithm to simultane- ously optimize energy consumption and delay. This algorithm includes two heuristics to initialize the population, a feedback- based cooperative search mechanism, multiple problem-specific evolutionary operators, and multiple environmental selection strategies. To solve distributed assembly flow-shop scheduling problem, Zhao et al. [44] improve the water wave optimization algorithm with a variable neighborhood search and a reinforce- ment learning mechanism. Zhao et al. [45] proposed a hyperheuris- tic with Q-learning to solve the energy-efficient distributed blocking flow shop scheduling problem. Wang et al. [46] suggested a hybrid adaptive differential evolution algorithm to optimize the completion time, delay time, and energy consumption for job- shop scheduling problems. Pan et al. [47] designed a knowledge- based two-population optimization algorithm to reduce energy consumption and tardiness for distributed energy-efficient scheduling problems. These works provide great inspiration for this study. However, the optimization problems in these fields are quite different from the energy-efficient scheduling of cloud computing workflows, such as the dynamic voltage/frequency scal- ing technique of computing resources and the on-demand supply of cloud resources. Thus, these intelligent algorithms cannot be directly applied.

Main contributions

The deficiencies of the existing works drive us to integrate the domain knowledge of cloud workflow execution into the evolu- tionary optimization framework to improve energy efficiency. Our core contributions are below.

We derive the optimal execution time that minimizes energy consumption for each workflow task by considering the nonlin- ear power consumption model.
Based on the optimal execution time of each task, we propose a mechanism to distribute workflow slack time (difference between its completion time and deadline) among tasks with complex structures for energy conservation by adjusting the voltage and frequency.



The idle time gaps caused by task precedence constraints are excavated to further reduce dynamic energy consumption whilst satisfying workflows’ deadline constraints.
We conduct comparison experiments to demonstrate the supe- rior energy efficiency of the proposed approach.

Paper organization

The rest of this paper is as follows: Section 2 introduces the models for power consumption and workflows, and then formu- lates the workflow scheduling problem as a single-objective con- strained problem. Section 3 develops the energy-aware intelligent algorithm, followed by its numerical validations in Sec- tion 4. Section 5 concludes this paper and points out two research directions.

Preliminaries and modeling

This section formulates the constrained single-objective work- flow scheduling problem by defining the power consumption model, workflow model; optimization objective; as well as prece- dence and deadline constraints.

Power consumption model

A  cloud  datacentre  houses  a  set  of  virtual machines R = {r1; r2; ··· ; rm}, providing computing resources to satisfy the user-defined quality of services. A virtual machine rk can be described as quadruple {v´k; v^k; ´f ; ^f }, where v´k and v^k represent
its lower and upper supply voltage; ´f k and ^f k represent its lower and upper operation frequency.
The power consumption of rk can be roughly divided into static
part ps and dynamic part pd [24,48]. Static power consumption
Workflow model

A workflow application often contains a set of tasks with data dependencies and is commonly abstracted to a directed acyclic graph.  Formally,  a  workflow  is  depicted  by  a  3-tuple G = {T; E; D}, where T = {t1; t2; ·· · ; tn} denotes a set of tasks,
E C T × T  represents  a  set  of  directed  edges  among  data-
dependent tasks, and D indicates the total deadline of this workflow.
An edge ei;j ∈ E represents the data dependency from ti to tj, in
which ti is the predecessor task of tj and tj is the successor task of ti. The weight of a directed edge w(ei;j) denotes the size of files trans- ferred from ti to tj. The signs Pi and Si represent the sets of ti’s immediate predecessor tasks and successor tasks, respectively. Due to data dependency, a task cannot start runing until all its pre-
decessor tasks have completed and all the input files have arrived.
To visualize the workflow model, Fig. 1 provides an example of a workflow. For this workflow, it consists of six data-dependent tasks T = {t1; t2; ··· ; t6}, and the data dependencies among tasks are	defined	by	the	directed	edges	as E = {e1;3; e1;5; e2;3; e3;4; e4;5; e4;6}. For task t5, the set of its immediate
predecessor tasks is P5 = {t1; t4}. The set of the immediate succes-
sor tasks for task t1 is S1 = {t3; t5}.

Problem formulation

This work aims to optimize energy consumption while meeting the user-specified deadline constraints for workflows. Based on the models of power consumption and workflow application, this sub- section formulates the constrained single-objective optimization problem.
In cloud datacentres embracing dynamic voltage/frequency scaling technology, the runtime of task ti on virtual machine rk

k	k	depends on the computation length of ti and the frequency of rk,

refers to the energy consumption per unit time when the virtual machine is completely idle. It is an inherent attribute of the virtual machine and has nothing to do with its working frequency. Dynamic power consumption is caused by the virtual machine exe- cuting the applications, and can be adjusted via dynamic voltage/ frequency scaling technology. Then, the total power consumption of a virtual machine at time t refers to the summation of static and dynamic powers:
pk (t)= ps + pd(t).	(1)
i.e.,
si k =  li  ;	(6)
k
where li and f k denote the computation length of ti and frequency of
rk, respectively.
When two immediate data-dependent tasks are executed by the same virtual machine, the data transmission time between
them is negligible, whilst they are executed by different virtual

k	k
machines, the data transmission time can be estimated based on

The dynamic power of a virtual machine is directly proportional to its supply voltage squared and frequency [24], and can be described as
pd(t)= ak · vk (t)2 · f k (t);	(2)
where ak denotes the coefficient of proportionality; vk (t) and f k (t)
denote the supply voltage and frequency at time t, respectively.
As the frequency is proportional to the supply voltage, the (2) can be rewritten as:
pd(t)= ak · f k (t)3;	(3)
the data size w(ei;j) and bandwidth b. Assume rg and rk denote the virtual machines used to execute two immediate data- dependent tasks tp and ti. The data transmission time fp;i from tp to ti can be described as:

Assume p^k is the maximum dynamic power consumption of rk, its proportionality coefficient ak can be approximated as
p^k
ak = ^3 ;	(4)
k
Based on (1), (3), and (4), the total power consumption of rk at
time t can be rewritten as below:
p (t)= ps + p^k · f (t)3.	(5)

k	k	^3	k k

Fig. 1. Example of a DAG.





fp,i
0,	if rg = rk,
=	w(ei,j ) ,  otherwise.

(7)
diverse user-defined deadlines. To handle this highly challenging problem, this section designs an intelligent algorithm to reduce the energy consumption of workflow execution. The proposal first

The start time s´i,k and finish time s^i,k of task ti on virtual machine rk can be calculated as:
s´i,k = max{ak, max{s^p,* + fp,i}},	(8)
tp∈Pi

s^i,k = s´i,k + si,k ,	(9)
where ak indicates the available time of virtual machine rk to exe- cute ti, Pi represents the set of ti’s immediate predecessor tasks. If rk has not been used, ak is its set-up time; otherwise, ak is the finish time of the last task executed on rk.
Then, the completion time of a workflow, referring to the max- imum finish time among all its tasks, can be described as:
s_ = maxs^i,*.	(10)
ti ∈T

Based on (8) and (9), the start time st(r ) and finish time ft(r ) of
iteratively evolves the mapping from workflow tasks to cloud resources. Then, it excavates the workflow slack time (difference between its completion time and deadline) to adjust the execution time/frequency of workflow tasks for overall energy conservation. The proposal also excavates the idle time gaps caused by task precedence constraints to further reduce dynamic energy consumption.
Fig. 2 provides an example to visualize the above ideas. Fig. 2a shows the Gantt chart of the scheduling result for three tasks, i.e., t1, t2, and t3. Since task t3 needs to wait for the output files from task t2, an idle time gap between t3 and t1 is left on virtual machine r1. As shown in (3), the dynamic power consumption is propor-
tional to the third power of the frequency. Intuitively, reducing the execution frequency of task t1 will definitely reduce the dynamic energy consumption, as shown in Fig. 2b. As shown in Fig. 2, the completion time of this workflow is 14 s. Assuming that

k
virtual machine rk can be obtained as below:
st(rk)= mins´i,k,
ti ∈T
ft(rk)= mins^i,k.
ti∈T
k




(11)
the deadline of this workflow is 20 s, its slack time between dead- line and completion time is 20 — 14 = 6 seconds, which will be dis- tributed to each workflow task to reduce execution frequency, such reducing overall energy consumption.

Hence the energy consumption of rk to execute workflow tasks is computed as:

Solution representation

ec	Z ft(rk ) p

t dt
The workflow scheduling in clouds involves arranging tasks’

k =
st(rk )
Z ft(rk )



k ( )

s
p^k	3



(12)
execution order, mapping tasks to resources and optimizing task execution time. To simplify the evolution process, we attempt to evolve the mappings from tasks to resources and the execution


Then, the total energy consumption for executing a workflow is
their downward rank [49], which is recursively defined as follows:

EC = X

eck

rank
( maxr
k ∈Rsi,k,	if Pi = £,


k=1
(ti)= 
maxtp ∈P {rank(tp)+ maxsi,k + w(ep,i)/b},  otherwise,

X Z ft(rk ) s

 

p^k	3



(13)
i	rk ∈R




Considering the high energy consumption in cloud datacentres, this paper attempts to allocate the workflow tasks to a suitable set of virtual machines and adjust the execution time of each task, so as to reduce the energy consumption before the user-specified overall deadline. The considered optimization problem can be summarized mathematically as follows:
8> Minimize	EC,
where Pi denotes the set of immediate predecessor tasks of task ti, R denotes the candidate resource pool, si,k is the execution time of ti on resource rk, w(ep,i)/b denotes the data transmission time from tp
to ti. The downward ranks are calculated recursively by traversing the DAG downward starting from the tasks without predecessors.
Basically, the downward rank of a task stands for the longest distance from it to the tasks without predecessors. According to
the definition in (15), the rank of a task is greater than that of all

s.t.
><
s_ 6 D,
s´i,k P max{s^p,* + fp,i}, 6ti ∈ T,
tp∈Pi

(14)
its predecessors. When tasks are sorted according to their down- ward ranks, a task must be next to all its predecessors, inevitably

´f k 6 f k(t) 6 ^f k, 6rk ∈ R,
v´k 6 vk(t) 6 v^k, 6rk ∈ R,
where the optimization objective is to minimize the energy con- sumption of workflow execution; the four constraints respectively come from the workflow’s deadline, data dependencies among tasks, minimum and maximum CPU frequencies of virtual machi- nes, as well as minimum and maximum supply voltages of virtual machines.

Algorithm design

Energy-aware workflow scheduling for cloud platforms is con- fronted with the non-linear power consumption model, sophisti- cated workflow structures, heterogeneous cloud resources, and
fulfilling the precedence constraints among tasks.
In this paper, we encode a solution as two n-dimensional vec- tors. The first vector corresponds to the mapping from tasks to vir- tual machines, where an index denotes a task, and its value denotes the virtual machine where this task will be executed. The second segment corresponds to the mapping from tasks to execution time, where an index denotes a task, and its value denotes the execution time of this task.
Fig. 3 gives the encodings for the Gantt charts in Fig. 2. As shown Fig. 3a, the values of the first vector (i.e., 1, 2, and 1) indicate that tasks t1 and t3 are mapped to the virtual machine r1, and task t2 is mapped to r2. Then, the values of the second vector indicate that the execution time of these three tasks is 5, 8, and 4 s, respec- tively. Since the Gantt chart in Fig. 2b is obtained by adjusting t1’s execution time from 5 to 10 s, the corresponding encoding only changes the execution time of task t1, as shown in Fig. 3b.



Main process

For the proposed energy-aware intelligent scheduling algorithm (EIS), its main process is summarized in Algorithm 1. Its elemental inputs contain the data about the workflow, candidate resource pool, population size of the algorithm, and the maximum function evaluations as stop condition. When the EIS meets the stop condi- tion, it will select and output a scheduling solution with the min- imum energy consumption.
ferential evolution [50] and particle swarm optimization [51] (Line 4). The above reproduction process is to evolve the mappings from tasks to resources, i.e., the first encoding segments of solutions. In this process, the task execution time on the mapped resource is set to the maximum value. Then, the Function SlackTimeAssignment() is called to reduce energy consumption by distributing workflow slack time to extend the runtime of each task (Line 5). Next, the Function IdleTimeGrab() is called to extend some tasks’ runtime by mining the idle time gaps between tasks to achieve the purpose

		of reducing dynamic energy consumption (Line 6). The above two

Algorithm 1: Main Process of EIS


























 

functions are to optimize the execution time of each task, i.e., the second encoding segments of solutions. These two functions are detailed in Algorithm 2 and Algorithm 3, respectively. Based on the mappings from workflow tasks to cloud resources and the mappings from workflow tasks to rutime, the proposed algorithm evaluates the energy consumption of each scheduling solution to generate an offspring population (Line 7). Also, the number of func- tion evaluations used FEs is updated (Line 8).
During the population selection process, solutions in the off- spring population Q compete with the solutions of the parent pop- ulation P one by one, and the solutions with lower energy consumption will survive to the next generation (Lines 9–13). Pi represents the i-th schedule solution in population P. Similarly, Qi corresponds to the i-th solution in Q. EC(Pi) and EC(Qi) denote the energy consumption of schedule solutions Pi and Qi, respec- tively. When the proposal reaches the stop condition, it checks each solution in the final population and selects a schedule solu- tion with minimum energy consumption (Lines 15–20).

Energy-aware mechanisms

To reduce the voltages/frequencies of task executions for energy saving, the Function SlackTimeAssignment() distribute the work- flow slack time (difference between its completion time and dead- line) to each workflow task according to its optimal runtime that minimizes energy consumption. We derive the optimal runtime of a task as follows.
Assume the operating frequency of a virtual machine rk is fixed
for executing a task ti, and ti’s computation length is li. The runtime of this task is a variable, expressed as si,k . Based on (6), the operat- ing frequency of rk is f k (t) = li/si,k . Then, the functional relationship between energy consumption and task runtime can be expressed
as follows:



As shown in Algorithm 1, the proposal follows the popular


ec	Z st(rk )+si,k ps
p^k f

t 3dt

initialization, reproduction and selection. In the initialization stage,
the proposal arbitrarily generates a population (Line 1), and

= ps · si k +
p^k	li 3
·(	)

· si k

(16)

employs parameter FEs to record the number of function evalua- tions that have been used (Line 2). Since we explore the energy saving of workflow execution in dynamic voltage/frequency scaling-enabled cloud datacentres, each schedule solution in pop-

k	,	^3
k
p
= ps · si,k +
k
si,k
(li)3
·	.
(si,k )

ulation P is defined to contain two decision vectors: mappings from tasks to resources as well as mappings from tasks to execution
Let ∂(eck,i ) = 0 for optimization, we obtain
i,k

time. Since the mappings from task to runtime involves sophisti- cated constraints, see (14) for details, the random generation of this vector is easy to violate these constraints. Thus, during the ini-
ps — 2 ·
p^k
^3
k
(li)3


(si k )3
= 0.	(17)

tialization phase, the task runtime on the mapped resource is set to the minimum value, that is, the runtime under the maximum fre- quency/power consumption of the mapped resource. After initial- ization, the EIS iterates the two processes of population reproduction and selection until the number of function evalua- tions used FEs reaches the pre-specified maximum value MFE.
In the population reproduction stage, N decision vectors describing the mappings from workflow tasks to cloud resources are generated using the classic reproduction operators, such as dif-
Based on (17), the optimal execution time of ti can be computed
as
 li   3 2p^k


i,k	^3	s
k	k
The pseudo-code of Function SlackTimeAssignment() is illus- trated in Algorithm 2. This function first calculates the start and finish time of each workflow task based on its minimum runtime



on the corresponding virtual machine, thus obtaining the comple- tion time of the entire workflow. Then, the workflow slack time between its deadline and completion time is available. Based on the optimal runtime in (18), the workflow slack time is distributed to appropriately extend tasks’ runtime, so as to reduce the operat- ing frequencies of virtual machines to achieve the purpose of energy saving.
above calculations are based on the minimum runtime of the tasks on the mapped resources. Then, the completion time ct and slack time slaT of the workflow are obtained (Lines 14–15).
After that, Function SlackTimeAssignment() calculates the opti- mal runtime of each task (Lines 17–19), where OETi records the optimal runtime for the i-th task in the workflow. Based on work- flow slack time and task optimal runtime, this function assigns the

		real runtime for each task (Lines 21–23). From the operation in line

Algorithm 2: Function SlackTimeAssignment(DV , G, R)
22, we can see that the slack time is distributed to each task in pro-

portion to its optimal runtime, i.e., OETi/Pn  OETh. Besides, the
real runtime of a task cannot exceed its optimal value. Next, this function updates each task’s start time and finish time based on its real runtime (Line 24).
The pseudo-code of Function IdleTimeGrab() is shown in Algo- rithm 3. This function excavates the idle time gaps caused by task precedence constraints to further reduce dynamic energy con- sumption whilst satisfying workflows’ deadline constraints.


Algorithm 3: Function IdleTimeGrab(DV , G)























	
As  shown  in  Algorithm 2,  Function  SlackTimeAssignment()
receives a set of decision variables DV, each of which corresponds		
to a mapping from tasks to resources. According to each decision

variable, denoted as →x, this function assigns runtime to each work- flow task. It calculates the completion time of the workflow as fol- lows (Lines 3–14). The RT is used to record the ready time of the R resources for executing the next task (Line 3). Starting from the tasks without predecessors, all the workflow tasks are traversed downward to calculate their start time sti and finish time fti (Lines
4–13). The value of the parameter xi indicates the index of the
resource mapped to task ti, and the operation in line 5 stands for that a task’s start time is greater than the resource’s ready time. The operation in line 7 is to calculate the latest time at when a task completes receiving the input files from all its predecessors. Also, the start time of a task should be larger than at to fulfill the prece- dence constraints (Lines 8–10). The operation in line 12 is to obtain the finish time for each workflow task. It is worth noting that the
As shown in Algorithm 3, Function IdleTimeGrab receives a set of decision variables DV, each of which contains a mapping from tasks to resources and a mapping from tasks to runtime, which has been determined by Function SlackTimeAssignment(). The 1 × n vector RET is used to record the real runtime for each workflow task (Lines
2 and 14). For a decision variable →x in DV, this function traverses
each task to obtain its latest finish time lfti, before which complet- ing this task will not delay the start time of all its successors and the tasks executed after it. For a task ti, its latest finish time is first initialized to the start time of the task appended immediately after it (Line 6), which can avoid affecting the start of subsequent tasks. Then, each successor task of task ti is traversed to update its latest finish time (Lines 7–12). Next, the real runtime of task ti is updated (Line 13). If a task is at the end of the task queue on a resource, i.e.,



v4
v3
1 v2 v1



r2










0	5	10	15

(a)

v4
v3
1 v2 v1



r2





0	5	10	15

(b)


Fig. 2. Example of energy conservation by adjusting task execution time/frequency.


t1	t2	t3

x
t1	t2	t3

x '


(a)	(b)

Fig. 3. Two encoding examples.



ta = £, its runtime remains unchanged (Lines 14–15). After that, this function updates each task’s start time and finish time based on its real runtime (Line 18).

Performance evaluation

This section presents the experimental verification for the pro- posal in detail, e.g., experiment configurations including datasets, parameters, and a performance metric, as well as experimental results and analysis.

Experimental setting

In our evaluation, we select three relevant and recent meta- heuristics, i.e., DMFO-DE [52], PSO-COGENT [53], and ERTS [54],
for performance comparison. The brief descriptions of these three metaheuristics are as follows.
DMFO-DE is a hybrid discrete optimization algorithm. It combi- nes an opposition-based Moth-Flame Optimization with the Differ- ential Evolution to map workflow tasks to cloud resources, and uses the heterogeneous earliest finish time rule to determine the task execution order.
PSO-COGENT is a particle swarm optimization-based resource allocation algorithm to reduce the energy consumption of cloud datacentres by formulating deadlines as constraints.
ERTS integrates new genetic operators and a frequency scaling strategy for resource provisioning and task scheduling in DVFS- enabled cloud workflows.
To compare the four energy-aware workflow scheduling algo- rithms, we employ five types of real-world workflows with differ-
ent sizes [55], including Montage with 25, 50, 100, and 1000 tasks,
Epigenomics with 24, 46, 100, and 997 tasks, Inspiral with 30, 50,
100, and 1000 tasks, Cybershake with 30, 50, 100, and 1000 tasks,
and Sipht with 30, 60, 100, and 1000 tasks. Fig. 4 depicts the topo- logical structures of five different real-world workflow applications with around 30 tasks.
These workflows come from different fields. For instance, Mon- tage [56] is a flexible toolkit to assemble and process large sky images in the astronomy field. Epigenomics [57] is a pipeline data processing for automatic genome sequencing operations in bioin- formatics applications. Inspiral workflow [58] is to detect gravita- tional radiation generated during the most violent events in astrophysics. Cybershake [59] is a powerful analysis tool to study earthquake hazards. Sipht [60] is a high-throughput technology program for kingdom-wide prediction and functional annotation of bacterial sRNA-encoding genes in the bioinformatics field.
To set the deadline constraint for a workflow, we estimate its minimum completion time by allocating each workflow task to a virtual machine with the most computationally powerful configu-
ration. The symbol s_ m denotes a workflow’s minimum completion
time. We introduce a constraint factor a to control different con- straints, and the deadline of a workflow is set as follows.
D = a · s_ m.	(19)
From the formula (19), it can be easily derived that with the increase of factor a, the deadline of a workflow becomes more relaxed.
The proposed EIS employs the uniform crossover and bit-flip mutation operators evolve the mappings from tasks to cloud resources. Mutation rate is set as 1/n, where n denotes the number




Fig. 4. Five real-world workflows [55].



of tasks. Besides, the parameter settings of the three comparison algorithms follow the recommended values of the original papers. To ensure the fairness of performance comparison for different algorithms, the population size of the four algorithms is set to 100. The maximum number of function evaluations (FEs) is set as n × 4e3, where n is the number of decision variables. Besides, we conduct all the experiments on a PC with 8 GB RAM, two Intel CPUs i5-6500, and 64-bit Windows 10 operating system. On this PC, we
install MATLAB R2020b to run these four algorithms.

Impact of deadline

To evaluate the effect of deadline constraint factor a on the per- formance of the proposal and the three competitors, i.e., DMFO-DE, PSO-COGENT, and ERTS, we increase the factor a from 1.5 to 6.0 with a step of 0.5, and stabilize other parameters. Fig. 5 illustrates
the change of energy consumption by prolonging the deadline on Montage_50, Epigenomics_46, Inspiral_50, CyberShake_50, and Sipht_30 workflows. The mark Montage_50 denotes the Montage workflow with 25 tasks, the other four marks can be similarly parsed.
From Fig. 5, we can see that with the increase of factor a, the
energy consumed by the four energy-aware workflow scheduling algorithms decreases correspondingly. This trend can be attributed to the following facts. Cloud datacentres are heterogeneous and elastic, which means that the power-performance ratios of differ- ent virtual machines vary greatly and the number of accessible vir-
tual machines is  sufficient. Increasing factor a to  prolong
workflows’ deadlines gives each workflow task a greater opportu- nity to run on a virtual machine with a higher power-performance ratio. This helps reduce the overall energy consumption of work- flow executions.
In Fig. 5, one noticeable phenomenon is that when the deadline constraint factor a increases to a certain extent, the energy con- sumption of the four algorithms tends to be stable. The main rea- son is that prolonging workflow deadlines is conducive to
extending the runtime of each task to reduce the dynamic energy consumption, but the static energy consumption increases accord- ingly. When the deadline increases to a certain extent, the reduced dynamic energy consumption will be offset by static energy con- sumption. Another striking phenomenon is that in different work- flow instances, the factor values when energy consumption tends to be stable are different. For example, the energy consumed by
the four algorithms tends to be stable when the factor a is larger
than 3.0 on the Montage_50 workflow, while that is larger than
5.0 on the Inspiral_50 workflow. This is due to the significant dif- ferences in the topological structures of different types of workflows.
In comparison with the three competitors, the proposed EIS achieves the lowest energy consumption under different deadline constraint factors. Considering the Montage_50 workflow in Fig. 5a, on average the proposed EIS achieves 30.89%, 27.46%, and 18.36% improvement in comparison to DMFO-DE, PSO-COGENT, and ERT, respectively. The EIS poses similar advantages in solving the optimization probelms derived from the other four workflows, i.e., Epigenomics_46, Inspiral_50, CyberShake_50, and Sipht_30.




1.4

1.3

1.2

1.1

1

0.9

1.4

1.2

1

0.8


2	3	4	5	6	2	3	4	5	6

(a)	(b)



4.5

4

3.5

3

2.5













2	3	4	5	6
4.5

4

3.5

3

2.5













2	3	4	5	6


	
(c)	(d)


3.4
3.2
3
2.8
2.6
2.4
2.2

2	3	4	5	6

(e)
Fig. 5. Impact of workflow deadline on energy consumption.





5

4.5

4

3.5

3













0.5	1	1.5	2	2.5	3
FEs	 104
(a)
2.4
2.3
2.2
2.1
2
1.9
1.8
1.7













0.5	1	1.5	2	2.5	3
FEs	 104
(b)



11
11
10	10
9	9
8
8
7
7	6


0.5	1	1.5	2	2.5	3
FEs	 104
(c)
0.5	1	1.5	2	2.5	3
FEs	 104
(d)




7

6.5

6

5.5

5

0.5	1	1.5	2	2.5	3
FEs	 104
(e)
Fig. 6. Change of energy consumption with evolution process.



Although the ERT and the proposed EIS both leverage dynamic voltage/frequency scaling technology to reduce energy consump- tion, the proposed EIS still shows overwhelming advantages. The reason is that EIS benefits from the workflow slack time distribu- tion mechanism. In summary, the proposed EIS achieves the lowest energy consumption while satisfying workflows’ deadline constraints.

Trends of energy consumption

This set of experiments attempts to compare the convergence rates of the four algorithms. We select five workflow applications, i.e.,  Montage_100,  Epigenomics_100,  Inspiral_100,  Cyber-
Shake_100, and Sipht_100, and fix their deadline constraint factor as a = 1.5. Fig. 6 exhibits the changes in energy consumption as the evolution progresses. The parameter FEs corresponds to the
number of function evaluations that have been used.
The intuitive impression of Fig. 6 is that the energy consumed by the four algorithms ascends as the number of function evalua- tions increases, especially in the initial search stage. This is because a cloud datacentre has enough virtual machines and the quality of randomly initialized solutions is low. In such a scenario, bio- inspired optimization algorithms can quickly explore better solu- tions. Among the three existing energy-aware workflow schedul- ing algorithms, the ERT has obvious advantages in convergence speed and value. It comes down to the fact that considerable energy can be reducing by slowing down the voltages/frequencies of virtual machines for executing some non-critical tasks, such that the energy consumption of workflow executions can be reduced without violating the deadline constraints. Compared with the competitor ERT, the proposed EIS poses better convergence speed and value. The primary reason is that the proposal not only mines the idle time gaps to slow down the voltages/frequencies for energy conservation, but also reasonably distributes the workflow slack time to slow down the voltages/frequencies.

Comparison results

To further compare the performance of the four algorithms, this set of experiments is to consider the schedule solution with the lowest energy consumption in the output population of each algo-
rithm. We set the deadline constraint factor as a = 1.5, and the FEs
as n × 4e3. Table 1 provides a comparative summary of the energy consumption of the proposed EIS and the three competitors, i.e., DMFO-DE, PSO-COGENT, and ERTS, in solving optimization prob- lem derived from 20 frequently-used workflows. This table includes the mean and standard deviation (in brackets) of energy consumption of 30 repeated experiments.
As shown in Table 1, the proposed EIS poses a higher energy- saving performance in all workflow applications. The reason is that the competitors DMFO-DE and PSO-COGENT focus on evolving the mappings from workflow tasks to resources to reduce energy con- sumption, and did not consider the dynamic voltage/frequency scaling technique. Although the competitor ERT employs the dynamic voltage/frequency scaling technique, its energy consump- tion is much higher than the proposed EIS. Such superiority of EIS can be attributed to the following two facts. First of all, the EIS dis- tributes workflow slack time (difference between its completion time and deadline) among tasks based on the optimal execution time of each task for energy conservation by adjusting the voltage and frequency. Secondly, the EIS excavates the idle time gaps caused by task precedence constraints to further reduce dynamic energy consumption whilst satisfying workflows’ deadline constraints.
Besides, with the increase of workflow scale, the EIS improves the competitors more obviously. For instance, in comparison to ERTS, the EIS achieves 16.43% improvement in the Sipht workflow with 30 tasks, while that increases to 19.38% in the Sipht workflow with 100 tasks. In general, as the workflow scale increases, the workflows’ topological structures become more complex, and the corresponding optimization problems become more difficult. Hence, the improvement of the EIS in scheduling larger-scale workflows demonstrates its advantages in handling optimization problems derived from complex workflows.

Ablation analysis

The proposed EIS mainly includes two energy-saving compo- nents. Function SlackTimeAssignment() is to reduce energy con- sumption by distributing workflow slack time to extend the runtime of each task (Line 5, Algorithm 1). Function IdleTimeGrab() is to extend some tasks’ runtime by mining the idle




Table 1
Energy consumption (Joule) of the four algorithms on 15 workflows.





3.5

3.4

3.3

3.2

3.1

3









8
7.5
7
6.5
6
5.5
5













Non-STA	Non-ITG	EIS
Variants
(a)












Non-STA	Non-ITG	EIS
Variants
(c)
1.9


1.85


1.8


1.75








7.5
7
6.5
6
5.5
5
4.5













Non-STA	Non-ITG	EIS
Variants
(b)












Non-STA	Non-ITG	EIS
Variants
(d)


Fig. 7. Performance comparison of the proposal and its variants.


time gaps between tasks to achieve the purpose of reducing dynamic energy consumption (Line 6, Algorithm 1). To verify their performance contribution, we construct two variants of the pro- posed EIS, denoted as Non-STA and Non-ITG. Non-STA is con- structed by removing the Function SlackTimeAssignment(), while Non-ITG is constructed by removing the Function IdleTimeGrab(). Based on four workflows, i.e., Montage_100, Epigenomics_100, Inspiral_100, and Cybershake_100, the energy consumption of Non-STA, Non-ITG, and EIS is compared in Fig. 7.
The main component difference between variant Non-STA and EIS is that Non-STA does not employ Function SlackTimeAssignment(). Then, the amount of energy reduced by EIS over Non-STA can be attributed to the performance contribu- tion of Function SlackTimeAssignment(). Similarly, the energy reduction of EIS over Non-ITG corresponds to the performance con- tribution of Function IdleTimeGrab(). According to the comparison results on the four workflows, we can see that Function SlackTimeAssignment() contributes more to energy saving. For example, on workflow Montage_100, Function.
SlackTimeAssignment() reduces the energy consumption from
3.42e + 4 to 3.03e + 4, while Function SlackTimeAssignment()
reduces the energy consumption from 3.11e + 4 to 3.03e + 4.
Conclusions and future work

This paper strives to optimize the energy consumed by execut- ing workflow applications in a cloud datacentre. According to the characteristics of cloud resources and applications, this paper pro- vides the energy power and workflow models, and formulates the optimization problem with deadline and precedence constraints. Then, an energy-aware intelligent scheduling algorithm to explore the workflow slack time and idle time gaps between tasks to reduce dynamic energy consumption whilst satisfying workflows’ deadline constraints. Also, extensive comparison results demonstrate the proposal’s superior performance in terms of energy saving.
Based on this research, it is promising to holistically explore the energy-efficient optimization problem for other types of subsys- tems including cooling, network, storage, and cache in cloud plat- forms. Besides, integrating the proposed techniques into real- world cloud platforms is another research direction.

CRediT authorship contribution statement

Min Cao: Conceptualization, Methodology, Software, Writing – original draft. Yaoyu Li: Data curation, Supervision. Xupeng Wen:



Visualization, Investigation. Yue Zhao: Supervision, Writing – review & editing. Jianghan Zhu: Software, Validation.

Declaration of Competing Interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

Acknowledgments

This research work is supported by the National Natural Science Foundation of China (71801218), the Science and Technology Inno- vation Program of Hunan Provincial (2022RC1241).

References

Armbrust M, Fox A, Griffith R, Joseph AD, Katz R, Konwinski A, Lee G, Patterson D, Rabkin A, Stoica I. A view of cloud computing. Commun ACM 2010;53 (4):50–8.
Elsherbiny S, Eldaydamony E, Alrahmawy M, Reyad AE. An extended intelligent water drops algorithm for workflow scheduling in cloud computing environment. Egypt Inf J 2018;19(1):33–55.
Lee YC, Han H, Zomaya AY, Yousif M. Resource-efficient workflow scheduling in clouds. Knowl-Based Syst 2015;80:153–62.
Chen H, Zhu X, Qiu D, Liu L, Du Z. Scheduling for workflows with security- sensitive intermediate data by selective tasks duplication in clouds. IEEE Trans Parallel Distrib Syst 2017;28(9):2674–88.
Li M, Tian Z, Du X, Yuan X, Shan C, Guizani M. Power normalized cepstral robust features of deep neural networks in a cloud computing data privacy protection scheme. Neurocomputing 2023;518:165–73.
Gartner, Gartner forecasts worldwide public cloud: End-user spending to grow 23% in 2021.
Wang J, Palanisamy B, Xu J. Sustainability-aware resource provisioning in data centers. In: IEEE 6th International Conference on Collaboration and Internet Computing (CIC). IEEE; 2020. p. 60–9.
Ilager S, Buyya R. Energy and thermal-aware resource management of cloud data centres: A taxonomy and future directions, arXiv preprint arXiv:2107.02342.
Cao B, Sun Z, Zhang J, Gu Y. Resource allocation in 5G IoV architecture based on SDN and fog-cloud computing. IEEE Trans Intell Transp Syst 2021;22 (6):3832–40.
Wang Y, Liu Y, Xia M. Construction of a multi-source heterogeneous hybrid platform for big data. J Comput Methods Sci Eng 2021;21(3):713–22.
Sun J, Chen Y, Dai M, Zhang W, Sangaiah AK, Sun G, Han H. Energy efficient deployment of a service function chain for sustainable cloud applications. Sustainability 2018;10(10):3499.
Gill SS, Garraghan P, Stankovski V, Casale G, Thulasiram RK, Ghosh SK, Ramamohanarao K, Buyya R. Holistic resource management for sustainable and reliable cloud computing: An innovative solution to global challenge. J Syst Softw 2019;155:104–29.
Freitag C, Berners-Lee M, Widdicks K, Knowles B, Blair G, Friday A. The climate impact of ICT: A review of estimates, trends and regulations, arXiv preprint arXiv:2102.02622.
Thaman J, Singh M. Green cloud environment by using robust planning algorithm. Egypt Inf J 2017;18(3):205–14.
Marahatta A, Pirbhulal S, Zhang F, Parizi RM, Choo K-KR, Liu Z. Classification- based and energy-efficient dynamic task scheduling scheme for virtualized cloud data center. IEEE Trans Cloud Comput 2021;9(4):1376–90.
Lv Z, Chen D, Lv H. Smart city construction and management by digital twins and BIM big data in COVID-19 scenario. ACM Trans Multimedia Comput Commun Appl 2022;18(2s):1–21.
Chen H, Zhu X, Liu G, Pedrycz W. Uncertainty-aware online scheduling for real-time workflows in cloud service environment. IEEE Trans Serv Comput 2021;14(4):1167–78.
Bharany S, Badotra S, Sharma S, Rani S, Alazab M, Jhaveri RH, Gadekallu TR. Energy efficient fault tolerance techniques in green cloud computing: A systematic survey and taxonomy. Sustain Energy Technol Assessments 2022;53:102613.
Medara R, Singh RS. A review on energy-aware scheduling techniques for workflows in IaaS clouds. Wireless Pers Commun 2022;125:1545–84.
Choudhary A, Govil MC, Singh G, Awasthi LK, Pilli ES. Energy-aware scientific workflow scheduling in cloud environment. Cluster Comput 2022;25 (6):3845–74.
Ali HGEDH, Saroit IA, Kotb AM. Grouped tasks scheduling algorithm based on QoS in cloud computing network. Egypt Inf J 2017;18(1):11–9.
Calheiros RN, Buyya R. Meeting deadlines of scientific workflows in public clouds with tasks replication. IEEE Trans Parallel Distrib Syst 2014;25 (7):1787–96.
Garg N, Singh D, Goraya MS. Energy and resource efficient workflow scheduling in a virtualized cloud environment. Cluster Comput 2021;24 (2):767–97.
Lee YC, Zomaya AY. Energy conscious scheduling for distributed computing systems under different operating conditions. IEEE Trans Parallel Distrib Syst 2011;22(8):1374–81.
Li Z, Ge J, Hu H, Song W, Hu H, Luo B. Cost and energy aware scheduling algorithm for scientific workflows with deadline constraint in clouds. IEEE Trans Serv Comput 2018;11(4):713–26.
Safari M, Khorsand R. PL-DVFS: combining power-aware list-based scheduling algorithm with DVFS technique for real-time tasks in cloud computing. J Supercomput 2018;74(10):5578–600.
Qureshi B. Profile-based power-aware workflow scheduling framework for energy-efficient data centers. Future Gener Comput Syst 2019;94:453–67.
Rani R, Garg R. Power and temperature-aware workflow scheduling considering deadline constraint in cloud. Arab J Sci Eng 2020;45 (12):10775–91.
Fan G, Chen X, Li Z, Yu H, Zhang Y. An energy-efficient dynamic scheduling method of deadline-constrained workflows in a cloud environment. IEEE Trans Netw Serv Manage 2022 (in press).
Kalra M, Singh S. A review of metaheuristic scheduling techniques in cloud computing. Egypt Inf J 2015;16(3):275–95.
Houssein EH, Gad AG, Wazery YM, Suganthan PN. Task scheduling in cloud computing based on meta-heuristics: review, taxonomy, open challenges, and future trends. Swarm Evol Comput 2021;62:100841.
Konjaang JK, Xu L. Meta-heuristic approaches for effective scheduling in infrastructure as a service cloud: A systematic review. J Netw Syst Manage 2021;29(2):1–57.
Tarafdar A, Debnath M, Khatua S, Das RK. Energy and makespan aware scheduling of deadline sensitive tasks in the cloud environment. J Grid Comput 2021;19:1–25.
Malik N, Sardaraz M, Tahir M, Shah B, Ali G, Moreira F. Energy-efficient load balancing algorithm for workflow scheduling in cloud data centers using queuing and thresholds. Appl Sci 2021;11(13):5849.
Li H, Xu G, Wang D, Zhou M, Yuan Y, Alabdulwahab A. Chaotic-nondominated- sorting owl search algorithm for energy-aware multi-workflow scheduling in hybrid clouds. IEEE Trans Sustain Comput 2022;7(3):595–608.
Qi L, Chen Y, Yuan Y, Fu S, Zhang X, Xu X. A QoS-aware virtual machine scheduling method for energy conservation in cloud-based cyber-physical systems. World Wide Web 2020;23(2):1275–97.
Hussain M, Wei L-F, Rehman A, Abbas F, Hussain A, Ali M. Deadline- constrained energy-aware workflow scheduling in geographically distributed cloud data centers. Future Gener Comput Syst 2022;132:211–22.
Domanal SG, Guddeti RMR, Buyya R. A hybrid bio-inspired algorithm for scheduling and resource management in cloud environment. IEEE Trans Serv Comput 2017;13(1):3–15.
Cao K, Wang B, Ding H, Lv L, Tian J, Hu H, Gong F. Achieving reliable and secure communications in wireless-powered NOMA systems. IEEE Trans Veh Technol 2021;70(2):1978–83.
Dai X, Xiao Z, Jiang H, Alazab M, Lui JC, Min G, Dustdar S, Liu J. Task offloading for cloud-assisted fog computing with dynamic service caching in enterprise management systems. IEEE Trans Industr Inf 2022;19(1):662–72.
Xiao Z, Shu J, Jiang H, Min G, Chen H, Han Z. Perception task offloading with collaborative computation for autonomous driving. IEEE J Sel Areas Commun 2023;41(2):457–73.
Zhao F, Jiang T, Wang L. A reinforcement learning driven cooperative meta- heuristic algorithm for energy-efficient distributed no-wait flow-shop scheduling with sequence-dependent setup time. IEEE Trans Industr Inf 2022 (in press).
Wang J-J, Wang L. A cooperative memetic algorithm with feedback for the energy-aware distributed flow-shops with flexible assembly scheduling. Comput Ind Eng 2022;168:108126.
Zhao F, Zhang L, Cao J, Tang J. A cooperative water wave optimization algorithm with reinforcement learning for the distributed assembly no-idle flowshop scheduling problem. Comput Ind Eng 2021;153:107082.
Zhao F, Di S, Wang L. A hyperheuristic with Q-learning for the multiobjective energy-efficient distributed blocking flow shop scheduling problem. IEEE Trans Cybern 2022 [in press].
Wang G-G, Gao D, Pedrycz W. Solving multiobjective fuzzy job-shop scheduling problem by a hybrid adaptive differential evolution algorithm. IEEE Trans Industr Inf 2022;18(12):8519–28.
Pan Z, Lei D, Wang L. A knowledge-based two-population optimization algorithm for distributed energy-efficient parallel machines scheduling. IEEE Trans Cybern 2022;52(6):5051–63.
Yan A, Yang K, Huang Z, Zhang J, Cui J, Fang X, Yi M, Wen X. A double-node- upset self-recoverable latch design for high performance and low power application. IEEE Trans Circuits Syst II Express Briefs 2018;66(2):287–91.
Topcuoglu H, Hariri S, Wu M-Y. Performance-effective and low-complexity task scheduling for heterogeneous computing. IEEE Trans Parallel Distrib Syst 2002;13(3):260–74.
Das S, Suganthan PN. Differential evolution: A survey of the state-of-the-art. IEEE Trans Evol Comput 2010;15(1):4–31.
Poli R, Kennedy J, Blackwell T. Particle swarm optimization. Swarm Intell 2007;1(1):33–57.



Ahmed OH, Lu J, Xu Q, Ahmed AM, Rahmani AM, Hosseinzadeh M. Using differential evolution and Moth-Flame optimization for scientific workflow scheduling in fog computing. Appl Soft Comput 2021;112:107744.
Kumar M, Sharma SC. PSO-COGENT: Cost and energy efficient scheduling in cloud environment with deadline constraint. Sustain Comput: Inf Syst 2018;19:147–64.
Cao E, Musa S, Chen M, Wei T, Wei X, Fu X, Qiu M. Energy and reliability-aware task scheduling for cost optimization of DVFS-enabled cloud workflows. IEEE Trans Cloud Comput 2022 [in press].
Juve G, Chervenak A, Deelman E, Bharathi S, Mehta G, Vahi K. Characterizing and profiling scientific workflows. Future Gener Comput Syst 2013;29 (3):682–92.
Berriman GB, Deelman E, Good JC, Jacob JC, Katz DS, Kesselman C, Laity AC, Prince TA, Singh G, Su M-H. Montage: a grid-enabled engine for delivering
custom science-grade mosaics on demand. Optimizing scientific return for astronomy through information technologies, Vol. 5493. SPIE; 2004. p. 221–32.
Laird PW. Institutional profile: The usc epigenome center. Epigenomics 2009;1 (1):29–31.
Abbott B, Abbott R, Adhikari R, Ajith P, Allen B, Allen G, Amin R, Anderson S, Anderson W, Arain M. LIGO: the laser interferometer gravitational-wave observatory. Rep Prog Phys 2009;72(7):076901.
Graves R, Jordan TH, Callaghan S, Deelman E, Field E, Juve G, Kesselman C, Maechling P, Mehta G, Milner K. Cybershake: A physics-based seismic hazard model for southern California. Pure Appl Geophys 2011;168(3):367–81.
Livny J, Teonadi H, Livny M, Waldor MK. High-throughput, kingdom-wide prediction and annotation of bacterial non-coding RNAs. PLOS ONE 2008;3(9): e3197.
