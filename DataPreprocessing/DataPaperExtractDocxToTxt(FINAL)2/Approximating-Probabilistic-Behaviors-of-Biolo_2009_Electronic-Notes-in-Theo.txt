

Electronic Notes in Theoretical Computer Science 229 (2009) 165–182
www.elsevier.com/locate/entcs

Approximating Probabilistic Behaviors of Biological Systems Using Abstract Interpretation
Alessio Coletta1 ,2 , Roberta Gori1 ,3 and Francesca Levi 1 ,4
Department of Computer Science, University of Pisa, Italy

Abstract
In this paper we apply the Abstract Interpretation approach [8,9] for approximating the behavior of biological systems, modeled specifically using the Chemical Ground Form calculus [4], a new stochastic calculus rich enough to model the dynamics of biochemical reactions.
Our analysis computes an Interval Markov Chain (IMC) that safely approximates the Discrete-Time Markov Chain, describing the probabilistic behavior of the system, and reports both lower and upper bounds for probabilistic temporal properties. Our analysis has several advantages: (i) the method is effective (even for infinite state systems) and allows us to systematically derive an IMC from an abstract labeled transition sys-
tem; (ii) using intervals for abstracting the multiplicity of reagents allows us to achieve conservative bounds for the concrete probabilities of a set of concrete experiments which differs only for initial concentrations.
Keywords: Stochastic π-calculus, Abstract Interpretation, Probabilistic Temporal Properties


Introduction
Process calculi, originally designed for modeling distributed and mobile systems, are nowadays one of the most popular formalisms for the specification of biologi- cal systems. In this new application domain, a great effort has been devoted for adapting the traditional models to characterize the molecular and biochemical as- pects of biological systems. On one hand, the proposals, such as BioAmbients [33], Beta-Binders [31], and Brane calculi [3], aim at expressing the concepts of hier- archy, compartment and membrane, which play a key role in the organization of biomolecular systems. On the other hand, there is a new interest in the design of

1 This work has been partially supported by the MIUR project Bisca.
2 Email: coletta@di.unipi.it
3 Email: gori@di.unipi.it
4 Email: levifran@di.unipi.it

1571-0661/© 2009 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2009.02.011

calculi, such as stochastic π-calculus [30,32,34], able to capture the quantitative
aspect (both time and probability) of real life applications.
The use of process calculi as a specification language offers a range of well es- tablished techniques for analysis and verification that can now be applied also to biological systems. The stochastic simulators, such as [34,27,28] for π-calculus, could be used to realize virtual experiments on biological systems models in order to test possible hypotheses and to guide future in vivo experimentations.
An orthogonal approach is that based on the verification of temporal properties by means of model checking techniques, recently extended also to probabilistic and stochastic models [19,21,5]. The study of temporal properties could give to biologists interesting information about the possible behavior of complex biological systems, such as pathways and networks of proteins.
Unfortunately, the practical application of automatic tools to biological systems revealed serious problems. One specific feature of biological processes is that they are composed by a huge number of processes with identical behavior, such as thou- sands of molecules of the same type. Thus, the state space of the transition system is often very large. Moreover, different scenarios have typically to be analyzed in or- der to infer interesting information from a biological point of view. For example, the hypotheses have to be tested several times just varying the initial concentrations.
Approximation techniques [6,11] preserving the validation of temporal proper- ties have been established to be one of the most effective ways for overcoming these limitations. In particular, static analysis techniques have been widely applied for computing safe approximations of the (run-time) behavior of a system, even for infi- nite systems, in the setting of purely qualitative process calculi. For distributed and mobile systems a variety of analyses [2,15,16,17,18,23,25,26,29] have been proposed, aimed at proving properties, such as invariance and even of more general temporal properties.
In this paper we investigate the application of approximation techniques able to handle quantitative temporal properties. As a specification language we consider a simple calculus, the Chemical Ground Form (CGF)[4], a subset of π-calculus without communication enriched with transition rates that determine the stochas- tic behavior. The calculus is rich enough for suitably modeling the dynamics of biochemical reactions. Furthermore, we consider the validation of probabilistic temporal properties, such as those expressible in the logic PCTL [21]. Examples of interesting probabilistic temporal properties could be: which is the probability to reach a state where the concentration of molecule A is greater than n? which is the probability that always (in each state) the concentration of molecule A is greater than the one of molecule B?
Thus, our goal is to approximate the Discrete-Time Markov Chain (DTMC)
[21] that describes the probabilistic behavior of a CGF process. We recall that the DTMC can be derived in a standard way from the Continuos-Time Markov Chain (CTMC) [21] that describes the stochastic semantics. Indeed, the probability of moving from one state to another is given by the constant proportional to the rate. In stochastic calculi the CTMC is derived from a suitable Labeled Transition

System (LTS) semantics, by calculating the rate of each move. The main difficulty of this translation is to be able to count the number of distinct reagents combinations described by the same transition. In the case of CGF, Cardelli [4] introduces a LTS semantics, where processes are represented by (ordered) multisets of reagents, and where transitions are identified by distinct labels and record the corresponding rate. As expected, the rate of each transition is calculated from the rate of the basic actions, that participate to the move, by taking into account the number of distinct combinations.
Our methodology for abstracting the DTMC, modeling a CGF process, is based on the application of Abstract Interpretation [8,9], a general theory of semantics approximations, that allows us to handle finite as well as infinite systems. As it is well known, the choice of an adequate concrete semantics is fundamental in order to derive a not too coarse abstraction. This motivates the introduction of a new LTS semantics for CGF, based on the representation of processes as multisets and on the explicit labeling of processes in order to distinguish the basic actions. Moreover, we design transitions recording information about the labels of the actions that participate to the move, about their rates, and about their number of occurrences (in place of the rate of the move). We also introduce a corresponding translation from LTS into CTMC (and then DTMC), where the rate, for each transition, is obviously computed by exploiting the rate of basic actions, that participate to the move, and their multiplicities. Our LTS semantics is obviously equivalent to the semantics of [4], meaning that the derived CTMC coincide.
Our LTS semantics, however, supports a natural abstraction where the informa- tion about the multiplicities of reagents, present in each solution, is approximated by adopting the well-know domain of intervals of integers [7]. The abstraction of states, that can be formalized as a Galois connection [9], supports the definition of a corresponding abstract LTS semantics, where multiplicities are replaced by intervals of multiplicities both in states and in transitions. Thus, the abstract tran- sitions record information about the labels of the actions that participate to the move, about their rates, and about their possible number of occurrences.
Moreover, we show that our abstract LTS semantics is adequate for abstract model checking, by introducing a translation into Interval Markov Chains (IMC) [35,14,20], a model that combines together probabilistic and non-deterministic steps. In an IMC transitions are labeled with intervals of probabilities, modeling the uncer- tainty about the behavior of the system introduced by the abstraction. Intuitively, the lower bound of the interval represents an under approximation, while the upper bound represents an over approximation of the concrete probabilities. This model gives lower and upper bounds on the probability of a given temporal property rather than exact values, that are obtained by considering the worst-case and best-case scenario w.r.t. all the non-deterministic choices.
The key step of the translation from abstract LTS into IMC is the computation of intervals of probabilities from intervals of multiplicities. We achieve very accu- rate intervals of probabilities by profitably exploiting the information reported by transition labels. Our approach is correct, meaning that the derived IMC gives con-



E ::= 0 | X = S, E	Environment
S  ::= 0 | πλ.P + S	Molecules
P ::= 0 | X|P	Solutions
π ::= ar | a¯r | τr  r ∈ R+ Basic Actions

Table 1 Syntax of CGF

servative bounds for probabilistic temporal properties. In order to formalize these results we follow a traditional approach (see for example [11,12,13,35,20]) based on the definition of suitable approximation orders. For reasons of space, in this paper we focus on probabilistic reachability properties; the abstraction is however correct for full PCTL.
Finally, we present a simple example showing that our analysis is able to compute correct lower and upper bounds on the concrete probability of a set of concrete experiments, e.g. w.r.t. different initial concentrations.

Chemical Ground Form
We present the CGF calculus [4], a subset of π-calculus where basic actions are related to rates, the parameters of the exponential distribution that characterize stochastic calculi [30,27].
The syntax of (labeled) CGF is defined in Table 1. We consider a set N (ranged over by a, b, c,.. .) of names, a set L (ranged over by λ,μ .. .) of labels, and a set X (ranged over by X,Y ,....) of variables (representing reagents).
A CGF is a pair (E, P ) where E is a species environment and P is a solution. The environment E is a (finite) list of reagent definitions Xi = Si for distinct variables Xi and molecules Si describing the interaction capabilities. A molecule S may do nothing, or may change after a delay or may interact with other reagents; the standard notation of process algebras is adopted. Hence, a delay at rate r is represented by τr; the input and output on a channel a at rate r are represented by ar and a¯r model, respectively (each channel always has the same rate). A solution P is a parallel composition of variables, that is a finite list of reagents.
Notice that we assume labeled basic actions in order to identify exactly the actions involved in interactions. For these purposes, however, we have to consider well-labeled environments; an environment E is well-labeled if the labels occurring in the definitions of E are all distinct. In the following, we assume that in a CGF (E, P ), E is well-labeled and each variable X occurring in E or in P has a corresponding definition in E.
Given an environment E and a label λ ∈ L, we use the notation E.X.λ to indicate the process πλ.P provided that X = ... + πλ.P + ... is the definition of X occurring in E. Moreover, we use L(E.X) to denote the set of labels appearing in the definition of X in E.
We introduce a LTS semantics for CGF based on the natural representation of solutions as multisets of reagents.

Definition 2.1 [Multiset] A multiset is a function M : X → N. We use M for the set of multisets.
In the following, we call M (X) the multiplicity of a reagent X in the multiset
M . We may also use the standard representation for multisets as sets of pair (m, X), where m is the multiplicity of reagent X, and we may omit the pairs with multiplicity 0. For multisets we use the standard operations of sum and difference
⊕ and g, such that ∀X ∈ X ,
M ⊕ N (X)= M (X)+ N (X)
M g N (X)= M (X)−N (X)  where n−m = n − m if n − m ≥ 0, 0 otherwise.
For describing the behavior of a multiset we adopt a labeled transition relation of the form
Θ,Δ,r
M −−−→ M
where r ∈ R+ is a rate, Θ ∈ L = L ∪ (L × L), Δ ∈ Q = N ∪ (N × N) such that arity(Θ) = arity(Δ). Here, Θ reports the label (the labels) of the basic action (the basic actions) that participate to the move, Δ reports consistent information about the multiplicity (the multiplicities), and r is the related rate.
The transition relation for multisets is defined by the rules Table 2 (we are tacitly assuming to reason with respect to a given environment E). For translating solutions into multisets we exploit a function [ ] : P → M, where P is the set of solutions, defined in the obvious way.
There are two transition rules: one for delay actions, and one for synchronization. Rule (Delay) models the execution of a process τrλ.Q appearing in the definition of a reagent X. The transition records the label λ together with the multiplicity of X (e.g M (X)) as well as the rate of delay r. Rule (Sync) models the synchronization between two complementary processes arλ.Q1 and a¯rμ.Q2 appearing in the definition reagents X and Y (that may even coincide). The transition records the labels λ and μ together with the multiplicities of X and Y (e.g M (X) and M (Y )) as well as the rate of the channel r.
It is worth noticing that we admit transitions that may report even a zero multiplicity; this choice simplifies the definition of the abstraction.



E.X.λ = τrλ.Q λ,M (X),r
M −−−−−−−→ (M g (X, 1)) ⊕ [[Q]]
(Delay)



E.X.λ = arλ.Q1	E.Y.μ = a¯rμ.Q2 (λ,μ),(M (X),M (Y )),r
M −−−−−−−−−−−−−−−→ ((M g (X, 1)) g (Y, 1)) ⊕ [[Q1]] ⊕ [[Q2]]
(Sync)



Table 2 Transition relation

We recall that our processes are well-labeled, e.g. basic actions have distinct labels. As a consequence, the outgoing transitions for a solution M have distinct labels too.
Definition 2.2 [LTS] A labeled transition system (LTS) is a tuple (S, →, M0, E) where: (i) S ⊆ M is the set of states and M0 ∈ S is the initial state; (ii) and,

→⊆ S × L × Q × R+
M ,M Θ,Δ2,r2
× S is a set of transitions, such that, for each M
Θ,Δ1,r1
−−−−−→

1	−−−−−→ M2, we have Δ1 = Δ2, r1 = r2 and M1 = M2.
For an environment E and M0 ∈ M, we use LTS((E, M0)) = (S, →, M0, E) for
the LTS obtained as usual by transitive closure from the initial state M0. The LTS of a CGF (E, P ) is therefore LTS((E, [[P ])).
In the following we use LT S to denote the set of LTS. Moreover, we use

Ts(M, M ') = {M  Θ,Δ,r
M ' for some Θ, Δ and r} for describing the transitions

from a M to M '.

Discrete-Time Markov Chains
We present the probabilistic semantics of CGF by means of a translation from LTS into DTMC. We also recall the verification for probabilistic reachability properties over DTMC; more details on probabilistic model checking can be found in [21].
Given a finite or countable set of states S ⊆ M we denote with
Distr(S)= {ρ | ρ : S → [0, 1] and ΣM ∈S ρ(M )= 1}	SDistr(S)= {ρ | ρ : S → [0, 1]}
the set of (discrete) probability distributions and of pseudo-distributions on S, re- spectively.
Definition 3.1 [DTMC] A Discrete-Time Markov Chain is a triple (S, P, M0) where: (i) S ⊆ M is a ﬁnite or countable set of states and M0 ∈ S is the ini- tial state; and, (ii) P: S → Distr(S) is the probability transition function.
In DTMC state transitions are equipped with probabilities, namely P(M )(M ') reports the probability of moving from state M to state M '. In the following, we use MC to denote the set of DTMC.
In order to derive a DTMC from the LTS, we have to calculate, for each multiset M and M ', the probability of moving from M to M '. First, we extract the rate corresponding to the move from M to M ' (namely the rate of the corresponding CTMC) by exploiting the information reported by transition labels. Then, we achieve the related probability by considering as usual the constant proportional to the rate.
Θ,Δ,r
To this aim, we introduce the concept of rate of a transition. Let t = M −−−→
M ' we define


> n · m ·cr	Θ= (λ, μ) ∧ Δ= (n, m) ∧ λ ∈ L(E.X) ∧ μ ∈ L(E.Y ) ∧ X /= Y.

For computing rate(t) we take into account the number of distinct transitions t that may occur in the multiset M . More in details, the rate r of the basic action (actions) related to Θ is multiplied by the number of distinct combinations appear- ing in M (by exploiting the information recorded by Δ). The resulting rate may

be even zero. This is the case, for example, whenever a reagent X with multiplicity one interacts with X itself.
Then, we introduce functions R : S × S → R>=0 and E : S → R>=0, such that
R(M, M ')= Σt∈Ts(M,M ') rate(t)	E(M )= ΣM '∈S R(M, M ').

Intuitively, R(M, M ') reports the rate corresponding to the move from M to M ', while E(M ) is the exit rate. As usual in stochastic calculi, the probability of moving from M to M ' is computed from R(M, M ') and from the exit rate E(M ).
Definition 3.2 [Derivation of the DTMC] We define a probabilistic translation function H : LT S → MC such that H((S, →, M0, E)) = (S, P, M0), where P : S → Distr(S) is the probability transition function, such that for each M ∈ S,
if E(M )= 0, then P(M )(M ') = 0, for each M ' /= M , and P(M, M )= 1;
if E(M ) > 0, then for each M ', P(M )(M ')= R(M, M ')/E(M ).
We are interested in the probability of reaching a state satisfying a given prop- erty, starting from the initial state. Formally, we have to evaluate the probability of a set of paths.
Let (S, P, M0) bea DTMC.A path π is a non-empty (finite of infinite) sequence of states of S. We denote the i-th state in a path π, starting from 0, by π[i] and the length of π by |π|, where |π| = ∞ if π is infinite. The set of paths over S is denoted by Paths(S), while the subset of finite paths is denoted by FPaths(S). The cylinder corresponding to π is the set of all paths prefixed by π. Formally, C(π) = {ππ' | π' ∈ Paths(S),π ∈ FPaths(S)} and C(M ) denotes the set of paths starting from the state M .
Definition 3.3 [Probability of Paths] Let (S, P, M0) bea DTMC.
Let C = π∈FPaths(S) C(π) be the cylinders, B be the smallest σ-algebra containing C, and M ∈ M a state. The tuple (Paths(S), B, PM ) is a probability space, where PM is the unique measure satisfying, for all path M0 ... Mn,
,,⎨1	if M0 = M ∧ n =0 

PM (C(M0 ... Mn)) =
P(M0, M1) · ... · P(Mn−1, Mn)	if M0 = M ∧ n > 0
,,0	otherwise

Our reachability properties are parametric w.r.t. a set AP of propositional symbols (ranged over by A,B ) and w.r.t. a corresponding notion of satisfaction for multisets M. As usual we use M ▶ A to say that M satisfies A, and M /▶ A to say that this is not the case.
Definition 3.4 [Reachability Properties] Let mc = (S, P, M0) be a DTMC. The probability of reaching a state satisfying A ∈ AP , starting from M ∈ S, is defined as ReachA,mc(M )= PM ({π ∈ C(M ) | π[i] ▶ A for some i ≥ 0}).









M0
(λ,μ),(1,1),r1
M1	M3


M4




M0
(3/13)
M1
M3	1
M4	1

M2	M5
(δ,η),(1,10),r2
M2
M5	1


Figure 1. The LTS and the corresponding DTMC

Example 3.5 We consider a simple chemical reaction formed by a complexation and a degradation, i.e. a reaction where two molecules X and Y may bind together to form a multimolecular complex XY but where molecule X may be degraded by molecule W . This situation can be formalized by the following environment,
E ::= X = aλ .0+ bδ .0,  Y = a¯μ .XY,  W = ¯bη .W,  XY = 0.
r1	r2	r1	r2
Reagent X may either synchronize with reagent Y along channel a at rate r1 (and produce XY ) or it may synchronize with reagent W along channel b at rate r2 (and produce W ).
By examining the evolution of the system for the initial solution M0 we obtain the LTS (depicted in Fig.1) where 5 ,
M0 = {(2,X), (2,Y ), (10,W )} M1 = {(1,X), (1,Y ), (10,W ), (1,XY )} M2 = {(1,X), (2,Y ), (10,W )} M3 = {(2,XY ), (10,W )}  M4 = {(1,XY ), (1,Y )(10,W )} M5 = {(2,Y ), (10,W )}

All the transitions are obtained by rule Sync; as an example we comment the
(λ,μ),(2,2),r1
case of M0. Transition M0 −−−−−−−−→ M1 models the binding, i.e. the synchro-
nization between X and Y along channel a. The transition records the labels of the basic actions and the multiplicities of reagents X and Y , respectively, and the
(δ,η),(2,10),r2
rate r1. By contrast, transition M0 −−−−−−−−→ M2 models the degradation, i.e the
synchronization between X and W along channel b.
If we assume r1 = 3r2, showing that the complexation is three times faster than the degradation, we derive the DTMC, depicted in Fig.1 6 . In order to infer relevant information about our biological system, it would be convenient to compute the probability of reaching a state where a given quantity of complexes XY appear. As an example, we consider the probability to reach a state where at least two bindings XY are created, i.e., the probability that no degradation will take place and that all the reagents X will bind to reagents Y . Since only state M3 (depicted in bold) satisfies the previous requirement the probability to reach state M3 will be (3/8) · (3/13) = (9/104). This shows that even if the rate of the binding is (three times) greater that the one of degradation, the concentration of reagent W makes the degradation more likely to happen than the binding of reagent X and Y . 

5 We do not to indicate transitions (and consequently states) where a zero multiplicity may appear.
6 As usual, we picture an arrow between M and M ' provided that the probability is greater than zero.

Abstract LTS
We define the abstract LTS semantics for CGF.
Abstract states. We present the abstract states and we formalize the relation with multisets as a standard Galois connection [9]. In order to approximate the information related to the multiplicities of reagents present in a solution we adopt the well-know domain of intervals of integers [7]. In particular, let I = {[m, n] | m ∈ N,n ∈ N ∪ {∞} ∧ m ≤ n}.
Over intervals we consider the standard union ∪◦ and the induced order ≤I , defined as follows,
I∪◦J = [min(a, c), max(b, d)] for I = [a, b],J = [c, d];  (ii) I ≤I J iff I∪◦J = J .
The abstract states are defined in the obvious way by replacing multiplicities with intervals of multiplicities.
Definition 4.1 [Abstract states] An abstract state is a function M ◦ : X → I. We use M◦ for the set of abstract states.
Notice that a multiset M ∈ M has a corresponding abstract state, where each multiplicity, such as n, is replaced by the exact interval [n, n]. In the following, we may write M ◦ for the abstract version of a multiset M ; analogously, we may use S◦ for sets of multisets S.
Since an interval represents a set of multiplicities, it is immediate to define the following approximation order over abstract states.
Definition 4.2 [Order on States] Let M ◦,M ◦ ∈ M◦, we say that M ◦±◦M ◦ iff, for
1	2	1	2
all reagent X ∈ X , M ◦(X) ≤I M ◦(X).
1	2
The relation between multisets and abstract states is formalized as a Galois con- nection. The abstraction function α : P(M) → M◦ reports the best approximation for each set of multisets S. Formally, we have to take the l.u.b. (denoted by H◦) of the abstraction of each multiset M ∈ S. Its counterpart is the concretization func- tion γ : M◦ → P(M) that reports the set of multisets represented by an abstract state.
Definition 4.3 We define α : P(M) → M◦ and γ : M◦ → P(M) such that,
for each S  ∈  P(M) and M ◦  ∈  M◦:  (i) α(S) = .◦	M ◦; (ii) γ(M ◦) = {M '  |

M '◦
±◦M
}.
M ∈S

The abstraction and concretization functions give a Galois connection.
Theorem 4.4 The pair (α, γ) is a Galois connection between (P(M), ⊆) and (M◦, ±◦).
Abstract LTS. We give the domain of abstract LTS including a notion of ordering for expressing precision and correctness of approximations, in the style of [11].
The abstract transitions are defined analogously to the concrete case by replacing the information about multiplicities with intervals of multiplicities. Thus, we have
◦ Θ,Δ◦,r	◦
M −−−−→ M
◦

where Θ ∈ L, Δ◦ ∈ Q◦ = I ∪ (I × I), with arity(Θ) = arity(Δ◦).
Analogously as in the concrete case the outgoing transitions from an abstract state M ◦ have distinct labels.
Definition 4.5 [Abstract LTS] An abstract labeled transition system is a tuple (S◦, →◦,M ◦, E) where: (i) S◦ ⊆ M◦ is a set of abstract states and M ◦ ∈ S◦ is
0	0
the initial state; (ii) and, →◦⊆ S◦ × L^ × Q^◦ × R+ × S◦ is a set of abstract tran-

sitions, such that for each M ◦
Δ◦ = Δ◦ and M ◦ = M ◦.
Θ,Δ◦,r1
−−−−1−→
◦
M ◦, M ◦
Θ,Δ◦,r2
−−−−2−→
◦
we have r1 = r2,

1	2	1	2
In the following we use LT S◦ to denote the set of abstract LTS. We also assume that the notations defined for LTS are adapted in the obvious way.
We introduce the notion of best approximation of an LTS. The most precise abstract LTS can obviously obtained by replacing, both for states and transitions, the multiplicities with the most precise intervals.
Definition 4.6 [Best Abstraction] We define αlts : LT S → LT S◦, such that

◦	◦	◦
7	◦	◦ Θ,Δ◦,r	◦
Θ,Δ,r

αlts((S, →, M0, E)) = (S
M1 ∈→}.
, → , M0
, E) where
→ = {M
−−−−→ M | M −−−→
◦

Notice that αlts does not effectively introduce any approximation. For expressing the correctness of an abstract LTS with respect to a concrete one, we introduce an
approximation order ±◦ . Intuitively, lts◦ is a correct approximation of lts provided
that αlts(lts)±◦ lts◦.
For these purposes, we assume to extend the order ≤I over intervals to pairs of intervals. Given Δ1◦, Δ2◦ ∈ Q^◦ we define Δ1◦ ≤I Δ2◦ component-wise.
Definition 4.7 [Order on abstract LTS] Let lts◦ = (S◦, →i ,M ◦ , E) with i ∈ {1, 2}
i	i	◦	0,i
be two abstract LTS. For M ◦ ∈ S◦,M ◦ ∈ S◦, we say that M ◦ “ M ◦ (M ◦ simulates

1	1	2	2
1	2	2

M ◦) iff: (i) M ◦±◦M ◦,; (ii) for each t◦ = M ◦
Θ,Δ1 ,r
N ◦ ∈→1 there exists t◦ =

1	1	2
Θ,Δ◦,r
1	1 −−−−→	1	◦	2

M ◦ −−−−2→ N ◦ ∈→2, such that Δ◦ ≤I Δ◦ and N ◦ “ N ◦.

2	◦	2	◦
1	2	1	2

We say that lts◦ ±◦ lts◦ if M ◦ “ M ◦ .

1	lts	2
0,1
0,2

As expected, the definition of the order for abstract LTS is based on a simulation

between abstract states.  More in details, M ◦
simulates M ◦
◦
whenever: (i) M ◦

approximates M ◦; (ii) each transition of M ◦, such as M ◦
Θ,Δ1 ,r
N ◦, is matched

1
Θ,Δ◦,r
1	1 −−−−→	1

by a transition M ◦ −−−−2→ N ◦. Notice that it must be the case that the transitions
2	◦	2
are related to the same label Θ and that Δ◦ ≤I Δ◦, showing that the information
1	2
about multiplicities is properly approximated.
The analysis. We define an effective method to derive an abstract LTS that safely approximates the concrete one. The abstract transition relation for abstract states is defined by the rules Table 3 (as previously, we are tacitly assuming to

7 We assume that Δ◦ is the best abstraction of Δ, derived component-wise in the obvious way.

reason with respect to a given environment E). The rules are adapted from the concrete ones by replacing multiplicities with intervals of multiplicities, and exploit the abstract counterpart of the concrete operation ⊕ and g. The abstract operations are defined as follows:
M ◦⊕◦N ◦(X)= M ◦(X)+ N ◦(X),  I + J = [min(I)+ min(J ), max(I)+ max(J )]
M ◦g◦N ◦(X)= M ◦(X) − N (X),  I − J = [min(I)−min(J ), max(I)−max(J )]
b	b
for the abstract LTS, obtained for the initial abstract state M ◦ by transitive closure.



E.X.λ = τrλ.Q
◦

(Delay-abs)

◦ λ,M (X),r	◦ ◦	◦	◦
M −−−−−−−−→◦ (M g {([0, 0],X)})⊕ [[Q]]


E.X.λ = arλ.Q1	E.Y.μ = a¯rμ.Q2
◦	◦

(Sync-abs)

◦ (λ,μ),(M (X),M (Y )),r	◦  ◦
◦	◦ ◦	◦

M −−−−−−−−−−−−−−−−−→◦ (M
g {([1, 1],X), ([1, 1],Y )})⊕ [[Q1]]
⊕ [[Q2]]



Table 3
Abstract transition relation

The following theorem shows that the abstract LTS computed for an abstract state M ◦ is a correct approximation of the LTS, for each M represented by M ◦.
Theorem 4.8 Let E be an environment and M ◦ ∈ ł◦ be an abstract state. For each multiset M ' ∈ γ(M ◦), we have αlts(LTS((E, M '))) ±lts LTS◦((E, M ◦)).
For a sake of simplicity we proposed an approximation which admits infinite abstract LTS. Note, however, that further approximations able to deal with infinite systems can be easily derived by means of widening operators [10]. In this context, for example, it is sufficient to replace the abstract operator ⊕◦ by its parametric
version ⊕◦, which, given k ∈ N, is defined as follows


◦ ◦  ◦
( [min(M ◦(X)) + min(N ◦(X)), ∞]  if max(M ◦(X)) + max(N ◦(X)) > k,



Interval Markov Chains
We adopt the model of Interval Markov Chains (IMC) proposed in [14,20] in order to abstract DTMC. The correctness and precision of the abstraction are formalized, similarly as in [12,13,14], by introducing a notion of best abstraction and an order over IMC.
Definition 5.1 An Interval Discrete-Time Markov Chain (IMC) is a tuple
(S◦, P−, P+,M ◦) where: (i) S◦ ⊆ ł◦ is a ﬁnite or countable set of abstract states and M0◦ ∈ S◦ is the initial state; (ii) and, P−, P+ : S◦ → SDistr(S◦) are the lower and upper probability functions, such that for all M1◦, M2◦ ∈ S◦, P−(M1◦)(M2◦) ≤ P+(M1◦)(M2◦).
In the following we use łC◦ to denote the set of IMC. The IMC model combines together non-deterministic and probabilistic steps similarly as in Markov Decision

Process (MDP). Here, P−(M1◦)(M2◦) and P+(M1◦)(M2◦) define intervals of prob- abilities, that represent lower and upper bounds for the transition probabilities of moving from M1◦ to M2◦. Thus, for each abstract state there is a choice for the distribution yielding the probability to reach any other state. As usual, the non- determinism is resolved by a scheduler that chooses an admissible distribution for each step.
Definition 5.2 Let mc◦ = (S◦, P−, P+,M ◦) be an IMC and let M ◦ ∈ S◦. We say that a distribution ρ ∈ Distr(S◦) is admissible for M ◦ iff, for each M '◦ ∈ S◦, P−(M ◦)(M '◦) ≤ ρ(M '◦) ≤ P+(M ◦)(M '◦). We use ADistrmc◦ (M ◦) for the admissible distributions for M ◦.
The notion of path and cylinder for IMC are analogous to that presented for DTMC in Section 3. We therefore use the same notation.
Definition 5.3 [Scheduler] Let mc◦ = (S◦, P−, P+,M ◦) be an IMC. A scheduler is a function Π: FPaths(S◦) → Distr(S◦) such that Π(π) ∈ ADistrmc◦ (πlast) for each path π ∈ FPaths(S◦). We use Adv(mc◦) to denote the set of schedulers.
Given a scheduler Π ∈ Adv(mc◦) a probability space over paths can be defined analogously as for DTMC (see Definition 3.3). Thus, we use PΠ ◦ for the probability
starting from the abstract state M ◦ w.r.t. the scheduler Π.
In order to define the abstract validation of probabilistic reachability properties, we introduce a may and must notion of satisfaction for abstract states and proposi- tional symbols. We say that an abstract state M ◦ must satisfy A, M ◦ ▶∀ A, iff for each M ∈ γ(M ◦) we have M ▶ A. Analogously, an abstract state M ◦ may satisfy A, M ◦ ▶∃ A, iff there exists M ∈ γ(M ◦) such that M ▶ A.
An IMC gives both under and over approximations of the probability of reach- ability properties, that can be computed by considering the worst and best proba- bilities w.r.t. all the schedulers.
Definition 5.4 [Reachability Properties] Let mc◦ = (S◦, P−, P+,M ◦) be an IMC. The lower and upper bound of the probability of reaching a state satisfying A ∈ AP , starting from M ◦ ∈ S◦, are defined as follows


Reach−	◦ (M ◦)=	inf
PΠ ◦ ({π◦ ∈ C(M ◦) | π◦[i] ▶∀ A for some i ≥ 0})

A,mc
Π∈Adv(mc◦) M

Reach+	◦ (M ◦)=	sup
PΠ ◦ ({π◦ ∈ C(M ◦) | π◦[i] ▶∃ A for some i ≥ 0})

A,mc
M
Π∈Adv(mc◦)

We define a function αMC : łC → łC◦ that gives the best abstraction of a DTMC. Since there is no effective abstraction of states, similarly as for LTS, the derived probabilities are exact.
Definition 5.5 [Best Abstraction] We define αMC : łC → łC◦ such that
αMC((S, P, M0)) = (S◦, Pα−, Pα+, M0◦) where Pα−(M1◦, M2◦)= Pα+(M1◦, M2◦)= 
P(M1)(M2), for each M1, M2 ∈ S.

Effective approximations of a DTMC mc can be introduced by considering an

IMC mc◦ such that αMC(mc)±◦
mc◦, where ±◦
is defined as follows.

Definition 5.6 [Order on IMC] Let mc◦ = (Si◦, P−, P+,M ◦ ) be two IMC for i ∈
i	i	i	0,i
{1, 2}. Given two abstract states Mi◦ ∈ Si◦ for i ∈ {1, 2}, we say that M1◦“mcM2◦
( M2◦ simulates M1◦) iff
M1◦±◦M2◦;
for each distribution ρ1 ∈ ADistr(M1◦) there exists a function H : S1◦ → S2◦
and a distribution ρ2 ∈ ADistr(M2◦) such that,

for each M ◦ ∈ S2◦, ρ2(M ◦)= ΣM

'◦∈H

−1(M ◦)
ρ1(M '◦).

for each M '◦ ∈ S1◦, if H(M '◦)= M ◦ then M '◦ “mc M ◦.

Moreover, we say that mc1◦ ±◦
mc2◦ iff M ◦
“mc M ◦ .

The order uses a sort of probabilistic simulation similarly as in [14,13]. Intu- itively, M2◦ simulates M1◦ whenever: (i) M2◦ approximates M1◦: (ii) each dis- tribution of M1◦ is matched by a corresponding distribution of M2◦, where the probabilities of the target states are eventually summed up.
The simulation “mc provides sufficient conditions for the preservation of ex- tremum probabilities, as stated by the following theorem.
Theorem 5.7 (Soundness of the order) Let mc◦ = (Si◦, P−, P+,M ◦ ) be two
i	i	i	0,i
IMC and let Mi◦ ∈ Si◦ be two abstract states, for i ∈ {1, 2}. If M1◦ “mc M2◦, then for each propositional symbol A ∈ AP , we have


Reach−
(M2◦) ≤ Reach−
(M1◦) ≤ Reach+
(M1◦) ≤ Reach+
(M2◦)

A,mc2
A,mc1
A,mc1
A,mc2

Derivation of IMC
We define the abstract counterpart of the probabilistic translation function H : LT S → łC. Our abstract LTS reports on transitions information about the label of the process (the labels of the processes) that participate to the move, the interval (the intervals) representing a possible range for its (their) multiplicities, and the rate of the basic action. Therefore, it should be well understood that the abstract rate associated to each transition is an interval of rates. ¿From this kind of information, both lower and upper bounds for the probabilities of moving from an abstract state to another could obviously be calculated.
It is convenient, however, to maintain the calculation of the intervals of rates symbolic in order not to lose relational information on quantities of different oc- currences of the same reagent. This means that the interval of rates assigned to each abstract transition will be represented by a symbolic expression on reagent variables. More in details, we adopt expressions such as (e, c) where: (i) e ∈ Z is a symbolic expression over variables X ; (ii) c ∈ C is a set of membership constraints of the form X ∈ I. We require that each expression (e, c) is well-formed meaning that, for each variable X occurring in e, there exists one and only one constraint X ∈ I occurring in c.

Hence, we define the abstract rate of a transition as follows. Given a transition
◦	◦ Θ,Δ◦,r	◦

t = M
−−−−→ M we have
◦
8>< (X · r, {X ∈ I})	Θ = λ, λ ∈ L(E.X) ∧ Δ◦ = I,

rate◦(t◦)= 
(X · (X−1) · r, {X ∈ I})	Θ = (λ, μ) ∧ Δ = (I, I) ∧ λ, μ ∈ L(E.X),
(X · Y · r, {X ∈ I1,Y ∈ I2}) Θ = (λ, μ) and Δ◦ = (I1, I2) ∧ λ ∈ L(E.X),μ ∈ L(E.Y ) ∧ X /= Y.

Moreover, we introduce the functions R◦ : S◦ × S◦ → Z × C, and E◦ : S◦ → Z × C, analogously as in the concrete case. We have

R◦(M ◦,M '◦)= Σ◦	◦ rate◦(t◦),	E◦(M ◦)= Σ◦ ◦

R◦(M ◦,M '◦),

t◦∈Ts(M ◦ ,M ' )
(e1, c1)op◦(e2, c2)= (e1 op e2, 

{X ∈  ◦
M ' ∈S◦
I}) for op ∈ {+, /}.


Both lower and upper bounds of the probability of moving from M ◦ to M '◦ can be determined by R◦(M ◦,M '◦) and by E◦(M ◦).
Definition 6.1 [Derivation of the IMC] We define an abstract probabilistic transla- tion function H◦ : LT S◦ → łC◦ such that H◦((S◦, →◦, M0◦, E)) = (S◦, P−, P+,M ◦), where P−, P+ : S◦ → SDistr(S◦) are the lower and upper probability functions, such that for all M1◦ ∈ S◦
if max(E◦(M ◦)) = 0, then P+(M ◦)(M ◦)= P−(M ◦)(M ◦) = 0, for each M ◦ /=
1	1	2	1	2	1
M ◦, and P+(M ◦)(M ◦)= P−(M ◦)(M ◦)= 1;
2	1	1	1	1
if max(E◦(M ◦)) > 0 then
if min(E◦(M ◦)) = 0 then P+(M ◦)(M ◦)=1 and P−(M ◦)(M ◦)= 0,
1	1	1	1	1
for each M ◦, if min(R◦(M ◦,M ◦)) = 0 then P−(M ◦)(M ◦)=0 else
2	1	2	1	2
P−(M ◦)(M ◦)= min(R◦(M ◦,M ◦)/◦E◦(M ◦)),
1	2	1	2	1
for each M ◦, if max(R◦(M ◦,M ◦)) = 0 then P+(M ◦)(M ◦)= 0 else
2	1	2	1	2
P+(M ◦)(M ◦)= max(R◦(M ◦,M ◦)/◦E◦(M ◦)).
1	2	1	2	1
Intuitively, the lower and upper bound probabilities for the move from M ◦ to M '◦ are computed by minimizing and maximizing the solution of R◦(M ◦,M '◦)/◦E◦(M ◦), respectively. This reasoning has to be properly combined with the special cases when max(E◦(M ◦)) = 0 or min(E◦(M ◦)) = 0. When max(E◦(M ◦)) = 0 all the states represented by M ◦ are stable, while when min(E◦(M ◦)) = 0 a state represented by M ◦ is stable.
Note that in order to find the maximum and minimum of an expression (e, c) ∈ Z × C, when it’s not trivial, it’s sufficient to evaluate the expression e for the stationary points (that can be found by differentiate e and by setting the result equal to 0) and for the boundaries of the intervals in c constraining variables of e.
The following theorems state the soundness of our approach.

Theorem 6.2 Let lts◦
= (Si◦, →i◦, M0,i◦) be two abstract LTS. If lts◦ ±◦
lts◦,

then also H◦(lts◦) ±◦
i
H◦(lts◦).
1	lts	2

1	mc	2
Theorem 6.3 Let E be an environment and M0 ∈ ł be a multiset. We have
αMC(H(LTS((E, M0)))) ±◦  H◦(αlts(LTS((E, M0)))).

◦	(λ,μ),[0,1],[0,3],r1	◦
3	6


◦	◦
1	7


◦	◦
M	4
◦	◦
2	8
M ◦	◦
5	(δ,η),[0,1],[1,10],r2	9

Figure 2. The abstract LTS

Example 6.4 By considering the chemical reaction described by the environment E of Example 3.5, we show the ability of our analysis for predicting the proba- bilistic behavior of the reaction, described by E, w.r.t. different initial concen- trations. Thus, we describe the abstraction for the initial abstract state M ◦ =
{([2, 3],X),([2, 5],Y ), ([1, 10],W )}, modeling simultaneously a set of experiments.
The obtained abstract LTS and IMC are depicted in Fig.2 and 3 (assuming
r1 = 3r2), where
M ◦ = {([1, 2],X), ([1, 4],Y ), ([1, 10],W ), ([1, 1],XY )}  M ◦ = {([1, 2],X), ([2, 5],Y ), ([1, 10],W )}
1 ◦	◦
M	= {([0, 1],X), ([0, 3],Y )([1, 10],W ), ([2, 2],XY )} M	= {([0, 1],X), ([1, 4],Y ), ([1, 10],W ), ([1, 1],XY )}

3
M5◦ = {([0, 1],X), ([2, 5],Y ), ([1, 10],W )}  M6◦
4
= {([0, 2],Y )([1, 10],W ), ([3, 3],XY )}

M7◦ = {([0, 3],Y )([1, 10],W ), ([2, 2],XY )}  M8◦ = {([1, 4],Y )([1, 10],W ), ([1, 1],XY )}
M9◦ = {([2, 5],Y )([1, 10],W )}
It is convenient to consider again the probabilistic reachability previously dis- cussed. We consider the minimum and maximum probabilities (denoted by P−(M ◦) and P+(M ◦), respectively) to reach, from M ◦, a state where at least two binding XY appear.
The states that contain at least two occurrences of XY are M3, M7 and M6.







[(18/48),(45/48)]






◦
1
[0,1]
 
◦
3


[0,(9/10)]


[1,1]

[1,1]


M ◦

[(1/16),(15/24)]






2

[(1/16),(5/8)]
[0,(12/13)]

4


◦
8


◦




[1,1]



M5	M9	[1,1]
[0,1]
Figure 3. The IMC

As a consequence, we have P−(M ◦) = P−(M ◦) = P−(M ◦) = 1, and P+(M ◦) = 
3	6	7	3
P+(M ◦)= P+(M ◦) = 1. Moreover, we have also P−(M ◦)= P−(M ◦)= P−(M ◦)= 
6	7	8	9	5
0 and P+(M ◦)= P+(M ◦)= P+(M ◦)= 0.
8	9	5
Let us consider the lower bound.	The most important observation is that

P−(M ◦) = 0 since there is a self-loop that allows state M ◦
not to reach state

4
M ◦. Thus, we have also P−(M ◦) = 0. Moreover, for M ◦
4
we have to consider

7	2	1
the admissible distributions ρ that minimize ρ(M ◦)(M3◦) · P−(M ◦)+ ρ(M ◦)(M4◦) ·
1	3	1
P−(M ◦) = ρ(M ◦)(M3◦).  Thus, we derive P−(M ◦) = (3/13), and analogously
4	1	1
P−(M ◦)= (18/48) · P−(M ◦)+ (30/48) · P−(M ◦)= (18/48) · (3/13).
1	2
For the upper bound we obtain in a similar way P+(M ◦)= (45/48) · P+(M ◦)+ 
(3/48) · P+(M ◦)= (45/48) · ((12/13) + (1/13) ·(12/13)) +(3/48) · ((15/16) ·(12/13)).
The most relevant difference is that in this case P+(M ◦)= (12/13) by maximizing
the probability of moving from M ◦ to M ◦.
4	7
Finally, we consider the probability of reaching a state where at least three
bindings XY are created. In this case, only state M ◦ satisfies the requirement, and we obtain P−(M ◦)=0 and P+(M ◦)= (45/48) · (12/13) · (9/10).
It is worth noticing that the result of our analysis is very accurate. In fact, for the reachability properties previously considered both the lower and the upper bound correspond to the concrete probability of one of the experiments represented by the abstract initial state M ◦. For example, the lower bound of the probability that we reach a state where at least two binding XY are created is (18/48) · (3/13), e.g. the probability of the “worst case’ ’ concrete experiment we have illustrated in Example
3.5. This result could not be achieved without the relational information about the number of occurrences of reagents, that we profitably exploit for computing the intervals of probabilities.	 

Conclusions and Related Work
A few abstraction methods for probabilistic and non-deterministic systems, modeled as DTMC or MDP, have been recently proposed. The proposals of [12,13,14] present similar approaches, based on MDP and IMC, respectively. The abstract model is built over a partition of the concrete state space by computing the abstract proba- bilities from the concrete probabilities. As a consequence, these approaches handle finite state systems only. Huth [20] proposes a more general approach based on IMC where the abstraction of states is formalized using a sort of abstract interpretation. Even if the framework admits infinite state systems, no effective methods for deriv- ing an abstract model for a given language is investigated. The technique of [22] extends the approaches of [12,13], using games, in order to more accurately abstract MDP. De Alfaro [1] proposes an original method for the abstraction of finite state MDP, based on regions. Monnieux [24] proposes an approximation method, based on abstract interpretation, for the validation of trace properties of probabilistic and non-deterministic transition systems. Techniques of backward and forward analysis are successfully applied.
By contrast, our approach introduces an effective method (even for infinite state

systems) for computing an abstraction of the DTMC, based on the abstraction of the LTS. As it is outlined in Example 6.4, such an abstraction is particularly adequate for achieving lower and upper bounds on the concrete probability of a set of experiments, represented by the initial abstract multiset.
A great advantage of our framework, based on abstract interpretation, is that new analyses could be designed by introducing new abstract LTS semantics. For example, the domain of intervals could be replaced with more precise numerical domains able to model also relational information, such as the domain of convex polyhedra. Moreover, we would like to investigate a parametric version of our framework where the partitioning of intervals, and thus of abstract states, could be realized in a coarser or finer way. This could give the possibility to find a trade-off between precision and complexity and also to study refinement techniques, guided by the formula .We also believe that the methodology proposed for CGF could be extended to the full calculus with communication [28].

References
L. de Alfaro and Pritam Roy. Magnifying-Lens Abstraction for Markov decision Process. Proc. of CAV ’07, LNCS 4590, 325–338, 2007.
C. Bodei, P.Degano, F.Nielson and H.Riis Nielson. Static Analysis for the Pi-Calculus with Applications to Security. Information and Computation, 168: 68-92, 2001.
L. Cardelli. Brane Calculi. Proc. of CMSB ’04, LNCS 3082, 257–278, 2004.
L. Cardelli. On Process Rate Semantics. Theoretical Computer Science, 391 190–215, 2008.
N. Chabrier, Marc Chiaverini, Vincent Danos and F. Fages. Modeling and Querying Biomolecular Interaction Networks. Theoretical Computer Science 325(1), 25-44, 2004.
M. Clarke, O. Grumberg and E. Long. Model Checking and Abstraction. TOPLAS, 16(5), 1512-1542, 1994.
P. Cousot and R. Cousot. Static Determination of Dynamic Properties of Programs. Proc. of POPL’76
, 106–130, 1976.
P. Cousot and R. Cousot. Abstract Interpretation: A Unified Lattice Model for Static Analysis of Programs by Construction or Approximation of Fixpoints. Proc. of POPL’77, 238–252, 1977.
P. Cousot and R. Cousot. Systematic Design of Program Analysis Frameworks. Proc. of POPL’79 , 269–282, 1979.
P. Cousot and R. Cousot. Comparing the Galois Connection and Widening/Narrowing Approaches to Abstract Interpretation. Proc. of PLILP’92, LNCS 631, 269–295, 1992.
D. Dams, R. Gerth and O. Grumberg. Abstract Interpretation of Reactive Systems. TOPLAS, 19(2), 253-291, 1997.
P. D’Argenio, B. Jeannet, H. Jensen and K. Larsen. Reachability Analysis of Probabilistic Systems by Successive Refinements. Proc. of PAPM-PROMIV’01, LNCS 2165, 39–56, 2001.
P. D’Argenio, B. Jeannet, H. Jensen and K. Larsen. Reduction and Refinement Strategies for Probabilistic Analysis. Proc. of PAPM-PROMIV’02, LNCS 2399, 57–76, 2002.
H. Fecher, M. Leucker and V. Wolf. Don’t Know in Probabilistic Systems. Proc. of SPIN’06, LNCS 3925, 71–88, 2006.
J. Feret. Occurrence Counting Analysis for the pi-calculus. ENTCS 39 (2), 2001.
J. Feret. Abstract Interpretation-Based Static Analysis of Mobile Ambients. Proc. of SAS’01, LNCS 2126, 412-430, Springer Verlag, 2001.
R.Gori and F. Levi. A new occurrence Counting analysis for BioAmbients. Proc. of APLAS ’05, LNCS 3780, 381–400, 2005.


R.Gori and F. Levi. An Analysis for proving Temporal Properties of Biological Systems. Proc. of APLAS ’06, LNCS 4279, 234–252, 2006.
A. Hinton, M. Kwiatkowska, G. Norma and D. Parker. PRISM: a tool for automatic verification of probabilistic systems. Proc. of TACAS’06, LNCS 3920, 441-444, Springer-Verlag, 2006.
M. Huth. On finite-state approximants for probabilistic computation tree logic. Theoretical Computer Science, 346(1), 113–134, 2005.
M. Kwiatkowska. Model checking for probability and time: from theory to practice. Proc. of LICS’ 03, 351–360, 2003.
M. Kwiatkowska, G. Norman and D. Parker. Game-based Abstraction for Markov Decision Processes.
Proc. of QEST’06, 157–166, 2006.
F. Levi and S. Maffeis. On Abstract Interpretation of Mobile Ambients. Information and Computation 188, 179–240, 2004.
D. Monnieaux. Abstract interpretation of programs as Markov Decision Processes. Science of Computer Programming, 58(1-2), 179–205, 2005.
F. Nielson, H.R. Nielson, R.R. Hansen. Validating firewalls using flow logics. Theoretical Computer Science, 283(2), 381-418, 2002.
F. Nielson, H.R. Nielson and H. Pilegaard. Spatial Analysis of BioAmbients. Proc. of SAS’04, LNCS 3148, pp. 69–83, Springer-Verlag, 2004.
A. Phillips and L. Cardelli. A Correct Abstract Machine for the Stochastic Pi-calculus. Proc. of BioCONCUR ’04, ENTCS, 2004.
A. Phillips and L. Cardelli. Efficient, Correct Simulation of Biological Processes in the Stochastic Pi-calculus. Proc. of CMSB ’07, LNCS 4695, 184–199, 2007.
H. Pilegaard, F. Nielson and H.R. Nielson. Static Analysis of a Model of the LDL Degradation Pathway.
Proc. of CMSB’05, 2005.
C.Priami. Stochastic π-calculus. The Computer Journal, 38, 578–589,1995.
C.Priami and P. Quaglia. Beta binders for biological interactions. Proc. of CMSB’04, LNCS 3082,20– 33,2005.
C. Priami, A. Regev, W. Silverman and E. Shapiro. Application of a stochastic name-passing calculus to representation and simulation of molecular processes. Information Processing Letters, 80 (1), 25–31, 2001.
A. Regev, E. M. Panina, W. Silverman, L. Cardelli and E. Shapiro. BioAmbients: an Abstraction for Biological Compartments. Theoretical Computer Science, 325, 141–167, 2004.
A. Regev, W. Silverman and E. Shapiro. Representation and Simulation of Biochemical Processes using the pi-calculus process algebra. Proc. of the Pacific Symposium on Biocomputing 2001, 6, 459– 470, 2001.
K. Sen, M. Viswanathan and G. Agha. Model Checking Markov Chains in the Presence of Uncernainties. Proc. of TACAS’06, LNCS 3920, 394-410, 2006.
