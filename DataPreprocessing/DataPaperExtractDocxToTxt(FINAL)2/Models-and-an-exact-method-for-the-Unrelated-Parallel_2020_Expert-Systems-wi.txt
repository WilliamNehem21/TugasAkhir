

		


Models and an exact method for the Unrelated Parallel Machine scheduling problem with setups and resources
Luis Fanjul-Peyro
Grupo de Sistemas de Optimización Aplicada, Instituto Tecnológico de Informática, Ciudad Politécnica de la Innovación, Edifico 8G, Acc. B, Universitat Politècnica de València, Camino de Vera s/n, València 46021, Spain


a r t i c l e	i n f o	a b s t r a c t

	

Article history:
Received 11 May 2019
Revised 2 October 2019
Accepted 8 January 2020
Available online 10 January 2020

Keywords:
Scheduling Parallel machines
Sequence dependent setup times Additional resources
Makespan
This paper deals with the Unrelated Parallel Machine scheduling problem with Setups and Resources (UPMSR) with the objective of minimizing makespan. Processing times and setups depend on machine and job. The necessary resources could be: specific resources for processing, needed for processing a job on a machine; specific resources for setups, needed to do the previous setup before a job is processed on a machine; shared resources, understanding these as unspecific resources that could also be needed in both processing or setup. The number of scarce resources depends on machine and job. As an industrial example, in a plastic processing plant molds are the specific resource for processing machines, cleaning equipment is the specific resource for setups and workers are the unspecific shared resource to oper- ate processing machines and setup cleaning equipment. A mixed integer linear program is presented to model this problem. Also a three phase algorithm based on mathematical exact method is introduced. Model and algorithm are tested in a comprehensive and extensive computational campaign. Tests show good results for different combinations of useE of resources and in most cases come to less than 2.7% of gap against lower bound for instances of 400 jobs.

© 2020 The Author. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)



Introduction

In industry, scheduling and a limited number of resources is present in most of its activities. All of this forces to optimize ob- jectives. Literature deals with a great number of different prob- lems. One of the most studied problems is called parallel machine scheduling problem. In this problem, in parallel, a set of jobs have to be processed on a set of machines. In our case, each job is pro- cessed on only one machine without preemption and each ma- chine can process only one job at a time. We call the general case Unrelated in which the processing time depends on the machine where the job is assigned. An example of the Unrelated Parallel Machine scheduling problem (UPM) is a ceramics plant where dif- ferent orders have to be assigned to the different kilns they have. One of the most common objectives for this kind of problem is to minimize maximum completion time, known as makespan or Cmax. Denoted as R//Cmax (Graham,Lawler,Lenstra,& Rinnooy Kan,1979), this problem is NP-Hard even for the case of two identical par- allel machines (Lenstra, Rinnooy Kan, & Brucker, 1977). The most common additional consideration to add to UPM problems is setup

E-mail address: lfpeyro@hotmail.com
time which consists in preparing a machine before a job is pro- cessed on this machine (adjustments, cleaning, reconfigurations, etc.). Scheduling with setups literature is quite extensive. A recent review (Allahverdi, 2015) shows hundreds of papers in the last 10 years that deal with UPM setups. A general case is when setup time depends on the machine and job sequence, namely Unrelated Parallel Machine scheduling problem with sequence dependence Setup times (UPMS), which is considered in this paper. Notice UPM is only an assignment problem (the order of jobs assigned on each machine is not important) but when setups are present (UPMS) it is an assignment and sequencing problem. This makes the problem harder to solve. A recent paper (Fanjul-Peyro, Ruiz, & Perea, 2019) deals with the UPMS problem studying instances up to 1000 jobs. On the other hand, the literature does not usually deal with the issue of resources necessary to do the jobs. In many indus- trial problems, not taking resources into account is not solving the real problem. In this paper it is assumed that a discrete amount of scarce renewable resources are needed. This amount depends on the job and on the machine. The resources are assumed as re- newable, meaning that after their use they are again available and discrete, because there are an integer number of resources, this is similar to papers such as Błazewicz, Lenstra, and Rinnooy Kan


https://doi.org/10.1016/j.eswax.2020.100022
2590-1885/© 2020 The Author. Published by Elsevier Ltd. This is an open access article under the CC BY-NC-ND license. (http://creativecommons.org/licenses/by-nc-nd/4.0/)



(1983) or Słowinski (1980) They are also considered unspecified since there is no pre-fixed job-machine assignment, and dynamic because resources are not fixed for the whole time horizon. Unre- lated Parallel Machine scheduling problem with Resources is called UPMR. Recent papers such as Fanjul-Peyro, Perea, and Ruiz (2017) and Fleszar and Hindi (2018) deal with this UPMR problem with instances up to 1000 jobs.
In this paper on the other hand, we are going to consider setups and different kinds of resources: (1) specific resources for process- ing, needed to process a job on a machine, (2) specific resources for setups, needed to do the previous setup before a job is pro- cessed on a machine, (3) shared resources, understanding these as unspecific resources that could also be needed in both process- ing or setup. As an example, consider a plastic processing plant where cleaning equipment is the specific setup resource, molds are the specific processing resource and workers are the unspe- cific shared resource. Workers are needed to operate the cleaning equipment (setups) and line machines (processing). Another exam- ple is when you have specialized workers as in a ceramics factory where there are setup engineers (setups), technical line workers (processing) and unspecialized helpers that have auxiliary tasks to help in both, processing or setups. Notice that when resources are present, not only assignment and sequencing is needed because it is also necessary to know the time when each job begins since idle time can be present due to resource shortages (referred to in this paper as timing). When considering, for the Unrelated Parallel Machine scheduling problem, the Setup and Resources, it could be called UPMSR. This problem has been seldom studied and not for all the constraint combinations present in this paper.
In this paper, a mathematical integer linear program is intro-
duced to solve all combinations of use of resources (in processing, setups and/or shared), obtaining solutions for up to 50 jobs and 8 machines. An exact algorithm is also presented which adapts the methodology used in the most recent papers mentioned in this section for UPMR and UPMS problems. The algorithm allows us to obtain solutions for up to 400 jobs and 8 machines in tests that in most cases give a gap between solutions and lower bounds (LB) of less than 2.7% for the largest instances.
The rest of the paper is structured as follows: Section 2 reviews the literature related to the problem. In Section 3 problem defini- tion and examples are present. Section 4 introduces the complete mathematical model for UPMSR. In Section 5 a three phase exact algorithm is explained (phase of assignment, sequencing and tim- ing). In Section 6 all computational experiments are shown and fi- nally in Section 7 the conclusion and future research are presented. An appendix shows more models and tests.


Literature review

In the literature UPM is widely studied (see Pinedo, 2016 for some references). However UPMS is comparatively less studied. Different mixed integer linear programs (MILP) have been pro- posed for UPMS. Based on the model presented in Guinet (1991), an improvement of this model can be found in Vallada and
On the other hand, a more specific or simpler version of UPMR has been studied in recent decades. In Błazewicz, Kubiak, Röck, and Szwarcfiter (1987) this problem for identical machine and mini- mization of flow time was studied. A static version of this prob- lem where allocation of resources on machines are fixed for the whole time horizon was proposed, for instance, in Daniels, Hua, and Webster (1999) with identical machines. In that paper, a MILP and heuristics were presented. A dynamic version is also studied in papers as Grigoriev, Sviridenko, and Uetz (2007). Where pro- cessing times depend on the number of allocated resources. A simplified version of UPMR was studied in Edis and Oguz (2011) where resources do not depend on the machine where the job is assigned. Other models and variants were studied in Edis and Ozkarahan (2012) or Edis, Oguz and Ozkarahan (2013). Finally, Fanjul-Peyro, Perea, and Ruiz (2017) presented a model based on strip-packing problems to solve a more complex dynamic and unspecific version of UPMR, the kind of resources used in the present paper. For a survey of two-dimensional packing problems see Lodi, Martello, and Monaci (2002). Recently, in Fleszar and Hindi (2018) a two stage and complete model based on constraint programing (CP) has been presented, reaching, in the two stage case, up to 1000 jobs with a gap with LB less than 1.9%.
Recently, some paper deals with a specific case of parallel ma-
chines scheduling problem with setup and resources. Akyol Özer and Saraç (2019) deal with identical parallel machine with the ob- jective of minimizing total weighted completion times where re- sources are only necessary for processing. Bektur and Saraç (2019) studied the case of a common server as a resource for setups which implies that setups can be made only on one machine at the same time by the common server and setup times do not de- pend on the machine. As we can see, these two are particular ver- sions of the problem we study in this paper, which can be seen as a more general case.
In the current paper we focus on MILP and exact methods, therefore no heuristic nor metaheuristic algorithms are presented leaving these for future research as in recent articles with re- sources (but without setups) such as Zheng and Wang (2016) and Villa, Vallada, and Fanjul-Peyro (2018). But nevertheless, to the best of the author’s knowledge, no paper deals with the complete gen- eralizations used here, that include setups and resources together.


Problem deﬁnition and example

In order to present a complete example we need to define the input data for UPMSR. Superscripts are used in this paper to repre- sent the type of resources and they are not index nor power func- tions.

A set of n jobs to be processed, N = {1,… n}, indexed by j, k.
A set of m machines that can process the jobs, denoted as
M = {1,…, m}, indexed by i.
pij denotes the time needed for machine i to process job j.
sijk denotes the time needed to do the setup on machine i be- tween job j and k.

Ruiz (2011) Later, a more eﬃcient MILP was presented in Avalos-
P
max
are the maximum amount of Processing resources. Some

Rosales, Angel-Bello, and Alvarez (2015) showing eﬃcient MILP. In this last paper mentioned, instances up to 60 jobs and 8 machines are solved. Using this MILP as base for a master problem in an it-
of these are needed specifically for Processing jobs, and cannot be used for machine setups. We denote these resources as P- resources.

erative algorithm, Tran, Araujo, and Beck (2016) obtained solutions
S
max
are the maximum amount of Setups resources. Some of

for instances of UPMS up to 120 jobs. In a recent paper, Fanjul- Peyro, Ruiz, and Perea (2019) got better results than these two pre- viously mentioned papers introducing improvements in model and
these are needed specifically for machine Setups, and cannot be used for job processing. We denote these resources as S- resources.

an exact algorithm. MILP gives solutions for up to 400 jobs and the
H
max
are the maximum amount of unspecific resources that are

new exact algorithm could reach up to 1000 jobs and 8 machines with relative deviation from lower bounds (LB) below 0.8%.
sHared by both, needs of job processing and needs of machine setups. We denote these resources as H-resources.




Fig. 1. UPMS instance and optimal solution.


rP are the resources of P-resources needed to process job j on
able resources are fixed to 6 in this case (rS
= 6). Note that the

machine i.
value of this maximum and the distribution of values of rS
are

S
ijk
are the resources of S-resources needed to do setup be-
different than the previous case since the resources are completely

tween job j and job k on machine i.
rHp are the resources of H-resources needed to process job j on machine i. Processing job j on machine i could require rP of P-resources and rHp of H-resources.
rHp are the resources of H-resources needed to do setup be-
ijk
tween job j and job k on machine i. Doing the setup on ma-
different. The relevant values are highlighted in italics and the rest
of values are fixed to 5.
In Fig. 3 case (a), for time t=4 and t=5 the sum of resources is 8 and, as is shown in the graph below it, this exceeds the maximum available resources that are 6. Graph (b) represents the same solu- tion but without exceeding the maximum value and finally graph
(c) represents the optimal solution with this type of resources. No-

chine i between jobs j and k could require rS
and rHp of H-resources.
ijk
of S-resources
tice that the optimal solution is not exactly the same as in the previous example.
For unspecific resources (H-resources) that can be used by both

The values for pij, sijk

P
max

S
max

H
max
, rP , rHp, rS , rHs are non-
processing and setups, Fig. 4 shows the values of resources neces- sary to do the setups and processing. The maximum available re-

negative integers, satisfying: rP ≤ rP
, rHp ≤ rH
, rS
≤ rS
, rHs ≤
sources for both are fixed to 9 in this case (rH
= 9). The available


H
max
ij	max  ij 
max
ijk
max
ijk
max
resources are shared and this means that resources of this type

A complete instance of 4 jobs and 2 machines are presented. In the first place we show an optimal solution for UPMS and after- wards different types of resources are introduced. Then we found the optimal solution with the type of resources introduced. As is previously mentioned, all resources are renewable and needed dur- ing all processing and setup times.
Firstly in Fig. 1 we show the values of processing time pij and setup time sijk that we use in all these examples and the optimal solution for this UPMS problem is also obtained.
Notice that no relevant setups are fixed to high values (=7) and all diagonal is zero. It is usual to use diagonal to record the initial setup of a job when it exists, i.e. the setup necessary to do on a machine i before the first job j scheduled on this machine is pro- cessed.
In the rest of graphs in this section, the number just above jobs/setups in machine 1 and just below jobs/setups on machine 2 represents the resources needed.
We introduce in Fig. 2 the resources specific to process each
can be used in both. Note that the value of the maximum and the
distribution of values are different than previous cases since the resources are completely different. The used values are in bold and the rest of values are fixed to 8.
In Fig. 4 case (a), for time t=3 the sum of resources is 12 since J1 uses 6 shared resources (H-resources) for processing and for setup in machine 2, after doing J3 and before doing J4, it needs another 6 shared resources. Setups at t=4 and t=5 need a total of 12 H-resources. Finally, for t=6, processing jobs J2 and J4 need a sum of 12 H-resources. As is shown in the graph below it, in all these times the sum exceeds the maximum available resources that are 9. Graph (b) represents the same solution but without ex- ceeding the maximum value and finally graph (c) with a Cmax = 9 represents the optimal solution with this type of resource with a value of maximum completion time of 8.
Finally, the most complex type is when all these types of re- sources are present, i.e. specific for processing (P-resources), spe- cific for setups (S-resources) and unspecific to be shared by both

job (P-resources). In this example, the value of rP
is fixed to 3.
processing and setups (H-resources). In Fig. 5 we use the previous

In Fig. 2 case (a), for time t=1 and t=2 the sum of resources is 4 and exceeds the maximum available resources that are 3 as is shown in the graph below graph (a) with the amount of resources used for each time. In case (b) the same solution (same assignment and sequence) is presented but without excess of use of resources, but with a maximum completion time of 8. However, with a new reassignment and a new sequence, optimal solution with resources is shown in (c), with Cmax equal to 7.
Then, In the Fig. 2 and following figures, graph (a) represents the solution of UPMS with the resources recorder. These graphs a) have an excess of use of resources. Graph (b) shows the same solu- tion (same assignment and sequence) being sure not to exceed the resources limit. Finally, graph (c) represents an optimal solution with these resources. These optimal solutions, in these examples, imply reassigning all jobs in spite of the new assignment having higher values of processing time for all jobs except J1.
For specific setup resources (S-resources), Fig. 3 shows the val- ues of resources necessary to do the setups. The maximum avail-
values used for all types of resources. Above machine 1 and below
machine 2 appears a set of three values that represent in the first place the needs of specific resources for processing (P-resources). In second place and in italics the specific resources needed for se- tups (S-resources) and in third place and in bold the need of un- specific resources shared by both (H-resources). So, the three num- bers above M1 and below M2 represent: P-resources/S-resources/H- resources.
In Fig. 5 case (a) we notice that for t=1 and t=2 the sum of the first values (P-resources) is 4 and this exceeds the maximum for this type of resource that is 3 (rP = 3). In second place in italics both values are zero since these represent the needs of spe- cific resources for setup (S-resources) with rS = 6 and in these times machines are only processing jobs. In third place, the shared resources (H-resources) add up to 9 and does not exceed the max- imum for this type of resource that is 9 (rH = 9). In t=3 pro- cessing of J1 in machine 1 is simultaneous with the setup on machine 2 after doing J3 and before starting J4 (s234 ). In this time




Fig. 2. Problem with specific processing resources.


Fig. 3. Problem with specific setup resources.


t=3 the first term adds up to 2 (processing), the second term adds up to 4 (setups) and the third term adds up to 12 (shared). This last term exceeds the maximum for this type of resource (=9). For t=4 and t=5 the sums of the three terms are respectively 0, 8, 12 with the limits of 3, 6, 9. Finally for t=6 the sums are 2, 0, 12. In case (b) the same solution is represented taking into account not to
exceed the different limits of types of resources with a Cmax = 11 and finally case (c) shows the optimal solution with all types of resources with a Cmax = 9.
Notice that all optimal solutions found for the different types of resources are different in these examples, showing that each prob- lem could have different results.




Fig. 4. Problem with unspecific resources shared by processing and setup.



Fig. 5. Problem with specific processing resources, specific setup resources & unspecific resources used by both.




Model for UPM with setups and resources
In this section we formally describe the model used to solve the Unrelated Parallel Machine scheduling problem with Setups and Resources (UPMSR). As aforementioned, the UPMSR problem con-
being processed (and consequently starts at C j − pij ). In Fig. 2 case (a), completion times of jobs are C1 = 3, C2 = 6, C3 = 2, C4 = 6. Sim- ilarly, for resources, RP represents the point or coordinate on the
P-resources axis where job j has its finish or upper limit of use of resources (and consequently its start or lower limit is RP − rP ).

j	ij 

tains elements of the problem with only setups (and no resources)
In Fig. 2 case (a), the coordinates of resources are RP = 2, RP = 1,

-UPMS- and elements of the problem with only resources (and no	P	P	1	2

setups) -UPMR-. In particular, the MILP for the UPMS problem was widely studied and in a recent paper (Fanjul-Peyro, Ruiz, & Perea, 2019), the MILP used gives solutions up to 400 jobs and an exact method based on that MILP give solutions up to 1000 jobs and 8 machines with less than 0.8% of gap against Lower Bound. So, an adaptation of this MILP has been used in this paper. In the case of the UPMR problem, a packing-based model of Fanjul-Peyro, Perea, and Ruiz (2017) has been used recently. In this paper we improve and adapt this model.
In the UPMR problem, there are two dimensions, the comple- tion times of processing jobs and the resources needed for the processing of the jobs. In Appendix A an example of this can be reviewed. Completion time of each job j is denoted by Cj which
sets the point or coordinate on the time axis when job j is finished
R3 = 4, R4 = 2. However, for the complete UPMSR problem we
need to take into account not only two factors but six: the pro-
cessing times, the setup times, the P-resources, the S-resources, the H-resources used for job processing and the H-resources used for machine setups.
For formal definition of variables in this paper we use small letters to refer to data and capital letters to refer to variables. The variables used are the following:

Xijk = 1 if job k is processed immediately after job j on machine
i, zero otherwise.
Yik = 1 if job k is processed on machine i, zero otherwise.
Cj ≥ 0 is the completion time of job j (coordinate where pro- cessing of job j finishes in the time dimension).



Bj ≥ 0 is the completion time of the setup before the processing of job j on the corresponding machine (coordinate where setup of job j finishes in the time dimension).
UPMSR since idle times can be present and therefore we need to know finishing times and not only the schedule.
- Completion times for setup and processing

P	P

R j ∈ [0, rmax] is the coordinate of processing of job j in the
P-resources dimension (the specific resources employed in the processing of job j).
Bk ≤ Ck
—	Yik
i∈M
pik
,  ∀k ∈ N,	(10)

RS ∈ [0, rS
] is the coordinate of setup of job j in the S-
Ã	Σ	!	Σ
	 	

	


RHp ∈ [0, rH
] is the coordinate of processing of job j in the H-

j	max
C ≥ Σ s X

+ Σ p Y , k ∈ N	(12)


RHs ∈ [0, rH ] is the coordinate of setup of job j in the H-

j	max
resources dimension (the unspecific shared resources employed in the setup before processing of job j).
DP , EP , F S , GS , UHp, V Hp, UHs, V Hs  are  binary  auxiliary  vari-
Constraints (10) ensure that setup finishing time of job k is ear-
lier than processing starting time of job k. Constraints (11) force that, if job j is the previous job done before job k on machine i, the finishing time of setup k must be later than the finishing time

jk  jk  jk
jk	jk	jk
jk	jk

ables with value = 1 when the variable value studied in con- straints (time or resources) of job k is higher than the variable value studied (time or resources) of job j. The letters in super- script are used to indicate the dimension where they are used. A is a suﬃciently large constant.
We describe the complete model of UPMSR analyzing it in



of processing of job j plus setup time of job k. Constraints (12) set the lower limit of completion time of job k (Ck ) to be greater or equal than the sum of its setup plus its processing time. Upper
limits for Ck have been provided previously by (8) which set this limit below or equal to Cmax.
- Limits of resources
RP  ≥ RP ≥ Σ Y rP , k ∈ N	(13)




cessing and finally the constraints for resources are presented.
The model for UPMSR consists of:
S
max
S
k
i∈M,l∈N
XilkrS , k ∈ N	(14)

- UPMS problem structure
RH ≥ RHp ≥ Σ Y rHp, RH


≥ RHs ≥ Σ
X rHs, k ∈ N	(15)

j∈N0,Σk∈N,k/= j
Σ
sijk Xijk +	pikYik ≤ Cmax, i ∈ M	(2)
k∈N
Constraints (13) and (14) set the coordinate limits of specific resources used in processing (RP ) and used specifically in setups (RS ). Constraints (15) are the same for unspecific shared resources.


k∈N
Yik = 1, k ∈ N	(4)
i∈M
Notice that the maximum shared resources available (Rmax ) is the same for shared resources used in processing or shared resources used in setups.
A new model of UPMR, simpler and more eﬃcient than previ-

Yik =
j∈N0 , j/=k
Y = Σ
Xikj, i ∈ M, k ∈ N	(5)

X , i ∈ M, k ∈ N	(6)
ously published, is shown in Appendix A. We adapted it to UPMSR problem.
- Specific resources in processing (P-resources)

ik	ijk
C − C + A(1 − DP ) ≥ Σ Y
p ,	∀ j, k	(16)

j∈N0 , j/=k
C − C + A¡1 − X
¢ ≥ s

+ p , i ∈ M, j ∈ N , k ∈ N, j /= k  (7)
k	j	jk
¡	¢



i∈M
ik ik



Xijk ∈ {0, 1}, Yij ≥ 0, Ck ≥ 0, C0 = 0	(9)
DP + DP + EP + EP ≥ 1,	∀ j, k > j	(18)

jk	kj	jk	kj 

In this model the Eq. (1) shows the objective. Constraints
(2) define the makespan where all setup times and processing times are added for each machine. In constraints (3) state that no more than one job is scheduled after the dummy job at the begin- ning of each machine. Constraints (4) force each job to be assigned
Constraints (16) show that, if Ck is later than Cj , the difference between these completion times is greater than or equal to the processing time of job k. As an example, binary auxiliary variable P has value 1 when this occurs and zero otherwise. In (17), if
RP is greater than RP , the difference between coordinates of these

to only one machine. Constraints (5) and (6) ensure that only one	k	j

job be done immediately before and only one job immediately af- ter (respectively) each job if jobs are assigned to the same ma- chine, including dummy jobs. Constraints (7) break subtours and give the right scheduling order, because if k is processed after j on machine i, it forces finishing the processing of job k not earlier than sijk + pik . Constraints (8) establish the relationship between Cmax and completion times of each job. Finally, (9) set the limits of variables and set the completion time of the dummy jobs to zero. Notice that Yik can be relaxed to positive variable instead of binary since (5) and (6) are a sum of binary variables and (4) make its sum be equal to 1. The variations introduced in Fanjul-Peyro, Ruiz,
resource points is greater than or equal to the resources used in
the job k processing. Finally, (18) force at least one of the binary variables to have the value of 1. Since the constraints are defined for all j and k, this implies that, at least: or j precedes k without overlapping its processing on the time axis, or k precedes j with- out overlapping its processing on the time axis, or the j resource coordinate is higher than k without overlapping its processing re- sources on the P-resources axis, or the k resource coordinate is higher than j without overlapping its processing resources on the P-resources axis.
- Specific resources in setups (S-resources)

and Perea (2019) that improve the UPMS model, adapted from TSP problem and based on scheduling order of jobs, cannot apply in
S
jk
i∈M,l∈N0
Xilksilk,	∀ j, k	(19)

Rs − Rs + A¡1 − GS ¢ ≥ Σ XilkrS ,	∀ j, k	(20)



UPMSR-S: UPMSR with only specific setup resources.




F S + F S + GS + GS ≥ 1,	∀ j, k > j	(21)
UPMSR-P+S: UPMSR with specific processing resources and

jk	kj 
jk	kj 
specific setup resources.

Constraints (19) show that, if Bk is later than Bj , the difference between these setup finishing times is greater than or equal to the setup time of job k. As an example, binary auxiliary variable F S
has value 1 when this occurs and zero otherwise. In (20), if RS is greater than RS , the difference between coordinates of these re- source points is greater than or equal to the resources used in the job k setup. Finally, (21) force at least one of the binary variables to
have the value of 1. Since the constraints are defined for all j and k, this implies that, at least: or j precedes k without overlapping its setups on the time axis, or k precedes j without overlapping its setups on the time axis, or the j resource coordinate is higher than k without overlapping its setup resources on the S-resources axis, or the k resource coordinate is higher than j without overlapping its setup resources on the S-resources axis.
- Unspecific resources shared by both processing and setup When considering shared resources, it is necessary not to over-
lap unspecific shared processing resources among themselves nor overlap unspecific shared setup resources among themselves. The unspecific shared processing resources should also not overlap with the unspecific shared setup resources. For shared resources in the processing/processing case, in the previous constraints to avoid this overlap (16), (17), and (18), it is necessary to change su- perscript P for superscript Hp. Similarly, in the setup/setup case, in the previous constraints to avoid overlap (19), (20) and (21), it is necessary to change superscript S for superscript Hs. Finally, in the processing/setup case, order is important and its inverse, setup/processing, should be considered together as is shown in the following constraints:
C − B j + A(1 − UHp ) ≥ Σ Y p ,  ∀ j, k	(22)


UPMSR-P+H: UPMSR with specific processing resources and unspecific shared resources.
UPMSR-S+H: UPMSR with specific setup resources and unspe- cific shared resources.
UPMSR-P+S+H: UPMSR with specific processing resources, specific setup resources and unspecific shared resources.

The MILP model for UPMSR is modeled by constraints (1)–(26). However, from (13) to the end, depending on the resources present in each case, not all constraints could be necessary. A deeper study where we formalize all these different problems is included in Appendix A.
Although it is not studied in this paper, extending the problem to more than one resource of each type is as simple as adding a new set of constraints of the corresponding type, e.g. if two differ- ent specific processing resources are included, it would be enough to add a new set of constraints (16), (17), and (18) with their corre- sponding values. Other models based on time are studied but ob- tain worse results as is shown is Appendix A.


Three phase exact algorithm (TPhA)

To solve UPMSR problem we need to find: the assignment for each job on a machine, the sequencing of the set of jobs on each machine and the timing of each processing and setup of each job. So, we could divide the problem into three parts: assignment, se- quencing and timing. As Tran, Araujo, and Beck (2016) and Fanjul- Peyro Ruiz, and Perea (2019) showed, the UPMS problem could be solved dividing into two phases: the previous assignment and the


B − C + A¡1 − UHs¢ ≥ Σ X s ,  ∀ j, k	(23)

lem, a timing of setups and processing should also be determined

RHp − RHs + A¡1 − V Hp¢ ≥ Σ Y rHp,  ∀ j, k	(24)

RH s − RH p + A¡1 − V H s¢ ≥ Σ XiljrH s,  ∀ j, k	(25)



UHp + UHs + V Hp + V Hs ≥ 1,  ∀ j, k	(26)
In constraints (22) we show the case when the processing fin- ishing time of job k is later than the setup finishing time of job j. Therefore, the difference between finishing times must be equal to or greater than the processing time of job k. In this case, as an ex- ample, the binary auxiliary variable UHp has value 1 when setup of
jk
job j is done before processing job k and zero otherwise. In (23) we find the inverse case, where setup finishing time of job j is later than processing finishing time of job k. So, the difference between finishing times must be equal to or greater than the setup time of job j. Eqs. (24) and (25) are similar constraints to the two pre- vious constraints but in this case using shared resources. Finally,
(26) force at least one of the binary variables to have the value of 1.
The model presented for UPMSR in Eqs. (1)–(26) can be reduced to other problems previously studied when setups or resources are not present, such as UPM, UPMS or UPMR. On the other hand, when setups and different types of resources are present, seven combinations of them can generate seven different problems:
phase are developed. In line 1 of the pseudocode we set the finish
variable (STOP) to false, the best value found (BestVal) -or best Cmax
- to infinite, the Lower Bound (LB) to zero and the remaining time (TimeLeft) to the initial available time (TotalTime). In Line 2 a loop is made as long as the finish variable (STOP) does not change or there is some time left. Line 3 is a label to indicate the assignment phase, developed in line 4 with Master MILP using Eqs. (1)–(6) as Section 5.2 explains. Line 5 indicates the Master solution SolM with its values of Cmax and Yik obtained (CM , Y M ). Line 6 checks if Mas- ter Cmax (CM ) is lower than the best solution found previously; if not, the sequencing and timing phase should be done to deter- mine a complete solution. Line 7 checks if the solution of Master is an optimal solution for Master MILP. If it is optimal, we need to do the timing and sequencing phase together in order to obtain a complete optimal solution for Master assignment and introduce a cut to avoid this local optimum as explained in Section 5.4. When the Master solution is not optimal, it is not necessary to do both phases together and the algorithm gets better results by doing the sequencing and timing phase separately. In this case, as the label in Line 8 expresses, the sequencing phase is done with its MILP (Line 10) obtaining the sequencing solution SolS with its particular

- UPMSR-P: UPMSR with only specific processing resources.
values for Cmax, Yik, Xijk (CS
, Y M, XS ), explained in Section 5.3.






After sequencing, the timing phase begins (label in Line 12). In this case we use a Constraint Program (CP) model explained in Section 5.4. The CP model is more eﬃcient than the MILP model when only the resources should be set, as tests prove shown in Appendix A. We call this phase Temporality3a, and its correspond- ing solution and Cmax are named SolTa and CTa (Line 15). In the case that the Master solution is optimal, as aforementioned, we need to do the sequencing and timing phase together, as the la- bel in Line 18 indicates. For this case, the complete MILP is used. We denoted this phase Temporality3b and its corresponding solu- tion and Cmax are named SolTb and CT b (Line 21). These results can be introduced in the Cut Eq. (38) as shown in Line 23 (e.g. the value of CT b is used as the Cq value in the Cut Equation – see Section 5.4). Now this cut, can be introduced in the Master model to avoid a local optimum. Finally, if Master Cmax (CM ) is not lower than the best solution found previously (Line 6), it can be equal or higher (Line 26). In the case of higher values of Master Cmax, the algorithm continues searching for a new Master solution. In the case that Master Cmax is equal to the best solution found and if the Master solution is optimal (Line 27), this implies that the master optimal makespan matches with the best feasible solution stored. So, the global optimal solution for the complete problem has been found.

Phase 1: assignment

In the assignment phase we are going to set the variables Yik which determine the machine i where job k is assigned. In order to do this, we use the constraints (1)–(6) and relax the variables Xijk as nonnegative continues variables while making Yik binary. This reduced MILP is denoted as Master. In the first iteration of Mas- ter we let a gap of 2% as in paper Tran, Araujo and Beck (2016) or 90% of time limit as in paper Fanjul-Peyro, Ruiz, and Perea (2019) in order to have time to find a feasible first solution in succes- sive phases. Later, after finishing the rest of phases, the algorithm returns to phase 1 to obtain a new Master solution. However, mas- ter solution is only an assignment solution since cycles could be present until Eqs. (7)–(9) are introduced in the next phase. No- tice that an optimal solution of Master is always a lower bound
for a complete problem. This is not true for the rest of solutions obtained in other phases as we show later.

Phase 2: sequencing

In the second phase we set the sequencing. With the vari- ables Yik fixed with values obtained in phase one, we now use the constraints (1)–(9). We obtain as result the value of variables Xijk which give us a sequencing of assigned jobs on their machine. The solution is not a feasible solution since no resources have been in- troduced yet. Moreover, the solution found in this phase is not a valid lower bound for a complete problem. In a new example of 5 jobs and 2 machines we are going to suppose that the process- ing and setup times are: p11 = 2, p12 = 1, p23 = 2, p24 = 1, p15 = 1, p25 = 2, s112 = 1, s234 = 1, s125 = 3, s245 = 1, s121 = 3, and others values are so large that they will not be considered, e.g. =10 (and initial setups are zero). In Fig. 6 case (a) we can see an optimal solution from Master -only assignment -; in case (b), when se- tups are introduced in this previous assignment, gives a value of 8 for Cmax. It cannot be considered a valid lower bound since, as is shown in case (c), optimal solution has a value of 7. Notice that job 5 should be assigned to machine 1 (processing time of 1) in the case of Master (no setups present), case (a). But when setups are taken into account along with processing times – case (c), job 5 has to be assigned to machine 2 even if its processing time in this machine is 2 because setups for this job change from machine 1 to machine 2.

Phase 3: timing

In phase 3 a feasible solution is searched for. Assignment vari- ables Yik are fixed and sequencing variables Xijk are found. To find a feasible solution we now used a whole model in order to intro- duce resources. Setting these variables makes the MILP easier to solve but resource constraints are still large. Moreover, when the assignment phase obtains an optimal master solution, the com- plete feasible solution needs to be an optimal solution for this assignment in order to introduce a cut that allows the algorithm to get out of local optimal solution. It implies that the sequenc- ing phase should be skipped when in the assignment phase (Mas- ter) we obtain an optimal solution. In this case, a whole MILP is used with only variables Yik fixed. As an example of this is Fig. 7, which uses the same values as Fig. 6 but without job 5 and with Rmax = 3. Graph (a) shows an optimal solution from Master and with setups but there is a lack of resources at time 1 and 2 since the sum of resources used in the processing of jobs 1 and 3 ex- ceeds the maximum resources. In (b), the solution is feasible tak- ing into account the resources, with makespan of 6. However, it is not the optimal solution for this assignment as graph (c) shows. A different sequence – but the same assignment – on machine 1 gives an optimal makespan of 5. So, in order to introduce a cut that establishes a lower bound for a particular assignment, when phase 1 gives an optimal master solution, we do the sequencing and timing phases together.
As a result of the previous discussion, we can discriminate two cases in the timing phase: one when master solutions are not op- timal and one when they are. In the first case, when the solution from Master is not optimal, the sequencing phase is done before the timing phase is carried out with Xijk variables fixed by the se-
quencing phase. From now on we will refer to this first case as
phase 3a. In the second case, when the solution from Master is optimal, as Fig. 7 shows, we need to execute a whole MILP with only Yik variables fixed. We denoted this phase as 3b. For the tim- ing phase, a Constraint Programing (CP) could be used instead of MILP with solvers. This also is used by Fleszar and Hindi (2018) in the case of UPMR problem. As we later show in tests, CP performs




Fig. 6. Opt. Master Sol. with setups introduced, is not a valid LB.


Fig. 7. Master Opt. Sol. implies doing sequencing & timing phases together.


well in the case 3a, when it is not an optimal master solution and it is only necessary to set the resources in the solution. However, in the 3b case (optimal master solution) we need to do sequenc-


h
max

≥ Σ rhp_Y ∗

+ Σ rhs_X ∗


(37)

ing and timing together. The performance of CP decreases and does not reach the results of MILP. The low performance is due in part to the fact that CP finds it diﬃcult to solve UPMS problem, com- pared with MILP. Nevertheless, CP shows very good behavior when only resources need to be set.
To codify CP we need the following data and variables:
Y ∗ are the variables set in the assignment phase each with its value pik .
In (27) variables are defined. Notice that they are fixed and will come from phase of assignment and sequencing. From (28)–(31) resources used by processing and setup are defined. In (32) the objective is shown as the minimization of the maximum of when each processing job ends. Eq. (33) forces that after each setup its corresponding job should be done. Eq. (34) forces that after each job a new setup should be done except for the last job k in each machine which has no X ∗ . Notice that the subscript in this case

∗
ijk
are the variables set in the sequencing phase each with its
is ikj instead of ijk. Eqs. (35)–(37) are proposed to avoid the use of

value sijk .
rP∗ are the values of specific processing resources assigned.
rS∗ are the values of specific setup resources sequenced.
rHp∗ are the values of assigned shared resources for processing.
ik
rHs∗ are the values of assigned shared resources for setups.
pulse represents a cumulative function of CP.
more resources at the same time than maximum resources avail-
able of each type.
On the other hand, in the case that the master solution is op- timal, and after solving timing and sequencing together (3b), it is necessary to introduce a cut in Master in order to avoid this local optimal solution. Since idle times could be present, in (38) a new cut is introduced.

The CP used in phase 3a (no optimal solution from Master) is:
Cmax ≥ Cq Ã Σ
Y − n + 1!

(38)

intervalV ar Y ∗, X ∗
(27)
max	ik

ik	ijk
∗	¡ ∗	P ∗¢
i∈M,k∈Nq

In (38) is forced that when in iteration q the value of the feasi-
	


rs_X ∗ = pulse¡X ∗ , rS∗¢	(29)
Cmax in the Master problem as long as the assignment is the same
as in iteration q (Nq). Notice that, if all assignments are the same,



rhp_Y ∗ = pulse¡Y ∗, rH p∗¢	(30)

its assignment). A cut used in Tran, Araujo, and (Beck, 2016) for



rhs_X ∗ = pulse¡X ∗ , rHs∗¢	(31)
present.

minimize¡max£endO f ¡Y ∗¢¤¢	(32)
endBe foreStart ¡X ∗ , Y ∗¢	(33)



We show in this section the result obtained in a computational campaign. Small, medium and large instances are used. The solvers

endBe foreStart ¡Y ∗, X ∗ ¢	(34)

mance but a comparison with the use of CPLEX instead will also


p
max


s
max
ik
≥ Σ rp_Y ∗

≥ Σ rs_X ∗
ik j

(35)


(36)
be shown. All experiments were carried out on virtual machines with 2 cores and 16 GB of RAM with Windows 10 64 bits as op- erating system. Virtual machines come from an Openstack virtual- ization platform running on 12 blades with 12 cores AMD Opteron Abu Dhabi 6344 processors 2.6 GHz and 256 GB of RAM each one.



The experiments were randomly distributed on virtual machines in order to speed up the completion of all experiments without paral- lel computing. The programming language was C#.NET of Microsoft Visual Studio 2015 IDE.
The main indicator used is the gap between Lower Bound and
Table 1
comparative of MILP for UPMR.
gap	RPD	%Opt	%RPD=0 UPMR-pre model	385.99	 16.57	 0.22	 3.56
UPMR-new model	173.85	7.58	3.11	12.22

∗
max
obtained by different models or exact methods expressed in
UPMSR-model	7.17	6.70	34.22	36.22

percentage as 100(C∗  /LB − 1).

In small instances, we set the time limit in 3 hours for the model and the Three Phase Algorithm (TPhA) in order to give them the same time as Tran, Araujo, and Beck (2016). For medium and large instances used with TPhA, 30 min was fixed as the time limit. However, the behavior of the Three Phase Algorithm with time is also studied in Appendix A.

Instances

For the instances, we have to consider several factors such as number of jobs, number of machines, magnitude and dispersion of the processing times and setup times, amount of resources avail- able and their consumption by jobs. So, to introduce the UPMSR problem, we based the instances used in this paper on those present among the literature for UPMS and UPMR problems.
In this paper the distribution times for processing and se- tups use the same as Tran, Araujo, and Beck (2016) and Fanjul-Peyro, Ruiz, and Perea (2019), previously introduced in Arnaout, Rabadi, and Musa (2010) with small, medium and large instances. With a processing distribution of U(50,100) and setup distribution of U(50,100), small instances were randomly generated with 10, 20, 30, 40, 50 jobs and 4, 6, 8 machines. Initial setup is used in all instances.
For setup times, the triangular inequality must be satisfied i.e. sijk ≤ sijl + pil + silk, i ∈ m; j, k, l ∈ n to avoid situations when do- ing a third job between a pair of jobs uses less time than only do- ing the original pair of jobs.
For resources, following Fanjul-Peyro, Perea, and Ruiz (2017) or Fleszar and Hindi (2018), a distribution of U(1, 9) is applied for assigned resources. Resources could be specific for Processing (P), specific for Setups (S) or sHared by both, processing and setups
(H) and all combinations of them i.e. P+S, P+H, S+H, P+S+H. To- tal: seven types of resources. The maximum number of resources has been determined so that all instances can be feasible. Since resources required has a value between 1 and 9 the Rmaxis:

Table 2
Gap & % of optimal solution of Model and TPhA with small instances. In type cases with sHared (H) resources higher than 30 jobs are omitted since some no feasible solutions are found. Time limit = 180-min.




exposed in (40)–(47) and our final model of UPMSR. We use in Table 1 the same medium instances tested and the same time limit (an hour) that is used in the aforementioned paper.
In Table 1 “UPMR-pre model” represents the MILP published in Fanjul-Peyro, Perea, and Ruiz (2017). The MILP expressed in Eqs (40)–(47) is denoted as “UPMR-new model”. Finally, “UPMSR- model” is how we named the MILP used in this paper to solve in- stances with setups and resources but applied to this case, where all setups are zero. Gap is the difference between solution and LB found by each MILP. RPD (relative percentage difference) shows the difference between solutions found for each MILP and the best LB found by the three models. As we can see, UPMR-new improves the UPMR-pre results and our UPMSR MILP applied to the UPMR problem obtains the best results. Notice that the UPMR problem is easier than the UPMSR problem studied in the present paper since no setups are present. However, gap of model for the UPMR prob- lem have similar values to values obtained for the UPMSR problem as we can see in the next results. Remember all of these results are


Rmax = m
rmax + r
2

min = m
9 + 1


2
= 5m	(39)
only to compare MILP in UPMR and not for other exact methods as
in Fleszar and Hindi (2018) which improves these results. However this study allowed us to know which MILP of UPMR was better to

Five replicates are made for each combination. So, the total
number of small instances are 5×3×7×5 = 525.
For medium instances the number of jobs used is 60, 80, 100, 120, and 140 with 4, 6, and 8 machines, for a total of 5×3×7×5 = 525 medium instances. Finally, we generate a similar number of instances with 200, 250, 300, 350, and 400 jobs, and
4, 6, and 8 machines for testing a total of 5×3×5×7 = 525 large instances. A total of 1575 instances are studied.
However, in Appendix A, different distribution times for setups and processing are studied as well as calibration tests.
The executable to generate instances, the executable to run the model or the Three Phase Algorithm and all tables are included in on line materials.

Results and discussion

To know the values range obtained by the results in this paper we will previously show a comparative in Table 1 of values of the easier case of UPMR problem (with no setups). In this table we compare the results obtained by MILP proposed in the UPMR paper Fanjul-Peyro, Perea, & Ruiz, 2017) with the new model for UPMR
adapt to the UPMSR problem.
Now, in order to know the performance of the new MILP model introduced for the UPMSR problem, we use the small instances. For more complex types of resources i.e. when shared resources are present, the model returns no feasible solutions for some in- stances larger than 30 jobs, even with three hours of execution. Table 2 shows the results for the MILP model and the Three Phase Algorithm (TPhA) for small instances with three hours as the time limit.
In Table 2 the heading jobs represents the interval of number of jobs, 10–50 for resources needed in Processing (P), in Setup (S), and in Processing and Setup (P+S) types. Only jobs from 10 to 30 are shown in the case of the shared resources type (H) being present since the model returns some no feasible solutions for in- stances higher than 30 in these cases. The different type of prob- lems with shared resources included are those that need this re- source type: only sHared (H), processing and shared (P+H), setups and shared (S+H), process, setup and shared (P+S+H). The per- centage of gap (difference between solution obtained and LB) and percentage of optimal solution found are represented. As can be appreciated, the average gap of the Three Phase Algorithm (6.94)



Table 3
Gap of TPhA with medium instances. Time limit = 30 min.



Table 4
Gap of TPhA with large instances. Time limit = 30 min.




improves the average gap of the model (8.94). However, notice that in cases with shared resources, TPhA improves the gap results of the model and the model improves TPhA otherwise. In fact, for these cases without shared resources, when the number of jobs is equal to 50, TPhA also improves the results of the model. More- over, as is shown in Appendix A, with other distributions, TPhA improves gaps of the model in most cases. The model obtains bet- ter percentages of optimal solutions, even in cases when it does not obtain better gaps. The reason for this is in part due to the fact that TPhA obtains its LBs only from master (phase 1) whereas the model can improve its LBs gradually. So, TPhA gives better gaps even when it finds worse LBs by improving the solution even more. Higher values of gaps are found in more diﬃcult cases (S+H and P+S+H) and lower values can be observed when only specific re- sources are present (P and S cases).
For Medium instances (jobs from 60 to 140), the results for TPhA with 30 minutes as time limit are shown in Table 3.
MILP is not included since in all types for some instances (in many cases, all instances) the model does not return any feasible solutions. As the total average shows, the gap value for medium instances (6.94%) is similar to the gap value for small instances (6.94%). There is a decrease of gaps with the increase of jobs from an average of 8.28% for 60 jobs to 6.21% for 140 jobs. A reason for this comes from the fact that setup increases its amount of data exponentially with the number of jobs. For each machine a matrix of n x n values is present in setups, so for each job on each ma- chine n possible values can be selected. When number n is large, setup selection usually is the lowest possible value among setup time data. So, it seems that this helps the solver to find good solu- tions. In fact, gap is increasing with number of jobs up to 50 jobs and begins to decrease from 60 jobs on. No optimal solutions are found, so all instances used up the 30 min time limit.When the av- erage results of each type of resource are analyzed we can observe that average gap values are below 5.1% in the first five cases, and only for the last two harder cases have values below 13.4% with respect to their LBs.
Finally, for large instances from 200 to 400 jobs, Table 4 shows
a similar behavior to that of medium instances: an increase of jobs gives a decrease of gap. Now the total average value of gap is 5.09% but with average values lower than 3.1% for all types of resource except the last two studied.
Summarizing, for small instances the MILP proposed can give solutions for these hard problems but with the increase of num- ber of jobs, it begins to fail. TPhA can obtain similar values to the model for small instances and can reach solutions at least up to 400 jobs. Despite the fact that gap seems great, especially for more complex cases, an approximate solution and lower bound can be presented. Gap decreases with the increase of number of jobs, reaching less than 2.7% for 400 jobs and all types of resources ex- cept the last two harder problems when resources specific for se- tups and shared resources are both present.

7. Conclusion and future research

We have presented a scheduling problem with the objective of minimizing makespan. In the problem studied we have taken into account processing times, setup times and scarce resources specific for processing and/or resources specific for setups and/or resources shared for both processing and setups. This brings us closer to re- alistic problems such as in plastic processing plants or ceramics factories. A MILP has been developed to model all combinations of these factors. MILP is used to obtain results for small instances. The Three Phase Algorthm (TPhA) is introduced to improve the re- sults of the model and make it possible to reach larger instances. TPhA splits the problem in three phases, assignment, sequencing and timing, with a new cut introduced to avoid local optimal. This method allows us to get results up to 400 jobs in 30 min with av- erage gap (results vs lower bound) being around 4.7% for all pos- sible combinations. It falls below 2% in average when we do not consider the two harder cases, when specific resources for setups and shared resources are both present. Therefore, the Three Phase Algorithm exposed seems a good first approximation for this prob- lem.
As future research, heuristics or metaheuristics could be a way
to improve these results. Also, some different objectives could be considered such as tardiness or total completion time.

Declaration of Competing Interest

The authors declare that they have no known competing finan- cial interests or personal relationships that could have appeared to influence the work reported in this paper.

CRediT authorship contribution statement

Luis Fanjul-Peyro: Conceptualization, Methodology, Software, Validation, Formal analysis, Investigation, Writing - original draft, Writing - review & editing, Visualization.

Acknowledgments

The authors are partly supported by the Spanish Ministry of Sci- ence, Innovation, and Universities, under the project “OPTEP-Port Terminal Operations Optimization” (No. RTI2018-094940-B-I00) fi- nanced with Fondo Europeo de Desarrollo Regional (FEDER) funds.

Supplementary materials

Supplementary material associated with this article can be found, in the online version, at doi:10.1016/j.eswax.2020.100022.

Appendix A

This appendix includes:
A new, more compact and eﬃcient model for the UPMR prob- lem and an example of using strip-packing methods for this problem.


Other possible models for the UPMSR problem but obtaining worse results than the proposed model.
Adaptation of UPMRS model to different problems
Tests with other distributions of times for setups and process- ing.
Calibrations: use of CPLEX instead of Gurobi for the several MILPs used, results with different time limits, use of CP in phase 3b (sequencing and timing together) and results using model instead of CP in phase 3a (timing with fixed assignment and sequencing).

New model for UPMR based on strip-packing

In a strip-packing 2D problem a set of rectangles must be fit in a box which has one of its dimensions fixed. The objective is to minimize the other dimension. Each rectangle has a height and a width. An approximation to the UPMR could be to understand that each job has a width, delimited by its processing time (pik ), and a height, set by the number of resources needed for it to be processed (rik ). In strip-packing it is necessary to know the local- ization of each rectangle, in this case through the coordinates of its top-right corner. In the case of UPMR, where axis x represents time and axis y the number of resources, the coordinates will be given by Ck and Rk respectively. Ck represents the time when job k is fin- ished and Rk the point or height of number of resources. The fixed dimension is height H set by Rmax and total width W is determined by Cmax. In Fig. 8 two jobs (J1, J2) must be scheduled. J1 is previ- ously set and J2 could have two (among other) different positions: later than J1 without overlap on x (J2a) or higher than J1 without overlap on y (J2b). Notice that, in the UPMR problem, jobs can be assigned to any machine since there are no pre-assigned jobs. So, m cases (one for each machine) should be defined for each job.
Based on the UPMR strip-packing model introduced in Fanjul- Peyro, Perea, and Ruiz (2017), a new more compact and eﬃcient MILP is presented, where Wjk , Hjk are auxiliary binary variables.
Yik + Yij − Wjk − Wkj ≤ 1, ∀ j ∈ N, k ∈ N, j /= k, k > j	(45) Wjk + Wkj + Hjk + Hkj ≥ 1, ∀ j ∈ N, k ∈ N, j /= k, k > j	(46) Ck ≥ 0, Rk ≥ 0, Hjk, Wjk, Yik ∈ {0, 1}	 (47)
In (40) the objective function minimizes makespan. The bounds limits for time and resources are set by (41). Each job can only be assigned to one machine through (42). Constrains (43) and
(44) avoid overlap in time and resources axes respectively.
(45) takes into account that if job j and job k are on the same machine, it is necessary and suﬃcient not to overlap on the time axis. In the general case, constraints (46) ensure that there is no overlap neither in the time axis nor in the resources axis. Finally,
(47) defines variable types. In the UPMSR model, (45) is not nec- essary since (5)–(7) prevent it.

Other models for UPMSR based on time

Other MILP models can be considered. In the MILP employed in this paper, a spatial approximation is used when the strip-packing problem is adapted to be used with resources. However, a tempo- ral approximation could be used. Based on Edis, Oguz, and Ozkara- han (2013) and as was done in paper Fanjul-Peyro, Perea, and Ruiz (2017), the temporal model used there was adapted to UPMSR but has problems giving solutions even for instances of 10 jobs (not included here). A more promising temporal approach came when the finishing times of processing and setup were considered. The new input data for UPMSR temporal MILP are:
Eikt = 1 if k is being processed on machine i in time t, zero otherwise.
Fijkt = 1 if k is in setup after job j on machine i in time t, zero otherwise.
tmax is a time that is large enough
For this MILP we use Eqs. (1)–(9) and define Bk as the time when setup of job k is finished.

min Cmax Cmax ≥ Ck ≥

Σi∈M


Yik pik, Rmax ≥ Rk ≥

Σi∈M
(40)
Yikrik, ∀k ∈ N	(41)
Eikt ≥ Yik pik, ∀i, k ∈ N	(48)
t≤tmax

Yik = 1, ∀k ∈ N	(42)
i∈M
Ck ≥	tEikt , ∀k ∈ N, t ≤ tmax	(49)
i∈M

Ck − Cj + A 1 − Wjk ≥	pikYik, ∀ j ∈ N, k ∈ N, j /= k	(43)
i∈M
Ck + 1 − Σ Yik


pik
≤ Σ tE


ikt
+ AÃ1 − Σ E


ikt
!, ∀k ∈ N, t ≤ t


max

Rk − R j
+ M¡1 − Hjk
≥	rikYik, ∀ j ∈ N, k ∈ N, j /= k	(44)
i∈M
i∈M
i∈M
i∈M

(50)




Fig. 8. UPMR from strip-packing




Fijkt ≥ Xijksijk, ∀i, k ∈ N, j ∈ N0	(51)
t≤tmax
UPMR: UPM with Resources
In the case of UPMR the resources present are only specific for processing (since there are no setups). Therefore constraints (14),
(15) and (19)–(26) are not useful. Since there are not any setups

Bk ≥
i∈M, j∈N0
tFijkt , ∀k ∈ N, t ≤ tmax,	(52)
in UPMR, all sijk are zero but if we do not set all Xijk to zero, con- straints help us to avoid overlap in the time axis. So only Eqs. (1)–
(13) and (16)–(18) are taken into account. This reduced model im-


Bk + 1 −

i∈MΣ, j∈N0


Xijksijk ≤

i∈MΣ, j∈N0


tFijkt + A
Ã1 −

i∈MΣ, j∈N0
Fijkt !,
proves the results of the best model published of UPMR and also improves the results of the new model specific for UPMR, intro- duced in Appendix A, as we show later in computational results.

∀k ∈ N, t ≤ tmax	(53)
Constraints (48) and (49) are the limits of Eikt . Eq. (50) forces when Eikt has value 1 at time t, this time t is greater than or equal to the difference between the completion time of job k and its pro- cessing time plus 1. Constraints (51)–(53) represent the same but for setups, where the time to be considered is sijk .
For resources, this model uses simpler constraints than the
MILP model presented in this paper.
Resources in processing

UPMSR-P: UPM with Setup and Resources (UPMSR) with only specific processing resources
In cases where there are always setups and any type of re- source is present, equations from (1) to (12) are always present. In the case with only specific processing resources (P-resources) we should add constraints relative to these resources which are
for limits, and (16)–(18) to avoid their overlaps.

Σ rP Eijt ≤ RP  , ∀t ≤ tmax	(54)


UPMSR-S: UPMSR with only specific setup resources




Constraints (54) force that, at each time t, the sum of all re- sources used in processing should be lower than or equal to the maximum resources to process.
Resources in setups
present. In the case with only specific setup resources (S-
resources), the limit for these resources are set by (14). In order to avoid overlaps when S-resources are taken into account, we should add constraints (19)–(21).

Σ	rS Fijkt ≤ RS



, ∀t ≤ tmax	(55)

Constraints (55) force that, at each time t, the sum of all re- sources used in setups should be lower than or equal to the maxi- mum resources for setups.
Shared resources
both processing and setups, we need Eqs. (1)–(12) and constraints for the limit of shared resources, i.e. (15). In this case, when the resources could be used in processing or in setups indis- tinctly, we should take into account the contribution to the to-
tal use of this type of resource that processing and setups re-

i∈MΣ,k∈N
Hp ik
i∈M, j∈N,k∈N, j/=k
rHs Fijkt
H
max
, ∀t ≤ tmax	(56)
quire. Consequently, we should consider the relationship of pro- cessing/processing, setup/setup and processing/setup. So besides

Constraints (56) force that, at each time t, the sum of all shared resources used in processing and all shared resources used in se- tups should be lower than or equal to the maximum shared re- sources.
This model only allows us to reach results up to 30 jobs in the easier cases (resources only in processing or in setups) but not in all cases. So, that put it quite far from the results of MILP used in this paper which has results up to 50 jobs.
Reduction of UPMSR model to different problems

Different types of problems can be studied with the model pro- posed in Eqs. (1)–(26). In the next subsection we formalize these different problems.
UPM: Unrelated Parallel Machine
In the case of UPM no resources are present, therefore con- straints (13)–(26) are not useful. Since there are not any setups in UPM, all sijk are zero and all Xijk could be set to zero. In this case, only Eqs. (1), (2), and (4) are taken into account. This reduced model is the classical model used for UPM problems.

UPMS: UPM with Setups
In the case of UPMS no resources are present, therefore con- straints (13)–(26) are not useful. Since no idle times are present in UPMS problem, constraints (10)–(12) are not necessary. So only Eqs. (1)–(9) need to be taken into account. This reduced model is like the Avalos-Rosales, Angel-Bello, and Alvarez (2015) model for UPMS problem.
constraints (22)–(26) to avoid processing/setup overlaps, we need to use constraints (16)–(18) changing superscript P for superscript H to avoid processing/processing overlap in the use of shared re- sources. Also we need to use constraints (19)–(21) in order to pre- vent overlaps in the setup/setup case of use of shared resources, changing superscript S for H. Summarizing, we should use Eqs. (1)–
and (15)–(26) changing superscripts P and S for H.


UPMSR-P+S: UPMSR with specific processing resources and specific setup resources
This case is a combination of cases UPMSR-P and UPMSR-S. So we need constraints (1)–(12) as general constraints and (13) and
as limit constraints. Finally (16)–(18) are for specific process- ing resources and (19)–(21) for specific setup resources constraints.


UPMSR-P+H: UPMSR with specific processing resources and unspecific shared resources
This case is a combination of cases UPMSR-P and UPMSR-H. So we need constraints (1)–(12) as general constraints (13)–(15) as limit constraints. Constraints (16)–(18) are for specific processing resources (P-resources). For shared resources we need a new set of constraints (16)–(18) changing superscripts P for H in order to avoid overlap in the case of processing/processing shared resources (H-resources). For shared resources we also need to avoid overlap in the case of setup/setup with constraints (19)–(21) changing su- perscripts S for H and finally constraints (22)–(26) to avoid pro- cessing/setup overlap.



Table 5
Model and TPhA with different distributions.




UPMSR-S+H: UPMSR with specific setup resources and unspecific shared resources
This case is a combination of cases UPMSR-S and UPMSR-H. So we need constraints (1)–(12) as general constraints and (14) and
as limit constraints. Constraints (19)–(21) are for specific setup resources (S-resources). For shared resources we need a new set of constraints (19)–(21) changing superscripts S for H in order to avoid overlap in the case of setup/setup shared resources (H- resources). For shared resources we also need to avoid the over- lap processing/processing case with constraints (16)–(18) changing superscripts P for H and finally constraints (22)–(26) to avoid pro- cessing/setup overlap.

UPMSR-P+S+H: UPMSR with specific processing resources, specific setup resources and unspecific shared resources
This case is a combination of cases UPMSR-P, UPMSR-S and UPMSR-H. So we need constraints (1)–(12) as general constraints,
–(15) as limit constraints. Constraints (16)–(18) are for spe- cific processing resources (P-resources). Constraints (19)–(21) are for specific setup resources (S-resources). For shared resources we need a new set of constraints (16)–(18) changing superscripts P for H in order to avoid overlap in the case of processing/processing shared resources. We also need a new set of constraints (19)–(21) changing superscripts S for H in order to avoid overlap in the case of setup/setup shared resources. Finally, constraints (22)–(26) are to avoid processing/setup overlap.

Other distributions

In order to make this paper more clear, only balanced in- stances used in papers such as Arnaout, Rabadi, and Musa (2010), Tran, Araujo, and Beck (2016) or Fanjul-Peyro, Ruiz, and Perea (2019) are exposed. However, other distributions used in papers such as Fanjul-Peyro, Perea, and Ruiz (2017) or Fleszar and Hindi (2018) will be studied now.
These distributions include five different types of processing times, three uniform random distributions and two more distribu- tions with high dependence of jobs and machines, respectively. The five distributions are:
U(1, 100), U(10, 100), U(100, 200) which generate random num- bers between the lower and higher limits (both included) for each pik.
Correlated jobs where each job k has a fixed time with inde-
pendence of machine according to pk = U(1, 100) to which is added an additional time pr = U(1, 20). Therefore pik = pk +
r	ik
ik
Correlated machines where each machine i has a fixed time to
process a job with independence of which job it is, according to pi = U(1, 100) to which is added an additional time pr =
Table 6
TPhA with different dominion.

p=U(100,200)	p=U(100,200)	p=U(50,100)	p=U(50,100) s=U(100,200)	s=U(50,100)	s=U(100,200)	s=U(50,100)
 	
Avg.	6.20	6.69	6.59	6.13
 	


U(1,49) and U(1,99) and five resource types i.e. specifically for pro- cessing (P), specifically for setups (S), shared resources (H), when types P and S are present (P+S), and when all types are present (P+S+H). Notice that the number of instances rises to 3,750. Since the model returns no feasible solutions up to 30 jobs when shared resources are present (cases H and P+S+H), only jobs from 10 to 30 are considered in these cases and jobs from 10 to 50 for other cases (P, S, P+S) as in the main part of the paper.
As Table 5 shows, in all cases the Three Phase Algorithm im- proves gap results of the model and the model can demonstrate more solutions as optimal. Higher values of gap are shown in the Job Correlated case, and U(1,100) distribution gives lower values. In the Machine Correlated case, the model gives a poor value for an instance of 50 jobs which is discarded since Table 2 is used to compare the model with TPhA.
For the five types of resources considered here, the average TPhA (6.52%) obtains a similar value for gap to the average of these five types of instances used in the main part of the paper (6.16%). But the model, with a 9.15% of average gap, worsens the instances results of the paper which are on average 6.21% for these five types. However, with respect to the percentage of optimal so- lutions, these distributions obtain better results than the ones in the paper. Here the model finds 46.48% of optimal solutions versus 32.06% in the paper, and TPhA here finds 36.06% versus 22.22% in the paper.
In Arnaout, Rabadi, and Musa (2010) a balanced distribution of setup and processing times are used. However, a processing do- minion or setup dominion can be also used. In order to maintain the same dispersion of 100 units as most commonly used in the literature and to fix similar values for maximum and minimum val- ues we use the following distributions:
Balanced: p=U(100,200); s=U(100,200).
Processing dominion: p=U(100,200); s=U(50,100).
Setup dominion: p=U(50,100); s=U(100,200).
In Table 6 we also include the results obtained in the paper for balanced distribution of p=U(50,100) and s=U(50,100) in order to compare it.
On average, the results are similar for all these distributions. For each distribution it seems that some types of resources make the problem easier or harder, which depends on dominion. The bal- anced distributions have intermediate values. No appreciable dif- ferences can be found in the use of the two different balanced dis- tributions studied.

Calibrations
Some aspects should be clarified as to why in the paper we use different components. One of these is the use of Gurobi or Cplex solvers. In the case of the model, Cplex begins to fail at 40 jobs for easier types of resources whereas Gurobi has no problem up to 50

U(1, 20). Therefore p
= p + pr .
ik	jobs. In the case of TPhA, Table 7 shows that results improve using

ik	i	ik
In Table 5 we studied for small instances the five process- ing distribution times proposed, with setup distribution time of
Gurobi in a set of 150 medium instances of calibration. Another question is why to use CP for phase 3a (timing with assignment and sequencing variables fixed). The reason is because using MILP



Table 7
Variants of TPhA.
Allahverdi, A. (2015). The third comprehensive survey on scheduling problems with setup times/costs. European Journal of Operational Research, 246(2), 345–378. doi:10.1016/j.ejor.2015.04.004.
Arnaout, J., Rabadi, G., & Musa, R. (2010). A two-stage ant colony optimization algorithm to minimize the makespan on unrelated parallel machines with sequence-dependent setup times. Journal of Intelligent Manufacturing, 21(6), 693–701. doi:10.1007/s10845-009-0246-1.
Avalos-Rosales, O., Angel-Bello, F., & Alvarez, A. (2015). Eﬃcient metaheuristic algo- rithm and re-formulations for the unrelated parallel machine scheduling prob- lem with sequence and machine-dependent setup times. The International Jour- nal of Advanced Manufacturing Technology, 76(9-12), 1705–1718. doi:10.1007/ s00170-014-6390-6.
Bektur, G., & Saraç, T. (2019). A mathematical model and heuristic algorithms for an unrelated parallel machine scheduling problem with sequence-dependent setup times, machine eligibility restrictions and a common server. Computers & Oper- ations Research, 103, 46–63. doi:10.1016/j.cor.2018.10.010.
Błazewicz, J., Kubiak, W., Röck, H., & Szwarcfiter, J. (1987). Minimizing mean flow-



in these cases returns no feasible solutions in jobs higher than 100 jobs. Remember that the phase 3a is done with each master solu- tion found and it is necessary to find a feasible solution. The phase 3b is done only with the optimal solutions of master. In these cases we have previous feasible solutions coming from not optimal mas- ter solutions which have been made feasible. If TPhA finds diﬃ- culties in exiting phase 3b in some instances, we have a feasible previous solution. But if TPhA fails in phase 3a perhaps no high quality solution could be found. That is the reason why in phase 3a it is better to use CP but in phase 3b MILP should be used, i.e. If MILP finds it hard to solve a particular instance in phase 3b, we have a previous solution, but if not, MILP gives solutions that are a bit better than CP in phase 3b as we will see below. Finally, the use of CP in phase 3b (sequencing and timing together) versus the use of MILP shows a difference in favor of MILP as Table 7 shows. With these values we make the decisions of which ones should be selected to be used in the paper. However, the differences in Table 7 are small. In the ANOVA test, P-value of 0.51 shows that no significant differences in means are present. In a brief study of larger instances, it shows minor differences between average val- ues observed in Table 7. Moreover, for large instances it is recom- mendable to use CP in phase 3b since MILP can produce problems of out of memory.
In order to delimit the time to be used in TPhA, we test a set
of 150 calibration instances in a time test shown in Table 8. As can be seen in Table 8 each increase of time improves the results a bit (between 3% and 7%), however the difference between 5 min and 180 min improves up to 65% in one case (S) with an average of 29%. The difference is reduced to 13% on average with a maximum of 32% (S) when we consider 30 min vs 180 min. Moreover, 30 min is the minimum time to obtain feasible solutions for the largest instances of 400 jobs and 8 machines. However, ANOVA with P- value of 0.53 shows that no significant differences between means are present.
References

Akyol Özer, E., & Saraç, T. (2019). MIP models and a matheuristic algorithm for an identical parallel machine scheduling problem under multiple copies of shared resources constraints. An Official Journal of the Spanish Society of Statistics and Operations Research, 27, 94–124. doi:10.1007/s11750-018-00494-x.
Edis, E. B., & Oguz, C. (2011). In Parallel machine scheduling with additional resources: A Lagrangian-based constraint programming approach: 6697 (pp. 92–98). Lecture Notes in Computer Science. doi:10.1007/978-3-642-21311-3_10.
Edis, E. B., & Ozkarahan, I. (2012). Solution approaches for a real-life resource-constrained parallel machine scheduling problem. The International Journal of Advanced Manufacturing Technology, 58, 1141–1153. doi:10.1007/ s00170-011-3454-8.
Edis, E. B., Oguz, C., & Ozkarahan, I. (2013). Parallel machine scheduling with ad- ditional resources: Notation, classification, models and solution methods. Euro- pean Journal of Operational Research, 230(3), 449–463. doi:10.1016/j.ejor.2013.02. 042.
Fanjul-Peyro, L., Perea, F., & Ruiz, R. (2017). Models and matheuristics for the unre- lated parallel machine scheduling problem with additional resources. European Journal of Operational Research, 260(2), 482–493. doi:10.1016/j.ejor.2017.01.002.
Fanjul-Peyro, L., Ruiz, R., & Perea, F. (2019). Reformulations and an exact algorithm for unrelated parallel machine scheduling problems with setup times. Comput- ers & Operations Research, 101, 173–182. doi:10.1016/j.cor.2018.07.007.
Fleszar, K., & Hindi, K. S. (2018). Algorithms for the unrelated parallel machine scheduling problem with a resource constraint. European Journal of Operational Research, 271(3), 839–848. doi:10.1016/j.ejor.2018.05.056.
Graham, R., Lawler, E., Lenstra, J., & Rinnooy Kan, A. (1979). Optimization and ap- proximation in deterministic sequencing and scheduling: A survey. Annals of Discrete Mathematics, 5, 287–326. doi:10.1016/S0167-5060(08)70356-X.
Grigoriev, A., Sviridenko, M., & Uetz, M. (2007). Machine scheduling with re- source dependent processing times. Mathematical Programming, 110(1), 209–
228. doi:10.1007/s10107-006-0059-3.
Guinet, A. (1991). Textile production systems: A succession of non-identical paral- lel processor shops. Journal of the Operational Research Society, 42(8), 655–671. doi:10.1057/jors.1991.132.
Lenstra, J., Rinnooy Kan, A., & Brucker, P. (1977). Complexity of machine scheduling problems. Annals of Discrete Mathematics, 1, 343–362. doi:10.1016/ S0167-5060(08)70743-X.
Lodi, A., Martello, S., & Monaci, M. (2002). Two-dimensional packing problems: A survey. European Journal of Operational Research, 141(2), 241–252. doi:10.1016/ S0377-2217(02)00123-6.
Pinedo, M. L. (2016). Scheduling: Theory, algorithms and systems (5th ed.). New York, USA: Springer. doi:10.1007/978-3-319-26580-3.
Słowinski, R. (1980). Two approaches to problems of resource allocation among project activities: A comparative study. Journal of the Operational Research So- ciety, 31(8), 711–723. doi:10.1057/jors.1980.134.
Tran, T., Araujo, A., & Beck, J. (2016). Decomposition methods for the parallel ma- chine scheduling problem with setups. Informs Journal on Computing, 28(1), 1–
195. doi:10.1287/ijoc.2015.0666.
Vallada, E., & Ruiz, R. (2011). A genetic algorithm for the unrelated parallel machine scheduling problem with sequence dependent setup times. European Journal of Operational Research, 211(3), 612–622. doi:10.1016/j.ejor.2011.01.011.
Villa, F., Vallada, E., & Fanjul-Peyro, L. (2018). Heuristic algorithms for the unrelated parallel machine scheduling problem with one scarce additional resource. Expert Systems with Applications, 93, 28–38. doi:10.1016/j.eswa.2017.09.054.
Zheng, X., & Wang, L. (2016). A two-stage adaptive fruit fly optimization algorithm for unrelated parallel machine scheduling problem with additional resource constraints. Expert Systems with Applications, 65, 28–39. doi:10.1016/j.eswa.2016. 08.039.
