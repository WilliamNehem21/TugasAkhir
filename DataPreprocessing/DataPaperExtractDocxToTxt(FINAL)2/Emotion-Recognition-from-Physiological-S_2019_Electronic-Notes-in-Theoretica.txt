Available online at www.sciencedirect.com


Electronic Notes in Theoretical Computer Science 343 (2019) 35–55
www.elsevier.com/locate/entcs

Emotion Recognition from Physiological Signal Analysis: A Review
Egger Maria1, Ley Matthias1, Hanke Sten
AIT Austrian Institute of Technology GmbH, Vienna, Austria Email: maria.egger@ait.ac.at, matthias.ley@ait.ac.at, sten.hanke@ait.ac.at

Abstract
Human computer interaction is increasingly utilized in smart home, industry 4.0 and personal health. Communication between human and computer can benefit by a flawless exchange of emotions. As emotions have substantial influence on cognitive processes of the human brain such as learning, memory, perception and problem solving, emotional interactions benefit different applications. It can further be relevant in modern health care especially in interaction with patients suffering from stress or depression. Additionally rehabilitation applications, guiding patients through their rehabilitation training while adapting to the patients emotional state, would be highly motivating and might lead to a faster recovery. Depending on the application area, different systems for emotion recognition suit different purposes. The aim of this work is to give an overview of methods to recognize emotions and to compare their applicability based on existing studies. This review paper should enable practitioners, researchers and engineers to find a system most suitable for certain applications. An entirely contact-less method is to analyze facial features with the help of a video camera. This is useful when computers, smart-phones or tablets with integrated cameras are included in the task. Smart wearables provide contact with the skin and physiological parameters such as electro-dermal activity and heart related signals can be recorded unobtrusively also during dynamical tasks. Next to unimodal solutions, multimodal affective computing systems are analyzed since they promise higher classification accuracy. Accuracy varies based on the amount of detected emotions, extracted features, classification method and the quality of the database. Electroencephalography achieves 88.86 % accuracy for four emotions, multimodal measurements (Electrocardiography, Electromyography and bio-signals) 79.3 %
for four emotive states, facial recognition 89 % for seven states and speech recognition 80.46 % for happiness and sadness. Looking forward, heart-related parameters might be an option to measure emotions accurately and unobtrusive with the help of smart wearables. This can be used in dynamic or outdoor tasks. Facial recognition on the other hand is a useful contact-less tool when it comes to emotion recognition during computer interaction.
Keywords: emotion recognition, facial recognition, HRV, emotional intelligence, affective computing, state of the art, review


Introduction
The provision of personalized and adaptive solutions is of increasing importance when providing Information and Communications Technology (ICT) services to humans. Besides the ability to sense context the ability to understand the users emotions is therefore of interest. It has been shown that human-machine-interaction

1 These authors contributed equally to this work

https://doi.org/10.1016/j.entcs.2019.04.009 1571-0661/© 2019 Published by Elsevier B.V.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

follows natural and social principles on a human-to-human interaction basic. Ekman
[24] pointed out the impact of reading emotions: “If B perceives A’s facial expression of emotion, B’s behavior toward A may change, and A’s notice of this may influence or determine A’s experience of emotion“ [24]. For a successful affective computing three consecutive procedures are necessary: Detecting the emotions, the emotional behavior generation (synthesis, adaption and expression) and the emotion elicitation (figure 1).



















Fig. 1. Affective loop theory inlcuding emotion detection, emotional behavior generation and emotion elicitation. Image modified from A. Paiva [60].

Emotion recognition can therefore be considered as a crucial machine capacity in human-machine-communications. This review focuses on the technical procedures of emotion detection and their benefits and limitations for specific applications. It has been shown that communication between humans and computers benefits from sensor based emotion recognition since humans feel uncomfortable when emotions are absent [5]. According to Reeves et al. [70] people treat computers the same way as they treat people. Hence, computers should also respond to their users humanely. Benefits of human-computer-interaction in a social context were proven [10]. Emo- tions are further essential for motivation and learning [64]. Affective interaction could be beneficial while studying with the help of computers and could improve ones mental state. Emotion recognition on the side of the artificial system serves the effectiveness of the communication between the machine and its user and fur- ther serves to accomplish a certain goal: A joint action which is ”any form of social interaction whereby two or more individuals coordinate their actions in space and time to bring about a change in the environment” [76] is going on between the artificial system and the human. Joint action should be improved on the side of the artificial system by adapting emotionally to the human. Ciceri et al. [16] explained the management of joint action between humans and computers and its emotional adaption in a graphic (figure 2).
It shows that on the side of the artificial system tasks and actions should modu-













Fig. 2. Joint action between artificial systems and their users based on emotional adaption and joint action (ECA = embodied conversational agent). Image modified from [16].

Table 1
Main emotions defined by psychologists between 1897 and 2001.

late depending on the users emotion for a successful joint action. Not only human- computer-interaction would profit from emotion recognition. It would also help in the field of psychology to identify patients emotions who are unable to express their feelings. For example patients with autism spectrum disorder or patients diagnosed with the locked-in-syndrome could benefit from emotion recognition. Furthermore, the health care sector is increasingly dependent on technological applications and devices. Patients interacting with a virtual avatar, which is able to read and tune according to the users emotions could improve their motivation. For rehabilitative applications this could even lead to a faster and higher recovery success and im- proves the quality of life [81]. Emotions are transmitted in all modalities of human communication like words, tone of voice, facial expression, body language as well as several bio-parameters (as a reaction of the autonomic nervous systems) like heart rate variability, skin conductance etc. Mostly all of these modalities can be mea- sured with different technologies and sensors, since with developing technology the possibilities for automated emotion recognition have been improved as well. These methods vary in their potential regarding which emotions can be detected, their ac- curacy, the options for validating the results as well as their usability under different circumstances.
To avoid mistakes in emotion recognition and to design a reliable set up, a deeper understanding of emotion modeling, processing and its expression is necessary.
As stated by Ekman, Parrot, Frijda, Plutchik, Tomkins and Matsumoto, there are few basic emotions (Table 1) valid for all ages and cultural differences. Ek-

man and Friesen are responsible that six emotions (anger, happiness, fear, surprise, disgust and sadness) are universally recognized [25]. In 1873 Wilhelm Wundt [88] designed a novel three-dimensional emotion classification system: the circumplex model. Three axes describe the valence, arousal and intensity of emotions. To classify emotions many studies refer to the two dimensional model based on the circumplex model, where valence describes the range from negative to positive emo- tions and arousal the active to passive scale. High valence and high arousal displays a pleasant feeling with high activity, which describes emotions such as happiness and excitement [67]. Similar models, such as the Geneva Emotion Wheel find ap- plication in most works regarding emotion measurement [73]. Emotions can then be assigned to the two dimensional emotion system as presented in figure 3.

ACTIVE












PASSIVE

Fig. 3. Two dimension emotion system modified from W. Wundt [88].


According to David G. Myers, emotions include physiological arousal, expres- sive behaviors and conscious experience [55]. Concerning the conscious processing of emotion different theories exist: The James-Lange Theory from 1884 (figure 4 A) suggests that physiological activity takes place before the emotional experience [39]. The Cannon-Bard Theory (figure 4 B) claims that physiological and emotional expe- rience takes place simultaneously without causing each other [14]. This also means that physiological reactions to emotions can be experienced without feeling the emotion. The Schachter-Singer Theory (figure 4 C) also known as the Two-Factor Theory proposes that emotions origin from both the physiological response and its cognitive interpretation depending on the circumstances and experiences [72]. Ac- cording to the Cognitive Appraisal Theory (figure 4 D) also known as the Lazarus Theory of emotion, cognitive labeling of a stimulus occurs before experiencing emo- tions [49]. This theory also implies that the interpretation of the stimulus based on experience and personality is crucial for the intensity and nature of the emotion experienced.


A	B	C	D














Fig. 4. Emotion theories With A: James-Lang Theory [39] B: Cannon-Bard Theory [14] C: Schachter-Singer Theory [72] D: Cognitive Appraisal Theory [49]


Emotions influence the activity of the Autonomous Nervous System (ANS) which in turn regulates various body parameters [80]. Hence, the Heart Rate Variabil- ity (HRV), Electrodermal Activity (EDA), temperature and respiration patterns are often analyzed to recognize emotions. Next to emotions influenced by the ANS, facial expressions are investigated which can be more controlled. Also measurement of brainwaves are investigated since emotion originates in the brain. Most applica- tions are used in everyday situations which desire a handy and unobtrusive method to recognize emotions. For health care applications, computers, tablets or smart- phones with equipped cameras can be used to perform Facial Recognition (FR) with the help of novel algorithms using deep learning. In 1872 Charles Darwin [20] examined how humans and animals express emotions. Based on this, Ekman wrote the book ”Unmasking the Face” in 2003, describing in detail the change of facial features as a result of spontaneous change of emotions [27][23]. For example, the angularity of the lips or the wrinkles at the forehead are analyzed to classify certain emotions. FR is a promising method to classify emotions in an unobtrusive way while achieving high accuracy [61]. On the other hand, it is prone to falsified results since facial expressions can be deliberately be influenced. A less falsified method is emotion recognition from Electroencephalography (EEG). Though, the complexity of the experimental set up is not to be ignored. Some physiological signals are miss- ing the two-dimensional information of both valence and arousal, which makes it necessary to combine two physiological parameters to make assumption about the emotive state.

Emotion recognition
The chain of work for recognizing emotions requires a sequence of tasks starting with the evocation of emotions. Furthermore, the emotional features from the physiological signals have to be defined and extracted. Therefrom, emotions are recognized using trained classifiers. In the following different procedures but also the difficulties and limitations are described.



Fig. 5. Examples from the IAPS to evoke specific emotions [48].
Evocation of Emotions
A high-quality database is important to evoke emotions and to generate comparable results. The International Affective Picture System (IAPS) database uses pictures and is often preferred due to its simplicity [78,48]. Figure 5 shows examples taken from the IAPS to evoke specific emotions. Emotions can be subject-elicited (related to memories of the subject) or event-elicited (subject independent pictures related to events) [64]. The International Affective Digital Sounds (IADS) database utilizes acoustic emotional stimuli [8]. A combination of video and audio data can be used by showing movie/film clips [50,58]. Pictures showing people with different emotional expressions can also be used to evoke the same emotions in the participant by empathy [11]. A database of affective norms provides a large set of normative emotional ratings for English words [7] as well as for a collection of brief texts [8]. Listening to stories can be used to evoke emotions [34]. Tasks inducing cognitive and physical stress can be used to elicit stress related emotions [85].
Feature Extraction
Due to the nature of bio-signals which are partly stochastic, partly semi-periodic, it is necessary to extract certain features to enable further classification of emotions. After measuring the raw bio-signals, specific features are extracted and calculated. Popular features for various bio-signals include frequencies, amplitudes, maxima and minima. For FR features anatomic landmarks or relations between certain regions are exploited. However, each bio-signal has to be investigated separately as extracted features might vary in their usefulness for the classification of emotions.
Classiﬁer
After measuring the bio-signals and the extraction of emotional features, classifiers are trained to identify the emotional states. Popular are the classifiers Support Vector Machine (SVM) [56,54], Canonical Correlation analysis (CCA) [50], Artificial Neural Network (ANN) [82], Fisher linear discriminant projection [36], K-Nearest Neighbor (KNN) [69], Adaptive Neuro-Fuzzy Interference System (ANFIS) [40] or




Fig. 6. Self-assessment manikin from M. Bradley and P. Lang [31] symbolizing the valence (top) and the arousal (bottom) degree.
the Bayesian network method [17]. For the same data different classifier result in different accuracy. The majority of studies uses the SVM or the fisher linear discriminant classifier. Depending on the training set it needs to be tested which of them results in higher accuracy.
Emotion evaluation
Emotion evaluation is a problem, which is not easy to overcome. One way is to correlate the results to a gold standard, such as FR or EEG. However, those readings might be prone to error, as the signals might be evoked by false emotions. Another way to classify emotions measured from various parameters is to compare the results to a self-assessment questionnaire. The Self Assessment Manikin (SAM) from M. Bradley and P. Lang [7] is widely used. It is a pictorial, non-verbal questionnaire to evaluate emotions (figure 6). Another method is to rate the patients emotion based on facial expressions and overall impressions by a psychological expert. Lastly, data can be correlated against the material used to elicit the emotions, such as the IAPS where each one has a defined level of valence and arousal. However, the perception of emotions vary highly depending on experience, cultural differences, age and many other factors, which makes evaluation a challenging task.
Difficulties
Emotion detection from visible parameters such as facial expression, gesture or speech is influenced by the subjects culture [22], age [38] and gender. In some cultures expressions like anger or grief are considered dishonorable and are discour- aged, leading the participant to replace their feeling with a fake smile [29]. Outward physical expressions rely on the manipulation of social masking and lead to falsi- fied emotion classification. The underlying emotional state may only be classified by measuring internal parameters. The Noldus FaceReader software [82] states to work on this issue by classifying so called action units, as described in section 3.2. Multimodal systems might also help to overcome this problem of fake emotions. The ANS reacts according to a persons emotional state and can not be tricked as easily. However, the interpretation of a single parameter comes along with side-effects, such

as the possibility to influence the data by environmental influences such as tempera- ture or humidity. Also individual factors might come into play, whether one person might feel happiness for certain stimuli, when another persons feel shame or anger. There is no definite rule-book for the elicitation, classification or evaluation of hu- man emotions. Also, information about the purpose of the study may influence the subjects experience of emotions. Another fact to consider is that laboratory settings differ from real-world situations and results can not be transferred directly [64]. Classification systems can either be user-dependent or user-independent. User- dependent systems require calibration for each user before classification, while in- dependent systems can recognize emotions from unknown users without individual calibration. User-dependent systems usually achieve higher accuracy, but a user- independent system is admirable for reaching a wider audience. The latter method usually achieves lower accuracy [41] and requires more sophisticated algorithms to detect emotions correctly.
Methods
Several bio-parameters, elicited by the activity of the ANS can be used to acquire data for emotion recognition. Their usability depends on the field of application. Systems vary in accuracy, distinction between the number of recognized emotions and mobility during the measurement. The different measurement methods and their benefits and limitations are described in the following.
Electroencephalography (EEG)
EEG can be used to determine emotions with high accuracy but is best used in a clinical environment due to its time-consuming set-up process and its noise-sensible characteristic. Two-dimensional (valence and arousal) measurements are possible. According to Choppin [15] a high valence correlates to a high frontal alpha power as well as a high right parietal beta power. Arousal is coherent to high beta but low alpha power at the parietal lobe. Negative emotions such as fear cause a right frontal activation while positive emotions show high left frontal activation. Ramirez et al. [68] created the equations 1 and 2 to express the relation between valence and arousal levels depending on the brain wave spectrum at different cortical areas. Mehmood et al. [54] analyzed five frequency bands to classify four emotional states. Using the SVM classifier they achieved 58 % accuracy.


Arousal =
βF 3+ βF 4+ βAF 3+ βAF 4
αF 3+ αF 4+ αAF 3+ αAF 4
(1)

V alence = αF 3 − αF 4	(2)
Facial Recognition (FR)
FR is a powerful tool for contact-less emotion recognition without attaching any sensors to the patient. A web-cam connected to a computing unit is sufficient, mak-

ing FR usable on the fly, for example to measure people passing by or to measure during an interaction with an app or avatar. FR is, similar to emotion recognition with EEG, based on deep learning and allows accuracy in a comparable manner. Every frame of the visual data gets analyzed and certain features (i.e. anatomical landmarks) are extracted. The facial features are then used to train classifiers based on the data of a training set, which then allows the system to estimate emotions [12]. OpenFace is a free and open source project utilizing deep neural networks [3]. It is able to detect facial landmarks, estimate the head pose, the eyes gaze movement and expresses the data in facial action units. Commercial available Software Develop- ment Kits (SDKs) often offer simple solutions for implementing FR in projects. The Affectiva Emotion SDK (Affectiva, Massachusetts, U.S.A.) [53] offers a six-month trial for students and is able to identify 20 expressions and seven emotions based on the Facial Action Coding System (FACS) by Ekman [26], with a broad support for operating systems (iOS, Android, Web, Windows, Linux, macOS, Unity, Raspberry Pi). The Crowdsight SDK (Sightcorp, Amsterdam, Netherlands) offers a free trial and is able to estimate age, gender, ethnicity, head pose, attention span and 6 facial expressions. When it comes to usability, a commercial system called FaceReader [82] by Noldus Information Technology B.V., Wageningen, Netherlands, provides soft- ware to simplify emotion recognition with many additional features. FaceReader recognizes six basic emotions, tracks the eye gaze, tracks 20 action units, allows pre- sentation of stimuli and synchronization with other bio-signals. Additionally, action units are measured based on the connection between certain face areas, such as the lip-, eyebrow- and cheek-movement. The tracking of action units might be utilized to differentiate between real and fake emotions. FaceReader claims to measure fa- cial expressions with an accuracy of 97 % for happy and of 80 % for angry [82]. The FaceReader has been validated using facial Electromyography (EMG) measuring the musculus zygomaticus and musculus corrugator activity. The zygomaticus ac- tivity correlates moderately with the recognition of happiness (r = 0.723) over any other emotion, while the corrugator activity correlates weakly with the recognition of angriness (r = 0.55) [19].

Speech Recognition (SR) and Voice Recognition (VR)
SR can be used to extract semantics from speech [59] while VR is the analysis of the acoustics of spoken words. They are useful for man-machine interactions such as computer tutorials, interactive movies, games or for in-car safety mechanisms, where the system reacts according to the users emotions [74]. Further, it can be used as a diagnostic tool to identify depression and suicidal risks [30]. Also in mobile communication in general it is a tool to detect frustration and stress. Although the usability is high, the accuracy is not comparable to EEG or FR. VR is a challenging task, as it is not clear which features are important for the distinction between emotions. The acoustic variability regarding speaking style and speaking rate directly affect extracted features such as pitch and frequency [4]. Furthermore, one or more emotions might be perceived while the acoustic output sounds the same. Also long-term emotions might overwhelm momentary emotions. For the definition

of emotions through speech according to Williams and Stevens [86], positive arousal correlates with loud, fast and enunciated speech with high-frequency energy, a high average pitch and a wide pitch range. Negative arousal is linked to slow, low-pitched and less high-frequent energy speech. Therefore the extracted acoustic features (pitch, time, quality of voice, articulation) highly correlate with ones feelings [13]. For valence the distinction is difficult and comes with lower accuracy rates, as high valence corresponds with both anger and happiness. Research shows no agreement if acoustic features correlate with the valence dimension [51].
Another valuable study from 2011 gathered speech-, quality- and spectral-related features among others, compared characteristics of emotional speech databases and elaborated speech processing and combining of acoustic features with other infor- mation sources [28].

Heart Rate Variability (HRV)
Heart related bio-signals such as the HRV, a measure of beat-to-beat temporal changes of the heart rate, give deep insight to the emotional system of the human body. Therefore, Electrocardiography (ECG) is measured and HRV calculated. The usability of measuring the ECG in everyday use is limited. Although smart textile garments are available tracking the ECG properly. A novel technique preferable for a mobile use is measuring the Blood Volume Pulse (BVP) by Photoplethysmogra- phy (PPG) which is widely used in smart watches to calculate the HRV. A study concluded that HRV calculated from ECG correlates up to 88 % with calculations from PPG data [65]. An older study achieved 95 % correlation [32]. BVP is de- scribed by the pulse-wave of the heart and the volume of the blood flowing through a vessel. It can be monitored non-invasive with the help of PPG. A PPG sensor uses a light emitting diode and a photo-diode to record the pulse waveform. It can be used almost anywhere on the body, as the smallest superficial blood vessels are sufficient for the sensor to recognize a pulse-wave. PPG is preferable over ECG to track cardiac activity when the patient is moving or due to its easy mounting [42]. A reduced HRV is linked to psychiatric illnesses (depression, anxiety, alcohol use disorders) [44,37]. The heart rate is the most natural choice for arousal detection using comparison of sympathetic and parasympathetic frequency bands of the time series. However, it is highly dependent on the position of the body during monitor- ing. For emotion classification from the HRV different features can be calculated, some are explained in table 2. A study from 2016 selected 5 features (CVRR, LF, HF, LFratio, SD) from a total of 13 features to recognize 5 emotions (sad, angry, fear, happy, relax). The highest accuracy was achieved using a SVM classifier. The best selected feature set achieved an average accuracy of 56.9 % [33]. An other study from 2010 [52] measured BVP, EMG, Electrodermal Activity (EDA), Skin Temperature (SKT) and Respiration (RSP) in a multi-modal approach. A sta- tistical classifier was trained using SVM and the Fisher linear discriminant with the goal to learn the corresponding emotion for the present set of features. Both methods were compared using the extracted feature vectors from multiple subjects (7 male, 3 female) under the same emotional stimuli (a total of six emotions - amuse-

Time domain
Frequency domain

Statistical analysis

Table 2
Features to calculate from HRV for emotion classification [33].

ment, contentment, disgust, fear, no emotion, sadness). An average performance of 28.83 % was achieved using the Fisher linear discriminant and 46.5 % using the SVM classifier.
Electrodermal Activity (EDA)
EDA measures the resistance of the skin and therefore the skin conductivity by applying two electrodes to the finger. This makes it necessary to at least cover the finger-tips with sensors, which could be integrated into a glove for mobile measure- ments. The eccrine glands are responsible for thermo regulation of the human body. Though, eccrine glands of the palm are also known for a more sensitive and emo- tion correlated behavior than other sweat glands. Accordingly, the electro-dermal skin response can be measured at the palm by applying small voltage to the skin [6,75,46]. The skin conductivity decreases during relaxed states, and increases when exposed to effort [84]. Features consider range, amplitude, rise duration, values of first, time and frequency [87,71]. A study from 2010 [83] used a textile smart-glove to recognize arousal based on electro-dermal response. Features such as Root Mean Square (RMS), skewness, kurtosis, frequency, magnitude as well as non-linear fea- tures were extracted and 35 volunteers were tested. The image database IAPS was used to elicit emotions, the Quadratic Bayes Normal Classifier (QDC) to classify the extracted features into four levels of arousal. For a total of 20 features, an average accuracy of 64.32 % was achieved.
Respiration (RSP)
RSP is defined by the thoracal activity and can be measured using resistive wire strains [21], PPG [2] or foam-based pressure sensors [9]. The sensory elements can then be integrated into a textile garment. A decreased respiration frequency in- dicates a relaxed state. Deep and fast breathing can indicate happiness or anger. Momentary interruption of respiration indicate tension. Irregular respiration pat-

terns are a sign of negative valence and arousal, where shallow and rapid respiration suggests concentration or fear. Depressive emotions are connected to shallow and slow respiration patterns [46]. A project from 2017 utilized respiration patterns to recognize emotions based on deep learning algorithms [89] with accuracy for valence and arousal of 73.06 % and 80.78 % respectively, based on the DEAP data set.

Skin Temperature (SKT)
SKT can be used to identify if a person is relaxed or not. Measured on the finger-tip, dilated vessels will make the tip warmer during relaxation or colder during stress or anxiety when the vessels constrict. Similar to recognition with EDA, SKT sensors could be implemented into a glove. A study from 2012 used such a smart-glove and elicited emotions with video-clips taken from movies or public databases. For feature extraction, the skin temperature was converted into an electrical signal. The measured arousal was categorized into five states, which then were translated into emotions. The study stated, that positive states are easier to recognize than negative ones [77]. An other study from Min Woo Park et al. differentiated within sadness and happiness and from analyzing SKT they reached 89.29 % classification accuracy [62].

Electromyography (EMG)
EMG requires the attachment of electrodes to the skin, ideally with preceding clean- ing of the skin to ensure conductivity. This makes EMG preferable in a clinical setup, as most system use wire connections between the electrodes and the receiving computing unit. However, EMG enables single-dimensional valence measurements, making it a valuable addition for other single-dimensional bio signals. Mental stress leads to activation of the sympathetic nervous system which causes higher tension of muscles, especially at the upper trapezius muscle near the neck [46]. Furthermore, facial muscles are measurable (FBS) to detect facial expressions indicating specific emotional states [47]. While contracting, muscles create potentials which can be measured from the surface of the skin. A strong contraction comes along with a high frequency of the EMG signal and is further correlated to an increase of valence [46]. A Japanese study from 2010 proposed a system to recognize emotions based on Galvanic Skin Response (GSR) and EMG, where GSR is linked to the arousal and EMG to the valence of the subjects [57].

Recent Works
The following Table 3 presents recent studies representative for each measurement method. Main keywords which were used for the literature research were: emo- tion recognition, emotion classification, emotion prediction and affective computing. Listed are the used method, number of participants, measured emotion, stimulus, extracted features, classification method and achieved accuracy. Works with im- precise details regarding the features or accuracy calculation were excluded. Also


Table 3
Recent studies concerning emotion recognition measuring physiological data (Electrocardiography (ECG), Heart Rate Variability (HRV), Electroencephalography (EEG), Facial Recognition (FR), Forehead
Bio-Signal (FBS), Speech Recognition (SR), Electrodermal Activity (EDA), Skin Temperature (SKT), Blood Volume Pulse (BVP), Respiration (RSP))

studies concerning emotion recognition from participants suffering from mental dis- orders were not included in this review.
Discussion and Conclusion
Some studies break down their data reasonably, while others are less transparent, making comparison a difficult task. For example the study mentioned in section 3.7 measuring SKT on the finger tips divided their extracted features into linear and non-linear data. The difference for the accuracy of the two data sets was enormous, which is why an average over the two data sets was calculated, combining all ex- tracted features into one statement. Furthermore, the elicitation method plays a crucial role. Randomly selected participants might have a disadvantage over trained


Table 4
Comparison of measurement methods regarding benefits, limitations and application area


participants (i.e. people with a professional theater education) when it comes to emotion expression. Depending on the application area different systems are prefer- able. Table 4 gives an overview of previous elaborated methods.
EEG, FR, SR and heart related methods (ECG, BVP) allow standalone mea- surements as they are independently able to recognize emotions in a two dimensional valence and arousal scale. RSP plays a special role: emotion recognition is limited to the certain states as panic, fear, concentration or depression but to detect a wider spectrum of emotions is not possible. EDA and SKT solely detect the arousal level, while by measuring the EMG only the valence level can be classified. Therefore, it is recommended to combine data from bio signal based methods to be able to recognize the full range of emotions. This multimodal approach provides the ad- vantage of higher accuracy since emotions are classified by each method and are then matched up, though the set up is more complex. For clinical research, a static setup measuring EEG, EMG, ECG or BVP might be suitable, as it allows measure- ments with high accuracy and a diverse emotion classification. A similar system was

proposed in 2004 by Haag [35] who achieved an accuracy of 89.73 % and 63.76 % for arousal and respectively valence. When increasing the bandwidth of the clas- sification, meaning the search interval in which the emotion is expected, accuracy of 96.58 % (arousal) and 89.93 % (valence) was achieved. However, the system is prone to movement artifacts due to the sensitivity of EEG. A study from 2008 [43] optimized their system for emotion recognition for race car drivers by utilizing facial EMG, ECG, RSP and EDA with an overall performance of 79.3 % accuracy for five emotional states (high stress, low stress, disappointment, euphoria, neutral). This system might be suitable for dynamical situations where the subject is in motion. For the observation of a person in a mobile manner, for example in elderly care, ECG might be a preferable solution. Mobile garments, for example the Ambiotex-Shirt, ambiotex GmbH, Tu¨bingen, Germany, allows measurement of ECG and therefore the extraction of HRV features [79] [31]. With the help of ECG and respectively HRV the distinction of five emotions with an accuracy of 56.9 % is possible. This might be used to monitor patients during every-day and outdoor activities. For home use, or the use in a medical or psychological practise, remote and unobtrusive emotion recognition might be the best solution for measuring emotions on the fly. This might happen either by interacting with an avatar through a PC, smart-phone or tablet, or by mounting a web-cam in the room for continuous measurements. FR can provide high accuracy of up to 89 % for seven emotional states, achieved with the Noldus FaceReader [82]. This seems to be the most developed algorithm for face recognition. SR might fulfill a similar purpose, but when it comes to dif- ferentiation between many emotions the accuracy suffers drastically (from 80.46 % for only happy and sad to 49 % for six emotions [17]). In the case of emotion recognition with the remaining mentioned bio-signals, the biggest obstacle is that some signals only measure in one dimension (only valence or only arousal). EMG measures valence, while i.e. EDA and SKT measure arousal. Those signals could be combined to enable two-dimensional statements and respectively assumptions of emotions. However, with HRV, RSP and BVP two-dimensional measurement (both valence and arousal) is possible. Therefore, BVP seems to be a highly valuable pa- rameter for future research, as pulse rate variability measured with PPG has a high correlation of over 95 % with HRV measured with ECG [32]. If such correlations can be proven for BVP measurements taken from the wrist, emotion recognition with the sole use of a smart-watch would be possible. Regarding the accuracy pre- sented in table 3 it is important to note, that the quality of data is crucial for the outcome of the work. The quality of data is related to the application of sensors, lightning conditions, individual state of health of the participants, pre-selection of participants and many more complex factors, depending on the used method. Also, it is important to consider if the data is recorded in a laboratory setting or during an realistic everyday situation. The study from Agrafioti et al [1], based on ECG compared two experimental setups: an active situation by playing a video game and passive setting by using visual stimuli. Both were conducted in a laboratory setting at the Affect and Cognition Laboratory, University of Toronto. The study working with HRV from Guo et al. [33] used a mobile garment for acquiring an ECG signal in a laboratory setting, while the participants watched video clips, assumingly in a

sitting position. The mobile garment might be a cause for the mediocre accuracy. The study from Mehmood et al. [54] measuring EEG used the Emotiv-EPOC, Emo- tiv Inc., San Francisco, CA in a laboratory setting. However, using EEG in a mobile setup is not advisable as the attachment of the sensors is sensible to shock and move- ment. The study using FR [82] did not explicitly state their measuring setup. For FR it might be irrelevant whether the measurement happens in a real or laboratory condition, as long as the lightning conditions suffice to distinguish the shapes of the face from the background. The study measuring ECG and FBS by Naji et al. [56] used a comfortable chair in a quiet room with minimal light for their measurement and a headphone to play music. Notable is the relatively low accuracy for the mea- surement with sole ECG data. For recording the BIOPAC MP100 system, Biopac Inc., Santa Barbara, CA was used. Dai et al. [17] analyzed SR. Utterances from professional actors were recorded directly into WAVES+ data files with a sampling rate of 22.05 kHz. The relatively high accuracy for happy and sad might depend on the ability of the actors to communicate their emotions in a precise and dis- tinctive way. The data from the multimodal FR, SR and gesture measurement [45] was simultaneously recorded during a summer school of the HUMAINE EU-IST project in Genova. One camera recorded the participants faces (facial recognition) and a second the silhouettes (gesture recognition). Restrictions regarding the par- ticipants clothing and behaviour were given, the location and lightning conditions of the setup was not stated. This study proves that the fusion of multimodal data benefits the overall prediction rate. The multimodal study from 2004 [35] used the sensor set ProComp+, Thought Technology Ltd., Quebec, Canada, for acquiring their data in a laboratory setting. The study stated that their form and method of application is anything but intuitive and natural, therefore accuracy rates might drop in a mobile setup. In resemblance the multimodal study from 2008 [43] used a multi-sensoral wearable which was assumingly evaluated in a laboratory setting. It consists of four parts: a balaclava containing the facial EMG, ECG and RSP on the thorax, EDA inside a glove and a data acquisition and communication module. The system was designed for race drivers. It would be interesting to see how the system performs when measuring basic emotions. The last multimodal study from 2010 [52] used the PROCOMP Infiniti system, Thought Technology Ltd., Quebec, Canada in a laboratory setting, where the participants were requested to be as re- laxed as possible. The study used no ECG data, which might explain the mediocre accuracy.
To summarize regarding the mobility in emotion recognition, with smart wear-
ables such as smart watches or smart fabrics, it is possible to measure ECG, EDA, SKT, BVP and RSP data in a mobile and unobtrusive way. The Empatica E4 wristband, Empatica Inc., Cambridge, MA has developed a novel method to mea- sure PPG and EDA from the wrist. Methods like FR and SR solely depend on background information (lightning, noise). With smart wearables being an upcom- ing trend in modern society previous stationary methods might be utilized more during everyday activities and unobtrusive emotion recognition.

Acknowledgement
This work has been funded by the AAL Joint Program project FollowMe (Project. No.: AAL-2015-2-108). The authors would like to thank the members of the projects consortium for their valuable inputs.

References
Agrafioti, F., D. Hatzinakos and A. K. Anderson, ECG pattern analysis for emotion detection, IEEE Transactions on Affective Computing 3 (2012), pp. 102–115.
Axisa, F., A. Dittmar and G. Delhomme, Smart clothes for the monitoring in real time and conditions of physiological, emotional and sensorial reactions of human, , 4 (2003), pp. 3744–3747.
Baltrusaitis, T., OpenFace: an open source facial behaviour analysis toolkit, Github (2018).
URL https://github.com/TadasBaltrusaitis/OpenFace

Banse, R. and K. R. Scherer, Acoustic Profiles in Vocal Emotion Expression, Journal of Personality and Social Psychology 70 (1996), pp. 614–636.
Beale, R. and C. Peter, The role of affect and emotion in HCI, in: Lecture Notes in Computer Science (including subseries Lecture Notes in Artificial Intelligence and Lecture Notes in Bioinformatics), 2008,
pp. 1–11.
Benedek, M. and C. Kaernbach, A continuous measure of phasic electrodermal activity, Journal of Neuroscience Methods 190 (2010), pp. 80–91.
Bradley, M. and P. J. Lang, Measuring emotion: The self-assessment manikin and the semantic differential, Journal of Behavior Therapy and Experimental Psychiatry 25 (1994), pp. 49–59.
URL http://www.sciencedirect.com/science/article/pii/0005791694900639

Bradley, M. M. and P. J. Lang, The International Affective Digitized Sounds Affective Ratings of Sounds and Instruction Manual, Technical report B-3. University of Florida, Gainesville, Fl. (2007),
p. 2946.
URL	http://scholar.google.com/scholar?hl=en&btnG=Search&q=intitle:The+International+
Affective+Digitized+Sounds+Affective+Ratings+of+Sounds+and+Instruction+Manual#1

Brady, S., L. E. Dunne, R. Tynan, D. Diamond, B. Smyth and G. M. O’Hare, Garment-based monitoring of respiration rate using a foam pressure sensor, , 2005, 2005, pp. 214–215.
Breazeal, C., Emotion and Sociable Humanoid Robots, Int. J. Hum.-Comput. Stud. 59 (2003), pp. 119– 155.
URL  http://web.media.mit.edu/~cynthiab/Papers/Breazeal-ijhcs02-final.pdf

Brennan, A. M., A. W. F. Harris and L. M. Williams, Neural processing of facial expressions of emotion in first onset psychosis, Psychiatry Research 219 (2014), pp. 477–485.
URL http://dx.doi.org/10.1016/j.psychres.2014.06.017

Busso, C., Z. Deng, S. Yildirim, M. Bulut, C. M. Lee, A. Kazemzadeh, S. Lee, U. Neumann and S. Narayanan, Analysis of emotion recognition using facial expressions, speech and multimodal information, in: Proceedings of the 6th international conference on Multimodal interfaces - ICMI ’04 (2004), p. 205.
URL http://portal.acm.org/citation.cfm?doid=1027933.1027968
Cahn, J., The Generation of Affect in Synthesized Speech, Journal of the American Voice I/O Society
8 (1990), pp. 1–19.
URL http://media.mit.edu/speech/papers/1990/cahn_AVIOSJ90_affect.pdf

Cannon, W. B., The James-Lange theory of emotions: a critical examination and an alternative theory. By Walter B. Cannon, 1927., The American journal of psychology (1987).
Choppin, A., EEG-Based Human Interface for Disabled Individuals : Emotion Expression with Neural Networks Submitted for the Master Degree, Emotion (2000).
Ciceri, R. and S. Balzarotti, From signals to emotions: Applying emotion models to HM affective interactions., Affective Computing. InTech 3 (2008), p. 978.


Dai, K., H. J. Fell and J. MacAuslan, Recognizing emotion in speech using neural networks, in: Proceedings of the 4th IASTED International Conference on Telehealth and Assistive Technologies, Telehealth/AT 2008, 2008, pp. 31–36.
URL  http://www.scopus.com/inward/record.url?eid=2-s2.0-62649110768&partnerID=tZOtx3y1
Damasio, A. R., The Emotions, Nature 110 (1922), pp. 730–731.
D’Arcey, J. T. and J. Trevor, “Assessing the validity of FaceReader using facial EMG,” Master’s thesis, Faculty of California State University, Chico (2013).
URL  http://csuchico-dspace.calstate.edu/handle/10211.3/10211.4_610

Darwin, C., “The expression of the emotions in man and animals.” American Association for the Advancement of Science, 1872, 351 pp.
URL  http://content.apa.org/books/10001-000

De Rossi, D., F. Carpi, F. Lorussi, A. Mazzoldi, R. Paradiso, E. P. Scilingo and A. Tognetti, Electroactive fabrics and wearable biomonitoring devices, Autex Research Journal 3 (2003), pp. 180–185.
Ekman, P., Universals and cultural differences in facial expressions of emotion, Nebraska Symposium on Motivation 19 (1971), pp. 207–283.
Ekman, P., Facial expression, Nonverbal behavior and communication NJ (1977), pp. 97–116.
Ekman, P., W. V. Freisen and S. Ancoli, Facial signs of emotional experience, Journal of Personality and Social Psychology 39 (1980), pp. 1125–1134.
Ekman, P. and W. V. Friesen, Constants across cultures in the face and emotion, Journal of Personality and Social Psychology 17 (1971), pp. 124–129.
Ekman, P. and W. V. Friesen, Facial Action Coding System: A Technique for the Measurement of Facial Movement, in: Consulting Psychologists Press, Palo Alto, Salt Lake City, 1978 .
Ekman, P. and W. V. Friesen, “Unmasking the face: A guide to recognizing emotions from facial clues,” Ishk, 2003.
El Ayadi, M., M. S. Kamel and F. Karray, Survey on speech emotion recognition: Features, classification schemes, and databases, Pattern Recognition 44 (2011), pp. 572–587.
URL http://dx.doi.org/10.1016/j.patcog.2010.09.020

Fragopanagos, N. and J. G. Taylor, Emotion recognition in human-computer interaction, Neural Networks 18 (2005), pp. 389–405.
France, D. J. and R. G. Shiavi, Acoustical properties of speech as indicators of depression and suicidal risk, IEEE Transactions on Biomedical Engineering 47 (2000), pp. 829–837.
Gradl, S., T. Cibis, J. Lauber, R. Richer, R. Rybalko, N. Pfeiffer, H. Leutheuser, M. Wirth, V. von Tscharner and B. Eskofier, Wearable Current-Based ECG Monitoring System with Non-Insulated Electrodes for Underwater Application, Applied Sciences 7 (2017), p. 1277.
URL http://www.mdpi.com/2076-3417/7/12/1277

Greve, M., E. Kviesis-kipge, O. Rubenis, U. Rubins and V. Mecnika, Comparison of Pulse Rate Variability Derived from Digital Photoplethysmography over the Temporal Artery with the Heart Rate Variability Derived from a Polar Heart Rate Monitor, Proceedings of the 7Th conference of European Study Group on Cardiovascular Oscillations (2012), pp. 1–3.
Guo, H.-W., Y.-S. Huang, C.-H. Lin, J.-C. Chien, K. Haraikawa and J.-S. Shieh, Heart Rate Variability Signal Features for Emotion Recognition by Using Principal Component Analysis and Support Vectors Machine, 2016 IEEE 16th International Conference on Bioinformatics and Bioengineering (BIBE) (2016), pp. 274–277.
URL http://ieeexplore.ieee.org/document/7789995/

H. R. Lv, W. J. Y. J. D., Z. L. Lin, Emotion recognition based on pressure sensor keyboards, 2008 IEEE International Conference on Multimedia and Expo (2008), pp. 1089–1092.
URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4607628

Haag, A., S. Goronzy, P. Schaich and J. Williams, “Emotion Recognition Using Bio-sensors: First Steps towards an Automatic System,” Springer, Berlin, 2004, 36–48 pp.
URL  http://link.springer.com/10.1007/978-3-540-24842-2_4
Healey, J. and R. Picard, Digital processing of affective signals, , 6, 1998, pp. 3749–3752.
Ingjaldsson, J. T., J. C. Laberg and J. F. Thayer, Reduced heart rate variability in chronic alcohol abuse: Relationship with negative mood, chronic thought suppression, and compulsive drinking, Biological Psychiatry 54 (2003), pp. 1427–1436.


Isaacowitz, D. M., C. E. L¨ockenhoff, R. D. Lane, R. Wright, L. Sechrest, R. Riedel and P. T. Costa, Age differences in recognition of emotion in lexical stimuli and facial expressions, Psychology and Aging 22 (2007), pp. 147–159.
James, W., What is an Emotion?, Mind (1884).
Jang, J. . R., ANFIS: adaptive-network-based fuzzy inference system, IEEE Transactions on Systems, Man, and Cybernetics 23 (1993), pp. 665–685.
Jerritta, S., M. Murugappan, R. Nagarajan and K. Wan, Physiological signals based human emotion Recognition: a review, in: 2011 IEEE 7th International Colloquium on Signal Processing and its Applications (2011), pp. 410–415.
URL http://ieeexplore.ieee.org/document/5759912/
Jones, D., The Blood Volume Pulse - Biofeedback Basics Fixxl Ltd (2018).
URL	https:
//www.biofeedback-tech.com/articles/2016/3/24/the-blood-volume-pulse-biofeedback-basics

Katsis, C. D., N. Katertsidis, G. Ganiatsas and D. I. Fotiadis, Toward emotion recognition in car-racing drivers: A biosignal processing approach, IEEE Transactions on Systems, Man, and Cybernetics Part A:Systems and Humans 38 (2008), pp. 502–512.
Kemp, A. H., D. S. Quintana, R. L. Kuhnert, K. Griffiths, I. B. Hickie and A. J. Guastella, Oxytocin Increases Heart Rate Variability in Humans at Rest: Implications for Social Approach-Related Motivation and Capacity for Social Engagement, PLoS ONE 7 (2012).
Kessous, L., G. Castellano and G. Caridakis, Multimodal emotion recognition in speech-based interaction using facial expression, body gesture and acoustic analysis, Journal on Multimodal User Interfaces 3 (2010), pp. 33–48.
Kim, J. and E. Andr´e, Emotion recognition based on physiological changes in music listening, IEEE Transactions on Pattern Analysis and Machine Intelligence 30 (2008), pp. 2067–2083.
Kulic, D. and E. Croft, Affective State Estimation for HumanRobot Interaction, IEEE Transactions on Robotics 23 (2007), pp. 991–1000.
Lang, P. J., M. M. Bradley and B. N. Cuthbert, International affective picture system (IAPS): Affective ratings of pictures and instruction manual. Technical Report A-8., Technical Report A-8. (2008),
pp. Technical Report A–8.
Lazarus, R. S., Thoughts on the relations between emotion and cognition, American Psychologist (1982).
Li, L. and J.-h. Chen, Emotion Recognition Using Physiological Signals from Multiple Subjects, in: 2006 International Conference on Intelligent Information Hiding and Multimedia, 2006, pp. 355–358.
URL http://ieeexplore.ieee.org/lpdocs/epic03/wrapper.htm?arnumber=4041736

Liscombe, J. J., Prosody and speaker state: Paralinguistics, pragmatics, and proficiency, ProQuest Dissertations and Theses (2007), p. 218.
URL	http://search.proquest.com/docview/304862547?accountid=13042%5Cnhttp://oxfordsfx.
hosted.exlibrisgroup.com/oxford?url_ver=Z39.88-2004&rft_val_fmt=info:ofi/fmt:kev:mtx: dissertation&genre=dissertations+%26+theses&sid=ProQ:ProQuest+Dissertations+%26+Theses+Gl

Maaoui, C. and A. Pruski, Emotion recognition through physiological signals for human-machine communication, Cutting Edge Robotics (2010), pp. 317–333.
URL	http://www.intechopen.com/source/pdfs/12200/InTech-Emotion_recognition_through_
physiological_signals_for_human_machine_communication.pdf

McDuff, D., A. Mahmoud, M. Mavadati, M. Amr, J. Turcot and R. e. Kaliouby, AFFDEX SDK: A Cross-Platform Real-Time Multi-Face Expression Recognition Toolkit, Proceedings of the 2016 CHI Conference Extended Abstracts on Human Factors in Computing Systems - CHI EA ’16 (2016),
pp. 3723–3726.
Mehmood, R. M. and H. J. Lee, A novel feature extraction method based on late positive potential for emotion recognition in human brain signal patterns, Computers and Electrical Engineering (2016).
Myers, D. G., “Psychology Chapter 13,” Worth Publishers, New York, USA, 1998, fifth edit edition, 610 pp.
Naji, M., M. Firoozabadi and P. Azadfallah, Classification of Music-Induced Emotions Based on Information Fusion of Forehead Biosignals and Electrocardiogram, Cognitive Computation 6 (2014),
pp. 241–252.
Nakasone, A., H. Prendinger and M. Ishizuka, Emotion Recognition from Electromyography and Skin Conductance, The 5th International Workshop on Biosignal Interpretation (2005), pp. 219–222.

Nasoz, F., K. Alvarez, C. L. Lisetti and N. Finkelstein, Emotion recognition from physiological signals using wireless sensors for presence technologies, Cognition, Technology & Work 6 (2004), pp. 4–14.
URL  http://link.springer.com/10.1007/s10111-003-0143-x

Nicholson, J., K. Takahashi and R. Nakatsu, Emotion Recognition in Speech Using Neural Networks, Neural Computing & Applications 9 (2000), pp. 290–296.
URL http://link.springer.com/10.1007/s005210070006
Paiva, A., I. Leite and T. Ribeiro, Emotion Modelling for Social Robots, Ict.Usc.Edu (2012).
Pantic, M. and L. J. Rothkrantz, Toward an affect-sensitive multimodal human-computer interaction, Proceedings of the IEEE 91 (2003), pp. 1370–1390.
URL http://ieeexplore.ieee.org/document/1230215/

Park, M. W., C. J. Kim, M. Hwang and E. C. Lee, Individual emotion classification between happiness and sadness by analyzing photoplethysmography and skin temperature, in: Proceedings - 2013 4th World Congress on Software Engineering, WCSE 2013, 2013, pp. 190–194.
Parrott, W. G., “Emotions in Social Psychology,” Psychology Press, 2000, 378 pp.
Picard, R. W., E. Vyzas and J. Healey, Toward machine emotional intelligence: Analysis of affective physiological state, IEEE Transactions on Pattern Analysis and Machine Intelligence 23 (2001),
pp. 1175–1191.
Pinheiro, N., R. Couceiro, J. Henriques, J. Muehlsteff, I. Quintal, L. Goncalves and P. Carvalho, Can PPG be used for HRV analysis?, in: 2016 38th Annual International Conference of the IEEE Engineering in Medicine and Biology Society (EMBC) (2016), pp. 2945–2949.
URL http://ieeexplore.ieee.org/document/7591347/

Plutchik, R., The nature of emotions: Human emotions have deep evolutionary roots, American Scientist 89 (2001), pp. 344–350.
Posner, J., J. A. Russell and B. S. Peterson, The circumplex model of affect: An integrative approach to affective neuroscience, cognitive development, and psychopathology, Development and Psychopathology 17 (2005), pp. 715–734.
URL http://www.journals.cambridge.org/abstract_S0954579405050340

Ramirez, R., M. Palencia-Lefler, S. Giraldo and Z. Vamvakousis, Musical neurofeedback for treating depression in elderly people, Frontiers in Neuroscience 9 (2015).
Rani, P., C. Liu, N. Sarkar and E. Vanman, An empirical study of machine learning techniques for affect recognition in human-robot interaction, Pattern Analysis and Applications (2006).
Reeves, B. and C. Nass, “The Media equation: how people treat computers, television, and new media,” Cambridge University Press, 1997.
Rigas, G., C. D. Katsis, G. Ganiatsas and D. I. Fotiadis, A User Independent, Biosignal Based, Emotion Recognition Method, in: C. Conati, K. McCoy and G. Paliouras, editors, User Modeling 2007 (2007),
pp. 314–318.
URL  http://link.springer.com/10.1007/978-3-540-73078-1_36
Schachter, S. and J. E. Singer, Psychological Review., Psychological Review (1962).
Scherer, K. R., What are emotions? and how can they be measured? (2005).
URL  https://www.affective-sciences.org/files/6914/6720/3988/2005_Scherer_SSI.pdf

Schuller, B., G. Rigoll and M. Lang, Speech emotion recognition combining acoustic features and linguistic information in a hybrid support vector machine - belief network architecture, Acoustics, Speech, and Signal Processing 1 (2004), pp. 577–580.
URL file:///C:/Users/mjb228/Downloads/01326051.pdf

Schwartz, S., G. E., Social psychophysiology, in: Encyclopedia of Social Psychology, Academic Press, 1973 pp. 377–416.
Sebanz, N., H. Bekkering and G. Knoblich, Joint action: Bodies and minds moving together (2006).
Shivakumar, G. and P. a. Vijaya, Emotion Recognition Using Finger Tip Temperature: First Step towards an Automatic System, International Journal of Computer and Electrical Engineering 4 (2012),
pp. 252–255.
Silva, C., A. C. Ferreira, I. Soares and F. Esteves, Emotions under the skin autonomic reactivity to emotional pictures in insecure attachment, Journal of Psychophysiology 29 (2015), pp. 161–170. URL
https://www2.unifesp.br/dpsicobio/adap/instructions.pdfhttp://www.unifesp.br/dpsicobio/   adap/instructions.pdf%5Cnhttp://econtent.hogrefe.com/doi/abs/10.1027/0269-8803/a000147


Tantinger, D., M. Zrenner, N. R. Lang, H. Leutheuser, B. M. Eskofier, C. Weigand and M. Struck, Human authentication implemented for mobile applications based on ECG-data acquired from sensorized garments, , 42, 2015, pp. 417–420.
Tecce, J., 21, Psychophysiology, 1996, (5th ed.) edition, 61–62 pp. URL
http://ovidsp.ovid.com/ovidweb.cgi?T=JS&PAGE=reference&D=psyc5&NEWS=N&AN=2006-13464-000

Tivatansakul, S., M. Ohkura, S. Puangpontip and T. Achalakul, Emotional healthcare system: Emotion detection by facial expressions using Japanese database, in: 2014 6th Computer Science and Electronic Engineering Conference, CEEC 2014 - Conference Proceedings (2014), pp. 41–46.
URL http://ieeexplore.ieee.org/document/6958552/
Uyl, M. J. D. and H. V. Kuilenburg, The FaceReader : Online facial expression recognition, Psychology
2005 (2005), pp. 589–590.
Valenza, G., A. Lanat, E. P. Scilingo and D. De Rossi, Towards a smart glove: Arousal recognition based on textile Electrodermal Response, in: 2010 Annual International Conference of the IEEE Engineering in Medicine and Biology Society, EMBC’10, 2010, pp. 3598–3601.
Villarejo, M. V., B. G. Zapirain and A. M. Zorrilla, A stress sensor based on galvanic skin response (GSR) controlled by ZigBee, Sensors (Switzerland) 12 (2012), pp. 6075–6101.
URL http://rstb.royalsocietypublishing.org/content/364/1535/3575.short

Vizer, L. M., L. Zhou and A. Sears, Automated stress detection using keystroke and linguistic features: An exploratory study, International Journal of Human Computer Studies 67 (2009), pp. 870–886.
URL http://dx.doi.org/10.1016/j.ijhcs.2009.07.005

Williams, C. E. and K. N. Stevens, Emotions and Speech: Some Acoustical Correlates, The Journal of the Acoustical Society of America 52 (1972), pp. 1238–1250.
URL http://asa.scitation.org/doi/10.1121/1.1913238

Wu, G., G. Liu and M. Hao, The analysis of emotion recognition from GSR based on PSO, in: Proceedings - 2010 International Symposium on Intelligence Information Processing and Trusted Computing, IPTC 2010, 2010, pp. 360–363.
Wundt, W., Principles of physiological psychology, 1873, in: Readings in the history of psychology, East Norwalk, CT, US: Appleton-Century-Crofts., 1948 pp. 248–250.
Zhang, Q., X. Chen, Q. Zhan, T. Yang and S. Xia, Respiration-based emotion recognition with deep learning, Computers in Industry 92-93 (2017), pp. 84–90.
