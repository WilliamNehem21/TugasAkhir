







Full length article
An intelligent validation system for diagnostic and prognosis of ultrasound fetal growth analysis using Neuro-Fuzzy based on genetic algorithm
Prabhpreet Kaur ⇑, Gurvinder Singh, Parminder Kaur
Department of Computer Engineering & Technology, Department of Computer Science, Guru Nanak Dev University, Amritsar, Punjab, India



a r t i c l e  i n f o 

Article history:
Received 21 January 2018
Revised 2 September 2018
Accepted 7 October 2018
Available online 26 October 2018

Keywords:
Ultrasound (US)
Artificial Neural Network (ANN) Computer-Aided Diagnostic (CAD) Wavelet
Normal-Shrink Classification learner
a b s t r a c t 

Acquisition of the standard plane is the prerequisite of biometric measurement and diagnosis during the ultrasound (US) examination. Based upon the analysis of existing algorithms for the automatic fetal development measurement, a new algorithm known as Neuro-Fuzzy based on genetic algorithm is devel- oped. Firstly, the fetal ultrasound benchmark image is auto-pre-processed using Normal Shrink Homomorphic technique. Secondly, the features are extracted using Gray Level Co-occurrence Matrix (GLCM), Grey Level Run Length Matrix (GLRLM), Intensity Histogram (IH) and Rotation Invariant Moments (IM). Thirdly, Neuro-Fuzzy using Genetic approach is used to distinguish among the fetus growth as abnormal or normal. Experimental results using benchmark and live dataset demonstrate that the developed method achieves an accuracy of 97% as compared to the state-of- art methods in terms of parameters such as Sensitivity, Specificity, Recall, F-Measure &Precision Rate. The use of area under the receiver of characteristics(AUC) and confusion matrix as assessment indicators is also cross-validated using various classification methods by achieving best accuracy rate of Support Vector Machine (SVM)
i.e. 98.7% as compare to other classification methods such as KNN, Ensemble methods, Linear Discriminant Analysis(LDA) and Decision Tree whereas ROC curve covers 0.9992 SVM.
© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo
University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/
licenses/by-nc-nd/4.0/).





Introduction

There is a significant advancement in the analysis of ultrasound imaging and now use of modern machine learning into the medical image analysis field, this art is more challenging for the researchers [1]. Computer Aided Diagnosis (CAD) has become one of the major research subjects in medical imaging and diagnostic radiology. In this research paper, the motivation and philosophy for early devel- opment of CAD are presented together with the current status and future potential of CAD. The general research framework proposed to further investigation and development. This framework is inspired by road sign detection and is being extended and applied

* Corresponding author.
E-mail address: prabhpreet.cst@gndu.ac.in (P. Kaur).
Peer review under responsibility of Faculty of Computers and Information, Cairo University.
this work for liver cancer diagnosis and progression. Ultrasound imaging modality is quite popular and most widely used modality for visualizing and studying the medical images for any disease conditions without causing any pain or discomfort to the patient [2,21,23]. Ultrasound imaging is widely used due to less costly and non-persistent nature as compared to other imaging modali- ties. The diagnosis is performed on various diseases based on image features such as the Echogenicity, Legion Shape, and Echo Texture [3]. Accurate ultrasound-based fetal biometric measure- ments are important for delivery of high quality obstetrical health care. Common measurements include: the Bi-Parietal Diameter (BDP), Head Circumference (HC), Abdominal Circumference (AC), Femur Length (FL), Humerus Length (HL), and the Crown Rump Length (CRL). The American Institute of Ultrasound in Medicine (AIUM) publishes guidelines for measuring these values. These val- ues help diagnose fetal pathology including growth restriction, microcephaly, and macrosomia. In addition, these are utilized to estimate the gestational age (GA) of the fetus (i.e., length of preg- nancy in weeks and days). Accurate estimation of GA is important to determine the expected delivery date, assess the fetal size and monitor fetal growth as in Fig. 1 [4,45–48,50,51,57].



https://doi.org/10.1016/j.eij.2018.10.002
1110-8665/© 2019 Production and hosting by Elsevier B.V. on behalf of Faculty of Computers and Information, Cairo University. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).




Fig. 1. Fetal Ultrasound images of (a) head, (b) femur, (c) abdomen, (d) whole fetus of age 13 weeks.


The study explained the fetal ultrasound image segmentation as a semi-automated system to better estimate the fetal weight (EFW) [5]. Four homogeneous fetal parameters are measured by the mentioned system: abdominal circumference, abdominal cir-
cumference, head circumference and biparietal diameter. The dif- ferent classification algorithms used to evaluate EFW, head and abdomen measurements are support vector machine, regression analysis and neural network. The fundamental goal is to get a




Table 1
Related work of Medical Ultrasound Image Denoising with Data Mining Techniques.


period proficient morphology-based calculation to perceive femur form in fetal ultrasound pictures, refine its shape for programmed length estimation, and in this way, achieving precision and repro- ducibility of estimation [6]. The pictures acquired from the sub- jects were at first prepared utilizing morphological administrators to expel the foundation from the picture. From that point, to refine the state of the femur, the pictures were trans- formed, utilizing the morphological administrators, till a solitary pixel – wide skeleton of the femur was accessible in the most time-powerful way. The skeleton-end-indicates are expected be the femur end-focuses, and the femur length is computed as the separation between the end-focuses to appraise gestational age. The proposed calculation has been tried on genuine clinical images,
and has demonstrated that the estimations made by the proposed strategy are reliable and in great concurrence with the manual technique for estimation. The investigation of two strategies, one the locale developing and another, a variation of split and consol- idation calculations for division groupings of fetal ultrasound pic- tures is discussed in [7]. They depict an intuitive framework that can quickly process and section a self-assertive number of high- lights. The UI was assembled utilizing the Tcl/Tk toolbox, which is publically accessible. The greatest shortcoming of the framework is the absence of compelling measures that can assess the exact- ness of the division. It likewise calls for procedures that are more tolerant of the commotion and antiques since district developing calculation is profoundly touchy to the nearby neighborhood.





Fig. 2. Baseline of Literature Review of Ultrasound Fetus Development from increasing automated image analysis on Y-axis from left to right along with the Ultrasound Detection Protocol on X-axis.




Fig. 3. Proposed Framework.


Another probability would portray of the limit utilizing Binary Space Partitioning tree (BSP). The full programmed division and utilizing factual highlights recognizes ordinary and ultrasonic tumor liver pictures are proposed in [8]. The speckle noise in ultra- sound pictures was separated utilizing pinnacle and valley strat- egy, picture was smoothened and again a moment organize channel was embraced to additionally channel the commotion and to enhance the nature of picture. The peak and valley channel speaks to an intriguing swap for the middle channel keeping in mind the end goal to enhance the effectiveness. The utilization of contourlet change for de-speckling the ultrasound pictures of ovar- ies, dynamic forms without edge strategy for division and fluffy rationale for characterization represented in [9]. The experimenta- tion has been finished utilizing test ultrasound pictures of ovaries and the outcomes are contrasted and the deductions drawn by interim based classifier and furthermore those drawn by the experts.
The extracted picture highlights utilizing distinctive calcula- tions that are indicated with building models with inner modules is illustrated in [10]. In this a novel FPGA-based engineering for
constant extraction of four GLCM highlights was actualized. A 128 * 128 size dim level picture was taken for include extraction. The Digital picture highlights were extricated utilizing measurable second request technique. Utilizing this measurable technique we got the picture extraction in dark scale level examination. In this examination numerous essential highlights of ultrasound kidney pictures have been separated, for example, Intensity Histogram (IH) include, Invariant minutes (IM), Gray level co-event grids (GLCM), Gray level run length networks (GLRLM) and ‘Consoli- dated’ list of capabilities was create from mix of all the four high- lights [11]. Altogether, 48 highlights of each picture were ascertained. Consolidated element achieved a 100% exactness rate in preparing datasets. It effectively arranged 94 examples from 94 occurrences. On testing datasets it accurately ordered 35 occasions out of 40 with a 87.5% exactness rate. This uncovered the use of COMBINED element was moderately powerful. A technique pro- posal for arrangement of liver ultrasound pictures in light of sur- face investigation is experimented [12]. It utilizes an arrangement of seven surface highlights having high discrimina- tive power which can be utilized by radiologists to characterize

the liver. Highlight extraction is done utilizing the accompanying surface models: Spatial Gray Level Co-event Matrix, Gray Level Dif- ference Statistics, First request Statistics, Fourier Power Spectrum, Statistical Feature Matrix, Law’s Texture Energy Measures and Fractal Features. In light of the after effects of Linear Discriminative Analysis (LDA) trailed by box-plot examination and Pearson’s con- nection coefficient, 7 best highlights from an arrangement of 35 highlights are chosen. These chose highlights are then melded uti- lizing a direct classifier. A proposed object was to build up a mod- ernized, non obtrusive strategies for the programmed conclusion of HCC, in view of data acquired from ultrasound pictures [13]. The part analyzed that the prevalent request ‘‘Gray Level Co-event Matrices (GLCM)” and the interrelated parameters have in the change of HCC portrayal and evaluated. The best spatial relations between the pixels that incite the most imperative shows, for the third, fifth and seventh demand ‘‘GLCM” is chosen. The better request GLCM highlights drove than an acknowledgment rate arranged over 81%. A similar investigation of different surface highlights extracted from liver ultrasonic pictures by utilizing Mul- tilayer Perceptron (MLP), a sort of fake neural system, to contem- plate the nearness of infection conditions proposed [3]. Correlation of the general execution of all the component classi- fiers presumed that ‘‘blended list of capabilities” is the best list of capabilities. It demonstrated magnificent rate accuracy for the training data set. The Gray Level Run Length Matrix (GLRLM) include indicates better outcomes when the network was tested against unknown data. Both GLRLM and blended list of capabilities indicated magnificent exactness in preparing and in addition test- ing. The principle test presented [14], to locate the ideal design of modular neural systems, which implies discovering the ideal num- ber of modules, layers and hubs of the neural system, with the fuzzy gravitational scan calculation for an example acknowledg- ment application and moreover give an examination the first grav- itational approach. The gravitational search calculation utilizing fluffy rationale changes the alpha parameter esteem along the
emphasis and as outcome the calculation progressively applies an alternate G value, in the meantime having effect on the gravita- tional power to the operators lastly changing their increasing speed giving a chance to specialists to investigate other great arrangements in the search space and enhancing the last outcome. A hybrid approach combining traditional texture analysis meth- ods with deep learning for the automatic detection and measure- ment of abdominal contour from 2-D fetal ultrasound images discussed in [15]. Noteworthy better division was gotten when the forecasts from CNN are joined with those from slope boosting machine (GBM) utilizing histogram of situated inclination (HOG)
highlights.
Data mining is a step in the knowledge discovery process con- sisting of data mining algorithms that used to finds patterns or models in data. Data Mining also can be define as an analytic pro- cess designed to explore large amounts of data in search for consis- tent patterns and systematic relationships between variables and then to validate the findings by applying the detected patterns to new subsets of data. Classification is the most commonly applied data mining technique, which employs a set of pre-classified examples to develop a model that can classify the population of records at large. In classification techniques a model is built based on training data and applied to test data. WEKA is an open source data mining tool which includes implementation of data mining algorithms. Using WEKA the comparison of various algorithm i.e. ADTree, Bayes Network, Decision Table, J48, Logistic, Naive Bayes, NB Tree, PART, RBF Network and SMO done [16]. The interactive CDS system proposed an automated real-time image enhancement, segmentation, disease classification and progression in order to enable efficient diagnosis of cancer patients at early stages [2]. Among classifiers SVM delivered 95.29% precision comes about and performed better among the machine learning algorithms. For analyze purposes, the system and classifiers was connected to liver malignancy dataset for 200 patients. Class circulations are 120 favorable and 80 harmful in this dataset.


Table 2
Comparison of diverse filtering techniques [34].

Filters	Merits	Demerits	Properties

Median Filter	More robust, do not build any unrealistic pixel
More Computational Time	Edge preserves without blurring

Lee Filter	Speckle noise diminished	More computational time	Multiplicative filter
Kaun Filter	More efficient than lee filter	Noise still present on edges	ENL parameter needs to be computed

Wiener Filter	Superior for both noise removal and
smoothing
Need to estimate power spectrum of original image
Trade off between inverse filtering and noise smoothing





Fig. 4. Wavelet Based Denoising.

Now-a-days, the data mining along with image denoising in the medical literature is very popular and challenging. Various types of noises and filtering techniques using transform methods in image processing and there use as the input in the feature extraction and selection using machine learning are strongly needed in diagnosing the fetal development in ultrasound images. Such type of diagnosis is done as a comparative analysis of published work of in Table 1. The published work has been compared on the basis of 11 impor- tant specifications i.e. major contributions, application domain, G (Gray level co-occurrence matrices)/GL (Gray level run length
matrices)/I (Invariant moments), F (Filtering), S (Speckle), WT (Wavelet Transform), GA (Genetic Algorithm), Fu (Fuzzy), ANN (Artificial Neural Network), DT (Decision Tree), SVM (Support Vector Machine), Ac (Accuracy). To place our work in perception, Fig. 2 schematically summarizes how some of the state-of-the- art literature in fetal ultrasound images analysis maps between the interpretation and evaluation (using classification algorithms) with standard baseline validation point.
In addition to this section the research problem was introduced and its practical relevance was highlighted, Section 2 presents the




Fig. 5. Working of 2DWT US fetus Image.


Fig. 6. Workflow of image pre-processing step, (a) ‘‘original ultrasound image”, (b) ROI ‘‘image”, (c) Enhanced Normal Shrink Wavelet Cropped Image [35].


Fig. 7. Layered working of ANN.

proposed framework and providing details about each operational stage. Section 3 provides the experimental results obtained and an analysis of the results leading to the conclusions and Future Scope that are provided in Section 4.

Proposed methodology

The proposed method is an extension of the algorithm given by Kalyan [11] and includes a more extensive evaluation as well as detailed feature analysis. The proposed methodology consists of four phases: A) pre-processing B) Feature Extraction and Selection
C) Hybrid Approach Fuzzy-Neural-Genetic and D) Validation Result (see Fig. 3).

Pre-processing
There are multifarious filters to diminish the speckle noise. Each filter has its own statistical values. Different filter gives different results for different images. Several state-of-art denoising filtering techniques that is Wiener, Median, Lee and Kuan are being ana- lyzed by the researchers to improve the performance of these fil- ters in terms of quality, reduction of run time and many more factors [26,33,34].

Speckle noise reduction filtering techniques
Median Filter. Median filter is a non-linear technique that works best with impulse noise (salt & pepper noise) while retain- ing sharp edges in the image. This filter types the encompassing pixels value in the window pane with an orderly collection and replaces the centre pixel within the define window pane with the middle value in the collection [34]


Speckle noise diminishes the contrast of the image and cause
bi(x; y) =
(s;t)∈Sxy
median{j(s; t)}	(1)

ringing artifacts on the edges of the image. Therefore it diminishes the ability of human to distinguish between the coarse and fine details of the image [26,33,34].


Table 3
Parameter description.

yjk	neuron j’s output from k’s layer
bjk	bias weight for neuron j in layer k
wijk	link weights
uk	Activation transfer functions


{Where ‘j(s, t)’ = local window around the pixel (x, y) in the image ‘g’ to be processed and Resulting signal}.

Lee Filter. The Lee filter is based on the approach i.e. the variance over an area. If variance is Low, smoothing will be per- formed. But if variance is High near edges, it will not be performed [32,34,61]. Image can be approximated as given below
Img (i; j) = Im + W * Cp — Im	(2)
{Where ‘Img(i, j)’ = Despeckled Image, ‘Im’ = Mean Intensity Value,
‘W’ = Weighted Function,‘Cp’=Center Element in the Window}.




Fig. 8. Proposed working of Hybrid Neuro-Fuzzy-Genetic Approach.

Weighted function given as:
W = r2/ r2 + q2	(3)
[Where ‘r2' is the variance of the pixels values within the filter win-
dow i.e.
Kaun Filter. The Kuan filter considered to be more superior to the Lee filter. It does not make approximation on the noise vari- ance within the filter window. It simply models the multiplicative noise i.e. speckle noise into an additive linear form, but it relies on the Equivalent Noise Look (ENL) from an image to determine a dif- ferent weighted ‘W’ to perform the filtering [26,27,34]

N—1
2	2
i=0
{Where ‘N’ is the size of filter window, ‘Xi’ is the pixel value within
the filter window at indices ‘i’}and ‘q2 ’ is the additive noise variance
W	1	C C  1	C	6
The weighted function is computed from the estimated noise variation coefficient of the image, ‘Cu’ given as:

of the image given as [34]:
Cu = pﬃ(ﬃ1ﬃﬃﬃ/ﬃﬃﬃEﬃﬃNﬃﬃﬃﬃLﬃﬃ)ﬃ
(7)

q2 = "1/M	#
N—1
(Yi)2
(5)
And ‘Ci’ is the variation coefficient of the image given as:
C = S/I
(8)

i=0	i	m

{Where ‘M’ is the size of the image, ‘Yi’ is the value of each pixel in the image.}
{Where ‘S’ is the standard deviation in filter window and ‘Im’ is mean intensity value within the window} [34]







Fig. 10. SVM best case graph.


Fig. 11. SVM over-fitting case [41].


Weiner Filter. Wiener is a low pass filter that is an intensity image which is degraded by regular power additive noise. Wiener 2 runs on the pixel wise adaptive Wiener method which is based on statistical information estimation from a local area neighborhood of every pixel [8]. M = Wiener2(N,[i j], noise) filters the image where ‘N’ is pixel wise adaptive Wiener filtering, using neighborhoods of size i-by-j to calculate the local image mean and standard deviation. The default value for ‘i’ and ‘j’ is 3. The additive noise is assumed to be Gaussian white noise.[M, noise] = Wiener2 (N, [i j]) also estimates the additive noise power before doing the filtering. Wiener2 returns this estimate in the form of noise. The Wiener2 function can be applied to a Wiener filter (a kind of linear filter) to an image adap- tively, tailoring itself to the neighborhood image variance. If the vari- ance is large, Wiener2 performs little smoothing otherwise Wiener2 performs more smoothing [34] (see Table 2).

Wavelet based denoising
The steps explaining the Wavelet based Denoising of Medical US fetal images as in Fig. 4 [60]:

Perform a suitable wavelet transform of the noisy data; (the wavelet basis may be chosen based on various factors
Perform a soft thresholding of the wavelet coefficients where the threshold depends on the noise variance; (when the wavelet bases are chosen as in step 1, thresholding kills the effect of the noise without killing the effect of the signal).
The coefficients obtained from ‘step b’ are then padded with zeros to produce legitimate wavelet transform and this is inverted to obtain the signal estimate and the denoised out- put image [24,25,27].

Implementation of 2D wavelet transform
The Discrete Wavelet Transform is identical to a hierarchical sub-band system where the sub- bands are logarithmically spaced in frequency and represent octave-band decomposition. By apply- ing DWT, the image is actually divided i.e., decomposed into four sub-bands and critically sub-sampled as shown in Fig. 5:


Table 4
Confusion Matrix.

Actual Class


Negative	Positive

including computational burden, and ability to compress the L2 energy of the signal into a very few, very large
Predicted Class
Negative True Negative rate (TN)
False Negative Rate (FN)

coefficients).
Positive	True positive rate(TP)  False Positive rate(FP)



These sub-bands labeled LH1, HL1 and HH1 represent the finest scale wavelet coefficients i.e., detail images while the sub-band LL1 corresponds to coarse level coefficients i.e., approximation image. To obtain the next coarse level of wavelet coefficients, the sub- band LL1 alone is further decomposed and critically sampled using
Perform multiscale decomposition of the image corrupted by Gaussian noise using wavelet transform.
Estimate the noise variance ' r'2 from subband HH1 using
formula:
 median |Yij| 2

similar filter bank.
r2 =
0.6745
, 'Y' ∈ subbandHH1	(9)

Wavelet based Homomorphic Normal Shrink technique
In the homomorphic techniques, the wavelet filtering is applied to the image-logarithm followed by an exponential operation. Zong
For each level, compute the scale parameter ‘b’ using the equation:

et al. proposed a homomorphic wavelet shrinkage technique to separate the speckle noise from the original image [26,35,67,68].
Normal Shrink is an adaptive threshold estimation method for image denoising in the wavelet domain based on the generalized Gaussian distribution (GGD) modelling of sub-band coefficients. It is computationally more efficient and adaptive because the parameters required for estimating the threshold depend on sub- band data.
The steps of Normal Shrink for image denoising are as follows:

1) Take the logarithmic transform of the speckled image.
b = slﬃﬃoﬃﬃﬃgﬃﬃ ﬃﬃﬃﬃLﬃﬃkﬃﬃ ﬃﬃﬃ
For each sub-band (except the low pass residual):
Compute the standard deviation 'ry '
Compute threshold TN using equation
r2 ry
Apply soft thresholding to the noisy coefficients.
(10)
(11)




Invert the multiscale decomposition to reconstruct denoised image .
Take the exponential of the reconstructed image obtained from step 6 [26,35].

The image preprocessing work flow is analyzed in the Fig. 6 which include original benchmark image, ROI (Region of Interest)
and the enhanced Normal Shrink Wavelet based cropped filtered image.

Feature extraction and selection

Feature extraction is used to discover efficient measures so as to present the abnormalities appeared within medical image. After




Fig. 13. Benchmark Image of different Filtering Mechanism as compare to NormalShrink.



pre-processing of the images, that signifies the data-cleaning phase, attributes related to the classification are taken from the cleaned images using techniques explained as follows [52–54,64].

Intensity Histogram (IH) features
Intensity Histogram texture measures tend to be computed through original image values and it comes within the category of first-order statistics. They don’t take into account connections
with neighborhood pixel. Features obtained using this strategy is listed along with their equations in [35,36].

Gray level Co-occurrence Matrix (GLCM)
GLCM is also referred as ‘‘Spatial Dependency”. It is one of the most commonly used mathematical tools for extracting informa- tion about texture from images. This technique always concen- trates on the pixel intensity level of the neighboring pixel. GLCM




Fig. 15. Benchmark Image of different Filtering Mechanism as compare to NormalShrink.



always accounts for the particular position of the pixel comparative to other pixel. This is a simple, which indicates how often the distinct combinations of pixel brightness values forms in medical images [35–38].
Gray level run length Matrix (GLRLM)
It is a matrix through which features related to texture analysis can be extracted. For any given 2D image, GLRLM is basically a 2D matrix within which component ‘‘p(k,l)” provides total number of




Fig. 17. Live Dataset Image of different Filtering Mechanism as compare to NormalShrink.


consecutives operations of length ‘‘l” at grey level ‘‘k”. Here ‘‘M” symbolizes maximum run length (run length is considered to be a number of neighboring pixels which possess same grey level intensity in a specific direction).The representation of the feature extracted by GLRLM is explained in [35,39].

Rotation Invariant moments (IM)
The concept of making use of moments inside shape recognition became popular around 1962 when Hu utilized algebraic invari- ants in order to discover a set of invariants. Hu’s seven moment invariants are usually invariant in the form of translation, alter- ations in scale as well as rotation [39,40]. Hence it explains the image regardless of its location, size as well as rotation. The moment variants are usually specified interims of normalized moments [41,65].

Feature selection
All features were extracted from image and the resultant data contains many redundant or irrelevant features. Features selection technique is used to remove those redundant and irrelevant fea- tures and to find the significant features, which are useful in fur- ther analysis. Feature Selection was performed using WEKA [42] software of Version 3.6.9. WEKA is compatible with and recognizes only ‘.arff’ data files. Therefore ‘.arff’ file was generated which con- tains the value of features, that were extracted (including both nor- mal as well as abnormal). In feature extraction process, total 16 features were extracted from each image but whole of these fea- tures cannot be supplied to the neural network because the num- ber of features is high. Although each feature is important in classification only few of these features are very significant in clas- sifying and identification of the disease conditions. Therefore
instead of using all of these features as input, only those features, which have high significance, were selected [35].

Classifier

In CAD system classification is essential among different domains, and its performance relies upon the effective perfor- mance of different characteristics and the classifier selection.
Machine learning techniques in the field of medical imaging can be commonly utilize, computer vision, pattern recognition, etc. Here ANN has been employed for diagnosis of fetal development and acquires the substantial classification accuracy. But we pre- sume that ANN has definitely not revealed its advantage; therefore in our work, we enhance artificial neural network with fuzzy logic and compare the classification results with ANN.

ANN
ANN being motivated by biological nervous systems comprises of simple elements. The structure of ANN is formed by an ‘input’ layer, more than one ‘hidden’ layers, and one ‘output’ layer define the structure of ANN connected using directed graph with the nodes are known as neurons. Multi-layer perceptron (MLP) net- work, most practical architecture of ANN in which each and every neuron is associated a number of its neighbours, together with varying weights symbolizing the particular relative effect with the several neuron advices towards other neurons as shown in Fig. 7 [42,49,66].
The measured weighted summation with the inputs can be passed on to hidden neurons, exactly where it is actually modified employing an activation function. Subsequently, a result with the hidden neurons functions as inputs to the output neuron exactly





Table 5
Comparison of Spatial Filters with Homomorphic Filters.









where they go through one more transformation. The output of Multi-layer perceptron neural network can be described as given below [35] (See Table 3):
must indicate the design as well as learning algorithm. The disad- vantages associated with these methods appear complementary and hence it is obvious to think about developing an integrated sys-
tem merging both the techniques. Though the ability to learn is a

yjk = /k
Nk—1
i=1
Wijk Yi(k—1) + bjk !
(12)
benefit of FIS, the creation of linguistic rule is a benefit of ANN.
Neuro-Fuzzy hybridization generates a hybrid smart system which synergizes these both methods by blending the human like reasoning form of fuzzy systems with the learning and connection-



Pseudo code:
Function multilayer perceptron returns weights Inserttraining data as an input
Targets for weighted values and learning rate Localvariables:weights: a node
Wt ← initialize weight
for m ← 1 to number of examples do Until the termination condition is not satisfied do
for n ← 0 to number of features do
Wt [m] ← Wt [n] + rate ⁄ (Target-Sum[m] – Weighted Sum
[m] ⁄ training[m][n]
End
Termination condition achieved then Return weights
End End





Neuro-Fuzzy
Integration of ANN with FIS have attached the growing attention of researchers in numerous scientific as well as engineering fields because of increasing necessity of smart systems to resolve the real life issues. ANN learns from scratch through simply changing the interconnections among layers. In case we have knowledge con- veyed within linguistic rules, it will be easy to construct FIS, or in case indicate the fuzzy operators, fuzzy sets, along with knowledge base. Likewise for building an ANN for an application the consumer
ist architecture of neural networks. Neuro-Fuzzy hybridization is referred as Fuzzy Neural network (FNN) or Neuro-Fuzzy system (NFS). NFS includes the human like reasoning form of fuzzy sys- tems by using fuzzy sets and a linguistic design composed of a set of IF-THEN fuzzy rules. The key strength of Neuro-Fuzzy sys- tems is the universal approximates quality having capability to solicit interpretable IF-THEN rules. The effectiveness of Neuro- Fuzzy systems consists of two contradictory needs within fuzzy modeling: interpretability versus accuracy among which only one prevails. The Neuro-Fuzzy within fuzzy modeling study discipline is split among two areas: the Mamdani model, which is a linguistic fuzzy model centred on interpretability and the Takagi-Sugeno-Kang (TSK) model, which is a precise fuzzy model centred on accuracy.

Hybrid Neuro-Fuzzy-Genetic approach
The steps involved in classification of the ultrasound images of hybrid approach of Neuro-Fuzzy by Genetic approach are in Fig. 8 [22]:

Training and testing of hybrid approach
WEKA tool is used as a data mining process to make prediction on data. Neuro- fuzzy using genetic algorithm was employed for classifying normal and abnormal fetal characteristics and also to determine which feature classifier is best for classification. Fig. 9 determines the training, testing and predicted phases of the classi- fication learner and the performance is validated in next phase using ‘‘confusion matrix” and ‘‘receiver operating characteristic (ROC) curve analysis”.





Prediction algorithms.
Decision tree. A Decision tree is the predictive modelling approaches used in machine learning, data mining, and statistics. It is a supervised learning algorithm, that is, this algorithm consists of an outcome variable which is to be predicted from the given set of variables. Using these variables we generate a function and map the input to the desired output. The population is divided into two or more homogeneous sets in this algorithm using the significant attributes independent variables to develop various distinct groups whenever possible. In a decision tree, the leaf nodes represent the target variables and the input variable
is denoted by a path from parent node to children node [28–31,41,55,56,58,59,62,63,69].
Decision trees used in data mining are two main types:
Classification                       tree It is called as classification tree when the class to which the data
belongs is predicted as the outcome. Let us consider an example of a medical test, suppose there are two classes first for the blood test and second for cancer consisting of various independent variables. If the blood test outcome is positive then cancer is detected, hence






Fig. 22. Graphical Representation of Benchmark Image.



here we predict a class of all the variables whose outcome is pos- itive [1].

Regression tree

It is called as regression tree when the predicted outcome is a single value or a real number. Let us consider an example of a hotel, suppose here we have two classes, first the number of bedrooms and second the house prices. Using this algorithm the computer will anticipate the costs of the house in light of the quantity of rooms, hence a value is predicted here instead of a class [1]. The term which refers to both the above processes is called Classifica- tion and Regression Tree analysis.
Advantages:

The major key factor is the data preparation. The data prepara- tion required for Decision trees is quite easy for the users and requires a very little effort. If we have a dataset which measures cost in millions and loan age in years, then it will require some sort of normalization or scaling before we apply and implement a regression model and interpret correlation. In decision trees, the tree structure remains the same even if the transformation does not take place. Hence such transformations are unneces- sary in this algorithm [43].
Further, when apply decision tree algorithm on a training data- set, the top few nodes of the tree are essentially the most




Fig. 24. Graphical Representation of LIVE Dataset.











Fig. 26. Graphical Representation of LIVE Dataset.




Table 6
GLCM parametric result.



important variables in the dataset and feature selection is com- pletely automated, hence decision tree performs feature selec- tion implicitly [43].
Another advantage is the tree performance is not affected by the non-linear relationships between parameters. Whereas
simple regression models give out failing results because of highly nonlinear relationships between variables making such models invalid. But, decision trees do not need any assumptions of linearity in the data. Hence, they can be used in the environ- ment where the parameters are nonlinearly related [41].

Disadvantages:

But there are some limitations of a decision tree too, the more decisions there are in a tree, the less accurate any expected out- comes are likely to be. They are basically based on expectation than reality [41].
When actual decisions are made the results may be different from those you planned. This may result in an unrealistic and bad decision [43].
Also, Decision trees are also prone to indicate errors in classifi- cation, thus owing to diversity in perceptions and the con- straints of applying statistical tools [43].
Preparing a decision tree requires advanced knowledge in sta- tistical and quantitative analysis, this raises the possibility of training people hence increasing the cost [43].

Linear discriminant analysis (LDA). LDA is a method use in statistics, pattern recognition and machine learning to find a linear combination of features that characterizes or separate two or more classes of object or events. It’s dimensionally reduction technique in the pre processing step for pattern classification and machine learning applications.
SVM (Support Vector Machine). SVM is one of the classifi- cation methods. This algorithm is used to ‘‘plot each data item as a point in n-dimensional space” where ‘n’ belongs to the ‘‘number of features” [41].
The working of SVM is explained by the Maximal-Margin Clas- sifier. The numeric input variable is (x) in your data forms an n- dimensional space. Suppose there are two input variables, then it would form a two-dimensional space. A hyperplane line in the graph splits the input variable space. The hyperplane selected in SVM separates the points in the input variable space by their class.
The classes could be either class 0 or 1. Let us visualize a line in the two-dimension graph. Assume that all of our input points can be separated completely by this hyperplane. Let us consider an example,
B0 + (B1 * X1) + (B2 * X2) = 0	(13)
Here the slope of the line is determined by the coefficients B1
and B2 and the intercept B0 is found by using the learning algo- rithm. X1 and X2 are the two input variables. Classification is made using this line. When the input values are plugged in the equation, we can determine whether a new point is above the line or below the line [44].
If the new point is above the line then the equation returns a value greater than 0 and we can say that the point belongs to the (class 0) first class. If the new point is below the line then the equation returns a value less than 0 and that the point belongs to the (class 1) second class. A value close to the line gives a value close to zero and it becomes difficult to classify the point. For larger magnitude values, the model has more con- fidence in the prediction. For training data that are linearly sep- arable, select two parallel hyperplanes. They can be separated into two classes of data, and the distance between them is very large. The region bounded by these two planes is called the ‘‘margin”. The maximum-margin hyperplane lies halfway between them [20]. The SVM algorithm is implemented using a kernel. The equation for predicting a new input using the dot product of the input (x) and each support vector (xi) is as shown below in the equation:
f(x) = B0 + sum(ai * (x, xi))	(14)
The coefficients ‘B0’ and ‘ai’ must be estimated from the training
data by the learning algorithm for each input [41].





Let’s take the first figure as an example, see many red and green dots. Here comes the main advantage of SVM over logistic regression. In logistic regression, it would become difficult to separate out the red and green dots which can be easily done by SVM.
Now considering the second figure, SVM might end up with over-fitting, that is, it may form small bubble like boundaries which may harm the predictions (See Figs. 10 and 11).


Fine KNN. KNN is one of the simplest algorithms to understand but work well in practice. It is non parametric lazy learning algorithm. Non parametric means it doesn’t make any assumption on the underlying data distribution. And lazy means it doesn’t use the training data points to do any generalization.
Ensemble boosted. Ensemble learning is a machine learn- ing concept in which idea is to train multiple models (learners) to solve the machine problems. The main advantages of ensemble learning are reduced variance and reduced bias.

Validation

This matrix illustrates the actual versus the predicted class in classification problems, where each column represents the instances in an actual class and rows represent the instances in a predicted class. True positives (TP) and true negatives (TN) shows the number of samples correctly classified in the positive and neg- ative classes, while false positives (FP) and false negatives ‘‘(FN) represent the number of misclassified positive” and ‘‘negative examples respectively” [35] (Table 4).





Fig. 28. Interpretation of GLCM Parameter on Benchmark Image.




Accuracy represents how many predictions of the classifier were in fact correct, whereas the error rate is the percentage of misclassified examples in total. Nevertheless, the accu- racy/error might not be appropriate performance measures
Specificity represents how accurately the classifier behaves in terms of predicting the negative class (in the recurrence prob- lem this is the percentage of patients without recurrence iden- tified as such)

for imbalanced datasets where the class priors are very differ- ent, because they will be strongly biased toward the majority class.
Specificity
TN
= TN + FP	(17)



Accuracy =	TP + TN	(15)
TP + TN + FP + FN
Recall or sensitivity represents how many positive examples
the classifier was able to correctly identify (in the recurrence problem this is the percentage of patients with recurrence iden- tified as such).
Precision shows the proportion of the correctly predicted
positive cases relative to all the predicted positive ones (in the recurrence problem this is the percentage of patients identified as having recurrence that actually recur).
Precision =  TP	(18)
TP + FP
F-measure is defined as the harmonic mean of precision and



Recall =		TP TP + FN
(16)
recall, providing a balance between both of them that better reflects the performance of a classifier in the presence of an underrepresented class.




Fig. 30. Interpretation of GLCM Parameter on Benchmark Image.




F-measure =		2TP  2TP + FP + FN
(19)
Experimental setup and performance evaluation

Fall Out or False Positive rate is defined as probability of fal- sely rejecting the null hypothesis for a particular test.
Fallout = 1 — specificity	(20)
ROC Curve: The ROC curve is utilized in computing the pre-
dictive accuracy of the proposed model. This signifies the TPR as well as FPR. The region within the ROC curve called AUC categorized among the finest techniques for comparing classifiers within two issues. The test outcomes perform bet- ter when the ROC curve goes up rapidly in the direction of upper left corner of the graph else when the value of AUC is greater. Region near 1 demonstrates the reliable examination while region near 0.5 indicates the unreliable evaluation [35].
This section represents the experimental setup and discusses the performance evaluation of the proposed system. The dataset of fetal ultrasound is available online on http://www3.medical. philips.com is considered as benchmark for public use for exper- imental study and research. The live dataset is collected from Civil Hospital, Jalandhar, Punjab and 12 live fetal images are con- sidered using the proposed scenario but only three live dataset results are shown by designed work along with the online bench- mark images. Experimental section is divided into five sections
i.e. datasets, pre-processed results and evaluation metrics, Feature extracted results (GLCM, GLRLM, Invariant Histogram) out of which only GLCM result are shown using Proposed and Base result comparison and Validation result using Classification Learner.

















Fig. 32. Interpretation of GLCM Parameter on Live Dataset.




Table 7
Comparison using Confusion Matrix and ROC.





Table 8
Correctly and Incorrectly Instances in Confusion Matrix.

Base	Proposed


Us_orig.jpg	‘‘a b c <– classified as”	‘‘a b c <– classified as” 25 0 0 | a = 1	25 0 0 | a = 1
0 25 0 | b = 2	0 25 0 | b = 2
0 18 7 | c = 3	0 8 17 | c = 3
Us1.jpg	‘‘a b c <– classified as”	‘‘a b c <– classified as”


Dataset

The experimental result are compared and predicted on the dataset available as benchmark and live dataset (See Fig. 12).
Table 9
Standard Terms for Validating the Algorithms.
Pre-processed state-of-art filtering mechanism

The pre-processed includes the comparison of various basic fil- tering techniques i.e. Median, Wiener, Kuan with the Normal Shrink Filtering which is passed as input to the next step.
The qualitative interpretation from Figs. 13–19 predict that whether it is the benchmark image or live dataset the Normal Shrink Filtering Mechanism is better as compare to the basic Filter- ing Techniques Median, Wiener and Kaun Respectively.
Table 5 analyses proves quantitatively using various metrics PSNR, SNR, S/MSE, EPI, CoC that in benchmark images and live dataset Normal Shrink is better than other filters which is the out- come of the pre-processed step.
Graphical Result in Figs. 20–26 given by PSNR is better in Nor- mal Shrink than other filters with high value in each filter and same is the scenario in other parameters SNR, S/MSE, EPI and CoC.

Feature extraction (GLCM)

The feature extraction step includes four types: GLCM, GLRLM, IH and IM. GLCM parametric result is reflected in Table 6 on the basis of base paper (ANN) and Proposed Methodology (Neuro-Fuzzy-Genetic).
The values in the variation is graphically proved from Figs. 27–33 using GLCM metrics in base paper and proposed paper. Like value of R/MSE is decreased from base paper to proposed paper, which means improvement in images are correctly instanced.

Confusion matrix and ROC

The base paper (ANN) and Proposed (Neuro-Fuzzy-Genetic) approach results is compared using Confusion matrix and ROC curve using accuracy parameters TP, FP, Precision, Recall, F- Measure and ROC Area in Tables 7 and 8 is the class instances result on correctly and incorrectly prediction.



3.5. Validation result

The result is validated with classification learner which includes confusion matrix and ROC using different classification algorithms
i.e. Simple Tree, Linear Discriminant, SVM, KNN, Ensemble Boosted Trees as in Table 9.
In this validation results, the previous fourth step result is proved of confusion matrix and ROC (AUC) from Figs. 34–47, value




Fig. 34. Validation of Classification Algorithms using Confusion Matrix.






Fig. 36. Validation of Classification Algorithms using Confusion Matrix.






Fig. 38. Validation of Classification Algorithms using Confusion Matrix.







Fig. 40. Validation of Classification Algorithms using Confusion Matrix.





Fig. 42. Validation of Classification Algorithms using Confusion Matrix.


































Fig. 44. Validation of Classification Algorithms using Confusion Matrix.







Fig. 46. Validation of Classification Algorithms using Confusion Matrix.





of SVM is higher than that of Fine KNN, Ensemble Boosted, tree & LDA, which means improvement in images using classification algorithms.

Conclusion & future Scope

CAD plays an important role in predicting fetal growth recently. In this thesis work, an automatic mass segmentation and classification method is presented; the aim is to overcome the accuracy and sensitivity limitations of the current solutions in fetal growth and development, firstly, In this research, the comparison of different denoising filtering methods is done with NormalShrink Homomorphic Filtering method in pre-processed step. Experi- ments are conducted to access the better performance from all denoising filtering methods. The result reflects that both qualita- tively and quantitatively Normal Shrink Homomorphic Filtering Methods outperforms as compare to spatial filters and finally, dis- tinguish the mass between normal and abnormal using genetic algorithm based neuro-fuzzy classifier. Experimental results are very encouraging: with respect to other existing state-of-the-art classification method including ANN and GA based Neuro-Fuzzy, the proposed approach has achieved the best classification accu- racy of 96% on the database of fetal images. When conducting experiment on both benchmark database and live database, method also proved to be the best. The values of sensitivity, speci- ficity, precision and F-measure are much better than those obtained with the other methods. To evaluate the availability of the feature data and the classification accuracy more comprehen- sively, we also use the AUC of ROC as assessment indicators. All of these measured indicators verify the effectiveness of the pro- posed method in this research work.
The promising results may be due to the following aspects: The first is the improved segmentation approach, and it can locate and segment not only the mass automatically but also with lower depen- dence on the initial active contour at the same time with stronger capability of convergence; moreover, this segmentation method is robust to the interference of blurry areas and tissue and able to con- verge to the object precisely. It is the important foundation of the follow-up work; secondly, the feature extraction method is also essential. The 32 extraction features represent the characteristic of mass area comprehensively. It does great influence on the following classification process. Last and most important, the genetic algo- rithm based neuro-fuzzy classifier used in this work improves the classification accuracy significantly. The random forest is selected for classification because of its interesting properties: (1) deal with high dimension feature without feature selection, (2) accommodate many predictor variables, (3) during creating random forests, gener- alization error using unbiased estimation, and (4) run new data through previously generated forests to generate classifications.
These promising results clearly demonstrate the great potential of the proposed approach in the automatic image processing and classification of biomedical data. The proposed approach can assist the radiologist in performing an in-depth exploration of the fetal growth and development within a short time and improve the accurate diagnosis of abnormal growth of fetal. The last step include the validation using classification learner and SVM is the best validated approach in ROC curve 0.9998 area covered and Confusion Matrix accuracy rate is 97%.

Future work

These experimental results are based on a limited sample size database, and the future investigation will pay much attention to creating a larger fetal database. Also, augmentation of the present work to develop a CAD system for the early detection of the abnor-
mal fetal growth, and extension of the proposed CAD system on other medical diagnosis problems for consistency will be developed.

References

Maraci MA, Bridge CP, Napolitano R, Papageorghiou A, Noble JA. A framework for analysis of linear ultrasound videos to detect fetal presentation and heartbeat. J Med Image Anal 2017;37:22–36.
Ali Liaqat. Intelligent image processing techniques for cancer progression detection, recognition and prediction in the human liver. IEEE Symposium; 2014. https://doi.org/10.1109/CICARE.2014.7007830.
Kalyan Karthik. Artificial neural network application in the diagnosis of disease conditions with liver ultrasound images. Adv Bioinf 2014.
Carneiro Gustavo, Georgescu Bogdan, Good Sara. Knowledge-based automated fetal biometrics using syngo Auto OB measurements. Siem Med Solut 2008;67.
Yu Zhen, Tan Ee-Leng, Ni Dong, Qin Jing, Chen Siping, Li Shengli, et al. A Deep Convolutional Neural Network Based Framework for Automatic Fetal Facial Standard Plane Recognition. IEEE J Biomed Health Inf 2016:1–12. https://doi. org/10.1109/JBHI.2017.2705031.
Rawat Vidhi, Jain Alok, Shrimali Vibhakar, Rawat Abhishek. Automatic detection of fetal abnormality using head and abdominal circumference. Int Conf Comput Collective Intell 2016:25–534. https://doi.org/10.1007/978-3- 319-45246-3_50.
Subramanian Kalpathi R, Lawrence Dina M, Taghi Mostafavi M. Interactive segmentation and analysis of fetal ultrasound images.
Pradeep Kumar BP, Prathap C, Dharshith CN. An automatic approach for segmentation of ultrasound liver images. Int J Emerg Technol Adv Eng 2013.
Hiremath PS, Tegnoor Jyothi R. Fuzzy inference system for follicle detection in ultrasound images of ovaries. Soft Comput 2014;18(7):1353–62.
Harsh Vardhan M et al. GLCM architecture for image extraction. Int J Adv Res Electron Commun Eng (IJARECE) 2014;3(1).
Kalyan Karthik. Application of artificial neural networks towards the determination of presence of disease conditions in ultrasound images of kidney. Int J Comput Eng Technol 2013;4(5):232–43.
Singh Mandeep, Singh Sukhwinder, Gupta Savita. An information fusion based method for liver classification using texture analysis of ultrasound images. Inf Fusion 2014;19:91–6.
Mitrea Delia. The role of the superior order glcm in the characterization and recognition of the liver tumors from ultrasound images. Radioengineering 2012.
Beatriz Gonzalez. Fuzzy logic in the gravitational search algorithm for the optimization of modular neural networks in pattern recognition. Exp Syst Appl 2015;42(14):5839–47.
Ravishankar Hariharan. Hybrid approach for automatic segmentation of fetal abdomen from ultrasound images using deep learning. IEEE; 2016.
Deshmukh Bharat, Patil Ajay S, Pawar BV. ‘‘comparison of classification algorithms using WEKA on various datasets. Int J Comput Sci Inf Technol 2011;4(2):85–90.
Mitrea D. Exploring the textural parameters obtained from ultrasound images for modeling the liver pathological stages in the evolution towards hepatocellular carcinoma. In: IEEE Int Conf Automat, Quality Test, Robot.
Suganya R, Rajaram S. Content based image retrieval of ultrasound liver diseases based on hybrid approach. Am J Appl Sci 2012;9(6):938.
Rahmatullah Bahbibi, Papageorghiou Aris T, Alison Noble J. Image analysis using machine learning: anatomical landmarks detection in fetal ultrasound images. IEEE; 2012.
Oliver A, Freixenet J, Marti R, Pont J, Perez E, Denton ERE, Zwiggelaar R. A Novel breast tissue density classification methodology. IEEE Trans Inf Technol Biomed 2008;12:55–65. https://doi.org/10.1109/TITB.2007.903514.
Cheng HD, Shan J, Ju W, et al. Automated breast cancer detectionand classification using ultrasound images: a survey. J Pattern Recog 2010;43:299–317.
Keles A, Keles A, Yavuz U. Expert system based on neuro-fuzzy rules for diagnosis breast cancer. Expert Syst Appl 2011;38:5719–26.
Jalalian A, Mashohor SB, Mahmud HR, Saripan MI, Ramli AR, Karasfi B. Computer-aided detection/diagnosis of breast cancer in mammography and ultrasound: a review. Clin Imaging June 2013;37(3):420–6. https://doi.org/ 10.1016/j.clinimag.2012.09.024.
Kaur Lakhwinder, Gupta Savita, Chauhan RC. ‘Image denoising using Wavelet Thresholding’, 2002.
Gupta Savita, Kaur L, Chauhan RC, Saxena SC. A wavelet based statistical approach for speckle reduction in medical ultrasound images. MBEC Med Biol, Eng Comput 2004;42:189–92.
Gupta S, Chauhan RC, Saxena SC. Locally adaptive wavelet domain Bayesian Processor for denoising medical ultrasound images using speckle modelling based on Rayleigh distribution. IEEE Poc Vis Image Signal Process 2005;152(1).
Gupta S, Kaur L, Chauhan RC, Saxena SC. A versatile technique for visual enhancement of medical ultrasound images. Digital Sig Process May 2007;17:542–60.
Bommanna Raja K, Madheswaran M, Thyagarajah K. Ultrasound kidney image analysis for computerized disorder identification and classification using content descriptive power spectrum features. J Med Syst 2007;31(5):307–17.
Gao Zhifan, Hau William Kongto, Lu Minhua, Huang Wenhua, Zhang Heye, Wu Wanqing, et al. Automated Framework for Detecting Lumen and Media-


Adventitia Borders in Intravascular Ultrasound Images. Ultrasound Med Biol 2015;41(7):2001–21. https://doi.org/10.1016/j.ultrasmedbio.2015.03.022.
Athira PK, Mathew Linda Sara. Fetal anomaly detection in ultrasound image. Int J Comput Appl 2015;129(9):1–4.
Yang SH, Huang WC, Yang SY, Yang E, Yang JM. Validation of new ultrasound parameters for quantifying pelvic floor muscle contraction. Ultrasound Obstet Gynecol 2009;33:465–71. https://doi.org/10.1002/uog.6338.
Ganatra A, Kosta YP, Panchal G, Gajjar C. Initial classification through back propagation in a neural network following optimization through ga to evaluate the fitness of an algorithm. Int J Comput Sci Inf Technol 2011;3(1): 98–116. https://doi.org/10.5121/ijcsit.2011.3108.
Yoshida H, Casalino D. Wavelet packet based texture analysis for differentiation between benign and malignant liver tumors in ultrasound images. Phys in Med Biol 2003;48:3735–53.
Kaur Prabhpreet, Singh Gurvinder, Kaur Parminder. Image Enhancement of Ultrasound Images using Multifarious Denoising Filters and GA. p. 2375–84. https://doi.org/978-1-5090-2029-4.
Kaur Prabhpreet, Singh Gurvinder, Kaur Parminder. Accurate prediction of fetal images for measuring growth of fetus using genetic algorithm and back- propagation technique of neural network. Int J Sci Res Comput Sci Eng 2017;5 (3):30–41.
Haralick Robert M. Statistical and structural approaches to texture. Proc IEEE 1979;67(5):786–804.
Selvarajah S, Kodituwakku SR. Analysis and comparison of texture features for content based image retrieval. Int J Latest Trends Comput 2011;2(1).
Poonguzhali S, Ravindran G. Automatic classification of focal lesions in ultrasound liver images using combined texture features. Inf Technol J 2008;7(1):205–9.
Pietikainen Matti, Ojala Timo, Zelin Xu. Rotation-invariant texture classification using feature distributions. Pattern Recogn 2000;33(1):43–52.
Flusser, Jan, Tom Suk. Rotation moment invariants for recognition of symmetric objects. IEEE Transactions on Image Processing 15.12 (2006): 3784–90. B.F. Branstetter, Basics of Imaging Informatics: Part1, Radiology, 243, 2007, 656–67.
Sharma Narendra, Aman Bajpai, Litoriya Ratnesh. Comparison the various clustering algorithms of weka tools. Facilities 2012;4(7).
Basheer IA, Hajmeer M. Artificial neural networks: fundamentals, computing, design, and application. J Microbiol Methods 2000;43(1):3–31.
Andrey Philippe, Boudier Thomas. Adaptive active contours (snakes) for the segmentation of complex structures in biological images. ImageJ Conf 2006.
Minuillon J, Rosemary Tate A. Classifier combination for in vivo magnetic resonance spectra of brain tumors. Lecture Notes in Computer Science (LNCS 2364) 2002:282–92.
Loughna Pam et al. Fetal size and dating: charts recommended for clinical obstetric practice. Ultrasound 2009;17(3):160–6.
Hearn-Stebbins Bobbi. Normal fetal growth assessment: a review of literature and current practice. J Diagn Med Sonogr 1995;11(4):176–87.
Pramanik Manojit, Gupta Madhumita, Krishnan Kajoli Banerjee. Enhancing reproducibility of ultrasonic measurements by new users. In: SPIE Medical Imaging. International Society for Optics and Photonics; 2013.
Espinoza Jimmy et al. Does the use of automated fetal biometry improve clinical work flow efficiency? J Ultrasound Med 2013;32(5):847–50.
Sujana H, Swarnamani S. Application of Artificial Neural Networks for the classification of liver lesions by texture parameters. Ultrasoundin Med Biol 1996;22(9):1177–81.

Chikui T, Yoshiura K. Sonographic texture characterization of salivary gland tumors by fractal analysis. Ultrasound in Med Biol 2005;31(10):1297–304.
Ramaswamy S, Tamayo P. Multiclass cancer diagnosis using gene expression signatures. PNAS 2001;98(26):15149–54.
Chen X. Multi-class feature selection for texture classification. Pattern Recog Lett 2006;27:1685–91.
Baiying Lei, Yuan Yao, Siping Chen, Shengli Li, Wanjun Li, Dong Ni, Tianfu Wang, ‘‘Discriminative Learning for Automatic Staging of Placental Maturity via Multi-layer Fisher Vector”, Scientific Reports, p. 1–11, July 2015, http://doi. org/10.1038/srep12818.
Lei Baiying, Tan Ee-Leng, Chen Siping, Zhuo Liu, Li Shengli, Ni Dong, Wang Tianfu. Automatic Recognition of Fetal Facial Standard Plane in Ultrasound Image via Fisher Vector, May 2015, http://doi.org/10.1371/journal.pone. 0121838.g005.
Aschkenasy Schlomo V, Jansen Christian, Osterwalder Remo, Linka Andre, Unser Michael, Marsch Stephan, et al. Unsupervised image classification of medical ultrasound data by multiresolution elastic registration. Ultrasound Med Biol 2006;32(7):1047–54. https://doi.org/10.1016/j.ultrasmedbio. 2006.03.010.
Rahman Rashedur M, Afroz Farhana. Comparison of various classification techniques using different data mining tools for diabetes diagnosis. J Software Eng Appl 2013;6:85–97. https://doi.org/10.4236/jsea.2013.63013.
Athira PK, Mathew Linda Sara. Fetal anomaly detection in ultrasound image. Int J Comput Appl 2015;129(9):1–4.
Kumar Narander, Khatri Sabita. Significance of data mining in disease classification and prediction for mining clinical data: a review. Int J Ad Res Comput Sci 2017;8(5).
You Haowen, Rumbe George. Comparative study of classification techniques on breast cancer FNA biopsy data. Int J Artif Intell Interact Multimedia 2010;1
(3):6–13. https://doi.org/10.9781/ijimai.2010.131.
Tan HS. Denoising of noise speckle in radar image, 2001.
Arivazhagan S, Deivalakshmi S, Kannan K, Gajbhiye BN, Muralidhar C, Lukose Sijo N, et al. Performance analysis of wavelet filters for image denoising. Adv Comput Sci Technol 2007;1(1):1–10.
Ali Aida, Shamsuddin Siti Mariyam, Ralescu Anca L. Classification with class imbalance problem: A Review. International Journal Advance Soft Computation Application 2015;7(3).
Janghel RR, Mehra Anuj, Shukla Anupam, Tiwari Ritu. Intelligent diagnostic system for the diagnosis and prognosis of breast cancer using ANN. J Comput 2011;3(3).
Ahmadian, Mostafa, A, Abolhassani M, Alam, N, Gitti M. ‘‘An Efficient Texture Feature Extraction Method for Classification of Liver Sonography Based on Gabor Wavelet”, Medicon, Tehran, Iran, 2004.
Pietikainen M, Ojala T, Xu Z, Pietikainen M. Rotation-invariant texture classification using feature distributions. Pattern Recogn 2000;33(1):43–52.
Seetha M, Muralikrishna I, Deekshatulu B, Malleswari B, Hegde P. Artificial neural networks and other methods of image classification. J Theor Appl Inf Technol 2008;4(11).
Suematsu N, Ishida Y, Hayashi A, Kanbara T. Region based image retrieval using wavelet transform.
Andrey P, Boudier T. Adaptive active contours.
Poonguzhali S, Ravindran G. Automatic classification of focal lesions in ultrasound liver images using combined texture features. Inf Technol J 2001;7(1).
