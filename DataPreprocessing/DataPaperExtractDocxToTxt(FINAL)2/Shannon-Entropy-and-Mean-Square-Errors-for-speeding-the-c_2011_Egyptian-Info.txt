
ORIGINAL ARTICLE

Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:
A comparative approach
Hussein Aly Kamel Rady

El Shorouk Academy, Computer Science Department, El Shorouk – Cairo, P.O. Box 3, El Shorouk, Egypt

Received 18 May 2011; revised 22 September 2011; accepted 26 September 2011
Available online 9 November 2011

Abstract Improving the efficiency and convergence rate of the Multilayer Backpropagation Neu- ral Network Algorithms is an active area of research. The last years have witnessed an increasing attention to entropy based criteria in adaptive systems. Several principles were proposed based on the maximization or minimization of entropic cost functions. One way of entropy criteria in learning systems is to minimize the entropy of the error between two variables: typically one is the output of the learning system and the other is the target. In this paper, improving the efficiency and convergence rate of Multilayer Backpropagation (BP) Neural Networks was proposed. The usual Mean Square Error (MSE) minimization principle is substituted by the minimization of Shan- non Entropy (SE) of the differences between the multilayer perceptions output and the desired tar- get. These two cost functions are studied, analyzed and tested with two different activation functions namely, the Cauchy and the hyperbolic tangent activation functions. The comparative approach indicates that the Degree of convergence using Shannon Entropy cost function is higher than its counterpart using MSE and that MSE speeds the convergence than Shannon Entropy.
© 2011 Faculty of Computers and Information, Cairo University.
Production and hosting by Elsevier B.V. All rights reserved.




E-mail address: dr_Hussein_Rady@yahoo.com

1110-8665 © 2011 Faculty of Computers and Information, Cairo University. Production and hosting by Elsevier B.V. All rights reserved.

Peer review under responsibility of Faculty of Computers and Information, Cairo University.
doi:10.1016/j.eij.2011.09.002
Introduction

Artificial Neural Networks (ANNs) has been a hot topic in recent years in cognitive science, computational intelligence and intelligent information processing [1–7]. They have emerged as an important tool for classification. The recent vast research activities in neural classification have established that neural networks are a promising alternative to various conven- tional classification methods [8,9]. On the other hand, a Neural Network is a well known as one of powerful computing tools to solve optimization problems. Due to massive computing unit neurons and parallel mechanism of neural network

198	H.A.K. Rady


approach it can solve the large-scale problem efficiently and optimal solution can be obtained [10]. The advantage of neural networks lies in the following theoretical aspects. First, neural networks are data driven self-adaptive methods in that they can adjust themselves to the data without any explicit specifi- cation of functional or distributional form for the underlying model. Second, they are universal functional approximators in that neural networks can approximate any function with arbitrary accuracy. Third, neural networks are nonlinear mod- els, which makes them flexible in modeling real world complex relationships. Finally, neural networks are able to estimate the posterior probabilities, which provides the basis for establish- ing classification rule and performing statistical analysis.
The feedforward neural network [11–15] is the simplest (and therefore, the most common) ANN architecture in terms of information flow direction. Many of neural network archi- tectures are variations of the feedforward neural network [16]. Backpropagation (BP) is the most broadly used learning meth- od for feedforward neural networks [17,11,18,14]. There are two practical ways to implement the Backpropagation algo- rithm: batch updating approach and online updating approach. Corresponding to the standard gradient method, the batch updating approach accumulates the weight correc- tion over all the training samples before actually performing the update. On the other hand, the online updating approach updates the network weights immediately after each training sample is fed [1,19].
Information theory is commonly used in coding and com- munication applications and more recently, it has also been used in classification. In information theoretic classification, a learner is viewed as an agent that gathers information from some external sources. Information theoretic quantities have been widely used for future extraction and selection [20]. As defined in information theory, entropy is a measure of the uncertainty of a particular outcome in a random process [1,21]. The entropy of a random variable is a measure of the uncertainty of the random variable; it is a measure of the amount of information required on the average to describe the random variable. Entropy is a nonlinear function to repre- sent information we can learn from unknown data. In the learning process, we learn some constraints on the probability distribution of the training data from their entropy.
Usually error backpropagation for neural network learning is made using MSE as the cost function [22]. During the learn- ing process, the ANN goes through stages in which the reduc- tion of the error can be extremely slow. These periods of stagnation can influence learning times. In order to resolve this problem, the MSE are replaced by entropy error function [23,8,24]. Simulation results using this error function shows a better network performance with a shorter stagnation period. Accordingly, our purpose is the use of the minimization of the error entropy instead of the MSE as a cost function for classification purposes. Let the error e(j)= T(j) Y(j) repre- sent the difference between the target T of the j output neuron and its output Y, at a given time t. The MSE of the variable e(j) can be replaced by its EEM counterpart.
MSE has been a popular criterion in the training of all adaptive systems including artificial neural networks. The two main reasons behind this choice are analytical tractability and the assumption that real-life random phenomena may be sufficiently described by second-order statistics. The Gaussian probability density function (pdf) is determined only by its
first- and second-order statistics, and the effect of linear systems on low order statistics is well known. Under these lin- earity and Gaussianity assumptions, further supported by the central limit theorem, MSE, which solely constrains second-or- der statistics, would be able to extract all possible information from a signal whose statistics are solely defined by its mean and variance [25]. On the other hand, MSE can extract all the information in the data provided that the dynamic system is linear and the noise is Gaussian distributed. However, when the system becomes nonlinear and the noise distribution is non-Gaussian, MSE fails to capture all the information in the error sequences. In this case an alternative criterion is needed in order to achieve optimality. Entropy is a natural extension beyond MSE since entropy is a function of probabil- ity density function (pdf), which considers all high order statis- tics [26]. Various optimization techniques were suggested for improving the efficiency of error minimization process or in other words the training efficiency [27,28].
The rest of the paper is organized as follows. Related work is outlined in Section 2. Section 3 introduces the Multilayer Backpropagation Neural Networks. Section 4 introduces the Mean Square Error. Shannon Entropy was discussed and ana- lyzed in Section 5. Simulated results were discussed in Section 6 for Shannon Entropy and in Section 7 for Mean Square Error. Section 8 compares Shannon Entropy and MSE. Final- ly Conclusions are outlined in Section 9.

Related work

Entropy, which is introduced by Shannon, is a scalar quantity that provides a measure for the average information contained in a given probability distribution function. By definition, information is a function of the pdf; hence, entropy as an opti- mality criterion extends MSE. When entropy is minimized, all moments of the error pdf (not only the second moments) are constrained. The entropy criterion can generally be utilized as an alternative for MSE in supervised adaptation, but it is particularly appealing in dynamic modeling [25]. MSE can ex- tract all the information in the data provided that the dynamic system is linear and the noise is Gaussian distributed. How- ever, when the system becomes nonlinear and the noise distri- bution is non-Gaussian, MSE fails to capture all the information in the error sequences. Entropy is a natural exten- sion beyond MSE since entropy is a function of probability density function (pdf), which considers all high order statistics [26].
Many researchers introduces the theoretical concepts of using Error Entropy Minimization as a cost function for arti- ficial Neural Networks. In [26], Xu et al. discusses the informa- tion theoretic learning and states that entropy, which measures the average information content in a random variable with a particular probability distribution was previously proposed as a criterion for supervised adaptive filter training and it was shown to provide better neural network generalization compared to MSE. In [22], Alexandre and Sa introduces the Error Entropy Minimization approach to replace the MSE, as the cost function of a learning system, with the entropy of the error. They discusses the theoretical basis of the Renyi’s quadratic entropy. In their experimental results, they used three values of Learning rates which are 0.1, 0.2, and 0.3 with MSE and EEM for different smoothing parameters and they

Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	199

calculate the rate of convergence. The methodology of their experimental results differ from our methodology. We used different activation functions as well as different learning rates. They used a recurrent Neural Networks, but I used Backprob- agation Neural Networks.
Silva et al. in [29], introduces the concepts of Neural Net- work Classification using Shannon’s Entropy with a variable learning rate. They used the EEM algorithm with Shannon En- tropy that performed very well when compared to MSE and Cross Entropy. Their results show the effectiveness of entropic criteria, in particular Shannon’s Entropy, as cost functions in classification tasks. In [21], Erdogmus et al. discusses the Quadratic Entropy Estimator. In [30], Erdogmus et al. also proposed a cost function that tries to minimize the MSE, while it pays attention to maintaining the variation between consec- utive errors small. In [31], William and Hoffman investigates Error Entropy and MSE Minimization for lossless Image. In [25], Erdogmus and Principe, says that, we propose minimiza- tion of error entropy as a more robust criterion for dynamic modeling and an alternative to MSE in other supervised learn- ing applications using nonlinear systems such as nonlinear sys- tem identification with neural networks. In [32], Bromiley et al. studied and compared Shannon Entropy, Renyi’s Entropy, and Information.

Multilayer Backpropagation Neural Networks

The Artificial Neural Networks are known as the ‘‘universal approximators’’ and ‘‘computational models’’ with particular characteristics such as the ability to learn or adapt, to organize or to generalize data [33]. Up-to-date designing a (near) opti- mal network architecture is made by a human expert and requires a tedious trial and error process [16,34,35,7]. On the other hand, they are simplified mathematical approximations of biological neural networks in terms of structure as well as function. In general, there are two aspects of ANN function- ing: (1) the mechanism of information flow starting from the presynaptic neuron to postsynaptic neuron across the network and (2) the mechanism of learning that dictates the adjustment of measures of synaptic strength to minimize a selected cost or error function (a measure of the difference between the ANN output and the desired output). Research in these areas has re- sulted in a wide variety of powerful ANNs based on novel for- mulations of the input space, neuron, type and number of synaptic connections, direction of information flow in the ANN, cost or error function, learning mechanism, output space, and various combinations of these [16,36].
One of the most commonly used supervised Artificial Neu- ral Network (ANN) model is backpropagation network that uses backpropagation learning algorithm [37–39]. Backpropa- gation algorithm is one of the well-known algorithms in neural networks. It is one of the most common supervised training methods [40]. Training is usually carried out by iterative updating of weights based on minimizing the Mean Square Error. In the output layer, the error signal is the difference between the desired and the output values. Then the error signal is fed back through the steepest descent algorithm to the lower layers to update the weights of the network. The weights of the network are adjusted by the algorithm such that the error is decreased along a descent direction. Traditionally, two parameters, called learning rate and momentum factor,
are used for controlling the weight adjustment along the descent direction and for dampening oscillations. However, the convergence rate of the BP algorithm is relatively slow, especially for networks with more than one hidden layer. The reason for this is the saturation behavior of the activation function used for the hidden and output layers. Since the out- put of a unit exists in the saturation area, the corresponding descent gradient takes a very small value, even if the output er- ror is large, leading to very little progress in the weight adjust- ment. The Backpropagation algorithm may be described with the following three steps, which have to be applied several times in an iteration.

Forward computation of input signal of training sam- ple and determination of neural network response.
Computation of an error between desired response and neural network response.
Backward computation of the error and calculation of corrections to synaptic weights and biases [36,30].


Activation functions

The activation function can adjust the step, position and mapping scope simultaneously, so it has stronger non-linear mapping capabilities [41,33]. Activation function in a back- propagation network defines the way to obtain output of a neuron given the collective input from source synapses. The Backpropagation algorithm requires the activation function to be continuous and differentiable. The following two activation functions were proposed and used in our simulated results [36].

The hyperbolic tangent function
F v tanh v	exp(v)— exp(—v)	1
exp(v)+ exp(—v)
The limiting values of this function are  1 and +1.
The derivative of F() with respect to v is
F'(v)= sech2(v)= [1 — tanh2(v)]
= [1 — F (v)] [1 — F(v)][1 + F(v)]	(2)
For a neuron j located in the output layer
dj = (1 — Oj)(1 + Oj)(Tj — Oj)	(3)
For a neuron j located in the hidden layer
dj = (1 — Oj)(1 + Oj) X dkwjk	(4)


where dk is the error gradient at unit k to which a connection points from hidden unit j.

The Cauchy distribution function
The formula for the cumulative distribution function for the Cauchy distribution is:
F(v)= 0.5 + 1 tan—1v	(5)

F' v	1 	1		6
p 1 + tan2F(v)

200	H.A.K. Rady
For a neuron j located in the output layer	Substituting p(x, ci) = p(ci|x)p(x) in Eq. (10) gives:


dj =
1


p(1 + tan2Oj)

(Tj — Oj)	(7)
D = Z
M


j=1

M


i=1
[yi (x)— di] p(ci|x))

p(x)dx

For a neuron j located in the hidden layer
	1	 X




M	M
= E	[yi(x)— di] p(cj|x)

 


(11)

where dk is the error gradient at unit k to which a connection
M	M
= E	[y2(x)p(cj|x)— 2y (x)dip(cj|x)+ d2p(cj|x)]

points from hidden unit j.

Learning rates

j=1
i	i
i=1
i

(12)

Learning rate is one of the parameters which governs how fast a neural network learns and how effective the training is. The learning rate (g) is the conventional BP learning rule is a deci-
obtain
M
D = E
i


M
[y (x)— 2y (x)




M
dip(cj|x)+ 
j=1

d2p(cj|x)])

sive factor in regard to the size of the weights adjustments made at each iteration and hence affects the convergence rate.
i
i 1
(XM
i
j=1
i
j 1
2	)

Nevertheless, the best choice of g is problem dependent and
may need some trial-and-error before a good choice is found. If the chosen value of g is too large for the error surface, the search path will oscillate about the ideal path and converge more slowly than a direct descent, on the other hand, if the
= E


(XM

i=1
[yi (x)— 2yi(x)E{di|x}+ E{di |x}]


2)

(13)

chosen value of g is too small, the descent will progress in very small steps significantly increasing the total time to conver- gence [42].

Mean Square Errors
= E
i=1

M
= E
i=1
[yi(x)— E{di|x}]

[yi(x)— E{di|x}] )




M
+ E
i=1

var{di|x})
(14)


(15)

where var{di|x}E{d2|x}— E2{di|x} is a conditional variance of

In statistics, the Mean Squared Error (MSE) of an estimator is one of many ways to quantify the amount by which an estima- tor differs from the true value of the quantity being estimated. MSE measures the average of the square of the ‘‘error.’’ The error is the amount by which the estimator differs from the quantity to be estimated. The difference occurs because of ran- domness or because the estimator does not account for infor- mation that could produce a more accurate estimate.
The Least Mean Square LMS cost function has been used more frequently than any alternative cost function in Neural Networks. It yields good performance with large data bases on real world. LMS error cost function is the most often used error function despite being criticized for its lack of conver- gence speed and a higher possibility of being trapped in a local minima in the network training process [8].
For the general multi-class in Multilayer Neural Networks, let us consider the problem of assigning an input vector x =
{xi: i = 1, ..., D} to one of M classes {ci: i = 1, 2,..., M}. Let ci denote the corresponding class of x, {yi(x): i = 1, 2,..., M} the outputs of the network, and {di: i = 1, 2,.. . , M} the target
outputs for all output nodes. With the least mean square cost function, the network parameters are chosen to minimize the following:
i
D is achieved by choosing network parameters to minimize the
first term of Eq. (15) which is simply the mean-squared error between the network output yi(x) and the conditional expecta- tion of the target outputs. Thus, when network parameters are chosen to minimize a LMS cost function, outputs estimate the conditional expectation of the target outputs so as to minimize the mean squared error [8].

Shannon’s entropy error function

In 1948 Shannon introduced a general uncertainty measure on random variables which takes different probabilities among states into account. For a discrete random variable x, Shannon’s entropy is defined as [29,32]:
H(x)= — X f(x)log2 f(x)	(16)

here f is the probability distribution of x. For a continuous random variable x, Shannon Entropy is defined as [43]:
H(x)= —  ∞ f(x) log f(x)dx	(17)
—∞
where f is the probability density function of x.
In order to compute the Shannon Entropy of the error one


M
D = E
i=1
[yi (x)— di] )

(9)
must choose a value for the smoothing parameter in the Parzen Window method, that is best suited for a specific data set [30,44,27]. The value of the smoothing parameter is always

where E{Æ} is the expectation operator. Denoting the joint
probability of the input and the ith class by p(x, ci), we obtain
experimentally selected. One of the problems of pdf estimation using the Parzen Window method, besides the choice of the

M
D =
j=1
M


i=1

[yi (x)— di]
)p(x; ci)dx	(10)
kernel, is the choice of the smoothing parameter h. The Parzen window estimator does not assume any functional form of the unknown pdf, as it allows its shape to be entirely determined

Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	201

by the data without having to choose a location of the centers. The pdf is estimated by placing a well-defined kernel function
where h is the smoothing parameter (bandwidth) of the Stan- dard Gaussian Kernel K given by

on each data point and then determining a common width de- noted as the smoothing parameter. In Parzen windowing, the pdf is approximated by a sum of even, symmetric kernels whose centers are translated to the sample points. A suitable
and commonly used Kernel function is the Gaussian. The
 1 	1
K x	exp	x
2p	2
from (18) and (19) we find that
(20)

Gaussian function is preferable because it is continuously dif- ferentiable, and therefore the sum of Gaussian functions is
 	N
H^(E)= — N
n=1
 1  N
log
Nh l=1
K  e(n)— e(l) 
(21)

continuously differentiable on the space of real vectors of any dimension [31,45].
From (20) and (21) we find that:

1 XN	1	XN
ÿ 1 e(n)—e(l) 2 

5.1. Shannon Entropy for multilayer perceptrons

Consider a Multilayer Perceptron (MLP) with one hidden
H^(E)= — N
n=1
log
Nh,2ﬃﬃﬃpﬃﬃ l=1
e —2( h  )
(22)

layer with output y and a target variable (class membership for each example in the dataset), t. for each example we mea- sure the error using e(n)= t(n)  y(n), n = 1, 2,..., N where N is the total number of examples. We only consider the two- class problem; thus we set t { 1, 1}. The proposed Back- propagation algorithm does not use expression (17) directly
as a cost function, but, instead it uses a Shannon’s entropy esti- mator with mean square consistency given by
Simulated results for Shannon Entropy

In this section, the Shannon Entropy was used as a cost func- tion. Two different activation functions and different learning rates were used to compare the results as follows.
Fig. 1a shows the actual outputs using the different learning rates shown in the figure as an increasing sequences using Shannon Entropy and Cauchy activation function for the last 10 iterations. At the beginning of these iterations, we find that

 	N
H^(E)= — N
n=1
log f^(e(n))	(18)
degree of convergence at learning rate (LR) = 0.1 is larger than the degree of convergence at LR = 0.2 which is greater than LR = 0.3 which is greater than its counterpart using

where E is the error (difference) random variable. For the esti-
mation of f(x) we use the nonparametric kernel estimator
^	 1  XN	 e(n)— e(l) 

LR = 0.4. At the last iteration, we find that the four points nearly coincide. Fig. 1b shows the Shannon Error Entropy (SEE) using Cauchy activation function at different learning rates for the last 10 iterations. They represent a decreasing

































Figure 1	Actual output and Shannon Error Entropy using different learning rates and Cauchy activation function for the last 10 iterations. Smoothing parameter h = 0.01.

202	H.A.K. Rady



beginning of these last 10 iterations, we find that at the first iteration, SEE at learning rate = 0.1 < SEE at learning rate = 0.2 < SEE at learning rate = 0.3 < SEE at learning rate = 0.4.
Table 1 compares the degree of convergence, number of iterations and Shannon Error Entropy for different learning rates shown in the table with Tanh and Cauchy activation functions. The table shows that:

The maximum degree of convergence = 98.1310499 using Tanh activation function for learning rate = 0.4, witch speeds the convergence rate (number of iterations = 62).
For learning rate = 0.1, the Cauchy activation func- tion produces maximum number of iterations (slowing the convergence rate).
For different learning rates, the Tanh activation func- tion speeds the convergence rate than using Cauchy activation function.
Fig. 2a shows an increasing sequence of the actual outputs for the last 10 iterations using Cauchy and Tanh activation functions. At the beginning of these iterations, the actual out- put using Cauchy activation function is more converged than using Tanh activation function until the last iteration were the two actual outputs nearly coincide. Fig. 2b shows a decreasing sequence for Shannon Error Entropy using both Cauchy and Tanh activation functions until the stopping crite- ria met. At the beginning of these iterations, the SEE using Cauchy activation function is low than its counterparts using Tanh activation function.
Table 2 shows an increasing sequence of the degree of con- vergence for learning rate = 0.1 and learning rate = 0.2 for Cauchy activation function. At the last iteration, the degree of convergence for learning rate = 0.2 is higher than its coun- terpart using learning rate = 0.1.
Table 3 shows the degree of convergence and Shannon Error Entropy for the last 10 iterations for learning rate = 0.3 and smoothing parameter = 0.01. From the table, we find that
































Figure 2	Actual outputs and SEE using Cauchy and Tanh Activation Functions for the last 10 iterations – Shannon Entropy for learning rate = 0.3, h = 0.01.

Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	203
























204	H.A.K. Rady



























Figure 3	The actual outputs and MSE for the last 10 iterations using MSE and Cauchy activation function for different learning rates.





the degree of convergence is an increasing sequence but Shan- non Error Entropy is a decreasing sequence until the stopping criterion met. The last iteration shows that the degree of con- vergence using tanh is higher than the degree of convergence using Cauchy activation function.
Table 4 shows a statistic including the mean, Standard Deviation, Max and Min for the different learning rates shown in the table using Tanh activation function. It shows that:

The mean value of the actual outputs using learning rate = 0.1 is the largest while for learning rate = 0.4 it is the lowest.
The largest maximum value of the actual output occurs when learning rate = 0.4 and it has also the slowest minimum value.

Table 5 shows a statistic contains Mean, Standard Devia- tion Max and Min. the table shows that:

The Mean value for learning rate = 0.1 is the largest.
The maximum value for learning rate = 0.3 is the largest.
The minimum value for learning rate = 0.4 is the lowest.
Simulated results for Mean Square Error

In this section, the Mean Square Error was used as a cost func- tion. Two different activation functions and different learning rates were used to compare the results as follows.
Fig. 3a shows the actual output using the last 10 iterations and Cauchy activation function for different learning rates shown in the figure. All represents an increasing sequence of the actual outputs until the last iteration. At the beginning of these last 10 iterations, we find that for learning rate = 0.1 the actual output is the nearest to the desired output than all the others. At the last iteration all the four curves are nearly coincide. Fig. 3b showing a decreasing sequences until the stopping criteria met. At the beginning of these iterations, we find that the MSE using learning rate = 0.1 is the lowest. At the last iteration all the errors nearly coincide.
Table 6 compares the degree of convergence, the number of iterations and the Mean Square Error for both Cauchy and Tanh activation functions. It shows that:

Tanh activation function produces number of itera- tions less than its counterpart using Cauchy activation function for all learning rates.

Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	205






























Figure 4	Actual outputs and MSE using Cauchy and Tanh activation functions for the last 10 Iterations when learning rate = 0.2.





The learning rate = 0.4 speeds the convergence rate than all other learning rates.
When the learning rate increases, the number of itera- tions decreases for both activation functions.
The degree of convergence is the largest when learning rate = 0.4 using Tanh activation function.

Fig. 4a shows an increasing sequences for the actual out- puts using both Cauchy and Tanh activation functions for the last 10 iterations and learning rate = 0.2. At the beginning of these last 10 iterations, we find that Cauchy activation function causes a more convergence to the actual output to the desired output than using Tanh activation function. At the last iteration the actual output nearly coincide to each
other. Fig. 4b shows the Mean Square Error using Cauchy and Tanh activation functions. At the beginning of these iter- ations, we find that the MSE using Cauchy is less than the MSE using Tanh activation function. At the last iteration they are nearly coincide.
Table 7 shows the actual output and Mean Square Error using learning rate = 0.2 for the last 10 iterations. It follows that:


The actual outputs represents an increasing sequence until the last iteration.
The Mean Square Error represents a decreasing sequence until the stopping criterion met.




206	H.A.K. Rady

















Figure 5 Comparing the number of iterations using Shannon Entropy and MSE.


The last iteration indicates that the degree of conver- gence using Tanh activation function is greater than its counterpart using Cauchy activation function.
The MSE using Tanh activation function is less than the MSE using Cauchy activation function.

Results for comparing Shannon Entropy and Mean Square Error

In this section, the Shannon Entropy and MSE were used as a cost function. Two different activation functions and different learning rates were used to compare the results as follows.
Fig. 5a compares the number of iterations using Cauchy activation function between the Shannon Entropy and MSE and shows that the number of iterations using MSE is less than


the number of iterations using Shannon Entropy. While Fig. 5b compares the number of iterations using Tanh activa- tion function between the Shannon Entropy and MSE and shows that the number of iterations using MSE also less than the number of iterations using Shannon Entropy.
Table 8 compares the number of iterations using Shannon Entropy and Mean Square Error for different learning rates with Cauchy and Tanh activation functions. It shows that:

Generally the number of iterations decreases when the learning rates in the table increases except for learning rate = 0.4 using Shannon Entropy.
MSE speeds the rate of convergence than Shannon Entropy for both Cauchy and Tanh activation func- tions for each learning rate.
For learning rate = 0.1, Shannon Entropy produces the largest number of iterations.









Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	207





























Figure 6	Comparing the actual outputs and SEE versus MSE using Cauchy activation function for learning rate = 0.1.


For learning rate = 0.4, MSE produces the smallest number of iterations.

Table 9 compares the degree of convergence for Shannon Entropy and MSE using the two activation functions Tanh and Cauchy. It shows that:
For all learning rates the degree of convergence using Shannon Entropy is greater than its counterpart using MSE.
The largest degree of convergence occurred by Shan- non Entropy when learning rate = 0.4 using Tanh acti- vation function.





























Figure 7	Comparing the actual outputs and SEE versus MSE using Tanh activation function for learning rate = 0.1.

208	H.A.K. Rady


For MSE, the smallest degree of convergence occurs when learning rate = 0.1 using Tanh activation function.

Table 10 compares the degree of convergence and the error using Shannon Entropy and MSE for the last 10 iterations using Cauchy activation function and learning rate = 0.1. The table shows that:

The degree of convergence represented as an increasing sequence until the last iteration met.
The Error represented as a decreasing sequence until the stopping criteria met.
The degree of convergence using Shannon Entropy is more than its counterpart using MSE.

Fig. 6a represents a decreasing sequence for SEE and MSE using Cauchy activation function and learning rate = 0.1. At the beginning of these iterations, we find that the SEE is less than the MSE. At the last iteration they represent two coincide points. Fig. 6b shows that the actual output using Shannon Entropy is larger than its counterpart using MSE for every iteration.
Fig. 7a shows that the actual output using Shannon Entro- py is larger than its counterpart using MSE for every iteration. Fig. 7b represents a decreasing sequence for SEE and MSE using Tanh activation function and learning rate = 0.2. At the beginning of these iterations, we find that the MSE is less than the SEE. At the last iteration they represent nearly two coincide points.
Table 11 compares the degree of convergence and the error using Shannon Entropy and MSE for the last 10 iterations using Tanh activation function and learning rate = 0.2. The table shows that:

The degree of convergence represented as an increasing sequence until the last iteration met.
The Error represented as a decreasing sequence until the stopping criteria met.
The degree of convergence using Shannon Entropy is more than its counterpart using MSE.
Conclusions

Slow convergence and long training times are still the disad- vantages mentioned using neural networks. In this paper, we present a comparative study between Shannon Entropy and Mean Square Errors for improving the training efficiency (degree of convergence and convergence rate) of Multilayer Backpropagation Neural Network Algorithms. Minimizing the Error using MSE was derived. Shannon Entropy Estimator was also derived. Several concluding remarks were obtained in Sections 6–8. We also outline the following conclusions:

Mean Square Error speeds the convergence than Shan- non Entropy.
The degree of convergence using Shannon Entropy is higher than the degree of convergence using MSE.
Generally speaking, all the number of iterations using Shannon Entropy is greater than the number of itera- tions using MSE for all activation functions and all learning rates.
The actual output using Shannon Entropy is greater than the actual output using MSE for arctangent and Cauchy activation functions and all learning rates.
The arctangent activation function increases the con- vergence rate (speeds the convergence) than the Cauchy activation function using both Shannon Entropy and MSE, since the number of iterations using arctangent activation function is less than the number of iterations using Cauchy activation function for Shannon Entropy and MSE and all learning rates.
When the learning rate increases, the number of itera- tions decreases for Shannon Entropy and MSE cost functions and also for arctangent and Cauchy activa- tion functions. On the other hand, increasing the learning rate speeds the convergence of the Backprob- agation Neural Network for the cost and activation functions.
For the same learning rate, the number of iterations using arctangent activation function is less than the number of iterations using Cauchy activation function.
References

[1] Wu W, Wang J, Cheng M, Li Z. Convergence analysis of online gradient method for BP neural networks. Neural Networks

















Shannon Entropy and Mean Square Errors for speeding the convergence of Multilayer Neural Networks:	209


Rady H. Classification of multilayer neural networks using cross entropy and mean square errors. El Shorouk J ACM – Adv Comput Sci 2008.
Zhang G. Neural networks for classification: a survey. IEEE Trans Syst Man Cyber – Part C: Appl Rev 2000;30(4).
Abd El-Wahed W, Zaki E, El-Refaey A. Artificial immune system based neural networks for solving multi-objective programming problems. Egyptian Inform J 2010;11(2):56–65.
Shao H, Zheng G. Boundedness and convergence of online gradient method with penalty and momentum. Neurocomputing 2011;74:765–70.
Sun J. Local coupled feedforward neural network. Neural Networks 2009.
Rajapandian V, Gunaseeli N. Modified standard backpropaga- tion algorithm with optimum initialization for feedforward neural networks. Int J Imaging Sci Eng 2007;1(3).
Ahlawat A, Pandey S. A variant of back-propagation algorithm for multilayer feed-forward network. In: International conference (information research & applications); 2007.
Alsmadi M, Omar K, Noah S. Back propagation algorithm: the best algorithm among the multi-layer perceptron algorithm. IJCSNS Int J Comput Sci Network Security 2009;9(4).
Ghash-Dastidar S, Adeli H. A new supervised learning algorithm for multiple spiking neural networks with application in epilepsy and seizure detection. Neural Networks 2009;22:1419–31.
Shao H, Zheng G. Convergence analysis of a back-propagation algorithm with adaptive momentum. Neurocomputing 2011;74:749–52.
Iranmanesh S, Mahdavi M. A differential adaptive learning rate method for back-propagation neural networks. World Acad Sci Eng Technol 2009;50.
Krissilov A, Krissilov D, Oleshko N. Application of the sufficiency principle in acceleration of neural networks training. Int J Inform Theor Appl 1997;10.
Meynet J, thiran J. Information theoretic combination of pattern classifiers. Pattern Recogn 2010;43:3412–21.
Erdogmus D, Principe J, Kim S, Sanchez J. A recursive Renyi’s entropy estimator. IEEE; 2002.
Alexandre L, Sa J. Error entropy minimization for LsTM training. In: ICANN. Berlin, Heidelberg: Springer-Verlag; 2006.
p. 244–53.
Nasr G, Badr E, Joun C. Cross entropy error function in neural networks: forecasting gasoline demand. In: Proceedings FLAIRS-
02. American Association for Artificial Intelligence; 2002.
<www.aaai.org>.
Erdogmus Santamaria D, Principe J. Entropy minimization for supervised digital communications channel equalization. IEEE Trans Signal Process 2002;50(5).
Erdogmus D, Principe J. An error-entropy minimization algo- rithm for supervised training of nonlinear adaptive systems. IEEE Trans Signal Process 2002;50(7).
Xu J, Erdogmus D, Principe J. Minimum error entropy Luen- berger observer. In: American control conference, Portland, OR, USA, June 8–10, 2005.

Nawi N, Ghazali R, Salleh M. An approach to improve back- propagation algorithm by using adaptive gain. Biomed Soft Comput Human Sci 2010;16(2):125–34.
Al Bayati A, Sulaiman N, Sadiq G. A modified conjugate gradient formula for back propagation neural algorithm. J Comput Sci 2009;5(11):849–56.
Silva L, Sa M, Alexandre L. Neural network classification using Shannon’s Entropy. In: Proceedings of the European symposium on artificial neural networks, ESANN’ 2005, Bruges, Belgium, 27– 29 April 2005.
Ismail Z, Jamaluddin F. A backpropagation method for fore- casting electricity load demand. J Appl Sci 2008;8(13):2428–34.
William P, Hoffman M. Error entropy and mean square error minimization for lossless images compression. IEEE; 2006.
Bromiley P, Thacker N, Thacker E. Shannon entropy, Renyi entropy, and information. Tina 2004.
Mellitus. Improved gradient descent back propagation neural networks for diagnoses of type II diabetes. Global J Comput Sci Technol 2010;9(5 Ver 2.0):94–110.
Kiranyaz S, Ince T, Yildirim A, Gabbouj M. Evolutionary artificial neural networks by multi-dimensional particle swarm optimization. Neural Networks 2009;22:1448–62.
Srinivasan V, Eswaran C, Sriraam N. Approximate entropy-based epileptic EEG detection using artificial neural networks. IEEE Trans Inform Technol Biomed 2007;11(3).
Rady H. A comparative approach to accelerate backpropagation neural network learning using different activation functions. El Shorouk J ACM – Adv Comput Sci 2009.
Gross B, Hanna D. Artificial neural networks capable of learning spatiotemporal chemical diffusion in the cortical brain. Pattern Recogn 2010;43:3910–21.
Rady H. Different aspects for enhancing the backpropagation neural networks. El Shorouk J ACM – Adv Comput Sci 2007.
Nawi N, Ransing R, Ransing M. An improved conjugate gradient based learning algorithm for back propagation neural networks. Int J Comput Intell 2007;4(1).
Kandil M, Mohamed F, Saleh F, Fayek M. A new approach for optimizing backpropagation training with variable gain using Pso. In: GVIP 05 Conf. Cairo (Egypt): CICC, 19–21 December 2005.
Debes K, Koenig A, Gross H. Transfer functions in artificial neural networks. Brains Minds Media 2005.
Yu C, Liu B. A backprobagation algorithm with adaptive learning rate and momentum coefficient. IEEE; 2002.
Erdogmus D, Rao Y, Principe J. Recursive least squares for an entropy regularized MSE cost function. In: European symposium on artificial neural networks, Bruges, Belgium, 23–25 April 2003.
p. 451–6.
Espindola G, Pantaleao E. Parzen windows and nonparametric desnsity estimation applied in high resolution imagery classifica- tion. In: Anais XIV Simoio Brasileiro de Sensoriamento Remoto, Natal, Brasil, 25–30 abril. INPE. p. 6869–73.
Shwartz S, Zibulevsky M, Schechner Y. Fast kernel entropy estimation and optimization. Signal Process 2005;85:1045–58.
