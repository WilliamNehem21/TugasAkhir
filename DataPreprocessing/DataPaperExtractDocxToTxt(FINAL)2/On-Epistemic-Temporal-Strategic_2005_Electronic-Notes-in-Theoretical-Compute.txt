Electronic Notes in Theoretical Computer Science 126 (2005) 77–92  
www.elsevier.com/locate/entcs


On Epistemic Temporal Strategic Logic
Sieuwert van Otterloo1
Department of Computer Science University of Liverpool Liverpool, United Kingdom
Geert Jonker2
Institute for information and Computer Science Universiteit Utrecht
Utrecht, The Netherlands

Abstract
ATEL is one of the most expressive logics for reasoning about knowledge, time and strategies. Several issues around the interpretation of this logic are still unresolved. This paper contributes to the ongoing discussion by showing that agents do not have to know a specific strategy for doing something in order to have a capability. Furthermore we claim that agents can possess so-called strategic knowledge that is derived from their knowledge of strategies being played. In order to prove these claims we present an alternative interpretation of ATEL over extensive game forms. For the definition of abilities we use strategy domination, and to deal with strategic knowledge we include strategy profiles in the model. We illustrate the interpretation issues mentioned using several small examples. Furthermore we show how perfect recall and perfect memory can be characterized.
Keywords: alternating time, epistemic, temporal, strategic, logic, model checking


Introduction
Reasoning about knowledge, time and strategies is important in the analysis of multi agent systems. Several logical frameworks and languages have been developed to capture these notions. In this paper we discuss one such lan- guage called ATEL [15], which is a logic to reason about knowledge, time and

1 Email: sieuwert@csc.liv.ac.uk
2 Email: geertj@cs.uu.nl



1571-0661 © 2005 Elsevier B.V. Open access under CC BY-NC-ND license.
doi:10.1016/j.entcs.2004.11.014


strategies. This logic can be applied to the formal analysis of a wide range of systems, for instance distributed protocols, synchronisation and security, but also to any issue that can be modeled as an extensive game, such as argumen- tation, auctions or language games. In this paper we are concerned with the interpretation of ATEL. The original interpretation of the language featured attractive computational properties, but suffered from some counterintuitive properties, which led to a number of refinements [16,9,8]. In this paper we provide a new interpretation for ATEL. In order to keep things simple we do this for turn based systems, but we see no reason why the approach could not be extended to the more general class of concurrent systems.
The history of ATEL started with the definition of Computation Tree Logic, a logic for branching time models [4]. Using this logic one can express many temporal properties of distributed or concurrent systems, such as ‘all possible computations will reach this state’ or ‘this state eventually occurs in at least one computation’. It was discovered by Alur and others [1] that this logic could be extended to reason about multiagent systems without changing the complexity of model checking. They extended the language with coalition operators and called it Alternating-time Temporal Logic (ATL). Using ATL one could express properties like ‘This coalition can ensure that a state is reached’ or ‘This coalition cannot avoid that p always holds’. Van der Hoek and Wooldridge realized that it would be useful to reason about the knowledge of agents within ATL, and extended the language with a knowledge operator calling it ATEL [15]. They gave the kn owledge operator a very intuitive interpreted systems semantics [6] but did not alter the notion of a strategy used in the coalition operator. The result was that agents were assumed to always be able to make different choices in different states, even if the agent could not distinguish these states. Assuming that agents can make different choices in states they cannot distinguish is counterintuitive [9] and different semantics along the lines of imperfect recall ATL [13] were developed by Jonker, Van der Hoek and Jamroga [9,8].
In plain English, ATEL originally assumed that a coalition of agents can achieve something if a strategy to achieve it exists. This condition has been strengthened in the sources cited:
A coalition of agents can achieve X if they have a uniform strategy (feasible strategy, or strategy-under-imperfect information) that achieves X [9]
A coalition of agents can achieve X if they have a uniform strategy of which they know that it achieves X. [9,8]
In section 2 we define the notion of a uniform strategy formally, but it can be thought of as a strategy with extra restrictions so that it does not use facts


that an agent is not supposed to know. One of the main points of this paper is that this latter condition is too strong. A coalition does not have to be able to identify a strategy of which they know it will be successful. A coalition of sensible agents will choose, if they cannot identify a foolproof strategy, the best strategy that they can come up with. More precisely, they will choose a strategy that is not dominated by any other strategy. We say that a coalition of agents can do X if any undominated strategy achieves X. We will define domination later in this paper.
A second point of this paper is that the knowledge of a coalition does not have to depend only on the state of a game or system, but also on the strate- gies they employ. It seems safe to assume that agents know what strategies they employ, and that this gives them extra information about the future. This phenomenon can be called strategic knowledge [5]. The interpretation developed here addresses this issue by assuming all agents know their own strategy. We stress that this is intended not as the final answer on this is- sue, but as a demonstration how one can incorporate some form of strategic knowledge.
The language CTL is in fact a syntactical restricted subset of the language CTL∗. The same is true for ATL and ATL∗ and also for ATEL and ATEL∗. The unstarred versions have received the most attention, because they have a low model checking complexity [16]. In this paper however we are more inter- ested in the meaning of interpretations for the languages than in complexity. Therefore we prefer to work with atel∗, the language without restrictions, rather than to define an interpretation only for ATEL.
In section 2 we present necessary definitions. Section 3 contains examples. In section 4 we define an interpretation for atel∗. Section 5 uses the logic to analyse the examples of section 3. In section 6 two theorems are proven and section 7 is the conclusion.

Extensive Game Forms
Games are models for interaction between agents with different and possibly competing objectives [12]. An extensive game gives a detailed description of such interaction. It shows which decisions are made in order to reach an outcome. It can be represented in a game tree, where each leaf corresponds to an outcome and each node corresponds to a choice between options. The preferences of all agents are part of an extensive game. In many cases one wants to study the structure of a game independent of the preferences of the agents. In that case one can use the idea of a game form, which is an extensive game without preference function. The notion of an extensive game goes back


to Kuhn [11]. We have adapted the next definition from Osborne [12]. Since the structure encodes the fact that agents may not be sure what exactly the current state is when making a decision, we speak of an extensive game with imperfect information.
We have adopted notation fashionable in game theory [12] and notation used in coalition logic [16]. The set of all agents is denoted Σ and Γ is used for a subset of agents. Individual agents are denoted X, Y . For game forms and game form interpretations F, G are used. Strategies are called S, T or SΓ, TΓ, to indicate for which group the strategy is. A game form and a strategy together form a model, denoted by M. Formulas are denoted φ, ψ and atomic propositions p, q. P is a set of propositions. These propositions are interpreted using a function π, so that π(h) is the set of propositions that hold at h. Actions are a, b and histories are called h, h' or j. To improve readability, we sometimes abuse notation a little bit and write XY for the set of agents
{X, Y }.

Game Form
A game form is a tuple (Σ, H, Ow, ∼), where Σ is a finite set of agents and H is a non-empty set of histories. The set H must be prefix-closed, which means that for any sequence ha ∈ H also h ∈ H. We use the special symbol ϵ to denote the empty sequence. The set of all actions available after h is denoted A(h) = {a|ha ∈ H}. A history h ∈ H is terminal if A(h) = ∅. The set of all terminal histories of H is denoted Z(H).
The function Ow(h) : H \ Z(H) → Σ defines which player chooses the next action. Intuitively the agent Ow(h) owns the history h, but we can also say that it decides h, controls h or has the initiative in h. For each agent X ∈ Σ, the relation ∼X is an equivalence relation between histories, where h ∼X j expresses the fact that agent X cannot tell the difference between having gone through history h and history j. One condition applies: if Ow(h)= X and h' ∼X h then also Ow(h')= X and further A(h)= A(h'). This condition ensures that an agent knows when it can select a action and that it knows which actions are available. This definition is taken from Osborne and Rubinstein [12], definition 200.1, where it is called an extensive game form. We have extended the information sets such that agents also have information when they are not in charge, which is a c ommon extension for logical purposes [14,3].

Game Form Interpretation
A game form interpretation is defined as a tuple (Σ, H, Ow, ∼, P, π). The first elements (Σ, H, Ow, ∼) are a game form. The set P contains propositions


and π : H → 2P is a function that assigns to each history the set of propo- sitions that are true in the final state of that history. The idea is that these propositions can be used to refer to certain histories, for instance to histories where an agent achieves a certain goal.

Strategies
A strategy SΓ for a coalition Γ is a function that takes a history h such that Ow(h) ∈ Γ and returns a non-empty set of actions SΓ(h) such that SΓ(h) ⊆ A(h). This means that strategies can be non-deterministic. We sometimes call the strategy SΓ a strategy proﬁle to indicate that it contains a strategy for every agent in Γ. There is no fundamental difference between strategies and strategy profiles in this paper. A strategy SΓ is uniform iff for
all h, j with h ∼Ow(h) j it is the case that SΓ(h)= SΓ(j). A uniform strategy thus prescribes the same actions in histories that an agent cannot distinguish.
For the purposes of this paper, we think of a game form as a description of a commonly known protocol for interaction between autonomous agents. All agents have preferences and it is common knowledge that these are private, thus not known to other agents. It is also commonly known that agents with different objectives cannot communicate outside the structure of the game. If an agent adopts a certain strategy, for instance to reach a certain goal, then the agent itself knows which strategy it is playing, but other agents do not know this. If a coalition of agents has a goal, then it is assumed that agents have the right means of coordination in order to select a group strategy.
Mental capabilities are included in the definition of a game form. We assume that if an agent is for instance forgetful, that this has been encoded in the equivalence relation in the game form. The relations in the game form thus represent what an agent knows from all its sources, not just its observations. Under this assumption it makes sense to relate properties of agents, such as perfect recall and perfect memory in section 6, to properties of the equivalence relations.

Scenarios
To illustrate the principles of ATEL covered in this paper we will first have a look at some examples. The first game form we study is game form G1 in figure 1. In this game agent A can first choose between action 1 and action
2. Agent B then chooses for either action 3 or action 4. The dashed line indicated that agent B does not know what agent A has done when B has to choose. For this game form we are interested in the strategic abilities of agent
B. In the original ATEL interpretation agent B has a strategy in t0 to satisfy

A
1	t0	2 B
3 t1 4	3 t1 4

t2, p,q A
A
t2	t2,p 
t2,q 

Fig. 1. A simple game form G1

q. If A chooses action 1 it would chooses action 3, if A chooses action 2 it would choose action 4. However, at t1 agent B does not know whether agent A has played action 1 or 2. In terms of Jonker [9], agent B does not have a uniform strategy to satisfy q in t0. The conclusion is thus that for situation where we have a unique starting point, demanding that agents should have a uniform strategy i s sufficient to get intuitive results.
If we start in one of the two nodes satisfying t1, then things are a bit different. Agent B does have a uniform strategy to achieve q in for instance the leftmost t1 situation, and also a uniform strategy that would work in the rightmost situation, but does not know which one to employ because he does not know what is the current situation. In terms of Jonker and Van der Hoek and Jamroga, it cannot identify the right strategy. If agent B would want to satisfy p, it would be able to identify a uniform strategy in t1: choosing action 3.
But what if agent B would want to satisfy p ∧ q? At t1 it would not be able to identify a uniform strategy, since it does not know whether agent A has played action 1 or 2. Nevertheless, it has a ‘best bet’ strategy: playing action 3. In the view of the agent, this strategy might make it reach its goal. There is no other strategy that is better than this one, so we call this strategy undominated. It would be dominated if there existed a strategy that performed better in some of the indistinguishable states, and equally in the others. We will define this more formally in section 4. Let us now assume that we are in the left situation, after action 1. We notice that if agent B plays his ‘best bet’ strategy, it will achieve his goal. We therefore say that agent B is able to satisfy p ∧ q, or that p ∧ q is achievable for agent B.
Game form G2 in figure 2 illustrates that an agent can have several un- dominated strategies. We assume the current situation is the one after action
2. Agent B wants to make p true in the next states. The agent has two ac- tions to choose from, and it cannot identify which action is best for achieving
p. In the left situation, the appropriate strategy would be action 4. In the right situation, the appropriate strategy would be action 5. For the middle situation however it does not matter which strategy the agent chooses. Either

A
p	p	p	p
Fig. 2. Game form G2
A
p q
Fig. 3. Imperfect Memory Game Form

action leads to a desired state. Thus, the agent has two strategies that might make him reach his goal and to him it is not clear whether to use one or the other, they are both undominated. The best he can do is to randomly choose one of the two. Fortunately, since agent A has played action 2, agent B will reach his goal with any of his undominated strategies. Therefore, we say that agent B is able to satisfy p, or that p is achievable for agent B.
Both in game forms G1 and G2, the ability of agent B to reach his goal was facilitated by agent A’s choice. In G1, we saw that agent B was able to satisfy p and q because agent A had chosen to play action 1. We say that agent A has the ability to give agent B the ability to satisfy p and q. The fact that agent A moves before agent B does, does not imply that agent B cannot enable agent A to reach certain goals. If agent B decides beforehand to play action 3, it in a way gives agent A the ability to satisfy (eventually) p and q. In figure 3 another game form is depicted. This game form is unusual since agents lose information, according to the relations indicated. Agent A seems not to remember, after action 1 that it has chosen this action. Similarly, B knows the difference between histories 1 and 2 but one step later it cannot distinguish 13 and 23. Later in this paper we will show that we can thus say that agent A does not have perfect recall, but does have perfect memory. Agent B even has imperfect memory. In either case this indicates that we are not talking about perfectly rational agents, since perfectly rational agents would be able to remember everything they see and do. These properties are
discussed in section 3 and section 6.




p
Fig. 4. Block Pushing

The last scenario pictured in figure 4 deals with the question whether agents can lose abilities by choosing certain strategies. This is a minor issue, not very related to the previous issues. Nevertheless it illustrates one issue in the interpretation of ATEL. The story behind this situation is the following. In top right and middle states, the agent can push a block down, after which q holds and the block stays down. It also have the ability to push the block to the right. In the rightmost position the agent can no longer push the block down. Assume that the leftmost state is the current state. The interesting property of this model is that the agent has the ability to make p true in the next state. According to our intuitions, it cannot get rid of this ability in one step. If we assume that agents can unconditionally commit to strategies, so that they cannot change their strategy later, than A would be able, by committing to go to the rightmost state, to causing itself not to be able to achieve p. This is not what we think of as intuitive. Therefore we assume that agents can always change their own strategy at a later moment.
Finally, we turn back to game form G1 again. Consider the scenario where agent A has decided to play the strategy of choosing action 2. At time t0, this influences the knowledge of agent A. It now knows for example that agent B is not able to satisfy p ∧ q anymore. This is called strategic knowledge [5]. It is a kind of provisional knowledge; provided that an agent will follow the strategy it has committed to, it has more knowledge about the future. The same holds for a coalition; provided that they follow the same strategy and that this is common knowledge between them, their knowledge about the future increases. The next section shows how strategic knowledge, along with the other principles explained above, can be formalized using a new interpretation of atel∗.


Strategic Temporal Epistemic Logic
The language atel∗ is the smallest language L such that for any formula
φ, ψ ∈ L, any coalition of agents Γ and any agent X it is the case that:


φ ∨ ψ ∈ L	¬φ ∈ L	 φ ∈ L	φ U ψ ∈ L
oφ ∈ L	KXφ ∈ L	  Γ  φ ∈ L
This language is in fact a mixture between AT L∗ and epistemic logic [6]. The reader will be familiar with disjunction (φ ∨ ψ, ‘or’) and negation (¬φ, ‘not’). The temporal operators  , U and osay something about the future.  φ means that φ is true in all future states. The formula φ U ψ expresses the fact that at a certain point in the future ψ becomes true, and until that time φ is true. The next-state operator oφ expresses the fact that in the next state φ is true. The epistemic operator KXφ indicates that agent X knows that φ holds. The coalition operator  Γ  φ expresses that the set of agents Γ can ensure that φ holds.
We interpret formulas φ over a model M and a history h and write M, h |= φ if the formula is true. Unlike previous interpretations we include strategies in the model. A model M is a pair (F, SΣ) where F is a game form with interpretation and SΣ a strategy for all agents.
We define the neutral strategy S0 as the strategy which returns all available actions: S0(h) = A(h). Strategies for different coalitions can be combined using the function combine into a function for a larger coalition. The strategy combine(Γ, SΓ,T ) is equal to SΓ for agents in Γ and equal to T for other
agents:
combine(Γ,S ,T )(h)= ⎧⎨ SΓ(h)	if Ow(h) ∈ Γ
⎩ T (h)	if Ow(h) ∈/ Γ
A model M = (F, SΣ) contains information about all strategies that the agents currently use. An agent only knows his own strategy. It knows it will follow that strategy, but assumes that nothing more is known about others than that they adhere to the neutral strategy S0 . Thus, when evaluating the knowledge of an agent about the future, we use the model that is the result of the agent using its strategy, while the others use the neutral strat- egy. This strategy is in fact the least restrictive strategy indistinguishable to the agent. We define it using the operator k(M, X) which is defined by k((F, SΣ),X) = (F, combine(X, SΣ, S0 )). The function k is used in the defi-
nition of the knowledge operator.
In order to define the meaning of the strategic operator  Γ  , we use the idea of undominated strategies. Informally a strategy S dominates T if S is strictly better than T for reaching a certain goal. A coalition of rational agents, we assume, will not play a strategy if that strategy is dominated. To define domination, we first we need to define two other operators. The operator update is similar to k: it returns the model that represents the view


of agents in coalition Γ after the adoption of strategy SΓ. Agents in Γ adhere to SΓ but other agents can act in any way they want:
update(M, SΓ)= (F, combine(Γ, SΓ, S0 ))
Furthermore, we call a strategy SΓ successful (success(M, h, SΓ, φ)) in history h (for Γ with respect to φ) if and only if update(M, SΓ),h |= φ. A strategy SΓ dominates a strategy TΓ with respect to a model M, a goal φ and a history h if two conditions are met : There is a history h' ∼Γ h such that success(M, h, SΓ, φ) but not success(M, h, TΓ, φ), and secondly there is no history j such that success(M, j, TΓ, φ) but not success(M, j, SΓ, φ). This definition of dominance makes the domination relation transitive and asym- metric. These properties ensure that any nonempty, finite set of strategies contains at least one strategy not dominated by another strategy in the set. We say that achievable(M, h, Γ, φ) if for all undomina ted strategies SΓ it is the case that success(M, h, SΓ, φ). The reason that we quantify over all undominated strategies is that we imagine that a coalition picks randomly any undominated strategy, since it has no reason to prefer one undominated strat- egy over the other. Therefore success is only guaranteed if all undominated strategies are successful. Using all these notions we define the interpretation of atel∗ as follows:
M, h |=p	iff p ∈ π(h)
M, h |=¬φ	iff not M, h |= φ
M, h |=φ ∨ ψ	iff M, h |= φ or M, h |= ψ
M, h |=KXφ	iff ∀h' : h' ∼X h =⇒ k(M, X), h' |= φ
M, h |=φ U ψ	iff ∀j ∈ Z(H(SΣ, h))∃i∀k : |h| ≤ k < i =⇒
M, j0 ... jk |= φ) ∧ M, j0 ... ji |= ψ
M, h |=  φ	iff ∀h' ∈ H(SΣ, h): M, h' |= φ
M, h |= oφ	iff ∀a ∈ SΣ(h): M, ha |= φ M, h |=   Γ   φ	iff achievable(M, h, Γ, φ)
The set H(S, h) contains all histories of H that start with h and are con- sistent (after h) with the strategy S. It can be defined recursively. H(S, h) is the smallest set H' such that h ∈ H' and ∀h' ∈ H', ∀a ∈ S(h'): h'a ∈ H'.

Examples
In section 3 we have introduced four game forms. In this section we use these game forms to show the interpretation of example formulas. For all examples

the model Mi is defined as (Gi, S0 ). The first examples deal with temporal
properties.
M1,ϵ |= t0 ∧ ot1 ∧ o ot2	Initially t0 holds, then t1 and then t2 M1,ϵ |=  (t2 → ¬t1)	If t2 holds, then not t1
M1,ϵ |= TU t2	Eventually t2 holds
We have argued that in the game form G1 after action 1, the agent B can achieve p, but not q. It can also achieve p ∧ q but it does not know that it can achieve this fact. The translations of these facts are given here. In model G2 similar properties hold and these are also given.
M1, 1 |=  B	op
Agent B can make p true in the next state.
M1, 1 |= ¬ B	oq
AgentB cannot make q true in the next state
M1, 1 |=  B	o(p ∧ q) Agent B can make p and q together true in the next state
M1, 1 |= ¬KB  B	o(p ∧ q) Agent B does not know it can make p and q true
M2, 1 |= ¬ B	op
Agent B cannot make p true in the next state
M2, 2 |=  B	op
Agent B can make p true in the next state
M2, 2 |= ¬KB  B	op
Agent B does not know it can make p true in the next state
In game form G3, agent A does not remember the choices it has made in the past. Agent B does not always know its previous observations. This is expressed in the next statements.
M3,ϵ |= KA  A	op


Agent A knows it can make p true in the next state
M3,ϵ |= ¬  A	oKAp
Agent A cannot know p in the next state
M3, 1 |= KB oq
Agent B knows q is true in the next state
M3, 1 |= ¬ oKBq
In the next state, agent B does not know that q is true
The next model, G4, shows that agents cannot in general commit them- selves to act against their future preferences. The next formulas show that in this model, agent A does not want to make p true, and moreover A is afraid that in the future it might do something against its current desire not to have
p. This can for instance happen if A is threatened, or if A is a democratic committee so that the current members might not trust all future members.
M4,ϵ |=  A	o op
Agent A can make p true in the second next state
M4,ϵ |= ¬  A  ¬  A	op
Agent A cannot ensure it cannot make p true in the next state. It cannot get rid of abilities by just adopting a certain strategy.
M4,ϵ |= ¬ A	o¬  A	op
Agent A cannot ensure that next it cannot make p true. Similar to the previous example, but with an extra step.
M4,ϵ |=  A	o o¬  A	op
Agent A can ensure that after two steps, it no longer has the ability to make
p true.
Turning back to game form G1, we give an example of how an agent can have strategic knowledge. Suppose that agent A has committed to the strategy of playing action 2. It then knows that agent B will not be able to achieve


the satisfaction of p ∧ q anymore. Let SAB be the strategy profile consisting of the strategy of playing action 2 for agent A and the neutral strategy for agent
B. We can then represent the knowledge described above as:

(G1, SA,B),ϵ |= KA¬  B	o op ∧ q
Agent A knows that B is not able to satisfy p ∧ q



Perfect Recall and Perfect Memory
Agents have perfect recall if they never forget their previous observations and the actions they have chosen [12]. Similarly they have perfect memory [10,2] if they do not forget their observations. Traditionally game theory has focused on perfect recall agents, but artificial agents in multiagent systems often do not have these properties. These artificial agents are computer programs with a limited amount of memory. For the complexity of solving games, or model checking temporal formulas, it is relevant whether the systems have perfect recall of perfect memory [10,7]. Therefore we present here two theorems that characterize whether a game form interpretation has perfect recall and perfect memory. Especially we want to illustrate the difference between perfect recall and perfect memory, since this difference does not appear in temporal logic without strategies. Making use of the strategy profiles that we ha ve included in the model is necessary for the proof of the perfect recall property. The authors are not sure whether a different theorem regarding perfect recall could hold for previous interpretations of atel∗.
It can be argued that perfect recall is not a property of a game, but a property of a player in a game. However we think of the equivalence relations
∼X in a game form (Σ, H, Ow, [∼a]a∈Σ) as representing the knowledge of the agents. We thus assumed that the capabilities of the agents have been in- cluded in the equivalence relations. Thus we view perfect recall as a property that a game form can or cannot have. We define perfect recall in terms of observations; an agent has perfect recall if it remembers all its observations, including the actions it has chosen. Let the OX(h) be the function returning the ordered list of all observations and actions chosen by agent X in history h. Then an agent X has perfect recall if and only if h ∼X j ↔ OX(h)= OX(j). The function OX can be defined recursively. The observation function of ha contains all observations of h, plus maybe the action a (if Ow(h)= X) and the observation made in ha. Using such recursive definition, it is not hard to show

that the property of perfect recall is equivalent to the next two properties.
ha ∼X jb → h ∼X j Ow(h)= X ∧ ha ∼X jb → a = b
This characterisation of perfect recall is the one we use in the next theorem.
Theorem 1 Let F = (Σ, H, Ow, [∼X ]X∈Σ) and X ∈ Σ. X has perfect recall in F if and only if for every P, π, every φ, every strategy SΣ and every h:
((F, P, π), SΣ),h |= KX  oφ →	oKXφ
Suppose that X has perfect recall in F and let P, π, φ, SΣ,h be given. Let G = (F, P, π) and suppose that (G, SΣ),h |= KX oφ. Define M' = (G, S' ) = k((G, SΣ),X). Let a ∈ SΣ(h) and jb ∼X ha. ¿From the perfect recall properties we know that h ∼X j. From the definition of k one can see that for any h' it is the case that SΣ(h') ⊆ S' (h'). Using (G, SΣ),h |= KX oφ we can conclude that M',j |= oφ. If Ow(h) = X then Ow(j) = X and we know that b = a and therefore b ∈ SΣ(h) ⊆ S' . If Ow(h) /= X then
S' (j) = A(j) and thus b ∈ S' (j). From M',j |=  oφ and b ∈ S' (j) we can
Σ	Σ	Σ
conclude that M', jb |= φ. Since we have shown this for an arbitrary jb ∼X ha
we conclude that (G, SΣ),h |=	oKXφ.
For the second half, assume that for every P, π, every φ, every strategy SΣ
and every h:
((F, P, π), SΣ),h |= KX  oφ →	oKXφ
Let G = (F, P, π) and let ha ∈ H. Take jb such that ha ∼X jb. Let P = {p} and define π such that π(j'b')= {p} iff h ∼X j' and b' ∈ A(j'). Let S = S0 . This interpretation ensures that (G, S),h |= KX op. We can derive from the assumptions that (G, S),h |= oKXp. Thus for every a ∈ A(h) it is the case that (G, S), ha |= KXp. Since ha ∼X jb, we conclude that (G, S), jb |= p. By definition of π(p) this gives us h ∼X j.
Let G = (F, P, π) and let ha ∈ H be such that Ow(h)= X. take jb ∈ H such that ha ∼X jb. From the previous part we can already conclude that h ∼X j. Let P = {p} and let S = SΣ be a strategy such that S(h) = {a}. define π such that π(j'b') = {p} iff b' = a and j' ∼X h. These definitions ensure that (G, S),h |= KX op. We can derive (G, S),h |= oKXp. From the definition of next we know that (G, S), ha |= KXp and thus that (G, S), jb |= p. By definition of p we conclude that a = b. This concludes the proof.
An agent with perfect memory remembers all its previous observations. Let MX(h) be the function returning the ordered list of all observations by agent X in history h (excluding the actions it has chosen). We define that an agent X has perfect memory if and only if h ∼X j ↔ MX(h) = MX(j).


Again, one can define the observation function M recursively. Using such recursive definition, it is not hard to show that the property of perfect memory is equivalent to ha ∼X jb → h ∼X j.
Theorem 2 Let F = (Σ, H, Ow, [∼X]X∈Σ) be a game form and X ∈ Σ. X
has perfect memory in F if and only if for every P, π, every φ, and every h:
((F, P, π), S0 ),h |= KX oφ →	oKXφ
In this theorem, instead of just any strategy we use the neutral strategy S0 .
An important property of this strategy is that k((F, S0 ),X)= (F, S0 ).
Σ	Σ
For the first half of the proof, let X have perfect memory in F and assume
that P, π, φ and h are given. Let M = ((F, P, π), S0 ) and assume that M, h |= KX oφ. Let a ∈ A(h) and jb any history with jb ∼X ha. From the perfect memory property we know that j ∼X h. We can derive that k(M, X),j |= oφ. Since M contains the neutral strategy, k(M, X)= M and thus M, j |= oφ. This means that M, jb |= φ. Since we have shown this for an arbitrary jb with with jb ∼X ha we can conclude that M, h |= oKXφ.
For the second part, assume that for every P, π, φ and h: ((F, P, π), S0 ),h |= KX oφ →  oKXφ. Let ha ∼X jb be given. Define P = {p} and π such that π(h'a') = {p} iff h' ∼X h and a' ∈ A(h'). This definition ensures that M, h |= KX op. We can conclude that M, h |= oKXp. From this formula one can derive that M, ha |= KXp and thus that M, jb |= p. this means that π(jb)= {p} and thus that j ∼X h. Therefore X has perfect memory in F .

Conclusions and Further research
We have presented the logic atel∗ and given a new interpretation to the oper- ator associated with strategic ability. Its advantages over previously suggested definitions are that the meaning corresponds, in the authors’ opinions, best with the natural meaning of having a strategy in an extensive game. A char- acteristic feature is that under this interpretation agents may have abilities they do not know they have. We have shown that one can characterize the properties perfect recall and perfect memory with an atel∗ formula.
Future work could focus on extending the language, for instance with com- mon knowledge. An interesting question is what the complexity is of evalu- ating this logic over finite state systems. Furthermore it would be interesting to see whether similar semantics can be given for agents that do know each others’ strategy immediately, or for coalitions that are not able to coordinate their actions. Finally it would be interesting to apply this logic to example problems such as the Russian Cards problem [18] and the Dining Cryptogra-

phers problem [17].

References
R. Alur, T. A. Henzinger, and O. Kupferman. Alternating-time temporal logic. In Proceedings of the 38th IEEE Symposium on Foundations of Computer Science, pages 100–109, Florida, October 1997.
G. Bonanno. Memory and perfect recall in extensive games. Games and Economic Behavior, 47:237–256, 2004.
G. Bonanno. Memory implies von neumann-morgenstern games. Knowledge Rationality and Action, to appear, 2004.
E.M. Clarke and E.A. Emerson. Design and synthesis of synchronisation skeletons using branching time temporal logic. Lecture Notes in Computer Science, 131:52–71, 1981.
S. Druiven. Knowledge development in games of imperfect information, 2002. Universiteit Groningen Master Thesis.
R. Fagin, J. Halpern, Y. Moses, and M. Vardi. Reasoning about knowledge. The MIT Press: Cambridge, MA, 1995.
J. Halpern, R van der Meyden, and M. Vardi. Complete axiomatizations for reasoning about knowledge and time. SIAM Journal on Computing, 33:674–703, 2004.
W. Jamroga and W. van der Hoek. Some remarks on alternating-time temporal epistemic logic. submitted, 2003.
G. Jonker. Feasible strategies in alternating-time temporal epistemic logic, 2003. Universiteit Utrecht Master Thesis.
D. Koller and N. Megiddo. The complexity of two-person zero-sum games in extensive form.
Games and Economic Behavior 4, 4:528–552, October 1992.
H Kuhn. Extensive games and the problem of information. Contributions to the Theory of Games, II:193–216, 1953.
M. J. Osborne and A. Rubinstein. A Course in Game Theory. The MIT Press: Cambridge, MA, 1994.
Pierre-Yves Schobbens. Alternating-time logic with imperfect recall. In Wiebe van der Hoek, Alessio Lomuscio, Erik de Vink, and Mike Wooldrige, editors, Electronic Notes in Theoretical Computer Science, volume 85. Elsevier, 2004.
J. van Benthem. Games in dynamic-epistemic logic. Bulletin of Economic Research, 53(4):219– 248, 2001.
W. van der Hoek and M. Wooldridge. Tractable multiagent planning for epistemic goals. In Proceedings of the First International Joint Conference on Autonomous Agents and Multiagent Systems (AAMAS-2002), pages 1167–1174, Bologna, Italy, 2002.
W. van der Hoek and M. Wooldridge. Cooperation, knowledge, and time: Alternating-time temporal epistemic logic and its applications. Studia Logica, 75(4):125–157, 2003.
R. van der Meyden and K. Su. Symbolic model checking the knowledge of the dining cryptographers. under review, 2004.
H. P. van Ditmarsch. The russian cards problem. Studia Logica, 75(4):31–62, 2003.
