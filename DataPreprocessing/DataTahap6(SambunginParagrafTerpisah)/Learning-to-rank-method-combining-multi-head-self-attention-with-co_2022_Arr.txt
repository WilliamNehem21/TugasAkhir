The existing methods of learning to rank often ignore the relationship between ranking features. If the relationship between them can be fully utilized, the performance of learning to rank methods can be improved. Aiming at this problem, an approach of learning to rank that combines a multi-head self-attention mechanism with Conditional Generative Adversarial Nets (CGAN) is proposed in this paper, named *GAN-LTR. The proposed approach improves some design ideas of Information Retrieval Generative Adversarial Networks (IRGAN) framework applied to web search, and a new network model is constructed by integrating convolution layer, multi-head self-attention layer, residual layer, fully connected layer, batch normalization, and dropout technologies into the generator and discriminator of Conditional Generative Adversarial Nets (CGAN). The convolutional neural network is used to extract the ranking feature representation of the hidden layer and capture the internal correlation and interactive information between features. The multi-head self-attention mechanism is used to fuse feature information in multiple vector subspaces and capture the attention weight of features, so as to assign appropriate weights to different features. The experimental results on the MQ2008-semi learning to rank dataset show that compared with IRGAN, our proposed learning to rank method *GAN-LTR has certain performance advantages in various performance indicators on the whole.

Search and recommendation are the most dominant ways to access information in the Internet era, and learning to rank is one of the key techniques. Learning to rank [1], which uses machine learning methods to train ranking models to solve ranking problems, is a research hot spot of information retrieval and machine learning, which plays an important position in practical applications such as search engines and recommendation systems. More and more learning to rank methods are widely used in these scenarios.

The first pioneering work on solving information retrieval prob- lems using Generative Adversarial Networks (GAN) is the Information Retrieval Generative Adversarial Network (IRGAN) proposed by Wang et al. [2], which implements a unified description of two schools of thinking of generative retrieval models and discriminative retrieval models in information retrieval modeling. IRGAN uses the idea of con- frontation between generator and discriminator in GAN, and adopts a minimax algorithm in game theory to integrate the generative retrieval model and discriminative retrieval model into a unified framework in a way of confrontation training, so that the two models can improve each other, and finally make the retrieved documents more accurate. The full score paper won the nomination award for the best paper of SIGIR2017, which is both very innovative and very practical. Just like GAN in other fields, it has brought a change in the research paradigm of information retrieval [9].

Therefore, based on these research foundations described above, and inspired by their ideas, this paper intends to integrate the attention mechanism and the generative adversarial networks to design the learning to rank method, that is, to propose a method of learning to rank, named *GAN-LTR, which combines multi-head self-attention with Conditional Generative Adversarial Nets (CGAN) [24]. *GAN-LTR method will improve some design ideas of IRGAN framework applied to web search, that is to say, this method integrates the convolutional layer, multi-head self-attention layer, residual layer, fully connected layer, batch normalization and dropout technology into the genera- tor and discriminator of the CGAN framework. Moreover, it uses the softsign activation function to replace the tanh activation function in IRGAN to construct a new network model, so as to further improve the performance of the learning to rank method.

side is a new network model reconstructed for the generator and discriminator in CGAN, and the generator and discriminator use the same network model. In the new network model, Transpose is the transpose operation of the input matrix, and Batch Normalization (BN) is the batch normalization operation, which is used several times in the model to alleviate the problem of gradient vanishing, so as to improve the stability of the network. Flatten is a flattening operation for the input matrix to transform the matrix into a one-dimensional vector. Activation represents the activation function, and relu and softsign are used as the activation function, where relu is used as the activation function for the output of the previous layers of the network model, and softsign is used as an alternative to tanh activation function in the last two layers to better alleviate the problem of gradient vanishing. Dropout represents that hidden neurons are deactivated randomly, and

transformation during computation. The parameters of the linear trans- formation of each head are different and learnable, so as to ensure that the model learns relevant features from different representation sub- spaces [26]. The proposed method *GAN-LTR in this paper incorporates the multi-head self-attention layer to model the internal dependencies among the ranking features and aggregate these features to obtain higher-level features.

Xie et al. [27] explores the aggregated residual transformation of deep neural networks. Inspired by its idea of aggregated residual transformation, we apply it to the network model of *GAN-LTR. In order to take into account the properties of the residual layer and *GAN-LTR that combines the multi-head self-attention mechanism and conditional generative adversarial nets, we re-code the IRGAN ex- perimental code1 and perform experiments on the same dataset of learning to rank MQ2008-semi applied to the web search experiments of IRGAN, then compare and analyze the experimental results with IRGAN applied to the dataset in web search. For other representa- tive methods of learning to rank, i.e., RankNet, LambdaRank and LambdaMART, since IRGAN has been compared with them in the literature [2] in terms of performance, their experimental results show that IRGAN method has brought significant performance improvements

Aiming at the fact that the existing learning to rank methods often ignore the relationship between ranking features, this paper proposes a new method of learning to rank, named *GAN-LTR, that combines a multi-head self-attention mechanism with Conditional Generative Adversarial Nets (CGAN). This method adopts the sampling method and training process of IRGAN, and improves some design ideas of applying the IRGAN framework to web search. *GAN-LTR has made the following improvements on the basis of IRGAN: In the network model of CGAN, the convolutional layer and multi-head self-attention layer are added to extract the local features and global combined features of the documents and assign appropriate weights to different ranking features, the residual layer is added to avoid the gradient vanishing and degradation problems caused by the network being too deep, batch normalization is added to enhance the stability of the network, the dropout technique is added to randomly deactivate the hidden units to avoid over-fitting, and the activation function hyperbolic tangent tanh function is modified to a softsign activation function with a smoother curve to avoid gradient vanishing. The experimental results on the MQ2008-semi learning to rank dataset show that: compared with the experimental results of IRGAN applied to web search, the learning to rank method *GAN-LTR proposed in this paper obtains better results in various performance indicators. As a whole, *GAN-LTR has certain performance advantages.

