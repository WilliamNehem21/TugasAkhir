(a) where the nugget effect is significant; (b) where the simultaneous use of high-dimensional numerical and categorical types of data to estimate the target, or; (c) where non-traditional data, such as descriptive geological data can be useful during resource estimation (Nwaila et al., 2020; Zhang et al., 2021).

2007). Like any other mathematical and statistics-based techniques, uses of fractal modelling can be limited by mathematically complexity. More recently, fractal models have been added to geographic information system software libraries to provide easy end-user access to aid analysis of large geochemical exploration and environmental pollution data (Carranza, 2009; Chen et al., 2017; Yu et al., 2019).

by the gradual pro-gradation of deltas into that sea (Smith et al., 1993; Johnson et al., 1996). The Beaufort Group (<7000 m thick) is a fluvially derived succession of alternating mudstones, siltstones and sandstones caused by the coalescence of the pro-gradation deltas into broad alluvial

plains (Johnson et al., 1997; Rubidge et al., 2000). Afterwards, upward-fining siltstones and sandstones of the Stormberg Group (1200 m thick) reflect the progressive aridification of the basin, leading to an aeolian sand-dune landscape (Johnson, 1994). Finally, sedimentation was replaced with wide-spread volcanism with the extrusion of the Drakensberg food basalts and abundant intrusive sills and dykes (up to 6600 m thick), which altogether define the Karoo large igneous province (SACS, 1980; Smith et al., 1993; Catuneanu et al., 2005; Svensen et al., 2012). The origin of the magma is related to the ascent of a deep mantle plume associated with the break-up of Gondwana (Storey, 1995; Storey and Kyle, 1997; Buiter and Torsvik, 2014). Textures vary from aphyric to

porphyritic to coarsely crystalline and gabbroic in appearance, with coarser grain sizes found at the centre of some of the thicker sills. Some of the sheet intrusions are layered, caused by magmatic differentiation (Smith et al., 1993). Within southern Africa, the Drakensberg flood ba- salts are divided into four provinces, namely (a) tholeiitic lavas (main Karoo and Aranos basins), (b) olivine-poor basalts and rhyolitic to dacitic lavas (Lebombo Basin), (c) olivine-rich basalts (Lebombo and Zimbab- wean basins), and (d) silica-rich basalts (Huab Basin) (Duncan et al., 1984). The Drakensberg flood basalts initially covered most of southern Africa by the Late Jurassic, but today are only preserved in association with the Karoo basins (Du Toit, 1954).

was determined using either XRF, inductively coupled plasma-mass spectrometry (ICP-MS) or inductively coupled plasma-optical emission spectrometry (ICP-OES) depending on the available instruments during the time of the analysis, e.g. in the 1980s, trace elements were analysed using XRF. Various certified reference materials were used during the major, minor and trace element analyses. For the dataset, only the major and minor elements were normalised to unity. Some trace elements feature a substantial portion of the data near the instrumental lower detection limits, and as such, their concentration data is heavily quan- tised (e.g. low U, Th and radiogenic Pb concentrations in mafic rocks).

All rock-forming elements (major and minor elements) are used as machine learning features. Ratios of elements or any other mathematical manipulations of features as part of data-preprocessing are known as feature engineering, whose purpose is to measurably improve the per- formance of the algorithms (Hastie et al., 2009; Domingos, 2012). Feature space is a vector space (Hastie et al., 2009) and in the case of log-ratio-transformed compositional data, the feature space is unaffected

There are many supervised regression algorithms. Algorithm selec- tion is based on many factors, such as (a) computation time, (b) data density including feature space density, (c) bias-variance trade-off, (d) function complexity, (e) feature space dimensionality, (f) input and prediction noise, and (g) feature interactions. An experimental approach is often used to maximise some combination of factors. Prediction error can be interpreted through the bias, variance and noise model. The bias is the tendency of the model to default to some class label. The variance gauges the relative change in the model output given a change in the model's input. The noise is the portion of prediction error that is neither bias nor variance. The total prediction error is the quadrature sum of the three sources of errors. Algorithms generally exhibit different behaviours in terms of their bias and variance, and it might be desirable to trade some bias for a greater reduction in variance in a particular context.

the feature space geometry. The prediction targets (i.e. trace elements to be predicted) are not transformed, as they are not part of the feature space. If the features encode characteristics of rocks, which also differ by rock type, then the machine can identify these differences and their re- lationships that can be used for predictive modelling. Whether the choice of the embedding vector space (and therefore the log-ratio trans- formation) is appropriate should be evaluated empirically using perfor- mance metrics (Gu et al., 2018). In this study, we adopt two performance metrics, the coefficient of determination (CoD or R2) to assess model fitting, and the median absolute prediction error (MAPE) to assess typical prediction performance. MAPE is more robust to outliers compared to the CoD metric.

The influence of rock type on prediction performance is important to understand whether particular rock types (e.g. volcanic rocks) are detrimental to the overall prediction performance. This information provides a basis to determine the generalisability of the method proposed in this study to new environments. A combination of MAPE and CoD were used to monitor prediction performance changes relative to the removal of various rocks in the studied magmatic suites, again using 5-fold cross- this sense, increases in the quality of the models (higher explanatory power models, e.g. a higher CoD) increases the baseline of geochemical anomalies. In effect, a higher quality (more predictive) model renders the geochemical anomalies more selective and less noisy. Therefore, to be able to predict any element of interest as accurately as possible is the main consideration to increasing the targeting selectivity of prospectivity maps. A higher-quality model is capable of filtering out some of the regional variability of an element of interest that are not associated with potentially interesting geochemical anomalies.

Since we do not use PCA or other dimensionality reduction tech- niques, the approach is better suited to using solely the major and minor elements content as features and because we do not assume the existence of minerals in the samples, and therefore, stoichiometry in the data. This distinction should be more prominent where the feature interactions are increasingly nonlinear, such as in geological samples that may be highly non-stoichiometric (volcanic glasses, highly altered clays, etc.).

implementation, all of the algorithm selection, model tuning and pre- diction performance profiling, to mapping are automated within a single workflow. In addition, the objectivity of the resulting prospectivity maps is increased, because there is no need to interpret intermediary results (i.e. the stoichiometry and meaning of principal components). However, this does result in a loss of scientific insight into the geological processes that may be operating in the sampled region if the prospectivity maps are not further interpreted after their generation.

The authors would like to thank an anonymous reviewer and Pierre- Marc Godbout (Geological Survey of Canada) for their enlightening comments which have greatly improved this paper. Daniel Frederick Wright (Geological Survey of Canada) is also thanked. We thank Hua Wang for editorial handling.

