five models considered above. Most machine learning methods only use the single best value of each parameter when making a prediction. But since the parameters are learned from the data, they are uncertain, and this uncertainty should be propagated into the prediction. Parameter uncertainty decreases as the sample sizes increases, so to better illustrate the effect of parameter uncertainty on predictions, a smaller dataset was made by taking every eighth data point from the previous example.

val from a Bayesian model that accounts for parameter uncertainty. The dashed black lines show the 95% PI from a classic quadratic regres- sion model which ignores parameter uncertainty, and note how they are slightly narrower. The mean function is identical for both the Bayesian and classic model.

Models typically have many more parameters than this example (the state-of-the-art Generative Pre-trained Transformer 3 (GPT-3) deep learning language model has 175 billion parameters [6]) and simply collecting more data is often not an option to reduce parameter uncer- tainty because more data enables more complex models to be fit (e.g. including nonlinear terms and interactions), which then increases the number of parameters.

hence are uncertain. Furthermore, some predictors such as cLogP are the output of other (imperfect) prediction models and therefore are also uncertain. Finally, some predictors are not measured directly but are es- timated from a standard curve, which introduces additional uncertainty because the curves may not be not perfectly calibrated.

Berkson error can be introduced when converting continuous values into bins or groups. For example, compounds are categorised as active versus inactive, despite having a range of activity values. Or, compounds are classified as having no, mild, or severe toxicity, even though com- pounds will have a range of toxicity levels within each category. Binning can also lead to misclassification error, where a compound is placed into the incorrect category. This can occur if the measured assay value dif- fered from the true value and fell on the wrong side of a threshold. Hence, binning is strongly discouraged [33,35]. Misclassification can also occur due to incorrect diagnoses, labelling errors, or data-entry er- rors. The standard response to these known and often large sources of

Data are truncated when values outside of a range are omitted, and the number of omitted values is unknown. For example, for objects to be segmented as a cell in a standard image analysis, they must have a minimum user-defined cell size. Smaller cells will therefore not be in- cluded in the analysis, and hence both the estimated cell size and prop- erties that are correlated with cell size can differ from their true values, thereby introducing both uncertainty and bias.

generated value. This is a form of multiple imputation and Blackwell, Honaker, and King describe a more sophisticated method of generat- ing new data by taking the correlations between variables into account [2]. Each dataset is then analysed separately, and the predictions from each analysis are combined. Variations between the different datasets will lead to different parameter estimates, which in turn will lead to

We use the second method to illustrate the effect of ignoring mea- surement error in two ways. First, we assume the training data is mea- sured without error, but the test data is measured with error. Then we assume that the training data is measured with error but the test data is not. In both cases we compare the result to the standard approach of ignoring measurement error in both the training and test data. The test parameter uncertainty is already account for. And if the test data are measured with error, we can use the approach in 5 B to draw multiple samples for each test sample and feed them all through the prediction models. The more sources of uncertainty accounted for the more com- plex the prediction model. Hence, sources of uncertainty that make little contribution to the overall prediction uncertainty can be ignored.

truncation at zero to represent our prior uncertainty in the parameters. All the models were fit to the data with the Turing package in Julia. The No-U-Turn Sampler (NUTS) was used to update the uncertainty in the parameters after conditioning on the data. Three chains with 10,000 samples each were used and convergence was assessed with graphical

The above examples used a single distribution function, but flexi- bility can be increased by using mixtures of distributions. For example, outliers can be modelled with a mixture of Gaussian distributions: one to account for the regular observations and the second to account for the outliers. Metabolite, gene, and protein levels are non-negative and often positively skewed, and hence gamma or lognormal distributions may be appropriate. But these distributions are only defined for values tion to make it a covariance function. Covariance functions are not dis- cussed here but they are also useful for modelling hierarchical or nested data [24,48], and for modelling dependencies in time or space. Neu- ral networks [21,56,68] and Bayesian additive regression trees (BART) [12,60] are other options for flexible mean functions.

Finally, not all sources of uncertainty can be captured. Many sources of uncertainty discussed above arise because many modelling options are available, and different choices lead to different predictions. All of the choices relate to the prediction model, but many decisions need to be made outside of the model. We refer to these extra-model choices as the project workflow and they include experimental decisions such as the technology, cell-line, assay, antibodies, protocol, and so on. Also included are data processing pipelines where raw data are cleaned, transformed, categorised, coded, and normalised before they are entered into a prediction model. A single workflow is commonly used, with the untested assumption that variations in the workflow will lead to the same predictions and results. However, variations in workflows and an- alytic decisions do lead to variations results [4,23,32,57,58,61,62].

