Software bugs consume a significant amount of software developer resources, resulting in financial losses [1]. For these reasons, sizeable research fields developed around bugs in software, providing novel and advanced tools and approaches. In real world software develop- ment, this progress is most prevalent in the application of ante-mortem approaches that aim at preventing the introduction of bugs, as for example bug prediction, static checking, and automated testing.

In this paper, we propose a machine learning (ML) based classifier that predicts the fault type of a bug based on its textual bug report. This ML based approach tries to encapsulate a small aspect of what would commonly be considered developer knowledge and experience in debugging. A priori information about the underlying fault type can support inexperienced developers in their choice of debugging approaches and tools.

(3) We perform a user survey to establish a baseline of human clas- sifier performance on this task. (4) We introduce preprocessing steps tailored specifically for bug reports and we apply ensemble learning methodologies on the classification problem. (5) We evaluate the classi- fication performance of various classical ML algorithms, preprocessing steps, and ensemble approaches. (6) We evaluate the performance for inter-project application. Our main findings of this Journal version are:

The remainder of this paper is structured as follows: Section 2 describes the problem of fault type prediction using bug reports. In Section 3, we discuss related work and similar classification endeavors. In Section 4, we discuss the background of this work including exist- ing bug classification schemata, ML based classification, and natural language processing (NLP). In Section 5, we present our experimental setup and approach, followed by our research questions and results in Section 6. In Section 7, we discuss the internal and external threats to validity. Section 8 concludes our work and discusses future research. All datasets and implementations are publicly available (see Section 8).

These issues make bug reports a challenging target for NLP ap- proaches. Our task of fault type classification is further complicated by lack of information or misleading information in such bug tickets. We therefore do not expect to see the high classification performance scores known from classic NLP showcase problems.

Ray et al. [10] used ML classifiers to investigate programming language and code quality metrics in open-source projects. They apply five different root cause categories: Algorithmic, Concurrency, Memory, generic Programming, and Unknown. Since their approach is focused on the analysis of fixed bugs, they train their ML approach on com- mit messages (a posteriori approach). However, we are interested to provide information to the developers before they have fixed the bug. Therefore, we train our classifier on the textual bug report instead of the commit messages (a priori approach).

Ni et al. [11] predicted the root cause type from the code changes by converting the code changes into abstract syntax trees (ASTs) and then using a Tree-based Convolutional Neural Network (TBCNN). They distinguished six main root cause categories (function, interface, logic, computation, assignment, and others) and 21 sub-categories. In con- trast to our work, this classification was performed post mortem on the bug fix.

Particularly interesting in the context of fault localization is the approach proposed by Fang et al. [22] that classifies bug reports as informative or uninformative. This approach can be used as a prepro- cessing step in information retrieval (IR) approaches to filter out those bug reports where IR promises little insights.

First, we provide an overview of existing bug classification schemata (Section 4.1). Afterwards, we briefly explain the used classifiers, and statistical methods (Section 4.2) and the most important terms w.r.t. natural language processing (Section 4.3). Finally, we provide the formal definitions of the used performance metrics (Section 4.4).

e.g. severity, impact, and root cause. All classification schemata are of course intended to fulfill a certain purpose, and their dimensions, depth, and detail are selected to achieve the set goal. These purposes range from investigations into process optimization to enable auto- mated triage and prioritization, to research into different areas of the software development process, to the support of techniques such as automated repair.

Polski et al. [24] discussed the application of existing fault classifi- cation schemata and bug distributions for fault injection, and provided an overview on fault classification schemata. Endres [25] performed one of the earliest attempts at bug classification to investigate higher level causes (e.g., technological, organizational, historic).

Multinomial Naive Bayes (MNB) are probabilistic classifiers based on Bayes theorem. Naive in this context stands for the assumption that probabilities of features are independent. MNB classifiers are fast and easy to use and can provide a performance baseline to compare other classifiers against.

Support Vector Machines (SVM) are non-probabilistic approaches that can be used for regression and binary classification. SVMs con- struct a hyperplane in the feature space separating the classes. For multi-class classification problems, multiple SVMs are trained simulta- neously in a one-vs-all or one-vs-one setup.

carry an understanding of the underlying problem, while also carrying a certain level of technical expertise and associated vocabulary. Further, the corpus of commit messages is distinct from the corpus of textual bug reports that constitutes the inputs of our models. This reduces unwanted biases towards certain keywords in our machine learning experiments.

Dataset quality and verification. Researcher 1 performed the manual classification described above. Six months later, Researcher 1 reclassi- fied 100 randomly sampled and blinded issues from the set for internal verification. In this internal verification step, Researcher 1 scored 0.95 for Cohens Kappa, that suggests almost perfect agreement [35] and a weighted average F1 of 0.96.

Approach: We performed an online user survey tasking participants to classify textual bug reports according to their fault type in four categories. The survey was promoted via email and direct messages to professional developers at resident software companies and to master students in the field of computer science. It can be assumed that the majority of survey participants were students. Anonymous participation was allowed, while participants disclosing a contact e-mail address could win vouchers of a well-known online store. Multiple submissions from the same person were allowed. For each submission, we tracked IP address, email address (optional), time spent on each bug report, and the corresponding answers for each bug report.

Results: We received 51 submissions. We carefully investigated the times spent and IP addresses and did not find any submissions indica- tive of being made with malicious intent. The submissions provide us with a total of 510 manually classified bug reports for our analysis. Removal of bug reports originating from three projects as discussed in , leaves us with 483 classified bugs as the basis for the following discussion.

We compared the length in characters and length in lines, for both the original bug reports, and the bug reports after artifact removal, as well as the lengths in characters and lines of removed artifacts. Further, we compared the number of occurrences of exception names in the bug tickets, as well as the number of bug tickets that contain such exception names. However, none of these metrics show any significant differences between the misclassifications and correct classifications, which can be explained by significant overlap of contained bug tickets in both correct and incorrect sets because of the small original dataset size of 496 bug tickets.

Analog to our analysis in RQ2, we investigated the bug tickets that were either always correctly classified (99 bugs), or always incor- rectly classified (24 bugs). The always correct classified bug reports are headed by the Other class (36 bugs), followed by Memory (30 bugs), Concurrency (29 bugs), and Semantic (4 bugs) classes. The always incorrectly classified bug reports are headed by the Semantic class (10 bugs), followed by Memory and Concurrency classes (each 5 bugs), and Other class (4 bugs). Manual examination of the always misclassified bugs revealed 6 bugs that we consider hard to classify and 6 that we consider impossible to classify with the given information; the remainder provide enough information and context. The impossible to classify bugs are e.g., reports on improper function that arose from wrong or improper documentation and were fixed as documentation corrections or enhancements, or reports on incorrect function where the root cause was missing UI resource files.

However, besides the discussed test splits, there is another signif- icant difference to our RQ2 experiments regarding the training splits: bug reports from netty are always in the training sets. It is possible that netty bug reports are important to the training process. Unfortunately, we cannot remove netty from the training set while maintaining bal- ance, because of its significant size of 84 items (including 40 memory bugs).

Further, hidden biases may reside in bug reports from any single software project. To counter this threat, we sourced this dataset from 71 different open source Java projects covering various organizations, software domains, and deployment targets, ranging from search engines and database applications to small Java libraries and mobile applica- tions. To validate our results and to demonstrate the transferability of our approach, we performed our experiments on test/training splits along software projects, ensuring that bug reports in the test set are from different software projects than the bug reports used in training. A threat to external validity is that our dataset contains only Java bugs. In addition, only open source projects hosted on GitHub were considered in this work. We can therefore not generalize our findings

The manual classification performed by researcher 1 is also subject to misclassifications and therefore a threat to internal validity. To counter this threat, a second researcher independently classified a blinded random sample of the dataset, and researcher 1 re-classified a blinded random sample six months after the initial classification. We used these additional samples to calculate inter-rater agreement scores, to quantify the quality of our dataset.

The majority of participants for our survey are master degree stu- dents in computer science. These participants are to be considered non-experts, as they are not involved in the development of the soft- ware projects sourcing our datasets. Further, participants performed classification without provision of the original project context. The resulting scores are therefore on the lower end of human classification performance for the given task.

We have investigated human classifier performance on this task, followed by experiments using classical ML algorithms on this multi- class classification problem. The mean classification performance of non-expert human classifiers was rather low, with a mean weighted average F1 score of 0.62. The best single classifier model (Logistic Regression) has 0.66 macro average F1.

Our investigation into ML approaches highlighted advantages and disadvantages of certain NLP preprocessing steps and different ML algorithms. To exploit the gained insights, we used ensemble methods that combine multiple classifier models and preprocessing pipelines. Using such ensemble methods, we achieved mean macro average F1 scores of 0.69.

While our results are promising when compared to our human classifier performance baseline, application of our models as debugging support in a production environment scarcely warranted at this stage and future research in this direction is needed. However, our approach in its current form can support researchers in their effort of creating bug benchmarks of specific bug types for experiments on specialized debugging tools. Our datasets and implementations are made publicly available on GitHub.8 and Zenodo9 We hope that other researchers benefit from our detailed investigation and the provided artifacts.

