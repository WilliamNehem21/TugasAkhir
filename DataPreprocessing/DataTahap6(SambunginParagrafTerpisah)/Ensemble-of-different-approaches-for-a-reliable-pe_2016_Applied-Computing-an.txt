In this paper we improve the performance of state-of-the- art person re-identification systems using an ensemble of methods combined by weighted sum rule. The different systems utilize different color spaces and several texture and color features for describing the images. To the best of our knowledge, this is the first work in which several different state-of-the-art person re-identification systems, and their variants, are combined to obtain a more robust approach.

To demonstrate the generality of our system, we validate our approach on the following well-known datasets: CAVIAR4REID, IAS, and VIPeR. Moreover, we test our sys- tem on a dataset derived from VIPeR, which we call VIPeR45 because it contains 45 image pairs from VIPeR that focus on some of the most difficult samples to re-identify images of per- sons containing strong pose changes, for instance, or wearing very similar clothing. VIPeR45 was created because person re-identification performance was tested in [7] using a dataset that was built in a similar fashion (i.e., using 45 difficult image pairs extracted from VIPeR); the human subjects obtained a Rank(1) of 75% and a Rank(10) of ~100% [7]. Thus, it is possible for other researchers in person re-identification to use VIPeR45 for approximately comparing the performance of their computer vision systems with the performance of

The remainder of this paper is organized as follows. In Section 2 we describe the base approaches used in our system and provide details of our weighted ensemble. In Section 3, we describe the datasets used in our experiments, and in Section 4 we provide the experimental results. Finally, in Section 5 we summarize the significance of our work and highlight some future directions of exploration.

In this work we compare and combine different recent state-of- the-art person re-identification systems, viz. a representation that combines biologically inspired features and covariance descriptors, called gBiCov [15], Symmetry-Driven Accumula- tion of Local Features (SDALF) [8], Custom Pictorial Structures (CPS) [7] based on chromatic content and color displacement (CCD), Color Invariants (CI) [12], and the Skeleton-based Person Signature (SPS) technique [17]. More- over, we propose variants of such approaches, obtained by varying the features used for describing the images and by using different distance measures. Each of these state-of-the- art systems, our variants, and the different color spaces, distance measures (specifically, the Jeffery Divergence measure, which obtains the best performance), and the color and texture descriptors used in our approaches are described in this section.

To improve the performance of each system, we utilize not only the RGB color space but also several other color spaces. A color space is an abstract mathematical model describing the way colors can be represented [4]. The input images in the tested databases are given in the RGB color space. To explore other spaces, the original images are transformed into the following codings: YUV, HSV, HSL, and XYZ.

YUV defines color in terms of one luma/brightness (Y) component and two chrominance (UV) components, taking into account human perception by reducing bandwidth for the two chrominance components. HSV (hue-saturation- value) and HSL (hue-saturation-lightness) are common cylindrical-coordinate representations of points in the RGB color space. The XYZ color space defines three primaries that are not tied to any particular physical device but rather are points that lie outside the visible gamut, thereby completely encoding all color perceptions possible in the real world.

features (BIF) [20] and covariance descriptors [22], specifically by encoding the difference between BIF features at different scales. This image representation efficiently measures the sim- ilarity between two persons without needing a preprocessing step (e.g., to extract the background) since it is robust to illumination, scale, and background changes.

In Step 1 BIF features are extracted using Gabor filters and the max operator. Color images are split into the three HSV color channels and convolved with Gabor filters at 24 different scales, with neighboring scales grouped into 12 different bands. The BIF magnitude images (Bii e [1, .. ., 12]) are obtained using the max operator within the same band of Gabor features.

In Step 2 similarity of BIF features is computed at neighboring scales using a covariance descriptor. The BIF magnitude images are divided into small overlapping regions to retain the spatial information, and the difference between the corresponding regions of the different bands and the covariance descriptors is computed, i.e., for each region the difference of covariance descriptors between two consecutive bands is computed as

Proposed in [8] SDALF is a method that models three aspects of human appearance: (i) the overall chromatic content, (ii) the spatial arrangement of colors in specific regions, and (iii) the presence of recurrent local motifs with high entropy. This information is derived from different body parts and weighted by exploiting symmetry and asymmetry perceptual principles. This combination makes SDALF robust against very low res- olution, occlusions, pose, viewpoint, and illumination. SDALF exploits both single-shot and multiple-shot approaches; in other words, the larger the number of images of a given person, the greater the expressivity of SDALF. In the descrip- tion of SDALF that follows, the harder case of a single-shot approach will be described.

legs, respectively. R0 (the head) is discarded because its size is small and contains little information. Given R1 and R2, the y-axis of symmetry is located in jLRk = argminj CH(j, d)+ S(j, d), where k = (1, 2) and d is fixed to J/4.

After fitting PS the chromatic content and color displace- ment (CCD) in each of the six parts is considered. Chromatic content is computed using HSV color histograms, where hue and saturation are jointly taken by a two-dimensional histogram, along with a distinct count of full black to take into account areas of low brightness. Since different parts have different sizes (e.g., the torso is roughly three times larger than the head), part histograms are multiplied by a set of N weights.

PartsSC uses the Shape Context (SC) descriptor introduced in [2]. SC is a 2D log-polar histogram counting the number of points falling in radius log(r) and orientation h from the reference point. Two cases are possible given a set of N color observations O = {x1, .. ., xN} within some color space: in the first case observations are given without spatial informa- tion. In the second case, observations are labeled li = 1, if they come from the upper part of the object, or li = 0 if they refer to the lower part. Two different signatures are extracted that correspond to these two cases.

The Skeleton-based Person Signature (SPS) technique evaluates a signature vector for a given target based on the body pose. It takes as input the result of a skeletal tracker, namely a set of body joints, and evaluates a set of local descriptors on the image patches around each joint. It should be noted that the best- performing skeletal tracker algorithms work on 3D data, while when it comes to features the 2D approach performs better than the 3D counterpart. To improve the overall performance, the SPS is evaluated exploiting not only the 3D point cloud provided by the 3D sensor, but also the 2D image of the scene, which is usu- ally also provided by 3D sensors. The skeleton joints are found in the 3D domain by the tracker; they are then projected onto the image plane (thanks to the calibration between 3D sensor and 2D camera). Once the joints are available in the image domain, they are exploited as keypoints for evaluating the local features. Each feature evaluated around a keypoint provides a feature vec- tor (also called descriptor): the complete signature, describing the whole target, is obtained by concatenating all the feature vectors of the different body joints, following a pre-determined order.

SIFT is also tested for SPS since it is the best descriptor among those that were tested in [17], where SPS was first pro- posed. SIFT [14] is widely used in robotics. It is a keypoint detector and a descriptor invariant to image scale and rotation; it is also robust to changes in illumination, noise, and minor affine transformations. SIFT is computed as an 8-binned histogram of gradient distribution within the region around each keypoint. The descriptor is normalized to unit length to obtain illumination invariance.

VIPeR (Viewpoint Invariant Pedestrian Recognition) is a dataset composed of a large number of people (632) that are seen at different viewpoints and is available at http:// vision.soe.ucsc.edu/node/178. Only one image pair for each person is available, and people are framed at a distance. This dataset is a widely used benchmark. As reported in the literature, results on VIPeR are produced by ten runs, each consisting of a partition of 316 randomly selected image pairs. Since this dataset is composed of 2D images, the skeletal tracker is unable to provide body joints; however, for a small subset of images (45 image pairs), keypoints were manually added by a human operator (we call this subset of images VIPeR45).

CAVIAR is a dataset where 72 different people were col- lected for the EC funded CAVIAR project/IST 2001 37540 and is available at http://groups.inf.ed.ac.uk/vision/ CAVIAR/CAVIARDATA1/. Caviar is a dataset in which multiple test cases are considered; the CAVIAR4REID (Caviar for Re-identification) test case was used in our exper- iments. CAVIAR4REID is characterized by high level of occlusions, pose changes, and low-resolution images. As in the case of VIPeR, this dataset is composed by 2D images; keypoints were manually added by a human operator. We performed the same single-frame test described in [7], the frame selection is performed five times and the average results are reported.

different auto-exposure levels of the Kinect sensor. Moreover, CI has a low computational time. Our recommendation is to use the CI ensemble with RGB and YUV only when low com- putational power is available and in cases where there are no pronounced illumination changes.

that unlike the results reported in the original paper [7] we did not change the parameters of this approach for the different datasets. Moreover, examining the results of CPS, it is clear that the proposed approach for extracting features from the mask obtained by CPS works quite well. The differ- ent distances, however, are quite similar in performance, except that the Jeffrey distance outperforms the Euclidean and angle distance measures.

To obtain a more realistic validation of our approach, we used the same parameters in all the tested datasets. In other words, we did not optimize the performance of our systems for each dataset (to avoid overfitting). Nonetheless, our fusion method outperforms the average performance of all the stand- alone approaches. It should be noted that the results reported for SPS on the IAS dataset in [17] are not comparable with those reported in this paper. In [17] the extracted skeletons of low quality were removed; in the experiments reported here, the entire IAS dataset is used (i.e., no frames are pruned).

An interesting example of the difference in performance when a method is optimized for a dataset can be observed in the performance of CPS. If CPS is optimized for VIPER, it obtains a Rank(10) of ~57%, while CPS optimized for CAVIAREID obtains a Rank(10) of ~53% (using our set of images1). In contrast, if we use a set of parameters that remain the same for both datasets, we obtain a rank(10) of 43.8% and 50.3%, respectively, clearly lower than those obtained when separately optimizing the parameters for each dataset.

The new methods proposed in this paper were tested across several benchmark databases: CAVIAR4REID; VIPeR; VIPeR45; IAS. The experimental results demonstrate that the proposed approach provides significant improvements over baseline algorithms. The VIPER45 is a new dataset of 45 image pairs taken from VIPeR that focus on difficult samples with strong pose changes and with subjects wearing similar clothing. It was created because human beings were tested in [7] in a dataset that was built in a similar fashion (i.e., using 45 difficult image pairs extracted from VIPeR). It is thus possible for other researchers in person re-identification to use VIPeR45 for approximately comparing the performance of their computer vision systems with the performance of human beings.

