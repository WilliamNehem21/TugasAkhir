To address the constraints of the above described feature extraction techniques, word-embedding models were proposed. Word-embedding models solve problems by extracting semantic and syntactic details from word representations [29]. The widespread usage of word- embedding has refocused the attention of numerous research projects on neural networks [30]. A plethora of studies focused on the sentence- level classification challenge as a broader problem of SA [31]. Word2vec [32] and Glove [33] are two most commonly used word-embedding models for text transformation. Word2Vec is built on the basis of two models (Continuous Bag-of-Words (CBoW) and Skip-gram mod- els) [34]. The CBoW predicts a word based on its context, while the Skip-gram predicts a word based on a central word or target word. On the other hand, the GloVe embedding method is a global log-bilinear regression model for word representation that generates vectors based on the co-occurrence of words and the matrix factorization method.

This section reviews the literature that has previously been done in text classification. The literature review may be subdivided into two sections. The first one will discuss the word embeddings and state- of-the-art transformers. The second part will discuss the classification models of SA.

Traditional embedding models used for sentiment analysis cannot deal with OOV words and can potentially lose sentimental information. These techniques also have a drawback that they consider similar words from different sentences into the same context. However, it is evident that words from different sentences would have different contexts [45]. In the last two years, transformer-based word embedding models have generated vectors for many text classification tasks. Similarly, a BERT model that was trained on a Chinese Wikipedia corpus, was used for enhancing the performance of Chinese stock reviews with a fully connected layer [45], and with BiGRU [46]. In [47], the authors have developed a personality identification technique based on the BERT embeddings. They had discovered that the personality recognition from the text using the BERT model might enhance accuracy significantly. Authors in [38], had compared various deep models using different embeddings for SA of drug reviews. They had applied embeddings of pre-trained clinical BERT with LSTM and got compromising results. A comparative study had been conducted between word2vec and BERT on Tunisian SA and thus concluded that BERT with CNN achieved the highest results in terms of accuracy [48].

accomplishes task through encoder. Encoders, are a neural network architecture taken from the transformer and used to create encoded representations of text. The pertained BERT-Mini has four encoder layers. Each encoder block has two sub-layers which are multi-head attention and feed-forward.

The tokenizer in this approach first verifies whether the word is in the vocabulary or not. If not, then it attempts for breaking down the word into the possible maximum number of subwords available in the vocabulary, and as a last alternative, it decomposes the word into individual characters. So, once the vocabulary is established, we apply it to tokenization.

In this research, after extracting the intensities (scores of positive, negative and neutral) of each review, the data is labelled on the basis of predefined conditions. If the negative score of the review is more than neutral and positive scores, then the review is labelled as negative. If positive class contains the highest score than other two classes, then it is labelled as positive. Thus, at the end, this method produced fully pre-processed data that is used for generating numerical form of these text reviews.

dimension filter. When the filter is traversed in max-pooling mode, the highest or biggest value at each patch of the filter is selected as a result of the selection. As a result, the output of the max-pooling layer would be a pooled feature map that contained the most prominent/important features of the preceding feature map. The resultant feature matrix has

The proposed scheme is evaluated on different datasets for its scal- ability and efficiency to accurately evaluate techniques on diverse cor- pora with varying domains and sizes. Four state-of-the-art datasets [58] are taken from diverse domains. We have conducted our experiments on positive and negative reviews and avoid neutral reviews. The de- scription of datasets is given below: technique has been assessed by obtaining the overall scores for recall, precision, f-score, and accuracy. These scores are calculated using the confusion matrix. The Receiver Operating Characteristic (ROC) and the Area Under the Curve (AUC), are also used for evaluating the effectiveness of the model. Many text classification tasks, including SA, make significant use of these performance matrices. The BERT-based CBRNN model is compared with other deep learning models using different embedding schemes.

of accuracy and AUC values, respectively. Word2vec-based LSTM got 0.98% recall which is same as to the BERT-based CBRNN model while the proposed CBRNN model has the highest precision rate 0.98% than the other models. So, it can be observed that the proposed model got

AUC. The obtained results are then compared with the most commonly used embedding models, such as glove and word2vec. The proposed model obtained significant improvement in f1-score 0.2%, accuracy 0.3% and AUC 0.4%. The experimental results concluded that the pro- posed CBRNN model is more efficient as compared to the other models. Finally, the BERT-based CBRNN model can applied in industries for performing SA of their products.

