Deep learning (DL) is a subset of ML inspired by the structure of the human brain (Elshawi et al., 2021). The central difference is that in tra- ditional machine learning, all of the data analysis and theories develop- ment like decision trees, logistic regressions, naive Bayes, and support vector machines was done essentially by the programmer who looked carefully at a particular problem and then designed features that would be useful features for handling this problem. These kinds of systems would end up with millions of hand-designed features. It turns out that the machine was learning almost nothing but only run- ning a learning numerical optimization algorithm to do numeric optimi- zation by putting a parameter weight in front of each feature and adjusting those numbers to optimize performance. However, deep learning is part of this field which is called representation learning. The idea of representation learning is to feed to computers raw signals from the world, whether it is visual signals or language signals, and (Roy et al., n.d.) proposed a real-time object detection model that was developed based on the YOLOv4 algorithm. They included CSP1-n block in the backbone, CSP2-n module in the neck, and DenseNet in the backbone to optimize feature extraction, transfer, and reuse. The proposed detection model was used to detect four different diseases in tomato plants. 300 images from each of the four different tomato plant diseases are collected from the publicly available Kaggle Dataset to construct a dataset consisting of 1200 images and augmented to ob- tain the custom dataset of 12,000 images. The model outperforms the existing state-of-the-art detection models in detection accuracy and speed with mean average precision (mAP) value of 96.29% compared to 92.84 for YOLOv4.

bone, SPP block, and modified path aggregation network PANet in the YOLOv4 framework. The model was used to detect different growth stages of mango. A total of 420 original images consisting of 105 images from each of the four growth phases have been considered to construct the original dataset and augmentation has been applied expanding the original dataset tenfold. The mean average precision (mAP) of the pro- posed model has reached up to 96.20% at a detection rate of 44.2 FPS. The proposed Dense-YOLOv4 has outperformed the state-of-the-art YOLOv4 with a 4.73% increase in mAP.

(Kundu et al., 2021) used the YOLOv5 model for the classification and quality testing of seeds. The dataset consisted of 3954 images of seeds of pearl millet, healthy and diseased maize, and clustered was constructed. The model achieved the precision and recall of 99% in classifying the seeds into two classes pearl millet and maize with the quality of healthy or diseased.

(Li et al., 2022) used a dataset of 6371 images of Strawberry pow- dery mildew (PM) and infected leaves (IL). The original YOLOv4 back- bone and neck were replaced by their proposed backbone and neck with depth-wise convolution and hybrid attention mechanism. They combined the proposed backbone and neck, forming four new network structures, the best one was named DAC-YOLOv4. DAC-YOLOv4 used the depth-wise convolution and CSPNet structure concept. Compared with YOLOv4, the mean average precision (mAP) of DAC-YOLOv4 reaches 72.7%, while the size is greatly compressed.

Images for our dataset were manually collected. The images consid- ered the variability between different leguminous seed types. The 11 types of leguminous seeds selected to be the research objects of this study are Glycine max, Lens culinaris-dark, Lens culinaris-yellow, Lupinus albus, Medicago sativa, Phaseolus vulgaris-pink, Phaseolus vulgaris-red, Phaseolus vulgaris-white, Trifolium alexandrinum, Trigonella foenun graecum, and Vicia faba.

We used the Darknet deep learning framework for the YOLOv4 model. Now ready, the images and annotations data were input into the model. For the Faster R-CNN model, we used TensorFlow deep learning framework, which needed the .xml annotations data to be con- verted into the TFRecord data type. Then the dataset was randomly split into train, validation, and test sets with ratios of 80%, 10%, and 10%, respectively.

All object detectors Consist of a backbone, neck, and detection head. First, the input image is fed to the backbone which compresses features down through a convolutional neural network. Unlike image classifica- tion, object detection backbones are not the end of the network. Predic- tions can't be made off only of them; Localization needs to be along with classification. Localization is the task of locating an object in the image by drawing multiple bounding boxes, so the feature layers of the convolutional backbone need to be mixed and held up in light of one an- other. The combination of backbone feature layers happens in the neck then the detection occurs in the head.

Two models were used and compared for object detection. The Faster R-CNN Model was developed from R-CNN and Fast R-CNN. Like all the R-CNN family, Faster R-CNN is a region-based well-established two-stage object detector, which means the detection happens in two stages. The Faster R-CNN architecture consists of a backbone and two main networks or, in other words, three networks. First is the backbone that functions as a feature extractor by running a convolutional neural network on the original map to extract basic features and generate a feature map. In this study, Inceptionv2 pre-trained on the MS COCO dataset was chosen as the backbone. The two main networks, the first network is a simple regional proposal network (RPN) that proposes a set of regions of interest not using a selective search algorithm like its predecessors. The second network is an evaluation network or a detection network that processes both the feature map and the regions of interest generated by the previous networks by a classification layer and a bounding box regression layer, generating the class and bounding box.

The original YOLO (You Only Look Once) used a Darknet flexible framework that was written in low-level languages and has produced a series of the best real-time object detectors in computer vision. The YOLO series moves ever forward with the publication of YOLOv4 in the past couple of months. YOLOv4 outperforms other object detection models by a significant margin in inference speed. The Original YOLO was the first object detection network to combine the problem of drawing bounding boxes and identifying class labels in one end-to- end differentiable network. YOLOv2 made a number of iterative im- provements on top of YOLO, including BatchNorm, higher resolution, and anchor boxes. YOLOv3 built upon previous models by adding an ab- jectness score to bounding box prediction, added connections to the backbone network layers, and made predictions at three separate levels of granularity to improve performance on smaller objects.

A trade-off between precision and recall performance can be ad- justed by the model's final layer softmax threshold. Increasing the threshold would decrease the number of FP which will lead to higher precision and lower recall. Similarly, to increase recall we need to de- crease the number of FN which will reduce precision. Commonly in ob- ject detection tasks, precision needs to be high (predicted positives to be TP). Precision and recall are widely used along with other metrics such as accuracy, which is simply a ratio of correctly predicted observation to the total observations accuracy but more useful than accuracy. Accuracy is a great measure but only when you have symmetric datasets where values of false positives and false negatives have a similar cost. However, the F1 score takes both false positives and false negatives into account. If the cost of false positives and false negatives are very different, it's better to look at both Precision and Recall.

The IoU would be used to determine if a predicted bounding box (BB) is TP, FP, or FN. The TN is not evaluated as each image is assumed to have an object in it. Traditionally, IoU is set to 0.5. when the object detection model run on an image, a predicted bounding box would be defined to be a TP if the IoU is >0.5, FP if either IoU < 0.5 or the bounding box is duplicated, and FN If the object detection model missed the target either because there is no detection at all or the predicted BB has an IoU > 0.5 but has the wrong classification, the predicted BB would be FN.

Small object detection has always been a research hotspot in the field of object detection. In agricultural production, many seeds are very small and mostly have similar colors, making seed detection a chal- lenge. This research used object detection models for the accurate iden- tification and positioning of small seeds. Mosaic data augmentation in YOLOv4 was utilized in the training stage, which mixed four images into one image. The utilization of Mosaic data augmentation has proven

In this paper, the leguminous seeds dataset was collected and with the use of transfer learning two deep learning-based models were trained. YOLOv4 proved to improve the accuracy and the runtime with less computation load. The model can be applied to the detection of seed images in a variety of complex backgrounds at multiple scales, and in multi-angle environments. With an error rate of <2% and a run- ning time of <2 s, the YOLOv4 model constitutes an effective tool for the detection of leguminous seeds.

