Initial weight choice is an important aspect of the training mechanism for sigmoidal feedforward artificial neural networks. Usually weights are initialized to small random values in the same interval. A proposal is made in the paper to initialize weights such that the input layer to the hidden layer weights are initialized to random values in a manner that weights for distinct hidden nodes belong to distinct intervals. The training algorithm used in the paper is the Resilient Backpropagation algorithm. The efficiency and efficacy of the proposed weight initialization method is demonstrated on 6 function approximation tasks. The obtained results indicate that when the networks are initialized by the proposed method, the networks can reach deeper minimum of the error functional during training, generalize better (have lesser error on data that is not used for training) and are faster in convergence as compared to the usual random weight initialization method.

The Resilient Backpropagation algorithm is used for training the network. For the purpose of training 200 input data sets are generated by uniform random sampling of the input domain of the function, and the corresponding output calculated from the function to create the training data set. For testing the generalization capability of the trained network(s), a similar set with 1000 data values is generated and called the test set.

Moreover, from the generalization experiments we may infer that the networks trained after initialization by the proposed method, have better generalization behavior. That is, the error on data not used for training is lower for networks initialized by IWI. For the generalization experiment, the ratio of the best result for the random weight initialization method (WTR) to the result for the proposed method (IWI) lies between [1.08,2.71] across the function approximation task. Thus, for the generalization experiment also, the proposed method has error on an average across problems that is lower by a factor of about 2 for networks trained using the IWI method.

In the current work, a proposal for distribution of SFFANN input to hidden weight and thresholds of the hidden weight is made in a manner such that these weights associated with distinct hidden nodes lie in disjoint intervals. On a set of 6 function approximation task, the efficiency and efficacy of the proposed method is demonstrated. That is, networks that are initialized by the proposed method, can be trained to achieve deeper minima as compared to random weight initialization method; they generalize better and are faster in training.

