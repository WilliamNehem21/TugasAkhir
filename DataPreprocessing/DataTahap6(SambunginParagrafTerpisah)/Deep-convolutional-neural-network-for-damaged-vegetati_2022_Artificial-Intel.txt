Performing accurate and automated semantic segmentation of vegetation is a first algorithmic step towards more complex models that can extract accurate biological information on crop health, weed presence and phenological state, among others. Traditionally, models based on normalized difference vegetation index (NDVI), near infrared channel (NIR) or RGB have been a good indicator of vegetation presence. However, these methods are not suit- able for accurately segmenting vegetation showing damage, which precludes their use for downstream pheno- typing algorithms. In this paper, we propose a comprehensive method for robust vegetation segmentation in RGB images that can cope with damaged vegetation. The method consists of a first regression convolutional neu- ral network to estimate a virtual NIR channel from an RGB image. Second, we compute two newly proposed veg- etation indices from this estimated virtual NIR: the infrared-dark channel subtraction (IDCS) and infrared-dark channel ratio (IDCR) indices. Finally, both the RGB image and the estimated indices are fed into a semantic seg- mentation deep convolutional neural network to train a model to segment vegetation regardless of damage or condition. The model was tested on 84 plots containing thirteen vegetation species showing different degrees of damage and acquired over 28 days. The results show that the best segmentation is obtained when the input image is augmented with the proposed virtual NIR channel (F1=0.94) and with the proposed IDCR and IDCS veg- etation indices (F1=0.95) derived from the estimated NIR channel, while the use of only the image or RGB indi- ces lead to inferior performance (RGB(F1=0.90) NIR(F1=0.82) or NDVI(F1=0.89) channel). The proposed

Several works have tried to correlate the NDVI and other Vegetation Indices (VIs) based on light channels with vegetation coverage estima- tion (Price, 1992; Huete et al., 1997; Wu et al., 2007; Zhengwei et al., 2009; Roth and Streit, 2018; Devia et al., 2019; Ren and Zhou, 2019). Most of them are based on acquiring experimental data on soil and leaf reflectance of different light channels and correlate those reflec- tance with combinations of such channels (which are called Vegetation Indices). They also try to correlate those Vegetation Indices with the ac- tual biomass measurements (fresh weight, dry weight) by means of lin- ear and non-linear regressive analysis. The aforementioned research works correlate the different VIs with the actual biomass weighting by means of pixel-wise linear or non-linear regression analysis.

Some more modern research integrates the spatial information dis- tribution from the RGB pixels to infer the NIR channel. For example, Khan et al. (2018); Lima et al., 2019 propose a method to estimate sev- eral vegetation indices by means of a neural network. However, they use a neural network that do not estimate the vegetation indexes at pixel level resolution, only the average vegetation index for the whole

To avoid bias, the distribution of images across the training, valida- tion and test datasets was selected by plots. This means that each crop field (plot) was assigned an identification number and all images be- longing to the same crop field (plot) were assigned to the same subset of data (train, validation, test). This ensures that images from the same plot taken on consecutive days are assigned to the same set, avoiding contamination of the training, validation and test sets. Eighty percent of the crop fields (plots) were randomly chosen for training, while the remaining 20% were distributed into validation and test sets, and all images were incorporated into the set determined by their crop field (plot) number resulting into 24 plots for training, 2 for validation and 3 for testing.

In our approach, we propose the use of a semantic regression neural network to estimate a virtual NIR channel from an RGB image. This vir- tual channel is then used to enrich the original RGB image to generate a multi-channel image that is used to train a multispectral vegetation seg- mentation convolutional neural network. This multichannel image in- cludes not only the original red, green and blue channels and the estimated virtual NIR channel but also different multi-spectral indices derived from this four channels.

The last layer of this network has been substituted by a sigmoid ac- tivation function and the loss function has been replaced by a mean ab- solute error loss in order to learn a pixel-wise regression transformation that translates the image from the source to the target domain. The last layer consists of a 224x224x1 that performs NIR reconstruction. The network is trained by minimizing a mean absolute error loss function which can be enhanced by an adversarial perceptual loss function fol- lowing a pix2pix architecture (Isola et al., 2017). The addition of the per- ceptual adversarial loss functions ensures that the generated image is visually plausible and the estimated image cannot be distinguished from a real image by a specifically trained network. Similar approach has been followed by Aslahishahri et al. (2021); de Lima et al., 2022 to include a perceptual adversarial loss. However, they use UNet architec- ture as generator which increases the number of trainable parameters. Proposed fully convolutional DenseNet network has 2,3 M parameters whereas its UNet counterpart has 24 M instead.

A second network with similar structure as the one defined in Section 4.1 is used for plant coverage estimation. In this case, the input layer of the network size is MxNxK, where M and N represents the height and the width of the input image and K the number of chan- nels used. In our case the number of input channels is K = 7. These channels are composed by the R, G, B channels of the original image and all the estimated channels by the previous methods (NIR, NDVI, IDCS and IDCR). The final layer of this network is composed by two out- put channels of size M and N (MxNx2) resembling the original size of the image. one of the output channels maps the estimation for the plant coverage segmentation meanwhile the other contains the other classes. A softmax activation layer that ensures the mutually exclusive- ness of the two classes. This network is minimized over a categorical cross-entropy loss function.

neural network to get the estimated NIR channel. Additional vegetation indices (NDVI, IDCS and IDCR) are calculated from the R, G, B and the es- timated NIR channels. Training is performed over the training set of the dataset described in 3. After training, the validation subset of the dataset was used to calculate the optimal thresholds values that maximized the balanced accuracy (BAC). These thresholds were applied over the test- ing set. In order to measure the effect of using estimated NIR channels instead the real ones, specific experiments using real NIR channel have been performed.

of the model. The precision is calculated as the number of true vegeta- tion pixels divided by the number of all predicted vegetation pixels whereas the recall is the number of true positive vegetation pixels di- vided by the number of all true vegetation pixels. For unbalanced datasets in semantic segmentation, F-Score is normally preferred to BAC as it ignores the effect of the true negative class (non vegetation pixels). TP, TN, FN and FP refers to true positives, true negatives, false negatives and false positives respectively.

We have proposed a convolutional semantic regression network to estimate a virtual near infrared channel from an RGB image (RGB2NIR) that can optionally incorporate an adversarial loss. With this adversarial loss, the proposed network can accurately estimate the NIR channel (p_value = 0.96, RMS=4%). This demonstrates that the adversarial loss contributes to generate more efficient estimation for the NIR chan- nel than just employing a convolutional semantic regression network.

We have introduced two novel vegetation indices such as Infrared- Dark-Channel-Substractive index (IDCS) and the Infrared-Dark- Channel Ratio index (IDCR). We have proven that these indices have good separability properties to differentiate vegetation regardless its damage. These vegetation indices can be used independently for vege- tation segmentation purposes independently.

