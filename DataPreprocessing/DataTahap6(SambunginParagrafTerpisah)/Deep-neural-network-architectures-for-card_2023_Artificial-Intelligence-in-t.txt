Imaging plays a fundamental role in the effective diagnosis, staging, management, and monitoring of various cardiac pathologies. Successful radiological analysis relies on accurate image segmentation, a technically arduous process, prone to human-error. To overcome the laborious and time-consuming nature of cardiac image analysis, deep learning approaches have been developed, enabling the accurate, time-efficient, and highly per- sonalised diagnosis, staging and management of cardiac pathologies.

Here, we present a review of over 60 papers, proposing deep learning models for cardiac image segmentation. We summarise the theoretical basis of Convolutional Neural Networks, Fully Convolutional Neural Networks, U- Net, V-Net, No-New-U-Net (nnU-Net), Transformer Networks, DeepLab, Generative Adversarial Networks, Auto Encoders and Recurrent Neural Networks. In addition, we identify pertinent performance-enhancing measures including adaptive convolutional kernels, atrous convolutions, attention gates, and deep supervision modules.

Cardiovascular diseases are the prevailing cause of mortality and morbidity globally, encompassing diverse pathologies affecting the heart and its arteries [1]. Profuse advancements in medical imaging over the last decade form the cornerstone of the clinical diagnosis, staging and management of cardiac diseases [2]. As the clinical environment shifts towards personalised medicine, individualised quantification and analysis of cardiac structure and function is paramount [2]. Nonetheless,

Cardiac imaging analysis involves image acquisition, image seg- mentation and Region-of-Interest (ROI) definition [6]. Image segmen- tation, the focus of this paper, involves classifying the pixels within an image of an organ or substructure based on their specific features [7]. Following this, feature extraction, selection and classification may occur to yield the desired outcome [6]. Thus, accurate and efficient image segmentation is critical to successful cardiac image interpretation, enabling advanced structural processing, 3D reconstruction, and cardiac motion analysis [7]. Cardiac image segmentation constitutes a techni- cally challenging task, significantly influenced by image quality and artefact [6]. The segmentation approach falls into three broad cate- gories: manual, semi-automatic and automatic segmentation [6].

architectures used in cardiac image segmentation, advanced building blocks that can be applied to enhance results, and commonly employed loss functions. Then, we describe the methodology of the literature re- view and present a simplified version of the results. Finally, we sum- marise the top performing NN architectures across various cardiac segmentation tasks, delineate key challenges currently encountered by state-of-the-art segmentation models, and suggest areas of future investigation.

more important than its location, significantly increasing computational efficiency [9]. For example, when segmenting cardiac images, at times it may be important to merely recognise the location of the major heart chambers, however, it might not be necessary to view these structures with optimal pixels [9]. Furthermore, pooling is indispensable to image processing tasks that deal with inputs of disparate sizing [9]. Next, the fully connected layer establishes the features most vital to successful prediction, and thus decreases the dimensionality of features from the preceding layer [20]. Finally, a fix-sized vector is produced as the model output [20].

tation precision [20]. The cropped feature maps from the contracting pathway are thus connected to the expansive pathway, and projected into two convolutions, each proceeded by a ReLu function [21]. The final layer is marked by a single convolution that maps the output [21].

V-Net is a deep learning methodology used for semantic segmenta- tion, designed to overcome the deep and wide nature of CNN layers [25]. This architecture employs a reversible mechanism and asymmetrical convolutions maintaining image size and quality [25]. As a result, V-Net can train high-quality images on a single GPU [25]. This model compiles Contextual Pyramid Pooling modules and versatile modules [25].

built secondary to the dependencies, available for prompt deployment and application without increasing computational load [26]. Three distinctive U-Net models are then automatically generated, a 2D model, 3D model, and a 3D cascaded U-Net, and the best-performing is selected [26]. Hence, nnU-Net proposes an end-to-end automated segmentation methodology with state-of-the-art performance standards.

Transformer encompasses a DL model that was initially developed for natural language processing but has recently been introduced to the image processing domain [27]. Transformers alone do not employ CNN-based architectures; however, modifications have been performed leading to the creation of TransUNet [27]. This model utilises a Vision Transformer (VIT) as the encoder and a CNN as the decoder [27]. The VIT deploys the transformer architecture onto fix-sized patches present within the image, the linear embeddings provided by these patches are then input into a Transformer model [28]. Thus, TransUNet overcomes the lack of spatial context produced by only VIT models [27].

[17]. Although models such as U-Net have integrated de-convolutional layers (up-sampling layers) to maintain spatial resolution, DeepLab utilises an alternative mechanism entitled atrous convolution [17]. Atrous convolution is analogous to down-sampling layers in CNN models, however, it broadens the receptive field while preserving feature map spatial dimension [17]. DeepLab employs Atrous Spatial Pyramid Pooling (ASPP) to aid in handling multi-scale images, con- trolling feature response density to obtain multi-scale context [17]. Resultantly, while FCNs and U-Net are more commonly used in biomedical image segmentation, DeepLab provides a deeper model ar- chitecture with a greater number of features, potentially better suited to complex segmentation tasks [31].

In the context of image segmentation, replacing the generator network with a segmentation network enables the GAN to differentiate between predicted segmentation tasks and the ground truth [20]. However, this approach is associated with difficulties in training, and maintaining segmentation quality [34]. Resultantly, GAN variants have been developed, with one of the most successful being the segmentation adversarial network (SegAN), using a fully convolutional GAN for pixel-to-pixel segmentation [34].

A recent innovation that combines Swin Transformers with GANs is the Swin Transformer-based GAN for multi-modal medical image translation entitled MMTrans, coined by Yan et al. in 2022 [35]. MMTrans is composed of a generator based on the SwinIR architecture (Swin Transformer that can predict deformable vector fields), skilled at generating images within the same category of the modality of choice [35]. After the generator, there is a registration network that corrects any minor mismatches between source and target domain images [35]. Finally, MMTrans contains a discriminator, built using a CNN that dis- cerns whether the target image is most like the generator or the real image [35].

Attention Gates (AGs) offer a solution to the computationally expensive nature of traditional CNN models, aiming to use model pa- rameters and intermediate feature maps more efficiently [43]. AGs enable automatic structural focus with minimal supervision, delineating the features most relevant to a specific task, and repressing less relevant features and regions [43]. Resultantly, AGs eradicate the need for external structural localisation without compromising prediction accu- racy, simultaneously reducing the computational overload associated with CNNs [43]. Multiplicative and additive attention are the two existing types of AGs that can be embedded into any CNN architecture [43].

Deep Supervision Modules (DSV) generate multiple segmentation maps at all levels of resolution, transposed to build secondary segmen- tation maps [44]. This is accomplished by up-sampling the element-wise sum of adjacent resolution segmentation maps until the highest resolu- tion is reached [44]. Resultantly, DSV improves the number of features that can be learnt and optimises model convergence [45].

most popular loss function in image classification and segmentation [47]. Cross-entropy can be used to summarise probability errors in pixel-wise segmentation [20]. Mean-dice loss is another widely employed segmentation-specific function, built as an adaptation to the dice coefficient, a value that calculates the similarity between two im- ages [47]. Weighted cross-entropy and weighted dice-loss, form two variations of the loss functions, using weighted loss terms to overcome class imbalance, and include rare classes or objects [20]. Unified focal loss generalises both dice loss and cross-entropy loss to tackle class imbalance within data sets [48]. This function allows a single hyper- parameter to be fine-tuned as opposed to the six hyperparameters associated with traditional focal loss functions, making it more efficient [48].

This dataset was used in the MICCAI 2017 Conference in a challenge terminating in 2022. The training dataset was comprised of 100 patients (66.67%), 20 patients from each group, while testing data consisted of 50 patients (33.33%), 10 patients from each group. Ground-truth seg- ments were developed by two experienced cardiologists. The segmen- tation targets were LV, RV and Myo. Results and architecture details of the three top-performing segmentation models from the ACDC challenge

The Multi-Centre, Multi-Vendor and Multi-Disease Cardiac Image Segmentation Challenge (M&Ms) encompasses a data set used in the MICCAI 2020 Conference [115]. This dataset is composed of 375 CMR datasets obtained from four MRI scanners across six hospitals in three countries. Thus, when compared to previous challenges such as ACDC, M&Ms provides a heterogeneous compilation of data aiming to reflect the high degree of variability between images obtained from different vendors and locations. Patients included demonstrated diverse cardiac pathologies including hypertrophic cardiomyopathy, dilated cardio- myopathy, coronary heart disease, abnormal RV, myocarditis, ischaemic cardiomyopathy, and healthy volunteer.

achieving a mean dice score of 96.70% for LA and RA segmentation [90]. The outlined approach suggests an improved U-Net, characterised by U-shaped upper and lower sampling layers, built using residual the- ory (ResNet) as the selected encoder-decoder. The suggested residual module aims to limit model depth by delaying gradient convergence during network propagation [90]. Furthermore, sampling modules aid

an obstacle when building deep NNs in cardiac image segmentation. As a result, models are prone to over-fitting, and efficient classification re- quires the deployment of further advanced techniques. For example, two regularisation techniques, weight regularisation and dropout, are often used to optimise learning. The former involves adding weight penalties to the loss function based on the relevance of the input, while the latter

As an attempt to increase the magnitude of training data, strategies such as cross-modality image segmentation have been proposed. This method uses feature adaptation to alter an input image from an unde- sired imaging modality to the modality of choice, indicating that CT images could be used to train a CMR segmentation model [116]. Multi-atlas-based segmentation proposes an alternative approach where an anatomical atlas library containing pre-segmented cardiac structures is transformed into target images for the segmentation model [117].

accuracies approaching 100% [118]. Muller et al. suggests that evalu- ation bias represents a severe obstacle to widespread clinical applica- bility and proposes a guideline to evaluate research reliability in image segmentation [118]. To safely implement image segmentation models within clinical settings, doubts regarding evaluation bias and true model accuracy must be addressed.

tation have only reached mean dice scores of 87.1% [120]. While this model provides a key advantage as it can classify motion artefact and perform image segmentation in one step, increasing training data and employing data augmentation techniques will improve segmentation accuracy [120].

As innovation within the biomedical imaging domain progresses, user-friendly, end-to-end image processing frameworks are becoming increasingly necessary. Medial Open Network for AI (MONAI) is an open source, easily operated, end-to-end biomedical imaging platform, enabling image labelling, transformation, segmentation, and model deployment [124]. Thus, MONAI aims to integrate state-of-the-art findings in biomedical imaging DL solutions into a single platform, driving scientific progression in the field [124].

Over the last decade there has been a steep rise within the field of cardiac ML. Initiatives such as euCanSHare (http://www.eucanshare. eu/) have led to the establishment of international, multi-cohort car- diovascular research platforms, driving innovation, and increasing the prospect of clinical application. Cardiac image segmentation has been fundamental to this, aiming to reach a highly personalised, patient- centred, accurate and time-efficient approach to the diagnosis and management of cardiac pathologies. Image segmentation forms a field of biomedical ML with high clinical acceptance, as it reduces clinician workload without significantly intervening in their decision-making process [5].

Bui V, Hsu LY, Chang LC, Sun AY, Tran L, Shanbhag SM, et al. DeepHeartCT: a fully automatic artificial intelligence hybrid framework based on convolutional neural network and multi-atlas segmentation for multi-structure cardiac computed tomography angiography image segmentation. Front Artif Intell 2022; 5:1059007.

