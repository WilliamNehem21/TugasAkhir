Due to advancements in technology, many integrated circuits are fabricated to develop an artificial system that could perform "intelligent" tasks similar to those performed by the human brain. Many of them use off-chip learning method either by analog hardware or massively by parallel computers. This proposed work is about a trainable neural chip using Field Programmable Gate Array (FPGA) as this helps in learning capability by exploiting the inherent parallelism of neural network. By this fast prototyping is possible for real-time applications, such as speech recognition, speech synthesis, image processing, pattern recognition and classification. In this work on-chip learning method is designed for standard benchmark XOR problem using back propagation based multilayer perceptron and is implemented in VIRTEX-E FPGA using VHDL. The design works at 5.332 MHz and the total gate count is 4, 73,237.

One of the emerging applications of Very Large Scale of Integration (VLSI) is standalone neural network chip. This stand alone neural network chip could surpass the capabilities of conventional computer-based pattern recognition systems and they are used in pattern classification, data processing, electrical load forecasting, power control systems, quantitative weather forecasting, games development, optimization problems etc. [1]. Artificial Neural Networks (ANNs) are powerful tool for modeling especially when underlying data relationship is unknown. It offers a completely different approach to solve the real-time problems and they are known as sixth generation of computing techniques [2].

This paper aims at the design of on-chip learning Multilayer Perceptron (MLP) based neural network with Back Propagation (BP) algorithm for learning to solve XOR problem. Section 1.1 reviews the architecture and Section 1.2 describes the learning algorithm of neural network. Section 2 deals with the implementation of on- chip learning followed by discussion of results.

Each neuron in all layers is stimulated simultaneously and forwards the output to next layer. Finally the output is produced at the output layer by applying the activation function to calculate the value of weighted sum. This final output is given to the error calculation module to find MSE. Error comparison module of global controller is used for checking the condition of learning. If the error value is greater than the threshold value, global controller sends enabling signal to backward phase controller. The backward phase of BP algorithm consists of two main important phase

At first gradient value of output neuron is calculated and back propagated to previous hidden layer for its gradient calculation. After the gradient calculation, weight updating process takes place simultaneously for each layer and every weight of neuron. The new updated value is stored in RAM for future use. Thus completion of weight updating process indicates one complete training set and these two stages are repeated for all other input patterns until the network is sufficiently trained.

