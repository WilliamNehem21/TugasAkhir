Due to its lightly pungent, crisp, and smoky taste, the New Mexico chili pepper (Capsicum annuum) is widely popular in the Southwest re- gion of the USA. Sometimes referred to as chiles, the pepper in New Mexico is a cash crop with an annual harvesting of approximately 8000 to 10,000 acres and is used for consumption, processing into dried spice, or decorations (strung on ristras) (Bosland et al., 1991).

where zi, j, k is the input of the activation function located at (i, j). Other variants include Leaky ReLU (and many other variants), ELU, Maxout, and Probout. We refer the reader to the following reference for an in- depth analysis of CNN components (Gu et al., 2018).

Object detection has a rich history of literature with vast improve- ments made recently that utilize computer vision to identify where and what an object is in space. Literature generally accepts the notation that two eras exist for the progression of object detection; traditional object detection before 2014 and the introduction of Deep Learning (DL) for object detection after 2014 (Zou et al., 2019). Before DL was uti- lized for object detection, many investigations were limited due to com- putation power and trainable image representation. We refer the reader to (Zou et al., 2019) for a comprehensive survey on object detection.

Viola and Jones (2001), introduced the Viola-Jones detector with the purpose of robust rapid and real-time detection. In this work, the au- thors focus on simple features rather than pixels to speed the detection process up. Three main contributions were made in the study. 1) Integral Image which is a method for using intermediate image representation for rapid rectangle computation, 2) an algorithm based on AdaBoost for feature extraction by combing some features to form an effective classifier, and 3) a cascade method for classification to discard or pass on viable information for further analysis and ultimately, increased de- tection performance. Viola and Jones (2004) demonstrated the robust- ness of this method by applying the techniques to facial recognition. Viola et al. (2005), utilized the cascade method for moving person de- tection along with AdaBoost for increasingly complex rejection regions based on a rule system.

Another cornerstone in object detection was introducing the Histo- grams of Oriented Gradient (HOG) detection system presented in Dalal and Triggs (2005). In this investigation, Dalal et al. created a method that utilizes edge direction and gradient intensity to determine the appearance of an image. This task is accomplished by separating the

The Deformable Part-based Model (DPM) was developed by Felzenszwalb et al. (2008) and builds off of the architecture from (Dalal and Triggs, 2005). Using the underlying blocks, the model estab- lishes HOG descriptors via histogram gradient magnitudes within each 1D pixel. Filters are used to dictate weights in the detection window. Learning is accomplished on the PASCAL training data with a latent SVM. Using DPM, the authors won 2007, 2008, and 2009 Pascal Visual Object Classes (VOC) detection challenge (Everingham et al., 2007).

For a comprehensive survey of Convolutional Neural Networks (CNN), we refer the reader to (Gu et al., 2018). DL approaches advanced significantly in 2012 with the introduction of AlexNet that was based on LeNet-5 architecture, just with a deeper structure (Krizhevsky et al., 2012).

improvements on the fast R-CNN design in the same year to move the algorithm closer to real-time detection speeds. This version became the first end-to-end and close to real-time object detector (Zou et al., 2019). In the first stage, a Region Proposal Network (RPN) considers a candidate bounding box. In the second stage, feature extraction is com- pleted using Region of Interest (RoI) pooling (RoIPool) from each candi- date which executes classification and bounding box regression (Ren et al., 2015).

2) how the model augments data on high-quality images, and 3) optimi- zation of the convolutional and pooling layers of their Faster R-CNN model. Specifically, feature extraction was completed with a CNN model VGG-16 using 13 convolutional layers, 13 ReLu layers, and four pooling layers. Two loss functions were added to optimize the convolutional and pooling layers, allowing the parameters to adjust au- tomatically during training. They found that automatically tuning pa- rameters resulted in 58 ms/image detection speed with mAP % of

Fast R-CNN, and Faster R-CNN in terms of detection speed and mAP%. When the authors fed their dataset to YOLOv3 they achieved a 40 ms/ image detection speed and mAP % of 90.03. So, they were able to soften the gap of performance between the Faster R-CNN and YOLOv3 architectures.

Both cameras were mounted over the chili plants at a fixed dis- tance of 130 cm with a light background. The thermal camera was fully controlled with the FLIR Tools; In fact, pre or post-processing could be done on an individual image, using ResearchIR software. Using an IR lens with a focal length of 13.1 mm, 112 thermal images (Besides 112 RGBs) were captured every other day over three months. On each day, four different images were taken from different directions for a single plant. Some of the images were captured in an outdoor setting and in natural light to validate the proposed technique.

Where, TP is true positive, FP is false positive, and FN is false neg- ative. True positives are when the model correctly classifies the object detection. False positive occurs when there is incorrect object detec- tion. False negative occurs when ground truth bounding boxes are present in the image and the model fails to detect the object in the image. True negative (TN) is not considered for this application since it would be correctly not labeling parts of the image that do not have truth bounding boxes.

For our implementation of the Mask R-CNN architecture, we utilize FPN for feature extraction which is leveraged from a ConvNet's pyrami- dal feature hierarchical system (Lin et al., 2017). To do this, a region- based object detector or Region-of-Interest (RoI) pooling extracts fea- tures from the matrix. The RoI is assigned as: where, bx, by, bh, andbw are bounding box center coordinates at x, y and width and height of the prediction box. tx, y, w, h are network outputs. Object scores are calculated as the probability (between 0 and 1) that an object is inside a bounding box from a sigmoid activation function. We report all findings in Section 4.3.

This section provides the results of all experimentation completed during training. Here we show the viability of certain techniques ap- plied to a chili pepper dataset in an environment with complex back- grounds and good ambient illumination. Section 4.1 details the HOG algorithm results, Section 4.2 provides results for the Mask R-CNN archi- tecture, and Section 4.3 shows results from real-time detection using the YOLOv3 architecture. The RGB dataset was considered for experi- mentation on the HOG, Mask R-CNN, and YOLOv3 models.

Computer vision is a highly researched area with many new and cutting-edge technology. As a result, terminology can sometimes get confusing. AP was first introduced by VOC2007 (Everingham et al., 2007) and the calculation is applied to every image within the labeled set and measures the average AP of the entire set. We follow the COCO definition for mAP terminology as: where, APi is the average precision at image i and N is the total number of images. The mAP metric is usually used as a final metric to compare performance throughout all object categories (Zou et al., 2019), which we use to evaluate experiments with both the Mask R-CNN and YOLOv3 models.

algorithm has identified and located all chili peppers in the image (with different colors), assigned a prediction bounding box, and labeled it with the appropriate label (chili pepper). As covered in Section 3.2, precision and recall are determined by evaluating truth bounding boxes and predicted bounding boxes. While training on the RGB dataset, the Mask R-CNN performed with a train mAP of 0.872 and test mAP of 0.896 with an overall computational time of 40.79 s per test image.

training loss drops below 2.0 roughly at the 250th epoch. After training was complete, we evaluated the model using mAP performance metrics. Both training and testing resulted in 100% precision accuracy (mAP) with a computational time of 3.64 s per test image.

algorithm has superior capabilities to the Mask-RCNN with over ten times the computational speed on the chili dataset. The YOLOv3 model introduces a spike in model performance for real-time detection (~3 s predictions) and demonstrates the possibility of being utilized during real-time robotic harvesting. However, we reported a lack of confidence in detection when heavy amounts of debris and paper over- lapping are present in RGB images. In this study, it was shown that non- destructive thermal imaging can resolve these issues. In particular, YOLOv3 demonstrated an ability to detect peppers in areas with high debris and pepper overlapping with the thermal imagery; however the Mask R-CNN model did not detect well on the thermal imagery. It was shown that mapping temperature differences between the pepper and plant/debris can provide significant features for object detection in real-time and can help improve accuracy predictions with heavy debris, variant ambient lighting, and overlapping of peppers. In addition, suc- cessful thermal imaging for real-time robotic harvesting could allow the harvesting period to become more efficient and open up harvesting opportunity in the evening hours or low light situations.

Further investigation should be pursued to improve object detection when dense debris and poor ambient light are present. One issue in our experimentation is that only one class was used during training. This issue could lead to overfitting and could be the reason that YOLOv3 per- formed so well on the datasets. Additional classes and variations in im- agery type should be utilized during training and testing to avoid overfitting. We plan to expand the number and type of images in the dataset and determine if the YOLOv3 (or other models) can exceed per- formance expectations during real-time robotic harvesting in dense de- bris and poor lighting environments with peppers in natural environment.

