The detection of personality traits is a critical text analytics task in natural language processing (NLP), but achieving interpretability with deep learning models remains a challenge. Transfer learning with the Universal Language Model Fine-Tuning (ULMFiT) has proven to be an effective approach for personality traits classification in NLP. This approach has wide-ranging implications in fields such as health and forensics, and there is a growing demand for the evaluation of online documents due to their widespread dissemination across various languages.

Transfer learning has shown significant promise in computer vision but has only recently gained success in NLP through ULMFiT, which leverages language modeling and deep pretraining word representation techniques. This model architecture has demonstrated the ability to generalize features learned from one task to another, leading to state-of-the-art results.

Our study introduces an efficient and explainable model based on language modeling, proposing a method that encodes essays from word level to sentence level and uses 3-dimensional convolution to understand the article's structure. We apply gradual unfreezing and differential learning rate techniques along with specific dropout and fine-tuning strategies, resulting in significant improvements in accuracy across various personality traits.

Furthermore, we achieved competitive statistical margins, with the proposed model outperforming the Agreeableness (AGR) trait by a significant margin of 2.54% and showing competitive performance in other traits as well. By training the model on both general-domain and task-specific datasets using forward and backward language models, we were able to build two versions of the proposed architecture and achieve the best accuracy results through an ensemble of both. These findings are presented in the study, which also outlines the related work, model architecture, experimental setup, and future directions for research.