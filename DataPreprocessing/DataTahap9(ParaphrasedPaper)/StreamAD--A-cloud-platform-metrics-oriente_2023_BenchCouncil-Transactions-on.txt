The rapid expansion of the cloud platform market has resulted in a massive scale of operations. However, this growth also presents significant challenges for site reliability engineers (SREs) in identifying and diagnosing faults within large-scale cloud platforms. The ongoing operation of computing infrastructures providing services must ensure service level agreements (SLAs) to customers. Unexpected service downtime can have a substantial impact on stability objectives and result in significant financial losses.

Accurate and efficient anomaly detection algorithms are essential for maintaining service level agreements. There is a high demand for algorithms that can detect metric anomalies in real time and continually update their models to ensure accurate and reliable detection, particularly in the presence of significant changes in data distribution, also known as concept drift.

Metrics play a fundamental role in cloud platforms, serving as numerical values or counters that represent the state of the system. These metrics are essential for monitoring various aspects of the cloud platform, with Google Cloud metrics covering categories such as CPU, system, memory, block, and network.

The StreamAD system is designed to utilize machine learning for metric monitoring, allowing for receiving and analyzing streaming data and identifying anomalies, which are then notified to users and managed through a customized dashboard by SREs. Several benchmarks and datasets, such as TODS, NAB, Exathlon, UTSD, and others, are discussed in the academic paper, focusing on different aspects of anomaly detection.

The paper also discusses various datasets for evaluating anomaly detection algorithms, such as AIOPS_KPI, Micro, and Gaia, and proposes evaluation criteria for assessing the quality of anomaly detection algorithms.

In conclusion, the paper introduces StreamAD, a benchmark for unsupervised online anomaly detection tailored to cloud metrics. It includes comparisons of various anomaly detection algorithms and evaluates their effectiveness, efficiency, and memory resource consumption. The authors recommend using specific algorithms based on the type of data being monitored, and discuss future directions for enhancing benchmark evaluation.