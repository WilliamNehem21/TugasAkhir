Due to technological advancements, there has been a rise in the fabrication of integrated circuits to create artificial systems capable of carrying out "intelligent" tasks akin to the human brain. Many of these systems use off-chip learning methods, such as analog hardware or parallel computers, to achieve their intelligence. This study proposes the development of a trainable neural chip utilizing field programmable gate array (FPGA), leveraging the inherent parallelism of neural networks to facilitate learning capabilities. This approach enables fast prototyping for real-time applications like speech recognition, speech synthesis, image processing, pattern recognition, and classification. The research focuses on designing an on-chip learning method for a standard benchmark XOR problem using a backpropagation-based multilayer perceptron, which is then implemented in a Virtex-E FPGA using VHDL. The designed system operates at 5.332 MHz with a total gate count of 4,73,237.

An emerging application of Very Large Scale Integration (VLSI) is the development of standalone neural network chips. These chips have the potential to outperform traditional computer-based pattern recognition systems and are applied in various fields, including pattern classification, data processing, electrical load forecasting, power control systems, quantitative weather forecasting, games development, and optimization problems. Artificial neural networks (ANNs) are a powerful tool for modeling, particularly in scenarios where the underlying data relationships are unknown. They offer a distinct approach to solving real-time problems and are often referred to as the sixth generation of computing techniques.

This paper aims to design an on-chip learning multilayer perceptron (MLP) based neural network with a backpropagation (BP) algorithm to learn to solve the XOR problem. Section 1.1 provides an overview of the architecture, while section 1.2 explains the learning algorithm of the neural network. Section 2 focuses on the implementation of on-chip learning, followed by a discussion of the results.

In the proposed model, each neuron in all layers is simultaneously stimulated, forwarding the output to the next layer. The final output is generated at the output layer by applying an activation function to calculate the weighted sum. This output is then fed to the error calculation module to compute the mean squared error (MSE). A global controller's error comparison module is utilized to check the learning condition. If the error value exceeds the threshold, the global controller sends an enabling signal to the backward phase controller. The backward phase of the BP algorithm consists of two main stages.

First, the gradient value of the output neuron is calculated and back-propagated to the previous hidden layer for gradient calculation. After gradient calculation, the weight updating process occurs simultaneously for each layer and every neuron's weight. The newly updated values are stored in RAM for future use. Completion of the weight updating process indicates the training of one complete training set, and these two stages are repeated for all other input patterns until the network is adequately trained.