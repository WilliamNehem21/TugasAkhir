The five models mentioned earlier are typically based on machine learning methods that only utilize the best single value of each parameter for making predictions. However, as these parameters are learned from data, their uncertainty should be taken into account in the prediction process. As the sample sizes increase, the parameter uncertainty decreases. To demonstrate the impact of parameter uncertainty on predictions, a smaller dataset was created by selecting every eighth data point from the previous example.

A Bayesian model that considers parameter uncertainty provides a probability interval (PI) as the uncertainty measure. When compared to a classic quadratic regression model that neglects parameter uncertainty, the PI from the Bayesian model has slightly narrower dashed black lines. The mean function remains the same for both the Bayesian and classic models.

It is common for models to have a larger number of parameters than the example given (e.g., the GPT-3 deep learning language model has 175 billion parameters). Gathering more data to reduce parameter uncertainty is often not feasible because it enables the fitting of more complex models and increases the number of parameters.

In addition to parameter uncertainty, there are other sources of uncertainty such as predictors being the output of imperfect prediction models, uncertainty from estimated predictors, and introduction of Berkson error when converting continuous values into bins or groups. Binning is discouraged due to the potential for misclassification error.

Another source of uncertainty arises from truncated data when values outside a specified range are omitted, leading to unknown number of omitted values. Additionally, uncertainty and bias can be introduced when estimated properties correlated with the omitted values differ from their true values.

To account for uncertain measurements, a method of generating new data that incorporates correlations between variables is described. By taking the correlations into account, each dataset can be analyzed separately and the resulting predictions can be combined.

Parameter uncertainty is accounted for in a test scenario where the training data is assumed to be measured without error, while the test data is measured with error. The effect of ignoring measurement error on predictions is then compared to the standard approach of ignoring measurement error in both the training and test data.

Various methods, such as the no-u-turn sampler (NUTS) and mixtures of distributions, are used to update and model uncertainty in the parameters, allowing for increased flexibility in the modeling process. However, not all sources of uncertainty can be captured, as several factors external to the model, including experimental decisions and data processing pipelines, may also contribute to uncertainty in the predictions and results.

In summary, taking into account parameter uncertainty, uncertain predictors, measurement error, and external project workflow choices is essential for making accurate predictions in modeling.