Automated methods for controlling gear shifts are available, and manual calibration is typically performed through time-consuming road tests, involving adjustments to parameter maps until the resulting shifts align with the intended characteristics of the engineer overseeing the process. Numerous objective criteria, such as the vibration dose value (VDV) and the amplitude and root mean squared (RMS) value of longitudinal acceleration at the driver seat, exist to express subjective shift quality. Various approaches to automation of calibration, leveraging fuzzy logic, evolutionary algorithms, or gradient-based optimization over high-order polynomial functions mapping control parameters to comfort, have been developed and published.

Sommer Obando applies tabular Q-learning and SARSA algorithms to control normal force on a friction clutch in order to reduce clutch judder. However, this work simplifies the control task considerably and neglects several aspects of realistic automotive transmissions, such as nonlinear actuator dynamics and partial observability. Additionally, a significant loss of performance is reported when transferring the agents from simulations to the experimental transmission.

Lampe et al. use a DDPG agent to learn a closed-loop policy in a simplified simulation, demonstrating the agent's ability to control clutch engagement for vehicle start-up in automated manual transmissions (AMT) and dual-clutch transmissions (DCT). However, no validation on a physical transmission or car is conducted.

The conventional actuation logic for most gear shifts consists of open-loop control and a subordinate PI-controller in combination with adaptive schemes to achieve the desired trajectory of the clutch slip speed. This shift process can be divided into two major phases: filling the piston until it touches the friction plates, and the synchronization phase. During filling, no change of variables can be measured, so the open-loop controller follows a predefined current trajectory based on parameter maps designed for a range of operation points like temperature and torque demand. However, this conventional control strategy has several drawbacks, such as a limited allowed actuation range and the inability to predict and counteract future offsets.

For the shifting control task, two state-of-the-art model-free deep reinforcement learning (DRL) approaches, Proximal Policy Optimization (PPO) and Soft Actor Critic (SAC), are considered. SAC is an actor-critic Q-learning off-policy DRL algorithm, learning a stochastic policy that maximizes the expected return and the entropy of the policy. It incorporates the soft MDP formulation and utilizes concepts of continuous deep Q-learning, a replay buffer, target Q-networks, and clipped double Q-learning.

Moreover, the authors express gratitude to individuals who contributed to the implementation and promotion of novel techniques for transmission control.