A critical step in music emotion recognition (MER) involves feature extraction and classification. The selection of appropriate features from music signals is crucial as they effectively represent the music and allow for efficient computation. Prior research on feature extraction from music has mostly focused on timbre features. One well-known timbre feature is Mel-Frequency Cepstral Coefficients (MFCC), which has demonstrated high performance in speech recognition and is now being investigated for its potential application in music modeling. This study aims to explore new features for identifying musical emotions, building upon existing research on MER.

Spectrum features are derived from the short-time Fourier transform (STFT) of an audio signal. MFCC, a prominent spectrum feature, has proven effective in tasks related to both speech and music recognition. It leverages auditory principles and the decorrelating property of the cepstrum to compute features for segments of music files. In this work, we utilize the linear prediction (LP) residual and the phase of the analytic signal derived from the LP residual to extract emotion-specific information from the excitation source signal, thereby enhancing the recognition of musical emotions.

The architecture of the artificial neural network (AANN) includes input and output layers with linear activation functions and hidden layers with the option of either linear or nonlinear activation functions. Research on three-layer AANN models has shown that using nonlinear activation functions at the hidden units can effectively cluster input data in a linear subspace. Additionally, it has been theoretically established that relaxing the network's constraints in terms of layers allows for the clustering of input data in a nonlinear subspace. Consequently, a five-layer AANN model was employed, utilizing a clustering algorithm such as k-means clustering to cluster training vectors into hidden layer units, ultimately achieving optimal performance in training and testing the residual phase features for each emotion.

During testing, the extracted MFCC features from the test utterances are inputted into a Support Vector Machine (SVM) model, and the distance between the feature vectors and the SVM hyperplane is calculated to determine the positive score for each emotion model. Based on these scores, the category of emotion is determined, resulting in an average emotion recognition performance of 94.0%.

In this study, a linear prediction order of 16 is employed to derive the LP residual, which is extracted from emotional music signals using a pre-emphasizing first-order digital filter and a 20 ms frame size with a 50% overlap between adjacent frames. The highest Hilbert envelopes around 40 samples for each frame are then extracted. A single SVM model is developed for each emotion, resulting in an average emotion recognition performance of approximately 56.0% after testing all the test utterances.

The 40-dimensional feature vector extracted from each frame is utilized as input for the SVM model, with a single SVM model created for each music emotion. All music emotions belonging to the same category are merged into a single category, resulting in the creation of five SVM models. During the training phase, the SVM is trained to distinguish acoustic features of each emotion, further contributing to the overall recognition performance.