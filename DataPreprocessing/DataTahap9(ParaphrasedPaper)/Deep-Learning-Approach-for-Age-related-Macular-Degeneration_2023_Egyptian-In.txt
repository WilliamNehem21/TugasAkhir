The fundamental component of deep learning architecture is the convolutional layer, which extracts features from retinal images using a set of filters. Each filter, represented as a weights matrix, slides over the input, computing a dot product to produce an output value and create a feature map indicating the detected features. Multiple filters are utilized in a convolutional layer to detect different features, and the size or resolution of the feature maps can be adjusted using padding, stride, dilation, or pooling. This forms the basis for the development of various blocks for different deep learning model architectures.

One such block is the convolution block, which is a key component of most deep learning models, such as convolutional neural networks (CNNs). It comprises multiple layers that perform convolution, pooling, activation, and other operations on the input data, making it popular in tasks such as image classification and object detection.

Another type of layer is the reduction block, which aims to reduce the spatial dimensions of feature maps using pooling or strided convolution to decrease computational cost and memory usage while retaining essential information. Reduction blocks can also enhance the network's generalization ability and receptive field, and are often used before or after inception or residual blocks.

Our study focuses on training deep learning models, including Inceptionv3, NASNetLarge, and ResNet152v2, using transfer learning with retinal images from the Chula-AMD dataset and an independent dataset to evaluate the models' generalization ability. We provide guidance for researchers on selecting the most suitable deep learning model for AMD classification based on accuracy and generalization.

The paper is organized into sections discussing the use of deep learning models for AMD detection, the materials and research methodology in the study, the experimental results and discussions of trained models for 2-labels and 3-labels AMD classifications, and a conclusion.

Wet AMD is a rare and severe form of late AMD that rapidly damages the macula, often requiring early intervention to improve outcomes.

In our experiments, we utilize early stopping during the training process to terminate it based on the accuracy of the trained deep learning model on the validation dataset. We collect precision and recall values to measure the performance of the models in our experiments.

Transfer learning, a technique commonly used in computer vision, involves using a pre-trained deep learning model for a related task to achieve high performance, especially with small datasets. It also saves time and computing resources compared to training a model from scratch.

For future work, we plan to conduct a dataset for 4-labels AMD classification (e.g., normal, early AMD, intermediate AMD, advanced AMD) and train the DenseNet201 deep learning model using this dataset. We also intend to evaluate the performance of the DenseNet architecture for classifying other eye diseases, such as diabetic retinopathy and glaucoma.

The paper includes biographical information about the authors and their research interests.