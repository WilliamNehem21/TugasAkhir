Distributed Denial-of-Service (DDoS) attacks leverage multiple distributed resources, such as servers, services, or networks, to target victims. With the increasing reliance on the internet for data exchange and delivery, the frequency of DDoS attacks has surged. These attacks aim to render online services unavailable by flooding the target network with traffic from various sources, thereby impeding legitimate user access. Although DDoS attacks emerged in 1998, widespread recognition of their impact only occurred when they targeted corporations and organizations in July 1999. Subsequent studies have revealed a continuous rise in the number of DDoS attacks, prompting security practitioners to conduct experiments to detect them. However, the identification of DDoS attacks using a machine learning intelligent prediction model necessitates a careful selection of features from security event data or databases to optimize performance. The presence of a large number of features can impede the efficiency of machine learning by slowing down the training data analysis.

Unlike DDoS attacks, which utilize a multitude of distributed resources, Denial-of-Service (DoS) attacks involve a single computer and internet connection to overwhelm a targeted resource or system, disrupting network services and causing substantial losses. Due to its simplicity, DoS attacks can be executed and controlled by inexperienced threat actors. Various types of DoS attacks, such as volumetric attacks, syn flooding, fragmentation attacks, TCP-state exhaustion attacks, application layer attacks, and plashing, pose significant challenges to network security.

Anomaly-based methodologies create a baseline profile of normal network, program, or system behavior, enabling the implementation of a learning system that can predict outcomes based on previously acquired data, including unknown and new attacks (zero-day). Soft computing techniques, such as fuzzy reasoning and artificial neural networks, are commonly employed to identify known and supervised attacks. The ability of machine learning-based methods to capture interdependencies, adaptability, and flexibility contributes to their effectiveness in detecting features associated with attacks, such as packet size, packet rate, and bit rate.

The Boruta algorithm, developed by Witold Rudnicki and Miron Kursa, serves as a wrapper around random forest to identify important features in the dataset with the outcome variable. However, the exhaustive process of feature selection using the Boruta algorithm can be time-consuming due to the large number of features, limiting traceability.

Supervised learning, which utilizes labeled datasets to train data for accurate prediction and classification of outcomes, includes two types: classification and regression. Random forest, a well-known classification model in machine learning, employs a large number of decision trees with child and parent nodes to predict new unlabeled data.

The Multilayer Perceptron (MLP) algorithm, which consists of input, hidden, and output layers, employs the backpropagation or supervised learning technique to train the network. Its capability for mapping, regression, and generalization makes it suitable for complex tasks in machine learning. Decision tree algorithms, such as ID3 (Iterative Dichotomiser 3) and C4.5, offer accurate prediction while also explaining patterns, handling missing values, and estimating error rates.

Despite the advantages of the MLP algorithm, particularly in minimizing prediction errors and generalization, it may be susceptible to underfitting or overfitting based on the number of hidden neurons selected during the training process. Another crucial aspect of developing a classifier involves feature selection to enhance prediction accuracy by eliminating irrelevant data.

The study found that the random forest algorithm exhibited the highest accuracy among all classifiers when detecting DDoS attacks, providing a foundation for this section. However, the tuning of random forest's performance through hyperparameters, such as the number of decision trees and features, must be carefully managed to avoid diminishing accuracy rates. In contrast, the Boruta algorithm, while effective in feature selection, may be time-consuming due to prolonged execution times, obstructing detailed traceability and analysis due to the dataset's large number of features.