The recognition of human actions in realistic videos has practical applications such as online video surveillance and content-based video management. Existing single action recognition methods struggle to distinguish similar action categories due to complex background settings in realistic videos. In this study, a novel action-scene model is investigated to capture contextual relationships between actions and scenes in realistic videos. Using a generative probabilistic framework, the model infers actions directly from the background based on visual words, requiring little prior knowledge of scene categories. Experimental results on a realistic video dataset demonstrate the effectiveness of the action-scene model for recognizing actions in background settings, showing robust performance even when the features are noisy.

Traditional methods based on scene detectors rely on predefined scene and action categories for all videos or require time-consuming training of detectors, which can impact recognition performance. In contrast, the proposed approach aims to learn contextual cues using a generative framework without reliance on specific action and scene categories. The model captures the relationship between actions, scenes, and background features, requiring only the number of scenes instead of scene categories.

The proposed action-scene model differs from detector-based methods by learning contextual cues without the need for action and scene categories. Inspired by recent work on action recognition in static images and unsupervised learning methods, the model represents actions and scenes in videos using a generative probabilistic approach. The model is developed by segmenting videos into person and background regions, detecting person regions using an object detector and mean shift tracking, and extracting background features such as color histograms, gist descriptors, and sift descriptors from key frames.

The action-scene model treats each video as a mixture of action categories, where each action is a probability distribution over scenes and each scene is associated with a distribution over visual words. The model not only identifies which action occurs in a video, but also determines the associated scene.

The study evaluates the proposed approach on a challenging YouTube dataset, showcasing its effectiveness in recognizing 11 action classes performed under various scenes. The experimental results demonstrate that the addition of contextual cues improves recognition precision and robustness when applied to realistic video datasets.