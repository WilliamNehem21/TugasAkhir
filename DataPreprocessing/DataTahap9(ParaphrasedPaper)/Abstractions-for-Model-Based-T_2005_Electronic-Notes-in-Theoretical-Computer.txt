The use of techniques such as model checking and theorem proving in software development involves operating on abstractions, typically represented as formalized transition systems or behavior models. These techniques assume that the software being analyzed is embedded within a correctly functioning environment that aligns with the implicit assumptions in the model. However, given the complexity of environments, formal verification technologies cannot guarantee whether the environment behaves as expected within the system being assessed. This points to a fundamental feature of verification technologies, where model checking and theorem proving necessarily work with abstractions, while testing operates on actual devices within their real environments, with both activities being essential.

This paper presents a comprehensive inventory and discussion of relevant abstractions for model-based testing, exploring the relationship between testing models and other development process models. The nature of abstraction in the context of model-based development and testing is examined, followed by a review of the abstractions commonly used in testing literature. The paper then discusses the limitations of different abstractions and concludes with a summary, supplemented by citations of related work as appropriate.

The paper argues that the abstraction gap, which is bridged by compilers and linkers, is largely specific to particular domains. While certain concepts, such as procedures, are universal to all programming languages, other abstractions, like the Swing API for GUI programming and the ISO-OSI stack for communication, are confined to specific domains. Developing abstraction techniques that are domain-specific and can be automatically translated to a concrete level using compilers and linkers presents a significant and challenging task.

In summary, abstractions can manifest as simplifications where missing information is automatically inserted, or as deliberate omissions to maintain simplicity. These simplifications tend to be domain-specific, and the ultimate objective of model-based development is to identify applicable abstractions for all systems within a given domain.

The paper provides examples of applying abstractions in testing scenarios, such as creating small test models for error-prone components, and the use of separate models to test different functionalities. However, it also cautions about potential drawbacks of intensive data and temporal abstractions, highlighting that excessive abstraction can lead to information loss and hinder feature interaction detection.

Despite these limitations, the paper contends that model-based testing is a promising methodology for verifying complex systems and improving the quality of software and hardware. It emphasizes the importance of recognizing testing as inherently model-based, given its reliance on explicitly defined intended behaviors or models.

In conclusion, the paper underscores the prevalence and importance of abstractions in software engineering, highlighting their role as language constructs, runtime environment components, interface elements, and architectural features. It also emphasizes the relationship between informal and incomplete system specifications and the creation of test models, underscoring the role of test models as supplements that meticulously specify crucial parts of the system for verification purposes.