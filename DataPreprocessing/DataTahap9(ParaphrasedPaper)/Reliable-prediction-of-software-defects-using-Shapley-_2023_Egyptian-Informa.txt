Anticipating faulty software components can have a significant impact on efficiently allocating testing resources to modules at risk of faults, thereby increasing the business value of software projects. Most existing studies on software defect prediction rely on conventional supervised machine learning algorithms. However, due to imbalanced software datasets, the reported findings may not provide reliable insights into the performance of these models. Moreover, it is crucial to interpret the output of machine learning models used in fault prediction techniques to discern the contribution of each feature to the model output. In this research, a new framework for predicting software defects using eleven machine learning classifiers across twelve different datasets is proposed. To address the issue of data imbalance, a synthetic minority oversampling technique (SMOTE) is utilized. Additionally, four nature-inspired search algorithms, including particle swarm optimization, genetic algorithm, harmony algorithm, and ant colony optimization, are used for feature selection. The Shapley additive explanation model is also employed to highlight the most influential features. The outcomes demonstrate that gradient boosting, stochastic gradient boosting, decision trees, and categorical boosting outperform other tested models, achieving over 90% accuracy and ROC-AUC. Moreover, the ant colony optimization technique is found to outperform the other feature extraction techniques.

The paper is organized as follows: Section 2 presents related studies in the field, Section 3 details the proposed software defects prediction framework, Section 4 outlines the evaluation metrics and hyperparameters tuning process, Section 5 discusses the experimental results and showcases classifiers output explanations using SHAP, and finally Section 6 concludes the paper and suggests future directions.

Numerous research studies have been conducted in the domain of software defect prediction using different datasets and machine learning classifiers. For instance, a study utilizing seven datasets from the National Aeronautics and Space Administration tested six machine learning classifiers and found that neural networks offered the highest accuracy of 93% with 10-fold cross-validation. Another research employed a firefly search-based algorithm for feature selection and demonstrated improved accuracy for the support vector machine classifier. Moreover, deep learning approaches were combined with genetic algorithm and particle swarm optimization for feature selection, achieving an accuracy of 98.47% using 10-fold cross-validation on five NASA datasets. Furthermore, a novel method combining various feature filter methods outperformed individual methods, enhancing the accuracy of decision trees and naive bayes classifiers. Additionally, ensemble learning techniques utilizing boosted SMOTE and voting were found to yield high accuracies for software defect prediction.

The problem of imbalanced target classes in software datasets was addressed using the SMOTE oversampling technique, particularly in the context of NASA datasets. SMOTE addresses the imbalance by generating synthetic data samples to increase minority class samples, thus enhancing the performance of machine learning classifiers.

The paper also introduces and discusses various machine learning classifiers and their features, such as knn, random forest, xgboost, and others, highlighting their applicability and effectiveness for software defect prediction.

Overall, the research findings demonstrate the effectiveness of the proposed classifiers for predicting software defects, particularly in combination with data balancing techniques and meta-heuristic feature selection methods. The findings showcase improved testing accuracy, with boosting methods and SMOTE being particularly effective in enhancing classifier performance. Furthermore, the study highlights the most influential features for predictive models, such as lines of code comments, design density, and Halsted content metrics. The paper also emphasizes the performance of various machine learning classifiers across different datasets, providing valuable insights into their accuracy and effectiveness for software defect prediction.