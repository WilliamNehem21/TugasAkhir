Recognizing and extracting human emotions is crucial for facilitating effective communication between machines and humans. The ability to understand and respond to human emotions is essential for machines to engage in emotion-based communication. In this study, we focus on leveraging insights from human psychology to develop versatile agents capable of recognizing and generating human emotions. Our approach involves a comprehensive analysis of brain waves, voice sounds, and visual images as carriers of emotional information, encompassing phonation, facial expressions, and speech patterns. Through an in-depth examination of statistical data derived from the latest research in brain science and psychology, we aim to establish transition networks that model human psychological states. Our goal is to create a speaker-word model for simulating psychological changes and emotional expressions in a computer, develop an emotion interface, and establish a theoretical framework and methodology for emotion communication. We propose a novel approach to recognizing human emotions based on mental state transition networks and discuss an emotion estimation method based on sentence patterns of emotion occurrence events, presenting new findings from our research.

Contemporary information communication primarily emphasizes verbal information, placing less emphasis on the nonverbal information conveyed through human emotions. Despite advances in human interface technology, such as voice recognition, voice synthesis, and virtual reality, significant challenges persist in accurately processing affective information and recognizing human emotions. This has led to widespread reluctance among individuals to interact with machines in various domains, including business and medical care systems.

Our research is centered on understanding human cognitive traits and aims to develop models for measuring emotions in speakers and simulating emotions in computers, enabling these systems to recognize and generate artificial emotions in a broad range of scenarios. To achieve this, we analyze the information contained in brain waves, voice sounds, visual images, and speech patterns from the perspective of mental characteristics. Additionally, we conduct an in-depth analysis of extensive statistical data based on the latest developments in neurology and psychology, with the aim of deriving a mental state transition network. By constructing and utilizing a speaker-word model, we explore methods for simulating changes in mental states and emotional expressions in a computer. Our research endeavors to develop an emotion interface and establish a theoretical system and methodology for future emotion communication.

Our proposed model consists of two components: the human emotion recognition engine (HMRE) and the machine emotion creation engine (MECE). The HMRE encompasses modules for emotion recognition based on linguistic, phonetic, and expressive information, utilizing a corpus, an ontology, and an individualization database to recognize human emotions. On the other hand, the MECE involves processes for sensibility language, sensibility sound voice, and emotion face, drawing from a mental state transition network used in both engines and a psychological questionnaire experiment.

We characterize appearance information as "emotion energy", which is currently obtained from linguistic, phonetic, and expressive information. We present a method for acquiring emotion energy specifically from phonetic information.

In the realm of image expression analysis, we employ the minimum distance identification method in conjunction with the Facial Action Coding System (FACS). The expressions are ranked based on the number of matched feature values, and the method is used to identify the closest match. Whether the match meets a specified threshold or is close to the next-ranked expressions, the minimum distance identification method is utilized. It operates on the input vector and those in the dictionary, and if the FACS expression aligns with the minimum distance identification expression, it is considered the answer. Otherwise, the answer is the one with the larger distance from the second-best candidate.

Moreover, in our study, Action Units (AUs) in the Facial Action Coding System are mapped to expressions, with each AU designated as operating (ON) or non-operating (OFF). The expression is determined based on the agreement of ON and OFF AUs, with the expression possessing the highest agreement selected as the candidate expression.

Earlier studies on emotions did not prioritize integrating internal and external aspects due to technical difficulties. However, our research introduces a new paradigm for the mental state transition network in humans, as well as methods for human emotion recognition, artificial emotion creation, mental state transition of artificial emotion, and affective presentation. These innovative approaches combine external feature information with physical reactions (mental state transition) and are anticipated to significantly contribute to the international society by advancing future communication business.

We extend our gratitude to Dr. Shingo Kuroiwa, David B. Bracewell, Junko Minato, and all our colleagues involved in this project. The experimental systems described in this paper were developed by Kazuyuki Matsumoto (emotion measurement based on speech pattern), Shunji Mitsuyoshi (emotion estimation based on sound voice), Nobuo Nagao, Takahiro Kuroda, and Jia Ma (facial expression recognition), and Hua Xiang and Peilin Jiang (psychological experiment).