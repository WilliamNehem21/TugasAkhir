A growing number of methods have been proposed to address the task of completing knowledge graphs. This study specifically focuses on knowledge graph embedding (KGE) techniques, where a KGE model learns a low-dimensional representation of each entity and relation in the graph and combines these embeddings to produce a scalar value representing a measure of plausibility.

In recent years, there has been a surge of interest in machine learning with graph structured data, including homogeneous graph embeddings, graph-specific neural models, and knowledge graph embeddings. While many new model architectures have been introduced, there has been limited exploration of how these models are affected by choices made in the machine learning pipeline, such as hyperparameter values, model initializations, and dataset splits. Existing work has shown that simpler baseline approaches with well-tuned hyperparameters can outperform more complex models in various graph-related tasks.

A study comparing seven knowledge graph embedding techniques on non-biomedical benchmark datasets revealed that earlier and simpler models can be competitive when trained using modern techniques. However, this study did not investigate the impact of model initialization or dataset splits on performance. Another study compared 19 knowledge graph embedding approaches across eight benchmark datasets and found that suitably tuned simple models can outperform complex ones. However, this study did not specifically assess performance on drug discovery datasets.

In the biomedical domain, various homogeneous graph embedding techniques have been evaluated for tasks such as drug-drug and protein-protein interactions, with a focus on understanding how different hyperparameters affect predictive performance. Additionally, the use of knowledge graphs in drug discovery has gained traction for tasks such as drug target identification, with entities representing genes, diseases, or drugs and relations capturing interactions.

Several public knowledge graphs, such as hetionet, drkg, openbiolink, and biokg, are available for use in drug discovery. These graphs have been explored with the aim of predicting missing links between entities, such as genes and diseases, and various knowledge graph embedding models have been utilized in this context.

Challenges exist in evaluating knowledge graph embedding models, particularly when applied to biomedical knowledge graphs, which often exhibit different topological structures compared to benchmark datasets. Additionally, the choice of evaluation protocol can significantly impact comparative performance, highlighting the importance of reproducibility in evaluation procedures.

The study utilized popular knowledge graph embedding models and evaluated their performance using metrics such as mean reciprocal rank (MRR) and hits@k. The experiments were conducted using the PyKEEN framework and the Optuna library for hyperparameter optimization, with a consistent software environment across all experiments.

The findings of the study indicate the competitive performance of simpler models and the significance of model training setup and hyperparameter values as crucial factors in model performance. Notably, the study revealed the importance of optimizing hyperparameters for specific relation types and highlighted the potential impact of trivial examples on model performance, particularly in the context of drug discovery.