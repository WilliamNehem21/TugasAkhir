Texture is a characteristic of a specific area within an image. Texture analysis plays a crucial role in pattern recognition and computer vision, but remains a significant challenge. Haralick's work in the 1970s, particularly his development of the co-occurrence matrix, represents a seminal contribution. Subsequently, random Markov fields emerged as a contemporary approach used primarily for segmentation. Laws introduced a method to compute image energy using filters in the late 1970s. During the 1980s, fractal geometry emerged as a powerful theory for obtaining texture features. In addition, local binary patterns (LBP) and other feature extraction methods have been proposed, aimed at capturing information beyond pixel intensities through specific operations. 

The coefficient weights of a bilateral filter are determined based on pixel intensity variation and spatial proximity. Each pixel is replaced by a weighted average of intensities within a specific window. The weighting function assigns higher weights to pixels that are both close to the central pixel and similar to it. 

Following the normalization of texture features using the max-min normalization method due to the wide range of numerical values, three key statistics—absolute mean (absm), mean square (ms), and entropy—are evaluated for each normalized feature vector as textural parameters.

The Salzburg Texture Image Database (Stex) comprises 476 color texture images captured in Salzburg, Austria, and is used in texture retrieval experiments. The majority of images in the Stex dataset are stationary and exhibit greater chromatic richness than Vistex.

In conducting experiments with Stex, it was observed that classification accuracies increased for the absolute mean filter and standard deviation filter. The proposed model achieved better classification accuracy for the mean filter only, with the best accuracy achieved at 95.33% for a specific combination of parameters.