The support vector machine (SVM), developed by Cortes and Vapnik in 1995, is a widely utilized classification algorithm that aims to identify a hyperplane with maximum margin to effectively separate different data classes. This hyperplane represents a specific type of linear model where a database containing two classes of data can be distinctly segregated into two groups, with each group representing a separate class. The samples closest to the maximum-margin hyperplane are referred to as support vectors. In cases where linear decision boundaries are not feasible, the original dataset (x) can be transformed into a new space (f(x)) using the kernel trick. In this new space, a linear decision boundary can be found to classify the samples into their respective classes.

Feature selection, also known as variable elimination, aids in comprehending data, mitigating the impact of high dimensionality, reducing computational time, and enhancing classifier performance (Chandrashekar and Sahin, 2014). Correlation-based feature selection (CFS) is employed to identify a subset of the most important variables, while random forest importance (RFI) is utilized to order the selected variables. CFS selects a feature subset containing features that are highly correlated with the class but uncorrelated with each other (Hall, 1999), while RFI utilizes the random forest classifier to gauge the importance of input variables (Breiman, 2001). This technique functions as a filter feature selector by employing variable ranking as the primary criterion for variable selection.

The performance of the SVM classification was assessed through 10-fold cross validation. This method involves dividing the dataset (in this instance, 83 samples) into 10 folds and conducting 10 classifications. Each classification entails using nine folds to train the model and the remaining fold to test the model, resulting in a precision rate (accuracy). This process is repeated until all 10 folds have been utilized for testing. The classification model's performance is then determined as the average of the accuracy obtained across the 10 classifications. The results show some overlap between classes, with the selected variables by CFS demonstrating greater discriminative power than all phenolic and volatile compounds when analyzing the distance among the samples.

The study identified a group of phenolic and volatile compounds as the most discriminative, with the #09 classification model utilizing seven phenolic and two volatile compounds leading to a 93.97% accuracy. Conversely, using only one variable resulted in a lower accuracy of 68.49%, with accuracy increasing as more variables were incorporated. However, certain variables were found to contribute minimally to the classification model, resulting in low predictive performance.

In conclusion, the study emphasizes the importance of feature subset selection in effectively classifying commercial categories of South American wines, with specific phenolic and volatile compounds playing a crucial role in classification. Prior research has also highlighted the significance of selected chemicals in categorizing wines from different regions.