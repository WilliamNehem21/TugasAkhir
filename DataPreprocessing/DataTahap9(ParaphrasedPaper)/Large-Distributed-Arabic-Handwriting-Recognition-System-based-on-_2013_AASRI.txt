This paper introduces a distributed system for Arabic handwriting OCR that is robust, efficient, and scalable. The system utilizes a parallel fastdtw algorithm implemented through cloud computing technologies, specifically employing the Hadoop, MapReduce, and Cascading techniques. The experiments were conducted using Amazon EC2 Elastic Map Reduce and Amazon Simple Storage Service (S3) with a large-scale dataset from the IFN/ENIT database.

Through experiments and evaluations of various Arabic handwriting OCR systems, it was found that the Euclidean distance technique for classification is less robust and more fragile compared to the dynamic time warp (DTW) algorithm. However, one major drawback of DTW is its slow response time due to the significant computational requirements. To address this issue, the paper proposes leveraging cloud computing technologies to speed up the OCR system based on the DTW algorithm.

The paper is organized as follows: Section 2 provides an overview of the DTW algorithm, particularly fastdtw, and its application in Arabic character recognition. Section 3 introduces the Hadoop, MapReduce, and Cascading models, while Section 4 details the proposed approach. The experimental results and discussion are presented in Section 5, and the conclusion and future work are described in the final section.

Given the exponential time and space complexity of DTW, it is only practical for small to medium datasets (<3,000), especially when dealing with long time series. The fastdtw algorithm is proposed as a solution for this limitation, leveraging multi-resolution techniques.

MapReduce is presented as a tool for parallelizing problems that process large datasets across distributed architectures, such as clusters or grid computing. Amazon Elastic MapReduce is utilized to efficiently handle large volumes of data by distributing computational work across a cluster of virtual servers in the Amazon cloud, managed using the open-source Hadoop framework.

To evaluate the proposed system, a corpus of 16,000 pages (with 370 characters per page) and a reference database of 345 shapes representing different Arabic alphabets, randomly chosen from the IFN/ENIT dataset, are used. The IFN/ENIT dataset is preprocessed and normalized, with features extracted using wavelet transform.

The implementation details include the use of a system with 2 GB of RAM running Windows XP, with Cygwin for Linux command execution and Java as the programming language. Amazon EC2 instance configurations include 7.5 GB and extra large instance with 15 GB, with S3 used for managing input and output data.