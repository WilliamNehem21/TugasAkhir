Residential demand response programs seek to activate demand flexibility at the household level. Recently, reinforcement learning (RL) has garnered considerable interest for such applications. However, a major challenge with RL algorithms is their data efficiency. New RL algorithms, such as Proximal Policy Optimization (PPO), have attempted to address this challenge by improving data efficiency. Furthermore, combining RL with transfer learning has been proposed as a means to mitigate this challenge. In this study, we aim to enhance the performance of transfer learning by integrating domain knowledge of demand response into the learning process. We evaluate our approach in a demand response use case where peak shaving and self-consumption are incentivized through a capacity tariff. Our adapted version of PPO, combined with transfer learning, demonstrates a 14.51% cost reduction compared to a regular hysteresis controller and a 6.68% cost reduction compared to traditional PPO.

In a field-test involving batteries and heat pumps, a 68% average self-consumption rate was achieved. This means that, on average, 68% of heat pump energy was covered by local PV generation. The capacity tariff demand response application considered in our study differs from previous work in that the goal is not to maximize self-consumption but rather to minimize peak power consumption. The capacity tariff scenario examined here is different from those previously analyzed, such as the one proposed by the Flemish regulator in Belgium (VREG). The demand response use case of peak power reduction is of interest in various regions.

We tested our approach using real household data obtained from a field-test in the Netherlands. Although the use case considers the Flemish tariff design, the method is generally applicable. Additionally, reducing peak power demand is a widespread demand response setting.

The paper is organized into four sections. Section 2 provides a detailed formulation of the capacity tariff design, formulates the Markov decision process (MDP), and outlines the RL algorithm and its modifications. Section 3 presents the experiments and discusses their results. Finally, Section 4 concludes the work and provides directions for future research.

An additional challenge addressed in this study is reducing the need for extensive local PV and demand forecasts. The learned control policy is intended to be applied to different residential buildings and households, thus requiring policy transfer. To facilitate policy transfer, the state-space is designed to be independent of environment parameters, ensuring adaptability across different households and seasons.

The results of the simulations are presented in this section, comparing the performance differences of the main control approaches. Our experiments indicate that data generated or collected for other purposes can be leveraged to improve RL performance, potentially aiding the adoption of RL-based control. Furthermore, incorporating expert knowledge into the learning process has shown promise in mitigating drawbacks associated with model-free learning.

Future work will focus on incorporating local control to reduce the maximum main power within an aggregator framework, providing an opportunity for Flemish residential consumers to adopt smart control approaches for electric water heaters and other time-of-use controlled loads (TCLs).