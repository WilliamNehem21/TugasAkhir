The transformer model demonstrates that both forms of simulation are valid, with upward simulation being complete on its own. This is due to the ability of downward simulation to be expressed by a Galois connection, which serves as the fundamental technique in this context. The proof that the two simulations produce all data refinements in the relational setting is further strengthened in the transformer setting, establishing that a composition of upward simulations also achieves this outcome.

In this context, the distributional model of He et al. corresponds to the relational model, while the expectation-transformer model of Morgan et al. aligns with the transformer model. A Galois connection exists between these models.

Section 3 explores data refinement for probabilistic programs. The establishment of structure on the spaces of simulations is followed by the observation that neither type of simulation alone achieves completeness for data refinement in the distributional model. Progress in this context is hindered by the weakened laws of probabilistic programming, and the section concludes with a discussion.

The pair (i, f) possesses additional properties. For example, hi and hf serve as upper bounds for upward simulation. For simplicity, [hi, hf] denotes the set of functions satisfying refinement(2), without implying that it is totally ordered.

Significant work remains in formulating a comprehensive technique for data refinement in the probabilistic setting. The role of the power-set construction in the probabilistic case requires further examination. Theorem 3 reveals that, unlike in the standard case, it is inadequate to consider deterministic datatypes alone. Furthermore, the result is highly reliant on the semantic model of computations and the manner in which the semantics of a module is formulated. The former aspect is inherited from the standard case, whereas the latter is a novel consideration.