Bayesian networks are graphical models used to efficiently organize probabilistic information. These models are particularly valuable for probabilistic reasoning, as they can be utilized for inferring updated probabilities based on specific evidence. This can be especially beneficial in fields such as medicine, where symptoms and measurements can serve as evidence to assist doctors in making informed decisions.

The concept of expectation-maximization (EM) involves the e-part, where the focus is on inference, and the m-part, where the emphasis shifts towards learning. In EM, the e-and m-parts are alternated, although in this particular discussion, the attention is solely on the e-part, assuming the channel e is given and remains fixed.

In a subsequent example, the claim also pertains to an instance of expectation-maximization. However, the approach taken is fundamentally distinct from the traditional use of expectation-maximization. This paper highlights the disparity as being attributable to the contrast between m-learning and c-learning along a channel.

Upon analysis of these examples, it becomes apparent that they carry out disparate computations in their explanation of expectation-maximization, utilizing m-learning and c-learning along a channel, respectively. The fact that they employ the same expectation-maximization terminology despite performing different actions remains unclear.

A notable observation in this context is that c-learning along a channel is additively compositional, which enables it to handle additional data as it becomes available by conducting another c-learning step. This represents a significant advantage of c-learning over m-learning.

The extension of the channel-based analysis from the e-part of the EM algorithm to the entirety of EM involves not only learning the state, but also learning the channel (as the m-part). The EM algorithm entails iterating these e-and m-parts until a certain level of stability is achieved, which will be further described in an extensive version of this paper.