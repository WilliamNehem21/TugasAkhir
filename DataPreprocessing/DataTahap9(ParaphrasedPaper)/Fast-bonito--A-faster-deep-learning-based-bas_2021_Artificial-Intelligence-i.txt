The neural network structure of bonito is based on quartznet, which was originally designed for speech recognition. However, we believed that this architecture might not be optimal for translating raw electronic signals into bases. Our investigation into the bonito neural network architecture revealed potential bottlenecks that could be restricting its performance. The bonito backbone comprises multiple tcsconv-bn-relu modules, each of which includes time-channel separable convolutions (tcsconv), batch normalization (bn), and a rectified linear unit (relu) activation function. While this architecture significantly reduces the model's parameter count, its inference engine library for the hardware is poorly optimized, leading to some degree of slowdown.

The controller module in the neural architecture search (NAS) approach is defined by the search space. A sample-eval-update loop is employed to train the controller, where at each step, a batch of models is sampled and trained for several epochs in the trainer module. The models' inference latency and accuracy are then measured, and a multiobjective reward based on accuracy and latency is computed. This reward is used as input for the controller, and its parameters are updated to maximize the expected reward.

The study utilized bonito version 0.2.3, and a variant known as fast-bonito was also developed using the same training and validation datasets. Bonito is an active project with ongoing feature releases, and we plan to continually integrate the new bonito features into fast-bonito in the future.