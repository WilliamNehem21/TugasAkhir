The selection of initial weights plays a crucial role in training sigmoidal feedforward artificial neural networks. Typically, weights are initialized to small random values within the same interval. This paper suggests a new approach to weight initialization, where the weights from the input layer to the hidden layer are randomly initialized in such a way that weights for different hidden nodes belong to distinct intervals. The study employs the resilient backpropagation algorithm for training and demonstrates the effectiveness of the proposed weight initialization method across six function approximation tasks. Results indicate that networks initialized using this method reach lower error minima during training, exhibit improved generalization, and converge faster compared to networks using the usual random weight initialization method.

The resilient backpropagation algorithm is utilized to train the network using 200 input data sets generated through uniform random sampling of the function's input domain. Corresponding output values are calculated to create the training data set. To test the generalization capability of the trained networks, a similar set of 1000 data values is generated and referred to as the test set.

Furthermore, the generalization experiments reveal that networks initialized using the proposed method demonstrate superior generalization behavior. Specifically, the error on data not used for training is lower for networks initialized by this proposed method. The results of the generalization experiment indicate that, across the function approximation tasks, the proposed method leads to an average error reduction of approximately two-fold compared to the random weight initialization method.

In summary, the paper introduces a novel approach for distributing input-to-hidden weights and hidden thresholds in sigmoidal feedforward artificial neural networks, demonstrating its effectiveness across six function approximation tasks. Networks initialized using this method achieve deeper minima during training, exhibit improved generalization, and converge faster than those initialized using the usual random weight initialization method.