The method for evaluating fairness described in section 2 can be applied to simulate any trust management system, provided that the agents' utilities are defined. To demonstrate the practical use of this method in assessing fairness, two case studies are presented in the subsequent sections. The first case study (section 3) focuses on evaluating a new reputation algorithm, while the second case study (section 4) examines the resilience of a simple reputation algorithm to discrimination. The paper concludes with a discussion on the interplay between fairness and trust.

Fairness needs to be clearly defined in order to be considered in the evaluation of trust management systems. This section will explore and define the concept of fairness in an abstract manner, and the following two sections will illustrate how fairness can be practically integrated into the evaluation of reputation systems and algorithms.

When a user chooses to post negative feedback despite facing negative incentives, it suggests that the user is highly dissatisfied with the contractor. In such cases, the negative feedback should perhaps carry more weight in influencing the reputation than an ordinary positive feedback. This approach can be referred to as the enhancement of negative feedbacks.

During the design of the simulator, a decision had to be made regarding the creation of a sufficiently realistic, yet not overly complex, model of the auction system, user behavior, and the reputation system. The simulation mostly stays true to the actual reputation system, with the only simplification being the consideration of positive and negative feedback. User behavior is also quite realistic, as users take reputation into account when choosing a business partner and decide whether to report feedback depending on its type. Users are capable of providing dishonest feedback and transactions, and they can employ transaction strategies based on their individual interaction history and the reputation value of the other participant.

The simulator allows for the computation of reputations using all available feedback and any implemented algorithm. The simulation results include the reputations of individual agents and the total payoffs from all transactions. The payoffs are influenced by the operation of the reputation system; for instance, if agents give very few feedbacks, reputations will be random, and the payoffs of trustworthy agents will decrease. The simulator enables the assessment of the effectiveness of implemented reputation algorithms.

To validate the concept of implicit and stressed negative feedbacks, a reputation algorithm was implemented using the described approach. Two modes of user feedback behavior were simulated: one in which all agents always provided truthful feedback (perfect feedback), and the other in which agents followed a specific rule for posting feedback (poor feedback). The parameters of poor feedback were derived from an analysis of data obtained from a Polish internet auction house, consisting of over 650,000 completed auctions by a sample group of 10,000 buyers over six months. While real user behavior is more complex, this approximation suffices for evaluating the potential use of implicit feedback in a simple reputation algorithm.

All experiments were carried out using the Monte Carlo method. The paper presents the average results from 10 simulation runs, along with 95% confidence intervals. The outcomes of each simulation included the average payoffs and the Gini coefficients of honest and dishonest agents.

In the first case study, an open market was considered, where a group of agents traded goods. The majority of agents belonged to the "old agents" group, while newcomers formed the "new agents" group. Old agents typically traded fairly with other old agents but often behaved dishonestly towards new agents, who initially tended to act fairly. New agents could leave the market if their losses became too high, or after a while, they might join the group of old agents.

The two case studies demonstrated that fairness can be integrated into the evaluation of trust management systems in a controlled environment. Considering fairness leads to different designs for reputation algorithms, and fairness serves as a valuable criterion in the assessment of reputation systems. However, it is based on the premise that trust management systems should, whenever possible, be designed to treat their users fairly. The paper raises the question of whether fairness should be a goal of trust management systems and, if so, how this goal can be practically achieved. If trading in an internet auction house were only to affect reputation based on fair behavior, the trust management system would be perfectly fair, and the distribution of agent utilities should be influenced only by legitimate factors. The challenge lies in designing such trust management systems.

Fair behavior can be an emergent property that depends on the fairness of individual agents. The trust management system should provide incentives for fair behavior, even in the absence of centralized control and global information. For instance, a peer-to-peer file sharing system employing trust management can also aim for fair treatment of users, despite the impossibility of maintaining global information and centralized control. Global information is only employed in the laboratory for the evaluation of such a system, whereas the system itself functions based on local information only.

It has been noted that trust has another dimension tied to norms, values, or principles of human agents. If an agent perceives that another agent has high principles and acts in accordance with social norms, she is more likely to trust him. This observation highlights that trust can be influenced by the fair behavior of agents.