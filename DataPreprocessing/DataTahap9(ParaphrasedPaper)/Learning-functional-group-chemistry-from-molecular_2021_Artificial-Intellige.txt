Substructures representing commonly observed functional groups (FGs) were extracted from compounds using the FG identification algorithm developed by Ertl. This method identifies heteroatoms and certain types of connected carbon atoms within a molecule, and then combines subsets of these marked atoms into FGs. The FGs are extracted along with atom environment information and assigned to specific layers in the neural network model. Both max-pooling and dropout layers were incorporated into the model to avoid overfitting. Additionally, for transfer learning, the Inception-v3 (IV-3) architecture was slightly modified by replacing the last fully connected layer with three fully connected layers of different output dimensions. Sigmoid and softmax activation functions were used for FG multi-label classification and atom-centered (AC) prediction, respectively. 

Molecular Matched Pairs (MMPs) can be represented in a single graph using the Condensed Graph of Reaction (CGR) approach, which combines reactants and products graphs based on the superposition of invariant parts. Each node in the resulting CGR represents an atom, and each edge represents a bond. A Python script was used to generate MMP CGRs, which were then converted into pseudo-molecules using the RDKit API. The resulting pseudo-molecules connect the shared core of an MMP with the exchanged substituent fragments via a single bond, and the smaller fragment is connected with a hypothetical zero-order bond. 

The Inception-FG model successfully recognized specific FGs that distinguish between compound-forming atom-centered (AC) and non-AC MMPs, while the I-IN model detected more general chemical features covering both the core structure and FGs. These findings provide a rationale for successful transfer learning by the I-FG model due to its ability to specifically recognize FGs. Additionally, the paper "TensorFlow: A System for Large-Scale Machine Learning" was referenced for its insights on machine learning systems.