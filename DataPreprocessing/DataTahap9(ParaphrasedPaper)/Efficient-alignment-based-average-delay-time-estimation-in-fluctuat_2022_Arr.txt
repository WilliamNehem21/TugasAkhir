We introduce an algorithm for estimating the average delay time between two time-varying delay time series based on alignment. While the minimum cost alignments may be numerous, our algorithm is able to consider all of them and, following the alignment process, operates in time linear to the number of nodes in the minimum cost alignment graph, which is at most the length squared. Through numerical experiments, we demonstrate the efficiency of our algorithm as compared to the naive enumeration algorithm that uses recursive calls to traverse the graph. This type of time delay estimation has been extensively studied in fields such as sonar and radar systems, seismology, and geophysics, where the cross-correlation method is typically employed assuming constant delay. More recent work has focused on using more realistic models and spatial prediction techniques.

To obtain the mean time delay by the minimum-cost alignments, one approach is to enumerate all the minimum-cost alignments, calculate their delay times at each aligned position, and then average them. However, this strategy can be inefficient in the worst case due to the exponential number of minimum-cost alignments.

In this paper, we describe a method for calculating the warping-based cost and analyze its time and space complexities in section 3. We demonstrate the efficiency of our calculation method through numerical experiments in section 4. The paper concludes with a summary and discussion of future directions in section 5. Additionally, the calculation method for the gap-based cost is explained in the appendix to clarify the slight differences in calculations between the two types of costs.