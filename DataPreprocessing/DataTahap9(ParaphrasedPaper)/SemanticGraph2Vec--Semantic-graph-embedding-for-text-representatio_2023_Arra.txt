The use of graph embedding as a technique for representing graphs in a low-dimensional space while preserving their structure has gained significant attention in recent years. However, most existing graph embedding methods do not consider the semantic relationships between graph vertices during the learning process. In response to this, our paper introduces a new approach called semanticgraph2vec which takes into account the semantic relationships between vertices when learning low-dimensional representations. Our approach extends prior work by incorporating semantic walks, rather than random walks, to capture more meaningful embeddings for textual graphs. We evaluate the performance of semanticgraph2vec on a part-of-speech tagging task and demonstrate its superiority over two state-of-the-art baseline methods in terms of precision and F1 score.

Graphs are a valuable data structure for representing relationships between different objects, and extracting structural information from graphs is a central goal of graph-based algorithms. The aim of graph embedding is to map the graph into a low-dimensional vector space, where the features of vertices can be learned in relation to the graph structure. The representations obtained through graph embedding are used for various machine learning and natural language processing applications.

This paper is structured as follows: Section 3 outlines the main research objectives and contributions, Section 2 provides a brief review of related work on graph embedding, Section 4 describes the proposed semanticgraph2vec model, and Section 5 discusses the experimental results. Finally, the conclusions are summarized in Section 6.

Most existing graph embedding models have focused on preserving the structural properties of graphs, overlooking the semantic relationships between vertices. Our proposed model, semanticgraph2vec, is the first to consider semantic relationships during the learning process, resulting in more informative embeddings that capture rich semantic relationships between different types of vertices in heterogeneous information networks. While it is challenging to compare different methods due to the use of various datasets, most researchers evaluate their approaches using macro-F1 and micro-F1 measures and demonstrate improved results compared to baseline algorithms. There has been increased attention to this research area in recent years, with many models enhancing the baseline algorithm (e.g., deepwalk) in terms of walk generation.

Few studies have considered semantic relationships between graph components, and most semantic graph embedding approaches focus on specific applications or types of graphs. Our research aims to develop a novel semantic graph embedding model that considers semantic relationships between words to enhance various NLP tasks, such as textual entailment and part-of-speech tagging. The main contributions of our research include selecting walks with the lowest rank during the sampling process, calculating the sum of semantic relationship ranks in each walk, ordering semantic walks based on their rank, and considering the most important walks in the vertex learning process. Inspired by the skipgram model, we learn vertex representations using stochastic gradient descent with negative sampling.

In our experiments, we evaluated the proposed semanticgraph2vec model using a custom dataset extracted from Arabic news articles for part-of-speech tagging. We compared its performance with other baseline graph embedding models, considering metrics such as precision, recall, and F-score. The evaluation process involved making correct decisions in line with the ground truth labels in the data collection, and we found that semanticgraph2vec outperformed the baseline models, achieving improved accuracy in part-of-speech tagging tasks.