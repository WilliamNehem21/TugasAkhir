Historically, machine learning (ML) and deep learning (DL) systems have been trained and deployed on hardware platforms that adhere to the von Neumann computing architecture. Although graphic processing units (GPUs) have been utilized in recent years to parallelize and accelerate the performance of these workloads, they still face performance bottlenecks due to the large volume of data being transmitted between physically separate memory and processing units. An innovative approach to address this bottleneck is the use of in-memory computing (IMC), where certain computational tasks are carried out within the memory itself.

Modern computer-aided design (CAD) simulation frameworks have departed from the traditional spice-based simulation and have instead adopted contemporary software engineering practices. Furthermore, these frameworks are capable of accurately modeling non-ideal device characteristics, peripheral circuitry, and modular crossbar tiles, while also being compatible with high-level language APIs. This paper focuses on memristive in-memory computing (IMC) systems for DL system deployment, providing an overview of existing simulation frameworks and related tools used to model large-scale MDLs.

The structure of the paper is as follows: Section 2 presents preliminaries related to modeling and simulating in-memory MDLs. Section 3 provides an overview of existing CAD tools for in-memory MDLs. In Section 4, comparisons are made among modern simulation frameworks for in-memory MDLs, and two MDLs architectures are simulated. Section 5 offers an outlook for MDLs simulation frameworks, and finally, Section 6 concludes the paper.

The paper discusses resistance/conductance values and inputs as WL voltages, along with the architecture of tiled crossbar systems comprising modular crossbar tiles connected using a shared bus. These tiles are linked to additional circuitry to perform batch-normalization, pooling, activation functions, and other computations that cannot be executed in-memory. They also consist of crossbar arrays with peripheral circuitry.

The paper also describes the simulation of the training routine of the VGG-8 network architecture and the inference routine of the GoogLeNet network architecture using the CIFAR-10 dataset. Notably, larger and more complex networks could not be reliably trained using existing simulation frameworks with CUDA support on a single GPU, even with 32GB of VRAM.

Assumptions were made on the number of conductance states and the resolution of the analog-to-digital converters (ADCs), and multiple runs of the inference routine simulations were conducted to report mean and standard deviation values across all runs. For training routine simulations, mean and standard deviation values were reported across all training epochs. All comparison codes used for these simulations are publicly accessible and can be adapted for comparisons using different hardware technologies, network architectures, and hyperparameters.

Furthermore, the paper details the simulation of training and inference routines using MemTorch, DNN_NeuroSim_v2.1, and the IBM Analog Hardware Acceleration Kit, along with simulations using the native PyTorch ML library for comparison. Consistently, the exact same hyperparameters were used for all baseline implementations, and torch.cuda.amp was utilized to quantize all network parameters to 16 bits to improve performance.
