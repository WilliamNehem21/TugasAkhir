To address the limitations of the previously mentioned feature extraction methods, word-embedding models were suggested as a solution. These models are adept at capturing semantic and syntactic information from word representations. The widespread adoption of word-embedding techniques has led to increased interest in neural network research. Many studies have concentrated on the broader problem of sentiment analysis by focusing on sentence-level classification. Word2vec and GloVe are the two most commonly utilized word-embedding models for text transformation. Word2vec is based on two models - continuous bag-of-words (CBOW) and skip-gram - where CBOW predicts a word based on its context, while skip-gram predicts a word based on a target word. On the other hand, the GloVe embedding approach is a global log-bilinear regression model that generates word vectors based on word co-occurrence and matrix factorization.

This section reviews the literature on text classification, which can be divided into two parts. The first part will discuss word embeddings and state-of-the-art transformers, while the second part will focus on sentiment analysis classification models.

Traditional embedding models used for sentiment analysis struggle with out-of-vocabulary (OOV) words and may lose sentiment information. They also have the drawback of considering similar words from different sentences within the same context, despite the fact that words from different sentences may have different contexts. Over the past two years, transformer-based word embedding models have been employed in various text classification tasks. For example, a BERT model trained on a Chinese Wikipedia corpus was utilized to enhance the performance of Chinese stock reviews with a fully connected layer and bigru. Similarly, researchers developed a personality identification technique using BERT embeddings and found that personality recognition using the BERT model significantly improved accuracy. Another study compared various deep models using different embeddings for sentiment analysis of drug reviews and found that applying pre-trained clinical BERT with LSTM produced compromised results. Additionally, a comparative study between Word2vec and BERT on Tunisian sentiment analysis concluded that BERT with CNN achieved the highest accuracy.

The task is accomplished through encoders, which are a neural network architecture derived from the transformer and used to create encoded text representations. The pretrained BERT-mini model consists of four encoder layers, each of which contains two sub-layers: multi-head attention and feed-forward.

In this research, after extracting the intensities (scores of positive, negative, and neutral) of each review, the data is labeled based on predefined conditions. If the negative score of the review is higher than the neutral and positive scores, the review is labeled as negative. If the positive class has the highest score, it is labeled as positive. This method facilitates the generation of fully pre-processed data in numerical form for text reviews.

The proposed scheme is evaluated on various datasets to assess its scalability and efficiency in accurately evaluating techniques on diverse corpora with varying domains and sizes. The experiments were conducted on positive and negative reviews, excluding neutral reviews, using four state-of-the-art datasets from diverse domains. The performance of the model is evaluated using overall scores for recall, precision, F-score, and accuracy, calculated through the confusion matrix. The receiver operating characteristic (ROC) and area under the curve (AUC) are also used to evaluate the model's effectiveness. These performance matrices are widely used in many text classification tasks, including sentiment analysis. The BERT-based CBRNN model is compared with other deep learning models using different embedding schemes based on accuracy and AUC values. For instance, the Word2vec-based LSTM and the proposed CBRNN model showed similar recall rates of 0.98%, while the CBRNN model exhibited the highest precision rate at 0.98%. The experimental results demonstrated a significant improvement in F1-score (0.2%), accuracy (0.3%), and AUC (0.4%) when compared to the proposed model against other commonly used embedding models such as GloVe and Word2vec. Consequently, the proposed CBRNN model is considered to be more efficient than other models. Finally, the BERT-based CBRNN model shows potential for application in industries for performing sentiment analysis of their products.