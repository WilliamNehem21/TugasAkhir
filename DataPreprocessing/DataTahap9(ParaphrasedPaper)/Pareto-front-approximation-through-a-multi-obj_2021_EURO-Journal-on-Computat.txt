To address these limitations, recent research has introduced descent methods that extend traditional scalar optimization techniques to handle both constrained and unconstrained multi-objective problems. This study focuses on one such algorithm, an extension of the scalar augmented Lagrangian method proposed by Birgin and Martinez in 2014, adapted to the multi-objective case by Cocchi and Lapucci in 2020.

In this article, we conduct a rigorous formal analysis of Algorithm 3 from a theoretical standpoint. We demonstrate the well-defined nature of the procedure and establish its asymptotic convergence properties. Prior to proceeding with the analysis, an assumption is introduced.

For the computational experiments, we conducted 10 runs using different seeds for the pseudo-random number generator. Each run was allocated the same time limit as other algorithms (2 minutes). Upon completing the 10 runs, we compared the obtained fronts based on a purity metric and selected the best one among them. The reference front for comparison was derived from combining the fronts based on the seed used for random operations. However, it is important to note that, given the favorable conditions for NSGA-II, the overall comparison should be viewed as biased in favor of this algorithm to some extent. The other methods (FRONT-ALAMO, DMS, MOSQP) are deterministic and therefore executed only once.

Our benchmark includes slightly modified versions of the BNH and LAP problems from Cocchi and Lapucci (2020), as well as a modified form of the OSY problem originally proposed by Osyczka and Kundu in 1995 (details on the modified problem can be found in Appendix A).

In this study, we focus on smooth multi-objective optimization problems subject to convex constraints, specifically aiming to generate high-quality approximations of Pareto fronts for this category of problems. Following a brief review of existing literature, we propose an augmented Lagrangian method tailored for this objective, designed by Cocchi and Lapucci (2020) to produce a single Pareto-stationary solution. At each iteration, the algorithm manages a list of points that are mutually non-dominated and Pareto-stationary with respect to the current multi-objective augmented Lagrangian. Line searches along steepest common and partial descent directions are utilized to explore the objective space, with updates to the penalty parameter and Lagrange multipliers considering constraints violations committed by all points in the current list.

Moreover, comprehensive computational experiments demonstrate that our method outperforms the SQP algorithm based on popular metrics for multi-objective optimization. We also compared our proposed procedure with state-of-the-art derivative-free (DMS) and genetic (NSGA-II) approaches, and our method consistently yielded superior results.