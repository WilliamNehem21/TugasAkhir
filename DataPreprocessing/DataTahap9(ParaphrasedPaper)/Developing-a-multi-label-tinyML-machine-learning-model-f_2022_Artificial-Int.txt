Alipio and colleagues (2017) introduced a Bayesian network (BN) algorithm in the hydroponics field to automate plant growth by regulating light intensity, soil acidity, water temperature, and humidity. Their approach led to a 66.67% increase in production compared to manual control. However, BNs generate probability density functions rather than directly estimating parameter values, necessitating the implementation of an additional output neuron with a sigmoid activation function producing values between 0 and 1. The training process involved 250 epochs with the Adam optimizer and categorical cross-entropy cost function, using batches of 2048 samples. Model performance was tracked by computing the cost function and accuracy at the end of each epoch for the training and validation sets.

The study found that more complex multi-layer perceptrons (MLPs) led to improved model performance, with a general trend of better performance with more neurons and vice versa. This was logical, as a more complex architecture allowed for the fitting of intricate data patterns, while attempting to fit a simple model to a complex data distribution resulted in poor performance due to an insufficient number of trainable parameters. The models within two groups were denoted based on the number of neurons in the hidden layers.

The mean loss converged to 0.16 and the mean accuracy to 0.97 during training and cross-validation, without significant differences between the two sets, indicating optimal learning without underfitting or overfitting. Further evaluation of the model with 7 neurons in the first hidden layer and 8 neurons in the second layer on the test set yielded a satisfactory accuracy.

The work focused on developing a TinyML-oriented solution for machine learning-based autonomous greenhouse microclimate control using multivariate sensed data (5 variables). The study experimented with various hyperparameters while training 90 model instances with a robust five-fold training and cross-validation process. From the evaluation metrics, multiple MLPs performed well for the task, but for TinyML, selecting the model with the fewest parameters for optimized computations was necessary.

While the obtained performance metrics were satisfactory, the study suggests avenues for improvement, such as focusing on modifying model parameters and hyperparameters, or collecting additional and more refined data to enhance performance.