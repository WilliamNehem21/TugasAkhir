Our most effective deep learning model achieved an f-measure of 53.59% by implementing a dynamically updated learning rate on the imbalanced bug dataset, which comprises 8780 (18%) bugged and 38,838 (82%) not bugged java classes. The sole approach that outperformed it was a random forest classifier, which exhibited a marginal improvement of 0.12%, while a combined ensemble model of the two led to an f-measure of 55.27%. Additionally, a separate experiment indicated that these deep learning results could potentially improve further with an increase in data points, suggesting that data quantity is more advantageous for neural networks than for other algorithms.

The paper is structured as follows: Section 2 reviews the relevant literature, while Section 3 provides a detailed description of our methodology. Section 4 outlines our process and corresponding findings, and Section 5 lists the potential threats to the validity of our results. Finally, we summarize our conclusions and outline potential future work in Section 6.

With the increased computational power, deep learning has become applicable to a wide range of problems, including image classification, speech recognition, and natural language processing. Given these successes, it is reasonable to apply deep learning to the task of bug prediction as well.

We selected 50% upsampling as the best-performing option for our SDNNC strategy, as it also produced similarly good results for the other algorithms. Moving forward, we treat it as a fixed dimension to focus on algorithm-specific hyperparameters. However, we acknowledge that while it was beyond the scope of this study, further research is warranted to replicate the experiments with different resampling amounts.