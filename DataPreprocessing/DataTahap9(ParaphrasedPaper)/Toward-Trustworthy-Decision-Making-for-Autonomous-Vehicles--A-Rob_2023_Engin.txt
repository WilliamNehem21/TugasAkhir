In this paper, we present a comprehensive overview of the implementation specifications for the proposed method. Algorithm 2 delineates the rrl-sg approach for reliable autonomous driving decision-making. The initial parameters for the actor, adversary, and critic models were initialized using a random distribution. Regarding interactions with the environment, our agent's design of the state, action, and reward functions was crucial for executing the proposed system.

In this research, we focused on determining the states of the six nearest social vehicles in the lane of the ego vehicle and adjacent lanes as observations for the autonomous driving agent (i.e., the ego vehicle). The state space of the autonomous driving agent comprised 15 dimensions, incorporating the relative distance and velocity of the surrounding social vehicles, as well as the velocity, acceleration, and lane index of the ego vehicle. The lane index represents the location of the ego vehicle.

The experimental environment was devoid of stationary or moving obstacles, which meant that the autonomous driving agent needed to maintain a straight trajectory without interference from an adversary model. We evaluated each policy model as the agent drove from one side to the other over a 150-time step period. In the test scenario with adversarial attacks, the attacks commenced at the 75th time step. The agent could perform five decision-making actions: turning right, turning left, maintaining the current state, accelerating, and decelerating.