In today's computing landscape, applications are increasingly being run on distributed environments that are provisioned using on-demand infrastructures. The use of technologies such as application containers has streamlined the management of complex systems, and within this context, microservices-based architectures present a promising solution for software development and scalability. This paper proposes an approach for evaluating the automatic scalability of microservices architectures deployed in both public and private clouds. The approach utilizes a fluid Petri net model to characterize the platform, and real trace data to consider a realistic scenario. The primary focus is on assessing performance, costs, and energy consumption from the perspectives of both the service provider and the infrastructure provider.

The objective of our modeling process is to assess the performance, costs, and energy consumption associated with provisioning microservices-based architectures from the viewpoints of both the service provider and the infrastructure provider. The evaluation is divided into two key aspects: first, we examine autoscaling strategies, and then we delve into provisioning schemes. Autoscaling strategies are outlined using pseudo-code algorithms, while provisioning schemes are modeled using fluid stochastic Petri nets (FSPN).

Autoscaling strategies are defined by two critical features: the initial allocation and the consolidation policies. The initial allocation policy governs how different microservices are assigned to available virtual machines (VMs) when the system starts operations. The consolidation policy determines how microservices are mapped to available VMs based on changes in workload dynamics, such as fluctuations in demand for different microservice instances.

In the subsequent phase, we consider available cloud resources and their management policies. As costs are influenced by resource usage in terms of time and volume, the number of VMs in use emerges as a significant cost factor. Our study focuses on two main scenarios: private clouds and public clouds.

Regarding public clouds, costs are standardized based on the number of VMs used per unit of time billing, for example, per hour. Thus, cost minimization is tied to optimizing the utilization of the minimal number of VMs within each billing unit while maximizing resource exploitation.

VMs are characterized by a startup time (tup) and a shutdown time (tdown). During these phases, VMs consume energy despite being unable to serve incoming traffic. In this study, deterministic transitions, such as "ready" and "starting," model the startup phase, while "off" and "shutting down" transitions model the shutdown phase.

Another cost-influencing factor is the time-slot based billing policy commonly employed by cloud providers. In this study, a time-billing period of 8 hours is used to accentuate its impact. By holding VMs active until the next billing period, the system can better accommodate workload fluctuations and provide more resources than strictly necessary, potentially improving overall service quality.

The results indicate that purchased VM time increases in a step function pattern, due to the quantization of billing periods. Notably, the full consolidation policy (strategy C) incurs the least cost in terms of running hours, while the non-consolidating policy is the most expensive.