The absence of confusion among markers is a critical aspect of our research focus. Our work concentrates on technologies that provide alternative solutions to this issue by utilizing inertial measurement units (IMUs), particularly in the 9-axis variant, which can monitor acceleration and angular rate. While IMUs can offer high precision in short-term contexts, they are susceptible to long-term drifts due to the absence of an absolute positional reference.

The concept of embodying full-body motion control in a virtual reality (VR) experience is defined as "embodiment." To successfully embody an avatar, it is essential to address limitations such as potential discontinuities caused by self-occlusions and the lack of hand movement, both of which diminish the sense of body ownership. In scenarios where increasing the number of cameras is impractical, and does not fully resolve the problem, we propose an alternative approach to inverse kinematics. Our real-time algorithm, based on machine learning, addresses occlusion issues through a predictive model.

The paper is structured into three main sections. In the first section, we provide an overview of the existing state-of-the-art and compare our method with other approaches. The second section details the dataset features used to train our model. In the third section, we discuss three baseline methods for correcting occlusions and introduce a more sophisticated model based on neural networks to handle occlusions and inverse kinematics with focus on limbs, neglecting the specific case of fingers.

While previous research has focused on assuming the availability of an underlying skeleton model, our work diverges by employing a passive system to label markers for the fingers and face, utilizing Gaussian mixture models (GMMs) for fast occlusion recovery and ghost marker detection. However, this approach does not address the prediction of marker positions during occlusions.

Prior work has attempted to address the self-occlusion problem through methods such as fitting hand posture using sphere meshes and employing convolutional neural networks (CNNs) to localize the hand center and regress 3D joint locations. Our approach is related to machine learning for inverse kinematics and builds upon existing techniques by incorporating a deep learning framework to constrain the output to feasible postures.

In terms of motion capture, various approaches exist such as using CNNs for keypoint estimation and sensor fusion from multiple data sources. Our methodology employs an active motion capture system that tags each marker with a unique ID. The use of an active system allows for absolute tracking of the markers.

To address differences in data streams from various motion capture systems, a sensor fusion algorithm is vital. The algorithm is divided into several steps, including the utilization of a regularization technique to handle marker positions in the presence of noise.

Aiming to provide a more natural interaction with the virtual world, our approach allows for simultaneous actions and complex tasks in virtual reality. The use of both hands in VR facilitates actions such as handling large objects and performing simultaneous tasks, enhancing the user experience.

In conclusion, while existing methods primarily focus on passive motion capture and limb reconstruction, our approach targets active-marker technologies to address occlusions. We highlight the need for a data-driven approach for hand and fingers reconstruction and emphasize the importance of consistent self-contact for supporting avatar body ownership. We also outline future directions for our work, including experimenting with different neural network architectures and pluggable encoders, extending the architecture to support multi-hand interactions, and improving enforcement of consistent self-contact.