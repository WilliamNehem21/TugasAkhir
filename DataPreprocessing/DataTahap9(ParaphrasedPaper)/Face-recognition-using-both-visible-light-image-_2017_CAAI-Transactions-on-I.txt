Biometrics, a key area of pattern recognition, encompasses various techniques, with face recognition being particularly appealing. Yet, implementing face recognition in real-world scenarios remains a formidable challenge due to the non-rigidity of the face and its susceptibility to varying factors like facial expressions, age, angles, and illumination intensity. In recent years, deep learning has gained traction, with the AlexNet, designed by Alex Krizhevsky, demonstrating significant success by securing the top spot in the ILSVRC-2012 competition and outperforming the runner-up by nearly 10 percent.

In engineering applications employing deep learning algorithms, a substantial amount of training data is essential. To emphasize the significance of adequate training data for deep networks, researchers from Baidu utilized mobile phone cameras to amass a collection of face images, using a trained deep learning model to achieve an 85% accuracy with a false alarm rate of 0.0001. Upon incorporating additional training data (consisting of images of Chinese celebrities sourced from websites), the accuracy improved to 92.5% while maintaining the false alarm rate at 0.0001. This example underscores the necessity and usefulness of increased training data for enhancing the performance of deep networks.

Addressing the challenge of illumination change through deep networks requires the availability of extensive training data exhibiting various levels of illumination intensity, which is arduous to obtain in real-world scenarios. In this paper, a deep network system is devised using both visible light and near-infrared images to tackle the illumination change problem, leveraging the advantages of visible light images in capturing detailed facial features while exploiting the reduced sensitivity of near-infrared images to illumination changes. The system involves training a deep network model using public visible light face data resources, referred to as the first model, followed by retraining with near-infrared face images to create the second model. The extracted features from the two types of images are combined using a score fusion strategy, resulting in a model that demonstrates notable performance in practical application scenarios.

The paper also categorizes classical algorithms for face recognition into three main groups: local feature methods, subspace-based methods, and sparse representation methods, each with distinct approaches to addressing specific challenges in face recognition.

The study also introduces the Sunwin face database, which comprises 4000 face images from 100 subjects, captured under various lighting conditions using both visible light and near-infrared cameras. The database is employed to train and evaluate the proposed face recognition system.

Furthermore, the paper presents three fusion strategies – pixel-based fusion, feature-level fusion, and score-level-based fusion – with the score-based fusion strategy emerging as superior for face recognition, as it avoids information loss and consistently achieves better experimental results.

The proposed face recognition system integrates visible light and near-infrared images, processing them through separate models before employing a weighted combination strategy for score fusion. Experimental results demonstrate the superiority of this approach over traditional face recognition algorithms like LBP.

In summary, the paper introduces a CNN-based model that utilizes both visible light and near-infrared images for face recognition, along with an adaptive score fusion strategy, resulting in a robust face feature extraction model that is resilient to illumination variation. Experimental validations across different datasets confirm the enhanced performance of the proposed method.