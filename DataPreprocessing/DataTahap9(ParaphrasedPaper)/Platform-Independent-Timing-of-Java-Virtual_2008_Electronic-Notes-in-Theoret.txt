The rest of this paper is organized as follows: Section 2 covers the background and related work in this area, while Section 3 outlines our experimental design and methodology. In Section 4, we present and analyze our results for platform-independent instruction timing, and in Section 5, we compare these results with those obtained using the rdtsc assembly instruction. Finally, Section 6 concludes the paper and suggests potential future research directions in this field.

We can represent this quantization effect using standard statistical techniques to achieve high-resolution timing in the presence of low-resolution clocks. Lilja discusses the use of Bernoulli trials to achieve high-resolution timing. Beilner presents a model of event timing and provides timing results using his technique for the time taken to pass a message between two processes. Danzig et al. utilize the technique to develop a hardware micro-timer for timing machine code execution on Sun 3 and Sun 4 workstations.

On the other hand, research on bytecode timing is relatively limited. Herder et al. present the results for Java bytecode timing, although the technique used to obtain such results is not documented. Wong et al. propose a technique for measuring bytecode execution times, but the final technique is not platform-independent and relies on native method invocations.

The analysis of machine-level instruction execution timings has also been conducted. Peuto et al.'s work aimed to produce an instruction timing model to measure CPU performance. Architecture manufacturers such as Intel and IBM also detail the execution time in clock ticks of the machine instructions associated with their architectures.

Albert et al. propose a framework for the cost analysis of Java bytecode. This technique involves transforming the iterative bytecode structure to a recursive representation and inferring cost relations from this representation. In addition, Albert et al. apply their framework to Java programs that include operations such as recursion, single loop methods, nested loops, list traversal, as well as dynamic dispatching.

Stark et al. present a decomposition of the JVM into a number of sub-machines, each capable of executing particular subsets of the JVM instruction set. We also adopt this approach by focusing on individual cores of the JVM and their respective programming paradigms, believing that it can lead to a clearer understanding of the interaction of a Java application and the JVM. Specifically, we concentrate on reporting timing results of the 137 JVM instructions that compose the imperative core of the JVM.

The platform independence of our technique is ensured by using standard Java library timing methods. We considered two timing methods: `System.currentTimeMillis` and `System.nanoTime`, and chose the former for its timing precision and accuracy, as `System.nanoTime` cannot guarantee nanosecond accuracy. All Java timing classes containing the JVM bytecode instructions to be timed were engineered using the byte code engineering library (BCEL).

All experiments were conducted on a Rocks Linux cluster containing 100 nodes. Each node contained a 1.13GHz Intel Pentium III dual-core processor with 1GB of RAM and a cache size of 512KB, running the Linux CentOS operating system release 4.4 (kernel version 2.6.9) at run level 3 with no X-server. The Sun Java(TM) 2 Runtime Environment, Standard Edition (build 1.5.0.07) was used.

Identifying groups of instructions, where each instruction within a group exhibits similar execution time, would aid in reducing the dimensionality of any timing model relying on the execution time of each JVM instruction. In performing the cluster analysis, we chose a granularity value as a cut-off point so that when the instructions in a cluster differ by less than the granularity value, we do not subdivide them further. Thus, there is a trade-off: small granularity values provide better accuracy at the cost of a larger number of groupings.

In this paper, we have presented a platform-independent technique for timing Java bytecode instructions. We have investigated the effect of timer overhead and the importance of subtracting this quantity from instruction timings. We have characterized the execution times of all Java imperative instructions and considered the clustering of instructions based on their execution times. Finally, we have compared our technique against instruction timings obtained using the rdtsc assembly instruction.

The contributions of this paper are: first, we have presented a technique that statistically estimates the execution time of Java instructions within a particular confidence level, allowing us to quantify the error associated with each instruction timing. We have characterized instruction execution times and identified a group of imperative instructions, primarily floating-point conversion instructions, that execute considerably slower than all other instructions. We have presented a technique that clusters instruction timings within a predefined granularity. Finally, we have identified a strong positive linear relationship between instruction times acquired using our statistical method and those acquired using the rdtsc assembly instruction. We have modeled this linear relationship and found that platform-independent instruction timing analysis underestimates the execution times of instructions by approximately 23%, but the strength of the linear model still enables accurate calibration of the measurements.

For future work, we intend to quantify the effect of processor pipelines and cache miss rates on instruction timings. We also plan to conduct our experiments on different platforms and investigate the correlation of results acquired from different JVM implementations. Furthermore, we intend to extend our instruction timing model to include instructions from within other cores of the JVM. As part of future work, we also intend to apply our results to existing JVM models, using our instruction execution times as part of a larger model to predict the execution times of Java applications.