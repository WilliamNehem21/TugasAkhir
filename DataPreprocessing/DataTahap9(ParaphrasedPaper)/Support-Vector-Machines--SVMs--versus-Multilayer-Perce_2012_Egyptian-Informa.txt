Current networks, including support vector machines (SVMs) and other kernel-based learning algorithms, outperform artificial neural networks and other models on widely used benchmark problems. This is due to the localized learning approach of SVMs, which allows the model to adapt to the complexity of the data and achieve good performance on new, unseen data. Unlike neural networks, SVMs provide a single solution at the global minimum of the optimized functional, eliminating the issue of multiple solutions associated with local minima. Additionally, SVMs are less reliant on heuristics and offer a more flexible structure.

This study introduces a new kernel function called the Gaussian Radial Basis Polynomial Function (GRPF) to enhance the classification accuracy of SVMs for both linear and non-linear datasets. The goal is to compare SVMs with different kernels against the back-propagation learning algorithm in classification tasks and assess the performance of the proposed algorithm against algorithms utilizing Gaussian and polynomial kernels on non-separable datasets with multiple attributes. The results demonstrate that the proposed kernel yields high classification accuracy across various datasets, particularly those with high dimensions.

The paper is organized as follows: Section 2 describes the SVM classifier, while Section 3 covers the multi-layer perception (MLP) classifier. In Section 4, SVMs are presented using a new kernel function, and Section 5 discusses the optimization of kernel parameters. Section 6 presents the comparison results between SVMs and MLP classifiers, and the conclusion is provided in Section 7.

Today's network architecture, employed for both classification and regression, includes multilayer perceptrons (MLPs), which are feed-forward neural networks consisting of multiple layers of nodes with unidirectional connections. These networks are often trained using backpropagation. The learning process of an MLP network involves processing input vectors to produce output signals based on adapted weights, with corrective adjustments made to achieve the desired response.

The proposed kernel function offers generality and advantages over existing kernels such as Polynomial Radial Basis Functions (PRBF) by varying the values of parameters. The study evaluates SVMs constructed with the proposed kernels and compares their accuracy with MLP classifiers across datasets with varying attributes. The GRPF kernel achieves the highest accuracy, particularly for datasets with numerous attributes.

In MLP classifiers, the complexity is controlled by limiting the number of hidden units, whereas SVM complexity is independent of the dataset's dimensions. SVMs minimize structural risk, while MLP classifiers minimize empirical risk. As a result, SVMs are effective in obtaining the optimum separating surface and achieving high performance on unseen data points.

The primary distinction lies in the complexity of the networks. MLP networks employ a small number of hidden neurons for global approximation, while SVMs, based on local approximation, utilize a larger number of hidden units. The SVM approach formulates its learning problem to reduce the number of operations in learning mode, making it notably faster for large datasets.