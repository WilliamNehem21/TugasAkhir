The human auditory system can determine the spatial location of objects using spherical coordinates, including direction (azimuth, elevation), and distance. Past research primarily focused on azimuth estimation, but more recent studies have shown increased interest in elevation. However, distance perception has typically been addressed using microphone arrays, with less consideration for binaural audition. As azimuth and distance are crucial for position estimation, several studies have investigated their relationship and the influence of cues of direction and distance on each other, finding that combining azimuth and distance estimation maximizes localization accuracy. Yet, most studies have only focused on one of these factors, either as given information that improves accuracy or to study their influence. Although distance represents depth information and destination for mobile robots, most 3D systems have ignored distance, with only a few researches correlating azimuth and distance for position estimation based on microphone arrays for tracking mobile objects. Therefore, this paper proposes a joint estimation of azimuth and distance for position estimation based on statistical features of binaural signals.

The research then proceeds to describe the model approach, features extraction and selection, classification approach, simulation, and the database details, before presenting experiment results and evaluations. The paper concludes by asserting that the presented system robustly estimates the position of a sound source in both distance and direction perception in reverberant environments based on a set of statistical properties of binaural cues, and provides high accuracy performance results across various scenarios and positions.

In the context of azimuth estimation, the primary cues are interaural time difference (ITD) and interaural level difference (ILD), which have been extensively studied in the development of localization systems. Recent research has focused on estimating azimuth based on joint ITD and ILD features, including the development of a Gaussian mixture model and the use of a neural network approach to estimate azimuth in a humanoid robotic context. The paper also outlines the use of the expectation-maximization algorithm with a maximum of 300 iterations to estimate parameters, and the application of variance normalization to account for feature scale variations.

The proposed system provides a combined feature vector as input to Gaussian mixture models for classification, enabling the estimation of the corresponding position of the sound source. The system's robust and high accuracy performance in different scenarios, as well as its ability to adjust to untrained positions, are also highlighted.