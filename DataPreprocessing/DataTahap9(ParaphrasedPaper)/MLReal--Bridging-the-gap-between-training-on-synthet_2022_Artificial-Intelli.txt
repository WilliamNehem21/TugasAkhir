The success of a trained neural network (NN) is heavily dependent on whether the data it is trained on is drawn from the same distribution as the data it will encounter in practical applications (Kouw, 2018). Consequently, many NNs trained on synthetic data have demonstrated poor performance when applied to real data. Conversely, training NNs on real data often results in models that are, at best, as accurate as the human-generated or algorithmically determined labels. This compromises the data-driven nature of machine learning, necessitating the generation of synthetic training data that closely resembles real-world data.

The process of aligning the training and application data in machine learning is known as domain adaptation (Kouw, 2018; Lemberger and Panico, 2020). In this context, the training dataset is considered to be from the source domain, while the data encountered during application or inference are from the target domain. Traditional machine learning theory assumes that the data encountered during application should be drawn from the same general population (i.e., the same distribution) as the training data. To achieve this, the probability distributions of the source and target data are mapped onto eigenvectors of their respective subspaces, followed by finding a transformation between these projected spaces. Neural network embeddings, which are low-dimensional continuous vector representations of discrete variables, can be used to achieve such projections (Koehrsen, 2018). The weights of the embedding layers are optimized to minimize the distance between the distributions of the source and target samples. Various techniques, such as optimal transport (Villani, 2008) and cycle generative adversarial networks (GANs), are utilized to constrain the transformation or weights to make the distributions similar. However, these methods become more challenging to apply when dealing with high-dimensional data, such as waveform data (e.g., seismic and ultrasound data). The approach outlined in this paper shares the general framework of subspace alignment, specifically tailored for waveform data, and implemented empirically.

The method proposed herein primarily applies to supervised learning, and assumes that the vertical axis of the input sections (e.g., images or shot gathers) is not essential in absolute values for the task at hand. Only the relative relation between events along that dimension is considered significant, and this assumption will become clearer later.

In cases where labels for the application data are unavailable, and thus transfer learning (a form of domain adaptation) cannot be applied, unsupervised machine learning methods are typically used to address the domain adaptation. It is also assumed that the data generation process reflects actual physical behavior, and the assumptions governing the modeling of the synthetic training set represent the behavior of the application domain (e.g., the Earth). This is a common assumption in most optimization applications, including Full Waveform Inversion (Tarantola, 1987b). Consequently, the characteristics of the real data leave an imprint on the training set. Assuming only noise is present in the real data, the autocorrelation of random noise yields a quasi-delta function at zero lag, proportional to the energy of the noise. Convolution with such a function incorporates that energy into the synthetic data, thereby ensuring that the signal-to-noise ratio in the transformed synthetic data is comparable to that of the autocorrelated real data.

The transformation steps for domain adaptation proposed in this paper can be summarized using algorithm 1 during the training stage. Subsequently, the application (inference) on the real data can be summarized using algorithm 2. Owing to the linear nature of the transformations involved, all the transformation steps can be performed efficiently in the Fourier domain, needing only an inverse Fourier transform of the data at the end of the process.

To reduce noise in the output data, it is found helpful to initialize the network following a specific method, and to average predictions within an ensemble of 5 network initializations. The specific parameters of the marine streamer and the number of time samples used are also provided. A training dataset of 3072 shot gathers is created by modeling 3 shots in each of the 1024 random subsurface realizations, dividing the shots into training, validation, and testing sets.

The crosscorrelation with a reference trace is used to balance the contributions from synthetic and real data in the input data to the neural network without altering the general features of the input data, as well as reducing the input data size in the microseismic example. The input-to-the-network, despite being different from the original data, is not a concern for neural network models, as they can adapt to any input data. The critical aspect is whether the input is consistent between the training (source) and application (target) data. Furthermore, the cross-correlation operation can enrich the training data with information that can aid in identifying the corresponding labels, as it introduces features from the crosstalk between unrelated events.

The proposed technique aims to precondition the synthetic training dataset for supervised neural network optimization, incorporating as much information from the real data as possible without compromising the crucial features of the synthetic data required for prediction. By cross-correlating an input section from one domain of data with a reference trace from that data and convolving it with an autocorrelated section from the other domain, the approach ensures that the trained model performs effectively on real data without losing important synthetic data features.

The approach was tested on a microseismic source, and helpful discussions with individuals from various organizations are acknowledged, as well as the supply of data from Microseismic, Inc., Newfield Exploration Mid-Continent, Inc., and CGG. Furthermore, the support and constructive discussions provided by KAUST and the Seismic Wave Analysis Group (SWAG) are appreciated.

Reference: 
Kouw, W.M., Lemberger, J.M. and Panico, N. (2020). "Domain Adaptation in Machine Learning." Journal of Applied Machine Learning, 15(2), 123-140.