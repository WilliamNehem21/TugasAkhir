The readability of counter-examples is significantly influenced by the nature of the model and the properties being verified. As the model becomes more abstract, it becomes more challenging to interpret the counter-examples. Abstraction often involves excluding certain data from the model, making it difficult for domain experts to understand the counter-examples when relevant information is missing. Conversely, an abundance of irrelevant data can also hinder the interpretation of counter-examples.

The exclusion of signals in our abstracted model affects the interpretability of counter-examples, as it reduces the number of state variables and obscures the aspects of the signals. While this abstraction improves efficiency, it creates challenges for users in understanding the aspects of the signals. Consequently, despite the model's accuracy, signaling engineers find it disconcerting.

When verifying railway interlockings, counter-examples can become overwhelming due to the inclusion of variables for every route, track segment, point, and train, even if an error occurs in just one route. This complexity can be daunting, especially in cases where a small verification area contains a substantial number of variables.

To address these challenges, an algorithm has been designed to interpret counter-examples and identify the specific error that led to the safety violation. This algorithm aims to present only the relevant data to the user and provide detailed error descriptions without causing confusion.

In addressing the usability issues associated with counter-examples, two approaches—animation and interpretation—were considered. While animation was deemed unsuitable due to the significant abstraction of the model, an interpreter was developed based on signaling principles to effectively handle counter-examples. This approach aims to enhance the interpretability and usability of counter-examples for end users by providing accurate and relevant information.