After establishing a local ordering among the instructions within a single thread, it is necessary to articulate the behavior of multiple threads that execute concurrently. The only mode of communication between threads is through the use of stores and loads. At a high level, any multithreaded execution must be serializable modulo reordering. The formal definition of serializability in a transactional setting is more intricate than the definition of non-transactional serializability.

It's imperative to note that the described procedure only depicts one of several possible executions of this fragment. It is feasible for l5 to instead observe s2. In such an instance, there would be no defined ordering between s2 and s3, and l6 could observe either s1 or s4.

The procedure outlined above aims to be clear, but it is not a normalizing strategy. A program containing an infinite loop can get stuck in the graph generation and execution phases without resolving a load. More complex strategies exist to address this issue (one starting point is to avoid unfolding or execution past an unresolved load).

The distinguishing factor between speculation and mere reordering is the possibility of speculation going awry. Speculation can be described in the graph-based formalism in two ways: first, through value speculation, which involves guessing values and verifying them later; and second, by resolving instructions early, before all of their dependencies have been satisfied. This approach can result in violations of store atomicity. The paper discusses a specific example called address aliasing speculation, arguing that while it permits new behaviors compared to a non-speculative model, it leads to a simpler and easier-to-understand memory model.

The operational framework presented in section 4.1 restricts load resolution to a transaction that contains a mix of resolved and unresolved loads. Conceptually, permitting load resolution to occur in any thread is straightforward: simply discard a behavior if it is found to violate the rules of store atomicity. However, an actual system can only work with one behavior at a time. When an inconsistency occurs, decisions need to be made about which instructions to roll back.

The literature on memory models examines the balance between elegant, simple specification and efficient implementation. Collier is a standard reference for computer architects and established the tradition of reasoning from examples, which this paper continues. The tutorial by Adve and Gharachorloo provides an accessible introduction to the foundations of memory consistency.

In a transactional setting, data races are still possible if shared data is manipulated outside a transaction, despite the use of locks. The notion of properly synchronized programs will remain relevant, albeit under simplified assumptions. The community has just begun formulating transactional consistency protocols comparable to release consistency.

A better understanding of transactional serialization is needed. Store atomicity captures which instructions must be ordered in any serialization of an execution. In practice, many implementations interleave the instructions of multiple transactions without causing issues. The conditions outlined in section 3.5 provide a starting point, but further refinement is undoubtedly possible.

There should be tools available for verifying memory model violations without the need to compute serializations. Graph-based approaches such as tsotool have demonstrated their effectiveness in this area. Techniques similar to those described in the paper have been suggested for checking transactional memory models. It would also be feasible to adapt the techniques of umm to perform exhaustive model checking in a transactional setting.

The paper hopes that transactional models are simpler to understand, particularly for programmers. It has been conjectured that transactional techniques, particularly batched updates, can scale well even with a relatively strong memory model. However, it remains to be seen how well these claims stand up on very large systems with tens, hundreds, or even thousands of multi-core CPUs.

The work was targeted towards computer architects, and the authors express gratitude for the support and feedback received from various individuals and groups, including Maurice Herlihy, Victor Luchangco, Yossi Lev, the Scalable Synchronization Research Group at Sun Microsystems Laboratories, and the Fortress team. Krste Asanovic played a vital role in targeting the work to computer architects, and the authors also acknowledge the support received from the IBM PERCS project and the DARPA HPCS program.