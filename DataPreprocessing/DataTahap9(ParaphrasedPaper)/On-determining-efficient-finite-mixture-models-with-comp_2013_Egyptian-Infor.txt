This paper introduces a novel algorithm for learning and evaluating finite mixture models (FMMs) for data clustering using a newly proposed criterion. The algorithm, known as the EMCE (Essential and Compact Components) algorithm, aims to identify the most efficient FMM for clustering input data by minimizing the proposed criterion. The selected FMM is characterized by its compact and essential components, which exhibit minimal mutual information and overlapping. The performance of the EMCE algorithm is compared with other existing algorithms, and the results demonstrate its superiority, particularly with small and sparsely distributed data or data generated from overlapping clusters.

Existing criteria for estimating the number of FMM components, such as the Bayesian Information Criterion (BIC) and the Minimum Message Length (MML) criteria, have limitations in handling non-Gaussian cluster shapes, overlapping clusters, and sparsely distributed data. Additionally, criteria based on penalized likelihood compromise the goodness of fitting the FMM to the data with the complexity of the model, making them sensitive to increases in the number of features in the input data set.

The EMCE algorithm addresses these limitations by integrating unsupervised learning and FMM optimization using the EMCE criterion. It employs both random parameter initialization and the CEM (Cross-Entropy Method) algorithm to mitigate the risk of sub-optimal results and boundary effects. The algorithm iteratively learns and evaluates different FMMs, removing unnecessary components based on their mixing weights. Experimental results using various data sets demonstrate the effectiveness of the EMCE algorithm in producing superior clustering results, particularly with small and sparse data.

In summary, the EMCE algorithm presents a new approach to model selection for data clustering, overcoming the limitations of existing algorithms and criteria. It offers a single framework for FMM estimation and selection, and its performance surpasses that of other algorithms, especially with small and sparse data.