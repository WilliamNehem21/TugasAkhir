The majority of current methods for handwriting recognition follow a two-stage approach, involving text segmentation and text recognition. The initial stage employs a hidden Markov model (HMM) to segment the text [23,24], while the second stage focuses on text recognition. Despite the individual effectiveness of each stage, they suffer from inherent limitations. For example, manual construction of ground truth segmentation and transcription labels at the line level is both time-consuming and expensive. Additionally, errors in segmentation can lead to recognition inaccuracies.

The research conducted experiments on publicly available datasets of handwritten paragraphs, including Rimes, READ 2016, and IAM. The study achieved competitive results when compared to state-of-the-art models that use ground-truth paragraph segmentation. The paper is organized as follows: Section 2 discusses related methods and modeling choices; Section 3 describes the proposed modification and system details; Section 4 presents the experiment results; and Section 6 offers a brief discussion comparing the proposed system with other methods, suggesting potential improvements, and discussing the challenges of applying it to full documents.

Bluche and Messina, as well as Puigcerver, have adopted a more parallel architecture by replacing the MDLSTM encoder with a CNN while still relying on CTC. They present the segmentation and recognition tasks as separate networks, drawing inspiration from object recognition methods. By focusing on modular techniques, the authors propose a pipeline, providing detailed descriptions. The proposed approach distinguishes handwritten text areas, applies word-level segmentation based on object recognition, and logically arranges the words. In contrast to methods focusing on line-level recognition, the authors offer an end-to-end paradigm based on word-level recognition.

Unlike most existing handwritten text recognition models, which require prior image segmentation to extract text components such as characters, words, and lines, these approaches have several disadvantages. Image-text segmentation methods rely on heuristics or feature engineering that are sensitive to significant data variations. Many segmentation-based methods fail to correctly handle slanted text, curvature, and bold features, resorting to erroneous properties and heuristics. What's more, cleanly separating text units in real-world handwriting poses a significant challenge, as lines may be warped or merged with non-textual symbols and other visual elements. The literature provides further information on these limitations [25,26]. An alternative technique for deriving a general transcription from externally transcribed text segments may introduce errors and reduce performance.

Text portions in handwritten documents typically require stitching, as they are often concatenated with spaces or new lines, potentially leading to loss of formatting and indentation. This limitation indicates that smaller text sizes may overlap and intersperse with more significant parts of information that have not been transcribed. Fortunately, the proposed model circumvents these issues by implicitly processing and learning from data end-to-end. Thus, the proposed model is easily implementable with optimal hyperparameters, and it can concurrently perform segmentation and recognition tasks using advanced training and searching methods without human intervention.

The proposed model incorporates an encoder and decoder modules coupled with a multi-head self-attention mechanism and positional encoding. This enables the model to focus on specific feature maps at the current time step. By utilizing joint attention and a segmentation-free model, the neural network computes split attention weights on the visual representation, allowing for implicit line segmentation. Images are processed using a CNN for feature extraction, and the transformer encodes and decodes these features using multi-headed self-attention layers. A joint attention module decodes the paragraph image at the character level. The proposed method uses the ResNeSt50 as the convolutional network to extract the 2D feature representation. Prior to feeding data through the transformer encoder, 2D positional encodings are added to the feature vector. Four encoders and six decoder layers are applied, each containing self-attention and feed-forward neural network (FFNN) components.

Positional encodings are generated and concatenated with the feature maps to capture spatial information. The concatenated feature maps and positional encodings are input to the transformer, which comprises encoder and decoder components utilizing multi-head self-attention mechanisms to capture dependencies between features and positional encodings. The target text is embedded and processed through a 1D positional encoding layer. The decoder applies multi-head self-attention to the feature maps and positional encodings, with the final output being the predicted text.

In contrast to SKNet, the split is achieved by conducting convolution with various kernel sizes to select the optimal kernel size channel-wise for each. In contrast, ResNeSt performs convolution with kernels of the same size as a branch selection. Multiple models are generated and then assembled using divided attention by ResNeSt.

The IAM handwriting dataset is a compilation of handwritten text documents widely used for research in handwriting recognition. It contains over 1500 pages of handwritten text, comprising various handwriting styles and languages, with manually transcribed and annotated documents, making it a valuable resource for training and evaluating handwriting recognition algorithms. In this study, the IAM dataset, which includes grayscale images of English handwriting at a resolution of 300 dpi and provides segmentation at the page, paragraph, line, and word levels along with corresponding transcriptions, is utilized.

The READ2016 handwriting dataset was proposed for the ICFHR 2016 competition on handwritten text recognition. It consists of a subset of the Ratsprotokolle collection used in the READ project and includes color images of early modern German handwriting. The dataset offers segmentation at the page, paragraph, and line levels. In this study, the preprocessing steps of [original paper] are followed, and for validation, the last 100 training images are utilized, in line with common practices. The dataset provides segmentation and transcription at the paragraph, line, and word levels, with paragraph segmentation levels being used in this study.

The proposed method utilizes a ResNeSt50 convolutional neural network and a transformer network for supervised learning, aiming to learn the underlying structure of input paragraph images and their corresponding transcriptions. During the training phase, the model is trained on a training dataset, and during testing, the trained model weights are used to predict transcriptions for a testing dataset. The model is designed to take paragraph-level images as input and is trained end-to-end on a given dataset, with the model weights saved based on the optimal training period and subsequently used by the paragraph recognition model to extract features. This approach leverages convolutional and transformer-based architectures to capture complex features in input images and corresponding transcriptions, presenting a powerful approach to handwriting paragraph recognition.