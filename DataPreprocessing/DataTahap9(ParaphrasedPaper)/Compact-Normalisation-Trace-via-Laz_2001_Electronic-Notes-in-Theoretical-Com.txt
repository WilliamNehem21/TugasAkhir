The instantiated right-hand side (RHS) is provided by Elan, resulting in linear worst-case complexity of Coq's syntactic pattern matching based on the size of this redex. As the size of terms can be very large, Elan initiates the proof search, and Coq later verifies this proof. To ensure compatibility, Coq and Elan must operate on the same canonical (confluent and terminating) term rewriting system (TRS). In this context, Elan should deliver as concise traces as possible to Coq to minimize the replay time. This time is influenced not only by the number of rewrite steps but also by the positions of contracted redices, as contracting inner redices generates larger proof objects.

Lazy rewriting normalizes a term to its lazy normal forms, where all active subterms are in head normal form (HNF). Some lazy subterms may be reducible, but their reduction may be infinite if the TRS is not terminating. However, all lazy subterms can be recursively normalized until they reach HNF. Furthermore, the authors present a method for simulating lazy rewriting through innermost rewriting with respect to a new TRS obtained by transforming the original TRS, a process known as thunkification.

Thunkification only operates on TRSs where non-variable terms are not placed on the lazy arguments of a function symbol in the left-hand sides of rewrite rules. To address this, the authors transform the original TRS into a minimal TRS, where each left-hand side contains no more than two function symbols. This transformation generates a larger number of new but simpler rules and new function symbols. While the minimal TRS is suitable for the abstract rewriting machine (ARM), it is not optimal for Elan, as Elan's compiler uses an improved version of the many-to-one pattern matching algorithm.

The paper also describes the process of annotating and decorating subterms for lazy rewriting, along with the rules and transformations used in the lazy rewriting process. The authors introduce a new signature and illustrate how lazy subterms are "thunked" using new function symbols, which allows the recovery of the structure of lazy subterms. Moreover, the paper presents a procedure for normalizing lazy argument subterms to ensure correctness.

In conclusion, the paper provides an in-depth exploration of lazy rewriting, thunkification, and the transformation of TRSs, shedding light on their interrelations and proposing a normalisation procedure based on lazy rewriting.

If you need a further detailed or specific paraphrasing, feel free to ask!