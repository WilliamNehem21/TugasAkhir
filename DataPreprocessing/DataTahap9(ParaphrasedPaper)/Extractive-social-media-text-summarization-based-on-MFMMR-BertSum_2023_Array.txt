The rapid evolution of computer technology has resulted in a deluge of textual information, leading to inefficiencies in processing knowledge. To address this issue, various techniques for text summarization have been developed, including statistics, graph sorting, machine learning, and deep learning. However, the inherent complexity of text often hampers the abstract extraction process, and redundant information is not effectively processed. This paper introduces the Multi-Features Maximal Marginal Relevance BERT (MFMMR-BERTSUM) model for extractive summarization, which leverages the pretrained BERT model to address the text summarization task. The model integrates a classification layer for extractive summarization and utilizes the maximal marginal relevance component to eliminate information redundancy and enhance the quality of the summary. The proposed method demonstrates superior performance compared to other sentence-level extractive summarization baseline methods on the CNN/DailyMail dataset, validating its effectiveness. 

In the era of big data, the rapid expansion of social media constantly generates copious amounts of information. Text content, being a dominant medium in social media, is an efficient way to disseminate real-time news and opinions. However, the abundance of descriptive and interpretive content often obscures the core message, hindering the timely acquisition of crucial information. Text summarization, which condenses lengthy text into a concise abstract while preserving its original meaning, is instrumental in efficiently extracting key information from massive text. Automatic text summarization methods can be categorized into extractive summarization and abstractive summarization. Extractive summarization involves extracting keywords from the source document to form a summary but may lack coherence between sentences and contain redundant information. On the other hand, abstractive summarization generates new words to form a summary based on the content of the source document. 

The development of deep learning techniques has catalyzed significant advancements in natural language processing. The BERT model, which is pretrained on vast datasets, exhibits strong generalization capabilities. However, BERT is designed for word-level inputs, while extractive summarization is a sentence-level task, making it impractical to directly fine-tune the pretrained BERT model for automatic text summarization tasks. 

The advent of deep learning technology has brought about substantial progress in the field of extractive text summarization. The BERT model has revolutionized natural language processing by capturing bidirectional context relationships in statements. BERTSUM, a BERT-based text summarization model, was proposed as the first of its kind. Several modifications to the BERT model's embedding have been made to facilitate extractive summarization. Other researchers have also made enhancements to BERT, such as adding a hierarchical graph mask to leverage structural information between different semantic levels and using the k-means algorithm for cluster analysis. 

The BERT model's semantically-rich representation capabilities effectively address challenges such as polysemy and long-distance dependencies in natural language processing. The input representation of the BERT model consists of token embeddings, segment embeddings, and position embeddings. The model's multi-layer bidirectional transformer structure can be fine-tuned on downstream tasks by connecting it to neural networks. 

Experimental results indicate that the MFMMR algorithm yields the best summary performance with a word vector dimension of 100. Moreover, the proposed MFMMR-BERTSUM model demonstrates significant improvements compared to baseline approaches in terms of alignment with reference summaries across different sliding window scales, word order, and sentence structure. The hybrid model SummaRunner-PGN also outperforms individual models, suggesting that hybrid models combining the advantages of extractive and abstractive summarization have great potential for further development. 

This research introduces the MFMMR algorithm, which considers multiple features in the conventional maximal marginal relevance (MMR) sentence scoring process and mitigates the limitations of relying on a single feature for calculating sentence scores. The MFMMR-BERTSUM model, proposed by modifying the input representation of BERT and adding a classification layer with MMR components, addresses the redundancy problem in extractive summarization. Experiments conducted on the CNN/DailyMail dataset demonstrate significant improvements compared to baseline approaches.

Junqing An is an associate professor at the School of Computer Science, China University of Geosciences, with research interests in data mining, natural language processing, high-performance computing, and optimization. Yuewei Wang is a doctoral candidate at China University of Geosciences, researching machine learning, big data analytics, digital earth, and high-performance computing.