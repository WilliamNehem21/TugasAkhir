The potential of spiking neural networks (SNNs) in addressing artificial intelligence (AI) applications is substantial. However, existing benchmarks for SNNs have predominantly focused on evaluating them for brain science, which encompasses distinct neural network architectures and objectives. While a few benchmarks have assessed SNNs for AI, these evaluations have been limited to either a single stage of training and inference or a partial processing stage without information on accuracy. As a result, there is a lack of comprehensive benchmarks that cover both training and inference stages and provide a complete training process up to a specific accuracy level.

Two benchmarks have been proposed to evaluate SNNs for AI. One benchmark, introduced by Ostrau et al., focuses on the inference stage by converting a pre-trained deep neural network (DNN) model to an SNN model and providing accuracy information. However, it does not address the training phase or alternative learning rules. Another benchmark by Kulkarni et al. encompasses both the training and inference stages but employs simpler neural network architectures. The use of the spike-timing-dependent plasticity (STDP) learning rule in the Kulkarni et al. benchmark yielded lower convergence accuracy and greater fluctuations compared to backpropagation and conversion-based learning rules.

It was observed that the performance of SNNs can vary between central processing units (CPUs) and graphics processing units (GPUs), with the CPU performing better than the GPU for small SNN networks. This disparity may arise from the limited size of the SNN networks, leading to short GPU computation times that cannot offset the synchronization overhead between the CPU and GPU. Furthermore, the software framework used for simulating SNNs may not be optimally designed to exploit the full potential of GPUs. In the context of recurrent networks, the training time on a GPU using leaky integrate-and-fire (LIF) neurons is notably longer than that on a CPU. Future work plans to explore larger SNN networks and optimize both the software framework and the mapping of SNN workloads to GPU hardware.

Several benchmarks have been proposed to study computational neuroscience, which uses mathematical models and computer simulations to understand how electrical and chemical signals process and represent information in the brain. To ensure reproducibility and usability, the benchmarks incorporated various measures to address the intrinsic stochastic nature of AI algorithms, such as setting the same random seed for all benchmarks and using a consistent experiment environment through Docker.

SNNBench, a benchmark suite designed for AI applications, focuses on image classification and speech recognition as benchmarking tasks. The suite includes state-of-the-art spiking neural architectures widely accepted and highly cited in SNN research. Furthermore, SNNBench provides workloads with different connection types to leverage mature and efficient connection types developed in deep learning.

The benchmarks were conducted through a range of experiments to demonstrate the effectiveness of SNNBench, encompassing workload characterization, reproducibility, and the impact of learning rules and computing units. The experiments provided insights into the performance of different SNN workloads, highlighting the significance of considering both training and inference phases, as well as the impact of various factors on performance.

Additionally, the experiments showed that while SNN networks achieved comparable accuracy to DNN networks under certain conditions, further parameter tuning and optimization are necessary to improve SNN performance. The benchmarks also revealed that the STDP learning rule, while challenging to use, becomes the sole option for unsupervised learning. Notably, hardware designed for training and inference phases may differ, necessitating comprehensive benchmarks that cover both phases to inform hardware design.

In conclusion, SNNBench offers a valuable tool for evaluating and optimizing SNN algorithms, software, and hardware for AI scenarios. The benchmarks provide insights into the performance of SNNs, shedding light on their potential for addressing AI applications and guiding future developments in this field.