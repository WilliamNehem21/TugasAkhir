The performance of deep learning workloads in large-scale settings is becoming increasingly significant in the design, development, and deployment of next-generation high-performance computing systems. As deep learning applications heavily depend on deep learning frameworks and underlying compute stacks (CPU/GPU), it is crucial to develop a comprehensive understanding of the compute kernels, models, and frameworks within popular deep learning stacks. This understanding is essential for evaluating their impact on science-driven, mission-critical applications. At Oak Ridge Leadership Computing Facility (OLCF), a set of micro and macro deep learning benchmarks, established through collaboration among Oak Ridge, Argonne, and Livermore (CORAL), is employed to assess the AI readiness of next-generation supercomputers. This paper presents early observations and performance benchmarks comparing the NVIDIA V100-based Summit system with its CUDA stack and an AMD MI100-based testbed system with its ROCm stack. The paper takes a layered perspective on deep learning benchmarking and identifies opportunities for future optimizations in the considered technologies.

The share of deep learning scientific applications in high-performance computing (HPC) allocation portfolios has steadily increased. Consequently, the performance of the deep learning stack has become a critical consideration in the procurement of next-generation HPC infrastructures. The CORAL collaboration has established a set of benchmarks to evaluate the hardware and software competitiveness for the acquisition of the DOE leadership class platforms: Frontier at Oak Ridge, Aurora at Argonne, and El Capitan at Livermore. The CORAL-2 benchmarks suite includes deep learning workloads for the first time, consisting of microbenchmarks such as DeepBench and deep learning suites, including both ResNet50 on ImageNet and application benchmarks such as CANDLE. In comparison to the industry-led benchmarking effort, MLCommons HPC (also referred to as MLPerf HPC), the CORAL-2 benchmarks focus more on throughput and fundamental building blocks.

Despite the increasing complexities of deep neural net (DNN) models, compute operations essentially boil down to three types of mathematical kernels: general matrix multiplication (GEMM), convolution, and recurrent operations. Distributed training at scale has become common practice, and communication operations must also be considered. The overall performance of deep learning applications is mostly determined by the hardware/software stack for these mathematical operations and communication, in addition to the importance of input/output (I/O), which was not considered due to the absence of an I/O bottleneck in the benchmarks.

The paper is organized as follows: Section 2 provides background on traditional simulation-based HPC workloads versus emerging deep learning workloads and an overview of the deep learning benchmarks proposed for the CORAL systems. Section 3 details the layered approach, methodology, and metrics used for performance evaluation and comparison. Section 4 presents results based on the proposed methodology, covering compute kernel, model and workloads, frameworks, and applications, aiming to provide an end-to-end perspective on key performance metrics. Section 5 presents conclusions and discusses opportunities for future work.

In addition to FLOPs, the roofline model is an important metric for gauging compute and memory performance. The paper introduces the concept of the roofline model as a visualization to demonstrate the bottleneck of the benchmark and hardware, whether it is compute or memory bound. Additionally, resource utilization is tracked as another important way to understand the performance of deep learning applications. The paper uses NVIDIA-SMI for the V100 GPUs on Summit and ROCM-SMI for the MI100 GPUs on Spock to monitor memory usage and GPU utilization for the framework and application benchmarks. Communication kernels are highlighted as increasingly important due to the common practice of distributed training to manage growing data and model sizes.

The paper argues that understanding kernel and model level performance, and framework level scalability, is more important than application FLOP counts, given current scientific deep learning use cases and patterns. Using the CORAL-2 deep learning benchmarks, the performance of Spock, an early-access testbed system for Frontier, is evaluated. Compared to the V100-based Summit system with the CUDA deep learning stack, the MI100-based Spock with the ROCm deep learning stack shows an edge in single precision performance for most kernel and model benchmarking tasks. However, there is a current gap in its half precision performance, specifically for TensorFlow. The roofline modeling also suggests opportunities for improvement in the ROCm stack, which is still maturing.

The paper also explores using machine learning to model the relationship between input parameters and benchmark performance outcomes. Furthermore, a one-on-one comparison of resource utilization for the two deep learning stacks on the same workloads sheds light on the sources of performance differences. The paper emphasizes that these methods do not provide conclusive insight into underlying implementations or bottlenecks, but they do shed light on potential directions for future optimizations in the deep learning stacks.

Finally, the paper notes that Spock is a testbed early access system, and its benchmarking results and comparisons are presented to illustrate the systematic approach to deep learning benchmarking. The kernels and frameworks are maturing and will continue to evolve, and therefore, specific observations reported in this paper are likely to change, although it does not affect the overall methodology presented.

This research was sponsored by and used resources of the Oak Ridge Leadership Computing Facility (OLCF), which is a DOE Office of Science user facility at the Oak Ridge National Laboratory supported by the U.S. Department of Energy under contract no. DE-AC05-00OR22725.