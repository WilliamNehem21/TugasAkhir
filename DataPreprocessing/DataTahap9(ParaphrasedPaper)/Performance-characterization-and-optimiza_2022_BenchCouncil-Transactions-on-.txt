The issue of over-parameterization in deep neural networks leads to high storage and computational resource requirements. Pruning, a technique for eliminating redundant parameters from neural networks, can effectively address this issue by reducing storage and computation costs. While existing pruning methods such as filter pruning and element-wise pruning offer trade-offs between efficiency and accuracy, this study focuses on analyzing the performance characteristics of sparse neural networks pruned using different patterns, including element-wise, vector-wise, block-wise, and group-wise. Based on this analysis, the study proposes an optimized implementation of group-wise sparse DNN inference, which demonstrates improved efficiency in utilizing GPUs. Experimental results on various neural network models show that the proposed group-wise pruning pattern leads to much lower inference latency on GPUs compared to other sparse patterns and existing group-wise pattern implementations. Furthermore, the study extends the group-wise pattern to linear layers and demonstrates that the proposed implementation outperforms existing approaches in terms of inference time reduction.

Researchers have proposed various methods to compress DNN models in order to reduce storage and computation costs. Pruning is one such effective model compression method, which aims to remove redundant parameters from a neural network to achieve reduced model size and computational operations, leading to lower inference time.

This study thoroughly analyzes different sparse DNNs, including element-wise, vector-wise, block-wise, and group-wise patterns. It proposes an efficient implementation of group-wise sparse DNN inference, optimizing the utilization of GPUs. In addition to advancements in pruning patterns, the study also focuses on the efficient implementation and execution of pruned models, which aligns with the broader focus of the research community on exploring better sparse models and algorithms.

Overall, the study's proposed group-wise pruning pattern and its optimized implementation significantly improve the inference speed of DNN models compared to existing approaches, demonstrating superior performance when the remaining parameters of the model are below 75%.