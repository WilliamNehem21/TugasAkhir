The model has significantly reduced complexity compared to previous techniques. This method utilizes the limited-memory Broyden-Fletcher-Goldfarb-Shanno (L-BFGS) technique to find the optimal point, aiming to determine certain factors at the training stage and extend the conditional probability of the training data. The computational time has been greatly reduced by employing forward and backward methods to calculate the conditional probability and compute the gradients.

Several learning models for emotion recognition on speech data, including hidden conditional random fields (HCRF) and other models, have been discussed. To address the limitations of existing models, a novel approach based on the HCRF method is presented, which explicitly exploits a combination of full covariance Gaussian distributions. This approach applies and tests the model on speech data to recognize emotions and compares its results with those obtained by hidden Markov models (HMM) and HCRF with diagonal covariance Gaussian functions.

A dataset called Emo-DB consists of expressive exclamations of 10 actors and actresses from Germany, with utterances in seven defined emotional states. Successful attempts have been evaluated by a group of judges, and only those recognized by 80% of the listeners have been included.

The computational complexity of the proposed approach is briefly discussed in comparison to others. The existing HCRF algorithm computes gradients using forward and backward algorithms, while the proposed approach executes these algorithms once and caches the results in memory for later use. Audio-based emotion recognition has been a major focus in recent years, with issues such as feature extraction and high similarity among different emotions affecting accuracy.

A new version of the HCRF algorithm using full covariance Gaussian density functions is presented, and it has been theoretically and experimentally shown to have more precise recognition rates than existing algorithms. These improvements are statistically validated using p-values for testing and comparisons. The proposed approach has linear complexity, unlike existing methods with quadratic complexity, extending the functionality of HCRF for more practical and scalable applications in audio-based emotion recognition and related areas including speech recognition, acoustic-based context awareness, and gesture recognition.