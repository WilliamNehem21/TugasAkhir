To gain a better understanding of real-world failures and the potential limitations of cutting-edge tools, we conducted an empirical study on 277 user-reported storage failures. The study provides a detailed analysis of the issues across various dimensions, such as the time taken to resolve them and the kernel components involved. This quantitative assessment highlights the practical challenges. Furthermore, we delved deeply into a subset of storage issues to focus on measuring the observations that the tools allow developers to make—referred to as observability. We derived specific metrics to qualitatively and quantitatively measure observability, demonstrating different design tradeoffs in terms of debugging information and overhead. Notably, we observed abnormal behavior in both tools when applied to diagnose some complex cases. Additionally, we found that neither tool could provide low-level information on how the persistent storage states change—a critical aspect for understanding storage failures. To address this limitation, we developed lightweight extensions to enable such low-level observability.

Our study also revealed that neither tool could directly provide low-level information, such as storage device commands, crucial for understanding host-device interactions in the storage stack. To address this, we explored different ways to enhance both ftrace and panda, demonstrating it is feasible to improve their low-level observability without relying on specialized hardware.

In addition, we proposed to measure the observability of debugging tools, an important concept for improving system reliability. We outlined three desired properties for observability—visibility, repeatability, and expressibility—and advocated practical methodologies to measure it using concrete metrics in real-world scenarios.

It's important to note that the results should be interpreted in light of certain limitations. For instance, the dataset may not be exhaustive due to refinement efforts and limited user reporting. Additionally, the specific software environment, such as different OS distributions and kernel versions, can significantly influence manifestation symptoms of issues, emphasizing the importance of considering the software environment when diagnosing failures.

Furthermore, our work highlighted that software rather than hardware is the primary source of storage failures, suggesting the critical importance of observing the behavior of the storage software stack for failure diagnosis. Our study also found that while ftrace and panda provided useful information for the majority of cases evaluated, they both exhibited abnormal behavior for some tricky cases.

In our endeavor to measure observability, we proposed potential extensions and improvements to enhance low-level observability, particularly focusing on the storage software and device interaction. We discussed extensions to capture command-level information and enhance the observability of both ftrace and panda, without relying on specialized hardware.

Ultimately, our study aims to inspire further research in improving the robustness of computer systems and invites future efforts in reproducing and packaging various types of bug cases, deriving additional metrics for desirable system properties, and measuring other tools or features.

In summary, our work lays the foundation for measuring the observability of state-of-the-art tools and identifying potential limitations in diagnosing realistic storage failures. We hope this will inspire further advancements to address these challenges and improve system reliability.