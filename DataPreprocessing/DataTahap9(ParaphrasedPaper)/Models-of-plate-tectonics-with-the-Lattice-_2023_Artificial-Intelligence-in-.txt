The development of plate tectonics was the result of extensive new data on seafloor bathymetry and a pioneering approach to their analysis. In the coming years, advancements in geodynamics modeling will likely involve integrating artificial intelligence with high-performance computing (HPC) simulations. While there is a growing number of geoscientists utilizing HPC, there is a pressing need for efficient geodynamic tools to generate the large amount of training and validation data required for training machine learning algorithms. New numerical methods for geodynamics must be designed to maintain performance on parallel computers and to account for non-linear rheologies responsible for tectonic plate fragmentation.

Calculations on a computer system with 16 CPU 2.2 GHz Xeon cores showed that a viscous simulation took 13.8 minutes. The lbm kernel calculations achieved 78% of peak performance per core, while the total lbm algorithm (including streaming and inter-core communications) ran at 44% of peak performance per core. The computer code is capable of running in both 2D and 3D.

Patterns of convection differ between viscous and non-viscous cases, with the former exhibiting more distributed convection in the upper layer, characterized by smaller cells or horizontally flowing broad regions.

Supervised learning techniques, such as convolutional neural networks (CNNs) and support vector machines, are being explored for analyzing full convection models. Efforts have been made to extract fundamental parameters like Prandtl and Rayleigh numbers from CNNs applied to lbm models of convection, using a dataset comprising 200 snapshots of 88 convection simulations to train and test the neural network-based classifier. A future possibility is the use of generative artificial intelligence for predicting solutions of mantle dynamics models, as demonstrated in recent studies of atmospheric and general convection.

Although these approaches hold promise for the future of geodynamics modeling, the current challenge lies in generating a large number of high-quality, detailed, and reliable training samples. The lbm method offers a unique opportunity for producing these datasets, as it is highly scalable to a large number of processors with high performance and can model both high and low Prandtl numbers.