The rest of this paper is organized as follows: Section 2 presents and discusses related studies. Section 3 demonstrates the proposed research methodology. Section 4 discusses the evaluation results. Finally, in Section 5, we conclude and discuss future work.

In order to improve classification accuracy, a stacked autoencoder was used to extract features at the third layer, and a decision tree classification algorithm was applied at the fourth layer. The model was tested using a combination of two open-source datasets: the CTU-13 dataset of botnet traffic, which includes thirteen scenarios with different botnet samples, and ISOT, containing both malicious and benign traffic. The use of 10-fold cross-validation resulted in an accuracy of 98.7%.

The datasets provided are unbalanced, with the human dataset being nearly 28 times larger than the bot dataset. Addressing this common challenge of unbalanced datasets, the research employed under-sampling by randomly selecting a subset of examples from the human dataset. Specifically, 86 random samples from the human dataset were chosen and merged with the bot dataset to create an almost balanced training dataset.

Validation involves evaluating whether mathematical models explaining relations among features sufficiently represent the dataset. While error estimation for the classification model is typically created at the end of the training phase, this approach may not clarify how well the models will work on unseen data. Therefore, the study utilized cross-validation.

This research considered the F1 score, which provides an overview of both precision and recall by taking the harmonic average of the two metrics. Additionally, the terms TP, FP, FN, and TN were defined in the context of the research. For example, TP denotes the set of examples correctly classified as bots, while TN represents the set of examples accurately categorized as human. 

Several machine learning algorithms, including k-nearest neighbors (KNN), decision trees, and naive Bayes (NB), were evaluated. Notably, NB produced the worst classification performance across all evaluation metrics. This can be attributed to the assumption underlying NB that all input features are independent and of equal importance when building the classification model. However, this assumption may not always hold true, as some features may be more important than others when creating classification models.