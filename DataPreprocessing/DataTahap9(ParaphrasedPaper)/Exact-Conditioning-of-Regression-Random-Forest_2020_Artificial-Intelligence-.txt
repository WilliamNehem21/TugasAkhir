The regression random forest is an ensemble learning method described by Breiman (2001) that constructs multiple regression tree models from bootstrap samples of the training data. It introduces randomness into the tree-growing process by considering only a subset of predictor variables for split-point selection at each node, thereby reducing the likelihood of strong predictor variables being consistently chosen and preventing excessive correlation among the regression trees. By combining the predictions of these trees, the method aims to reduce prediction variance and improve accuracy by predicting the mean of all individual regression tree predictions.

Chiles and Delfiner (2012) used the R package rgeostats to perform simulations, creating a scenario where ground-truth data is available across the entire study area, revealing a non-linear relationship between the response variable and predictor variables with some interactions. Additionally, the response variable shows spatial auto-correlation and a non-Gaussian distribution.

When comparing the spatial prediction maps generated by regression-kriging, traditional regression random forest, and the proposed regression random forest, it was found that the spatial prediction map produced by regression-kriging is smoother than those from the traditional regression random forest and the proposed regression random forest.