The fundamental nature of training programs lies in their function as a specific type of computer program. One relevant perspective within this domain is the concept of state-and-effect triangles, which highlights the dual role of programs as both state and predicate transformers. While originally rooted in quantum computing, this framework has found diverse applications, encompassing deterministic and probabilistic computations.

In recent times, the significance of neural network architecture for its effectiveness and trainability in specific problem areas has become evident. Consequently, there has been a proliferation of tailored architectures, each designed for its intended use. Our objective is not to encapsulate the wide array of specialized neural networks within a single framework, but instead to present neural networks as a manifestation of the duality between state and predicate transformers. Thus, we will focus on a simple, appropriately general neural network type known as the multilayer perceptron (MLP).

This paper commences by outlining MLPs, the constituent layers, and their forward semantics as a state transformation (section 2). Subsequently, in section 3, we detail the corresponding backward transformation on loss functions and utilize this to formulate backpropagation in section 4. Finally, in section 5, we delve into the compositional nature of backpropagation by portraying it as a functor and compare our approach to other works.

A recent paper provides a categorical analysis of neural networks, highlighting the compositional nature of backpropagation through its exposition as a functor. In this section, we initially expound on the functoriality of backpropagation within the existing framework, followed by a comparison with other approaches.

In this paper, we have investigated neural networks within a state-and-effect framework, characterizing the application of a neural network to an input as a type of state transformation through Kleisli composition, and the backpropagation of loss across the network as a form of predicate transformation on losses. Additionally, we have noted that the compositional nature of backpropagation corresponds to the functoriality of a mapping between a category of states-and-effects and the category of neural networks.

To illustrate this perspective on neural networks, we have deliberately focused on a simple subclass of established network architectures and established a category of multilayer perceptrons (MLPs). However, we believe it is feasible to develop a more comprehensive categorical framework capable of encompassing a wider range of network architectures, which may be the subject of future research.