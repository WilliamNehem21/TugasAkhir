The practice of cyclic debugging remains crucial for pinpointing programming faults, yet any form of non-determinism poses challenges in this process. Concurrency, particularly in languages such as Java and C#, introduces significant non-determinism, for which current debuggers offer limited support in handling multi-threaded applications.

Numerous solutions have been proposed to capture and deterministically replay the execution of multi-threaded Java applications. We introduce a separate replay facility that can enforce a thread schedule to mimic the execution of a multi-threaded Java application on a uniprocessor. Our replay facility is independent of any specific virtual machine, allowing the use of any compliant virtual machine for replay. We present an overview of related work and offer a detailed description of our thread scheduling format, the mechanisms utilized in our implementation, and the performance results of the replay mechanism. Additionally, we outline a more general format for thread schedules and address open issues before presenting concluding remarks.

Our approach to replay and representation of a thread schedule bears similarities to the work of Russinovich and Cogswell, who reconstructed the scheduling of execution on a uniprocessor for the Mach operating system. Our implementation is based on a modified debug version of the Mach system libraries, and it requires the compilation of applications with specific flags to facilitate replay. We focus specifically on replay of non-determinism introduced by concurrency, but our schedule format is flexible enough to accommodate additional commands for replaying other forms of non-determinism.

During replay, the engine ensures that only the specified thread from the schedule can proceed with its execution, while the remaining threads are kept in a blocked state. To enable proper replay, the application is instrumented with calls to the replay engine at locations where certain actions should occur in the schedule. Additionally, we address the issue of emulating certain behaviors at the level of the virtual machine during replay.

We provide insights into capturing execution traces on the fly and discuss a series of experiments that evaluate the performance and portability of our replay engine. We also mention an extension of Java Pathfinder that generates schedules for replay. Furthermore, we describe how a schedule can be generated dynamically during the execution of a Java application and highlight the role of the replay engine and instrumentation facility in the Jnuke project.

Ultimately, while source code debugging is a preferred approach in most cases, transparency at the bytecode level is crucial for understanding certain concurrency errors. We also discuss the potential for transparent stepping through bytecode and the implementation of additional attributes in class files to assist in the debugging and replay processes.