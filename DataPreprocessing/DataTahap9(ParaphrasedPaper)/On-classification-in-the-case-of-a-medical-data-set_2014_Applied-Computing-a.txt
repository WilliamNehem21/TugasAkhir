In our previous research, we developed a method for analyzing eye movements in otoneurological tests. The goal was to automatically differentiate between valid and invalid nystagmic eye movements, as only valid eye movements can be used for diagnosing otoneurological patients. Typically, invalid eye movements are corrupted by noise or artifacts. To address this, we explored the use of machine learning methods to classify nystagmic eye movement candidates as "rejected" or "accepted" based on their validity. However, we encountered challenges due to the complex distribution of the data, which made the classification task difficult.

Our initial cleaning procedure assumed that the data could be divided into two distinct classes, with the rejected eye movements having values above certain upper bounds. After manually selecting nystagmic eye movement candidates, we found that there were 2171 accepted and 3818 rejected beats. After automatic selection, these numbers changed to 2517 and 3472, and after using both methods jointly, the numbers were 1645 and 4344, respectively. Subsequently, after cleaning the data by reducing the larger class of rejected beats, the numbers for each class were 2171, 2517, and 1645 in these three situations.

We observed that in some cases, the automatic selection method performed better than the combined manual and automatic methods. This discrepancy was attributed to the variation in manual selection criteria, whereas the automatic selection was more consistent. Additionally, we found that using greater nearest neighbor numbers (5, 7, 9, or 11) than the original 3 improved the classification results slightly, as it led to more elements being removed from the majority class. However, this improvement was not consistent across all cases, emphasizing the importance of considering the unique properties and distribution of the data.