The objective of neural network pruning methods is to eliminate redundant weights in a dense model. These methods can be categorized into weight pruning or channel pruning, depending on the level of granularity. Weight pruning involves zeroing out individual weights to create a sparse set of weight tensors, which reduces model size but introduces irregular memory access and may lead to limited or negative speedups on general-purpose hardware. In contrast, channel pruning methods remove entire channels to compress the model, which changes the dimension of weight tensors while maintaining a dense format that is well-suited to high rank of feature maps. Some researchers have found that pruned networks can achieve the same accuracy regardless of whether they inherit the weights of the original network. This suggests that the essence of channel pruning lies in finding optimal channel numbers in each layer, rather than selecting unimportant channels based on arbitrary designs. To address this, a latency-aware automatic channel pruning (LACP) method is proposed in this paper, which aims to automatically search for the optimal pruned network structure by determining the channel numbers in convolutional layers instead of focusing on important channels. The paper is organized as follows: Section 2 reviews related works, Section 3 presents the proposed LACP method, Section 4 details the experimental results and analysis, and Section 5 concludes the paper. The proposed approach is evaluated through experiments on various datasets and models, and the results show improved inference acceleration while maintaining higher accuracy.

The paper proposes a novel approach, different from conventional methods, that leverages automatic channel pruning to find optimal pruned network structures. Rather than manually selecting important channels to prune, the proposed method uses evolutionary search to identify the optimal channel number for each layer. The study addresses the challenging aspect of finding the optimal network structure by employing an approach that shrinks the search space effectively. This approach is based on an analysis of the inference latency of pruned convolutional layers on GPUs, revealing that the latency presents a staircase pattern with different channel numbers. This observation is utilized to significantly reduce the search space of pruned structures, facilitating an evolutional procedure to efficiently search for low-latency and accurate pruned networks. 

Experimental evaluations conducted on public datasets demonstrate the superiority of the proposed approach in achieving better inference acceleration without compromising accuracy. The study also outlines areas for future investigation, including a more comprehensive understanding of the GPU tail effect and the generalization of the method to different hardware platforms.

Finally, the authors acknowledge the support received for this work, which includes funding from the National Natural Science Foundation of China and the Youth Innovation Promotion Association of the Chinese Academy of Sciences. Experiments were conducted on a supercomputer system at the University of Science and Technology of China.

The paper presents a novel approach to channel pruning that moves away from manual selection of important channels, instead adopting an automatic method to identify optimal pruned network structures. The proposed approach effectively addresses the challenge of finding the optimal network structure by leveraging the analysis of inference latency to shrink the search space and enable efficient search for low-latency and accurate pruned networks. Experimental evaluations demonstrate the superiority of the proposed approach in achieving better inference acceleration while maintaining higher accuracy on public datasets. The study also outlines avenues for future research, including further investigation of the GPU tail effect and the generalization of the method to different hardware platforms. The authors acknowledge financial support received for this work, including funding from the National Natural Science Foundation of China and the Youth Innovation Promotion Association of the Chinese Academy of Sciences, as well as access to the supercomputer system at the University of Science and Technology of China for conducting experiments.