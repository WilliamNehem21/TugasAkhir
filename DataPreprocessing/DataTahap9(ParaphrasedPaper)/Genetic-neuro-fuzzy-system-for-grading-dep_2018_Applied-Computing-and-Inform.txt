The performance of the NN-FL system described above is heavily reliant on various rules and weights, such as v1, v2, v3, v4, v5, v6, v7, and w1. The structure of the NNFL system can change when the rule base of the FLC is optimized, which involves removing certain rules. Many studies have been conducted on developing an optimal NNFL system, and in this study, two different approaches are discussed.

The first approach involves keeping the rule base unchanged during training and only optimizing the weights v1, v2, v3, v4, v5, v6, v7, and w1 using the backpropagation algorithm, typically by minimizing the mean squared error in prediction.

However, approach 1 has limitations, such as encountering local minima problems, and only optimizing the weights. As a result, in approach 2, both the rule base (RB) and the data base (DB) are attempted to be optimized using a genetic algorithm (GA) with a 4454-bit long string. The data base is represented by the first 80 bits (10 bits for each variable), while the remaining 4374 bits are used to express the rule base (two bits to represent a rule: 00 for medium, 01 and 10 for moderate, and 11 for severe).

To identify redundant rules, the concept of the importance factor is used. Rules with relatively low importance factors are considered redundant and may be deleted from the rule base, with the aim of ensuring that non-firing of rules does not occur. This is achieved by using the probabilities of occurrence of the linguistic terms for each input condition based on the number of times each term is fired during training.
