Certain mining techniques produce opaque models that are difficult for domain experts to analyze. Models such as support vector machines or neural networks are generally challenging to comprehend, while the understandability of decision rules or decision trees depends on their complexity.

Human input in the form of cost-sensitive considerations for misclassification is crucial, yet many existing approaches lack cost sensitivity or require a predefined cost matrix. In reality, problems are cost-sensitive, but the exact cost matrix may not be known, and costs are often interdependent.

Multi-objective optimization allows the system to find rules that balance trade-offs between objectives such as reducing false positives, false negatives, and custom cost functions that consider relationships between instances or estimate model understandability. This eliminates the need to specify a cost matrix at the outset.

The system should allow users to interact with the data and results at any time, without the need to wait for the system or vice versa.

In a case study in a medium-sized software company, data from software repositories on code change parts and corresponding remarks were used to derive rules characterizing change parts that do not require review.

For domain-specific use cases, cost functions are necessary for estimating the effort expended by domain experts, the number of uncovered actions, and the cognitive complexity of understanding the rules. These cost functions should be minimized by the learning algorithm.

The process of interaction should be iterative, allowing users to modify defined conditions and receive feedback about the results at any time.

In evaluating a real-world dataset, the GIMO algorithm exhibited favorable performance compared to existing approaches for mining decision rules. The interactive feedback provided by domain experts significantly contributed to the overall result, resulting in a notable reduction in missed actions at the cost of very few. 

The GIMO algorithm is among the best performing algorithms for most metrics and the interactive feedback provided by domain experts was instrumental to its overall result.

The explainability of opaque models is an area of active research, and a multi-objective, user-informed approach could be beneficial in achieving a balance between model understandability and approximation quality.
