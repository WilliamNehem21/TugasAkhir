The use of federated learning (FL) enables multiple parties to collectively train models without exposing private datasets. However, while FL prevents the sharing of raw data, there is still a risk of extracting personal or confidential data from the shared models. To address this, secure multi-party computation (MPC) is employed to aggregate locally-trained models in a privacy-preserving manner. Nonetheless, this approach comes with drawbacks such as high communication costs and poor scalability in a decentralized environment. To mitigate these issues, we propose a novel communication-efficient MPC-enabled federated learning framework, referred to as CE-FED. CE-FED is designed as a hierarchical mechanism that forms a model aggregation committee with a small number of members and performs global model aggregation only among committee members, rather than involving all participants. We have developed a prototype and conducted experiments with different datasets to demonstrate that CE-FED achieves high accuracy, communication efficiency, and scalability without compromising privacy.

The rest of this paper is organized as follows: Section 2 provides background information and discusses related work, Section 3 presents the CE-FED framework, Section 4 describes the experimental analysis and performance evaluations, and Section 5 presents the conclusions.

In federated learning, there is a potential risk that centralized service providers could exploit the data and machine learning models for unauthorized purposes, as the data and models are no longer under the control of the data owner.

The scope of federated learning has been extended to collaborations across multiple organizations, including horizontal FL and vertical FL, which are distinguished based on the distribution of data over sample and feature spaces.

Secure MPC is a privacy-preserving technique that allows for secure computation over sensitive data. It involves techniques such as garbled circuits and secret sharing, with the latter being a commonly used MPC protocol.

To address the potential risk of malicious users intercepting model parameters and extracting sensitive data during model aggregation in federated learning, MPC methods like additive secret sharing or Shamir secret sharing can be employed to encrypt the gradients/model updates before aggregation.

In previous work, we proposed a two-phase MPC-enabled FL framework to improve scalability. Our current work introduces CE-FED, which employs hierarchical model aggregation, and presents new performance experiments and analyses with different datasets.

CE-FED selects a small number of clients as committee members to use MPC for hierarchical aggregation of the local models of all FL clients, thus avoiding the need to share model parameters with all participants. The framework operates in two phases, where FL clients located close to each other form groups to securely aggregate their local models using MPC to create intra-group models. A committee member is then elected from each group based on latency, and in the second phase, the committee members work together to aggregate inter-group models using MPC.

To evaluate the effectiveness of the CE-FED framework, we used PyTorch and Python as the machine learning library and considered three public image datasets: MNIST, CIFAR-10, and Fashion-MNIST. Our experiments focused on the performance and effectiveness of using both independent and identically distributed (IID) and non-IID datasets, and we used accuracy and communication cost as performance metrics.

The experimental results demonstrate that federated learning achieves improved test accuracy compared to local training and is comparable to centralized learning. Additionally, CE-FED effectively reduces communication costs while maintaining similar levels of accuracy and privacy preservation.

The paper also includes biographical information about the authors: Renuga Kanagavelu, Qingsong Wei, and Zengxiang Li, including their academic and professional backgrounds.

Overall, the proposed CE-FED framework addresses the challenges of federated learning by introducing a communication-efficient MPC-enabled approach that achieves high accuracy, communication efficiency, and scalability while preserving privacy.