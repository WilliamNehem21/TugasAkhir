Spatial aggregation involves the process of summarizing motion features for each joint and capturing the spatial relationships between different joints. Generalizing motion features per joint means obtaining independent features regardless of the specific joint, such that features observed in one joint can be applied to others. Similarly, generalizing spatial dependencies among joints refers to acquiring reference relationships between joints to understand each motion, such as the interconnected movement of shoulder, elbow, and wrist to produce a straight-line trajectory of the fingertips. Prior approaches have focused on expressing spatial dependencies using network structures, such as graph convolution and graph attention, where joint-wise features and spatial dependencies across joints are considered. While methods using motion trees as adjacency matrices can describe spatial proximity in link connection relationships, they struggle to reference distant joints. On the other hand, the generalization of spatial dependence continues to be an unsolved challenge.

The paper emphasizes that the generalization issues differ between position-based and angle-based data description methods, which have been previously evaluated as independent problems. The position-based description can capture features representing a wide range of motions, while the angle-based description is advantageous for capturing features representing local actions. Consequently, the approach integrates the angle-based description to provide features for each node of the joint graph and the position-based description to offer spatial dependence during feature aggregation.

The study focuses on generalizing spatial dependency by utilizing data structures instead of network structures. Network structures typically employ either position-based or angle-based descriptions for data structures describing input/output poses. Position-based description directly uses the 3D position of each joint acquired by motion capture, while the angle-based description determines a skeletal reference shape. The attention mechanism is used to aggregate input data, and absolute positional encoding (APE) is utilized to obtain features depending on absolute time and specific joints, while relative positional encoding (RPE) is used to obtain generalized features that do not distinguish the timing of occurrence.

The model's performance is evaluated based on the mean per joint position error (MPJPE) metric and compared with the results of several recent methods such as Res-sup, TrajGNN, DMGNN, and MSR-GCN. The study also investigates the impact of data description on both long-term and short-term predictions, demonstrating that the position-based description affects long-term prediction by detecting global features of the whole body, while the angle-based description involves short-term prediction by detecting local features of each joint unit.

In addition, the paper presents an inference that predicts motion classes with high accuracy, even for non-periodic and difficult-to-generalize movements, by selecting the appropriate architecture for extracting features of position and angle. The results indicate that the proposed model outperforms state-of-the-art methods in short-term prediction.

The work additionally refers to the "Attention is All You Need" paper by Vaswani et al. (2017) in the proceedings of the Neural Information Processing Systems (NeurIPS), which serves as a foundational reference for the attention mechanisms employed in the study.