Fork-join and split-merge queueing systems are mathematical models that represent parallel task processing systems, where incoming tasks are divided into n subtasks and processed by a group of servers with different capabilities. The original task is considered complete once all its associated subtasks have been served. The performance of split-merge and fork-join systems is commonly assessed based on two metrics: task response time and subtask dispersion. Recent research has focused on reducing subtask dispersion, or the product of task response time and subtask dispersion, by introducing delays to specific subtasks. These delays can either be pre-computed statically or varied dynamically based on the system's state before the subtask service begins. It is assumed that once a subtask is in service, it cannot be interrupted.

A dynamic optimization strategy that benefits both metrics is the removal of delays on any subtask with a sibling that has already completed service. This paper integrates such a policy into existing methods for computing optimal subtask delays in split-merge and fork-join systems. Through two case studies, it is shown that implementing this strategy affects the optimal delays calculated and leads to improved subtask dispersion values compared to existing techniques. In some instances, it is advantageous to initially delay the processing of non-bottleneck subtasks until the bottleneck subtask has completed service.

Split-merge and fork-join systems are abstractions of parallel queueing networks that describe the flow and processing of tasks in parallel networks. In these systems, incoming tasks are divided into subtasks, each of which must be serviced before the entire task is considered complete. Two key performance metrics for such systems are task response time, representing the time from task entry to completion of all subtasks, and subtask dispersion, referring to the difference in time between the completion of the first and last subtasks. While task response time has been extensively studied, subtask dispersion has received limited attention until recently.

In dynamic delay systems, removing delays on subtasks with completed sibling services benefits both task response time and subtask dispersion. This paper proposes a new method for calculating subtask delays in split-merge and fork-join systems, considering this optimization. Starting with 2-server split-merge systems with deterministic and exponential service, the paper provides an intuitive understanding of the technique and then applies it to a 3-server test case, demonstrating substantial reductions in subtask dispersion compared to existing methods.

This paper introduces fundamental concepts essential to grasp its content, including split-merge and fork-join systems, which are queueing network models for parallel subtask processing, as well as related quantitative metrics such as response time, subtask dispersion, and a trade-off metric. The trade-off metric can be used for decision-making when considering both subtask...

When a task enters service, it is divided into n subtasks, each processed by its own server with a probability distribution for the service time. The task is considered serviced once all n subtasks have been served.

Task response time is the duration for a task to be fully processed, starting when the task enters the system and stopping when all subtasks are serviced.

This section introduces a method to calculate subtask dispersion in split-merge systems, where a start work signal is sent to sibling subtasks once a subtask is completed. The model assumes that a delay applied to a subtask can be interrupted at any point before the subtask begins service. However, once a subtask starts service, it will be uninterrupted until completion. The start work signal is sent because removing delays after the first sibling subtask finishes service reduces both subtask dispersion and task response time.

Gj(t, tj, dj) represents the probability distribution function of the service time of the jth server. If tj < dj, the jth server has not started service, and the servicing begins immediately. Otherwise, the server has already started servicing, and the service time is adjusted to account for previous work. This two-part g-function is a key difference compared to earlier work.

The described methods leverage interruptions to start processing of sibling tasks once a subtask is completed. The results are calculated using the same simulation framework as the fork-join systems in the previous subsection.

The improvement in subtask dispersion, however, comes at the expense of task response time. Method 6 is superior in subtask dispersion compared to the old dispersion minimisation technique (Method 2) but has a significantly higher response time than the trade-off technique (Method 3) and vanilla technique (Method 1). Nonetheless, some of these issues can be addressed by Method 7, the fork-join version of Method 6. While Method 7 has slightly increased subtask dispersion compared to Method 6, the task response time is still higher than in the two other fork-join systems (Methods 4 and 5).