The ongoing research focuses on enhancing the efficiency and convergence speed of multilayer backpropagation neural network algorithms. Recently, there has been a growing interest in leveraging entropy-based criteria in adaptive systems, with principles being proposed based on the maximization or minimization of entropic cost functions. One approach to incorporating entropy criteria into learning systems involves minimizing the entropy of the error between two variables, typically the output of the learning system and the target variable. This study proposes a method to improve the efficiency and convergence rate of multilayer backpropagation neural networks by replacing the traditional mean square error (MSE) minimization principle with the minimization of Shannon entropy (SE) of the discrepancies between the network output and the desired target. The study investigates and compares these two cost functions using different activation functions (Cauchy and hyperbolic tangent), and the results indicate that using the Shannon entropy cost function leads to greater convergence than MSE, while MSE results in faster convergence than Shannon entropy.

Neural networks offer advantages in solving large-scale problems efficiently and in obtaining optimal solutions. Their flexibility as data-driven, self-adaptive, universal functional approximators, and nonlinear models allows them to model complex relationships and estimate posterior probabilities for classification and statistical analysis purposes.

Traditionally, error backpropagation for neural network learning employs MSE as the cost function, but this can lead to slow reduction of errors during the learning process, resulting in periods of stagnation that prolong learning times. To address this issue, some studies have replaced MSE with entropy error functions, resulting in better network performance and shorter stagnation periods. This paper aims to use the minimization of error entropy as a cost function for classification purposes, replacing MSE. The paper proceeds to discuss related work, introduce multilayer backpropagation neural networks, and analyze mean square error and Shannon entropy. Section 6 presents simulated results for Shannon entropy, while Section 7 discusses mean square error, and Section 8 provides a comparative analysis of Shannon entropy and MSE. Finally, the paper concludes by outlining its findings in Section 9.

The backpropagation algorithm, while widely used for controlling weight adjustment and reducing oscillations, exhibits relatively slow convergence rates, particularly for networks with multiple hidden layers. This can be attributed to the saturation behavior of the activation functions used for the hidden and output layers, as outputs in saturation areas yield small descent gradients, hindering weight adjustment progress. The backpropagation algorithm involves several steps that need to be iteratively applied. Each step involves computing the error for each example in the dataset and adjusting the weights accordingly. Furthermore, the proposed backpropagation algorithm does not directly use certain mathematical expressions.