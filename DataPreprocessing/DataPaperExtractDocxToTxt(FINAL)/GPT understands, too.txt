Journal Pre-proof

GPT understands, too

Xiao Liu, Yanan Zheng, Zhengxiao Du, Ming Ding, Yujie Qian, Zhilin Yang, Jie Tang


PII:	S2666-6510(23)00014-1
DOI:	https://doi.org/10.1016/j.aiopen.2023.08.012 Reference:	AIOPEN 62
To appear in:	AI Open

Received date : 19 August 2023
Accepted date : 22 August 2023

Please cite this article as: X. Liu, Y. Zheng, Z. Du et al., GPT understands, too. AI Open (2023), doi: https://doi.org/10.1016/j.aiopen.2023.08.012.

This is a PDF file of an article that has undergone enhancements after acceptance, such as the addition of a cover page and metadata, and formatting for readability, but it is not yet the definitive version of record. This version will undergo additional copyediting, typesetting and review before it is published in its final form, but we are providing this version to give early visibility of the article. Please note that, during the production process, errors may be discovered which could affect the content, and all legal disclaimers that apply to the journal pertain.
© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/ by/4.0/).

Manuscript	Click here to view linked References 



1	GPT Understands, Too
2
3
4	Xiao Liu1∗, Yanan Zheng1∗, Zhengxiao Du1, Ming Ding1, Yujie Qian2,
5	Zhilin Yang1†, Jie Tang1†
6	1Tsinghua University	2Massachusetts Institute of Technology
8
9
10
11

12	Abstract
13
Prompting a pretrained language model with
natural language patterns has been proved effec-
tive for natural language understanding (NLU).
However, our preliminary study reveals that
manual discrete prompts often lead to unsta-
ble performance—e.g., changing a single word
in the prompt might result in substantial per-
formance drop. We propose a novel method
P-Tuning that employs trainable continuous
prompt embeddings in concatenation with dis-
crete prompts. Empirically, P-Tuning not only
stabilizes training by minimizing the gap be-
tween various discrete prompts, but also im-
proves performance by a sizeable margin on
a wide range of NLU tasks including LAMA
and SuperGLUE. P-Tuning is generally effec-
tive for both frozen and tuned language models,
under both the fully-supervised and few-shot
settings.
35
36	1  Introduction
37
Pretrained language models (PLMs; Brown et al.,
2020) have significantly advanced the performance
of natural language understanding (NLU). PLMs
are trained with different pretraining objectives,
such as masked language modeling (Devlin et al.,
2018), autoregressive language modeling (Radford
et al., 2019), seq2seq (Raffel et al., 2019), and per-
mutation language modeling (Yang et al., 2019).
PLMs can be further enhanced with prompting
(Brown et al., 2020; Schick and Schütze, 2020),
which employs manually written prompt patterns as
additional input to a language model. With prompt-
ing while PLMs are either finetuned on a small la-
beled dataset or frozen for direct inference on down-
stream tasks. Prompting has significantly improved
the performance of many NLU tasks (Brown et al.,
2020; Schick and Schütze, 2020).
 	
† corresponding to: Zhilin Yang (zhiliny@tsinghua.edu.cn)
and Jie Tang (jietang@tsinghua.edu.cn)
∗ indicates equal contribution.


Prompt	P@1	P@1
w/o PT w/ PT
[X] is located in [Y]. (original)	31.3	57.8
[X] is located in which country or state? [Y]. 19.8	57.8 
[X] is located in which country? [Y].	31.4	58.1
 is located in which country? In [Y].	51.1	58.1 
Table 1: Discrete prompts suffer from instability (high variance), while P-Tuning stabilizes and improves per- formance. Results are precision@1 on LAMA-TREx P17 with BERT-base-cased. “PT” refers to P-Tuning, which trains additional continuous prompts in concate- nation with discrete prompts.
However, we observe that manual discrete prompts suffer from a large degree of instability. As shown in Table 1, with a frozen language model, changing a single word in the prompt might result in substantial performance drop. As we will show in Section 3, when the language model is tuned, the instability problem is alleviated but the perfor- mance difference between different prompts is still sizeable, especially in the few-shot setting. Such an instability issue of discrete prompts poses a crit- ical challenge in practice. Recent approaches of automatic prompting have attempted to search for a better-performing prompt given a task (Shin et al., 2020; Gao et al., 2020; Jiang et al., 2020b), but these methods do not change the unstable nature of discrete prompts.
To reduce the instability of discrete prompts, we propose a novel method P-Tuning that em- ploys trainable continuous prompt embeddings in concatenation with discrete prompts. Specifically, given a discrete prompt as the input, P-Tuning con- catenates continuous prompt embeddings with the discrete prompt tokens and feeds them as the input to the language model. The continuous prompts are updated by backpropagation to optimize the task objective. The intuition is that continuous prompts incorporate a certain degree of learnability into the input, which may learn to offset the effects of mi-







nor changes in discrete prompts to improve training
stability. To further improve performance, we em-
ploy a prompt encoder using LSTMs or MLPs to
model the dependency between continuous prompt
embeddings.
We experiment with two NLU benchmarks: the
LAMA (Petroni et al., 2019) knowledge probing
and SuperGLUE (Wang et al., 2019a). On LAMA,
with the language model frozen, P-Tuning out-
performs manual discrete prompts and searched
prompts by 20+ points and 9 points respectively
with the same pretrained models. On SuperGLUE,
with the language model finetuned, P-Tuning out-
performs PET (Schick and Schütze, 2020) with
the best discrete prompts under both the fully-
18	supervised and few-shot settings. In addition to im-
19	proving performance, our results show that across
20	a wide range of tasks and settings, P-Tuning sub-
21	stantially reduces the performance gap between dif-
22	ferent discrete prompts, which results in improved
24	stability for language model adaptation.
25
26	2  Method
27
2.1  Issues with Discrete Prompts
Prompting employs natural language patterns as
additional inputs to pretrained language models for
adaptation to downstream tasks (Brown et al., 2020;
Schick and Schütze, 2020). Prior work (Zheng
et al., 2021) has pointed out that prompting has
achieved consistent and substantial improvements
on a number of NLP tasks. However, it still re-
mains a challenging problem of how to write high-
performing discrete prompts.
We performed preliminary experiments using
different manual prompts on the LAMA knowledge
probing task (Petroni et al., 2019), which aims to
extract triplet knowledge from a language model
by predicting the tail entities. Results in Table 1
show that manual discrete prompts lead to unstable
performance. For example, if we compare the last
two prompts in the table, changing a single word
in prompt causes a drastic decrease of 20 points in
performance.
In light of the challenge, recent works propose to
automate the search procedure of discrete prompts
by mining the training corpus (Jiang et al., 2020b),
gradient-based searching (Shin et al., 2020), and us-
ing pretrained generative models (Gao et al., 2020).
However, these works aim at searching for better-
performing prompts but do not change the nature
of instability for discrete prompts. In addition to
the instability issue, searching in the discrete space might not be able to fully leverage the gradients from backpropagation, which will potentially result in suboptimal solutions. To this end, we explore the possibility of training continuous prompts to stabilize and improve the performance of language model adaptation.
2.2  P-Tuning
Formally, let M be a pretrained language model with a hidden size of h and a vocabulary size of
|V|. Let {(xi, yi))}i be a labeled dataset for an NLU task, where x0:n = {x0, x1, ..., xn} is an
input consisting of a sequence of discrete tokens, and y ∈ Y is a label. Our goal is to estimate the conditional probability for classification fM(x) = pˆ(y|x) with parameters of M either finetuned or frozen.
Prompting was proposed in the format of discrete tokens (Schick and Schütze, 2020). Let [Di] be a discrete prompt token. Each prompt can be described as a template T  =
{[D0:i], x, [D(i+1):j ], y, [D(j+1):k]}, which could organize the labeled data (including the inputs x and the label y) into a sequence of text tokens, such
that the task could be reformulated as filling in the blanks of the input text. For example, for the task of predicting a country’s capital (LAMA-TREx P36), a prompt could be “The capital of [INPUT] is [LA- BEL].” With a piece of labeled data “(Britain, Lon- don)”, the reformulated text would be “The capital of Britain is [MASK].”, where “[MASK]" should predict the given label “London”. Both discrete prompts and discrete data are together mapped into input embeddings:
{e(D0)...e(Di), e(x0), ..., e(xn), ..., e(Dk)}
through the pretrained embedding layer, where e ∈
R|V|×d.
However, as is discussed in Section 2.1, such discrete prompts tend to be extremely unstable and might not be optimal with back-propagation. Therefore, we propose P-Tuning that uses contin- uous prompt embeddings to improve and stabilize prompting. Let [Pi] be the ith continuous prompt embedding. The prompt template for P-Tuning is as follows:
T = {[P0:i], x, [P(i+1):j ], y, [P(j+1):k]}
P-Tuning leverages an extra embedding function
f : [Pi] → hi to map the template to
{h , ..., h , e(x), h	, ..., h , e(y), h	, ..., h }

0	i
i+1	j
j+1	k









1
2
3
4
5
6
7
8
9
Figure 1: An example of prompt search for “The capital of Britain is [MASK]”. Given the context (blue zone,
“Britain”) and target (red zone, “[MASK]”), the orange zone refer to the prompt. In (a), the prompt generator only
receives discrete rewards; on the contrary, in (b) the continuous prompt embeddings and prompt encoder can be
optimized in a differentiable way.
14
15
Finally, we update the embeddings {Pi}k	to op-

timize a task loss function.
It is noteworthy that we can also concatenate
discrete prompts with continuous prompts, which
performs better and is adopted throughout our ex-
periments. P-Tuning is applicable to both frozen
and finetuned language models.
24
25	2.3  Prompt Encoder
26
In the aforementioned framework, we employ a
mapping function f to map trainable embeddings
{Pi} to model inputs {hi}. The intuition is that
by using a mapping function, it is more conve-
nient to model the dependency between different
prompt embeddings, compared to using indepen-
34	dent learnable embeddings. In our implementation,
35	we use a lightweight neural network to formulate
36	the function f . Specifically, we experiment with
37	using long short-term memory (LSTM) networks,
38	multi-layer perceptrons (MLPs), and the identity
40	mapping function in Section 3.
41
42	3  Experiments
43
We include two NLU benchmarks: LAMA (Petroni
et al., 2019) for knowledge probing (§ 3.1) and Su-
perGLUE (Wang et al., 2019a) for general natural
language understanding. On SuperGLUE, we con-
sider both the fully-supervised learning (§ 3.2) and
few-shot learning (§ 3.3) settings.
On LAMA, following Shin et al. (2020); Jiang
et al. (2020b), language models are frozen and only
the discrete or continious prompts are tuned. For
SuperGLUE, following Schick and Schütze (2020);
Zheng et al. (2021), language models are tuned. In
our setting, we jointly optimize the language model
parameters and the continuous prompts. This setup
not only follows the common, standard settings in
prior work, but also allows evaluating P-Tuning





Table 2: Task settings and summary of results in our experiments. P-tuning shows improvement over base- lines on all task settings, and can stabilize performance on LAMA and Few SG. For Full SG, the gap between discrete prompts is not large and training is stable even without P-Tuning. (Full SG: fully-supervised learn- ing on SuperGLUE; Few SG: few-shot SuperGLUE; Improved: overall performance improved; Stabilized: training stabilized by minimizing difference between discrete prompts).


with both tuned and frozen language models.
The overall task setup and a summary of results are shown in Table 2.
Knowledge Probing
Setup
Knowledge probing, or referred to as fact retrieval, evaluates how much real-world knowledge has language models gained from pre-training. The LAMA (Petroni et al., 2019) dataset evaluates it with cloze tests created from triples selected in the knowledge bases.
Datasets and vocabulary. LAMA enforces all answers in single-token format. We first adopt the original LAMA-TREx dataset, consisting of 41 Wikidata relations and altogether 34,039 test- ing triples (namely LAMA-34k, which covers all BERT vocabularies). Since different pretrained models share distinct vocabularies, to allow direct comparison, we follow previous work (Shin et al., 2020) to adopt a subset that covers the intersection of GPT’s and BERT’s vocabularies. This is caled








1
Original
(MP)

BERT-base BERT-large E-BERT

31.1
32.3
36.2

 	
5
6
7
8	GPT2-medium (345M)	20.3	46.5 (+26.2)

9	BERT-base	48.3
10	BERT-large	50.6
11
GPT2-xl (1.5B)	22.8	54.4 (+31.6)
MegatronLM (11B)	23.1	64.2 (+41.1)

Table 3: Knowledge probing Precision@1 on LAMA-34k (left) and LAMA-29k (right). P-tuning outperforms all
the discrete prompt searching baselines. (MP: Manual prompt; PT: P-tuning).
14
15

LAMA-29k. We again follow Shin et al. (2020) to
construct the training, development, and test data
to allow for fair comparison.
20
Setup. LAMA has provided a handcraft prompt
for each relation, as shown in Table 1, which are
effective but likely sub-optimal. For bidirectional
masked language models, we only need to replace
“[X]” with the subject entity and “[Y]” with the
[MASK] token; for unidirectional language models
such as GPT, following LAMA’s original setting
on Transformer-XL (Dai et al., 2019), we use the
network output just before the target position.
31
The number of prompt tokens and positions are
selected based on the development sets, and for
simplicity we choose the (3, sub, org_prompt, 3,
obj, 3) template for bidirectional models and (3,
sub, org_prompt, 3, obj) for unidirectional models
as this configuration performs well for most rela-
tions (where the number indicates the number of
continuous prompt tokens). Continuous prompts
are concatenated with original discrete prompts.
During the prompt training, we set the learning rate
to 1e-5 and use the Adam optimizer.
45
46
47	3.1.2  Main results
49
The results are presented in Table 3. P-tuning sig-
nificantly improves the best results of knowledge
probing from 43.3% to 50.6% on LAMA-34k and
from 45.2% to 64.2% on LAMA-29k. Moreover,
P-tuning outperforms previous discrete prompt
searching approaches such as AutoPrompt (Shin
et al., 2020) and LPAQA (Jiang et al., 2020b) on
the same-size models. This confirms our intuition
in Section 2 that discrete prompts might not be
optimal.
Fully-supervised Learning
Setup
Dataset. To evaluate P-tuning on fully-supervised learning tasks, we adopt the SuperGLUE bench- mark (Wang et al., 2019b), consisting of 8 challeng- ing natural language understanding (NLU) tasks. We focus on 7 of them since the ReCoRD (Zhang et al., 2018) task adopts no discrete prompts, thus P-tuning is not directly applicable. The tasks in- clude question answering (BoolQ (Clark et al., 2019a) & MultiRC (Khashabi et al., 2018)), tex- tual entailment (CB (De Marneffe et al., 2019) & RTE (Dagan et al., 2005)), co-reference resolution (WiC (Pilehvar and Camacho-Collados, 2018)), causal reasoning (COPA (Roemmele et al., 2011)), and word sense disambiguation (WSC (Levesque et al., 2012)).
Comparison methods. We experiment with P- tuning on both unidirectional and bidirectional pretrained models, i.e., GPT and BERT. We include four variants BERT-Base, BERT-Large, GPT2-Base, and GPT-medium. For each model, we compare standard classification finetuning, PET (Schick and Schütze, 2020) (a typical fine- tuning method based on manual discrete prompts) and our P-tuning.
Configuration.  We use the same metrics as in (Wang et al., 2019b). For fully-supervised learn- ing, we use a large training set to finetune pre- trained models and use a development set for hyper- parameter and model selection. Specifically, the AdamW optimizer with a linearly decayed learn- ing rate is used for training. We use a learning rate of {1e − 5, 2e − 5, 3e − 5}, a batch size of
{16, 32}, and a warm-up ratio of {0.0, 0.05, 0.1}. For small datasets (i.e., COPA, WSC, CB, RTE),







(a) Fully-supervised performance with base-scale models.

1
2
3
4
5
6
7
8
9
10	(b) Fully-supervised performance with large-scale models.
11
12
13
14
15
16
17
18
19
20	1 We report the same results taken from SuperGLUE (Wang et al., 2019a).
21
22	Table 4: Fully-supervised performance on SuperGLUE development set.
24

25	we fine-tune pretrained models for 20 epochs. For
larger datasets (i.e., WiC, BoolQ, MultiRC), we
reduce the number of training epochs to be 10 as
the model converges earlier. Early stopping is used
to avoid over-fitting the training data.
32	3.2.2  Main Results
The main results of fully-supervised learning are
shown in Table 4. We observe that P-tuning can
improve fully-supervised learning performance on
both BERTs and GPTs. (1) Specifically, on the
BERT-Base model, P-tuning achieves best perfor-
mance on 5/7 tasks, while with BERT-Large, P-
tuning outperforms other methods on 4/7 tasks.
The exceptions are WiC and MultiRC, both of
which have relatively large training sets. We find
that P-tuning might not have large gains over CLS-
FT on such high-resource tasks, while benefits
more on low-resource tasks. On average, P-tuning
improves over the considered baselines. (2) On
GPT2-Base and GPT2-Medium models, P-tuning
consistently achieves the best performance on all
tasks.
53
54	3.3  Few-Shot Learning
55
While GPT-3 has shown decent few-shot learning
potential with handcrafted prompts, it still struggles
on some of the challenging tasks (e.g., natural lan-
guage inference) (Brown et al., 2020). We are mo-
tivated to study whether P-tuning can also improve
the few-shot learning performance of pretrained models on challenging tasks.
Setup
Few-shot Evaluation. The few-shot performance is sensitive to lots of factors (e.g., the order of train- ing examples, random seed, and prompt patterns), and thus suffers from high variance (Zhao et al., 2021a; Lu et al., 2021; Zhang et al., 2020). There- fore, the few-shot evaluation strategy should make sure that the improvements are indeed from an im- proved method instead of variance. To this end, we follow the FewNLU evaluation procedure (Zheng et al., 2021) that has addressed and handled the issue. Specifically, we use random data splits to perform model selection only on a small labeled set to prevent overfitting a large dev set.
Dataset. We use the few-shot SuperGLUE (also known as FewGLUE) benchmark (Schick and Schütze, 2020) and follow the setting in prior work (Zheng et al., 2021) in terms of data split construc- tion.
Baseline and Hyper-parameter. In few-shot learn- ing, we again compare P-tuning with PET (Schick and Schütze, 2020), which was shown to out- perform GPT-3 on some of the tasks. Similar to (Schick and Schütze, 2020), we use ALBERT- xxLarge as the base model. For hyper-parameters that are shared by PET and P-tuning (e.g., learn-







ing rate, maximum training step, evaluation fre-
quency), we use the same search space for fair
comparison. Specifically, we search the learning
rate in {1e − 5, 2e − 5}, the maximum training
step in {250, 500}, and the evaluation frequency in
6	{0.02, 0.04}.
7	Construction of Prompt Patterns. For PET, we
use the same manual prompts reported by Schick
and Schütze (2020). When constructing prompt
patterns for P-tuning, based on the same manual
prompts as PET, we insert different numbers of
continuous prompt tokens into different positions,
thus formulating a number of pattern candidates.
We then select the best pattern for P-tuning using
the validation strategy of FewNLU (Zheng et al.,
2021). We also conduct further analysis of the num-
ber and the position of continuous prompt tokens
in §3.3.3.
22
23	3.3.2  Main Results
24	Few-Shot Performance. Table 5 shows the main
results of few-shot learning. We find that, on AL-
BERT, P-tuning consistently outperform PET on
average by more than 1 points. It outperforms
PromptTuning by more than 13 points. It proves
that by automatically learning continuous prompt
tokens, the pretrained models can achieve better
few-shot performance on NLU tasks.
34
35	3.3.3  Ablation Study
36
Type of Prompt Encoder Prior work (Shin et al.,
2020) proposes to simply use an MLP as the prompt
encoder, we perform further ablation analysis for
prompt encoder selection, and results are shown
in Table 8. We consider LSTM, MLP, and EMB
(i.e., we directly optimize the word embeddings
without using additional parameters). From the
results, we can see that LSTM, MLP, and EMB
all work as a prompt encoder. Results show that
both LSTM and MLP generally work well on these
tasks, while EMB is unstable and can substantially
under-perform the other two on some tasks (e.g,.
WiC and CB). To sum up, both LSTM and MLP
could be taken into account when working on new
tasks.
Location of Prompt Tokens To study at which
location to insert continuous prompt tokens, we
perform experiments as Table 7 shows. From the
results, we have the following findings.
1. By comparing #1 (or #2) with #3 (or #4), we find
that it would be better if we insert continuous prompt tokens at the location where it does not segment the sentences. For example, in case#1, “[P]” breaks the completeness of sentence “[Hy- pothesis]?” while in case#3, “[P]” is located between sentences.
By comparing #2 (or #3) with #4, we find that there’s no special preference for placing on the edge or in the middle of the inputs.
It is suggested to write a number of pattern can- didates and then search over them for the best for each task.
Number of Prompt Tokens We also study the in- fluence of the number of prompt tokens and show the results in Table 7. By comparing #3, #6, #7, and #8, we can conclude that the number of prompt tokens has a great impact on the few-shot perfor- mance. However, it is not that a larger number of prompt tokens would always be better. We conjec- ture that it could be that due to the limited training data, it becomes difficult to learn the parameters when excessively increasing the number of contin- uous prompt tokens. In practice, it is suggested to search for the best number of prompt tokens through model selection.

3.3.4 Comparison with Discrete Prompt Search
Prior work (Gao et al., 2020) proposed to automati- cally search discrete prompts and achieved better results than those of manual prompts. We now proceed to compare P-Tuning with auto-searched discrete prompts. For fair comparison, we follow the setting of LM-BFF (Gao et al., 2020) to also conduct experiments on some of the GLUE tasks (Wang et al., 2018) with RoBERTa-Large model (Liu et al., 2019). Since the the evaluation proto- cols have large impacts on few-shot performance, we use the top-3 discrete prompts searched by LM- BFF and experiment with using only the discrete prompts and additionally applying P-Tuning. For P-Tuning, the prompt patterns are constructed by concatenating the same discrete prompts as well as continuous prompts. Results in Table 9 show that additionally incorporating continuous prompts can further improve few-shot performance. P-Tuning is easy to be combined with existing discrete prompts, while further improving stability as discussed in Section 3.4.









1
2
3
4
Table 5: The few-shot performance of PET (Schick and Schütze, 2020), Prompt Tuning (Lester et al., 2021) and our
P-tuning over seven tasks based on ALBERT. Each result is averaged over 4 runs with different data splits. Results
show that P-tuning consistently improves average few-shot performance by more than 1 point compared to PET and
by more than 13 points compared to Prompt Tuning.
10		
11
12
13
14
15
16
17
18
Table 6: Upper table: Few-shot learning (FSL) of PET and P-tuning in terms of each pattern on SuperGLUE with
ALBERT; Lower table: Manual prompt (MP) and P-tuning performance on LAMA-P17 with BERT-base-cased.
For each column, P-tuning and compared methods share the same manual prompts, while P-tuning additionally
concatenates continuous prompt tokens. We report the standard deviation over multiple results of different patterns.
Results show that P-tuning achieves smaller standard deviation, proving that P-tuning can improve stability w.r.t.
the choice of discrete patterns.
26
27

3.4  Stabilizing Language Model Adaptation
In the above sections, we have shown that P-Tuning
improves over performance across multiple set-
tings. Now we present results to demonstrate that
P-Tuning also stabilizes language model adapta-
tion; i.e., reducing the differences between differ-
ent prompts. As we have shown in Table 1, manual
prompts have a large impact on the performance.
When it comes to few-shot learning, the perfor-
mance gap of different prompts is prominent due
to the sensitivity of few-shot learning (Zheng et al.,
2021). Results in Table 6 show that P-tuning im-
proves the performance of the worst-performing
patterns (e.g., P#5), and achieves a smaller stan-
dard deviation over multiple patterns. Compared to
PET-FT, P-tuning increases the stability w.r.t. the
choice of patterns.
On LAMA, we observe similar a phenomenon
that while manual prompts often yield quite volatile
results, appending trainable continuous prompts on
top of the manual prompts can stabilize their per-
formances, reducing the standard deviation from
10.1 to 0.46.
56
57	4  Related work
58
Language Model Prompting.  GPT-3 (Brown
et al., 2020) uses in-context examples (Liu et al.,
2021; Zhao et al., 2021b) as a way of prompting to transfer knowledge from pretraining to downstream tasks. Schick and Schütze (2020) proposed to use cloze patterns, which removes the constraint that the masked token is the last token of the sentence. This further minimizes the gap between pretrain- ing and downstream tasks. To improve prompting for NLU, recent works have proposed methods to automatically search for high-performing prompts by mining the training corpus (Jiang et al., 2020b), gradient-based search (Shin et al., 2020), or using pretrained generative models (Gao et al., 2020). Our approach is different from these prior works in that we resort to using continuous prompt em- beddings, which are found to be complementary to discrete prompts in our experiments.
Recently, some concurrent works also proposed the use of continuous prompts. Prefix-tuning (Li and Liang, 2021) adds continuous prompts at the beginning of the sequence for each layer. In con- trast to our work, prefix-tuning targets natural lan- guage generation tasks.
In the area of NLU, a few concurrent methods were proposed based on continuous prompts, fo- cusing on improving knowledge probing (Qin and Eisner, 2021; Zhong et al., 2021). Lester et al. (2021) showed that with large pretrained models, only tuning continuous prompts with a frozen lan-









1
2
3
4
5
6
7
8
9
Table 7: The few-shot performance of P-tuning on the CB task on ALBERT with different prompt patterns. “Seg.”
means whether the inserted prompt tokens segment complete sentences. “Pos.” indicates inserting the prompt tokens
at the edge or in the middle of the inputs. “[P]” is continuous prompt token. “[M]” is the mask token.
13

14		
15
16
17
18
19
20
21
Table 8: The few-shot performance on WiC, CB and
BoolQ tasks with ALBERT using different prompt en-
coders. Results show that both LSTM and MLP gener-
ally work well on these tasks, while EMB is unstable
and can substantially under-perform the other two on
some tasks (e.g,. WiC and CB). “EMB” means using an
identity mapping for the prompt encoder.
29
 	
Task	LM-BFF (Auto)  P-Tuning
32
33
34
35
36	Table 9:	Few-shot performance of automatically
searched prompts and P-Tuning. We evaluated LM-
BFF (Auto) using the reported top-3 searched patterns
under our evaluation procedure. P-Tuning also uses
the same discrete prompts, in concatenation with con-
tinuous prompts. Results show that P-Tuning can be
effectively combined with existing discrete patterns and
achieve further performance improvement.
45
46
guage model achieves comparable performance to
full-model tuning.
Compared to these concurrent works on NLU,
P-Tuning reaches a unique conclusion that contin-
uous prompts improve performance and stabilize
training with either frozen or tuned models under
both the few-shot and fully-supervised settings. For
example, no concurrent works have shown that
continuous prompts can improve performance with
a tuned language model. Technically, P-Tuning
also has a few unique designs such as using hy-
brid continuous-discrete prompts and employing a
prompt encoder.

Knowledge in Language Models. Self- supervised (Liu et al., 2020) pre-trained language models (Han et al., 2021) including GPT (Rad- ford et al., 2019), BERT (Devlin et al., 2018), XL- Net (Yang et al., 2019), RoBERTa (Liu et al., 2019) have been observed to learn not only contextual- ized text representations but also linguistic and world knowledge. (Hewitt and Manning, 2019) demonstrates that contextualized representations produced by language models can form a parse tree in the embedding space. (Vig, 2019; Clark et al., 2019b) look into the multi-head attention patterns within transformers and discover that certain atten- tion heads may correspond to some grammatical functions, including co-reference and noun modi- fiers. LAMA (Petroni et al., 2019, 2020) propose the LAMA task that leverages cloze tests to pre- dict the fact triples of knowledge bases to examine language model’s ability of memorizing facts with answers in the single-token format. In (Wang et al., 2020), the authors investigate the attention matrices to find evidence about knowledge triples contained in the context. (Jiang et al., 2020a) develops a multi-token fact retrieval dataset based on LAMA.


5  Conclusions

In this paper, we present a method P-Tuning that uses continuous prompts in concatenation with dis- crete prompts. P-Tuning improves performance and stabilizes training for pretrained language model adaptation. P-Tuning is effective with both tuned and frozen language models under both the few-shot and fully-supervised setings.






References
Tom B Brown, Benjamin Mann, Nick Ryder, Melanie
Subbiah, Jared Kaplan, Prafulla Dhariwal, Arvind
Neelakantan, Pranav Shyam, Girish Sastry, Amanda
Askell, et al. 2020. Language models are few-shot
learners. arXiv preprint arXiv:2005.14165.
6
Christopher Clark, Kenton Lee, Ming-Wei Chang,
Tom Kwiatkowski, Michael Collins, and Kristina
Toutanova. 2019a. Boolq: Exploring the surprising
difficulty of natural yes/no questions. In Proceedings
of the 2019 Conference of the North American Chap-
ter of the Association for Computational Linguistics:
Human Language Technologies, Volume 1 (Long and
Short Papers), pages 2924–2936.
Kevin Clark, Urvashi Khandelwal, Omer Levy, and
Christopher D Manning. 2019b. What does bert look
at? an analysis of bert’s attention. arXiv preprint
arXiv:1906.04341.
Ido Dagan, Oren Glickman, and Bernardo Magnini.
2005. The pascal recognising textual entailment chal-
lenge. In Machine Learning Challenges Workshop,
pages 177–190. Springer.
24
Zihang Dai, Zhilin Yang, Yiming Yang, Jaime Car-
bonell, Quoc V Le, and Ruslan Salakhutdinov.
2019.  Transformer-xl: Attentive language mod-
els beyond a fixed-length context. arXiv preprint
arXiv:1901.02860.
Marie-Catherine De Marneffe, Mandy Simons, and Ju-
dith Tonhauser. 2019. The commitmentbank: Inves-
tigating projection in naturally occurring discourse.
In proceedings of Sinn und Bedeutung, volume 23,
pages 107–124.
35
Jacob Devlin, Ming-Wei Chang, Kenton Lee, and
Kristina Toutanova. 2018. Bert: Pre-training of deep
bidirectional transformers for language understand-
ing. arXiv preprint arXiv:1810.04805.
40
Tianyu Gao, Adam Fisch, and Danqi Chen. 2020.
Making pre-trained language models better few-shot
learners. arXiv preprint arXiv:2012.15723.
Xu Han, Zhengyan Zhang, Ning Ding, Yuxian Gu, Xiao
Liu, Yuqi Huo, Jiezhong Qiu, Liang Zhang, Wentao
Han, Minlie Huang, et al. 2021. Pre-trained models:
Past, present and future. AI Open.
John Hewitt and Christopher D. Manning. 2019. A
structural probe for finding syntax in word representa-
tions. In North American Chapter of the Association
for Computational Linguistics: Human Language
Technologies (NAACL). Association for Computa-
tional Linguistics.
55
Zhengbao Jiang, Antonios Anastasopoulos, Jun Araki,
Haibo Ding, and Graham Neubig. 2020a. X-factr:
Multilingual factual knowledge retrieval from pre-
trained language models. In Proceedings of the 2020
Conference on Empirical Methods in Natural Lan-
guage Processing (EMNLP), pages 5943–5959.
Zhengbao Jiang, Frank F Xu, Jun Araki, and Graham Neubig. 2020b. How can we know what language models know? Transactions of the Association for Computational Linguistics, 8:423–438.
Daniel Khashabi, Snigdha Chaturvedi, Michael Roth, Shyam Upadhyay, and Dan Roth. 2018. Looking beyond the surface: A challenge set for reading com- prehension over multiple sentences. In Proceedings of the 2018 Conference of the North American Chap- ter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Pa- pers), pages 252–262.
Brian Lester, Rami Al-Rfou, and Noah Constant. 2021. The power of scale for parameter-efficient prompt tuning. ArXiv, abs/2104.08691.
Hector Levesque, Ernest Davis, and Leora Morgenstern. 2012. The winograd schema challenge. In Thir- teenth International Conference on the Principles of Knowledge Representation and Reasoning. Citeseer.
Xiang Lisa Li and Percy Liang. 2021. Prefix-tuning: Optimizing continuous prompts for generation. arXiv preprint arXiv:2101.00190.

Jiachang Liu, Dinghan Shen, Yizhe Zhang, Bill Dolan, Lawrence Carin, and Weizhu Chen. 2021. What makes good in-context examples for gpt-3? arXiv preprint arXiv:2101.06804.
Xiao Liu, Fanjin Zhang, Zhenyu Hou, Zhaoyu Wang, Li Mian, Jing Zhang, and Jie Tang. 2020. Self- supervised learning: Generative or contrastive. arXiv preprint arXiv:2006.08218, 1(2).
Yinhan Liu, Myle Ott, Naman Goyal, Jingfei Du, Man- dar Joshi, Danqi Chen, Omer Levy, Mike Lewis, Luke Zettlemoyer, and Veselin Stoyanov. 2019. Roberta: A robustly optimized bert pretraining ap- proach. arXiv preprint arXiv:1907.11692.
Yao Lu, Max Bartolo, Alastair Moore, Sebastian Riedel, and Pontus Stenetorp. 2021. Fantastically ordered prompts and where to find them: Over- coming few-shot prompt order sensitivity. CoRR, abs/2104.08786.
Fabio Petroni, Patrick Lewis, Aleksandra Piktus, Tim Rocktäschel, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2020. How context affects lan- guage models’ factual predictions. arXiv preprint arXiv:2005.04611.
Fabio Petroni, Tim Rocktäschel, Patrick Lewis, An- ton Bakhtin, Yuxiang Wu, Alexander H Miller, and Sebastian Riedel. 2019. Language models as knowl- edge bases? arXiv preprint arXiv:1909.01066.
Mohammad Taher Pilehvar and José Camacho-Collados. 2018. Wic: 10, 000 example pairs for eval- uating context-sensitive representations. CoRR, abs/1808.09121.







Guanghui Qin and J. Eisner. 2021. Learning how to ask:
Querying lms with mixtures of soft prompts. ArXiv,
abs/2104.06599.
3
Alec Radford, Jeffrey Wu, Rewon Child, David Luan,
Dario Amodei, and Ilya Sutskever. 2019. Language
models are unsupervised multitask learners. OpenAI
blog, 1(8):9.
Colin Raffel, Noam Shazeer, Adam Roberts, Katherine
Lee, Sharan Narang, Michael Matena, Yanqi Zhou,
Wei Li, and Peter J Liu. 2019. Exploring the limits
of transfer learning with a unified text-to-text trans-
former. arXiv preprint arXiv:1910.10683.
13
Melissa Roemmele, Cosmin Adrian Bejan, and An-
drew S Gordon. 2011. Choice of plausible alter-
natives: An evaluation of commonsense causal rea-
soning. In AAAI Spring Symposium: Logical Formal-
izations of Commonsense Reasoning, pages 90–95.
19
Timo Schick and Hinrich Schütze. 2020. It’s not just
size that matters: Small language models are also
few-shot learners. Computing Research Repository,
arXiv:2009.07118.
24
Taylor Shin, Yasaman Razeghi, Robert L Logan IV,
Eric Wallace, and Sameer Singh. 2020. Autoprompt:
Eliciting knowledge from language models with
automatically generated prompts.  arXiv preprint
arXiv:2010.15980.
30
Jesse Vig. 2019.  A multiscale visualization of at-
tention in the transformer model. arXiv preprint
arXiv:1906.05714.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R Bowman. 2019a. Superglue: A stickier
benchmark for general-purpose language understand-
ing systems. arXiv preprint arXiv:1905.00537.
Alex Wang, Yada Pruksachatkun, Nikita Nangia, Aman-
preet Singh, Julian Michael, Felix Hill, Omer Levy,
and Samuel R. Bowman. 2019b. SuperGLUE: A
Stickier Benchmark for General-Purpose Language
Understanding Systems. In NeurIPS 2019, pages
3261–3275.
46
Alex Wang, Amanpreet Singh, Julian Michael, Felix
Hill, Omer Levy, and Samuel R. Bowman. 2018.
Glue: A multi-task benchmark and analysis plat-
form for natural language understanding.  ArXiv,
abs/1804.07461.
52
Chenguang Wang, Xiao Liu, and Dawn Song. 2020.
Language models are open knowledge graphs. arXiv
preprint arXiv:2010.11967.
56
Zhilin Yang, Zihang Dai, Yiming Yang, Jaime Car-
bonell, Ruslan Salakhutdinov, and Quoc V Le.
2019. Xlnet: Generalized autoregressive pretrain-
ing for language understanding.  arXiv preprint
arXiv:1906.08237.
Sheng Zhang, Xiaodong Liu, Jingjing Liu, Jianfeng Gao, Kevin Duh, and Benjamin Van Durme. 2018. Record: Bridging the gap between human and ma- chine commonsense reading comprehension. arXiv preprint arXiv:1810.12885.
Tianyi Zhang, Felix Wu, Arzoo Katiyar, Kilian Q. Wein- berger, and Yoav Artzi. 2020. Revisiting few-sample BERT fine-tuning. CoRR, abs/2006.05987.
Tony Z. Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021a. Calibrate before use: Im- proving few-shot performance of language models. CoRR, abs/2102.09690.
Tony Z Zhao, Eric Wallace, Shi Feng, Dan Klein, and Sameer Singh. 2021b. Calibrate before use: Improv- ing few-shot performance of language models. arXiv preprint arXiv:2102.09690.
Yanan Zheng, Jing Zhou, Yujie Qian, Ming Ding, Jian Li, Ruslan Salakhutdinov, Jie Tang, Sebastian Ruder, and Zhilin Yang. 2021. Fewnlu: Benchmarking state- of-the-art methods for few-shot natural language un- derstanding.
Zexuan Zhong, Dan Friedman, and Danqi Chen. 2021. Factual probing is [mask]: Learning vs. learning to recall. ArXiv, abs/2104.05240.











Declaratiio if ioterettt

☒ The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.

The authors declare the following financial interests/personal relationships which may be considered as potential competing interests:
