Artificial Intelligence in Agriculture 5 (2021) 142–156











Image processing algorithms for in-field cotton boll detection in natural lighting conditions
Naseeb Singh a,⁎, V.K. Tewari a, P.K. Biswas b, C.M. Pareek a, L.K. Dhruw a
a Department of Agricultural and Food Engineering, IIT Kharagpur, Kharagpur 721 302, India
b Department of Electronics and Electrical Communication Engineering, IIT Kharagpur, Kharagpur 721 302, India



a r t i c l e	i n f o

Article history:
Received 19 April 2021
Received in revised form 5 July 2021 Accepted 8 July 2021
Available online 10 July 2021

Keywords:
Cotton recognition Image segmentation Color models
Color thresholding
a b s t r a c t

In developing countries, the cotton harvesting operation is currently being performed manually. Due to the mo- notonous nature of this task and the involvement of a considerable amount of labor, this operation becomes very tedious and costly. The harvesting robots can be a good alternative for the selective picking of cotton bolls from the field. In this study, an attempt has been made to develop the image processing algorithms for in-field cotton boll detection in natural lighting conditions for the cotton harvesting robot. Four image processing algorithms namely color difference, band ratio, YCbCr method, and chromatic aberration were proposed for the real-time segmentation of cotton bolls under natural outdoor light conditions. The performance of developed image pro- cessing algorithms was evaluated and the experimental results revealed that the chromatic aberration method outperforms as compared to other developed algorithms. The chromatic aberration method showed the highest identification rate of 91.05% with false positive and false negative rates of 6.99% and 4.88% respectively, among all the proposed algorithms. The highest sensitivity and specificity were found to be 81.31% and 97.53%, respectively, using the chromatic aberration method. Overall, the chromatic aberration approach demonstrated a very prom- ising performance for in-field cotton bolls detection under natural lighting conditions which confirms its appli- cability for the robotic cotton harvesters.
© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Introduction

Cotton, a major source of natural fiber, is an important cash crop in India (Konduru et al., 2013), harvesting of which is a monotonous task and currently being performed manually. The mechanization level of cotton harvesting in India is nil (Mehta et al., 2019) and at the same time, labor cost is increasing as the percentage of agricultural workers to total workers decreased from 59.1% in 1991 to 54.6% in 2011 and projected to be 40.6% in 2020 (Mehta et al., 2019). As manual picking is tedious and costly, human workers can be replaced by harvesting ro- bots which can reduce the harvesting cost (Arefi et al., 2011). Therefore, harvesting robots can be a good alternative for the selective picking of cotton bolls.
In the past, numerous researchers have studied robotic fruit harvest- ing; for instance, tomato harvesting (Kondo et al., 1996, 2010; Lee et al., 1999; Monta et al., 1998; Yabo et al., 2016; Zhao et al., 2016b), straw- berry harvesting (Han et al., 2012; Hayashi et al., 2010; Xiong et al., 2019; Yamamoto et al., 2010), cucumber harvesting (Henten et al., 2009, 2006, 2003, 2002), apple harvesting (Baeten et al., 2008; Bulanon and Kataoka, 2010; De-An et al., 2011; Li et al., 2016b;

* Corresponding author.
E-mail address: naseeb501@gmail.com (N. Singh).
Nguyen et al., 2013; Silwal et al., 2017), cherry harvesting (Kondo et al., 1996; Tanigaki et al., 2008), sweet pepper harvesting (Bac et al., 2016; Hemming et al., 2014), citrus harvesting (Cai et al., 2009; Edan et al., 1990; Harrell et al., 1990; Mehta and Burks, 2014; Wang et al., 2019), kiwifruit harvesting (Mu et al., 2020; Scarfe et al., 2009; Williams et al., 2019), etc.
The first major task of a harvesting robot is to recognize the fruit using machine vision (Willigenburg et al., 2004), in which digital im- ages of harvesting scenes captured using cameras and fruits were de- tected using image processing algorithms. In the past, image processing was used for weed recognition (Dammer, 2016; Lee et al., 1999; Tang et al., 2016; Zhang et al., 2016; Zheng et al., 2017), plant dis- eases identification (Camargo and Smith, 2009; Lu et al., 2017; Singh, 2019; Singh and Misra, 2017; Tewari et al., 2020), leaf area determina- tion (Chaudhary et al., 2012; Nyakwende et al., 1997; Vázquez- Arellano et al., 2018), etc.
In agriculture, image processing was used by numerous re- searchers for fruit recognition. Bulanon et al. (2002a) in their study concluded that out of the RGB model, rg-chromaticity method, and Lu- minance and Red Color Difference method, rg-chromaticity method is the most suitable for recognition of Fuji apples. In another study, Bulanon et al. (2002b) detect the Fuji apples with a success rate of over 88% by using the difference between luminance and red color,


https://doi.org/10.1016/j.aiia.2021.07.002
2589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/).






while Ji et al. (2012) used color features as well as shape features to detect apples and achieved a recognition rate of 93.3%. For the detec- tion of strawberries, Xiong et al. (2019) used the RGB color model and achieved a precision and recall value of 0.72. Bachche and Oka (2013) tested various color space models and successfully detected sweet peppers with a recognition rate of 84% by using the HSV color space model. Zhao et al. (2016a) detected 93% of the targeted tomatoes in their study. To pick oranges, a vision system for a harvesting robot was developed by Hannan et al. (2010) in which the R/(R + G + B) feature was used for the identification of oranges. Arefi et al. (2011) used a combination of RGB, HIS, and YIQ color spaces to recog- nize the ripen tomato with an accuracy of 96.36%. Using RGB color model, Putra and Soni (2018) developed several vegetation indices to estimate photosynthetic pigments and Nitrogen critical level of Ro- busta Coffee plant and achieved coefficient of determination (R2) as high as 0.8536 using their proposed indices.
For a cotton-picking robot, the first task will be the recognition of cotton bolls in the field. The correct identification of cotton bolls in natural illumination conditions is very crucial for robotic cotton har- vesting. In-field cotton detection is a challenging task due to various sizes and irregular shapes of cotton bolls. Illumination conditions also make cotton segmentation more challenging as colors of cotton bolls can be bright during high illumination and dull during low illu- mination (during the evening or cloudy conditions), which makes cot- ton boll detection more difficult. In agriculture, segmentation of the region of interest under varying illumination is always a challenging task because images can contain shadowed and lighted parts which can produce poor threshold results. Putra and Soni (2020) enhanced the measurement accuracy of chlorophyll and nitrogen content under uncertain natural light conditions using a consumer-grade RGB digital camera. Putra and Soni (2017) satisfactorily assessed bio- physical properties of vegetation like plant phenology under different illumination. In recent times, various image processing techniques were also successfully used for the detection of cotton bolls. Jin- shuai et al. (2011) achieved an accuracy of 90.44% for cotton boll de- tection by using YCbCr color space and fisher discrimination analysis. Li et al. (2016a) used the region-based semantic image segmentation method for in-field cotton detection and compare their proposed
method with other well-established methods. Their proposed method yields the highest performances of 77.3%, 99.3%, and 97.0% for sensi- tivity, specificity, and accuracy, respectively. An unsupervised domain adaption method called NCADA for in-field identification of cotton bolls was proposed by Li et al. (2020) and reported efficiency of 86.4%. Wang et al. (2008) detect 85% of cotton bolls correctly by using the color subtraction method and freeman chain coding to re- move noises. Using ultra-fine spatial resolution UAV images, Yeom et al. (2018) detect the open cotton bolls with an accuracy of 88%. To increase the harvesting accuracy of cotton harvesting robots, there is a necessity to develop new methods and indices which can identify the in-field cotton bolls in different illumination conditions with minimum image processing time as well as with minimum pos- sible false positive and false negative errors.
Hence, this study aimed to develop cotton segmentation algorithms for natural illumination conditions such that with single threshold value in an algorithm, cotton boll can be detected in morning, afternoon or evening time with minimum errors and evaluate them based on image processing time and segmentation accuracy. For this, four new methods of cotton segmentation were introduced in addition to other existing methods (Jin-shuai et al., 2011; Li et al., 2020; Li et al., 2016a; Wang et al., 2008) which can be used for cotton harvesting robots to recognize cotton bolls. After analyzing four color spaces, RGB and YCbCr color spaces were used for cotton segmentation in natural light conditions, optimum thresholds were selected and morphological opening and closing operations were performed to remove noises and holes in the segmented binary image. The performance of proposed algorithms was evaluated in terms of hits rate, false positive, false neg- ative, processing time, sensitivity, specificity, and accuracy.

Methodology

Data collection

In this study, a total 135 number of RGB color images (640 × 480 pixels) were captured under varying illumination conditions (Table 1). A 0.922-megapixel digital camera (Logitech Webcam C270) that used a Complementary Metal Oxide Semiconductor (CMOS) as an image sen- sor having a focal length of 4.6 mm and diagonal field of view of 55°, and a laptop (8 GB RAM, Intel Core i5 CPU, and Windows 10 operating sys- tem) were used for the cotton image acquisition process. We used dig- ital camera with a configuration of fixed focus, automatic exposure time, and custom white balance mode. Image acquisition and image process- ing were performed in Matlab® ver. R2017 (Mathworks Inc., Natick, MA. USA) (Beale et al., 2017).

Color model analysis

The cotton bolls can be detected easily based on their color features. The commonly popular color space models for feature segmentation from color images includes the RGB (Lurstwut and Pornpanomchai, 2017; Tewari et al., 2020), the HSV (Yang et al., 2015), the L*a*b (Hu


Table 1
Number of images captured with varying distance and illumination.




et al., 2015; Zhao et al., 2016a), and the YCbCr (Jin-shuai et al., 2011) color models. The selection of a proper color space model is very crucial for the successful detection of the particular object from the given color images. In past, various researchers have implemented different color
space models for various agricultural applications (Bachche and Oka,


mx(i,j) = max IR(i,j), IG(i,j), IB(i,j)	(1)


mn(i,j) = min IR(i,j), IG(i,j), IB(i,j)	(2)

2013; Hamuda et al., 2017; Tewari et al., 2020). In these studies, image processing was not carried out under natural illumination condi-
60 × IG(i,j)−IB(i,j)	I
mx−mn

R(i,j)> max I
G(i,j), IB(i,j) 1
C

tions and hence, challenges for image segmentation under natural illu- mination were not addressed. Therefore, in this study, four-color
H(i, j) =	180 × IB(i,j)−IR(i,j)	I
mx−mn
B@
G(i,j)> max I

R(i,j), IB(i,j) 
CA
(3)

space models, i.e., RGB, normalized RGB (sRGB), HSV, and YCbCr were
300 × IR(i,j)−IG(i,j)	I
> max I	, I

analyzed to determine the suitable color space model for in-field cotton bolls segmentation under natural illumination conditions. Initially, the
mx−mn
B(i,j) 
R(i,j) G(i,j) 

captured RGB images were converted into different color spaces using the Eqs. (1)–(7). Then the color channels of cotton bolls, stems, and leaves were extracted for each color space model and plotted as a boxplot for analysis as shown in Fig. 1. From Fig. 1, it can be observed that the cotton bolls color components are comparatively more distin- guishable from that of the stems and leaves in the case of RGB and YCbCr color spaces as compared to the sRGB and HSV color space model. Therefore, in this study, the RGB and the YCbCr color spaces
S(i, j) = hmx−mni	(4)
V (i, j) = [mx]	(5)

The normalized color r, g, and b in RGB color space are defined using Eq. (6) (Yang et al., 2015), where R, G, and B are the color components of the cotton field image.

were selected for developing the image processing algorithms for in-
field cotton detection.
The Eqs. (1)–(5) are used to transform the RGB color space into HSV
r =	R
R + G + B
, g =		G  R + G + B
, b =	B
R + G + B
, r + g + b = 1	(6)

color space (Camargo and Smith, 2009). Given that (Ii,j) exists in RGB color space, then:
The conversion between RGB color space and YCbCr color space is described by Eq. (7) (Shaik et al., 2015).





Fig. 1. Color channels of cotton bolls, stems, and leaves in RGB, sRGB, HSV, and YCbCr color space models.



2 Y 3
2 0 3
2 0.299	0.587	0.114
32 R 3
Color component ratio method
In this method, the ratios of R, G, and B components of cotton, stem,

64 Cb 75 = 64 128 75 + 64 −0.169  −0.331	0.500
7564 G 75	(7)
and leaves were studied to segment cotton bolls from the background.

Cr	128
0.500	−0.419  −0.081	B
Color component ratios for cotton bolls, stems, and leaves were com- puted as given in Eqs. (9) to (11).

where, Y ϵ [16,235], Cb ϵ [16,240], Cr ϵ [16,240].

Cotton bolls segmentation algorithms

In this study, two-color space models, i.e., RGB and YCbCr were se- lected and further utilized for developing in-field cotton detection algo- rithms. In past, many researchers used RGB (Bulanon et al., 2002b; Ji et al., 2016; Xiang et al., 2011; Xu and Ying, 2004; Zhao et al., 2005) and YCbCr (Moallem et al., 2017; Sabzi et al., 2020, 2017) color space models to extract the region of interest using color thresholding seg- mentation in different agricultural applications. A total of four image processing algorithms were developed, out of which, three algorithms
rRB = R/B	(9)

rBG = B/G	(10)

rRG = R/G	(11)

These ratios were plotted against pixels' numbers as shown in Fig. 3 and it can be observed that the rBG values for cotton bolls were isolated from the rBG values of stem and leaves. Hence, the rBG values were fur- ther utilized for segmenting the cotton bolls from the stem and leaves. The threshold function for cotton bolls segmentation using rBG value was defined by Eq. (12).

based on the color difference method, the color component ratio method, and the chromatic aberration method, were developed by uti- lizing the RGB color model, and one algorithm was developed by utiliz-
f (x, y) =  (1, 1, 1)
(0, 0, 0)
if rBG ≥ T if rBG < T
(12)

ing the YCbCr color model. The working flowcharts of all the proposed cotton bolls detection algorithms were shown in Fig. 9. The details of each cotton boll detection algorithm are given in the following sections.

Color difference method
To develop the cotton bolls detection algorithm based on the color difference method, initially, the distribution of R vs. G, R vs. B, and B vs. G for color data of cotton bolls, leaves, and stems were plotted, as shown in Fig. 2. From this figure, it can be observed that the distribution of B vs. G values for cotton bolls are isolated from the B vs. G values of leaves and stem whereas, in R vs. G and R vs. B color value distribution, cotton bolls values are overlapped with leaves values which may cause the false detection of leaves pixels as cotton bolls pixels. Thus, the B and G values can be utilized for cotton bolls segmentation from the back- ground. Therefore, a new parameter was developed by subtracting the
Where T is the threshold value for rBG.

Chromatic aberration method
In this method, initially, the red (R), green (G), and blue (B) compo- nents of the cotton bolls, stem, and leaves for all illumination conditions were extracted from the original captured RGB image and plotted, as shown in Fig. 4. From this figure, it can be observed that the R, G, and B values of cotton, stem, and leaves are not isolated completely as the G and B components of leaves are overlapped with the cotton color components. Based on this information, the various combinations of R, G, and B components were tested to isolate the cotton bolls from stems and leaves. Finally, a chromatic aberration equation (Eq. (13)) was found most suitable for differentiating the cotton bolls from the stem and leaves.

green component from the blue component, i.e., B-G, and the threshold
CA = B  R	G
(13)

function for cotton bolls segmentation using (B-G) was defined by	3	3 Eq. (8).


f x, y	(1, 1, 1),  if (B−G) ≥ T
(0, 0, 0),  if (B−G) < T


(8)
Where CA is chromatic aberration value; R denotes red component in
RGB color space; G denotes green component in RGB color space, and B denotes blue component in RGB color space.
The CA values of the values of cotton, stem, and leaves for all illumi- nation conditions, which were calculated using Eq. (13), are shown in Fig. 5. From this figure, an apparent deviation of CA values of the cotton

Where, T is the threshold value for (B-G) and R, G, B is the color com- ponent value of the image pixel (x, y).
bolls can be observed from that of the stem and leaves. Hence, the cot- ton bolls could be successfully segmented from the stem and leaves




Fig. 2. Distribution of R vs. G, R vs. B, and B vs. G of cotton bolls, stems, and leaves.




Fig. 3. R/B, R/G, and B/G ratios for cotton, stem, and leaves.



based on their CA values. The threshold function for cotton bolls seg- mentation using CA values was defined by Eq. (14).
isolated from that of stem and leaves. So, cotton bolls can be segmented from the background using appropriate thresholding values of Y and Cb components.

f (x, y) =  (1, 1, 1)
(0, 0, 0)
if CA ≥ T if CA < T
(14)

Threshold values selection



Where, T is the threshold value of chromatic aberration, and f (x, y) is the value of the pixel (x, y) in the final binary image obtained after im- plementation of the color thresholding technique. In the binary image, the white color portion (the pixel value is 1) represents the cotton bolls, and the black color portion (the pixel value is 0) represents the back- ground, i.e., stem and leaves.

YCbCr color model
To develop the cotton bolls detection algorithm based on the YCbCr color space model, initially, the RGB image was first converted into YCbCr color space and Y, Cb, and Cr components of the cotton bolls, stem, and leaves were plotted as shown in Figs. 6–8. From Fig. 6, it can be observed that the distribution of Y vs. Cb values of cotton bolls is
The selection of appropriate threshold values is very crucial for proper segmentation of input images and successful recognition of the cotton bolls using the proposed algorithms. In past, many researchers used various threshold methods for the segmentation of region of inter- est from an image (Bulanon et al., 2002b; Hamuda et al., 2017; Hu et al., 2015; Ireri et al., 2019; Montalvo et al., 2013; Singh, 2019; Tsai and Tseng, 2012; Vitzrabin and Edan, 2016; Yang et al., 2015). In this study, the color thresholding method was applied to segment images and recognize cotton bolls.
With a purpose to minimize false detection, optimum threshold values were found out using three performance indices namely preci- sion, recall, and F1 score (Rodríguez et al., 2020) which are formally expressed by Eqs. (15)–(17).





Fig. 4. Distribution of RGB values of cotton, leaves, and stem. (Rc – red color component of the cotton; Rl – red color component of the leaves; Rs – red color component of the stem, etc.)




Fig. 5. Distribution of chromatic aberration values of cotton, leaves, and stem.




Precision =	TP	(15)
TP + FP

Recall =	TP	(16)
TP + FN
single measure, the F1 score was used in this study. For perfect precision and recall values, the F1 value should be 1.
In each proposed algorithm, different threshold values were used, and TP, FP, and FN were counted for that threshold which was used to calculate precision, recall, and F1 score for that particular threshold value.

F1 =
2 × (Precision × Recall)
Precision + Recall	(17)
With a primary aim to identify cotton bolls correctly, for an algo- rithm, if 70% pixels of a cotton boll are correctly classified as cotton boll pixels, then in the present study, it was assumed that the algorithm

TP is the number of cotton bolls predicted by the algorithm correctly. FP is the background predicted by the algorithm incorrectly as cotton bolls and FN is the number of cotton bolls predicted by the algorithm in- correctly as background.
Precision measures the fraction of correctly classified cotton bolls among the total classified as cotton bolls while recall measures the frac- tion of correctly classified cotton bolls among the actual number of cot- ton bolls present. To combine properties of precision and recall into a
classified that cotton boll correctly. Precision, recall, and F1 score values were drawn against threshold values on abscissas as shown in Figs. 10–12. The point where precision, recall, and F1 score curves meet is the value at which TP is higher and FP is lower and was consid- ered as an optimum threshold value. Table 2 shows the optimum threshold values for each algorithm used in the present study.
To segment the cotton bolls from the background (stem and leaves), an appropriate threshold value of the CA, i.e., T = 37, was chosen from





Fig. 6. Distribution of Cb and Y components of cotton, stem, and leaves.




Fig. 7. Distribution of Cr and Y components of cotton, stem, and leaves.



Fig. 12, as the overlapping of CA values of cotton bolls and background (stem and leaves) was minimum for CA > 37.


Opening and closing morphological operations

After applying the thresholding operations to the input color image (Fig. 13 (A)), a binary image (Fig. 13 (B)) with each white pixel representing cotton boll was obtained. However, this contained noises in the shape of small clusters of pixels that do not belong to cotton but were detected as cotton pixels, and some cotton pixels were split be- tween several clusters, instead of being one cluster. The closing mor- phological operation was then applied to connect close clusters, using dilation followed by erosion implemented with a 5 × 5 pixel square neighborhood (Fig. 13 (C)). To remove noises, a morphological opening operation by erosion followed by dilation with a neighborhood of 5 × 5
pixel square was applied (Fig. 13 (D)). Due to randomness in the orien- tation of noises, a square shape was used in this study.


Ground truth images of cotton bolls

Ground truth images are the ones in which regions of interest (ROI) are segmented using a more accurate method as compare to propose method. Hu et al. (2015) compared their proposed automatic segmen- tation algorithm for bananas with the manually segmented bananas re- gion, which acts as ground truth in their study. Tsai and Tseng (2012) compared the accuracy of their proposed color detection method with the traditional color detection method based on HSL color space. Bachche and Oka (2013) obtained ground truth distance data by manu- ally measuring the distance for the accuracy testing of depth coordinate in their study.






Fig. 8. Distribution of Cb and Cr components of cotton, stem, and leaves.




Fig. 9. Cotton boll segmentation flowchart.


In the present study, for the performance evaluation of proposed segmentation methods, ground truth images of cotton bolls are needed. Therefore, cotton bolls in RGB images were manually segmented using the Adobe Photoshop CC software (Adobe Systems, San Jose, CA), and
pixels were counted for each cotton boll.
cotton (MC) indicates that the algorithm missed a cotton boll to recog- nize it as a cotton boll and treated it as background. Based on this, hits rate, false-positive rate, and false-negative rate were calculated by using Eqs. (18)–(20).


Performance evaluation of developed algorithms

In this study, two approaches were used to evaluate the perfor- mance of the proposed algorithms. In the first approach, true detected cotton (TDC), false detected cotton (FDC), and missed cotton (MC) were counted. True detected cotton (TDC) indicates an object in the seg- mented image which is recognized as cotton boll when it is a cotton boll.
Hits rate =	TDC
TDC + MC

False positive =	FDC
TDC + MC

False negative =	MC
TDC + MC
(18)


(19)


(20)

False detected cotton (FDC) indicates an object in the segmented image which is recognized as cotton boll when it is not a cotton boll. Missed
In the second approach, the performance was evaluated by using the number of pixels of true detected cotton and background. In this




Fig. 10. Precision, recall, and F1 score values against threshold values for the (B-G) method.




Fig. 11. Precision, recall, and F1 score values against threshold values for the B/G method.


approach, sensitivity (Se), specificity (Sp), and accuracy (Ac) terms were used for performance evaluation (Omid, 2011) and calculated by using Eqs. (21)−(23).
(Eq. (23)) was found out by dividing the number of pixels identified correctly and total number of pixels. To measure the effectiveness and robustness of the proposed algorithms, mean values (μ) and standard
deviations (σ) were also calculated.

Sensitivity =	TP
TP + FN

Specificity =	TN
FP + TN

Accuracy =	TP + TN
TP + FN + FP + TN
(21)


(22)


(23)


Results and discussion

The RGB color images of cotton plants, captured in different illumi- nation conditions, were selected for the performance evaluation of pro- posed cotton bolls detection algorithms. The proposed algorithms were implemented in Matlab® ver. R2017 (Mathworks Inc., Natick, MA. USA) and cotton bolls segmented results are illustrated in Fig. 14. It can be ob-

Where TP is the cotton's pixels predicted by the algorithm correctly. TN is the background's pixels predicated by the algorithm correctly. FP is the background's pixels predicted by the algorithm incorrectly as cot- ton's pixels. And, FN is the cotton's pixels predicted by algorithm incor- rectly as background's pixels.
The sensitivity (Eq. (21)) is thus a measure of the accuracy of algo- rithms in true cotton detection, and specificity (Eq. (22)) is a measure of the accuracy of algorithms in true background detection. Accuracy
served that all the cotton bolls having a size above 500 pixels are cor- rectly identified by all the proposed methods. However, there are over-segmentation (for example: for (B-G) method (3rd row, 1st col- umn, and 3rd row, 2nd column), CA method (4th row, 4th column), B/G method (5th row, 3rd column), YCbCr method (6th row, 3rd column) and under-segmentation (for example: for (B-G) method (3rd row, 6th column), B/G method (5th row, 6th column, and 5th row, 2nd column), YCbCr method (6th row, 6th column and 6th row, 1st column)) in many




Fig. 12. Precision, recall, and F1 score values against threshold values for the CA method.






Fig. 13. Morphological opening and closing operations on the binary image. (A) RGB image. (B) Binary image after thresholding (C) Binary image after the morphological closing operation (D) Binary image after the morphological opening operation
cases. Some cotton bolls, which were small in size, were missed, while in some cases background was identified as cotton incorrectly. All these results were quantified in Tables 3, 4, and 5.
The comparison between the correctly detected cotton bolls using proposed algorithms and manually counted cotton bolls is given in Table 3. In this comparison, over-segmentation and under- segmentation were not taken into consideration. From Table 3, it can be observed that the chromatic aberration method correctly identified 468 cotton bolls out of 514 cotton bolls followed by the YCbCr method, using which, 461 cotton bolls were identified correctly. In all three sub- classes i.e., 8:00–11:00 h, 11:00–15:00 h, and 15:00–17:00 h, the chro- matic aberration method outperforms the other proposed methods in the identification of cotton bolls.
The performance of proposed algorithms was also compared in terms of hits, false positive, and false negative (Table 4). It can be ob- served that more than 86% of cotton bolls can be identified correctly using proposed algorithms with a false-positive rate below 14%. The highest hits rate (91.05%) was achieved with the chromatic aberration method followed by the YCbCr method (89.68%). By changing the threshold value, the hit rate for this method can be increased further but with an increased rate of false-positive. Jin-shuai et al. (2011) achieved an accuracy of 90.44% using YCbCr color space. In this study, an accuracy of 88.52% was achieved using the color difference method whereas Wang et al. (2008) in their study, achieved an accu- racy of 88.09% using the color subtraction method and freeman chain coding.





Fig. 14. An example of cotton bolls detection results from the proposed methods for a set of images captured in various light conditions and various complex backgrounds. (1st row - raw RGB images; 2nd row – ground truth images; 3rd row – (B-G) method; 4th row – CA method; 5th row – B/G method; 6th row – YCbCr method).




Fig. 15. Comparison of hits rate of proposed methods during the morning, afternoon, and evening.



The performance of proposed algorithms in terms of sensitivity, specificity, and accuracy is given in Table 5. From Table 5, it can be ob- served that all the proposed algorithms achieved sensitivity, specificity, and accuracy of more than 79%, 97%, and 94% respectively. Li et al. (2016a) detected cotton using region-based semantic image segmenta- tion method and achieved 77.3%, 99.3%, and 97.0% sensitivity, specific- ity, and accuracy values respectively on forward images whereas, in the present study, the performance of chromatic aberration was found to be higher among other proposed methods in the present study with sensitivity, specificity and accuracy values of 88.69%, 97.53%, and 95.79% respectively.
The performance of the proposed algorithms was also evaluated during morning, afternoon, and evening time. For this, a total 60 num- ber of images were captured for morning time (with same settings as shown in Table 1), and 12 groups (each having five randomly selected images) were formed; and the hits rate, false positive, and false negative
were counted for each group of images using the proposed algorithms. The same process was repeated for afternoon and evening time images. The hits rate for proposed methods for the morning, afternoon, and eve- ning time is shown in Fig. 15. From this figure, it can be observed that the chromatic aberration method outperforms other proposed methods for correctly detecting the cotton bolls. The false-positive rates during the morning, afternoon, and evening time is shown in Fig. 16. From this figure, it can be observed that false-positive rates were higher dur- ing the afternoon as compared to evening and morning time for all pro- posed algorithms. This was because of high illumination during the afternoon and due to the reflection of light from stem and leaves, these were considered as cotton bolls by algorithms. The sample cases of false-positive detection due to the reflectance of light from the leaves are shown in Fig. 17. Fig. 17 (A) and (B) show the cotton plant images in RGB and YCbCr color models respectively and Fig. 17 (C) and (D) show the segmented images obtained using YCbCr and CA method





Fig. 16. Comparison of false-positive rate of proposed methods during the morning, afternoon, and evening.




Fig. 17. False-positive detection due to reflectance of light from leaves.




Fig. 18. Comparison of false-negative rate of proposed methods during the morning, afternoon, and evening.



Table 2
Selected optimum threshold values for proposed methods.

Method	Optimum threshold
Color difference method, B-G	20
Band ratio method, B/G	0.90
Chromatic Aberration method	37
YCbCr color space method	Cb = 150; Y = 145
respectively. The false-negative rates during the morning, afternoon, and evening time is shown in Fig. 18. From this figure, it can be observed that the false-negative rate was higher during the evening time as
compared to the morning and the afternoon time. It may be because of less illumination during the evening, some of the cotton bolls were in- correctly identified as stem or leaves or soil.
The performance of developed cotton boll detection algorithms was also compared based on their processing time for each image (Table 6). The results revealed that the chromatic aberration method took lesser processing time as compared to other developed algorithms. This may be because, in the chromatic aberration method, the input image doesn't need to be converted into a double-precision data type and un- like in the YCbCr method, no color space conversion is needed. From Table 6, it can be observed that the processing time also varies with dif- ferent images for the same algorithm (for example, for the chromatic aberration method, the minimum processing time was 0.327 s and the

Table 3
Comparison of the number of cotton bolls recognized using proposed algorithms against manually counted cotton bolls.


























Table 4
Performance of cotton segmentation algorithms based on the number of cotton boll recognized.





Table 5
Performance of cotton segmentation algorithms based on the number of pixel counts. Algorithm	Sensitivity (Se)	Specificity (Sp)	Accuracy (Ac)













Table 6
Comparison of processing time by proposed methods for processing each image.




maximum time was 0.429 s), it was observed that with an increase in the number of cotton bolls, processing time increases.


Conclusions

This study proposed the image processing algorithms for in-field cotton boll detection in natural lighting conditions. Initially, four-color space models, i.e., RGB, sRGB, HIS, and YCbCr were tested to determine the suitable color space model for cotton bolls detection. Out of them, the RGB and the YCbCr color spaces were selected and further utilized for developing the in-field cotton detection algorithms. A total of four image processing algorithms were developed for the real-time segmen- tation of cotton bolls under natural outdoor light conditions, out of which, three cotton detection algorithms based on the color difference method, the color component ratio method, and the chromatic


















aberration method, were developed by utilizing the RGB color model, and one algorithm was developed by utilizing the YCbCr color model. The performance of developed image processing algorithms was evalu- ated and the experimental results revealed that the chromatic aberra- tion method outperforms as compared to other developed algorithms. The chromatic aberration method showed the highest identification rate of 91.05% with false positive and false negative rates of 6.99% and 4.88% respectively, among all the proposed algorithms. The highest sen- sitivity and specificity were found to be 81.31% and 97.53%, respectively, using the chromatic aberration method. As hits rate for all proposed methods was found to be above 86% with a maximum false-positive rate below 14%, therefore, despite several challenges, such as variations in light conditions, complex background, similarities in some color fea- tures, although all the developed algorithms demonstrated good perfor- mance for cotton boll detection but the chromatic aberration method outperform as compared to other developed algorithms. Overall, the chromatic aberration approach demonstrated a propitious performance for in-field cotton bolls detection under natural lighting conditions which confirms its applicability for the robotic cotton harvesters.
The proposed algorithms have two limitations, i.e., the false positive detection due to the reflection of light and the inability in separating the overlapped cotton bolls. Therefore, although the experimental results indicated that the proposed methods demonstrate a high degree of ro- bustness and accuracy under varying natural outdoor light conditions, further study is required to reduce the false positive due to reflection of light by stem or leaves, false positive due to sky or clouds and splitting the overlapped cotton bolls.

Declaration of Competing Interest

The authors declared that there is no conflict of interest.

Acknowledgment

The financial support received from the Indian Institute of Technol- ogy Kharagpur, Kharagpur, West Bengal, India.

References

Arefi, A., Motlagh, A.M., Mollazade, K., Teimourlou, R.F., 2011. Recognition and localization of ripen tomato based on machine vision. Aust. J. Crop. Sci. 5, 1144–1149.
Bac, C.W., Roorda, T., Reshef, R., Berman, S., Hemming, J., van Henten, E.J., 2016. Analysis of a motion planning problem for sweet-pepper harvesting in a dense obstacle environ- ment. Biosyst. Eng. 146, 85–97. https://doi.org/10.1016/j.biosystemseng.2015.07.004.
Bachche, S., Oka, K., 2013. Distinction of green sweet peppers by using various color space models and computation of 3 dimensional location coordinates of recognized green sweet peppers based on parallel stereovision system. J. Syst. Des. Dyn. 7, 178–196. https://doi.org/10.1299/jsdd.7.178.
Baeten, J., Donné, K., Boedrij, S., Beckers, W., Claesen, E., 2008. Autonomous fruit picking machine: a robotic apple harvester. Springer Tracts Adv. Robot. 42, 531–539. https://doi.org/10.1007/978-3-540-75404-6_51.



Beale, M.H., Hagan, M.T., Demuth, H.B., 2017. MATLAB: Neural Network Toolbox User ‘S Guide.
Bulanon, D.M., Kataoka, T., 2010. Fruit detection system and an end effector for robotic harvesting of Fuji apples. Agric. Eng. Int. CIGR J. 12, 203–210.
Bulanon, D., Kataoka, T., Ota, Y., Hiroma, T., 2002a. A color model for recognition of apples by a robotic harvesting system. J. JAPANESE Soc. Agric. Mach. 64, 123–133. https:// doi.org/10.11357/jsam1937.64.5_123.
Bulanon, D., Kataoka, T., Ota, Y., Hiroma, T., 2002b. A segmentation algorithm for the au- tomatic recognition of Fuji apples at harvest. Biosyst. Eng. 83, 405–412. https://doi. org/10.1006/bioe.2002.0132.
Cai, J., Zhou, X., Wang, F., 2009. Obstacle identification of citrus harvesting robot. Trans.
Chinese Soc. Agric. Mach. 40, 171–175.
Camargo, A., Smith, J.S., 2009. An image-processing based algorithm to automatically identify plant disease visual symptoms. Biosyst. Eng. 102, 9–21. https://doi.org/ 10.1016/j.biosystemseng.2008.09.030.
Chaudhary, P., Godara, S., Cheeran, A.N., Chaudhari, A.K., 2012. Fast and accurate method for leaf area measurement. Int. J. Comput. Appl. 49, 22–25. https://doi.org/10.5120/ 7655-0757.
Dammer, K., 2016. Real-time variable-rate herbicide application for weed control in car- rots. Weed Res. 56, 237–246. https://doi.org/10.1111/wre.12205.
De-An, Z., Jidong, L., Wei, J., Ying, Z., Yu, C., 2011. Design and control of an apple harvesting robot. Biosyst. Eng. 110, 112–122. https://doi.org/10.1016/j.biosystemseng.2011.
07.005.
Edan, Y., Flash, T., Shmulevich, I., Sarig, Y., Peiper, U.M., 1990. An algorithm defining the motions of a citrus picking robot. J. Agric. Eng. Res. 46, 259–273.
Hamuda, E., Ginley, B.M., Glavin, M., Jones, E., 2017. Automatic crop detection under field conditions using the HSV colour space and morphological operations. Comput. Elec- tron. Agric. 133, 97–107. https://doi.org/10.1016/j.compag.2016.11.021.
Han, K.S., Kim, Si Chan, Lee, Y.B., Kim, Sang Chul, Im, D.H., Choi, H.K., Hwang, H., 2012. Strawberry harvesting robot for bench-type cultivation. J. Biosyst. Eng. 37, 65–74. https://doi.org/10.5307/jbe.2012.37.1.065.
Hannan, M.W., Burks, T.F., Bulanon, D.M., 2010. A machine vision algorithm combining adaptive segmentation and shape analysis for orange fruit detection. Agric. Eng. Int. CIGR J. XI, 1–17.
Harrell, R.C., Adsit, P.D., Munilla, R.D., Slaughter, D.C., 1990. Robotic picking of citrus.
Robotica 8, 269–278.
Hayashi, S., Shigematsu, K., Yamamoto, S., Kobayashi, K., Kohno, Y., Kamata, J., Kurita, M., 2010. Evaluation of a strawberry-harvesting robot in a field test. Biosyst. Eng. 105, 160–171. https://doi.org/10.1016/j.biosystemseng.2009.09.011.
Hemming, J., Bac, C.W., Van Tuijl, B.A.J., Barth, R., Bontsema, J., 2014. A robot for harvesting sweet-pepper in greenhouses. Proceed. Int. Conf. Agric. Eng. 6–10.
Henten, V.E.J., Hemming, J., Van Tuijl, B.A.J., Kornet, J.G., Meuleman, J., Bontsema, J., Van Os, E.A., 2002. An autonomous robot for harvesting cucumbers in greenhouses. Auton. Robot. 13, 241–258. https://doi.org/10.1023/A:1020568125418.
Henten, V.E.J., Van Tuijl, B.A.J., Hemming, J., Kornet, J.G., Bontsema, J., Van Os, E.A., 2003. Field test of an autonomous cucumber picking robot. Biosyst. Eng. 86, 305–313. https://doi.org/10.1016/j.biosystemseng.2003.08.002.
Henten, V.E.J., Van Tuijl, B.A.J., Hoogakker, G.J., Van Der Weerd, M.J., Hemming, J., Kornet, J.G., Bontsema, J., 2006. An autonomous robot for de-leafing cucumber plants grown in a high-wire cultivation system. Biosyst. Eng. 94, 317–323. https://doi.org/10.1016/ j.biosystemseng.2006.03.005.
Henten, V.E.J., Van'’t Slot, D.A., Hol, C.W.J., Van Willigenburg, L.G., 2009. Optimal manipu- lator design for a cucumber harvesting robot. Comput. Electron. Agric. 65, 247–257. https://doi.org/10.1016/j.compag.2008.11.004.
Hu, M.-H., Dong, Q.-L., Liu, B.-L., Pan, L.-Q., Walshaw, J., 2015. Image segmentation of ba- nanas in a crate using a multiple threshold method. J. Food Process Eng., 1–6 https:// doi.org/10.1111/jfpe.12233.
Ireri, D., Belal, E., Okinda, C., Makange, N., Ji, C., 2019. A computer vision system for defect discrimination and grading in tomatoes using machine learning and image process- ing. Artif. Intell. Agric. 2, 28–37. https://doi.org/10.1016/j.aiia.2019.06.001.
Ji, W., Zhao, D., Cheng, F., Xu, B., Zhang, Y., Wang, J., 2012. Automatic recognition vision system guided for apple harvesting robot. Comput. Electr. Eng. 38, 1186–1195. https://doi.org/10.1016/j.compeleceng.2011.11.005.
Ji, W., Meng, X., Tao, Y., Xu, B., Zhao, D., 2016. Fast segmentation of colour apple image under all-weather natural conditions for vision recognition of picking robots. Int.
J. Adv. Robot. Syst. 1–9. https://doi.org/10.5772/62265.
Jin-shuai, L.I.U., Hui-cheng, L.A.I., Zhen-hong, J.I.A., 2011. Image segmentation of cotton based on ycbccr color space and fisher discrimination analysis. Acta Agron. Sin. 37, 1274–1279. https://doi.org/10.3724/SP.J.1006.2011.01274.
Kondo, N., Nishitsuji, Y., Ling, P.P., Ting, K.C., 1996. Visual feedback guided robotic cherry tomato harvesting. Trans. Am. Soc. Agric. Eng. 39, 2331–2338. https://doi.org/ 10.13031/2013.27744.
Kondo, N., Yata, K., Iida, M., Shiigi, T., Monta, M., Kurita, M., Omori, H., 2010. Development of an end-effector for a tomato cluster harvesting robot. Eng. Agric. Environ. Food 3, 20–24. https://doi.org/10.1016/S1881-8366(10)80007-2.
Konduru, S., Yamazaki, F., Paggi, M., 2013. A study of mechanization of cotton harvesting in India and its implications. J. Agric. Sci. Technol. B 3, 789.
Lee, W.S., Slaughter, D.C., Giles, D.K., 1999. Robotic weed control system for tomatoes.
Precis. Agric. 1, 95–113. https://doi.org/10.1023/A:1009977903204.
Li, Y., Cao, Z., Lu, H., Xiao, Y., Zhu, Y., Cremers, A.B., 2016a. In-field cotton detection via region-based semantic image segmentation. Comput. Electron. Agric. 127, 475–486. https://doi.org/10.1016/j.compag.2016.07.006.
Li, J., Karkee, M., Zhang, Q., Xiao, K., Feng, T., 2016b. Characterizing apple picking patterns for robotic harvesting. Comput. Electron. Agric. 127, 633–640. https://doi.org/ 10.1016/j.compag.2016.07.024.
Li, Y., Cao, Z., Lu, H., Xu, W., 2020. Unsupervised domain adaptation for in-field cotton boll status identification. Comput. Electron. Agric. 178, 1–7. https://doi.org/10.1016/j. compag.2020.105745.
Lu, J., Hu, J., Zhao, G., Mei, F., Zhang, C., 2017. An in-field automatic wheat disease diagno- sis system. Comput. Electron. Agric. 142, 369–379. https://doi.org/10.1016/j. compag.2017.09.012.
Lurstwut, B., Pornpanomchai, C., 2017. Image analysis based on color, shape and texture for rice seed (Oryza sativa L.) germination evaluation. Agric. Nat. Resour. 51, 383–389. https://doi.org/10.1016/j.anres.2017.12.002.
Mehta, S.S., Burks, T.F., 2014. Vision-based control of robotic manipulator for citrus har- vesting. Comput. Electron. Agric. 102, 146–158. https://doi.org/10.1016/j. compag.2014.01.003.
Mehta, C.R., Chandel, N.S., Jena, P.C., Jha, A., 2019. Indian agriculture counting on farm mechanization. Agric. Mech. Asia, Africa Lat. Am. 84–89.
Moallem, P., Serajoddin, A., Pourghassem, H., 2017. Computer vision-based apple grading for golden delicious apples based on surface features. Inf. Process. Agric. 4, 33–40. https://doi.org/10.1016/j.inpa.2016.10.003.
Monta, M., Kondo, N., Ting, K.C., 1998. End-effectors for tomato harvesting robot. Artif.
Intell. Rev. 12, 11–25. https://doi.org/10.1023/a:1006595416751.
Montalvo, M., Guerrero, J.M., Romeo, J., Emmi, L., Guijarro, M., Pajares, G., 2013. Automatic expert system for weeds/crops identification in images from maize fields. Expert Syst. Appl. 40, 75–82. https://doi.org/10.1016/j.eswa.2012.07.034.
Mu, L., Cui, G., Liu, Y., Cui, Y., Fu, L., Gejima, Y., 2020. Design and simulation of an inte- grated end-effector for picking kiwifruit by robot. Inf. Process. Agric. 7, 58–71. https://doi.org/10.1016/j.inpa.2019.05.004.
Nguyen, T.T., Kayacan, E., De Baedemaeker, J., Saeys, W., 2013. Task and motion planning for apple harvesting robot. IFAC Proceedings Volumes (IFAC-PapersOnline). IFAC https://doi.org/10.3182/20130828-2-SF-3019.00063.
Nyakwende, E., Paull, C.J., Atherton, J.G., 1997. Non-destructive determination of leaf area in tomato plants using image processing. J. Hortic. Sci. 72, 255–262. https://doi.org/ 10.1080/14620316.1997.11515512.
Omid, M., 2011. Design of an expert system for sorting pistachio nuts through decision tree and fuzzy logic classifier. Expert Syst. Appl. 38, 4339–4347. https://doi.org/ 10.1016/j.eswa.2010.09.103.
Putra, W.B.T., Soni, P., 2017. Evaluating NIR-Red and NIR-Red edge external filters with digital cameras for assessing vegetation indices under different illumination. Infrared Phys. Technol. 81, 148–156. https://doi.org/10.1016/j.infrared.2017.01.007.
Putra, W.B.T., Soni, P., 2018. Enhanced broadband greenness in assessing chlorophyll a and b, carotenoid, and nitrogen in Robusta coffee plantations using a digital camera. Precis. Agric. 19, 238–256. https://doi.org/10.1007/s11119-017-9513-x.
Putra, B.T.W., Soni, P., 2020. Improving nitrogen assessment with an RGB camera across uncertain natural light from above-canopy measurements. Precis. Agric. 21, 147–159. https://doi.org/10.1007/s11119-019-09656-8.
Rodríguez, J.P., Corrales, D.C., Aubertot, J.N., Corrales, J.C., 2020. A computer vision system for automatic cherry beans detection on coffee trees. Pattern Recogn. Lett. 136, 142–153. https://doi.org/10.1016/j.patrec.2020.05.034.
Sabzi, S., Abbaspour-gilandeh, Y., Javadikia, H., 2017. Machine vision system for the auto- matic segmentation of plants under different lighting conditions. Biosyst. Eng. 161, 157–173. https://doi.org/10.1016/j.biosystemseng.2017.06.021.
Sabzi, S., Abbaspour-gilandeh, Y., Ignacio, J., 2020. An automatic visible-range video weed detection, segmentation and classification prototype in potato field. Heliyon 6. https://doi.org/10.1016/j.heliyon.2020.e03685 1–17.
Scarfe, A.J., Flemmer, R.C., Bakker, H.H., Flemmer, C.L., 2009. Development of an autono- mous kiwifruit picking robot. ICARA 2009 - Proc. 4th Int. Conf. Auton. Robot. Agents,
pp. 380–384 https://doi.org/10.1109/ICARA.2000.4804023.
Shaik, K.B., Ganesan, P., Kalist, V., Sathish, B.S., Jenitha, J.M.M., 2015. Comparative study of skin color detection and segmentation in hsv and ycbcr color space. Procedia Comput. Sci. 57, 41–48. https://doi.org/10.1016/j.procs.2015.07.362.
Silwal, A., Davidson, J.R., Karkee, M., Mo, C., Zhang, Q., Lewis, K., 2017. Design, integration, and field evaluation of a robotic apple harvester. J. F. Robot. 34, 1140–1159. https:// doi.org/10.1002/rob.21715.
Singh, V., 2019. Sunflower leaf diseases detection using image segmentation based on particle swarm optimization. Artif. Intell. Agric. 3, 62–68. https://doi.org/10.1016/j. aiia.2019.09.002.
Singh, V., Misra, A.K., 2017. Detection of plant leaf diseases using image segmentation and soft computing techniques. Inf. Process. Agric. 4, 41–49. https://doi.org/10.1016/j. inpa.2016.10.005.
Tang, J.L., Chen, X.Q., Miao, R.H., Wang, D., 2016. Weed detection using image processing under different illumination for site-specific areas spraying. Comput. Electron. Agric. 122, 103–111. https://doi.org/10.1016/j.compag.2015.12.016.
Tanigaki, K., Fujiura, T., Akase, A., Imagawa, J., 2008. Cherry-harvesting robot. Comput.
Electron. Agric. 63, 65–72. https://doi.org/10.1016/j.compag.2008.01.018.
Tewari, V.K., Pareek, C.M., Lal, G., Dhruw, L.K., Singh, N., 2020. Image processing based real-time variable-rate chemical spraying system for disease control in paddy crop. Artif. Intell. Agric. 4, 21–30. https://doi.org/10.1016/j.aiia.2020.01.002.
Tsai, S.-H., Tseng, Y.-H., 2012. A novel color detection method based on HSL color space for robotic soccer competition. Comput. Math. Appl. 64, 1291–1300. https://doi.org/ 10.1016/j.camwa.2012.03.073.
Vázquez-Arellano, M., Reiser, D., Paraforos, D.S., Garrido-Izard, M., Griepentrog, H.W., 2018. Leaf area estimation of reconstructed maize plants using a time-of-flight cam- era based on different scan directions. Robotics 7 (4). https://doi.org/10.3390/robot- ics7040063 63, 1-12.
Vitzrabin, E., Edan, Y., 2016. Adaptive thresholding with fusion using a RGBD sensor for red sweet-pepper detection. Biosyst. Eng. 146, 45–56. https://doi.org/10.1016/j. biosystemseng.2015.12.002.



Wang, Y., Zhu, X., Ji, C., 2008. Machine vision based cotton recognition for cotton harvest- ing robot. IFIP Int. Fed. Inf. Process. 259, 1421–1425. https://doi.org/10.1007/978-0- 387-77253-0_92.
Wang, Y., Yang, Y., Yang, C., Zhao, H., Chen, G., Zhang, Z., Fu, S., Zhang, M., Xu, H., 2019. End-effector with a bite mode for harvesting citrus fruit in random stalk orientation environment. Comput. Electron. Agric. 157, 454–470. https://doi.org/10.1016/j. compag.2019.01.015.
Williams, H.A.M., Jones, M.H., Nejati, M., Seabright, M.J., Bell, J., Penhall, N.D., Barnett, J.J., Duke, M.D., Scarfe, A.J., Ahn, H.S., Lim, J.Y., MacDonald, B.A., 2019. Robotic kiwifruit harvesting using machine vision, convolutional neural networks, and robotic arms. Biosyst. Eng. 181, 140–156. https://doi.org/10.1016/j.biosystemseng.2019.03.007.
Willigenburg, V.L.G., Hol, C.W.J., Van Henten, E.J., 2004. On-line near minimum-time path planning and control of an industrial robot for picking fruits. Comput. Electron. Agric. 44, 223–237. https://doi.org/10.1016/j.compag.2004.05.004.
Xiang, R., Ying, Y., Jiang, H., 2011. Research on image segmentation methods of tomato in natural conditions. Int. Congr. Image Signal Process. 1268–1272.
Xiong, Y., Peng, C., Grimstad, L., From, P.J., Isler, V., 2019. Development and field evalua- tion of a strawberry harvesting robot with a cable-driven gripper. Comput. Electron. Agric. 157, 392–402. https://doi.org/10.1016/j.compag.2019.01.009.
Xu, H., Ying, Y., 2004. Citrus fruit recognition using color image analysis. Intell. Robot. Comput. Vis. XXII Algorithms, Tech. Act. Vis. 5608, 321–328. https://doi.org/ 10.1117/12.570736.
Yabo, G.W., Wang, G., Yabo, W., 2016. Design of end-effector for tomato robotic harvest- ing. IFAC-PapersOnLine 49, 190–193. https://doi.org/10.1016/j.ifacol.2016.10.035.
Yamamoto, S., Hayashi, S., Saito, S., Ochiai, Y., Yamashita, T., Sugano, S., 2010. Develop- ment of robotic strawberry harvester to approach target fruit from hanging bench side. IFAC Proc. Vol. 3. https://doi.org/10.3182/20101206-3-jp-3009.00016.
Yang, W., Wang, S., Zhao, X., Zhang, J., Feng, J., 2015. Greenness identification based on HSV decision tree. Inf. Process. Agric. 2, 149–160. https://doi.org/10.1016/j. inpa.2015.07.003.
Yeom, J., Jung, J., Chang, A., Maeda, M., Landivar, J., 2018. Automated open cotton boll de- tection for yield estimation using unmanned aircraft vehicle. Remote Sens. 10, 1–20. https://doi.org/10.3390/rs10121895.
Zhang, Q., Karkee, M., Qin, L., Qin, L., Kaewkorn, C.S., Washington, M., 2016. Design and evaluation of a levelling system for a weeding robot. IFAC-PapersOnLine 49, 299–304. https://doi.org/10.1016/j.ifacol.2016.10.055.
Zhao, J., Tow, J., Katupitiya, J., 2005. On-tree fruit recognition using texture properties and color data. 2005 IEEE/RSJ Int. Conf. Intell. Robot. Syst. IROS, 263–268 https://doi.org/ 10.1109/IROS.2005.1545592.
Zhao, Y., Gong, L., Huang, Y., Liu, C., 2016a. Robust tomato recognition for robotic harvest- ing using feature images fusion. Sensors 173, 1–12. https://doi.org/10.3390/ s16020173.
Zhao, Y., Gong, L., Liu, C., Huang, Y., 2016b. Dual-arm robot design and testing for harvest- ing tomato in greenhouse. IFAC-PapersOnLine 49, 161–165. https://doi.org/10.1016/j. ifacol.2016.10.030.
Zheng, Y., Zhu, Q., Huang, M., Guo, Y., Qin, J., 2017. Maize and weed classification using color indices with support vector data description in outdoor fields. Comput. Elec- tron. Agric. 141, 215–222. https://doi.org/10.1016/j.compag.2017.07.028.
