Artificial Intelligence in Agriculture 7 (2023) 58–68











t-SNE: A study on reducing the dimensionality of hyperspectral data for the regression problem of estimating oenological parameters
Rui Silva a,⁎, Pedro Melo-Pinto a,b,⁎⁎
a CITAB - Centre for the Research and Technology of Agro-Environmental and Biological Sciences, Inov4Agro-Institute for Innovation, Capacity Building and Sustainability of Agri-Food Production,
Universidade de Trás-os-Montes e Alto Douro, Quinta dos Prados, Vila Real 5000-801, Portugal
b Departamento de Engenharias, Escola de Ciências e Tecnologia, Universidade de Trás-os-Montes e Alto Douro, Quinta dos Prados, Vila Real 5000-801, Portugal



a r t i c l e	i n f o


Article history:
Received 22 September 2022
Received in revised form 20 February 2023 Accepted 21 February 2023
Available online 6 March 2023


Keywords:
Hyperspectral images Dimensionality reduction Regression
T-SNE
Support vector machines Wine grape berries
a b s t r a c t

In recent years there is a growing importance in using machine learning techniques to improve procedures in precision agriculture: in this work we perform a study on models capable of predicting oenological parameters from hyperspectral images of wine grape berries, a specially relevant topic to boost production tasks for winemakers. Specifically, we explore the capabilities of a novel technique mostly used for visualization, t-Distributed Stochastic Neighbor Embedding (t-SNE), for reducing the dimensionality of the highly complex hyperspectral data and compare its performance with Principal Component Analysis (PCA) method, which de- spite the introduction of many nonlinear dimensionality reduction techniques over the years, had achieved the best results for real-world data across several studies in literature. Additionally we explore the potential of Kernel t-SNE, an extension to the t-SNE method that allows for the usage of the technique in streaming data or online scenarios. Our results show that, in a direct comparison, t-SNE achieves better metrics than PCA for most of the data sets in this work and that the regressor (Support Vector Regression, SVR) performs better with the t-SNE reduced features as inputs, accomplishing better predictions with lower error rates. Comparing the results with current literature, our shallow learning model paired with t-SNE achieves either better or on par results than those reported, even competing with more advanced models that use deep learning techniques, which should propel the introduction of t-SNE in more studies that require dimensionality reduction.
© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).






Introduction

In recent years the wine industry has evolved in a way to introduce as many new technologies as possible to improve production proce- dures: one of its main focuses is to obtain grapes for wine production in an environmentally friendly manner, without destroying them in the process and choosing them according to quality features. The tradi- tional approach relies on laboratorial analysis to assess some select oe- nological parameters which, alongside destroying the grapes used for analysis, is a cost and time-heavy method. The logical next step was to find some sort of imaging technique that allowed to obtain clean infor- mation about the grapes, and hyperspectral imaging found the most success in this task.

* Corresponding author.
⁎⁎ Corresponding author at: CITAB - Centre for the Research and Technology of Agro- Environmental and Biological Sciences, Inov4Agro-Institute for Innovation, Capacity Building and Sustainability of Agri-Food Production, Universidade de Trás-os-Montes e Alto Douro, Quinta dos Prados, Vila Real 5000-801, Portugal.
E-mail addresses: ruimsilva@utad.pt (R. Silva), pmelo@utad.pt (P. Melo-Pinto).
Hyperspectral imaging (Gowen et al., 2007; Hall et al., 2002) is a technique that shows the light reflection and absorption on an object as a function of wavelength: it collects both spatial and spectral infor- mation, requiring robust models that are capable of extracting knowl- edge from the patterns present in the spectra. Our recent studies (Fernandes et al., 2011, 2015; Gomes et al., 2014a, 2014b, 2017b, 2021a, 2021b; Gomes and Melo-Pinto, 2021; Silva et al., 2018) found some success in combining these images with machine/deep learning algorithms that are capable of regression from hyperspectral images; however, one of the biggest issues to tackle in order to obtain generali- zation capacity from these models is the ability to reduce the input space, which allows to consistently identify the main features in the spectra and accurately predict oenological parameters. This led to a study of a wide variety of dimensionality reduction methods (Silva and Melo-Pinto, 2021) that showed that despite several advances and new techniques, Principal Component Analysis (PCA) (Wold et al., 1987) was still the method that allowed the algorithm to achieve the best predictions and the best generalization capacity for the case of predicting oenological parameters from hyperspectral images of wine grape berries. Following this line of research, arose the need of applying


https://doi.org/10.1016/j.aiia.2023.02.003
2589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY license (http:// creativecommons.org/licenses/by/4.0/).



the t-Distributed Stochastic Neighbor Embedding (t-SNE) technique for the dimensionality reduction of hyperspectral images.
t-SNE (Van der Maaten and Hinton, 2008) is a technique that visual- ises high-dimensional data by giving each data point a location in a two or three-dimensional map, reducing the tendency to crowd points to- gether and therefore creating more structured visualisations of the data. We decided to conduct a study of the application of t-SNE to hyperspectral imaging data since, as concluded in (Silva and Melo- Pinto, 2021) and mentioned in (Van der Maaten and Hinton, 2008), other dimensionality reduction techniques showcase a strong perfor- mance on artificial data sets but fail to translate that performance to real-world data, mostly because of their failure to retain both the local and global structure of the data in a single map: t-SNE has shown to be capable of capturing much of the local structure of the high dimen- sional data as well as revealing a global structure in it.
Furthermore, it is also mentioned in (Van der Maaten and Hinton, 2008) that it is unclear how t-SNE will perform on general dimensional- ity reduction tasks, and we decided to evaluate its performance on hyperspectral imaging data - reviewing the current state-of-the-art shows some applications of t-SNE to hyperspectral data:
in (Miao et al., 2018) the authors use t-SNE to reduce the dimension- ality of hyperspectral images of maize kernels and perform classification.
in (Hariharan, 2021) t-SNE is applied to reduce the dimensionality of hyperspectral open-access data sets (i.e.: Indian Pines data set) and perform classification while tackling the curse of high dimensionality on a limited number of training samples.
in (Zhang et al., 2018) t-SNE is combined with a Deep Convolutional Generative Adversarial Network (DCGAN) to extract spectra-spatial features and perform dimensionality reduction on hyperspectral images.
in (Devassy and George, 2020) t-SNE is used to perform clustering and
obtain a better visualization on a hyperspectral database of inks from 60 different pens.
in (Gao et al., 2019) the authors combine t-SNE with a Convolutional Neural Network (CNN) to reduce the dimensionality and perform classification on hyperspectral images with a large number of bands but an insufficient number of sample pixels for each class.
in (Pouyet et al., 2018) t-SNE is also used to obtain visualisations of hyperspectral images in 2D scatter plots.

Additionally, there are also relevant applications of t-SNE outside of hyperspectral images:
in (Gisbrecht et al., 2012) the authors pave the way towards efficient nonlinear dimensionality reduction proposing an extension of t-SNE with the linear basis function.
in (Alibert, 2019) t-SNE is applied to better visualise and represent
planetary systems in a two-dimensional space.
in (Anowar et al., 2021) the authors compare the performance of several dimensionality reduction methods on open-access data sets.

However, and as far as we are aware up to date, there is not a study of the capability of t-SNE to reduce the dimensionality of a data set to then perform predictions with a machine learning algorithm (regres- sion problem); additionally, we also perform this study on real-world samples (most of the studies are performed on artificial data sets) of hyperspectral images of wine grape berries, a problem with extremely high variability between harvest years and vintages of wine grapes; our study is also relevant because we perform t-SNE on small data sets (due to the inherent difficulties in acquiring training samples) in con- trast to most works that have great amounts of data: combining all these aspects, we believe we perform a true test to the capability of t- SNE to reduce the dimensionality of real-world hyperspectral images in a problem with real-world applications. To further enhance our
study of the t-SNE technique, we also decided to apply a variation of this method, Kernel t-SNE. Kernel t-SNE (Gisbrecht et al., 2015) extends the non-parametric dimensionality reduction technique to an explicit mapping by fixing the parametric form x → fw(x) = y and optimizing the parameters of fw instead of the projection coordinates: this enables to map large data sets in linear time by training a mapping on a small sub-sample only, with good generalization capacity. However, since Kernel t-SNE is applied to a subset of the training samples only, the results may differ when compared to t-SNE that is applied to the full data set, due to missing information in the data used for training of the map. In order to close this information gap, the authors (Schulz and Hammer, 2015) introduced Fisher Kernel t-SNE: a set of data points xi is equipped with the pairwise Fisher metric, estimated based on the class labels taking simple linear approximations for the path integrals and, using t-SNE, a new training set X′ is obtained by taking the auxiliary label information into account (the calculated pairwise distances of data computed based on the Fisher metric); the authors then infer a kernel t- SNE mapping which is adapted to the label information due to the infor- mation inherent in the training set, and the resulting map is adapted to the information encoded in the training set - this technique can be re- ferred to as Fisher t-SNE or Fisher kernel t-SNE. Both these techniques attempt to further optimize the capability of t-SNE in real-world appli- cations and while we decided to study kernel t-SNE, we opted to not im- plement Fisher kernel-SNE due to the high computational cost of calculating the Fisher information matrices, which renders the solution unfitting for a possible real-time analysis in the vineyards.
As for the rest of this paper, in Sub-Section 2.1 we describe the
hyperspectral imaging procedure, with an in-depth look into our exper- imental setup and to the way we perform the reflectance measurements to construct the training sets; Sub-Section 2.2 provides a brief theoreti- cal background of the dimensionality reduction methods applied; Sub-Section 2.3 introduces the algorithm chosen to perform regression, the Support Vector Regression (SVR) technique; in Sub-Section 2.4 we discuss our approach to avoid over-fitting and achieve maximum gener- alization capacity via the usage of a cross-validation technique; in Sub-Section 2.5 we give insights about the grape sampling procedure and provide the data sets description to give a better understanding of the data and its high variability; Section 3 presents the results obtained for the prediction of the oenological parameters for each dimensionality reduction technique and for each oenological parameter, alongside a discussion of said results and how they fare when compared to other state-of-the-art publications; Section 4 summarizes our findings and concludes about the work, while also denoting future guidelines for improvement.

Materials and methods

Hyperspectral images

Experimental setup
Hyperspectral measurements were performed in line with our pre- vious work (Silva and Melo-Pinto, 2021) using the following image ac- quisition system (Fig. 1): a hyperspectral camera, composed of a JAI Pulnix (JAI, Yokohama, Japan) black and white camera and a Specim Imspector V10E spectrograph (Specim, Oulu, Finland); lighting, by means of a lamp holder with 300 × 300 × 175 mm3 (length × width × height) that held four 20 W, 12 V halogen lamps and two 40W, 220V blue reflector lamps (Spotline, Philips, Eindhoven, The Netherlands). The halogen lamps were powered by continuous current power sup- plies to avoid light flickering and the reflector lamps were powered at only 110V to reduce lighting and prevent camera saturation. Each com- ponent was bough directly from their respective manufacturers and the image acquisition system was assemble by the authors. The resulting images have a spatial resolution of 1040 × 1392 pixels, where the 1040 pixels correspond to the wavelength channels, ranging between 380 and 1028 nm, with approximately 0.6 nm width for each channel,


	


Fig. 1. Mock-up of the setup used for hyperspectral imaging (Fernandes et al., 2011).
Fig. 3. Hyperspectral image of a wine grape berry sample before segmentation and reflectance measurements.



and the 1392 pixels stand for the spatial dimension (one line over the samples) with approximately 110 mm of width. The distance between the camera and the sample base was set to 420 mm and the camera was controlled with the Coyote software from JAI. All the hyperspectral measurements were done inside a dark room and at room temperature (20 °C).
Each hyperspectral image is acquired for six grape berries and in three different berry rotations of approximately 120° between posi- tions, with a single line taken solely over the berry equator when con- sidering the pedicel as the pole, as seen below in Fig. 2.
The final hyperspectral image for each rotation and position is the average of 32 different hyperspectral images acquired during a period of time of 4 s, with the camera acquiring 8 images per second: this method reduces measurement noise, and also provides with some in- formation of the spatial dimensions (since we are not using it directly on line-scan hyperspectral images) that is reflected on the averaged value. As expected, there are observable patterns of light reflection and absorption in the main areas where the wine grape berries lie in
more light (white strips). After finishing the process of obtaining an individual hyperspectral image for each wine grape berry we carry out reflectance measurements to build the final data sets.

Reflectance measurements
Reflectance is a function of the light wavelength and is defined as the ratio between the light intensity reflected by an object and the light that illuminates that object. Despite the fact that the measurements could be carried out using other modalities, like transmittance or interactance, we chose the reflectance mode as input because the different patterns of reflectance and absorption across wavelengths will allow for the identification of chemical compounds and because, contrary to the other referred modes, imaging is possible without the need for the spec- trometer/camera to touch the samples (Gomes et al., 2021a, 2021b; Gomes and Melo-Pinto, 2021; Silva and Melo-Pinto, 2021). For a posi- tion represented by vector x and at wavelength λ, the reflectance R can be expressed as:

the Spectralon (full reflection surface) and, to get individual images
for each wine grape berry, we use a threshold-based segmentation method. Fig. 3 shows an example of a hyperspectral image captured
R(x, λ) = α(x, λ) — σ (x, λ)
μ(x, λ) — σ (x, λ)
(1)

by the aforementioned experimental setup before segmentation.
Observing Fig. 3 it is possible to denote that the spots where the grapes lie for imaging are clear to absorb more light (black strips) while the spots with no grapes (the remainder of the Spectralon) reflect



Fig. 2. Imaging line over each berry (Gomes et al., 2017a).
where α is the light intensity reflected from the grape; μ is the light in- tensity reflected from a reference total reflectance target; and σ is the dark current signal, which is electronic noise. For each wine grape berry we took 32 hyperspectral images, with three different berry rotations, and the resulting spectrum was then normalized (using max-min nor- malization) to avoid variations in the measured light intensities. Fig. 4 shows the result of the reflectance measurements in one of the data sets that will be used in the present work.

Dimensionality reduction

t-distributed stochastic neighbor embedding
t-SNE (Van der Maaten and Hinton, 2008) is a technique that is capa- ble of capturing the local structure of the high-dimensional data while also revealing global structure, such as the presence of clusters at sev- eral scales. When a problem requires dimensionality reduction common goals between different applications can be highlighted, such as


heavy-tailed probability distribution to address the crowding problem in the original technique: in summary, t-SNE minimizes the Kullback- Leibler divergences between the high-dimensional and the latent spaces with the cost function:


C = ∑KL(P ∥Q ) = ∑∑p
log p j∣i

(2)

i	i
i	i j
j∣i
q j∣i






















Fig. 4. Reflectance values obtained for a complete data set (Silva and Melo-Pinto, 2021).



preserving as much significant structure or information in the high- dimensional data as in the low-dimensional representation/projection;
where P is the conditional probability distribution in the original space and Qi is the conditional probability distribution in the latent space. Since the Kullback-Leibler divergence is not symmetric, the error in the pairwise distances in the low-dimensional representation will be weighted differently: the usage of widely separated map points to rep- resent nearby data points will have a larger cost while the usage of nearby map points to represent widely separated data points will have a smaller cost - this means that there is a focus on retaining the local structure of the data in the low-dimensional representation.
Hence, the variance of the Gaussian that is centered on data point xi, the parameter σi, will never be optimally represented for all data points in the data set because the density of the data is likely to vary: in denser regions a smaller value of σ will be more appropriate than in sparser regions. t-SNE will then perform a binary search for the value of σi that produces a Pi with a fixed perplexity that is user-specified and defined as:
Perp (P ) = 2H(P )	(3)

increase the means of interpretation of the data in the lower dimension;	i
and minimize the information loss in the new, low-dimensional repre-

sentation of the data. In our previous studies (Silva and Melo-Pinto, 2021) PCA established itself as the technique with the best results in terms of easing the interpretation of the data by the regressor, obtaining the representations that contained the most preserved information and, in most cases, the representations that led to less erroneous predictions by the machine learning algorithm. When we compare these two tech- niques, we can denote some relevant differences:
PCA is a deterministic algorithm commonly used for feature extrac- tion, while t-SNE is a randomized algorithm that is mostly used for vi- sualization purposes only;
PCA applies a linear technique where the focus is on keeping the dis-
similar points apart in a lower-dimensional space, while t-SNE applies a non-linear technique that attempts to keep the similar data points close together in the lower-dimensional space;
PCA transforms the original data by preserving the variance in the
data using eigenvalues matrices and is very affected by outliers, while t-SNE preserves the local structure of the data by using student t-distributions to compute the similarity between two points in the lower-dimensional space (which helps address crowding and optimi- zations problems) and is not as strongly impacted by outliers.


Henceforth, since t-SNE is mostly used for visualization purposes it is unclear how it will perform in a general dimensionality reduction task, and by having such contrasting characteristics to the technique that ob- tained the best results thus far, we were intrigued to conduct this appli- cation and study the outcome since we concluded previously (Silva and Melo-Pinto, 2021) that, in non-linear techniques, local learners seemed to have better results than global learners, but we never tested a non- linear technique capable of preserving both local and global structure; and also, since the problem of estimating oenological parameters from wine grape berries possesses high variability due to the hundreds of dif- ferent varieties and the different harvest years, a technique capable of

where H(Pi) is the Shannon entropy of Pi measured in bits. One can then interpret perplexity as a simple measure of the effective number of neighbors with typical values varying between 5 and 50: in this work, we used the same range of values for all the experiments, picking the perplexity value that originated the predictions with the least error value.

Kernel t-SNE
t-SNE is a non-parametric technique that provides a high-to-low di- mensional representation of a given data set without a mapping for- mula on how to project further points not included in the original set: while it grants a higher degree of flexibility (since no constraints have to be met) it means that the result of the visualization step entirely de- pends on the formalization of the mapping procedure and that there is not a direct way to map additional points after obtaining the projections of the data set; this fact renders t-SNE unsuitable for the visualization of streaming data or online scenarios. Additionally, non-parametric tech- niques are unfitting for large data sets since they display at least a qua- dratic complexity. To tackle these drawbacks (Gisbrecht et al., 2015) introduces Kernel t-SNE.
Kernel t-SNE is an approach that maintains the flexibility of t-SNE while displaying generalization ability within out-of-sample exten- sions: it extends the non-parametric t-SNE to an explicit mapping by fixing the parametric form x → fw(x) = y and optimizing the parameters of fw instead of the projection coordinates. The mapping fw = y underlying kernel t-SNE follows the form:
x → y(x) = ∑α ⋅  k x, xj	(4)
j	i	l

where aj ∈ Y are parameters corresponding to points in the projection space and the data xj are taken as a fixed sample. k is the Gaussian kernel parameterized by the bandwidth σj:
k x, x = exp −0.5¨x−x ¨2/σ 2	(5)

t-SNE originates from Stochastic Neighbor Embedding (SNE) (Hinton and Roweis, 2002), but uses a student t-distribution with a
This generalized linear mapping allows training to be performed in a simple way (given a set of samples xi and y(xi) is available). Parameters



αj can be determined by a least squares solution of the mapping. Thus, in kernel t-SNE a standard t-SNE is applied to the subset X′ to obtain a training set and afterwards the previous analytical solution is used to obtain the parameters of the mapping: once this is done the full set X can be projected in linear time by applying the mapping y. In this work, the bandwidth σ was defined as 0.5 for all experiments, while we used the same range for the perplexity value in kernel t-SNE as the one used in t-SNE.

Regression model: Support vector regression

After capturing the hyperspectral images and obtaining the re- flectance measurements, dimensionality reduction techniques (such as t-SNE) are applied to the data sets to obtain a low- dimensional representation of the points that is better suited to
serve as an input for a classification or, for the case of this work, re-
Cross-validation and evaluation metrics

n-Fold Cross-Validation
At this point of the models pipeline it is of paramount importance to guarantee that the results obtained are not skewed in any form by the training stage, since it is very common for these algorithms to suffer from “overfitting” - the statistical model achieving a perfect fit against its training data and being unable to perform accurately against unseen data - to tackle this issue a cross-validation approach was implemented. The n-fold cross-validation method (Lendasse et al., 2003; Remesan and Mathew, 2015) splits the data set X into K parts of equal size with the kth set forming the validation set Xval and the remaining sets forming the training set Xlearn: the training of a model g is performed
using Xlearn and the error Ek(g) is calculated as:
∑K g xval — yval 2

gression model: in this paper we chose the Support Vector Regres- sion (SVR) algorithm (Vapnik, 1999) since it obtained state-of-the-
Ek(g) =
i=1
i	i
N/K
(8)

art results (Silva et al., 2018) for the particular application of
with (xval, yval) the elements of Xval and g(xval) the estimation of yval by

predicting oenological parameters from hyperspectral images of wine grape berries when compared to Neural Networks (NN) (Janik et al., 2007), Partial Least Squares (PLS) (Arana et al., 2005) or
model g. This process loops for k varying from 1 to K and the average error is computed as:
∑K  E  g

Least-Squares Support Vector Machines (LSSVM) (Cao et al., 2010)
models; additionally, to guarantee that for the case studies present in this work the SVR algorithm obtained superior results and that
Ebgen (g) =
  k=1 k ( )	(9)
K

the behaviour exhibited by PCA, t-SNE and Kernel t-SNE followed similar tendencies, we also applied NN for single vintage data sets - results can be found in Appendix A.
The support vector algorithm is described as follows (Smola and Schölkopf, 2004): given a set of training data {(x1, y1), …,(xn, yn)} ⊂ χ ∈ ℝ, where χ represents the space of input patterns, the
goal is to determine a function f(x) = 〈w, x〉 + b, w ∈ χ, b ∈ ℝ
that deviates a maximum of ɛ from the real measured targets yi for
the entire training set while, simultaneously, being as flat as possible. This convex optimization problem can be written as the minimization of the Euclidean norm for the cases of a linear function; to extend the SV machine to nonlinear functions the so-called “SV expansion” is introduced:
n
∗
i
In this work, we choose the number of folds K varying from a range
of 5–10: we choose a smaller number of folds for smaller data sets and a higher number for data sets with more samples.

Trustworthiness and continuity
In order to evaluate the performance of t-SNE and kernel t-SNE and compare it to a standard PCA approach for dimensionality reduction some metric that measures the quality of the low-dimensional embed- dings must be employed: based on (Du, 2019; Venna and Kaski, 2006) we implemented the trustworthiness and continuity metrics.
Trustworthiness and Continuity are metrics that attempt to measure the degree of similarity of the local structure of the data between its original high-dimensional state and its low-dimensional visualization obtained via the application of a dimensionality reduction technique. In particular, trustworthiness evaluates if the neighbors chosen are the
same in both representations and is defined by:

i=1

where ω is described as a linear combination of the training patterns	T k 
( ) = 1 — A(k) ∑ ∑ rT(i, j) — k	(10)

and cases with αi > 0 are the support vectors. This SV expansion comes to light by writing the optimization problem in its dual formulation from both the objective function and the corresponding constraints. Ad- ditionally, a computationally efficient form of mapping the input vectors into a high-dimensional feature space with a nonlinear mapping comes with the usage of a suitable kernel function, obtaining the nonlinear regression functions of the form:
n
∗
i
i=1
i=1 j ∈ Uk (i) 

where N is the sample size, k is the number of nearest neighbors, A(k) is a scaling function and Uk(i) is the set of points that are among the k nearest neighbors of i in the low-dimensional space but not in the high-dimensional one. rT(i, j) is the rank of point j in Uk(i) according to the pairwise distance from i in the original high-dimensional space. On the other hand, continuity attempts to quantify how well the local structure was maintained after its transformation to a low- dimensional visualization and is defined by:

where κ is the chosen kernel function. This transformation allows the	C k	N

model to solve the optimization problem in a more suitable feature space; there is a wide variety of kernels to choose from that are suited for different types of applications, with local kernels being based on dis- tance (only the data points near each other influence the kernel values) and global kernels being based on dot product (data points far away from each other still influence the kernel values): with the different tests performed in previous works (Silva et al., 2018) we chose a Gauss- ian kernel for the current paper since it achieved the best results for our application; as for the well-known hyperparameters C and γ, we per- formed a grid search for each experiment with C values ranging from 80 to 120 and γ ranging from 1e−5 − 1e−1.
( ) = 1 — A(k) ∑ ∑ rC (i, j) — k	(11)
i=1 j ∈ Vk (i) 

where Vk(i) is the set of points that are among the k nearest neighbors of data point i in the original high-dimensional space but are not neighbors in the visualization. rC(i, j) is the rank of the point j in Vk(i) ordered by the pairwise distances between j and i in the low-dimensional visualiza- tion.

Root mean square error (RMSE) and determination coefficient R2
Concerning the evaluation of the performance of the dimensionality reduction techniques we also measure the generalization error of the



regressor trained on the low-dimensional data representation (Sanguinetti, 2008): this process allows the comparison of the predic- tion results with other state-of-the-art publications. Root mean square error (RMSE) is defined as:
sﬃ∑ﬃﬃﬃﬃﬃﬃNﬃﬃﬃ1ﬃﬃ ﬃﬃbyﬃﬃﬃiﬃﬃﬃ—ﬃﬃﬃﬃﬃyﬃﬃiﬃﬃ ﬃﬃ2ﬃﬃ
Table 1
Experiment details for each case study.

RMSE =

where yi
  i=	
N — 1

is the reference value and yi
(12)

is the model estimate. The

generalization error is the only well-established practice in literature to evaluate the accuracy of a model and we measure the RMSE for the hold-out test set; however, it seems that for this particular area of re- search the usage of the determination coefficient (R2) as an indicator of quality of prediction is very common. R2 is defined as:




Case An: cases where the training and test sets use the same single vintage of TF wine grape berries;

R2 =
σyy^  2

σyσy
(13)
Case Bn: cases where the training set employs multiple vintages of TF wine grape berries and the hold-out test set uses the same multiple vintages of TF wine grape berries.

where σy,y^ is the covariance between y and y^ and σi, σ y^ are the respective standard deviations. While we present the R2 indicator in
the results section to serve as a sort of comparison benchmark to other works in literature for the prediction of oenological parameters in hyperspectral images of wine grape berries, it is important to denote that, according to (Spiess and Neumeyer, 2010), R2 is not well- defined for non-linear regression since it shows extreme bias to higher parameterized models and, in a background of low and medium exper- imental noise, this indicator cannot compensate the effect of the num- ber of increasing parameters.

Grape sampling and data set analysis

Regarding the variety of wine grape berries used to frame the data sets for this work, we chose grapes of the native Portuguese variety Touriga Franca (TF) mainly due to its' importance for Port wine produc- tion in the Portuguese Douro region, as well as due to its' importance to our industrial partner Symington Family Estates: the TF variety is so im- portant for both producers and oenologists due to its' resistance to most plant diseases, the strong flavour and aroma of the resulting wine and due to its' high concentration of tannins, which guarantees that the wine will age well. The grapes were harvested from Quinta do Bonfim in Pinhão, Portugal in the years (refers to the years of sample collecting, do not mistake for year of vine tree plantation) of 2012 (240 samples), 2013 (81 samples), 2014 (120 samples), 2016 (407 samples) and 2017 (540 samples) from different regions in the vineyard and with dif- ferent levels of maturity; each sample is composed of six grape berries collected from a single bunch and the ground-truth results are obtained via laboratory analysis using validated standard methods (Carbonneau and Champagnol, 1993; Office International de la Vigne and du Vin, 1990).
Henceforth, we conducted a study of the models ability to predict the pH index and sugar content on different vintages of wine grape berries from the TF variety: we chose to evaluate these oenological pa- rameters since they are highly researched due to their correlation with flavour, colour and overall grape ripeness stage; additionally, due to several factors such as climate change, soil quality, sun exposition, water assessment, altitude and harvest time, a large variability is pres- ent in the vineyards and consequently in the quality of the grapes and their oenological profile, which makes the evaluation of the models' ca- pacity across different vintages a true test to its' generalization potential. Appendix B contains a descriptive statistics analysis of the ground-truth results for each oenological parameter in the different TF data sets used; 7 presents ANOVA (one-way analysis of variance) tests to verify signif- icant differences between the means of the different data sets.
In order to achieve a more comprehensible form of presenting the results, we decided to split the analysis according to the characteristics of the training set and the hold-out test set for each run:
Case Cn: cases where the training set uses single/multiple vintages of TF wine grape berries and the hold-out test set uses a different vintage of TF wine grape berries.


For each case study we create a hold-out test set with 10% of the size of the correspondent training set; for each training set we perform cross-validation with a 90/10% split into training/validation folders. Table 1 provides details on all the experiments performed.
In each of the experiments previously described we present the RMSE and R2 obtained by the regressor for the hold-out test set for each oeno- logical parameter (pH index and sugar content) and for each dimension- ality reduction method employed (PCA, t-SNE and kernel t-SNE) - as mentioned previously, we compare the performance of t-SNE and kernel t-SNE against PCA since in our previous work (Silva and Melo-Pinto, 2021), for these same data sets and case studies, PCA outperformed a wide variety of nonlinear methods establishing itself as the best technique to reduce our inputs, which goes in accordance to several studies in liter- ature that show that, for real-world data, PCA still outperforms the most advanced techniques introduced in recent years; the trustworthiness and continuity metrics are calculated for each data set for every dimen- sionality reduction method implemented. Appendix D summarizes the best published results in literature and how they compare with the best results in this work, split by corresponding case studies.

Results and discussion

Analysis

Table 2 and Table 3 present the results (best results are underlined) obtained for the trustworthiness and continuity measures for all data sets. Observing these tables, it is noticeable that t-SNE achieves the best results in trustworthiness across the different data sets but the con- tinuity results are a bit closer: this means that while t-SNE does a better job of choosing the same neighbors in both the higher and lower dimen- sional representations, how well the local structure was maintained after its transformation (defined by continuity) is on par between both methods; however, it is important to denote that for the data

Table 2
Trustworthiness T(20) for each dimensionality reduction technique in each individual data set.


Trustworthiness




Table 3
Continuity C(20) for each dimensionality reduction technique in each individual data set.
Table 5
Results obtained for sugar content in the hold-out test set for each case study.

Sugar Content (°Brix)


PCA	t-SNE	Kernel t-SNE

Case

		
R2	RMSE	R2	RMSE	R2	RMSE






sets with a higher number of samples and a higher intrinsic dimension- ality, t-SNE gets the best results which might be an indicator of this method being better suited to reduce the dimensionality of more com- plex samples, a trait that is also present on Kernel t-SNE results.
Table 4 showcases the results obtained in the prediction of the pH index for the different case studies and dimensionality reduction methods. In a direct comparison, the t-SNE technique gets the best re- sults for almost every single case study, with smaller error rates and higher determination coefficient values. As highlighted previously by the trustworthiness and continuity measures, it is also important to no- tice that the Kernel t-SNE method achieves smaller error rates in the last case studies in comparison to the PCA method: these are the cases where training sample number increases significantly and where the in- trinsic dimensionality of the data rises. While the regressor perfor- mance is quite stable across the different case studies, it is still noticeable that the smallest error rates are obtained in cases where the training and test sets use the same single vintage of TF wine grape berries: this shows that despite having a very strong generalization ca- pacity, the model still cannot predict the pH index for cases of unseen vintages in the testing phase with such certainty as it does for when the training set includes samples of that same vintage: this can be con- sidered normal, since the complexity of the prediction significantly in- creases (Appendix C Table C.1 shows that there is a significant difference in the means between almost every single vintage) and an even bigger drop-off in results would not be surprising, since we are using a shallow learning regressor to operate the predictions and pH index values are extremely vulnerable to even the smallest changes in external factors (such as weather, water availability, etc.). Additionally, we conducted a paired t-test between the predictions obtained by each of the dimensionality reduction methods and no difference in the means between the sets was found.
Table 5 presents the results obtained for the prediction of sugar con- tent for the different case studies and dimensionality reduction methods: once again, the t-SNE technique achieves the best results for almost every case study, with smaller error rates and higher determination coefficient values; additionally, it is once again important to emphasize that both t- SNE and Kernel t-SNE results for cases where the intrinsic dimensionality and number of samples rises are better than the ones obtained by PCA. As for the generalization capacity of the regressor, for the sugar content pre- diction the drop-off in results (higher error rate) is more noticeable than for the case of the pH index, which could be expected since we are


Table 4
Results obtained for pH index in the hold-out test set for each case study.

pH Index


PCA	t-SNE	Kernel t-SNE
Case








working with more sparsely distributed sample points (Appendix B pre- sents descriptive statistics of the samples for each data set in Table B.1 and Table B.2); this increase in error rate can be even more perceptible in cases where the TF 2014 vintage is employed in training or testing (cases A3, B2, B3, C2 and C3), which can be explained by the fact that this particular vintage had particularly low values for the sugar content, with a much smaller mean value when compared to the other vintages, causing the regressor to have bigger problems in generalizing its predic- tions - however, we still consider the generalization capacity of the model to be very acceptable, specially considering we are using a shallow learning regressor that lacks the capability of more powerful deep learn- ing techniques. The paired t-test to investigate statistical significant differ- ences between the means of the predictions given by each dimensionality reduction method showed that there were significant differences in the means of the predictions obtained by the PCA and Kernel t-SNE methods when compared to the PCA for case studies A3 and C3, which possibly jus- tifies the difference in the results with the fact that the regressor high- lighted different features for learning in these sets.

Comparison

Comparing our results with top state-of-art works (Appendix D Table D.1 provides a comparative review table), t-SNE obtained the best RMSE and R2 values for sugar content estimation out of all studies where the training and test sets use the same single vintage of wine grape berries and also out of all studies where the test set uses a differ- ent vintage of wine grape berries; regarding case study B (same multi- ple vintages of wine grape berries employed in both the training and hold-out test sets) t-SNE achieved on par and competitive results, only being outdone by the Convolutional Neural Networks (CNN) model in (Gomes et al., 2021a) and by the Partial Least Squares (PLS) models pre- sented by (Caballero et al., 2011) and (Fadock et al., 2016): in the au- thors opinion, this emphasizes even more the capabilities of the t-SNE technique for dimensionality reduction, since we are able to obtain competitive or superior results with a shallow learning regressor versus models that already employ more advanced, deep learning solutions for prediction; considering the differences in methodology between different research groups (different data sets, different setups for hyperspectral imaging, different modes for reflectance/transmittance/ interactance measurements, different techniques for prediction, etc.) we also consider that a direct comparison, with the same pipeline and samples, between different techniques provide better conclusions about the dimensionality reduction step and, in this matter, we show- case that t-SNE achieves better results than PCA for the same case stud- ies, which is a novelty since no other works can be found that report this superiority in real-world data.
As for the pH index estimation, t-SNE achieved very competitive re- sults, even obtaining the best RMSE and R2 values of all studies where the training and test sets use the same single vintage of wine grape berries; when comparing case studies with multiple vintages of wine



grape berries or with a different vintage used in the hold-out test set, despite showcasing a very strong performance, our model still falls a bit short out of the results obtained by the CNN in (Gomes et al., 2021a, 2021b): as mentioned previously, we believe that a direct com- parison between techniques provides more meaningful information re- garding the dimensionality reduction step, since it is hard to pinpoint if the difference in results derives from a weaker dimensionality reduction technique, from a weaker regressor, or from the overall differences in methodology and samples - in this matter and in our case studies, we once again highlight the superiority of t-SNE when compared to PCA, in- dependently of the regressor employed (Appendix A shows that while using a NN regressor the t-SNE still outperforms PCA).

Conclusions

An evaluation of the performance of t-SNE and Kernel t-SNE tech- niques for dimensionality reduction was carried out in real-world data of hyperspectral images of wine grape berries. Our study combined these techniques with a machine learning regressor (Support Vector Re- gression) to predict sugar content and pH index values in different vin- tages of wine grape berries, a highly complex problem due to high variability in samples across different vintages and varieties of wine grapes. Our results show that not only t-SNE is a technique suited for di- mensionality reduction tasks (it is normally used for visualization pur- poses only), but it even surpasses the results obtained by PCA for almost every single case study, a very rare achievement since most re- cent dimensionality reduction techniques introduced showcase a strong performance on artificial data sets but fail to replicate it in real-world data, usually falling short of the performance obtained by PCA. Addition- ally, Kernel t-SNE also showed a strong performance, opening the possi- bility of using a t-SNE extension for dimensionality reduction tasks in streaming data or online scenarios.
Comparing our results with state-of-art works, our model combined of a shallow learning regressor with t-SNE to reduce the dimensionality of the inputs achieved either better or on par results with the ones pre- sented in literature, even competing with works using more refined deep learning models (like CNNs); this highlights the capability of t- SNE, since previous models that combined a shallow learning regressor
with PCA could not compete with the results obtained by more ad- vanced models. Additionally, the development of accurate shallow learning models for oenological parameter estimation from hyperspectral images of wine grapes is extremely important, since ac- quiring the necessary number of samples to allow for the usage of a deep learning model that does not overfit to the training set and has generalization capacity is very costly and time-consuming.
Future works should expand on the possibility of using t-SNE in the dimensionality reduction step when combined with deep learning models, since it is possible that an increase in performance could be achieved: additionally, we also believe that further work in this area should be directed into deep learning models and artificial data synthe- sis, since it is very difficult and costly to obtain new samples and there is a huge variability present in the spectra of different varieties and vin- tages of wine grapes.

CRediT authorship contribution statement

Rui Silva: Conceptualization, Software, Validation, Formal analysis, Investigation, Writing – original draft, Visualization. Pedro Melo- Pinto: Methodology, Resources, Data curation, Writing – review & editing, Supervision, Project administration, Funding acquisition.

Declaration of Competing Interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influ- ence the work reported in this paper.

Acknowledgements

This work is supported by National Funds by FCT — Portuguese Foundation for Science and Technology, under the project UIDB/ 04033/2020. The authors also gratefully acknowledge the support from National funding by FCT, Portuguese Foundation for Science and Tech- nology, through the individual research grant (SFRH/BD/137216/ 2018) and from NVIDIA Corporation with the donation of the Titan X Pascal GPU used for this research.


Appendix A

Table A.1
Results obtained for pH index in the hold-out test set for case studies A with SVR and NN.
pH index

PCA	t-SNE	Kernel t-SNE
Model	Case













Table A.2
Results obtained for sugar content in the hold-out test set for case studies A with SVR and NN. Sugar content




Appendix B

Table B.1
Descriptive statistics for the pH index of all TF vintages.




Table B.2
Descriptive statistics for the sugar content of all TF vintages.




Appendix C

Table C.1
One-way ANOVA for the pH index of the laboratory results. Tukey simultaneous tests for differences of means

















H0: All means are equal; Significance: α = 0.05; Individual Confidence = 99.55%.

Table C.2
One-way ANOVA for the sugar content of the laboratory results. Tukey simultaneous tests for differences of means

















H0: All means are equal; Significance: α = 0.05; Individual Confidence = 99.55%.



Appendix D

Table D.1
Summary of the best results published in literature for the prediction of oenological parameters on hyperspectral images of wine grape berries.
RMSE	R2
C	(Gomes et al., 2021a)	CNN	1.085	0.183	–	–
(Silva and Melo-Pinto, 2021)	NN	1.060	–	0.986	–
PW: Present Work.


References

Alibert, Y., 2019. New metric to quantify the similarity between planetary systems: appli- cation to dimensionality reduction using t-SNE. Astron. Astrophys. 624, A45.
Anowar, F., Sadaoui, S., Selim, B., 2021. Conceptual and empirical comparison of dimen- sionality reduction algorithms (PCA, KPCA, LDA, MDS, SVD, LLE, ISOMAP, LE, ICA, t- SNE). Comput. Sci. Rev. 40, 100378.
Arana, I., Jarén, C., Arazuri, S., 2005. Maturity, variety and origin determination in white grapes (vitis vinifera l.) using near infrared reflectance technology. J. Near Infrared Spectrosc. 13, 349–357.
Bueno, J., Hernández-Hierro, J., Rodrguez-Pulido, F., Heredia, F., 2014. Determination of technological maturity of grapes and total phenolic compounds of grape skins in red and white cultivars during ripening by near infrared hyperspectral image: a pre- liminary approach. Food Chem. 152, 586–591.
Caballero, V., Pérez-Marn, D., López, M., Sánchez, M., 2011. Optimization of nir spectral data management for quality control of grape bunches during on-vine ripening. Sensors 11, 6109–6124.
Cao, F., Wu, D., He, Y., 2010. Soluble solids content and ph prediction and varieties dis- crimination of grapes based on visible–near infrared spectroscopy. Comput. Electron. Agric. 71, S15–S18.
Carbonneau, A., Champagnol, F., 1993. Nouveaux systèmes de culture integré du vignoble.
Programme AIR.
Costa, D., Mesa, N., Freire, M., Ramos, R., Mederos, B., 2019. Development of predictive models for quality and maturation stage attributes of wine grapes using Vis-nir reflectance spectroscopy. Postharvest Biol. Technol. 150, 166–178.
Cozzolino, D., Cynkar, W., Janik, L., Dambergs, B., Francis, L., Gishen, M., 2004. Measure- ment of colour, total soluble solids and ph in whole red grapes using visible and near infrared spectroscopy. Proceedings of the 12th Australian Wine Industry Techni- cal Conference, Melbourne, Australia, pp. 24–29.
Devassy, B., George, S., 2020. Dimensionality reduction and visualisation of hyperspectral ink data using t-SNE. Forensic Sci. Int. 311, 110194.
Du, T., 2019. Dimensionality reduction techniques for visualizing morphometric data: comparing principal component analysis to nonlinear methods. Evol. Biol. 46, 106–121.
Fadock, M., Brown, R., Reynolds, A., 2016. Visible-near infrared reflectance spectroscopy for nondestructive analysis of red wine grapes. Am. J. Enol. Vitic. 67, 38–46.
Fernandes, A., Oliveira, P., Moura, J., Oliveira, A., Falco, V., Correia, M., Melo-Pinto, P., 2011. Determination of anthocyanin concentration in whole grape skins using hyperspectral imaging and adaptive boosting neural networks. J. Food Eng. 105, 216–226.
Fernandes, A., Franco, C., Mendes-Ferreira, A., Mendes-Faia, A., da Costa, P., Melo-Pinto, P., 2015. Brix, pH and anthocyanin content determination in whole port wine grape berries by hyperspectral imaging and neural networks. Comput. Electron. Agric. 115, 88–96.
Gao, L., Gu, D., Zhuang, L., Ren, J., Yang, D., Zhang, B., 2019. Combining t-distributed sto- chastic neighbor embedding with convolutional neural networks for hyperspectral image classification. IEEE Geosci. Remote Sens. Lett. 17, 1368–1372.
Gisbrecht, A., Mokbel, B., Hammer, B., 2012. Linear basis-function t-SNE for fast nonlinear dimensionality reduction. The 2012 International Joint Conference on Neural Net- works, pp. 1–8.
Gisbrecht, A., Schulz, A., Hammer, B., 2015. Parametric nonlinear dimensionality reduc- tion using kernel t-SNE. Neurocomputing 147, 71–82.
Gomes, V., Melo-Pinto, P., 2021. Towards robust Machine Learning models for grape ripe- ness assessment. IEEE 18th International Joint Conference on Computer Science and Software Engineering (JCSSE), pp. 1–5.
Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2014a. Comparison of different approaches for the Prediction of Sugar Content in Whole Port Wine Grape Berries using Hyperspectral Imaging. ENBIS 14: 14th Annual Conference of the European Network for Business and Industrial Statistics, p. 1.
Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2014b. Determination of Sugar Content in Whole Port Wine Grape Berries Combining Hyperspectral Imaging with Neural Networks Methodologies. IEEE Symposium Series on Computational Intelligence.
Gomes, V., Fernandes, A., Faia, A., Melo-Pinto, P., 2017a. Comparison of different approaches for the prediction of sugar content in new vintages of whole port wine grape berries. Comput. Electron. Agric. 140, 244–254.
Gomes, V., Fernandes, A., Martins-Lopes, P., Pereira, L., Faia, A., Melo-Pinto, P., 2017b. Characterization of neural network generalization in the determination of pH and an- thocyanin content of wine grape in new vintages and varieties. Food Chem. 218, 40–46.
Gomes, V., Mendes-Ferreira, A., Melo-Pinto, P., 2021a. Application of hyperspectral imag- ing and deep learning for robust prediction of sugar and pH levels in wine grape berries. Sensors 21, 3459.
Gomes, V., Reis, M., Rovira-Más, F., Mendes-Ferreira, A., Melo-Pinto, P., 2021b. Prediction of sugar content in port wine vintage grapes using machine learning and hyperspectral imaging. Processes 9, 1241.
Gowen, A., O’Donnell, C., Cullen, P., Downed, G., Frias, J., 2007. Hyperspectral imaging – an emerging process analytical tool for food quality and safety control. Trends Food Sci. Technol. 18, 590–598.
Hall, A., Lamb, D., Holzapfel, B., Louis, J., 2002. Optical remote sensing applications in viti- culture - a review. Aust. J. Grape Wine Res. 36–47.
Hariharan, S., 2021. Analysing effect of t-SNE and 1-D CNN on performance of hyperspectral image classification. Turk. J. Comput. Math. Educ. 12, 1828–1833.
Hinton, G., Roweis, S., 2002. Stochastic neighbor embedding. Advances in Neural Informa- tion Processing Systems, pp. 833–840.
Janik, L., Cozzolino, D., Dambergs, R., Cynkar, W., Gishen, M., 2007. The prediction of total anthocyanin concentration in red-grape homogenates using visible-near-infrared spectroscopy and artificial neural networks. Anal. Chim. Acta 594, 107–118.
Lendasse, A., Wertz, V., Verleysen, M., 2003. Model selection with cross-validations and bootstraps—Application to time series prediction with rbfn models. Artificial Neural



Networks and Neural Information Processing—ICANN/ICONIP 2003. Springer,
pp. 573–580.
Miao, A., Zhuang, J., Tang, Y., He, Y., Chu, X., Luo, S., 2018. Hyperspectral image-based variety classification of waxy maize seeds by the t-SNE model and procrustes analysis. Sensors 18, 4391.
Office International de la Vigne and du Vin, 1990. Recueil des méthodes internationales d’analyse des vins et des moûts. OIV.
Pouyet, E., Rohani, N., Katsaggelos, A., Cossairt, O., Walton, M., 2018. Innovative data reduction and visualisation stratagy for hyperspectral imaging datasets using t-SNE approach. Pure Appl. Chem. 90, 493–506.
Remesan, R., Mathew, J., 2015. Model data selection and data pre-processing approaches.
Hydrological Data Driven Modelling. Springer, pp. 41–70.
Sanguinetti, G., 2008. Dimensionality reduction of clustered data sets. IEEE Trans. Pattern Anal. Mach. Intell. 30, 535–540.
Schulz, A., Hammer, B., 2015. Discriminative dimensionality reduction for regression problems using the fisher metric. International Joint Conference on Neural Networks,
pp. 1–8.
Silva, R., Melo-Pinto, P., 2021. A review of different dimensionality reduction methods for the prediction of sugar content from hyperspectral images of wine grape berries. Appl. Soft Comput. 113.
Silva, R., Gomes, V., Mendes-Faia, A., Melo-Pinto, P., 2018. Using support vector regression and hyperspectral imaging for the prediction of oenological parameters on different vintages and varieties of wine grape berries. Remote Sens. 10, 312.
Smola, A., Schölkopf, B., 2004. A tutorial on support vector regression. Stat. Comput. 14, 199–222.
Spiess, A., Neumeyer, N., 2010. An evaluation of r2 as an inadequate measure for nonlin- ear models in pharmacological and biochemical research: a Monte Carlo approach. BMC Pharmacol. 10, 1–11.
Van der Maaten, L., Hinton, G., 2008. Visualizing data using t-SNE. J. Mach. Learn. Res. 9. Vapnik, V., 1999. The Nature of Statistical Learning Theory. Springer Science & Business
Media.
Venna, J., Kaski, S., 2006. Visualizing Gene Interaction Graphs with Local Multidimen- sional Scaling. ESANN, Citeseer, pp. 557–562.
Wold, S., Esbensen, K., Geladi, P., 1987. Principal component analysis. Chemom. Intell. Lab.
Syst. 2, 37–52.
Zhang, J., Chen, L., Zhou, L., Liang, X., Li, J., 2018. An efficient hyperspectral image retrieval method: deep spectral-spatial feature extraction with DCGAN and dimensionality reduction using t-SNE-based NM hashing. Remote Sens. 10, 271.
