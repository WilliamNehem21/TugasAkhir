Array 1-2 (2019) 100001

		




Sparse representation of 3D images for piecewise dimensionality reduction with high quality reconstruction
Laura Rebollo-Neira *, Daniel Whitehouse
Mathematics Department, Aston University, B47ET, Birmingham, UK



A R T I C L E I N F O


Keywords:
Image representation with dictionaries Greedy pursuit algorithms
A B S T R A C T

Sparse representation of 3D images is considered within the context of data reduction. The goal is to produce high quality approximations of 3D images using fewer elementary components than the number of intensity points in the 3D array. This is achieved by means of a highly redundant dictionary and a dedicated pursuit strategy especially designed for low memory requirements. The benefit of the proposed framework is illustrated in the first instance by demonstrating the gain in dimensionality reduction obtained when approximating true color images as very thin 3D arrays, instead of performing an independent channel by channel approximation. The full power of the approach is further exemplified by producing high quality approximations of hyper-spectral images with a reduction of up to 371 times the number of data points in the representation.





Introduction

Sparse representation of 2D images has been a subject of extensive research in the last fifteen years [1–3]. Applications which benefit from sparsity range from image restoration [4,5] and classification [6–8] to feature extraction [9,10] and super resolution reconstructions [11,12]. While sparse representation of 3D arrays has received less attention, the advantage of modeling these arrays as a superposition of 3D elementary components is recognized in previous publications [13–16].
At present, the most widely used multichannel images in every day life are true color images. The simplest way of sparsely representing these images is channel by channel, or adding constraints of correlation across colors [4,17]. However, as demonstrated in this work, sparsity in the representation of true color images can increase substantially if the approximation is realized by means of 3D elements taken from a highly redundant dictionary. The effect is of course more pronounced for arrays involving more channels, such as hyper-spectral images.
From a practical view point, the current drawbacks of 3D sparse modeling using a large dictionary are (i) storage requirements and (ii) the complexity of the concomitant calculations. In this paper we propose a method which, by addressing (i) leaves room for possible high perfor- mance implementations using Graphics Processing Unit (GPU) pro- gramming. While the approach is illustrated using Central Processing Unit (CPU) programming, the storage requirements are shown to fit within 48 Kb's of fast access shared memory of a GPU when the
approximation of a 3D image is realized with a partition block size of 8 ×8 × 8 and with a separable dictionary of redundancy 125.
The main contributions of the paper are listed below.

The low memory implementation of the Orthogonal Matching Pursuit (OMP) strategy, called Self Projected Matching Pursuit (SPMP) [18] is dedicated to operating in 3D (SPMP3D) with separable dictionaries. This technique delivers an iterative solution to the 3D least squares problem which requires much less storage than direct linear algebra methods. It could therefore be also applied with any other of the pursuit strategies that include a least squares step [14,19–22].
The C++ MEX file for the SPMP3D method has been made available on a dedicated website [23]. All the scripts for reproducing the results of the paper in the MATLAB environment have also been placed on that website.
Remarkable reduction in the dimensionality of the representation of true color images and hyper-spectral images, with high quality reconstruction, is demonstrated using highly redundant and highly coherent separable dictionaries.

The results suggest that the method may be of assistance to image processing applications which rely on a transformation for data reduction as a first step of further processing. For examples of relevant applications we refer to Refs. [24–28].



* Corresponding author.
E-mail address: rebollol@aston.ac.uk (L. Rebollo-Neira).

https://doi.org/10.1016/j.array.2019.100001
Received 10 February 2019; Received in revised form 17 April 2019; Accepted 29 May 2019
Available online 20 June 2019
2590-0056/© 2019 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).



Notational convention
D = D x  D y  D zof three 1D dictionaries D x = {dx ∈ RNx

Mx
}m=1

D y = {dy ∈ RNy My
, and D z = {dz ∈ RNz Mz
, with MxMy Mz = M.

R represents the set of real numbers. Boldface letters are used to
m	m=1
m	m=1

indicate Euclidean vectors, 2D and 3D arrays. Standard mathematical fonts indicate components, e.g., d ∈ RN is a vector of components d(i)∈ R; i = 1;…;N. The elements of a 3D array I ∈ RNx ×Ny ×Nz are indicated as I(i;j;
m); i = 1;…; Nx; j = 1;…; Ny ; m = 1;…; Nz. Moreover, for each m-value
Im ∈ RNx ×Ny stands for the 2D array of elements Im(i;j) = I(i;j;m); i = 1;…;
Nx; j = 1;…;Ny , which, when not leaving room for ambiguity will also be


For computational purposes the 1D dictionaries are stored as three matrices Dx ∈ RNx ×Mx , Dy ∈ RNy ×My and Dz ∈ RNz ×Mz . Suppose now that a 3D array I ∈ RNx ×Ny ×Nz is to be approximated by an atomic decomposition
of the form
Ik = X c(n)dxx  dyy  dz z ;	(3)



GT.
The inner product between 3D arrays, say I ∈ RN ×N ×N and G
where for n = 1;…;kthe atoms dxx , dyy and dzz are selected from the given

RNx ×Ny ×Nz , is given as:

N	Ny  Nz
1D dictionaries. The common step of the techniques we consider for constructing approximations of the form (3) is the stepwise selection of
the atoms in the atomic decomposition. On setting k = 1and I0 = 0at

DG I
Xx X X



x  y	z

For G ∈ RNx ×Ny ×Nz with tensor product structure, i.e. for G = gx  gy 
gz, with gx ∈ RNx ; gy ∈ RNy and gz ∈ RNz , we further have


ℓx; ℓy; ℓz = arg max dx  dy  dz; Rk—1⟩


 ;	(4)

 G; I⟩3D
Nz
=	⟨gx; Imgy gz(m)= ⟨p; gz ;	(1)
m=1
k  k  k
n	i	s	3D
n=1;…;M
i=1;…;My s=1;…;Mz
 


where for each value of m the vector Imgy ∈ RNx arises by the standard matrix-vector multiplication rule and p ∈ RNz is given by its components
p(m) = ⟨gx; Imgy ⟩; m = 1; …; Nz. Note that ⟨p; gz⟩indicates the Euclidean
inner product in 1D, i.e.

Nz
p; gz⟩=	p(m)gz(m).
m=1
The definition (1)induces the norm I3D = p⟨ﬃﬃIﬃﬃ;ﬃﬃIﬃﬃ⟩ﬃﬃ3ﬃﬃDﬃﬃ.

Sparse representation of multi-channel images

Suppose that a 3D image, given as an array I ∈ RNx ×Ny ×Nz of intensity pixels, is to be approximated by the linear decomposition

k
k
n
n=1

where each c(n)is a scalar and each Dℓ is an element of RNx ×Ny ×Nz to be selected from a set, D = {Dn}M , called a ‘dictionary’.
A sparse approximation of I ∈ RNx ×Ny ×Nz is an approximation of the form (2) such that the number k of elements in the decomposition is significantly smaller than N = NxNy Nz. The terms in the decomposition
(2) are taken from a large redundant dictionary, from where the elements Dℓn in (2), called ‘atoms’, are chosen according to an optimality criterion. Within the redundant dictionary framework for approximation, the problem of finding the sparsest decomposition of a given multi-channel image can be formulated as follows: Given an image and a dictionary,


with Rk—1 = I — Ik—1. It is the determination of the coefficients c(n); n =
1; …; kin (3) that gives rise to pursuit strategies which go with different
names.

Matching Pursuit in 3D (MP3D)

The MP approach in 3D would simply calculate the coefficients in (3)
as
c(n)= Ddxx  dyy  dz z ; Rn—1⟩3D;  n = 1; …; k.	(5)
The main drawback of the MP method is that it may select linearly dependent atoms. Moreover, that approximation is not stepwise optimal because at iteration k the coefficients (5) do not minimize the norm of the residual error. The pursuit strategy that overcomes these limitations is the so called OMP [30].

Orthogonal Matching Pursuit in 3D

The implementation of OMP in 3D (OMP3D) we describe here is the 3D extension of the implementation of OMP in 2D given in Ref. [32]. An alternative algorithm called Kronecker-OMP, which is based on the Tucker representation of a tensor, is discussed in Ref. [14]. Our algorithm is based on adaptive biorthogonalization and Gram-Schmidt orthogo- nalization procedures, as proposed in Ref. [31] for the one dimensional case.
In order to ensure the coefficients c(n); n = 1;…;kinvolved in (3) are

approximate the image by the ‘atomic decomposition’ (2) such that the number k of atoms is minimum. Unfortunately, the numerical minimization of the number of terms to produce an approximation up to a desired
error, involves a combinatorial problem for exhaustive search. Hence, the
k 2 3D
fulfill that

k
= ⟨Rk;Rk⟩3D
is minimum, the decomposition (3) should

solution is intractable. Consequently, instead of looking for the sparsest
Ik = X c(n)dxx  dyy  dz z = bPV I;	(6)

solution, one looks for a ‘satisfactory solution’, i.e., a solution such that the number of k-terms in (2) is considerably smaller than the image dimension. For 2D images this can be effectively achieved by greedy

n=1


where





PVk
ℓn	ℓn	ℓn	k



is  the  orthogonal  projection  operator  onto  Vk =


[30] methods, if dedicated to 2D separable dictionaries [14,18,32,33].


		
PbV I. The required representation of PbV is of the form PbV I = Pk An⟨

natural.
Let's assume that a 3D dictionary is obtained as the tensor product
n
An = dxx  dyy  dxz . The concomitant biorthogonal reciprocal set Bk;

n = 1; …; kcomprises the unique elements of RNx ×Ny ×Nz satisfying the



conditions:
⟨An; Bk ⟩3D = δn;m = 8< 1  if n = m
where the coefficients a(n); n = 1;…;kare arbitrary numbers. Every array
I ∈ RNx ×Ny ×Nz can be expressed as
k

Vk = span{Bk}k  . :

For  ~Ikto	be	the	optimal	representation	of	Iin	V  =

span{dxx  dyy  dzz k  , in the sense of minimizing the norm of the


Thus, the coefficients c(n); n = 1; …; Nin (6) which guarantee mini-
ℓn	ℓn
R~
}
n=1

R~ = 0. The SPMP3D method fulfills

mum norm of the residual error are calculated as
residual , it should be true that PbVk



k n
The required arrays Bk; n = 1;…;kshould be upgraded and updated to account for each newly selected atom. Starting from k = 1; R0 = I, B1 =
this property by approximating R~ in Vk, via the MP method, and sub-
tracting that component from R~ . The following algorithm describes the
whole procedure.
Starting from k = 0and R0 = I, at each iteration, implement the steps below.

W1 = A1 = dxx  dyy  dzz , where

ℓ1	ℓ1	ℓ1
i) Increase k ← k +
1and apply the criterion (4) for selecting the triple of


and set

k  k k

k  k  k


ℓx ; ℓy ; ℓz =	arg max

 dx  dy  dz; Rk—1⟩ 
c(k)= Ddxx  dyy  dz z ; Rk—1⟩

1  1  1

n=1;…;Mx
n	i	s	3D
ℓk	ℓk	ℓk	3D

i = 1; …; My
s = 1; …; Mz 

at iteration k + 1the indices ℓx





; ℓy





; ℓz
 


corresponding to the new
and implement the update of the residue Rk =
dzz as follows:For s = 1; …Nzcalculate
k

Rk—1 —
c(k)dxx 
k
dyy 
ℓk

k+1
x	y
		

k+1
k+1
ΔRk

s	Rk—1
s	c k dx  dy  Tdz  s

reciprocal set Bk+1; n = 1;…;k + 1is adapted and upgraded by extending the recursion formula given in Ref. [31] as follows.
to update Rk as


Bk+1 = Bk — Bk+1 Ak 1 ; Bk ⟩
;  n = 1; …; k;
Rk = R
k—1
— ΔRk.


Bk+1 = Wk 1 ,  Wk 1  2 ;

with
n  n  n
selected atoms, and a tolerance


k
W	= A	n 	⟩ ;
ε for the projection error, realize the
orthogonal projection up to that error as follows. Set j = 1, R~ 0 =

	
including, for numerical accuracy, the re-orthogonalization step:
For n = 1; …; kevaluate
α(n) = Ddx  dy  dz ; R~ j—1 ⟩ ;	(9)

W	← W	X	Wn	W W
x	y	z
n	n	n




Although the image approximation is carried out by partitioning the images into relatively small 3D blocks, memory requirements of the OMP3D method are high. Indeed, the above are 2(k + 1)nonseparable
arrays each of dimension N = NxNy Nzwhich need to be stored in double
k* = arg max α(n) .	(10)
n=1;…;k
The value k* signalizes the indices ℓx ; ℓy ; ℓz corresponding to the

k	k	k

precision. Hence, we consider next a low memory implementation of the orthogonal projection step, which avoids having to store the arrays Wn; n = 1; …; kand Bk; n = 1; …; kand fully exploits the separability of the
already selected atoms with maximum correlation with the residual R~ j—1.
c(k*) ← c(k*)+ α(k*)

dictionary.
and for s = 1; …; Nzevaluate


Self Projected Matching Pursuit in 3D (SPMP3D)

ΔR~ j ( :; :; s)= α(k*) dx


dy
ℓk*

T z
ℓz*

(s)

The Self Projected Matching Pursuit (SPMP) methodology was introduced in Ref. [18] and conceived to be used with separable dictio- naries in 2D (SPMP2D). Because the technique is based on calculations of inner products, it can be easily extended to operate in 3D (SPMP3D).
Suppose that at iteration k the selection process has chosen the atoms

to update the residual R~ j as
R~ j = R~ j—1 — ΔR~ j .

labelled by the triple of indices {ℓx; ℓy ; ℓz k
and let ~Ikbe the atomic
If |α(k*)| < εstop. Otherwise update the coefficient


decomposition

k
n  n  n }n=1


This step subtracts from the residual a component in Vkand add that
k

~Ik = X a(n)dx

  dy B  dz ;	(7)
component to the approximation. ~I



Increase j ← j + 1and repeat the steps a) - c) to keep subtracting components of R~ j in Vkuntil iteration, J say, for which the stopping criterion b) is met. This criterion indicates that, up to tolerance ε, the
residual has no component in Vkso that one can set Rk = R~ J—1.

Continue with steps i) and ii) to keep enlarging Vkuntil, for a required tolerance error ρ, the condition Rk < ρis reached.
Remark 1.  For each fixed value k the rate of convergence


Algorithm 3Implementation of the self projection steps a) - c). Procedure [R~ ; c] = Proj3D(R; Dx ; Dy ; Dz ; L; c; ε; MaxJ).
Input: As in Algorithm 2, plus the coefficients of the atomic decomposition c, a tolerance parameter ε for the numerical error of the projection, and a maximum
number of permitted iterations, MaxJ.
Output: Orthogonal residual R~ . Coefficients ~c of the optimized atomic decomposition.
for j = 1 : MaxJ
{Selection of atoms using Algorithm 2}
[α*; k*] = SelTrip(R; Dx ; Dy ; Dz ; L)
{Check stopping criterion}

limI —
j→∞
R~ j =
bPVk
if |α*| < ε then
stop end if

through the steps a) - c) above is given in Ref. [34] for the one dimen- sional case. The proof for 3D is identical to that proof, because a 3D array can be represented as a long 1D vector. What varies is the implementa- tion. A vectorized version of the algorithm would not be applicable in this
{Update the coefficients}
c(k*) ← c(k*)+ α*
{Update the residual}
for s = 1 : Nz then
R(:; :; s) ← R(:; :; s)— α*(dxx )TR(:; :; s)dyy dz (s)

context.
end for end for
k*	ℓk*  k

Implementation details
{For clarity in the description only, we re-name here the residual and coefficients}
R = R; c = c

The bulk of the computational burden in the SPMP3D method lies in		
the realization of the selection of atoms (4). Algorithm 1 outlines a

procedure implementing the process. It should be stressed once again that the algorithm is designed to use as little memory as possible, rather than to reduce complexity.

Algorithm 1 Implementation of the selection of atoms (c.f. (4)) Procedure [α; ℓx; ℓy ; ℓz] = Sel3DAtom(R; Dx ; Dy ; Dz )
Input: 3D array R, matrices Dx , Dy Dz the columns of which are the atoms in the corresponding dictionaries.
Output: selected indices ℓx; ℓy ; ℓz, and α = ⟨dxx  dyy  dz z ; R⟩3D
Due to computational complexity and memory requirements, pursuit strategies using general dictionaries can only be implemented on an image partitioned into small blocks.
We consider nonoverappling blocks. The approximation of each block is carried out independently of the others. When the approximation of all the blocks is concluded, these are assembled together to produce the approximation of the whole image. While the sparsity results yielded by

{Initiate the algorithm}
ℓ	ℓ	ℓ
the OMP3D and the SPMP3D methods are theoretically equivalent, we

(Nz ; Mz )= size(Dz ) ; Mx = size(Dx ; 2); My = size(Dy ; 2) q = zeros(Mx ; My )α = 0
for m = 1 : Mz do
q(:; :) = 0
for s = 1 : Nz
q(:; :) = q(:; :)+ DTR(:; :; s)Dy dz (s)
end for
{Realize (4) by finding the partial maximum, and its argument, for each m-plane}
[l1 ; l2 ; q~] = max(|q(:; :)|)
ifq~ > αthen
α = q~; ℓx = l1 ; ℓy = l2 ; ℓz = m
end if
end for



At iteration k the outputs of Algorithm 1 are saved as c(k)= α and L(k;
1 : 3) = (ℓx;ℓy ;ℓz). The implementation details for selecting the triple of indices at the projection step are given in Algorithm 2. This is used in Algorithm 3 for the realization of the actual projection to recalculate the coefficients in the atomic decomposition.

Algorithm 2Selection of the triple of indices from the reduced dictionary (c.f. (10)) Procedure [α*; k*] = SelTrip(R; Dx ; Dy ; Dz ; L)
Input: As in Algorithm 1 plus the array L, with the triple of indices
L(n; 1 : 3)= (ℓx; ℓy ; ℓz ); n = 1…k
have seen that the latter implementation is much more economic in terms of storage demands. As discussed in Remark 2 below, this feature makes the SPMP3D algorithm suitable for possible GPU implementations using only the fast access shared memory. Assuming for simplicity in the no- tation that a 3D image is partitioned into cubes of size N3 and the dic-
tionaries D x, D y and D z are all of the same size Nb × rNb, where r > 1 is
the redundancy of the 1D dictionary, the SPMP3D algorithm storage needs are as follows.

Two N3 arrays for the intensity block in the image partition and the residual of the corresponding approximation.
Three matrices of size Nb × rNb for each dictionary, in case they are different.
A r2 × N2 array for the selection of indices in Algorithm 1.
A vector of k real numbers to store the coefficients of the atomic decomposition and k vectors of size 3 to store the indices of the atoms in the atomic decomposition. The value of k is the total number of atoms in the approximation of the block.

Since the stepwise complexity is dominated by the selection of indices

Output: *
n  n  n
(c.f. (10)) to update the coefficients and
(c.f. (4)), within this setup it is O(r3 N5) and for true color images

k and the corresponding values of α
residual {Initiate the algorithm}
α* = 0
for n = 1 : k do
p = 0
for s = 1 : Nz do
b
O(r3 N3).
Remark 2. By considering blocks of size 8 ×8 × 8 and dictionaries of redundancy r = 5 in each dimension, the above listed storage needs of
the SPMP3D algorithm comfortably fit the fast access shared memory of a

x T	y  z

p = p + (dℓx ) R(:; :; s)dℓy dℓz (s)
GPU in CUDA, which currently is 48 Kb. Indeed, in the worst-case sce-

n	n  n
end for	3

if |p| > |α*| then
k* = n and α* = p
end if end for
nario (corresponding to an approximation of zero error using k = 8 atoms for the approximation of an 8 ×8 × 8 block) SPMP3D would require 38 Kb to store most of the arrays in double precision, except for those with the selected indices which contain integer numbers. This still leaves 10 Kb for temporary variables to be used within calculations.


Mixed dictionaries

A key factor for the success in the construction of sparse representa- tions is to have a good dictionary. While a number of techniques for

atoms hi; i = 1; …; 4 in the left graph of Fig. 1 are generated by dis- cretization of linear B-spline functions of different support. For m = 1; 2; 3; 4 those functions are defined as follows:

learning dictionaries from training data have been proposed in the literature [35–42], they are not designed for learning large and highly coherent separable dictionaries. Nevertheless, previous works [18,32,33, 43] have demonstrated that highly redundant and highly coherent
separable dictionaries, which are easy to construct, achieve remarkable
x m
hm(x)=	2 —
m
>
if  0 ≤ x < m
if  m ≤ x < 2m


(11)

levels of sparsity in the representation of 2D images. Such dictionaries are not specific to a particular class of images. A discrimination is only made to take into account whether the approximation is carried out in the pixel
intensity or in the wavelet domain.
0	otherwise.

The remaining prototypes, h5; h6 and h7, in the left graph of Fig. 1 are generated taken the derivatives of the previous functions: h5(x) = 

'	'	'

As will be illustrated by the numerical examples in the next section, the approximation of the images we are considering are sparser when realized in the wavelet domain (wd). This entails the following steps:
(h2(x)) , h6(x)= (h3(x)) and h7 = (h4(x)) . The corresponding dictio- naries D Hm ; m = 1; …; 7 are built by discretization of the variable x in
(11) and sequential translation of one sampling point, i.e.,

D H = {wh (n)hm(i — n)|Nx; i = 1; …; Nx}M
;  m = 1; …; 7;

Apply a wavelet transform to each channel Im; m = 1; …; Nz to obtain	m	m
n=1

the arrays Um;m = 1;…;Nz. For the numerical examples we have used
the 9/7 Cohen-Daubechies-Feauveau biorthogonal wavelet transform
where the notation hm(i — n)|Nx indicates the restriction to be an array of size Nx. The numbers wh (n); n = 1; …; M; m = 1; …; 7 are normaliza-

[44].	m	x

Approximate the array U ∈ RNx ×Ny ×Nz exactly as it is done in the pixel domain (pd).
Apply the inverse wavelet transform to the approximated planes to
tion factors. The dictionary D Lp arises by the union of the dictionaries
D H ; m = 1; …; 7 i.e., D x = ∪7  D H . The whole mixed dictionary
D x is finally formed as D x = D x ∪D x∪D x . For the other dimension

recover the approximated intensity channels.
pd
we take D y
pd
= D x .
C	S	Lp


The mixed dictionary we consider for the 2D approximation consists of two sub-dictionaries: A trigonometric dictionary, D x , which is the common sub-dictionary for the approximation in both domains, and a dictionary of localized atoms, which contains atoms of different shapes
For approximations in the wd we use the dictionary of localized atoms D x as proposed in Ref. [33], which is built by translation of the pro- totype atoms pi; i = 1; …; 7 in the right graph of Fig. 1. Notice that p1 =
h1 and p3 = h5. The remaining prototypes are given by the vectors:
p2 = (1; 1; 0; 0; …; 0)⊥ ∈ RNx ; p4 = (1; 1; 1; 0; …; 0)⊥ ∈ RNx ; p5 =

when used in each domain.
x	(—1; 1; 1; 0; …; 0)⊥ ∈ RNx ;  p
=  (1; —1; 1; 0; …; 0)⊥ ∈ RNx ;  p  =

The trigonometric dictionary is the union of the dictionaries D C and	6	7

D x defined below:
(—1; —1; 1; 0; …; 0)⊥ ∈ RNx ; The corresponding dictionaries D P ; m = 1;
…; 7 are built as in the previous case by sequential translation of one

x	π(2i — 1)(n — 1)
 Mx
sampling point,

D C =
wc(n)cos
2Mx
; i = 1; …; Nx
n=1
D	= w
(n)p (i — n) N ; i = 1; …; N }M
;  m = 1; …; 7;

S	2Mx
n=1
where the numbers wpm (n); n = 1; …; M; m = 1; …; 7 are normalization

factors. The dictionaries D P ; m = 1; …; 7 give rise to D x = ∪7
D P .

where wc(n) and w (n); n = 1; …; M
are normalization factors, and
m	Lw
x	x	x
i=1	m

s	x	The latter generates the mixed dictionary D wd = D C∪D S∪D x
and

usually Mx = 2Nx. Thus, the trigonometric dictionary is constructed as	y	x	Lw

D x = D x ∪D x.
D wd = D wd.

T	C	S
x	The  corresponding  2D  dictionaries  D pd = D x
  D y
and

For approximations in the pd we add the dictionary, D Lp, which is	y
pd	pd

built by translation of the prototype atoms in the left graph of Fig. 1. This type of dictionary is inspired by a general result holding for continuous spline spaces. Namely, that spline spaces on a compact interval can be spanned by dictionaries of B-splines of broader support than the corre- sponding B-spline basis functions [45,46]. Thus, the first 4 prototype
D wd = D x D wd are very large, but never used as such. All the cal- culations are carried out using the 1D dictionaries. In order to demon- strate the gain in sparsity attained by the approximation of 3D images by
partitioning into 3D blocks, we use dictionaries D wd and D pd only for the approximation of the single channel 2D images. For the 3D case we


	
Fig. 1. The left graph illustrates the prototype atoms which generate by translation the dictionaries D Hm ; m = 1; …; 7. The prototypes in the right graph generate by translation the dictionaries.D Pm ; m = 1; …; 7.



maintain the redundancy of the 3D dictionary equivalent to that of the 2D
x
			

where for the 3D representation K = PQ 1kq; with kq the number of

Notice that D P1 is the standard Euclidean basis for RNx , also called the Dirac's basis, i.e., the basis arising by translation of the first atom in Fig. 1.
decomposition of a Nz-channel image, each channel is partitioned into
P = (Nx · Ny)/N2 blocks Ip z; p = 1;…; P, which are approximated by the

Notice that D~ x ⊂D x
and D~ x ⊂D x
. We also consider D~ y	D~ x
b	;
and

pd	pd
pd	wd
pd =	pd
2D atomic decompositions

~ z = D~ x , but taking Nx = Nz. The redundancy of the resulting dic- tionary D~	= D~ x  D~ y  D~ z is equivalent to the redundancy of the
  



kp;l
Ikp;l =	cp;l (n)dx



  dy

;  l = 1; …; N ;	(14)

D~ wd = D~ pd.
where the indices ℓx;p;l; ℓy;p;l are selected for each channel l by the

n	n

Numerical results
OMP2D algorithm. Accordingly, the number K in (13) is given as K =

PNz PP
kp;l, with kp;l the number of atoms in the atomic decomposi-

The merit of the simultaneous approximation of multiple channel images is illustrated in this section by recourse to two numerical exam- ples. Firstly we make the comparison between the sparsity produced by the joint approximation of the Red-Green-Blue (RGB) channel images partitioned into blocks of size Nb ×Nb × 3 and the sparsity obtained by the independent approximation of each channel partitioned into blocks of size Nb × Nb. Secondly, the full power of the approach is illustrated through the gain in sparsity attained by approximating hyper-spectral
images partitioned into 3D blocks, vs the plane by plane approximation.
tion (14).
Notice that the SR is a measure of the reduction of dimensionality for representing an image. The larger the value of the SR the smaller the dimensionality of the atomic decomposition representing the whole image. The required quality of the approximation is ensured with respect to the Mean Structural SIMilarity (MSSIM) index [47,48] and the clas- sical Peak Signal-to-Noise Ratio (PSNR), which for a 3D image is defined as

In both cases, once the approximation of each 3D block Iq
in the
PSNR = 10 log10
(Imax)2


MSE
I — IK
;  MSE = 	3D;

image partition is completed, for q = 1; …; Q the kq-term atomic decomposition of the corresponding block is expressed in the form
kq
Ikq =	cq(n)dxx;q  dyy;q  dz z;q .	(12)
Nx · Ny · Nz

where Imax is the maximum intensity range and IK the image approxi- mation.




The sparsity of the representation of an image of dimension N = Nx ·
Ny · Nz is measured by the Sparsity Ratio (SR), which is defined as:
SR = N;	(13)
K
Example I

In this example we use the Kodak data set consisting of 24 true color images shown in Fig. 2.
The approximations are realized in both domains by maintaining the




Fig. 2. Illustration of the Kodak data set consisting of 24 true color images, credit Rich Franzen [49]. The size of these images is 768 × 512 × 3, for most of them, except for numbers 4, 9, 10, 17, 18 and 19, which are of size 512 × 768 × 3. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)



same redundancy in the 2D and 3D dictionaries. For the independent approximation of the 2D channels the partitions are realized with blocks of size 8 × 8 and 16 × 16 (a partition of block size 24× 24 does not improve results for this data set). Accordingly, the simultaneous approximation of the 3 color channels involves partitions of block size 8 ×8 × 3 and 16 ×16 × 3 respectively.
As already discussed, for the independent approximation of the 2D channels we consider the dictionaries D pd (in the pd) and D wd (in the wd) as given in Sec. 3.4. For the simultaneous approximation of the 3
channels we consider the dictionaries D~ pd given in the same section.
Both dictionaries have redundancy of 125.


The average values of SR (SR), with respect to the 24 images in the set, are given in Table 1 for the approaches and partitions indicated by the first column.
All the results in the left half of the table correspond to PSNR = 45 dB and all the results in the right half correspond to PSNR = 41 dB. The third and fifth columns give the standard deviations (std). For completeness we have also produced the SR rendered by nonlinear thresholding of the wavelets coefficients (last row in the table). Notice that the resulting sparsity is poor in comparison with the other 2D results.
All the results were obtained in the MATLAB environment on a notebook 2.9 GHz dual core i7 3520 M CPU and 4 GB of memory. For the channel by channel approximation a C++ MEX file implementing OMP2D was used. For the 3D approximation SPMP3D was implemented by a C++ MEX file.
As observed in Table 1 the largest SR is achieved in the wd and partition 16 ×16 × 3 (c.f. last but one row of Table 1). However, the results obtained by partition 8 ×8 × 3 are very close (c.f. last row of the upper half of Table 1) and constitute a better tradeoff between SR and approximation time.
Fig. 3 shows the actual values of SRs for this partition in the wd for each of the 24 images in the data set (c.f. Fig. 2). The average time for the 3D approximation was 53 s per image.
Fig. 4 demonstrates the gain in visual quality obtained when the approximation of Images 3, 7 and 12 are realized simultaneously in 3D, instead of independently for each 2D channel. In both cases the SR is fixed at a high value SR = 63.5. While the 3D approximation is still of good quality (c.f. images on the left in Fig. 4) the distortion of the channel by channel approximation is very noticeable even at the scale of the figure (c.f. images on the right in Fig. 4).

 
Fig. 3. SR for the 45 dB approximation, in the wd, of each of the 24 images in the Kodak data set (c.f. Fig. 2 enumerated from top left to bottom right). The results for the independent approximation of each 2D color channel are repre- sented by the filled circles and those corresponding to the simultaneous approximation of the 3 channels are represented by the filled squares. The corresponding partitions are of size 8 × 8 and 8 × 8 × 3. (For interpretation of the references to color in this figure legend, the reader is referred to the Web version of this article.)

the three channels in the block, which is roughly the kq-value corre- sponding to the equivalent block in the gray scale image. The right graph corresponds to kq for each block of size 8 ×8 × 3 in the 3D approxima- tion. Both approximations are realized in the pd. The lower graph is the image given as 3 channels of 512 × 768 pixels each. It follows from the figure that the points corresponding to the 3D approximation give mode details about the image.

Example II

We consider now the approximation of the hyper-spectral images illustrated in Fig. 6. Details on the images acquisition and processing are described in Refs. [50–52].
All four images are of size 1016 × 1336 × 32, and have been
approximated in partitions of block size N × N , with N = 8; 16, and 24

As a final remark it is worth noting that the number kq of atoms in the
for the 2D approximation, and 8
b	b	b

approximation of each block q of an image partition produces a mean- ingful summary of local sparsity.
The upper graphs of Fig. 5 are a representation of the piecewise sparsity corresponding to Image 22 in the Kodak data set. Both graphs are arrays of 64 × 96 points. Each point corresponds to the number kq of
×8 × 8 for the 3D approximation. For the 2D channel by channel approximation we use the dictionaries D pd and D wd as defined in Sec. 3.4. For the 3D approximation we maintain
the redundancy as in 2D using the dictionary D~ pd introduced Sec. 3.4
and D~ a = D~ a .

atoms in the approximation of a block q. The left graph corresponds to block size 8 × 8 in the 2D approximation, by taking the average kq over
Because the range of intensity varies across the images, in order to compare SRs with different approaches we fix the Signal to Noise Ratio (SNR)


Table 1
Mean value of the SR, with respect to the 24 images in the set, obtained with the 2D and 3D approximations in both the pd and wd for two different sizes of the image partition. The last row in the table gives the results corresponding to standard nonlinear thresholding of wavelet coefficients, to achieve the same quality of the approximation as with the dictionaries: PSNR = 45dB (left half) and PSNR = 41dB (right half).




Fig. 4. Approximations of Images 3, 7 and 12 in the Kodak data set, for SR = 63.5. The images on the left are the 3D approximations. The images on the right are the 2D channel by channel approximations.


Fig. 5. The upper graphs are a representation of the piecewise sparsity corresponding to Image 22 in the Kodak data set. Both graphs are arrays of 64× 96 points. Each point corresponds to the number kq of atoms in the approximation of a block q. The left graph corresponds to the 2D approximation and the right graph to the 3D approximation. The lower graph is the image given as 3 channels of 512 × 768 pixels each.




Fig. 6. Illustration of the hyper-spectral images available on [53,54]. From top left to bottom right in Table 2 are labelled as Ribei., Graff., Rose, and Col. The of size of all four images is 1016 ×1336 × 32 pixels.



SNR = 10 log10
!.	(15)

excellent visual quality.
The SRs produced by the 3D approximation are indicated by SR3D


and


Every block in the partition is approximated up to the same error. With all the approaches, two global values of SNR (31 dB and 33 dB) were considered. These values of SNR correspond to the values of PSNR shown in Tables 2 and 3. In all of the cases the approximations are of



Table 2
Values of SR for the approximation in the pixel-intensity domain of the images listed in the first row. SR2D indicates the SR for the plane by plane approximation in partition of block side Nb = 8, 16, and 24. SR3D corresponds to a partition in 3D blocks of size 8 × 8 × 8. The times for completing the approximations are
given immediately below the sparsity results in minutes.
those produced by the 2D plane by plane approximation by SR2D. The times for completing the approximations are given in the row right after the corresponding sparsity result.
Remark 3. In both Tables 2 and 3 the values of SR3D are significantly larger than the values of SR2D, except for the Col. image and 24 × 24 blocks. For this image we were able to increase the 3D block size up to 16 ×16 × 16 and the results for SNR = 31dB are SR3D = 357 in the pd and SR3D = 892 in the wd (35 min and 10 min respectively). For SNR = 33 dB SR3D = 247 in the pd and SR3D = 590 in the wd (55 min and 20 min respectively).



Table 3
Same description as in Table 2, but the approximations are realized by applying
first a wavelet transform to each of the 32 channels.


Image	Ribei.	Graff.	Rose	Col. SNR = 31 dB




















Fig. 7. Illustration of two remote sensing hyper-spectral images taken from Ref. [55]. The graph on the left is the Urban image (size 320 ×320 × 128 pixels). The graph on the right is a portion of the University of Pavia image (256 ×256 × 96 pixels).




 
Fig. 8. SR vs SNR values for the 3D approximation in both the pd and wd for the Urban and University of Pavia remote sensing images.



On comparing the two tables a drastic improvement in the values of SR3D is observed when the approximation is realized in the wavelet domain. This feature is a consequence of the fact that the planes of the natural images are very sparse in the wavelet domain. In order to high- light differences we produce next the SR3D corresponding to the two remote sensing images in Fig. 7. The graph on the left represents the Urban remote sensing hyper-spectral image taken from Ref. [55]. The graph on the right is a portion of the University of Pavia image also taken
from Ref. [55].
Fig. 8 plots the SR vs four values of SNR, corresponding to the 3D approximations of the Urban and University of Pavia images in both the pd and wd.
Notice that the results in the pd are much closer to the results in the wd than they are in the case of the natural images in Fig. 6. This is because, as illustrated in Fig. 9, the planes of the remote sensing images are not as sparse in the wd as the planes of the natural images are.

Conclusions

High quality approximation of 3D images has been considered within the context of data reduction. A remarkable improvement in sparsity achieved by the simultaneous approximation of multiple channels has been illustrated through numerical experiments of different natures. Firstly it was demonstrated that a standard data set of RGB images can be approximated at high quality using far fewer elementary components if each image is treated as a very thin 3D array instead of as 3 independent 2D arrays. Secondly the full power of the approach was demonstrated through the approximation of hyper-spectral images. For the hyper- spectral natural images the sparsity is remarkably higher if the approx- imation is realized in the wavelet domain. For the remote sensing images the domain of approximation has less influence because, as opposed to natural images, these images are not as sparse in the wavelet domain as natural images are.
Taking into account the major reduction of dimensionality demon- strated by the numerical examples in this work, we feel confident that the proposed approach will be of assistance to the broad range of image processing applications which rely on a transformation for data reduction as a first step of further processing.





Fig. 9. Absolute value of the wavelet transform of a plane in the Col. image (left graph) and in the University of Pavia image (right graph).



Declaration of Competing Interest

The authors declare no conflict of interest.

Acknowledgments

We are also indebted to P. Getreuer, for making available the wave- letcdf97 MATLAB function that we have used for the transformation of each single channel image to the wavelet domain.

References

Wright J, Ma Yi, Mairal J, Sapiro G, Huang TS, Yan S. Sparse representation for computer vision and pattern recognition. Proc of the IEEE 2010;98:1031–44.
Elad M. Sparse and redundant representations: from theory to applications in signal and image processing. Springer; 2010.
Zhang Z, Xu Y, Yang J, Li X, Zhang D. A survey of sparse representation: algorithms and applications. IEEE access; 2015.
Mairal J, Eldar M, Sapiro G. Sparse representation for color image restoration. IEEE Trans Image Process 2008;17:53–69.
Dong W, Zhang L, Shi G, Li X. Nonlocally centralized sparse representation for image restoration. IEEE Trans Image Process 2013;22:1620–30.
Gou J, Hou B, Ou W, Mao Q, Yang H, Liu Y. Several robust extensions of collaborative representation for image classification. Neurocomputing 2018. https://doi.org/10.1016/j.neucom.2018.06.089.
Gou J, Yi Z, Zhang D, Zhan Y, Shen X, Du L. Sparsity and geometry preserving graph embedding for dimensionality reduction. IEEE Access 2018;6:75748–66. 10.1109/ ACCESS.2018.2884027.
Gou J, Wang L, Yi Z, Lv J, Mao Q, Yuan Y-H. A new discriminative collaborative neighbor representation method for robust face recognition. IEEE Access 2018;6: 74713–27. https://doi.org/10.1109/ACCESS.2018.2883527. 2018.
Wright J, Yang AY, Ganesh A. Robust face recognition via sparse representation. IEEE Trans Pattern Anal Mach Intell 2009;31:210–27.
Yuan XT, Liu X, Yan S. Visual classification with multitask joint sparse representation. IEEE Trans Image Process 2012;21:4349–60.
Yang J, Wright J, Huang T. Image super-resolution via sparse representation. IEEE Trans Image Process 2010;19:2861–73.
Zhang Y, Liu J, Yang W, Guo Z. Image super-resolution based on structure- modulated sparse representation. IEEE Trans Image Process 2015;9:2797–810.
Dabov K, Foi A, Katkovnik V. Image denoising by sparse 3-D transform-domain collaborative filtering. IEEE Trans Image Process 2007;16:2080–95.
Caiafa CF, Cichocki A. Computing sparse representations of multidimensional signals Using Kronecker Bases. Neural Comput 2013;25:186–220.
Cichocki A, Mandic D, De Lathauweri L, Zhou G, Zhao Q, Caiafai C, Phan HA. Tensor decompositions for signal processing applications: from two-way to multiway component analysis. IEEE Signal Process Mag 2015;32:145–63.
Dai Q, Yoo S, Kappeler A. Sparse representation-based multiple frame video super- resolution”. IEEE Trans Image Process 2017;26:2080–95.
Mousavi HS, Monga V. Sparsity-based color image super resolution via exploiting cross channel constraints. IEEE Trans Image Process 2017;26:5094–106.
Rebollo-Neira L, Bowley J. Sparse representation of astronomical images. J Opt Soc Am 2013;30:758–68.
Donoho DL, Tsaig Y, Drori I, Starck J. Stagewise orthogonal matching pursuit. IEEE Trans Inf Theory 2006;58:1094–121.
Needell D, Tropp JA. CoSaMP: Iterative signal recovery from incomplete and inaccurate samples”. Appl Comput Harmon Anal 2009;26:301–21.
Eldar Y, Kuppinger P, Bio€lcskei H. Block-sparse signals: uncertainty relations and
efficient recovery. IEEE Trans Signal Process 2010;58:3042–54.
Rebollo-Neira L, Matiol R, Bibi S. Hierarchized block wise image approximation by greedy pursuit strategies. IEEE Signal Process Lett 2013;20:1175–8.
L. Rebollo-Neira, http://www.nonlinear-approx.info/examples/node09.html (Last access April 2019)..
Tao JianWen, Wen Shiting, Hu Wenjun. Robust domain adaptation image classification via sparse and low rank representation. J Vis Commun Image Represent 2015;33:134–48.
Nie Wei-Zhi, Liu An-An, Su Yu-Ting. 3D object retrieval based on sparse coding in weak supervision. J Vis Commun Image Represent 2016;37:40–5.
Gao Z, Li SH, Zhu YJ, Wang C, Zhang H. Collaborative sparse representation leaning model for RGBD action recognition. J Vis Commun Image Represent 2017;48: 442–52.
Zhang Youqiang, Guo Cao, Li Xuesong, Wang Bisheng. Cascaded random forest for hyperspectral image classification. IEEE JSelect Top Appl Earth Observ Remote Sens 2018;11:1082–94.
Li Guiji, Peng Manman, Ke Nai, Li Zhiyong, Li Keqin. Visual tracking via context- aware local sparse appearance model. J Vis Commun Image Represent 2018;56: 92–105.
Mallat S, Zhang Z. Matching pursuit with time-frequency dictionaries. IEEE Trans Signal Process 1993;41(12):3397–415.
Pati YC, Rezaiifar R, Krishnaprasad PS. Orthogonal matching pursuit: recursive function approximation with applications to wavelet decomposition,” Proc. of the 27th ACSSC, vol. 1; 1993. p. 40–4.
Rebollo-Neira L, Lowe D. Optimized orthogonal matching pursuit approach. IEEE Signal Process Lett 2002;9:137–40.
Rebollo-Neira L, Bowley J, Constantinides A, Plastino A. Self contained encrypted image folding. Physica A 2012;391:5858–70.
Rebollo-Neira L. Effective sparse representation of X-Ray medical images. Int J Num Method Biomed Eng 2017. https://doi.org/10.1002/cnm.2886.
Rebollo-Neira L, Rozloˇzník M, Sasmal P. Analysis of a low memory implementation
of the Orthogonal Matching Pursuit greedy strategy. http://arxiv.org/abs/1 609.00053; 2017.
Kreutz-Delgado K, Murray JF, Rao BD, Engan K, Lee Te-Won, Sejnowski TJ. Dictionary learning algorithms for sparse representation. Neurocomputing 2003;15: 349–96.
Aharon M, Elad M, Bruckstein A. K-SVD: an algorithm for designing overcomplete dictionaries for sparse representation. IEEE Trans Signal Process 2006;54:4311–22.
Rubinstein R, Zibulevsky M, Elad M. Double sparsity: learning sparse dictionaries for sparse signal approximation. IEEE Trans Signal Process 2010;58:1553–64.
Tosi´c I, Frossard P. Dictionary Learning: what is the right representation for my
signal? IEEE Signal Process Mag 2011;28:27–38.
Zepeda J, Guillemot C, Kijak E. Image compression using sparse representations and the iteration-tuned and aligned dictionary. IEEE J Select Top Signal Process 2011;5: 1061–73.
Hawe S, Seibert M, Kleinsteuber M. Separable dictionary learning. In: Proceedings of the 2013 IEEE conference on computer vision and pattern recognition; 2013.
p. 438–45.
Srinivas M, Naidu RR, Sastry CS, Krishna Mohana C. Content based medical image retrieval using dictionary learning. Neurocomputing 2015;168:880–95.
Wen B, Ravishankar S, Bresler Y. Structured overcomplete sparsifying transform learning with convergence guarantees and applications. Int J Comput Vis 2015;114: 137–67.
Rebollo-Neira L. A competitive scheme for storing sparse representation of X-Ray medical images. PLoS One 2018. https://doi.org/10.1371/journal.pone.0201455.
Cohen A, Daubechies I, Feauveau JC. Biorthogonal bases of compactly supported wavelets. Commun Pure Appl Math 1992;45:485–560.
Andrle M, Rebollo-Neira L. Cardinal B-spline dictionaries on a compact interval. Appl Comput Harmon Anal 2005;18:336–46.
Rebollo-Neira L, Xu Z. Adaptive non-uniform B-spline dictionaries on a compact interval. Signal Process 2010;90:2308–13.
Wang Z, Bovik AC, Sheikh HR, Simoncelli EP. Image quality assessment: from error visibility to structural similarity. IEEE Trans Image Process 2004;13:600–12.
I. Kowalik-Urbaniak, D. Brunet, J. Wang, D. Koff, N. Smolarski-Koff, E. Vrscay, B. Wallace, and Z. Wang, “The quest for ‘diagnostically lossless’ medical image compression: a comparative study of objective quality metrics for compressed medical images”, Proc. SPIE 9037, Medical Imaging 2014: Image Perception, Observer Performance, and Technology Assessment, 903717 (March 11, 2014); doi: 10.1117/12.2043196.
R. Franzen, http://r0k.us/graphics/kodak/(Last access April 2019)..
Foster DH, Nascimento SMC, Amano K. Information limits on neural identification of colored surfaces in natural scenes. Vis Neurosci 2004;21:331–6.
Foster DH, Amano K, Nascimento SMC, Foster MJ. Frequency of metamerism in natural scenes. J Opt Soc Am 2006;23:2359–72.
Nascimento SMC, Amano K, Forster D. Spatial distributions of local illumination color in natural scenes. Vis Res 2016;120:39–44.
D. H. Foster, https://personalpages.manchester.ac.uk/staff/d.h.foster/Hyperspectra l_images_of_natural_scenes_04.html (Last access April 2019)..
D. H. Foster, http://personalpages.manchester.ac.uk/staff/d.h.foster/Local_Ill umination_HSIs/Local_Illumination_HSIs_2015.html (Last access April 2019)..
http://lesun.weebly.com/hyperspectral-data-set.html (Last access April 2019)..
