Combining clusterings in the belief function framework☆
Feng Li a, Shoumei Li a, Thierry Denœux b,c,*,1
a Beijing University of Technology, College of Applied Sciences, Beijing, China
b Universit�e de Technologie de Compi�egne, CNRS, Heudiasyc (UMR 7253), France
c Institut Universitaire de France, Paris, France
A R T I C L E I N F O
Keywords:
Evidence theory
Belief functions
Clustering ensemble
Intuitionistic fuzzy relation
A B S T R A C T
In this paper, we propose a clustering ensemble method based on Dempster-Shafer Theory. In the ﬁrst step, base
partitions are generated by evidential clustering algorithms such as the evidential c-means or EVCLUS. Base credal
partitions are then converted to their relational representations, which are combined by averaging. The combined
relational representation is then made transitive using the theory of intuitionistic fuzzy relations. Finally, the
consensus solution is obtained by minimizing an error function. Experiments with simulated and real datasets
show the good performances of this method.
1. Introduction
Clustering is an important task in Machine Learning and Pattern
Recognition. It is a statistical method to divide objects into groups, in
such a way that objects are similar within each group, and dissimilar
across different groups. Clustering methods have proved useful in many
real-world application domains, such as data mining, image segmenta-
tion, etc.
According to the form of clustering output, we can distinguish be-
tween hard and soft partitional clustering; the latter includes fuzzy and
evidential clustering. In particular, evidential clustering, based on
Dempster-Shafer (DS) theory (also called the theory of belief functions)
[8,31], has recently attracted the attention of many researchers.
Evidential clustering computes a credal partition, which describes cluster
membership uncertainty using DS mass functions.
In recent years, several evidential clustering algorithms have been
developed. Denœux and Masson (2004) [12] ﬁrst introduced an
evidential relational clustering method called EVCLUS. This method
ﬁnds a credal partition such that the degree of conﬂict between the mass
functions associated with any two objects match their dissimilarity.
Antoine et al. [3] proposed a constrained version of EVCLUS, called
CEVCLUS, which utilizes prior information provided as pairwise con-
straints. Denœux et al. [14] introduced a faster version of EVCLUS, called
k-EVCLUS, where a new cost function is deﬁned and optimized by an
iterative row-wise quadratic programming (IRQP) algorithm. Li et al.
[23] further expanded the k-EVCLUS method by taking prior knowledge
into account.
In Ref. [26], Masson and Denœux introduced the evidential c-means
algorithm (ECM), which is an extension of the classic and fuzzy c-means
in the framework of DS theory. The ECM alternatively searches for the
best credal partition and the best prototypes. Masson et al. [27] proposed
a variant of ECM for dissimilarity data, called RECM. Antoine et al. [2]
introduced a constrained version of ECM (called CECM) by considering
prior knowledge. Liu et al. [25] proposed another variant of the ECM
algorithm, called CCM, by introducing the notion of meta-cluster. Zhou
et al. [43] extended the median c-means and median fuzzy c-means to the
Median Evidential c-means (MECM). Denœux et al. (2015) [10] intro-
duced a new evidential clustering algorithm (Ek-NNclus) based on the
evidential k nearest neighbor rule.
Different clustering algorithms may obtain different clustering results
for one dataset, and even a single algorithm with different initializations
may yield different solutions. It is generally agreed that there is no best
single clustering algorithm [1]. To solve this problem and further
improve the robustness, consistency and stability of the solution, clus-
tering ensemble methods have emerged as an approach for combining
multiple clustering results into an improved solution. Among those, the
Evidence Accumulation Clustering (EAC) method [16,17] has attracted a
lot of attention. It constructs a co-association matrix from base partitions,
☆ This research was supported by grant No. 11571024 from NSFC, and by the Overseas Talent program from the Beijing Government.
* Corresponding author. Universit�e de Technologie de Compi�egne, CNRS, Heudiasyc (UMR 7253), France.
E-mail address: tdenoeux@utc.fr (T. Denœux).
1 Professor Thierry Denoeux, one of the authors of this paper, is the Editor-in-Chief of Array. The editorial process for this manuscript was handled independently
and the manuscript was subject to the Journal’s usual peer review process.
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100018
Received 11 October 2019; Received in revised form 20 December 2019; Accepted 21 January 2020
Available online 31 January 2020
2590-0056/© 2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).
Array 6 (2020) 100018
which can be processed by a hierarchical clustering algorithm to obtain
the ﬁnal solution.
In this paper, we introduce an evidential clustering ensemble method
that can be seen as an extension of the EAC method in the DS framework.
In our method, base evidential partitions are generated by evidential
clustering algorithms. As noted in Ref. [14], evidential partitions allow
for ambiguity, uncertainty or doubt in the assignment of objects to
clusters, and constitute a rich and informative description of the clus-
tering structure of a dataset. The base credal partitions are transformed
into their relational representations [11], which are combined using
different rules of DS theory. The fused relational representation corre-
sponds to the co-association matrix in the EAC method. It can be seen as
deﬁning an intuitionistic fuzzy relation [22], which is made transitive to
obtain an intuitionistic fuzzy equivalence relation. The ﬁnal evidential
partition is obtained by minimizing an objective function based on the
credal Rand index [11]. Experiments with real and simulated data show
that our approach can reveal the underlying clustering structure of
complex-shape datasets and achieve better results than EAC and single
clustering algorithms.
The rest of this paper is organized as follows. Basic notions are ﬁrst
recalled in Section 2. Our method is then described in Section 3, and
experimental results are reported in Section 4. Finally, Section 5 con-
cludes the paper.
2. Background
In this section, we brieﬂy introduce some basic notions used in this
paper. The main deﬁnitions of DS theory are ﬁrst recalled in Section 2.1.
The notions of credal partition and relational representation are then
reviewed in Section 2.2. Section 2.3 introduces intuitionistic fuzzy re-
lations and the transitive closure theorem. We then describe the EAC
method in Section 2.4, and we review some related work in Section 2.5.
2.1. DS theory
Let Ω ¼ fω1;…;ωcgbe a ﬁnite set. A mass function on Ωis a mapping
from the power set 2Ωto ½0; 1�, satisfying the condition
X
A�Ω
mðAÞ ¼ 1:
(1)
Each subset A of Ωsuch that mðAÞ > 0is called a focal set. In DS theory,
a mass function is viewed as a piece of evidence about some question of
interest, for which the true answer, denoted by ω, is supposed to be an
element of Ω. For any nonempty focal set A, mðAÞis a measure of the
belief that is committed exactly to A [31]. The mass mð ∅Þassigned to the
empty set is a measure of the belief that the true answer might not belong
to Ω. The mass mðΩÞis a measure of ignorance. A mass function is said to
be logical if it has only one focal set. It is said to be normalized if the empty
set is not a focal set, and unnormalized otherwise. An unnormalized mass
function m can be converted into a normalized one m�by Dempster's
normalization operation deﬁned by m�ð ∅Þ ¼ 0and
m�ðAÞ ¼
mðAÞ
1 � mð∅Þ:
(2)
Given a mass function m, the corresponding belief and plausibility
functions are deﬁned, respectively, as
belðAÞ ¼
X
∅6¼B�A
mðBÞ
and
plðAÞ ¼
X
B\A6¼∅
mðBÞ;
for all A � Ω. Clearly, functions beland plare linked by the relation
plðAÞ ¼ 1 � belðAÞ, where Adenotes the complement of A. The quantity
belðAÞrepresents the degree of total support in A, while plðAÞcan be
interpreted as the degree to which the evidence is consistent with A.
Different combination rules have been proposed in the literature. For
example, the conjunctive rule [32] and the dual disjunctive rule [33] are
deﬁned, respectively, as
(3a)
(3b)
for any two mass functions m1and m2 on the same frame Ωand all A � Ω.
As shown by Smets [33], the conjunctive rule assumes that all mass
functions to be combined are derived from reliable sources of informa-
tion, whereas the disjunctive rule only assumes that at least one source of
information is reliable, but we do not know which one. Dempster's rule,
denoted by �, is deﬁned by the conjunctive rule (3a) followed by
normalization (2), i.e., ðm1 �m2Þð ∅Þ ¼ 0 and
(4)
for all A 6¼ ∅.
The Dubois-Prade (DP) rule [15] assumes that when two sources are
not in conﬂict, they are both reliable, and at least one is correct when
they are conﬂicting. Speciﬁcally, for two mass functions m1and m2, their
combination by the DP rule, denoted by m1 ⊞ m2, is
ðm1 ⊞ m2ÞðAÞ ¼
X
B\C¼A
B\C6¼∅
m1ðBÞm2ðCÞ þ
X
B[C¼A
B\C¼∅
m1ðBÞm2ðCÞ; 8A 6¼ ∅:
(5)
It can be seen as a reasonable trade-off between the conjunctive and
disjunctive rules.
Let us now assume that mass function m represents our current state
of knowledge about ω, and we need to choose one or several elements of
Ωas our estimate about the true answer. Decision rules in the DS
framework are reviewed in Ref. [7]. Here, we only mention two rules that
will be used in the sequel. The maximum plausibility rule selects the
element ω�with the highest plausibility,
ω� ¼ argmax
ω2Ω plðfωgÞ:
(6)
This rule yields one single result. In contrast, the interval dominance
rule [7] is based on the following dominance relation: ω dominates ω
0, iff
belðfωgÞ > plðfω
0gÞ. Then the set of its maximal (non-dominated) ele-
ments can be obtained as
Ω� ¼ fω 2 ΩjplðfωgÞ � belðfω
0gÞ; 8ω
0 2 Ωg:
(7)
Instead of reaching a single decision, this rule selects a set of potential
results.
2.2. Credal partitions
Let O be a set of n objects, and Ω ¼ fω1;…;ωcgthe set of clusters. Each
object is assumed to belong to at most one cluster. Uncertain knowledge
about the cluster-membership of object oiis represented by a mass func-
tion mion Ω. A credal partition [12] is deﬁned as an n-tuple M ¼ ðm1; ::;
mnÞ. The notion of credal partition is more general than those of hard or
fuzzy partitions, and a credal partition can be summarized into a parti-
tion of any other type [9]. Credal partitions also encompass rough par-
titions as a special case [14,30]: a rough partition corresponds to a credal
partition in which all mass functions miare logical, namely miðAiÞ ¼ 1for
some Ai � Ω. The lower and upper approximations of cluster ωkcan then be
F. Li et al.
Array 6 (2020) 100018
2
deﬁned as follows,
ωL
k ¼ foi 2 O jAi ¼ fωkgg
(8a)
and
ωU
k ¼ foi 2 O jωk 2 Aig:
(8b)
For a non-logical mass function mi, the set Aican be selected by the
interval dominance rule (7), after which a rough partition can be
obtained.
Suppose miand mjare two mass functions related to objects i and j. We
consider the frame Θij ¼ fsij; :sijg, where sijmeans that “Objects i and j
belong to the same cluster”, and :sijmeans that “Objects i and j belong to
different clusters”. A mass function mijon Θijrepresenting our beliefs
about the joint cluster-membership of objects i and j can be computed
from miand mjas follows [11]:
mijð ∅ Þ ¼ mið ∅ Þ þ mjð ∅ Þ � mið ∅ Þmjð ∅ Þ;
(9a)
mij
��
sij
��
¼
X
k¼1
miðfωkgÞmjðfωkgÞ;
(9b)
mij
��
:sij
��
¼
X
A\B¼∅
miðAÞmjðBÞ � mijð ∅ Þ;
(9c)
mij
�
Θij
�
¼
X
A\B6¼∅
miðAÞmjðBÞ � mij
��
sij
��
;
(9d)
The tuple ℛ ¼ ðmijÞ1�i�j�nis called the relational representation of M.
We note that mijð ∅Þ ¼ 0whenever mið ∅Þ ¼ 0and mjð ∅Þ ¼ 0. Given two
credal partitions M and M
0and their relational representations ℛand
ℛ0,the credal Rand index [11] is deﬁned as
ρSð0Þ ¼ 1 �
P
i<jδ
�
mij; m
0
ij
�
nðn � 1Þ=2
;
(10)
where δ is the Jousselme's distance [20]. In the special case where mass
functions are normal, i.e., mijð ∅Þ ¼ m
0
ijð ∅Þ ¼ 0, we can write mass
function mijas a vector
mij ¼
�
mij
��
sij
��
; mij
��
:sij
��
; mij
�
Θij
��T:
Jousselme's distance between mijand m
0
ijis then deﬁned as
δ
�
mij; m
0
ij
�
¼
�1
2
�
mij � m
0
ij
�TJ
�
mij � m0ij
��1=2
;
where Jis the Jaccard matrix
J ¼
0
@
1
0
1=2
0
1
1=2
1=2
1=2
1
1
A:
(11)
The range of ρSis ½0; 1�, and it boils down to the Rand index when both
M and M
0are hard partitions. The credal Rand index measures the simi-
larity between any two soft partitions.
2.3. Intuitionistic fuzzy equivalence relation
Fuzzy relations. In classical set theory, an equivalence relation pro-
vides a partition of the underlying set into disjoint equivalence classes. In
fuzzy set theory, a fuzzy relation R on a ﬁnite set X is deﬁned as a fuzzy
subset of the Cartesian product X2, i.e., a mapping from X2to
Ref. ½0; 1�[29]. Each membership value Rðx; yÞrepresents the degree to
which x stands in relation R with y. For a fuzzy relation R, we deﬁne the
following properties:
Reflexivity:For all x 2 X Rðx; xÞ ¼ 1
Irreflexivity: For all x 2 X Rðx; xÞ ¼ 0
Symmetry: For all ðx; yÞ 2 X2 Rðx; yÞ ¼ Rðy; xÞ
Transitivity: For all ðx; y; zÞ 2 X3 minðRðx; yÞ; Rðy; zÞÞ � Rðx; zÞ
Furthermore, R is dual transitive iff 1 � Ris transitive. If a fuzzy rela-
tion is reﬂexive, symmetric and transitive, it is called a fuzzy equivalence
relation.
Sometimes, another triangular norm (t-norm) than the minimum is
used in the deﬁnition of transitivity. For a t-norm T, R is said to be T-
transitive if for all ðx;y;zÞ 2 X3,
TðRðx; yÞ; Rðy; zÞÞ � Rðx; zÞ:
The max-T composition for two fuzzy relations R and Q on X is the
fuzzy relation R ∘ Qdeﬁned by
ðR ∘ QÞðx; yÞ ¼ max
z2X TðRðx; zÞ; Qðz; yÞÞ:
(12)
Denoting R ∘ Ras R2, the T-transitivity property can be expressed as
R2 � R.
The max-T transitive closure Rof a fuzzy relation R is the smallest
max-T transitive fuzzy relation containing R. It can be computed as
R ¼ [
∞
i¼1 Ri;
(13)
where [ is the fuzzy set union based on the maximum t-conorm. In
particular, if R is a reﬂexive and symmetric fuzzy relation on a ﬁnite set X
of cardinality n, then R ¼ Rn�1[4].
Intuitionistic Fuzzy relations. The notion of Intuitionistic Fuzzy Relation
(IFR) is a further generalization of relations based on the theory of
intuitionistic fuzzy sets [22]. An intuitionistic fuzzy subset (IFS) A of X is
a pair of mappings μA : X → ½0; 1�and νA : X → ½0; 1�such that μAðxÞ þ
νAðxÞ � 1for each x 2 X. The values μAðxÞand νAðxÞrepresent, respec-
tively, the membership degree and non-membership degree of element x
in the set A. The pair ðμAðxÞ;νAðxÞÞis called an intuitionistic fuzzy value
(IFV).
Let L be the set of all IFVs, i.e., L ¼ fα ¼ ðμα;ναÞjμα 2 ½0; 1�;να 2 ½0; 1�;
μα þ να � 1g. A partial ordering relation on L can be deﬁned as follow:
α�Lα
0 ⇔ μα � μα'
and
να � να';
for all ðα; α
0Þ 2 L2. Any pair ðα; α
0Þ 2 L2has a unique least upper bound
α _ α
0and a unique greatest lower bound α ^ α
0given, respectively, by
α _ α
0 ¼ ðmaxðμα; μα'Þ; minðνα; να'ÞÞ
and
α ^ α' ¼ ðminðμα; μα'Þ; maxðνα; να'ÞÞ:
Thus ðL;�LÞis a complete lattice, with top ð1; 0Þand bottom ð0; 1Þ.
An IFR on a non-empty set X is an IFS of X2, i.e., a mapping R: X2 → L.
For an IFR R we deﬁne the following properties:
Reflexivity: For all x 2 X Rðx; xÞ ¼ ð1; 0Þ;
Symmetry: For all ðx; yÞ 2 X2 Rðx; yÞ ¼ Rðy; xÞ;
Transitivity: For all ðx; y; zÞ 2 X3 Vy2XðRðx; yÞ ^ Rðy; zÞÞ�LRðx; zÞ;
A reﬂexive, symmetric and transitive IFR is called an Intuitionistic
Fuzzy Equivalence Relation (IFER). An IFR R ¼ ðμR; νRÞis an IFER if and
only if μRis reﬂexive, symmetric and transitive, and νRis irreﬂexive,
F. Li et al.
Array 6 (2020) 100018
3
symmetric and dual transitive [22]. Consequently, the transitivity of an
IFR can be obtained by making μRand 1 � νRtransitive. In the experiments
reported in Section 4, we consider three t-norms: the minimum, the
product, and the Lukasiewicz t-norm deﬁned as Tða;bÞ ¼ maxð0;a þ b �
1Þ.
2.4. Evidence Accumulation Clustering
In this section, we brieﬂy summarize the EAC method [16,17]. More
details can be found in Refs. [18,38,42]. The EAC method uses the
co-association matrix to avoid the label correspondence problem. More
precisely, assume that a dataset has n objects O ¼ fo1;o2;…;ong. Suppose
that N base partitions P1;…;PNhave been obtained in the ﬁrst step. In the
second step, each base partition Pbis mapped to a co-association matrix
Sbof size N � Nwith general term sb
ij ¼ Iðcb
i ¼ cb
j Þ; where cb
i is the cluster
index of xiin Pband I is the indicator function. The co-association matrix,
denoted as S� ¼ ðs�
ijÞ, is the average of all Sb; its general term is
s�
ij ¼ 1
N
X
N
b¼1
sb
ij:
(14)
Each element s�
ijrepresents the proportion of base partitions in which
objects oiand ojare assigned to the same cluster. The co-association ma-
trix can be treated as a new similarity matrix and used as input to single-
linkage hierarchical clustering.
In EAC, the co-association matrix is computed by only taking into
account whether two objects belong to the same cluster or not. Some
researchers [19,37,40] have proposed to use additional information to
construct a similarity measure that is more expressive about the rela-
tionship between objects. For instance, Yang [41] proposed a fuzzy
co-association matrix to summarize the ensemble of fuzzy partitions,
where the membership of an object to clusters is expressed by a fuzzy
membership function.
We can remark that, in the EAC method, the numbers of clusters in the
base partitions do not need to be close to the “true” number of clusters.
Indeed, in Ref. [17], the authors construct the base partitions using the
k-means algorithms with a large (and sometimes random) number of
clusters. For instance, in one dataset with two clusters, they construct
base partitions with up to 80 clusters. The underlying assumption is that
objects that truly belong to true same cluster are likely to be assigned to
the same cluster in different partitions, which is summarized in the
co-association matrix. The number of clusters in the ﬁnal aggregated
partition is determined by analyzing the dendrogram after applying
single-link hierarchical clustering to the co-association matrix.
In this paper, we propose to extend the EAC method in the framework
of DS theory. In our method, the membership of an object to clusters is
represented by a mass function in the credal partition, which contains
more information than fuzzy and hard partitions. To better exploit this
type of information, we use the relational interpretation recalled in
Section 2.2 to measure the “similarity” between objects, which can be
seen as a generalization of the co-association matrix. To capture the
neighborhood relationship, we make the combined relational represen-
tation transitive based on IFR theory recalled in Section 2.3.
2.5. Other related work
In the ensemble clustering literature, most contributions focus on
hard partitions, and some are based on fuzzy partitions. Only a few
methods are based on DS theory. DS theory is a sound approach for
ensemble clustering methods, because it provides ways to combine
different pieces of evidence. Preliminary results have already demon-
strated the feasibility of this approach. For example, in Refs. [13,28] the
authors propose to deﬁne mass functions on the lattice of interval par-
titions of a set of objects; they obtain a consensus belief function by a
suitable combination rule, and use hierarchical clustering to get the ﬁnal
partition. Unlike other direct methods relying on a voting process, Li
et al. [24] introduce another direct approach based on Dempster's rule of
combination; their method consists of two steps: ﬁnding the correspon-
dence labels and using the combination rule to produce the ﬁnal result.
Although these ensemble clustering methods are rooted in Dempster's
theory, they still consider hard partitions as input and also output of the
procedure. Wang et al. [39] propose an ensemble clustering method for
evidential partitions. After solving the label correspondence problem, the
ﬁnal results are obtained by combining the selected base partitions. Due
to high computational complexity, they only consider the ﬁxed (true)
number of clusters in the base partitions.
In this paper, we combine credal partitions, a very general form of
partitions that can be generated by hard, fuzzy, rough or evidential
clustering algorithms. After mapping the base partitions to their rela-
tional representations, we combine these partitions in a coarser frame,
where we only need to consider two focal sets. The combined relational
representation is made transitive using the theory of IFRs. Finally, we
generate an evidential partition that matches the combined relational
representation, providing a much more informative output than can be
obtained using hard or fuzzy clustering methods.
3. Evidential clustering ensemble method
In this section, we introduce the proposed method. The generation
and combination of base credal partitions are described in Section 3.1.
The computation of transitive closures to make the combined relational
representation transitive is then addressed in Section 3.2, and the method
for computing the ﬁnal credal partition is presented in Section 3.3.
3.1. Generation of base partitions
In the ﬁrst step of our method, base partitions can be obtained by
hard, fuzzy and rough clustering methods, which all produce special
forms of credal partitions. In this paper, we focus on base partitions
generated by evidential clustering methods. We assume that we start
with N base partitions Mb ¼ ðmb
1; …; mb
nÞ, b ¼ 1; …; Ngenerated by
evidential clustering algorithms such as ECM or EVCLUS. The number of
clusters in base partition b is denoted by kb. Before converting credal
partitions to their relational representations, we compute the average
mass assigned to the empty set for each object i as
bmið ∅ Þ ¼ 1
N
X
N
b¼1
mb
i ð ∅ Þ;
i ¼ 1; …; n;
(15)
and we normalize each base credal partition by (2). We denote the b-th
normalized credal partition as Mb* ¼ ðmb*
1 ; …; mb*
n Þ, where mb*
i is the
normalized mass function deﬁned by mb*
i ð ∅Þ ¼ 0and
mb*
i ðAÞ ¼
mb
i ðAÞ
1 � mb
i ð∅Þ;
(16)
for all nonempty subset A of Ω. The reason for this normalization is that
the analogy between relation representations and IFRs developed in
Section 3.2 requires the mass functions to be normalized. However, we
cannot just discard the mass on the empty set, because it is useful to
detect outliers. This information will be utilized in the last step of our
method (see Section 3.3).
After the relational representations ℛb*have been computed, they can
be combined using different rules. The combined relational representa-
tion is denoted by ℛ� ¼ ðm�
ijÞ1�i�j�n. For example, using the average rule
we get
F. Li et al.
Array 6 (2020) 100018
4
m�
ij ¼ 1
N
X
N
b¼1
mb*
ij :
We can remark that, when the base partitions are hard, the n �
nmatrix with general term ½m�
ijðfsgÞ�boils down to the co-association
matrix (14). As a consequence, EAC is a special case of our method.
Other combination rules introduced in Section 2.1 can also be used.
3.2. Transitivity
The combined relational representation ℛ�sometimes cannot be
successfully exploited because it lacks a notion of transitivity, namely: if
we believe that objects i and j belong to the same cluster, and objects j
and k also belong to the same cluster, then we should believe that this is
also the case for objects i and k. When applying hierarchical clustering in
the second step, this property is used implicitly in the EAC method.
A relational representation is not a classical fuzzy relation, because it
speciﬁes two numbers for each pair ði;jÞ; the degree of belief that i and j
belong to the same cluster, and the degree of belief that they do not
belong to the same cluster. We can observe the similarity between this
kind of information and IFRs recalled in Section 2.3. Using this formal
analogy, we consider the combined relational representation ℛ�as an
IFR, and we make it transitive using the techniques reviewed in Section
2.3.
For a normalized mass function m�
ijon Θij, the degrees of belief in
sijand :sijare, respectively,
bel�
ij
��
sij
��
¼ m�
ij
��
sij
��
and
bel�
ij
��
:sij
��
¼ m�
ij
��
:sij
��
;
with bel�
ijðfsijgÞ þ bel�
ijðf:sijgÞ � 1. These numbers deﬁne an IFR R ¼ ðμR;
νRÞon the set O of objects, with
μR
�
oi; oj
�
¼ bel�
ij
��
sij
��
and
νR
�
oi; oj
�
¼ bel�
ij
��
:sij
��
:
This IFR is reﬂexive (as bel�
iiðfsiigÞ ¼ 1and bel�
iiðf:siigÞ ¼ 0for all i) and
symmetric, but it is usually not transitive. As recalled in Section 2.3, R is
transitive iff μRis transitive and νRis dual transitive, i.e., νd
R ¼ 1 � νRis
transitive. To obtain an IFER, we thus need to replace μRand νd
Rby their
transitive closures. We observe that
νd
R
�
oi; oj
�
¼ 1 � bel�
ij
��
:sij
��
¼ pl�
ij
��
sij
��
:
Denoting by μRand νd
Rthe transitive closures of μRand νd
R, let
belij
��
sij
��
¼ μR
�
oi; oj
�
and
plij
��
sij
��
¼ νd
R
�
oi; oj
�
:
These belief and plausibility values correspond to new mass functions
mijsuch that mijð ∅Þ ¼ 0and
mij
��
sij
��
¼ belij
��
sij
��
;
(17a)
mij
��
:sij
��
¼ 1 � plij
��
sij
��
;
(17b)
mij
�
Θij
�
¼ plij
��
sij
��
� belij
��
sij
��
:
(17c)
The new relational representation ℛ ¼ ðmijÞ1�i�j�nwill hereafter be
referred to as the transitivized combined relational representation.
3.3. Recovering the combined credal partition
In the EAC method, the ﬁnal result is obtained by applying hierar-
chical clustering to the association matrix. We could apply the same
procedure to matrices ðbelijðfsijgÞÞor ðplijðfsijgÞ, similarly to what was
proposed in Ref. [28]. However, our objective is to recover a credal
partition, which cannot be obtained by a hierarchical clustering
algorithm.
Our approach is to ﬁnd a normalized credal partition M�whose
relation representation is as close as possible to the transitivized com-
bined relational representation ℛ, closeness being measured by the
credal Rand index (10). We need to ﬁx the number krof clusters as well as
the focal sets in the credal partition M�. For example, we can take the
singletons and Ωas focal sets, or we can also consider some pairs of
clusters.
We thus seek a normal credal partition M�, solution of the maximi-
zation problem
max
M� ρSðℛðM�Þ; ℛÞ;
(18)
where ℛðM�Þis the relational representation of M�. From (10), maxi-
mizing (18) is equivalent to minimizing the following stress (error)
function:
SðM�Þ ¼
X
i<j
�
m�
ij � mij
�T
J
�
m�
ij � mij
�
;
(19)
where
m�
ij ¼
�
m�
ij
��
sij
��
; m�
ij
��
:sij
��
; m�
ij
�
Θij
��T
;
mij ¼
�
mijðfsgÞ; mijðf:sgÞ; mij
�
Θij
��T;
and Jis the Jaccard matrix (11). To solve (19), let us write m�
ijas a function
of mass functions m�
i and m�
j in matrix form. Assuming that each mass
function m�
i has f focal sets F1;…;Ff , it can be written as an f-vector m�
i ¼
ðm�
i ðF1Þ; …; m�
i ðFf ÞÞT, and the credal partition can be written as an n �
fmatrix M� ¼ ðm�
1; …; m�
nÞT. Let S ¼ ðSkℓÞand C ¼ ðCklÞbe the f �
fmatrices deﬁned as follows:
Skl ¼
�
1
if k ¼ l and jFkj ¼ 1
0
otherwise;
(20)
and
Ckl ¼
�
1
if Fk \ Fl ¼ ∅;
0
otherwise:
(21)
We have:
m�
ij
��
sij
��
¼
�
m�
i
�TS m�
j ;
(22a)
m�
ij
��
:sij
��
¼
�
m�
i
�TC m�
j ;
(22b)
m�
ij
�
Θij
�
¼ 1 � m�
ij
��
sij
��
� m�
ij
��
:sij
��
:
(22c)
We can observe that m�
ijis linear in m�
i and, consequently, SðM�Þis
quadratic in m�
i , the other mass functions being ﬁxed. Consequently, we
can minimize SðM�Þusing a cyclic coordinate descent algorithm, mini-
mizing SðM�Þwith respect to each m�
i in turn, while keeping the other
m�
j constant; this is the iterative row-wise quadratic programming (IRQP)
algorithm [35], also used in Ref. [14]. Using this approach, we minimize
F. Li et al.
Array 6 (2020) 100018
5
at each step the following cost function:
gi
�
m�
i
�
¼
X
n
j¼1
j6¼i
�
m�
ij � mij
�T
J
�
m�
ij � mij
�
;
(23)
which is quadratic in m�
i . To simplify the expression of this function, let
us deﬁne the matrix Ajof size 3 � 3fas
Aj ¼ I3 �
�
m�
j
�T
:
(24)
where I3is the identity matrix of size 3 � 3and � is the Kronecker
product, and the matrix Bof size 3f � fas
B ¼
0
@
S
C
1 � S � C
1
A:
(25)
with these notations, (23) can be written as
giðmiÞ ¼
X
n
j¼1
j6¼i
�
AjBm�
i � mij
�TJ
�
AjBm�
i � mij
�
(26)
Developing the right-hand side of (26) and rearranging the terms, we
obtain
gi
�
m�
i
�
¼
�
m�
i
�TΣm�
i þ uTm�
i þ c0;
(27)
with
Σ ¼ BT
0
B
B
B
B
@
X
j¼1
j6¼i
AT
j JAj
1
C
C
C
C
A
B
(28a)
u ¼ � 2
0
B
B
B
B
@
X
j¼1
j6¼i
mT
ijJAj
1
C
C
C
C
A
B
(28b)
c0 ¼
X
j¼1
j6¼i
mT
ijJmij:
(28c)
Minimizing gðmiÞunder the constraints ðm�
i ÞT1 ¼ 1(where 1 ¼
ð1; …; 1ÞT) and m�
i � 0is a quadratic programming (QP) problem, which
can be solved efﬁciently with any QP solver. As we iteratively update
each row of M�, the overall stress SðM�Þdecreases and eventually reaches
a local minimum. As in Ref. [14], we compute the following running
mean after each cycle of the algorithm as e0 ¼ 1and
et ¼ 0:5et�1 þ 0:5 jSt � St�1j
St�1
;
(29)
where t is the iteration counter and Stis the error at iteration t. The al-
gorithm stops when etbecomes less than some given threshold ε.
Let bM
� ¼ ð bm�
1;…; bm�
nÞdenote the normalized credal partition obtained
after convergence of the algorithm. The last step is to “denormalize” it
using the masses bmð ∅Þon the empty set computed in (15). This is done by
multiplying each mass bm�ðAÞwith A 6¼ ∅by 1 � bmð ∅Þ:
bmiðAÞ ¼ ½1 � bmið ∅ Þ�bm�
i ðAÞ;
(30)
for all A 2 2Ω n f ∅g. The ﬁnal consensus credal partition is bM ¼ ð bm1;
…; bmnÞ. The whole procedure is summarized in Algorithm 1.
Algorithm 1
Summary of the method.
Require N credal partitions Mb ¼ ðmb
1; …; mb
nÞ
T, b ¼ 1;…;N, combination rule,
number krof clusters and focal sets F1;…;Ff of the combined credal partition
Compute bmið ∅Þ, i ¼ 1;…;nusing (15)
Normalize the base credal partitions by (16)
Compute the base relational representations ℛb* ¼ ðmb*
ij Þ, b ¼ 1;…;N
Compute the combined relational representation ℛ� ¼ ðm�
ijÞR ¼ ðmijÞ
t ← 0, e0 ← 1
Initialize M�randomly, compute its relational representation
Compute S0using (19)
while et � εdo
t ← t þ 1
St ← 0
for i ¼ 1to n do
Compute Ajfor all j 6¼ iusing (24)
Compute Σ, uand c using (28)
Find m�ðtÞ
i
by minimizing (27) subject to ðm�
i ÞT1 ¼ 1and m�
i � 0
Replace row i of M�by ðm�ðtÞ
i
Þ
T
St ← St þ giðm�ðtÞ
i
Þ
end for
et ← 0:5et�1 þ 0:5jSt � St�1j=St�1
end while
Let bM
� ¼ ð bm�
1;…; bm�
nÞwith bm�
i ¼ m�ðtÞ
i
, i ¼ 1;…;n
Denormalize the combined credal partition using (30)
return Credal partition bM
Concerning the time complexity of the method, converting each base
credal
partition
to
its
relational
representations
requires
Oðn2Þoperations, where n is the number of objects, and combining the
average partition takes Oðn2NÞ. To compute the transitive closure, Lee
[21] describes an optimal algorithm with Oðn2Þtime complexity. Conse-
quently, the calculation of the transitivized combined relational repre-
sentation
ℛcan
be
performed
in
Oðn2NÞoperations.
The
most
computationally demanding step of the method is to recover the credal
partition from the obtained relational representation. The complexity of
this step depends on the Quadratic Programming (QP) problem (27)
solved at each iteration. As the Jaccard matrix (11) is positive deﬁnite
[5], so is matrix Σin (27) (of size f � f, where f is the number of focal
sets). Consequently, the quadratic function (27) being minimized in
convex. It is known [36] that convex QP problems can be solved in
polynomial time. The computing time of the optimization can be
controlled by limited the number of focal sets in the recovered credal
partition.
4. Experiments
In this section, we evaluate our approach using various datasets. In
Section 4.1, we study the inﬂuence of different parameters using simu-
lated datasets. Detailed results with simulated and real datasets are then
reported in Section 4.2.
4.1. Qualitative experiments
In this section, we present the results on simulated datasets to
investigate different stages of the ensemble procedure. We study the ef-
fect of different combination rules and transitive closure with different t-
norms.
Effect of different combination rules
We ﬁrst investigate the effect of different combination rules on the
Fourclass data, consisting of four classes in two-dimensional space, each
generated from a Gaussian distribution. From Fig. 1a, we can see that the
F. Li et al.
Array 6 (2020) 100018
6
Fig. 1. Results for the Fourclass data with different combination rules: original data (a), one of the base partitions (b), average rule (c), Dempster's rule (d), disjunctive
rule (e), DP rule (f).
F. Li et al.
Array 6 (2020) 100018
7
clusters overlap, and there is an outlier.
In the ﬁrst step, we ran the ECM algorithm [6,26] with the Fourclass
dataset for generating base partitions. To obtain useful information, we
use the two-step approach introduced in Refs. [14,34] to extract infor-
mative focal sets. More precisely, the ECM algorithm was ﬁrst run with ∅,
the singletons and Ωas focal sets. Based on the obtained credal partition,
the similarity between each pairs of clusters fωj;ωlgwas measured by sðj;
lÞ ¼ Pn
i¼1plijplil, where plijand plilare the normalized plausibility that
object i belongs, respectively, to cluster j and l. The pairs fωj;ωlgof mutual
K ¼ 2nearest neighbors were then selected as informative focal sets. In
the second step, the evidential clustering initialized with the credal
partition computed in the previous step was run again with those infor-
mative focal sets. In Ref. [14], this method has been shown to be tractable
and to yield good results even when the number of clusters is large.
In this experiment, we generated N ¼ 20base partitions, each of
which had kb ¼ 15clusters, and we set δ to the 0.2-quantile of the dis-
similarities between objects. The diversity of the partitions is obtained by
setting the number of clusters to a large number and using different
random initializations. We considered the four combination rules
reviewed in Section 2.1: average, Dempster, disjunctive and DP. For
simplicity, after combining the relational representations we did not
make the corresponding IFR transitive. We recovered a credal partition
with four clusters, kr ¼ 4. In the second step, we ﬁrstly recovered a credal
partition with the mass on the singletons and Ω, and secondly with the
mass on the informative focal sets as we did in the generation step. One of
the base partitions is shown in Fig. 1b. In Fig. 1b–f, each point is repre-
sented by a symbol corresponding to its true class, and a color corre-
sponding to the maximum plausibility cluster (6). The convex hulls of the
lower and upper approximations computed using the interval dominance
rule (7) are displayed using solid and broken lines, respectively.
From Fig. 1d, we can see that Dempster's rule fails to capture the
structure of this dataset. As recalled in Section 2.1, important assump-
tions for Dempster's rule are that all the mass functions come from reli-
able and independent sources; it is clear that none of these assumptions is
satisﬁed in our case. When generating the base partitions, the number of
clusters is set to a larger value than the number of “natural” clusters, and
the base partitions, being based on the same data, are not independent.
The DP rule behaves better than Dempster's rule, as it relaxed the
assumption of perfect sources. However, the DP rule also cannot recog-
nize the structure of the dataset (Fig. 1f). The average and disjunctive
rules both yield acceptable results (Fig. 1c and e), but the credal partition
obtained by the disjunctive rule is too imprecise: the inner approxima-
tions of the clusters are very small and the upper approximations are
identical, which means that all objects possibly belong to any cluster
(Fig. 1e). This result is due to the very cautious assumption underlying
this combination rule (at least one source is reliable). In contrast, the
average rule successfully identiﬁes the objects that can be clustered with
high conﬁdence (those in the inner approximations), as well as the ob-
jects in the overlapping area between clusters (Fig. 1c). As the average
rule appears to be the most effective, it was used in subsequent
experiments.
Effect of transitive closure with different t-norms
In this section, we investigate the effect of transitive closure with
respect to different t-norms on Half-rings data. This dataset is composed of
two clusters in two-dimensional space, separated by a nonlinear
boundary (Fig. 2a). Such non-spherical clusters are typically difﬁcult to
identify without prior information. The parameter settings were the same
as before, except that we set kb ¼ 15, N ¼ 20, kr ¼ 2and δ equal to the
0.1-quantile of the dissimilarities between objects in this experiment. We
combined the relational representations by the average rule and made
the belief and plausibility matrix transitive with respect to the minimum,
Lukasiewicz and product t-norms. We also considered the results without
transitivity comparison. One of the base partitions is shown in Fig. 2b.
The recovered partitions with the different transitive closure
operations are shown in Fig. 2c–e, and the recovered partition without
transitivity is displayed in Fig. 2f. In these ﬁgures, each point is repre-
sented by a symbol corresponding to its true class, and a color corre-
sponding to the obtained cluster using the maximal plausibility rule. We
can
see
that,
without
transitivity,
our
method fails
to
identify
nonspherical clusters (Fig. 2f). The results with the Lukasiewicz t-norm
are similar to those without transitivity (Fig. 2d). The best result is ob-
tained with the minimum t-norm (Fig. 2c).
Discovering clusters with complex shape is one of the most chal-
lenging issues in clustering. In this experiment, we have show that our
method has the ability to discover such clusters after making the com-
bined relational representation transitive in the sense of IFRs.
4.2. Quantitative experiments
In this section, we apply our method to the simulated and real data
summarized in Table 1. Five simulated datasets2 are shown in Fig. 3; the
ﬁrst four datasets contain complex shape clusters, while R15 contains a
comparatively larger number of well-separated clusters. All the real
datasets can be found in the UCI Repository of machine learning data-
bases.3 For all the datasets considered in this study, we assume that the
“ground-truth” partition exists. To compare an evidential partition with
the true partition, we ﬁrst converted it to a hard partition using the
maximum plausibility rule, and we computed the adjusted Rand index
(ARI) between the derived hard partition and the true partition.
We used the ECM algorithm with informative pair of clusters to
generate base partitions for each dataset. We considered three cases for
the number of clusters in the base partitions (Table 2): ﬁxed small
number (case 1); randomly selected from an interval (case 2) and ﬁxed
large number (case 3). We averaged the relational representations, and
we computed the transitive closure operations with the minimum,
product and Lukasiewicz t-norms. We also included the solution without
transitivity for comparison. The true number of clusters was assumed to
be known; in practice, this number can often be guessed using, e.g.,
visualization techniques. When recovering the combined credal parti-
tion, we ﬁrst used only singletons and Ωas focal sets (denoted by “simple”
in the table), and then we included informative pairs informative pairs
(denoted by “pairs” in the table) as explained in Section 4.1. The
ensemble size was N ¼ 20. The procedure was run 10 times for each
experiment. The average ARI values are shown in Table 3, and the
standard deviations are shown in parentheses.
For comparison, we show the results of the ECM algorithm with the
true number of clusters. For this method, only ∅and singletons were
treated as focal sets, and we ﬁxed δ ¼ 100. We also compared our
method with the EAC method: in the ﬁrst step we used the hard c-means
algorithm to obtain N ¼ 100base partitions; in the second step, we used
single-linkage hierarchical clustering to obtain the true number of clus-
ters. For the EAC method, we also considered three cases as we did in our
method. The results for the ECM algorithm and the EAC method are
shown in the ﬁrst and second columns of Table 3.
From Table 3, we can see that the results obtained by our method are
better than those obtained by the ECM algorithm, except for the Wine
data. When compared to the EAC method, our method achieves higher
accuracy and better stability, especially for real datasets. The EAC
method performs very well with simulated datasets, but better or similar
results were obtained with our method, except for the Spiral dataset.
Comparing the results obtained with different t-norms for the tran-
sitive closure operation, we can see that the minimum t-norm often
performs well, except with the R15 and Seeds dataset, for which better
results are obtained with the other t-norms, or even without transitivity.
The results with the Lukasiewicz and product t-norms are often similar to
those obtained without transitivity. Generally, it seems that making the
2 Available at http://cs.joensuu.ﬁ/sipu/datasets/.
3 Available at http://archive.ics.uci.edu/ml.
F. Li et al.
Array 6 (2020) 100018
8
Fig. 2. Results for Halfrings data: original data (a), one of base partitions (b), and recovered partition from transitive closures the minimum (c), Lukasiewicz (d) and
product (e) t-norms, as well as without transitivity (f).
F. Li et al.
Array 6 (2020) 100018
9
combined relational minimum-transitive is beneﬁcial for datasets with
complex-shaped clusters, but it can sometimes degrade the performance
for datasets with overlapping clusters. The reason is that, where there is
an overlapping area between clusters and we make the fuzzy relation
transitive, objects from different clusters become similar to each other,
which hinders the performance of the method. As far as the number of
clusters in the base partitions is concerned, better results are generally
obtained in Cases 2 and 3, i.e., with a larger number of clusters.
5. Conclusion
We have presented a method for combining clusterings in the DS
framework. Each base clustering is assumed to take the form of a credal
partition, in which the clustering membership of each object is allowed to
be uncertain and represented by a mass function. This very general
formalism encompasses hard, fuzzy and rough partitions as special cases
[9]. Credal partitions of special forms can be generated by hard, fuzzy or
rough clustering algorithms, and general credal partitions can be ob-
tained by evidential clustering procedures such a EVCLUS [14], ECM
[26], BPEC [34], CCM [25], etc.
Base credal partitions cannot be combined directly, because there is
not always a clear correspondence between clusters in different parti-
tions; in particular, base credal partitions can have different numbers of
clusters. To circumvent this difﬁculty, we proposed to convert each base
credal partition to its relational representation, deﬁned as the collection
of pairwise mass functions describing the uncertain joint cluster-
membership for each pair of objects. After the normalized relational
Table 1
Datasets used in the experiments.
Dataset
Number of objects
Number of clusters
Number of attributes
Aggregation
788
7
2
Compound
399
6
2
Flame
240
2
2
Spiral
312
3
2
R15
600
15
2
Iris
150
3
4
Seeds
210
3
7
Wine
178
3
13
Ecoli
307
4
5
Fig. 3. Simulated datasets: Aggregation (a), Compound (b), Flame (c), Spiral (d) and R15 (e).
Table 2
Number of clusters in base partitions.
Dataset
Case 1
Case 2
Case 3
Aggregation
10
[10,20]
20
Compound
10
[10,20]
20
Flame
8
[8,15]
15
Spiral
30
[30,40]
40
R15
20
[20,30]
30
Iris
8
[8,15]
15
Seeds
5
[5,10]
10
Wine
5
[5,10]
10
Ecoli
8
[8,15]
15
F. Li et al.
Array 6 (2020) 100018
10
representations have been computed, they can be aggregated using any
combination rule of DS theory. The best results have been obtained with
the averaging operator.
Using the similarity between relational representations and intui-
tionistic fuzzy relations studied in Ref. [22], we have proposed a way to
transitivize the combined relational representation by computing the
transitive closures of two fuzzy relations, based on a t-norm. Our
experimental results suggest that the minimum t-norm often yields the
best results, especially for datasets with complex-shaped clusters. How-
ever, making the relational representation transitive does not always
improve the results, and may even degrade them in the case of datasets
with many spherical clusters.
After the combined relational representation has been computed, the
last step of our method consists in constructing a credal partition whose
relational representation is as close as possible to the combined relational
representation obtained in the previous step. We have proposed an error
measure based on Jousselme's metric, which can be minimized using a
grouped coordinate descent algorithm that solves a convex quadratic
optimization problem at each step. After a normalized credal partition
has been obtained, we “denormalize” it by assigning to the empty set the
average of the masses assigned to the empty set by the base partitions,
which provide useful information to signal outliers.
We have applied this method to a variety of simulated and real
datasets. It has been shown to perform well in terms of adjusted Rand
index as compared to the EAC method and to the ECM algorithm alone. It
should also be emphasized that, in contrast with EAC and most existing
ensemble clustering methods, our approach computes a credal partition,
which constitutes a richer description of the clustering structure of a
dataset, as compared to hard or fuzzy partitions.
Although very encouraging, these results are still preliminary. The
determination of the number of clusters remains a crucial issue that re-
mains to be thoroughly investigated. The application of this approach to
very big datasets with a large number of clusters still represents a chal-
lenge. Finally, we could apply this approach not only to combine credal
partitions, but also to combine all kinds of partitions generated by all
kinds of clustering algorithms. These research directions will be inves-
tigated in future work.
CRediT authorship contribution statement
Feng Li: Methodology, Investigation, Writing - original draft. Shou-
mei Li: Supervision, Funding acquisition. Thierry Denœux: Conceptu-
alization, Methodology, Writing - review & editing, Supervision.
References
[1] Akbari E, Dahlan HM, Ibrahim R, Alizadeh H. Hierarchical cluster ensemble
selection. Eng Appl Artif Intell 2015;39:146–56.
[2] Antoine V, Quost B, Masson M-H, Denoeux T. CECM: constrained evidential c-
means algorithm. Comput Stat Data Anal 2012;56(4):894–914.
[3] Antoine V, Quost B, Masson M-H, Denoeux T. CEVCLUS: evidential clustering with
instance-level constraints for relational data. Soft Computing 2014;18(7):1321–35.
[4] Boixader D, Jacas J, Recasens J. Fuzzy equivalence relations: advanced material. In:
Dubois D, Prade H, editors. Fundamentals of fuzzy sets. Boston: Kluwer Academic
Publishers; 2000. p. 261–90.
[5] Bouchard M, Jousselme A-L, Dor�e P-E. A proof for the positive deﬁniteness of the
Jaccard index matrix. Int J Approx Reason 2013;54(5):615–26.
[6] Denœux T. evclust: evidential Clustering. R package version 1.0.3. URL,
https://CRAN.R-project.org/package=evclust; 2016.
[7] Denoeux T. Decision-making with belief functions: a review. Int J Approx Reason
2019;109:87–110.
[8] Denœux T, Dubois D, Prade H. Representations of uncertainty in artiﬁcial
intelligence: beyond probability and possibility. In: Marquis P, Papini O, Prade H,
editors. A guided tour of artiﬁcial intelligence research. Springer Verlag; 2020. Ch.
4.
[9] Denoeux T, Kanjanatarakul O, September. Beyond fuzzy, possibilistic and rough: an
investigation of belief functions in clustering. In: Soft methods for data science
(proc. Of the 8th international conference on soft methods in probability and
statistics SMPS 2016). Vol. AISC 456 of advances in intelligent and soft computing.
Rome, Italy: Springer-Verlag; 2016. p. 157–64.
[10] Denoeux T, Kanjanatarakul O, Sriboonchitta S. EK-NNclus: a clustering procedure
based on the evidential k-nearest neighbor rule. Knowl Base Syst 2015;88:57–69.
[11] Denoeux T, Li S, Sriboonchitta S. Evaluating and comparing soft partitions: an
approach based on Dempster-Shafer theory. IEEE Trans Fuzzy Syst 2018;26(3):
1231–44.
[12] Denœux T, Masson M-H. EVCLUS: evidential clustering of proximity data. IEEE
Trans Syst Man Cybern B Cybern 2004;34(1):95–109.
[13] Denœux T, Masson M-H. Evidential reasoning in large partially ordered sets. Ann
Oper Res 2011;195(1):135–61.
[14] Denœux T, Sriboonchitta S, Kanjanatarakul O. Evidential clustering of large
dissimilarity data. Knowl Base Syst 2016;106:179–95.
[15] Dubois D, Prade H. Representation and combination of uncertainty with belief
functions and possibility measures. Comput Intell 1988;4(3):244–64.
Table 3
Average ARI results for simulated and real datasets. The best results are shown in bold.
ECM
EAC
minimum
Lukasiewicz
product
no transitivity
simple
pairs
simple
pairs
simple
pairs
simple
pairs [b]
Aggregation
case 1
0.68(0.05)
0.81(0.06)
0.83(0.08)
0.73(0.08)
0.72(0.07)
0.72(0.07)
0.79(0.07)
0.82(0.09)
0.72(0.06)
0.71(0.05) [t]
case 2
0.87(0.04)
0.97(0.04)
0.95(0.07)
0.69(0.05)
0.67(0.05)
0.82(0.07)
0.76(0.05)
0.76(0.06)
0.7(0.05)
case 3
0.91(0.07)
0.98(0.01)
0.98(0.01)
0.73(0.04)
0.71(0.05)
0.81(0.08)
0.77(0.06)
0.74(0.03)
0.73(0.04) [b]
Compound
case 1
0.53(0.09)
0.65(0.09)
0.78(0.02)
0.72(0.03)
0.52(0.01)
0.52(0.01)
0.51(0.04)
0.5(0.06)
0.52(0.04)
0.52(0.04) [t]
case 2
0.86(0.03)
0.87(0.04)
0.78(0.07)
0.48(0.06)
0.49(0.05)
0.5(0.04)
0.47(0.06)
0.49(0.05)
0.49(0.05)
case 3
0.89(0.01)
0.73(0.1)
0.88(0.05)
0.41(0.02)
0.41(0.02)
0.45(0.06)
0.45(0.06)
0.42(0.04)
0.39(0) [b]
Flame
case 1
0.49(0)
0.93(0)
0.9(0.02)
0.9(0.02)
0.75(0.14)
0.75(0.14)
0.92(0.01)
0.92(0.01)
0.79(0.05)
0.79(0.05) [t]
case 2
0.19(0.39)
0.91(0.02)
0.91(0.02)
0.69(0.33)
0.69(0.33)
0.93(0.04)
0.93(0.04)
0.82(0.03)
0.82(0.03)
case 3
�0.01(0.06)
0.92(0.01)
0.92(0.01)
0.47(0.25)
0.47(0.25)
0.66(0.23)
0.66(0.23)
0.53(0.3)
0.53(0.3) [b]
Spiral
case 1
0.01(0)
1(0)
0.82(0.21)
0.72(0.28)
0.01(0)
0.01(0)
0.01(0.01)
0.01(0)
0.01(0)
0.01(0) [t]
case 2
1(0)
0.88(0.16)
0.87(0.16)
0.03(0.01)
0.03(0.01)
0.05(0.03)
0.03(0.01)
0.04(0.04)
0.04(0.01)
case 3
1(0)
0.96(0.06)
0.76(0.19)
0.08(0.06)
0.11(0.07)
0.14(0.02)
0.14(0.11)
0.06(0.04)
0.03(0.01) [b]
R15
case 1
0.93(0.07)
0.99(0)
0.56(0.12)
0.57(0.08)
0.99(0)
0.99(0)
0.98(0.03)
0.97(0.03)
0.99(0)
0.99(0)[t]
case 2
0.99(0)
0.67(0.09)
0.62(0.07)
0.99(0)
0.99(0)
0.99(0)
0.99(0)
0.99(0)
0.99(0)
case 3
0.99(0)
0.86(0.02)
0.66(0.11)
0.99(0)
0.99(0)
0.99(0)
0.99(0)
0.99(0)
0.99(0)[b]
Iris
case 1
0.73(0)
0.6(0.04)
0.7(0.03)
0.69(0.05)
0.84(0.03)
0.8(0.04)
0.86(0.03)
0.85(0.03)
0.84(0.03)
0.8(0.03) [t]
case 2
0.57(0)
0.76(0.06)
0.68(0.04)
0.79(0.05)
0.73(0.02)
0.8(0.05)
0.75(0.03)
0.82(0.05)
0.74(0.02)
case 3
0.57(0)
0.85(0.04)
0.78(0.04)
0.77(0.05)
0.7(0.03)
0.76(0.03)
0.7(0.03)
0.74(0.11)
0.69(0.03) [b]
Seeds
case 1
0.78(0.01)
0.29(0.09)
0.48(0)
0.48(0.02)
0.78(0.01)
0.78(0.02)
0.78(0)
0.79(0)
0.78(0.01)
0.78(0.01) [t]
case 2
0.22(0.08)
0.72(0.03)
0.66(0.08)
0.77(0.02)
0.78(0.02)
0.75(0.01)
0.75(0.02)
0.78(0.02)
0.77(0.02)
case 3
0.24(0)
0.73(0.09)
0.55(0.07)
0.77(0.02)
0.72(0.04)
0.74(0.03)
0.7(0.04)
0.76(0.04)
0.7(0.06) [b]
Wine
case 1
0.9(0)
0.35(0.24)
0.66(0.18)
0.8(0.09)
0.71(0.13)
0.88(0.02)
0.78(0.18)
0.85(0.07)
0.77(0.16)
0.87(0.01) [t]
case 2
0.09(0.2)
0.78(0.09)
0.73(0.13)
0.68(0.17)
0.7(0.17)
0.73(0.18)
0.73(0.21)
0.7(0.16)
0.69(0.13)
case 3
0.17(0.23)
0.83(0.05)
0.83(0.05)
0.58(0.17)
0.58(0.17)
0.78(0.1)
0.8(0.07)
0.71(0.19)
0.72(0.19) [b]
Ecoli
case 1
0.52(0)
0.47(0.16)
0.57(0.07)
0.62(0.1)
0.64(0.09)
0.62(0.09)
0.65(0.09)
0.6(0.09)
0.62(0.09)
0.62(0.09) [t]
case 2
0.51(0.04)
0.6(0.08)
0.55(0.11)
0.62(0.1)
0.63(0.1)
0.66(0.08)
0.67(0.07)
0.69(0.06)
0.69(0.07)
case 3
0.48(0.01)
0.45(0.11)
0.47(0.13)
0.62(0.13)
0.63(0.11)
0.62(0.14)
0.62(0.1)
0.62(0.1)
0.61(0.1) [b]
F. Li et al.
Array 6 (2020) 100018
11
[16] Fred ALN, Jain AK. Data clustering using evidence accumulation. In: International
conference on pattern recognition; 2002. p. 276–80.
[17] Fred ALN, Jain AK. Combining multiple clusterings using evidence accumulation.
IEEE Trans Pattern Anal Mach Intell 2005;27(6):835–50.
[18] Ghaemi R, Sulaiman m n, Ibrahim H, Mustapha N. A survey: clustering ensembles
techniques. World Acad Sci Eng Technol 2009;38:644–53.
[19] Iam-on N, Boongoen T, Garrett S. Reﬁning pairwise similarity matrix for cluster
ensemble problem with cluster relations. Discovery Science 2008:222–33.
[20] Jousselme A-L, Grenier D, Boss�e E. A new distance between two bodies of evidence.
Inf Fusion 2001;2(2):91–101.
[21] Lee H-S. An optimal algorithm for computing the max-min transitive closure of a
fuzzy similarity matrix. Fuzzy Set Syst 2001;123(1):129–36.
[22] Li B, He W. The structures of intuitionistic fuzzy equivalence relations. Inf Sci 2014;
278:883–99.
[23] Li F, Li S, Denoeux T. k-CEVCLUS: constrained evidential clustering of large
dissimilarity data. Knowl Base Syst 2018;142:29–44.
[24] Li F, Qian Y, Wang J, Liang J. Multigranulation information fusion: a Dempster-
Shafer evidence theory-based clustering ensemble method. Inf Sci 2017;378:
389–409.
[25] Liu Z, Pan Q, Dezert J, Mercier G. Credal c-means clustering method based on belief
functions. Knowl Base Syst 2015;74:119–32.
[26] Masson M-H, Denoeux T. ECM: an evidential version of the fuzzy c-means
algorithm. Pattern Recogn 2008;41(4):1384–97.
[27] Masson M-H, Denœux T. RECM: relational evidential c-means algorithm. Pattern
Recogn Lett 2009;30(11):1015–26.
[28] Masson M-H, Denoeux T. Ensemble clustering in the belief functions framework. Int
J Approx Reason 2011;52(1):92–109.
[29] Ovchinnikov S. An introduction to fuzzy relations. In: Dubois D, Prade H, editors.
Fundamentals of fuzzy sets. Boston: Kluwer Academic Publishers; 2000. p. 233–59.
[30] Peters G, Crespo F, Lingras P, Weber R. Soft clustering: fuzzy and rough approaches
and their extensions and derivatives. Int J Approx Reason 2013;54(2):307–22.
[31] Shafer G, et al. A mathematical theory of evidence. Princeton university press;
1976.
[32] Smets P. The combination of evidence in the transferable belief model. IEEE Trans
Pattern Anal Mach Intell 1990;12(5):447–58.
[33] Smets P. Belief functions: the disjunctive rule of combination and the generalized
bayesian theorem. Int J Approx Reason 1993;9(1):1–35.
[34] Su Z-G, Denoeux T. BPEC: belief-peaks evidential clustering. IEEE Trans Fuzzy Syst
2019;27(1):111–23.
[35] ter Braak CJF, Kourmpetis Y, Kiers HAL, Bink MCAM. Approximating a similarity
matrix by a latent class model: a reappraisal of additive fuzzy clustering. Comput
Stat Data Anal 2009;53(8):3183–93.
[36] Vavasis SA. Complexity theory: quadratic programming. Boston, MA: Springer US;
2001. p. 304–7.
[37] Vega-Pons S, Ruiz-Shulcloper J. Clustering ensemble method for heterogeneous
partitions. Progress in pattern recognition, image analysis, computer vision, and
applications, 5856; 2009. p. 481–8.
[38] Vega-Pons S, Ruiz-Shulcloper J. A survey of clustering ensemble algorithms. Int J
Pattern Recogn Artif Intell 2011;25(3):337–72.
[39] Wang X, Han D, Han C. Ensemble clustering based on evidence theory. In: 2017
20th international conference on information fusion (fusion); 2017. p. 759–67.
[40] Wang X, Yang C, Zhou J. Clustering aggregation by probability accumulation.
Pattern Recogn 2009;42(5):668–75.
[41] Yang L, Lv H, Wang W. Soft cluster ensemble based on fuzzy similarity measure.
IMACS: multiconference on computational engineering in systems applications, 1;
2006. p. 1994–7. 2.
[42] Zhan J-M, Chen J-T, Xing J-Q. Research advance of clustering ensemble algorithm.
In: 2017 international conference on wavelet analysis and pattern recognition
(ICWAPR); 2017. p. 109–14.
[43] Zhou K, Martin A, Pan Q, Liu Z-G. Median evidential c-means algorithm and its
application to community detection. Knowl Base Syst 2015;74:69–88.
F. Li et al.
Array 6 (2020) 100018
12
