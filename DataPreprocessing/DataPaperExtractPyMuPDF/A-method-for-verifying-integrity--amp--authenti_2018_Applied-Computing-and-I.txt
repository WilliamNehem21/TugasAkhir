A method for verifying integrity & authenticating digital media
Martin Harran a, William Farrelly a, Kevin Curran b,⇑
a Letterkenny Institute of Technology, Donegal, Ireland
b Ulster University, Derry, United Kingdom
a r t i c l e
i n f o
Article history:
Received 6 March 2017
Revised 22 May 2017
Accepted 24 May 2017
Available online 31 May 2017
Keywords:
Secure authentication
Integrity
Digital certiﬁcates
Security
Network security
a b s t r a c t
Due to their massive popularity, image ﬁles, especially JPEG, offer high potential as carriers of other infor-
mation. Much of the work to date on this has focused on stenographic ways of hiding information using
least signiﬁcant bit techniques but we believe that the ﬁndings in this project have exposed other ways of
doing this. We demonstrate that a digital certiﬁcate relating to an image ﬁle can be inserted inside that
image ﬁle along with accompanying metadata containing references to the issuing company.
Notwithstanding variations between devices and across operating systems and applications, a JPEG ﬁle
holds its structure very well. Where changes do take place, this is generally in the metadata area and does
not affect the encoded image data which is the heart of the ﬁle and the part that needs to be veriﬁable.
References to the issuing company can be inserted into the metadata for the ﬁle. There is an advantage of
having the digital certiﬁcate as an integral part of the ﬁle to which it applies and consequently travelling
with the ﬁle. We ultimately prove that the metadata within a ﬁle offers the potential to include data that
can be used to prove integrity, authenticity and provenance of the digital content within the ﬁle.
� 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
1. Introduction
More of today’s communications are moving to exclusively dig-
ital format with no hard copy backups being kept. This has impli-
cations regarding integrity, authentication and provenance in
various areas such as litigation where it is necessary that both
sides are satisﬁed with the integrity of digital evidence or in the
insurance industry where claims can succeed or fail on very
detailed terms and conditions; it may be necessary to know exactly
what the terms and conditions were at the time the policy was cre-
ated [1]. There is also a need to verify the terms of a contract that
applied when the contract was agreed and establish dates of orig-
inal creation when copyright issues arise regarding digital content
[2]. Likewise, real world requirements exist to establish prior
knowledge
before
signing
Non-Disclosure
Agreements.
An
important aspect of this is that the raw data content of a ﬁle can
be veriﬁed even when the metadata is altered due to the ﬁle
moving across operating systems or devices.
Widespread and easily available tools have become common for
video synthesis. It is important to ensure the authenticity of video
uses such as in courts of law, surveillance systems, advertisements
and the movie industry. There is therefore a large body of research
in the ares of video authentication and tampering detection
techniques [3–5]. There is also research using Darwin Information
Typing Architecture in protecting the integrity of digital publishing
applications [6].
Although there are many tools available to create, encrypt and
extract data, there is relatively little available in the area of estab-
lishing integrity, authentication and provenance. For instance, [7]
creates and issues a certiﬁcate containing a SHA 256 hash value
of the submitted media ﬁle along with the user details and a times-
tamp. That certiﬁcate is in turn veriﬁed by a digital certiﬁcate
issued by leading certiﬁcation authority Comodo. The problem
here however is that the original ﬁle and the certiﬁcation are sep-
arate entities and could easily become separated during distribu-
tion or circulation of the ﬁle. In general, wrapping the ﬁle and its
certiﬁcation inside an outer envelope is no real guarantee that
the ﬁeld and its certiﬁcation will remain together. Inserting the
certiﬁcation into the ﬁle is the only apparent reliable method
and is the approach used by leading companies such as Microsoft
and Adobe for digital signing of Ofﬁce and PDF documents [8]. A
limitation with inserting the certiﬁcate inside the ﬁle is that it
http://dx.doi.org/10.1016/j.aci.2017.05.006
2210-8327/� 2017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
⇑ Corresponding author.
E-mail address: kj.curran@ulster.ac.uk (K. Curran).
Peer review under responsibility of King Saud University.
Production and hosting by Elsevier
Applied Computing and Informatics 14 (2018) 145–158
Contents lists available at ScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
must follow the metadata speciﬁcation set out for the particular
ﬁle type which means that distinct software applications or com-
ponents have to be developed for every ﬁle type.
This research outlines a method in which integrity, authentica-
tion and provenance can be established for raw data within a ﬁle
even when the metadata attached to it has changed. We also
achieve this in a manner in which a digital proof stamp can be
made visual to help achieve virality for the end users.
2. Digital ﬁngerprinting
Digital ﬁngerprinting is based on the use of a mathematical
function to produce a numerical value where the function takes
an arbitrary length of data as its input and outputs a numerical
value of a speciﬁed length; 128 bits (16 bytes), 256 bits (32 bytes)
and 512 bits (64 bytes) are typical lengths. The output value of
such a function is generally referred to as a hash value or simply
hash. In order to be useful, the function used to produce a digital
ﬁngerprint must be quick to use but in order to be trustworthy
for authentication purposes, it must also meet certain security
requirements [9]. For this reason, digital ﬁngerprinting uses crypto-
graphic hash functions; the authors describe the general security
properties of these functions as preimage resistance, second
preimage resistance (or weak collision resistance) and collision
resistance (or strong collision resistance) where:
� Preimage resistance means that for a hash function H and an
output value of z, it is computationally infeasible1 to ﬁnd an input
value m such that z ¼ HðmÞ. This property is also known as one-
wayness.
� Second preimage resistance means that given an input m1, it is
computationally infeasible to ﬁnd a different message m2 where
Hðm1Þ ¼ Hðm2Þ. This is also known as weak collision resistance.
� Collision resistance means that it is computationally infeasible
to ﬁnd two different inputs where m1 and m2 where Hðm1Þ ¼
Hðm2Þ. This is also known as strong collision resistance.
Ref. [10] stated that collision resistance implies second preim-
age resistance but does not guarantee preimage resistance. Ref.
[11] pointed out that that the validity of those claims is limited
to
speciﬁc
deﬁnitions
of
the
various
terms.
The
subtleties
involved, however, have no impact on this project as only second
preimage resistance applies in regard to the use of digital ﬁnger-
prints for ﬁle authentication - a potential attacker has access to
both the original input (the ﬁle) and the output (the hash value
for the ﬁle).
A wide range of algorithms have been developed to meet the
requistite security requirements as well as having the underlying
performance requirement of computational speed. Two of the most
commonly encountered functions are MD5 and the SHA family of
functions. MD5 (Message Digest algorithm 5) was developed by
Ref. [12] who had previously developed earlier versions MD2 –
MD4. It became extremely popular following its release but begin-
ning in 1996, vulnerabilities were increasingly identiﬁed in regard
to collision resistance. Ref. [13] stated that although practical
applications were not yet threatened, it ‘‘comes rather close” and
advised users to move to other algorithms. Eight years later, Ref.
[14] announced collisions based on the full hash. Four years after
that, Ref. [15] based MD5 collision vulnerabilities to create a rogue
digital certiﬁcate which would be accepted by all common
browsers. The credibility of MD5 was effectively terminated in
December that year by the release of a Vulnerability Notice by
US-CERT [16] which stated that ‘‘it [MD5] should be considered
cryptographically broken and unsuitable for further use”. Although
MD5 is still adequate for checking ﬁle consistency and other non-
secure applications, the known weaknesses should have effectively
ended its use for digital signatures. It is still in widespread use,
however, as late as 2012 when it was used to fake a Microsoft dig-
ital signature in the FLAME malware attack [17]. Rivest has pro-
duced an MD6 version but after submitting it to the NIST open
competition for SHA-3, he withdrew it stating that it was not ready
for use [18]. Following the decline of MD5, the predominant algo-
rithms in use today are members of the SHA family.
2.1. Secure hash algorithm
Secure Hash Algorithm (SHA) is a family of cryptographic hash
functions developed by the US National Security agency (NSA) and
published as standards by the US National Institute of Science and
Technology (NIST). It is the required algorithm for secure applica-
tions used by US government agencies. An important feature of the
SHA algorithms is that they implement an avalanche effect which
means that a minor change to the input cascades into a major
change in the output value.
The ﬁrst version, SHA-1 produces a 160-bit value and was
released in 1993; it was, however, withdrawn shortly after publi-
cation due to an undisclosed vulnerability and a modiﬁed version
was released two years later [19]. In 2002, NIST published the
SHA-2 family of functions. Unlike SHA-1 with hash size ﬁxed at
160 bits, SHA-2 is offered in six versions producing a range of out-
put sizes from 224 to 512 bits of which the most widely used are
SHA-256 and SHA-512. Like MD5 and SHA-1, the SHA-2 functions
are based on the Merkle–Damgård construction. The algorithm
used for SHA-256 is:
1. A 256-bit buffer is created, made up of 8 � 32-bit words which
are initialised with the ﬁrst 32 bits of the fractional parts of the
square roots of the ﬁrst 8 primes.
2. A 64 element table of constants is prepared using the ﬁrst 32
bits of the fractional parts of the cube roots of the ﬁrst 64
primes
The input is padded with an initial bit ‘1’ and the length of the
original input expressed as a 64-bit integer, separated by the
required number of zeros required to make the message length,
including the padding, a multiple of 512 bits.
3. Each 512-bit block is processed through 64 rounds where each
round involves a series of operations comprised of bitwise oper-
ations and modular addition.
4. The value of the buffer on completion of each block is the initial
value for the following block; at the end of the ﬁnal block, the
buffer contains the hash value.
Despite offering better security and faster performance, SHA-2
was only adopted slowly over the next three years with SHA-1
remaining in extensive use. The main contributory reasons were
that SHA-2 was not supported on systems using Windows XP
(SP2) or older, there was no perceived urgency as no collisions
had yet been found in SHA-1 and SHA-256 is about 2.2 times
slower than SHA-1 though some of that drop can be reduced by
a move to SHA-512 on 64-bit hardware and operating systems
[20].
Ref. [21] revealed collision attacks on the full SHA-1 function.
This work did not completely undermine the practical reliability
of the function – it was a strong collision attack which did not
impact on the preimage – but the following year, NIST instructed
1 An expression used in cryptology to signify that, given enough time and
resources, it may be theoretically possible to decrypt the result of a cryptographic
function but the time and resources actually available make it impractical in any
meaningful way.
146
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
federal agencies to stop using SHA-1 as soon as practical and stated
that after 2010, SHA-2 would be compulsory for digital signatures,
digital time stamping and other applications requiring collision
resistance [22]. Although SHA-1 is still in extensive use and is still
secure for most practical purposes, applications that continue
using it are likely to run into ever increasing problems as software
developers drop support from new versions of their applications;
Microsoft, Chrome and Mozilla, for example, have all announced
that SHA-1 will not be supported in browser versions beyond 2016.
No signiﬁcant attack on SHA-2 has ever been demonstrated.
Nevertheless, in what can be regarded as a pre-emptive move, NIST
launched an open competition in 2007 for the development of
SHA-3. The reason for adopting an open competition approach
was to ﬁnd an alternative to the Merkle–Damgård construction
as that had been the basis of the now discredited MD5 and SHA-
1 as well as SHA-2. The eventual winner, KECCAK using what is
known as the sponge construction, was announced in 2012 and
the new standard published three years later [23]. The pre-
emptive nature of SHA-3 is clearly indicated by NIST whose current
policy on hash functions [24] states that there is no need to transi-
tion applications from SHA-2 to SHA-3.
2.2. RSA
The original concept of a digital authentication scheme was ﬁrst
proposed by Difﬁe and Hellman [25], who revolutionised cryptog-
raphy by proposing a method of creating a shared secret over pub-
lic or unsecured channel which would enable traditional symmetric
cryptography to be used electronically. In their paper, they also
discussed the idea of a system of public key cryptography based
on public and private keys and also the need for one-way authenti-
cation but did not propose solutions for either of these. Two years
later, such a solution was developed at Massachusetts Institute of
Technology (MIT) by three researchers [12] which became known
by the initial letters of their surnames, RSA (Rivest- Shamir- Adle-
man). The RSA public/private key algorithm is based on the difﬁ-
culty of factoring large numbers and produces a private key and
a public key where a message encrypted using the public key can
be decrypted using the private key but calculating the private
key from the public key is computationally infeasible. This is
known as asymmetric encryption. The RSA algorithm is too slow
for encrypting lengthy messages but it can be used to create a
shared secret which can then be used for high-speed symmetrical
encryption.
2.3. Digital signatures
Ref. [12] also proposed that authentication of content, whether
encrypted or in plain-text, could be achieved by using the keys to
encrypt and decrypt a digital ﬁngerprint. The overall process, a
message accompanied by a hash of the message encrypted with
the sender’s private key constitute what is known as a digital signa-
ture. This process fulﬁlls the twin requirement of authentication i.e.
identiﬁcation and non-repudiation. If the calculated hash value
matches the decrypted hash value from Alice, then Bob can be con-
ﬁdent that the message was indeed sent by her and the message
has not changed in transit. Alice, in turn, cannot deny sending
the message provided that ker private key was genuinely used to
encrypt the hash. That was the main outstanding problem – estab-
lishing a trustworthy way to distribute public keys. The processes
are open to man-in-the-middle attacks where a third party could
substitute their own public key for that belonging to Alice and send
a different ﬁle and hash to Bob and he will be duped into believing
it is a genuine message from Alice.
Reliable distribution of public keys can be achieved through the
use of digital certiﬁcates, also known as public key certiﬁcates, which
are electronic documents that verify the owner of a public key.
These certiﬁcates are issued by a Certiﬁcation Authority (CA), an
organisation which specialises in the business of issuing certiﬁ-
cates, usually on a commercial basis and which can be trusted by
recipients. The certiﬁcate is a binding between an entity and its
associated public key and contains information about the issuing
CA as well as information about the person or organisation to
whom the certiﬁcate applies. This information is generally pre-
sented in the format of an X.509 certiﬁcate. Digital certiﬁcates
issued by third-party certiﬁcation authorities are the backbone of
secure Internet connections based on Transport Layer Security
(TLS), formerly known as Secure Sockets Layer (SSL); a secure con-
nection is most recognisable when a website uses Hypertext Trans-
fer Protocol Secure (HTTPS) and a padlock icon appears in the URL
or at the bottom of the browser. The digital certiﬁcate for the site
can be viewed by clicking on the padlock. Until comparatively
recently, secure connections using SSL/TLS were primarily used
for webpages involving ﬁnancial transactions but they are coming
into wider use in popular sites such as Google and social media
sites such as Facebook and Twitter.
The highest level of CA is known as a Root CA and public keys for
these root CAs have to be authenticated using the issuer’s own
public key. These keys therefore have to be readily available and
are built into operating systems and browsers. In addition to root
trusted CAs, a range of other categories is included such as Trusted
Publishers, Trusted People and Other People. These are managed
through a hierarchical structure where higher levels of CA can del-
egate authority to lower level CAs for speciﬁc uses. This requires a
chain of trust between the root CA and the ones descended from it.
Despite this chain of trust and a proliferation of intermediary
CAs and digital certiﬁcate resellers, the market in reality is domi-
nated by a small number of organisations. In the TLS/SSL market
for April 2016 for example, just 5 players issued 89% of the certiﬁ-
cates – Comodo had 41% of the market, Symantec (including Veri-
sign) had 26%, Godaddy had 12% and GlobalSign had just under
10%. No other player had more than 1%. Digital certiﬁcates can
be used for a wide range of applications ranging from email to
ﬁnancial transactions and CAs offer certiﬁcates for speciﬁc uses.
Various types of use require different levels of security in regard
to owner identiﬁcation and CAs typically offer different classes of
certiﬁcate to reﬂect this, ranging from those where minimal check-
ing is done on the applicant to those where the applicant must
undergo a rigorous identiﬁcation process. Unfortunately, there is
no standardised procedure for these classes and their implementa-
tion varies among issuers. Extended Validation (EV) is an enhanced
procedure for validating the owners of websites, developed
through a voluntary group of certiﬁcation authorities [25]. The
need for that improved level of authentication was demonstrated
in 2011 when an intruder hacked into the systems of an Italian
reseller of Comodo digital certiﬁcate and directly into the systems
of leading Dutch certiﬁcation authority DigiNotar and managed to
get digital certiﬁcates that appeared to be issued in the names of
various high-proﬁle2 organisations [26]. Comodo identiﬁed the
problem with the Italian reseller within hours and were able to
revoke the certiﬁcates and notify customers thus minimising dam-
age to their own reputation. In the case of DigiNotar, however, the
company’s systems were found to be so insecure that the root certiﬁ-
cate had to be marked as untrusted which in turn invalidated all cer-
tiﬁcates issued under the root and, despite the intervention of the
Dutch government, the company was forced into bankruptcy. These
attacks, however, were an exempliﬁcation of the adage that any sys-
tem is only as secure as the procedures it implements – they did not
2 Reputed to include Gmail, Hotmail and Yahoo Mail but that has not been
conﬁrmed.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
147
undermine the underlying principle of either RSA or digital certiﬁ-
cates. RSA –based digital certiﬁcates remain the main technique
for digital authentication in use today.
3. Utilising metadata for authenticity
A core research question we had was whether ‘‘Is it possible to
add additional data to the existing metadata of a ﬁle in such a way
that the added data that can be used to check the integrity, authen-
ticity and provenance of the digital content within the ﬁle?”
This necessitated reviewing existing methods of editing and
inserting metadata into ﬁles to determine what limitations exist
and whether existing applications could be adapted to fulﬁl the
requirement rather than developing a new software artefact from
scratch. There is limited native support in operating systems for
metadata editing. Most Linux distros do not have any native means
of viewing metadata although applications such as ExifTool can be
installed. The most notable exception is Ubuntu desktop which dis-
plays some of the metadata similar to Mac OS X, again uneditable.
An example can be seen in Fig. 1.
Unlike Mac OS X and Linux, Windows Explorer does provide the
facility to edit several tags which have been marked with an aster-
isk in Fig. 1. Editing in Windows Explorer, however, causes radical
changes to the structure of the JPEG. There is a wide range of imag-
ing processing and management applications that handle editing
and inserting metadata to various degrees. The most powerful
and ﬂexible application available is the open source ExifTool by
Phil Harvey (available from www.sno.phy.queensu.ca/~phil/exif-
tool/). It covers a wide range of ﬁle types, executables as well as
image ﬁles and offers extensive functionality for both viewing
and editing metadata. It is available both as a Perl library and as
a command line application and it is the application of choice for
Linux [27] due to the fact that the author has published the code
and invited the open source community to participate in its ongo-
ing development.
A number of Windows front-ends for ExifTool have been devel-
oped such as XnViewMP (available from http://www.xnview.com/
en/xnviewmp/) but these have limited editing facilities. ExifTool is
at its most powerful at the command line where it can be used to
view and/or edit not only all tags for all known image formats but
also for a wide variety of other ﬁle types including executables.
Open source Exiv2 by Andreas Huggel (available from http://
www.exiv2.org/getting-started.html) is similar to Exif tool but is
published as a C++ library and command line utility. It offers very
similar functionality to ExifTool but does not cover as wide a range
of ﬁle types.
EXIF is supported by many Graphical User Interface (GUI) appli-
cations for reading metadata but editing facilities are generally
limited, especially where tags need to be inserted. This author
did ﬁnd a couple of shareware applications for doing so – Exif Pilot
(available
from
www.colorpilot.com/exif.html)
and
Photome
(available from www.photome.de/) but their functionality is
restricted.
IPTC/XMP metadata is generally well supported. Due to the
relationship between IPTC and Adobe, the latter’s ﬂagship ﬁle man-
agement application, Bridge, gives extensive support to IPTC across
the full range of Adobe applications - Acrobat, ImageReady, InDe-
sign and PhotoShop as well as JPEG and PNG – and is the applica-
tion of choice for managing IPTC and XMP. Other applications
support IPTC to various degrees; several freeware applications
such as IrfanView and XnViewMP offer good view-only functional-
ity but Gimp, the main open source alternative to Photoshop, offers
some limited IPTC/XMP editing facilities.
There are also some API libraries available such as Windows
Imaging Component in Windows .Net Framework which support
editing and inserting metadata in both JPEG and PNG images.
Problems have being found with that library [28]. Apple OS X also
has a programmatic way of accessing metadata using Spotlight and
the tag kMDItem although detailed study of that was not carried
out.
In regard to the proposed software artefact, careful considera-
tion was given to using either ExifTool or Exiv2 which are available
as opensource libraries but a unanimous decision was made to
write a new application form scratch. The main reason for this
was that both applications/libraries were far more wide ranging
than needed for the proposed application and it was difﬁcult to just
extract the speciﬁc functionality needed; also, there was the added
complexity with ExifTool of having to access a Perl module from
within C++. The most decisive factor, however, was that neither
of these libraries would support inserting a digital certiﬁcate. It
would be easier to add in new code for handling metadata rather
than trying to include 3rd party modules. Both Microsoft Ofﬁce
and Adobe Acrobat provide the facility to insert digital certiﬁcates
into their relative document types and the new 3MF ﬁle type for
3D printing has the facility built in. These are all for proprietary ﬁle
types, however, based on XML structures and are not applicable to
JPEG, PNG or plain text ﬁles. Despite extensive searching, no off-
the-shelf library or application could be found to insert data into
its own section in these ﬁle types. Developers and programmers
have sought to add data other than metadata into JPEG ﬁles in par-
ticular, sometimes for good reasons such as watermarking images
and other times for nefarious ones; although an image ﬁle cannot
execute any code, the Zeus banking Trojan used a JPEG ﬁle to dis-
guise
and
distribute
a
crucial
conﬁguration
code.
These
approaches, however, involve changing the image data to add
watermarking or by stenographic techniques using the Least Signif-
icant Bit. Also, stenographic techniques are used to hide informa-
tion whereas we would like the inclusion of a digital certiﬁcate
to be as visible as possible. There is no native support for editing
metadata in PNG ﬁles in any of the main operating systems (Win-
dows, OS X, Linux). A number of applications were tried including
Fig. 1. Metadata edit and view - Windows Explorer.
148
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
PNGCrush and TweakPNG but the only one that was found to be
really worthwhile was ExifTool.
We conclude that a digital certiﬁcate needs to be inserted into a
unique ﬁeld due to the risk of it being overwritten by other meta-
data editing applications. In effect, that meant that existing appli-
cations and libraries were of little use and an application that could
handle inserting a digital certiﬁcate would have to be developed
from scratch. That required a more in-depth analysis of the two
image types selected. Details of that work follow in the next
section.
3.1. JPEG
The ﬁrst two bytes in a JPEG ﬁle are the ﬁle signature with a
value of 0xFFD8. This is known as the Start Of Image (SOI) marker
and it is matched at the end of the ﬁle by a two-byte End Of Image
(EOI) marker with a value of 0xFFD9. Inside these markers, the ﬁle
contains the encoded image data preceded by a number of seg-
ments as shown in Fig. 2. All segments are in the form where the
ﬁrst two bytes have a tag identifying the type of segment and
the next two bytes indicate the length of the segment, excluding
the bytes for the tag but including these two. In the example
shown in Fig. 3, the ﬁrst two bytes – 0xFFD8 - are the JPEG File Sig-
nature. Bytes 3 and 4 are the segment tag - 0xFFE1 - and the length
of the segment is 0xEDF6 bytes. The rest of the segment contains
the segment data. As the length has to be expressed in two bytes,
the maximum length of a segment excluding its tag is 64K. Some
segments can, however, occur multiple times which allows that
limit to be overcome. Although there are a range of deﬁned seg-
ment tags, there is nothing to prevent the use of a custom segment.
Provided the format is followed, a well behaved application which
does not recognise the tag will simply use the stated length to
jump to the next segment. The ﬁrst group of segments after the
SOI markers hold the metadata for the ﬁle which will be discussed
in detail in the next section as this is the primary focus of the soft-
ware solution devised.
The metadata segments are followed by segments holding data
relevant to the compression algorithm. JPEG is a lossy compression
technique which uses quantization and Huffman coding. Depend-
ing upon the way the image was created or edited, other segments
may be present for parameters such as Restart Interval (DRI). Com-
pression information segments are of no direct interest in this pro-
ject but the fact that they can be restructured has to be taken into
account; for example, inserting an extra metadata ﬁeld in various
applications may split a single DQT or DHT segment into multiple
segments which collectively hold the exact same information as
the original single segment Even though the contained data has
not changed, the simple act of splitting it will change the hash
value for the ﬁle. The encoded image data is not held in a segment
as there is no restriction on its length. The encoded data is assumed
to start immediately after the SOS segment and ﬁnish immediately
before the EOI marker. It should also be noted that as well as the
main image, a JPEG usually contains a thumbnail nested inside
the metadata section which will be stored as separate JPEG with
its own SOI, compression information segments, encoded data
and EOI; it will not usually contain any metadata.
3.1.1. JPEG metadata segments
Sixteen application segments are allocated for metadata, referred
to as APP0. . . APP15 using the markers 0xFFE0 to 0xFFEFc [29]. An
application segment can be used for any purpose and multiple
instances of a particular segnment can be used within a ﬁle; the
speciﬁc purpose of each segment is indicated by a signature imme-
diately following the two bytes stating segment size. Metadada
readers should skip over a segment if they do not recognise the
signature. The most common segments apart from APP0 and
APP1 are:
APP2: Originally dedicated to Flashpix tags. Introduced in 1996,
the Flashpix format is now obsolete but some of its structures
are included in the EXIF speciﬁcation and may still be encoun-
tered in some cameras; also used by Photoshop when editing a
camera image.
APP13: used by Adobe for Photoshop tags.
APP14: used by Adobe for image encoding information relating
to DCT ﬁlters.
APP0 must be used for JFIF information where it is included,
signed with the ASCII values for the character ‘‘JFIF”. It is used by
software applications for compatibility with other applications
and does not appear in an image straight from camera. It is there-
fore of no direct interest to this project except in regard to taking
account of its possible presence when analysing or processing a
ﬁle. Strictly speaking, the JFIF and EXIF speciﬁcations are incom-
patible because both of the speciﬁcations state that the segment
containing their data should be the ﬁrst segment in the JPEG ﬁle.
In practice, they coexist without any problems and when a JPEG ﬁle
is created in a digital camera and later edited in a software appli-
cation, it will typically contain JFIF data in its ﬁrst segment marked
as APP0 and EXIF data in its second segment marked as APP1. APP1
is used by both EXIF and XMP for metadata and there will be two
APP1 Segments if the ﬁle contains both EXIF and XMP. The signa-
ture for an EXIF segment is the ASCII values for ‘‘Exif” followed
by two nulls. Fig. 3 shows the EXIF signature highlighted.
The signature for an XMP segment is the ASCII values for http://
ns.adobe.com/xap/1.0/nx00; an example is shown in Fig. 4. Analy-
sis of ﬁles using a hex editor is excessively laborious and tedious so
we developed JPEGReader, an application to view JPEG structures
in a much simpler way. A different version, PNGReader, was also
developed for reading and modifying PNG ﬁles. The following
screenshots from JPEGReader illustrate the different ways in which
segments can be used inside a JPEG ﬁle.
Fig. 5 shows the segments structure for an image ﬁle straight
from a digital camera. The ﬁle contains a single APP1 segment
holding EXIF data, some of which can be seen in the text values.
The markers combo box. JPEGReader also provides some details
about the segment such as its start address in the ﬁle and its
length. The APP1 segment is followed by ﬁve segments relating
to the decompression – DQT, DRI, SOF0, DHT and SOS. The start
of encoded data is indicated by 0000 () as encoded data is not in
a segment and therefore has no segment marker. Fig. 6 shows
Fig. 2. JPEG Overall Structure.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
149
the structure after adding some metadata through Windows
Explorer. A second APP1 segment has been added to hold XMP
data. The XMP signature can be seen in the values box below the
markers combo box. Windows Explorer has split the DQT data into
two segments and the DHT data into four segments. There has been
no overall change to the data and despite extensive research, it has
not been possible to ﬁnd an explanation for why Windows splits
these segments.
Fig. 7 shows the structure for the original image with the same
metadata added using Bridge, an Adobe application for image man-
agement (but not image processing). Like Windows Explorer,
Bridge adds a second APP1 to hold XMP. For some unknown rea-
son, it also adds an APP13 segment holding some Photoshop style
metadata Photoshop related data, even though this particular
image has not been processed in Photoshop and that same infor-
mation is already stored in the XMP segment. Unlike Windows
Explorer, the segments containing compression related data are
left as single segments (see Fig. 8).
Fig. 7 shows the effect of opening the original ﬁle in Photoshop
and re-saving it. Although no changes have been made to the
image, it is treated as having been processed by software so an
APP0 segment is added to the start to hold JFIF data. As well as
the second APP1 and the APP13 segments added by the previous
example in Bridge, an additional APP14 segment is added to hold
additional Photoshop data.
At this stage of the research, it was clear that application seg-
ments offer a great range of ﬂexibility and it seemed that an appli-
cation segment could be an ideal place for holding a digital
Fig. 3. EXIF Signature.
Fig. 4. XMP Signature.
Fig. 5. JPEG straight from camera.
150
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
certiﬁcate. The potential for this was conﬁrmed by a ‘quick and
dirty’ test, manually inserting an existing certiﬁcate into the orig-
inal image used in the previous examples. The results are shown
in Fig. 9. The digital certiﬁcate has been inserted into an APP1 seg-
ment and the text box shows the text to which the digital certiﬁ-
cate
applies.
Initially,
it
was
thought that
a
less
common
application segment such as APP8 should be used but it was found
that anything other than APP1, APP13 or APP14 could, in some cir-
cumstances, be deleted by the iOS platform which is the target of
our mobile application. APP1 was found to be generally safe from
overwriting.
3.2. EXIF metadata structure
EXIF metadata is always contained in an APP1 segment which is
speciﬁed to be the ﬁrst segment in the ﬁle but can in practice be
preceded by an APP0 segment. The presence of EXIF metadata is
indicated by the EXIF signature immediately after the two bytes
Fig. 6. JPEG edited in Windows Explorer.
Fig. 7. JPEG edited in Adobe Bridge.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
151
stating the segment size. That signature is the ASCII values for
‘‘Exif” followed by two nulls. The EXIF structure is based on the
TIFF speciﬁcation and the EXIF signature is immediately followed
by an 8-byte TIFF header. An example of a TIFF header is shown
in Fig. 10. The format is as follows (hex values shown in italics
are from Fig. 10) (see Fig. 11).
Bytes 0–1: 0x4D4D. Byte order within the TIFF segment. There
are only two legal values – 0x4949 (‘‘II”) indicating little endian
and 0x4D4D (‘‘MM”) indicating big endian. The letters ‘‘I” and
‘‘M” are from Intel and Motorola, the two dominant chipset manu-
facturers at the time TIFF was developed; Intel used a little endian
architecture and Motorola a big endian one. TIFF endianness varies
among camera manufacturers – Canon uses little endian, Pentax
use big endian – and among mobile phone manufacturers - Sam-
sung uses little endian while Apple use big endian. Byte 0 also
has an important role for the calculation of offsets; TIFF makes
extensive use of these offsets and they are all calculated from the
address of byte 0.
Bytes 2–3: 0x002A. An arbitrary but carefully chosen number
(42) that further identiﬁes the ﬁle as a TIFF. Effectively, the ﬁrst
two endianness bytes along with these two give TIFF a unique
signature.
Bytes 4–7: 0x0000000A. The offset (in bytes) of the ﬁrst Image
File Directory (IFD) which holds information about the image as
well as pointers to the actual image data. The value 0x0000000A
in this case added to the start address of the TIFF header –
0x000C – tells us that the ﬁrst Image File Directory starts at
0x0016.
An Image File Directory has a structure where the ﬁrst two
bytes state the number of ﬁelds that the IFD contains. The ﬁnal 4
bytes of the IFD give the offset to the next IFD from the start of
the TIFF header; they are set to zero if this is the ﬁnal IFD.
Fig. 8. JPEG edited in Adobe Photoshop.
Fig. 9. JPEG ﬁle with certiﬁcate inserted.
152
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
Fig. 12 shows the physical ﬁle layout for a range of devices
including three digital cameras from leading manufacturers, an
iPhone, a Windows Phone, and two Android Phones - one high
end and one low end. Several things are worth noting in Fig. 12.
I. The sequence of IFDs varies between devices; Pentax and
Canon, for example put Interop IFD before IFD1 which comes
last in the sequence but Nikon put the Interop IFD last.
II. A number of the devices do not include an Interop IFD at all
even though this is mandated in the DCF which they are all
supposed to follow. The default values for the Interops tags,
however, are ‘‘R98” in the ﬁrst tag which is a reference to the
1998 DCF that ﬁrst introduced the Interops IFD and Version
1.0 in the second tag. Closer analysis showed that all the
devices that include the IDF have those default values;
whilst documentation of this could not be found, it seems
reasonable to conclude that devices that do not speciﬁcally
include the IFD work are in fact using those defaults.
III. Although all the devices have chosen to place values for each
IFD immediately following the IFD, that is not strictly neces-
sary as will be seen later when editing metadata is
discussed.
3.3. Metadata tags
IFD0 tags are not included as there are 231 of them but only a
handful are generally used in JPEG. Although there are no rules
in the EXIF speciﬁcation about the order of IFDs, all tags inside
an IFD must be in numerical order. The ﬁrst 10 tags shown
(F0. . .F9) are generally used in IFD0 as well as the Exif and GPS tags
which can be seen at the bottom of the list of tags in this. There are
4 tags identiﬁed as unknown. No documentation could be found
for those tags, they are not listed in the TIFF or EXIF speciﬁcations
and their values are all set to zero, so they appear to be private
ones added by Huawei but either not used or reserved for future
use. Hauwei, however, appear to have broken TIFF rules here –
all private tags used in a TIFF IFD are mandated to use id markers
above 0x8000 [30] as can be seen, for example, with the Exif and
GPS private tags. Examples of EXIF and GPS tags in the same ﬁle
are shown in Fig. 13.
There are over 30 GPS tags available; Latitude, Longitude and
Altitude tags are ubiquitous; other GPS tags vary between devices.
IFD1 holds the image thumbnail and can, in theory, contain any
TIFF tags i.e. the same as IDF0. In practice, however, only the tags
in Fig. 14 are generally used. The tags for ThumbnailOffset
(0x0201) and ThumbnailLength (0x0201) are used only in IFD1,
having been deprecated for IFD0 where the image data is taken
to start immediately after the SOS segment and run to the end of
the ﬁle, ending with 0xFFD9. The tag MakerNote (0x927C) is stored
inside the EXIF IFD and is another tag that is used to point to data
stored in an IFD type structure. It is intended for free use by the
camera or device manufacturer and generally contains extended
data about the camera, the lens and the various settings for when
the image was captured. Not all manufacturers use MakerNote,
Hauwei for instance do not and there are major inconsistencies
among manufacturers who do include it. First of all, manufacturers
use different lengths for the initial string of bytes that usually act
as a signature for the manufacturer. Secondly, whilst the actual
metadata is stored in an IFD structure, some manufacturers include
an internal TIFF header and this becomes the reference point for
calculating offsets to the tag values. Other manufacturers calculate
offsets relevant to the TIFF header at the start of the APP1 segment;
that means that if the tag values get moved (see next section) then
the offsets have to be all recalculated or they will be wrong. In
practice, that recalculation is made extremely difﬁcult if not
impossible due to very little if any information about MakerNote
Fig. 10. TIFF Signature.
Fig. 11. Image File Directory.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
153
being published by the manufacturers; most of the information
that is available has come from reverse engineering by researchers
like Phil Harvey, developer of ExifTool [31] which reads and writes
metadata for a vast range of ﬁle formats, executables as well as
image, audio and video ﬁles. His website provides the most com-
prehensive list available of information relating to metadata across
these formats including all known JPEG, TIFF and EXIF tags [32].
Leading camera manufacturer Canon includes over 80 tags in
MakerNote identiﬁed by ExifTool.
4. Evaluation
Changes in metadata do not necessarily affect the integrity of
a ﬁle if the core data of the ﬁle is not changed. This is no differ-
ent from paper documents where notations added to a document
do not affect the validity of the document. If a person receives an
image ﬁle from someone else and, for example, adds a tag to help
ﬁnd it afterwards then that should not affect the integrity of the
actual image. That principle should equally apply to metadata
changes arising from the transfer of a ﬁle between devices or
operating systems which should not be invalidated so long as
the image data is not altered. This should also apply to the com-
pression segments in a JPEG ﬁle which essentially are metadata
about the compression used; for example, if a single DHT seg-
ment is split into four that should not affect the validity of the
ﬁle again as long as the encoded image data is not affected.
We propose that the primary ﬁngerprint for a JPEG ﬁle should
be the hash value for the encoded image data. This is analogous
Fig. 12. JPEG ﬁle layout for different devices.
Fig. 13. Tags in EXIF IFD.
154
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
to the approach taken by Microsoft Word and Adobe Acrobat
which concatenate a digital signature with the document to
which it relates and, when the document is being veriﬁed, the
digital certiﬁcate is ignored for computing the hash value of
the document.
We also retain three ﬁngerprints - metadata, compression and
image data which we developed in JPEGReader as it will allow us
to build into its visibility approach a green/amber/red symbolism
where green means a ﬁle has been unaltered since the digital cer-
tiﬁcate was issued, amber means that metadata or compression
has been altered but the core image data is not changed so the dig-
ital certiﬁcate is still valid and red means that the digital certiﬁcate
is no longer valid as the core image data has been altered. A key
requirement is the ability to insert additional data as well as a dig-
ital certiﬁcate. For example, the comment metadata ﬁeld should
hold a URL to a website where a user can check the validity of a ﬁle
without having third party software installed by users. We also
incorporate automatic copyright stamping. In order to retain max-
imum ﬂexibility, Title, Author, Copyright and Comments are enu-
merated ﬁelds within our software but additional ﬁelds could be
inserted by supplying the appropriate tag index along with the
values.
Changes can take place in metadata either deliberately by a
user or inadvertently as a result of moving the ﬁle between
devices and operating systems. It would therefore be safest to
insert and digital certiﬁcate into its own application segment.
iOS has intolerance for application segments other than APP1,
APP13 and APP 14 in images received by email. Like other appli-
cations, however, it does allow multiple instance of APP1. It was
therefore decided to use an APP1 segment to hold the digital cer-
tiﬁcate. This APP1 would be distinguished from other APP1s by its
own signature. An example of a JPEG with a digital certiﬁcate
inserted is shown in Fig. 15. The ﬁrst APP1 is selected on the left
and the EXIF ﬁelds can be seen in the values; the second APP1 is
selected on the right and the signature can be seen in the values
along with the initial part of the certiﬁcate. The overall algorith-
mic approach for creating a ﬁle with references and a digital cer-
tiﬁcate inserted can be summarised as:
Overall Process
External
Application
This
Library
1
Open ﬁle
X
2
Get reference
X
3
Insert reference & accompanying
data into metadata
X
4
Calculate SHA256 hash value for
metadata segments.
X
5
Calculate SHA256 hash value for
compression segments
X
6
Calculate SHA256 hash value for
image data.
X
7
Return hash values as pointer to
array of pointers.
X
8
Return hash values to owner
X
9
Get digital certiﬁcate
X
10
Insert digital certiﬁcate into own
segment.
X
11
Return ﬁle as pointer to new data
stream.
X
12
Save and close ﬁle.
X
The corresponding process to verify a ﬁle is straightforward:
1 Extract digital certiﬁcate from ﬁle.
2 Read hash values from digital certiﬁcate.
3 Compute hash values for each section of the ﬁle - metadata seg-
ments, compression segments and encoded image data.
4 Compare computed hash values to the values extracted from
the digital certiﬁcate and report as appropriate:
0 = the ﬁle has been unaltered since the digital certiﬁcate was
issued.
1 = the metadata or compression has been altered but the core
image data is not changed so the digital certiﬁcate is still valid.
2 = the digital certiﬁcate is no longer valid as the core image
data has been altered
Fig. 14. Tags in IFD1.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
155
We developed JPEGReader and PNGReader to assist in the anal-
ysis of image ﬁles. These were developed in C# using Microsoft
Visual Studio. In order to keep the header ﬁle uncluttered, the code
was written used the Pimpl Idiom, also known as the Cheshire Cat or
Opaque Pointer which is a method of avoiding the forward declara-
tion of all private variables and methods in a C++ header ﬁle [33].
Our documented code is available at http://www.api.masters.mar-
tinharran.com. Our work shows that it is possible to insert a digital
certiﬁcate into a JPEG ﬁle and to edit the EXIF metadata ﬁelds to
show references to a certiﬁcate. These would be visible in native
applications such as Windows Explorer and OS X Inspector as well
as third party image viewers and editors.
Independent testing was carried out by Georgia Tech out on the
original Digiprove process.3 The procedure used by Georgia Tech
was replicated for the process developed in this project:
Step 1: For selected ﬁle samples, hash values were pre-
calculated using the same encryption used by us (software used
was sha256deep from md5deep version 3.4).
Step 2: Each ﬁle was then put through our content signing pro-
cess to generate a digitally-signed content certiﬁcate.
Step 3: A 3rd party digital signature veriﬁcation utility (Certiﬁ-
cates, Microsoft Corporation, Version: 5.1.2600.0) was used to
verify the integrity of the signature using our public X.509 dig-
ital identity certiﬁcate (issued by VeriSign).
Test metadata was inserted into an image ﬁle (baby.jpg) using
the Photoprove writer library. The SHA256 value for this ﬁle was
calculated using sha256deep from md5deep v4.3. The result: c7
a0a355a9b5926f7c332807e4ca380e378e20aee652a1f6d066276aa
b6324fe was then put through a content signing process which
yielded the same SHA256 value as Step 1. The resulting PS7 ﬁle
produced and emailed was inserted into an APP1 using the Photo-
prove writer library. In order to conﬁrm persistence, the ﬁle was
copied across devices and operating systems in a similar process
to that used earlier for testing persistence across operating systems
i.e. Windows via USB to Linux via DropBox to OS X via iCloud to
iPhone via Windows Explorer to Windows 10 (baby _ﬁnal.jpg). A
hex editor – HxD – was then used to manually strip the digital cer-
tiﬁcate back out of the ﬁnal ﬁle and save it as a PS7 ﬁle (CertEx-
tracted.p7s) and the JPEG minus the stripped APP1 was resaved
(baby_ﬁnal_cert_extracted.jpg). This allowed the processed ﬁle to
be verifoied against the extracted digital certiﬁcate. SHA256 was
recalculated for the stripped ﬁle and was identical. The PS7 ﬁle
extracted at Step 2 was opened in PS7Viewer, which veriﬁed the
digital signature. P7S viewer also has the facility to open the orig-
inal unsigned text to which the digital signature applies; this is
shown in Fig. 16 where the ﬁle ﬁngerprint can be seen (line 10)
along with the date/time stamp and other information inserted.
Again, that ﬁngerprint matches the one calculated in Step 2 using
sha256Deep.
We next checked the effect of sending the ﬁle with the inserted
certiﬁcate to an iPhone by email and comparing the original with
the image saved by the iPhone. As it was already known from
the previous testing that the iPhone would change the metadata
by creating a new thumbnail, there was no point in comparing
the overall hash value but Fig. 17 shows that the second APP1 hold-
ing the digital certiﬁcate has remained intact and Fig. 18 shows
that only the hash value for the metadata section has changed,
the compression segments and image data are unchanged.
Fig. 15. Digital certiﬁcate in an APP1.
Fig. 16. Testing - view original text.
3 Report for process evaluation & penetration testing service (D9507) Submitted by:
Mayur Ramgir, Georgia Tech Ireland, Athlone Business & Technology Park, Gar-
rycastle, Dublin Road, Athlone mayur.ramgir@gtri.gatech.edu.
156
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
5. Conclusion
We demonstrate that a digital certiﬁcate relating to an image
ﬁle can be inserted inside that image ﬁle along with accompanying
metadata containing references to the issuing company. Notwith-
standing variations between devices and across operating systems
and applications, a JPEG ﬁle holds its structure very well. Where
changes do take place, this is generally in the metadata area and
does not affect the encoded image data which is the heart of the
ﬁle and the part that needs to be veriﬁable. References to the issu-
ing company can be inserted into the metadata for the ﬁle. There is
an advantage of having the digital certiﬁcate as an integral part of
the ﬁle to which it applies and consequently travelling with the
ﬁle. We ultimately prove that the metadata within a ﬁle offers
the potential to include data that can be used to prove integrity,
authenticity and provenance of the digital content within the ﬁle.
Our research has also uncovered a wide range of previously
unknown aspects of how metadata gets handled by various operat-
ing systems, devices and image management applications. This is
knowledge that could be valuable to others working in the ﬁeld.
An unanticipated beneﬁt of the work has been the software tools
developed for examining and analysing image ﬁles. Those tools
have already attracted the interest of the wider forensics commu-
nity. Due to their massive popularity, image ﬁles, especially JPEG,
offer high high potential as carriers of other information. Much
of the work to date on this has focused on stenographic ways of
hiding information using least signiﬁcant bit techniques but we
believe that the ﬁndings in this project have exposed other ways
of doing this. If a digital certiﬁcate can be inserted into a JPEG, so
could the payload for a virus or a communication between criminal
or terrorist organisations.
Appendix A. Supplementary material
Supplementary data associated with this article can be found, in
the online version, at http://dx.doi.org/10.1016/j.aci.2017.05.006.
References
[1] F. Buccafurri, L. Fotia, G. Lax, Allowing privacy-preserving analysis of social
network likes, in: Eleventh Annual International Conference On Privacy,
Security And Trust, Pst 2013, Eleventh Annual International Conference on
Privacy, Security and Trust, PST 2013, IEEE Computer Society, Tarragona
(Spain) (USA), 2013.
[2] F. Buccafurri, L. Fotia, G. Lax, Allowing Non-identifying Information Disclosure
in Citizen Opinion Evaluation, Egovis/edem 2013, August 26–28, 2013, in:
Proceedings of International Conference on Electronic Government and the
Information Systems Perspective and International Conference on Electronic
Democracy (EGOVIS and EDEM 2013), Springer Verlag, Prague, Czech Republic
(DEU), 2013, pp. 241–254.
[3] K.N. Sowmya, H.R. Chennamma, Video authentication using watermark and
digital signature—a study, in: S. Satapathy, V. Prasad, B. Rani, S. Udgata, K.
Raju (Eds.), Proceedings of the First International Conference on Computational
Intelligence and Informatics Advances in Intelligent Systems and Computing,
vol. 507, Springer, Singapore, 2017.
[4] F. Buccafurri, L. Fotia, G. Lax, Allowing Continuous Evaluation of Citizen
Opinions Through Social Networks. International Conference on Electronic
Government and The Information Systems
Perspective and Intern,
in:
Fig. 17. Testing email to iPhone - Digital Certiﬁcate.
Fig. 18. Testing email - hash values.
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
157
Proceedings of International Conference on Electronic Government and the
Information Systems Perspective and International Conference on Electronic
Democracy (EGOVIS and EDEM 2012), Springer Verlag, Vienna (DEU), 2012.
[5] F. Buccafurri, L. Fotia, G. Lax, Privacy-Preserving Resource Evaluation in Social
Networks, Tenth Annual Conference on Privacy, Security and Trust (pst 2012),
in: Proceedings of the Tenth Annual Conference on Privacy, Security and Trust,
IEEE Computer Society, Paris (USA), 2012, pp. 51–58.
[6] Y. Ye, K. Xie, Q. Zeng, A study on DITA in digital publishing, in: Proc. SPIE
10322, Seventh International Conference on Electronics and Information
Engineering,
103224N
(January
23,
2017);
http://dx.doi.org/10.1117/12.
2265276.
[7] C. Kinsella, Establishing Proof of Existence and Possession of Digital Content,
Irish
Patent
Ofﬁce
84803,
2008.
<https://eregister.patentsofﬁce.ie/
HttpHandler/Handler.ashx?HandlerType=PDF&DocumentType=PT&
DocumentId=22647205>.
[8] Microsoft Ofﬁce Support. Digital Signatures and Certiﬁcates, Ofﬁce Support,
2016.
<https://support.ofﬁce.com/en-us/article/Digital-signatures-and-
certiﬁcates-8186cd15-e7ac-4a16-8597-22bd163e8e96>.
[9] P. Christof, P. Jan, Understanding Cryptography: A Textbook for Students and
Practitioners, Springer, Berlin, Heidelberg, 2010.
[10] A.J. Menezes, P.C. Van Oorschot, S.A. Vanstone, Handbook of Applied
Cryptography, CRC Press, 1996.
[11] P. Rogaway, T. Shrimpton, Cryptographic hash-function basics: deﬁnitions,
implications, and separations for preimage resistance, second-preimage
resistance, and collision resistance, Fast Software Encryption (2004) 371–
388 (Springer).
[12] R. Rivest, RFC 1321: The MD5 message-digest algorithm, April 1992’, Status:
INFORMATIONAl, 1992.
[13] H. Dobbertin, The status of MD5 after a recent attack, CryptoBytes 2 (2) (1996).
[14] X. Wang, Y.L. Yin, H. Yu, Finding collisions in the full SHA-1, in: Advances in
Cryptology–CRYPTO 2005, Springer, 2005, pp. 17–36.
[15] A. Sotirov, M. Stevens, J. Appelbaum, A.K. Lenstra, D. Molnar, D.A. Osvik, B. de
Weger, MD5 considered harmful today, creating a rogue CA certiﬁcate’, in:
25th Annual Chaos Communication Congress, 2008.
[16] C.R. Dougherty, Vulnerability Note VU# 836068 MD5 Vulnerable To Collision
Attacks’, 2008. <https://www.kb.cert.org/vuls/id/836068>.
[17] Swiat. Flame Malware Collision Attack Explained, Microsoft Technet, 2012.
Available:
<https://blogs.technet.microsoft.com/srd/2012/06/06/ﬂame-
malware-collision-attack-explained/>.
[18] R.L. Rivest, A. Shamir, L. Adleman, A method for obtaining digital signatures
and public-key cryptosystems, Commun. ACM 21 (2) (1978) 120–126.
[19] NIST. ‘Secure Hash Standard (SHS)’, FIPS PUB 180-1, 1995. Available: <http://
www.umich.edu/~x509/ssleay/ﬁp180/ﬁp180-1.htm>.
[20] S. Gueron, S. Johnson, J. Walker, SHA-512/256, Information Technology: New
Generations (ITNG), in: 2011 Eighth International Conference on, IEEE, 2011,
pp. 354–358.
[21] X. Wang, D. Feng, X. Lai, H. Yu, Collisions for Hash Functions MD4, MD5,
HAVAL-128 and RIPEMD, IACR Cryptology ePrint Archive, 2004, 199.
[22] NIST, ‘Secure Hash Standard (SHS)’, FIPS PUB 180-2, 2002. Available: <http://
csrc.nist.gov/publications/ﬁps/ﬁps180-2/ﬁps180-2.pdf>.
[23] NIST.
‘SHA3
Standard:
Permutation-based
hash
and
extendable-output
functions’, FIPS PUB 202, 2015. Available: <http://nvlpubs.nist.gov/nistpubs/
FIPS/NIST.FIPS.202.pdf>.
[24] NIST, NIST’S POLICY ON HASH FUNCTIONS, National Institute of Standards and
Technology,
2016.
Available:
<http://csrc.nist.gov/groups/ST/hash/policy.
html>.
[25] W. Difﬁe, M.E. Hellman, New directions in cryptography, IEEE Trans. Inform.
Theory 22 (6) (1976) 644–654.
[26] N. Leavitt, Internet security under attack: the undermining of digital
certiﬁcates, Computer 44 (12) (2011) 17–20.
[27] M. Fioretti, How to Add Metadata to Digital Pictures from the Command Line,
Linux.com,
2008.
Available:
<https://www.linux.com/news/how-add-
metadata-digital-pictures-command-line>.
[28] J.S. Moreland, Dream.In.Code -> Updating Jpeg Metadata Wthout Loss of
Quality [online], Dream In Code, 2010. Available: <http://www.dreamincode.
net/forums/blog/1017/entry-2305-updating-jpeg-metadata-wthout-loss-of-
quality/>.
[29] P. Harvey, JPEG Tags, ExifTool, 2016. <http://www.sno.phy.queensu.ca/~phil/
exiftool/TagNames/JPEG.html>.
[30] IPTC, Embedded Metadata Initiative - Manifesto [online], Embedded Metadata
Initiative, 2016. <http://www.embeddedmetadata.org/embedded-metatdata-
manifesto.php>.
[31] P. Harvey, ExifTool by Phil Harvey, ExifTool, 2016. Available: <http://www.
sno.phy.queensu.ca/~phil/exiftool/>.
[32] P. Harvey, ExifTool Tag Names, ExifTool, 2016. <http://www.sno.phy.queensu.
ca/~phil/exiftool/TagNames/index.html>.
[33] Z. Miners, Facebook, Twitter Called out for Deleting Photo Metadata [online],
Network World, 2013. Available: <http://www.networkworld.com/article/
2164374/software/facebook–twitter-called-out-for-deleting-photo-metadata.
html>.
158
M. Harran et al. / Applied Computing and Informatics 14 (2018) 145–158
