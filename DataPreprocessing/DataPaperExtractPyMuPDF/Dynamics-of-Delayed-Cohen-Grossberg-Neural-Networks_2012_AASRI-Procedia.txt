 AASRI Procedia  3 ( 2012 )  254 – 261 
2212-6716 © 2012 The Authors. Published by Elsevier B.V. 
Selection and/or peer review under responsibility of American Applied Science Research Institute
doi: 10.1016/j.aasri.2012.11.042 
 
 
2012 AASRI Conference on Modeling, Identification and Control 
Dynamics of delayed Cohen-Grossberg neural networks 
Ancheng Chang
Changlin Peng and Chuangxia Huang* 
College of Mathematics and Computing Science, Changsha University of  Science and Technology, Changsha, Hunan 410114, China. 
 
Abstract 
This paper studies the boundedness of Cohen-Grossberg neural networks with discrete delays and distributed delays 
(CGNN). Applying Lyapunov function and linear matrix inequalities technique (LMI), some novel sufficient conditions 
on the issue of the uniformly ultimate boundness, the existence of an attractor and the globally exponential stability for 
CGNN are established, which can be easily checked by the effective LMI toolbox in Matlab in practice. 
 
© 2012 Published by Elsevier B.V. Selection and/or peer review under responsibility of American Applied 
Science Research Institute 
 
Keywords: Linear matrix inequalities technique (LMI), Neural Networks, Distributed delay, Boundedness, Attractor, Stability. 
1. Introduction 
In recent years, much attention has been paid on neural networks since they have been fruitfully applied in 
signal and image processing [1,2,3]. These applications rely crucially on the analysis of the dynamical behavior 
[4,5,6,7]. Among them, CGNN [8] can be described as follows 
 
1
( )
( ( ))
( ( ))
(
( ))
,
[
]
n
i
i
i
i
i
ij
j
j
i
j
x t
x t
x t
a f
x t
J
                                                                    (1) 
 
 
* Corresponding author. Tel.:0-086-731-85258787; fax: 0-086-731-85258787. 
E-mail: cxiahuang@126.com.. 
Available online at www.sciencedirect.com
© 2012 The Authors. Published by Elsevier B.V. 
Selection and/or peer review under responsibility of American Applied Science Research Institute
Open access under CC BY-NC-ND license.
Open access under CC BY-NC-ND license.
255
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
where 
0,
2
t
n
; n  corresponds to the number of units in a neural network; 
( )
ix t  denotes the potential 
(or voltage) of cell  i at time t ; 
jf ( ( ))
x t
 denotes a non-linear output function; 
( ( ))
0
i x t
 represents an 
amplification function; 
( ( ))
i x t
 represents an appropriately behaved function; the n n  connection matrix 
(
ij )
n n
A
a
 denotes the strengths of connectivity between cells, and if the output from neuron j excites 
(respectively, inhibits) neuron i , then 
0
aij
(respectively, 
0)
aij
,  
iJ  denotes an external input source. 
 During hardware implementation, time delays do exist due to finite switching speed of the amplifiers and 
communication time and may lead to an oscillation which is degenerate to the instability of networks 
furthermore. For model (1),    Ye et al. [9] introduced delays by considering the following delay differential 
equations 
1
( )
( ( ))
( ( ))
(
(
))
,
1,
, .
[
]
n
i
i
i
i
i
ij
j
j
j
i
j
x t
x t
x t
a f
x t
J i
n                                         (2) 
 
 Although constant fixed delays in the models of delayed feedback systems serve as a good approximation 
in simple circuits consisting of a small number of cells, neural networks usually have a spatial extent due to 
the presence of an amount of parallel pathways with a variety of axon sizes and lengths [10, 12]. Therefore, 
there will be a distribution of conduction velocities along these pathways. 
In this paper, we will consider the following CGNN model with mixed delays (discrete delays and       
distributed delays): 
 
1
1
( )
( ( ))
( ( ))
(
( ))
(
(
)) 
[
n
n
i
i
i
i
i
ij
j
j
ij
j
j
j
j
j
x t
x t
x t
a f
x t
b f
x t
 
 
1
(
( ))d
.]
j
n
t
ij
j
j
i
t h
j
c
f
x s
s
J
     (3) 
 
Over the past decades, the stability of neural networks has been intensively investigated. In fact, except      
for stability property, boundedness is also one of the foundational concepts of dynamical neural networks, 
which plays an important role in investigation for the uniqueness of equilibrium point (periodic solutions), 
global asymptotic stability, global exponentially stable and its synchronization and so on [14, 15]. To the best 
of the authors  knowledge, few authors have considered on the ultimate boundedness and attractor for CGNN 
with interval time-varying delays and distributed time-varying delays. 
As is well known, compared with linear matrix inequalities (LMI) result, algebraic result is more 
conservative, and criteria in terms of LMI can be easily checked by using the powerful MATLAB LMI 
toolbox. This motivates us to investigate the problem of the ultimate boundedness and attractor for CGNN in 
this paper. 
2. Problem formulation 
System (3) for convenience can be rewritten as the following vector form 
 
( )
( ( ))
( ( ))
( ( ))
( (
))
( ( ))d
[
]
t
t h
x t
x t
x t
AF x t
BF x t
C
F x s
s
J                     (4) 
 
( ( ))
( ).
x t H t
Where,
1
( )
(
( ),
,
( ))T
n
n
x t
x t
x t
R  is the neural state vector; 
( ( ))
x t
 
1
1
diag(
(
( )),
,
(
( )))
n n
n
n
x t
x t
R
;   
( ( ))
x t
, 
( ( ))
F x t
 are appropriate dimensions functions; 
  
1
( ,
,
n )T
, 
1
( ,
,
n )T
h
h
h
;  
( 1
,
,
) ,
n T
J
J
J
( )
( ( ))
( ( ))
( (
))
H t
x t
AF x t
BF x t
 
256  
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
( ( ))
t
C t h
F x s
J . The discrete delays and distributed delays are bounded:
*
1
0
,
max{ };
i
i
i n
 
*
1
0
,
max{ };
i
i
i n
h
h
h
 
*
max{ , *
h }
, here 
*
, *
h ,
 are scalars. As usual, the initial conditions 
associated with system (4) are given in the form  ( )
( ),
x t
t
  
0,
t
 where ( )t  is a differentiable 
vector-valued function. 
Throughout this paper, we make the following assumptions. 
(
H1 )
 We assume that the delay kernels satisfy
(0)
0,
1,
,
jf
j
n, and there exist constants 
jl  and
jL , 
1,2,
, ,
i
n  such that     
 
( )
( )
,
,
,
.
j
j
j
j
f
x
f
y
l
L
x y
R x
y
x
y
 
 
(
H2 )
There exist positive constants 
i ,
i   such that 
                                                                                       
( ( ))
;
i
i
i
i
a x t
 
(
H3 )
 There exist positive constants
jb , such that         
                                                                                   
2
( )
(
( ))
( ).
j
j
j
j
j
x t
x t
b x t
 
Remark 2.1 The constants 
jl  and
jL  can be positive, negative or zero. Therefore, the activation functions 
( ( ))
f x t
 are more general than the forms|
( ) |
|
|,
0,
1,2,
,
j
j
j
f u
K
u
K
j
n . 
Definition 2.1 [13] System (4) is uniformly ultimately bounded, if there is  
0
B
, for any constant 
0 , there is 
(
'
)
0
'
t
t
, such that 
0
( , , )
x t t
B  for all 
0
', 0
0,
,
t
t
t t
 where 
0
0
1
0
( , , )
max sup |
(
, , ) |.
i
i n
s
x t t
x t
s t
 
 
Lemma 2.1 [11] For any positive definite constant matrix 
n n ,
W
R
scalar 
0
r
, vector function 
 
( ):[
, ]
n ,
u t
t
r t
R
 
0
t
, then
0
0
0
( )d
( )d
( )
( )d .
(
)
(
)
r
r
r
T
T
u s
s
W
u s
s
r
u
s Wu s
s  
3. Main Results 
Theorem 3.1 For a given constant
0
a
, if there is positive-definite matrix 
1
2
diag(
,
,
n ),
P
p p
p
 
1
2
diag(
,
,
)
i
i
i
in
D
D
D
D
, 
1,2
i
, 
,
Q R  such that the following condition holds 
 
257
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
11
12
13
22
24
33
44
55
56
66
77
0
0
0
0
0
0
0
0
0
0
0
0
0
0,
0
0
PB
PC
                                                                       (5) 
 
where  
11
12
22
0,
Q
Q
Q
Q
 
11
12
22
0,
S
S
S
S
11
12
22
0,
R
R
R
R
 
0,
1,2,
Di
i
 
 
*
*
*
11
1
2
11
3
1
2
,
a /
a
P
a
P
P
Se
h R
D
*
*
12
,
Se a /
 
*
13
12
12
4
1,
PA
Q
h R
D
*
*
*
22
11
3
2
/
,
a
a
e
Q
D
Se
 
*
24
12
4
2,
e a
Q
D
*
33
22
1
22,
Q
D
h R
*
44
22
2,
e a
Q
D
 
*
*
55
11
R e ah /
h ,
*
*
56
12
,
R e ah /
h
*
*
66
22
R e ah /
h ,
*
2
77
S , 
1
diag{1/ 1
,1/
n},
 
2
1
diag{ ,
,
n},
b
b
 
3
1
1
diag{
,
,
},
n
n
l L
l L
 
4
1
1
diag{(
)/2,
,(
)/2},
n
n
l
L
l
L
 
the symbol '*'  within the matrix represents the symmetric term of the matrix, then System (4) is uniformly 
ultimately bounded. 
 Proof. Choosing the following Lyapunov functional 
 
1
2
3
4
( )
( )
( )
( )
( ),
V t
V t
V t
V t
V t
                                                                                    (6) 
 
where 
( )
1
0
1
( )
2
d ,
( )
j
n
x
t
at
j
j
j
s
V t
p e
s
s
2( )
( )
( )d ,
t
as
T
t
V t
e
s Q
s
s  ( )
( ),
( ( ))
[
]
T
T
T
t
x t F
x t
, 
 
0
3( )
( )
( )d d ,
t
as
T
t
V t
e x
s Sx s
s
  
0
4( )
( )
( )d d
t
as
T
h
t
V t
e
s R
s
s
. 
 
Computing the derivative of 
1( )
V t  along the trajectory of system (4), one can get  
 
1( )
V t =
( )
0
1
2
( )d
[
j
n
x
t
at
j
j
j
s
ap e
s
s
( )
(
( ))]
at
j
j
j
j
p e x t
x t
 
 
258  
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
 
2
( )
( ( ))
2
( )
[
T
T
x t PAF x t
x t PJ
2
( )
( (
))
Tx
t PBF x t
2
( )
( ( ))d
.
]
t
T
at
x t PC t h
F x s
s e
  (7)             
 
According to Assumption (
2
H ), we obtain the following inequalities 
 
( )
2
0
2
d
( ).
( )
jx
t
j
j
j
j
j
s
a
ap
s
p x t
s
                                                                                (8) 
 
From Assumption (
3
H ), inequalities (7) and (8), we obtain 
 
1( )
V t
1
2
( )
( )
2
( )
( )
[
]
at
T
T
ae
x t
Px t
x t
Px t
2
( )
( ( ))
[
Tx t PAF x t
                                           (9) 
 
                2
( )
( (
))
Tx
t PBF x t
2
( )
( ( ))d
t
T
t h
x
t PC
F x s
s
( )
( )
]
T
T
at
x t Px t
J PJ e
. 
 
Similarly, computing the derivative of $V_2(t)$ along the trajectory of system (4), one can get 
 
 
2( )
V t
( ),
( ))
( ),
( ))
[
] [
]
at
T
T
T
T
T
e
x t F x t
Q x t F x t
 
 
              
(
)
(
( )),
(
))
[
]
a t
T
T
e
x t
t
F x t
(
( )),
(
))
[
]
T
T
T
Q x t
t
F x t
                           (10) 
 
   =
11
12
( )
( )
( ( ))
( )
at[
T
T
T
e
x t Q x t
F
x t Q x t
12
22
( )
( ( ))
( ( ))
( ( ))]
T
T
x t Q F x t
F
x t Q F x t
 
 
              
*
(
)
11
(
)
(
)
[
a t
T
e
x t
Q x t
12
( (
))
(
)
T
T
F
x t
Q x t
 
 
                              
12
(
)
( (
))
Tx
t
Q F x t
22
( (
))
( (
))]
FT
x t
Q F x t
. 
 
Computing the derivative of 
3( )
V t  along the trajectory of system (4), one can get 
 
             
3( )
V t = 
0
( )
( )
[ at
e xT
t Sx t
(
)
(
)
(
) d]
a t
T
e
x
t
Sx t
 
 
                        
*
( )
( )
at
e xT
t Sx t
*
(
)
( )
( )d ,
t
a t
T
t
e
x
s Sx s
s                                                          (11) 
 
Where 
*
1max{ }.
i
i n
 Denote 
1
2
Max{
,
,
,
n}
, we obtain 
 
*
( )
( )
at
e xT
t Sx t =
*
( ( ))
( )
( ( ))
( )
[
]
at
T
e
x t H t
S
x t H t  
*
2
( )
( ).
at
e HT
t SH t
             (12)                
259
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
  
Using Lemma 2.1, the following inequality is easily obtained 
 
 
*
(
)
( )
( )d
t
a t
T
t
e
x
s Sx s
s
*
(
)
*
( )d
( )d
(
) (
)
a t
t
t
T
t
t
e
x s
s
S
x s
s
                                              (13) 
 
=
*
(
)
/ *
( )
( )
2
(
)
( )
[
a t
T
T
e
x t Sx t
x t
Sx t
(
)
(
)]
Tx t
Sx t
. 
Similarly, computing the derivative of 
4( )
V t  along the trajectory of system (4), one can get 
 
      
4( )
V t
*
11
12
( )
( )
2
( ( ))
( )
at[
T
T
T
h e
x
t R x t
F
x t R x t
( ( )) 22
( ( ))]
FT
x t R F x t
                         (14) 
 
                                
*
(
)
*
( )
)d
/
d
(
(
) (
)
t
t
T
a t h
t h
t h
s
s
R
s
s e
h  
 
From Assumption 
( 1
H )
, we have 
 
                                       
(
( ))
(
( ))
0,
1,2,
, .
( )
( )
[
][
]
j
j
j
j
j
j
j
j
f
x t
f
x t
l
L
j
n
x t
x t
                            (15) 
 
Then we obtain 
 
                                    
3
1
4
1
1
( )
( )
0,
( ( ))
( ( ))
T
at
D
D
x t
x t
e
D
F x t
F x t
                                     (16) 
 
and 
                          
3
2
4
2
2
(
( ))
( (
( )))
T
D
D
x t
t
D
F x t
t
(
( ))
0
( (
( )))
at
x t
t
e
F x t
t
.                          (17) 
 
Denote 
( )
(
( ),
(
),
( ( )),
( (
)),(
( )d ) ,(
( ( ))d ) ,
( ( )) ,
)
 
t
t
T
T
T
T
T
T
T
T
T
t h
t h
M
t
x t x t
F
x t
F
x t
x s
s
F x s
s
H
x t
 
combing with (9)-(17), we have 
1
2
1
( )
( )
( )
( )
( )
.
at
T
at
T
V t
V t
V t
e M
t
M t
e J PJ                       (18) 
Therefore, one obtains 
2
2
1
1
( )
( (0))
,
at
at
T
K e
x t
V x
a e J PJ                                            (19)  
where 
1
1
/
min{a
}
j
j
j n
K
np
, which implies 
                                                                       
2
2
1
1
( )
[
( (0))
]/
.
at
T
x t
e
V x
a J PJ K                (20) 
260  
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
If one choose
1
1 1/2
[(1
)/
]
0
T
B
a J PJ K
, then for any constant 
0 and 
, there 
is
(
'
)
0
'
t
t
, such that  
( (0)) 2
1
e at
V x
for all 
'
t
t . According to Definition 2.1, we have  
( ,0, )
x t
B  for all  
'
t
t . That is to say, system (4) is   uniformly ultimately bounded.  
 
Theorem 3.2 If all of the conditions of Theorem 3.1 hold, then there exists an attractor 
B  for the 
solutions of system (4), where
0
{ ( ):
( )
,
}
 
B
x t
x t
B t
t
. 
Proof.  If one choose
1
1 1/2
[(1
)/
]
0
T
B
a J PJ K
, Theorem 3.1 shows that for any
, there is '
0
t
, 
such that 
( ,0, )
x t
B  for all 
'
t
t .  Let  
B  denote by 
0
{ ( ):
( )
,
}
 
.
B
x t
x t
B t
t
 Clearly,  
B is closed, bounded and invariant. Furthermore, limsup inf
( ;0, )
0
B
t
y
x t
y
. Therefore, 
B  is an 
attractor for the solutions of system (4).  
Theorem 3.3 In addition to all of the conditions of Theorem 3.1 hold, if 
0
J
, then system (4) has a 
trivial solution ( )
0
x t
 and the trivial solution of system (4) is globally exponentially stable. 
Proof. If
0
J
, then system (4) has a trivial solution ( )
0
x t
. From Theorem 3.1, one has 
                                                
2
2
( ;0, )
at
x t
K e
 for all 
                                                               (21)  
 
 where
2
2
( (0)) / 1
K
V x
K .  Therefore, the trivial solution of system (4) is globally exponentially stable.  
4. Conclusions 
In this paper, the dynamics of Cohen-Grossberg neural networks with mixed delays is investigated. Novel 
multiple Lyapunov-Krasovkii functionals are designed to get new sufficient conditions guaranteeing the 
uniformly ultimate boundedness, the existence of an attractor and the globally exponential stability. The 
derived conditions are expressed in terms of LMIs, which are more relax than algebraic formulation and can 
be easily checked by the effective LMI toolbox in Matlab in practice. 
Acknowledgements 
This work was supported in part by National Natural Science Foundation of China ( No.11101053), the 
Key Project of Chinese Ministry of Education (No.211118), the Excellent Youth Foundation of Educational 
Committee of Hunan Provincial (No.10B002), Science and Technology Project of Hunan of China (No. 
2010FK3025, No. 2012SK3096). 
References 
[1] Z. Yuan, L. Huang, D. Hu, B. Liu. Convergence of Nonautonomous Cohen-Grossberg-Type neural 
networks with variable delays. IEEE Trans. Neural Netw., 2008; 19, 140-147.  
[2] T. Roska, L.O. Chua. Cellular neural networks with nonlinear and delay-type template. Int. J. Circuit 
Theor. Appl., 1992; 20, 469-481.  
261
 Ancheng Chang et al. /  AASRI Procedia  3 ( 2012 )  254 – 261 
 
[3] D. Liu, A.N. Michel. Celular neural networks for associative memories. IEEE Trans. Circuits. Syst., 1993; 
40, 119-121.  
[4] P. Venetianer, T. Roska. Image compression by delayed CNNs. IEEE Trans. Circuits. Syst. I, 1998; 45, 
205-215.  
[5] T. Chen. 
. Neural Networks, 2001; 14, 
977-980.  
[6] K. Lu, D. Xu, Z. Yang. Global attraction and stability for Cohen-Grossberg neural networks with delays. 
Neural Networks, 2006; 19, 1538-1549.  
[7] H. Chen, etl.. Image-processing algorithms realized by discrete-time cellular neural networks and their 
circuit implementations. Chaos, Solitons, Fractals, 2006; 29, 1100-1108.  
[8] M. Cohen, S. Grossberg. Absolute stability and global pattern formation and parallel memory storage by 
competitive neural networks. IEEE Trans. Syst. Man Cybern., 1983; 13, 815-821.  
[9] H. Ye, A. Michel, K. Wang. Qualitative analysis of Cohen-Grossberg neural networks with multiple 
delays.  Phys. Rev. E, 1995; 50, 2611-2618.  
[10] J. Cao, K. Yuan, H. Li. Global asymptotical stability of gneralized recurrent neural networks with 
multiple discrete delays and distributed delays.  IEEE Trans. Neural Networks, 2006; 17, 1646-1651.  
[11] K. Yuan, J. Cao, J. Li. Robust stability of switched Cohen-Grossberg neural networks with mixed time-
varying delays.  IEEE Trans. Syst. Man Cybern, 2006; 36, 1356-1363.  
[12] J. Lian, K. Zhang. Exponential stability for switched Cohen-Grossberg neural networks with average 
dwell time. Nonlinear Dyn., 2011; 63, 331-343.  
[13] J. Cao, J. Liang. Boundedness and stability for Cohen-Grossberg neural networks with time-varying 
delays. J. Math. Anal. Appl., 2004; 296, 665-685.  
[14] H. Zhang, Y. Wang. Stability analysis of Markovian jumping stochastic Cohen-Grossberg neural 
networks with mixed time delays. IEEE Trans. Neural Netw., 2008; 19, 366-370.  
[15] C. Huang, L. Huang. Dynamics of a class of Cohen-Grossberg neural networks with time-varying delays.  
Nonlinear Anal. RWA, 2007; 8, 40-52. 
