Array 19 (2023) 100320
Available online 9 September 2023
2590-0056/© 2023 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).
Multiple robust approaches for EEG-based driving fatigue detection 
and classification 
Sunil Kumar Prabhakar, Dong-Ok Won * 
Department of Artificial Intelligence Convergence, Hallym University, Chuncheon, 24252, South Korea   
A R T I C L E  I N F O   
Keywords: 
MDS 
SVR 
SVM 
Hilbert transform 
ELM 
ANFIS 
TQWT 
A B S T R A C T   
Electroencephalography (EEG) signals are used to evaluate the activities of the brain. For the accidents occurring 
on the road, one of the primary reasons is driver fatigueness and it can be easily identified by the EEG. In this 
work, five efficient and robust approaches for the EEG-based driving fatigue detection and classification are 
proposed. In the first proposed strategy, the concept of Multi-Dimensional Scaling (MDS) and Singular Value 
Decomposition (SVD) are merged and then the Fuzzy C Means based Support Vector Regression (FCM-SVR) 
classification module is utilized to get the output. In the second proposed strategy, the Marginal Fisher Analysis 
(MFA) is implemented and the concepts of conditional feature mapping and cross domain transfer learning are 
implemented and classified with machine learning classifiers. In the third proposed strategy, the concepts of 
Flexible Analytic Wavelet Transform (FAWT) and Tunable Q Wavelet Transform (TQWT) are implemented and 
merged and then it is classified with Extreme Learning Machine (ELM), Kernel ELM and Adaptive Neuro Fuzzy 
Inference System (ANFIS) classifiers. In the fourth proposed strategy, the concepts of Correntropy spectral 
density and Lyapunov exponent with Rosenstein algorithm is implemented and then the multi distance signal 
level difference is computed followed by the calculation of the Geodesic minimum distance to the Riemannian 
means and finally tangent space mapping is implemented to it before feeding it to classification. In the fifth or 
final proposed strategy, the Hilbert Huang Transform (HHT) is implemented and then the Hilbert marginal 
spectrum is computed. Then using the Blackhole optimization algorithm, the features are selected and finally it is 
classified with Cascade Adaboost classifier. The proposed techniques are applied on publicly available EEG 
datasets and the best result of 99.13% is obtained when the proposed Correntropy spectral density and Lyapunov 
exponent with Rosenstein algorithm is implemented with the multi distance signal level difference followed by 
the calculation of the Geodesic minimum distance to the Riemannian means and finally tangent space mapping is 
implemented with Support Vector Machine (SVM) classifier.   
1. Introduction 
Staying alert is quite crucial for many jobs such as driving, con-
struction works and security guarding during night shifts etc. [1] The 
primary reason for all the accidents in the world is due to the drowsiness 
of the drivers. Drowsy driving can occur in any scenario but predomi-
nantly occurs in long-distance transportation. Several factors cause ac-
cidents due to fatigueness and drowsiness [2]. For drowsiness related 
crashes, a lot of factors are responsible such as length of the journey, 
weather condition, uniformity of the road, driving history of the driver, 
time of occurrence and the health condition of the driver. Before the 
onset of drowsiness, the driver must be alerted and it is one of the best 
methods to prevent the crash caused due to drowsy driving [3]. New 
devices have been developed by car industries so that driver drowsiness 
can be predicted well. Numerous drowsiness detection systems are being 
offered by car companies. A variety of vehicle-based indices are 
measured by these built in solutions so that fatigueness and the onset of 
drowsiness can be easily detected. To detect the drowsiness of the 
drivers, a variety of methods have been investigated and it includes 
vehicle-based measures, behavioral measures, or physiological mea-
sures [4]. To reflect the performance of the driver during driving, many 
vehicles based assistive technologies are used such as steering-angle 
sensors, ultrasonic sensors, GPS etc. Some of the drawbacks of these 
vehicle based assistive technologies include the interfering factors such 
as wind and rutted road surfaces, design, usage and assessment of the 
technology adaptable to each car brand. Behavioral measures include 
* Corresponding author. 
E-mail address: dongok.won@hallym.ac.kr (D.-O. Won).  
Contents lists available at ScienceDirect 
Array 
journal homepage: www.sciencedirect.com/journal/array 
https://doi.org/10.1016/j.array.2023.100320 
Received 12 June 2023; Received in revised form 17 July 2023; Accepted 6 September 2023   
Array 19 (2023) 100320
2
the facial behaviour of the driver including eye blinking, yawning, 
posture of the head, eye closure etc. [5]. Such measures can be observed 
by a camera and the driver can be alerted immediately if such symptoms 
are observed. However, being monitored in one′s privacy continuously 
throughout the journey is a huge issue. A lot of factors such as con-
sumption of a heavy meal during daytime, medications, lack of proper 
sleep, overthinking due to financial and family problems can cause 
drowsiness [6]. However, a lot of other accompanying factors are also 
present when drowsiness is referred too. An important impact on the 
progression of drowsiness is related to age and gender. Other important 
factors are smoking addiction leading to a poor appetite and sleep 
pattern, health issues caused due to anxiety, hypertension, high blood 
pressure etc. If one is feeling drowsy, there is always a state of reduced 
alertness and so it causes a performance degradation in the work and the 
person would not be able to think properly. The sudden changes in the 
behavioral pattern and sudden decline in cognitive functions can be 
easily measured and quantified using EEG signals [7]. 
With the advent of EEG and machine learning technologies, a lot of 
automated techniques have been proposed in the past with respect to the 
EEG based driving fatigue detection and classification. Using a two-level 
learning Hierarchy Radial Basis Function, the EEG based driving fatigue 
detection was analyzed by Ren et al. [8]. Depending on the log-Mel 
spectrogram and Convolutional Recurrent Neural Network (CRNN), 
the EEG driving fatigue detection was analyzed by Gao et al. [9]. Using 
multilevel feature extraction and iterative hybrid feature selection, the 
EEG based driving fatigue detection was analyzed by Tuncer et al. [10]. 
The research on fatigue driving detection using forehead EEG depending 
on adaptive multi-scale entropy was done by Luo et al. [11]. The chaotic 
entropy analysis of cortical sources obtained from scalp EEG signals was 
used for the driver fatigue detection by Chaudhuri and Routray [12]. 
Sparse-deep belief networks were used to improve the EEG based driver 
fatigue classification by Chai et al. [13]. The analysis of feature fatigue 
EEG signals based on wavelet entropy was analyzed by Wang et al. [14]. 
The single channel EEG-based mental fatigue detection based on deep 
belief networks was analyzed by Li et al. [15]. The EEG based driving 
fatigue detection using flexible analytic wavelet transform and multi-
boosting approaches was done by Subasi et al. [16]. The sample 
entropy-based method for real driving fatigue detection with multi-
channel EEG was done by Zhang et al. [17]. The modified PCANet 
method was used for driving fatigue detection from EEG by Ma et al. 
[18]. With the help of multiple entropy fusion analysis in an EEG based 
system, the driver fatigue detection systems were analyzed by Min et al. 
[19]. Using wearable EEG based on CNN, the vehicle driver drowsiness 
detection was analyzed by Zhu et al. [20]. A comprehensive review on 
the fatigue driving detection based on oculography was done by Tian 
and Cao [21]. With a dynamical encoder-decoder modelling framework, 
the driver drowsiness estimation using EEG signals was done by Are-
fnezhad et al. [22]. A light GBM based on EEG analysis technique for 
driver mental states classification was done by Zeng et al. [23]. The 
driving fatigue detection based on EEG signals of forehead area alone 
was performed by Mu et al. [24]. The EEG based drivers mental fatigue 
detection using Event-related synchronization/desynchronization and 
Hurst exponent was one by Rodriguez et al. [25]. An ensemble classifier 
for driver′s fatigue detection based on a single EEG channel was done by 
Wang et al. [26]. A new hand -modelled learning framework for driving 
fatigue detection using EEG signals was done by Dogan et al. [] [27] and 
a dynamic center and multi threshold point based stable feature 
extraction network for driver fatigue detection utilizing EEG signals was 
done by Tuncer et al. [28]. 
However, in this paper five efficient strategies are proposed for the 
detection and classification of driving fatigueness from EEG signals. 
The main contributions of this paper are as follows.  
a) The pre-processing of the EEG signals was done using Independent 
Component Analysis (ICA) [29]. In proposed strategy 1, after 
pre-processing the MDS and SVD are implemented and then it is 
merged and sent to the FCM based SVR classification module to get 
the output.  
b) In proposed strategy 2, after pre-processing the MFA is implemented 
and then the conditional feature mapping and cross domain transfer 
learning is implemented and finally classified with machine learning 
classifiers.  
c) In proposed strategy 3, after pre-processing the FAWT and TQWT is 
implemented and merged, and then it is classified with ELM, Kernel 
ELM and ANFIS classifiers.  
d) In the proposed strategy 4, after pre-processing, the Correntropy 
spectral density and Lyapunov with Rosenstein algorithm is imple-
mented and then the multi distance signal level difference is 
computed. Then the geometry of the SPD matrices is computed and 
the Geodesic minimum distance to the Riemannian means is calcu-
lated. Finally tangent space mapping is implemented to it before 
feeding it to classification with machine learning classifiers.  
e) In the proposed strategy 5, after pre-processing, the HHT is 
computed and then the Hilbert marginal spectrum is computed. Then 
using the Blackhole algorithm, the features are selected and finally it 
is classified with Cascade Adaboost classifier. The reasons for the 
algorithms chosen in the proposed methods are done after an in- 
depth study of the individual properties of the algorithms. 
The organization of the paper is as follows: In section 2, all the 
proposed strategies are explained and in section 3, the results and dis-
cussion are implemented followed by the conclusion in section 4. 
2. Proposed strategies 
2.1. Proposed technique 1 
The proposed technique 1 uses the idea of MDS and SVD for the 
purpose of dimensionality reduction. Then an FCM clustering based SVR 
technique is utilized to assess the performance of driving fatigue 
detection from EEG signals. The block diagram of the work is shown in 
Fig. 1. 
Fig. 1. Implementation of Dimension Reduction with FCM based SVR Classification Module.  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
3
2.1.1. MDS 
A lot of irrelevant or useless information is present in the high- 
dimensional vector which usually leads to a high computational 
burden to the machine learning classification. MDS is implemented to 
the pre-processed EEG signals so that the feature dimensions are reduced 
and therefore the classification accuracy should be improved greatly 
[30]. If only the similarity matrix can be obtained in between the ob-
jects, then reconstructing the Euclidean distance paved the way for 
developing MDS. Fitting the original data into low dimensional 
co-ordinate systems is the main idea behind MDS so that the trans-
formation could be minimized which is caused by dimension reduction. 
In between the paired samples, the similarity is estimated by the MDS. 
Before and after the dimension reduction, the similarity between the 
original samples is kept as constant as possible. Assuming a sample 
vector pi(1 ≤ i ≤ N), the detailed computation steps of the classical MDS 
is as follows: 
Step 1.
Distance matrix D is constructed. For every pair of vectors pi 
and pj(1 ≤ j ≤ N), with the help of Euclidean distance di,j, the distance 
matrix D = (di,j)N×N is assessed easily. 
di,j =
⃒⃒pi − pj
⃒⃒
(1)  
d2
i,j = pT
i pi + pT
j pj − 2pT
j pj
(2)  
Step 2.
Computation of inner product matrix. From the distance ma-
trix D, the inner product matrix B can be easily obtained and it is 
expressed as: 
B = − 1
2 MD2M
(3)  
where M = U − 1
N uuT, ′U′ denotes a unit matrix and ′u′ denotes a unit 
column vector with length N. 
Step 3.
Computation of low dimensional representation. Initially, the 
first ′k′ maximum eigen values are computed followed by the assessment 
of the corresponding eigen vectors of matrix B. Then to generate a low 
dimensional representation Z, the first ′k′ columns of G are employed as 
follows: 
G = UkV
1
/
2
k
(4) 
The diagonal matrix of the k maximum eigen values of B are denoted 
by Vk, where Uk is a matrix composed of the first ‘k’ orthogonal eigen 
vectors retained by M. The block diagram of the proposed work is shown 
in Fig. 2. 
2.1.2. SVD 
To reduce the high dimensional matrix into a low dimensional ma-
trix, SVD is highly useful [31]. The underlying meaning of the infor-
mation can be easily retained by SVD and some essential features can be 
extracted easily. A complex matrix can be represented by multiplying 
these smaller and simpler matrices. In the SVD theory, it is presumed 
that a m × n order matrix is present where all the elements belong to the 
real field. This can be represented by the SVD matrix M as follows: 
M = UΛVT
(5)  
where a m × m order matrix is represented by U, the diagonal matrix of 
m × n is represented by ∧ and an n × n order matrix is represented by VT. 
M = U ∧ VT =
∑r
i=1λiuivT
i =
∑r
i=1λiMi
(6) 
The ith column vector of the matrices U and V are represented as ui 
and vi. The ith singular value of the matrix M is represented as λi. The 
information contained by the sub matrix is reflected by each singular 
value λi. 
2.1.3. FCM- SVR technique 
Once the dimensionality is reduced, it is fed to the FCM-SVR classi-
fication module. In order to cluster the data in unsupervised learning, a 
well-known algorithm is FCM [32]. A set of object data {p1, p2, ..., pn}⊂ 
Rs×n is portioned into ′c′ fuzzy clusters by means of minimizing an 
objective function J(U, V) as follows: 
J(U, V) =
∑c
i=1
∑n
k=1um
ik‖pk − vi‖2
2
(7)  
where an object datum is represented by pk = [p1,k, p2,k, ..., ps,k]T. The ith 
cluster prototype is indicated by vi and the jth attribute of pk is repre-
sented by pj,k. The matrix of cluster prototype be represented as V =
[vij] = [v1, v2, ..., vc]T ∈ Rc×s. The fuzzification parameter is represented 
by f and m ∈ (1,∞). The Euclidean norm in Rs is denoted by ‖⋅‖2. The 
degree which pk belongs to the ith cluster is represented by the mem-
bership uik and the following condition has to be satisfied as follows: 
∑c
i=1uik = 1(k = 1, 2, ..., n; ∀i, k : uik ∈ [0, 1])
(8) 
The partition matrix is represented as U = [uik] ∈ Rc×n. With the help 
of some constraints, the vital conditions for minimization are done and 
so the following iterative updates formulae are obtained for the proto-
type and the partition matrix as follows: 
Fig. 2. Implementation of MFA with Conditional Feature Mapping based Cross domain transfer learning with machine learning classification module.  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
4
vi =
∑n
k=1um
ikpk
∑n
k=1um
ik
(i = 1, 2, ..., c)
(9)  
uik =
[∑c
t=1
(‖pk − vi‖2
2
‖pk − vt‖2
2
) 1
m−1]−1⎛
⎝
i = 1, 2, ..., c
and
k = 1, 2, .., n
⎞
⎠
(10) 
Unless the changes in the partition matrix values are lower than a 
particular predetermined threshold, the iterations are proceeded with it. 
2.1.4. Implementation of FCM – SVR for EEG based driving fatigue 
detection 
Support Vector Regression is dependent on Support Vector Machine 
(SVM) whose main intention is to assess the complex relationship be-
tween the input and the response of interest by means of mapping the 
data into a feature space with high dimension [33]. Assuming that the ith 
input and ith response of interest is indicated by pi = (pi1, ..., pid) and qi 
respectively. The constitution of the regression model of SVR is 
expressed as: 
q = wT⋅φ(p) + b
(11)  
where the feature maps are indicated by φ. The bias term is indicated by 
b and the weight vector is represented as w. By means of minimizing a 
cost function (C) which has a penalized regression error, the SVR model 
is built and is expressed as follows: 
C = 1
2wT⋅w + 1
2 γ
∑n
i=1e2
i
(12) 
In order to penalize large weights and to regularize weight sizes, the 
first part of cost function C used is a weight decay. For all the training 
data, the regression error is represented by the second part. The relative 
weight between them is determined by the parameter γ. Lagrange 
multiplier technique is utilized to optimize the cost function C as 
follows: 
L(w, b, e : α) = 1
2‖w‖2 + γ
∑n
i=1e2
i −
∑n
i=1αi ×
{
wT.φ(pi) + b + ei − qi
}
(13)  
where the Lagrange multipliers are indicated as αi. The optimal solution 
is obtained by means of setting the partial first derivatives to zero. Thus, 
w =
∑n
i=1αiφ(pi) =
∑n
i=1γeiφ(pi)
(14)  
where the utilization of a positive definite kernel is done as follows: 
K
(
pi,pj
)
= φ(pi)Tφ(pi)
(15) 
The original regression model is thus expressed as follows: 
q =
∑n
i=1αiφ(pi)Tφ(p) + b =
∑n
i=1αi〈φ(pi)T, φ(p)〉 + b
(16) 
In order to evaluate the point of qj, it is assessed as: 
qj =
∑n
i=1αi〈φ(pi)T, φ
(
pj
)
〉 + b
(17) 
By solving a set of linear equation, α vector can be obtained as 
follows: 
⎡
⎢⎣
K + 1
γ
1n
1T
n
0
⎤
⎥⎦
[ α
b
]
=
[
q
0
]
(18) 
The solution is thus represented as follows: 
[ α
b
]
=
⎡
⎢⎣
K + 1
γ
1n
1T
n
0
⎤
⎥⎦
−1[
q
0
]
(19) 
The Kernel function used is Radial Basis Function (RBF) in our work. 
2.2. Proposed technique 2 
In this proposed approach 2, once the pre-processing is done, MFA is 
applied initially and then the concept of conditional feature mapping is 
implemented. The concept of cross-domain transfer learning is proposed 
and implemented finally and classified with some machine learning 
classifiers. The block diagram of the work is shown in Fig. 2. 
2.2.1. Marginal Fisher Analysis 
To the pre-processed EEG signals, MFA is applied initially [34]. To 
design the intrinsic graph to explain the intra-class compactness, the 
famous graph-embedding based framework utilized is MFA. In order to 
explain the inter-class separation with the penalty graph also, this 
technique is used widely. In the standard MFA method, the distance 
measurement technique is the traditional Euclidean distance whereas in 
this paper the distance measurement technique utilized is the inner 
product. The solution process of the algorithm can be optimized and 
simultaneously it is kept consistent with the distance measurement 
technique. The computation of the proximity degree of same class in the 
intrinsic graph Gc is done by means of assessing the sum of the distance 
values between each sample and its nearest K sample points from the 
similar class, so that the intra-class compactness Sc is achieved and 
expressed as: 
Si =
∑
i
∑
i∈Nk(j)
or
j ∈ Nk(i)
(
MTpi
)TMTpj
(20)  
where the feature transformation matrix is represented as M. An index 
set of the nearest K sample points from the similar class of sample pi is 
represented by Nk(i). The computation of the proximity degree of inter- 
class marginal points in the penalty graph Gp is assessment by the sum of 
the distance values between each marginal singular points and its 
nearest m sample point from various other classes and the expression of 
inter-class separation Sp is done as follows: 
Sp =
∑
i
∑
(i,j)∈pm(ci)
or
(i, j) ∈ pm
(
cj
)
(
MTpi
)TMTpj
(21)  
where an index set of the nearest m pairs of marginal samples in 
{(i, j)|i ∈ πci,j ∕∈ πci} is represented by p(ci). The intra-class compactness is 
minimized and the inter-class separation is maximized so that with the 
help of graph embedding, the matrix M can be obtained and the 
expression of the objective function is done as follows: 
M = argmin
M
Sc
Sp
(22) 
The principle domain adaptation implies that a common feature 
space is built up where a similar distribution is present by both the 
source domain and target domain. The knowledge learnt in the source 
domain is transferred to the target domain so that the mapping matrix W 
is obtained. Assume that the source domain G and targe domain H have 
′c′ classes. Assuming that ′a′ samples in G and is expressed as P = [p1, p2,
..., pa] and ′b′ samples in H and is expressed as Q = [q1, q2, ..., qb]. The 
conditional feature mapping is expressed as follows. 
2.2.2. Conditional feature mapping 
The transfer learning will be highly influenced when the features of 
target domain are projected to the source domain. The probability is 
quite similar for the source domain samples which is being chosen. In 
the source domain, there are many singular points which may have a 
weak performance in inter-class separation. To the bounding areas in the 
source domain, the projection of the target domain will be done if these 
points are chosen and therefore some negative effects may be produced 
on the feature mappings [35]. Therefore, choosing the source domain 
samples in a random manner is unreasonable and so to deal with the 
negative effects, the feature mappings is restricted from the target 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
5
domain to the singular points in source domain. Between each sample 
point pi and sample point pj from the same class, the distance value Dij is 
calculated. The distance measurement method utilized is inner product 
and represented as Dij = pT
i pj,i ∕= j. In between each sample point pi and 
other sample points from the same class, the sum of the distance value Di 
is calculated as Di = ∑tk
j=1Dij, where the number of sample points of class 
k is represented as tk. The value of Di is sorted in ascending order for 
every point pi(1 ≤ i ≤ tk). The final sk points are processed as singular 
points and other points are processed as non-singular ones. The feature 
mapping from the target domain will be restricted for singular points 
and the feature mapping will be promoted for non-singular ones and 
therefore across the domains, the conditional feature mapping is 
realized. 
2.2.3. Cross domain transfer learning 
The standard transfer learning technique is implemented based on 
the conditional feature mapping mechanism [36]. The regularization is 
applied to W so that the deviation ψ(w) is minimized and therefore the 
over-fitting can be avoided. The expression of the objective function is as 
follows: 
min
W ψ(w)
s.t.
f1
(
PTWQ
)
≥ 0, 1 ≤ 0 ≤ c
(23)  
where the inter-domain constraints are specified by fi(PTWQ). LogDet 
regularization is used for W and it is specified as ψ(w) where it is pro-
jected as the sum of the singular values (σ1, σ2, ..., σz) of W and it is 
represented as ψ(w) = Σψi(σj), where the scalar function is denoted by 
ψi. In between the samples, the similarity function is expressed with the 
form of inner product. If the inner product KG = PTP and KH = QTQ, 
then the optimal solution of the problem is represented as: 
W = PK−1
/
2
G LK−1
/
2
H QT,
(24)  
where L is an a × b matrix 
The respective constraints are built if the intra-class compactness and 
inter-class separation is considered. To express the labelled samples in 
source domain G, (pi, lG
i ) is used, where lG
i is the label of pi and to express 
the labelled sample in target domain H, (qi, lH
i ) is used, where lH
i is the 
label of qi. The conversion of the constraints is done as follows for the 
sample pairs composed of the samples in G and H as follows: 
fij
(
piWqj
)
= pT
i Wqj − l, lG
i = lH
j
(25)  
fij
(
piWqj
)
= u − pT
i Wqj, lG
i ∕= lH
j
(26)  
where l indicates the lower limit of the similarity among the sample 
points and n indicate the upper limit of the similarity among the sample 
point. The similarity should be as high as possible for the sample pairs 
with the same classes but for other difficult classes, it is made smaller. A 
scaling function of ψi(σj) = σ2
j /2 is adopted so that the mapping 
learning problem can be converted as follows: 
min
W
∑z
j=1σ2
j
/
2
s.t.pT
1Wq′
j − l, lG
i = lH
j
u − pT
i Wq′
j, lG
i ∕= lH
j
(27)  
where p/
i = Mpi and q/
j = Mqj. A relaxation coefficient can be added as 
this is a strict convex optimization problem and the above expression is 
expressed as follows: 
min
W
∑z
j=1σ2
j
/
2 + λ
∑
i
∑
j
fij
[(
Mpi
)TW
(
Mqj
)]
s.t.pi ∈ P, qj ∈ Q
(28) 
Using domain adaption, the mapping matrix W is learnt given the 
features of both strong class and weak class. The weak class features P 
are extracted and projected them using W into the latent space so that 
the corresponding strong class features Q = PW are obtained. The 
inferred strong class features are used for testing and testing using a 
machine learning classifier and in our work four classifiers are consid-
ered such as NBC, KNN, SVM and Adaboost respectively. 
2.3. Proposed technique 3 
In this proposed approach 3, once the pre-processing is done, FAWT 
and TQWT are applied initially and then it is analyzed with ELM, ELM 
Kernel and ANFIS. The block diagram of the work is shown in Fig. 3. 
2.3.1. FAWT 
For multi-resolution analysis, processing of non-stationary and 
nonlinear signals and for the time-frequency localization purposes, 
wavelet transforms are used widely. Since it has some restrictions, such 
as dealing with low-resolution at high frequency sub bands and some 
problems with respect to shift variance, WT is not widely preferred. To 
overcome the above limitations, FAWT is used [37]. Arbitrary sampling 
rates are possessed by both low-pass and high-pass filters and so it leads 
to a flexible time-frequency partition manner. A good shift-invariance is 
also possessed by FAWT and for the analysis of complex oscillatory 
signals, FAWT is used widely. The parameters such as redundancy, 
dilation factor and the quality factor (QF) are controlled by FAWT. The 
EEG signals can be analyzed easily with pliable parameter such as p, q, r,
s and β. The up and down sampling factor are represented as p and q for 
the low pass channel. The up and down sampling factor are represented 
as r and s for the low pass channel respectively. The QF is weighed by a 
position constant β and their relationship is expressed as: 
QF = 2 − β/β
(29) 
The decomposition of the raw EEG signal is done into ′l′ level with the 
help of an iterative filter bank. Every level comprises of 2 high-pass 
filters and one low pass filter respectively. In our work, the values are 
as follows: p = 2, q = 4, r = 0.5, s = 1, β = (0.8r) /s. The level of 
decomposition is kept at l = 10. The sub-band EEG signals reconstructed 
Fig. 3. Analysis of FAWT, TQWT with ELM, ELM kernel and ANFIS  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
6
by FAWT are specified by sub bands. 
2.3.2. Tunable Q-wavelet transform 
The analysis of oscillatory EEG signals is analyzed by the TQWT by 
means of using adjustable input parameters such as Q-factor, no of 
decomposition level (L) and oversampling rate (r) [38]. The generation 
of L+1 sub-band signals are done by the L level decomposition of input 
EEG signal p(n). The low-pass signals are indicated by aL(n) and the 
high-pass sub band signals are specified by bL(n) respectively. The 
two-channel analysis filter bank is used at every decomposition level in 
an iterative manner. The sampling rate at every low-pass sub band is 
represented as αfs and sampling rate at every high-pass sub-band is 
represented as βfs respectively. The scaling factors are represented by α 
and β respectively, and the sampling frequency of the input EEG signal 
p(n) is denoted as fs respectively. The scale factors α and β are used to 
perform the low-pass scaling and high-pass scaling respectively. The 
low-pass and high-pass filters are present in the two-channel filter bank 
and it is denoted as H0(w) and H1(w) respectively, where the angular 
frequency is denoted by w. A restricted redundancy is offered by the 
filter bank of the TQWT so that a perfect reconstruction is achieved by 
the following relation: 0 < α < 1, 0 < β < 1 and α + β > 1. For L-level 
decomposition of the input EEG signal p(n), the sampling rate for the 
low-pass sub band signal aL(n) is αLfs while the sampling rate for high 
pass sub band signal bL(n) is αL−1βfs. At stage L, the frequency response 
of the low-pass filter HL
0(w) and the high-pass filter HL
1(w) is expressed as 
follows: 
H0(w)L =
⎧
⎨
⎩
∏
L−1
m=1
H0(w/αm), for
(w) ≤ αLπ
0, for
αLπ < |w| ≤ π
(30)  
H1(w)L =
⎧
⎨
⎩
H1
(
w
/αL−1) ∏
L−2
m=0
H1(w/αm), for (1 − β)αL−1π ≤ |w| ≤ αL−1π
0,
for other w ∈ [−π, π]
(31)  
where L ∈ N. In terms of θ(w), Hl(w) and Hh(w) are defined which is the 
frequency response of the Daubechies filter and is expressed as follows: 
H0(w) = θ
(w + (β − 1)π
α + β − 1
)
(32)  
H1(w) = θ
( απ − w
α + β − 1
)
(33)  
θ(w) = 1
2 (1 + cosw)
̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅̅
2 − cos w
√
(34) 
The Q-factor and r are pre-defined in TQWT implementation. The 
scaling parameters β and α are assessed using these values as follows: 
β =
2
Q + 1
(35)  
α = 1 − β
r
(36) 
Using the following relation, the maximum number of decomposi-
tion level Lmax is assessed as follows: 
Lmax =
⌊log(βN/8)
log(1/α)
⌋
(37)  
where the length of the input EEG signal is expressed as p(n). The Q 
factor helps to control the number of oscillations of the wavelet. The 
overlapping of the frequency response is governed by the parameter r. If 
the frequency response is narrowed, then the Q-factor is increased. 
Therefore, to cover the frequency range of the EEG signal, more and 
more number of decomposition levels are required. Overlapping of the 
adjacent frequency responses can be done by increasing the r value with 
a fixed Q-factor. 
2.3.3. ELM algorithm 
The ELM algorithm deals with randomly chosen input weights, 
hidden nodes and output weights which are analytically determined 
[39]. It has a pretty good generalization performance. A standard ELM 
classifier uses J hidden nodes where infinitely differentiable activation 
functions are used and it could easily approximate arbitrary samples 
with zero error. It implies that for a given training set (xi, Ti), i = 1, 2, ..
., N where Xi = [xi1, xi2, ..., xin]T ∈ Rn and Ti = [ti1, ti2, ..., tij]T ∈ Rj, there 
always exist βq, wi and bq that makes the following equation true 
∑
J
q=1
βqf
(
wi.Xi + bq
)
= oi = Ti, i = 1, 2, ..., N
(38)  
where the weight vector that helps to connect the qth hidden node with 
the output node is represented by βq. The SLFNs output for the ith sample 
is represented as oi. The label vector of the ith sample is represented as Ti 
and the weight vector connecting the ith sample and qth hidden node is 
denoted as wi. The bias of the qth hidden node is represented as bq and 
the activation function is represented as f( ⋅). The above equation can 
also be replaced with the following equation as follows: 
Hw,b,x,β = T; β =
⎡
⎢⎢⎣
βT
1
:
βT
J
⎤
⎥⎥⎦
J×J
; T =
⎡
⎢⎢⎣
tT
1
:
tN
⎤
⎥⎥⎦
N×J
(39)  
where the hidden-layer output matrix is termed as Hw,b,x. 
H(w1, ..., wJ, b1, ..., bJ, x1, ..., xN) =
⎡
⎣
f(w1.x1 + b1)
....
f(wJ.xN + bJ)
:
:
:
f(w1.xN + b1)
.....
f(wJ.xN + bJ)
⎤
⎦
N×J
(40) 
By means of computing the corresponding least-squares solution β =
H†
w,b,xT, the smallest training error can be achieved, where H†
w,b,xT rep-
resents the Moore-Penrose generalized inverse of Hw,b,x. The following 
three steps are present altogether in the ELM training algorithm. 
Step 1: The hidden node parameter wi and bq are assigned randomly, 
q = 1, 2,...,N. 
Step 2: The hidden-layer output matrix H†
w,b,x and its Moore-Penrose 
generalized inverse H†
w,b,x is computed. 
Step 3: The output weight β is calculated. 
2.3.4. ELM kernel algorithm 
Minimizing the training error T − Hβ2 and the norm of the output 
weights is aimed by the training process. A constrained optimization 
issue is used to represent the training process as follows: 
Minimize 
Ψ(β, ξ) = 1
2β2 + 1
2 C
∑
N
i=1
ξi2,
(41) 
Subject to: 
h(xi)β = ti − ξi, i = 1, 2, ..., N
(42)  
where the regularization factor C is used to control the tradeoff between 
the proximity of the training data to the smoothness of the decision 
function, so that there is a good improvement in the generalization 
performance. To solve the above optimization problem, Lagrange 
multiplier technique is used widely. If matrix ((I /C) +HTH) is not sin-
gular, then the solution β is obtained as follows: 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
7
β = HT(
I
/
C + HTH
)−1T
(43) 
Based on Mercer′s condition, Kernel technique can be implemented 
into ELM [40]. Depending on the above equation, the output vectors f(x)
of ELM Kernel is specified as follows: 
f(x) = h(x)β = h(x)HT
(
HHT + I
C
)−1
T
(44)  
f(x) =
⎡
⎢⎢⎣
K(xi, x1)
K(xi, x2)
:
K(xi, xN)
⎤
⎥⎥⎦
T
( I
C + K
)−1
T
(45)  
where, 
K = HTH =
⎡
⎣
K(x1, x1)
...
K(x1, xN)
:
:
:
K(xN, xN)
...
K(xN, xN)
⎤
⎦
(46) 
The number of training samples utilized for ELM Kernel is repre-
sented by N. 
2.3.5. Adaptive neuro-fuzzy inference systems 
To evaluate and assess the ability of the features in classifying the 
EEG signals, ANFIS is also used [41]. The features in the dataset are 
learnt by the ANFIS so that the system parameters can be adjusted well 
based on a given error criterion. For analyzing various types of signals, 
ANFIS is widely used. The training of the ANFIS classifiers is done with 
the backpropagation gradient descent technique and the least squares 
method so that the generalization is improved. The training of the ANFIS 
classifiers is done with these techniques using the input functions. The 
samples are given the binary target values of (1,0) and (0,1) respec-
tively. By utilizing a generalized bell-shaped membership function, the 
design of the fuzzy rule architecture of the ANFIS classifiers was done as 
follows: 
μji(pi) =
(
1 +
[pi − cji
aji
]2bji)−1
(47)  
where the adaptable parameters are expressed as (aji,bji,cji). Then first- 
order Sugeno type ANFIS models with the respective inputs and outputs 
are implemented. In the following form, the rules of the first order 
Sugeno fuzzy models are designed as: 
Ri : IF(pi is Ui1, ..., pm
is
Uim )
THEN q is gi(p1, ..., pm) = b0 + b1p1 + ... + bmpm
(48)  
where the ith rule of the fuzzy system is expressed as Ri, pi(i = 1, ..., m) are 
the inputs to the fuzzy system and q is the output of the fuzzy system. 
The adaptable parameters are bi(i = 0, 1, ...., m). The ANFIS output is 
expressed as: 
F =
∑
jgj(a1, a2, ..., am)πiμUji(ai)
∑
j
∏
iμUji(ai)
(49) 
The degree of membership of ai(i = 1, 2, ..., m) is expressed as μUji(ai)
to the antecedent linguistic term Uji for the ith rule of the fuzzy system. 
2.4. Proposed technique 4 
In this proposed approach 4, once the pre-processing is done, CSD 
and Lyapunov Rosenstein algorithm is implemented and then the Multi 
distance signal level difference is computed. Then Geometry of SPD 
matrices are computed following the assessment of geodesic minimum 
distance to the Riemannian mean is done. Finally, the concept of TSM is 
implemented and classified with Machine learning classifiers. The block 
diagram of the work is shown in Fig. 4. 
2.4.1. Correntropy spectral density 
The corresponding function F(m) for a stationary and discrete signal 
p(n), n = 1, 2, ..., S is expressed as: 
F(m)≜E[K(p(n), p(n − m))]
(50)  
where the number of EEG signal samples are represented by S and the 
time delta is represented as m. The expected value operated is indicated 
by E[ ⋅] and the symmetric positive-definite kernel is expressed by K(⋅, ⋅). 
The centered coentropy function is expressed as follows: 
Fc(m) ≜ E[K((p(n), p(n − m)))] − Ep(n)Ep(n−m)[K((p(n), p(n − m)))]
(51)  
where the marginal expected values of the Kernel K(⋅, ⋅) with respect to 
p(n) and p(n −m) are expressed as Ep(n) and Ep(n−m) respectively. In this 
work, the kernel function used is a Gaussian Kernel function. 
It is expressed as follows [39]: 
K(p(n), p(n − m)) =
1̅̅̅̅̅̅̅̅
2πσ
√
exp
[
− (p(n) − p(n − m))
2σ2
]
(52) 
The following equation is used to determine the σ bandwidth and it is 
represented as: 
σ = 0.9AL−1
5
(53)  
where A denote a small value. The inner product of two vectors in 
feature space is represented by any symmetric, positive-definite Kernel 
represented as the reproductive kernel Hilbert space (RKHS). The inner 
product depends on the RKHS operator and Φ(p(n)) is the mapping of 
signal p(n) into feature space. Correntropy function is similar to an or-
dinary correlation function and it is also positive-definite and so it is 
utilized in many signal processing applications. CSD is dependent on the 
Fourier transform of the centered Correntropy function and it is a pos-
itive definite matrix. A favorable performance for signals with Gaussian 
and non-Gaussian statistics is exhibited by CSD. Spectral estimation 
Fig. 4. Block diagram of the proposed technique 4.  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
8
technique can be easily used by CSD as only a few parameters need to be 
set. The spectral resolution of the CSD is much better than the PSD due to 
the versatility of the coentropy function. The higher order statistical 
moments are extracted easily by the coentropy function and so the ef-
ficiency of CSD is easily enhanced. 
2.4.2. Lyapunov exponents with rosenstein algorithm 
The first step lies in the reconstruction aspect of the attractor dy-
namics from a single time series. Method of delays is used as one goal of 
the work so that a fast and versatile algorithm is developed [42]. The 
reconstructed trajectory p is expressed as a matrix where every row is a 
phase-space vector.  
(i) Vector pi in phase space: 
pi = (p(ti)), p(ti + τ), ..., p(ti + (e − 1) ∗ τ)
(54)  
where the reconstruction delay is expressed by τ, embedding dimension 
is represented as e and ti ∈ [1,T − (e − 1)τ]. 
(ii) It is defined that λ, in the theory d(t) = C exp(λ(t)), the diver-
gence of the jth pair of nearest neighbours diverge at a rate pro-
jected by the largest Lyapunov exponent and is represented as: 
dj(i) = Cje(i.Δt)
where the initial separation is expressed as Cj. By computing the loga-
rithm on both sides of the above equation, 
ln dj(i) ≈ ln Cj + λ1(i.Δt)
(55) 
A collection of approximately parallel lines for (j = 1, 2, ..., m) is 
represented by the above equation where each slope is proportional to 
λ1. The calculation of the largest Lyapunov exponent is done using a 
least-square fit to the average line and is expressed as follows: 
q(i) = 1
Δt 〈ln dj(i)〉
(56)  
where the average over all values of j is indicated as 〈 ⋅〉. Computing the 
accurate values of λ1 is done by this process of averaging. Normalizing 
the separation of the neighbours is done by Cj. 
2.4.3. Multi distance signal level difference 
A modification of the gray-level difference (GCD) is the multi dis-
tance signal level difference [43]. The absolute value of the difference of 
two adjacent pixels in the diagonal, horizontal and vertical directions 
give us the GLD. The calculation of the GLD in the horizontal direction is 
as follows: 
q(i, j) = |p(i, j) − p(i, j + D)|
(57)  
where the pixel distance is denoted by D. As the signal utilized is 1D (one 
dimensional), then the above equation can be modified as follows: 
qd(i) = |p(i) − p(i + d)|,
(58)  
where i = 1, 2, ..., N − d and d = 1, 2,...,k. 
2.4.4. Geometry of SPD matrices 
Assuming Ai specifies a short segment of continuous EEG signals, Ai 
is indicated as follows: 
Ai = [At+Ti, ..., At+Ti+Ts−1] ∈ Rn×Ts
(59)  
where the ith trial of signal movement initiating at time t = Ti is indi-
cated by Ai. The number of sampled points of the selected segment is 
indicated by Ts. The spatial covariance matrix (SCM) Pi ∈ Rn×n for the ith 
trial is computed as follows: 
Pi =
1
Ts − 1PiPT
i
(60) 
There are two ways to classify EEG signals depending on the SCM in 
the Riemannian manifold and it is through the concept of computation 
of the Geodesic minimum distance to the Riemannian mean and the 
calculation of TSM followed by the analysis with machine learning 
classifiers. 
2.4.5. Geodesic minimum distance to the riemannian mean 
Between the two SPD matrices M1 and M2 in M(n), the Riemannian 
distance [44] is expressed as follows: 
δR(M1, M2) =
⃦⃦log
(
M−1
1 M2
)⃦⃦
F =
[ ∑
n
i=1
log2λi
]1
/
2
(61) 
If there are Z SPD matrices M1,...,Mz, the geometric mean (GM) in the 
Riemannian sense is expressed as follows: 
GM(M1, ..., Mz) = argmin
M∈M(n)
∑
z
i=1
δ2
R(M, Mi)
(62) 
The Riemannian distance between two unknown class M to the 
Riemannian mean point of each class is computed so that the unknown 
class is classified into categories which corresponds to the shortest dis-
tance. This is achieved by using Riemannian distance to Riemannian 
mean (MDRM). 
2.4.6. Tangent space mapping (TSM) 
With the help of a differentiable Riemannian manifold F, the indi-
cation of the SPD matrix of M is done. Every tangent vector Si is obtained 
as the derivative at t = 0 between M and the exponential mapping Mi =
ExpM(Si) and is expressed as follows: 
ExpM(Si) = Mi = M
1
/
2 exp
(
M−1
/
2SiM−1
/
2)
M
1
/
2
(63) 
The logarithmic mapping is used to express the inverse mapping and 
is expressed as follows: 
logM(Mi) = Si = M
1
/
2 log
(
M−1
/
2MiM−1
/
2)
M
1
/
2
(64) 
The Riemannian mean of I > 1SPD matrices is evaluated using the 
Riemannian geodesic distance as: 
GD(M1, ..., MI) = argmin
M∈M(n)
∑
I
i=1
δ2
R(M, Mi)
(65) 
With the help of the tangent space which is situated at the geometric 
mean of the whole set trials 
MGD = GD(Mi, i = 1, ..., I)
(66) 
The mapping of each SCM Mi is done into this tangent space so that 
the set of z = n(n +1) /2 dimensional vectors are yielded and it is rep-
resented as [45]: 
Si = upper
⎛
⎜
⎝M−1
/
2
GD log(Mi)M−1
/
2
GD
⎞
⎟
⎠
(67) 
Numerous classification algorithms are implemented in the Rie-
mannian space. 
2.5. Proposed technique 5 
In this proposed work, once the pre-processing of EEG signals are 
done, HHT and Hilbert marginal spectrum is computed and the best 
features are obtained by Black Hole optimization algorithm and finally it 
is classified by the cascade Adaboost classifier. The block diagram of the 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
9
work is shown in shown in Fig. 5. 
2.5.1. HHT and hilbert marginal spectrum 
To analyze non-linear and non-stationary signals, HHT is highly 
useful [46]. It is generally not subject to the Heisenberg Uncertainty 
principle. Empirical Mode Decomposition (EMD) and HT are the two 
important parts of HHT. A new adaptive EEG signal time-frequency 
processing technique is EMD. The EEG signal p(t) is decomposed by 
into a finite intrinsic mode function (IMF) based on the signal scale. The 
local characteristic EEG signals of various time scales of the original EEG 
signal are contained by the IMF. Two important criteria are present for 
understanding the IMF concept. Throughout the data segment, the 
number of zero crossings and the number of extreme points must be 
more or less equal. Secondly, the average of the lower envelope found by 
the local minimum points and the average of the upper envelope formed 
by the local maximum points is zero. From the upper and lower envelope 
sequences, the average curve sequences are obtained for a given EEG 
signal p(t) is as follows: 
m1(t) = pmax(t) + pmin(t)
2
(68) 
The residual component of the data is obtained by finding a new time 
series and is represented as the difference between p(t) and the average 
curve sequence. 
h1(t) = p(t) − m1(t)
(69) 
Unless the judgement criteria are not satisfied, the above two pro-
cesses are repeated and the remaining component is obtained as: 
r2(t) = p(t) − h1(t)
(70) 
Until the termination condition is satisfied, the remaining sequence 
repeats the equations so that multiple IMFs are obtained. 
p(t) =
∑n
i=1ci + rn
(71) 
The IMF component is represented as ci and the residual function is 
denoted by rn. A Hilbert transform is performed on ci and an analytic 
signal is constructed and a Hilbert marginal spectrum is obtained [47]. 
Convolving the signal with 1/πt is essentially a 90-degree phase shifter 
and it is performed by HT. The efficiency of the signal is improved by the 
Hilbert marginal spectrum. A higher frequency resolution is obtained by 
the Hilbert marginal spectrum and it also represents a variable 
frequency. 
H[ci] = 1
π
∫∞
−∞
ci(τ)
t − τdτ
(72)  
si(t) = ci(t) + jH[ci] = ai(t)ejψ1(t)
(73) 
The instantaneous amplitude is represented by ai(t) and the instan-
taneous phase is represented by ψi(t). 
H(f, t) = Re
∑n
i=1ai(t)ej2π∫
fi(t)dt
(74)  
h(i) =
∫∞
−∞
H(f, t)dt
(75) 
The instantaneous frequency is represented by fi(t). 
2.5.2. Black-hole algorithm 
The essential features are then chosen by the Black-hole algorithm 
[48]. Black-hole algorithm is used here as it is relatively easy to 
implement and is somewhat free from the tuning parameter issues. This 
algorithm utilizes a method of exploration/exploitation which is totally 
free of external components so that the probability of being affected to 
unexpected changes is reduced drastically. In every evaluation, the 
black hole algorithm in optimization problem converges to global 
optimal, whereas some of the other standard algorithms like PSO, ACO 
and GA might get stuck in local optimal solutions. The black hole al-
gorithm is inspired by the law of attraction/absorption and it is 
dependent on the phenomenon of the same name which occurs in outer 
space. Three main fundamentals are present in this algorithm as follows.  
A) A star present in space is considered as a solution to the problem. 
The random generation of a certain number of stars are done as it 
is a population-based algorithm.  
B) The selection of the black hole is done. A black hole indicates the 
star which has the best performance of all the solutions.  
C) By using the absorption formula, the movement and generation of 
new stars are carried out as follows: 
xd
i (t + 1) = xd
i (t) + α
[
xd
bh − xd
i (t)
]
, ∀i ∈ {1, ..., n}
(76)  
where the dth component of the ith star in the iteration t is represented as 
xd
i (t). The dth component of the black hole in the search space is repre-
sented as xd
bh. The number of solutions is represented by n and a random 
uniform number of distributions between zero and one is represented as 
α. The dth component of the location of the ith star in the next iteration is 
represented by xd
i (t + 1). The event horizon is actually a radius R which 
is originated by the black hole. The black hole can be easily absorbed 
and destroyed in case a star crosses the horizon, so that a new star could 
be created randomly. It is observed as the probability of crossing the 
even horizon and is computed as follows: 
R =
fbh
∑n
i=1fi
(77)  
where the performance value that has the best solution is represented by 
fbn and the value associated with the quality of the ith star is represented 
by fi and the number of stars in represented by n. The star crosses the 
event horizon when the distance between the black hole and the star is 
less than the radius. The absorption of this star happens and the random 
generation of the new one is done. The variability offered by even ho-
rizon is highlighted so that the common problem of local optimum 
stagnation is resolved completely. 
Approximation to good solutions is the most vital characteristic of 
incomplete data processing algorithm and this concept is utilized as stop 
Fig. 5. Block Diagram of the proposed technique 5.  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
10
criteria. It is quite difficult to assess the quality of found solutions in 
situations where the optimal solutions are not known apriori. In such a 
case, the ideal stop criteria depend on the total number of executed it-
erations in the algorithm. The stop criteria in our work are initially set as 
1000 off-line iterations. The optimization procedure is displayed in Al-
gorithm 1. The random generation of the initial n star population for 
every intrinsic signal starts accordingly. 
Algorithm 1: Black hole Algorithm implemented to the concept. 
Input: number of stars (solution), stop criteria (maximum of 
iterations). 
Output: the black hole, the best feature.  
(1) Generate the first generation of n stars in the search space.  
(2) Select the ideal solutions as the black hole.  
(3) While a good enough solution has not been reached in a 
maximum of iterations do  
(4) For all stars si, (∀i = 1, ..., n) do  
(5) Classifier Feature performance  
(6) Location changes of si happens according to equation (76)  
(7) If si is better than black hole, then  
(8) Select the current solution si as black hole.  
(9) End if  
(10) {Sicross to the event horizon defined by equation (77)}  
(11) If r > fbh/∑n
i=1fi then  
(12) Replace si with a new star in search space.  
(13) End if  
(14) End for  
(15) End while  
(16) Return results. 
The degree of variability in an algorithm is allowed by randomness. 
The process of absorption of the algorithm in the loop statement is 
carried out. The calculation of the quality of each solution is done and it 
is assessed by the performance of the feature. The solution is considered 
to have a high quality if the rating value is close to 1. The solution is 
considered to be of low quality if the rating value is close to 0. When the 
stars are absorbed by the black hole, the solutions are generated. For 
each intrinsic signal, this process generates a real number of pre-
dictabilities. The locations are swapped if a star reaches a value which is 
better than the black hole. A new value is generated randomly if a star 
crosses the event horizon of the black hole. It is evaluated based on a 
random variable with uniform distribution r ∼ [0, 1] and this entire 
procedure is fully iterated. A proportion between the fitness of the star 
and the combined value of all fitness is used so that the quality of the 
solutions is measured and this value is termed as event horizon. The 
absorption of the star will be done of this percentage value is less than r 
which is generated in a random manner. Thus, a good variability to the 
solutions is provided by the non-deterministic process. Finally, when an 
adequate solution is reached, the loop statement terminates. By means of 
updating the solution in a certain number of iterations, this condition 
can be easily determined. The best solutions are memorized and eval-
uated at the end. 
2.5.3. Cascade-Adaboost classifier 
The features selected by the Black-hole optimization algorithm are 
then fed to the Cascade-Adaboost classifier [49]. A parallel classifier is 
Adaboost classifier which combines many linear weak classifiers. The 
weak classifier can be enhanced by the AdaBoost training algorithm so 
that self-adaptive goals can be easily achieved. In the given feature set, 
the focus is on the classification of one dimension by every weak clas-
sifier. By adding a lot of weak classifiers, several key features can be 
easily focused on by the AdaBoost classifier. By adding a lot of weak 
classifiers, several key features can be easily focused on by the AdaBoost 
classifier. Once the addition of the weak classifier is done, the employ-
ment of the minimum error is calculated so that the weight value of this 
weak classifier is computed. Then for all the training samples, the weight 
values of this weak classifier are computed. Then for all the training 
samples, the weight values are re-adjusted and ultimately these weight 
values are chosen as an input to the next training iteration. In every 
training iteration, a new weak classifier is added and so there is a 
minimal improvement in the Adaboost classifier. Ultimately, the result 
of the strong classifier H(F) is expressed as follows: 
H(F) =
∑
T
t=1
βtht(fi)
(78)  
where the feature set is expressed as F. F = {f1, f2, f3, f4, f5}. The weak 
classifier appended in the tth training iteration is expressed as ht. The key 
feature selected by the classifier ht is expressed as fi. The corresponding 
weight value of every weak classifier is represented by βt and the 
number of training iteration is represented by T. By training weak 
classifiers, the discrimination of features is explored and so the Adaboost 
classifiers are organized in a cascade way. Therefore, simple weak 
classifiers are considered with which the target of the cascade-Adaboost 
classifiers is also manipulated. The simple threshold classifiers are 
chosen as weak classifiers and is expressed as follows. 
ht(fi) =
{ 1
Tlower ≤ fi ≤ Tupper
0
otherwise
(79)  
where Tlower and Tupper represented the threshold for the weak classifier 
ht and is obtained using any search technique. Now the positive training 
samples and negative training samples are initiated with different 
weight values. Assuming {fi, yi} be the training samples, fi, i = 1, ..., n is 
the feature set, yi ∈ {1, −1} indicate the snoring or non-snoring class. 
Assuming that there are p positive samples and q negative samples in the 
training set, their respective weight values can be projected as follows: 
wp = q.wn
(80)  
p.wp + q.wn = 1
(81)  
where the number of positive samples is represented by p and the 
number of negative samples is represented by q. The weight values of 
positive samples and negative samples are represented by wp and wn 
respectively. The sum of all these T weak classifiers give us the strong 
classifier H(F). The strong classifier is expressed as follows: 
H(F) = max
s
∑
T
t=1
βs
ths
t
(
f s
i
)
(82)  
where fs
i , i = 1, ..., n are the features at scale s. The training of the T weak 
classifiers are done for each scale s so that the corresponding weight 
values βs
t are computed. By adding a lot of new weak classifiers, the 
overall accuracy of Adaboost classifier can increase but still it could fail 
if some challenging scenarios are present. Therefore, multiple Adaboost 
classifiers are organized in a cascade fashion using cascade-Adaboost 
classifier which can enhance accuracy. If the input feature vector F =
{f1, ..., fn} which is selected by the Adaboost classifier is assessed as a 
negative sample, then it would be removed in the training set and as a 
result it would not progress to the next Adaboost classifier. Conse-
quently, the number of training samples in training set is mitigated if the 
input feature vector F is assessed as a positive sample, then this partic-
ular training sample progresses to the next layer for classification. The 
training samples on the end layer are quite similar to each other and so 
there is a good focus on similar training samples by the Adaboost clas-
sifiers. As a result, cascade-Adaboost classifiers have the versatility to 
classify the selected features efficiently giving a high classification ac-
curacy. 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
11
3. Results and discussion 
For this study, a publicly available EEG dataset was used [50]. For 
the acquisition of EEG signals, a brain cap with 32 electrodes was used. 
The utilized dataset in this work has EEG signals with 32 channels. The 
age group of the subjects was between 17 and 25 and the EEG data was 
collected from 16 healthy subjects (8 males and 8 female). With the help 
of a driving simulator and a brain cap, the EEG signals were collected. 
Prior to the experiment, no stimulants were used by subjects such as tea, 
coffee, alcohol, beer, energy drinks etc. The standard metrics utilized 
here for the analysis was Sensitivity, Specificity and Accuracy and is 
represented by the following formulae as follows: 
Sensitivity =
TP
TP + FN
(83)  
Specificity =
TN
TN + FP
(84)  
Accuracy =
TP + TN
TP + TN + FP + FN
(85)  
where TP means True Positive, TN means True Negative, FP indicates 
False Positive and FN indicates False Negative respectively. To obtain 
the results for the proposed works, a 10-fold cross-validation scheme has 
been utilized in our study. As far as the first proposed strategy is con-
cerned, the kernel function used is RBF in FCM-SVR model and the value 
of C is set as 50. For the KNN classifier used in this work, the value of K is 
set as 8 in our experiment. The Gamma value used for SVM is set as 0.04 
in our experiment. Table 1 shows the result analysis of the proposed 
MDS and SVD with FCM-SVR technique where a high classification ac-
curacy of 97.99% is obtained. Table 2 shows the proposed method of 
using MFA and conditional feature mapping and cross domain transfer 
learning with machine learning classifiers, where a high classification 
accuracy of 99.12% is obtained with SVM classifier. Table 3 shows the 
proposed method of using FAWT and TQWT hybrid classifier with ANFIS 
reporting a high classification accuracy of 98.18%. Table 4 shows the 
proposed CSD and Lyapunov exponents with multi distance signal level 
based tangent space mapping and SVM classifier reporting a classifica-
tion accuracy of 99.13% for SVM classifier. Table 5 shows the proposed 
HHT with Hilbert Huang transform with the proposed BH based cascade 
Adaboost classifier reporting an accuracy of 98.08%. 
Fig. 6 shows the performance comparison of classification accuracy 
among the proposed methods. It is evident a very high classification 
accuracy is obtained for the proposed CSD and Lyapunov exponents with 
multi distance signal based tangent space mapping concept with SVM 
machine learning classifier. 
3.1. Comparison with other works 
Table 6 shows the current works with the previous works compari-
son. Our works produced a good classification accuracy when compared 
to the previous works, however Tuncer et al. [10], produced a slightly 
higher classification accuracy by more or less 1% increase in a few cases. 
Our future works aim to improve a very high classification accuracy by 
incorporating a lot of other versatile methods, but comparatively our 
methods produce a good classification accuracy. 
The proposed MDS + SVD + FCM-SVR produced a classification 
accuracy of 97.99% and the reason for this could be attributed to the 
intrinsic properties of MDS and SVD and as it was combined with FCM- 
SVR, this high classification accuracy was obtained. The Proposed MFA 
+ Conditional Feature Mapping + Cross domain transfer learning with 
machine learning classifiers produced a high classification accuracy of 
99.12% and the reason for obtaining this high classification accuracy 
could be attributed to the combination factors of MFA and conditional 
feature mapping accompanied with cross domain transfer learning. The 
proposed FAWT + TQWT with ANFIS produced a high classification 
accuracy of 98.18% and this reason is due to the combined performance 
of the properties of the versions of the wavelet transform with ANFIS 
Table 1 
Results of the MDS + SVD based FCM-SVR technique.  
Dimensionality Reduction 
Technique 
Classification 
Technique 
Classification 
Accuracy 
MDS 
FCM + SVR 
92.46% 
SVD 
FCM + SVR 
91.47% 
Proposed MDS + SVD 
FCM + SVR 
97.99%  
Table 2 
Results of the proposed MFA-conditional feature mapping – cross domain 
transfer learning with machine learning classifiers.  
Proposed Method 
NBC 
KNN 
SVM 
Adaboost 
Proposed MFA +
Conditional Feature Mapping +
Cross domain Transfer learning 
98.17% 
97.89% 
99.12% 
98.98%  
Table 3 
Results of the proposed FAWT-TQWT with ELM, KELM and ANFIS classifiers.  
Proposed Method 
ELM 
KELM 
ANFIS 
FAWT 
89.45% 
92.34% 
93.33% 
TQWT 
91.46% 
94.56% 
91.25% 
Proposed FAWT + TQWT 
94.54% 
97.89% 
98.18%  
Table 4 
Results of the proposed CSD and Lyapunov exponents with multi distance signal 
level based tangent space mapping with machine learning classifiers.  
Proposed 
Method 
NBC 
KNN 
SVM 
Adaboost 
DT 
RF 
Proposed 
CSD and 
Lyapunov 
exponents 
with Multi 
distance 
signal 
level 
based 
Tangent 
Space 
Mapping 
98.45% 
96.54% 
99.13% 
97.45% 
97.89% 
95.66%  
Table 5 
Results of the proposed HHT-Hilbert Huang Transform with BH based cascade Adaboost classifier.  
Proposed Method 
Cascade Adaboost Classifier 
Black Hole 
Optimization 
Artificial Bee Colony 
Optimization 
Ant Colony 
Optimization 
Particle Swarm 
Optimization 
Glowworm 
Swarm 
Optimization 
Proposed HHT + Hilbert Marginal Spectral with swarm 
intelligence-based cascade Adaboost classifier 
98.08% 
94.34% 
91.34% 
97.98% 
96.99%  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
12
classifier. The proposed CSD and Lyapunov exponents with multi dis-
tance signal level based TSM with SVM produced the highest classifi-
cation accuracy of 99.13% and the reason can be contributed to the 
nature of the components used in this experiment, that when it was 
hybrid together a higher classification accuracy was obtained. Finally, 
when the proposed HHT with Hilbert marginal spectrum and black hole 
optimization with cascade Adaboost classifier was obtained, a high 
classification accuracy of 98.08% was obtained and this could be due to 
the contribution of the best feature selection technique chosen here (BH 
optimization) so that only the best features are considered for classifi-
cation producing a high classification accuracy. 
The merits of the work include the selection of the best components 
or ingredients for the hybrid models to be developed. Each and every 
individual model has some beautiful properties embedded within it and 
when such individual models are hybrid together a good ensemble 
model is obtained which performs with a higher classification accuracy. 
The limitations of this work include the non-applicability of the hybrid 
models to be used in telemedicine and wireless applications for remote 
health care systems, which the authors intend to address in future. 
4. Conclusion and future work 
For the main reasons causing traffic accidents, the primary reason 
can be attributed to the drivers′ fatigue. Estimating and preventing 
driving fatigue is one of the main intentions of automated techniques. In 
the past decades, various techniques have been proposed to detect the 
fatigueness of the driver such as video imaging technology, vehicle 
sensor parameter and physiological signals like EOG, EEG etc. Research 
has shown that EEG is an effective technique to identify driver′s fatigue. 
As EEG is relatively inexpensive and has a high temporal resolution, EEG 
is widely preferred. To analyze and detect the fatigueness of the driver, 
many algorithms based on EEG signals have been proposed. In this work, 
five efficient strategies were proposed and the high classification accu-
racy of 99.13% was obtained when the CSD and Lyapunov exponents 
with multi distance signal level based tangent space mapping and SVM is 
utilized. The second-best classification accuracy of 99.12% is obtained 
with the proposed MFA and conditional feature mapping and cross 
domain transfer learning technique is utilized. The third best classifi-
cation accuracy of 98.18% is obtained when the proposed FAWT and 
TQWT technique is utilized with ANFIS classifier. Future works aim to 
incorporate a plethora of other machine learning techniques, deep 
learning techniques and transfer learning techniques for developing an 
efficient fatigue detection system from EEG signals. 
Credit author statement 
Sunil Kumar Prabhakar:Conceptualization, Methodology, Software, 
Fig. 6. Performance comparison of Classification Accuracy among the proposed methods.  
Table 6 
Performance Comparison table with our work and previous works.  
Authors 
Dataset 
Method Used 
Classification 
Accuracy 
Luo et al. 
[11] 
Collected 
Dataset 
Adaptive multiscale entropy 
95.37% 
Wang et al. 
[14] 
Collected 
Dataset 
Wavelet entropy 
90.70% 
Li et al. 
[15] 
Collected 
Dataset 
Deep Belief Network 
98.86% 
Tuncer 
et al. [10] 
Qui dataset 
1D-DWT + RFINCA + Fine KNN 
100%   
1D-DWT + RFINCA + MEDIUM 
KNN 
99.38%   
1D-DWT + RFINCA +
WEIGHTED KNN 
99.79%   
1D-DWT + RFINCA + BOOSTED 
TREE 
98.12%   
1D-DWT + RFINCA + BAGGED 
TREE 
98.33%   
1D-DWT + RFINCA +
SUBSPACE KNN 
99.48% 
Proposed 
Works 
Qui dataset 
Proposed MDS + SVD with FCM- 
SVR 
97.99%   
Proposed MFA +
Conditional Feature Mapping +
Cross domain Transfer learning 
99.12%   
Proposed FAWT + TQWT with 
ANFIS 
98.18%   
Proposed CSD and Lyapunov 
exponents with Multi distance 
signal level based Tangent Space 
Mapping with SVM 
99.13%   
Proposed HHT + Hilbert 
marginal spectrum with BH and 
cascade Adaboost classifier 
98.08%  
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
13
Data curation, Validation, Writing Original draft preparation, Formal 
Analysis, Investigation, Dong-Ok Won: Visualization, Supervision, 
Writing- Reviewing and Editing, Project Administration, Funding 
Acquisition. 
Funding and acknowledgement 
This work was supported by the National Research Foundation of 
Korea (NRF) grant funded by the Korea government (MSIT) (No. 
2022R1A5A8019303) and partly funded by the Ministry of Education 
(RS-2023-00250246) and partly supported by the Bio&Medical Tech-
nology Development Program of the NRF funded by the Korean gov-
ernment (MSIT) (No. RS-2023-00223501) and partly supported by 
Institute of Information & Communications Technology Planning & 
Evaluation (IITP) grant funded by the Korea government (MSIT) (No. 
2021-0-02068, Artificial Intelligence Innovation Hub). 
Declaration of competing interest 
The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper. 
Data availability 
Publicly available dataset is obtained from “T. Qiu, Data for: 
Research on Fatigue Driving Detection Based on Adaptive Multi-scale 
Entropy, Mendeley Data, 2019.” 
References 
[1] Liu JP, Zhang C, Zheng CX. Estimation of the cortical functional connectivity by 
directed transfer function during mental fatigue. Appl Ergon 2011;42(1):114–21. 
[2] Gharagozlou F, Nasl Saraji G, Mazloumi A, et al. Detecting driver mental fatigue 
based on EEG alpha power changes during simulated driving. Iran J Public Health 
2015;44(12):1693–700. 
[3] Zhao C, Zheng C, ZhaoJ M, Tu LiuY. Automatic classification of driving mental 
fatigue with EEG by wavelet packet energy and KPCA-SVM. International Journal 
of Innovative Computing, Information and Control 2011;7(3):1157–68. 
[4] Yang G, Lin Y, Bhattacharya P. A driver fatigue recognition model based on 
information fusion and dynamic Bayesian network. Inf Sci 2010;180(10):1942–54. 
[5] Zhang G, Yau KKW, Zhang X, Li Y. Traffic accidents involving fatigue driving and 
their extent of casualties. Accid Anal Prev 2016;87:34–42. 
[6] Fletcher A, McCulloch K, Baulk SD, Dawson D. Countermeasures to driver fatigue: a 
review of public awareness campaigns and legal approaches. Aust N Z J Publ 
Health 2005;29(5):471–6. 
[7] Hsieh C-S, Tai C-C. An improved and portable eye-blink duration detection system 
to warn of driver fatigue. Instrum Sci Technol 2013;41(5):429–44. 
[8] Ren Z, Li R, Chen B, Zhang H, Ma Y, Wang C, Lin Y, Zhang Y. EEG-based driving 
fatigue detection using a two-level learning Hierarchy radial Basis function. Front 
Neurorob 2021;15:618408. https://doi.org/10.3389/fnbot.2021.618408. 
[9] Gao D, Tang X, Wan M, Huang G, Zhang Y. EEG driving fatigue detection based on 
log-Mel spectrogram and convolutional recurrent neural networks. Front Neurosci 
2023;17:1136609. https://doi.org/10.3389/fnins.2023.1136609. 
[10] Tuncer T, Dogan S, Subasi A. EEG-based driving fatigue detection using multilevel 
feature extraction and iterative hybrid feature selection. Biomed Signal Process 
Control July 2021;68:102591. 
[11] Luo H, Qiu T, Liu C, Huang P. Research on fatigue driving detection using forehead 
EEG based on adaptive multi-scale entropy Biomed. Signal Process. Control 2019; 
51:50–8. 
[12] Chaudhuri A, Routray A. Driver fatigue detection through chaotic entropy analysis 
of cortical sources obtained from scalp EEG signals. IEEE Trans Intell Transport 
Syst 2019;21. 
[13] Chai R, Ling SH, San PP, Naik GR, Nguyen TN, Tran Y, Craig A, Nguyen HT. 
Improving EEG-based driver fatigue classification using sparse-deep belief 
networks. Front Neurosci 2017;11:103. 
[14] Wang Q, Li Y, Liu X. Analysis of feature fatigue EEG signals based on wavelet 
entropy. Int J Pattern Recogn Artif Intell 2018;32. Article 1854023. 
[15] Li P, Jiang W, Su F. Single-channel EEG-based mental fatigue detection based on 
deep belief network 2016 38th annual international conference of the IEEE 
engineering in medicine and biology society (EMBC). IEEE; 2016. p. 367–70. 
[16] Subasi A, Saikia A, Bagedo K, Singh A, Hazarika A. EEG-based driver fatigue 
detection using FAWT and multiboosting approaches. IEEE Trans Ind Inf Oct. 2022; 
18(10):6602–9. https://doi.org/10.1109/TII.2022.3167470. 
[17] Zhang T, Chen J, He E, Wang H. Sample-entropy-based method for real driving 
fatigue detection with multichannel electroencephalogram. Appl Sci 2021;11: 
10279. https://doi.org/10.3390/app112110279. 
[18] Ma Y, Chen B, Li R, Wang C, Wang J, She Q, Luo Z, Zhang Y. Driving fatigue 
detection from EEG using a modified PCANet method. Comput Intell Neurosci 
2019;2019. https://doi.org/10.1155/2019/4721863. Article ID 4721863, 9 pages. 
[19] Min J, Wang P, Hu J. Driver fatigue detection through multiple entropy fusion 
analysis in an EEG-based system. PLoS One 2017;12(12):e0188756. https://doi. 
org/10.1371/journal.pone.0188756. 
[20] Zhu M, Chen J, Li H, et al. Vehicle driver drowsiness detection method using 
wearable EEG based on convolution neural network. Neural Comput Appl 2021;33: 
13965–80. https://doi.org/10.1007/s00521-021-06038-y. 
[21] Tian Y, Cao J. Fatigue driving detection based on electrooculography: a review. 
J Image Video Proc 2021;2021:33. https://doi.org/10.1186/s13640-021-00575-1. 
[22] Arefnezhad S, Hamet J, Eichberger A, et al. Driver drowsiness estimation using 
EEG signals with a dynamical encoder–decoder modeling framework. Sci Rep 
2022;12:2650. https://doi.org/10.1038/s41598-022-05810-x. 
[23] Zeng H, Yang C, Zhang H, Wu Z, Zhang J, Dai G, Babiloni F, Kong W. A LightGBM- 
Based EEG analysis method for driver mental states classification. Comput Intell 
Neurosci 2019. Article ID 3761203. 
[24] Mu Z, Hu J, Yin J. Driving fatigue detecting based on EEG signals of forehead area. 
Int J Pattern Recogn Artif Intell 2017;31(no. 05):1750011. 
[25] Rodriguez DGG, et al. EEG based drivers mental fatigue detection using ERD/ERS 
and Hurst exponent, ICMHI’22. Proceedings of the 6th International Conference on 
Medical and Health Informatics May 2022:159–62. 
[26] Wang P, Min J, Hu J. Ensemble classifier for driver′s fatigue detection based on a 
single EEG channel. IET Intell Transp Syst Dec 2018;10(issue 10):1322–8. 
[27] Dogan S, Tuncer I, Baygin M, et al. A new hand-modeled learning framework for 
driving fatigue detection using EEG signals. Neural Comput Appl 2023;35: 
14837–54. https://doi.org/10.1007/s00521-023-08491-3. 
[28] Tuncer T, Dogan S, Ertam F, et al. A dynamic center and multi threshold point 
based stable feature extraction network for driver fatigue detection utilizing EEG 
signals. Cogn Neurodyn 2021;15:223–37. https://doi.org/10.1007/s11571-020- 
09601-w. 
[29] Xu N, Gao X, Hong B, Miao X, Gao S, Yang F. BCI competition 2003–data set IIb: 
enhancing P300 wave detection using ICA-based subspace projections for BCI 
applications. IEEE (Inst Electr Electron Eng) Trans Biomed Eng 2004;51(6): 
1067–72. 
[30] Zhang Y, Zhang X, Liu W, Luo Y, Yu E, Zou K, Liu X. Automatic sleep staging using 
multi-dimensional feature extraction and multi-kernel fuzzy Support vector 
machine. Journal of Healthcare Engineering 2014;5. Article ID 410705, 16 pages. 
[31] Wu F, Chou C, Lu Y, Chung J. Modeling electromechanical overcurrent relays using 
singular value decomposition. J Appl Math 2012;2012. https://doi.org/10.1155/ 
2012/104952. Article ID 104952, 18 pages. 
[32] Bezdek JC. Pattern recognition with fuzzy objective function algorithms. New 
York, NY, USA: Plenum Press; 1981. 
[33] Peng X. TSVR: an efficient twin Support vector machine for regression. Neural 
Network 2010;23(3):365–72. 
[34] Yan S, Xu D, Zhang B, Zhang H-J, Yang Q, Lin S. Graph embedding and extensions: 
a general framework for dimensionality reduction. IEEE Trans Pattern Anal Mach 
Intell 2007;29(1):40–51. 
[35] Jiang Y, Zheng Y, Hou S, Chang Y, Gee J. Multimodal image alignment via linear 
mapping between feature modalities. Journal of Healthcare Engineering 2017; 
2017. https://doi.org/10.1155/2017/8625951. Article ID 8625951, 6 pages. 
[36] Li S, Song SJ, Huang G, Wu C. Cross-domain extreme learning machines for domain 
adaptation. IEEE Transactions on Systems, Man, and Cybernetics: Systems 2019;49 
(6):1194–207. 
[37] Sharma M, Pachori RB, Rajendra Acharya U. A new approach to characterize 
epileptic seizures using analytic time-frequency flexible wavelet transform and 
fractal dimension. Pattern Recogn Lett 2017;94:172–9. 
[38] Baygin M, Barua PD, Chakraborty S, et al. CCPNet136; automated detection of 
schizophrenia using carbon chain pattern and iterative TQWT technique with EEG 
signals. Physiol Meas 2023 March 14;(3):44. 
[39] Huang GB, Zhu QH, Siew CK. Extreme learning machine: theory and applications. 
Neurocomputing 2006;70(1–3):489–501. 
[40] Huang GB, Wang DH, Lan Y. Extreme learning machines: a survey. International 
Journal of Machine Learning and Cybernetics 2011;2(2):107–22. 
[41] Wu K. Analysis of the scientific meaning of several concepts related to entropy. 
J Dialectics Nat 1996;5:67–74. 
[42] Yang C, Wu Q. On stabilization of bipedal robots during disturbed standing using 
the concept of Lyapunov exponents. Robotica 2006;24(5):621–4. 
[43] Brkovi´c M, Simi´c M. Multidimensional optimization of signal space distance 
parameters in WLAN positioning. Sci World J 2014;2014. https://doi.org/ 
10.1155/2014/986061. Article ID 986061, 6 pages. 
[44] O′Neill B. Semi-riemannian geometry. New York, NY, USA: Academic Press; 1983. 
[45] Zhang Z, Zha H. Principal manifolds and nonlinear dimensionality reduction via 
tangent space alignment. SIAM J Sci Comput 2005;26(1):313–38. 
[46] Gu P, Wen YK. A record-based method for the generation of tridirectional uniform 
hazard-response spectra and ground motions using the Hilbert-Huang transform. 
Bull Seismol Soc Am 2007;97(5):1539–56. 
[47] Han D, Wei S, Shi P, Zhang Y, Gao K, Tian N. Damage identification of a derrick 
steel structure based on the HHT marginal spectrum amplitude curvature 
difference. Shock Vib 2017;2017. https://doi.org/10.1155/2017/1062949. Article 
ID 1062949, 9 pages. 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
Array 19 (2023) 100320
14
[48] Farahmandian M, Hatamlou A. Solving optimization problems using black hole 
algorithm. J Adv Comput Sci Technol 2015;4:68–74. 
[49] Wang Z, Yoon S, Xie S, Lu Y, Park DS. A high accuracy pedestrian detection system 
combining a cascade AdaBoost detector and random vector functional-link net. Sci 
World J 2014;2014. https://doi.org/10.1155/2014/105089. Article ID 105089, 7 
pages. 
[50] Qiu T. Data for: research on fatigue driving detection based on adaptive multi-scale 
entropy. Mendeley Data; 2019. 
S.K. Prabhakar and D.-O. Won                                                                                                                                                                                                              
