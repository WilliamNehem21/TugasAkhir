Retrieval of ﬂower videos based on a query with multiple species
of ﬂowers
V.K. Jyothi a,⁎, V.N. Manjunath Aradhya b, Y.H. Sharath Kumar c, D.S. Guru a
a Department of Studies in Computer Science, Manasagangothri, University of Mysore, Mysore 570 006, Karnataka, India
b Department of Computer Applications, JSS Science and technology University, Mysore, Karnataka, India
c Department of Information Science and Engineering, Maharaja Institute of Technology Mysore (MITM), Manday 571438, Karnataka, India
a b s t r a c t
a r t i c l e
i n f o
Article history:
Received 25 August 2020
Received in revised form 6 November 2021
Accepted 6 November 2021
Available online 14 November 2021
Searching, recognizing and retrieving a video of interest from a large collection of a video data is an instantaneous
requirement. This requirement has been recognized as an active area of research in computer vision, machine
learning and pattern recognition. Flower video recognition and retrieval is vital in the ﬁeld of ﬂoriculture and hor-
ticulture. In this paper we propose a model for the retrieval of videos of ﬂowers. Initially, videos are represented
with keyframes and ﬂowers in keyframes are segmented from their background. Then, the model is analysed by
features extracted from ﬂower regions of the keyframe. A Linear Discriminant Analysis (LDA) is adapted for the
extraction of discriminating features. Multiclass Support Vector Machine (MSVM) classiﬁer is applied to identify
the class of the query video. Experiments have been conducted on relatively large dataset of our own, consisting
of 7788 videos of 30 different species of ﬂowers captured from three different devices. Generally, retrieval of
ﬂower videos is addressed by the use of a query video consisting of a ﬂower of a single species. In this work
we made an attempt to develop a system consisting of retrieval of similar videos for a query video consisting
of ﬂowers of different species.
© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Keywords:
Flower region of interest (FRoI)
Linear discriminant analysis (LDA)
Retrieval of ﬂower videos
Multiclass support vector machine
Contents
1.
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
2.
Related works . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
2.1.
Previous work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
2.2.
Contributions of the proposed work. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
263
3.
Proposed work
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
3.1.
Preprocessing . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
3.1.1.
Gaussian Mixture model (GMM) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
264
3.1.2.
Extraction of ﬂower region of interest (FRoI). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
3.2.
Extraction of features . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
3.2.1.
Texture features
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
3.2.2.
Scale invariant feature transform (SIFT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
265
3.2.3.
Entire keyframe. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
3.2.4.
All ﬂower regions of interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
3.2.5.
Maximum ﬂower region of interest . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
267
3.2.6.
Linear discriminant analysis (LDA) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
271
3.3.
Retrieval: query claiming identity of class . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
4.
Experiments and results . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
4.1.
Datasets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
273
4.2.
All FRoI's . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
4.3.
Max. FRoI. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
274
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
⁎ Corresponding author.
E-mail addresses: jyothivk.mca@gmail.com (V.K. Jyothi), aradhya@sjce.ac.in (V.N.M. Aradhya), dsg@compsci.uni-mysore.ac.in (D.S. Guru).
https://doi.org/10.1016/j.aiia.2021.11.001
2589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
4.4.
Max. FRoI with LDA . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.5.
Comparative study between proposed work and previous work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.6.
Result analysis and discussion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
275
4.7.
Query with multiple class ﬂowers in videos . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
5.
Comparative study between proposed work and deep learning model. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
6.
Future work . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
276
7.
Conclusion
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
277
1. Introduction
Due to the ease of availability of recent video capturing devices such
as cameras, mobiles, storage media, users can easily capture and store a
large number of videos. Video contains a large information than images.
A single video can capture the reality better than thousands of images.
Recently, video databases have become much larger, hence there is
need for automatic analysis and retrieval system with the minimum in-
tervention of human is essentially required. Video has become a signif-
icant element of communication environment. Users can search and
share desired videos due to networking technology, which has made
developing an automated system to search and retrieve videos. And it
is an interesting and active research (Shen et al., 2016). Videos are cat-
egorized into different domains for example sports, news, surveillance,
commercials, medical etc., again domain speciﬁc videos are categorized
into different subcategories/classes (Geetha et al., 2009).
Data acquisition tools with recent technological advancements
allowed researchers/scientists to acquire data from different application
domains in the form of images and videos, these are a large and complex
in nature (Mufti et al., 2021). One of the important aspects of organic life
is its outstanding diversity. There exists a very large number of species
of ﬂowers in the world and the estimation of ﬂower species ranges be-
tween 2,20,000 and 4,20,000 (Chaitra et al., 2021). Specialized knowl-
edge is required to recognize the taxonomic information of ﬂowers.
Plant Identiﬁcation skills and Taxonomic knowledge is restricted to a
limited number of individuals (Jyothi et al., 2018, Wäldchen et al.,
2018). To address the taxonomist's ﬂower species identiﬁcation re-
quirement, a signiﬁcant amount of research work has been carried out
in the ﬁeld of Artiﬁcial Intelligence and Video/Image Processing for au-
tomatic ﬂower recognition and retrieval.
Developing a ﬂower video retrieval system is a domain speciﬁc with
many applications. It is an application in the ﬁeld of ﬂoriculture for com-
mercial trades. Floriculture is one of the important commercial trades in
agriculture (Guru et al., 2010). Day to day there is an increase in the de-
mand for ﬂowers. Floriculture involves nursery, ﬂower trade, seed pro-
duction from ﬂowers (Guru et al., 2011). Further, it is found useful in
horticulture, interest in knowing the ﬂower names for decoration, cos-
metics and medicinal use etc., (Das et al., 1999a, 1999b). Due to the de-
velopment of technology in business, trader can store a large volume of
videos. Instead of visiting the nurseries for their desired ﬂowers, users
can analyse the entire ﬂower before purchasing it and its seeds. Also,
they can view different species of ﬂowers along with different variants
available in each species. Further it ﬁnds applications such as medicinal,
cosmetics, industrial use for the extraction of oils from ﬂowers and dec-
oration etc., (Das et al., 1999a, 1999b). In such cases, it is essential to de-
velop an automated system to search and retrieve videos of ﬂowers of
user's interest. Therefore, the proposed research motivates to design
an automated system for the retrieval of users desired videos of ﬂowers.
The challenges involved in ﬂower videos to design a retrieval system are
illumination: light variations differ from different angles and varied sea-
sonal time; variation in viewpoint: videos with varying viewpoint of
ﬂowers changes appearance of the ﬂower in size, shape, pose and rota-
tion; cluttered background, variation among intra class and inter class,
multiple instances of ﬂowers in videos etc. To design a video retrieval
system, two main prominent methods are used to increase retrieval
performance. First is to ﬁnd more appropriate features to describe
videos and second is an appropriate dimensionality reduction method
for selecting most discriminative features.
2. Related works
Generally, the video retrieval system retrieves similar videos based
on query by example. An example may be an image, keywords, sketch,
object, video, video frame etc., (Hu et al., 2011). In the literature we
found retrieval of videos based on an object (Morand et al., 2010),
frame (Shekar et al., 2016), video (Geetha et al., 2009; Gao et al.,
2009; Han et al., 2014; Liang et al., 2012), keywords (Priya and
Domnic, 2014). For the retrieval of videos the features and algorithms
such as optical ﬂow tensor and Hidden Markov Models (HMMs) (Gao
et al., 2009), the multi-modal spectral clustering and ranking algorithm
(Han et al., 2014), block wise intensity comparison (Geetha et al., 2009),
Scale Invariant Feature Transform (SIFT) (Zhu et al., 2016), Bag-of-
Features (Cui et al., 2016), dynamic weighted similarity measure with
color and edge descriptors (Liang et al., 2012) are used. When a set of
features are used to represent of a video, then the dimension of features
may be high. If the dimension of the feature vector is high, the video re-
trieval system consumes more computational time. It can be reduced
with the feature dimensionality reduction techniques. The dimensional-
ity reduction techniques such as Principal Component Analysis (PCA)
(Geetha et al., 2009), Fisher Discriminant Ratio (Shen et al., 2016), Lin-
ear Discriminant Analysis (Gao et al., 2009), semi-supervised linear dis-
criminant analysis (Wang et al., 2016), supervised linear dimensionality
reduction (Cui et al., 2016), nonparametric discriminant analysis (Khan
et al., 2012) are utilized to reduce the feature dimension in other video
retrieval systems.
2.1. Previous work
In proposed work, to design a ﬂower video retrieval system the fea-
tures of previous work (Guru et al., 2018a, 2018b) such as GLCM
(Haralick et al., 1973), LBP (Ojala et al., 2002) and SIFT Lowe (2004)
are utilized. Instead of extracting features from entire keyframe, fea-
tures are extracted in two different modes from each keyframe of the
video. Initially, from all Flower Region of Interest (FRoI), secondly,
from maximum Flower Region of Interest (Max.FRoI). A dimensionality
reduction method is introduced for the features extracted from Max.
FRoI, to improve the performance of the system with greater extent,
which leads the fast accessing of videos. In the previous work (Guru
et al., 2018a, 2018b) the query video consists of a single class of ﬂowers.
In the present work along with single class of ﬂower videos query video
also consists of multiclass ﬂowers. The dataset considered in the present
work is relatively large. The comparative study is made with previous
work to show the effectiveness of the proposed work.
2.2. Contributions of the proposed work
The contributions are summarized as follows.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
263
a) Creation of a reasonably large dataset of videos of ﬂowers which
shall be made available public for research purpose.
b) Proposal of fusion of features strategy to improve the performance of
the existing model.
c) Proposal of an algorithmic model for the retrieval of videos of
ﬂowers using all ﬂower regions of interest.
d) Proposal of a model for the retrieval of videos of ﬂowers with maxi-
mum ﬂower regions of interest
e) Adoption of a dimensionality reduction approach to improve the ef-
ﬁciency of the system.
f) Addressed retrieval of videos of ﬂowers even when a query video
contains ﬂowers of more than one class.
g) Compared the proposed model with earlier proposed model and a
deep learning model.
3. Proposed work
The proposed model comprises three stages namely, preprocessing,
extraction of features and retrieval. The block diagram of the proposed
ﬂower video retrieval system using Flower Region of Interest (FRoI) is
as shown in Fig. 1. (See Table 1.)
3.1. Preprocessing
The preprocessing stage involves the processes such as selection of
keyframes, segmentation and extraction of ﬂower region of Interest
(FRoI). The proposed system initially converts video to frames. Suppose
that the ﬂower video dataset ‘X' consists of ‘vn’ number of samples and it
is stated as
X ¼ xv1, xv2, xv3, .. . , xvi, . .. , xvn
f
g
ð1Þ
Let the ﬂower video xvi consists of a ﬁnite set of ‘FN’ number of frames
and it is deﬁned as
xvi ¼ F1, F2, F3, . .. , Fi, . .. , FN
f
g
ð2Þ
Then the keyframes of the video xvi are selected using GMM cluster
based algorithmic model (Guru et al., 2018a, 2018b). Here, Block wise
entropy feature is extracted from each frame of the video and similar
frames are grouped together using Gaussian Mixture Model and the
frame near to each cluster centroid are selected as keyframes of the
video. GMM is explained in section 3.1.1. When the set of keyframes
are selected from xvi, then the video xvi is represented as ‘Ky’ number
of keyframes and is deﬁned as,
Ky ¼
k1, k2, k3, .. . , ki, .. . , ky
�
�
ð3Þ
The ﬂowers in keyframes are segmented from their background
using statistical region merging algorithm (Nock and Nielsen, 2004).
The keyframes after segmentation can be deﬁned as
SKy ¼
sk1, sk2, sk3, . .. , ski, . . . , sky
�
�
ð4Þ
3.1.1. Gaussian Mixture model (GMM)
Gaussian Mixture model (GMM) is a statistical and unsupervised
learning model. GMM (Stauffer and Grimson, 1999), preserves content
of the scene, the idea behind GMM is to describe pixels, some of
which represent the background while the others represent the fore-
ground in the scene. A ﬁnite number of mixtures of Gaussian distribu-
tions are used to generate data points. It preserves the sub-sampling
property; it leads for clustering data points. The GMM parameters are
estimated from data using the maximum expectation algorithm. A
GMM is a weighted sum of several Gaussian densities. Therefore, in
the present work to create clusters GMM is used for the selection of
keyframes. Clusters are created by ﬁtting the Gaussian distribution on
data (x) with ‘n’ features, the Gaussian function is deﬁned as (Chen
et al., 2015).
f x
ð Þ ¼
1
σ
ﬃﬃﬃﬃﬃﬃ
2π
p
e− x−μ
ð
Þ2
2σ2
ð5Þ
Fig. 1. Block diagram of the proposed class based ﬂower video retrieval system.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
264
Where μ is the mean and σ is the standard deviation of data (fea-
tures) ‘x’.
3.1.2. Extraction of ﬂower region of interest (FRoI)
After the process of segmentation of keyframes, all ﬂower regions
are selected using connected component analysis and the selected
ﬂower regions are named as Flower Regions of Interest (FRoI's) (refer
Fig. 1). Then from FRoI's of each keyframe, features such as GLCM, LBP
and SIFT are extracted for further processing.
3.2. Extraction of features
Video visual features such as color, texture, local invariant features,
etc., play an important role in the retrieval of videos (Hong et al.,
2014; Li et al., 2015). Some of the different species of ﬂowers are similar
in color. For example, we can ﬁnd red colored rose, hibiscus, bougainvil-
lea belongs to three different species. Therefore, color feature many not
discriminate ﬂowers from one species to another. There exists a large
intra class variability and inter class similarity in the dataset. Due to
this there are two prime motivations in the selection of features to de-
scribe ﬂowers in videos. Primarily, the texture of an individual species
of ﬂowers are similar, therefore textural features are used to describe
the ﬂowers in videos. Secondly, the ﬂowers in videos consists of varia-
tion in view point and illumination, in such cases features extracted
from Scale Invariant Feature Transform Lowe (2004) are considered.
3.2.1. Texture features
Texture of an image/frame contain unique visual patterns. Texture
features describes the object surface, these features are independent
of object color Hu et al. (2011). The videos of ﬂowers consist of large
intra class variation such as variation in color of ﬂowers. Therefore, to
describe the ﬂower region, texture features play a vital role. In this
work, texture features namely, Gray Level Co-occurrence Matrix and
Local Binary Pattern are used.
3.2.1.1. Gray level co-occurrence matrix (GLCM). GLCM describes the tex-
ture of ﬂower in terms of statistical information. In the current work, the
system extracts 14 different gray level co-occurrence of statistical values
(Haralick et al., 1973) are extracted from each FRoI. These features are
represented as a feature vector.
3.2.1.2. Local binary pattern (LBP). LBP describes the texture description
in terms of local features of the ﬂower region. An approach to recog-
nize local binary patterns of image texture, and their occurrence histo-
gram proved that LBP is a powerful texture feature (Ojala et al., 2002).
It is robust in terms of variation and transformation of the gray scale.
In the proposed work, the system extracts LBP features (Ojala et al.,
2002) which are invariant to local grayscale variations in the FRoI.
LBP texture features are extracted using 3 × 3 neighbourhood by the
value of centre pixel, the pixels of eight neighbors are thresholded.
In 3 × 3 neighbourhood, the centre pixel LBP value is obtained by
thresholded binary values are weighted by powers of two and
summed up.
3.2.2. Scale invariant feature transform (SIFT)
SIFT plays a vital role in video retrieval for the analysis of the video
content (Zhu et al., 2016). In SIFT the set of image features are generated
in 4 stages Lowe (2004). In the ﬁrst stage, the model searched over all
scales and image locations to identify interest points that are invariant
to orientation and scale. In the second stage, at each location, model is
determined scale and location which is named as keypoint localization.
In the third stage, based on local image gradient directions, orientations
are assigned to each keypoint location. Finally, at the selected scale in
the region around each keypoint, it generates descriptors, with a kernel
of 4 × 4 histogram of 8 bins. These histograms compute the direction
and magnitude of the gradient in the region of 16 × 16 pixels. The histo-
grams results are represented in the form of descriptors. In the current
work these feature descriptors are used to describe the FRoI's Lowe
(2004).
To design the proposed model, the features such as Gray Level
Co-occurrence Matrix (GLCM) (Haralick et al., 1973), Local Binary
Pattern (LBP) (Ojala et al., 2002) and Scale Invariant Feature Trans-
form (SIFT) features proposed by Lowe (2004) are extracted. Initially
we propose to accomplish extracting these features by considering
an entire keyframe after segmentation (Guru et al., 2018a, 2018b).
Subsequently, we employ the extraction of features on all ﬂower re-
gions of each keyframe of the video. And ﬁnally, we accomplish
extraction of these features by selecting the Maximum Flower Re-
gion among all ﬂower regions of the keyframe for the purpose of
retrieval.
Table 1
Summary of mentioned technologies and applications in related works.
Sl.
No.
Algorithms
Applications
References
1
optical ﬂow and Hidden Markov Models
retrieval of videos
Gao et al., 2009
2
multi-modal spectral clustering and ranking algorithm
retrieval of videos
Han et al., 2014
3
Principal Component Analysis
Feature dimensionality reduction
Geetha et al., 2009
4.
Fisher Discriminant Ratio, Linear Discriminant Analysis, semi-supervised linear discriminant
analysis, supervised linear dimensionality reduction, nonparametric discriminant analysis
Feature dimensionality reduction
Shen et al., 2016
Gao et al., 2009
Wang et al., 2016
Cui et al., 2016
Khan et al., 2012
Fig. 2. Extraction of features from all ﬂower regions of interest.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
265
Fig. 3. Extraction of Maximum Flower Region of Interest (Max. FRoI).
Fig. 4. Samples of ﬂower videos with large intraclass variation from 30 classes of videos.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
266
3.2.3. Entire keyframe
In this method (Guru et al., 2018a, 2018b), the model extracts the
features such as Gray Level Co-occurrence Matrix (GLCM) (Haralick
et al., 1973), Local Binary Pattern (LBP) (Ojala et al., 2002) and Scale In-
variant Feature Transform (SIFT) Lowe (2004) from an entire keyframe
after segmentation and generates feature vector. Then, in the proposed
model these features are fused like GLCM+LBP, GLCM+SIFT, LBP + SIFT,
GLCM+LBP + SIFT to improve the performance of the system. The
video xvi is represented as a set of features and is deﬁned as,
xvi ¼
f f 1, f 2, f 3, . .. , f i, .. . , f N
g
ð6Þ
Then, xvi = FiMi, where FiMi = {f1, f2, f3, …, fi, …, fN}, similarly, features
for all videos of a data base ‘X' of Eq. (1) is deﬁned as,
RD ¼
F1M1 xv1
ð
Þ; F2M2 xv2
ð
Þ; F3M3 xv3
ð
Þ; …; FiMi xvi
ð
Þ; …; FnMn xvn
ð
Þ
f
g
ð7Þ
Where F1M1(xv1), F2M2(xv2), F3M3(xv3), …, FiMi(xvi), …, FnMn(xvn) are
the feature matrices of the videos xv1, xv2, xv3, …, xvi, …, xvn respectively
in Eq. (1).
3.2.4. All ﬂower regions of interest
The proposed system extracts features such as GLCM (Haralick et al.,
1973), LBP (Ojala et al., 2002) and SIFT Lowe (2004) from all ﬂower re-
gions of keyframes and is as shown in Fig. 2. In the proposed model
these features are fused like GLCM+LBP, GLCM+SIFT, LBP + SIFT,
GLCM+LBP + SIFT to improve the performance of the system. Let Rr
be the number of selected ﬂower regions of a keyframe ski in Eq. (4).
Then, ski with number of ﬂower regions is deﬁned as.
ski ¼ R1SKi, R2SKi, R3SKi, .. . , RrSKi
f
g
ð8Þ
Then, the feature vector of all regions is represented as
R1SKi ¼
½ f 11, f 12, f 13, . .. , f 1M
�, R2SKi
¼
½ f 21, f 22, f 23, . .. , f 2M
�, . . . , R1SKi ¼
½ f r1, f r2, f r3, .. . , f rM
�
Finally, the feature vector of all the regions of a keyframe ski as
shown in Eq. (8) is represented as,
F1M1
d ¼
½f 11; f 12; f 13; …; f 1M
�; f 21; f 22; f 23; …; f 2M
½
�; …; f r1; f r2; f r3; …; f rM
½
�
f
g
Then, all regions of a keyframe ski is deﬁned as,
F1Md
1 ¼ ∀RiSKi ∈ ski
ð9Þ
Where F1M1
d is the feature matrix of the video xvi of the Eq. (1)
consists of ∀RiSKi all regions of a keyframe ski in Eq. (8).
Then, the feature vector of all FRoI's of all keyframes of a video xvi can
be deﬁned as.
FMd xvi
ð
Þ ¼ ∀FjMd
j ∈ SKy
ð10Þ
Where FMd(xvi) is the feature matrix of the video xvi of Eq. (1)
consists of all feature matrices of all ‘y’ keyframes of a video as shown
in Eq. (4).
The feature dimension of a video xvi i.e., FMd(xvi) consists of the
features extracted from all regions of each keyframe of the video xvi.
Similarly, the feature vectors obtained for all videos of a database ‘X'
can be deﬁned as,
RD ¼ FMd xv1
ð
Þ; FMd xv2
ð
Þ; FMd xv3
ð
Þ; …; FMd xvi
ð
Þ; …; FMd xvn
ð
Þ
ð11Þ
3.2.5. Maximum ﬂower region of interest
In this method, features such as GLCM (Haralick et al., 1973), LBP
(Ojala et al., 2002) and SIFT Lowe (2004) are extracted from the Maxi-
mum Flower Region of Interest (Max. FRoI) among all ﬂower regions
of each keyframe of a video, then features are fused like GLCM+LBP,
GLCM+SIFT, LBP + SIFT, GLCM+LBP + SIFT to improve the perfor-
mance of the system.. Fig. 3 shows the selected ﬂower region. Max.
FRoI is obtained by selecting the maximum ﬂower region i.e., the ﬂower
region having high density of pixels and is the largest area among all the
regions in each keyframe. When there is only one ﬂower region in the
keyframe then that will be considered as Max. FRoI as shown in Fig. 3.
It reduces the dimension of the features of the proposed retrieval sys-
tem as compared with all FRoI's. Through Max. FRoI model, the efﬁ-
ciency can be improved. The features are extracted, after selecting
Max. FRoI from each keyframe of Eq. (4). Therefore, the feature vector
deﬁned in Eq. (8) can be redeﬁned in this case as,
MFiMd
i ¼ Max RiSKi
ð
Þ ∈ ski
ð12Þ
Where MFiMi
d is the feature matrix of the video xvi of the Eq. (1)
consists of Max(RiSKi) maximum ﬂower region of a keyframe ski in
Eq. (8).
Then the feature matrix of Max. FRoI of all keyframes of a video xvi
can be deﬁned as
FMd xvi
ð
Þ ¼ ∀ MFjMd
j
�
�
∈ SKy
ð13Þ
Sl. 
No.
No. of 
Classes in a 
video
Multi class
Flower Video
Flower
Region of
Interest
Correctly 
Identified?
1
2
Yes
Yes
2
2
Yes
No
3
2
Yes
Yes
4
2
Yes
No
5
2
Yes
Yes
6
2
Yes
Yes
7
2
Yes
Yes
8
3
Yes
Yes
No
Fig. 5. Query acquiring the identity of the class for multiclass ﬂower video.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
267
Fig. 6. Features extracted from all FRoI's for SGGP dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
Fig. 7. Features extracted from all FRoI's for Sonycyber Shot dataset: (a) 30% Train - 70% Test, (b) 50% Train - 50% Test, (c) 70% Train - 30% Test.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
268
Fig. 8. Features extracted from all FRoI's for Canon dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
Fig. 9. Features extracted from Max FRoI for SGGP dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
269
Fig. 10. Features extracted from Max FRoI for Sonycyber Shot dataset: (a) 30% Train–70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
Fig. 11. Features extracted from Max FRoI for Canon dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
270
Where FMd(xvi) is the feature matrix of the video xvi of Eq. (1)
consists of maximum ﬂower region feature matrices of all ‘y’
keyframes of a video as shown in Eq. (4).
The feature dimension of a video xvi i.e., FMd(xvi) in Eq. (13) consists
of the features extracted from maximum ﬂower region of each
keyframe of the video xvi. Similarly feature vectors for all videos of data-
base ‘X' are obtained and are deﬁned as,
RD ¼ FMd xv1
ð
Þ; FMd xv2
ð
Þ; FMd xv3
ð
Þ; …; FMd xvi
ð
Þ; …; FMd xvn
ð
Þ
ð14Þ
Further, even though the Max.FRoI reduces the dimension of the fea-
tures of the proposed retrieval system as compared with all FRoI's, to
improve the efﬁciency of the retrieval system, the most discriminant
features from Max. FRoI are obtained using LDA and is discussed in sec-
tion 3.2.4. The feature dimension of a video xvi as shown in Eq. (13) is
represented as the reduced discriminant features obtained from Max.
FRoI using LDA and it can be deﬁned as
FMd xv1
ð
Þ ¼ ∀ MFjMd
j
�
�
∈ SKy
ð15Þ
Where j = 1 to ‘y’ keyframes of a video xvi as shown in Eq. (4).
Finally, the reduced feature vectors for all videos of the database ‘X',
are deﬁned as,
RD ¼
�
DR FMd xv1
ð
Þ
�
�
DR FMd xv2
ð
Þ
�
�
DR FMd xv3
ð
Þ
�
�
…DR FMd xvi
ð
Þ
�
�
…DR FMd xvn
ð
Þ
�
��
ð16Þ
3.2.6. Linear discriminant analysis (LDA)
LDA is a supervised dimensionality reduction method (Belhumeur
et al., 1999). Ronald Fisher in 1936 proposed discriminant analysis, to
ﬁnd a new feature space from original feature space. LDA plays a vital
role in order to maximize class separability and preserves the within
Table 2
(a). SGGP Dataset: Train 30% - Test 70%. (b). SGGP Dataset: Train 50% - Test 50%. (c). SGGP
Dataset: Train 70% - Test 30%.
(a)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.2
0.18
0.15
0.17
LBP (Ojala et al., 2002)
0.13
0.11
0.07
0.09
SIFT Lowe (2004)
0.99
0.99
1
0.99
GLCM+LBP
0.16
0.15
0.1
0.12
GLCM+SIFT
0.99
0.99
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.99
0.99
0.99
0.99
(b)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.22
0.21
0.18
0.19
LBP (Ojala et al., 2002)
0.13
0.12
0.08
0.09
SIFT Lowe (2004)
0.99
0.99
0.99
0.99
GLCM+LBP
0.19
0.18
0.12
0.14
GLCM+SIFT
0.99
0.99
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.98
0.98
0.98
0.98
(c)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.33
0.31
0.3
0.29
LBP (Ojala et al., 2002)
0.15
0.16
0.1
0.11
SIFT Lowe (2004)
0.99
0.99
1
0.99
GLCM+LBP
0.2
0.2
0.1
0.16
GLCM+SIFT
0.99
0.99
1
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.99
0.99
1
0.99
Table 3
(a). Sonycyber Shot Dataset: Train 30% -Test 70%. (b). Sonycyber Shot Dataset: Train 50%
-Test 50%. (c). Sonycyber Shot Dataset: Train 70% -Test 30%.
(a)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.21
0.2
0.17
0.18
LBP (Ojala et al., 2002)
0.38
0.41
0.37
0.37
SIFT Lowe (2004)
0.99
0.99
0.99
0.99
GLCM+LBP
0.45
0.44
0.43
0.42
GLCM+SIFT
0.99
0.99
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.97
0.99
0.96
0.97
(b)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.24
0.24
0.2
0.22
LBP (Ojala et al., 2002)
0.37
0.38
0.4
0.35
SIFT Lowe (2004)
0.99
0.99
1
1
GLCM+LBP
0.49
0.51
0.5
0.47
GLCM+SIFT
0.99
0.99
1
1
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.99
0.99
1
0.99
(c)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.28
0.44
0.2
0.3
LBP (Ojala et al., 2002)
0.39
0.37
0.5
0.43
SIFT Lowe (2004)
0.99
0.99
1
1
GLCM+LBP
0.54
0.54
0.5
0.53
GLCM+SIFT
0.99
0.99
1
1
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
0.99
0.99
1
1
Table 4
(a). Canon Shot Dataset: Train 30% -Test 70%. (b). Canon Shot Dataset: Train 50% -Test 50%.
(c). Canon Shot Dataset: Train 70% -Test 30%.
(a)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.54
0.53
0.51
0.5
LBP (Ojala et al., 2002)
0.63
0.68
0.64
0.64
SIFT Lowe (2004)
0.99
0.99
0.99
0.99
GLCM+LBP
0.81
0.83
0.78
0.8
GLCM+SIFT
0.99
0.99
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
1
1
1
1
(b)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.56
0.55
0.52
0.53
LBP (Ojala et al., 2002)
0.66
0.7
0.67
0.67
SIFT Lowe (2004)
0.99
0.99
0.99
0.99
GLCM+LBP
0.82
0.86
0.8
0.82
GLCM+SIFT
0.99
1
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
1
1
1
1
(c)
Features
Accuracy
Precision
Recall
F-Measure
GLCM (Haralick et al., 1973)
0.63
0.64
0.61
0.6
LBP (Ojala et al., 2002)
0.69
0.73
0.71
0.7
SIFT Lowe (2004)
0.99
0.99
0.99
0.99
GLCM+LBP
0.85
0.87
0.86
0.86
GLCM+SIFT
0.99
1
0.99
0.99
LBP + SIFT
1
1
1
1
GLCM+LBP + SIFT
1
1
1
1
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
271
Fig. 12. Features extracted from entire keyframe for SGGP dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
Fig. 13. Features extracted from entire keyframe for Sonycyber Shot dataset (a)30%Train-70%Test (b)50%Train-50% Test (c)70%Train-30% Test.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
272
class similarity. It maximizes the distance between the projected data of
inter classes and minimizes the distance between the predictable data
of the intra class (Gyamﬁ et al., 2018; Wang et al., 2016) and hence in
the current work we have applied LDA for the reduction of feature di-
mension.
The reduced dimension of the feature vector is deﬁned as follows,
DR FMVi
ð
Þ ¼
f f 1, f 2, f 3, . . . , f dr
g
ð17Þ
For the retrieval of videos, the proposed model utilizes reduced fea-
tures obtained after dimensionality reduction. The reduced feature vec-
tor of FMVi consists of 30 features.
3.3. Retrieval: query claiming identity of class
Initially, for a given query video ‘QV’, the system acquires the identity
of the class using Multiclass Support Vector Machine (MSVM). Then the
similar videos are retrieved from the predicted class. For the retrieval of
a query video, the model is trained with two different set of features
explained in section 3.2.2 and section 3.2.3 and the experimental
results are shown in section 4.
Support vector machine (SVM) is a computationally powerful tool
for supervised learning (Kumar and Gopal, 2011, and Khan et al.,
2012). Support vector machine is a vector-space-based classiﬁcation
method for both linear and non-linear data. The fundamental idea of
SVM classiﬁer is to ﬁnd the optimal separating hyperplane between
two classes. For more information please refer (Vapnik (1998) and
Duda et al., 1997).
4. Experiments and results
4.1. Datasets
Dataset is a fundamental requirement to test the efﬁciency of any
automatic system designed. To conduct experiments, relatively large
dataset is required. Since the standard ﬂower video dataset is not pub-
licly available, we created ﬂower video datasets. To create ﬂower
video datasets, we used three devices namely, Samsung Galaxy Grand
Prime (SGGP) mobile, Sonycyber Shot camera and Canon camera.
Fig. 14. Features extracted from entire keyframe for Canon dataset: (a) 30% Train – 70% Test, (b) 50% Train – 50% Test, (c) 70% Train – 30% Test.
Table 5
Accuracy obtained for feature combinations with different modes of extraction of features with 70% training and 30% testing.
Sl. No.
Modes of extraction of features
Feature combination
Datasets (results in %)
SGGP
Sonycyber Shot
Canon
1
Entire keyframe
GLCM+LBP + SIFT
53.83
60.18
65.73
2
All FRoI's
GLCM+LBP + SIFT
53.83
63.56
52.36
3
Max. FRoI
GLCM+LBP + SIFT
60.59
67.07
75.79
4
Max. FRoI with LDA
LBP + SIFT
100
100
100
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
273
SGGP dataset consists of 2611 videos of 8 M pixels. Sonycyber Shot cam-
era dataset consists of 2521videos of 14.1 M pixels. And Canon camera
cosists of 2656 videos of 16 M pixels. Videos captured with the duration
ranges from 4 to 60 s. We have captured 30 different species of ﬂowers
from all the three devices. There exists a small inter class and large intra
class variations. Videos captured in the real environment during sum-
mer, rainy and winter seasons. Videos involved the challenges such as
viewpoint variations, illumination, cluttered background, and multiple
instances of the ﬂowers. Flower video samples with large intra-class
variations from the dataset we created are shown in Fig. 4.
Along with the above mentioned three datasets, we created a
dataset with multiple classes of ﬂowers in a video for querying. The
dataset contains two and three different classes of ﬂowers. The samples
of these ﬂower videos are shown in Fig. 5.
The performance of the proposed model is analysed in different
modes of extraction of features. Results of the features extracted from
all FRoI's as shown in the section 4.2, the features extracted from Max-
imum FRoI (MFRoI) is as shown section 4.3 and the features extracted
from Maximum FRoI (MFRoI) with LDA is as shown in section 4.4. And
also, the results obtained in previous work of extracting features from
entire keyframe (Guru et al., 2018a, 2018b) are shown in section 4.5.
The dataset we created is used to conduct experiments. In order to eval-
uate the system, metrices such as accuracy, precision, recall and F-
measure are used and are given below. The results are tabulated with
varying training and testing videos.
Accuracy ¼ Sum of videos retrieved correctly
Total number of query videos
ð18Þ
Precision ¼ Total number of videos retrieved are relevant
Total number of videos retrieved
ð19Þ
Recall ¼
Total number of videos retrieved are relevant
Total number of similar videos in the database
ð20Þ
F−Measure ¼ 2∗Precision∗Recall
ðPrecision þ Recall
Þ
ð21Þ
4.2. All FRoI's
The result analysis of proposed retrieval system trained with the fea-
tures extracted from all FRoI's are shown in the following ﬁgures Fig. 6,
Fig. 7 and Fig. 8 for SGGP, Sonycyber Shot and Canon datasets respec-
tively. From the results we can observe that the accuracy of the system
in this approach achieved 53.83% for SGGP dataset, 52.36% for Sonycyber
Shot and 63.56% for Canon dataset for 70% training and 30% testing.
4.3. Max. FRoI
The result analysis of proposed retrieval system trained with the fea-
tures extracted from maximum ﬂower region of interest are shown in
the following ﬁgures: Fig. 9, Fig. 10 and Fig. 11 for SGGP, Sonycyber
Shot and Canon datasets respectively. From the results we can observe
that the accuracy of the system in this approach is achieved 60.59% for
SGGP dataset, 67.07% for Sonycyber Shot dataset and 75.79% for Canon
dataset for 70% training and 30% testing. Further, from the results we
Fig. 15. Comparative study between proposed work and deep learning model (Jyothi et al., 2018) for SGGP dataset.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
274
can observe that the Max. FRoI give improved results than all FRoI's for
all the three datasets.
4.4. Max. FRoI with LDA
In this section we obtain discriminant features from Max. FRoI using
LDA are passing to the model. It improves the retrieval performance by
identifying the class of the query video. Table 2(a) to Table 2(c), Table 3
(a) to Table 3(c) and Table 4(a) to Table 4(c) show the result analysis of
proposed retrieval system trained with the reduced features extracted
from Max. FRoI is as shown in Eq. (14) for SGGP, Sonycyber Shot and
Canon datasets respectively. Further, the tables show that the results
obtained from Max. FRoI with LDA gives good results than the results
obtained from other proposed modes.
4.5. Comparative study between proposed work and previous work
In the previous work (Guru et al., 2018a, 2018b) the features such as
Gray Level Co-occurrence Matrix (GLCM) (Haralick et al., 1973), Local
Binary Pattern (LBP) (Ojala et al., 2002) and Scale Invariant Feature
Transform (SIFT) Lowe (2004) are extracted from entire keyframe.
With the fusion of these features the model achieved good performance.
The retrieval accuracy of previous work (Guru et al., 2018a, 2018b)
achieved 53.83%, 60.18% and 65.73% are shown in Fig. 12, Fig. 13 and
Fig. 14 for SGGP, Sonycyber Shot and Canon datasets respectively. In
the proposed work, to further improve the retrieval performance,
GLCM (Haralick et al., 1973), LBP (Ojala et al., 2002) and SIFT Lowe
(2004) features are extracted in two different modalities as mentioned
in section 3.2.2 and 3.2.3. The features extracted from proposed retrieval
system using, Max. FRoI and Max. FRoI with LDA these two methods
give good results when compared to the previous work (Guru et al.,
2018a, 2018b). The comparison between the results obtained from
previous and proposed approaches namely, features extracted from
an entire keyframe, all FRoI's, Max. FRoI and Max.FRoI with LDA are
summarized in Table 5 for all datasets.
4.6. Result analysis and discussion
We have the following observations from the proposed system of ap-
proaches namely, features extracted from an entire keyframe, all FRoI's,
Max. FRoI and Max.FRoI with LDA. Features extracted from entire
keyframes of a video provide good results with the fusion of the features
GLCM+LBP + SIFT as shown in Fig. 12 to Fig. 14. All FRoI's approach
generates almost similar results for the combination of features GLCM
+LBP + SIFT as compared to the features extracted from an entire
keyframe as shown in Fig. 6 to Fig. 8 for SGGP, Sonycyber Shot and
Canon datasets respectively. Max. FRoI's approach generates good results
for the combination of features GLCM+LBP + SIFT as shown in Fig. 9 to
Fig. 11 for SGGP, Sonycyber Shot and Canon datasets respectively. From
the results we can observe that, this approach generates improved
results than the features extracted from entire keyframe. The proposed
approach Max. FRoI with LDA results show the effectiveness of the selec-
tion of more discriminating feature subset from original set using LDA.
The efﬁciency of the proposed system using Max. FRoI with LDA is im-
proved and achieved 100% performance for SGGP, Sonycyber Shot and
Canon datasets. Table 2 and Table 3 show the combination of features
Fig. 16. Comparative study between proposed work and deep learning model (Jyothi et al., 2018) for Sonycyber Shot dataset.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
275
LBP + SIFT achieves good performance for SGGP and Sonycyber Shot
datasets. Table 4 show that the combinations of features LBP + SIFT
and GLCM+LBP + SIFT achieves good performance for Canon dataset.
4.7. Query with multiple class ﬂowers in videos
Query video may contain multiple classes of ﬂowers in video. There
are two cases at this point. First, a query video consists of multiclass
ﬂowers in all frames, then the system retrieves similar videos from da-
tabase by considering Flower Regions of Interest. Second, a query video
consists of multiclass ﬂowers not in the same frame, video consists of
one class in some duration and then other classes in next duration. In
such case, we manually split (cut) the video into shots based on class
boundary, then for each shot the system retrieves similar videos based
on FRoI using MSVM. Fig. 5 shows the query acquiring the identity of
the class for multiclass ﬂowers in a video.
5. Comparative study between proposed work and deep learning
model
In (Jyothi et al., 2018), authors have proposed a ﬂower video re-
trieval system using deep leaning approach, here the similar videos for
a given query video are retrieved using Multiclass Support Vector Ma-
chine. For the extraction of features in (Jyothi et al., 2018), authors
have proposed three different modalities; entire keyframe, segmented
ﬂower region of a keyframe, and the gradient of ﬂower region are con-
sidered for feature extraction using Deep Convolutional Neural Network
of AlexNet architecture. Among these three modalities, the segmented
ﬂower region of a keyframe is achieved better results for smaller
dataset. In (Jyothi et al., 2018), the query video consists of a single
class of ﬂowers. In the present work along with single class of ﬂower
videos query video also consists of multiclass ﬂowers. The dataset con-
sidered in the present work is relatively large. The presented model is
compared against deep learning model (Jyothi et al., 2018) which re-
veals that the proposed one is superior to the existing in terms of re-
trieval results. The proposed system Max. FRoI with LDA is improved
and achieved 100% performance for larger sized datasets namely
SGGP, Sonycyber Shot and Canon. The retrieval results in terms of Accu-
racy, Precision, Recall and F-measure of existing work (Jyothi et al.,
2018) are compared with present work and the results are shown in
Fig. 15, Fig. 16 and Fig. 17 for SGGP, Sonycyber Shot and Canon datasets
respectively.
6. Future work
The research work presented in this paper can be further extended
in following ways:
a) An attempt on shot boundary or class boundary detection when a
video consists of multiple species of ﬂowers can be further explored.
Explanation: In the present work, when a video consists of one class
of ﬂowers in some duration and then other classes in next duration. In
such case, we manually split (cut) the video into shots. To overcome
this drawback a video shot/class boundary detection is essential.
Fig. 17. Comparative study between proposed work and deep learning model (Jyothi et al., 2018) for Canon dataset.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
276
Further, the research can be automized for shot boundary detection in-
stead of manually split the video shots.
b) The current research work limits the species of ﬂowers to 30.
Explanation: The current research work limits the class size of
ﬂowers to 30. There is scope for extending the class size and explore dif-
ferent methodologies to retrieve ﬂower videos in real time.
7. Conclusion
The main aim of this work is to discover the solution to a problem of
retrieval of videos of ﬂowers through query by video mechanism. The
presented system works based on keyframes represented for each
video. Keyframes are segmented using statistical region merging algo-
rithm. From the segmented keyframes features are extracted in three
different modalities namely, all regions of ﬂowers in the keyframe, max-
imum ﬂower region in the keyframe and ﬁnally, features of maximum
ﬂower region with a set of discriminating features generated by LDA.
The presented system is compared against our previous work (Guru
et al., 2018a, 2018b) and the deep learning retrieval system (Jyothi
et al., 2018), which reveals that the proposed system with Max. FRoI
with LDA is superior to the existing models in terms of retrieval results.
The proposed system retrieves similar videos when the query video
contains multiple classes of ﬂowers in a video. In proposed work,
when video consists of one class of ﬂowers in some duration and then
other classes in next duration. In such case, we manually split (cut)
the video into shots. Further, the research can be extended by
automizing the shot boundary detection instead of manually split the
video shots. And also, the current research work limits the species of
ﬂowers to 30, further the class size can be extended and can explore dif-
ferent methodologies to retrieve ﬂower videos in real time.
Declaration of Competing Interest
We don't have any conﬂict of interest.
References
Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., 1999. Eigenfaces vs. ﬁsherfaces: recogni-
tion using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intell. 19,
711–720.
Chaitra, K.N., Jyothi, V.K., Chandrajit, M., Guru, D.S., 2021. Flower classiﬁcation in videos: a
hog-PCA-NN method. 2nd International Conference on Artiﬁcial Intelligence: Ad-
vances and Applications, ICAIAA 2021.
Chen, W., Tian, Y., Wang, Y., Huang, T., 2015. Fixed-point gaussian mixture model for anal-
ysis friendly surveillance video coding. Comput. Vis. Image Underst. 142, 65–79.
Cui, M., Cui, J., Li, H., 2016. Dimensionality reduction for histogram features: a distance-
adaptive approach. Neurocomputing 173, 181–195.
Das, M., Manmatha, R., Riseman, E., 1999a. Indexing ﬂower patent images using domain
knowledge. IEEE Intell. Syst. 14 (5), 24–33.
Das, M., Manmatha, R., Riseman, E.M., 1999b. Indexing ﬂower patent images using do-
main knowledge. IEEE Intell. Syst. 14 (5), 24–33.
Duda, R.O., Hart, P.E., Stork, D.G., 1997. Pattern Classiﬁcation. Second edition. .
Gao, X., Xuelong, L., Jun, F., Dacheng, T., 2009. Shot-based video retrieval with optical ﬂow
tensor and hmms. Pattern Recogn. Lett. 30, 140–147.
Geetha, M.K., Palanivel, S., Ramalingam, V., 2009. A novel block intensity comparison code
for video classiﬁcation and retrieval. Expert Syst. Appl. 36, 6415–6420.
Guru, D.S., Sharath, Y.H., Manjunath, S., 2010. Texture features and knn in classiﬁcation of
ﬂower images. IJCA special Issue on Recent Trends in Image Processing and Pattern
Recognition, RTIPPR, pp. 21–29.
Guru, D.S., Sharath, Y.H., Manjunath, S., 2011. Texture features in ﬂower classiﬁcation.
Math. Comput. Model. 54, 1030–1036.
Guru, D.S., Jyothi, V.K., Kumar, Y.H.S., 2018a. Features fusion for retrieval of ﬂower videos.
Proceedings of DAL 2018, LNNS 43, pp. 221–234.
Guru, D.S., Jyothi, V.K., Kumar, Y.H.S., 2018b. Cluster based approaches for keyframe selec-
tion in natural ﬂower videos. Springer AISC 736, ISDA 2018, pp. 474–484.
Gyamﬁ, K.S., Brusey, J., Hunt, A., Gaura, E., 2018. Linear dimensionality reduction for clas-
siﬁcation via a sequential bayes error minimisation with an application to ﬂow meter
diagnostics. Expert Syst. Appl. 91, 252–262.
Han, J., Ji, X., Hu, X., Han, J., Liu, T., 2014. Clustering and retrieval of video shots based on
natural stimulus FMRI. Neurocomputing 144, 128–137.
Haralick, R.M., Shanmugam, K., Dinstein, I., 1973. Textural features for image classiﬁca-
tion. IEEE Trans. Syst. Man Cybernet. SMC-3 (6), 610–621.
Hong, R., Pan, J., Hao, S., Wang, M., Xue, F., Wu, X., 2014. Image quality assessment based
on matching pursuit. Inf. Sci. 273, 196–211.
Hu, W., Xie, N., Li, L., Xianglin, Z., Maybank, S., 2011. A survey on visual content-based
video indexing and retrieval. IEEE Trans. Syst. Man Cybernet. 41 (6), 797–819.
Jyothi, V.K., Guru, D.S., Sharath Kumar, Y.H., 2018. Deep learning for retrieval of natural
ﬂower videos. Elsevier Proc. Comput. Sci. 132, 1533–1542.
Khan, N.M., Ksantini, R., Ahmad, I.S., Boufama, B., 2012. A novel SVM+NDA model for
classiﬁcation with an application to face recognition. Pattern Recogn. 45, 66–79.
Kumar, M.A., Gopal, M., 2011. A hybrid SVM based decision tree. Pattern Recogn. 43,
3977–3987.
Li, J., Li, X., Yang, B., Sun, X., 2015. Segmentation-based image copy move forgery detec-
tion scheme. IEEE Trans. Inf. Foren. Secur. 10 (3), 507–518.
Liang, B., Xiao, W., Liu, X., 2012. Design of video retrieval system using MPEG-7 descrip-
tors. Proc. Eng. 29, 2578–2582.
Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoint. Int. J. Comput.
Vis. 60, 91–110.
Morand, C., Benois-Pineau, J., Domenger, J.P., Zepeda, J., Kijak, E., Guillemot, C., 2010. Scal-
able object-based video retrieval in hd video databases. Signal Process. Image
Commun. 25, 450–465.
Mufti, M., Shamim, K.M., McGinnity, Martin, Hussain, Amir, 2021. Deep learning in mining
biological data. Cogn. Comput. 13, 1–33.
Nock, R., Nielsen, F., 2004. Statistical region merging. IEEE Trans. Pattern Anal. Mach.
Intell. 26 (11), 1–7.
Ojala, T., Pietikainen, M., Maenpaa, T., 2002. Multiresolution gray scale and rotation in-
variant texture classiﬁcation with local binary patterns. IEEE Trans. Pattern Anal.
Mach. Intell. 24 (7), 971–987.
Priya, G.G.L., Domnic, S., 2014. Shot based keyframe extraction for ecological video
indexing and retrieval. Ecol. Inform. 23, 107–117.
Shekar, B.H., Uma, K.P., Holla, K.R., 2016. Video clip retrieval based on lbp variance. Proc.
Comput. Sci. 89, 828–835.
Shen, X.-J., Mu, L., Li, Z., Wu, H.-X., Gou, J.-P., Chen, X., 2016. Large-scale support vector
machine classiﬁcation with redundant data reduction. Neurocomputing 172,
189–197.
Stauffer, C., Grimson, W.E.L., 1999. Adaptive background mixture models for real-time
tracking. Proceedings of IEEE Conference on Computer Vision and Pattern Recogni-
tion.
Vapnik, V.N., 1998. Statistical Learning Theory. John wiley and sons, New York, USA.
Wäldchen, Jana, Rzanny, Michael, Seeland, Marco, Mäder, Patrick, 2018. Automated plant
species identiﬁcation—Trends and future directions. https://doi.org/10.1371/journal.
pcbi.1005993.
Wang, S., Lu, J., Gu, X., Du, H., Yang, J., 2016. Semi-supervised linear discriminant analysis
for dimension reduction and classiﬁcation. Pattern Recogn. 57, 179–189.
Zhu, Y., Huang, X., Huang, Q., Tian, Q., 2016. Large-scale video copy retrieval with tempo-
ral concentration sift. Neurocomputing 187, 83–91.
V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al.
Artiﬁcial Intelligence in Agriculture 5 (2021) 262–277
277
