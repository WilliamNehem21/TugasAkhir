Waveﬁeld solutions from machine learned functions constrained by the
Helmholtz equation
Tariq Alkhalifah a,*, Chao Song a, Umair bin Waheed b, Qi Hao c
a Physical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, 23955, Saudi Arabia
b Department of Geosciences, King Fahd University of Petroleum and Minerals, Dhahran, 31261, Saudi Arabia
c Center for Integrated Petroleum Research, King Fahd University of Petroleum and Minerals, Dhahran, 31261, Saudi Arabia
A R T I C L E I N F O
Keywords:
Helmholtz equation
Waveﬁelds
Modeling
Neural networks
Deep learning
A B S T R A C T
Solving the wave equation is one of the most (if not the most) fundamental problems we face as we try to illu-
minate the Earth using recorded seismic data. The Helmholtz equation provides waveﬁeld solutions that are
dimensionally reduced, per frequency, compared to the time domain, which is useful for many applications, like
full waveform inversion. However, our ability to attain such waveﬁeld solutions depends often on the size of the
model and the complexity of the wave equation. Thus, we use here a recently introduced framework based on
neural networks to predict functional solutions through setting the underlying physical equation as a loss function
to optimize the neural network (NN) parameters. For an input given by a location in the model space, the network
learns to predict the waveﬁeld value at that location, and its partial derivatives using a concept referred to as
automatic differentiation, to ﬁt, in our case, a form of the Helmholtz equation. We speciﬁcally seek the solution of
the scattered waveﬁeld considering a simple homogeneous background model that allows for analytical solutions
of the background waveﬁeld. Providing the NN with a reasonable number of random points from the model space
will ultimately train a fully connected deep NN to predict the scattered waveﬁeld function. The size of the
network depends mainly on the complexity of the desired waveﬁeld, with such complexity increasing with
increasing frequency and increasing model complexity. However, smaller networks can provide smoother
waveﬁelds that might be useful for inversion applications. Preliminary tests on a two-box-shaped scatterer model
with a source in the middle, as well as, the Marmousi model with a source at the surface demonstrate the potential
of the NN for this application. Additional tests on a 3D model demonstrate the potential versatility of the
approach.
1. Introduction
A fundamental part of using surface seismic recorded data to illumi-
nate the Earth is solving the wave equation (Claerbout, 1985). Solving
the wave equation numerically constitutes the majority of the compu-
tational cost and complexity in applications like seismic modeling, im-
aging, and waveform inversion. Time-domain solutions of the wave
equation dominate seismic applications as they are often efﬁcient and
comply with our natural understanding of wave evolution (Alterman and
Karal, 1968; Richards and Aki, 1980). However, frequency-domain so-
lutions, providing a reduction in dimensionality, recently gained addi-
tional attention with the rise of waveform inversion (Pratt, 1999; Sirgue
and Pratt, 2004). Such solutions are obtained by inverting the stiffness
matrix of the Helmholtz wave equation. However, the cost and
complexity of such a matrix inversion are intolerable as the model size
increases, like for high frequencies or 3D applications (Cl�ement et al.,
1990), or the wave equation is complex, like those in anisotropic media
(Wu and Alkhalifah, 2018a). This led (Sirgue et al., 2008) to suggest
using time-domain modelling to obtain waveﬁelds in the frequency
domain for waveform inversion applications. However, such solutions
are vulnerable to dispersion and stability errors (Wu and Alkhalifah,
2018b).
In recent years, researchers in our ﬁeld have utilized machine
learning algorithms to predict and classify fault locations, horizons, salt
boundaries, facies, as well as, velocity models (R€oth and Tarantola, 1994;
Wrona et al., 2018; Araya-Polo et al., 2019; Holm-Jensen and Hansen,
2020). Whether supervised or semi or unsupervised training, neural
networks have shown incredible ﬂexibility in adapting to various
* Corresponding author.
E-mail address: tariq.alkhalifah@kaust.edu.sa (T. Alkhalifah).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.08.002
Received 2 June 2021; Received in revised form 13 August 2021; Accepted 17 August 2021
Available online 8 September 2021
2666-5441/© 2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/).
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
geophysical tasks. Supervised learning was instrumental in predicting
low frequencies to help full waveform inversion (FWI) converge to an
accurate solution (Ovcharenko et al., 2019). Deep learning was also
utilized to develop ”a priori” models from well information to be used in
FWI (Mosser et al., 2018; Zhang and Alkhalifah, 2019). Even wave
propagation and wave equation solutions were facilitated using deep
neural networks (Sorteberg et al., 2018; Hughes et al., 2019).
Within the framework of utilizing deep neural networks as universal
function approximators (Liu and Nocedal, 1989) and under the banner of
physics-informed neural networks (PINN), Raisse et al. (2019) demon-
strated the network's ﬂexibility in learning how to extract desired func-
tional solutions to nonlinear partial differential equations, utilizing the
concept of automatic differentiation (Baydin et al., 2018). PINN has
found considerable traction in solving partial differential equations
(linear and nonlinear ones) ranging from cardiac activation mapping
(Sahli Costabal et al., 2020) to steady-state Navier-Stokes equation
(Dwivedi et al., 2021). Even within the framework of one dimensional
wave propagation, PINN was utilized to establish ﬂexible domain solu-
tions of the wave equation (Kissas et al., 2020). In all of these applica-
tions, the predicted solutions were smooth, which is a requirement of NN
as a universal function approximator (Pinkus, 1999). Waveﬁelds are
generally smooth, but they are often more complex in nature than other
physical phenomena. The complexity of the waveﬁeld increases at the
source, as it represents a singularity in the solution. Thus, to use ML to
predict waveﬁeld solutions will require larger neural networks, which
will eventually require larger computational resources. It will also
require, like most numerical methods, careful sampling of the source
region. As a result Song et al. (2021) suggested that we seek such NN
functions for the scattered waveﬁeld instead of the full waveﬁeld. Spe-
ciﬁcally, to avoid the need for adaptive training points for the neural
network (NN) to handle the expected source singularity bias, they solve
for the scattered waveﬁeld in the frequency domain and, thus, utilize the
corresponding Lippmann–Schwinger equation as the loss function to
train a deep fully connected neural network with inputs given by
(randomly chosen) points in space (within the domain of interest) and
outputs given by the complex scattered waveﬁeld at these points. In their
implementation, they focus on the application of their method on
anisotropic media. Our objective in this paper is to evaluate what exactly
an NN can predict of the waveﬁeld solution, especially at a reasonable
cost that can be utilized in practical applications.
Thus, here, we focus on the role of the model size, the solver, and
frequency of the waveﬁeld in predicting such scattered waveﬁeld solu-
tions. We will ﬁrst compare solutions for the Helmholtz equation (McFall
and Mahan, 2009) to those obtained for the scattered version of the
Helmholtz equation for the same network size and hyperparameters. We
will compare solutions at two different frequencies to assess the ability of
the NN model in handing higher frequencies. This is followed by inves-
tigating the role of the NN model size in smoothing the waveﬁeld solu-
tions by evaluating the corresponding velocity for the predicted
waveﬁelds. We test the performance of the NN on a two box-shaped
scatterer model, as well as, the Marmousi model, and in the process
show the sensitivity of the approach to model size and frequency. Further
testing on 3D will demonstrate the role of the NN parameters optimizer.
2. The Helmholtz equation
The wave equation is often solved in the time domain, and such so-
lutions are attained by extrapolating the waveﬁeld in time simulating
what happens in nature (Richards and Aki, 1980). Waveﬁelds in the time
domain, however, are large, as they are given by a four dimensional
function in 3D media or a three dimensional function in 2D media for a
given source. In addition, the time axis, often, requires ﬁne sampling to
avoid aliasing, and an even ﬁner sampling of time is required to avoid
instability when solving the wave equation using ﬁnite-difference
methods (Courant, 1928).
As a result of its linear nature, the wave equation can be easily
formulated in the frequency domain. In this case, the resulting Helmholtz
equation can be solved per frequency, with no requirements on frequency
sampling, admitting a reduction in dimensionality of the waveﬁeld so-
lution. The Helmholtz equation in an acoustic, isotropic, constant density
medium, described by the velocity, v, is given by:
�
r2 þ k2�
uðxÞ ¼ f ðxÞ; where k ¼ ω
v:
(1)
In this case, the solution of such an equation is a complex waveﬁeld,
u ¼ {ur, ui}, deﬁned in the Euclidean space, with x ¼ {x, y, z}, and a
function of the angular frequency, ω. As a result, our time-domain solu-
tion is nothing but a superposition of frequency-domain solutions.
The point source nature of the source function, f, admits a singularity
in the waveﬁeld solution at the point source location. Such a singularity
often causes inaccuracies in numerical solutions of the Helmholtz equa-
tion near the source. As suggested by Song et al. (2021), such a limitation
can be addressed by solving the Lippmann–Schwinger form of the wave
equation (Lippmann and Schwinger, 1950), instead. This equation is
exact as we do not apply the Born approximation, as we maintain the true
velocity on the right hand side of the equation. Thus, to somewhat
mitigate the source singularity, we solve for the scattered waveﬁeld,
δu ¼ u � u0, where u0 is the background waveﬁeld satisfying the same
wave equation (equation (1)) for the background velocity v0. Deﬁning
the velocity model perturbation, δm ¼ 1
v2 � 1
v2
0, the scattered waveﬁeld
satisﬁes
�
r2 þ ω2
v2
�
δu ¼ �ω2δm u0:
(2)
For the scattered waveﬁeld, the source function is no longer conﬁned
in space, like the point source. It now depends on the perturbation model,
which may extend the full space domain. To allow for efﬁcient evaluation
of the background waveﬁeld, we consider the background velocity, v0, to
be constant. For marine acquisition, we may choose this constant velocity
to equal the water velocity to reduce the effect of the source singularity
even further. The waveﬁeld in an acoustic isotropic medium in 3D for a
constant velocity and a point source located at xs, is given by:
u0ðxÞ ¼ ei ω
v0 jx�xsj
4πjx � xsj;
(3)
where i is the imaginary identity. For 2D applications, the solution for
acoustic isotropic media is given by
u0ðxÞ ¼ i
4Hð2Þ
0
�ω
v0
jx � xsj
�
;
(4)
where Hð2Þ
0
is the zero-order Hankel function of the second kind, here
x ¼ {x, z} (Richards and Aki, 1980).
Solving for the scattered waveﬁeld will allow us later to utilize
random samples of the space domain to train the neural network to
provide the functional solution representing the scattered waveﬁeld in
the frequency domain. The analytical solution for the background
waveﬁeld allows us to evaluate the waveﬁeld instantly at any random
point in the domain of interest. Next, we will see exactly how these
formulations help the training of a functional neural network (NN).
3. The neural network solution
Based on the physics-informed neural network (PINN) framework
introduced by Raissi et al. (2019), we utilize a neural network architec-
ture using fully connected layers to approximate a function. This function
is the scattered waveﬁeld solution of equation (2). Hornik et al. (1989).
have shown the ability of neural networks in approximating functions
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
12
that are smooth, like what we would expect from solving the wave
equation. The input to the network, like a function, is a location in space,
given in 2D by x and z coordinate values, and in 3D by x, y, and z co-
ordinate values. The output of the network consists of the real and
imaginary values of the complex scattered waveﬁeld at the input loca-
tion. Fig. 1 shows, in detail, the PINN concept for our application. We use
the network to evaluate the waveﬁeld and its second-order partial de-
rivatives in x and z, which is needed to evaluate the Laplacian operator
and the loss function. Thus, to train the network with equation (2), we
use the following loss function:
f ¼ 1
N
X
N
j¼1
��ω2mðjÞδuðjÞ
r þ r2δuðjÞ
r þ ω2δmðjÞuðjÞ
r0
��2
2þ
��ω2mðjÞδuðjÞ
i
þr2δuðjÞ
i þ ω2δmðjÞuðjÞ
i0
��2
2;
(5)
where N is the number of training samples, and j is the training sample
index. The two terms in the loss function correspond to the losses for the
real (δur) and imaginary (δui) parts of the scattered waveﬁeld, using the
real (ur0) and imaginary (ui0) parts of the background waveﬁeld. For the
loss function, we chose the background model to be simple enough
(homogeneous) so that the background waveﬁeld can be evaluated
analytically on the ﬂy. The details of the fully connected deep network
will be shared in the examples. The activation function between layers,
other than the last layer, in all the examples is an inverse tangent. The last
hidden layer connected to the output layer is linear. We chose to optimize
the loss function using an Adam optimizer followed by limited memory
BFGS iterations, all full-batch, gradient-based optimization algorithm
(Liu and Nocedal, 1989). The L-BFGS admits smoother and more robust
updates at a higher cost. We will show later the performance of both
optimizers separately for comparison.
The NN functional provides a continuous representation of the
waveﬁeld, as opposed to a grid based representation, and such a
continuous representation offers many beneﬁts. We can attain the solu-
tion at any point, no interpolation is needed, and the domain of coverage
can be of any shape. This can be beneﬁcial in the presence of topography.
However, this continuous functional representation has its limitations
that appear mainly when the waveﬁeld is complex, requiring larger
networks and more advanced training. This appears to be the case when
we have strong scattering and high frequencies. As a result, in the
following tests, and as we introduce the approach, we will focus on lower
frequencies and smoothed models. We will, nevertheless, also demon-
strate these limitations as we compute the velocity model that corre-
sponds to the predicted scattered waveﬁeld. This can be achieved by
solving the wave equation for the velocity model, and from equation (1),
this implies:
v2
approxðxÞ ¼ R
ω2uðxÞ
f ðxÞ � r2uðxÞ; where uðxÞ ¼ u0ðxÞ þ δuNNðxÞ;
(6)
δuNN is the predicted neural network scattered waveﬁeld solution,
and R corresponds to the real part. The process of solving equation (6)
must be handled with care and there are many ways to do so including
multiplying the numerator and denominator with the complex conjugate
of the denominator and adding a small positive number to the denomi-
nator to avoid dividing over zero (Song and Alkhalifah, 2020).
4. Testing the NN
We will test this NN framework initially on two 2D examples trying to
highlight some of its features and weaknesses. In the ﬁrst example, we
use a two box-shaped scatterer model with the source in the middle and
we look at the dependency of the prediction on the frequency. Then, we
apply the approach on the Marmousi model with the source on the sur-
face, and we test the dependency of the solution on the size of the neural
network. Finally, we apply the approach on a small 3D model and focus
on the role of the optimizer. The objective of these tests is to study the
ability of an NN to learn to a functional solution of the wave equation for
the scattered waveﬁeld as opposed to the waveﬁeld itself.
4.1. A two-scatterer model
In the ﬁrst model, we place two box-shaped perturbations in an
otherwise homogeneous background as shown in Fig. 2(a). The model
has 100 samples in both the x and z directions, with a sampling interval of
20 m. The corresponding (real part) of the 5 Hz waveﬁeld for a point
source (a delta function, one sample) in the center of the model is shown
in Fig. 2(b). The background model is given by a constant velocity of
2 km/s, and the corresponding waveﬁeld for the same source and fre-
quency is shown in Fig. 2(c). If we subtract the two waveﬁelds, we obtain
the true scattered waveﬁeld with the real part shown in Fig. 3(a), where
the energy, as expected, reﬂects scattering from the two box-shaped
scatterers. Using the loss function in equation (5), we train an 8-layer
deep fully connected neural network with 20 neurons in each layer to
represent the scattered waveﬁeld solution. We randomly chose 5000
samples from the space domain (xi, zi) for the training, and train for
100000 epochs of Adam updates and 20000 of LBFGS updates. This
number of samples used represents one fourth of the grid samples used to
solve the Helmholtz equation and it was necessary to arrive to the scat-
tered waveﬁeld solution shown in Fig. 3(b). The difference between the
true scattered waveﬁeld and the NN predicted one is shown in Fig. 3(c).
There are differences, but they are generally mild. The imaginary part of
the scattered waveﬁeld, not shown here, had similar accuracy.
Fig. 1. The neural network architecture (left side dashed box) with inputs (x, z) and outputs given by the real and imaginary parts of the (scattered) waveﬁeld. The
network is trained using a loss function given by the scattered wave equation (right side dashed box), in which the Laplacian components (δur,xx.δur,zz, δui,xx, δui,zz) are
evaluated using automatic differentiation of the NN. The loss function can be supported by boundary conditions.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
13
To justify inverting for the scattered waveﬁeld instead of the wave-
ﬁeld directly using the Helmholtz wave equation, we repeat the exact
experiment with the same number of randomly chosen training samples.
The loss function, in this case, is given by the Helmholtz wave equation
and to lessen the effect of a point source bias, we use an isotropic
Gaussian source with a variance of 2.5. Fig. 4(a) and (b) show the real
and imaginary parts, respectively, of the true waveﬁeld for the two-
scatterers model shown in Fig. 2(a). Fig. 4(c) and (d) show the real and
imaginary parts, respectively, of the NN predicted waveﬁeld for the same
model. The difference is large and this is attributed to the source singu-
larity in the Helmholtz equation, which requires better sampling of the
source area in the training data.
For an 8 Hz waveﬁeld, we use a larger network given by 40 neurons in
each of the 8 layers and we use 10000 samples in the training. The real
part of the predicted scattered waveﬁeld is shown in Fig. 5(a). The dif-
ference between this predicted waveﬁeld and the considered true nu-
merical solution, plotted at the same scale as in Fig. 5(a), is small as
shown in Fig. 5(b). To arrive to this solution, we used 150000 epochs of
Adam updates and 20000 of LBFGS updates as demonstrated in Fig. 5(c).
We use LBFGS at the end as it admits smoother updates we can rely on,
but it is generally more expensive. The sudden change in the behaviour of
the loss curve reﬂects the transition from Adam to LBFGS. Despite the
larger network, compared to the 5Hz case, and additional epochs, the
cost increase was less than 100%, and that is much smaller than the
additional cost we experience in solving for high frequencies using ﬁnite
difference methods, which tend to increase exponentially.
4.2. The Marmousi model
Now, we test the utilization of the NN PDE in solving for the waveﬁeld
for a slightly smoothed Marmousi model (Fig. 6(a)). A point source is
placed, this time, on the surface at location 4.5 km. We solve the
Helmholtz equation numerically to obtain the 3 Hz frequency waveﬁeld.
The background model is homogeneous with a velocity of 1.5 km/s in
which we can solve the waveﬁeld analytically. The difference between
the true and the background waveﬁelds, constituting the scattered
waveﬁeld is shown in Fig. 6(b) (real part) and 6(c) (imaginary part). The
background waveﬁeld and the model perturbations (difference between
the true model and the homogeneous background) are used in the cost
function given by equation (5) to invert for the NN parameters. We use,
this time, a 10-layer network with {128, 128, 64, 64, 32, 32, 16, 16, 8, 8}
neurons in the layers, respectively. We ﬁnd that this conﬁguration, given
by larger dimensional layers early, is generally more effective. We use
10000 random sample points for the training and the resulting loss over
20000 epochs of training is shown in Fig. 7(a). The trained network is
then used to evaluate the scattered waveﬁeld on a regular grid and the
resulting real part is shown in Fig. 7(b) and the imaginary part is shown
in Fig. 7(b).
The difference between the true scattered waveﬁeld and the NN
predicted one is shown in Fig. 8(a) (real part) and 8(b) (imaginary part).
The difference is generally small again, but here it seems to include more
coherent energy corresponding to some of the scattering. In other words,
the resulting NN predicted scattered waveﬁeld is smoother than the true
waveﬁeld. This is an expected feature of NN when we avoid overﬁtting,
the network acts as a smoother (Neal et al., 2018). We can further verify
this smoothness feature by using the wave equation (equation (6)) to
compute the velocity model corresponding to the predicted waveﬁeld as
shown in Fig. 8(c).
If we use a smaller network of 8 layers with {64, 64, 32, 32, 16, 16, 8,
8} neurons in the layers from left to right (we dropped the ﬁrst two layers
from the previous network), and use the same number of epochs, the
resulting real part of the predicted scattered waveﬁeld tends to be
smoother as shown in Fig. 9(a). The difference between the predicted and
true scattered waveﬁeld is shown in Fig. 9(b) plotted at the same scale.
The difference includes more energy than before. The increased
smoothness of the waveﬁeld can be veriﬁed by the resulting velocity
Fig. 2. a) A two-scatter model. b) The real part of a 5 Hz waveﬁeld for the velocity in (a) for a source in the middle, computed numerically, and considered true. c) The
real part of the 5 Hz waveﬁeld for the background model given by a velocity of 2 km/s, computed analytically.
Fig. 3. a) The scattered waveﬁeld given by the difference between the two waveﬁelds in Fig. 2(b) and (c) (True and background waveﬁelds). b) The NN predicted
scattered waveﬁeld on a regular grid. c) The difference between the actual and predicted scattered waveﬁelds.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
14
model calculated from the waveﬁeld and shown in Fig. 9(c). The velocity
model is smooth compared to the true model, reﬂecting the smooth na-
ture of the waveﬁeld.
On the other hand, if we actually use a 12-layer network by adding
two layers at the beginning of the original network with 256 neurons in
each of them, we end up with a network given by {256, 256, 128, 128,
64, 64, 32, 32, 16, 16, 8, 8} neurons in the layers from left to right. Using
the same number of epochs in the training of the same random samples in
space, we end up with the predicted scattered waveﬁeld with the real
part shown in Fig. 10(a). The difference between the predicted and true
scattered waveﬁeld is shown in Fig. 10(b) plotted at the same scale. It
contains less energy and that again can be veriﬁed by the resulting ve-
locity model calculated from the waveﬁeld and shown in Fig. 10(c). The
velocity model is clearly sharper and it is reasonably close to the true
velocity model shown in Fig. 6(a).
Thus, a larger network can provide more accurate waveﬁelds, but for
applications in gradient calculation for velocity model update, a perfect
scattered waveﬁeld is not necessary. The cost of training the 12-layer
network is 50% higher than the 10-layer one in spite that the number
of network model parameters increased by 4. Meanwhile, the cost of the
training the 8-layer network is two-third the cost of training the 10-layer
network, while the number of network parameters is one-fourth of that of
the 10-layer network. So, in summary, using this network architecture,
the cost of the training will increase by about 50% with the addition of
two layers of double the size of the ﬁrst (largest) layer, and we end up
with a higher resolution waveﬁeld.
Fig. 4. a) The real part of the true waveﬁeld. b) The imaginary part of the true waveﬁeld. c) The real part of the NN predicted waveﬁeld. d) The imaginary part of the
NN predicted waveﬁeld. The waveﬁelds correspond to the velocity model in Fig. 2(a).
Fig. 5. a) The NN predicted scattered 8Hz waveﬁeld for a source in the middle. b) The difference between the predicted scattered waveﬁeld and the one computed
numerically (true) plotted at the same scale as in a). c) The NN training loss function, which displays the loss using Adam followed by LBFGS.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
15
Fig. 6. a) The Marmousi model. b) The real part of the resulting 3 Hz waveﬁeld for a source on the surface located in the middle. c) The imaginary part of
the waveﬁeld.
Fig. 7. a) The loss function for the training of the NN. b) The real part of the predicted scattered waveﬁeld from the NN network. c) The imaginary part.
Fig. 8. a) The difference between Fig. 6(b) and 7(b) (True and predicted real parts of the scattered waveﬁelds). b) The difference between Fig. 6(c) and 7(c) (True and
predicted imaginary parts of the scattered waveﬁelds). c) The velocity model computed from the predicted waveﬁeld.
Fig. 9. a) The real part of the predicted scattered waveﬁeld using an 8-layer network. b) The difference between Fig. 6(b) and 9(a) (True and predicted real parts of the
scattered waveﬁelds). c) The velocity model computed from the predicted waveﬁeld.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
16
4.3. A 3D example and the optimizer
We consider a 3D cube extracted from the SEG/EAGE Overthrust
model (Aminzadeh et al., 1994) and slightly smoothed as shown in
Fig. 11(a). In this test, we also test the performance of two NN optimi-
zation algorithms, speciﬁcally Adam and LBFGS. The background ho-
mogeneous model has a velocity of 3.2 km/s. The difference between the
Helmholtz computed waveﬁeld for 10 Hz and the background waveﬁeld
for the same frequency provides us with the true scattered waveﬁeld for a
source located in the middle, with the real part of this scattered waveﬁeld
shown in Fig. 11(b). Since the Adam optimizer admits, as we saw earlier,
turbulent loss functions, for this example, we will test the performance of
the two network optimization algorithms separately: The Adam opti-
mizer and the limited memory BFGS algorithm. In this comparison, for
the Adam optimizer we use 150000 epochs and for the LBFGS we use
50000 epochs in the training, and the neurons in each of the 8 hidden
layers are {64, 64, 32, 32, 16, 16, 8, 8}, as shown in Fig. 11(c).
The loss function for the Adam optimizer is shown in Fig. 12(a) and,
in average, the loss reduces per epoch, but with Adam we notice the loss
function is bumpy and this has been realized by others in the application
of PINN. Considering the log scale of the vertical axis, such alterations are
slightly exaggerated in the Figure. The Adam update can be made
smoother by using a smaller learning rate, but that increases the number
of epochs. The real part of the resulting predicted scattered waveﬁeld is
shown in Fig. 12(b), and the difference between it and the true scattered
waveﬁeld is shown in Fig. 12(c). The difference, plotted at the same scale
as the scattered waveﬁeld, is small. On the other hand, using an LBFGS
optimizer, the loss function is smoother as shown in Fig. 13(a), and
seemingly admits a lower loss. However, the predicted scattered wave-
ﬁeld from the network, shown in Fig. 13(b), looks almost identical to the
one obtained from the Adam optimizer. This can be further veriﬁed by
observing the difference between the predicted scattered waveﬁeld and
the true one shown in Fig. 13(c), plotted again at the same scale. The
errors seem to be similar to that observed with the Adam optimizer. So
the effective differences in the loss has limited effect on the waveﬁeld.
Considering that the results from using the Adam and the LBFGS
optimizers are similar, and since the LBFGS optimizer is more expensive
depending on the memory parameters, we suggest, as Raissi et al. (2019)
suggested, using initially the Adam optimizer, which is extremely pop-
ular in ML optimizations.
5. Discussions
Machine learning provides a platform for predicting outputs by
mainly recognizing the corresponding patterns of the inputs through a
training process. Waveﬁelds are by deﬁnition smooth and differentiable
other than at the source, which is a requirement for a functional NN
solution output (Hornik et al., 1989). The input to the proposed neural
network is a location in space and the output is the waveﬁeld (or the
scattered waveﬁeld) that satisﬁes a cost function given by the Helmholtz
equation or its variant. These equations depend on the velocity and the
source function, or in our implementation, the background waveﬁeld and
the velocity perturbations. So the NN weights and biases are expected to
absorb the velocity and source information in their efforts to learn to
predict the solution of the wave equation. As the NN tries to learn the
waveﬁeld, the source location is, especially, inﬂuential as it determines
the epicentre of the waveﬁeld. The velocity has generally a second-order
effect on the wave shape, compared to the source location. Meanwhile,
the frequency mainly controls the wavelength. These facts can help us
decide on how to use the network for any successive waveﬁeld solutions.
For example, solutions for any additional velocity perturbations that
maybe extracted from any velocity model update procedure like migra-
tion velocity analysis or full waveform inversion. Speciﬁcally, the current
NN model can be used as an initial model for training on the updated
velocity.
By using the Born (Lippmann–Schwinger equation) version of the
wave equation instead of the Helmholtz solver, we avoided the bias
required in better sampling the source region in the training of the
network, necessary to mitigate the effect of the source singularity. So by
using a homogeneous background model in which the waveﬁeld can be
Fig. 10. a) The real part of the predicted scattered waveﬁeld using a 12-layer network. b) The difference between Fig. 6(b) and 10(a) (True and predicted real parts of
the scattered waveﬁelds). c) The velocity model computed from the predicted waveﬁeld.
Fig. 11. a) A 3D model. b) The real part of the resulting 10 Hz waveﬁeld for a source on the surface located in the middle. c) The NN architecture with dimensions of
the 8 hidden layers given by (64,64,32,32,16,16,8,8) from shallow to deep.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
17
solved analytically (and instantly), the perturbations (difference between
true and background models) will often extend the model domain, and
random samples of the model space can be used in the training. To
remove the signature of the source singularity from the scattered wave-
ﬁeld, the background velocity should be chosen equal to the velocity at
the source. For marine data, this is given by a velocity of approximately
1.5 km/s. In general, the accuracy of predicting the waveﬁeld depends
mainly on the complexity of the waveﬁeld. Near the source, the wave-
ﬁeld is complex, but it could also be complex in many other areas
depending on the velocity model. In this case, we will need a larger
neural network model, as well as better sampling of these complex re-
gions. The random training points will often sample the domain
reasonably well, but it does not take into account the complexity of the
waveﬁeld. From our observation, and especially with the Marmousi
model, for a ﬁxed neural network model size and random sampling of the
training, the NN provides a uniformly smooth waveﬁeld compared the
true one. We can also utilize the concept of collocation points, as well, as
adaptively adding points in regions requiring more emphasis (i.e., with
high residuals) (McFall and Mahan, 2009). These options have their own
cost. The number of training samples used to train the NN to predict the
scattered waveﬁeld is a delicate matter (Zhou and Wu, 2011). It directly
affects the cost of the training and yet it is necessary that we have enough
samples to accurately train the network to predict the waveﬁeld. Training
examples and their inﬂuence on the training is an ongoing research topic
in the machine learning community.
Another feature of using a cost function for NN training, like in Raissi
et al. (2019), we can ﬁt the boundary condition or even the data as part of
the objective and, thus, include two or more terms in the cost function.
For the data ﬁtting case, this amounts to something like the waveﬁeld
reconstruction method (Van Leeuwen and Herrmann, 2013), which is
also solved in the frequency domain and faces similar challenges with
regard to data and model sizes (Song et al., 2021). Thus, an important
feature of such neural network waveﬁeld solutions is the ﬁxed memory
requirements, mainly controlled by the architecture of the network. It is,
thus, independent of the size of the gridded velocity model. As we saw,
the errors associated with reducing the size of the network are not of the
dispersion kind, like for conventional numerical solvers considering the
velocity model discretization, but they manifest themselves in smoothing
the waveﬁeld.
The cost of training the neural network depends on the number of
random samples used in the training (the training set) to optimize the
network parameters, as well as, the size of the network. As the trained NN
tries to ﬁt the loss function given by the wave equation or its Born form
and any boundary conditions, the size of the network, including the
number of layers and neurons, deﬁnes the details of the predicted
waveﬁeld. As we saw, smaller networks, cheaper to train, admit
smoother waveﬁelds. Thus, the size of the neural network will depend on
the application and the objective involved in the application. For more
accurate waveﬁelds, the cost can be much higher than conventional
numerical methods, as we are solving an optimization problem starting
from random initialization. Transfer learning, in which we use the pre-
viously trained network as an initial NN model for a slightly updated
waveﬁeld due to a small shift in the source location or a change in the
velocity model, can reduce some of this cost (Song and Alkhalifah, 2021).
On the other hand, an application like waveform inversion may require
smoother waveﬁelds in the early iterations as we build up the back-
ground low wavenumber model and, thus, a neural network waveﬁeld
can help us obtain smoother velocities and/or gradients without the need
for smoothing or spatial ﬁltering. Intuitively, the smoother the waveﬁeld,
the less NN parameters we will need, which bodes well for effecient low
frequencies. Interestingly, the cost increase of enlarging the network to
predict waveﬁelds for higher frequencies, is less severe than that needed
for ﬁnite difference methods. However, we noticed that for high fre-
quencies the training is harder as we use inverse tangent activation
Fig. 12. a) The loss function for the training of the NN using an Adam optimizer. b) The real part of the predicted scattered waveﬁeld from the NN. c) The difference
between the predicted waveﬁeld and the true one in Fig. 11(b).
Fig. 13. a) The loss function for the training of the NN using an LBFGS optimizer. b) The real part of the predicted scattered waveﬁeld from the NN. c) The difference
between the predicted scattered waveﬁeld and the true one in Fig. 11(b).
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
18
functions to develop the sinusoidal waveﬁeld. An alternative is to use
sine activation functions, which we plan to investigate in the near future.
Though, at this early stage of using this functional ML solution, the
cost may not justify replacing the regular Helmholtz solvers for 2D, and
maybe even 3D, isotropic examples, the potential and ﬂexibility of the
approach will induce more interesting applications. This includes appli-
cations on more complex physics, like anisotropy (i.e. (Song et al., 2021))
and elasticity, where the regular Helmholtz solver becomes impractical.
This includes applications involving complex waveﬁelds in 3D, like those
for orthorhombic anisotropy, even if the perturbations are small. Con-
ventional frequency-domain solutions for such complex physics are hard
and somewhat beyond our capability, especially if the model size is large.
Machine learning is an optimal platform for large problems and large
data as it adapts to this objective and learns the appropriate solution by
recognizing patterns. Thus, for complex physics, our cost function will
change, and possibly include more terms, but the training machinery is
the same and does not involve solving for the inverse of a large matrix.
6. Conclusions
We trained a neural network to provide functional solutions to the
Helmholtz equation. To avoid the point source singularity, we use a fully
connected network that takes in space coordinates within the domain of
interest and outputs the real and imaginary parts of the scattered (instead
of the full) waveﬁeld in the frequency domain. The background velocity
is homogeneous, which admits analytical solutions of the background
waveﬁeld. With automatic differentiation, the network is capable, as
well, of evaluating the partial derivatives of the scattered waveﬁeld
necessary to evaluate the loss function given by the Lippmann Schwinger
form of the wave equation. This loss function is used to update the
network parameters. With a scattered waveﬁeld corresponding to per-
turbations spanning the space domain, the training of the network can be
performed with less random samples. However, the network and the
number of samples should increase with an increase in frequency. This
increase in cost is far less than the exponential increase we experience in
the case of increasing frequency for ﬁnite difference methods. Overall,
the output waveﬁelds are somewhat smoother than the exact ones, and
this can be attributed to the compromise feature of our relatively small
network and this feature might be useful for applications like waveform
inversion. In fact, the smaller the network, the smoother the output
scattered waveﬁeld.
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂuence
the work reported in this paper.
Acknowledgments
We thank KAUST and KFUPM for their support, and the seismic wave
analysis group (SWAG) for constructive discussions.
Appendix A. Supplementary data
Supplementary data to this article can be found online at https://do
i.org/10.1016/j.aiig.2021.08.002.
References
Alterman, Z., Karal Jr., F., 1968. Propagation of elastic waves in layered media by ﬁnite
difference methods. Bull. Seismol. Soc. Am. 58, 367–398.
Aminzadeh, F., Burkhard, N., Nicoletis, L., Rocca, F., Wyatt, K., 1994. SEG/EAGE 3-D
modeling project: 2nd update. Lead. Edge 13, 949–952.
Araya-Polo, M., Farris, S., Florez, M., 2019. Deep learning-driven velocity model building
workﬂow. Lead. Edge 38, 872a1–872a9.
Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M., 2018. Automatic
differentiation in machine learning: a survey. J. Mach. Learn. Res. 18.
Claerbout, J.F., 1985. Imaging the Earth's Interior, vol. 1. Blackwell scientiﬁc publications
Oxford.
Cl�ement, F., Kern, M., Rubin, C., 1990. Conjugate gradient type methods for the solution
of the 3D Helmholtz equation. In: Proceedings of the First Copper Mountain
Conference on Iterative Methods.
Courant, R., 1928. On the partial difference equations of mathematical physics. Math.
Ann. 100, 32–74.
Dwivedi, V., Parashar, N., Srinivasan, B., 2021. Distributed learning machines for solving
forward and inverse problems in partial differential equations. Neurocomputing 420,
299–316.
Holm-Jensen, T., Hansen, T.M., 2020. Linear waveform tomography inversion using
machine learning algorithms. Math. Geosci. 52, 31–51.
Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feedforward networks are
universal approximators. Neural Network. 2, 359–366.
Hughes, T.W., Williamson, I.A., Minkov, M., Fan, S., 2019. Wave physics as an analog
recurrent neural network. Science advances 5, eaay6946.
Kissas, G., Yang, Y., Hwuang, E., Witschey, W.R., Detre, J.A., Perdikaris, P., 2020.
Machine learning in cardiovascular ﬂows modeling: predicting arterial blood
pressure from non-invasive 4D ﬂow MRI data using physics-informed neural
networks. Comput. Methods Appl. Mech. Eng. 358, 112623.
Lippmann, B.A., Schwinger, J., 1950. Variational principles for scattering processes. I.
Phys. Rev. 79, 469.
Liu, D.C., Nocedal, J., 1989. On the limited memory BFGS method for large scale
optimization. Math. Program. 45, 503–528.
McFall, K.S., Mahan, J.R., 2009. Artiﬁcial neural network method for solution of
boundary value problems with exact satisfaction of arbitrary boundary conditions.
IEEE Trans. Neural Network. 20, 1221–1233.
Mosser, L., Dubrule, O., Blunt, M.J., 2018. Stochastic reconstruction of an oolitic
limestone by generative adversarial networks. Transport Porous Media 125, 81–103.
Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., Mitliagkas, I.,
2018. A Modern Take on the Bias-Variance Tradeoff in Neural Networks, arXiv
Preprint arXiv:1810.08591.
Ovcharenko, O., Kazei, V., Kalita, M., Peter, D., Alkhalifah, T., 2019. Deep learning for
low-frequency extrapolation from multioffset seismic data. Geophysics 84,
R989–R1001.
Pinkus, A., 1999. Approximation theory of the MLP model. Acta Numerica 1999 88,
143–195.
Pratt, R.G., 1999. Seismic waveform inversion in the frequency domain, Part 1: theory
and veriﬁcation in a physical scale model. Geophysics 64, 888–901.
Raissi, M., Perdikaris, P., Karniadakis, G.E., 2019. Physics-informed neural networks: a
deep learning framework for solving forward and inverse problems involving
nonlinear partial differential equations. J. Comput. Phys. 378, 686–707.
Richards, P.G., Aki, K., 1980. Quantitative Seismology: Theory and Methods, vol. 859.
Freeman, New York.
R€oth, G., Tarantola, A., 1994. Neural networks and inversion of seismic data. J. Geophys.
Res.: Solid Earth 99, 6753–6768.
Sahli Costabal, F., Yang, Y., Perdikaris, P., Hurtado, D.E., Kuhl, E., 2020. Physics-informed
neural networks for cardiac activation mapping. Frontiers in Physics 8, 42.
Sirgue, L., Pratt, R.G., 2004. Efﬁcient waveform inversion and imaging: a strategy for
selecting temporal frequencies. Geophysics 69, 231–248.
Sirgue, L., Etgen, J., Albertin, U., 2008. 3D frequency domain waveform inversion using
time domain ﬁnite difference methods. In: 70th EAGE Conference and Exhibition
Incorporating SPE EUROPEC 2008. European Association of Geoscientists &
Engineers.
Song, C., Alkhalifah, T.A., 2020. Efﬁcient waveﬁeld inversion with outer iterations and
total variation constraint. IEEE Trans. Geosci. Rem. Sens. 58, 5836–5846. https://
doi.org/10.1109/TGRS.2020.2971697.
Song, C., Alkhalifah, T., 2021. Waveﬁeld Reconstruction Inversion via Physics-Informed
Neural Networks, arXiv.
Song, C., Alkhalifah, T., Waheed, U.B., 2021. Solving the frequency-domain acoustic VTI
wave equation using physics-informed neural networks. Geophys. J. Int. 225,
846–859.
Sorteberg, W.E., Garasto, S., Pouplin, A.S., Cantwell, C.D., Bharath, A.A., 2018.
Approximating the Solution to Wave Propagation Using Deep Neural Networks, arXiv
Preprint arXiv:1812.01609.
Van Leeuwen, T., Herrmann, F.J., 2013. Mitigating local minima in full-waveform
inversion by expanding the search space. Geophys. J. Int. 195, 661–667.
Wrona, T., Pan, I., Gawthorpe, R.L., Fossen, H., 2018. Seismic facies analysis using
machine learning. Geophysics 83. O83–O95.
Wu, Z., Alkhalifah, T., 2018a. An efﬁcient Helmholtz solver for acoustic transversely
isotropic media. Geophysics 83, C75–C83.
Wu, Z., Alkhalifah, T., 2018b. A highly accurate ﬁnite-difference method with minimum
dispersion error for solving the helmholtz equation. J. Comput. Phys. 365, 350–361.
Zhang, Z.-D., Alkhalifah, T., 2019. Regularized elastic full-waveform inversion using deep
learning. Geophysics 84, R741–R751.
Zhou, Y., Wu, Y., 2011. Analyses on inﬂuence of training data set to neural network
supervised learning performance. In: Advances in Computer Science, Intelligent
System and Environment. Springer, pp. 19–25.
T. Alkhalifah et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 11–19
19
