Array 19 (2023) 100308
Available online 20 July 2023
2590-0056/© 2023 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/locate/array
Bleeding alert map (BAM): The identification method of the bleeding source
in real organs using datasets made on mimicking organs
Maina Sogabe a,1, Kaoru Ishikawa a,1, Toshihiro Takamatsu b, Koh Takeuchi c, Takahiro Kanno d,
Koji Fujimoto e, Tetsuro Miyazaki a, Toshihiro Kawase f, Toshihiko Sato g, Kenji Kawashima a,∗
a The Department of Information Physics and Computing, The University of Tokyo, Japan
b The department of Exploratory Oncology Research & Clinical Trial Center, National Cancer Center and Research Institute for Biomedical Sciences,Tokyo
University of Science, Japan
c Graduate School of Informatics, Kyoto University, Japan
d Riverfield Inc., Japan
e Real World Data Research and Development, Graduate School of Medicine, Kyoto University, Japan
f Department of Information and Communication Engineering, School of Engineering, Tokyo Denki University, Tokyo, Japan
g The Department of medicine, Fukuoka University, Japan
A R T I C L E
I N F O
Keywords:
Bleeding source estimation
Endoscopic surgery
Pix2Pix
Training data from mimicking organs
A B S T R A C T
In thoraco-laparoscopic surgery, the identification of a bleeding source is recognized as one of the most
important issues with hemostasis during operation. However, previously proposed techniques are only capable
of detecting an approximate bleeding region, not the precise location itself. To develop a system which
can accurately localize a bleeding source, post-bleeding images and their corresponding bleeding source
information may be required. However, to pinpoint bleeding sources from actual thoraco-laparoscopic surgery
images is no easy task even for an experienced surgeon. In previous studies, a surgeon could only provide
rectangular region information around a bleeding source. To address the problem, we have developed a
mimicking device that simulates bleeding from a vessel on an artificial organ for obtaining bleeding images
and precise bleeding source information at the same time. Using this information, we constructed a Generator
that can associate a bleeding image with the corresponding bleeding source by using Pix2Pix and created a
‘‘bleeding alert map (BAM)’’ which concerns the Predicted intensity of bleeding source in the endoscopic view.
The Generator successfully created BAMs from ex vivo lung bleeding images as well as actual organ bleeding
images captured in thoracoscopic surgeries. The results showed that the BAM Generator constructed only by
using the data from the mimicking device was effective in processing bleeding images from actual organs to
identify bleeding sources. The proposed system may be utilized during endoscopic surgery to present a BAM
which carries important information for hemostasis.
1. Introduction
With the advancement of endoscopy, thoraco-laparoscopic surgery
that enables minimally invasive operation is steadily becoming popular,
replacing laparotomies and thoracotomies [1–7]. One of the long-
standing problems with thoraco-laparoscopic surgery is the treatment
for bleeding in the abdominal cavity [8–11]. Laparoscopic surgery is
performed in a limited field of view and light source, so that the
surgeon has to concentrate on the visual information in the center
region of the device screen where he/she manipulates a surgical in-
strument [12,13]. Thus, a response to events such as bleeding occurring
near the screen boundary tends to be delayed, leading to unnecessary
∗ Corresponding author.
E-mail address: kkawa729@g.ecc.u-tokyo.ac.jp (K. Kawashima).
1 Equally contribution in this paper.
prolonged hemorrhage. Moreover, bleeding may obscure the surgical
field view, decreasing visibility and complicating the surgical opera-
tion. Of course, bleeding itself causes damage to tissues, increasing
the burden on patients, so that monitoring the extent of bleeding is
extremely important for the judgment of whether or not an endoscopic
surgery should be suspended [14,15].
In the field of wireless capsule endoscopy technique, there are sev-
eral researches that develop systems for automatic bleeding detection
from endoscopic images. For example, Jia and Meng [16] demonstrates
an approach to bleeding detection based on segmentation of bleeding
regions using deep learning. In addition, Caroppo et al. [17] proposed
https://doi.org/10.1016/j.array.2023.100308
Received 14 March 2023; Received in revised form 1 July 2023; Accepted 17 July 2023
Array 19 (2023) 100308
2
M. Sogabe et al.
a bleeding region detection method using transition learning that fuses
the minimum redundancy maximum relevance method and different
fusion rules.
Meanwhile, there are not many studies that utilize thoraco-laparo
scopic images. Regarding bleed detection, Okamoto et al. [18] devel-
oped a method that uses the combination of RGB values and HSV
values of images as input to a support vector machine to categorize
the presence/absence of bleeding. Garcia-Martinez et al. [19] proposed
a technique using RGB information from a single endoscopic image.
By comparing and analyzing the ratio of red/blue and the ratio of
red/green with the average pixel value throughout the operation as
the threshold value, the bleeding area and the background area are
separated to identify the existence of bleeding. The technique is partic-
ularly effective for large-scale hemorrhage. There are also researches
on a framework for estimating the bleeding area in more detail, where
a bounding box labeled by the surgeon is used for training data.
Among them, the estimation accuracy for a bleeding source and
bleeding area has been improved by using spatio-temporal information
of the site obtained from time-series image data [20]. However, if
the surgeon moves the camera during surgery, the position of the
bleeding source may not be constant, and it may be difficult to apply
this technique at such moments. Moreover, surrounding tissues may be
constantly deformed by the surgical instrument. If information from an
image at a single time point allows accurate estimation, it would greatly
contribute to the development of a safer hemostasis support system.
It should be also noted that hemostasis may be appropriately per-
formed by applying a certain concentrated pressure to the bleeding
site, so that identifying a bleeding source point, rather than a region,
enables more effective hemostasis, avoiding unnecessary damage to the
peripheral tissues [21]. However, when given as a point information,
the possibility is greater that the specified site is deviated from a true
bleeding source; in such cases, it is necessary to immediately move to
a next bleeding source candidate and resume hemostasis.
For this purpose, it would be more advantageous and informative
to provide the bleeding source candidate as an alert map, which shows
information on the intensity related with probability of being the
bleeding source for each point in the region, rather than providing the
candidate as a flat region.
When applying AI techniques such as deep learning to image pro-
cessing, crucial part is to obtain images suitable for the purpose of
a problem and their correct labels. In order to identify bleeding and
estimate its source, a pair set of a bleeding image and its corresponding
bleeding source location is required, but such pair is difficult to obtain
from actual surgical images. The issue was how to acquire training data
for which the bleeding starting point is accurately known. We might use
animal organs to obtain bleeding data, but the cost is high and there is
also an ethical problem from the viewpoint of the principles of the 3Rs
(Replacement, Reduction and Refinement) when using animal organs
[22].
To solve these problems, we have developed a platform for bleeding
source estimation which utilizes a newly created device, made up of
silicone and agar, that can imitate bleeding behavior of a living organ.
As we can arbitrarily position a bleeding source on the mimicking
organ, it is easy to generate training data which contain clear corre-
spondence between bleeding sources and subsequent bleeding images.
From these data, we create a so-called bleeding alert map (BAM)
that gives information about the probability of each point of being a
bleeding source.
To generate the BAM, Pix2Pix, one of the Image to Image translation
methods, was used for validation along with the generation of the
alert map.Pix2Pix, which was proposed by Isola et al. [23] , is a
technology that realizes image-to-image translation such as generating
an actual building image from its segmentation image and coloring an
edge image. It is a kind of conditional GAN (Generative Adversarial
Network). GAN is a network called a generative model that learns
training data and generates new data similar to them. GAN consists
of two networks, a Generator model and Discriminator model. The
Generator outputs a new composite image and the Discriminator model
determines whether the image is a genuine one from the original data
set or a fake one generated by the Generator. The Generator is trained
so that the generated image may look like the genuine image to ‘‘fool’’
the Discriminator, while the Discriminator is trained so that it may
correctly distinguish the genuine data and false data. At the end of
training, the Generator may be able to produce an image similar to
the training data.
We apply the trained Generator, which has been constructed by
Pix2Pix using the bleeding image and source pairs from the mimicking
device, to actual lung bleeding images as well as endoscopic images
of organs obtained during actual clinical surgeries, and evaluate the
performance of the Generator if it is capable of creating appropriate
BAMs.
The contents of this paper are as follows: In Section 2, we describe
our method for creating datasets with bleeding source information
using a developed mimicking organ and how we use pix2pix to generate
BAMs from the datasets. Section 3 describes the results of generated
BAMs from bleeding images obtained from both the mimicking and
actual organ in vivo. In Section 4, we discuss this technology’s potential
future applications and improvements, also addressing potential limi-
tations and proposed solutions. Finally, in Section 5, we conclude the
paper.
2. Materials and methods
The overview of the proposed method is shown in Fig. 1. The
proposed method consists of three phases. The first phase involves ex-
plaining a mimicking organ device capable of acquiring bleeding source
information and creating a pair of Ground truth BAM information and
bleeding images from the obtained information. In the second phase,
a Generator, which generates a BAM using Pix2Pix, is trained based
on the dataset from the first phase. Finally, the third phase involves
the generation of a BAM by inputting actual bleeding images into
the Generator trained in the second phase. The following subsection
provides detailed descriptions of the mimicking device and the process
of BAM generation.
2.1. Simulated vascular perfusion device
Fig. 2 shows a schematic diagram of the developed simulated blood
vessel perfusion device (mimicking device). The device consists of
a blood flow reproduction part and an organ part. The blood flow
reproduction part comprises a peristaltic pump (MP-3N, Eyela Tokyo
rikakikai Co., LTD., Tokyo, Japan), a 5 mm diameter connecting silicon
tube (ARAM Corporation, Osaka, Japan), and a 3 mm diameter silicon
tube embedded in the pseudo organ (AS ONE Corporation, Osaka,
Japan), with perfusion performed at 0.3–0.5 mm/s. The simulated
blood used was an aqueous solution of glycerin (Wakenyaku CO., LTD.,
Kyoto, Japan) (75% v/v) colored with acrylic paint. The concentration
of glycerin was adjusted to be close to the spread of blood in vivo.
The organ part includes a silicon base that imitates the surface of an
organ and a coating layer on the surface made of 3% agar (Hayashi
Pure Chemical IND., LTD., Japan), between which a silicon tube ‘‘blood
vessel’’ is embedded. ‘‘Bleeding’’ was caused by puncturing the surface
of the silicone tube with an 18G needle. Movie 1 shows a video of
simulated bleeding for the device.
2.2. Preparation of training and validation image sets using the mimicking
device
Images were taken with a handheld USB microscope and captured
in a 1280 × 720 RGB, MPEG4 format video (20 fps). Leica CLS 150X
(Leica Microsystems, Wetzlar, Germany) was employed as the light
source, and the surroundings were covered with a blackout curtain at
Array 19 (2023) 100308
3
M. Sogabe et al.
Fig. 1. Overview of making datasets phase and generating BAMs phase using Pix2Pix. Generator generates a BAM from RGB image input obtained from the mimicking device,
and Discriminator determines the BAM is fake or true. Generator and Discriminator are optimized through hostile learning by using the BAMs created by the Generator and the
ground truth BAMs. In the generating BAM phase, the BAM can be generated with inputs such as bleeding images from real organs. The BAMs are displayed in pseudo-color.
the time of imaging to reproduce the environment similar to that of an
abdominal cavity.
In one experiment, the bleeding source position coordinate infor-
mation and a total of 4 frames (one frame at the start of bleeding,
one last frame when the image became stationary, 2 random frames
in between the two frames) were collected. For each frame, 7 random
rotations, enlargements and reductions, inversions were performed as
data augmentation. The size of training/test images is 256 × 256 RGB,
and a total of 3535 images were prepared to be used as training data.
For test data, 200 pairs of input images and the ground truth BAMs
were used, obtained from a different pattern of the bleeding mimicking
device than that used for training. Additionally, to examine the changes
in the BAM intensity at the bleeding source over time, ten series of
video data were utilized for validation.
2.3. Acquisition of simulated bleeding images from a porcine lung by using
two types of camera devices
Simulated blood was circulated to the intratissue vein of an excised
porcine lung (10-months-old female, Shibaura organ, Tokyo, Japan) to
investigate whether the developed method is applicable to images from
a blood vessel running on the actual organ surface. A total of 10 images
were obtained using a handheld microscope. Separate experiments
were performed with the KARL STORZ Image1 D3-Link camera system
and TIPCAM1 telescope (30 fps, 1920 × 1080 RGB, KARL STORZ SE &
Co. KG, Tuttlingen, Germany) as an image capturing device to collect
another 10 images.
2.4. Acquisition of real bleeding images from laparotomy and thoracoscopy
in vivo
We also collected actual bleeding images during laparotomy and
endoscopic surgeries on living organs.
For the laparotomy images, we employed the video data from
laparotomy operation on a porcine (Zenno-premium-pig, 2.5-months-
old male, KAC Co., Ltd., Kyoto, Japan). The video data was obtained
for verification on hemostasis [24,25]. All procedures and protocols
were approved by the Animal Care and Use Committee of the Na-
tional Cancer Center (authorization number: K19-007). The images
were obtained using SONY FDR-AX55 (30 fps, 1920 × 1080 RGB, Sony
Array 19 (2023) 100308
4
M. Sogabe et al.
Fig. 2. Schematics of the simulated vascular perfusion device. (a): Overview of the device. Major components are a mimicking organ, a bloodstock container, and a circulation
device. The right side figure shows a magnified view of the mimicking organ part when simulated bleeding is occurring. (b): Details of the mimicking organ. The mimicking organ
consists of a colored silicon part that imitates the surface of an organ with a 3% agar layer that reproduces the epithelial feeling of an organ surface, and a silicon tube that
imitates a blood vessel installed between the agar and silicon. Bleeding is initiated by puncturing the tube with a needle. (For interpretation of the references to color in this
figure legend, the reader is referred to the web version of this article.)
Corporation, Tokyo, Japan). The organs to be imaged were a stomach,
small intestine, and spleen, where bleeding was caused by surgical
scissors, and the images were captured immediately after bleeding had
started.
For the endoscopic surgery images, we employed the video data
from a thoracoscopic operation on another porcine (LW species, female,
4-months-old female, Science Bleeding Co., Ltd., Chiba, Japan). The
data was also obtained during the operational check of a surgical robot,
where the research was approved by the Animal Research Committee
of Tokyo Medical and Dental University (authorization number: A2020-
061 A). Images were acquired using the KARL STORZ IMAGE1S camera
system and 3D TIPCAM1 SPIES video telescope (60 fps, 1920 × 1080
RGB). The organ to be imaged was a lung. In this case study, there is
no information on the bleeding source, because the images were from
an actual surgery removing one of the lungs, where the bleeding source
location was set as a rectangular region and labeled by surgeons and
veterinarians.
2.5. Ground truth BAMs for training
The coordinates of the bleeding source can be easily obtained
because it is possible to puncture the blood vessels of the mimicking
organ with a needle at an arbitrary position here. The grand truth
BAMs were defined from that information as a two-dimensional normal
distribution centered on the bleeding source with a maximum value
equal to 1:
𝑓(̂𝑥, 𝜇, 𝛴) = exp
(
−1
2 (̂𝑥 − 𝜇)𝑇 𝛴−1(̂𝑥 − 𝜇)
)
(1)
where 𝜇 is the known coordinate point of the bleeding source, and 𝛴
is the covariance matrix. In other words, the BAM intensity attenuates
with distance from the bleeding source. In this study, 𝜎 was chosen
as 𝜎2𝐼, where 𝐼 is a 2 × 2 identity matrix, and the parameter 𝜎 was
determined so that the circular region of the distribution with intensity
larger than 0.1 may cover approximately 5 times the diameter of the
silicon tube blood vessel. In particular, the diameter is 25 mm as the
real distance.
2.6. Construction of a BAM generator using Pix2Pix
Pix2Pix [23] was applied to construct a BAM Generator. The bleed-
ing images used for input and the ground truth BAMs were originally
1280 × 720 × 3 images, which then were scaled, rotated, and horizon-
tally and vertically shifted, to be converted into cropped 256 × 256 × 3
images for training input, with its details shown in the middle row
of Fig. 1. First, we define 𝐺 as a Generator that translates bleeding
images into BAMs. U-Net [26] was used for the architecture of the
Generator network. The encoder block includes a convolutional layer,
a batch normalization layer, and a Leaky ReLU layer. In the encoder
block, convolution was performed with 4 × 4 kernel and stride 2. The
𝛼 value for the Leaky ReLU was set to 0.2. Tanh was used as the
activation function for outputting the final output image. The decoder
block consists of a transposed convolutional layer, a dropout layer, and
a ReLU layer. The encoder and decoder blocks were connected by skip
connections. The dropout rate for the dropout layer was set to 0.5.
Also, we also defined 𝐷 as a Discriminator, which distinguishes the
generated image by the Generator and the ground truth BAM. Patch
GAN was employed for the Discriminator network architecture. Each
block consists of a convolutional layer, a batch normalization layer,
and a Leaky ReLU layer (𝛼 = 0.2). Patch GAN determines whether the
effective receptive field is genuine or fake, with its patch size set to
Array 19 (2023) 100308
5
M. Sogabe et al.
Fig. 3. The average of validation and training SSIM index in each epoch. Red and Green line indicates the case where 𝐿1 only and Pix2Pix, respectively.
70 × 70 in this study. The architecture of the Pix2Pix model used for
BAM generation in this study is shown in Supplementary Fig. 1.
The hostile loss functions of the networks are defined as follows.
𝑐𝐺𝐴𝑁(𝐺, 𝐷) =E𝑥,𝑦[log 𝐷(𝑥, 𝑦)]+
E𝑥,𝑧[log (1 − 𝐷(𝑥, 𝐺(𝑥, 𝑧))],
(2)
𝐿1(𝐺) = E𝑥,𝑦,𝑧
[‖𝑦 − 𝐺(𝑥, 𝑧)‖1
] .
(3)
Therefore, the final objective is
𝐺∗ = arg min
𝐺 max
𝐷 𝑐𝐺𝐴𝑁(𝐺, 𝐷) + 𝜆𝐿1(𝐺),
(4)
where the Discriminator 𝐷 learns to maximize , and the Generator
𝐺 learns to minimize . The Generator also considers the following
𝐿1 loss so as not to generate an extreme image ignoring the dataset
images. In this method, 𝜆 was set to 100. The results of learning with
𝐿1 loss only (𝐿1 only) were compared with the Pix2Pix to verify the
effectiveness of the Discriminator.
The neural network weights were initialized as a normal distribution
with a mean of 1 and a standard deviation of 0.02, while the bias term
was initialized to 0. Examples of an input image, Ground truth BAMs,
generated BAMs, and the outputs derived from the Discriminator are
shown in Supplementary Fig. 2. Adam optimizer with hyperparameters
𝛽1 = 0.5 and 𝛽2 = 0.999 was applied in the training. For the U-Net
Generator, the networks were trained through 100 epochs with the
learning rate 0.0002. The calculations were implemented on MATLAB
2021a (Mathworks, MA, USA) and it took 60 h for 200 epochs with
Nvidia RTX2080 Ti.
For validation, three new image datasets, each consisting of 50
mimetic hemorrhage scene were used for evaluation, and the number
of epochs used for evaluation was determined from the average. To
evaluate the validation data, the structural similarity (SSIM) index [27]
was used to determine the similarity between the created BAM images
and the pre-defined ground truth BAM images (Fig. 3). Another 200
imitation image data were employed for the test data. Furthermore,
in order to evaluate the generalization performance and verify the
adaptability of the Generator trained with the mimic organ to the
bleeding images from actual living organs, a total of 40 image sets
using the actual excised organ as well as intraoperative images acquired
under different conditions were utilized.
By using the Generator 𝐺 with the learned parameters, BAMs were
generated for the test data obtained from the simulated organ and for
other bleeding images from the actual organs. The final output BAMs
are obtained as 256 × 256 RGB images converted to 8-bit grayscale
images. The running time for creating the BAM was 0.098 s.
2.7. Image analysis and statistics
For the evaluation of the generated BAMs, the intensity value of
the BAM at the actual bleeding source is calculated. The threshold
value was set to 0.1, and the generation of the BAM was considered
as successful when the intensity was larger than the threshold. Two
kinds of evaluations were performed regarding the generation of the
alert map. One is the evaluation of the judgment accuracy of bleeding
state. The other is the evaluation of the extent to which the area of
the generated alert map overlaps with that of the BAM generated from
the ground truth or the bounding circle specified by humans, and how
much of the areas do not overlap. As for the accuracy of the bleeding
position estimation, the threshold of 0.1 was employed to determine if
a bleeding source exits. If the source actually existed, it was judged that
an alert map covering the bleeding position was successfully generated.
In order to verify the performance of the proposed method, the
accuracy rate, precision rate, and recall rate are introduced as follows.
accuracy rate =
𝑇 𝑃 + 𝑇 𝑁
𝑇 𝑃 + 𝑇 𝑁 + 𝐹𝑃 + 𝐹𝑁 ,
(5)
precision rate =
𝑇 𝑃
𝑇 𝑃 + 𝐹𝑃 ,
(6)
recall rate =
𝑇 𝑃
𝑇 𝑃 + 𝐹𝑁 ,
(7)
F1 score = 2 × recall rate × precision rate
recall rate + precision rate
,
(8)
where true positive is denoted as 𝑇 𝑃, false positive as 𝐹𝑃 , true negative
as 𝑇 𝑁, and false negative as 𝐹𝑁.
To generate the ROC (Receiver Operating Characteristic) curve, the
true positive rate and the false positive rate were calculated as follows:
true positive rate =
𝑇 𝑃
𝑇 𝑃 + 𝐹𝑁 ,
(9)
false positive rate =
𝐹𝑃
𝑇 𝑁 + 𝐹𝑃 ,
(10)
Array 19 (2023) 100308
6
M. Sogabe et al.
Fig. 4. Overview of the CAR and ICAR evaluations. The CAR indicates the ratio of generated BAMs to areas of ground truth BAMs above a threshold or areas labeled by humans;
the ICAR indicates the ratio of BAMs present outside the correct region.
Fig. 5. BAM generation from test data using 𝐿1 only and Pix2Pix. The left side image is the actual input image, and the middle image is the BAM created by 𝐿1 only method,
and the right side image is created by Pix2Pix. The white circular region indicates where the actual bleeding source was labeled as the ground truth. The intensity of the BAM
follows the color bar at the bottom center (scale bar= 5 mm). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this
article.)
Array 19 (2023) 100308
7
M. Sogabe et al.
Fig. 6. Accuracy of the bleeding source identification by the BAM. The accuracy is shown as a confusion matrix that summarizes the evaluation result for the 200 test data using
𝐿1 only and Pix2Pix. The color bar shows the number of elements. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of
this article.)
Fig. 7. The results of ROC curve using Pix2Pix and 𝐿1 only.
The ROC curve was generated by changing the display threshold of the
BAMs in increments of 0.1 from 0 to 1.
The CAR (correct alert rate) and ICAR (incorrect alert rate) are
indicators similar to the IoU in the object recognition area, but here
we use the different names to prevent confusion and show that they are
specifically defined for alert maps (Fig. 4). With the region inside the
ground truth alert map (intensity ≥ 0.1) or the bounding circle labeled
by humans defined as 𝐼𝑅 (inside region), the region outside it defined
as 𝑂𝑅 (outside region), and the number of pixels whose BAM intensity
is ≥ 0.1 as 𝐵𝐴𝑀𝑅 (BAM region), CAR and ICAR are obtained as
CAR = 𝑝𝑖𝑥𝑒𝑙(𝐼𝑅 ∩ 𝐵𝐴𝑀𝑅)
𝑝𝑖𝑥𝑒𝑙(𝐼𝑅)
∗ 100
(11)
ICAR = 𝑝𝑖𝑥𝑒𝑙(𝑂𝑅 ∩ 𝐵𝐴𝑀𝑅)
𝑝𝑖𝑥𝑒𝑙(𝑂𝑅)
∗ 100.
(12)
3. Results
3.1. Generation of BAMs for the test data from the mimicking device
Fig. 5 shows some of the test images used for the input to the
Generator and the resulting BAMs. Fig. 6 shows the identification
result of with-bleeding and without-bleeding sources using 𝐿1 only
and Pix2Pix on 200 test data. With the group of 𝐿1 only, there are
many cases of misrecognition where non-bleeding regions were often
recognized as bleeding regions, with the accuracy rate being 77.0%,
while the accuracy was improved to 90.5% for the BAM generation
using Pix2pix. The recall rates were about the same (93.3% and 94.0%)
for both cases. The precision rate was 79.5% for L1 only, while it was
higher at 93.4% for Pix2Pix. The F1 score was 0.94 when using Pix2Pix,
Table 1
The results of CAR and ICAR value in mimicking device.
Methods
CAR (%)
ICAR (%)
𝐿1 only
55.3
0.3
pix2pix
60.9*
0.4
n = 200.
*p value < 0.05.
and 0.85 with L1 only. Additionally, a ROC curve was generated from
the false positive rate/true positive rate of the generated BAM when the
display threshold of BAM was varied in steps of 0.1 from 0 to 1, and an
evaluation of AUC (Area Under the Curve) was conducted as well. The
result is shown in Fig. 7. The AUC of the generated BAM using Pix2Pix
was 0.91, whereas it was 0.78 in the case of L1 only. This result suggests
the effectiveness of the Discriminator in BAM generation.
Table 1 shows the CAR and ICAR results of the bleeding map
generation areas. The CAR values increased significantly compared to
the case of 𝐿1 only, and the average was about 55.3% in the case of 𝐿1
only, whereas it was about 60.9% in the case of Pix2Pix. No significant
difference was found for the ICAR values.
Images at the early stages of bleeding also yield low intensities
values. To account for these poor performances, we investigated how
the intensities of BAMs at the bleeding sources change depending on
the elapsed time from the start of bleeding. Fig. 8 shows the intensity
values created from the start and spread of bleeding for 10 cases of
video data. The intensities tend to be lower in the initial part.
3.2. Generation of BAMs from the porcine lung images obtained with the
handheld camera ex vivo
Using the Generator constructed from the data for the mimicking
device, we generated BAMs from the actual lung bleeding images. The
imaging condition was the same as that for the mimicking device.
Examples of the bleeding images and created BAMs are shown in Fig. 9.
Even with the use of the commodity camera, it was mostly possible to
generate correct BAMs. The CAR values were both about 80% (81% and
81.4% respectively). In both cases using 𝐿1 only and Pix2Pix methods,
the judgment was successful 9 out of 10 cases. In the one unsuccessful
example, there was a bleeding source outside the area, but an alert map
was generated within the imaging region.
3.3. Generation of BAMs from the porcine lung endoscopic images ex vivo
For similar excised organs, we also verified the results for the
images obtained by a clinical endoscope which is different from the
imaging device used for training. Additional difference from the images
used for training is in that the images are from actual organs and
they are obtained through an endoscope. The generated BAM image
is shown in Fig. 10. The judgment accuracy in the case of 𝐿1 only
Array 19 (2023) 100308
8
M. Sogabe et al.
Fig. 8. Time variation in the intensity of the BAM at the bleeding source from the onset of bleeding using Pix2Pix. BAMs are generated for all video images from the start of
bleeding to the 200th frame, which were acquired using the mimicking device. The time transition of the intensity of the BAM at the actual bleeding source is shown (𝑛 = 10,
error bar: the maximum and minimum intensity).
Table 2
The results of CAR and ICAR value in ex vivo and in vivo samples.
CAR (%)
Methods
Excised lung
Excised lung
Laparotomy (n = 10)
Thoracoscopic
+ USB camera (n = 10)
+ Endoscope (n = 10)
surgery (n = 10)
𝐿1 only
81.0
9.1
56.6
34.1
pix2pix
81.4
58.9*
63.6
71.0*
ICAR (%)
Methods
Excised lung
Excised lung
Laparotomy
Thoracoscopic
+ USB camera
+ Endoscope
surgery
𝐿1 only
5.8
5.4
7.7
8.4
pix2pix
3.6
4.3
9.2
12.2
*p value < 0.05 compared to each group.
was considerably deteriorated, with successful judgment being only
4 cases out of 10. On the other hand, in the case of Pix2Pix, the
judgment was successful in 8 cases out of 10. The CAR was as low as
9.1% for 𝐿1 only, while it was 58.9% for Pix2Pix. The ICAR values
were 5.4% and 4.3% respectively. In particular, with Pix2Pix, a BAM
was successfully created immediately after bleeding started, where the
amount of bleeding was still very small.
3.4. Generation of BAMs from the surgical images using porcine organs in
vivo
We made further verification of the generation accuracy for the
images obtained from actual organs of a living pig instead of excised
organs. Here, the verification was performed by using the camera
images (Fig. 10) during laparotomy and the endoscopic images (Fig. 11)
during thoracoscopic surgery.
In open surgery (Fig. 11), bleeding point information is available
because the surgeon actually incised the organ to cause bleeding. In
addition, the bleeding area was labeled by humans. The estimation
of the bleeding position was successful in 8 out of 10 cases for both
𝐿1 only and Pix2Pix. The CAR values using 𝐿1 only and Pix2Pix
were 56.6% and 63.6%, and the ICAR values were 7.7% and 9.2%,
respectively.
Furthermore, we verified the accuracy of BAM generation from
images during endoscopic surgery. Since it is difficult to obtain bleeding
source point information in this example, CAR and ICAR were verified
in terms of their meaning based on the labeling information. The CAR
value was as small as 34.1% when 𝐿1 only was used, whereas it
was 71.0% when the Discriminator was used, showing the significant
increase in the accuracy of alert map generation. As for ICAR, the values
using 𝐿1 only and Pix2Pix were 8.4% and 12.2% respectively; the value
for Pix2Pix is slightly higher, but there was no significant difference.
When there was no bleeding, the generated BAM did not give any areas
with bleeding source probability. Furthermore, in one case where blood
is present but hemostasis is already applied and there is no bleeding,
the generated BAM presented no bleeding source.
For the images from the thoracoscopic surgeries shown in Fig. 12.
When there was no bleeding, the generated BAM did not give any areas
with bleeding source probability. Furthermore, in one case where blood
is present but hemostasis is already applied and there is no bleeding,
the generated BAM presented no bleeding source.
4. Discussion
By using the BAM which is generated by the Generator constructed
with the help of Pix2Pix, it has become possible to obtain probability
information on a bleeding source on the endoscopic screen, while the
conventional approaches are only capable of determining a segmented
area where the bleeding source may exist. The accuracy at which a
BAM could be correctly generated was 90.5%. Moreover, it has become
Array 19 (2023) 100308
9
M. Sogabe et al.
Fig. 9. BAM generation for the bleeding images from the excised porcine lung (a handheld microscope) using 𝐿1 only and Pix2Pix. The images were acquired under the same
imaging conditions as the mimicking device. Blood was pumped into a blood vessel cannulated into the lung, and bleeding was caused by making a hole in the vessel by a needle.
The left column is the actual input image, the center column is the generated BAM, and the right column is the merged image. The arrows indicate the actual bleeding point.
The white circular region indicates where the actual bleeding source was labeled as the ground truth. The intensity of the BAM is shown according to the color bar at the lower
right (scale bar = 5 mm). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
possible to estimate a bleeding source location from a single endoscopic
image alone.
Table 2 summarizes the CAR and ICAR values when real tissues
were used. From this result, it was suggested that a robust method
is realized with the help of the Discriminator that does not impair
the accuracy of bleeding source position estimation even in different
imaging environment, although the generated range covered by the
alert map tends to be slightly wider.
However, there are several points to consider regarding the pro-
posed method. One of the concerns concerns the design of the ground
truth BAMs. The developed system displays the area with a bleeding
source as an intensity map of 0–1 instead of presenting an estimated
bleeding source as a coordinate point. The size of the BAM generated
in this study was about 0.5 to 2.0 cm. Since the equipment used for
hemostasis is generally larger than this size, it should be possible to
put the equipment accurately at the location specified by the BAM
with white color showing the highest intensity. However, further study
is needed on the design of the ground truth BAMs to achieve BAM
visualization for detecting more accurate bleeding source that can
withstand clinical use. In addition, when used by surgeons, the optimal
BAM intensity display and display range should be further designed
with an eye toward hemostatic accuracy and display preference.
The advantage of this proposed method is that does not require
preparing learning datasets acquired with a medical endoscope in vivo
or ex vivo from actual organs. Instead, by using the artificial organ sys-
tem and a commodity USB camera, it was possible to prepare datasets
for the training of a Generator capable of producing reliable BAMs.
As a result, we obtained accurate information on the bleeding source
location, which is difficult to acquire during surgery. The Generator
was then applied to the actual organs’ bleeding images obtained ex
vivo and in vivo by thoracoscope and endoscope to generate BAMs,
which successfully identified correct bleeding sources. These results
suggest that it is possible to develop a method that may adapt to actual
intraoperative images by using datasets obtained by the mimicking
device, which is cost-effective and conforms to the principles of the
3Rs.
Examples of failed BAM generation include the cases immediately
after the start of bleeding, the cases where the bleeding source is
Array 19 (2023) 100308
10
M. Sogabe et al.
Fig. 10. BAM generation for the bleeding images from the excised porcine lung (a clinically used endoscope) using 𝐿1 only and Pix2Pix. The left column is the actual input
image, the middle column is the generated BAM using 𝐿1 only, and the right column is the generated BAM using Pix2Pix. The arrows indicate the actual bleeding points. The
white circular region indicates where the actual bleeding source was labeled as the ground truth. The intensity of the bleeding alert map at the bleeding point is shown according
to the color bar in the lower right (Scale bar = 5 mm). (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
outside the image, and the case where there is a groove structure blood
flows in, which could not be expressed by the mimic device In most
of these cases, the data was obtained immediately after the initiation
of bleeding. However, the intensity of the BAM at the bleeding source
approached high intensity (>0.8) already after 50 frames (about 1.5 s)
on average, which implies that hemostasis treatment could be initiated
sufficiently early after the start of bleeding. In actual surgical situations,
it has also been difficult to recognize the existence of bleeding before it
has progressed to some extent by humans. So this method’s capability
of detecting bleeding and locating its source within about 1.5 s is
considered a significant improvement. However, this is the case for
the 256 × 256 RGB image used to generate the BAM. In this day
and age, endoscopes are becoming higher resolution, and endoscopic
surgeries using 4 K and 8 K endoscopes are on the rise. In this case, the
calculation time is expected to be longer, and it is necessary to consider
algorithm acceleration to obtain real-time performance. In addition,
when VR/AR is used for BAM information, there are issues such as
whether rendering and information presentation should be 2D or 3D,
and in such cases, speeding up the computation will be essential.
In this paper, Pix2Pix was employed as an image-to-image trans-
lation method for the verification of generated BAMs. BAMs, which
have been shown to provide richer information than the conventional
Array 19 (2023) 100308
11
M. Sogabe et al.
Fig. 11. BAM generation for the images obtained from actual pig laparotomy using 𝐿1 only and Pix2Pix. The left column is the actual input image, the middle column is the
generated BAM using 𝐿1 only, and the right column is the generated BAM using Pix2Pix. The arrow indicates the actual bleeding source. The white circular region indicates where
the actual bleeding source was labeled as the ground truth. The imaging system used was a SONY FDR-AX55. The input image to the Generator was a 256 × 256 RGB image, in
which the organ part was cropped out of the original and reduced by 0.3 to 0.5 times so that the resulting image may be close to the scale of the images used for the training.
The intensity of the BAM is shown according to the color bar at the bottom right. (For interpretation of the references to color in this figure legend, the reader is referred to the
web version of this article.)
method, will be improved for higher accuracy and visibility by using a
more state of the art image-to-image translation method in the future.
There are also several limitations of the proposed method. Fur-
thermore, the method was developed by assuming bleeding occurs in
blood vessels exposed on the tissue surface, because such vessels easily
come into contact with surgical instruments. However, the current
mimicking device does not assume a complicated shape and cannot
provide depth information, so that bleeding that may occur at blood
vessels running behind the organ or hidden in the back cannot be
accounted for. As future work, a mimicking device incorporating a
variety of features in the depth direction may be developed to further
improve the performance of the system.
The proposed method also proposes a new provision method of
bleeding state information called BAM. Since it differs from existing
bleeding areas and labeling problems, it cannot be evaluated by direct
image comparison. It is necessary to re-evaluate it in a clinical environ-
ment with scores such as improved hemostasis accuracy to clarify the
effectiveness of the proposed method in the future.
A BAM an alert map that not only shows the location of a bleeding
area but also has the expectation information of a bleeding source.
Such information can be applied to Context-aware Augmented Reality
in laparoscopic surgery [28,29], although issues such as real-time and
clear visualization need to be considered. Furthermore, the ability to ac-
quire detailed location information of a bleeding source would allow its
application to hemostasis assistance under robot-assisted laparoscopic
surgery as well as autonomous hemostasis.
5. Conclusion
In this paper, a simulated vascular perfusion system (mimicking
device) was developed, which is capable of generating pair informa-
tion of a post-bleeding image and a precise bleeding source location.
Such information pairs were used as the input to Pix2Pix learning to
construct a Generator that produces a ‘‘bleeding alert map (BAM)’’
presenting a location of the bleeding source as spatial distribution of
intensity related to probability. By using the Generator created from
the mimicking device data, BAMs for actual organs bleeding images
were successfully generated. With the spread of thoraco-laparoscopic
surgery, there is an increasing demand for an image processing system
which is capable of recognizing intraoperative bleeding and estimating
bleeding sources. This study has shown for the first time that the Gener-
ator created by using only the datasets from the mimicking device can
reliably identify bleeding sources from actual bleeding organ images.
This demonstrated that deep learning technology can be effectively
applied to medical image processing where it is difficult to collect
detailed learning datasets.
In the future, by adding more complexity to the mimicking device
so that we may collect data for more difficult situations where bleeding
comes from a deep layer of an organ which is outside the field view,
we will develop a more advanced system which can accommodate
such difficult circumstances. Furthermore, a safer hemostasis support
system will be completed by combining this method with an estimation
technique that uses time-series information.
Array 19 (2023) 100308
12
M. Sogabe et al.
Fig. 12. BAM generation for the images obtained from actual one-lungectomy using a thoracoscope. The left column is the actual input image, the middle column is the generated
BAM using 𝐿1 only, and the right column is the generated BAM using Pix2Pix. The white circular region indicates where the actual bleeding source was labeled as the ground
truth. The intensity of the BAM is shown according to the color bar at the bottom right. (For interpretation of the references to color in this figure legend, the reader is referred
to the web version of this article.)
CRediT authorship contribution statement
Maina Sogabe: Conceptualization, Methodology, Investigation, Data
Curation, Resources, Software, Writing- Original draft preparation,
Writing- Reviewing & Editing, Visualization, Funding acquisition. Kaoru
Ishikawa: Investigation, Data Curation, Resources, Writing- Original
draft preparation, Writing- Reviewing & Editing. Toshihiro Taka-
matsu: Data Curation, Resources. Koh Takeuchi: Software, Formal
analysis. Takahiro Kanno: Data Curation, Writing- Reviewing & Edit-
ing . Koji Fujimoto: Data Curation, Formal analysis. Tetsuro Miyazaki:
Data Curation, Writing- Reviewing & Editing. Toshihiro Kawase: Data
Curation. Toshihiko Sato: Resources. Kenji Kawashima: Supervision,
Project administration, Writing- Original draft preparation, Writing-
Reviewing & Editing.
Declaration of competing interest
The authors declare the following financial interests/personal re-
lationships which may be considered as potential competing inter-
ests: Kenji Kawashima reports was provided by The University of
Tokyo. Kenji Kawashima reports a relationship with Riverfield Inc. that
includes: equity or stocks.
Data availability
Data will be made available on request.
Acknowledgments
M. Sogabe is grateful to Dr. H. Sogabe, Dr. S. Uchida and Dr. N. Ito
for suggesting the topic covered in this paper and thanks Dr. H. Seki for
editing and proofreading this paper. This work was supported in part
by JSPS KAKENHI Grant Number 21K18074 (Japan).
Appendix A. Supplementary data
Supplementary material related to this article can be found online
at https://doi.org/10.1016/j.array.2023.100308.
Array 19 (2023) 100308
13
M. Sogabe et al.
References
[1] Bateman BG, Kolp LA, Mills S. Endoscopic versus laparotomy management of
endometriomas. Fertil Steril 1994;62:690–5. http://dx.doi.org/10.1016/S0015-
0282(16)56989-1.
[2] Weatherford DA, Stephenson JE, Taylor SM, Blackhurst D. Thoracoscopy versus
thoracotomy: Indications and advantages. Am Surg 1995;61:83–6.
[3] Whitson BA, Groth SS, Duval SJ, Swanson SJ, Maddaus MA. Surgery for early-
stage non-small cell lung cancer: A systematic review of the video-assisted
thoracoscopic surgery versus thoracotomy approaches to lobectomy. Ann Tho-
rac Surg 2008;86:2008–16. http://dx.doi.org/10.1016/j.athoracsur.2008.07.009,
discussion 2016.
[4] Medeiros LR, Rosa DD, Bozzetti MC, Fachel JM, Furness S, Garry R, et al. La-
paroscopy versus laparotomy for benign ovarian tumour. Cochrane Database Syst
Rev 2009;2:CD004751. http://dx.doi.org/10.1002/14651858.CD004751.pub3.
[5] Galaal K, Donkers H, Bryant A, Lopes AD. Laparoscopy versus laparotomy for
the management of early stage endometrial cancer. Cochrane Database Syst Rev
2012;9:CD006655. http://dx.doi.org/10.1002/14651858.CD006655.pub3.
[6] Borruto FA, Impellizzeri P, Montalto AS, Antonuccio P, Santacaterina E, Scal-
fari G, et al. Thoracoscopy versus thoracotomy for esophageal atresia and
tracheoesophageal fistula repair: Review of the literature and meta-analysis. Eur
J Pediatr Surg 2012;22:415–9. http://dx.doi.org/10.1055/s-0032-1329711.
[7] Gala RB, Margulies R, Steinberg A, Murphy M, Lukban J, Jeppson P, et al.
Systematic review of robotic surgery in gynecology: Robotic techniques compared
with laparoscopy and laparotomy. J Minim Invasive Gynecol 2014;21:353–61.
http://dx.doi.org/10.1016/j.jmig.2013.11.010.
[8] Mcginnis DE, Strup SE, Gomella LG. Management of hemorrhage during la-
paroscopy. J Endourol 2000;14:915–20. http://dx.doi.org/10.1089/end.2000.14.
915.
[9] Sugi K, Sudoh M, Hirazawa K, Matsuda E, Kaneda Y. Intrathoracic bleeding
during video-assisted thoracoscopic lobectomy and segmentectomy. Kyobu Geka
2003;56:928–31.
[10] Martay K, Dembo G, Vater Y, Charpentier K, Levy A, Bakthavatsalam R, et al.
Unexpected surgical difficulties leading to hemorrhage and gas embolus during
laparoscopic donor nephrectomy: A case report. Can J Anaesth 2003;50:891–4.
http://dx.doi.org/10.1007/BF03018734.
[11] Rosevear HM, Montgomery JS, Roberts WW, Wolf JS. Characterization and
management of postoperative hemorrhage following upper retroperitoneal la-
paroscopic surgery. J Urol 2006;176:1458–62. http://dx.doi.org/10.1016/j.juro.
2006.06.023.
[12] Vargas-Palacios A, Hulme C, Veale T, Downey CL. Systematic review of retraction
devices for laparoscopic surgery. Surg Innov 2016;23:90–101. http://dx.doi.org/
10.1177/1553350615587991.
[13] Bolton WS, Aruparayil NK, Chauhan M, Kitchen WR, Gnanaraj KJN, Ben-
ton AM, et al. Gasless laparoscopic surgery for minimally invasive surgery
in low-resource settings: Methods for evaluating surgical field of view and
abdominal wall lift force. Surg Innov 2021;28:513–5. http://dx.doi.org/10.1177/
1553350620964331.
[14] Boni L, Benevento A, Dionigi G, Dionigi R. A new device for minor bleed-
ing control and blunt dissection in minimally invasive surgery. Surg Endosc
2003;17:282–4. http://dx.doi.org/10.1007/s00464-002-9038-9.
[15] Miyazawa M, Aikawa M, Okada K, Watanabe Y, Okamoto K, Koyama I.
Laparoscopic liver resection using a monopolar soft-coagulation device to provide
maximum intraoperative bleeding control for the treatment of hepatocellular car-
cinoma. Surg Endosc 2018;32:2157–8. http://dx.doi.org/10.1007/s00464-017-
5829-x.
[16] Jia X, Meng MQ. A deep convolutional neural network for bleeding detection in
wireless capsule endoscopy images. In: Biology and society annuaire international
conference IEEE eng. Med. 2016, p. 639–42. http://dx.doi.org/10.1109/EMBC.
2016.7590783.
[17] Caroppo A, Leone A, Siciliano P. Deep transfer learning approaches for bleeding
detection in endoscopy images. Comput Med Imaging Graph 2021;88:101852.
http://dx.doi.org/10.1016/j.compmedimag.2020.101852.
[18] Okamoto T, Ohnishi T, Kawahira H, Dergachyava O, Jannin P, Haneishi H.
Real-time identification of blood regions for hemostasis support in laparoscopic
surgery. Signal Image Video Process 2019;13:405–12. http://dx.doi.org/10.1007/
s11760-018-1369-7.
[19] Garcia-Martinez A, Vicente-Samper JM, Sabater-Navarro JM. Automatic detection
of surgical haemorrhage using computer vision. Artif Intell Med 2017;78:55–60.
http://dx.doi.org/10.1016/j.artmed.2017.06.002.
[20] Jiang J, Lu Y, Hua S. Using spatiotemporal hybrid features for detecting bleeding
point in laparoscopic surgery. In: ICGIP, vol. 11720. 2020, 117200C. http:
//dx.doi.org/10.1117/12.2589338.
[21] D’Ambra L, Berti S, Bonfante P, Bianchi C, Gianquinto D, Falco E. Hemostatic
step-by-step procedure to control presacral bleeding during laparoscopic total
mesorectal excision. World J Surg 2009;33:812–5. http://dx.doi.org/10.1007/
s00268-008-9846-8.
[22] Russell WMS, Burch RL. The principles of humane experimental technique.
London: Methuen and Co. Ltd.; 1959.
[23] Isola P, Zhu JY, Zhou T, Efros AA. Image-to-image translation with condi-
tional adversarial networks. In: Proceedings of the IEEE computability sociaal
conference comput. vis. pattern recognit. 2018, p. 1125–34.
[24] Nomura Y, Takamatsu T, Kawano H, Miyahara H, Okino A, Yoshida M, et al.
Investigation of blood coagulation effect of nonthermal multigas plasma jet in
Vitro and in Vivo. J Surg Res 2017;219:302–9. http://dx.doi.org/10.1016/j.jss.
2017.06.055.
[25] Kurosawa M, Takamatsu T, Kawano H, Hayashi Y, Miyahara H, Ota S, et
al. Endoscopic hemostasis in porcine gastrointestinal tract using CO2 low-
temperature plasma jet. J Surg Res 2019;234:334–42. http://dx.doi.org/10.1016/
j.jss.2018.09.068.
[26] Ronneberger O, Fischer P, Brox T. U-net: Convolutional networks for biomedical
image segmentation. In: Int. conf. med. image comput. comput. assist. interv.
2015, p. 234–41. http://dx.doi.org/10.1007/978-3-319-24574-4_28.
[27] Wang Z, Bovik AC, Sheikh HR, Simoncelli EP. Image quality assessment: From
error visibility to structural similarity. IEEE Trans Image Process 2004;13:600–12.
http://dx.doi.org/10.1109/tip.2003.819861.
[28] Kersten-Oertel M, Jannin P, Collins DL. The state of the art of visualiza-
tion in mixed reality image guided surgery. Comput Med Imaging Graph
2013;37(2):98–112. http://dx.doi.org/10.1016/j.compmedimag.2013.01.009.
[29] Katić D, Wekerle AL, Görtler J, Spengler P, Bodenstedt S, Röhl S, et al. Context-
aware Augmented Reality in laparoscopic surgery. Comput Med Imaging Graph
2013;37(2):174–82. http://dx.doi.org/10.1016/j.compmedimag.2013.03.003.
