Applied Computing and Informatics 15 (2019) 182–190








Predicting rank for scientific research papers using supervised learning
Mohamed El Mohadab ⇑, Belaid Bouikhalene, Said Safi
Department of Mathematics and Informatics, Laboratory of Innovation in Mathematics and Application and Information Technologies (LIMATI), Polydisciplinary Faculty, Sultan Moulay Slimane University, PB 592 Beni Mellal, Morocco



a r t i c l e  i n f o

Article history:
Received 10 September 2017
Revised 14 February 2018
Accepted 17 February 2018
Available online 6 March 2018

Keywords:
Scientific research
Ranking scientific research papers Data mining
Supervised learning
Multilayer perceptron algorithm
a b s t r a c t 

Automatic data processing represents the future for the development of any system, especially in scien- tific research. In this paper, we describe one of the automatic classification methods applied to scientific research as a supervised learning task. Throughout the process, we identify the main features that are used as keys to play a significant role in terms of predicting the new rank under the supervised learning setup. First, we propose an overview of the work that has been realized in ranking scientific research papers. Second, we evaluate and compare some of state-of-the-art for the classification by supervised learning, semi-supervised learning and non-supervised learning. During the preliminary tests, we have obtained good results for performance on realistic corpus then we have compared performance metrics, such as NDCG, MAP, GMAP, F-Measure, Precision and Recall in order to define the influential features in our work.
© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is an
open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Introduction

Due to the fast development of information and communica- tions technologies, the university has firmly decided to facilitate the access and treatment for all processes, especially in scientific research in order to assist PhD students, professors and adminis- trative staff to deal with digital services that they need.
In recent years, research in ranking scientific research papers (SRP) from diversified fields of research has become a very impor- tant task because of the exponential growing of daily publication in journals and conferences, exceeding 50 million papers. Also, pre- dicting the future of any system represents another challenge that we can face generally, but mixing both problems is the case that we address in this research by predicting the new rank of scientific research paper.
Using machine learning [1] in ranking scientific research papers is a crucial research direction, because it contains distinct classes of supervised learning algorithms [2] with regard to prediction. Between the main important algorithms used in linear classifiers, we choose to work with Multilayer Perceptron Algorithm [3],

* Corresponding author.
E-mail address: m.elmohadab@gmail.com (M. El Mohadab).
Peer review under responsibility of King Saud University.

SMO Classifier [4], and Kstar Classifier [5]. The three classifiers rep- resent the neuron network with nodes and edges as papers and citations between the different authors in a similar set of informa- tion. The major reason for this choice is that a research paper net- work is a concrete example of the relation where the researcher collaborates with other research communities in the scientific domains in order to achieve their goals.
The related work gives us a vision on the approaches and meth- ods for the classification of scientific research papers, and which is grouped into two major axes: the first axis is ranking according to the query and the second is ranking according to the technical analysis link. The limitation of these two main axes classifies the existing papers to us but it does not propose a contribution con- cerning the future classification. The novelty brought in this work is manifested through the prediction of the future classification being based on the existing papers that will offer the researcher the paper with the highest rank in this field.
The rest of this paper is organized as follows: in the next section we review the related work in ranking scientific research papers. Section 3 shows the state of the art for the learning methods. Sec- tion 4 describes Methods. Section 5 shows Results and discussion. Finally, in section 6, we conclude and describe future research directions.


Related work

In the last few years, there has been a growing interest in rank- ing scientific research papers as one of the pillars of research in


https://doi.org/10.1016/j.aci.2018.02.002
2210-8327/© 2018 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.
This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).



ranking in general. Among the most used tools to rank journals, we find the impact factor [6], which is an approximation of the aver- age number of citations within a year given to the set of papers belonging to a journal published in the two preceding years.
Previous studies in ranking indicate that we have two axes; the first axis represents the relevance ranking algorithm which matches the ranking according to the query. Among the studies we find:
Vector Space Model [7]: is the way of representing text docu- ments as vectors of identifiers. It is used in information retrie-
val, information filtering, and relevancy and indexing ranking.
Latent semantic analysis [8]: is the way of analyzing the rela- tion between a set of concepts related to the term and a set of
documents. This technique is used in natural language processing.
Okapi BM25 [9]: is the way of matching documents by a ranking
function in search engines according to their relevance with a
search query. It is totally based on the probabilistic retrieval framework.
Boolean Ranking Model [10]: is the way of searching the user’s
query in the existent set based on classical set theory and Boo-
lean logic.

The second axis represents an important ranking model which ranks according to the link analysis technique. Among the models we find:
HITS [11]: is an analysis algorithm, the basic idea is that a web page serves two purposes: to provide information and or to sug-
gest links to pages on a topic.
PageRank [12]: is an algorithm working by calculating the num- ber of links to a page and also their quality to determine a close
estimation of the importance of the relevant documents.

Most algorithms used to rank SRP are divergent from PageRank or HITS, we find:
Topic Rank [13]: clusters papers into topics; between the main factors used, we have: topic, citation, date of publication, title,
and keyword.
Cite Rank [14]: is an algorithm working by ranking citation net- works based on their topology, between the main factors used,
we have: citation, title, and date of publication.
PTRA [15]: gives the paper age a higher impact, and depends highly on time of publication to rank the papers; among the
main factors used, we have: citation, publication venue, and publication date.

However, all researchers have concluded that both types of ranking algorithms (the relevant/the important) have some limita- tions, especially the relevance algorithm that is not used any more in ranking algorithms. However, studies on predicting a new rank are still lacking. In the next section, we will review the state of the art for the learning methods.



State of the art for the learning methods

In this section, we will provide a sort of overview of what has been found in the literature concerning the models which we have been using in our work. There are three families of differential learning methods: supervised learning, which requires prior label- ing of class data so that the model can train on them; unsupervised learning (clustering), without a prior information input; and
semi-supervised learning mode, which jointly manipulates unlabeled and labeled data.

Supervised approach

Supervised Approach is an automatic learning technique which leads automatically to produce rules from a learning database con- taining examples of cases already dealt with. Therefore, its aim is to generalize for unknown inputs that it has been apt to learn from the data which are already handled by experts; the purpose is to use this to determine a compact representation of the function of prediction, which at a new input x associates an output S (x). The three main approaches related to supervised learning are:
Neural Networks [16].
Hidden Markov Model [17].
Support Vector Machines [18].
The Neural Networks [16] is generally defined by three types of
parameters:
The interconnection pattern.
The activation function.
The learning process.
The Hidden Markov Model [17] is defined by two stochastic
processes: a Markov chain is defined by a set of states and the tran- sitions between the different states, so-called emission probabili- ties connected with each state. We will bring into focus the decision-making process, which is described by:
A finite set S of discrete states denoted s.
A finite set A of actions denoted a.
A transition function P: S × A ? P (S) where P (S) is the set of distributions of probability on S.

The Support Vector Machine [18] offers, in particular, a good approximation of the fundamental of minimization of structural risk. The method depends on the following ideas:
The data is projected in a large space by a transformation based on a linear, polynomial or Gaussian kernel.
The classes are disconnected by linear classifiers that maximize the margin in the transformed space.
The hyper planes can be determined by means of a few points which will be called ‘‘support vectors”.

The Boosting [19] is summarized as follows:
A large set of simple features.
Initialization weights for training sets.
For T rounds:
Normalize the weights.
For features from the set, train a classifier with a single fea- ture and examine the training error.
Determine the classifier with the lowest error.
Update the weights of the training sets.
The final classifier is the linear combination of the T classifiers.
Unsupervised approach

There are different reasons for choosing this type of learning such as the charge of developing manual labeling and the search for discriminatory characteristics in the first study or characteris- tics which grow over time. Unsupervised learning [20] is often



treated as a density estimation problem; the two main approaches used in unsupervised learning are:
K-Means Clustering [21].
Fuzzy C-Means [22].
The k-means clustering [21] is summarized as follows:
Place K points into the space expressed by the objects clustered;
Assign each object to the near centroid;
Recalculate the locations of the K centroids;
Repeat until the centroids are fixed. The metric is then calculated.

The fuzzy c-means algorithm [22] is very identical to the k- means algorithm:
Appoint a number of clusters.
Each point is given a random coefficient.
Repeat the algorithm until convergence.
Semi-supervised approach

To perform generic tasks of supervised learning while exploit- ing some labeled data simultaneously with multiple raw data. The first idea is using a non-supervised context of the outputs pre- dicted by the system itself in order to construct the desired outputs by applying a supervised technique. This approach is known as the directed decision. The second idea depends on the simultaneous use of two classifiers. They alternately act as a teacher and a pupil in an algorithm, iterative learning: the output calculated by one will be taken as the appropriate output by the other and recipro- cally until convergence.
The learning criterion is here to optimize the coherence between the two classifiers. This approach is known as self- supervision. The two main approaches to semi-supervised learning
[23] are:

Co-Training [2]: It is a machine learning algorithm used when there are only some labeled data and large amounts of unla-
beled data. One of its uses is in text mining for search engines.
Co-Boosting [24]: It may be seen as a combination of co-training and boosting.

Moreover, we have compared distinct machine learning algo- rithms. In our case of study, we have chosen to work with the supervised learning approach especially with the neuron network represented by the Multilayer Perceptron classifier [3]; the reasons for this choice are:
Presentation of a drive to the network.
Comparison of the network output with the targeted output.
Calculation of the error at the output of each neuron belonging to the network.
Definition of the increase or decrease required to obtain this value.
Adjustment of the weight of each connection to the lowest local error.
Granting blame to all previous neurons.
The reasons behind choosing SMO classifier [4] are:

Finding a Lagrange multiplier a 1 that violates the Karush–Ku hn–Tucker (KKT) conditions for the optimization problem.
Picking a second multiplier a 2 and optimize the pair (a 1, a 2).
Repeating steps 1 and 2 until convergence.
Also, the reasons for choosing Kstar classifier [5] are:

Kstar operates on-the-fly, which means that it does not require the graph to be explicitly available and stored in the main mem-
ory. Portions of the graph will be generated as needed.
Kstar can be guided using heuristic functions.
In the next section we will expose our new way to predict the
new rank by using supervised learning [2].

Methods

From a network viewpoint, papers can be seen as nodes in a network and the citations between papers as edges (see Figs. 1 and 2).
As we see in Fig. 3, in the network, each paper node X links to another paper node Y through citation between themes. This net- work can help us to have more information about authors, papers, type of papers, etc. Then, like all transfer a model, the score is cal- culated by the count of the number of citations will be transferred to the referenced papers. Also, we must split our data into subsec- tions to rank each paper into its division. As a case in point, we will treat information about different research publications in the field of computer science exclusively for Geographic Information System.
In this work, we take into account the following point:

The papers with high number of citations reflect the importance and the prestige of the author.
Scientific Gem [25] is always in the first rank in spite of their date of publication as a result of their recent citations.
Recent publication has always less citation despite their newest contribution.

In any machine, the learning algorithm determines the good features which can provide very good results. The application of our algorithm [26] for ranking depends on:
Paper Posted Time: The number of years since it has been pub- lished, by the formula:
o A = Current Year-Year of Publication.
Conference Score: The quality of any conference can be explored by the age of the conference, the continuity for the
conference, the number of papers in the proceeding and the Digital Library involved.


Fig. 1. Selection of data according to the category of learning.


Keywords: The order of the keywords reflects the topics and the interest for the work.
Publication Type: In general, the papers that are published in journals are more influential than the other types of publication
venues, and the importance of conference is less influential than journals and higher than workshops.
The Average Publication/Keyword:
This feature is given as below:

0: keyword not in the title of paper.
1: keyword in abstract.
2: keyword in the title of paper.
3: keyword in both title and abstract

The rank can be computed by the giving equation:
X	 1 
	











Fig. 2. Example of a neural network.



Fig. 3. The scientific papers network.



Author Score: The number of authors of each paper, and the number of publications for each author is used to calculate
author score, which is defined by:

X	 1 
DL 	1	
1 + log A
Ai: The number of papers published by the author.
N: The total number of all authors for the paper.
H: Constant with value 10.
Av: The average publication/keyword.
Anbr: The order of paper.
Dx: Download rates.
Type: The type of paper.
PR(i): The score calculated by the PageRank algorithm.
Log (A): Used to reduce the impact of old paper having the high- est number of citations that are called ‘‘Scientific gem” [25].

In this paper, we examine the possibility of predicting the new rank for scientific papers to help researchers to find papers that they are looking for; we choose to work with Weka [27], as a col- lection of machine learning algorithms.

Results and discussion

Dataset

We give a great importance to the construction of training data and test data because of their influence on our experience in this work; for this reason, the mobile window strategy has been adopted.
The appropriate size of test data must respect certain condi- tions. First of all, it must not be too small because it will converge easily; this will have consequences on the accuracy of the predic- tion because, on the one hand, it has not given enough data to sup- port the reasons enough and, on the other hand, it must not be too big because it is not necessary to converge (see Figs. 4–6).
We have chosen the moving windows strategy [28] compared to the sliding window strategy [29] because the prediction depends on the time factor, which is the case in our study. After the realization of some preliminary experience, we try to choose the two data model (test data and training data) which guarantees the lowest possible error rate.
In this research, we have exploited the bibliographic datasets
from Thomson-Reuters Web of Science [30], which has information

i=1
AiDL NH
about different research publications in distinct fields of science, basic metadata for 1.935 SRPs in Research Areas of Science, and

Download Rates: The number of downloads from the official
website of the journal reflects the importance for the given
work in the paper.
cover publications from 1996 to 2015. We use data from 1996 to 2015. It is to be noted that the Thomson-Reuters dataset that we used contained citation information till the year 2015 only.



Data pre-processing

The Web of Science dataset also contains information that is not useful in our algorithm. We need to pre-process the dataset to extract only the information that we will use in our algorithm. In data pre-processing, after the extraction and preparation of data, the final database contains: the title of paper, its author(s), key- words, paper posted time, the conference of publication, the given research paper cites, the download rates, also the average publica- tion/keyword, article number and finally the order of paper.
For our experiment, we split our data in two sections: the first is the data before 2012 that represent our training dataset that per- mits us to compute the rank for the papers; the second section con- tains the data that we wish to predict.
From the point of view of the users who were searching in 2012 for scientific research papers, the only available data is the training dataset, which means papers before 2012. Our system can predict for our user the papers that he/she wants with the highest rank in his/her subject.
These experiments have been designed to find the pertinent metrics such as: paper-id, author score, and number of papers pub- lished, average download rates, number of citations. These metrics can be calculated from our training dataset.
In our application, we apply some rules:
We don’t eliminate not integral data.
We don’t rank the papers whose author doesn’t remain in the training dataset.
We calculate the average of the rank that obtains all papers for each first author.




Mathematical model

Our Mathematical model can be modeled in a set theory. The system is represented as follows:
S= {I, P, O}
Where
S = represent the system,
Where I = inputs, represent the given features, P = {P1,P2.. .Pn}
Where P = Processes
P1P1 = check data in local server, P2 = store data at database, O = {O1, O2, .. . On}
Where O = outputs, represent the new rank for our papers.
We took the papers related to the topics of Geographic Informa- tion System in our data set by our algorithm [26] as the training dataset.
Rank values of our proposed algorithm are based on three prin- ciple points and depicted in table mentioned as supplementary material.



Fig. 5. Venn diagram of the proposed system.


Author and title where the names of the authors and the titles of papers are presented.
DOI (Digital Object Identifier) is a standardized method for the permanent identification of a published electronic object, a kind
of permanent code of scientific articles. Each paper has its own DOI.
Journal and year mean the date of paper publication and the
journal where is submitted.

Now, our aim is to predict scientific research papers shown in Fig. 9, that are in the evaluation dataset whose authors are all in the training dataset and we calculate their futures as:
We will start by defining some parameters for our analysis:

Precision: is the fraction of retrieved instances that are rele- vant, named Positive Predictive Value.
Recall: is the fraction of relevant instances that are retrieved, named Sensitivity.
F-Measure: is the harmonic mean of precision and recall; in other terms, it is the measure that combines precision and
recall, and it is defined as:

F-Measure = 2 × Recall × Precision
Recall + Precision
Cumulative Gain: is the sum of the graded relevance values of all results in a search result list, where reli is the graded rele-
vance of the result at position I; it is defined as:
p
CGp =	reli
i=1
Discounted Cumulative Gain [31]: is a particular rank position p; it is defined as:
p
DCG	i	
i=1 log2(i + 1)
Normalized Discounted Cumulative Gain [32]: search result lists vary in length depending on the query; it is defined as:
NDCGp =  DCGp 
IDCGp
Average Precision: summarizes a precision-recall curve as the weighted mean of precisions achieved at each threshold, where
Pn and Rn are the precision and recall at the nth threshold:
AP =	(Rn — Rn—1)Pn
n
Mean Average Precision [33]: is the average of the precision value obtained for the set of k documents after the document
is retrieved.
Q is {d1... dmj} and Rjk is the set of ranked retrieval results from the top result until you get to document dk, then


d	mj
MAPd = 1 X  1  XP(Rjk)

Fig. 4. The training dataset originating before 2012, and the evaluation dataset originating after 2012.
d j=1
mj k=1




Fig. 6. Data for prediction.


Geometric Mean Average Precision [34]: is the geometric mean of the average precision values for an information retrie-
val system over a set of n query topics; it is defined as:
GMAP = rn YﬃﬃﬃﬃﬃﬃﬃAﬃﬃﬃPﬃﬃnﬃﬃ
The test set gives us the following results for the three classifiers
after making some modification:
In Table 1, we can clearly see that the Multilayer Perceptron classifier has the highest NDCG26 [32] compared to the tow classi- fiers and this can provide a good performance for prediction.
Our network for predicting new rank after using Multilayer Per- ceptron is shown in the figure below:
As we see in Fig. 7, our network contains in the input layer met- rics: paper-id, author-score, number–paper-published, average- download-rates, average-number-citation, and one hidden layer with 17 nodes as the average between the number of inputs and outputs. Each connection node named neuron has a weight calcu- lated from their inputs with a sigmoid function [35] as:
Wnext = W + DW
DW = —learning rate × gradient + momentum × DWprevious
Finally, we have the output layer with 29 classes representing the
rank of scientific research papers. The predictions give us the result shown in Table 2.

Further comparison of the proposed algorithm

Now, we are analyzing the metrics used to predict our new rank. We propose four variant APC, APD, ADC and PDC explained as follow:
New Rank (APC): a proposed variant wherein paper-id, author score, number–paper-published and average-number-citation
are the parameters used.
New Rank (APD): a proposed variant based on the same param- eters of APC, but instead using average-number-citation we use
average download-rates.
New Rank (ADC): a proposed variant wherein we use paper-id, author score, average download-rates and average-number-
citation.
New Rank (PDC): a proposed variant based on paper-id, number-paper-published,	average-download-rates	and
average-number-citation.

Table 1
Performance for the three classifiers.





Fig. 7. The network for prediction.



In the figure below, we summarize the results for the four vari- ants of our proposed new rank.
In order to define the suitable variant of our new rank the three parameters: Precision, Recall and F-measures should have higher values. As we see in the figure, we conclude that the APD variant satisfied this condition (see Fig. 8).
As we see in the previous sub-section, MAP is just an average precision which most frequently used in research papers. In con- trast to MAP which can be considered as an arithmetic mean, GMAP is a geometric per precision; it used to highlight improve- ment for low performing subjects.


Table 2
Scientific research papers predicted by our new rank algorithm.




On the other hand, we should compare MAP and GMAP for the four variants. We clearly see in the Fig. 9 in term of APD, ADC and PDC the values are slightly close to each other compared to APC which has superior values.
In Fig. 10, we present a comparison between GMAP and MAP in term of the proposed new rank. We can clearly see that for each ranked paper GMAP and MAP are close to each other, instead of
some cases wherein we find that the values of GMAP are very less than MAP.
MAP and GMAP may be seen as similar measures of average ranking effectiveness of a system.
To sum up, the Figs. 9 and 10 shows that GMAP values are less than MAP and this lead to a perform ranking and at the same time reducing errors in our proposed system.






Fig. 8. Comparison of the performance.


Fig. 9. MAP vs. GMAP in different variants.



Fig. 10. GMAP vs. MAP in the proposed New Rank.



Conclusion

In this paper, we propose a new approach for predicting the new rank for scientific research papers. Our experimental evalua- tion has shown the efficient of the utilization of machine learning algorithm in the discipline of ranking. We provide an algorithm that use different metrics such us (including but not restricted to
paper-id, author-score, number–paper-published, average- download-rates, average-number-citation) in a one network. Moreover, we provide a comparison of the metrics and ranked them conforming to their prediction ability using metrics analysis algorithm, we think that this work can help researcher in other dis- cipline such us: financial sector, policies investigation, and terrorist behavior.
For future work, we plan to test our algorithm on additional datasets in order to determine how robust it is to the different val- ues of parametrs, and in different datasets, also we plane to make a survey to appraise the results by users.


Appendix A. Supplementary material

Supplementary data associated with this article can be found, in the online version, at https://doi.org/10.1016/j.aci.2018.02.002.

References

N.W. Alkharouf, D.C. Jamison, B.F. Matthews, Online analytical processing (OLAP): a fast and effective data mining tool for gene expression databases, J. Biomed. Biotechnol. 2005 (2005) 181–188, https://doi.org/10.1155/ JBB.2005.181.
M. Darnstädt, H.U. Simon, B. Szörényi, Supervised learning and co-training, Algorithmic Learn. Theory 519 (2014) 68–87, https://doi.org/10.1016/j. tcs.2013.09.020.
H. Ramchoun, M. Amine, J. Idrissi, Y. Ghanou, M. Ettaouil, Multilayer perceptron: architecture optimization and training, Int. J. Interact. Multimed. Artif. Intell. 4 (2016) 26, https://doi.org/10.9781/ijimai.2016.415.
X. Shao, K. Wu, B. Liao, Single directional SMO algorithm for least squares support vector machines [WWW Document], Intell. Neurosci. Comput. (2013), https://doi.org/10.1155/2013/968438.
S. Lee, M. Park, J. Park, H. Na, M. Kwon, Operator interface programs for KSTAR operation, Fusion Eng. Des. 88 (2013) 2835–2841, https://doi.org/10.1016/ j.fusengdes.2013.05.008.
K. Slim, A. Dupré, B. Le Roy, Impact factor: an assessment tool for journals or for scientists?, Anaesth Crit. Care Pain Med. 36 (2017) 347–348, https://doi. org/10.1016/j.accpm.2017.06.004.
Y. Du, W. Liu, X. Lv, G. Peng, An improved focused crawler based on semantic similarity vector space model, Appl. Soft Comput. 36 (2015) 392–407, https:// doi.org/10.1016/j.asoc.2015.07.026.
F. Zhuang, G. Karypis, X. Ning, Q. He, Z. Shi, Multi-view learning via probabilistic latent semantic analysis, Inf. Sci. 199 (2012) 20–30, https://doi. org/10.1016/j.ins.2012.02.058.
S. Robertson, H. Zaragoza, M. Taylor, Simple BM25 Extension to Multiple Weighted Fields, in: Proceedings of the Thirteenth ACM International Conference on Information and Knowledge Management, CIKM ’04. ACM, New York, NY, USA, 2004, pp. 42–49. https://doi.org/10.1145/ 1031171.1031181.
F. Lv, H. Zhang, J.g. Lou, S. Wang, D. Zhang, J. Zhao, CodeHow: Effective Code Search Based on API Understanding and Extended Boolean Model (E), in: 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE). Presented at the 2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE), 2015, pp. 260–270. https://doi.org/ 10.1109/ASE.2015.42.
X. Liu, H. Lin, C. Zhang, An improved HITS algorithm based on page-query similarity and page popularity, J. Comput. 7 (2012), https://doi.org/10.4304/ jcp.7.1.130-134.
X. Tan, A new extrapolation method for PageRank computations, J. Comput. Appl. Math. 313 (2017) 383–392, https://doi.org/10.1016/j.cam.2016.08.034.
A. Bougouin, F. Boudin, B. Daille, Topicrank: Graph-based topic ranking for keyphrase extraction, in: International Joint Conference on Natural Language Processing (IJCNLP), 2013. pp. 543–551.
P. Jomsri, S. Sanguansintukul, W. Choochaiwattana, CiteRank: combination similarity and static ranking with research paper searching, Int. J. Internet Technol. Secur. Trans. 3 (2011) 161–177, https://doi.org/10.1504/ IJITST.2011.039776.
M.A. Hasson, S.F. Lu, B.A. Hassoon, Scientific research paper ranking algorithm PTRA: a tradeoff between time and citation network, Appl. Mech. Mater. 551 (2014) 603–611, https://doi.org/10.4028/www.scientific.net/AMM.551.603.
F. Stahl, I. Jordanov, An overview of the use of neural networks for data mining tasks, Wiley Interdiscip Rev. Data Min. Knowl. Discov. 2 (2012) 193–208, https://doi.org/10.1002/widm.1052.
Y. Zheng, B. Jeon, L. Sun, J. Zhang, H. Zhang, Student’s t-hidden markov model for unsupervised learning using localized feature selection, IEEE Trans. Circuits Syst. Video Technol. 1–1 (2017), https://doi.org/10.1109/TCSVT.2017.2724940.
F.-J. González-Serrano, Á. Navia-Vázquez, A. Amor-Martín, Training support vector machines with privacy-protected data, Pattern Recognit. 72 (2017) 93– 107, https://doi.org/10.1016/j.patcog.2017.06.01.



W.W.Y. Ng, X. Zhou, X. Tian, X. Wang, D.S. Yeung, Bagging–boosting-based semi-supervised multi-hashing with query-adaptive re-ranking, Neurocomputing (2017), https://doi.org/10.1016/j.neucom.2017.09.042.
J. Yao, Q. Mao, S. Goodison, V. Mai, Y. Sun, Feature selection for unsupervised learning through local learning, Pattern Recognit. Lett. 53 (2015) 100–107, https://doi.org/10.1016/j.patrec.2014.11.006.
K.M. Kumar, A.R.M. Reddy, An efficient k-means clustering filtering algorithm using density based initial cluster centers, Inf. Sci. 418–419 (2017) 286–301, https://doi.org/10.1016/j.ins.2017.07.036.
Z. Zainuddin, O. Pauline, An effective fuzzy C-means algorithm based on symmetry similarity approach, Appl. Soft Comput. 35 (2015) 433–448, https:// doi.org/10.1016/j.asoc.2015.06.021.
X. Zhu, Semi-Supervised Learning, in: Encyclopedia of Machine Learning, Springer, Boston, MA, 2011, pp. 892–897, https://doi.org/10.1007/978-0-387- 30164-8_749.
S. Chen, S. Zhu, Y. Yan, Robust visual tracking via online semi-supervised co- boosting, Multimed. Syst. 22 (2016) 297–313, https://doi.org/10.1007/s00530-
015-0459-4.
P. Chen, H. Xie, S. Maslov, S. Redner, Finding scientific gems with Google’s PageRank algorithm, J. Informetr. 1 (2007) 8–15, https://doi.org/10.1016/j. joi.2006.06.001.
M. El Mohadab, B. Bouikhlaene, S. Safi, Towards an efficient algorithm for ranking scientific research papers, in: 2017 2nd IEEE international scientific event on internet of things: Recent innovations and challenges (SEIT). Presented at the 2017 2nd IEEE international scientific event on internet of things: Recent innovations and challenges (SEIT), Rabat, Morocco.
S. Lievens, B. De Baets, Supervised ranking in the weka environment, Inf. Sci. 180 (2010) 4763–4771, https://doi.org/10.1016/j.ins.2010.06.014.
J. Valluru, P. Lakhmani, S.C. Patwardhan, L.T. Biegler, Development of moving window state and parameter estimators under maximum likelihood and Bayesian frameworks, DYCOPS-CAB 2016 (60) (2017) 48–67, https://doi.org/ 10.1016/j.jprocont.2017.08.007.
I. Chakroun, T. Haber, T.J. Ashby, SW-SGD: The Sliding Window Stochastic Gradient Descent Algorithm. Procedia Comput. Sci., International Conference on Computational Science, ICCS 2017, 12–14 June 2017, Zurich, Switzerland 108, 2017, 2318–2322. https://doi.org/10.1016/j.procs.2017.05.082.
Web of Science library. Available at: http://www.webofknowledge.com [accessed in 2016].
G. Dupret, B. Piwowarski, Model Based Comparison of Discounted Cumulative Gain and Average Precision. Sel. Pap. 18th Int. Symp. String Process. Inf. Retr. SPIRE 2011 18, 49–62. https://doi.org/10.1016/j.jda.2012.10.002.
Y. Wang, L. Wang, Y. Li, D. He, T.-Y. Liu, W. Chen. A theoretical analysis of NDCG type ranking measures. ArXiv13046480 Cs Stat, 2013.
M. Thelwall, The precision of the arithmetic mean, geometric mean and percentiles for citation data: an experimental simulation modelling approach,
J. Informetr. 10 (2016) 110–123, https://doi.org/10.1016/j.joi.2015.12.001.
S. Robertson, On GMAP: and other transformations. (2006), https://doi.org/ 10.1145/1183614.1183630.
A. Iliev, N. Kyurkchiev, S. Markov, On the approximation of the step function by some sigmoid functions, Math. Comput. Simul., Biomath 2014 and Biomath 2015 133 (2017) 223–234, https://doi.org/10.1016/j.matcom.2015.11.
