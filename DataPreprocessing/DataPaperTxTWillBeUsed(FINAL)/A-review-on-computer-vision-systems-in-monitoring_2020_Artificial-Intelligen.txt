Artificial Intelligence in Agriculture 4 (2020) 184–208











A review on computer vision systems in monitoring of poultry: A welfare perspective
Cedric Okinda a, Innocent Nyalala a, Tchalla Korohou a, Celestine Okinda b, Jintao Wang a, Tracy Achieng c,
Patrick Wamalwa d, Tai Mang a, Mingxia Shen a,⁎
a College of Artificial Intelligence, Laboratory of Modern Facility Agriculture Technology and Equipment Engineering of Jiangsu Province, Nanjing Agricultural University, Jiangsu 210031, PR China
b College of Veterinary Science and Agriculture, University of Nairobi, Lower Kabete, Nairobi, Kenya
c Faculty of Bioscience and Engineering, Ghent University, Ghent, Belgium
d Faculty of Engineering, Department of Agricultural Engineering, Egerton University, Njoro, Kenya



a r t i c l e	i n f o


Article history:
Received 7 August 2020
Received in revised form 6 September 2020 Accepted 6 September 2020
Available online 9 September 2020


Keywords: Computer vision Deep learning Machine learning Monitoring Poultry
Welfare
a b s t r a c t

Monitoring of poultry welfare-related bio-processes and bio-responses is vital in welfare assessment and man- agement of welfare-related factors. With the current development in information technologies, computer vision has become a promising tool in the real-time automation of poultry monitoring systems due to its non-intrusive and non-invasive properties, and its ability to present a wide range of information. Hence, it can be applied to monitor several bio-processes and bio-responses. This review summarizes the current advances in poultry mon- itoring techniques based on computer vision systems, i.e., conventional machine learning-based and deep learning-based systems. A detailed presentation on the machine learning-based system was presented, i.e., pre-processing, segmentation, feature extraction, feature selection, and dimension reduction, and modeling. Similarly, deep learning approaches in poultry monitoring were also presented. Lastly, the challenges and possi- ble solutions presented by researches in poultry monitoring, such as variable illumination conditions, occlusion problems, and lack of augmented and labeled poultry datasets, were discussed.
© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Contents
Introduction	185
Precision livestock farming and animal welfare	185
Machine learning-based poultry monitoring systems	186
Image pre-processing	188
Region of interest segmentation	188
Features for segmentation and approaches	188
Region of interest validation	189
Feature extraction	190
Morphological features	190
Locomotor features	191
Optical flow measures	191
Other features	192
Feature selection and dimension reduction	193
Statistical analysis	193
Modeling techniques	193
Deep learning-based poultry monitoring systems	197
Deep learning categories	197
Convolutional Neural Networks (CNNs)	198
Recurrent and recursive neural networks	199
* Corresponding author.
E-mail address: mingxia@njau.edu.cn (M. Shen).
https://doi.org/10.1016/j.aiia.2020.09.002
2589-7217/© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

Pretrained Unsupervised Networks (PUNs)	199
Image pre-processing	200
Data augmentation	200
Deep learning applications	200
Challenges and future direction	201
Live weight estimation systems	201
Lameness detection systems	202
Health status classification systems	203
Poultry tracking systems	203
Behavior monitoring systems	204
Activities and other monitoring systems	204
Conclusions	205
Declaration of competing interest	205
Acknowledgments	205
References	205



Introduction

Food security concerns have mandated an increase in agricultural production due to the ever-growing world population with a projec- tion of over 9.6 billion people by the year 2050 (Gerland et al., 2014). However, environmental production constraints have resulted in a decline in the global per-capita cereal production since the early 1980s (Dyson, 1999). Additionally, there is an increasing preference for animal-based food proteins (FAO, 2018). Therefore, future global meat consumption is deemed to increase by 70% by 2050 (Berckmans, 2017). This increasing demand has resulted in intensive and extensive animal production. Currently, over 3535 million ani- mals are reared globally under extensive and intensive production systems, with a total annual production of 798 and 3029 million tons of milk and meat, respectively (Pulido et al., 2018; Wang et al., 2019b). A report by Henchion et al. (2014) on the trends of meat con- sumption indicated an increase in consumption of poultry meat and poultry meat products, with a projected increase within the next de- cade due to preferences of white meat with chicken being the favor- ite (OECD-FAO, 2017; Okinda et al., 2019). With the intensification of chicken production and the growing awareness of acceptable animal welfare conditions, animal health, efficiency, and sustainable envi- ronmental conditions have become challenging factors to fulfill (Berckmans, 2014). Hence, human surveillance has ceased to be a vi- able solution in livestock monitoring (Okinda et al., 2019). Precision Livestock Farming (PLF) has been used as a solution to these chal- lenges by providing efficient automated systems while at the same time maintaining animal welfare (Lehr, 2014). PLF acts as a support system to the stockmen to monitor various bio-processes and bio- responses related to animal welfare, health, and productivity (Banhazi et al., 2012; Berckmans, 2017; Wathes et al., 2008).
Vision-based PLF systems have become a rich research topic due to the current development in technology and the advantages of computer vision systems in animal monitoring. Computer vision sys- tems can provide non-intrusive, non-invasive, consistent, effective, and objective supervision. It provides an allowance for data record- ing for future usage and analysis. Additionally, computer vision re- duces tedious and labor-intensive processes. Furthermore, it provides a robust sensing technology that can be used to monitor nu- merous aspects of the farm. Computer vision is the process of apply- ing mathematics, computer science, and software programming to provide image-based automated process control. There are two cat- egories of computer vision-based systems, i.e., machine learning- based systems and deep learning-based systems. The former follows a typical image processing procedure (image acquisition, pre- processing, region of interest (ROI) extraction, feature extraction, and classification or regression). While the later perform classifica- tion or regression from an object recognition point of view based on Deep Neural Networks (DNN). General object recognition tasks in computer vision include image identification, object detection,


image classification, semantic segmentation, and specific object rec- ognition (Fujiyoshi et al., 2019).
The image identification problem involves verifying if an object in an image has the same pattern as a reference object. The verification is based on the difference (threshold distance) between the feature vectors of the reference pattern and the input image. Object detection problem involves finding the location of an object of a specific category in an image. It can be a single category or a multi-class object detection problem. In deep learning (DL), multi-class object detection can be per- formed by a single network as opposed to a conventional machine learning approach. The image classification problem involves finding the category(s) in which an object(s) in an image belongs. The semantic segmentation problem involves understanding the scene structure of an image, i.e., finding pixel-wise object categories. This problem is chal- lenging using classical machine learning techniques but can be over- come by the application of DL algorithms. Specific object recognition is a subtask of the general object detection problem. It involves the detec- tion of specifically defined objects in an image by detecting feature points using scale-invariant feature transform (SIFT) (Lowe, 2004) or learned invariant feature transform (LIFT) (LeCun et al., 1999). An over- view of computer vision applications in poultry monitoring is presented in Fig. 1.
The main components of a computer vision system are the camera
sensor, image processing board, software, and hardware. The camera sensor converts photons to electrical signals. In chicken monitoring sys- tems, visual light-based (charge-coupled devices (CCD) and comple- mentary metal-oxide-semiconductor (CMOS)), thermal and infrared (IR) depth-based sensors have been applied to acquire chicken images in different farm environments. The image processing board is also known as the digitizer, converts the visual image into numerical form (pixels). The software is the underlying image analysis code that per- forms image manipulations to achieve the desired output. Different pro- cessing algorithms have been developed and applied to the acquired images to perform the objective tasks based on a specific programming framework such as Matlab, ImageJ, and OpenCV, to mention a few. The hardware refers to all the connected components that make up the computer vision system, i.e., a camera sensor, connecting cables, com- puters, etc.
Despite the numerous advantages of computer vision systems, the performance of any vision systems in the monitoring of animals is greatly affected by the variation of ambient light conditions in the farm environment, color contrast between background and foreground, and occlusion problems. Nevertheless, several studies have been carried out to overcome these challenges.

Precision livestock farming and animal welfare

Animal welfare is a complex, dynamic, multifaceted policy issue with economic, scientific, ethical, and political dimensions that need to be addressed objectively in a scientifically credible manner. The




Fig. 1. An overview of the application of computer vision in poultry monitoring.


term animal welfare has been defined by several studies in different ways, depending on criteria and assessment. According to Bessei (2018), animal welfare ranges from a total perfect condition to extreme suffering. Characterized by diseases, physical damages (wounds and bone breakage), behavioral (displacement preening, stereotyped be- havior, feather pecking and cannibalism, aggression, and fear), physio- logical (stress), ethological, psychological, and positive feeling (dustbathing, preening, stretching and play) criteria. According to Dawkins (2017), good animal welfare is defined by good health (water, food, and lack of injury) and the animal having its needs and wants to be fulfilled at all times. Additionally, 12 criteria have been in- troduced as standards of good poultry welfare conditions (Welfare- Quality®, 2009). These criteria include thermal comfort, absence of dis- ease, absence of prolonged hunger, absence of injuries, absence of prolonged thirst, ease of movement, absence of pain due to manage- ment procedures, presence of comfort around resting, expression of other natural behavior, good human-animal relations, expression of so- cial behavior, and positive emotional state. Generally, these welfare criteria can be summarized as “animal health and animal want” (Dawkins, 2017).
Animal welfare is an emotive issue, but the question is, what is the
importance of animal welfare? In a general sense, good animal welfare benefits the stockman, the animal in question, and the consumer of the animal products. Without animal welfare, there would be no high- quality meat, eggs, or milk. In poultry production, if the birds are stressed, abused, and mistreated, the egg production by the layers will decline (Alm et al., 2016). Similarly, if broilers and other animals kept for meat are poorly handled and poor slaughtering practice, the meat will be of poor quality or contaminated (Faucitano, 2018; Shimokomaki et al., 2017). The benefits of animal welfare can be pre- sented in terms of improved animal health (Dawkins, 2017; Green et al., 2012; Salois and Baker, 2018), improved animal product quality (Dawkins, 2017; Llonch et al., 2015), reduced animal mortality rate (Dawkins, 2017; Salois and Baker, 2018), reduced risks of zoonotic dis- eases (Dawkins, 2017; Okinda et al., 2019), improved disease resistance (Dawkins, 2017; Hoerr, 2010), and farmers satisfaction (Hemsworth et al., 2015; Hemsworth and Coleman, 2010). However, with a good an- imal welfare practice, the prices of animal products have been seen to rise, and some consumers aren't willing to pay for high welfare practices (Healy, 2018; Heise and Theuvsen, 2018).
However, there are several hindrances in animal welfare evalua- tion, such as; the difficulty in measuring physiological and ethologi- cal responses in real-farm animal husbandry conditions (Alm et al., 2016; Bessei, 2018), the difficulty in distinguishing between normal,
abnormal, and disturbed behaviors (Bessei, 2018). Furthermore, from the studies of Hughes et al. (2018) based on human and animal neural responses, under constant conditions, then the welfare situa- tion won't be considered as well-being even in good conditions and management if the psychological balance isn't offset. Finally, animals are complex individual and time-variant (CIT) systems that are indi- vidually different and respond differently at different moments. Thus, they can't be analyzed as a typical classical steady-state system (Berckmans, 2006). Moreover, welfare condition indicators can be contradicting, and according to Alm et al. (2016), presently, there is no consensus on the ideal technique to access animal welfare. PLF itself hasn't lived up to its expectation. Lehr (2014) presented the main obstacles to the implementation of PLF as, lack of consistent marketing, lack of direct cooperation between farmers, biologists, engineers, and economists, little focus on data interpretation and control, the technological gap between consumers and modern farming, and lack of awareness, animal complexities, lack of robust- ness of developed techniques and technologies, and the difficulties and reliability of PLF systems in commercial farms. Despite the men- tioned hindrances on assessing animal welfare and adoption of PLF, studies have reported on several PLF techniques in the monitoring of various bio-responses and bio-processes aiming at improving effi- ciency, animal health, welfare, and farm economy in large-scale chicken production.
Several research efforts have been reported in literature on the development of computer vision systems for chicken monitoring of welfare-related issues such as weight, lameness, behaviors, temper- ature, activities, and health (Aydin, 2017a, 2017b; Mortensen et al., 2016; Okinda et al., 2019; Wang et al., 2019b; Zhuang et al., 2018; Zhuang and Zhang, 2019). This review aims at providing a proper synthesis of literature to provide clear guidance on the state-of- the-art techniques and the potential future direction on the monitor- ing of welfare-related bio-processes and bio-responses in chicken production. Therefore, this work will focus on up-to-date research advances to provide useful technical information for the develop- ment of more relevant and reliable computer vision techniques for the monitoring of welfare-related bio-processes in chicken production.

Machine learning-based poultry monitoring systems

As already mentioned, the machine learning-based system follows a typical image analysis procedure with the application of conventional machine learning algorithms, as shown in Fig. 2.







Image pre-processing

Image pre-processing operations are vital to obtaining a robust and efficient ROI segmentation performance. The pre-processing operations involve the following: resizing, color-space transformation, contrast en- hancement, normalization, and denoising. RGB is the widely applied color space, but due to the high correlation between R, G, and B color spaces, it's not suitable for object segmentation (Cheng et al., 2001). Hence, several transformation techniques have been explored by sev- eral studies aiming at achieving accurate image object segmentation. The commonly used color space models are RGB (red, green, and blue), LAB (L for brightness, A for values from red to green opponent colors along A-axis, and B for values from blue to yellow opponent colors along B-axis), HIS (hue, saturation, and intensity), HSV (hue, sat- uration, value), YCrCb (Y for luminance component, Cb and Cr for blue- difference and red-difference chroma components, respectively) just to mention a few (Ibraheem et al., 2012). All these color spaces are com- puted from RGB, as presented in Table 1.
As mentioned above, color transformation operation is always per-
formed to obtain better ROI segmentation results. Zhuang et al. (2018) applied the HSV and LAB (CIE L⁎a⁎b⁎) color spaces to extract poultry
image color features during background removal. The HSV color space is robust to variation in illumination and more aligned to human color perception (Hamuda et al., 2017), while the LAB color space is invariant to sensor sensitivity (Ireri et al., 2019). Based on the presentation by Zhuang et al. (2018), the S and V color spaces are not conducive for chicken segmentation because the resultant image intensities were widely distributed and divergent. However, H color space produced a clear visible broiler body segmentation, but the segmentation accuracy was somehow lower when compared to the a-b map. Therefore, Zhuang et al. (2018) applied the a-b map to describe the color space while l-a as an auxiliary description in broiler body segmentation. To in- tensify the contrast of colored images, Pereira et al. (2013) applied the HIS color space to contrast the background from the foreground. In HIS space, H represents the specific color, S represents how saturated a color is in comparison to white, and I represent the brightness of the color. Additionally, Guo et al. (2020) compared the visualization effect
of L⁎a⁎b⁎ and RGB (RG, RB, and GB) and established that the GB space
had higher classification and visualization efficiency. Most studies in chicken monitoring using visible light-based sensors are performed in RBG color space because of the numerous advantages of RGB color


Table 1
Color space transformation from RGB. Adopted from Wang et al. (2019a).


model i.e., its suitable for color display, easy to use and it's an intuitive model for color creation and manipulation (Chavolla et al., 2018).
The image resizing operation is performed to minimize the compu- tation cost and complexities by reducing image resolution size. Image cropping can also be implemented as an image resizing operation and ROI extraction as performed by Mehdizadeh et al. (2015) in the extrac- tion of the chicken head in beak and head motion analysis. Color images captured under varying or insufficient illumination have poor contrast and noise, which affects the performance of the subsequent image pro- cessing techniques. Contrast improvements can be performed during the image acquisition phase or as a pre-processing operation. A con- trasting background (dark floor for white birds) can be manually installed to obtain a clear outline of the birds (Amraei et al., 2018, 2017b, 2017a; Mollah et al., 2010; Pereira et al., 2013). The pre- processing contrast improvement technique is by grayscale pixel inten- sity enhancement by changing the grey-level between the range of 0 to
255. Additionally, global histogram equalization has been used to en- hance contrast and alleviate light variations by normalization of image histograms. Pereira et al. (2013) applied R, G, B, S, and I space pixels nor- malization before a deduction of the inverse of r and b mathematical op- erations on pixel matrices. However, in the tracking of birds, normalization operation is always performed to eliminate errors, i.e., noise occurring due to the difference in sizes of birds (Aydin et al., 2010). Generally, normalization is performed to scale the data to a rea- sonable extent and to turn the images into normalized non-dimensional data (Wang et al., 2019a).

Region of interest segmentation

Image object segmentation can be referred to as a process of forming connected objects with relatively homogenous properties by grouping related pixels together or partitioning an image into multiple segments with similar attributes (objects) (Ladický et al., 2009; Wang et al., 2019a). The main aim of the segmentation process is to transform an image to be more meaningful and easier to analyze and interpret (Pal and Pal, 1993). The meaningful segments, also known as ROI, is the ini- tial step in transforming a color or a grayscale image from a low-level image processing task to a high-level image description task. Therefore, it is one of the most critical tasks in object detection by image process- ing. The success of image processing and analysis depends on the effi- ciency and reliability of the segmentation process. However, the accurate partitioning of an image is quite challenging and a lot of studies have been performed to achieve efficient and robust ROI extraction.

Features for segmentation and approaches
Efficient discriminating features are fundamental in separating the

Color model
Channel  Color space transformation from RGB
background from the birds. For visual light-based sensors under con- stant light conditions, color is the most invariant feature against object

XYZ	X	X = 0.607R + 0.174G + 0.200B
Y	Y = 0.299R + 0.587G + 0.114B
Z	Z = 0.066R + 1.11B
translation, rotation, and partial occlusion (Wang et al., 2019a).
Zaninelli et al. (2018) applied this approach in the processing of ther- mographic images to perform background color thresholds. However,

L ∗a ∗b ∗	L ∗
L∗ =
116Y 1/3  if Y > k
903.3Y  if Y≤k
k = 0.008856
for the IR depth-based sensors, since the pixel intensities are distance (depth) values, distance threshold is often performed to remove the

a ∗	a ∗ = 500[f(X) − f(Y)] where
background (Jana, 2012; Okinda et al., 2019). Additionally, the sensor


f (t) = 
t  if t > k
7.787t + 0.1379 if t≤k
being invariant to variation in ambient light conditions, the depth infor-

b ∗	b ∗ = 200[f(Y) − f(Z)]
mation can be applied to assist in the segmentation process in 2D sys-

HSV	H
H = 8><
p(G−B)  if M = R
120 + p(B−R) if M = G where M = max {R, G, B},
tems (Okinda et al., 2019). Generally, depth images have a less odious task of background removal compared to color images. This can be ob-

240 + p(R−G) if M = B
m = min {R, G, B}, p = 60m
served in the study by Mortensen et al. (2016), where multiple broilers could be detected in a depth image, based on a height function defined over a depth image based on the watershed segmentation method. Sim- ilarly, in the study by Okinda et al. (2019), the background was removed by a simple depth threshold and image subtraction.
The main segmentation approaches can be grouped into three tech- niques: background subtraction, foreground detection, and learning- based techniques. The first two techniques are a two-stage process



involving foreground detection and region validation, while the latter is a model development technique. Threshold-based and background sub- traction techniques are the most widely used technique for foreground detection in chicken monitoring systems (Amraei et al., 2018, 2017a, 2017b; Aydin, 2017b; Aydin et al., 2015, 2013; De Wet et al., 2003; Leroy et al., 2006; Mollah et al., 2010; Sergeant et al., 1998; Zaninelli et al., 2018). Table 2 presents an overview of the segmentation tech- niques applied in chicken monitoring systems.
In the image background subtraction technique, the conventional approach is to obtain a background image depicting a scene without ob- jects of interest, and then to perform a frame-by-frame subtraction under the condition that the camera is static. The pixels for which the difference is above a set threshold are labeled as belonging to the ROI (animal). The background image can be manually captured or via an au- tomatically updating video frames, i.e., continuous frame averaging, loopy belief propagation, Gaussian mixture models, and frame copy out- side foreground neighborhoods (Li et al., 2019a). However, Sergeant et al. (1998) explained the shortcomings of the application of back- ground subtraction in chicken tracking as; high density of birds in a pen may exceed the 1:1 ratio of moving objects to background. If the background image is obtained by automatically updating continuous frames, only moving birds can be identified. Thus, stationary birds will be detected as the background. Additionally, poor contract between background and foreground is a hindrance to background subtraction.
The adaptive threshold technique based on Otsu (1979) is the most classical technique based on global intensity histograms (equalized) of an image to determine the threshold value. Otsu's method establishes a threshold that maximizes the variance of the pixel intensities between classes. To improve the threshold-based segmentation efficiency,
background subtraction is often performed before threshold segmenta- tion (Li et al., 2019a).
The model-based segmentation approach through computational expensive, but with proper training, can produce excellent segmenta- tion results. The template matching technique finds similar objects based on a visual template and image properties. Ellipse modeling and Active Shape Model (ASM) segmentation approaches are popular tem- plate matching techniques in object segmentation in animal studies (Leroy et al., 2006, 2005; Li et al., 2019a; Zhuang et al., 2018). An object's shape outline is tracked by fitting an ellipse in the ellipse modeling or Point Distribution Model (PMD) in ASM modeling. Zhuang et al. (2018) applied a bird's color features based on a template image by el- lipse modeling and color features. DL approaches have also been made as a segmentation technique, where a model is trained to detect partic- ular objects in an image. Convolution neural network (CNN) based de- tector and a correlation filter-based tracker has also been applied in pig detection (Zhang et al., 2019). In poultry tracking, Fang et al. (2020) applied a deep regression network to detect and track birds in a pen. Similarly, Zhuang and Zhang (2019) used a CNN model to detect and predict sick birds. More details on DL techniques are presented in Section 3.

Region of interest validation
Not all the extracted regions after foreground detections are often the ROI (may contain noise and misclassified pixels). A region validation is usually applied to improve quality by removing regions that aren't consistent with the features of the ROI. These are often artifacts present in the background, such as droppings and shadows due to feathers and the head within the foreground (Amraei et al., 2017a, 2017b; Mollah




Table 2
Main segmentation techniques in poultry monitoring systems.




et al., 2010). The most straightforward techniques for region validation are morphology operations and ellipse fitting (consistent with area and size constraints specified for the chickens).
Morphological operations based on erosion and dilation are the basic candidate validation processes performed to avoid discontinuities and isolated areas. Erosion and dilation functions remove and add pixels on an object boundary in an image, respectively. Erosion operation smoothens the contour of an object by eliminating both narrow isth- muses and thin protrusions while dilation operation performs hole fill- ing inside an object contour. The sizes and shapes of the structural elements used in these morphological operations determine the num- ber of pixels removed or added to the objects in an image (González et al., 2004). Additionally, these morphological operations assist in the removal of unnecessary noises from the image (image smoothening). However, noise removal and smoothening can be performed by filtering methods such as Gaussian, homomorphic, blur, adaptive median, and anisotropic diffusion filters (Nakarmi et al., 2014; Tania and Rowaida, 2016). Furthermore, image filtering can be performed as a precaution against over-segmentation in intensity based images (depth images) (Mortensen et al., 2016). In weight estimation, the head and the tail re- gions are often removed for an accurate estimation of weight by image shape features. In intensity-based images, Mortensen et al. (2016) ap- plied morphological opening to eliminate local minima due to the head while preserving local minima associated with the body. However, Amraei et al. (2017a), Amraei et al. (2017b), and Amraei et al. (2018) applied the Chen-Vese model to remove the chicken's head.
Another candidate validation technique involves the incorporation
from the whole shape or the contour region. Each section is further subdivided into structural and global, depending on whether the shape is represented as primitives or as a whole (Zhang and Lu, 2004). The commonly applied shape morphological properties are the shape geometric features, also known as shape simple descriptors (Okinda et al., 2019). Shape geometric features can be simple shape measure- ments or shape indices. Shape measurements are the properties of the ROI, such as area, perimeter, major and minor axis lengths, centroid, etc. Shape indices are a combination of shape measurements such as cir- cularity, eccentricity, average bending energy, convexity, etc. as illus- trated in Table 3. Shape indices have the advantage of being invariant to rotation, translation, and scale because they are dimensionless values (Zhang and Lu, 2004).
Shape geometric features can be categorized as 2D and 3D features depending on the dimensionality of the image to be analyzed. Several shape geometric features have been successfully applied in chicken monitoring systems, as presented in Table 4 (Note that the 2D geomet- ric features can be projected to 3D). Besides the mentioned geometric shape features and indices, Okinda et al. (2019) proposed the use of shape complexity measure, which is a function of entropy of the shape's medial axis transform (region-based shape descriptor) (Okinda et al., 2018b; Panagiotakis and Argyros, 2016). The complexity feature was dependent on the shape structure (number of skeletons). The more the shape structures, the higher the shape complexity; hence, this

Table 3
Shape geometric parameters applied in shape analysis.

of prior knowledge about the object's shape to achieve an accurate seg- mentation from noisy pre-processed data. Leroy et al. (2006) applied
Dimension Geometric
features
Definition

the ASM to model a 2D chicken shape and its deformations. However, the same study concluded that the main part of the chicken's body
could be well approximated using a simple ellipse shape as Point Distri-
2D	Centroid	g =

Area
n
n ∑ xi, ,yi i=1
n

bution Models (PDM), which was initially utilized in the previous study of Leroy et al. (2005).

Feature extraction


Perimeter

Radial distance
A	1 ∑ y  x −x  y
2 i=1
n
Pshape = ∑‖ xi+1, ,yi+1 −(xi, ,yi )‖
i=1
ρi = ‖pi − g‖

Features are visual characteristics that can be used to correlate to a specific bio-response or bio-process under investigation. These visual characteristics are extracted from the ROI. However, these features dif-
Convexity	 Phull 
Pshape
Solidity	Ashape
Ahull
Aspect ratio		(xmax −xmin ) (ymax −ymin )
Circularity	δCa = Ashape = 4πAshape

fer from the segmentation features that are used to extract the chicken from the background. In machine vision systems, these features have to be extracted manually from all images as opposed to DL techniques.
Generally, in chicken monitoring systems, these features can be divided
ratio (area) Circularity ratio (perimeter)
Acircle

δCp = Ashape
circle
Pshape

into three broad categories, i.e., morphological features, locomotor fea- tures, and optical flow measures (Aydin, 2017b; Dawkins et al., 2012;
Circle variance

δρ = σρ where μ

k−1
P = 1 ∑ ρi and σ
i=1
s1ﬃﬃﬃkﬃﬃ−ﬃﬃﬃ1ﬃﬃﬃﬃﬃρﬃﬃﬃﬃ−ﬃﬃﬃﬃﬃμﬃﬃﬃﬃﬃﬃ2ﬃﬃ


Mortensen et al., 2016; Okinda et al., 2019).

Morphological features
Morphological features describe the shape and size of an object and have been frequently applied in the description of several agricultural
products. The shape features have been applied in sick broiler detection
Average bending energy Hole area ratio Ellipse variance
n−1
δE = 1 ∑ κ(s)2
n s=0

 Ahole 
Ashape
δd = qρﬃﬃﬃTﬃﬃﬃMﬃﬃﬃﬃ−ﬃﬃﬃ1ﬃﬃﬃρﬃﬃiﬃﬃ

by posture analysis (Okinda et al., 2019; Zhuang et al., 2018). As changes in posture, such as depressed-bird-look-posture, indicates disease oc- currences. Size has long been an observed feature, even on human
Eccentricity	δe = λ1
Compactness δR = Ashape where Abox is the area of a bounding box Elongation	 Hbox 
Wbox

health and growth. Animal size can be used to detect the occurrence of several vitalities such as diseases, on feed conversion ratio, growth,
3D	Centroid	g
Surface area
n
3D = 1 ∑(xi, ,yi, ,zi)
i=1
k	k

and market readiness (Okinda et al., 2018a; Wongsriworaphon et al.,	A3Dshape = ∑ AT  = 1 ∑‖(pi2−pi1)∗(pi3−pi1)‖ where AT is

2015). In object analysis, morphological properties are defined by the

k  2	k
i=1	i=1

shape and structure of the object or parts of the object. Shape-based ob-
the area of the kth triangle with pkv vertices v = 1, 2, 3 Volume	k	k

ject analysis techniques are often preferred because of their stability
against sensor noise since they are invariant to light and color variations

Sphericity


3 i=1  k	k
1	2


6 i=1


γ =  Asphere  = π3 (6V 3Dshape )3

(Kurnianggoro and Jo, 2018).
S  A3Dshape
A3Dshape

Shape representation and description techniques can be divided into
Willmore
W = 1 R (k1−k2 2dA

two broad groups, i.e., contour-based and region-based methods. These classifications are categorized based on whether features are derived
Convexity	 A3Dhull 
A3Dshape

The applied shape morphological features in poultry monitoring systems.

Classification	Features	Description	Reference

Global
region-based shape descriptions
Area	Number of pixels within the ROI	Pereira et al. (2013), Zaninelli et al. (2018), Guo et al. (2020), Aydin et al.
(2015), Aydin (2017b), De Wet et al. (2003), Mollah et al. (2010), Mortensen et al. (2016), Amraei et al. (2018), Amraei et al. (2017b), Amraei et al. (2017a), Neves et al. (2015)

Minor axis length Major axis length
Length of the minor axis of a fitted ellipse that has the same normalized second central moments as the ROI Length of the major axis of a fitted ellipse that has the same normalized second central moments as the ROI
Leroy et al. (2006), Nakarmi et al. (2014), Amraei et al. (2018), Amraei et al. (2017b), Amraei et al. (2017a)
Leroy et al. (2006), Nakarmi et al. (2014), Amraei et al. (2018), Amraei et al. (2017b), Amraei et al. (2017a)

Centroid	Center of mass of the ROI	Leroy et al. (2006), Sergeant et al. (1998), Nakarmi et al. (2014), Aydin et al. (2015), Aydin (2017b), Nääs et al. (2018), Aydin et al. (2013)

Orientation	The angle between the x-axis and the major axis of a fitted ellipse that has the same second-moments as the ROI.
Leroy et al. (2006), Aydin et al. (2015), Aydin (2017b), Aydin (2017a)

Elongation	The ratio of height to width of the ROI's bounding box	Okinda et al. (2019), Zhuang et al. (2018)

Area-linear rate
The ratio of the area of the ROI to its perimeter	Zhuang et al. (2018)

Structural	Media axis	An image skeleton	Zhuang et al. (2018)

region-based shape descriptions
Radial distance
The distance between a point on the boundary and the centroid
Pereira et al. (2013), Mortensen et al. (2016)

Global
contour-based
Perimeter	The number of pixels around the boundary of a ROI	Pereira et al. (2013), De Wet et al. (2003), Mortensen et al. (2016), Amraei
et al. (2018), Amraei et al. (2017b), Amraei et al. (2017a)

shape descriptions
Eccentricity	The ratio of the Eigenvalues (λ1 and λ2) of a covariance matrix of a fitted ellipse over a ROI.
Okinda et al. (2019), Mortensen et al. (2016), Amraei et al. (2017b)

Compactness or rectangularity
The ratio between the size of shape compared to the size of its bounding box

Concavity	The measure of a shape's concaveness by how the derivative of its function is changing (curving in)
Zhuang et al. (2018)

Circularity ratio (perimeter) Circle variance
The ratio between the size of the ROI compared to the area of a circle that has the same perimeter with the ROI's perimeter
The ratio between standard deviation and averaged value of the radial distance from all points in the ROI's boundary
Zhuang et al. (2018)


Okinda et al. (2019)

Convexity	The ratio between the perimeter of the convex hull from a ROI compared to its perimeter
Okinda et al. (2019)




feature successfully described a chicken posture shape. Additionally, Pereira et al. (2013) introduced the shape coefficients features, which were indices computed from the area, perimeter, and radial distance (minimum and maximum radial distances).

Locomotor features
Locomotor features are one of the most important characteristics used to identify poultry gait score (GS) and have been widely applied in the monitoring of birds regarding lameness, activeness, and health. Generally, monitoring poultry mobility helps to detect the occurrence of an infection and infestation, and provides a basis to evaluate if the management procedures and environmental conditions are conducive (Aydin, 2017b, 2017a; Okinda et al., 2019). Based on the idea by Winter (1985), the movement of a subject is an effort rather than the cause of the underlying problem. Hence, the locomotor features can be subdivided into two categories, i.e., kinematic and kinetic features.
Kinetic analysis was introduced as a technique to analyze the pain levels by the pressure a bird exerts on a particular foot. Kinetic features are calculated by analyzing the walking forces on the toes of a chicken such as forces on the middle, medial, back, and lateral toes and metatar- sal pad as a chicken walk on a pedobarographic surface or a piezoelec- tric crystal sensor (Corr et al., 2007, 1998). Therefore, they are never applied in computer vision systems. Additionally, as Caplen et al. (2012) explained, few steps were analyzed in the kinetic analysis due to bird pausing or sitting on the surface; hence, high levels of data redundancy.
Kinematic features are calculated by analyzing the walking motion and speed of a bird by computing its body displacement using reference body positioned (markers are usually used) on the hook, knee, and metatarsus. The relative displacement of these reference markers can
be tracked in both 2D space and 3D space. 3D space required a calibrated stereo camera to compute the 3D kinematic data, as performed by Caplen et al. (2012) and Caplen et al. (2013). However, these are consid- ered intrusive kinematic features (markers in contact with the bird), which would be infeasible in a real fam environment and considering poultry welfare criterion. The non-contact kinematic moments of a bird include the walking velocity, acceleration, displacement, walk speed, body oscillations, and movement frequency (Aydin, 2017b; Dawkins et al., 2013; Nääs et al., 2018). In the analysis of feeding behav- ior, kinematic variables from mandibulations can be analyzed for an ef- fective feeder design and feeding behavior analysis (Mehdizadeh et al., 2015). Table 5 lists the mobility features that have been applied in chicken monitoring systems.

Optical flow measures
Optical flow can be defined as the pattern of visible motion of the ob- jects in the visual scene, or the distribution of the apparent velocity of motion of the brightness pattern in the image (Horn and Schunck, 1981). Therefore, optical flow analysis has been widely applied to detect motion in several studies based on “no flow” and “flow” analysis be- tween consecutive frames. Optical flow can be categorized as sparse (flow vectors of few pixels) and dense (flow vectors of all pixels) optical flow. However, dense optical flow is computationally expensive. There- fore, in most studies, image frames are often divided into pixel blocks (Colles et al., 2016; Dawkins et al., 2017, 2012). Optical flow techniques can further be categorized as; differential techniques, energy-based methods, region-based matching, and phase-based techniques (Barron et al., 1994; Horn and Schunck, 1981). Differential techniques compute the flow velocity from spatiotemporal derivatives of image intensity (Lucas and Kanade, 1981). Region-based matching defines flow velocity


Table 5
The applied locomotor features in poultry monitoring systems.


Speed	The distance walked by the bird or moved by a bird's body part per unit time.
Image analysis (number of acquired images depending on the capture frame rate as the bird moves across the walking test corridor)
ms−1	Okinda et al. (2019) mms−1	Aydin (2017b)

Qualisys Track Manager (QTM) software	mms−1	Caplen et al. (2012), Caplen et al. (2013)

Manually measured the path of a bird that walked for over 10 s using an acetate sheet was placed against the computer screen
Dawkins et al. (2013)

Image analysis (rate of change of beak displacement)	mm ms−1 Mehdizadeh et al.
(2015)

Velocity	Change in the chicken displacement per unit time  Change in displacement between two consecutive periods, divided
by the time difference
ms−1	Nääs et al. (2018)

Acceleration	The rate of change of a bird's walking or a bird's body part velocity
Change in the bird's velocity in a given amount of time	ms−2	Nääs et al. (2018) Change in the beak's velocity with respect to time	mm ms−2 Mehdizadeh et al.
(2015)





as a change that provides the best match between image regions at dif- ferent times. This technique was introduced due to the impracticability of Differential techniques due to noise and aliasing during image- acquisition and few numbers of frames (Anandan, 1989). Energy- based methods, also called frequency-based methods, apply velocity tuned filters. Finally, in phase-based techniques, the flow velocity is de- fined in terms of the phase behavior of the output of a band-pass filter (Barron et al., 1994).
The optical flow measures include spatial mean, skewness, variance, and kurtosis of the estimated flow velocities over the image. The de- scription of these features is given in Table 6. These measures are ob- tained from each frame in a time series. The average of these features is computed over a period of time to give a summary of the object being monitored. In poultry monitoring, optical flow analysis has been applied in monitoring of behavior and lameness (Colles et al., 2016; Dawkins et al., 2013, 2012, 2009).


Table 6
The optical flow measures.

Optical flow measures	Description



Other features
Apart from the features mentioned above, several other features have been developed and derived in poultry monitoring. The behavior sequence can be quantified to a fractal structure. Therefore, a correlation can be developed between the fractal-like properties of behavior se- quences and a bio-response or bio-process by determining the mea- sures of complexity in those behavior sequences (Marıa et al., 2004; Rutherford et al., 2004). Marıa et al. (2004) developed a fractal-like bi- nary behavior sequence for each chicken activity (observed). They per- formed a detrended fluctuation analysis to quantify the correlation properties of those fractals-like behavior sequences. This study established that the fractal complexity of the behavior sequence de- creased with an increase in stress due to insufficient energy to perform complex behaviors. Pixel profile, moving pixels, or the proportion of pixel changes have also been used as features in chicken tracking. Sergeant et al. (1998) performed an analysis of the frequency of the pixels of the ROI to determine the number of bids within the image. Ad- ditionally, Fraess et al. (2016) analyzed the percentage pixel change using EthoVision XT 10 (Noldus, Leesburg, VA, USA) to determine chicken activities from video frames. The difference in the intensity values between subsequent frames (at the same coordinates) can be
computed to determine the activity index. Similarly, Aydin et al.

Mean
μ t
N
1	m t
(2013) and Aydin et al. (2010) applied this technique in their statistical

( ) = B ∑
i=1
i( ) 
analysis of chicken activity and GS. However, the latter applied the

Variance
σ 2

Skewness
N
(t) =  1  ∑ (mi(t)−μ(t))2
i=1
1  N	3
eYeNamic software to measure the bird's activities. Moreover, Van Hertem et al. (2018) determined the flock activity index and distribu-

Kurtosis
γ1 (t) =
(B−1) ∑ (mi (t)−μ(t))
i=1
σ 3 (t) 
1  N	4
tion Index after image analysis by the eYeNamic software for GS predic- tion. In this sense, the activity index was a measure of the movement of

γ(t) =
(B−1) ∑ (mi (t)−μ(t))
i=1
σ 4 (t) 
the birds in the images, while the distribution Index was a measure of



Where B is the number of pixel blocks applied to compute the optical
flow.
the occupied floor space in the house (Kashiha et al., 2013; Neves et al., 2015). A similar technique was applied by Youssef et al. (2015)



and Kristensen et al. (2006) in developing a close loop chicken behavior control system based on dynamic activity index, ambient temperature, air velocity, and light intensity.
Regarding kinematic features and lameness, Reiter and Bessei (1997) suggested that lameness can be detected by analyzing the differences in the vertical and lateral movements of the left and right leg. Therefore, walking trajectory features has been used as dynamic features to assess lameness in broilers. Similarly, trajectory and rotation features of sequences of a bird's image can be used in behavior detection (Leroy et al., 2005). There are two scoring tech- niques in birds, i.e., GS (Kestin et al., 1992) and latency to lie down (LTL) (Berg and Sanotra, 2003; Weeks et al., 2002). However, GS is a subjective technique, while LTL is invasive as the bird has to come in contact with water. Therefore, to develop a non-invasive technique (Aydin, 2017a) applied depth feature from a 3D depth image to determine the LTL and number of lying events (NOL) to as- sess lameness using a 3D vision camera.

Feature selection and dimension reduction

Feature selection involves choosing a subset of relevant features after feature extraction engineering before model development. The ex- tracted features may contain irrelevant and redundant variables that would influence the modeling task. A feature selection criterion is re- quired to measure the significance of each feature and to remove extra- neous features, by selecting a subset of variables from an input data that efficiently describes the input while reducing the effects of noise and other irrelevant variables but still capable of producing a generalized model (Chandrashekar and Sahin, 2014). Therefore, feature selection helps to provide an in-depth understanding of the dataset, reduces computational complexities, reduces the curse of dimensionality effects, and improves the general performance of the model. From a data-type perspective, Li et al. (2017) categorized feature selection techniques as similarity-based, hybrid feature selection, information-theoretical- based, statistical-based, sparse-learning-based, reconstruction-based, and deep-learning-based methods. In chicken monitoring systems, few features are always extracted; therefore, most studies don't per- form feature selection. However, Amraei et al. (2017a) and Amraei et al. (2018) performed feature selection based on a statistical-based method (correlation analysis) to select the best predictors in chicken weight estimation.
Dimension reduction isn't the same as feature selection. The dif- ference is that the resulting set of features after feature selection is always a subset of the original set of features before the feature se- lection process. However, the resulting set after dimensionality re- duction techniques does not have to be a subset of the original set of features before the dimension reduction process (as in Principle Component Analysis (PCA)). Thus, feature selection applies a subop- timal procedure to remove redundant data with tractable computa- tions. In summary, feature selection works on data attributes based on variance, while dimension reduction works on Eigenvalue and Eigenvector, making feature selection a special case of dimension re- duction. Considering that data becomes sparser in high-dimensional space (the curse of dimensionality), thus, affecting algorithms de- signed for low-dimensional space. Hence, dimension reduction transfers the original dataset from high dimensional space to a lower-dimensional space while preserving the essential features by finding the optimal approximation of the original dataset. Dimension reduction algorithms can be categorized according to their imple- mentation process, i.e., Feature Selection, Kernel Method, Project Method, Manifold Learning, Dictionary Learning, Sparse Learning, and Artificial Neural Network (Huang et al., 2019). The eYeNamic software in the analysis of video frames in the study by Van Hertem et al. (2018) generated a large amount of data. Therefore, the PCA dimension reduction technique was applied while minimiz- ing information loss.
Statistical analysis

Statistical analysis is often performed to determine the statistical re- lationship (inference) between the extracted features and the bio- process or bio-response being monitored. Generally, statistical infer- ence is a comparison of detailed statistics between an observational dataset and an appropriate reference distribution to determine the sig- nificance of those statistics in terms of mean values, standard deviation, differences among the means, etc. Statistical inference is a powerful tool for drawing scientific conclusions that efficiently apply existing data or those collected for the specific purpose of testing hypotheses, provided various assumptions are met, and specific hypotheses are specified. Sta- tistical tests can be categorized into two groups, i.e., Parametric and Non-Parametric tests. The parametric statistical tests make assumptions on the parameters of the population distribution (data is assumed to be normally distributed). In contrast, non-parametric tests make no such assumptions (Okinda et al., 2019). For every parametric test, there is a shadow non- parametric test. The choice of a statistical test is deter- mined by the underlying goal, as presented in Fig. 3.
Statistical correlational analysis, i.e., Pearson correlation (parametric test), was applied by Dawkins et al. (2012) to determine the relation- ship between optical flow and bird mortality. The same approach was presented by Dawkins et al. (2013) in determining the relationship be- tween optical flow, behavior, and welfare. One-way ANOVA was also employed by Caplen et al. (2012) and Caplen et al. (2013). The former analyzed kinematic features regarding chicken weight and lameness, while the latter analyzed kinematic features to determine the response of lame broilers to non-steroidal anti-inflammatory drugs. Non- parametric tests such as the Friedman test, Dunn test, Spearman's Ranke Order correlation test, and Wilcoxon Signed-Rank Test have been successfully applied in chicken lameness (Aydin, 2017b, 2017a; Aydin et al., 2015, 2013, 2010), health (Okinda et al., 2019) and behavior (Kristensen et al., 2007) monitoring systems. A description of the ap- plied statistical analysis in the monitoring of poultry is presented in Table 7.

Modeling techniques

The modeling step in poultry monitoring systems can be categorized as regression and classification tasks. Based on different modeling tech- niques, the tasks can be performed by conventional machine learning or DL techniques. Machine learning algorithms are computerized model- ing approaches based on sample data (use statistics to find patterns in data) to make decisions or predictions without being reprogrammed time and again (Bhargava and Bansal, 2018; Wang et al., 2019a). Ma- chine learning algorithms are grouped into three categories; Supervised Learning, Unsupervised Learning, and Reinforcement Learning.
Supervised Learning is a governed learning technique that incorpo- rates the use of example inputs and their desired outputs, and the main objective is to learn the pattern (training) that maps the inputs into outputs (Alpaydin, 2020). Supervised Learning involves model training and model testing tasks. The model training process is an essen- tial procedure performed using labeled data as the supervisory signal. Supervised learning algorithms learn a function that can map new in- puts into outputs (prediction) by iterative optimization of an objective function. The model testing is the application of new or unknown data to the trained model and observing the accuracy of the predicted out- put. In chicken monitoring systems, the outputs are weight (regres- sion), health condition (classification), or behavior (classification), among others. Linear, nonlinear, and logit regression, Support vector machine (SVM), Support vector regression (SVR), and artificial neural networks (ANN) are the mostly applied supervised machine learning al- gorithms that have been applied in poultry monitoring systems.
Linear regression applies a statistical approach to model the rela- tionship between one or more independent variables and a (one) de- pendent variable by fitting a linear equation. In linear modeling, the









Fig. 3. The flow chart for the selection of appropriate statistical tests. Adopted from Jaykaran (2010).


Table 7
Summary of the computer vision-based chicken monitoring systems in the literature.

Monitored bio-process and bio-responses	n	Statistical analysis	Model	Software	Accuracy  References Investigation

Behavior Behavior in relation to stress conditions
Behavior analysis of individually caged poultry
Relation of optical flow patterns between behavior, mortality, GS, and leg health
Optical flow patterns in broiler in behavior classification and GS The relationship between optical flow, behavior and welfare
7	ANOVA	Linear regression	–	0.76 (R2)  Marıa et al. (2004)
18	–	Transfer function (TF)	–	–	Leroy et al. (2006)
24	Pearson correlation test	–	–	–	Dawkins
et al. (2012)

40,000  Correlation analysis	–	–	–	Dawkins
et al. (2009)
35,000  Pearson correlation test	–	–	–	Dawkins
et al. (2013)

Effects of different light sources and illuminances on behavior
16	Non-parametric analysis of variance
–	Statistical Analysis
Software (SAS)
–	Kristensen
et al. (2007)

Behavior classification	–	–	Classification tree	Weka® version
3.4.11
70.3%	Pereira et al. (2013)



Feeding behavior (beak and head motion during feeding)
~350	–	YOLO v3	Darknet framework	92.09%	Wang et al.
(2020)
3	ANOVA	Linear regression	Minitab 17®	99.2% (R2) Mehdizadeh et al. (2015)

Tracking and behavior classification  15	Multi-regression	–	R statistical package	95%	Nakarmi
et al. (2014)

Feeding and drinking behavior classification
60	ANOVA	Linear regression	Statistical Analysis Software (SAS)
96.5	Li et al. (2019b)

Behavior monitoring for early detection of Campylobacter
31	Multi-level models	R statistical package	Colles et al. (2016)

Behavioral response to feeding events
136	ANOVA	EthoVision XT 10
SigmaPlot 11
Fraess et al. (2016)

Behavior recognition	3087	–	CNN	Visual Studio OpenCV3.5.0
Kinect for Windows SDK
96.4%	Pu et al. (2018)

Effects of wearing a backpack on behavior, health, and productivity Effects of micro-environment conditions on behavior and activity
60	Mixed logistic regression	–	–	–	Stadig et al. (2018)
45	–	Discrete transfer function	–	94%	Youssef
et al. (2015)

Effects of feeder types on flock behavior
14,000  General Linear Model (GLM)	–	MATLAB Minitab 15 ®
–	Neves et al. (2015)

Recognition of behavior phenotypes of layers
5	–	Dynamic modeling	Observer ®	70–96%	Leroy et al.
(2005)

Tracking  Tracking of individual birds	13	–	–	–	95%	Sergeant
et al. (1998)
10	–	Deep regression	Python	0.73	Fang et al. (2020)
10	Particle filter	–	–	Fujii et al. (2009)

Detection of multiple nest occupations
Monitoring broiler chicken floor distribution
–	ANOVA	–	R statistical package	95.5%	Zaninelli
et al. (2018)
126	ANOVA	BP neural network	MATLAB	0.996 (R) Guo et al. (2020)

Real-time malfunctioning in a broiler house detector
28,000  –	Linear regression	eYeNamic system
MxControlCenter MATLAB
95.24%	Kashiha
et al. (2013)

Evaluation of a laying-hen tracking  6	–	Hybrid Support Vector Machine (HSVM)
OpenCV	0.79	Wang et al. (2016)

Hen tracking in an environmental preference chamber
4	–	Ellipse-fitting model	MATLAB	95.9
± 2.6%
Kashiha
et al. (2014)

Health	Monitoring of heat stress	Correlation	Faster R-CNN	Caffe	–	Lin et al.
(2018)
Automatic detection of sick chickens –	–	Residual neural network	–	95%	Zhang and Chen (2020)

Detection of sick broilers	20	–	SVM, Bayesian classifier, Random Forest and ANN
Visual Studio 2013
OpenCV 2.4.13
99.469%	Zhuang et al.
(2018)

Early detection and prediction of sick chickens
280	Friedman test, Spearman's correlation test, Wilcoxon Signed-Rank Test
SVM, ANN, and logit regression
Statistical Package for the Social Sciences (SPSS)
Kinect for Windows SDK
MATLAB
0.978	Okinda et al. (2019)

Classification of broiler droppings for intestinal disease detection
10,000  –	Faster R-CNN and YOLO-V3	Tensorflow framework
Darknet framework
93.3%	Wang et al. (2019b)

Detection of sick broilers	400,000 –	Improved Feature Fusion Single Shot MultiBox
OpenCV	99.7%	Zhuang and Zhang
(continued on next page)


Table 7 (continued)

Monitored bio-process and bio-responses	n	Statistical analysis	Model	Software	Accuracy  References Investigation
Detector (IFSSD)	(2019)

Prediction of welfare outcomes for broiler chickens
816,000 –	Bayesian multivariate linear model
–	–	Roberts
et al. (2012)

Head surface temperature extraction
20	–	–	MATLAB	92.77%	Xiong et al. (2019)

Activity	Identification of activities of chickens with different GS Identification of activities of chickens with different GS
30	Friedman test Dunn test
30	Friedman test Dunn test
MATLAB	–	Aydin et al. (2013)
eYeNamic software	–	Aydin et al. (2010)

Predicting broiler GS based on activity monitoring and flock data
196,000 ANOVA
Blande Altman method
–	eYeNamic software	0.53–0.74
(R2)
Van Hertem et al. (2018)

Effects of micro-environment conditions on behavior and activity Image analysis to measure activity index of poultry
45	–	Discrete transfer function	–	94%	Youssef
et al. (2015)
15	–	Linear regression	Turbo Pascal	0.5 (R2)	Bloemen
et al. (1997)

Effects of light intensity on the dynamic activity of broiler chickens
84	Spearman correlation ANOVA
Discrete transfer function	MATLAB, Statistical
Analysis Software (SAS)
Kristensen et al. (2006)

Lameness Classification of lying event to assess lameness of broilers
250	Friedman test Dunn test
–	MATLAB	–	Aydin et al. (2015)

Kinematic analysis regarding GS	–	ANOVA	–	MLwiN v2.22	–	Caplen et al.
(2012)

The response of lame broilers to non-steroidal anti-inflammatory drugs
32	ANOVA	–	MLwiN v2.22	–	Caplen et al. (2013)

Early detection system for lameness in broilers
250	Friedman test Dunn test
–	MATLAB	–	Aydin (2017b)

Estimating the GS of broiler chickens
300	–	Paraconsistent logic	MATLAB	50–100%	Nääs et al. (2018)

Assess the level of inactivity in broiler chickens
250	Friedman test Dunn test
Linear regression	Statistical analysis software (SAS) MATLAB
94.49%	Aydin (2017a)

Walking behavior of heavy and light broilers
36	General Linear Model (GLM)	–	LabView	–	Bokkers
et al. (2007)

Weight	Monitoring daily growth rates of broiler chickens
50	–	Non-linear regression	–	0.94–0.97 (R2)
De Wet et al. (2003)

Prediction of broiler chickens weight using 3D computer vision
48,000  –	Multivariate linear
regression ANN
Bayesian ANN
MATLAB	92.2%	Mortensen
et al. (2016)

Boiler live weight estimation	100	Paired t-test	Linear regression	IDRISI 32	0.99 (R2)  Mollah et al.
(2010)

30	Paired t-test, correlation analysis
ANN	MATLAB	0.98 (R2)  Amraei et al. (2017a)

20	Paired t-test	SVR	LIBSVM	0.98 (R2)  Amraei et al. (2017b)
30	Correlation analysis	Transform function	MATLAB	0.98 (R2)  Amraei et al.
(2018)


relationships are developed by a linear predictor function (Rencher and Christensen, 2012). However, for non-linear regression, the dependent variable is modeled as a non-linear function of one or more independent variables. In this case, data is fitted by successive approximations methods. The two regression techniques have been widely applied in chicken weight estimation. De Wet et al. (2003) developed two linear models to estimate chicken weight based on image object surface area and perimeter. The same technique was applied by Mollah et al. (2010) but incorporated the age of the chicken. For comparative analy- sis with other regression algorithms, Mortensen et al. (2016) also ap- plied linear regression based on 2D, 3D image features and age. The logit regression is applied when the dependent variable is a binary (di- chotomous). It models the relationship between a dependent binary variable and one or more independent variables. Okinda et al. (2019) applied logit regression to classify broiler chicken as sick or healthy using broiler morphological and locomotor features.
SVR is an extension of SVM to solve regression problems. SVM can perform both linear (non-probabilistic binary linear classifier) and non-linear classification by applying kernel functions to implicitly map inputs into high-dimensional feature space (Cortes and Vapnik, 1995). The kernel functions solve the quadratic programming problem of
separating support vectors in the training data vectors by finding the appropriate hyperplane. The SVM kernels include linear, polynomial (quadratic and cubic), and radial basis function (RBF) kernels (Nyalala et al., 2019; Okinda et al., 2020). RBF kernel was applied by Amraei et al. (2017b) in chicken weight estimation, while Okinda et al. (2019) applied all the SVM kernels mentioned above in chicken health status classification.
Biological neural networks inspired the ANN machine learning technique. The ANN simulates the way the human brain analyzes and processes information. Therefore, it's a non-linear statistical model. ANN consist of input, hidden, and output layer. The hidden layer transforms the input into output by solving an optimization problem by minimization of a loss function during optimization (Samarasinghe, 2016). The most basic types of ANN are the feedforward neural network and recurrent neural network, which are trained by the Backpropagation algorithms. The number of neu- rons in the hidden layer is often adjusted during training to minimize network error. This can be observed in the study by Mortensen et al. (2016) in the selection of 3 and 10 neurons in the hidden layer. How- ever, these regression models performed reasonably similarly. The Bayesian ANN being a probabilistic model using an ANN regression

function was also applied by Mortensen et al. (2016) due to its ap- proach to outlier detection. Furthermore, its performance was more superior to linear regression and other ANN models in broiler weight estimation.
To evaluate the performance of different backpropagation training al- gorithms, i.e., Gradient descent, Bayesian Regularization, Scaled Conju- gate Gradient, and Levenberg-Marquardt training algorithms, Amraei et al. (2017a) developed three ANN models for broiler weight estima- tion. Based on the training algorithms mentioned above, the Bayesian regulation training algorithm resulted in the best performing ANN model at an R2 of 0.983 and RMSE of 82.37 g on the testing data set. The performance of logit, SVM, and ANN classifiers were evaluated and compared by Okinda et al. (2019) in broiler health classification. The RBF SVM outperformed all the other models at an accuracy of 0.978. Sim- ilarly, Zhuang et al. (2018) reported the superiority of SVM at about 99.5% accuracy in sick birds detection. In determining the chicken distri- bution, Guo et al. (2020) applied a backpropagation neural network and a normalized chicken image surface area. Other supervised Learning that has been applied in chicken monitoring is the decision tree learning, which was implemented by Pereira et al. (2013) in the classification of broiler breeder behaviors whereby the extracted geometric features were the branches, while the leaves represented the behavior labels.
Unsupervised Learning and Reinforcement Learning haven't been applied to poultry monitoring systems (modeling techniques). How- ever, this review will present a brief highlight of the two learning tech- niques. Unsupervised Learning has no training labels for training samples, unlike supervised Learning. Unsupervised learning algorithms find suitable structures and patterns in unlabeled data by modeling of probability densities over inputs (Hastie et al., 2009). The two main techniques used in Unsupervised Learning are cluster analysis and prin- cipal component (Duda et al., 2001). Nevertheless, Zhuang et al. (2018) applied K-means clustering as an ROI segmentation technique of birds in a farm environment. The reinforcement algorithm learns via a feedback loop and focuses on finding a balance between exploration and exploita- tion (Kaelbling et al., 1996). It works on the Markov decision process (MDP) environment. Therefore, basic reinforcement learning is modeled as a Markov decision process. A detailed mathematical descrip- tion of these machine learning algorithms will not be presented in this study. Please refer to the corresponding publications for more insight.


Deep learning-based poultry monitoring systems

As already mentioned, the conventional machine learning-based poultry monitoring follows the procedure image acquisition, pre-
processing, segmentation (ROI extraction), feature extraction, and classification or regression, as presented in Section 2. However, seg- mentation, feature extraction, and selection engineering are arduous tasks. Furthermore, the performance of these algorithms is affected by sensor sensitivity, making them challenging in a real farm envi- ronment. DL approaches eliminate these arduous tasks by directly processing the image by the application of DNN, as shown in Fig. 4. Thus, DL is also considered as feature learning (Kamilaris and Prenafeta-Boldú, 2018). Additionally, DL models have achieved higher accuracy due to their ability to avoid errors associated with segmentation and erroneous feature vectors. Furthermore, DL allows massive parallelization of computations due to the complex models. Therefore, complex problems can be solved at faster computational speeds (Pan and Yang, 2010). Therefore, more research is currently focusing on optimum network architecture rather than on feature engineering in conventional image processing methodologies.
In DL, both the local and inter-relationships of data are learned in a hierarchical structure through several levels of abstraction (each layer transforms the input data from the previous layer into a new representation at a greater abstraction level). A non-linear function in each layer of a DL model transforms the data into representation in each layer. This hierarchical feature representation learning al- lows DL models to be successfully applied in classification and pre- dictions in various artificial intelligence applications, i.e., audio, raster-based data, time-series data (Kamilaris and Prenafeta-Boldú, 2018; Sehgal et al., 2017; Song et al., 2016). DL monitoring systems follow the steps image pre-processing, data augmentation, model- ing, and finally, classification or regression, as shown in Fig. 4. Similar to ANN, DL models are trained by a backpropagation algorithm to- gether with an optimization algorithm that updates the network weights to minimize the loss function. This section will present a re- view of various DL model architectures, data pre-processing tech- niques, data augmentation methodologies, and DL systems in the monitoring of poultry.


Deep learning categories

This study will present a brief discussion on some of the popular DL architectures, i.e., Convolutional Neural Networks (CNNs), Recurrent and Recursive Neural Networks, and Pretrained Unsupervised Net- works. Generally, each architecture has a specific appropriate area of ap- plication, and some are already pre-trained to provide accurate classification in particular domains (Kamilaris and Prenafeta-Boldú, 2018; Pan and Yang, 2010). The popular platforms for development




Fig. 4. The general workflow of deep learning-based chicken monitoring systems.

and testing of DL models are TensorFlow, Keras, Theano, Matlab, Pylearn2, Caffe, TFLearn, and PyTorch.

Convolutional Neural Networks (CNNs)
CNN is the most popular architecture applied in computer vision tasks and natural language processing. CNN is a multi-layered network that can learn features of a target to perform an autonomous detection. It comprises several neural layers, i.e., convolutional, non-linear activa- tion layer, pooling, and fully connected layers. Each layer transforms the input to output for neuron activation, which eventually leads to the final fully-connected layers, thus resulting in the mapping of an input to a 1D feature vector. CNN perform convolution instead of standard matrix multiplication in their layers as opposed to the conventional neural net- works. The main attributes of CNN are parameter sharing (tied weights, i.e., only a single set of parameters are learned for each location of an image) and sparse interactions (making the kernel smaller than the size of the input, hence, reduced memory utilization and computational overhead) (Hosseini et al., 2020). Fig. 5 presents the general structure of CNN architecture.
In the convolutional layers, CNN applies various kernels to convolve the entire image to generate feature maps. The non-linear activation layer, i.e., the Rectified linear unit layer (ReLU), improves the training speed and increases the non-linearity of the feature maps (inputs) by applying a function. The pooling layer reduces the spatial dimensions of the input volume. However, the pooling layer doesn't affect the depth but only the width and height of the input volume. This operation is referred to as down-sampling or subsampling. This decrease in size leads to low computation complexity in the proceeding layers and pre- vents overfitting. The fully-connected layers perform the high-level rea- soning in the neural network by converting 2D feature maps to a 1D feature vector. The obtained vectors could be fed forward into catego- ries for classification (object detection task) or as feature vectors for fur- ther processing. Several CNN architectures have been created over the years, i.e., LeNet, AlexNet, ResNet, GoogLeNet (Inception), VGGNet, MobileNet, SqueezeNet, and Capsule Networks (CapsNet).
ResNets, also known as Deep Residual Networks, presented a solu- tion to solve complex problems in CNNs as the network becomes deeper (vanishing gradients) (Balduzzi et al., 2017). ResNet consists of a series of residual modules (layers), and each layer is a function set to be per- formed on an input with the gradient signal capable of feedback to ear- lier layers via shortcut connections (Balduzzi et al., 2017; He et al., 2016;
Kawaguchi and Bengio, 2019). ResNets have the advantages of being more accurate and require less weight in some cases and being highly modular. Additionally, they can be designed to determine how deep a network can be. The main disadvantages of ResNets are that for a deeper network, the detection of errors becomes difficult. Additionally, if the network is too shallow, the learning might be very inefficient.
ResNets resulted in deeper networks, while Inception resulted in wider networks. Inception was intended to improve the computational efficiency in the training of larger networks (scaling up neural networks without compromising the computational cost) (Szegedy et al., 2016b). In a convolutional network, each layer extracts different types of infor- mation from the previous layer. An Inception module computes several different transformations over the same input map in parallel and con- catenates their results as a single output. To solve the computational bottleneck, Inception performs dimensionality reduction by the use of a 1 × 1 convolution across multiple channels to extract spatial informa- tion and compressing this information down to a lower dimension. Therefore, by reducing the number of input maps, Inception can stack different layer transformations in parallel, thus, resulting in simulta- neously wide and deep networks. Inception has evolved from the first version known as the GoogLeNet to Inception v2, v3, and v4. In v3, the 5 × 5 convolution was replaced with two consecutive 3 × 3 convolu- tions. The current version v4 applied the residual connections within each module resulting in an Inception-ResNet hybrid (Szegedy et al., 2016a).
The Xception stands for extreme inception. Consider that in a tradi- tional convolutional network, convolutional layers seek out correlations across both depth and space. While in Inception, a 1 × 1 convolution is used to project an original input into numerous separate, smaller input spaces. From each input space, different types of filters are applied to manipulate those smaller 3D blocks of data. However, in Xception, in- stead of partitioning the input data into several compressed primitives, the spatial correlations for each output channel is mapped separately, then performs a 1 × 1 depth-wise convolution to capture cross- channel correlations (Chollet, 2017). This operation can be referred to as depth-wise-separable-convolution, i.e., spatial convolution done in- dependently for each channel, followed by a point-wise convolution (1 × 1 convolution across channels) (Chollet, 2017).
The VGGNet follows the typical layout of basic convolutional net- works, i.e., a series of convolutional, max-pooling, and activation layers before the fully-connected classification layers at the end (Simonyan




Fig. 5. Convolutional neural network architecture.

and Zisserman, 2014). The MobileNet is essentially a rationalized ver- sion of the Xception architecture optimized for mobile applications. The SqueezeNet is powerful DL architecture that's efficient in low band- width platforms. It is based on a CNN architecture but with 50 times fewer parameters than AlexNet and maintains AlexNet-level accuracy on ImageNet (Iandola et al., 2016). The CapsNet, a multi-layer capsule system, is an advanced variation of CNNs that deepens in terms of nesting or internal structure (Sabour et al., 2017). It's mainly used for accurate image recognition tasks because it is robust to geometric dis- tortions and transformations. Thus, it can exceptionally handle orienta- tions, rotations, and translations.
CNN-based algorithms can be divided into two broad categories; two-stage target detection algorithms, i.e., R-CNN (Girshick et al., 2014), Fast R-CNN (Girshick, 2015), Faster R-CNN (Ren et al., 2015), Mask R-CNN (He et al., 2017), that uses Region Proposal Network (RPN) to generate the anchor boxes, after which the detection network performs prediction. The one-stage target detection algorithm is the second category that includes OverFeat (Sermanet et al., 2013), SSD (Liu et al., 2016), YOLO (Redmon et al., 2016), YOLO9000 (Redmon and Farhadi, 2017), YOLO v3 (Redmon and Farhadi, 2018). These algo- rithms predict the target location and category directly. Hence, they are faster than two-stage target detection algorithms and are applied in real-time detection systems.


Recurrent and recursive neural networks
These are networks that can handle time-series data, i.e., Recurrent Neural Network (RNN), Recursive Neural Network, Attention, and Long Short-term Memory (LSTM).
RNN is a network whose current output is based on both the present input data and the learning based on previous data. Therefore, RNN is applied in applications where the sequence in which data is presented is vital, i.e., machine translation, speech synthesis, and natural language processing. Every computed information is stored (hidden state vector) and utilized to compute the final output. However, the same input can result in different outputs depending on the previous inputs in the data series. RNN is referred to as recurrent because the same task is per- formed for every element in the series, resulting in the generation of dif- ferent fixed-size output vectors where the hidden state vector is updated for every input. Therefore, RNN captures both sequential and time dependencies between data (Gulli and Pal, 2017; Haque and Neubert, 2020; Hosseini et al., 2020). RNNs are suitable for sequential data because they share weights across time steps and can perform one to many, many to many, and many to one mapping. There are two varieties of RNN, the Bidirectional RNN (BRNN) and the Encoder- Decoder RNN (EDRNN). The output of a BRNN depends on both the past and future outputs, i.e., RNNs makes inferences from the present data point in a sequence relative to both future and previous data points. The EDRNN can map the input data sequence into variable-length out- put sequences (Hosseini et al., 2020). RNNs can be made deeper (adding multiple layers for faster learning and improved network performance) by adding more hidden state layers, adding more layers between the hidden state layer and the output layer, adding non-linear hidden layers between the input layer and the hidden state layer or applying all the three (Haque and Neubert, 2020).
Recursive neural networks have a return loop to feed the network into itself. This allows for the identification of input data constituents and their relationships through a binary tree structure and shared- weight matrix (Hosseini et al., 2020). Recursive neural networks are characterized by a top-down propagation method and a bottom-up feed-forward method. There are two main types of Recursive neural networks, i.e., supervised recursive neural tensor (applied in computer vision) and the semi-supervised recursive autoencoder (applied in sen- tence deconstruction). The main advantage of Recursive neural net- works over RNNs is that they can capture long-term dependencies efficiently. However, Recursive neural networks suffer from substantial
computational overhead than the RNNs (Goodfellow et al., 2016; Hosseini et al., 2020).
The LSTM is a special RNN that applies recurrent edges as a solution to the vanishing gradient problem. LSTM use memory cell to hold infor- mation and a set of gates (input, forget, and output gates) to indicate the status of the memory cell (Sundermeyer et al., 2015, 2012). The con- tents of the memory cell are modified by the input and forget gates con- ditions at each time step. The input gate selects the new information that should be added to the cell state. The forget gate selects which in- formation should be discarded from the cell state. The output state se- lects relevant information from the cell state as the output.

Pretrained Unsupervised Networks (PUNs)
PUNs are DL models whose hidden layers are trained by unsuper- vised learning to achieve an accurate fitting of the dataset. The layers are trained (unsupervised learning algorithm) independently sequen- tially, such that the input of a layer is the previously trained layer. The whole model is then fine-tuned using supervised learning after each layer has been pre-trained. Types of PUNs include Generative Adversar- ial Networks (GAN), Autoencoder, and Deep Belief Networks (DBNs).
An autoencoder neural network applies a backpropagation algo- rithm in an unsupervised environment. The input is compressed into a latent-space representation, and the output is the same or close to the input values (learn a representation for dimensionality reduction). They are popular in anomaly detection applications, i.e., fraud detection in financial transactions. The network comprises an encoder and de- coder parts, as shown in Fig. 6. The input data is compressed by the en- coder into latent-space representation, while the decoder performs the data reconstruction (output from the latent-space representation). Autoencoders cannot be applied as a generative model due to disconti- nuities in the latent space representations (Haque and Neubert, 2020). Therefore, variational autoencoders were introduced as a solution. Whereby the encoder outputs two vectors (mean and standard devia- tion) rather than one. This allowance enables the decoder to correctly decode values with small variations of the same input (Haque and Neubert, 2020). There are four main types of autoencoders, i.e., Vanilla, Multilayer, Convolutional, and Regularized autoencoders. The vanilla is the simplest autoencoder with a neural network with one hidden layer. Multilayer is an autoencoder with more hidden layers. Convolutional is an autoencoder with convolution layers instead of fully-connected layers. Lastly, the Regularized autoencoder applies a special loss function to improve performance.
GANs involve the training of two DL models (the generator and the discriminator) simultaneously that compete with each other. The gen- erator creates new instances by modeling a transform function during training. In comparison, the discriminator classifies if an instance origi- nates from the generator or the training data, while the former maxi- mizes the final classification error while the later minimizes the error between the generated data and the training data. Thus, the two net- works are referred to as adversaries. Hence, the whole network im- proves with each iteration during training. GANs are widely applied in computer vision, especially in image generation and also in speech, prose, and music because of GANs ability to mimic any distribution of data in any domain (Hosseini et al., 2020).
GANs have the advantage that it requires no deterministic bias, un- like the variational encoders, they allow for efficient training of models in a semi-supervised setting. However, the main drawbacks of GANs are that the performance of the generator and discriminator are crucial in the success of GAN, and the whole model fails if one system (generator and discriminator) fails. Additionally, training GAN is computationally expensive with high training time due to the two-model training.
DBNs are an extensive layered network structured by connecting several smaller unsupervised neural networks. A DBM is composed of Belief Net and the Restricted Boltzmann Machine (RBM) (Hosseini et al., 2020). Belief Net is composed of connected layers (binary unit layers), each assigned a weight function (layer-by-layer learning). The




Fig. 6. The autoencoder architecture.


probability of the binary outcome depends on the weight factor and the bias inputs. RBM is a stochastic RNN designed on the principles of energy-based models (EBMs) (Haque and Neubert, 2020; Hosseini et al., 2020). Learning is performed by minimization of the energy func- tion, and prediction is achieved by determining the values of residual variables that minimize the energy based on observed variables. The RBM consists of one input layer and one hidden layer without an output layer. Another type of RBM is the Deep Boltzmann Machine (DBM), characterized by undirected connections. Hence, DBM is robust in han- dling certainty due to noisy inputs.

Image pre-processing

Image pre-processing is performed before the image is fed as an input to the DL model. Image resizing is the most common image pre- processing procedure for the image to adapt to the DL model require- ments. In the deep regression network (AlexNet and ReLU activation function), Fang et al. (2020) resized the input image to 960 × 540 from 1920 × 1080 resolution. Similarly, Zhuang and Zhang (2019) per- formed a resizing operation to have a 512 × 512 input image resolution. In a comparative analysis between Faster R-CNN and YOLO v3 in recog- nition and classification of broiler droppings, Wang et al. (2019b) resized images from 5760 × 3240 resolution.
Data labeling which involve the creation of bounding boxes is an- other vital pre-processing procedure. Data labeling is often performed manually to reference the ground truth by a bounding box. Labeling software such as LabelImg (Windows-based) is applied to draw the bounding boxes and extract their co-ordinate locations. Ground truth labeling is a vital step in classification tasks as it provides a basis for per- formance evaluation of the proposed detector (Zhuang and Zhang, 2019). The procedures mentioned above are the main techniques that have been applied in poultry monitoring DL modeling systems. Other pre-processing operations include image segmentation to highlight the ROI hence, facilitating the learning process as performed by Fang
et al. (2020). Background removal or foreground pixel extraction can also be performed to reduce the effect of noise in the dataset (Kamilaris and Prenafeta-Boldú, 2018).

Data augmentation

DL models need a lot of training data to achieve an appropriate con- vergence for better recognition accuracy while at the same time, avoiding over-fitting. Therefore, data augmentation technique is per- formed to expand the training data by a dynamic transformation of the data without changing their classification. If k is the number of aug- mentation techniques applied, then the total number of images used in training will be (k + 1)-fold of the original dataset. Additionally, the image transformation effectively increases the training set without the need to store a large augmented training set. Table 8 presents the data augmentation techniques applied in the DL data processing.

Deep learning applications

Several studies have applied DL models in poultry monitoring sys- tems ranging from behavior classification, tracking, sick birds' detection, to droppings classification. Pu et al. (2018) developed a CNN detector to classify chicken flock behaviors at the feeders using color and depth im- ages (two parameter-sharing CNNs). His network consisted of three convolutional layers, each with a rectified linear unit (ReLU) activation function, a max-pooling layer with a local response normalization step. The system achieved an accuracy of 99.17% in the chicken behavior classification. A Faster R-CNN chicken activity detector combined with the temperature-humidity index (THI) was used to monitor heat stress in chicken (Lin et al., 2018). The detector applied the Zeiler and Fergus network (Zeiler and Fergus, 2014) as the base CNN. The chicken move- ment was determined by tracking the chicken location between subse- quent frames using the minimum distance matching and color feature matching techniques. As already mentioned, the detection speed of


Table 8
Data augmentation techniques.
Adopted from Shorten and Khoshgoftaar (2019).
Data augmentation techniques	Descriptions Classification	Method
normalization (LRN) layer and a pooling layer for max-pooling between the 1st and 2nd layer and between the 2nd and 3rd layers effectively prevents overfitting. This technique achieved a mixed tracking perfor- mance evaluation of 0.730 at a processing speed of 30.53 fps.
Apart from the mentioned machine learning and DL techniques,


Geometric transformations

Flipping	Horizontal and vertical mirroring
Rotation	Rotating an image by ±θ around the center of the image
Cropping	Reducing the size of the input image Translation	Shifting images left, right, up, or down
to avoid positional bias in the data Noise injection	Injecting a matrix of random values
drawn from a Gaussian distribution (Gaussian noise)
Color	Isolating a single-color channel such as R, G, or B and color histograms manipulation (brightness)
studies have applied other modeling techniques. The transfer function (TF) represents the relationship between the input and the output sig- nals of a control system for all possible input values. The parameters of a TF model can be estimated using several estimation techniques such as least squares (LS), state variable filters approach, instrument variable approach, generalized Poisson moment functions approach, etc. However, due to noise, the model parameters become asymptoti- cally biased when LS is applied. Therefore, Leroy et al. (2006) applied a simplified refined instrumental variable (SRIV) to estimate TF model parameters from an optimal shape posture parameters (ellipse shape
model) to determine two dynamic parameters to predict a chicken be-

Photometric transformations
Color space transformation
Conversion of RGB space to other color spaces i.e., HSV, YUV, CMY and LAB.
havior depending on the previous behavior. The technique successfully

Kernel filters	Sharpening	Sharpen the image edges by use of high
contrast vertical or horizontal edge
filter
Blurring	Blurring the image by use of Gaussian blur, average blur, uniform blur, and median filters
classified scratching, walking, and standing behaviors. As already men-
tioned, feature extraction engineering is an arduous task. Therefore, Zaninelli et al. (2018) performed bird recognition from a shape classifi- cation point of view. A normalized cross-correlation was performed be- tween a processed image and a template to detect multiple nest

Mixing images	Mixing images
together Random erasing	Dropout
regularization
Producing a new image by averaging the pixel values of images.
Selects an n × m patch of an image and mask it with either 0 s, 255 s, mean pixel values, or random values
occupancy (template comparison).
Animals are CIT systems that are individually different and respond differently at different moments. Therefore, they can't be analyzed as a typical classical steady-state system (Berckmans, 2006). Additionally,

Deep
learning-based augmentation
Feature space	Noise, extrapolating, and interpolating
by joining k nearest neighbors to form new instances in lower-dimensional representations in high-level layers
Adversarial training  The use of adversarial attacking in a
rival network to learn augmentations to images that result in misclassifications in its rival classification network.
Dawkins et al. (2009) and Dawkins et al. (2012) reported that there
was no simple association between acceleration and velocity, kinematic features with a bird's GS. Therefore, Nääs et al. (2018) allowed for con- tradictions within a degree of certainty in the estimation of broiler chicken GS using the kinematic features by applying inconsistency- tolerant, Paraconsistent logic.

Challenges and future direction

Generative adversarial network (GAN)-based
Creation of artificial instances from a dataset in a way that they retain similar characteristics to the original dataset

In an ideal environment and specific controlled chicken movements,

Neural style transfer  Manipulates the sequential
representations across a CNN, i.e., that the style of an image can be transferred to another while preserving its original content
the current computer vision methodologies provide auspicious results. However, in a real farm environment, the task of monitoring chicken be- comes complicated. Chickens in flocks are sometimes occluded by other chicken, hence, changing their morphological parameters. Additionally,

Meta-learning schemes
Applies a prepended neural network to learn augmentations via Neural Style Transfer, mixing images, and geometric transformations.
the variation of ambient light conditions and shadows significantly af- fect sensor stability. Therefore, further research and development are necessary to establish the commercial viability and applicability of these poultry monitoring systems. These challenges need to be ad- dressed through combined approaches between livestock science and

YOLO v3 is faster (real-time) compared to other two-stage target detec- tion algorithms because it is an end-to-end target detection algorithm. Wang et al. (2020) presented a real-time behavior detector. This system could detect six chicken behaviors at the highest mean precision rate of 94.72%. An improved SSD was introduced by Zhuang and Zhang (2019) for sick broiler detection. The introduced Improved Feature Fusion Sin- gle Shot MultiBox Detector (IFSSD) had the InceptionV3 architecture of a 1 × 1 convolution of three different size layers and features generated by a feature pyramid network. The detector could detect broilers and their health status simultaneously at a mean average precision (mAP) of 99.7%. As the number of network layers increases, there often arises the problems of difficulty of network optimization and disappearance of gradient descent. Therefore, Zhang and Chen (2020) developed a sick chicken detector based on ResNet residual network. Taking advan- tage of ResNet having an excellent training performance even for deep networks and improving its network structure, the proposed network adopted to different recognition environments. Fang et al. (2020) pre- sented the TBroiler tracker, whereby chicken tracking was performed as a regression task by developing a deep regression network composed of five convolutional layers and three fully-connected layers. Addition- ally, Fang et al. (2020) pointed out that adding a local response
engineering for improved overall performance and robustness of chicken monitoring in PLF.

Live weight estimation systems

Good animal welfare is characterized by good health and productiv- ity in livestock production. In comparison to human health, measuring individual weight and height is a common practice in a clinical check- up. Similarly, in livestock production, live body weight provides important information on feed conversion efficiency, growth, health, body uniformity, and market readiness (Okinda et al., 2018a; Wongsriworaphon et al., 2015). Additionally, monitoring animal weight during the entire growing period can be used to assess management strategies such as feeding rations and slaughter time. Hence, if the mea- sured weight doesn't coincide with the expected growth curve, then it would be a clear indication of a problem such as disease occurrences or other vitality issues for required counter-measures to be undertaken (Mollah et al., 2010; Mortensen et al., 2016). Therefore, animal live weight is an indicator of animal welfare conditions. Thus, it is necessary to accurately estimate an animal weight and weight distribution of the entire population throughout the rearing period.

Several studies have introduced chicken weighing systems based on computer vision based on 2D (Amraei et al., 2017b; De Wet et al., 2003; Mollah et al., 2010) and 3D (Mortensen et al., 2016) images. The basic principle of machine vision-based weighing systems is the correlation of image object shape geometric features to animal weight or volume. This is theoretically simple but quite challenging in a real farm environ- ment. As already mentioned, firstly, the bird's body must be segmented from the background (ROI extraction). Secondly, the chicken's body segmented from the image is to be presented by describing characteris- tics (feature extraction). Thirdly, these describing characteristics are correlated to the bodyweight by a mathematical model.
De Wet et al. (2003) pioneered the application of computer vision in chicken live weight estimation. The study developed linear and nonlin- ear regression models to correlate image area and perimeter geometric features to the real chicken live weight and achieved a relative error of 10% and 15% using surface-area and perimeter, respectively. Mollah et al. (2010) developed a similar system but accounted for the bird's age in their model to achieve 0.999 R2 with the highest relative error of 16.47%. In a real farm environment, chickens always flock together. Therefore, for a practical farm environment application, an automated, robust weighing system should be capable of estimating the live weight of each chicken in the flock. Amraei et al. (2017a) and Amraei et al. (2017b) reported having developed a multiple-bird weight estimation system based on an ellipse fitting technique to localize the chicken in a pen, after which 2D feature extraction was performed. Both studies re- ported an R2 of 0.98 based on ANN and SVM regression models, respec- tively. The mentioned systems were based on visible light-based sensors (RGB images). However, these sensors are susceptible to varia- tion in ambient light; hence, they are prone to errors (Okinda et al., 2019). Mortensen et al. (2016) applied the structured infrared-light (IR) based sensor, which is invariant to illumination conditions, to pre- dict the weight of broiler chicken based on both 2D and 3D image fea- tures at an average relative mean error of 7.8%.
Poultry weight estimation systems are mainly challenged by varia- tion of ambient lighting conditions and the localization of a bird when in a flock condition. To address the problem of variable light conditions, the solution would be the use of illuminant invariant cameras and flex- ible image sensors in the farm environment. IR-based depth cameras such as the Microsoft Kinect have been applied in weight estimation by Mortensen et al. (2016). However, IR depth cameras are sensitive to sunlight, thus, limiting their application to an indoor environment. However, illuminant invariant visual light-based cameras (Jansen-van Vuuren et al., 2016) are readily available in the market they haven't been applied in poultry monitoring systems. Providing a controlled lighting environment for visual light-based sensors can be another solu- tion, although it's challenging if not infeasible due to farm structure, size, and other complexities. Therefore, the potential research and de- velopment area could be to provide a controlled illumination in the farm environment for image acquisition in farmhouses and the use of il- luminant invariant cameras.
Occlusion and overlap of birds significantly affect morphological fea- tures, hence, affecting the performance of the regression model, as dem- onstrated in the study by Mortensen et al. (2016), whereby the estimation errors increased as the flock density increased. Some studies have approached this problem from a segmentation point of view. For example, Amraei et al. (2017b) applied the ellipse fitting technique to segment birds. However, in their presentation, the birds were not oc- cluded as depicted in the study by Mortensen et al. (2016), who applied a watershed algorithm using depth distance as the height function. Ad- ditionally, a bird is a non-rigid shape; hence template matching and par- tial shape matching are quite challenging. More future research should focus on efficient ROI segmentation techniques under occlusion and no occlusion. These techniques can be based on both template and non-template matching and DL techniques. Finally, the instantaneous expression of chicken behavior leads to shape deformation, such as flip- ping of wings, hence, affecting the model performance. The strategy
would be to incorporate behavior with weight estimation. Such that the introduced system should be invariant to instantaneous behavior expressions.

Lameness detection systems

The occurrence of lameness affects the mobility of any legged crea- ture. The term mobility refers to the quality or the state of being mobile or the ability to move. Generally, mobility is associated with walking or locomotion. In poultry, immobility is often a sign of chickens experienc- ing some discomfort. These discomforts may result from skeleton (leg) disorders, nutrition deficiencies and leg health (dermatitis), infestations (lice and mite), and diseases (Bessei, 2006; Bradshaw et al., 2002; Butcher et al., 1999; Knowles et al., 2008; Paul-Murphy and Hawkins, 2014). These factors can be categorized as genetical factors and environ- mental factors (illumination, bedding, ventilation, diseases, and stock- ing density) (Almeida Paz et al., 2010; Bessei, 2006; Knowles et al., 2008; Reiter and Bessei, 1997; Rozenboim et al., 2004; Tablante, 2013). Mobility is an important aspect of a living bio-organism. Being mo- bile is often perceived as being fit and in good health. Moreover, diffi- culty in walking by birds can result in starvation, thus, affecting the feed conversion ratio in terms of weight and growth, chest soiling, hock burns conditions, and being an easy target to be preyed on (Kestin et al., 2001; Paul-Murphy and Hawkins, 2014; Weeks et al., 2000). Additionally, leg disorders increase mortality culling, condemna- tions, and downgrades from trimming, which account for considerable economic losses. Furthermore, according to Welfare-Quality® (2009), the occurrence of the factors mentioned above is indicators of poor an- imal welfare conditions. Thus, monitoring the level of a bird's mobility provide an assessment for its welfare condition. Thorp and Duff (1988) described lameness as a range of injuries resulting from infective and non-infective sources. Additionally, based on pathological condi- tions resulting in lameness and leg weakness Bradshaw et al. (2002) classified poultry leg disorders as infectious, developmental, and degen- erative. Therefore, the general term describing the inability to walk nor- mally due to illness or injury affecting the foot or leg is lameness and leg
weakness.
As already mentioned, despite the successes of the kinetic tech- niques, they were time-consuming, had a lot of data redundancy, and could not provide continuous and automatic monitoring of birds. Hence, it can't be used as an early detection method. Kinematic moni- toring of birds was initially introduced by Abourachid (1991) to analyze the GS of turkeys. The same approach was applied by Caplen et al. (2012) to contrast the GS of broiler chickens and jungle fowl by the use of a 3D temporospatial poultry walk information acquisition system. In comparison, the study established that the jungle fowl had a better GS due to fast growth issues in broilers, which promotes a compensa- tory gait adaptation to minimize walking energy which triggers lame- ness. Additionally, lame broilers were observed to have a lower walking velocity and exhibited walking instability. A comparison of the effect of NSAID administration on lames by Caplen et al. (2013) established that there was an increase in the chicken walk speed after NSAID administration and concluded that the model could be useful in assessing lameness-associated pain in broiler chickens. Using a paraconsistent logic, Nääs et al. (2018) tracked the centroid of a chicken to compute its kinematic features (velocity and acceleration) to estimate the GS of broilers. Despite the success of kinematic analysis as a computer vision technique, it suffered a couple of setbacks, i.e., markers on the skin locations being displaced during movement, re- quired the bird to walk parallel to the camera for accurate measure- ments to be taken, and this technique was both intrusive and invasive to the birds.
In the analysis of locomotor patterns of chickens, layers' body moves in a straight line because their legs are always under the center of grav- ity of their body. However, for broilers, their center of gravity moves lat- erally towards the supporting leg. Therefore, the differences in

horizontal and vertical movements of the left and right legs, cycle period of feet movements, irregularities in the motion of the center of gravity of the body, and frequency of body center oscillation can be evaluated to detect lameness in birds (Reiter and Bessei, 1997). Based on these find- ings, alternatives to kinematic systems that could provide non- intrusive, non-invasive, automatic, and continuous systems for early de- tection of lameness in chicken were developed. A fully automated image monitoring technique by Aydin et al. (2010) was capable of measuring the activities of broiler chickens and relating these activities with their GS levels. This study established a significant relationship between manual GS and broiler activities, with higher GS having lower activities and GS3 being the most active. In the spatial use of mixed broilers, Aydin et al. (2013) returned the same results that GS0 to GS3 had more move- ments than all higher GS. Based on the LTL test and NOL, Aydin et al. (2015) introduced another automated image monitoring system for lameness detection. The two features (LTL and NOL) were then com- pared to the manual GS and established that NOL was positively signif- icantly correlated to GS. In contrast, LTL was negatively correlated with GS. These results indicated that this system could be used as a tool to as- sess lameness in broilers automatically. Taking the advantages of 3D depth sensor Aydin (2017a) computed LTL and NOL using image depth information. The results obtained were almost similar to the later study in that NOL was positively significantly correlated to GS (R2 = 0.934), while LTL was negatively significantly correlated to GS (R2 = 0.949). These findings justified the use of a 3D vision monitoring technique as a method of assessing lameness in broiler chicken. The most recent study of Aydin (2017b) based on kinematic parameters (lateral body oscillation, step length, step frequency, and walk speed) reported a correlation between the GS and these parameters at r = 0.861, 0.882, 0.831, 0.844, respectively. The study further established a statistical significance in all the feature parameter as a measure of lame- ness (regarding GS), hence, this system can be used to provide an early detection of lameness in broilers.
Variable light conditions and occlusion problems are also a hin-
drance to lameness detection systems in birds. Additionally, as much as kinematic posture trackers have yielded positive results, more re- search should be undertaken in the development of automated body position trackers without the use of markers (non-intrusive and non- invasive systems). Moreover, the applicable camera position in a real farm environment is still a challenge. Overhead camera positions are most preferred, i.e., non-invasive. Aydin (2017a) and Aydin (2017b) ap- plied overhead depth and RGB images, respectively. However, as al- ready mentioned, IR depth sensors are susceptible to sunlight, hence, limited to indoor applications or would limit the operation time of the system if it is installed in an outdoor environment. Additionally, RGB cameras are associated with visual light-based sensor errors.
Furthermore, these experiments were conducted in a controlled en- vironment, whereby the birds' movements were restricted. Therefore, further research should focus on lameness detection of chickens in a flock setting such that occlusion problems and dynamic movements are considered. Lastly, lameness in chicken is affected by several factors. However, the easiest to control from a stockman perspective are the en- vironmental factors such as illumination, bedding, ventilation, and stocking density. Therefore, more research should focus on the opti- mum environmental conditions for chicken regarding lameness.

Health status classification systems

According to Welfare-Quality® (2009), the health condition of a bird is a vital indicator of good welfare practice. The term being healthy can be characterized by the absence of a disease, whereby disease is any condition that causes a deviation from normal activities and functions. Poultry disease occurs due to the interaction between the birds, the en- vironment, and the infection agent (non-infectious and infectious). In- fectious agents include viruses, bacteria, fungi, and parasites, while non-infectious agents include chemical and physical toxins and
deficiency or excess of minerals and vitamins. The host factors include the bird's age, sex, breed, and immune status. Environmental factors, which are also management factors, include air quality and ventilation, stocking density, sanitation, feed quality, lighting program, and medica- tion and vaccination programs (Tablante, 2013). Poultry diseases can be categorized as Respiratory (Newcastle Disease, Fowl Pox, Avian Influ- enza, Infectious Bronchitis, Infectious Laryngotracheitis, Infectious Co- ryza, Aspergillosis, Swollen Head Syndrome), Viral Diseases (i.e., non- respiratory) (Marek's Disease, Infectious Bursal Disease, Lymphoid Leu- kosis, Avian Encephalomyelitis), and Non-respiratory Bacterial Diseases (Fowl Cholera, Necrotic Enteritis, Omphalitis, Ulcerative Enteritis, Botu- lism, Pullorum, Staphylococcus) (Butcher et al., 1999). Butcher et al. (1999) and Tablante (2013) made a presentation on common poultry diseases and infections, visual symptoms for each disease, and preven- tion and control. Furthermore, Damerow (2016) outlined how to recognize sick poultry by observation, dropping examination, and post- mortem examination. Current computer vision systems perform health classification based on the behavior (posture) (Okinda et al., 2019; Zhang and Chen, 2020; Zhuang et al., 2018; Zhuang and Zhang, 2019), chicken droppings (Wang et al., 2019b), locomotor (Okinda et al., 2019), and optical flow (Roberts et al., 2012).
Optical flow measures were used as an early detection system to predict the mortality, hock burn, and GS of birds using a Bayesian re- gression model (Roberts et al., 2012). Zhuang et al. (2018) correlated skeleton features of broiler posture images taken from the side at an ac- curacy of 99.469%. Due to the applicable camera position in a real farm environment, Okinda et al. (2019) extracted overhead image posture and achieved an accuracy of 0.978. Nevertheless, these researches were performed in a controlled environment, where variation in illumi- nation was not factored in, and the birds' behaviors and activities were controlled. DL detectors by Zhuang and Zhang (2019) and Zhang and Chen (2020) gave out satisfactory results of mean average precision of 99.7% and 93.7%, respectively. The main challenge in DL detection of chicken is the lack of an appropriate dataset. Zhuang and Zhang (2019) expressed that the currently available bird's datasets do not have a specific category called broilers; hence, it resulted in low recog- nition accuracy. Therefore, more research should be undertaken to de- velop a poultry dataset with specific categories such as broilers, layers, chicks, etc., that can be applied in poultry detection systems. However, this would be labor-intensive due to the retraining process of new appli- cations. Hence, much research should be focused on new learning tech- niques, i.e., adaptive learning and semi-supervised learning. In the study by Wang et al. (2019b) in the detection of digestive diseases in broilers based on color and viscosity of the droppings. However, the color varia- tions of dropping can also result from the type of feed. Similarly dropping viscosity will also vary depending on water intake. Thus, this technique would be challenging to apply in free range chickens or chickens with a diverse feeding program. Furthermore, this would not provide an early detection of disease occurrences. Body temperature is another important parameter in the evaluation of an animal's health status. However, temperature monitoring hasn't been widely applied in poultry. Nevertheless, Xiong et al. (2019) presented a system that could extract the temperature of the head region of a broiler from ther- mal images. Therefore, more research should be directed towards region-based temperature detection for infection detection in poultry.

Poultry tracking systems

Tracking of poultry is an essential parameter in the assessment of be- havioral (types of activities) and physical (lameness and health) indica- tors in poultry welfare. There is a need to automatically record the behavior and movement of birds continuously for welfare monitoring purpose and behavior phenotyping. Noldus and Jansen (2004) catego- rized automated video tracking systems as analog and digital video tracking systems. Analog systems detected high peaks in the voltage of a video signal, i.e., regions of high contrast between the bird and

background. However, these systems could only track one bird in a ded- icated experiment set up unit with restricted illumination and back- ground conditions. Digital systems allowed for pattern recognition techniques to be applied to image frames for the quantitative measure- ment of the birds. However, this system is limited by the computational speed and the complexity of the underlying software.
Digital image animal tracking systems such as EthoVision have long been applied to several laboratory animals since the 1990s in tracking and behavior classification (Noldus et al., 2001). The system's steps of operation include object identification (size and color), feature extrac- tion, feature changes from the previous frame, and tracking and behav- ior detection. The chicken tracking system was pioneered by Sergeant et al. (1998), who performed tracking based on centroid detection and curvature analysis to separate touching birds. However, centroid track- ing was challenging due to several factors such as; if a bird moves faster than the threshold value, the tracked centroid was lost on some occa- sions, the centroid was assigned to the noise region, total occlusion (bird squeezes under another bird) led to an ambiguity such as the tracked centroids interchanged. Fujii et al. (2009) applied a particle fil- ter algorithm to track poultry. The system applied two trackers, i.e., poultry trackers and exploring trackers. The former detected the chicken's location while the latter searched and corrected a failed poul- try tracker. Similar to the study by Sergeant et al. (1998), absolute occlu- sion and quick movements of birds were significant problems in this technique. Additionally, heads and tails could be detected as the poultry body during ellipse modeling. Furthermore, ambient light variation also influenced the performance of this system. Despite the ellipse fitting technique by Kashiha et al. (2014) reporting a superior performance, it lost track when the chicken moved very fast. Moreover, this technique was applied to individual birds and not as flocks. For multiple birds tracking, Nakarmi et al. (2014) incorporated Radio Frequency Identifi- cation (RFID) to identify and track birds when the vision system failed to maintain the identities of the tracked birds. However, tagging of birds with physical components is invasive and therefore affects their natural behaviors. In a comparative approach, Wang et al. (2016) intro- duced the hybrid support vector machine. They compared it to TLD (Tracking-Learning-Detection), the MeanShift Algorithm, the PLS (ob- ject tracking via partial least squares analysis), the Particle Filter Algo- rithm, and the Frag (fragment-based tracking method) in the tracking of chicken. A similar approach was presented by Fang et al. (2020), who applied deep regression network. The two techniques were robust and performed efficiently in chicken tracking in a flock setup. However, they were single object detection (only one bird tracking problem) and not a multi-objects detection problem, hence, could only detect and track one bird in a flock. More research and developments should be performed to develop a multi-object tracking such that several birds can be tracked simultaneously in a flock. Multi-object detection has re- cently received a lot of recognition and has already been applied in sick broiler detection by Zhuang and Zhang (2019). Additionally, more re- search should be driven towards DL networks using non-visible light- based sensors to eliminate the illumination variation problems and to allow for tracking to be performed even during dark hours (lighting regime).

Behavior monitoring systems

The discipline that is closely related to animal welfare is the animal behavior and is considered as behavioral indicators in welfare assess- ment. In a good welfare condition, the animals should be able to express their natural behavior patterns due to no or minimal stress. However, studies have reported the difficulty in differentiating between standard physiological stress and productive indicators of stress, and at times contradicted each other (Marıa et al., 2004). Additionally, sampling techniques or direct observation are invasive despite being the initial point for the development, validation, and implementation of non- invasive automated behavioral systems. Nevertheless, several poultry
behavior monitoring systems based on computer vision have been introduced.
From a complex system approach, Marıa et al. (2004) expressed that the complexity of the behavior of animals reduces with stress. The study applied fractal analysis rather than a conventional Euclidean geometry to the quantification of temporal heterogeneity of time series behavioral sequences. Optical flow analysis has also been used to correlate the op- tical flow measures to behavior and GS. Dawkins et al. (2009) presented that a higher mean optical flow is associated with greater bird activities in terms of striding and walking rate. Additionally, behavior was highly significantly correlated to GS. In another study by Dawkins et al. (2012), mean optical flow was negatively related to flock mortality, while kur- tosis and skew were both positively correlated to mortality, GS, and hock burn. Dawkins et al. (2013) pointed out that there exists no simple connection between optical flow and behavior when they found that the mean optical flow was not negatively correlated to birds sitting or lying nor a positive correlation between birds walking and optical flow. However, mortality, GS, leg health were positively correlated to birds sitting or lying and negatively correlated with birds walking. Colles et al. (2016) applied the same technique to detect Campylobacter infected chicken; the study established a higher mean and lower kurto- sis for infected birds. Despite the success of the optical flow analysis, they only presented the relationships between welfare indicators, be- haviors, and optical flow measures but not the type of behavior.
More research has been focused on developing behavior type recog- nition techniques based on developed ethograms as in the study by Pereira et al. (2013) and Marıa et al. (2004). Leroy et al. (2005) devel- oped a dynamic model to recognize six different laying hens' behaviors (sleeping, standing, sitting, grooming, pecking, and scratching). A simi- lar approach was presented by Pereira et al. (2013), who applied a clas- sification tree to identify nine chicken behaviors (Wing spreading, Drinking, Bristling, Resting, Scratching, Stretching, Mounting, Preening, and Inactivity). To eliminate the process of feature extraction engineer- ing and errors associated with visible light-based sensors, Pu et al. (2018) proposed two parameter-sharing CNNs for both RGB and depth images to classify flock behaviors at the feeders as non- crowded, a little crowded, and fairly crowded. A more straightforward technique to determine the number of birds at the feeder and drinker under temporal and spatial preferences was presented by Li et al. (2019b). However, the technique suffered occlusion problems. Behavior changes are instantaneous; therefore, real-time monitoring systems are of great significance. Wang et al. (2020) capitalized on the fast speed of YOLO v3 to develop a real-time behavior detector capable of classifying six behaviors mating, standing, feeding, spreading, fighting, and drinking.
More research and development should be focused on developing
behavior detection systems with illumination invariancy, factors in the background complexity, overlapping, and occlusion problems. The ap- plication of improved YOLO v3 has solved these problems, referred to as the YOLO v3-dense model (Tian et al., 2019), whereby DenseNet is used to process feature layers with compromised images (low resolu- tion, occluded objects). Similarly, the speed and performance of the real-time YOLO v3 system can be improved by extending the detection scale and down-sampling of feature fusion target detection layer (Ju et al., 2019). The mentioned improvements haven't been applied in chicken monitoring CNN-based systems. Therefore, more research should be performed to improve the performance of real-time detection models.

Activities and other monitoring systems

Animal activity is highly associated with behavior levels, GS, and health. In computer vision systems, activity is measured as percentage pixel change over the total area coverage over a period of time, i.e., the higher the activity levels, the higher the difference in pixel values. EthoVision XT and eYeNamic are software that can directly

compute the activity of birds from surveillance video input and has been applied in several studies (Aydin et al., 2010; Fraess et al., 2016; Van Hertem et al., 2018). The activity index of birds was assessed with rela- tion to thermal stress by Bloemen et al. (1997). During cold stress, the birds huddled together, while during thermal stress, the chickens occu- pied more floor space. A similar approach was presented by Kristensen et al. (2006) to evaluate the relation between illumination intensity and broiler activity. The broilers' activities were significantly higher during high-intensity periods. eYeNamic software was applied by Aydin et al. (2010) to correlate the activity index to GS. The study established that higher GS had significantly lower activities. Similar results were achieved by Aydin et al. (2013) in the spatial use of mixed chickens. To control chicken activities in a pen, similar to the work by Bloemen et al. (1997), Youssef et al. (2015) compared the dynamic variations of the activity index of chickens to a 2D spatial profile of airflow and the temperature pattern inside the pen. The study reported that during cold stress, the birds occupied low air velocity zones, while during ther- mal stress, the birds occupied high air velocity zones. The activity level and flock distribution data were used to determine the GS of birds by Van Hertem et al. (2018) based on eYeNamic software analysis. The study reported that GS and activity were negatively correlated, while GS and flock distribution was positively correlated. Therefore, flock GS could be predicted from continuous monitoring of flocks by video sur- veillance. Similarly, the animal distribution index was computed by eYeNamic software by Kashiha et al. (2013) to detect any problem in a broiler house such as thermal discomfort, insufficient feeds, and water. The studies mentioned above categorized any changes in a flock as activity without considering the intrinsic properties of change nor the type of changes occurring. Therefore, more research should be focused on activity deviation with behavior changes regarding welfare parameters.
Other computer vision-based monitoring systems such as floor dis-
tribution monitoring at drinking and feeding areas (Guo et al., 2020), ef- fect of feeder types (Neves et al., 2015), type of light illuminance (Kristensen et al., 2007), backpack (Stadig et al., 2018) on bird's behav- ior have also been presented regarding activity and behavior monitor- ing. However, animal behavior is a complex bio-response to both internal and external stimuli. Therefore, more research should be di- rected towards the drivers of behavioral responses such as pen con- struction designs and materials and structures inside the pen as well as health and micro-environment.


Conclusions

This review presents a summary of the current monitored bio- processes and bio-responses, how they qualify as welfare indicators, and the computer vision techniques applied in the surveillance and monitoring of these bioprocesses and bio-responses, the challenges in- volved, and possible solutions to these challenges. Both machine vision and DL techniques were discussed.
For conventional machine learning, the five procedures, i.e., preprocessing, segmentation, feature extraction, feature selection, and classification or regression, were discussed in detail. The difficulty of poultry monitoring lies in foreground detection due to the complex background, variations in illumination, and occlusion problems in a real farm environment. Several solutions have been proposed, through the application of non-visible light-based sensors, restriction of image acquisition time, factoring in the animal behavior, and using depth- based sensors for easier separation of occluded birds. The extracted ROI, i.e., the bird, can then be represented by the feature vectors; four features can be used for this task, i.e., morphological, locomotor, optical flow, and other features. These features are always mixed, and a dimen- sion reduction or feature selection engineering is applied to create a ro- bust and more generalized model. The final modeling procedure is conducted using regression or classification-based machine learning
approaches. For DL, the tasks of segmentation, feature extraction, and feature selection are eliminated by the use of CNNs.
Much success has been achieved in animal monitoring systems. However, there exist several challenging factors for real farm applica- tions (occlusion, lighting condition, etc.). Several studies have presented possible solutions. These studies presented good results in dedicated environments, hence, compromised a robustness and generalization ability of these systems. DL approaches have great potential. However, they require a vast amount of labeled dataset as an image dataset with ground truth annotations, and samples are of great significance in model development and for testing of algorithms.
Generally, appropriate image processing algorithms in computer vi- sion are essential for the poultry monitoring in the farm environment for precise localization of birds. This will be pivotal in the monitoring of several bioprocesses and bio-responses and also provide a solution to occlusion problems. Even though several challenges still exist, more researches are being performed to improve the monitoring systems in poultry.

Declaration of competing interest

All authors declare that they have no conflict of interest.

Acknowledgments

The project was funded by China National Key Research and Devel- opment Project (Grant No. 2017YFD0701602-2).

References
Abourachid, A., 1991. Comparative gait analysis of two strains of turkey, Meleagris gallopavo. Br. Poult. Sci. 32, 271–277. https://doi.org/10.1080/00071669108417350. Alm, M., Tauson, R., Holm, L., Wichman, A., Kalliokoski, O., Wall, H., 2016. Welfare indica- tors in laying hens in relation to nest exclusion. Poult. Sci. 95, 1238–1247. https://doi.
org/10.3382/ps/pew100.
Almeida Paz, I.C.L., Garcia, R., Bernardi, R., Nääs, I., Caldara, F.R., Freitas, L.W., Seno, L., Ferreira, V., Pereira, D.F., Cavichiolo, F., 2010. Selecting appropriate bedding to reduce locomotion problems in broilers. Brazilian J. Poult. Sci. 12, 189–195. https://doi.org/ 10.1590/S1516-635X2010000300008.
Alpaydin, E., 2020. Introduction to Machine Learning. MIT press.
Amraei, S., Abdanan Mehdizadeh, S., Salari, S., 2017a. Broiler weight estimation based on machine vision and artificial neural network. Br. Poult. Sci. 58, 200–205. https://doi. org/10.1080/00071668.2016.1259530.
Amraei, S., Mehdizadeh, S.A., Sallary, S., 2017b. Application of computer vision and sup- port vector regression for weight prediction of live broiler chicken. Eng. Agric. Envi- ron. food 10, 266–271. https://doi.org/10.1016/j.eaef.2017.04.003.
Amraei, S., Mehdizadeh, S.A., Nääs, I. de A., 2018. Development of a transfer function for weight prediction of live broiler chicken using machine vision. Eng. Agrícola 38, 776–782. https://doi.org/10.1590/1809-4430-eng.agric.v38n5p776-782/2018.
Anandan, P., 1989. A computational framework and an algorithm for the measurement of visual motion. Int. J. Comput. Vis. 2, 283–310. https://doi.org/10.1007/BF00158167.
Aydin, A., 2017a. Using 3D vision camera system to automatically assess the level of inac- tivity in broiler chickens. Comput. Electron. Agric. 135, 4–10. https://doi.org/10.1016/ j.compag.2017.01.024.
Aydin, A., 2017b. Development of an early detection system for lameness of broilers using computer vision. Comput. Electron. Agric. 136, 140–146. https://doi.org/10.1016/j. compag.2017.02.019.
Aydin, A., Cangar, O., Ozcan, S.E., Bahr, C., Berckmans, D., 2010. Application of a fully auto- matic analysis tool to assess the activity of broiler chickens with different gait scores. Comput. Electron. Agric. 73, 194–199. https://doi.org/10.1016/j.compag.2010.05.004. Aydin, A., Pluk, A., Leroy, T., Berckmans, D., Bahr, C., 2013. Automatic identification of ac- tivity and spatial use of broiler chickens with different gait scores. Trans. ASABE 56,
1123–1132. https://doi.org/10.13031/trans.56.9987.
Aydin, A., Bahr, C., Berckmans, D., 2015. Automatic classification of measures of lying to assess the lameness of broilers. Anim. Welf. 24, 335–343. https://doi.org/10.7120/ 09627286.24.3.335.
Balduzzi, D., Frean, M., Leary, L., Lewis, J.P., Ma, K.W.-D., McWilliams, B., 2017. The Shattered Gradients Problem: If resnets are the answer, then what is the question? International Conference on Machine Learning, pp. 342–350
Banhazi, T.M., Lehr, H., Black, J.L., Crabtree, H., Schofield, P., Tscharke, M., Berckmans, D., 2012. Precision livestock farming: an international review of scientific and commercial aspects. Int. J. Agric. Biol. Eng. 5, 1–9. https://doi.org/10.3965/j. ijabe.20120503.001.
Barron, J.L., Fleet, D.J., Beauchemin, S.S., 1994. Performance of optical flow techniques. Int.
J. Comput. Vis. 12, 43–77. https://doi.org/10.1007/BF01420984.
Berckmans, D., 2006. Automatic on-line monitoring of animals by precision livestock farming. Livest. Prod. Soc. 287. https://doi.org/10.3920/978-90-8686-567-3.

Berckmans, D., 2014. Precision Livestock Farming Technologies for Welfare Management in Intensive Livestock Systems. https://doi.org/10.20506/rst.33.1.2273.
Berckmans, D., 2017. General introduction to precision livestock farming. Anim. Front. 7, 6–11. https://doi.org/10.2527/af.2017.0102.
Berg, C., Sanotra, G.S., 2003. Can a modified latency-to-lie test be used to validate gait- scoring results in commercial broiler flocks? Anim. Welf. 12, 655–659.
Bessei, W., 2006. Welfare of broilers: a review. Worlds. Poult. Sci. J. 62, 455–466. https:// doi.org/10.1017/S0043933906001085.
Bessei, W., 2018. Impact of animal welfare on worldwide poultry production. Worlds.
Poult. Sci. J. 74, 211–224. https://doi.org/10.1017/S0043933918000028.
Bhargava, A., Bansal, A., 2018. Fruits and vegetables quality evaluation using computer vi- sion: a review. J. King Saud Univ. Inf. Sci. https://doi.org/10.1016/j.jksuci.2018.06.002. Bloemen, H., Aerts, J., Berckmans, D., Goedseels, V., 1997. Image analysis to measure activ- ity index of animals. Equine Vet. J. 29, 16–19. https://doi.org/10.1111/j.2042-
3306.1997.tb05044.x.
Bokkers, E.A.M., Zimmerman, P.H., Rodenburg, T.B., Koene, P., 2007. Walking behaviour of heavy and light broilers in an operant runway test with varying durations of feed deprivation and feed access. Appl. Anim. Behav. Sci. 108, 129–142. https://doi.org/ 10.1016/j.applanim.2006.10.011.
Bradshaw, R.H., Kirkden, R.D., Broom, D.M., 2002. A review of the aetiology and pathology of leg weakness in broilers in relation to welfare. Avian Poult. Biol. Rev. 13, 45–103. https://doi.org/10.3184/147020602783698421.
Butcher, G.D., Jacob, J.P., Mather, F.B., 1999. Common poultry diseases. PS47-series Vet.
Med. Anim. Clin. Sci. Dep. UF/IFAS Ext.
Caplen, G., Hothersall, B., Murrell, J.C., Nicol, C.J., Waterman-Pearson, A.E., Weeks, C.A., Colborne, G.R., 2012. Kinematic analysis quantifies gait abnormalities associated with lameness in broiler chickens and identifies evolutionary gait differences. PLoS One 7, e40800. https://doi.org/10.1371/journal.pone.0040800.
Caplen, G., Colborne, G.R., Hothersall, B., Nicol, C.J., Waterman-Pearson, A.E., Weeks, C.A., Murrell, J.C., 2013. Lame broiler chickens respond to non-steroidal anti- inflammatory drugs with objective changes in gait function: a controlled clinical trial. Vet. J. 196, 477–482. https://doi.org/10.1016/j.tvjl.2012.12.007.
Chandrashekar, G., Sahin, F., 2014. A survey on feature selection methods. Comput. Electr.
Eng. 40, 16–28. https://doi.org/10.1016/j.compeleceng.2013.11.024.
Chavolla, E., Zaldivar, D., Cuevas, E., Perez, M.A., 2018. Color spaces advantages and disad- vantages in image color clustering segmentation. Advances in Soft Computing and Machine Learning in Image Processing. Springer, pp. 3–22 https://doi.org/10.1007/ 978-3-319-63754-9_1.
Cheng, H.-D., Jiang, X.H., Sun, Y., Wang, J., 2001. Color image segmentation: advances and prospects. Pattern Recogn. 34, 2259–2281. https://doi.org/10.1016/S0031-3203(00)
00149-7.
Chollet, F., 2017. Xception: deep learning with depthwise separable convolutions. Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 1251–1258.
Colles, F.M., Cain, R.J., Nickson, T., Smith, A.L., Roberts, S.J., Maiden, M.C.J., Lunn, D., Dawkins, M.S., 2016. Monitoring chicken flock behaviour provides early warning of infection by human pathogen Campylobacter. Proc. R. Soc. B Biol. Sci. 283, 20152323. https://doi.org/10.1098/rspb.2015.2323.
Corr, S.A., McCorquodale, C.C., Gentle, M.J., 1998. Gait analysis of poultry. Res. Vet. Sci. 65, 233–238. https://doi.org/10.1016/S0034-5288(98)90149-7.
Corr, S.A., McCorquodale, C., McDonald, J., Gentle, M., McGovern, R., 2007. A force plate study of avian gait. J. Biomech. 40, 2037–2043. https://doi.org/10.1016/j. jbiomech.2006.09.014.
Cortes, C., Vapnik, V., 1995. Support-vector networks. Mach. Learn. 20, 273–297. https:// doi.org/10.1007/BF00994018.
Damerow, G., 2016. The Chicken Health Handbook: A Complete Guide to Maximizing Flock Health and Dealing With Disease. Storey Publishing.
Dawkins, M.S., 2017. Animal welfare and efficient farming: is conflict inevitable? Anim.
Prod. Sci. 57, 201–208. https://doi.org/10.1071/AN15383.
Dawkins, M.S., Lee, H., Waitt, C.D., Roberts, S.J., 2009. Optical flow patterns in broiler chicken flocks as automated measures of behaviour and gait. Appl. Anim. Behav. Sci. 119, 203–209.
Dawkins, M.S., Cain, R., Roberts, S.J., 2012. Optical flow, flock behaviour and chicken wel- fare. Anim. Behav. 84, 219–223. https://doi.org/10.1016/j.anbehav.2012.04.036.
Dawkins, M.S., Cain, R., Merelie, K., Roberts, S.J., 2013. In search of the behavioural corre- lates of optical flow patterns in the automated assessment of broiler chicken welfare. Appl. Anim. Behav. Sci. 145, 44–50. https://doi.org/10.1016/j.applanim.2013.02.001.
Dawkins, M., Roberts, S.J., Cain, R., Nickson, T., Donnelly, C., 2017. Early warning of footpad dermatitis and hockburn in broiler chicken flocks using optical flow, body weight and water consumption. Vet. Rec. 180. https://doi.org/10.1177/0967033520927519.
De Wet, L., Vranken, E., Chedad, A., Aerts, J.-M., Ceunen, J., Berckmans, D., 2003. Computer- assisted image analysis to quantify daily growth rates of broiler chickens. Br. Poult. Sci. 44, 524–532. https://doi.org/10.1080/00071660310001616192.
Duda, R.O., Hart, P.E., Stork, D.G., 2001. Unsupervised learning and clustering. Pattern Classif. 517–601.
Dyson, T., 1999. World food trends and prospects to 2025. Proc. Natl. Acad. Sci. 96, 5929–5936. https://doi.org/10.1073/pnas.96.11.5929.
Fang, C., Huang, J., Cuan, K., Zhuang, X., Zhang, T., 2020. Comparative study on poultry tar- get tracking algorithms based on a deep regression network. Biosyst. Eng. 190, 176–183. https://doi.org/10.1016/j.biosystemseng.2019.12.002.
FAO, 2018. World Food And Agriculture Statistical Pocketbook 2018.
Faucitano, L., 2018. Meat science and muscle biology symposium: international perspec- tives on animal handling and welfare and meat quality preslaughter handling prac- tices and their effects on animal welfare and pork quality. J. Anim. Sci. https://doi. org/10.1093/jas/skx064.
Fraess, G.A., Bench, C.J., Tierney, K.B., 2016. Automated behavioural response assessment to a feeding event in two heritage chicken breeds. Appl. Anim. Behav. Sci. 179, 74–81. https://doi.org/10.1016/j.applanim.2016.03.002.
Fujii, T., Yokoi, H., Tada, T., Suzuki, K., Tsukamoto, K., 2009. Poultry tracking system with camera using particle filters. 2008 IEEE International Conference on Robotics and Bio- mimetics. IEEE, pp. 1888–1893 https://doi.org/10.1109/ROBIO.2009.4913289.
Fujiyoshi, H., Hirakawa, T., Yamashita, T., 2019. Deep learning-based image recogni- tion for autonomous driving. IATSS Res. 43, 244–252. https://doi.org/10.1016/j. iatssr.2019.11.008.
Gerland, P., Raftery, A.E., Ševčíková, H., Li, N., Gu, D., Spoorenberg, T., Alkema, L., Fosdick, B.K., Chunn, J., Lalic, N., 2014. World population stabilization unlikely this century. Science (80-.) 346, 234–237. https://doi.org/10.1126/science.1257469.
Girshick, R., 2015. Fast r-cnn. Proceedings of the IEEE International Conference on Com- puter Vision, pp. 1440–1448.
Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014. Rich feature hierarchies for accurate ob- ject detection and semantic segmentation. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 580–587.
González, R.C., Woods, R.E., Eddins, S.L., 2004. Digital Image Processing Using MARLAB. Pearson.
Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. MIT press.
Green, L.E., Kaler, J., Wassink, G.J., King, E.M., Grogono Thomas, R., 2012. Impact of rapid treatment of sheep lame with footrot on welfare and economics and farmer attitudes to lameness in sheep. Anim. Welf. 21, 65–71. https://doi.org/ 10.7120/096272812X13345905673728.
Gulli, A., Pal, S., 2017. Deep Learning With Keras. Packt Publishing Ltd.
Guo, Y., Chai, L., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., 2020. A machine vision- based method for monitoring broiler chicken floor distribution. Sensors 20, 3179. https://doi.org/10.3390/s20113179.
Hamuda, E., Mc Ginley, B., Glavin, M., Jones, E., 2017. Automatic crop detection under field conditions using the HSV colour space and morphological operations. Comput. Elec- tron. Agric. 133, 97–107. https://doi.org/10.1016/j.compag.2016.11.021.
Haque, I.R.I., Neubert, J., 2020. Deep learning approaches to biomedical image segmenta- tion. Informatics Med. Unlocked 18, 100297. https://doi.org/10.1016/j. imu.2020.100297.
Hastie, T., Tibshirani, R., Friedman, J., 2009. Unsupervised learning. The Elements of Statis- tical Learning. Springer, pp. 485–585.
He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,
pp. 770–778.
He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn. Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 2961–2969.
Healy, S., 2018. Consumers, corporate policy and animal welfare. The Business of Farm Animal Welfare. ROUTLEDGE in association with GSE Research, pp. 64–72.
Heise, H., Theuvsen, L., 2018. Citizens’ understanding of welfare of animals on the farm: an empirical study. J. Appl. Anim. Welf. Sci. 21, 153–169. https://doi.org/10.1080/ 10888705.2017.1400439.
Hemsworth, P.H., Coleman, G.J., 2010. Human-livestock interactions: the stockperson and the productivity of intensively farmed animals. CABI. https://doi.org/10.1080/ 00480169.2014.966167.
Hemsworth, P.H., Mellor, D.J., Cronin, G.M., Tilbrook, A.J., 2015. Scientific assessment of animal welfare. N. Z. Vet. J. 63, 24–30.
Henchion, M., McCarthy, M., Resconi, V.C., Troy, D., 2014. Meat consumption: trends and quality matters. Meat Sci. 98, 561–568. https://doi.org/10.1016/j. meatsci.2014.06.007.
Hoerr, F.J., 2010. Clinical aspects of immunosuppression in poultry. Avian Dis. 54, 2–15. https://doi.org/10.1637/8909-043009-Review.1.
Horn, B.K.P., Schunck, B.G., 1981. Determining optical flow. Techniques and Applications of Image Understanding. International Society for Optics and Photonics,
pp. 319–331 https://doi.org/10.1016/0004-3702(81)90024-2.
Hosseini, M.-P., Lu, S., Kamaraj, K., Slowikowski, A., Venkatesh, H.C., 2020. Deep learning architectures. Deep Learning: Concepts and Architectures. Springer, pp. 1–24 https://doi.org/10.1007/978-3-030-31756-0_1.
Huang, X., Wu, L., Ye, Y., 2019. A review on dimensionality reduction techniques. Int.
J. Pattern Recognit. Artif. Intell. 33, 1950017. https://doi.org/10.1142/ S0218001419500174.
Hughes, B.L., Leong, J.K., Shiv, B., Zaki, J., 2018. Wanting to like: motivation influences be- havioral and neural responses to social feedback. bio Rxiv, 300657 https://doi.org/ 10.1101/300657.
Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016. SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5 MB model size. arXiv Prepr. abs/1602.07360, arXiv1602.07360. https://arxiv.org/abs/ 1602.07360.
Ibraheem, N.A., Hasan, M.M., Khan, R.Z., Mishra, P.K., 2012. Understanding color models: a review. ARPN J. Sci. Technol. 2, 265–275.
Ireri, D., Belal, E., Okinda, C., Makange, N., Ji, C., 2019. A computer vision system for defect discrimination and grading in tomatoes using machine learning and image process- ing. Artif. Intell. Agric. https://doi.org/10.1016/j.aiia.2019.06.001.
Jana, A., 2012. Kinect for Windows SDK Programming Guide. Packt Publishing Ltd.
Jansen-van Vuuren, R.D., Armin, A., Pandey, A.K., Burn, P.L., Meredith, P., 2016. Organic photodiodes: the future of full color detection and image sensing. Adv. Mater. 28, 4766–4802. https://doi.org/10.1002/adma.201505405.
Jaykaran, 2010. How to select appropriate statistical test? J. Pharm. Negat. Results 1, 61.
Ju, M., Luo, H., Wang, Z., Hui, B., Chang, Z., 2019. The application of improved YOLO V3 in multi-scale target detection. Appl. Sci. 9, 3775. https://doi.org/10.3390/app9183775. Kaelbling, L.P., Littman, M.L., Moore, A.W., 1996. Reinforcement learning: a survey. J. Artif.
Intell. Res. 4, 237–285. https://doi.org/10.1613/jair.301.

Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: a survey. Comput.
Electron. Agric. 147, 70–90. https://doi.org/10.1016/j.compag.2018.02.016.
Kashiha, M., Pluk, A., Bahr, C., Vranken, E., Berckmans, D., 2013. Development of an early warning system for a broiler house using computer vision. Biosyst. Eng. 116, 36–45. https://doi.org/10.1016/j.biosystemseng.2013.06.004.
Kashiha, M.A., Green, A.R., Sales, T.G., Bahr, C., Berckmans, D., Gates, R.S., 2014. Perfor- mance of an image analysis processing system for hen tracking in an environmental preference chamber. Poult. Sci. 93, 2439–2448. https://doi.org/10.3382/ps.2014- 04078.
Kawaguchi, K., Bengio, Y., 2019. Depth with nonlinearity creates no bad local minima in ResNets. Neural Netw. 118, 167–174. https://doi.org/10.1016/j.neunet.2019.06.009.
Kestin, S.C., Knowles, T.G., Tinch, A.E., Gregory, N.G., 1992. Prevalence of leg weakness in broiler chickens and its relationship with genotype. Vet. Rec. 131, 190–194.
Kestin, S.C., Gordon, S., Su, G., Sørensen, P., 2001. Relationships in broiler chickens be- tween lameness, liveweight, growth rate and age. Vet. Rec. 148, 195–197.
Knowles, T.G., Kestin, S.C., Haslam, S.M., Brown, S.N., Green, L.E., Butterworth, A., Pope, S.J., Pfeiffer, D., Nicol, C.J., 2008. Leg disorders in broiler chickens: prevalence, risk factors and prevention. PLoS One 3, e1545. https://doi.org/10.1371/journal.pone.0001545.
Kristensen, H.H., Aerts, J.-M., Leroy, T., Wathes, C.M., Berckmans, D., 2006. Modelling the dynamic activity of broiler chickens in response to step-wise changes in light intensity. Appl. Anim. Behav. Sci. 101, 125–143. https://doi.org/10.1016/j. applanim.2006.01.007.
Kristensen, H.H., Prescott, N.B., Perry, G.C., Ladewig, J., Ersbøll, A.K., Overvad, K.C., Wathes, C.M., 2007. The behaviour of broiler chickens in different light sources and illumi- nances. Appl. Anim. Behav. Sci. 103, 75–89. https://doi.org/10.1016/j. applanim.2006.04.017.
Kurnianggoro, L., Jo, K.-H., 2018. A survey of 2D shape representation: methods, evalua- tions, and future research directions. Neurocomputing 300, 1–16. https://doi.org/ 10.1016/j.neucom.2018.02.093.
Ladický, L., Russell, C., Kohli, P., Torr, P.H.S., 2009. Associative hierarchical crfs for object class image segmentation. 2009 IEEE 12th International Conference on Computer Vi- sion. IEEE, pp. 739–746 https://doi.org/10.1109/ICCV.2009.5459248.
LeCun, Y., Haffner, P., Bottou, L., Bengio, Y., 1999. Object recognition with gradient-based learning. Shape, Contour and Grouping in Computer Vision. Springer, pp. 319–345 https://doi.org/10.1007/3-540-46805-6_19.
Lehr, H., 2014. Recent advances in precision livestock farming. Int. Anim. Heal. J. 2, 44–49. Leroy, T., Vranken, E., Struelens, E., Sonck, B., Berckmans, D., 2005. Computer vision based recognition of behavior phenotypes of laying hens. 2005 ASAE Annual Meeting.
American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/2013.19471.
Leroy, T., Vranken, E., Van Brecht, A., Struelens, E., Sonck, B., Berckmans, D., 2006. A com- puter vision method for on-line behavioral quantification of individually caged poul- try. Trans. ASABE 49, 795–802. https://doi.org/10.13031/2013.20462.
Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.P., Tang, J., Liu, H., 2017. Feature selec- tion: a data perspective. ACM Comput. Surv. 50, 1–45. https://doi.org/10.1145/ 3136625.
Li, B., Liu, L., Shen, M., Sun, Y., Lu, M., 2019a. Group-housed pig detection in video surveil- lance of overhead views using multi-feature template matching. Biosyst. Eng. 181, 28–39. https://doi.org/10.1016/j.biosystemseng.2019.02.018.
Li, G., Zhao, Y., Chesser, G.D., Lowe, J.W., Purswell, J.L., 2019b. Image processing for analyz- ing broiler feeding and drinking behaviors. 2019 ASABE Annual International Meet- ing. American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/aim.201900165.
Lin, C.-Y., Hsieh, K.-W., Tsai, Y.-C., Kuo, Y.-F., 2018. Monitoring chicken heat stress using deep convolutional neural networks. 2018 ASABE Annual International Meeting. American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/aim.201800314.
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016. Ssd: single shot multibox detector. European Conference on Computer Vision. Springer,
pp. 21–37.
Llonch, P., King, E.M., Clarke, K.A., Downes, J.M., Green, L.E., 2015. A systematic review of animal based indicators of sheep welfare on farm, at market and during transport, and qualitative appraisal of their validity and feasibility for use in UK abattoirs. Vet.
J. 206, 289–297. https://doi.org/10.1016/j.tvjl.2015.10.019.
Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput.
Vis. 60, 91–110. https://doi.org/10.1023/B:VISI.0000029664.99615.94.
Lucas, B.D., Kanade, T., 1981. An Iterative Image Registration Technique With an Applica- tion to Stereo Vision.
Marıa, G.A., Escós, J., Alados, C.L., 2004. Complexity of behavioural sequences and their re- lation to stress conditions in chickens (Gallus gallus domesticus): a non-invasive technique to evaluate animal welfare. Appl. Anim. Behav. Sci. 86, 93–104. https:// doi.org/10.1016/j.applanim.2003.11.012.
Mehdizadeh, S.A., Neves, D.P., Tscharke, M., Nääs, I.A., Banhazi, T.M., 2015. Image analysis method to evaluate beak and head motion of broiler chickens during feeding. Comput. Electron. Agric. 114, 88–95. https://doi.org/10.1016/j.compag.2015.03.017.
Mollah, M.B.R., Hasan, M.A., Salam, M.A., Ali, M.A., 2010. Digital image analysis to estimate the live weight of broiler. Comput. Electron. Agric. 72, 48–52. https://doi.org/10.1016/ j.compag.2010.02.002.
Mortensen, A.K., Lisouski, P., Ahrendt, P., 2016. Weight prediction of broiler chickens using 3D computer vision. Comput. Electron. Agric. 123, 319–326. https://doi.org/ 10.1016/j.compag.2016.03.011.
Nääs, I. de A., Lozano, L.C.M., Mehdizadeh, S.A., Garcia, R.G., Abe, J.M., 2018. Paraconsistent logic used for estimating the gait score of broiler chickens. Biosyst. Eng. 173, 115–123. https://doi.org/10.1016/j.biosystemseng.2017.11.012.
Nakarmi, A.D., Tang, L., Xin, H., 2014. Automated tracking and behavior quantification of laying hens using 3D computer vision and radio frequency identification technolo- gies. Trans. ASABE 57, 1455–1472. https://doi.org/10.13031/trans.57.10505.
Neves, D.P., Mehdizadeh, S.A., Tscharke, M., de Alencar Nääs, I., Banhazi, T.M., 2015. Detec- tion of flock movement and behaviour of broiler chickens at different feeders using image analysis. Inf. Process. Agric. 2, 177–182. https://doi.org/10.1016/j. inpa.2015.08.002.
Noldus, L., Jansen, R.G., 2004. Measuring Broiler Chicken Behaviour and Welfare: Pros- pects for Automation.
Noldus, L.P.J.J., Spink, A.J., Tegelenbosch, R.A.J., 2001. EthoVision: a versatile video tracking system for automation of behavioral experiments. Behav. Res. Methods Instrum. Comput. 33, 398–414. https://doi.org/10.3758/BF03195394.
Nyalala, I., Okinda, C., Nyalala, L., Makange, N., Chao, Q., Chao, L., Yousaf, K., Chen, K., 2019. Tomato volume and mass estimation using computer vision and machine learning al- gorithms: cherry tomato model. J. Food Eng. 263, 288–298. https://doi.org/10.1016/j. jfoodeng.2019.07.012.
OECD-FAO, 2017. Organisation for Economic Co-operation and Development (OECD)/ Food and Agriculture Organization of the United Nations (FAO). 2017. Agric. Outlook 2017–2026 Spec. Focus Southeast Asia.
Okinda, C., Liu, L., Zhang, G., Shen, M., 2018a. Swine live weight estimation by adaptive neuro-fuzzy inference system. Indian J. Anim. Res., 52 https://doi.org/10.18805/ijar. v0iOF.7250.
Okinda, C., Lu, M., Nyalala, I., Li, J., Shen, M., 2018b. Asphyxia occurrence detection in sows during the farrowing phase by inter-birth interval evaluation. Comput. Electron. Agric. 152, 221–232. https://doi.org/10.1016/j.compag.2018.07.007.
Okinda, C., Lu, M., Liu, L., Nyalala, I., Muneri, C., Wang, J., Zhang, H., Shen, M., 2019. A machine vision system for early detection and prediction of sick birds: a broiler chicken model. Biosyst. Eng. 188, 229–242. https://doi.org/10.1016/j. biosystemseng.2019.09.015.
Okinda, C., Sun, Y., Nyalala, I., Korohou, T., Opiyo, S., Wang, J., Shen, M., 2020. Egg volume estimation based on image processing and computer vision. J. Food Eng., 110041 https://doi.org/10.1016/j.jfoodeng.2020.110041.
Otsu, N., 1979. A threshold selection method from gray-level histograms. IEEE Trans. Syst.
Man. Cybern. 9, 62–66. https://doi.org/10.1109/TSMC.1979.4310076.
Pal, N.R., Pal, S.K., 1993. A review on image segmentation techniques. Pattern Recogn. 26, 1277–1294. https://doi.org/10.1016/0031-3203(93)90135-J.
Pan, S.J., Yang, Q., 2010. A survey on transfer learning. IEEE Transactions on Knowledge and Data Engineering. 22, pp. 1345–1359. https://doi.org/10.1109/TKDE.2009.191.
Panagiotakis, C., Argyros, A., 2016. Parameter-free modelling of 2D shapes with ellipses.
Pattern Recogn. 53, 259–275. https://doi.org/10.1016/j.patcog.2015.11.004.
Paul-Murphy, J.R., Hawkins, M., 2014. Bird-specific considerations: recognizing pain be- havior in pet birds. Handbook of Veterinary Pain Management, Third edition Elsevier Inc., pp. 536–554 https://doi.org/10.1016/B978-0-323-08935-7.00026-0.
Pereira, D.F., Miyamoto, B.C.B., Maia, G.D.N., Sales, G.T., Magalhães, M.M., Gates, R.S., 2013. Machine vision to identify broiler breeder behavior. Comput. Electron. Agric. 99, 194–199. https://doi.org/10.1016/j.compag.2013.09.012.
Pu, H., Lian, J., Fan, M., 2018. Automatic recognition of flock behavior of chickens with convolutional neural network and kinect sensor. Int. J. Pattern Recognit. Artif. Intell. 32, 1850023. https://doi.org/10.1142/S0218001418500234.
Pulido, M., Barrena-González, J., Badgery, W., Rodrigo-Comino, J., Cerdà, A., 2018. Sustain- able grazing. Curr. Opin. Environ. Sci. Heal. 5, 42–46. https://doi.org/10.1016/j. coesh.2018.04.004.
Redmon, J., Farhadi, A., 2017. YOLO9000: better, faster, stronger. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 7263–7271 https://doi. org/10.1007/978-3-319-46448-0_2.
Redmon, J., Farhadi, A., 2018. Yolov3: an incremental improvement. arXiv Prepr. abs/ 1804.02767, arXiv1804.02767. https://arxiv.org/abs/1804.02767.
Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: unified, real- time object detection. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 779–788.
Reiter, K., Bessei, W., 1997. Gait analysis in laying hens and broilers with and without leg disorders. Equine Vet. J. 29, 110–112. https://doi.org/10.1111/j.2042-3306.1997. tb05067.x.
Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems,
pp. 91–99.
Rencher, A.C., Christensen, W.F., 2012. Chapter 10, Multivariate Regression–Section 10.1, Introduction. Methods Multivar. Anal. Wiley Ser. Probab. Stat. 709 p. 19.
Roberts, S.J., Cain, R., Dawkins, M.S., 2012. Prediction of welfare outcomes for broiler chickens using Bayesian regression on continuous optical flow data. J. R. Soc. Interface 9, 3436–3443. https://doi.org/10.1098/rsif.2012.0594.
Rozenboim, I., Biran, I., Chaiseha, Y., Yahav, S., Rosenstrauch, A., Sklan, D., Halevy, O., 2004. The effect of a green and blue monochromatic light combination on broiler growth and development. Poult. Sci. 83, 842–845. https://doi.org/10.1093/ps/83.5.842.
Rutherford, K.M.D., Haskell, M.J., Glasbey, C., Jones, R.B., Lawrence, A.B., 2004. Fractal anal- ysis of animal behaviour as an indicator of animal welfare. Anim. Welf. 13, 99–103.
Sabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules. Advances in Neural Information Processing Systems, pp. 3856–3866.
Salois, M., Baker, K., 2018. Factors Affecting Broiler Livability: Implications for Animal Welfare & Food Policy.
Samarasinghe, S., 2016. Neural Networks for Applied Sciences and Engineering: From Fundamentals to Complex Pattern Recognition. Auerbach publications.
Sehgal, G., Gupta, B., Paneri, K., Singh, K., Sharma, G., Shroff, G., 2017. Crop planning using stochastic visual optimization. 2017 IEEE Visualization in Data Science (VDS). IEEE,
pp. 47–51 https://doi.org/10.1109/VDS.2017.8573443.

Sergeant, D., Boyle, R., Forbes, M., 1998. Computer visual tracking of poultry. Comput.
Electron. Agric. 21, 1–18. https://doi.org/10.1016/S0168-1699(98)00025-8.
Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., 2013. Overfeat: inte- grated recognition, localization and detection using convolutional networks. arXiv Prepr. abs/1312.6229, arXiv1312.6229. https://arxiv.org/abs/1312.6229.
Shimokomaki, M., Ida, E.I., Soares, A.L., Oba, A., Kato, T., Pedrão, M.R., Coró, F.A.G., Carvalho, R.H., 2017. Animal welfare and meat quality: methodologies to reduce pre-slaughter stress in broiler chicken. Global Food Security and Wellness. Springer, pp. 301–313 https://doi.org/10.1007/978-1-4939-6496-3_16.
Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep learning. J. Big Data 6, 60. https://doi.org/10.1186/s40537-019-0197-0.
Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv Prepr. abs/1409.1556, arXiv1409.1556. https://arxiv.org/ abs/1409.1556.
Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., Yang, J., 2016. Modeling spatio-temporal distri- bution of soil moisture by deep learning-based cellular automata model. J. Arid Land 8, 734–748. https://doi.org/10.1007/s40333-016-0049-0.
Stadig, L.M., Rodenburg, T.B., Ampe, B., Reubens, B., Tuyttens, F.A.M., 2018. An automated positioning system for monitoring chickens’ location: effects of wearing a backpack on behaviour, leg health and production. Appl. Anim. Behav. Sci. 198, 83–88. https://doi.org/10.1016/j.applanim.2017.09.016.
Sundermeyer, M., Schlüter, R., Ney, H., 2012. LSTM neural networks for language model- ing. Thirteenth Annual Conference of the International Speech Communication Association.
Sundermeyer, M., Ney, H., Schlüter, R., 2015. From feedforward to recurrent LSTM neural networks for language modeling. IEEE/ACM Trans. Audio, Speech, Lang. Process. 23, 517–529. https://doi.org/10.1109/TASLP.2015.2400218.
Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2016a. Inception-v4, inception-resnet and the impact of residual connections on learning. arXiv Prepr. abs/1602.07261, arXiv1602.07261. https://arxiv.org/abs/1602.07261.
Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2016b. Rethinking the inception architecture for computer vision. Proceedings of the IEEE Conference on Computer Vision and Pattern Recognition, pp. 2818–2826.
Tablante, N.L., 2013. Common Poultry Diseases and Their Prevention. Univ. Maryl. Ext.
Tania, S., Rowaida, R., 2016. A comparative study of various image filtering techniques for removing various noisy pixels in aerial image. Int. J. Signal Process. Image Process. Pattern Recognit. 9, 113–124. https://doi.org/10.14257/ijsip.2016.9.3.10.
Thorp, B.H., Duff, S.R.I., 1988. Effect of exercise on the vascular pattern in the bone ex- tremities of broiler fowl. Res. Vet. Sci. 45, 72–77. https://doi.org/10.1016/S0034- 5288(18)30897-X.
Tian, Y., Yang, G., Wang, Z., Wang, H., Li, E., Liang, Z., 2019. Apple detection during differ- ent growth stages in orchards using the improved YOLO-V3 model. Comput. Electron. Agric. 157, 417–426. https://doi.org/10.1016/j.compag.2019.01.012.
Van Hertem, T., Norton, T., Berckmans, D., Vranken, E., 2018. Predicting broiler gait scores from activity monitoring and flock data. Biosyst. Eng. 173, 93–102. https://doi.org/ 10.1016/j.biosystemseng.2018.07.002.
Wang, C., Chen, H., Zhang, X., Meng, C., 2016. Evaluation of a laying-hen tracking algo- rithm based on a hybrid support vector machine. J. Anim. Sci. Biotechnol. 7, 1–10. https://doi.org/10.1186/s40104-016-0119-3.
Wang, A., Zhang, W., Wei, X., 2019a. A review on weed detection using ground-based ma- chine vision and image processing techniques. Comput. Electron. Agric. 158, 226–240. https://doi.org/10.1016/j.compag.2019.02.005.
Wang, J., Shen, M., Liu, L., Xu, Y., Okinda, C., 2019b. Recognition and classification of broiler droppings based on deep convolutional neural network. J. Sensors, 2019 https://doi. org/10.1155/2019/3823515.
Wang, J., Wang, N., Li, L., Ren, Z., 2020. Real-time behavior detection and judgment of egg breeders based on YOLO v3. Neural Comput. & Applic. 32, 5471–5481. https://doi.org/ 10.1007/s00521-019-04645-4.
Wathes, C.M., Kristensen, H.H., Aerts, J.-M., Berckmans, D., 2008. Is precision livestock farming an engineer’s daydream or nightmare, an animal’s friend or foe, and a farmer’s panacea or pitfall? Comput. Electron. Agric. 64, 2–10. https://doi.org/ 10.1016/j.compag.2008.05.005.
Weeks, C.A., Danbury, T.D., Davies, H.C., Hunt, P., Kestin, S.C., 2000. The behaviour of broiler chickens and its modification by lameness. Appl. Anim. Behav. Sci. 67, 111–125. https://doi.org/10.1016/S0168-1591(99)00102-1.
Weeks, C.A., Knowles, T.G., Gordon, R.G., Kerr, A.E., Peyton, S.T., Tilbrook, N.T., 2002. New method for objectively assessing lameness in broiler chickens. Vet. Rec. 151, 762–764. https://doi.org/10.1136/vr.151.25.762.
Welfare-Quality®, 2009. Welfare quality® assessment protocol for poultry (broilers, lay- ing hens). Welfare Quality® Consortium, Lelystad, NetherlandsQuality® (114 pp.).
Winter, D.A., 1985. Concerning the scientific basis for the diagnosis of pathological gait and for rehabilitation protocols. Physiother. Can. 37, 245–252.
Wongsriworaphon, A., Arnonkijpanich, B., Pathumnakul, S., 2015. An approach based on digital image analysis to estimate the live weights of pigs in farm environments. Comput. Electron. Agric. 115, 26–33. https://doi.org/10.1016/j.compag.2015.05.004.
Xiong, X., Lu, M., Yang, W., Duan, G., Yuan, Q., Shen, M., Norton, T., Berckmans, D., 2019. An automatic head surface temperature extraction method for top-view thermal image with individual broiler. Sensors 19, 5286. https://doi.org/10.3390/s19235286.
Youssef, A., Exadaktylos, V., Berckmans, D.A., 2015. Towards real-time control of chicken activity in a ventilated chamber. Biosyst. Eng. 135, 31–43. https://doi.org/10.1016/j. biosystemseng.2015.04.003.
Zaninelli, M., Redaelli, V., Luzi, F., Mitchell, M., Bontempo, V., Cattaneo, D., Dell’Orto, V., Savoini, G., 2018. Development of a machine vision method for the monitoring of lay- ing hens and detection of multiple nest occupations. Sensors 18, 132. https://doi.org/ 10.3390/s18010132.
Zeiler, M.D., Fergus, R., 2014. Visualizing and understanding convolutional networks. European Conference on Computer Vision. Springer, pp. 818–833 https://doi.org/ 10.1007/978-3-319-10590-1_53.
Zhang, H., Chen, C., 2020. Design of sick chicken automatic detection system based on im- proved residual network. 2020 IEEE 4th Information Technology, Networking, Elec- tronic and Automation Control Conference (ITNEC). IEEE, pp. 2480–2485 https:// doi.org/10.1007/978-3-319-50835-1_14.
Zhang, D., Lu, G., 2004. Review of shape representation and description techniques. Pat- tern Recogn. 37, 1–19. https://doi.org/10.1016/j.patcog.2003.07.008.
Zhang, L., Gray, H., Ye, X., Collins, L., Allinson, N., 2019. Automatic individual pig detection and tracking in pig farms. Sensors 19, 1188. https://doi.org/10.3390/s19051188.
Zhuang, X., Zhang, T., 2019. Detection of sick broilers by digital image processing and deep learning. Biosyst. Eng. 179, 106–116. https://doi.org/10.1016/j. biosystemseng.2019.01.003.
Zhuang, X., Bi, M., Guo, J., Wu, S., Zhang, T., 2018. Development of an early warning algo- rithm to detect sick broilers. Comput. Electron. Agric. 144, 102–113. https://doi.org/ 10.1016/j.compag.2017.11.032.
