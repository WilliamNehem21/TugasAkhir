Array 11 (2021) 100074

		




Wild Geese Algorithm: A novel algorithm for large scale optimization based on the natural life and death of wild geese
Mojtaba Ghasemi a, Abolfazl Rahimnejad b, Rasul Hemmati c, Ebrahim Akbari d,
S. Andrew Gadsden b,*
a Department of Electronics and Electrical Engineering, Shiraz University of Technology, Shiraz, Iran
b Department of Engineering Systems and Computing, University of Guelph, Guelph, Canada
c Department of Electrical and Computer Engineering, Marquette University, Milwaukee, USA
d Department of Electrical Engineering, Faculty of Engineering, University of Isfahan, Isfahan, Iran



A R T I C L E I N F O


Keywords:
Large-scale global optimization (LSGO) Wild Geese Algorithm (WGA)
Swarm-based method Engineering optimization
A B S T R A C T

In numerous real-life applications, nature-inspired population-based search algorithms have been applied to solve numerical optimization problems. This paper focuses on a simple and powerful swarm optimizer, named Wild Geese Algorithm (WGA), for large-scale global optimization whose efficiency and performance are verified using
large-scale test functions of IEEE CEC 2008 and CEC 2010 special sessions with high dimensions D ¼ 100, 500,
1000. WGA is inspired by wild geese in nature and models various aspects of their life such as evolution, regular
cooperative migration, and fatality. The effectiveness of WGA for finding the global optimal solutions of high- dimensional optimization problems is compared with that of other methods reported in the previous literature. Experimental results show that the proposed WGA has an efficient performance in solving a range of large-scale optimization problems, making it highly competitive among other large-scale optimization algorithms despite its simpler structure and easier implementation. The source code of the proposed WGA algorithm is publicly available at github.com/ebrahimakbary/WGA.





Introduction

Many practical optimization problems, which are called Large Scale Global Optimization (LSGO) problems, deal with a lot of decision vari- ables. Some practical LSGO problems are large-scale electronic systems design, scheduling problems, vehicle routing in large-scale traffic net- works, and inverse problem chemical kinetics. Many real-world optimi- zation problems involve optimization of a large number of control variables with various constraints. However, the classical mathematical programming methods do not generally provide good solutions for different optimization problems with different real-world complexities, due to the huge size of the problems [1]. The global optimization per- formance of the population-based algorithms often becomes weaker in such problems with increasing the dimension and complexity of the problem [2–4]. The practical large-scale optimization problems have been modeled with different benchmark test functions such as those presented in the CEC 2008 [5] and CEC 2010 [6].
Recently, many nature-inspired and population-based meta-heuristic optimization algorithms have been presented to deal with LSGO
problems with different real-world complexities such as nonlinearity, non-smoothness, non-convexity, mixed-integer nature, non- differentiability, etc. Some new nature-inspired optimization algo- rithms for solving the practical large-scale optimization problems are listed in Table 1. It should be mentioned that, the boldface rows of this table, show the methods which were used in the comparative study with the proposed WGA.
Wild geese have a long-distance, coordinated and organized travel, which can be used as an inspiration for a very appropriate optimization algorithm for high-dimension problems. Based on the general model of wild geese’ lives, a novel algorithm called Wild Geese Algorithm (WGA) is introduced in this paper, which have some main prominent charac- teristics compared to the previous algorithms including:
It is simple with low computational burden, and its implementation is easily performed.
It has proper and satisfactory power for different test functions, from different groups.



* Corresponding author.
E-mail address: gadsden@uoguelph.ca (S.A. Gadsden).

https://doi.org/10.1016/j.array.2021.100074
Received 23 December 2020; Received in revised form 1 May 2021; Accepted 16 June 2021
Available online 25 June 2021
2590-0056/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-
nc-nd/4.0/).



Table 1
Summary of some new nature-inspired optimization algorithms for solving the practical large-scale optimization problems.
Table 1 (continued )
Ref.	Year	Abbreviation	Short Description	Dimensions
under study


Real- world


distributions
[18]	2012	LSCBO	Large Scale Optimization Based on Co-ordinated Bacterial Dynamics and Opposite Numbers

100, 500,	No
1000
It is worth mentioning that although the proposed WGA may seem similar to PSO, especially due to the existence of personal best and global best concepts, it has some thorough distinctions of structure and formulation, the main of which can be listed as follows:



In WGA, all solutions are sorted based on their objective values so that each member of population moves using information from its adja- cent members in the sorted population.
In the proposed method, the formulation for calculating the velocity of each goose is completely different from the PSO and is based on the positions, velocities, and best positions of the goose and its adjacent geese in the sorted population, as well as the global best solution's position. While in PSO, the only parameter that is shared among all solutions is the position of the global best solution.
In the proposed WGA, two different solutions are generated per so- lution and are used for creating the next iteration's goose based on a mechanism similar to the crossover operator of differential evolution.
Finally, in the proposed algorithm, a population reduction policy is implemented which is accomplished by fatality (elimination) of the weakest goose of the population.

The rest of this paper is organized as follows. Section 2 presents the new proposed algorithm for large-scale optimization problems. Section 3 shows the experimental results. Finally, Section 4 presents the conclusions.

The proposed algorithm: Wild Geese Algorithm

In recent years, some new algorithm inspired from group movement and group search by animals have been proposed for large-scale global



Fig. 1. An ordered and coordinated migration of wild geese.



where xi;d, pi;d, and vi;d are the dth dimension of the current position, the best position, and the current velocity of the ith wild goose, respectively.
Note that in this study, rk;d; k = 1; 2; ...; 11 are uniformly distributed
random numbers between 0 and 1.
As observed in Eq. (1), the velocity and position changes of each wild goose (for instance i-th wild goose) depend on the velocities of their
upfront and rear members, i.e ( vIter —vIter ) , and also to the positions of its

continuous optimization [1]. In this paper, based on the different phases
adjacent members.
i+1
i—1

of wild geese's lives, including their rhythmic and coordinated group migration, reproduction and evolution and also deaths in the population of geese, a new efficient algorithm, named as Wild Geese Algorithm (WGA), is presented for high-dimensional optimization problems. In
According to the model from the migration of wild geese in Fig. 2 and Eq. (1), the wild geese use information from their adjacent individuals in the sorted population, as patterns for their movement and navigation,
and tend to reach those members (reduce their distances), i.e. xIter →

Fig. 1, a group ordered migration based on the position of wild geese is
pIter
Iter
Iter
Iter
Iter
Iter
Iter
i—1

shown. In general, the proposed WGA phases are as follows:

Ordered and coordinated group migration (or migration and displacement velocity phase)
Walking and searching for food by wild geese.
Reproduction and evolution of wild geese.
i ; xi  → pi+1; xi+1 → pi+2, and xi+2 → — pi—1.
Additionally, the global best member is used as another guide for the
movements of the whole flock; which is reflected in Eq. (2). This position change is carried out in an ordered form and coordinated with the upfront members in order to model the movement of all members as an ordered series, as shown in Figs. 1 and 2.

Death, migration and ordered evolution of wild geese.
xv = pIter + r  × r
×  gIter + pIter
— 2 × pIter + vIter+1	(2)


First, an initial population of wild geese are created, so that the po-
i;d
i;d
7;d
8;d
d	i+1;d
i;d
i;d

sition vector of the i-th wild goose is equal to xi. The best local position or personal best solution pi and migration velocity viare determined. Then, all wild geese populations are sorted from the best to the worst according to their target function.
In this modeling strategy, each wild goose exploits information from
its adjacent wild geese in the ordered population, and is directed by those individuals. The phases of WGA are further discussed in the subsequent
where gd is the global best position among all members.

Walking and searching for food by wild geese

This step is modeled in such a way that the i-th wild goose moves
towards its upfront member, i.e. the (i+1)-th goose (pIter → pIter ). In

another word, the i-th goose tries to reach the (i+1)-th goose (pIter — pIter ).

subsections.
i+1	i

The equation for walking and searching for food by the wild goose, xW is

An ordered and coordinated group migration (or migration and displacement velocity phase)
as follows:
xw = pIter + r9;d × r10;d × pIter
— pIter	(3)


As it is observed in Fig. 1, migration of wild geese is a group, coor-
i;d
i;d
i+1;d
i;d

dinated, ordered and under control migration, which is based on reach- ing the upfront and adjacent individuals in the sorted population. Velocity and displacement equations according to the coordinated ve- locity of the geese are given in Eq. (1) and Eq. (2).
Reproduction and evolution of wild geese

Another stage of wild geese's life is reproduction and evolution. In this paper, its modeling is performed so that a combination between migra-
tion equation (xV ) and walking and search for food equation (xW ) is used.

vIter+1 = r  × vIter + r
× vIter  — vIter	i	i

+r3;d × pIter — xIter
  + r4;d × pIter
— xIter 
(1)

 	




Fig. 2. The model of ordered and coordinated group migration of wild geese.



Death, migration and ordered evolution
The previous experiments from the literature show that for different optimization algorithms the population number and the iteration number
Algorithm 1 (continued )
19: XIter+1 ← Eq. (4);
20: end for
21: if xIter+1 < xmin

i;d	d

do not have the same level of influence on solving every types of prob- lems. For some functions, the size of algorithm's population is more important and more effective than the number of algorithm's iterations
22: xIter+1 ← xmin ;
23: end if
24: if xIter+1 > xmax

25: xIter+1 ← xmax ;

(e.g. F2 and F3 functions), and for some other functions the number of algorithm's iterations is more important and more effective than the size
i;d	d
26: end if
27: to evaluate the fitness of XIter+1

of WGA algorithm's population (e.g. F7 and F8 functions). In this paper,
28: if f (X

Iter
i
+1 )≤ f (PIter )

to overcome this problem and establish a compromised solution, the	i	i

death phase is employed in order to balance algorithm performance for all test functions. In this phase, the algorithm starts with the maximum population number Npinitial and during the algorithm iterations, the
29: PIter+1 ← XIter+1 ;
30: end if
31: if f (PIter+1)≤ f (G)
32: G ← PIter+1;

weaker members will be removed from the population based on Eq. (5)
and the population size will decrease linearly so that it reaches its final value Npfinal in the final iteration.
33: end if
34: end for
35: FEs = FEs + Np;
36: Np ← Eq. (5);
37: end while

0 Npinitial	1

Np = roundB@ — Npinitial — Npfinal *  FEs  CA	(5)
where FEs and FEsmax are the number of function evaluations and its maximum.

Algorithm 1
Demonstrates the optimization process of WGA.

Results and analysis of experimental evaluation studies

In this section, 20 widely used large scale test functions are exploited to show the efficiency and performance of the proposed algorithm. The formulation and characteristics of all CEC 2010 benchmark test functions are listed in Ref. [6].
The performance and robustness of WGA for solving real and large-

		scale optimization problems are characterized by two indices: 1) the

Algorithm 1:
1: to set values of the control parameters of WGA;

2: to generate the initial population (whose number are equal to Npinitial ) and VIter=1 = [0];
3: to evaluate the fitness of each population individual and FEs = Npinitial ;
4: to find the personal best position of all particles Npinitial (i = 1, 2, …, Npinitial ) in swarm Pi and the global best position G;
5: while the FEs till FEsmax do
6: Wild Goose populations are arranged from the best to the worst according to Fig. 2;
7: for i = 1 (best) to Np (worst) do
8: Select the sorted members i — 1th; i + 1th; and i + 2th;
9:    for    d     =    1    to     D    do {** An ordered and coordinated group migration based on Eq. (1) and Eq. (2) **}
10: VIter+1 ← Eq. (1);
11: end for
12: for d = 1 to D do
13: xV ← Eq. (2);
14: end for
15:  for  d  =  1  to  D  do {** Walking and search geese Eq. (3) **}
16: xW ← Eq. (3);
17: end for
18:  for  d  =  1  to  D  do {** Reproduction and evolution Eq. (4) **}
(continued on next column)
mean of best values of test function (Mean), and 2) the standard deviation (Std) indices.
Test functions include 1. Separable functions (F1 — F3), 2. Single- group m-nonseparable functions (F4 — F8), 3.  D -group m-nonseparable functions (F9 — F13), 4.  D-group m-nonseparable functions (F14 — F18), and 5. non-separable functions (F19 — F20), where m is the number of variables in each non-separable subcomponent, and D and m are assumed
as 1000 and 50, respectively. To show the efficiency of WGA, in all simulations of this paper, 25 independent simulations are used in each
simulations, the maximum number of fitness evaluations FEsmax is 3 × section for every test function, as in Refs. [6,22]. Furthermore, in all
106. In all tables, the + sign means the algorithm outperforms WGA, the
– sign means WGA outperforms the algorithm, and the = sign means WGA and the considered algorithm yield the same solution for the given
problem. It should be mentioned that, in all results tables, the boldface is used to emphasize the algorithm that achieves the best Mean index value for each problem.

Experimental setup

Influence of death phase on WGA performance
At first, to show the performance of the population reduction by death



Table 2
Average fitness values and standard deviations of results for test functions over 25 independent runs.




tested with a large population Np = 120 and a small population Np = 30. of Wild Geese, WGA is tested without considering the death phase and is population reduction from Np = 120 (Npinitial =120) to Np = 30 The suitable results were compared with those of WGA (considering
(Npfinal =30) using Eq. (5), where the results obtained for each function
are listed in Table 2. The results demonstrate that the proposed death
phase improves the efficiency of WGA for high-dimensional problems. The positive influence of death phase can be especially observed for test
functions F3, F6, F7, F11, F12, F16, and F17. Moreover, the convergence characteristics of this algorithm for 6 different functions of various types are depicted in Fig. 3, which verify the effectiveness of implementing death phase in WGA.
Why Cr = 0.5 in WGA for all test functions?
In this paper Cr = 0.5 is used for all simulations. To select a suitable value for Cr four different constant values other than 0.5, i.e. 0.1, 0.25,
0.75 and 0.9 are tested, whose results are presented in Table 3. As observed, the constant value 0.5 is the best value for different test functions of CEC 2010. It should be mentioned that in all simulation results tables, three values are reported for optimizing each test function with each algorithm; the first two demonstrate the average and standard deviation of fitness values of the obtained results. The third value shows the rank of that algorithm in terms of the mean index. Furthermore, three parameters are reported for each algorithm in all tables, i.e. Nb, Nw, Mr. Nb and Nw are the number of times the algorithm yields the best and the worst mean index, respectively; and Mr is the average rank of the algo- rithm achieved in solving all considered test functions.

Comparing WGA with recent optimization algorithms

CEC 2008 test functions
In this section, the results of WGA are compared with those of a series of the recently proposed optimization algorithms for large-scale prob- lems from CEC 2008 test functions with different high dimensions
including D = 100, D = 500 and D = 1000. The formulation and char- acteristics of CEC' 08 benchmark test functions are listed in Ref. [5] and
Table 4:
Two indices are exploited in this study to characterize the perfor- mance and robustness of WGA for solving real and large-scale optimi- zation problems with different dimensions: 1) mean of the best values of test function (Mean), and 2) standard deviation (Std). Tables 5–7 show the final best solutions of test functions’ optimization by WGA and those of large scale optimization algorithms including CSO [24], CCPSO2 [2], sep-CMA-ES [9], MLCC [7], and EPUS-PSO [8]. As seen, the proposed WGA is able to provide very efficient and competitive results in solving real and large-scale problems compared with the previously proposed algorithms. WGA proves itself as a promising technique for real and large scale shifted unimodal and multimodal optimization problems.

CEC 2010 test functions
As mentioned in the introduction section of this paper, numerous researches have been recently performed to achieve some algorithms and meta-heuristic optimization methods for high-dimension optimization problems. These studies and many other methods have been introduced to find a simple and quick method with the low computational burden. In
of CEC 2010 with D = 1000 are summarized [22], which was obtained Table 8, the results of previous researches for 20 different test functions with the same conditions as those of WGA. The summarized algorithms in
Table 8 include MLCC algorithm [7], differential evolution with coop- erative co-evolution and delta grouping DECC-D and DECC-DML [12], contribution based cooperative co-evolution and differential grouping CBCC1-DG and CBCC2-DG [22], differential evolution with cooperative co-evolution and random grouping (DECC-DG) [22]. The last two rows of Table 8 present the comparative indices for these algorithms.
The WGA algorithm has achieved the best results in 12 of 20 func- tions, i.e. F4, F5, F6, F7, F9, F10, F13, F14, F15, F17, F18, and F19. In
addition, for none of the test functions WGA has the worst results. Moreover, WGA reaches the best average rank (Mr). The proposed al- gorithm (WGA) outperforms MLCC algorithm in 18 out of 20 functions; only for the first two functions MLCC algorithm performs better. For the first function the average value of WGA is very close to that of MLCC algorithm. MLCC algorithm has different results for different test func- tions and has the worst results for 6 out of 20 functions. However, the proposed algorithm has acceptable and suitable results for most of the




Fig. 3. Average convergence of WGA on nine selected test functions over 25 independent runs.



test functions and dispersion of its results are less than those of the other algorithms. The comparison between WGA and DECC-D algorithm shows that WGA performs better for 18 out of 20 functions. Nonetheless, for functions F2 and F20, it gives a worse result compared to that of DECC-D. For function F2, the average value of WGA is very close to that obtained from DECC-D algorithm. Furthermore, DECC-D algorithm does not pro- vide a good quality solution for different test functions, for example for
F2 and F20 it has suitable results, but for F5 — F8, F10 — F12, and F15—
F17 its results are not acceptable compared to those of other algorithms.
Although DECC-DML algorithm outperforms WGA for five test functions, it has the worst solution for six functions. CBCC1-DG and CBCC2-DG algorithms are more successful than WGA for two and three functions, respectively; however, CBCC1-DG gives the best result for none of the functions and CBCC2-DG yields the best result for only function. DECC- DG algorithm performs better than WGA for 2 out of 20 test functions; however, it gives the worst solution for 4 test functions among all algorithms.
Test on real-world optimization problems
Here, the effectiveness of the proposed algorithm (WGA) was inves- tigated compared to genetic algorithm (GL-25) [34], DE with strategy adaptation (SaDE) [35], DE with control components and composite trial vector generation approaches (CoDE) [36], Standard particle swarm optimization (SPSO2013) [37], and heterogeneous comprehensive learning PSO with improved exploitation and exploration (HCLPSO) [38] on real-world usages including estimating the factor for frequency-modulated (FM) sound waves [39] and large-scale reliabili- ty-redundancy allocation optimization (RRAO) of a gas turbine [40].

Estimating the factor for frequency modulated sound waves

The greatly complex multimodal frequency-modulated (FM) sound synthesis optimizing problem plays a key role in various modern music systems for estimating the optimal factors of a FM sound wave synthesis [39]. The estimation of optimal factors of an FM sound wave synthesis is


Table 3
Average fitness values and standard deviations on test functions over 25 inde- pendent runs.


Table 4
Summary of CEC 08 Special Session benchmark test functions [5] for large scale global optimization.

sound waves for t defined in range of 1–100 are as follows [42]:
y(t)= x1 sin(x2tθ + x3 sin(x4tθ + x5 sin(x6tθ))),	(6)
y0(t)= 1.0*sin(0.5tθ — 1.5 * sin(4.8tθ + 2.0 * sin(4.9tθ))),	(7) where θ =  2π 
The optimization problem objective function is considered as the sum
of squared errors between y(t)(the approximated wave) and y0(t) (the target wave) with optimal value f(x) = 0 as follows:
100
f (x)=	(y(t)— y0(t))2.	(8)
t=0
RRAO constrained problem:

The nonlinear reliability-redundancy constrained optimization problems are mainly aimed at enhancing the system reliability (maxi- mizing the overall system reliability) through optimizing element re-
liabilities vector (r = (r1, r2, …, rm)) and redundancy assignment numbers vector (n= (n1, n2, …, nm)) for subsystems of the system. It is possible to formulate this problem as a nonlinear mixed-integer programming model
by choosing the system reliability as the objective function to be maxi- mized subjecting to several nonlinear constraints as follows [40]:
Maximize Rs = f (r, n),	(9)

subject to g(r, n)≤ l,
0 ≤ rd ≤ 1, nd ∈ Z+, 0 ≤ d ≤ m.
(10)










of D = 6 is only considered in accordance with [41,42]. Six components are included in the 6-dimensional parameter vector as x = [x1(a1), an optimization problem with D decision variables. In this work, the case
x2(ω1), x3(a2), x4(ω2), x5(a3), x6(ω3)] ranging between 6.35 and 6.5 for
all variables. The equations provided for the target and approximated
where Z+ is the set of positive integers, Rs represents the reliability of
various systems, f(.) and g(.) denote for the objective and constraint
functions of RRAO problem for the total parallel-series systems, respec-
volume. n= (n1, n2, …, nm) and r = (r1, r2, …, rm) show the redundancy tively, from which g(.) is usually related to the system cost, weight and allocation numbers and component reliabilities vectors for system's
subsystems including m subsystems, respectively. Moreover, l shows the limit of the system resources.
The overspeed detection was continually offered by the mechanical and electrical systems. By occurring an overspeed, the fuel source must be stopped through control valves (V1 to Vm). Fig. 4 represents a gas turbine's overspeed protection system for RRAO optimizing the mixed- integer non-linear problem. The large-scale test structure involves 40


Table 5
Results obtained by optimization algorithms for dimension 100 over 25 independent runs.

Results obtained by optimization algorithms for dimension 500 over 25 independent runs.


Table 7
Results obtained by optimization algorithms for dimension D = 1000 over 25 independent runs.



















decision variables (m*2 = 40). The input factors and data for the large- scale test system are provided in Ref. [43] with 20 subsystems.
It is possible to formulate this reliability optimization problem as:
represents the upper volume limit of the products of the subsystem.
The system cost limitationg2(r, n):

m
Maximize f5(r, n)=	[1 — (1 — rd)nd ].
d=1

(11)
m
g2(r, n)=	C(rd) nd + e0.25nd  ≤ C,
d=1

0.5 ≤ rd ≤ 1 — 10—6 , 0 ≤ d ≤ m
(13)

	


The system constraints include:

straintg1(r,                        n):  1) The combined weight, volume, and redundancy allocation con-
m
2 2
d d

where, C shows the upper cost limit of the system, C(rd) is the cost for all element with reliability rd at dth stage, and T is the operating time in
which the components are working.
The system weight limitationg3(r, n):


where vd
d=1
shows the volume of dth subsystem for all components and V
m
g3(r, n)=	wdnde0.25nd ≤ W	(14)
d=1


Table 8
Average fitness values and standard deviations on CEC 2010 functions over 25 independent runs.



The proposed WGA algorithm and the other 5 algorithms are applied
adjusted to 5.00E+04 and a large enough population size is chosen for all in these two real-world problems. For comparative studies, FEsmax are algorithms. Table 9 presents the optimization results (mean and standard
deviation) of different algorithms executed in 30 runs for solving the two problems. The best results are shown in boldface, which indicate that WGA provides efficient and better performance compared to the other 5 advanced algorithms for real-world optimization problems.






Fig. 4. The diagram block for a gas turbine's overspeed protection system.


Table 9
Average fitness values and standard deviations on real-world optimization problems.


Conclusion

The proposed Wild Goose Algorithm (WGA) is a simple and effective algorithm that has been designed and proposed for optimization of high- dimensional problems. This algorithm, which is inspired by wild geese found in nature, includes ordered and coordinated group migration, reproduction and evolution of geese, and also death in the population of geese. To show the performance of the proposed WGA algorithm for optimization of high-dimension problems, it is tested and compared with sep-CMA-ES, CCPSO2, CSO, EPUS-PSO, MLCC, DECCD, DECC-DML,
CBCC2-DG, CBCC1-DG and DECC-DG algorithms based on the func- tions of CEC 2008 and CEC 2010. One of the advantages of WGA is that it has only one control parameter, Cr. It is experimentally shown that WGA has better competitive results with respect to other mentioned algo- rithms, and outperforms all other algorithms for most of the test func- tions. Furthermore, WGA is a simple and basic algorithm for large-scale optimization which can be used for various real-world optimization problems. In recent years, numerous studies have been carried out in the area of high-dimension optimization, the most of which focused on cooperative co-evolution technique. In future, WGA may be embedded into the frameworks of different CC methods with various categories in order to improve its performance. Furthermore, WGA can be used for solving other real-world large-scale optimization problems.

Credit author statement

Mojtaba Ghasemi: Conceptualization, Methodology, Software, Writing – original draft preparation. Abolfazl Rahimnejad: Data curation, Software, Writing – original draft preparation. Rasul Hemmati: Writing – original draft preparation, Visualization, Investigation. Ebrahim Akbari: Software, Validation. S. Andrew Gadsden: Writing- Reviewing and Editing.


Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper.
Acknowledgements

We would like to acknowledge funding and support from the Natural Sciences and Engineering Research Council (NSERC) Discovery Grant (Gadsden).

References

Mahdavi S, Shiri ME, Rahnamayan S. Metaheuristics in large-scale global continues optimization: a survey. Inf Sci 2015;295:407–28. https://doi.org/10.1016/ j.ins.2014.10.042.
Li X, Yao X. Cooperatively coevolving particle swarms for large scale optimization. IEEE Trans Evol Comput 2012;16:210–24. https://doi.org/10.1109/ tevc.2011.2112662.
MacNish C, Yao X. Direction matters in high-dimensional optimisation. In: IEEE congr evol comput (IEEE world congr comput intell 2008; 2008. https://doi.org/ 10.1109/cec.2008.4631115.
Ali MZ, Awad NH, Suganthan PN. Multi-population differential evolution with balanced ensemble of mutation strategies for large-scale global optimization. Appl Soft Comput 2015;33:304–27. https://doi.org/10.1016/j.asoc.2015.04.019.
Tang K, Y´ao X, Suganthan PN, MacNish C, Chen Y-P, Chen C-M, et al. Benchmark
functions for the CEC’2008 special session and competition on large scale global optimization. Nat Inspired Comput Appl Lab USTC, China 2007;24:1–18.
Tang K, Li X, Suganthan PN, Yang Z, Weise T. Benchmark functions for the CEC 2010 special session and competition on large-scale global optimization. Nat Inspired Comput Appl Lab USTC, China n.d 2010.
Yang Z, Tang K, Yao X. Multilevel cooperative coevolution for large scale optimization. In: IEEE congr evol comput (IEEE world congr comput intell 2008; 2008. https://doi.org/10.1109/cec.2008.4631014.
Hsieh S-T, Sun T-Y, Liu C-C, Tsai S-J. Solving large scale global optimization using improved Particle Swarm Optimizer. IEEE Congr Evol Comput. 2008. https:// doi.org/10.1109/cec.2008.4631030. IEEE World Congr Comput Intell 2008.
Ros R, Hansen N. A simple modification in CMA-ES achieving linear time and space complexity. Parallel Probl Solving from Nat – PPSN X 2008:296–305. https:// doi.org/10.1007/978-3-540-87700-4_30.
Weber M, Neri F, Tirronen V. Shuffle or update parallel differential evolution for large-scale optimization. Soft Comput 2010;15:2089–107. https://doi.org/ 10.1007/s00500-010-0640-9.
Chen W, Weise T, Yang Z, Tang K. Large-scale global optimization using cooperative coevolution with variable interaction learning. Parallel probl solving from nature. PPSN XI 2010. https://doi.org/10.1007/978-3-642-15871-1_31. 300–9.
Omidvar MN, Li X, Yao X. Cooperative Co-evolution with delta grouping for large scale non-separable function optimization. In: IEEE congr evol comput; 2010. https://doi.org/10.1109/cec.2010.5585979.
Wang H, Wu Z, Rahnamayan S. Enhanced opposition-based differential evolution for solving high-dimensional continuous optimization problems. Soft Comput 2010; 15:2127–40. https://doi.org/10.1007/s00500-010-0642-7.
Hedar A-R, Ali AF. Tabu search with multi-level neighborhood structures for high dimensional problems. Appl Intell 2011;37:189–206. https://doi.org/10.1007/ s10489-011-0321-0.
Chu W, Gao X, Sorooshian S. A new evolutionary search strategy for global optimization of high-dimensional problems. Inf Sci 2011;181:4909–27. https:// doi.org/10.1016/j.ins.2011.06.024.
Takahama T, Sakai S. Large scale optimization by differential evolution with landscape modality detection and a diversity archive. In: IEEE congr evol comput; 2012. https://doi.org/10.1109/cec.2012.6252911. 2012.
Wang C, Gao J-H. A differential evolution algorithm with cooperative coevolutionary selection operation for high-dimensional optimization. Opt Lett 2012;8:477–92. https://doi.org/10.1007/s11590-012-0592-3.
Chowdhury JG, Chowdhury A, Sur A. Large scale optimization based on Co- ordinated bacterial dynamics and opposite numbers. Swarm. Evol Memetic Comput 2012;770–7. https://doi.org/10.1007/978-3-642-35380-2_90.
Wang H, Rahnamayan S, Wu Z. Parallel differential evolution with self-adapting control parameters and generalized opposition-based learning for solving high- dimensional optimization problems. J Parallel Distr Comput 2013;73:62–73. https://doi.org/10.1016/j.jpdc.2012.02.019.
Wang Y, Huang J, Dong WS, Yan JC, Tian CH, Li M, et al. Two-stage based ensemble optimization framework for large-scale global optimization. Eur J Oper Res 2013; 228:308–20. https://doi.org/10.1016/j.ejor.2012.12.021.
Fan J, Wang J, Han M. Cooperative coevolution for large-scale optimization based on Kernel Fuzzy clustering and variable trust region methods. IEEE Trans Fuzzy Syst 2014;22:829–39. https://doi.org/10.1109/tfuzz.2013.2276863.
Omidvar MN, Li X, Mei Y, Yao X. Cooperative Co-evolution with differential grouping for large scale optimization. IEEE Trans Evol Comput 2014;18:378–93. https://doi.org/10.1109/tevc.2013.2281543.
Segura C, Coello Coello CA, Hern´andez-Díaz AG. Improving the vector generation
strategy of Differential Evolution for large-scale optimization. Inf Sci 2015;323: 106–29. https://doi.org/10.1016/j.ins.2015.06.029.
Cheng R, Jin Y. A competitive swarm optimizer for large scale optimization. IEEE Trans Cybern 2015;45:191–204. https://doi.org/10.1109/tcyb.2014.2322602.
Singh D, Agrawal S. Self organizing migrating algorithm with quadratic interpolation for solving large scale global optimization problems. Appl Soft Comput J 2016;38:1040–8. https://doi.org/10.1016/j.asoc.2015.09.033.



Sun Y, Wang X, Chen Y, Liu Z. A modified whale optimization algorithm for large- scale global optimization problems. Expert Syst Appl 2018;114:563–77. https:// doi.org/10.1016/j.eswa.2018.08.027.
Li J, Guo L, Li Y, Liu C. Enhancing elephant herding optimization with novel individual updating strategies for large-scale optimization problems. Mathematics 2019;7:395. https://doi.org/10.3390/math7050395.
Shadravan S, Naji HR, Bardsiri VK. The Sailfish Optimizer: a novel nature-inspired metaheuristic algorithm for solving constrained engineering optimization problems. Eng Appl Artif Intell 2019;80:20–34. https://doi.org/10.1016/ j.engappai.2019.01.001.
Samareh Moosavi SH, Bardsiri VK. Poor and rich optimization algorithm: a new human-based and multi populations algorithm. Eng Appl Artif Intell 2019;86: 165–81. https://doi.org/10.1016/j.engappai.2019.08.025.
Cai X, Zhang J, Liang H, Wang L, Wu Q. An ensemble bat algorithm for large-scale optimization. Int J Mach Learn Cybern 2019;10:3099–113. https://doi.org/ 10.1007/s13042-019-01002-8.
Faramarzi A, Heidarinejad M, Stephens B, Mirjalili S. Equilibrium optimizer: a novel optimization algorithm. Knowl Base Syst 2020;191:105190. https://doi.org/ 10.1016/j.knosys.2019.105190.
Salih SQ, Alsewari ARA. A new algorithm for normal and large-scale optimization problems: nomadic People Optimizer. Neural Comput Appl 2020;32:10359–86. https://doi.org/10.1007/s00521-019-04575-1.
Bas¸ E, Ülker E. Improved social spider algorithm for large scale optimization. Artif Intell Rev 2020:1–36. https://doi.org/10.1007/s10462-020-09931-5.
García-Martínez C, Lozano M, Herrera F, Molina D, S´anchez AM. Global and local
real-coded genetic algorithms based on parent-centric crossover operators. Eur J Oper Res 2008;185:1088–113. https://doi.org/10.1016/j.ejor.2006.06.043.
Qin AK, Huang VL, Suganthan PN. Differential evolution algorithm with strategy adaptation for global numerical optimization. IEEE Trans Evol Comput 2009;13: 398–417. https://doi.org/10.1109/tevc.2008.927706.
Wang Y, Cai Z, Zhang Q. Differential evolution with composite trial vector generation strategies and control parameters. IEEE Trans Evol Comput 2011;15: 55–66. https://doi.org/10.1109/tevc.2010.2087271.
Zambrano-Bigiarini M, Clerc M, Rojas R. Standard particle swarm optimisation 2011 at CEC-2013: a baseline for future PSO improvements. In: IEEE congr. Evol. Comput., IEEE; 2013; 2013. p. 2337–44. https://doi.org/10.1109/ CEC.2013.6557848.
Lynn N, Suganthan PN. Heterogeneous comprehensive learning particle swarm optimization with enhanced exploration and exploitation. Swarm Evol Comput 2015;24:11–24. https://doi.org/10.1016/j.swevo.2015.05.002.
Das S, Suganthan PN. Problem definitions and evaluation criteria for CEC 2011 competition on testing evolutionary algorithms on real world optimization problems. Jadavpur Univ Nanyang Technol Univ Kolkata; 2010. p. 341–59.
Chen T-C. IAs based approach for reliability redundancy allocation problems. Appl Math Comput 2006;182:1556–67. https://doi.org/10.1016/j.amc.2006.05.044.
Das S, Abraham A, Chakraborty UK, Konar A. Differential evolution using a neighborhood-based mutation operator. IEEE Trans Evol Comput 2009;13:526–53. https://doi.org/10.1109/tevc.2008.2009457.
Wang H, Rahnamayan S, Sun H, Omran MGH. Gaussian bare-bones differential evolution. IEEE Trans Cybern 2013;43:634–47. https://doi.org/10.1109/ tsmcb.2012.2213808.
Zhang H, Hu X, Shao X, Li Z, Wang Y. IPSO-based hybrid approaches for reliability- redundancy allocation problems. Sci China Technol Sci 2013;56:2854–64. https:// doi.org/10.1007/s11431-013-5372-5.
