Artificial Intelligence in Agriculture 6 (2022) 23–33











Transfer Learning for Multi-Crop Leaf Disease Image Classification using Convolutional Neural Network VGG
Ananda S. Paymode ⁎, Vandana B. Malode
MGM's Jawaharlal Nehru Engineering College, Aurangabad 431001, Maharashtra, India



a r t i c l e	i n f o


Article history:
Received 9 October 2021
Received in revised form 8 December 2021 Accepted 30 December 2021
Available online 7 January 2022


Keywords:
Convolutional Neural Network (CNN) Artificial Intelligence (AI)
Visual Geometry Group (VGG) Multi-Crops Leaf Disease (MCLD)
a b s t r a c t

In recent times, the use of artificial intelligence (AI) in agriculture has become the most important. The technol- ogy adoption in agriculture if creatively approached. Controlling on the diseased leaves during the growing stages of crops is a crucial step. The disease detection, classification, and analysis of diseased leaves at an early stage, as well as possible solutions, are always helpful in agricultural progress. The disease detection and classification of different crops, especially tomatoes and grapes, is a major emphasis of our proposed research. The important ob- jective is to forecast the sort of illness that would affect grapes and tomato leaves at an early stage. The Convolutional Neural Network (CNN) methods are used for detecting Multi-Crops Leaf Disease (MCLD). The fea- tures extraction of images using a deep learning-based model classified the sick and healthy leaves. The CNN based Visual Geometry Group (VGG) model is used for improved performance measures. The crops leaves images dataset is considered for training and testing the model. The performance measure parameters, i.e., accuracy, sen- sitivity, specificity precision, recall and F1-score were calculated and monitored. The main objective of research with the proposed model is to make on-going improvements in the performance. The designed model classifies disease-affected leaves with greater accuracy. In the experiment proposed research has achieved an accuracy of 98.40% of grapes and 95.71% of tomatoes. The proposed research directly supports increasing food production in agriculture.
© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).





Introduction

To contribute to the development of nations, knowledge of agricul- ture sectors is crucial. Agriculture is a one-of-a-kind source of wealth that develops farmers. For a strong country, the development of farming is a necessity and a need in the global market. The world’s population is growing at an exponential rate, necessitating massive food production in the next 50 years. Information about different types of crops and dis- eases occurring at each level and its analysis at an early stage play a key and dynamic role in the agriculture sector. A farmer's main problem is the occurrence of various diseases on their crops. The disease classifica- tion and analysis of illnesses is a crucial concern for agriculture's optimum food yield. Food safety is a huge issue due to a lack of infra- structure and technology, so crop disease classification and identi- fication are important to be considered in the coming days. This is necessary for yield estimation, food security, and disease management. Detection and recognition of crops illnesses is an important study topic because it could be capable of monitoring huge fields of crops and de- tecting disease symptoms as soon as they occur on plant leaves. As a

* Corresponding author.
E-mail addresses: anandpaymode@gmail.com (A.S. Paymode), vandanamalode@jnec.ac.in (V.B. Malode).
result, finding a quick, efficient, least inexpensive, and effective approach to determine crops diseases instances is quite important (C. J. Chen et al., 2021).
Artificial intelligence (AI) provides considerable assistance to agri- culture, which enhances a nation's gross domestic product (GDP) mostly through this sector. Climate change, labour scarcity, rainy season uncertainty, natural disasters, and various diseases on plant leaves are all major issues in agriculture. The plant leaves recognition and detec- tion studies with edge intelligence applied to agriculture. There is a new advancement with different deep learning models that overcomes the challenge. The YOLOv3 neural network model is based on deep learning and is built on an embedded system and the NVIDIA Jetson TX2. The system is implemented on a drone, and photographs of plants are taken, pest positions are identified, and pesticides are applied as needed; this is a novel approach based on deep learning (Al Hiary et al., 2011).
Hyper spectral and multispectral knowledge acquisition techniques and applications have exhibited their utility in improving agricultural production and practises by providing farmers and agricultural manage- ment with crucial data on the elements impacting crop condition and growth. This technology has been widely employed in a variety of agri- cultural applications, including sustainable agriculture (Ang, 2021).


https://doi.org/10.1016/j.aiia.2021.12.002
2589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/).



Weed detection in vegetable plantations is more difficult than in crop plantations due to uneven plant spacing. Deep Learning technology is a novel method that blends with image processing. This approach con- centrates solely on recognising plants, avoiding the handling of numer- ous plant species. Furthermore, by reducing the amount of training image collection and even the complexity of weed detection, this tech- nique can improve plant diagnosis accuracy and performance (Jin et al., 2021).
The massive crop loss occurred because of the failure to predict dis- ease at an early stage, which always results in lower crop production. As a result, identifying and analysing crop diseases is a critical step in en- suring crop quality (Wu, 2020). As high computing speed and power have recently improved, the availability of massive datasets improves the system's efficiency.
In this section, there are various techniques for detecting and classi- fying crop leaf disease. We present the related survey as a system that employs a variety of classifier techniques. There are two types of combi- nations: serial and hybrid, with the combination of serial and parallel achieving the significant performance parameter within 600 images (Massi et al., 2020). The hybrid combination has a recognition rate of 91.11%, which is higher than the serial, parallel, and deep learning ap- proaches. For identifying and analyzing leaf illness, a deep learning convolutional neural network (CNN) model was used to classify healthy and sick images. The model train contained 25 different plants, 58 clas- ses’ sets, including healthy and diseased plants, and had 87,848 images. Using several (Ferentinos, 2018) model architectures, the best perfor- mance success rate was 97.53%. The Multi-Context Fusion Network (MCFN), a deep learning-based method, is built and prepared for crop disease detection. The MCFN aids in the extraction of visual information from 50,000 crop photos. The MCFN produced 77 common crops in- fected using a deep fusion model, with a 97.50% identification accuracy (Jin et al., 2021).
The identification of weeds in crops using the CovNet algorithm is also a potent and cutting-edge approach. In recent research, bounding boxes were drawn across cropped images and the model was trained. Colour-based segmentations are applied to images and colour informa- tion, and visual categorization is calculated for weed images. The colour index was examined with a genetic algorithm and Bayesian categoriza- tion (Jin et al., 2021). The deep residual network and the deep dense network are combined in the hybrid deep learning model. The hybrid deep learning model reduces training parameters while increasing ac- curacy by up to 95.00 % (Zhou et al., 2021).
Deep transfer learning is an amazing performance methodology for identifying plant diseases. For pre-trained datasets, Inception and ImageNet modules were utilized (Chen et al., 2020). The perfor- mance of pepper, vegetable, potato, and tomato leaf images in the plantvillage database was studied and enhanced using support vec- tor machine (SVM) and multi-layer perceptron. After training the model the system achieves a higher performance accuracy of 94.35
% (Kurmi et al., 2020).
To detect and recognize corn dietary sickness, a Deep Convolutional Neural Network was deployed., The recognition of corn leaf diseased ac- curacy was 88.46 %, and the usage of hardware, such as a raspberry pi3 with an Intel Movidius Neural Compute Stick and a system GPU that pre-trained the CNN Model, resulted in superior metric accuracy perfor- mance (Sun et al., 2020).
With the rapid growth of artificial intelligence and deep learn- ing technology, computer vision (CV) made a breakthrough. The CV-based approaches are commonly utilized for diagnosing grape leaf diseases. The principle component analysis (PCA) and back propagation methods aid in the diagnosis of grape diseases such as downy mildew and powdery mildew, with a research accuracy of 94.29 % (Xie et al., 2020), using VGGNet. The weights are initial- ized using ImageNet pre-trained datasets, and over through the real - world dataset, such approaches had a validation accuracy of
91.83 %.
Material & methods

Datasets

To support our research in the area of collection of images available from Pennsylvania state university named plantvillage dataset. The dataset plant-village included 152 crop solutions, 38 crop classes, and 19 crop categories, for 54,303 crop leaves images. In the datasets, high quality JPEG image format with 5471width and 3648 height pixels are available. In the pre-processing, de-nosing, segmentation and after images are 256 X 256 pixels (Gandhi et al., 2018). The plantvillage is a well-known dataset for crop disease, with a large number of public datasets available. A plantvillage dataset images were captured in the lab, thus they are used as training datasets. Our model tested on real field captured images, As a result, we must concentrate on developing our own field database. The test images were captured with a separate Megapixel camera and stored in a database. The datasets prepared in the field are available and be used in the proposed research. The agro- deep mobile application was used to capture some on-field crops images. The field photographs were taken with the redmi Note 5 Pro MIUI Global 11.0.5.0(PRIMEXM), Android Version PKQ1.180904.001, and a camera frame 4:3 high picture quality on 16 MP+5MP with f/2.2 aper- ture pixel, in a variety of natural environments. The disease-affected and healthy photos are the most common image categories collected for re- search purposes. Healthy spot contaminated, mosaic virus, yellow leaf curl virus, septoria leaf spot bacterial spot, early blight, late blight, leaf mould, septoria leaf spot, and spider mites are examples of tomato
imagery.

Proposed research

The schematic in Fig 1 depicts a potential view for multi-crop leaf disease classification and analysis. Initially, plant leaf disease images are collected and classified into several categories. Picture filtering, grey transformation, picture sharpening, and scaling are some of the image-processing techniques. By using data augmentation methods, new sample photos are created from available photos to enhance and prepare the dataset. Augmentation procedures like turning, translation, and randomized transformation are employed to enhance the size of the dataset. The photos are then used as input to the suggested approach for training the model in the following stage. The newly trained architec- tural model is used to anticipate previously unseen images. Eventually, the findings of plant disease detection and identification are achieved. Finally, complete details of these steps are depicted in later parts (Table 1).

Sample images category

The sample images of crops shown in Fig. 2 depict the category of field’s tomato leaf images of various disease and healthy classes. The im- ages are one-of-a-kind for each type of disease symptom, pattern, spot, and colour mark. Specific tomato plant leaf diseases such as bacterial wilt, leaf mold, and grey spots are identified and detected as disease im- pacted recognition traits (Paymode et al., 2020).
Fig. 3 depicts field images of grape vine leaves obtained in the Nashik district of Maharashtra, India. A grape category, Healthy 423, Black Rot 1180, Black Measles 1383 and Leaf Blight 1076 images were recorded, recognized, and captured. The datasets for grape plant leaves were gen- erated by adjusting the brightness and hue of images from the A to D category (See Figs. 4-5).
The second crop of tomatoes sampled Early blight 1000, Mosaic virus 373, Bacterial spot 2127, Late blight 1909, Leaf mould 952, Septoria leaf spot 1771, 1404 spot, spider mites 1676 and Yellow leaf curl 3209. The deep learning based methods are state-of-the-art in computer vision, which is used in image recognition and classification. In general, dataset col- lecting, data pre-processing, image segmentation, feature extraction, and classification are the four stages of Artificial Intelligence (AI) in agriculture




Fig 1. Proposed research system flow diagram.


approaches for crop leaf disease detection and classification utilising Convolutional Neural Network (CNN). A Google Colaboratory platform was used to pre-process the image, extraction of features, and classify it.

Image augmentation

The large number of datasets improves the learning algorithms' perfor- mance and prevents overfitting. Obtaining a real-time dataset for use as input to a training model is a complex and time-consuming operation. As
a result, data augmentation broadens the range of training data available to deep learning models. Deep learning-based augmentation approaches include image flipping, cropping, rotation, colour transformation, PCA colour augmentation, noise rejection, Generative Adversarial Networks (GANs), and Neural Style Transfer (NST) (Arun Pandian et al., 2019). The Faster DR-IACNN approach for detecting grape leaf diseases is based on deep learning. The automatic extraction of spots on leaves has a high de- tection speed and accuracy. There are 4449 original photographs and 62,286 photos developed using data augmentation techniques.




Table 1
A study of deep learning techniques with classification and recognition rate (See Fig. 12).

Approach	Classification	Model	Recognition rate (%)
Hybrid Combination (Massi et al., 2020)	Three SVM	SVM	91.11
Deep Learning (Ferentinos, 2018)	CNN	VGG	97.53
Multi-Context Fusion Network (MCFN) (Wu, 2020)	CNN	AlexNet &VGG16	97.50
Deep Transfer Learning (DTL) (Chen et al., 2020)	CNN	VGG	91.83
Machine Learning (Kurmi et al., 2020)	SVM	MLP	94.35
Deep Learning (Sun et al., 2020)	DCNN	DCNN	88.46
Deep Learning (Xie et al., 2020)	Faster DR-IACNN	Inception-v1 ResNet-v2	81.11




Fig 2. Sample tomato leaf images (A: Mosaic Virus, B: Healthy, C: Target Spot, D: Late Blight, E: Bacterial Spot, F: Septoria Spot, G: Spider Mite, H: Leaf Mold, I: Early Blight, J: Yellow Leaf.


The images are converted into a vector of fixed features through fea- ture extraction in segmentation. The color, texture, and shape are the system-adopted features. A means, confidence intervals, and sleekness have been employed as colored methods, with HSV and RGB color spaces being retrieved. The gray-level co-occurrence matrix is preferred when extracting texture features from a colour image. This approach is used to identify plant diseases.

Transfer learning

The model's optimization and training is a tough and time- consuming operation. A powerful graphical processing unit (GPU) is required for the training, as well as millions of training examples. However, transfer learning, which is employed in deep learning, solves all of the problems. The pre-trained Convolutional Neural Network (CNN) used in transfer learning is optimized for one task and transfers knowledge to different modes (Nevavuori et al., 2019). The multi-crop image dataset model comprises a size of 224 X 224. The residual network (ResNet) needed to be tweaked. In all ResNet models, the final layer before the softmax is a 7 X 7 average-pooling layer. A smaller image can fit into the network when the pooling size decreases. The basic picture preparation is necessary for the transfer learning considerations with the multi- cropped image dataset.

Results & discussion

Convolutional neural network

The convolutional layers, pooling layers, fully-connected layers, and dense layers constitute the architecture of the Convolutional Neural Network (CNN) (See Fig. 6). The layers' description is shown below.
Convolutional layer
Convolutional layers' fundamental function is to extract unique fea- tures from images. The implementation of convolutional layers on a normal basis facilitates the extraction of input features (Chen et al., 2020), The features extraction (Hi) among several layers in CNN is com- puted using the formula below.

Hi = φ (Hi−1 Wi + bi)	(1)


Where, Hi - Feature map, Wi –Weight, bi is offset and φ – Rectified Lin- ear Unit (RELU)

Pooling layers
The pooling layers are a crucial component of a Convolutional Neural Network (CNN). It shrinks the size of convolved features in dimension while simultaneously minimizing the computational resources neces- sary for image processing. Pooling arise categorized into two types: max pooling and average pooling. Max pooling returns the maximum value of images, whereas an average pooling returns the average value of the image section.

Drop-out layers
The dropout layers improve the capability of a trained model. It pro- vides regularization and prevents the model from over-fitting by de- creasing the correlation between the neurons. The drop out process is used in all the activation functions but it is scaled by factor (Liu, 2020).

Flatten layers
It collapses the spatial dimensions of the mapped pooled features while retaining the channel dimensions. The flattened layer adds extra dimensions and after it is transformed into a vector. The vectored feed




Fig 3. Sample grapes plant leaf images. (A: Grape Black Rot, B: Grape Esca (Black Measles), C: Healthy, D Grape Leaf blight (Isariopsis Leaf Spot).




Fig 4. Multi-crops image augmentation (a) (A: Original B: Rotate, C: Color, D: Image Point, E: Hstack, F: Size G: Gaussian Noise, H: Shape).


to fully connected layers also known as the dense layer or fully con- nected layers.


Fully-connected layers
which is introduced in the full vectors using rectified linear unit (RELU) activation. The versatility of class separation is greater when employing a support vector machine (SVM). The essentials of SVM are as described in the following:

n	N

Fully connected layers are needed for extracted images classification
Minimize 1/2 ∑ W2 + C ∑ ξ j	(2)

features because of their special purpose. The softmax function predicts earlier extracted image attributes from preceding layers. Softmax is a
1
j=1
j=1

multiclass classification activation function in the output layers. The neural network layer uses a multilayer perceptron model (MLP) as a classifier for two-class classification. The model with nonlinearity,
Where C is the tuning measure, subject to the constraint yj (W∙X + b) ≥ 1 – ζ, j = 1, 2, 3… N. The softmax parameter γ = 1 and C = 1 are used throughout training and test sets of the classification algorithm.




Fig 5. A B: H Stack, C: Original, D: Augmentation, E: Batch H stack, F: Adaptive Gaussians Noise.




Fig 6. Proposed convolutional neural network CNN architecture.


The ConvNet architecture design's main component is its depth. By defining additional design parameters and growing the network depth continuously, by adding more convolutional layers that are doable by using extremely small (3 x 3) convolution filters in all layers. As a result, they've developed substantially more accurate ConvNet architectures that not only reach state-of-the-art accuracy on fixed dataset classifica- tion and localisation tasks, but are also applicable to other image recog- nition datasets, where they perform admirably even when utilised as part of relatively simple pipelines(Simonyan and Zisserman, 2015).Our ConvNets are fed a fixed-size 224 x 224 RGB picture during training. The only pre-processing we perform is removing each pixel from the mean RGB value determined on the training set. We apply filters with a very small receptive field 3 x 3 to send the image through a stack of convolutional layers. We also use 1 x 1 convolution filters in one of the configurations, which are a linear change of the input channels (followed by non-linearity). The convolution stride is set to 1 pixel, and the spatial padding of the convolutional layer input is set to 1 pixel for 3 conv. layers so that the spatial resolution is kept after convolutional. Five max- pooling layers, which follow part of the convolutional layers, do spatial pooling (not all the convolutional layers are followed by max pooling) Max-pooling is done with stride 2 over a 2 x 2 pixel window.

VGG16

The Convolutional Neural Network based VGG16 pre-trained
The VGG model improved with large kernel-sized filters, with 11 and 5 convolutional layers with a 3 x 3-kernel filter size. The input image size is fixed at 224 x 224. Following image pre-processing, images were passed through a convolutional layer with a filter size of (3 x 3).For linear transformation of the input channel, the filter size is set to (1 x 1). The stride size is fixed to 1 and max pooling is performed with 2 x 2 sizes and stride set to 2. In the next steps, fully connected layers have the same configuration with 4096 channels in each layer. The final layer is the softmax activation layers, followed by the RELU activation functions (See Fig. 7).

Performance measure

The F1 score, accuracy matrix, and Receiver operating characteristic (ROC), as well as the area under the curve (AUC), are being used to eval- uate segmentation performance (AUC). The performance of the classi- fier is measured using evaluation metrics.

Accuracy metrics
The model performance for all classes is accurately measured. The accuracy is calculated by adding the total number of correct predictions to the total number of predictions. The performance parameter calcula- tion of precision and recall and F1-Score are measured. The accuracy is expressed in terms as follows.

models are used to improve the performance and classify the crop images as healthy and disease images. For quality detection and analysis of crop leaf images, the initial model transfers information from pre-trained VGG16 models. The Convolutional Neural Network
AC =
(TP + TN) (TP + FP + FN + TN)
(3)

(CNN) model retained new images of the field and learned to per- form a model for disease detection and classification (Alencastre- Miranda et al., 2021).
Where, TP is True Positive, TN True Negative, FN False Negative and FP False Positive Samples. The classifier performance measure using evaluation metrics are gives as;




Fig 7. Proposed convolutional neural network (CNN) VGG16 architecture.






Fig 8. Receiver operating characteristics FP versus TP





Table 2
Parameter setting for trained the model


Hyperparameter	Value Setting


Crops	Grapes & Tomatoes
Convolutional Layers	13
Max Pooling Layer	5
Dropout Rate	015/0.25/0.50
Activation Function	Relu, Softmax
Epochs	20/25/30/40/45
Learning Rate	0.00001/0.0001
Image Size	224 x 224 x 3



Receiver operating characteristic
The receiver operating characteristic (ROC) curve is used to understand deterministic indications of categorization sorting as well as computational modeling challenges. The curve is a graph that shows the ratio of false pos- itives to true positives under different standard limits (See Fig. 8).
A prototype also with largest true negative rate values was used to cor- rectly categorize defectives, and the model with the highest true positive rate values was used to correctly classify healthily. To boost productivity by reducing processing time for training and testing, the MCC (Matthews Correlation Coefficient) is employed for the total computation. MCC is a criterion for categorizing complex data into distinct categories. MCC is a su- perior method to accuracy which only has significant importance if the true positives, true negatives, false negatives, and false positives outcomes are all positive. The MCC ranges from 1 (poorest judgment) to 1 (perfect predictions), with an MCC of 0 suggesting a random guess.
The model is tuned by the number of epochs, hidden layers, hidden nodes, activation functions, dropout, learning rates, and batch size. The model performance is affected by hyper parameter tuning. The term "hyper parameter tuning" refers to the process of repeatedly adjusting hid- den layers, epochs, activation function, or learning rate. The model is fine- tuned to achieve the best accuracy while minimising the average loss.
The experimental analysis was carried out on Google research prod- ucts on Google Colaboratory. The Colaboratory platform supports python programming, and nearly all of the Python libraries are uploaded and installed for research purposes. The Python 3 Google Compute Engine backend (GPU) with RAM of 12.72 GB and disc space of 68.40 GB is avail- able while experimenting. The dataset is uploaded with the drive mounted, and the model is trained on the Google platform with high


TPR = (Sensitivity) =

FPR =	FP
(FP + TN)
TP


(TP + FN)

, TNR = (Specificity)
TN


(TN + FP)

(4)


(5)
configurations. A Python convert to image function is used for converting all the images to an array and fetching images from the directory.
The processed images come from a directory, and all label images are transformed using the label binarized sklearn python package. The total number of images is divided into train and test using train-test-split py- thon functions. The model parameters were set as shown in Table 2, and

Where, TPR is True Positive Rate, TNR True Negative Rate, and FPR False Positive Rate.
the model was trained to calculate all trainable and non-trainable pa- rameters. The Adam optimization algorithm is used to train the deep learning convolutional neural network model. The algorithm optimized

Precision =	TP
(TP + FP)
, Recall =	TP
(TN + FN)
(6)
the sparse gradient noise issue.
The input network uses 224 X 224 images, and the batch size is 30 for grapes and 25 for tomatoes, respectively, and the same test is per-

G−Mean
m
Recall
  1
7	formed for different epochs with batch size and learning rate. In every

=	∏
K=1
K	m	( )
polling layer with a 2 x 2-pool size and the RELU function utilized in the network, the model performs a max-pooling operation. The output

Here m represents the number of categories and G denotes the TNR and FPR accuracy ratio.
Mean average precision (mAP), which consists of Precision, Recall, and Mean, is the algorithm assessment standard employed. Image pro- cessing and detection rely heavily on the mAP. From the entire results, the accuracy has classified correctly. From the complete findings, the re- call is correctly classified.
The F1 score is another important metric for evaluating the algo- rithm. It's precision and recall fundamental that's presented as follows:
of the last layer is a oftmax-activation multi-crop-developed prediction. During the network's training phase, hyper parameters such as learning rate and epoch size were adjusted. The average accuracy achieved was 98.40% for grapes and 95.71% for tomatoes, respectively. The learning rate is tested at different values to optimize targeted performance mea- sured. The validation process is based on a total number of images from the multi-crop dataset. With the setting of different epochs and batch size, the accuracy improved and grew.
The crops-leaf images datasets are used to train the model and iden- tification of class and category of disease with transfer learning tech-

F1 Score = 2 × Precision × Recall
(Precision + Recall)
(8)
niques including VGG16. The original datasets are divided into training data 80%, validation data 10% and testing data 10%.




Table 3
Experimental results of the grape model for setting different parameters.



Table 4
Experimental results of the tomatoes model for setting different parameters.



Training and validation accuracy

Training and validation accuracy is measured by setting differ- ent values while training the model. The experiments were carried out at Google Colaboratory on the available RAM of 12.50 GB. While performing the experiment, different values are set for the follow- ing: the number of epochs, learning rate, dropout rate, and the number of images noted as training loss, training accuracy, valida- tion loss, and validation accuracy. A model's performance is mea- sured and verified on the grape and tomato crops' leaves. Table 3 and Table 4 show the details of the results of experiments carried on grapes and tomatoes, respectively.
Figures and graphs

A model's performance is measured and verified with training, test- ing, and validation methods for grapes and tomatoes leaves. Fig 9 and Fig. 10 show the training and validation accuracy and loss of the grape leaves and tomatoes, respectively.
The confusion matrix has been used to measure the performance pa- rameter for grapes and tomatoes leaves, as shown in Fig. 11. Experiment with the facts collected. The suggested approach is tested using our grapes and tomatoes image datasets, which were taken in a real-field with various backdrop and light intensities, similar to the tests done in Section 4.4.





Fig 9. Training and validation. (a) Accuracy and (b) loss of VGG16 grapes.



Fig 10. Training and validation. (a) Accuracy and (b) Loss of VGG16 tomatoes.




Fig 11. Confusion matrix. (a) Tomatoes and (b) grapes.






























Fig 12. Comparison between different model vs accuracy in percentage(%) with proposed VGG16 grapes.


To assure the diversity of sample images and avoid the over fitting problem, data augmentation techniques such as random rotation, flip- ping, and scale transform, as well as associated pre-processing activities, are used to extend the training samples. The processes are described in more detail below.
Image resize: The total images scaled into size of 224 x 224 pixels, for the model fit and minimum 200 images taken from each healthy and unhealthy category are augmented with data augmentation methods.
Image pre-processing: Image pre-processing is used to darken the different lengths of the image data, going to bring them into ratio


































Fig 13. Comparison between different model vs accuracy in percentage (%) with proposed VGG16 tomatoes.



and retaining the initial images' knowledge formation while attempting to prevent image deformation.
The dataset partition and training. In this section a selection of ran- dom sample images for proposed experiments and calculated with carried out the result as per Section 4.4.
Validation and testing. The testing is done on the images that were used to evaluate the model, and new images from outside modeling are used to check the model effectiveness. The output results are compared to the real categories, the effectiveness of the control that goes with them is computed.

The residual block collection and DesnseNet used in task of tomato leaf disease identification with RDN restructured model. After input image normalizing and adding the convolutional layer residual modules dense layer classify the tomato disease images with 95% accuracy dis- ease dataset (Zhou et al., 2021). The public data set of the AI Challenger Competition in 2018 used the Inception-ResNet-v2 model using the RELU activation function, with an accuracy of 86.1%. (Ai et al., 2020), under complex background conditions, the accuracy of VGG Net is
91.83 %. One more approach to INC-VGGN rice disease detection with an average accuracy of 80.38% for both "Phaeo- sphaeria Spot" and "Maize Eyespot" diseases (J. Chen et al., 2020) (See Fig. 13).

4. Conclusion

In this paper, there are two types of crop disease leaves were collected and prepared as a dataset with available data. The techniques of data aug- mentation, dataset pre-processing, training, and testing are applied to the convolutional neural network-based VGG16 model. The proposed model is built and tested to improve the performance measured and compared. The evaluation metrics parameters are higher and increased as compared to other available datasets and methods. Therefore, our proposed re- search work increased accuracy for grapes by 98.40% and for tomatoes by 95.71%. Always improving the performance of on-field crops, leaf im- ages and diseases classification and analysis is a critical step, but with our model achieved the highest performance, which supported agricultural development. The major focus of research is to provide advancement in the agriculture sector and an increase in food production. The collection and preparation of genuine datasets and applying to the deep learning models with multiple crops leaves images is a future target. In the future, the use of Inception V3 and ResNet-based CNN models for much deeper analysis of crop images is anticipated. Our work encourages and stimu- lates farmers, which ultimately raises farm income and helps to build up powerful countries.

Acknowledgements

Farmers from Nashik and Aurangabad, Maharashtra [India], contributed to the collection of real field crop images for research purposes. We would like to thank you Dr. Panjabrao Deshmukh Krushi Vidyapeet (Dr. PDKV), Akola, Maharashtra [India], for their encouragement and assistance.

Declaration of competing interest

The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influ- ence the work reported in this paper.

Credit author statement

We the undersigned declare that this manuscript is original, has not been published before and is not currently being considered for publica- tion elsewhere.
We would like to draw the attention of the Editor to the following publications of one or more of us that refer to aspects of the manuscript
presently being submitted. Where relevant copies of such publications are attached
We wish to draw the attention of the Editor to the following facts which may be considered as potential conflicts of interest and to signif- icant financial contributions to this work. [OR]
We wish to confirm that there are no known conflicts of interest as- sociated with this publication and there has been no significant financial support for this work that could have influenced its outcome.
We confirm that the manuscript has been read and approved by all named authors and that there are no other persons who satisfied the criteria for authorship but are not listed. We further confirm that the order of authors listed in the manuscript has been approved by all of us.
We confirm that we have given due consideration to the protection of intellectual property associated with this work and that there are no impediments to publication, including the timing of publication, with respect to intellectual property. In so doing we confirm that we have followed the regulations of our institutions concerning intellectual property.
We further confirm that any aspect of the work covered in this man- uscript that has involved either experimental human patients has been conducted with the ethical approval of all relevant bodies and that such approvals are acknowledged within the manuscript.
We understand that the Corresponding Author is the sole contact for the Editorial process (including Editorial Manager and direct communications with the office). He/she is responsible for commu- nicating with the other authors about progress, submissions of revi- sions and final approval of proofs. We confirm that we have provided a current, correct email address which is accessible by the Corre- sponding Author and which has been configured to accept email from biomaterials@elsevier.com.

References

Ai, Y., Sun, C., Tie, J., Cai, X., 2020. Research on recognition model of crop diseases and insect pests based on deep learning in harsh environments. IEEE Access 8, 171686–171693. https://doi.org/10.1109/access.2020.3025325.
Alencastre-Miranda, M., Johnson, R.M., Krebs, H.I., 2021. Convolutional neural networks and transfer learning for quality inspection of different sugarcane varieties. IEEE Trans. Indust. Inform. 17 (2), 787–794. https://doi.org/10.1109/TII.2020.2992229.
Ang, K.L.M., Seng, J.K.P., 2021. Big data and machine learning with hyperspectral informa- tion in agriculture. IEEE Access 9, 36699–36718. https://doi.org/10.1109/ACCESS. 2021.3051196.
Arun Pandian, J., Geetharamani, G., Annette, B., 2019. Data augmentation on plant leaf dis- ease image dataset using image manipulation and deep learning techniques. Pro- ceedings of the 2019 IEEE 9th International Conference on Advanced Computing, IACC 2019, pp. 199–204 https://doi.org/10.1109/IACC48062.2019.8971580.
Chen, J., Chen, J., Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020. Using deep transfer learning for image-based plant disease identification. Comput. Electr. Agricult. 173 (November 2019) 105393. https://doi.org/10.1016/j.compag.2020.105393.
Chen, C.J., Huang, Y.Y., Li, Y.S., Chen, Y.C., Chang, C.Y., Huang, Y.M., 2021. Identification of fruit tree pests with deep learning on embedded drone to achieve accurate pesticide spraying. IEEE Access 9, 21986–21997. https://doi.org/10.1109/ACCESS.2021.
3056082.
Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis. Comput. Elect. Agric. 145 (September 2017), 311–318. https://doi.org/10.1016/j. compag.2018.01.009.
Gandhi, R., Nimbalkar, S., Yelamanchili, N., Ponkshe, S., 2018. Plant disease detection using CNNs and GANs as an augmentative approach. 2018 IEEE International Conference on Innovative Research and Development, ICIRD 2018, no. May: 1–5 https://doi.org/10. 1109/ICIRD.2018.8376321.
Hiary, H., Al, S.B., Ahmad, M., Reyalat, M. Braik, ALRahamneh, Z., 2011. Fast and accurate detection and classification of plant diseases. Int. J. Comput. Appl. 17 (1), 31–38. https://doi.org/10.5120/2183-2754.
Jin, X., Che, J., Chen, Y., 2021. Weed identification using deep learning and image process- ing in vegetable plantation. IEEE Access 9, 10940–10950. https://doi.org/10.1109/AC- CESS.2021.3050296.
Kurmi, Y., Gangwar, S., Agrawal, D., Kumar, S., Srivastava, H.S., 2020. Leaf image analysis- based crop diseases classification. Signal Image Video Process. https://doi.org/10. 1007/s11760-020-01780-7.
Liu, S.Y., 2020. Artificial intelligence (AI) in agriculture. IT Profes. 22 (3), 14–15. https:// doi.org/10.1109/MITP.2020.2986121.
Massi, I.E., Mostafa, Y.E.-s., Yassa, E., Mammass, D., 2020. Combination of multiple classi- fiers for automatic recognition of diseases and damages on plant leaves. Signal Image Video Process. https://doi.org/10.1007/s11760-020-01797-y.



Nevavuori, P., Narra, N., Lipping, T., 2019. Crop yield prediction with deep convolutional neural networks. Comput. Elect. Agric. 163 (June) 104859. https://doi.org/10.1016/j. compag.2019.104859.
Paymode, A.S., Malode, V.B., Shinde, U.B., 2020. Artificial intelligence in agriculture for leaf disease detection and prediction: a review 13 (4), 3565–3573.
Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale image recognition. 3rd International Conference on Learning Representations, ICLR 2015 - Conference Track Proceedings . http://www.robots.ox.ac.uk/.
Sun, J., Yang, Y., He, X., Xiaohong, W., 2020. Northern maize leaf blight detection under complex field environment based on deep learning. IEEE Access 8, 33679–33688. https://doi.org/10.1109/ACCESS.2020.2973658.
Wu, W., Yang, T.L., Li, R., Chen, C., Liu, T., Zhou, K., Sun, C.M., Li, C.Y., Zhu, X.K., Guo, W.S.,
2020. Detection and enumeration of wheat grains based on a deep learning method under various scenarios and scales. J. Integr. Agric. 19 (8). https://doi.org/10.1016/ S2095-3119(19)62803-0.
Xie, X., Ma, Y., Liu, B., He, J., Li, S., Wang, H., 2020. A deep-learning-based real-time detec- tor for grape leaf diseases using improved convolutional neural networks. Front. Plant Sci. 11 (June) 1–14. https://doi.org/10.3389/fpls.2020.00751.
Zhou, C., Zhou, S., Xing, J., Song, J., 2021. Tomato leaf disease identification by restructured deep residual dense network. IEEE Access 9, 28822–28831. https://doi.org/10.1109/ ACCESS.2021.3058947.
