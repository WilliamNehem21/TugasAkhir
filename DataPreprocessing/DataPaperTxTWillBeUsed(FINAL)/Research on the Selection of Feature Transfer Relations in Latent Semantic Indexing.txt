Available online at www.sciencedirect.com



AASRI Procedia 3 (2012) 680 – 685




2012 AASRI Conference on Modeling, Identification and Control

Research On the Selection of Feature Transfer Relations in Latent Semantic Indexing
Dongyang Jianga,* , Wei Zhengb
aInformation Engineering Department, Liaoning Jidian Polytechnic,Dandong,118009, China
b Alibaba Group,Hangzhou, 310000,China




Abstract

The latent semantic index (LSI) has been widely used in many fields of natural language processing in which co- occurrence features can be captured by the transfer relations between the documents and in the documents. Document features with a higher frequency in the collection of the document are more likely to introduce some unreasonable feature transfer relations to the latent semantic space which affects the similarity between features and between documents in document sets in our recent study. In the paper a feature optimize technology in latent semantic indexing that uses feature transfer relation in documents and between documents is proposed. By the complete-link algorithm, the experimental results show that the method effectively improves the performance of latent semantic indexing.
Keywords: Latent semantic index, feature transfer, feature similar matrix;


Introduction
With the development of information technology, a lot of document resources are needed that helps the discovery of the theme, information retrieval, and so on. Therefore, text clustering technology came into being. It is a very important part of natural language processing. Text clustering technique made great success in document clustering. There are a large number of synonyms, near-synonym and other unique natural language phenomena in document clustering. We will use LSI to explore and resolve these linguistic phenomena to improve the performance of document clustering in the paper.
The feature transfer relations

* e-mail: linda164@163.com







2212-6716 © 2012 The Authors. Published by Elsevier B.V. Open access under CC BY-NC-ND license. Selection and/or peer review under responsibility of American Applied Science Research Institute doi:10.1016/j.aasri.2012.11.108


The co-occurrence information of the terms can be captured when Singular Value Decomposition (SVD) is decomposed proposed by [1]. The example preferred in [2] is as shown by table 1 and table 2 is the feature document matrix. The term weight represents word frequency of the term and the matrix is decompose by singular value dropping to a two-dimensional space. By the comparison between the similarity matrix decomposed shown by Table 3 and the one undecomposed, the similarity weights of the table 3 made obvious changes. In table 2 the similarity between the terms is zero, but there does not exist a value of 0 which means co-occurrence information of some terms is improved and some one is weakened. The similarity of 0 is between some terms undecomposed that means there is no or little relations. The undecomposed similarity of the user (t4) and human (t1) is 0 but decomposed similarity is 1.0003. By the changes of the similarity value, we can think "user" and "interface", "interface" and "human", "user" and "human" co-occur. In LSI "user" and "human" projected to the same dimension space.
Table 1. The technology Memorandum titles

number	article title

c1	Human machine interface for Lab ABC computer applications c2	A survey of user opinion of computer system response time c3	The EPS user interface management system
c4	System and human system engineering testing of EPS
c5	Relation of user-perceived response time to error measurement m1	The generation of random, binary, unordered trees
m2	The intersection graph of paths in trees
m3	Graph minors IV: Widths of trees and well-quasi-ordering m4	Graph minors: A survey


Table 2. Deerwester feature document matrix


The degree of similarity between features reflects the correlation of between the terms. The weight value not only reflects the correlation between the features but also embodies the co-occurrence information between the features in SVD space. As can be seen from Table 3, the similarity value of "time (t7)" and "graph(t11)" is 0.4988. These terms are from different classes and the variation of the terms can be considered as the co-occurrence of "time" and "user". Assuming in these nine articles, one common feature is in each document. In this semantic space generated by the document collection, for the mutual transmission between the terms, some feature co-occurrence information that not exists appear in the document so that some non- existent feature co-occurrence information which is noise data will generate between the documents. Fox example,a term X is added to the each document of Table 1 whose feature weight value is 1. Feature


document matrix in Table 1 are decompose by SVD and the similarity values between the features are gotten by the Equation (1) and whose similarity matrix is shown by Table 4. From the Table 4, the weight value of "compute(t3)", "response(t6)" and "time(t7)" are all 0.6925, compared with the corresponding ones in Table 4 that the weight value is weakened. As seen from Table 1, the common feature X added makes the weight value of these words weakened that should be very near.

T	 (T S  D T ) T T S  D T
D  S  T T T T S  D T K T
D  S  ( D  S  ) T
(1)


Table 3. Deerwester feature similarity matrix truncated to two-dimension


Table 4. Feature similarity matrix truncated to two-dimension with the features


The similarity degree between the documents mainly depends on the number of the co-occurrence features. In the generating latent semantic space, because of the transitivity between the features, the latent relations will be excavated. There is perhaps high similarity between the documents whose similarity are little or non. The similarity between the documents is calculated by Equation (1) and Table 6 is the similarity matrix after being adding the feature X. As seen from the data of the matrix, a clear distinction exists between the documents. The same type of documents has a higher similarity and different type of documents has a lower similarity. For example, there is a high similarity between "M4" and "c2" whose value is 1.1213. As these documents all have the term "survey", the "survey" weight value of co-occurrence is strengthened.



Table 5. similarity matrix between the documents


Table 6. Similarity matrix after being adding feature X

Experiments

corpus

The corpus Tancorpv 1.0 used in the experiment is from the Chinese Academy of Sciences , Dr Tan Songbo and the text classification corpus from Sogou Lab. 12 classes are randomly selected from the 12 categories in the Tancorpv 1.0 which is 2,400 texts in all named as the Chinese Academy of Science Corpus 1 , the smallest text is 1kb, and the largest one is 14.7kb. One thousand texts are randomly selected in 9 classes from the text corpus of the Sogou Lab whose largest class contains 200 documents and whose smallest class contains 80 documents. 3000 texts are randomly selected from 60 smaller classes named as the Chinese Academy of Science corpus 2.

Evaluation

The evaluation of the clustering effect in the paper refers to the evaluation methods in information retrieval and each clustering result is looked on as a result of the query so that for some ultimate clustering category r and the original scheduled class i, whose F-measure[3-4] precision and recall are defined as below:


recall
(i , r )
n (i , r ) / ni

(2)



precision
(i, r )
n (i, r ) / nr
(3)

Wherein n(i,r) is text data in the clustering including class i. nr

is the text number of class r. ni is the


text number included by the class i. Thus, the F-measure of clustering r and clustering i is calculated as below:



F ( i , r )
2		recall precision
( i , r )
( i , r )
precision  recall
( i , r )
( i , r )
(4)

F-measure of each class is the largest value of that category in all classes. According to the F-measure, the clustering performance is evaluated as below:



MacroF  1
 ni  max{
i	n

F ( i , r )}

(5)


The experiment and analysis

The features are firstly selected on the corpus. For the different experimental corpus, different thresholds ×FT are set (FT is the total number of documents for each experimental corpus;  is scale factor whose value in the range [0, 1] ; ×FT is rounded).  The feature of DFij >   FT is filtered off, forming a new feature space. The feature weight is calculated by TF-IDF[5], by the filtering feature document matrix generated through vector space model, then decomposing the matrix by SVD [6]. In LSI space, the text similarity is computed by the calculation method of the vector angle cosine. The clustering is by Complete- link algorithm.
Table 7. Clustering performance of feature transfer relation selection on LSI Sogou corpus


Table 8. Clustering performance of feature transfer relation selection on LSI Chinese Academy of Science corpus 1


As seen from the experimental results of Table 8, clustering performance firstly ascends and then descends with the increasing of  on Sogou corpus and the Chinese Academy of science corpus 1. On Sogou corpus, when  is 0.40, the clustering performance is highest. When  is 1, the clustering performance has similar states on the Chinese Academy of Science corpus 1. However, for the Chinese Academy of Science corpus 2,


the clustering performance ascends, lastly to be the highest. This shows that the selecting of the appropriate threshold and the filtering out features of the document frequency over the threshold can not only reduce the dimension of the feature space but also improve the performance of the clustering.  100% ) means the feature transfer relationship have not been selected. The F-measure value of the clustering results will not change any longer when the feature of document frequency less than 50% FT is as a new feature collection. From these three corpus, when the feature document frequency is reserved between 10% and 15%, some feature transfer relationship can be effectively filtered out in LSI space and unreasonable co-occurrence features and some noise data can be eliminated.
Table 9. Clustering performance of feature transfer relation selection on LSI Chinese Academy of Science corpus 2


Conclusion

In this paper, we think that the transfer number between features has a great impact on the performance of latent semantic indexing. As the feature transfer number increases, some non-existent feature co-occurrence information appear which affects the similarity between features so that affects the performance of the latent sematic indexing. Before the decomposing of SVD, the feature of document collection is selected by DF feature in order to reduce the feature transfer number and non-existent feature co-occurrence information. The DF method used by our paper can selected features with documents in document collection and simply filters the transfer number between features. The next step, we will study on the feature selection based on conditional entropy between the features and conditional entropy.


References
A. Kontostathis, W. M. Pottenger. A mathematical view of latent semantic indexing: Tracing term co- occurrences [R]. Technical report, LU-CSE-02-006, Dept.of Computer Science and Engineering, Lehigh University, 2002
Deerwester S. C., Dumais S. T, Landauer T. K, et al. Indexing by latent semantic analysis [J]. Journal of the American Society of Information Science, 1990, 41(6):391-407
Lewin D D, Ringuuette M. A comparison of two learning algorithms for text categorization[C]//proc of the Third Annual Symposium on Document Analysis and Information Retrieval,1994:81-93
Joachines T.Text categorization with support vector machines:learning with many relevant features[C]//proc of the 10th Eurospeech Conference on Machine Learning(ECML),1998:137-142
Wiemer-Hastings, P. How latent is latent semantic analysis? [A]. In: Proceedings of the Sixteenth International Joint Conference on Articial Intelligence [C], 1999:932-937
Ding C.H.Q. A similarity-based probability model for latent semantic indexing [A]. In Proceedings of the Twenty-second Annual International ACM/SIGIR Conference on Research and Development in Information
