Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Contents lists available at ScienceDirect

Artiﬁcial Intelligence in Agriculture

j o u r n a l h o m e p a g e : h t t p : / / w w w . k e a i p u b l i s h i n g . c o m / e n / j o u r n a l s / a r t i f i c i a l -
i n t e l l i g e n c e - i n - a g r i c u l t u r e /

Deep convolutional neural network models for weed detection
in polyhouse grown bell peppers
A. Subeesh ⁎, S. Bhole, K. Singh, N.S. Chandel, Y.A. Rajwade, K.V.R. Rao, S.P. Kumar, D. Jat

ICAR-Central Institute of Agricultural Engineering (CIAE), Bhopal, Madhya Pradesh, India

a r t i c l e

i n f o

a b s t r a c t

Article history:
Received 30 October 2021
Received in revised form 18 January 2022
Accepted 27 January 2022
Available online 10 February 2022

Keywords:
Bell pepper
Computer vision
Convolutional neural networks
Deep learning
Weed identiﬁcation

Conventional weed management approaches are inefﬁcient and non-suitable for integration with smart agricul-
tural machinery. Automatic identiﬁcation and classiﬁcation of weeds can play a vital role in weed management
contributing to better crop yields. Intelligent and smart spot-spraying system's efﬁciency relies on the accuracy
of the computer vision based detectors for autonomous weed control. In the present study, feasibility of deep
learning based techniques (Alexnet, GoogLeNet, InceptionV3, Xception) were evaluated in weed identiﬁcation
from RGB images of bell pepper ﬁeld. The models were trained with different values of epochs (10, 20,30),
batch sizes (16, 32), and hyperparameters were tuned to get optimal performance. The overall accuracy of the
selected models varied from 94.5 to 97.7%. Among the models, InceptionV3 exhibited superior performance at
30-epoch and 16-batch size with a 97.7% accuracy, 98.5% precision, and 97.8% recall. For this Inception3 model,
the type 1 error was obtained as 1.4% and type II error was 0.9%. The effectiveness of the deep learning model
presents a clear path towards integrating them with image-based herbicide applicators for precise weed
management.
© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).

1. Introduction

Bell pepper (Capsicum annum L.) is an important crop cultivated all
over the world owing to its commercial value and medicinal uses (De,
2003; Gonzalez-Diaz et al., 2009). Bell pepper is also known as ‘sweet
pepper’, ‘pepper’ or ‘capsicum’ and is cultivated for its fruit. India con-
tributes one-fourth of the world's production of bell peppers with an av-
erage annual production of 0.9 MT from an area of 0.885 million
hectares (Kumar et al., 2016). Like any other crop, bell peppers are sub-
ject to biotic and abiotic factors that negatively affect the yield, weed
being the major biotic factor (Chandel et al., 2021b). Being a poor com-
petitor with weeds, immediately after transplanting, the supply of irri-
gation for the crop may stimulate weed growth leading to yield loss
up to 97% (Amador-Ramírez, 2002; Campiglia et al., 2012). Pepper pro-
ductivity has decreased over the period in open ﬁelds compared to
polyhouse due to frequent pests and diseases (Jat et al., 2020).

Polyhouse cultivation in India needs to be enhanced in terms of tech-
nological intervention and reduction of operating costs (Singh et al.,
2019). Weeds not only cause a reduction in yield but also lower crop
quality, act as a reservoir for pests and diseases, and reduction in
human efﬁciency. This issue requires the utilization of a weed detection

⁎ Corresponding author.

E-mail address: subeesh18@gmail.com (A. Subeesh).

framework (Hasan et al., 2021). Weeds are persistent problems in
polyhouses and detract from the perceived quality of crops being
grown. Herbicides are a means to control weed growth, but they have
several negative consequences for the environment and human health
(Bah et al., 2018). The adoption of chemical and cultural control mea-
sures might have negative environmental consequences, when not ap-
propriately controlled. As most of the countries are facing labour
shortages and increased labour costs, automation of weed control sys-
tems is a need of the hour (Liu and Bruch, 2020; Subeesh and Mehta,
2021). Weed management in covered structures like polyhouses
needs much precision as the vapours from the chemicals can be trapped
in closed structures, resulting in damage to both workers as well as the
crop.

Weed recognition is one of those essential that requires digitization
and automation. Therefore, data-driven and image processing-based
techniques using Internet of things and variable rate application of in-
puts for real-time automation of system needs to be developed
(Mehta et al., 2021). Weed detection in crops is an inherently challeng-
ing problem to solve using digital technologies especially, using tradi-
tional image processing techniques (Wang et al., 2019). This is due to
the shapes and textures of both weed and crop. Varying illuminating
condition is another major challenge, when traditional image process-
ing techniques have been applied to solve this problem. If weeds are
identiﬁed at an earlier stage it leads to reduced cost of herbicides
(Espejo-Garcia et al., 2020). A typical weed identiﬁcation system follows

https://doi.org/10.1016/j.aiia.2022.01.002
2589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

four key steps: image data collection, pre-processing of images, feature
extraction and classiﬁcation (Shanmugam et al., 2020). Different ad-
vanced technologies are in vogue to execute these steps. The most
vital part of these steps is weed identiﬁcation and classiﬁcation. For
the last few years, Artﬁcial Intelligence (AI) and deep learning strategies
have been utilized for weed management with progressions in innova-
tion replacing the traditional approaches (Sharpe et al., 2019, 2020; Su,
2020).

In recent years, numerous studies have been carried for the automa-
tion of the process of identiﬁcation and classiﬁcation of weeds. Deep
learning based image processing in lettuce crops, three methods for
weed estimation were proposed by Osorio et al. (2020), which included
machine learning and deep learning models such as Support Vector Ma-
chines (SVM), YOLO and Mask R-CNN. The models achieved an F1 score
of 88%, 94%, and 94%, respectively for the crop detection. The SVM in this
study used histograms of oriented gradients (HOG) as feature descrip-
tors and YOLOV3 for object detection and Mask R-CNN for instance seg-
mentation. SVM also found to be successful in identifying aquatic weeds
(Pereira et al., 2012) and classiﬁcation of chilli and weeds in images
taken from single plants (Ahmed et al., 2012). Yu et al. (2019) reported
the efﬁciency of several deep convolutional neural network (DCNN)
models such as VGGNet, GoogLeNet and DetectNet for detection of
weeds in bermudagrass. The major ﬁnding was that DetectNet exhib-
ited high performance for detection of weeds while growing in dormant
bermudagrass, with F1 score of 0.99. There were numerous attempts
were made by researchers in identifying weeds in rice (Ashraf and
Khan, 2020; Barrero et al., 2016; Cheng and Matson, 2015), wheat
(Golzarian and Frick, 2011; Hameed and Amin, 2018), onion (Kim
et al., 2018; Parico and Ahamed, 2020; Sanchez et al., 2021), etc. Hu
et al. (2020) proposed a novel graph-based ‘Graph Weeds Net’ architec-
ture to recognize various types of weeds from RGB images collected
from complex rangelands and the model achieved good performance
with 98.1% accuracy. In the present study, we investigated the feasibility
of use of DCNN models such as Alexnet, GoogLeNet, InceptionV3 and
Xception in detecting weeds in the bell pepper cultivation.

2. Materials and methods

2.1. Image acquisition

The images were collected at ICAR-Central Institute of Agricultural
Engineering, Bhopal, Madhya Pradesh (longitude — 77°24′11.28″E and
latitude — 23°18′35.67”N). A hybrid variety of bell pepper known as
‘Indra’ was cultivated on raised beds according to standard package of
practices (Jat et al., 2020). The images were captured between 9 AM
to 5 PM inside a polyhouse under varying lighting conditions with a dig-
ital camera (Xiaomi Mi 11× mobile device's rear camera, which has a

triple camera setup with 48 MP, f/1.8, 26 mm (wide), 1/2“, 0.8μm,
PDAF 8 MP, f/2.2, 119° (ultrawide) 5 MP, f/2.4, 50mm (macro), 1/5.0”,
1.12 μm) at a ratio of 4:3, with a resolution of 4000 × 3000 pixels. A
total of 1106 individual images were collected from the polyhouse,
with 685 images of bell pepper and 421 images of various weeds. The
collected images were grouped into weed and crop categories. The
noise and lighting variations have been removed while pre-processing
the images. Further, data augmentation has been applied over the
dataset to enhance the size and quality of training datasets and prevent
overﬁtting (Shorten and Khoshgoftaar, 2019). The presence of weeds in
bell pepper grown in polyhouse is shown in Fig. 1.

The images from the dataset were further divided into training, test-
ing and validation. The modeling was performed with 80% of the images
for training, 10% for testing and 10% for validation from the total cap-
tured images.

2.2. Deep learning and CNN models

In the initial years, artiﬁcial intelligenceheavily relied on the rule-
based engines that can make predictions based on the ﬁxed and
predeﬁned rule sets generated by a human expert. However, as the
data got massive, a more data driven approach was required and ma-
chine learning was into the action. Machine learning is a collection of al-
gorithms and tools by which machines can understand patterns within
the data and perform reasoning about a speciﬁc task. Machine learning
has the capability to extract meaningful information from the data using
various algorithms. Deep learning (DL) can be considered as a next fron-
tier of machine learning; it is a subset of machine learning that makes
extensive use of neural networks. The advent of deep learning has
brought a revolution in the area of image analysis and computer vision
(Chandel et al., 2021a; Hemanth and Estrela, 2017).

Deep learning techniques applied to digital photographs can help
distinguish between crops and weeds beyond the limitations of conven-
tional image processing. A deep convolutional neural network (DCNN)
is a type of artiﬁcial intelligence that is extensively utilized in recent
years. In the initial days, the most that CNN could do was recognize
handwritten digits. At present, DCNN models are the most eccentric ap-
paratus in computer vision capable of analysing huge complex datasets,
having high computational limits. Numerous DL architectures exist for
image classiﬁcation like AlexNet (Krizhevsky et al., 2012), DenseNet
(Huang et al., 2017), EfﬁcientNet (Tan and Le, 2020), GoogLeNet
(Szegedy et al., 2015), InceptionNet (Szegedy et al., 2016), NASNet/
PNASNet/ENASNet (Adam and Lorraine, 2019), ResNeXt (Xie et al.,
2017), ResNet50 (He et al., 2016), XceptionNet (Chollet, 2017), SENet
(Hu et al., 2018), VGGNet (Simonyan and Zisserman, 2015) and ZFNet
(Howard et al., 2017). In this study for identifying weeds, the perfor-
four different CNN architectures AlexNet, GoogleNet,
mance of

Fig. 1. Presence of weeds in bell pepper grown in polyhouse.

48

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Table 1
Architectural characteristics of selected Convolutional Neural Network (CNN) models.

Model

Parameters (in millions)

Depth

Image input size

Model characteristics

AlexNet
GoogLeNet
InceptionV3
Xception

61.0
7.0
23.9
22.9

8
22
48
71

227 × 227
224 × 224
299 × 299
299 × 299

Simple architecture with 5 Convolutional layers and 3 Fully connected layers
Use of auxiliary classiﬁers, Going deeper with convolutions
Extended network of GoogLeNet, Reduced computational complexity
Linear stack of depthwise separable convolutions and residual connections, Modular architecture

InceptionV3 and Xception are investigated. The models were selected
based on the complexity and computational cost. AlexNet being the rel-
atively simplest models and InceptionV3 (48 layers) and GoogleNet, are
of moderate complexity and Xception with high complexity and depth.
Table 1 shows the architectural characteristics of these models.

AlexNet is an 8-layer CNN that outperformed all other models in a
large margin and won the ImageNet large scale visual recognition chal-
lenge held in 2012 (Krizhevsky et al., 2012). Although LeNet (LeCun
et al., 1995) had achieved decent results on early datasets of smaller
size, the performance while training CNNs on larger datasets were not
that impressive. AlexNet has achieved huge improvement over LeNet
and it was the ﬁrst architecture to adopt consecutive convolutional

layers with kernel sizes of (11 × 11), (5 × 5), and (3 × 3). VGG16
Model (Simonyan and Zisserman, 2015), which achieved 92.7% top-5
test accuracy in ImageNet Large-Scale Visual Recognition Challenge. It
was submitted to ILSVRC in 2014 where it became one of the popular
models. VGG16 showed improved performance over AlexNet by replac-
ing large kernel-sized ﬁlters (11 and 15 sizes present in AlexNet) with a
number of 3 × 3 sized kernels. The introduction of inception networks
has made a signiﬁcant impact in the ﬁeld of neural networks. The ﬁrst
version of inception model i.e. inceptionV1 is termed as GoogLeNet.
This had set a new state of the art for both classiﬁcation and object de-
tection problems. The GoogLeNet Architecture has a depth of 22 layers
with 27 pooling layers and 9 inception modules stacked in total

Fig. 2. Deep Learning based Image classiﬁcation pipeline used for weed identiﬁcation.

Fig. 3. Components of Image classiﬁcation using DCNN.

49

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Fig. 4. Accuracy and loss function plots (trained with epoch =30, batch size =16) a) AlexNet, b) GoogLeNet, c) InceptionV3, d) Xception.

(Szegedy et al., 2015). GoogLeNet had achieved a top-5 error rate of
6.67% in ILSVRC, which is very close to the human level performance.
In comparison to the AlexNet and GoogLeNet, InceptionV3 model
focuses on consuming less computational power. InceptionV3 model
optimizes the network with the help of factorized convolution, dimen-
sionality reduction, regularization and parallelized computations
(Szegedy et al., 2016). Xception is an extreme version of the inception.
Depth wise separable convolutional layers make Xception different

from the rest of the models and it follows a modular architecture
(Chollet, 2017).

2.3. Deep learning based image classiﬁcation framework for weed
identiﬁcation

The image classiﬁcation for weed detection began with the image
data collection. The data has been collected from the precision farming

50

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Fig. 5. Confusion Matrices indicating the performance of the selected models (trained on epoch = 30, batch size = 16) a) AlexNet, b)GoogLeNet, c) InceptionNet, d) Xception.

development centre located at ICAR – Central Institute of Agricultural
Engineering Bhopal, India.

The images collected have been pre-processed before handing over
to the classiﬁcation task (Fig. 2). The labels for each image have been de-
ﬁned and a binary classiﬁcation was performed in the experiment. The
models AlexNet, GoogLeNet, InceptionV3, and Xception models were

Table 2
Performance of inceptionV3 model.

Epoch

Batch size

Precision (%)

Recall (%)

F1 score (%)

Accuracy (%)

10

20

30

16
32
16
32
16
32

92.0
93.4
94.9
97.8
98.5
96.4

99.2
99.2
99.2
97.8
97.8
99.2

95.5
96.2
97.0
97.8
98.1
97.8

94.6
95.5
96.4
97.3
97.7
97.3

chosen for identifying the bell pepper and weed. The pre-trained
models speciﬁed here for image classiﬁcation are generally composed
of two components: a convolutional base and a classiﬁer (Fig. 3). The
convolutional base is responsible for the feature extraction and the clas-
siﬁer classiﬁes the input image based on the features extracted by the
convolutional base. In the classiﬁer part, the standard approach is to
use fully connected layers followed by an activation layer, which is gen-
erally a softmax activation. The softmax layer generates the output indi-
cating the probability of each class. The most probable class is chosen as
the predicted class.

The model was trained with the help of training data and before the
training got started, the model parameters like number of epochs, batch
size, learning rate, etc. were set. Validation set was used for unbiased
evaluation of the model. This helped to optimize the hyperparameters
of the model. After identifying the right parameters, the model was
trained again by setting these parameters. Once the model was

51

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Fig. 6. Performance of Deep Learning Models with respect to parameters epoch and batch size.

completely trained, test set was used to provide a ﬁnal real-world check
of the unseen data points to conﬁrm the proper functioning of the
model.

Accuracy ¼

TP þ TN
TP þ TN þ FP þ FN

2.4. Training, validation and testing

The images were pre-processed and trained using AlexNet,
GoogLeNet, Xception and Inception V3. During the training phase, the
model automatically extracts the relevant features of ‘bell pepper’
plant and ‘weed’ in general. This process can be applied directly to the
image captured from the ﬁelds, which are unseen to the model, and
the model can produce an accurate classiﬁcation. During the training
process, the loss function was set to stochastic gradient descent algo-
rithm and batch sizes were set to 16 and 32. The initial learning rate
was set to 0.001 and momentum to 0.9000. For all these models,
epoch and batch size were varied and performance across the models
were compared. The validation data used to validate our model that
helps in adjusting and tuning the hyperparameters. The main reason
for using the validation data is to prevent the model from overﬁtting.
Confusion matrix is effective in estimating the performance of the ma-
chine learning classiﬁcation models. In this study, we have a binary clas-
siﬁcation problem and each input sample was assigned to one of the
classes ‘weed’ or ‘bell pepper’. The row of the confusion matrix denotes
the predicted class and the column represents the actual class of the in-
stances.

Table 3
Performance of models AlexNet, GoogLeNet, InceptionV3 and Xception (Epoch = 30,
batch size = 16).

Model

Precision (%)

Recall (%)

F1 Score (%)

Accuracy (%)

AlexNet
GoogleNet
InceptionV3
Xception

95.6
94.9
98.5
95.6

98.5
98.5
97.8
99.2

97.0
96.7
98.1
97.4

96.4
95.9
97.7
96.8

ð1Þ

ð2Þ

ð3Þ

ð4Þ

Precision ¼ TP

TP þ FP

Sensitivity=Recall ¼ TP

TP þ FN

F1 Score ¼ 2∗Precision∗Recall
Precision þ Recall

Accuracy, precision, recall and F1 score matrices are used for evalu-
ating the model performance (Eqs. (1)–(4)). Here, TP denotes the
True Positive, indicating the number/percentage of instances of bell
peppers accurately predicted by the model, FP denotes the Fall Posi-
tive/type I error that shows the percentage of bell peppers misclassiﬁed
as weeds. FN or False Negative denotes the Type II error that shows the
number/percentage of weed images classiﬁed as bell pepper. TN or True
Negative indicates the number percentage of weed images that are
accurately predicted by the model.

3. Results and discussion

In this study, the performances of various deep learning models
were evaluated for identifying the weeds among the bell peppers. The
results indicated that all the models have performed satisfactorily
with an overall accuracy varying between 94.5 and 97.7%. The experi-
ment has been repeated with a number of epochs 10, 20, 30 and signif-
icant improvement has been observed in the accuracy. The number of
batch size should be a power of 2, to take the complete advantage of
the GPU processing (Kandel and Castelli, 2020). Two different values
of batch size 16 and 32 were selected for training the model. Fig. 4
shows the accuracy and loss function variation when the model is
trained with 30 epochs and 16 batch size. The plots indicate the loss
function for all the models has started converging from early epochs
without having large ﬂuctuations. There were no overﬁtting or

52

A. Subeesh, S. Bhole, K. Singh et al.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Fig. 7. Predictions and accuracy of InceptionV3 model (trained on epoch = 30, batch size =16) on the test images (The percentage in the ﬁgurehead represents the conﬁdence of the model
in predicting the speciﬁc class).

underﬁtting observed in AlexNet, GoogLeNet,
Xception models.

InceptionV3, and

In AlexNet, for a mini batch size of 16, the accuracy obtained for 10,
20, and 30 epochs were 96.8%, 95.6%, and 96.4% respectively. With mini
batch size 32, the accuracies for the model were 95.9%, 96.8%, 95.4% re-
spectively. Thus there was no steady improvement is observed while in-
creasing the number of epochs as well as the batch size for Alexnet. The
overall accuracies in alexnet were varied between 95.4% and 96.8%. For
GoogLeNet, the highest accuracy of 97.2% was obtained at 20 epochs
and batch size 16. In the Xception model, there is a slight improvement
in accuracy has been observed after increasing the number of epochs
from 10 to 30. The accuracies of Xception were varied between 95.9
and 96.9%.

The confusion matrices for weed and bell pepper identiﬁcation using
the selected deep learning models have been shown in Fig. 5. The diag-
onal values indicate the true estimations. The highest accuracy observed
was 97.7% for InceptionV3 when it was trained for 30 epochs with a
batch size of 16. For these hyperparameter values of batch size 16,
with epoch 30, the inceptionV3 model was able to successfully classify
135 bell pepper images out of 138 bell pepper images and only 3 images
of bell pepper were misclassiﬁed. Also, the model successfully identiﬁed
81 weed images out of 83 weed images and only 2 weed images were
misclassiﬁed. InceptionV3 model showed a steady rise in the accuracy
on increasing the number of epochs. The accuracies observed for
InceptionV3 for mini batch size 16 and epochs 10, 20, 30 were 94.6,
96.4 and 97.7%, respectively (Table 2). This improved performance of
InceptionV3 over other models is due to factorization into smaller con-
volutions and the use of auxiliary classiﬁers as regularizes. InceptionV3
architecture has made major improvements over the GoogLeNet archi-
tecture without compromising on the performance. Even though there
is an increase, the same trend of steady increase was not observed in

batch size 32 and the accuracies were 95.5, 97.2, and 97.3 respectively.
There was no further improvement in accuracy after increasing the
epochs from 20 to 30 (Fig. 6).

Table 3 compares the precision, recall, F1 scores of all the models
with the best performing which was trained with 30 epochs and a
batch size of 16. The recall was very high for Xception, which indi-
cated that the model was highly capable of making correct positive
predictions. The precision, accuracy, and F1 score of inceptionV3 at
30 epochs and 16 batch size are higher than the other models
and found to be the most effective model for weed and crop iden-
tiﬁcation. The Type I error for inceptionV3 at these parameters
setting was 1.4% and Type II error was observed as 0.9%. The same
inceptionV3 model was tested with random testing images and the
results are as shown in Fig. 7. The obtained results show that the
model is able to successfully identify the weed among the bell pepper
with very high conﬁdence.

4. Conclusion

Deep convolutional neural network-based weed detection is prom-
ising and supports automation of agricultural operations. This work
demonstrated the capability of using DCNN models for weed identiﬁca-
tion in bell pepper ﬁeld. In this study, four deep learning models
(Alexnet, GoogLeNet,
InceptionV3, Xception) have been applied
for the identiﬁcation of weeds present among the bell pepper ﬁeld.
InceptionV3 outperformed others in terms of precision, accuracy, and
recall. The potential future work includes detection of crop and weed
in real-time and execution of the weeding action by intelligent weeders
and/or site-speciﬁc herbicide applicators based on the decision made by
the DCNN models.

53

A. Subeesh, S. Bhole, K. Singh et al.

Credit author statement

A. Subeesh: Conceptualization, Data curation, Methodology, Formal
analysis, Writing – original draft. Sameer Bhole: Investigation, Data
curation, Validation, Writing-original draft. Karan Singh: Software, Su-
pervision, Writing - review & editing. NS Chandel: Data curation, Meth-
odology, Validation, Writing - original draft, Writing - review & editing.
YA Rajwade: Data curation, Methodology, Writing - original draft, Writ-
ing - review & editing. K.V.R Rao – Investigation, Resources, Supervision,
Validation, Writing - review & editing. SP Kumar - Validation, Writing -
review & editing. Dilip Jat: Data curation, Writing - review & editing.

Declaration of Competing Interest

The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂu-
ence the work reported in this paper.

References

Adam, G., Lorraine, J., 2019. Understanding Neural Architecture Search Techniques

ArXiv190400438 Cs Stat.

Ahmed, F., Al-Mamun, H.A., Bari, A.S.M.H., Hossain, E., Kwan, P., 2012. Classiﬁcation of
crops and weeds from digital images: a support vector machine approach. Crop
Prot. 40, 98–104. https://doi.org/10.1016/j.cropro.2012.04.024.

Amador-Ramírez, M.D., 2002. Critical period of weed control in transplanted chilli pepper.

Weed Res. 42, 203–209. https://doi.org/10.1046/j.1365-3180.2002.00278.x.

Ashraf, T., Khan, Y.N., 2020. Weed density classiﬁcation in rice crop using computer vision.
Comput. Electron. Agric. 175, 105590. https://doi.org/10.1016/j.compag.2020.
105590.

Bah, M.D., Haﬁane, A., Canals, R., 2018. Deep learning with unsupervised data labeling for
weed detection in line crops in UAV images. Remote Sens. 10, 1690. https://doi.org/
10.3390/rs10111690.

Barrero, O., Rojas, D., Gonzalez, C., Perdomo, S., 2016. Weed detection in rice ﬁelds using
aerial images and neural networks. 2016 XXI Symposium on Signal Processing, Im-
ages and Artiﬁcial Vision (STSIVA), pp. 1–4 https://doi.org/10.1109/STSIVA.2016.
7743317.

Campiglia, E., Radicetti, E., Mancinelli, R., 2012. Weed control strategies and yield re-
sponse in a pepper crop (Capsicum annuum L.) mulched with hairy vetch (Vicia
villosa Roth.) and oat (Avena sativa L.) residues. Crop Prot. 33, 65–73. https://doi.
org/10.1016/j.cropro.2011.09.016.

Chandel, N., Chakraborty, S., Rajwade, Y., Dubey, K., Tiwari, M.K., Jat, D., 2021a. Identifying
crop water stress using deep learning models. Neural Comput. Applic. 33. https://doi.
org/10.1007/s00521-020-05325-4.

Chandel, N.S., Chandel, A.K., Roul, A.K., Solanke, K.R., Mehta, C.R., 2021b. An integrated
inter- and intra-row weeding system for row crops. Crop Prot. 145, 105642.
https://doi.org/10.1016/j.cropro.2021.105642.

Cheng, B., Matson, E.T., 2015. A feature-based machine learning agent for automatic Rice
and weed discrimination.
In: Rutkowski, L., Korytkowski, M., Scherer, R.,
Tadeusiewicz, R., Zadeh, L.A., Zurada, J.M. (Eds.), Artiﬁcial Intelligence and Soft Com-
puting. Lecture Notes in Computer Science. Springer International Publishing, Cham,
pp. 517–527 https://doi.org/10.1007/978-3-319-19324-3_46.

Chollet, F., 2017. Xception: deep learning with depthwise separable convolutions. 2017
IEEE Conference on Computer Vision and Pattern Recognition (CVPR),
pp. 1800–1807 https://doi.org/10.1109/CVPR.2017.195.
De, A.K., 2003. Capsicum: The Genus Capsicum. CRC Press.
Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., 2020. Improving weeds identi-
ﬁcation with a repository of agricultural pre-trained deep neural networks. Comput.
Electron. Agric. 175, 105593. https://doi.org/10.1016/j.compag.2020.105593.

Golzarian, M.R., Frick, R.A., 2011. Classiﬁcation of images of wheat, ryegrass and brome
grass species at early growth stages using principal component analysis. Plant
Methods 7, 28. https://doi.org/10.1186/1746-4811-7-28.

Gonzalez-Diaz, L., Martinez Jimenez, M.P., Bastida, F., Gonzalez-Andujar, J., 2009. Decision
support system for integrated plant protection in pepper. Expert Syst. Appl. 36,
8975–8979. https://doi.org/10.1016/j.eswa.2008.11.038.

Hameed, S., Amin, I., 2018. Detection of weed and wheat using image processing. 2018
IEEE 5th International Conference on Engineering Technologies and Applied Sciences
(ICETAS), pp. 1–5 https://doi.org/10.1109/ICETAS.2018.8629137.

Hasan, A.S.M.M., Sohel, F., Diepeveen, D., Laga, H., Jones, M.G.K., 2021. A survey of deep
learning techniques for weed detection from images. Comput. Electron. Agric. 184,
106067. https://doi.org/10.1016/j.compag.2021.106067.

He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. 2016
IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770–778
https://doi.org/10.1109/CVPR.2016.90.

Hemanth, D.J., Estrela, V.V., 2017. Deep Learning for Image Processing Applications. IOS

Press.

Artiﬁcial Intelligence in Agriculture 6 (2022) 47–54

Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M.,
Adam, H., 2017. MobileNets: Efﬁcient Convolutional Neural Networks for Mobile
Vision Applications ArXiv170404861 Cs.

Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks. 2018 IEEE/CVF Confer-
ence on Computer Vision and Pattern Recognition, pp. 7132–7141 https://doi.org/
10.1109/CVPR.2018.00745.

Hu, K., Coleman, G., Zeng, S., Wang, Z., Walsh, M., 2020. Graph weeds net: a graph-based
deep learning method for weed recognition. Comput. Electron. Agric. 174, 105520.
https://doi.org/10.1016/j.compag.2020.105520.

Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connected
convolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Rec-
ognition (CVPR), pp. 2261–2269 https://doi.org/10.1109/CVPR.2017.243.

Jat, D., Rajwade, Y.A., Chandel, N.S., Dubey, K., Rao, K.V.R., 2020. Embedded system for reg-
ulating abiotic parameters for Capsicum cultivation in a polyhouse with comparison
to open-ﬁeld cultivation. Int. J. Veg. Sci. 26, 487–497.

Kandel, I., Castelli, M., 2020. The effect of batch size on the generalizability of the
convolutional neural networks on a histopathology dataset. ICT Express 6, 312–315.
https://doi.org/10.1016/j.icte.2020.04.010.

Kim, S., Lee, J.S., Kim, H.S., 2018. Deep learning-based automatic weed detection on onion

ﬁeld. Smart Media J. 7, 16–21. https://doi.org/10.30693/SMJ.2018.7.3.16.

Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classiﬁcation with deep
convolutional neural networks. Advances in Neural Information Processing Systems.
Curran Associates, Inc.

Kumar, P., Chauhan, R.S., Grover, R.K., 2016. Economic analysis of capsicum cultivation
under polyhouse and open ﬁeld conditions in Haryana. Int. J. Farm Sci. 6, 96–100.
LeCun, Y., Jackel, L.D., Bottou, L., Brunot, A., Cortes, C., Denker, J., Drucker, H., Guyon, I.,
Muller, U.A., Sackinger, E., 1995. Comparison of learning algorithms for handwritten
digit recognition. International Conference on Artiﬁcial Neural Networks. Perth,
Australia, pp. 53–60.

Liu, B., Bruch, R., 2020. Weed detection for selective spraying: a review. Curr. Robot. Rep.

1, 19–26. https://doi.org/10.1007/s43154-020-00001-w.

Mehta, C.R., Chandel, N.S., Rajwade, Y., 2021. Smart farm mechanization for sustainable

Indian agriculture. Ama Agric. Mech. Asia Afr. Lat. Am. 50, 99–105.

Osorio, K., Puerto, A., Pedraza, C., Jamaica, D., Rodríguez, L., 2020. A deep learning ap-
images.

proach for weed detection in lettuce crops using multispectral
AgriEngineering 2, 471–488. https://doi.org/10.3390/agriengineering2030032.
Parico, A.I.B., Ahamed, T., 2020. An aerial weed detection system for green onion crops
using the you only look once (YOLOv3) deep learning algorithm. Eng. Agric. Environ.
Food 13, 42–48. https://doi.org/10.37221/eaef.13.2_42.

Pereira, L.A.M., Nakamura, R.Y.M., de Souza, G.F.S., Martins, D., Papa, J.P., 2012. Aquatic

Weed Automatic Classiﬁcation Using Machine Learning Techniques.

Sanchez, P.R., Zhang, H., Ho, S.-S., De Padua, E., 2021. Comparison of one-stage object de-
tection models for weed detection in mulched onions. 2021 IEEE International Con-
ference on Imaging Systems and Techniques (IST), pp. 1–6 https://doi.org/10.1109/
IST50367.2021.9651352.

Shanmugam, S., Assunção, E., Mesquita, R., Veiros, A., Gaspar, P.D., 2020. Automated weed

detection systems: a review. KnE Eng. 271–284.

Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2019. Detection of Carolina Geranium (Gera-
nium carolinianum) growing in competition with strawberry using convolutional
neural networks. Weed Sci. 67, 239–245. https://doi.org/10.1017/wsc.2018.66.
Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2020. Goosegrass detection in strawberry and
tomato using a convolutional neural network. Sci. Rep. 10, 9548. https://doi.org/10.
1038/s41598-020-66505-9.

Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deep

learning. J. Big Data 6, 60. https://doi.org/10.1186/s40537-019-0197-0.

Simonyan, K., Zisserman, A., 2015. Very Deep Convolutional Networks for Large-Scale

Image Recognition ArXiv14091556 Cs.

Singh, M.C., Singh, G.K., Singh, J.P., 2019. Nutrient and water use efﬁciency of cucumbers
grown in soilless media under a naturally ventilated greenhouse. J. Agric. Sci. Technol.
21, 193–207.

Su, W.-H., 2020. Advanced machine learning in point spectroscopy, RGB- and
hyperspectral-imaging for automatic discriminations of crops and weeds: a review.
Smart Cities 3, 767–792. https://doi.org/10.3390/smartcities3030039.

Subeesh, A., Mehta, C.R., 2021. Automation and digitization of agriculture using artiﬁcial
intelligence and internet of things. Artif. Intell. Agric. 5, 278–291. https://doi.org/10.
1016/j.aiia.2021.11.004.

Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,
Rabinovich, A., 2015. Going deeper with convolutions. 2015 IEEE Conference on Com-
puter Vision and Pattern Recognition (CVPR), pp. 1–9 https://doi.org/10.1109/CVPR.
2015.7298594.

Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2016. Inception-v4, Inception-ResNet and

the Impact of Residual Connections on Learning ArXiv160207261 Cs.

Tan, M., Le, Q.V., 2020. EfﬁcientNet: Rethinking Model Scaling for Convolutional Neural

Networks ArXiv190511946 Cs Stat.

Wang, A., Zhang, W., Wei, X., 2019. A review on weed detection using ground-based ma-
chine vision and image processing techniques. Comput. Electron. Agric. 158,
226–240. https://doi.org/10.1016/j.compag.2019.02.005.

Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., 2017. Aggregated Residual Transformations for

Deep Neural Networks ArXiv161105431 Cs.

Yu, J., Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2019. Deep learning for image-based
weed detection in turfgrass. Eur. J. Agron. 104, 78–84. https://doi.org/10.1016/j.eja.
2019.01.004.

54

