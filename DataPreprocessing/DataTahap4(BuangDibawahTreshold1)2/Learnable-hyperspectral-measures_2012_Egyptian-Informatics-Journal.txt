Hyperspectral measures are alternative approach to derive discriminatory information regarding spectral signatures. These measures are simple and computationally light. They are able to capture the degree of similarity between two spec- trums. For example, in Spectral Angle Mapper (SAM), the similarity value ranges from 0 (highly similar) to 1 (highly dis- similar). The similarity value should, for instance, be less than

The proposed approach aims to develop learnable hyper- spectral measures as replacement for static threshold hyperspectral measures. This is done through using hyperspec- tral measures values as similarity patterns and employing a classifier. The classifier acts as an adaptive similarity threshold. The derived similarity patterns are flexible as they are able to capture the specific notion of similarity that is appropriate for each spectral region. Two similarity patterns are proposed. The first pattern is the cosine similarity vector for the second spectral derivative pair. The second pattern is a composite vec- tor of different similarity measures values.

Version 1.1 calculates the cosine similarity vectors for the sec- ond order derivatives of the spectral signature pairs. The vec- tors form similar and dissimilar patterns. The resulting patterns are classified by SVM that acts as an adaptive similar- ity threshold. Version 1.1 is applied on the full hyperspectral space of the spectral signature. In this section, we describe the steps of version 1.1.

C class mean vector. The result is five cosine similarity sub- space vectors forming one combined similar pattern. For each sample K not in class C, the cosine similarity is calculated be- tween the five subspaces of K and the corresponding five sub- spaces in C class mean vector. The result is five cosine

tral measures to form similar and dissimilar patterns. The resulting patterns are classified by SVM that acts as an adap- tive similarity threshold. Version 2.1 is applied on the full hyperspectral space of the spectral signature. Combining sim- ilarity values means consolidating the different statistics de- rived by similarity measures. The resulting composite vector of similarity values is used to discriminate each spectrum pair. In this section we describe the steps of version 2.1.

The second version calculates different similarity values using nine similarity measures to form similar and dissimilar pat- terns. SVM classifies the resulting patterns to act as adaptive similarity threshold. Each version has been implemented twice using full hyperspectral space and subspaces. Mathworks Mat- lab version R2009b has been used for implementing the hyper- spectral measures. LIBSVM [17], a support vector machines tool, has been used to handle the multi-class SVM types. The used SVM parameters have been derived from a research

According to analysis conducted by Wu and Chang in [10] on the test dataset, the spectral signatures of classes (2, 3, 4, 7, 10 and 12) are so close to each other, and the same condition for classes (1, 8, and 11). For classes (5, 14, and 15), they have less similar signatures. For classes (6, 13 and 16), their signatures are dissimilar. Classes 5 and 11 are highly mixed. Signal-to- Noise Ratio (SNR) at the time of data acquisition was lower than current AVIRIS standards. This means the noise level is high.

OvA separated each class from the rest classes, and devel- oped a classification model. Such procedure was not appropri- ate for highly mixed classes. Many of the separated classes contained spectral signatures that were close to spectral signa- tures of other classes. Therefore, SVM failed to discriminate the similarity patterns efficiently. The training complexity was high as each OvA classifier was trained using all available samples. As a result, the performance of OvA was poor.

OvO was much better than OvA as each OvO classifier was trained using samples of two classes only. The low number of samples causes smaller nonlinearity, shorter training times and significant information discrimination. As a result, OvO achieved better results than OvA.

The previous experiments have been applied on nine classes out of 17. This is because PCA approach avoids classifying the remaining eight classes as the samples of these classes were relatively small. By applying NCA on the neglected classes (1, 4, 7, 9, 13, 15, 16 and 17) using DistLearnKit,1 the classifica- tion accuracy for each neglected class was calculated. The following hypotheses have been set for right-tail Z-test: H0 (P1 6 P) and H1 (P1 > P). P and P1 are the average classifica- tion accuracies for the 17 class samples achieved by NCA ap- proach and version 2.2 of the proposed approach respectively. H0 is accepted when the calculated Z (Zc) 6 the tabular Z (ZT). H1 is accepted when the calculated Z (Zc) > the tabular Z (ZT). The calculated Z (Zc) is defined as:

subspaces. The calculated cosine weights for spectral fea- tures kept its power as they were normalized across small spectral regions. Both versions 1.1 and 1.2 were concerned only with the geometry of the spectral signatures. They did not capture any other discriminatory information such as: orthogonal projections information, correlation coeffi- cients, and probability distributions produced by the spec- tral signatures. Versions 2.1 and 2.2 have combined all of these characteristics.

using Hyperspectral measures values as similarity patterns and employing a classifier. The classifier acts as an adaptive simi- larity threshold. Two similarity patterns are proposed. The first pattern is the cosine similarity vector for the second spec- tral derivative pair. The second pattern is a composite vector of different similarity measures values. The resulting patterns are classified by SVM. The proposed approach is applied on full Hyperspectral space and sub-spaces.

The experiments have been applied on one of the most chal- lenging Hyperspectral datasets. This is done to test the robust- ness of the proposed approach compared to the best competitive approaches applied on the same dataset. The experimental evaluation showed that the proposed approach outperformed PCA and NCA approaches. By conducting a right-tail Z-test to compare the significance of version 2.2 of the proposed approach to the best competitive approach (NCA approach), the calculated Z value was 1.7047 and the one-tailed p-value was 0.0441. This means the two classifica- tion accuracies were significantly different.

PCA performance was poor. This is because PCA kept high variance bands and ignored low order bands containing discriminatory information. In addition, PCA failed to classify small size classes of the test dataset. Unlike PCA which is not directly related with the final classification performance, NCA was designed to directly optimize the expected leave-one-out (LOO) classification error on the training data. Therefore, NCA performance was far better than PCA. NCA developed a learnable distance metric by finding a linear transformation of input data to enable KNN to perform well in this trans- formed space. Although NCA achieved good results, it is com- putationally expensive.

The training time of PCA and NCA was so high compared to the proposed approach versions. Therefore, the larger the number of training samples, the longer the time needed to build classification models for both PCA and NCA. The re- sults imply that using simple learnable hyperspectral measures overcome complex or manually tuned techniques used in clas- sification tasks.

