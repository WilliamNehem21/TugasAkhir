Agents have a partial representation of the external environment and limited capabilities of interacting with it. An agent typically represents the external environment in terms of its individual perceptions of the environment. The type of such perceptions of course depends on the sensing capabilities owned by the agent.

We will focus on the way in which the behaviour of an agent may be influ- enced by its perceptions of the external environment, rather than on the way in which the agent will get such perceptions. For instance, we will abstract from the way in which a software agent accesses some piece of information available in the external environment (e.g., by receiving a message, by downloading a file, or by getting data from physical sensors).

At each moment, the external environment is hence represented by the perceptions of the agent. When agents are specified by logic programs, en- vironment perceptions can be naturally represented as Herbrand interpreta- tions. Moreover, while an agent is performing its computation, the external environment may arbitrarily and independently evolve. We therefore define an environment representation as follows.

Example 3.3 Consider for instance a walking robot that controls its walking speed on the base of its perception of the weather. Suppose that the robot can move at two different speeds (slow or fast), and that it is able to perceive three weather conditions (sun, rain and snow). A natural specification of the speed control is to move slowly when it rains, to move fast when it is sunny, and not to move when it is snows. The speed control of such a robot can then be described by the following program P :

where the set of possible environment perceptions is E = { {Red},{Black} }. Suppose that the initial budget of the agent is 100$. After the first round the agent will have either 99$ or 101$, depending on the result of wheel re- volvement. After the second round the agent will have 98$, 100$, or 102$, and so on and so forth. The possible behaviours of the agent are illustrated by the

In the previous section, we have informally discussed the idea of associating a probability distribution with the set of environment perceptions of an agent. Following Definition 4.1, a probabilistic environment representation can be now formally defined as a set of probabilistic interpretations. We will consider sets of probabilistic interpretations such that the probabilities associated with the interpretations form a probability distribution, as stated by the following definition.

In order to further analyse the behaviour of the agent, we may make some further assumptions on the probabilities of the environment perceptions. For instance, we may assume that the robot will operate in an environment where snow is a quite rare event, while sun and rain alternate one another with sun slightly prevailing over rain. The assumption that in each moment the weather will be sunny with .5 probability, rainy with .4 probability and snowing with

One of the properties of interest in the practice of multi-agent systems is which are the conclusions that the agent may draw by reacting to the external environment, the evolution of the latter being a priori unknown. The notion of possible beliefs was introduced in [4] to formally characterise the set of conclusions that a program P may possibly draw, starting with an initial set of interpretations I and reacting to a set of environment perceptions E . An effective, resource bounded characterisation of the possible beliefs of a program after n steps of computation can be formalised as follows.

It is worth observing that the notion of likely beliefs gives a handy repre- sentation of a subset of the probabilistic beliefs of practical use. Consider for instance again the problem of comparing the probabilistic behaviours of the walking robot when varying its speed control program. If the robot engineers consider .8 as their belief threshold, then they obtain:

The invariant of a conventional program defines the properties that hold at each stage of the program computation. Analogously, the invariant of a reac- tive program defines the largest set of conclusions that the program will be able to draw at any time in any environment. An effective, resource bounded characterisation of the invariant of a program after n steps of computation can be formalised as follows.

Example 5.13 Consider again the walking robot discussed in Examples 3.3 and 3.5. Suppose that the robot designers have discovered that their robot does not resist for a long time under the snow. The question of the engineers is now: Will the robot survive in the environment where it is supposed to operate? Or should we rather improve the robot resistance to ice to be reasonably certain that it will not fall KO?

Example 5.15 Consider again the question of the robot engineers discussed in Example 5.13. We observe that while the (non-probabilistic) invariant of the program is empty for any n > 1, the notion of probabilistic invariant supports a real probabilistic analysis of the robot behaviours. For instance, if the robot engineers choose .95 as their belief threshold for the probabilistic invariant, then they obtain:

Several efforts have been devoted to investigate the role of computational logic in multi-agent systems (see [14] for a quite recent road map). The IM- PACT system [16] is one of the best known examples of multi-agent system relying on computational logic. Agent beliefs are represented by agent pro- grams, in the style of logic programming, while integrity constraints and action bases are used to describe the actions that individual agents can perform. A number of interesting applications of IMPACT have been illustrated, includ- ing its recent extension to deal with temporal reasoning [8]. Several other efforts have focussed on the use of computational logic to represent incom- plete information in multi-agent systems. The use of abduction to represent incomplete communication environments [15], and the use of updates to rep- resent dynamically evolving knowledge [2,12] are two promising approaches that have been recently proposed. The agent-based architecture presented in

A considerable amount of work has been devoted in the logic program- ming community to model open programs and their composition (see [5] for a survey). While these works share with ours the adoption of the logic pro- gramming paradigm as specification language, they focus on the composition of static programs. In contrast, we focus on the composition of a program with an external dynamic environment, and analyse its reactive, incremental computations.

Abductive logic programming [10] is another approach to modeling in- complete knowledge. In this setting, agents may abduce external hypotheses provided that they satisfy existing integrity constraints. While we focus on bottom-up semantics, abductive logic programming is defined via proof proce- dures which combine backward reasoning with integrity constraint checking. A promising direction for further developments is to employ abduction to express forms of interaction among agents, as indicated in [6,7].

Finally, a large body of research has been devoted in the last two decades to the study of concurrency. The main focus of these activities is to model process interactions abstracting from internal computations steps. In contrast our focus is on the interplay between interaction and computation. It is worth mentioning that [13] explicitly introduces a notion of external environment which resembles ours, even if in a quite different context. Also the semantics of interaction presented in [1] employs an explicit representation of the envi- ronment. That semantics is formulated in categorical terms and it is based on linear logic and game semantics.

We have presented a simple logic-based formalization of the behaviours of agents capable of reacting to changes occurring in the external environment. In particular we have focussed on modeling the probabilistic behaviours of agents reacting to environment perceptions with an associated probability distribution. We have shown how the availability of a formal characterisation supports effective, resource bounded, quantitative analyses of the probabilistic behaviours of reactive agents, as illustrated by the notions of probabilistic beliefs and probabilistic invariants discussed in Section 5.

