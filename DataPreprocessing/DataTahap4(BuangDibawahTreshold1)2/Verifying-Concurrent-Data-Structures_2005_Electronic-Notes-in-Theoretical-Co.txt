Concurrent implementations of data structures are designed to allow many processes to execute operations on a data structure at the same time. To prove the correctness of such implementations, we must prove that all possi- ble interleavings of the atomic steps of these operations will produce correct results. Linearisability [11] is widely accepted as the appropriate correctness criterion for concurrent data structures, but does not appear to be used widely in mechanical proofs.

The relationship between H and S can be explained more clearly by aug- menting H with linearisation points corresponding to the points at which operations in H are deemed to occur. Let X .do opp(args) denote the lineari- sation point for an operation with invocation X .opp(args), which must occur after the invocation, and before any matching response. If H ' is an augmented history obtained by adding these linearisation points to a history H , we can construct a linearisation of H from H ' as follows:

The push by process 2 precedes the pop, but neither is ordered with respect to the push by process 1, and the pop is pending. This history can be linearised in five different ways by either leaving the pop incomplete or completing the pop by adding either S.popOk2(a) or S.popOk2(b), and then linearising the push operations appropriately.

The actions from various processes can be interleaved in all possible ways that are legal with respect to the stack specification. Thus, the executions produced by the stack IOA are precisely the augmented histories described in Section 2, and are linearisable for the reasons given there. We can show that the canonical automata construction described above produces an IOA that generates exactly the linearisable histories for a given data type; see [7] for details.

The push operation creates a new node and stores the value to be pushed in its val field. It then attempts to link the new node into the list. It takes a snapshot of Head in the local variable ss, sets the next field of the new node to this value, then attempts to make Head point to the new node. This will produce the correct result only if Head has not changed since the snapshot was taken, so a CAS 9 is used to atomically test whether Head is still the same as ss and if so change it to n, in which case the operation is complete. If Head has changed, the operation loops back to line 3 and tries again.

that the stack was empty. Otherwise, pop copies the next and val fields from the node pointed to by ss, and attempts to update Head to point to the value in the next field of the node it points to. This will only produce the correct result if Head has not changed since the snapshot was taken, so a CAS is used to atomically test whether Head is still the same as ss and if so change it to ssn, in which case the operation is complete and lv can be returned as the popped value. If Head has changed, the operation loops back to line 1 and tries again.

A subtle point, not apparent in the code, is that the correctness of the algorithm relies upon the fact that a successful pop operation does not free the memory used by the popped node, since other processes might also be trying to pop that node. The algorithm would also be correct if memory was recycled by a garbage collection mechanism that only reclaims storage when there are no pointers to it, but then to claim that the implementation is lock- free, we would have to show that the garbage collector was lock-free (see [6]). Other ways of recycling memory use version numbers to detect when storage has been recycled (e.g. see [16]).

We are also interested in finding ways in which we can simplify our proofs by using constructive approaches based on refinement. This might involve identifying common steps in the construction of nonblocking algorithms, and perhaps using a different formalism such as action systems. A similar endeav- our has been undertaken by Abrial and Cansell, using Event B (see [2]).

