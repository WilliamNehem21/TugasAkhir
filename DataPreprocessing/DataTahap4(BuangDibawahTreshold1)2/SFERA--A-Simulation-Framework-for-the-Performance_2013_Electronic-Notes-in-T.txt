This paper is structured as follows: We first provide a brief introduction to the restart method on the application level. We then dicuss aspects of the evaluation of restart strategies in service-oriented systems. Sections 4 and 5 introduce the SFERA framework. Application of the framework is illustrated in section 6. Section 7 concludes the paper.

Restart addresses faults that manifest in the completion-time behaviour of tasks. In a service-oriented system, many common faults such as system failures, network faults, or system overload result in higher completion times, including an infinite completion time in the case of a failure. In evaluating restart in SOA systems we therefore focus on completion times. Restart algorithms may be evaluated at various levels of detail, including measurements in test-beds, simulation, and analytical approaches.

In the measurement-based approach, one first implements a testbed comprising clients, servers, and restart algorithms, and then measures completion times with and without restart. Tools for automatic testbed generation such as PUPPET [2] and GENESIS [7] can be employed in setting up a testbed. With this approach, realistic system completion times and timeouts can be collected. The experiments are limited by available hardware and by long runtime.

A framework for simulation models of service-oriented architectures was pre- sented in [1]. The framework combines an extended version of process chains to describe the components of SOA and quantitative specifications at higher levels. The aim of the framework is to be able to simulate SOA system architecture and the network connecting independent services in detail. The framework in [1] focusses on precise modelling of systems implementing low-level details. While restart may be evaluated using this framework, scalability to large numbers of clients is limited by the highly-detailed approach.

Sources collect completion times either as raw data files or as statistics using the objects provided by OMNeT++. The data is written to one output file per client, which enables later investigation of the data using external scripts. For performance reasons, SFERA does not utilise the built-in data storage and analysis facilities of OMNeT++.

The testbed was composed of four web services with real functionality providing a combined service. Different services accomplish diverse tasks and occupy different resources. Individual services were deployed separately on dedicated machines, con- nected by a wired network. Two experiments were conducted - with and without injected packet-loss between the client and the testbed service. Because completion times are measured in a realistic system, they build a solid basis for the evaluation of system times.

Completion times were then approximated with acyclic phase-type distributions using the PhFit phase-type fitting tool [4]. The output of PhFit is a distribution which is an accurate model of the distribution of the collected completion times. This model was used in our evaluation as models for the service-times in work servers.

simulates one parallel client. The workflow is combined from three work servers. First, two servers using service times without faults are called in parallel. Once the administrative module receives answers from both parallel work servers, the next work server is called. This work server represents a faulty service and uses a model of service times from a testbed with packet loss. We ran simulations using the Fixed Intervals, Jacobson Karn, and QEST restart algorithms. Appropriate parameters for the restart algorithms were estimated based on the completion times from the testbed: The timeout for the Fixed Intervals algorithm was set around the mean of the observed completion times. The parameters for the histogram for QEST were adjusted based on empirical quantiles and variance. The Jacobson/Karn algorithm was set to its default values [5][8].

The next step of the experiment is to evaluate statistical properties of the exper- iments with restart for both experiments. First we analyse the empirical mean of completion times through all scenarios. We see the mean as an indicator for the impact of restart algorithms on the other statistical measures.

