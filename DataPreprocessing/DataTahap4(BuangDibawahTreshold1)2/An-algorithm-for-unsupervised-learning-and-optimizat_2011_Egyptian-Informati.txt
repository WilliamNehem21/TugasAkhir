Abstract In this paper, an algorithm is proposed to integrate the unsupervised learning with the optimization of the Finite Mixture Models (FMM). While learning parameters of the FMM the proposed algorithm minimizes the mutual information among components of the FMM provided that the reduction in the likelihood of the FMM to fit the input data is minimized. The performance of the proposed algorithm is compared with the performances of other algorithms in the literature. Results show the superiority of the proposed algorithm over the other algorithms especially with data sets that are sparsely distributed or generated from overlapped clusters.

Another group of criteria for the estimation of the number of FMM components is based on the mutual information. This group includes Data Entropy that is used to evaluate different mixture models with different number of components [21]. However, this criterion may overestimate the number of components in the presence of outliers, as it is biased toward producing separated components. Another criterion in this group based on the Bayesian-Kullback Ying-Yang learning theory [22] is proposed [23]. This criterion is used in determin- ing the number of FMM components [5]. However, due to the dependency on the EM algorithm for learning mixture model parameters this criterion has the same drawbacks of the penal- ized-likelihood criteria. Therefore, this criterion produces inac- curate results with small data sets [5]. Also, an algorithm that is based on the mutual information theory is proposed [20]. However, on the opposite of the algorithms that use the penal-

In this paper, an algorithm is proposed to determine both the number of components in the FMM and its parameters for fitting an input data set that may be sparsely distributed or generated from overlapped clusters. As it learns the FMM parameters the proposed algorithm minimizes the mutual information among components of the FMM while keeping the reduction in the likelihood of this FMM to fit the input data minimum. The rest of this paper is organized as follows: Section 2 presents an algorithm that is proposed to integrate the unsupervised learning and the optimization of the FMM using data sets that may be sparsely distributed or contain overlapped clusters. Section 3 presents a comparison study of the proposed algorithm and other algorithms such as the MI, the MML, and the BIC algorithms based on their results in clustering the input data and determining the number of FMM components. Section 4 presents the conclusions and

can be deleted only if dec(r)< avg(dec(k:r + 1)) + 3std(dec (k:r + 1)), where avg and std denote the average and the standard deviation. Since the TUMI algorithm is independent of the number of mixture parameters it is less sensitive to the number of features in the input data set than the algorithms that use penalized-likelihood criteria. Therefore, it can handle sparse data sets more accurately than these algorithms. In addi- tion, tuning the mutual information theory allows the TUMI algorithm to fit data sets that are generated from overlapped

Experiments are carried out to compare the performances of the TUMI, the MI, the MML, and the BIC algorithms in clus- tering and determining the number of the FMM components. All algorithms are implemented and experiments are carried out using the MATLAB software package. Data sets used are described in Section 3.1. The method of initialization and the convergence conditions of the EM algorithm are described

The mutual information is a symmetric measure to quantify the statistical information shared between two distributions [35]. Based on this fact this measure is used to quantify how good the clustering results obtained by a clustering algorithm for a certain data set is by comparing it to the true classifica- tion of this data set [36]. Let x and y be two random variables represent the true class labels [1.. .m] for a certain data set and the cluster labels [1.. .k] resulting from a clustering algorithm

where H(X) and H(Y) denote the entropy of X and Y. The NMI has the value of 1.0 when there is a one to one mapping between the clusters obtained and the true classes (i.e., k = m) of a given data set. Since this measure is not biased toward large k, it is preferred to compare different data partitions [36,37].

algorithm is better than the performance of the MI algorithm with all data sets. This is because of many reasons; first, delet- ing from the mixture model the smallest component that has positive mutual information with the rest of the mixture com- ponents causes the model fitting to the given data set to be minimally reduced. On the other hand, the MI algorithm de- letes the component that has the maximum positive mutual information with the rest of the mixture components. This

parameters that are more likely to be biased due to the sparse distribution of the feature vectors in the data space. Although the MI algorithm is mathematically more efficient than the TUMI algorithm results show that it can be highly inaccurate, and therefore this efficiency gain can be worthless. Third, using the likelihood function to tune the TUMI algorithm allows it to estimate the number of mixture components with high accuracy when the given data set is generated from partially overlapped clusters. On the other hand, this sort of tuning is not found in the MI algorithm and therefore it tends to under- estimate the number of mixture components when the given data set is generated from partially overlapped clusters.

