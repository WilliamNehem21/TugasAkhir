Lexicon based approach, Machine learning based approach and Hybrid approach are the three main used approaches for sentiment analysis. Lexicon based approach calculates the polarity of the doc- ument from the polarity of its words using dictionaries. It tends to have a good precision, but low recall in addition to the labour work needed to build the lexicons. This approach is adopted in [2]. Machine learning based approach depends on building classifiers as Support Vector Machine (SVM), Neural Network (NN), etc. The classifiers are trained to determine the document polarity as in

Being a valuable source of information for organizations, the need to make accurate SA is an important issue. Most opinions gathered from Arab social media is in dialectal Arabic, as the use of Modern Standard Arabic (MSA) in social media is rare. Research work that handles dialectal Arabic is still in its infancy and the accuracy still needs tuning. Also dialectal Arabic is a dynamic lan- guage where new words and terms are continuously being intro- duced by new generations, besides words usages changes over time. The proposed approach is based on the hybrid method that

builds lexicons and uses them to train a machine learning based algorithm. It introduces a semi-automatic learning system that copes with the dynamic nature of the dialectal Arabic as it crawls twitter for new tweets and extends the lexicon by extracting words that do not exist in it and try to guess their polarities. Hence, the name HILATSA (Hybrid Incremental Learning approach for Arabic Tweets Sentiment Analysis). The main contribution of this work is introducing a sentiment analysis tool for Arabic tweets along with its ability to cope with the rapid change of words and their usages. As a part of the proposed approach some essential lexicons are built (words lexicon, idioms lexicon, emoticon lexicon and spe- cial intensified words lexicon). Besides, two lists including the intensification tools and the negation tools are created. All these will be available for public use to overcome the problem of lack of dialectal Arabic resources. The paper also investigated the effec- tiveness of using Levenshtein distance algorithm in SA to cope with different word forms and misspelling.

The rest of the paper is organized as follow. Section two describes briefly the related work done in the field of SA. Sec- tion three provides the details of the methodology along with the used datasets, lexicons and classifiers. Section four presents the results achieved on the different datasets and discusses them. Finally, section five provides the conclusion of the whole work.

Building a sentiment analysis tool for social media is an emer- gent task due to its wide spread usage and the valuable informa- tion that can be produced from it. While a lot of work was done on English, the research on Arabic is still in its early stages and many of the resources are not available for public use. Although some of the built lexicons could be extended, to the best of our knowledge none of them were proved to be able to cope with the dynamic nature of the language over time without the need of retraining. This section provides a quick overview of some of both Arabic and English tools.

Ibrahim et al. [5] built a system for MSA and Colloquial Arabic. They created two lexicons one for words and another one for idioms. They detected negation tools, intensifiers, wishes and questions. A Support Vector Machine (SVM) classifier was used to detect the sentence polarity.

Duwairi et al. [6] converted the emotions as fx1, fx2 to their cor- responding words. Also, they converted both dialect sentences and Franco Arab words to their corresponding MSA. Their highest accu- racy was achieved by using Naive Bayes (NB) classifier.

Zhang et al. [12] proposed a new entity-level sentiment analysis method for Twitter with no manual labelling. First of all a lexicon was created manually in order to determine the tweet sentiment polarity. Then, opinion indicators as words and tokens were extracted using Chi Square based on the lexicon. Finally, a classifier was trained to identify the tweet sentiment polarity.

Nodarakis et al. [13] proposed a distributed parallel algorithm in Spark platform. In order to avoid the intensive manual annota- tion, tweets were labelled based on the emotions and the hashtags. A Bloom filter was applied to enhance the performance after build- ing the feature vector. Finally, All k Nearest Neighbour (AkNN) queries are used to classify the tweets.

Poria et al. [15] created a seven layer deep convolutional net- work for the aspect extraction sub-task. The system tags each word in the document as aspect or not aspect. Their approach gives bet- ter accuracy than both linguistic patterns and Conditional Random Fields (CRF).

If not found, Levenshtein Distance algorithm [28] is applied to get the similar word from the lexicon. The algorithm computes the distance between the word and each word in the lexicon. The distance is computed as the number of different, missing or added letters. The word in lexicon with the minimum distance is assumed

In this work SVM, L2 Logistic Regression and Recurrent Neural Network (RNN) classifiers are used. SVM is a linear classifier which creates a model to predict the class of the given data. It solves the problem by finding a general hyperplane with the maximum margin. It supports many formulas for classifications. This work uses C-SVM as the classifier where C is the penalty or the cost of wrong classification. The cost limits the training points of one class to fall in the other side (each side of the hyperplane represents a different class) of the plane. The higher the cost, the lower the number of points will be on the other side which in some cases may lead to over-fitting issue. Also, the lower the cost the higher will be the possibility of wrong classi- fications. When the cost function is high, the classifier selects a hyperplane (boundary) with a small margin which may give a good result regarding the training data, but may cause an over- fitting issue and low accuracy regarding the testing data. On the other hand, when the cost function is low the classifier selects

Logistic Regression is a discriminative classifier. It solves the problem by extracting weighted features, in order to produce the label which maximizes the probability of event occurrence. Regu- larization produces more general models and reduces overfitting by ignoring the less general features.

Using 10-fold cross validation with ASTD, MASTD, ArSAS, GS and the Syrian Corpus training datasets the word lexicon is built (different lexicon for each fold excluding the testing part) and the classifiers are trained. This section describes the results in terms of accuracy and average F1 score (Avg. F1). In addition, com- parisons of the achieved results with other available work are also provided.

In summary, SVM tends to have better accuracy in most cases which is consistent with [30]. That is due to the kernel trick which allows a better accuracy with higher dimension data. In addition to that, it is less sensitive to outliers and less prone to overfitting. Also, 3-Class sentiment analysis is more challenging than 2-Class which is expected. In addition, in most cases F1 score is higher for balanced datasets than unbalanced datasets.

Using the dataset (ArTwitter) from [21] a classifier is trained using the same lexicons from the previous part (built from datasets other than ArTwitter). After that, the tweets are classified using 10- fold cross validation for training and testing. Then the words lear- ner is used to learn words and update the lexicon. Finally, the tweets are classified again after updating the lexicon.

Because of the wide spread of social media, a lot of data is avail- able. However, the public resources are still limited. Another issue with the resources is the change of word usage over time, word meaning differs and new words are added from time to time. Auto- matically updating the lexicon has a good impact on the system to learn new words. Still the learning process must have some restric- tions or the system may learn wrongly. Manual annotating the

