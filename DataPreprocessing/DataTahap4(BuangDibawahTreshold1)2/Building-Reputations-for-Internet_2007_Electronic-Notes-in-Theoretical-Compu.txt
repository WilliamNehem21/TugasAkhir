A key insight behind client reputations is that a given host tends to be well- administered or poorly-administered over a considerable time, and that hosts that have behaved maliciously in the past warrant a lower trust since they are likely to misbehave in the future. A study of host scanning patterns [16] supports this assumption. It revealed that top 1024 scanners are responsible for 90% of all scans and that large scanners persist over a considerably long time. Blacklisting these repeat offenders would dramatically reduce scan traffic in the Internet. If each server used its own observations to assemble a list of good or bad clients, this list would be stale and incomplete because an average server communicates with a moderately large, slowly-changing circle of clients. Building a collective memory of client behavior by combining observations from multiple servers through reputations has a clear advantage over this isolated approach.

This is the first paper that provides a systematic overview of differences between provider and client reputations. We survey challenges that are unique to client reputation systems, assuming a realistic adversary model and an open reputation system where any entity can participate. We also discuss two different architectures for collection of client behavior information that is needed for reputation building: a reporter model, where servers rank their experiences with clients, and a monitor model where independent observers rank client behavior using observations of its traffic patterns. Finally, we show how a combination of reporter and monitor models can overcome major threats to reputation validity.

malicious activity in the con field. We assume that this field is standardized and specifies the type of a security incident (scan, worm, DDoS, anomaly) and some type-specific details, e.g., the range of scanned ports, the port targeted by a worm, the rate, the duration and the type of a DDoS attack, etc. The info-item i is anonymized by the reputation center a follows:

Definition 2.1 Adversary model : We assume that an attacker can compromise no more than M other machines, and organize them into a botnet. We will refer to these compromised machines, controlled by one attacker, as bots. Given that botnets of 500, 000 nodes have been discovered [10], M could be very large. An attacker can use bots to perform malicious activities in any chosen engagement pattern, and to fully participate in the reputation system by submitting info-items about bots and about other clients.

by engaging in legitimate transactions. For these reasons, we conclude that good info-items are meaningless and eliminate them from further discussion. We will assume that a client is good if no bad info-item was filed against it, as proposed in [1].

We now list several definitions that set foundations for a client reputation system. For simplicity, we only consider a system that prevents interaction with clients who have participated in scanning, DDoS or worm incidents. This model could be easily extended to penalize other malicious behaviors.

Definition 3.2 Current-bad client is a client that has been an object of a bad info- item at least once in the recent Tint seconds. Similarly, current-good client has not been an object of a bad info-item during the recent Tint seconds.

Definition 3.4 Reputation use. Client reputations are used in the following man- ner: (1) During DDoS attacks and worm spread incidents long-term reputations are used to identify bad clients. Traffic to a DDoS victim or to a worm probe port is prioritized, serving first good clients and then serving or dropping traffic from bad clients. (2) Short-term reputations are used during normal operation to identify current-good and current-bad clients. All traffic from current-bad clients is dropped. Traffic from current-good clients is accepted.

Higher price of a false positive. In a provider reputation model, reputation users are the clients whose goal is to find at least one trustworthy provider. If a provider is mistakenly assigned a low reputation, clients can migrate towards high-reputation providers so client satisfaction remains high. In a client reputation model, service providers are reputation users. Their security goal to avoid malicious clients conflicts with their business goal to serve as many clients as they can. If a client is mistakenly assigned a low reputation score this leads to business loss for providers, so the price of a false positive is high.

Definition 5.1 Report generation: We assume that a server submits a report after an interaction with a client. Only bad reports are submitted if the interaction was malicious, i.e., if the client has sent a scan, a worm probe or participated in a DDoS attack on the server.

Challenge 3: An attacker can fabricate a report from a given server. Coun- termeasure: A fabricated report will not be verifiable and will be rejected by the reputation system. Because we do not evaluate credibility of information sources via voting, fabricated reports cannot lower credibility of an alleged server. However, if we enforce a quota on the number of reports accepted from a given server in some interval, to control the load of the reputation centers, report fabrication could lead to a DoS attack on the server whose identity was stolen. To prevent this, each server must share a secret with the reputation centers and use this secret to sign its reports.

promised. Since anomalous behavior does not necessarily signal maliciousness, a prolonged anomalous behavior leads to generation of a suspicious info-item. Sus- picious info-items are similar to bad info-items; they carry a context field that provides more details about the observed anomaly (e.g., client contacted new des- tinations on a new service port, client started sending large traffic volume, client has been contacted by another bad client and possibly compromised). We will call a client that has been an object of at least one suspicious info-item, and no bad info-items, a suspicious client. Reputation users can devise their own policy how to treat suspicious clients during various security incidents, and based on the context of suspicious info-items.

client profiles, and they have to build profiles and detect anomalies online. Coun- termeasure: Monitors should only profile clients that produce sufficient traffic to warrant building a profile. Profiles of clients with similar behaviors should be com- bined to save space. Our previous research on host clustering using behavior profiles

[15] has shown that many hosts behave similarly and can be grouped into large and representative clusters. Host clusters can be used for anomaly detection, while sav- ing memory. It is also possible to build quality profiles from sampled traffic, which can reduce per-packet processing overhead. [15]

termeasure: Core-based filtering techniques, such as [11,8,5], can be used to filter out a majority of spoofed packets. Approaches such as using traffic from established TCP connections for observations are not applicable because a monitor may reside on asymmetric traffic paths and may not be able to detect established connections. The monitor model is thus more vulnerable to spoofing than the reporter model. In section 7.2 we discuss how a combination of these two models can lead to a strong

Reports must be aggregated into a reputation score, either by the reputation system or by reputation users. We now propose an aggregation method assuming the reporter-monitor model and that only bad and suspicious reports are generated. Bad reports that can be elicited via spoofed traffic will have a no-spoofing flag reset. Our aggregation rules ensure that such reports facilitate traffic prioritization during congestion events such as worm spread and DDoS attacks, but they cannot harm good clients during normal operation.

During normal operation calculate short-term reputations using observations dur- ing last Tint seconds only, and the formula 2. Accept traffic from clients with pos- itive reputations. Use context information from reports to make a policy decision about suspicious clients and clients with small negative reputations (e.g., if a con- text indicates a recent communication with a compromised client we may want to reject this communication). Reject traffic from clients with a large negative reputation.

The rest of the reputation system contains reporters, verifiers and reputation centers, and can be organized in a distributed manner. Servers would thus contact local reputation centers to submit reports and retrieve reputations, while reputation centers would periodically talk to exchange recent reports. Reputation centers can be organized into a peer-to-peer network to facilitate report exchange and existing approaches from provider reputation systems can be applied to ensure that com- promise of a reputation center cannot jeopardize credibility of client reputations.

Reputation scores or recent anonymized reports can be periodically downloaded by reputation users. These downloads can be scheduled to minimize congestion and they can be retrieved from a local reputation center. Since a client is presumed good in absence of bad reports, reputation users need only store identities of bad clients. Given that the largest botnet up to date contained half a million bots [10], we estimate the cost of such storage to several million records.

The reputation system must be protected from report and reputation fabrication through standard cryptographic means. Each reporter must exchange a shared or a public key with its local reputation centers and sign each report with this key. A reputation center must sign each reputation update with its private key and local servers must possess the corresponding public key to verify the signature. It is likely that an extensive key exchange infrastructure would be needed to support secure operation of the client reputation system. Existing key exchange mechanisms, proposed for provider reputation systems, can be reused in this novel context.

In [7], Hou et al. compute peer reputations based on satisfactory and unsatis- factory transactions; this last category is similar to the bad reports in our system. Authors do not consider cheating which is a major threat in the open client rep- utation system we proposed, and they assume that every peer rates every other peer, while in our system reporters cannot be rated through voting due to a strong adversary model.

In [3] authors propose a reputation system integrated with the Gnutella P2P net- work where participants use peer votes to discover trustworthy resource providers. Authors consider cheating via creation of false identities, and propose to discover clusters of such identities via grouping peers by their IP address. Proposed mech- anisms do not apply to our attacker model, which enables bots to vote using their real identities. In [4] authors extend the peer reputation system with resource rep- utations. This helps establishment of more reliable provider reputations but is not directly applicable to client reputation systems.

We have proposed a client reputation system that could be used to reduce unwanted traffic in the Internet. Such a system faces a unique set of challenges that we have surveyed in this paper and proposed countermeasures. While much work remains to be done in designing a practical, secure and usable client reputation system, we believe that our proposed reporter-monitor model has provided a good first step in this direction.

