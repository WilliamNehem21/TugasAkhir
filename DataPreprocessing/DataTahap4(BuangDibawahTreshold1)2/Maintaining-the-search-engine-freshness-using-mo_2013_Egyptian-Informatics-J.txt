The rest of the paper is organized as follows. Section 2 pro- vides an overview of centralized or traditional search engine design. The related work is discussed in Section 3. Section 4 de- scribes the proposed architecture of the distributed web crawl- ing and indexing system (DWCIS) and its work flow. Section 5 describes the experimental setup and Section 6 shows the experimental results and discussion. Finally, we conclude in Section 7.

Spiders: also referred to as web robots, or crawlers, are the programs behind a search engine that retrieve web pages by recursively following URL links (Uniform Resource Loca- tor) in pages using standard Hyper Text Transfer Protocol (HTTP). First, the spiders read from a list of starting-seed URLs and download the documents at these URLs. Each downloaded page is processed, and the URLs contained within it are extracted and added to the queue. Each spider then selects the next URL from the queue and continues the process until a satisfactory number of documents is down- loaded or local computing resources are exhausted. To improve speed, spiders usually connect simultaneously to multiple web servers in order to download documents in parallel, either using multiple threads of execution or asyn- chronous input/output.

Web page repository: Documents retrieved by the spiders are stored in a repository of web pages. To reduce the needed storage space, the pages are often compressed before being stored. The repository is usually in the form of a data- base, but it is also common for small-scale search engines to simply store the documents as files.

Indexers: An indexer processes the pages in the repository and builds an underlying index of the search engine. The indexer tokenizes each page into words and records the occurrence of each word in the page. The indexer is also used to calculate scores such as the term and document fre- quencies of each word, which can be used for search result rankings.

Query engines: A query engine accepts search queries from users and performs searches on the indexes. After retrieving search results from the indexes, the query engine is also responsible for ranking the search results according to con- tent analysis and link analysis scores. It is also responsible for generating a summary for each search result, often based on the web page repository. The query engine in some search engines is also responsible for caching the results of popular search queries. After all the processing, the query engine generates and renders a search result HTML page and sends it to the user interface.

These authors [7,10,11] have initiated the concept of mobile crawling by processing the use of mobile agents as the crawling units. The proposed concept surpasses the centralized architec- ture of the current web crawling systems by distributing the data retrieval process across the network. Mobile crawlers are able to perform remote operations such as data analysis and data compression at the data source before the data is transmitted over the network. However their systems ignore the distributed indexing, index updating and web page change detection.

Yadav et al. [13,14] have proposed checksum (hash value) based content level change detection. At the time of page crawling, only comparison will be made to the text code of that page. The main drawback of this technique is that if any change in that value is detected for the actual copy on the web as compared to the local copy, regardless it is significant or not, the page will be refreshed or re-crawled. Hence this technique results in network overload and wasted resources for the search engine.

Artail and Abi-Aad [15] have proposed a web page change detection approach based on restricting the similarity compu- tations between two versions of a given web page to the nodes with the same HTML tag type. Before performing the similar- ity computations, the HTML web page is transformed into an XML-like structure in which a node corresponds to an open- closed HTML tag. This tree structure uses a lot of storage space as well as causes a lot of inconvenience at time of refresh, as the tree structure has to be compared. Also this approach works only on the page types that can be transformed into an XML-like structure such as HTML pages.

Bal et al. [16,17] have proposed a novel indexing system based on mobile agents, which can filter out the HTML pages that have not been modified since last crawl through two web page change detection methods. The first method is the com- parison of page sizes of web pages at the time of page change detection. The second one uses the last modification date of web pages. These methods have the same drawback like the hashing method in above discussed related work as any insig- nificant change will change both the page size as well as its last modification date. This leads to overloading the search engine with processing web pages that will not change the index.

detection techniques that depend on page size, last modifica- tion date and hash value. Second, the document index of chan- ged pages is returned to the search engine instead of the pages themselves. Therefore, it reduces the network load results from crawling. Third, it reduces the computational load at the search engine side because the document indices of the web sites are already created at the web server and the search en- gine has to only create the inverted indices.

In particular, it provides state-of-the-art indexing and re- trieval functionalities, and supports the rapid development and evaluation of large-scale retrieval applications. Terrier is implemented in Java and was used to create both document in- dex and inverted index. We compressed the collection using WinZIP compression technique, and found that HTML docu- ments are compressed approximately 70%.

The goal of our performance evaluation is firstly, to establish the superiority of our change detection technique based on doc- ument index over other studied change detection techniques, like Last modification date (LMD), page size by [6,18] and hash value by [13,14]. Secondly to establish the superiority of the dis- tributed indexing approach against the currently centralized indexing approach, we measure performance in terms of the size of data transmitted across the network. As a data set for our experiments we used the Java programming tutorial as mentioned before in last section which is a set of 656 HTML pages (about 4.95 MB total). It is possible to reduce the net- work traffic further by compressing the pages before sending them to the Client Site (search engine). The HTML page can be compressed up to 30% of the actual size by using standard compressing tool such as WINZIP.

Mobile crawler using LMD (MC1): It is a migrating crawler based on Last modification date change detection tech- nique. It migrates to the web server and filters the non- modified pages using the comparison of last modification dates at the time of page change detection. It sends back only to the search engine the modified web pages compressed.

Mobile crawler withpage size (MC2): It is a mobile crawler based on page size change detection technique. It migrates to the web server and filters the non-modified pages using the comparison of page sizes at the time of page change detection. It sends back only to the search engine the mod- ified web pages compressed.

Mobile crawler with hash value (MC3): It is a migrating crawler based on hash value change detection technique. It migrates to the web server and filters the non-modified pages using the comparison of hash values at the time of page change detection. It sends back only to the search engine the modified web pages compressed. A hash function is any algorithm or subroutine that maps large message of variable length, called keys, to smaller string of a fixed length. The values returned by a hash function are called hash values, hash codes, hash sums, checksums or simply hashes.

Mobile crawler with document index 2 (MC5): It is a mobile crawler based on document index change detection tech- nique. It migrates to the web server and filters the non- modified pages using the comparison of document indices at the time of page change detection. It sends back only to the search engine the document indices of the modified web pages compressed. This crawler achieves the maximum reduction in the computational load on the search engine side because the document indices are already created. All what the search engine has to do afterwards is to collect the document indices from all the participating web sites and create the inverted index used for query-based search.

Sixth cycle (Non-Significant change cycle): In order to sim- ulate the case of insignificant page change, we change the 10 HTML web pages, added at the fourth cycle. The changes are divided into layout or structure change (e.g., changes in the position of HTML elements in the page, changes in comments), and attributes change (e.g., changes in fonts size and color, image size).

In case of page deletion, the migrating crawlers returned back to the search engine side with an ACL message containing the names of the deleted pages so that the manager agent can dis- card them when creating the document indices or inverted index. The size of this ACL message is negligible compared to the size of the web contents. In case of significant content change, we do not observe difference between MC1, MC2, MC3 and MC4 (LMD, Page Size, Hash Value and Docu- ment Index 1 based crawlers); they transferred 32.8 KB which is the size of the ten HTML pages compressed;

