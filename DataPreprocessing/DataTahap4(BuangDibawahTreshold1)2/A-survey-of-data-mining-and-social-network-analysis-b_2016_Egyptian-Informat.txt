Abstract With the increasing trend of online social networks in different domains, social network analysis has recently become the center of research. Online Social Networks (OSNs) have fetched the interest of researchers for their analysis of usage as well as detection of abnormal activities. Anomalous activities in social networks represent unusual and illegal activities exhibiting different behaviors than others present in the same structure. This paper discusses different types of anoma- lies and their novel categorization based on various characteristics. A review of number of tech- niques for preventing and detecting anomalies along with underlying assumptions and reasons for the presence of such anomalies is covered in this paper. The paper presents a review of number of data mining approaches used to detect anomalies. A special reference is made to the analysis of social network centric anomaly detection techniques which are broadly classified as behavior based, structure based and spectral based. Each one of this classification further incorporates number of techniques which are discussed in the paper. The paper has been concluded with different future directions and areas of research that could be addressed and worked upon.

Degrees in 1997 [3], Online Social Networks such as Twitter, LinkedIn and Facebook have attracted large number of peo- ple. At present, almost every domain is linked in one form or the other with the social networks. Be it entertainment, edu- cation, trading, business, communication etc., OSN has made an influence on each of them. For example, mostly companies have started promoting their brands and products on social networking sites to increase the popularity of their products which in turn enhances their sales [4].

The presence of anomalies in our data poses many prob- lems which need to be tackled carefully. For example, some sort of malicious users may construct a set of false identities and use them to communicate with a large random set of inno- cent users [17]. Hence, detection of these anomalous activities in a network is a big concern as their presence may lead to heavy losses. For example, in a computer network an anoma- lous traffic pattern could mean that a hacked computer is send- ing out sensitive data to an unauthorized destination [12]. Nowadays, not only the detection but the reason why these activities took place along with the methods to prevent these behaviors is on the rise. Here in this paper, various techniques used to detect and handle the anomalous behavior are covered. At first, a generalized view of various data mining techniques applicable to multiple domains and applications is given and then a special reference is given to some of the popular anom- aly detection methods applicable to social networks.

The paper is organized into different sections. Section 2 contains the novel categorization of anomalies on the basis of number of parameters. The major data mining and social network techniques for anomaly detection have been discussed in Sections 3 and 4 respectively. Finally, Section 5 presents conclusion along with some future directions that could be addressed.

Behavior attributes: Characteristics of an object are defined using these attributes and in a way help to identify the anomalous behavior of an object with respect to its context. In the temperature example, temperature, humidity etc. can be considered as behavior attributes.

[18] has evolved in social networks which depict the presence of anomalies based upon the different sources of data avail- able. For example, the same user may be present in different communities on different social networks. Similarly, a user may have similar kinds of friends on number of social net- works (e.g. Facebook, Google+) but completely different kinds of friends for another social network (e.g. Twitter). This depicts an unusual activity which can be considered as anomalous.

Unlabeled anomalies are related only to the network structure. No attribute of a node or an edge is taken into consideration. Their classification is mostly studied as follows and differ- ent techniques have been developed and deployed to detect these types of anomalies. A number of such techniques have

Static labeled anomalies are used in spam detection, for example, to detect opinion spam (which involves the fake pro- duct reviews). A set of hidden labels are usually assigned to the vertices and edges which are iteratively updated. In the pro- duct review system, a bipartite graph with one subset of ver- tices as users and other as products is taken in which the edges between the subsets represent the product reviews. Hid- den labels are assigned to both users and products. For users the label can be in the form of honest or fraudulent and for the products it could be either good or bad. A normal honest user will give accurate results i.e. for good products they give positive response and for bad ones they will give negative reviews whereas fraudulent users are understood to do the reverse.

This type of anomaly arises when we have dynamic net- works that change with time. Behavior of the data object is dif- ferent with respect to previous time period relative to the network structure. For example while considering only the pat- tern of interactions, there are maximum of six ways in which a maximal clique can evolve: shrinking, growing, splitting, merg- ing, appearing or vanishing [19]. All of these involve studying the network structure with respect to the network structure prevalent at some previous time period. Sometimes, the nor- mal behavior does not result in any network change; then, any neighborhood changes may also predict an anomalous behavior.

It arises when one data object deviates significantly from other observations resembling the basic anomaly definition. For example, while examining the student record, if a record is found where height of a student is entered as 56 ft, which is impossible, then it is taken as a white crow anomaly. These anomalies are mostly detected as particular nodes, edges, or subgraphs representing the abnormal behavior.

A data set or a network may contain more than one kind of anomaly. Some of these anomalies can be clubbed together to form a hybrid set. As an example, Savage et al. [13] studied the classification of anomalies as a combination of static/dynamic and labeled/unlabeled.

Deletion: Deletion involves the absence of an expected ver- tex or an edge. Sometimes, it even incorporates the concept of dangling edges i.e. with the deletion of a particular vertex all the adjacent edges to it may also have been deleted.

The major task involved in classification approach of super- vised methods is to make the classifier learn. A classifier can be constructed in numerous ways. For example, it can be neural network based [22,23], support vector machine [15,16,24], Bayesian network based [25,26] etc. Supervised anomaly detec- tion methods should keep in consideration the following two aspects:

mal are available. Using the small amount of labeled data a classifier can be constructed which then tries to label the unlabeled data. Hence, a model for normal data objects is built which is used to detect the anomalies in a way that the objects not fitting the normal model are classified as anomalies. This is the simplest approach called self-training used under semi-supervised model. Another method called as co-training can be employed where two or more classifiers train each other. Self-training is more sensitive to errors than co-training. It is known as semi-supervised as it partially functions as supervised methods because only the normal class is taught and the algorithm learns to identify anomalies by itself.

Proximity and distance terms used to represent similarity and dissimilarity respectively are the key approaches used for detection of anomalies in any network. Proximity based anom- aly detection approaches analyze each object with respect to its neighbors. It is assumed that normal data objects have a close proximity toward their neighbors i.e. they follow a dense neighborhood pattern whereas anomalous objects lie far away from their nearest neighbors. A number of k-nearest neighbor methods can be used which make use of various measures such as distance, density and other similarity measures to determine the proximity between the nodes. These proximity measures determine the efficiency of the methods. Proximity based meth- ods can be mainly classified into the following two categories:

For higher dimensions the approach can be improved using Hilbert space filling curve. The multi-dimensional space in grid based approach is extended by Angiulli and Pizzuti [29] to han- dle the high dimensional data more efficiently. Hilbert space filling curve is used along with HilOut algorithm, an algorithm defined to choose the anomalies based on their aggregate score with their neighbors rather than one absolute score. For each object o, weight w, is computed as [14]:

As the dimensionality increases the question about why and up to what extent the data object is an anomaly is of more con- cern rather than just predicting out anomalies. One of the sim- plest approaches toward it is to compute sparsity coefficient. The more negative its value is, sparser a cell (hypercube) is and more likely the objects in C are anomalies.

The major problem associated with distance based methods is its failure to detect local anomalies which can be easily over- come by density based methods. Density based approaches work by comparing the density of an object with density around its neighbors. For a normal object both densities are assumed to be same whereas for anomalous objects they are different. The concept of relative density is often used to mea- sure the degree of anomalous behavior of an object.

In order to make the approaches computationally less expensive some sort of statistical measures are added to them. For example, instead of using the densities as it is the compu- tation of standard deviation of densities led toward an approach named as Multi-granularity Deviation Factor (MDEF) as suggested by Papadimitriou et al. [36]. Similarly, Local Outlier Probability (LoOP) method [37] also makes use of the statistical measures and estimates the probabilistic LOF as a factor of ratio of densities to finally compute the measure called as LoOP.

As stated by Berkhin [38] clustering is considered as an unsu- pervised learning of a hidden data concept. Clusters of the data objects can be constructed using numerous methods such as, K-Means, K-Medoids for small data sets and CLARA [39], CLARANS [40] for large data sets and BIRCH [41], Chame- leon [42] for performing macro clustering on micro clusters. Cluster based methods follow a simple assumption that usually anomalies either belong to a small sparse cluster or do not belong to any cluster whereas the normal objects are a part of large and dense clusters. So, cluster based anomaly detec- tion approaches state the presence of anomaly in the following three cases:

For this case, simply the density based clustering approaches can be used simplest of which is the DBSCAN and its numer- ous variants. DBSCAN [43] checks the density around each object and the one being isolated or of lower density than others is considered as an anomaly. One of the striking fea- tures of this method is that it can detect the clusters of arbitrar- ily any shape. A number of improved variants of DBSCAN such as, FDBSCAN [44], L-DBSCAN [45], C-DBSCAN [46],

[51] etc. Out of all such measures the prominent one is the Shared Nearest Neighbor (SNN) method in which the similar- ity between the data points is identified based upon the number of nearest neighbors shared and hence the core points around which the clusters are to be built are identified. This approach helps to identify the dense as well as medium and sparse clusters.

No doubt, the proposed methods help to identify the anoma- lies but they focus more toward finding the clusters and consid- ering any point not related to any cluster as noise which in a way is assumed to be anomalous. Some of the cluster based methods also avoid finding the degree of anonymous behavior shown by each data object. In order to encounter such prob- lems, numerous advanced approaches have been proposed. For example, Cluster based Local outlier factor (CBLOF)

[52] and the corresponding algorithm FindCBLOF are used to mine the encountered anomalies. CBLOF is measured as a factor of both the cluster size to which object belongs and its distance from the cluster it is closest to. FindCBLOF uses the Squeezer algorithm which constructs the clusters out of which a set of large and small clusters are formed and CBLOF is calculated for every data point. In a similar fashion a num- ber of other techniques using different distance measures have also been proposed like, Self-organizing maps (SOM) an unsu- pervised method proposed by Kohonen [53], k-means cluster- ing [54,55], k-means++ [56,57]. As these techniques involve the computation of distance factor, therefore, they are a good way to handle the second case. Some of the semi-supervised methods proposed by Wu and Zhang [58], Vinueza and Grudic

This case is handled by defining a threshold value for the clusters and the objects belonging to low value clusters are considered as anomalous. FindCBLOF algorithm detects both the individual objects and points belonging to small clusters as anomalous by computing the similarity between the objects in the small cluster and the closest large cluster. CBLOF value for such points comes out to be very low. Apart from this, other applicable approaches are described in [52,60,61]. Distance or densities of the small clusters gen- erated are compared with those of large clusters and anoma- lies are detected. Numerous efficient techniques such as k-d trees and CD-trees are used to partition the data into clusters.

Along with the above techniques concepts of local cluster- ing, co-clustering, bi-clustering and subspace clustering have been used by Beutel et al. [62] to select the set of attackers in social network domain. In subspace clustering a cluster is defined as a subset of data objects which are similar to one another in terms of the above defined similarity measures such as distance, density or other such variants for a particular sub- space. For example, one of the subspace clustering algorithms, CLustering In QUEst (CLIQUE) proposed by Chang and Jin

In one class model, only a single labeled class is defined i.e. classifier is constructed to only define the normal class and all those data objects that belong to that class are treated as nor- mal whereas the ones that do not fit in the defined class are treated as anomalies. Some of the examples of one class models used for anomaly detection are one-class SVM [64], Gaussian model description (GAUSSD) [65], Principal component anal- ysis description (PCAD) [66], Parzen window classifier (PWC)

[67] etc. In each of them a decision boundary is set up. The data objects falling outside the decision boundary are treated as anomalous. One-class models help to detect new anomalous objects that are far from the other anomalous objects present in the given training set.

The other set of model called the multiclass model is used when the available data objects not only belong to a single class but to multiple classes. For example, the classification of a set of images of fruits into the probable classes of apples, oranges or mangoes. Every data object may be assigned only one label. Just like, a fruit may be classified as either one out of the three categories but not more than one at the same time.

Kruegel et al. [69] studied the detection of anomalous behavior using Bayesian classifiers. Using this approach, all the previously unknown attacks are also identified but with the generation of a number of false positives. A model of the normal behavior is available and deviations from these behav- iors are identified as anomalies. There are a number of models which evaluate different set of features and return different probabilistic values as anomalous scores which are aggregated into a single value. But in every such model the final decision process is conducted using a Bayesian classifier.

It makes use of a hyperplane as a decision boundary to sepa- rate the tuples of different classes from one another. The major task involved in SVM is the selection of best separating hyper- plane from among several of them. An approach toward this issue is the use of maximum marginal hyperplane (MMH) i.e. the one with largest margin is considered most accurate for classification. SVM as a classification measure can be used to detect the anomalies in various applications.

cases, pruning of the networks to include only most relevant relationships is done [83]. In most of the cases, the presence of an anomaly is considered as a binary property in which anomaly is either present or not, but in some applications the extent to which anomaly is present is considered by giving degree of being an outlier to each object in the data set. For example, this degree has been referred to as Local Outlier fac- tor (LOF) by Breunig et al. [33].

Structure context-based similarity: It is a local cluster or neighborhood based similarity in a way that nodes having similar neighborhood are considered as similar. For exam- ple, in social networks, different users getting recommenda- tion about a page or a community etc. from a number of mutual friends usually make similar decisions and help in determining how close they are.

Similarity based on random walks: This type of similarity could be well described by this example. Suppose, an information or message needs to be forwarded to multiple users. But at an initial stage it is sent to only two users A and B who forward it to their friends. Now, the closeness or similarity could be measured by the simultaneous receipt of the message from both A and B to the nodes. So, here similarity is addressed as a random walk measure over the network.

cluster building [89]. The proposed technique is a faster and efficient way to identify fake accounts as it only uses the attri- butes entered by a user during registration i.e. profile creation. The employed technique is a first in its form to detect the clus- ters of fake accounts usually created by a single user on a par- ticular social network, thereby superseding the existing techniques which only work and make deduction for a single account. The system was found to restrict around 2,50,000 fake accounts.

The structural properties have been used by most of the researchers working in social network domain to define a num- ber of new approaches for identifying anomalies in online social networks. As an example, Link mining, used by Getoor and Diehl [90] studies the structural properties of the networks to predict different behaviors of individuals in social networks. For instance, a normal tendency shows that consumers, whose friends spend a lot, spend a lot themselves. The concept of link analysis is applicable for both homogeneous and heteroge- neous networks, but in the concerned work the graphical struc- ture of heterogeneous networks with different types of nodes or edges is given more focus. By analyzing the association between different nodes it is usually found that the linked objects often have a set of correlated attributes. In other words, connectivity of two users can be checked by examining the common properties as what is usually observed is that the objects sharing some sort of common features are often found to be linked with each other. Getoor and Diehl [90] covered eight link mining tasks with their respective algorithms and grouped the defined tasks under three categories, namely object-related, link-related and graph-related. Most of the structure based link prediction methods show poor perfor- mance because of the involvement of prediction of future rela- tionships likely to occur [91]. Earlier also a number of advanced tasks such as anomalous link discovery (ALD) were proposed which involved only the prediction of anomalous relationships rather than all the involved relationships [92]. It was seen that almost every prediction model performed quite well for ALD.

Many already existing node-based and egonet-based fea- tures were studied recursively by Henderson et al., [94]. Some aggregate values were calculated on the already existing char- acteristics. Neighborhood information was retrieved using both node and egonet-based features and behavioral informa- tion was extracted using recursive features.

Akoglu et al. [6] utilized another structure based approach in which a number of pattern and law discoveries were used by to detect different types of anomalies in social network graph. To spot some strange nodes especially in weighted graphs an Oddball algorithm was proposed in which a number of new rules (power laws) were discovered to detect the deviation from the known normal behavior. A set of features were grouped into certain set of carefully chosen pairs and anomalous behav- ior was analyzed by examining the group structure. Groups were formed where the patterns of normal behavior (power laws) were observed and the points deviated from discovered patterns were flagged out to be considered as anomalous. A number of anomalous relationships were observed namely Near Stars or Near Cliques, Heavy Vicinities and Dominant Edges.

The concept of betweenness centrality formulated by Free- man [100] is modified to work for edges instead of vertices to find the number of shortest paths between a set of vertices that pass through the edge under consideration. The implication used is that the edges with high value of betweenness centrality state the points where a network is expected to break and hence are separated. Generally, in online social networks high betweenness centrality is found to be at the intersection of den- sely connected network groups. As a result, a number of signif- icant groups could be determined by removing the set of links from a graph, a concept also used by Newman [101].

randomness tests are applied which involves the computation of different non-random measures specially the non- randomness for nodes. This non-randomness characteristic is used by a popular algorithm known as SPCTRA to identify the anomalous users. A number of different subgraphs are cre- ated where attackers or anomalous nodes are likely to have dense subgraphs. From these subgraphs, a set of nodes are chosen for RLA groups. Finally, all the dense subgraphs formed by regular nodes are removed and hence, the only ones left are the subgraphs of attacking nodes. The proposed approach supersedes the previous approaches because of the effectiveness of spectral characteristics.

For example, Savage et al. [13] surveyed on different tech- niques applicable for each of the static/dynamic unlabeled/ labeled anomalies. Like the various techniques discussed in structure based approaches are used to identify static unlabeled anomalies. In a similar fashion, for detecting dynamic unlabeled anomalies apart from link prediction, other techniques such as Bayesian analysis and scan statistical approaches (mainly applicable to hypergraphs) are used with each approach hav- ing its own application and benefits. In case of labeled anoma- lies, a number of techniques have been proposed for static as well as dynamic networks. A number of approaches were dis- cussed in the survey paper for static labeled anomalies such as contextual anomalies, heavy vicinities, and opinion spam. As an example, for the detection of opinion spam a belief propa- gation method has been applied which deals with a set of hid- den labels. One more approach called Trust Rank was discussed that follows a link analysis perspective in which it is assumed that good nodes would never point to bad nodes. Two basic principles followed are as follows:

as well. The only constraint imposed is the addition of infor- mation regarding the attributes. In most of the approaches the network structure is considered as static for a fixed time period and in order to add the dynamic concept the behavior of different nodes/modules is compared at different time inter- vals. Signal processing works on such principles by using the probabilistic features.

Similarly, Akoglu et al. [103] gave a survey of different graph based anomaly detection methods covering both the static/dynamic and labeled/unlabeled constraints. In each net- work structure, different quantitative and qualitative tech- niques have been very well categorized into different sub modules such as structure based, window based, community based and feature based. Moreover, researchers have described a number of real-world applications where graph based anom- aly detection methods could be fit, for example, opinion spams, auction networks, social networks, telecommunication networks, trading networks, cyber crimes, security networks to name a few.

Recently, there has been an inclination toward detecting anomalies in dynamic networks. Therefore, a number of researchers are adding dynamic concept into their research work. For example, a number of anomaly detection techniques specially related to dynamic networks are recently surveyed by Ranshous et al. [104]. For instance, a scoring function is used to identify various types of anomalies. Categorization of anomalous behavior is based upon the scoring function being used along with the application area under consideration.

Also, the most significant and pertinent subset of nodes is used by Vigliotti and Hankin [105] to detect anomalous pat- terns in huge dynamic networks. In their work the experiments were performed on the temporal networks. Temporal informa- tion from two data sets namely VAST data set (2008) and Twitter data set was taken. In VAST data set, the telephonic calls among different nodes are examined. Also, the already available techniques are used to predefine an anomalous pat- tern and the projected approach is just validated over the working data set. But for the Twitter network being used no prior knowledge pertaining to anomalous patterns is already known, anomalous patterns and nodes need to be assumed and it has to be tested whether the stated hypothesis regarding anomalous or non-anomalous behavior is true or not.

But Gao et al. [107] proposed an advancement in the above approach by integrating both the network and data object information to detect the community anomalies. The proposed approach is called Community Outlier Detection algorithm (CODA) which makes use of a probabilistic mixture model designed for multivariate data objects (objects with multiple attributes). Statistical anomaly detection approaches were used to detect the community anomalies in which depending upon the type of data associated, different distributions were ana- lyzed where normal data objects were assumed to follow the defined distribution whereas anomalous objects deviate from it or follow some other distributions. In the proposed tech- nique, two types of data objects were used- continuous data and text data and for normal behavior they were found to fol- low Gaussian and multinomial distribution respectively. It was found that any encountered anomaly followed a uniform dis- tribution. A set of hidden variables for data objects and Hid- den Markov Random Field (HMRF) for the network links are worked upon by the defined ICM and EM based algo- rithms. In order to make it more effective a set of hyper graph parameters like, threshold (indicating few anomalies for its high value and more anomalies for the low value), link impor- tance (for the prediction of confidence level), number of com- ponents (small determining global anomalies and large the local ones) were also defined and used.

The paper is structured into five major sections. Section 1 described the importance and growing trend toward social net- works along with the presence of anomalous activities in it. A set of widely accepted formal definitions of anomaly have been tabulated. Section 2 classified the anomalies into various cate- gories based upon different parameters. Finally, Sections 3 and

Kisilevich S, Mansmann F, Keim D. P-DBSCAN: a density based clustering algorithm for exploration and analysis of attractive areas using collections of geo-tagged photos. In: Proceedings of the first international conference and exhibition on computing for geospatial research & application; 2010. p. 38.

