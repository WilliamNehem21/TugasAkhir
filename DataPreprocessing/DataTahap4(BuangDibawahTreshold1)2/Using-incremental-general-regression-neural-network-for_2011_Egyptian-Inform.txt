Abstract Finite mixture models (FMM) is a well-known pattern recognition method, in which parameters are commonly determined from complete data using the Expectation Maximization (EM) algorithm. In this paper, a new algorithm is proposed to determine FMM parameters from incomplete data. Compared with a modified EM algorithm that is proposed earlier the proposed algorithm has better performance than the modified EM algorithm when the dimensions containing missing values are at least moderately correlated with some of the complete dimensions.

The modified EM algorithm [23] is proposed for determining parameters of a mixture of multivariate normal distribution from incomplete data provided that missing values are Missing At Random (MAR) [21]. We refer to this algorithm as the EMH algorithm in the rest of this paper. The EMH algorithm is described as follows.

new proposed Data Augmentation (DA) computational algorithm for learning normal mixture models when the data are missing at random [24]. Experimental results show that DA imputation has considerable promising accuracy in the prediction of missing values when compared to the EM imputation, especially when the missing rate increases. How- ever, both algorithms impute missing values using mixture model parameters and hence their imputations are sensitive to the prior information about density functions of mixture components and the size of data that are fully observed. A supervised classification method, called robust mixture dis- criminant analysis (RMDA), is proposed to handle label noised data [25]. The RMDA algorithm uses only fully ob- served data to learn mixture model parameters, and then uses the resulting mixture model to estimate labels and detect noisy ones. However, imputations made by the RMDA algo- rithm are sensitive to prior information about density func- tions of mixture components, the size of data that are fully observed and assumptions such as all the uncertain labels are in one feature.

served values for each pattern. While in the M-step, the new estimates of the FMM parameters are determined using the observed data and the statistical moments determined in the E step. Both the E and the M steps are alternated until convergence. For more details and description of this algo- rithm see [23].

The Incremental General Regression Neural Network (IGRNN) [26] is proposed for estimating missing values in nu- meric data sets. It is shown that the IGRNN produces highly accurate estimations for missing values in the case of a data set that has strong correlations among its dimensions [26]. In this Section, it is proposed to combine this algorithm with the EM algorithm [18] for determining FMM parameters from incom- plete data. First, the proposed algorithm estimates missing val- ues in the data set using the IGRNN. Second, it estimates parameters of the FMM that can be used in clustering the data using the resulting complete data and the EM algorithm. The proposed algorithm is referred to as the IGRNNEM algorithm in the rest of this paper. The IGRNNEM algorithm is de- scribed as follows:

the comparison is made using different percentages of missing values in multiple dimensions and different percentages of noisy patterns in the data set. The added noise comes from a multivariate normal distribution whose parameters are N(0, 0.1*R), where the mean is at the origin of the space and the covariance matrix is 10% of the data covariance matrix. The noise is added randomly to the patterns. Four data sets are used in the comparison. The first and the second data sets

This data set is the Iris data set that is commonly used in sta- tistical experiments since it was used in [27]. The data contains 150 patterns. Each pattern is a vector in four dimensions. The patterns are representing three clusters such that every cluster

This data set is the Wine data set that is a well-known data set for statistical analysis. Both of this data set and the Iris data set can be obtained from the UCI machine learning reposi- tory.1 The data contains 178 patterns. Each pattern is a vector

in a 13-dimension space. The patterns represent three clusters, which contain 59, 71, and 48 patterns, respectively. The miss- ing values are put in the sixth and in the seventh dimensions. When missing values are MCAR, they are randomly scattered in the specified dimensions for each data set. Meanwhile, when missing values are MAR, they are randomly scattered in the specified dimensions for each data set restricted by the value of the second dimension of each pattern. The value of the sec- ond dimension of any pattern that has missing values should be greater than either 3 in the first data set, 1.5 in the second data set, 2.99 in the Iris data set, or 1.85 in the Wine data set.

algorithms compared is evaluated by computing the Mis- Clustering Error (MCE). Each pattern is allocated to the cluster that has the maximum posterior probability given that pattern, and the number of incorrectly clustered patterns (Nr) is computed. The MCE is then computed such that MCE = Nr/N, where N is the total number of patterns.

The above results show that the IGRNNEM algorithm produces smaller MCE and hence more accurate estimations for the missing values than the EMH algorithm when correla- tions among all dimensions or at least among dimensions that contain missing values and some of the other dimensions of the data are at least moderate. This is because the IGRNNEM algorithm uses more estimators (kernels) than the EMH algo- rithm. Therefore, the mixture model resulting from the IGRN- NEM algorithm fits the data better than that resulting from the EMH algorithm as more accurate data are contributing in fitting the model. On the other hand, both algorithms produce large errors in estimating missing values and therefore fitting of the mixture models are bad when correlations among dimensions of the data are small. In this case, the EMH algo- rithm produces better results than the IGRNNEM algorithm although its error level is high. This is because of that the EMH algorithm depends on local correlations among data dimensions given each cluster in estimating missing values in the data and hence in estimating parameters of the model component that represent that cluster. Local correlations can be better than the overall correlations among dimensions of the data provided that the shape of each cluster is not a circle, a sphere, or a hyper-sphere. These shapes results in too small correlations among dimensions given each cluster. This result agrees with conclusions of the recently published work that is based on local tuning of the General Regression Neural Networks [28].

