reversible speculative computing in large-scale parallel computing as a dynamic linear feedback control (op- timization) system model and evaluate its performance in terms of time and cost savings as compared to the traditional (forward) computing. We illustrate with an exact analogy in the form of vehicular travel under dynamic, delayed route information. The objective is to assist in making the optimal decision on what computational approach is to be chosen, by predicting the amount of time and cost savings (or losing) under different environments represented by different parameters and probability distribution functions. We consider the cases of Gaussian, exponential and log-normal distribution functions. The control system is intended for incorporating into speculative parallel applications such as optimistic parallel discrete event simulations to decide at runtime when and to what extent speculative execution can be performed gainfully.

4 This paper has been authored by UT-Battelle, LLC, under contract DE-AC05-00OR22725 with the U.S. Dept. of Energy. Accordingly, the U.S. Government retains and the publisher, by accepting the article for publication, acknowledges that the U.S. Government retains a non-exclusive, paid-up, irrevocable, world- wide license to publish or reproduce the published form of this manuscript, or allow others to do so, for

Reversible computing is a relaxation of conventional forward-only computing [1]. In reversible computing, execution is designed to make it possible to go forward as well as backward: the application running on any processor is designed to change the direction of execution on demand. Such a reversible execution framework is useful in many contexts, such as database transactions and parallel discrete event simulations. In these applications, inter-processor synchronization forms a major cost which arises from the need of each processor to get information often from other processors on the next local trajectory to follow. One way to avoid blocking and wasting time waiting for information from other processors is to proceed with a best estimate based on local information, and then rely on reversible computation to retrace any incorrect portions of the trajectory. These portions of local execution without waiting for perfect global knowledge are called speculative forays. The higher the fraction of correctness of the speculative foray, the greater the gain; the lower the fraction, the lower the gain (or, worse, the more negative the gain). It is very hard to ascertain the best strategy ahead of time; in fact, it is preferable to have a runtime controller that can dynamically make decisions on when and to what extent the speculative foray should be allowed. In other words, an online control system is needed so that the speculative forays are dynamically controlled.

Here, we investigate an approach based on a model-based control system design that is agnostic to the specific application. We explore different stochastic distri- butions for the main runtime dynamics, and analyze their efficacies in synthetic experiments. The overall problem is defined in terms of two competing objectives: time to completion and total cost for completion. We explore the space with repre- sentative (normalized) parameter values in order to gain an initial understanding of the efficacy of our approach. Our work presented here differs from previous analyses in the literature on rollback-based parallel computation. Previous works focused on analyzing the applications and application classes as a whole in determining apriori the average, best and worst cases of blocking and speculative strategies [5,6,7,8,9]. In contrast, we focus on the design problem of online control to dynamically choose (and potentially switch) between blocked and speculative execution, and also at- tempt to maintain the lengths of speculative forays as an option of runtime control.

Consider a setting in which a driver is driving a vehicle to some final destination F to which the path is incrementally obtained. Suppose the driver is currently at some milestone point A. Further suppose that, after reaching A some processing time R(t) is needed to determine the route to the next milestone point B. So, the vehicle is stationary while the driver waits for the next milestone B to be determined. To save time, the driver may guess the next route and start driving. By the time the next milestone B is determined, the vehicle may have already gone ahead from A to another milestone C. It has to now drive back from C to the point D at which the

The analogy of the preceding model of reversible speculative driving is exact with that of reversible computing [1] in large-scale parallel computation. The notion of awaiting the determination of route A-B corresponds to the true computational path that a processor needs to take in a correct computation. The speculative foray A-C corresponds to the path the processor may take when it avoids completely blocking its computation while awaiting information from other processors (i.e., for inter-processor synchronization and communication to complete). The common path A-D corresponds to the gain in computational time obtained by the parallel program as a result of its speculative foray. The fuel cost corresponds to the aggre- gate electrical energy consumed by the computation, both in forward and in reverse directions.

where Xk is the state at time k, Uk is the execution time duration at k, A and B are constant model parameters. The value of B represents the processor speed, and the parameters may be normalized such that A = 1. The value of B is positive when the computation is in forward direction (vehicle moves forward) and negative when the computational direction is reversed (vehicle moves backward).

Reverse computation time (deviation from actual route), TR, is random with means R = d, d/2, d/3, d/4, etc., where d is the forward computation distance. We focus on three common cases for the probability distributions of the reverse computating time: Gaussian, exponential, or lognormal.

1. It is expected to see that both scenarios arrive at the final state at about the same time since the mean of the reverse travel route is half of the mean of the forward execution length. Also, it is a coincidence that both scenarios consume the same energy cost. For example, if the additional energy fuel cost for staring up the computation at each time step in Strategy 1, S, is less (or more) than 6 units, Strategy 1 will consume less (or more) fuel cost than Strategy 2, while maintaining the same completion times for both scenarios.

2 consumed about 585 units more energy than Strategy 1. So, for the parameter values used in this scenario, Strategy 1 would be selected as optimal. Comparing this scenario with the Gaussian and exponentially distributed scenarios that have the same mean, we observe that the log-normally distributed reverse time performs the worst.

Now, we consider the case where the reverse execution length is log-normally distributed with mean R = d/8, while all other parameters are kept the same. We observe in this scenario that Strategy 2 has similar time savings as Strategy 1. Both scenarios arrive at the final state at about the same time. However, Strategy 2 consumed about 38.37 units more energy than Strategy 1. Analytical derivations for the time savings where both scenarios arrive at the same time are described next.

The developed technique can be employed in any parallel processing platform to speed up the computational process per unit time. It can be used as a prediction mechanism that predicts the performance of the presented strategies (Strategies 1 and 2 ) in terms of time savings and processing cost savings. The predictor inputs are based on the application environment. As discussed earlier, the predictor inputs are: the processing speed, B, the probability distribution of inter-arrival times of the sync instructions from other processors, TD, the probability distributions of the guessed operations (drive ahead) time, TF , and reverse operations (reverse trav- elled) time, TR, the extra (fuel) cost for staring up the processors (vehicle) from an idle state, S, and the total number of time steps to the final destination, K. These input parameters should be known or estimated from the application environment. The prediction accuracy depends on how close the estimated environment parame- ters are to the actual ones. The outputs of the predictor are the predicted time and processing cost savings (positive or negative) by using the proposed strategy (Strat- egy 2 ) over the baseline strategy (Strategy 1 ). Based on the predicted performance savings, the proper strategy will be selected.

difficulty of conducting analytical derivations due to assumptions 4 and 5 that place upper bounds on the distribution functions. The developed technique can be em- ployed in parallel processing platforms to speed up the speculative computational forays per unit time. It can be used as a prediction mechanism that predicts the performance of the two strategies in terms of time savings and net cost savings. It can be implemented in each processor or in one (or few) processor(s) and for generalizing the selected decision to all processors.

