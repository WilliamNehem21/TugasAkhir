Personality Traits Detection is one of the important problems as a text analytics task in Natural Language Processing (NLP). Text analytics is the process of finding out insight knowledge over written text. Although most deep learning models give high performance, they often lack interpretability. Computer Vision (CV) has been affected significantly with inductive transfer learning, however training from scratch and task-specific modifications are still wanted in many NLP techniques.

This paper addresses the problem of personality traits classification. We adopted the use of the Universal Language Model Fine-Tuning (ULMFiT) in personality traits detection. The model makes use of transfer learning rather than the classical shallow methods of word embedding and proved to be the most powerful model in many NLP problems.

This relation between the personality trait and the disease is extensively investigated and considered by health authorities. Per- sonality detection is also helpful in forensics. It may be helpful in some cases when the investigators know the personality traits of the individuals who were at the crime scene, decreasing the pool of potential suspects [3].

In many scientific articles the deep learning structures and NLP tasks are treated [3]. These articles use technical vocabulary which sometimes can be difficult to understand. There is now a huge demand for the rating of online documents because of their rapid spread in many different languages [3].

trained in a source task to another target task [4]. The power of transfer learning is very clear when the features learned from the source or base task are general and can be repurposed to the target tasks. Nowadays, most CV models base extracting the feature to a pretrained models like AlexNet, ResNet, MS-COCO, etc. [4]. How- ever in NLP models, the idea did not have such success until Howard and Ruder have proposed the ULMFiT [5]. Deep pre- training word representation was one of the biggest achievements of ULMFIT as moving from shallow to deep models. The concept of ULMFiT has proved the ability of transfer learning for the NLP world. Language Modeling (LM) was the password to ULMFiT to be state-of-the-art. The ULMFiT makes use of the Average stochas- tic gradient descent-Weighted Dropout Long Short Term Memory (AWD-LSTM) LM which was proposed by Stephen Merity [6]. LSTM is a specific type of the Recurrent Neural Networks (RNNs).

We give a general review of the related work in Section 2 and we show the proposed model architecture in Section 3. We explain and discuss the experimental setup, datasets used, evaluation mea- sures and basic results in Section 4. Conclusions and future work are presented in Section 5.

The state-of-the-art results using the essays dataset was intro- duced by Majumder et al. [1], they encoded the essays from word level to sentence level. They have used 3-dimensional convolution to learn the structure of an article. They gave future direction of research incorporate more features and preprocessing i.e if written text contain repeated words, sentences contains spam content or unrecognized character set etc. preprocessing filter those. Our study proposes an efficient and explainable model based on lan- guage modeling. LM aims to predict the previous word or next one given a list of words [6].

We train the model using gradual unfreezing (partially training the model from everything but the classification head frozen to the whole model training by unfreezing one layer at a time) and differ- ential learning rate (deeper layer gets a lower learning rate). We apply dropout of 0.3 to layers, 0.2 to RNN layers, 0.3 to input embedding layers, 0.05 to embedding layers, and weight dropout of 0.5 to the RNN hidden-to-hidden matrix. We fine-tuning the for- ward and backward LM for 10 epochs for each one. Fine-tuning took 200 to 400 s which depends on each class. The classifier is a model that is a bit heavier, so we have lower the batch size of 32. The classifier needs a little less dropout, so we pass dropout of

The proposed model outperformed the AGR trait with a signifi- cant statistical margin about 2.54% and with competitive statistical margins at least 0.5% in rest of traits. We trained the model on the forward and backward LMs for both the general-domain and our task specific dataset, both LMs backward and forward are used to build two versions of the same proposed architecture. For our best accuracy results, the final decision is the ensemble of both.

