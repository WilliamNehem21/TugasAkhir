Most security research focuses on the technical aspects of systems. We consider security from a user-centred point of view. We focus on cognitive processes that influence security of information flow from the user to the computer system. For this, we extend our framework developed for the verification of usability properties. Finally, we consider small examples to illustrate the ideas and approach, and show how some confidentiality leaks, caused by a combination of an inappropriate design and certain aspects of human cognition, can be detected within our framework.

Our work lies between the technical aspects of information-flow security and the social aspects of user-centred security. More specifically, we are interested in information flow; however, the locus of this flow is now not within a computer system but within the inputs provided to it by its user. We are not considering the social aspects of human-computer interaction and security. Instead, the focus of our attention is cognitive processes that influence information flow from the human user into the computer system.

Our cognitive architecture has proved of use for detecting various types of sys- tematic user errors in the context of usability and task completion [8,17]. Here our aim is to show that the behaviours emerging from this architecture also expose security problems and so facilitate the improvement of security aspects in user in- teraction design. To demonstrate this, we first informally discuss, from a security viewpoint, several examples of user error dealt with in our earlier work [8]. Then we consider an example of using the model checking tool SAL [14] to detect some con- fidentiality leaks emerging from our cognitive architecture and conditioned by the user interpretation of system prompts. More specifically, we consider security prob- lems that may arise from the combination of user habits and (relative) positioning of input fields in authentication interfaces. The examples are small and intended to illustrate an approach and ideas that we believe are more generally applicable.

User goals. A user enters an interaction with knowledge of the task and, in par- ticular, task dependent sub-goals that must be discharged. These sub-goals might concern information that must be communicated to the device or items (such as bank cards) that must be inserted into the device. Given the opportunity, people may attempt to discharge such goals, even when the device is prompting for a differ- ent action. We model such knowledge as user goals which represent a pre-determined partial plan that has arisen from knowledge of the task in hand, independent of the environment in which that task is performed. No fixed order is assumed over how user goals will be discharged.

User goals are modelled as an array, goals, which is a parameter of the User module. The state space of the user model consists of three parts: input variable in, output variable out, and global variable (memory) mem; the environment is modelled by a global variable, env. All of these are specified using type variables and are instantiated for each concrete interactive system. Each goal is specified by a record with the fields grd, tout, tmem and tenv. The grd field is discussed below. The remaining fields are relations from old to new states that describe how two components of the user model state (outputs out and memory mem) and environment env are updated by discharging this goal. These relations, provided when the generic user model is instantiated, are used to specify GoalTransition(i) as follows:

Since we are modelling the cognitive aspects of user actions, all three updates depend on the initial values of inputs (perceptions) and memory. In addition, each update depends on the old value of the component updated. The memory update also depends on the new value (out') of the outputs, since we usually assume the user remembers the actions just taken. The update of env must also satisfy a generic relation, possessions. It specifies universal physical constraints on possessions and their value, linking the events of taking and giving up a possession item with the corresponding increase or decrease in the number (counter) of items possessed. For example, it specifies that if an item is not given up then the user still has it. The counters of possession items are modelled as environment components. We omit further details since, in this paper, possession properties are not used in any way.

If the guarded command for committing to a user goal, GoalCommit, fires, it switches the commit flag for goal i to committed thus enabling the GoalTrans command. The predicate grd, extracted from the goals parameter, specifies when there are opportunities to discharge this user goal. Because we assign done to the corresponding element of the array gcommit in the GoalTrans command, once fired the command below will not execute again. If the user model discharges a goal, it will not do so again without an additional reason such as a device prompt.

Goal based task completion. Users intermittently, but persistently, termi- nate interactions as soon as their main goal has been achieved [6], even if subsidiary tasks generated in achieving the main goal have not been completed. A cash-point example is a person walking away with the cash but leaving the card. In the SAL specification, a condition that the user perceives as the main goal of the interaction is represented by a parameter PerceivedGoal of the User module. Goal based completion is then modelled as the guarded command Exit, which simply states that, once the predicate PerceivedGoal becomes true and there are no commit- ments to user goals and/or reactive actions, the user may complete the interaction. This action may still not be taken because the choice between enabled guarded commands is non-deterministic. Task completion is modelled by setting the local variable finished to ok, whereas the value notf means that the task is unfinished. No-option based task termination. If there is no apparent action that a person can take that will help complete the task then the person may terminate the interaction. For example, if, on a ticket machine, the user wishes to buy a weekly season ticket, but the options presented include nothing about season tickets, then

This states that the secondary goal is always eventually achieved once the perceived goal has been. Often secondary goals can be expressed as interaction invariants [8] which state that some property of the system state, that was perturbed to achieve the main goal, is restored. Previously, we viewed property (2) in terms of pure usability, applying it to, e.g. user possessions.

A persistent user error that emerges from the cognitive architecture is the post- completion error [6], where a user terminates an interaction with completion of subsidiary tasks outstanding. People have been found to make such errors even in lab conditions [6]. An example of this error, which is also a security breach, is when, with old cash machines, users persistently took cash but left their bank card. Within our cognitive architecture, such behaviour emerges because of an action (Exit) that allows a user to stop once the goal has been achieved. Using our verification frame- work, this is detected by checking property (2). For this, SecondaryGoal would state that the total value of user possessions (bank cards included) in a state is the same as it was before the interaction. The formal verification of a similar example is described in [8].

In this section, we extend our previous work and investigate how other security problems, not considered in that work, can be detected using our cognitive archi- tecture formalised in SAL. In particular, we show how user habits in combination with some designs, can lead to the incorrect interpretation of interface prompts, resulting in the leakage of confidential information. To determine whether such leakages are possible, we introduce into our framework a new entity, generic module tester. This module is instantiated by providing a collection of channels and a high security value. The instantiated module then checks whether this value can appear on one of the low security channels.

High; displayed[j] indicates whether j is displayed (visible) or not. The attribute label[j] is a value of type {NameLabel, PasswordLabel}. Finally, value[j] and display[j] represent, respectively, text entered and text displayed, which can differ when the entered text is hidden. The array value and booleans EnterPressed and Acknowledged are the inputs of the authentication procedure, whereas position, displayed, label, and display with booleans DisplayedEnter, IncompleteMsg, IncorrectMsg, LastAttemptMsg, LoginMsg, and AbortMsg are its outputs.

Now we instantiate the generic module User for the authentication task. We start by specifying the state space of the concrete user model. For each input box j, we assume that a person either sees it or not, and perceives its label and the text dis- played, represented by seen[j], label[j] and value[j], respectively. The percep- tion of whether the Enter button is active is denoted by EnterActive. The person also perceives whether an error, warning or authentication message is given, denoted by ErrorMsg, WarningMsg and LoginMsg. Variables InputName and InputPass de- note the perception of which of the two boxes prompts for the user name and which for the password. Finally, name and password denote the values the person per- ceives as a user name and password. All these components form a record type, In, which is used to instantiate the corresponding type variable in User.

Next, we specify variables related to the actions users might take. The text typed into box j is represented by value[j]. The booleans EnterPressed and Acknowledged denote whether the Enter button and a button to acknowledge mes- sages are pressed. These components form a record type, Out. We assume users remember their user name, name, and login password, password. They also remem- ber whether they already typed information into box j, denoted entered[j] (reset to false when an error message is acknowledged), and keep track of whether there was a failure to authenticate, denoted failed. These form a record type, Mem, which also records, in a component of the type Out, the actions taken in the previous step. The reality surrounding our system is given by a record type, Env. It includes the user name, name, and the correct password, password.

We assume that user knowledge of authentication includes the need to communi- cate (1) user name and (2) login password. This knowledge is specified as user goals (elements of array goals) instantiated by giving the action guard and the updates to the output component. For the goal of communicating the user name, the guard is that an input box, regarded as the name box, is seen. The output action is to enter the name as the user perceives it:

As discussed earlier, the need to communicate the name and password is mod- elled as user goals. However, it is plausible that the user makes an error when trying to achieve those goals, e.g., enters a wrong password or presses Enter when some box is empty. Errors can also occur due to user habits; relying on previous experience, the user might expect the input box for the name to precede that for the password. In such cases, once the error message has been acknowledged, the system prompts for a new authentication attempt. We assume that the user will respond to this prompt. The response is modelled as two reactive actions. In the case of the password, the action guard is that an input box is seen (as for the corresponding user goal) and the password was not entered, as indicated by the memory, in the previous step. The output action is the same as for the corresponding user goal. Finally, the memory update records the fact of entering the password:

So far we have specified an authentication interface and have developed a formal model of its user. As in reality, the state spaces of the two specifications are distinct. The changing interface state is first attended to then interpreted by the user. Next we specify this interpretation, thus connecting distinct state spaces. The speci- fication is given as a new SAL module, interpretation. The module, being a connector, has input variables that are the output variables of the interface, and an output variable that is the input (perception) component of the UserAuthenticate module (record in).

In the authentication task, the crucial aspect of user interpretation is the percep- tion of the meaning (function) of the two input boxes. Their function is indicated by labels, however, we assume that people may not pay sufficient attention to the labels. Instead, the user might assume the name box comes first. The perception of precedence depends on the layout (coordinates) of boxes in the interface window. Formally, we define the condition when the input box i precedes j as follows (here pos is an array of coordinates):

For the user name and password, the user relies on the memory unless a warning message is displayed. If so, we expect the user to be careful enough to provide the correct values. For simplicity, here we do not consider how this is actually achieved (perhaps they are taken from a notebook), assuming that the values from the environment specification are used.

As explained earlier, the perception of which of the two boxes is for the names and which for the passwords is more complicated; the results of this perception are assigned to InputName and InputLabel, respectively. We assume that, upon receiving a warning message, the user becomes more careful and interprets the input boxes by their labels. Otherwise, if there are major changes in the layout of the boxes, the interpretation is an arbitrary choice between the three cases defined above. If there are no major changes, the interpretation of the boxes is the same as in the previous step. The auxiliary variables s, p and l are not intended to represent aspects of cognition. Intuitively, they, and the related TRANSITION section, are used to store the previous interpretation which allows specifying that user interpretation

We have specified an authentication interface and its user model. Now the cor- rectness properties of this interactive system can be analysed. We start from the interface with no constraints on the layout of the input boxes (other than that they do not intersect). The usability property (1), the user eventually achieving the per- ceived goal, is satisfied by the interactive system. Next we proceed with the analysis of security aspects of the system.

Even though security properties for each concrete system can be specified sepa- rately, we prefer to take a generic approach as with the user model itself. We thus introduce a generic module, tester. The idea is that the module, composed with an interactive system, monitors the communication between the device and the user. When security is breached, it sets the variable SecurityBreach to true. What secu- rity aspects are monitored is determined by the instantiation of the module. It has three parameters. The type variable Chan represents the communication channels. The predicate filter specifies which of the channels are monitored. Finally, test denotes security sensitive data. When this data appears on a monitored channel, SecurityBreach is set to true. The transitions of the module are the following family of commands:

With this instantiation of tester, we check property (3); the verification fails. The counterexample produced by SAL indicates that the user enters the password into the name box. The analysis of the specifications reveals that this counterexample occurs because neither of the boxes precedes the other which confuses the user.

To illustrate these extensions, we considered a simple authentication interface. We showed 4 how the layout of input fields combined with user habits can influence the user (mis)interpretation of interface prompts, possibly leading to confidentiality leaks. We demonstrated how these leaks are detected using the SAL verification tools, and how the analysis of the SAL counterexamples can help in eradicating cognitively susceptible interface designs. The SAL environment was primarily cho- sen because of its support for higher-order specifications. This is necessary for developing a generic cognitive architecture as ours.

The user interpretation stage was introduced into our framework from general considerations. We previously showed how modelling user interpretation allows us to detect usability problems due to the shape-induced confusion over device prompts [17]. Here we showed that similar ideas apply in the context of security properties and their dependence on the layout of input fields. Finally, we also considered user habits, which we had not dealt with before.

