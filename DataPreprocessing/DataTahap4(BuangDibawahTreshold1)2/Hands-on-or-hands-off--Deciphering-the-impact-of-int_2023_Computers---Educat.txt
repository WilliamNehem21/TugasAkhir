VR is lauded for its ability to create immersive, hands-on learning environments, facilitating a sense of embodied learning Petersen, Pet- kakis, and Makransky (2022). Specifically, the Sense of Embodiment (SoE) - the phenomenon where one feels a virtual body as their own - is often credited as a significant enhancer of learning in VR (e.g., Mak- ransky and Petersen (2021); Petersen et al. (2022); Johnson-Glenberg (2018)). However, the underlying mechanisms as to why, and the in- sights into exactly how these environments should be designed to maximise cognitive learning, remain contentious.

Some may suggest the relationship between interactivity and learning in VR is simply on a continuum, with more interactivity leading to enhanced learning? We argue that more interactivity does not auto- matically translate to improved learning outcomes, especially in a pro- cedural learning context. The complexity of VR interactions, their congruence with the learning tasks, and the cognitive demands they place on learners must also be considered. As such, this study explores these nuances, attempting to better understand the mechanisms that influence embodied learning in VR.

of the procedures within the VR environment. Our work does not explore the transfer of learning from VR to real life, thus no tests were done outside of VR. We are purely focused on improving learning out- comes within a VR environment.

We conducted 2 studies comparing 2 conditions per study, Study 1 compared Conditions 1 & 2, while Study 2 compared Conditions 3 & 4. (C1) Interactions mirroring the real world: A fully interactive environment where the student carries out the specified tasks on their own. (C2) Observing, with indirect interactions with objects: students use clicking in- teractions to trigger pre-recorded animations that demonstrate the task. (C3) Observing, but freely moving: The student can freely navigate the environment and watch animations of the tasks. (C4) Observing from a fixed location: The student watches animations from a seated position. Following a survey and a distraction task, participants in all conditions were tested on their skills in an identical VR environment with a slightly modified task.

We did not find a significant difference in learning outcomes as a result of movement, in other words, between a room-scale (C3) and seated (C4) VR learning setting. However, we found that interaction techniques used for manipulating learning content can have a significant detrimental impact on learning. We found that non-mirroring in- teractions (C2) led to worse learning outcomes compared to C1. Finally, by comparing conditions across studies, we found no significant differ- ence between actively observing (C3) versus actively manipulating the

To the best of our knowledge, our work is the first to examine the impacts of the aforementioned interaction features on a procedural learning task. This has been called for by the community (see, e.g., Makransky and Petersen (2021)) as many existing interaction design studies use declarative tasks, which are less suited to examining embodied learning. We discuss this further in the related work section. Additionally, our work provides foundational knowledge to inform the design of VR learning systems going forward. For example, as VR gains traction in industry, companies may spend hundreds of thousands of dollars on fully interactive experiences when a more cost effective approach (observing a recorded animation) may produce similar results.

cognition theory), embodied learning and embodiment are different concepts. In practice, embodied learning is when a user uses their body in the learning process, for example using their arms or fingers to trace changes in population of a certain animal when numbers of their pred- ators rise or fall (Andrade (2017)). These factors are sometimes but not always present in VR, and differ in the degree to which they are present (eg. fully interactive versus seated with no interaction). But they are often referred to (along with level of immersion) as key factors in creating an effective exploratory, experiential and embodied learning environment in VR (Keifert et al. (2017)). So does that mean a hands-on approach always increases embodied learning, and does that always mean better learning outcomes?

Other factors that support learning in VR include the concept of active learning. Active learning is defined as engaging with the learning content to construct knowledge or understanding whether through discussion or through physical interaction (Waldrop (2015)). Some believe that simply being immersed in a virtual environment and observing learning content facilitates active learning given the embodied nature of VR (Mayer et al. (2022); Liao, Sung, Wang, Lin, and Cherng (2019); Ponder et al. (2003)). Others consider physical inter- action with the environment as necessary for active learning (John- son-Glenberg (2018)). So as the interaction fidelity increases in VR, do we make the learning more active and thus more impactful? Or does the embodied nature of VR create an active learning environment without the need for complex interactions in the environment?

Johnson et al. argue that for embodied learning to take place, it needs the interactions or movements to be congruent with the learning content (Johnson-Glenberg (2018); Skulmowski and Rey (2018)). For example, if students are learning about orbits they should move their hands or bodies in circular motions. However, this can be complicated to achieve and is highly dependent on the way the students perceive the movements thus can result in actually confusing students (Lindgren and Johnson-Glenberg (2013)).

While VR is typically considered an environment where embodied learning happens (Lindgren and Johnson-Glenberg (2013)), that may or may not be true depending on the design of the experience. Some ex- periences are stationary, where the user sits, simply observes their sur- roundings, and has little to no control over what happens next (Mayer et al. (2022)). Alternatively some environments allow interacting with

objects using raycasting (laser pointing) (Seo et al. (2017)). This is a common interaction technique, but its effectiveness for supporting embodied learning has not been tested. Comparatively, an environment can allow the user to walk around freely, interact with objects (pick them up, manipulate them), and determine the outcome of the experi- ence based on the choices they make and their behaviour. However, beyond testing their usability, studies have overlooked interaction techniques and their impact on learning outcomes (Moher, Johnson, Ohlsson, and Gillingham (1999); Seo et al. (2017); Yoo, Kim, and Lee (2020)). Usability testing, in the form of evaluating user engagement and learner confidence may not be sufficient measures of system design effectiveness due to the illusion of understanding (Seo et al. (2017)). The illusion of the understanding states that, in some cases, the student may feel confident during the learning stage, but in practice, they may find that they have not mastered the content to the level they had felt they had. So learner confidence does not always positively correlate with skill mastery.

Though the embodied learning theory is well established, the chal- lenge of how to apply it in VR training environments still remains. Due to limited evidence-based best practices for procedural VR training, the design and development of VR applications can at times be erratic (Biermann, Ajisafe, and Yoon (2022)). The lack of clarity on the mech- anisms that may facilitate the embodied learning process makes the design of embodied learning systems even harder. For example, while the importance of interaction for enhanced embodied cognition and design is understood, the practical application of the theories to proce- dural training in VR is lacking (Jin, Liu, Yarosh, Han, and Qian (2022); He et al. (2020); Mayer et al. (2022)).

are and how to get to them) (Ferguson, van den Broek, van Oostendorp, de Redelijkheid, and Giezeman (2020)). In the embodied learning literature different types of movement have been associated with enhanced embodied cognition (Abrahamson et al. (2020)). But in the context of VR not much else is known around how moving and walking in VR impacts learning outcomes.

Studies have conflicting results about how the level of interactivity impacts learning. For example, comparing a participant that only ob- serves a demonstration in VR versus one that interacts with the scene. Some believe interactivity enhances user experience and immersion (Seo et al. (2017)), others show that purely observational environments have comparable results to an interactive one (Rowe et al. (2017);

All conditions in our study were deployed in the same virtual envi- ronment. The minimalist design of our VR environment, including the simple background, was indeed intentional. Drawing from cognitive load theory per the work of (Sweller, van Merrienboer, & Paas, 1998), we aimed to prevent any extraneous cognitive processing that could potentially distract from the primary learning task at hand.

Study one compares C1 & C2 while Study 2 compares C3 & C4. Condi- tions C2, C3 & C4 were presented with pre-recorded animations of the appropriate steps that must be taken to operate the machine. These animations were similar to receiving a demonstration by another user within VR.

C1 - Interactions mirroring the real world: Participants received text prompts telling them how to operate the machine step-by-step which they had to learn. The interactions were designed to be real- istic, so all objects could be grabbed, picked up, and moved. Levers and wheels could be adjusted and rotated with the use of the con- trollers. The participants used the A key on the controller to proceed to the next instruction board. This was identical in all conditions. For

C3 - Observing, but freely moving: Participants received the same text prompts as the previous conditions but pre-recorded animations of the appropriate steps automatically started playing with each new text prompt. No clicking was necessary to activate the animations. Participants were able to walk around and watch the animations from any angle or as closely as they liked. Participants could also go back to the previous animation or click to go next. To go back and forth between the text prompts the buttons A and B on the controller were used. This was the same for all conditions, and not to be confused with the pointer clicking that was exclusive to condition

We made a conscious decision to use VR controllers instead of gesture-based controls for several reasons. First, the maturity of the gesture recognition technology still presents challenges that might in- fluence the learning outcomes in ways that are external to the factors we intended to study. Second, hand tracking does not allow for haptic feedback, which is crucial in tasks involving manipulation of virtual objects and tools. Therefore, our decision to use VR controllers was based on their capacity to provide both reliable interaction and haptic feedback.

asked for their consent again. We only used this task for participants to gain practice with the controls and we did not collect any data from this stage. The first author was in the room with participants at all times to ensure they were safe and the wire attached to the headset was not a tripping hazard.

Participants were then asked if they needed a break, before begin- ning a tutorial on how to interact with objects relevant to the test (pick them up, hold them etc.). This tutorial was only given if they were in conditions C2, C3, or C4 where they had not yet manipulated the ob- jects. If they were the C1 condition this step was skipped, and partici-

Participants were given the VR test which was the baseline all par- ticipants were compared against. This study did not explore the concept of transfer of learning from VR to real life, it was focused purely on optimizing learning outcomes within a VR environment. During the test if participants faced any difficulty with the controls, they were offered support, but no help or clues were given as to how to complete the task. The tests consisted of all the same steps as the training plus an additional step that was not included in the training. This additional step was to adjust the angle of the blade using the angle wheel to cut the wood at an angle. Participants were asked (using text prompts) to repeat all of the exact same steps in the same order with the addition of this extra step.

about the experience. They were asked about the training and how it helped or did not help them. They were asked about how they typically learn procedural tasks. Some interviews were brief (1 min) while others were longer (10 min+) as participants expressed everything that was helpful or distracting to them. We focused on understanding their challenges and factors that helped them.

The analysis showed that C2 led to significantly worse performance worse (mean of test scores for C2 = 11.8/18, versus 15.9/18 for C1) compared to the C1 condition (and the non-interactive conditions). This may be a surprising finding for those that believe more interaction in VR always leads to better engagement and learning outcomes. However the embodied learning literature might help shed light on this. This body of work highlights the importance of congruent movements with the task

we found that the use of a pointer (raycast) led to significantly lower performance than direct manipulation. Given the widespread use of pointer-based interactions in many VR systems, it highlights the importance of research on interaction design choices. Our study illus- trates the potential pitfalls of using interaction techniques, such as the pointer, that may seem intuitive but do not align (are not congruent) with the physical actions associated with the learning task. Furthermore, the excessive cognitive effort associated with the in-congruent pointer interactions likely led to extraneous cognitive load, diverting cognitive resources away from the learning task itself. Hence, our study provides additional evidence for the importance of minimizing extraneous cognitive load in VR learning environments, especially when designing interaction techniques.

We were surprised to find how similar the performance of those in the C3 condition (observation) was compared to the C1 condition (manipulation). So more interaction is not always justified when designing a VR learning experience. However, we believe there are a number of important things to note. While those in the C1 condition did not perform significantly better than those that had no interaction with the learning content, there were a larger group of participants that did not perform as well in the C3 condition. To be more specific, the C1 condition participants consistently performed well, while those in the C3 condition had a wider distribution. Perhaps, observation in VR in itself does result in embodied learning, even without interactions, given the sense of presence it creates (compared to simply watching a video, for example).

lead to similar learning outcomes as active manipulation, calling into question traditional assumptions about the superiority of interactive learning. These findings have significant theoretical implications for the understanding of embodied learning in VR. Particularly, they challenge the conventional notion that physical interactions are always necessary for embodied learning. Indeed, it seems that mere observation in an immersive VR environment can trigger a similar sense of embodiment, leading to effective learning.

cognitively less demanding, they found the test to be cognitively more demanding. This is reiterated in some of the interview responses, where the participants reported that the training content was easy and that they felt confident in their knowledge. However, in the test, they found themselves not as skilled as they had previously thought. This is in accordance with the Challenge Point Framework (Guadagnoli, Morin, and Dubrowski (2012)), which suggests that some level of challenge

Once the learners are optimally challenged, learning occurs efficiently. Beyond the optimal challenge point, both practice performance and the learning performance can suffer. Therefore, while designing the VR learning environments, an effort should be made to ensure that the tasks offer some level of challenge or difficulty to the participants but not so difficult that it impedes their learning (Nawaz et al. (2021); Nawaz, Srivastava, et al. (2020)).

Additionally, while the use of audio instructions was mentioned as future work, we believe the lack of it was a limitation of this study. Some researchers have found that text prompts do not work well in VR learning environments while others argue the opposite (Johnson-Glen- berg (2018); Baceviciute et al. (2020)). Several participants needed the researcher to simply read out the text prompts word for word before they understood what to do, and this need was reiterated in the in- terviews. The addition of the option to listen to the audio instructions could have allowed for the evaluation of a more robust learning system.

them. Several participants mentioned unused objects in the VR envi- ronment. Adding objects that were not part of the training to the test may also be a way to make the task harder and look more closely at the impact of interaction techniques, interaction fidelity and movement on learning. Or by perhaps putting all the tools in a tool box such that it is not within site anymore.

Text prompts have been shown not to be effective in some VR studies (Johnson-Glenberg (2018)), while others say text is ideal for remem- bering facts in VR (Baceviciute et al. (2020)). If the study design could be modified to include audio prompts (which many participants asked for), we could further test whether the training content is being optimally processed with audio or text prompts. The findings of these study may not generalize to all environments, given the diversity in VR learning experiences such as procedural learning versus declarative learning tasks.

The use of multimodal learning analytics to better understand and observe behaviours and actions of the participants in each condition would have helped us to quantitatively validate the interview results. By looking at gaze tracking data for example we could observe how often participants in each condition were distracted, or not looking at the relevant object. We could measure the distance covered by walking and correlate that to learning gains and so on.

