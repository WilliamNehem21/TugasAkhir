A control system consists of sensors, controllers, and actuators. Sensors gather data about the state of the system. Controllers use the sensor data to compute an appropriate control action. Actuators then perform the control action and thus effect a change in system state, which is then sensed by the sensors again in the next cycle. In such a feedback system, the controllers rely on sensors to provide good quality data to ensure that the system operates correctly and safely even in an uncertain environment.

the attack surface is growing swiftly, and it is not difficult to spoof sensors and inject wrong data into the system. There are known instances of GPS spoofing attacks on unmanned aerial vehicles [1]. Modern automobiles have also been shown to have several attack surfaces that an attacker can use to compromise computers/networks within a car and inject sensor spoofing attacks [5,2].

One way to improve security of any system would be to eliminate the attack vectors. This is an uphill battle since systems are increasingly becoming more connected and communicate extensively with other devices. Here, we take the view that it will be difficult to completely eliminate all attack surfaces, and inevitably, there will always be some channels available for an attacker to exploit.

Recently there has been increasing interest in studying control systems and cyber- physical systems under adversarial attack. There is considerable amount of work on attack-resilient state estimation, since if we can determine the correct state (even in presence of an attack), then the rest of the system can be used unchanged as it would just rely on the estimated state [4,9,7,8]. For linear systems, the state estimation problem in presence of attacks can be cast as an optimization problem [4,9]. Fawzi et al. [4] assume existence of redundant sensors and provide theoretical guarantees on the maximum number of attacks that can be handled. Extending the work in [4], an

Remark 2.2 Safety Runtime Monitor: The safety monitor is implicit in the above formulation in the form of the definition of the stealth attack. More precisely, we are assuming that the safety monitor is checking, at runtime, if the difference between a sensor output and the expected sensor value is at most stbound. As a result, in the stealth mode, the attacker is allowed to change (the compromised) sensor value by at most stbound: if the attacker changes a value by more than that bound, then the safety monitor will detect the attack and take corrective action.

Remark 2.3 Noise: The value stbound is a reflection of the fact that there is noise in sensor outputs, and the feedback controller is designed to tolerate that noise. We will choose the value stbound to guarantee that, if the error in sensor output is bounded by stbound, then the system remains safe. So, a priori, it is guaranteed that the attacker can not cause the system to enter an unsafe state by being in the stealth mode always.

{2.53, 2.54, 2.6, 2.7, 2.8}, we observe that ttoAttack decreases as tstealth increases from 2 to 4. In this case, the attacker can indeed benefit by remaining in stealth mode. However, ttoAttack rises with tstealth for all values of tstealth greather-than 4, so there is only a small window where an attacker can benefit from a stealth mode.

We generated the data for the plots shown in Section 3 using the two-step verification process supported by HybridSal. In the first step, the model is abstracted to an infinite-state discrete state transition system. In the second step, the abstract model is model-checked.

Remark 4.1 Choice of sensor under attack: The choice of the sensor under attack is insignificant for our results. If, for example, the sensor for gap was compromised (rather than the sensor for vf ), then we would obtain the same results, but for a constant factor in the values of stbound and evbound.

study on other systems: in fact, HybridSal can model more general hybrid dynam- ical systems and thus model mode switching and controllers with finite memory. Moreover, it should be possible to perform a similar exploration theoretically (and more generally). Intuitively, we expect that we will obtain similar results.

