In this paper, therefore, a popular data mining tool called the backpropagation learning neural network is implemented as an application running on graphics hardware. Since the recent graphics hardware has many vector processing units and high memory bandwidth, it is promising to accelerate the backpropagation learning task involving a lot of data-parallel computations. The evaluation results have demonstrated the great potential of our prototype implementation for massive backpropagation learning tasks. The graphics hardware can efficiently work especially if the task is implemented so as to use data-parallel instructions supported by the hardware.

Today, enormous amounts of data are being produced and accumulated everyday, at all times. As a result, it is very difficult to manually retrieve valuable information and useful knowledge from a huge sea of data. Because of the social demand, the computer technologies for knowledge discovery in databases, so-called data min- ing [4], have been receiving increasing interests. Although data mining is helpful to limit the scope of our information search, a data mining system has to process a large number of data in a practical time. As the database is growing rapidly, we need more and more computing power for future data mining.

The SETI@home project [1] has demonstrated the tremendous potential of vol- unteer computing, which uses idle computing resources on the Internet, to realize a large-scale data mining system. Furthermore, the latest game consoles have ex- cellent computing power. In the near future, hence, a huge number of idle game consoles will ubiquitously exist on the Internet. Effectively using such idle game consoles, volunteer computing is expected to realize an unprecedented scale data mining task.

In this paper, the learning task of a backpropagation neural network [11], which is one of the most popular tools for data mining [2], is implemented as an application running on GPU. As the backpropagation learning algorithm involves massive data parallelism, GPU is promising to accelerate the learning tasks. As the first step to establish such an effective implementation scheme, therefore, this paper shows the great potential of the GPU implementation for massive backpropagation learning tasks.

The outline of this paper is as follows. Sections 2 and 3 briefly review the backpropagation learning algorithm and GPU, respectively. Then, we propose an implementation scheme of the backpropagation learning task using GPU. Section 4 shows our experimental results to evaluate the performance of the GPU implemen- tation. Finally, Section 5 gives concluding remarks and our future work.

Artificial neural networks are the basic tools for data mining [2]. Their learning algorithms can roughly be categorized into supervised learning and unsupervised learning algorithms; supervised learning assumes that a set of ideal input-output vector mappings, a training data set, is given, while unsupervised learning does not. The backpropagation algorithm being discussed below is a typical supervised learning algorithm for multi-layered feed forward neural networks [11].

An MI-dimensional input vector in a training data set X = {x1, x2,..., xN } is denoted by xl = {x1l, x2l,..., xMIl}. Each component of an input vector, xil, is given to an input neuron, and propagated to hidden neurons via weighted links. The output value of a hidden neuron, ojl is calculated by

Non-graphics applications on GPUs usually exploit the programmability of the fragment shader. The fragment shader can operate colors of multiple fragments in parallel. The fragment shader can also fetch data from texture images on the video memory; the texture images can be used as the source operands of computing on the fragment shader. Moreover, SIMD instructions of the fragment shader are available to simultaneously operate four color channels: red, green, blue and alpha (RGBA) channels 6 . Therefore, we can see the fragment shader as a multi-grain SIMD parallel processor that can calculate multiple fragment colors in parallel, and further can use fine-grain SIMD parallel instructions for the calculations. As a result, the fragment shader can efficiently execute the component-wise matrix operations that independently determine each matrix component in a SIMD parallel fashion, such as addition of two huge matrices.

one texel is called data packing [8]. Data packing is necessary for multi-grain parallel processing by the fragment shader. Since data packing can reduce either the number of fragments or rendering passes to quarter, it can significantly accelerate many GPGPU applications.

As the backward propagation of errors at one output neuron to all the hidden neurons is performed in a single rendering pass as with the forward propagation, the backward propagation needs MO rendering passes in total. Updating all the weights between the input layer and the hidden layer also needs N rendering passes. Accordingly, GPU can execute one learning step of the backpropagation learning algorithm in total (MI + MH + MO + 2N + 4) rendering passes.

Data packing stores four signals into one texel [3]. In our prototype imple- mentation, this packing is used to reduce the size (the number of fragments) of each matrix in the forward and backward propagation phases to quarter, and the packing also allows each per-fragment program to fetch four matrix components by sampling a texture once. Furthermore, it can reduce the number of rendering passes to (MI + MH + MO + N/2 + 4), because the numbers of rendering passes in the weight updating phase is reduced to quarter. Therefore, the packing is significantly effective to improve the computational performance of the GPU implementation.

generally degrades as the texture size decreases, even though the total execution time decreases with the texture sizes. Accordingly, these experimental results clar- ify that the reduction in the number of rendering passes can accelerate the GPU implementation more significantly than the reduction in the texture sizes. For im- proving the efficiency, the data packing should be used so as to reduce the rendering passes rather than the texture sizes, if possible.

The goal of our project is to realize a large-scale data mining system by volunteer computing of game consoles with high-performance GPUs. As the first step, this paper has discussed the GPU implementation of one popular data mining tool, the backpropagation learning neural network. Then, our prototype implementation has clearly demonstrated the high performance of the GPU implementation especially for large-scale learning tasks.

