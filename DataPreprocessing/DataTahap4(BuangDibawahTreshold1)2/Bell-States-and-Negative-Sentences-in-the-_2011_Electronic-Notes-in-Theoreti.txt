We use Bell states to provide compositional distributed meaning for negative sentences of English. The lexical meaning of each word of the sentence is a context vector obtained within the distributed model of meaning. The meaning of the sentence lives within the tensor space of the vector spaces of the words. Mathematically speaking, the meaning of a sentence is the image of a quantizing functor from the compact closed category that models the grammatical structure of the sentence (using Lambek Pregroups) to the compact closed category of finite dimensional vector spaces where the lexical meaning of the words are modeled. The meaning is computed via composing eta and epsilon maps that create Bell states and do substitution and as such allow the information to flow among the words within the sentence.

Recently, some Theoretical Physicists and Mathematical Linguists have inde- pendently abandoned the monoidal structure of Quantales for the more expressive setting of compact closed categories. Lambek has used the setting of a compact bi-category [14], referred to as a Pregroup [10]; these have been applied to analyze syntax of many natural languages, from English and French to Japanese, Arabic, Persian and many others. Abramsky and Coecke [1] have used compact closed cate- gories to provide semantics for quantum protocols and as such have set a new basis for Quantum Logic. Similarities between models of Language and Physics have

Apart from syntax, these similarities also occur in the semantic models of natural languages, ranging from logical to distributed models of meaning. From the logical point of view, a category-theoretical semantics for pregroup grammars have been proposed in [13] in the form of compact bi-categories. From the distributed point of view, vector spaces are used to provide lexical meaning for words [16]. Moreover, the Quantum axiomatic of Hilbert spaces have been used to model semantics of natural languages in [18,19]. These models have found applications in information retrieval from documents, for example those on the web, and to find synonymous meanings for words [7].

We take the first step towards developing a logic for semantic derivations in natural languages. Motivated by the work of D. Clark in [3], we develop no- tation for a graded implication and use it to measure the degree of similarity between positive and negative sentences. Meanings of sentences can be derived from one another using this implication and the degree of this implication stands for how close the meanings of the sentences are to each other.

T (B ) is the free compact 2-category generated over the partial order B . We refer the reader for the details of this construction to the joint work of the first author with J. Lambek in [14]. Every element (w, t) of dictionary D is called a lexical entry in D.

In the distributed model of meaning, the lexical meaning of words are vectors in a possibly high dimensional vector space; one whose bases are certain words of a dictionary. Given a text or a collections of texts and fixing a neighborhood window of n words, one counts how many times a certain word appears in that window in the context of the bases. This provides us with a vector, that is the vector of the lexical meaning of that word.

As an example [4], consider the word dog and a vector space with bases eat, sleep, pet, and furry. If the word dog has eat in its context 6 times (in some text), sleep 5 times, pet 17 times, and furry 8 times, then the vector for dog in this space is (6,5,17,8). The advantage of representing meanings in this way is that the vector space gives us a notion of distance between words, so that the inner product (or some other measure) can be used to determine how close in meaning one word is to another. For example, one can form the vector of cat in the same space as that of dog and then observe that they have similar meanings in that context, which makes sense since cats and dogs are both pets and they both sleep, run and are furry.

Computational models along these lines have been built using large vector spaces (tens of thousands of context words/basis vectors) and large bodies of text (up to a billion words in some experiments). Experiments in constructing thesauri using these methods have been relatively successful. For example, the top 10 most similar nouns to introduction, according to the system of [7], are launch, implementation, advent, addition, adoption, arrival, absence, inclusion, creation.

