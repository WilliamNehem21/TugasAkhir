grading cannot be assured. On the other hand, Automated Scoring will perform fair scoring and can be repeated again and again with consistency [3]. The research in this direction will open new dimensions for researchers as it is an interdisciplinary work. If such a system be developed in any Indian language, it will open the doors for other similar Indian languages. It has been previously mainly implemented with interfaces for problem solving systems for specific domains [5].

This paper reviews and compares NLP based Automated Answer scoring approaches. Semantic similarity plays a vital role in NLP, Informational Retrieval, Text Mining, Q & A systems, text-related research and application area [7]. Today, Automated Scoring is still a difficult and interesting issue for researchers in artificial intelli- gence and NLP though many English Automated Scoring systems have been proposed and developed but with little success [8].

Automated Scoring can choose some tasks from NLP to the scor- ing process. Automated Scoring systems are a combination of var- ious techniques such as - NLP (NLP) along with, Linguistics, Artificial Intelligence (Machine Learning), Statistics and Web Tech- nologies, etc [12]. Current automatic essay grading mechanism relies on two aspects such as machine learning techniques and grammatical measures of quality. However, none of them which identifies statements of meaning (propositions) in the text. Hence, it proves to be an inappropriate for scoring the content of an answer [13].

Chen et al. (2010) [7] demonstrates the AES, which uses a voting algorithm based on an unsupervised learning approach. To build computational learning model, they use an unsupervised approach which does not use any reference text. Their approach is evaluated on a set of essays, written by different people, on a same topic. The

Hongbo Chen, Ben He, Baobin (2012) [9] introduced a Ranked- based learning approach to AES. They discussed supervised learn- ing problems in detailed steps. It also discusses how to apply learn- ing to rank to automated essay scoring, such as feature extraction and scoring. It regards automated essay scoring as a ranking prob- lem and plan to solve this problem by learning to rank algorithms. Learning to rank that automatically construct a ranking model or function to rank objects.

Luis Tandalla (2012) [16] discussed Scoring Short answer Essay. In their system concepts are spilt into smallest unit which are pre- defined expertly written model answers to closed end question. Their dependencies are automatically measured by tagging resul- tant concepts and their dependencies. Dependencies are pattern matched with the scoring scheme. The process is repeated for every student to answer.

Oleksandr Kolomiyets et al. (2011) [20] describe a question answering technology from an information retrieval perspective. Different levels of processing, yielding bag-of-words were dis- cussed along with the more complex representations integrating semantic roles, discourse analysis, POS, classification of the expected answer type, translation of the question into a SQL-like language or logical representation.

Kong Joo Lee et al. (2011) [13] introduced an automated English sentence evaluation system for students learning English as a sec- ond language. Firstly, the input sentence is analysed in order to detect possible errors such as syntactic errors and spelling errors. Secondly, the input sentence is compared with given answers to identify the differences as errors. Then the system evaluates the performance and the output produced by the system is compared with the result given by human ratters. The system still has the problems which most NLP systems experience. The problems are found in ambiguity resolution, handling parsing failure, lexicon incompleteness, and so on.

Paloma Moreda et al. (2011) [21] describe combining semantic information in question answering systems. Their study involves two proposals which are based on semantic rules and WordNet, which are to extract an answer in the general open - domain ques- tion answering (QA) system. The main objective of their research is to compare extracted answers with the existing QA systems with the named entities (NEs).

Matthias H. Heie et al. (2012) [17] explain question answering using statistical language modelling. Their work is to develop robust systems for different languages which overcome the need for highly tuned linguistic modules. They embedded this frame- work with an implementation in the TREC 2006 QA evaluations. As long as appropriated training data is available, this system can rapidly be adapted to any language.

Min-Chul Yang et al. (2015) [18] explain knowledge-based question answering using the semantic embedding space. In their study, the semantic embedding space is used to fulfil the goal to answer questions in any domains in which the embedding encodes the semantics of words and logical properties. This embedding- based inference approach for question answering allows the map- ping of factoid questions posed in a natural language ontological representation of the correct answers guided by the knowledge base.

Sofian Hazrina et al. (2016) [25] describe the advancements of disambiguation in the semantic question answering system. In any semantic question answering (SQA) system ambiguity is an obvious problem. Therefore, an SQA system has to select the cor- rect meaning with disambiguation solutions when the linguistic triples matched with multiple knowledge based (KB) concepts. The system mentions similar words in cases where linguistic tri- ples do not match with any knowledge based (KB) concept.

Boris Galitsky et al. (2017) [5] introduced matching parse thick- ets for open domain question answering, it supports complex ques- tion answering, which include multiple sentences, to handle as many constraints expressed in this question as possible. In this study, they explore how parse thickets of questions and answers can be matched. They demonstrate the necessity for using discourse-level analysis to answer complex questions.

Zhang Qiang et al. (2014) [28] describes the application of net- work automated essay scoring system in the college English writ- ing course. Basing on the network automated essay scoring system, he first introduces the development of this kind of system and then tries to optimize the design of classroom teaching of college Eng- lish writing. Finally, the paper proposes some potential problems of the automated essay scoring system and provides some useful suggestions for the teaching of college English writing. This paper effectively combines the traditional process writing method with the automated essay scoring system, and creates a highly effective essay training mechanism for the students.

LSA is a statistical method for automatic document indexing and retrieval. Latent semantic analysis (LSA) is a technique in NLP<https://en.wikipedia.org/wiki/Natural_language_processing>. Its advantage to other indexing techniques is that it creates a latent semantic space. Latent semantic indexing is the application of a particular mathematical technique, called Singular Value Decom- position or SVD, to a word-by-document matrix. LSA assumes that words that are close in meaning will occur in similar pieces of text. Limitation of LSA where a text is represented as an unordered col- lection of words.

Yonghong Yan et al. (2012) [27] demonstrates AES system using vector regression. Each essay is represented by the Vector Space Model (VSM). To implement the system, it gives score on several features, including the surface features such as the number of words, number of sentences, average word length, average sen- tence length etc. and complex features such as grammar checking, sentences. Both the words and part-of-speech tag are considered. They use the simple model and CVA (content vector analysis) model. They get 86% precision given the two-score deviation com- pared to human ratters.

Jovita, Linda et al. (2015) [10] introduced using the Vector Space Model in Question Answering System. The aim of their research is to represent knowledge, and retrieve the answer for a given ques- tion by using the Vector Space Model. The query is compared to the knowledge base by measuring their similarity. Thus, it suggests for future improvement to use topic modelling or some other approaches like Latent Semantic Analysis to construct the model of document representation.

the 88-note network, we reduced the size to two layers of 200 LSTM units as it performed better in our experiments. In the 12- chroma, we downsized it further having 100 LSTM units on the first layer and 50 LSTM units on the second layer. On top of the LSTM networks, a fully connected layer with sigmoid activation units are added as the output layer. Each output unit corresponds to one MIDI note or chroma (i.e. pitch class of the MIDI note).

The AMT systems return two types of MIDI-level features. The resulting features are combined by concatenation with either 88- note or 12-chroma AMT output features. The corresponding score MIDI was also converted into 88 note (or chroma) and the chroma onsets are elongated in the same manner before combined. We used euclidean distance to measure similarity between the two combined representations. We then applied the FastDTW algo- rithm [8] which is an approximate method to dynamic time warp- ing (DTW). FastDTW uses iterative multi-level approach with window constraints to reduce the complexity. Because of the high frame rate of the features, it is necessary to employ low-cost algo- rithm. While the original DTW algorithm has O(N2) time and space complexity, FastDTW operates in O(N) complexity with almost the

