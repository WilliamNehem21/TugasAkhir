The Robust Transform based on the Weighted Median operator algorithm calculates the transform of a signal when it has been exposed to impulsive noise. Since this algorithm demands very long execution time, it is not useful for real time signal processing systems. In this context, this work presents several strategies to improve its performance, such as the reduction of redundant calculations, optimization in the memory access, and a multithreads version of the algorithm. Besides, the original estimation method is modified to decrease even more the average execution time, keeping the quality level of the numeric results. The experimental results show a 30% performance improvement by reducing redundant calculations and optimizing the memory access, without making modifications to the estimation method and without using multi-threaded processing; 93% performance improvement by introducing modifications to the estimation method; and 97% performance improvement by incorporating the multi-threaded processing.

through a medium, it uses to be affected in a degenerative way. This effect is known as noise. Impulsive noise shows sharp increases in signals intensity, although the duration of these increases is short compared to the time between them. This kind of noises is usually modeled with distributions that have tails more weighted than those of the Gaussian Distribution, for example, the Laplacian Distribution.

In this sense, we propose several strategies to reduce the execution time of the RTWM algorithm, which consist on the reduction of redundant calculations, the use of the hierarchical memory system, and the use of multiple threads of execution. In addition, modifications are made to the original structure of the algorithm that allow improving the execution time while preserving the quality of the numerical results. The experimental results demonstrate an improvement in performance of up to 30% by reducing redundant calculations and optimizing the memory access, without making modifications to the estimation method and without using multi- threaded processing; 93% performance improvement when introducing modifications to the estimation method; and 97% performance improvement by incorporating multi-threaded processing.

The reminder of this work is organized as follows. Section 2 describes the ba- sis of the RTWM algorithm. In Section 3, we present the improvements related to redundant calculations, memory access, and the estimation method. Section 4 is dedicated to the multithreaded version. Experimental results are presented in Section 5. We conclude this work in Section 6.

However, when noise follows a heavy-tailed distribution bigger than those of the Gaussian distribution, the performance of this method degrades significantly. The Laplace distribution is a distribution that maximizes the probability when the location parameter is adjusted to the median [11]. This distribution is usually used to model phenomena that have heavy-tailed distribution bigger than those of the Gaussian distribution, for example, impulsive noise.

number of iterations has been made. The method used to determine the weighted median in each of the iterations has an important influence on the performance of the algorithm. In this work we use the method described in [12], whose algorithmic complexity is O(N ), with N being the size of the vector, and consists of selecting the pivot so that it is an element close to the median and thus increase the amount of items discarded per iteration.

the regularization parameter depends on the value of the component to be estimated. The closest known value is that obtained in the previous iteration and is therefore used for estimation. On the other hand, since the value of the numerator is unknown, it is suggested that the regularization parameter be refined as the iterations happen and the solution vector converges. In this way, initially this parameter is given high values to favor dispersion, decreasing on each iteration; thus, it is possible to have non-null components with less impact on the constitution of the estimated vector.

This optimization represents to save N multiplications and N sums, replacing them with just one sum. This is an important optimization because operations performed with complex floating point numbers and double precision are expensive at processing time. The Algorithm 2 shows the modifications made with respect to the original version presented in Section 2 on the lines marked in red.

b = 1 have been used. It shows that for the first iterations, no progress is made towards convergence. This can be attributed to the fact that in this region the regularization parameter has not been reduced enough to allow the appearance of non-null components and thus reduce the error in the estimate.

The parallelization scheme proposed for the RTWM algorithm consists on distribut- ing the estimation of the components of the transformed vector among the execution threads. In this way, the inner loop executed on each iteration is parallelized. This implies that each thread must be able to calculate the vectors z and w, calculate their weighted median, and update the vector x. In addition, the results of these updates must be gathered to construct the vector yj, with which the energy of the error reached in the current external iteration has to be checked.

Each thread must have private and shared memory resources, as well as synchronization mechanisms that allow avoiding race conditions when accessing shared data. This should be done trying to minimize the amount of private memory and the number of instructions made within critical sections (i.e., code sections where shared variables are accessed).

Shared memory: In the RTWM algorithm, the data structure that consumes a greatest amount of resources is the inverse transformation matrix. This consists of N 2 pairs of floating point numbers. This matrix and the y vector are not modified during execution, their entries are only used for reading. Because of this, these data structures are made available to the threads as shared memory between them.

The results of the algorithm are mainly the vector x, which contains the es- timated transform and the vector yj that maintains the estimate of the inverse transform. The vector yj is discarded at the end of the iterations, however, it main- tains the information with which the estimation of the components of the vector x are made. Therefore, the vector yj is the main channel through which threads can expose their results when working together. For this reason, the yj and x vectors are stored in the shared memory. However, to avoid excessive synchronization in- structions, each thread keeps a copy of the yj vector in its private memory and, at the end of each outer iteration, this copy vector is exchanged with yj, which has the results of the last iteration performed.

d. In this way, at the end of the estimation of the components that correspond to them, the threads can add up their accumulated contribution within the vector yj. The vector d must be reset to zero after each outer iteration.

persion and is symmetric. Therefore, to obtain vectors that, when applied the DFT, generates transformed with these characteristics, the transformed vector is first gen- erated by introducing dispersion and symmetry. Then, using the inverse transform, IDFT, the vector representing the signal without noise is calculated. The last step is to add Laplacian noise in the components of the vector obtained following the previous procedure.

energy is not used. The latter allows to appreciate in greater detail the numerical behavior of the estimate. Finally, the tests were performed on fixed dispersion percentages ranging from 0% to 90%. The experiments are carried out on an Intel Core i7-7500U computer, with four cores and 16GB of RAM, with Ubuntu.

200 a reduction in the performance of 8% can be seen, however for the other lengths an average improvement of 7% is obtained. Noting that for the latest optimizations the shorter lengths have received a cumulative improvement greater than the rest, this reduction is opposed to this, but maintains a balance of performance growth. For this reason and for the improvement in the quality of the numerical results, the adoption of this optimization is justified despite the reduction in shorter lengths.

Parallelization involves an important parameter that is the number of threads. In- creasing the number of threads does not necessarily generate a better performance in the program. This is due to the fact that the necessary mechanisms for the support and synchronization of the threads generate a considerable load in the execution of the program. For the experiments performed, a number of four threads was selected because the processor in which the algorithm is executed has four processing cores and is therefore optimized to support this number of threads.

In this paper we present five improvements in the performance of the iterative al- gorithm of calculation of robust transforms based on the weighted median operator (RTWM), presented in [9]. Improvements of up to 30% were achieved without mak- ing modifications to its estimation method and without using multi-threaded pro- cessing. By introducing modifications to the estimation method an improvement in the average performance of 93% is achieved and by incorporating the multi-threaded processing an average improvement of 97% is achieved. The development scheme

and methods used in this investigation can be applied to similar algorithms that also depend on the calculation of medians such as those presented in [13] and [14]. Since arithmetic operations represent a significant fraction of the instructions executed in the algorithm, greater improvements could be achieved by using graphic processing units (i.e., GPU), which are optimized for the calculation of arithmetic

