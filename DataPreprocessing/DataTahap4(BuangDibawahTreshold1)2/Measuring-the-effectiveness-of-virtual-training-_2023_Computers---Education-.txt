In some domains, virtual learning already plays an important role in education and training (e.g., in medicine; see Zhao, Jiang, & Ding, 2020). Much attention is now focused on developing the technical aspects of software and solving the problems that hinder its wider use. However, from the perspective of learning, another aspect cannot be neglected, namely the development of methods for testing the educational effec- tiveness of these tools. As we will show below, a rapid increase in the amount of research related to the use of virtual environments in educa- tion can be observed in recent years, even though interest in other research areas related to educational technologies has stabilized (Chen, Zou, et al., 2020; Chen, Xie, Zou, & Hwang, 2020). In light of these in- sights, it can be concluded that this field is in the process of establishing itself and now is the right time to consolidate the data obtained over the last two decades and analyze them in terms of the methods used to evaluate the effectiveness of virtual education. Systematization of the knowledge regarding the methods used so far will allow the strengths and weaknesses of each of them to be identified and will stimulate more thoughtful and theory-driven choices in future research. This, in turn, will contribute to the next step: redirecting the interest of researchers,

In modern times, life is almost impossible without any form of virtual environment (VE) or virtual reality (VR). These are defined as computer- generated displays that allow the user to perceive, feel, and interact with an environment that is similar to the physical one by using multiple sensory channels, input and output devices, and simulated scenarios (Jayaram et al., 1997; Parsons et al., 2017; Schroeder, 2008). These technologies may also be described in the reality-virtuality continuum (Milgram & Kishino, 1994). In this concept, there are two extremal points on a scale: pure reality and pure virtuality. Everything that falls between these categories is defined as mixed reality (MR). A distinction between augmented reality and augmented virtuality is also used by academics: augmented reality is the real world enhanced by virtual objects, while

augmented virtuality is a VE in which some elements of the real world are present. However, when placing environments on the reality-virtuality continuum, emphasis is most often placed on visual stimuli, but other modalities, such as sound, motion, haptics, flavor, and smell, can also be delivered through such environments (Skarbez et al., 2021).

There are many theories that can be applied in the design of educa- tional VEs. There are also studies that are not clearly embedded in any framework. In this section, the two most commonly used theoretical frameworks will be presented: Mayer's Cognitive Theory of Multimedia Learning (Mayer, 1997; Mayer & Moreno, 1998) and the Technology Acceptance Model (Davis, 1989).

Multimedia learning is defined as the reception of educational ma- terial in more than one mode (Mayer, 1997). The modes in which learning occurs are delivery media, presentation modes, and sensory mo- dalities. Almost all computer-mediated learning is multimedia learning, as it contains pictures and words. However, more advanced VT tools, such as immersive VR, can also serve as examples of multimedia learning. In immersive VR, learners are placed in a multimodal environment, where they experience not only sounds and pictures but a whole animated and interactive environment. Therefore, the cognitive theory of multimedia learning, from its basic concept, appears to be applicable to all types of VT.

In teractivity is a core concept in multimedia learning; in Mayer's theory, the learner is perceived as a knowledge constructor who actively selects and connects pieces of visual and verbal knowledge (Mayer, 1997, p. 4). It should be noted that this concept is shared by Kolb's experiential learning theory (Kolb, 1984), although Mayer's theory focuses more on cognition than on the experience. Furthermore, the design of an instructional application strongly impacts learning because it affects the learner's level of engagement in processing the material. The learner has to select some stimuli or information, then organize it into a mental model, and then integrate it into a comprehensive representation (Mayer, 1997). Mayer and Moreno (1998) draw five design principles based on

The technology acceptance model (Davis, 1989) aims to explain why people want to use a particular technological innovation. It is based, among others, on the theory of reasoned action (Fishbein & Ajzen, 1975), which uses preexisting attitudes and behavioral intention to predict actual human behavior. It also draws on Bandura's (1982) self-efficacy theory.

In the technology acceptance model, perceived ease of use and perceived usefulness serve as predictors of behavioral intention, which in turn should lead to actual behavior. Perceived usefulness is defined as the extent to which one believes that using an application will help one perform some tasks better, whereas perceived ease of use is the extent to which one believes that using an application will be effortless (Davis, 1989). The higher the perceived usefulness and perceived ease of use, the greater the chance that a person will use a system in the future.

The technology acceptance model, thanks to its simplicity, has been used in many studies on technological innovations (e.g., meta-analyses by King & He, 2006, and Schepers & Wetzels, 2007). Primarily, this theory is grounded in a vocational context, but it has also been used to explain user behaviors in other contexts, e.g., educational (Hou & Lin, 2017).

There are many different categories of the possible learning outcomes or the corresponding variables that can be used to measure training or learning effectiveness. The most popular are listed below. It should be noted that this categorization is not exhaustive as these measurements strongly depend on the specific content of the training.

First, knowledge concerning the learned topic can be measured. Declarative knowledge is intuitive and is a very convenient way of operationalizing outcomes since it may be measured and interpreted easily. Knowledge is most often measured using tests, and multiple- answer questions are the easiest way to assess and compare between individuals.

Second, skills can be measured. It is not surprising that such mea- surement is especially popular when the content of learning is practical (e.g., negotiation; Ding et al., 2020). A specific category of skill mea- surement is skill transfer. Skills can be taught and measured in various contexts. While the easiest-to-implement solution utilizes the learning context for measuring the learning outcome (in the present paper, this context is most probably some form of a VE), skills can also be measured in the target environment, which is referred to as skill transfer. Such measurement is particularly beneficial because it makes it possible to determine if the acquired skills can be transferred from an artificial environment to a real one that is similar or identical to the one in which the skills will be actually used in practice.

A common construct that is used in studies of training effectiveness is motivation, which is extremely useful as it indicates the willingness to participate in learning activities; without motivation, effective delivery of learning material is extremely difficult. Regular questionnaires (e.g., Sattar et al., 2019) are one way of measuring motivation, but due to the

The above-mentioned methods can, to some extent, be mapped onto Kirkpatrick's model of training evaluation, which has been used over many decades and remains effective (Kirkpatrick, 1976, 1994). This framework is fairly universal and can be applied to many forms of training, but it was primarily designed to work in a vocational context. Kirkpatrick begins by defining what evaluation actually means and what its goals are. The effectiveness of training programs is determined by evaluating them in order to improve existing programs and to identify and further exclude ineffective ones from practice. In this model, four levels of training effectiveness assessment are defined:

The objectives of this review are threefold. First, its purpose is to summarize the current state of the art in the effectiveness of VT assess- ment. To achieve this goal, a broad literature review was conducted. Based on the analysis of the reviewed articles, this review also aims to point out some methodological shortcomings and propose further research directions in the field of VT effectiveness. Lastly, a framework will be proposed in the hope that it will be useful for researchers con- ducting studies in this area.

Various types of technology were used in the analyzed studies. Obviously, these differences are to some extent dependent on time as new technologies become available every year, but some regularities can still be observed. The technology types were categorized according to the reality-virtuality continuum (Milgram & Kishino, 1994). Desktop and mobile device applications, as they do not fall directly into any of the categories of the continuum, were treated separately. Tools that use real-world input to enhance the virtual experience (e.g., haptic devices) were mostly placed in the Augmented Virtuality category. When different types of technology were used in one study (e.g., for different experi- mental conditions), the leading technology, which was usually the most advanced, was chosen for purposes of this categorization.

Interestingly, VT in areas such as sport and physical rehabilitation seems to have much greater potential than the number of published studies indicates. The dominance of the university context should not be surprising due to the early stage of development of these technologies. The probable reason for this is the ease of carrying out the tests and the availability of tools. However, it should be expected that, over time, the ratio of research in this context to other contexts (more directly related to possible areas of implementation) will change in favor of the latter. It cannot be expected that end users will be convinced to pay the costs of implementing such methods unless they have reliable data that confirm the legitimacy of the use of these methods in the specific domains of education in which they work.

existence and importance of using power analysis tools may gradually grow, we deliberately decided to draw papers from the last few years. Nevertheless, the obtained proportion seems unsatisfactory as it may result in erroneous research conclusions due to under- and overpowered designs. These studies may be misleading as they suggest the existence of effects that may not actually matter in pedagogical practice.

design. To briefly check whether there are reasons for considering this speculation, we randomly selected 20 papers from the two previous years (10 from 2019 to 10 from 2020; 22 experiments in total) and scanned the method descriptions in search of substantive justifications for the deci- sion regarding the sample size. The authors of only one of the scanned articles reported that they had used the G*Power application to deter- mine the sample size. Due to the assumption that awareness of the

2012; Hays & Vincenzi, 2000; Wener et al., 2015). The shortest training time reported was approximately 2 min (Burigat & Chittaro, 2016). It should also be noted that training time was not explicitly reported in many of the studies (117, 35%). However, this information should be provided so experimental procedures can be fully understood.

Because longitudinal designs are more complex but particularly informative, we decided to look at them in detail. In the next step, the 67 studies that were based on multiple VT sessions were analyzed. The following information was collected for each individual study which covered multiple sessions: the length of a single session, the number of sessions, the delay between sessions, and the length of the entire study program.

Regarding the length of the delay between sessions, there were 47 cases of missing data; therefore, this part of the analysis is the least reliable. It should be noted here that a large amount of missing infor- mation was observed in the reviewed articles in other cases, not only related to the delay length. Such a lack of attention to detail in describing study procedures makes it difficult to properly assess the methodology and results of studies, and it makes replication virtually impossible. Moreover, from the pedagogical perspective, these deficiencies may prevent (and certainly discourage) attempts to implement the studied pedagogical methods in practice. It is easy to imagine that an educator who is planning to attempt to implement a VT method that has been well

As noted above, many studies used more than one method; in particular, using a combination of methods from two different categories was the most popular choice (181 studies, 55%). A large number of studies used only one method to evaluate training effectiveness (134 studies, 41%), while only 15 studies used a combination of three different methods (5%). Approaching the evaluation of training effectiveness from at least two perspectives appears to be a good choice as more unique characteristics of the studied learning method can be captured. Omitting subjective experience evaluation could lead to the creation of tools that are highly effective in teaching certain skills or knowledge, but are at the same time extremely frustrating or stressful for end users.

As learning outcomes are, in most cases, the intended direct effect of the use of pedagogical methods, it is not surprising that learning out- comes played a significant role as the dependent variable (irrespective of the measurement method, be it subjective, objective, observational, or physiological). Learning outcomes can be measured using different methods and indices. Sometimes simple objective methods (such as knowledge or skill tests) are used; other constructs are sometimes used to draw conclusions regarding the learning outcomes achieved. The reviewed articles were analyzed in terms of the learning outcomes measured. It is not surprising that the most commonly used learning outcome metrics were subjective evaluation of training (measured by

the sample sizes were considerably large (over 2000 in some cases; Rein et al., 2018), but such large studies make it impossible to conduct more in-depth analyses with the use of more-complex methods. On the other hand, the studies which had small sample sizes (the smallest number of participants was 4 in the study by Herrington & Tacy, 2020) allowed those researchers to perform in-depth analysis of learning effectiveness, but this comes at the expense of limited quantitative inference possibil- ities. Triangulation of methods, individual interviews, and even psy- chophysiological measurements are possible with smaller sample sizes. This trade-off is difficult to solve, and the best solution depends on the specific objectives of a particular study. However, there is a gap for well-designed experiments with sound methodology and sample sizes that are sufficient to perform proper statistical analyses.

How the experimental groups were defined may raise objections in relation to some studies. In the case of 46 studies, there was no control group; moreover, as a point of reference for comparisons, another 9 studies used a group of people deprived of learning. In practice, such a decision makes it impossible to draw conclusions about the effectiveness of the tested methods.

challenge to researchers. Many different self-report methods are used, of which some more popular ones can be highlighted, such as the NASA- Task Load Index (Hart & Staveland, 1988) or the System Usability Scale (Bangor et al., 2008; Brooke, 1996). For performance evaluation, different indices (e.g. time, error rate) are used, but these must be content-specific to some extent. The evaluation part is just a small section of many of the reviewed papers, while the entire text focuses on describing the process of developing the educational application. While this may be very useful, the lack of systematic and methodologically sound evaluation leads to many different tools being developed which are used primarily for academic purposes but are not necessarily applied in practice. This variability in methods and evaluation techniques makes different studies on the topic incomparable or at least extremely difficult to compare. The development of and adherence to some kind of evalu- ation framework could be beneficial to researchers and users of VEs.

It is also important to account for publication bias. In many of the reviewed studies, virtual tools proved to be effective in some way. However, we cannot know how many studies produced nonsignificant results or proved the inferiority of VT and were thus never published. Therefore, all conclusions about the extent to which VEs are effective for learning purposes should be treated with caution.

Second, we propose that at least two measurement time points should be used, but ideally it would be three: a pretest, an immediate post-test, and a retention test. Using a pre-test and a post-test allows gain scores to be calculated, previous knowledge to be accounted for, and ceiling effects to be controlled; using a retention test allows the learning curve to be observed. Knowing that such delayed measurements may result in participant dropout, we suggest that a week-long delay is a reasonable choice as it is a good balance between minimizing the risk of participant dropout and maximizing the benefits of a knowledge retention mea- surement. For retention tests, we suggest emailing participants with a link to an online tool.

Third, the learning outcomes that are measured in a study should be carefully selected. We recommend that at least two types of outcome are used to ensure a broad understanding of the learning process. In the process of choosing the learning outcomes and the methods by which they will be evaluated, theoretical frameworks should be applied. For example, Kirkpatrick's approach can be used, and methods to measure the Reaction and Learning levels should be carefully chosen (Kirkpartick, 1976; 1994). When applicable, measurement of the higher levels of Kirkpartick's model (Behavior and Results) can also be used. However, these levels can be difficult to measure in an experimental model since the learning process is very often not an inherent part of a specific workplace or educational institution. Concepts from the technology acceptance model (Davis, 1989) can be used to measure reactions to

Lastly, taking into account publication bias and the realities of pub- lishing scientific papers, it is worth considering minimizing the risk of committing errors of the first and second type (sample size determination and power analysis). This issue is naturally related to the possibility of research preregistration, which should also be considered. Moreover, considering that much research is conducted on VEs that are the subject of development work in which researchers themselves participate, particular attention should be paid to identifying potential conflicts of interest.

hope that such a broad perspective will prove to be a valuable benchmark for researchers. There are two limitations to this framework. First, some or all of the criteria we suggest may seem obvious for many readers. Second, our intention was to propose the most-general guidelines that could be implemented in many domains after appropriate adaptation. Despite these limitations, we believe the present review provides valu- able insights into the methodology of research on VR-based learning effectiveness and could serve as a basis for future studies focused on this issue.

Yang, Q. F., Chang, S. C., Hwang, G. J., & Zou, D. (2020). Balancing cognitive complexity and gaming level: Effects of a cognitive complexity-based competition game on EFL students' English vocabulary learning performance, anxiety and behaviors. Computers & Education, 148, Article 103808.

