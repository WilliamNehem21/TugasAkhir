which train them are, at their core, a special kind of computer program. One per- spective on programs which is relevant in this domain are so-called state-and-effect triangles, which emphasize the dual nature of programs as both state and predicate transformers. This framework originated in quantum computing, but has a wide variety of applications including deterministic and probabilistic computations [6].

In recent years, it has become apparent that the architecture of a neural network is very important for its accuracy and trainability in particular problem domains [3]. This has resulted in a profligation of specialized architectures, each adapted to its application. Our goal here is not to express the wide variety of special neural networks in a single framework, but rather to describe neural networks generally as an instance of this duality between state and predicate transformers. Therefore, we shall work with a simple, suitably generic neural network type called the multilayer perceptron (MLP).

Outline. In this paper, we begin by describing MLPs, the layers they are com- posed of, and their forward semantics as a state transformation (Section 2). In Section 3, we give the corresponding backwards transformation on loss functions and use that to formulate backpropagation in Section 4. Finally, in Section 5, we discuss the compositional nature of backpropagation by casting it as a functor, and compare our work in particular to [1].

In a recent paper [1] a categorical analysis of neural networks is given. Its main result is compositionality of backpropagation, via a description of backpropaga- tion as a functor. In this section we first give a description of the functoriality of backpropagation in the current framework, and then give a comparison with [1].

In this paper, we have examined neural networks as programs in a state-and-effect framework. In particular, we have characterized the application of a neural network to an input as a kind of state transformation via Kleisli composition and backprop- agation of loss along the network as a kind of predicate transformation on losses. We also observed that the compositionality of backpropagation corresponds to the functoriality of a mapping between a category of states-and-effects to the category of neural networks.

For the sake of illustrating this perspective on neural networks, we have delib- erately chosen a simple subclass of the known network architectures and built a category of multilayer perceptron (MLPs). However, we believe it is possible to develop a richer categorical structure capable of capturing a much wider variety of network architectures. This may be the focus of future work.

