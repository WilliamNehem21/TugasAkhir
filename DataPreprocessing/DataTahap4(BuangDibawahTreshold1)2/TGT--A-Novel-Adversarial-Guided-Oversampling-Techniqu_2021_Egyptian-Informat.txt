With the volume of data increasing exponentially, there is a growing interest in helping people to ben- efit from their data regardless of its poor quality. One of the major data quality problems is the imbal- anced distribution of different categories existing in the data. Such problem would affect the performance of any possible of analysis and mining on the data. For instance, data with an imbalanced distribution has a negative effect on the performance achieved by most traditional classification tech- niques. This paper proposes TGT (Train Generate Test), a novel oversampling technique for handling imbalanced datasets problem. Using different learning strategies, TGT guarantees that the generated synthetic samples reside in minority regions. TGT showed a high improvement in performance of dif- ferent classification techniques when was experimented with five imbalanced datasets of different types.

A collection of data is called imbalanced if one class instances - was higher in number than the other. The class with more instances is referred to as the majority class, and the one with fewer instances is called the minority class [1,2]. Numerous recent researches on imbalanced datasets have generally agreed that because of this skew distribution of classes, the classifiers are biased towards the majority class and give very low classification accuracy towards the smaller classes. Classifier may also clas- sify any sample as the majority class and ignore the minority class [3].

tion [5]. The idea of sampling is based on changing the dataset so that a more balanced class distribution is created. Methods of sam- pling can be subdivided into oversampling and undersampling. Undersampling eliminates the number of instances of the majority class while oversampling generates minority class synthetic instances during preparation.

The rest of this paper is organized as follows. Section 2 presents the related work while section 3 introduces the proposed tech- nique to handle imbalanced data classification problem. Sections 4 and 5 show the details of the performance evaluation and con- cludes the paper.

[11] to resolve the SMOTE shortcomings on nonlinear problems through oversampling in the SVM feature space. WK-SMOTE mod- ifies SMOTE for non-linear separable data by producing the syn- thetic instances in the classifier feature space rather than the input data space. In comparison with the other baseline approaches on several metrics, the suggested oversampling algo- rithm together with a cost sensitive SVM model have demon- strated an enhancement in performance. Therefore, a hierarchical structure for multiclass imbalanced issues with a progressive class order is created. The suggested WK-Smote and the hierarchic structure are tested on real world industrial fault detection system. Two main variables are defined in SMOTE: N oversampling value and the k-neighbors. Nevertheless, in practical implementa- tions, the two variables randomly selected by users cannot be opti-

mized. However, the imbalance ratios of the data are completely distinct, making it more difficult to pick parameters in SMOTE. The authors in [12] proposed a new oversampling method relying on SMOTE to address the problem. This turns the problem of parameter selection into a multi-target optimization problem in SMOTE. To achieve their optimal solution, a new selection tech- nique called absolute dominance-based selection was introduced to search best values for SMOTE parameters [10].

In [15], the authors are presenting a novel technique of Self- Organizing Map based Oversampling (SOMO), that generates two-dimensional representations of the input space by applying a Self-Organizing Map to enable the effectiveness of artificial data point generation. SOMO consists of three main steps: First a Self- Organizing Map gives the original, typically high-dimensional, space a two-dimensional representation. Then it produces artificial instances in a cluster and then produces synthetic instances be- tween clusters. They also performed empirical experiments which enhanced the performance of methods, when SOMO produces arti- ficial data.

A new method for oversampling is proposed in [17] which uses the real value negative selection (RNS) method for generating syn- thetic minority instances with no real minority data necessar- ily available. The generated minority instances with rare actual minority data are merged with the majority data in order to pro- vide a method for the binary classification learning. In their exper- iments, they show the efficacy of RNS to prevent the over-sampling problems faced by conventional approaches such as noise genera- tion and redundant samples in the same clusters. However, we noticed from the results it performed well only for severely imbal- anced datasets.

The authors in [18] are suggesting radial based oversam- pling methods (RBO), which can identify areas in which minority class artificial instances are to be produced on a radial base depending upon the imbalance distribution estimate. They take into consideration data got from all classes, in contrast to tradi- tional multi-class over-sampling methods that use only minority class data. Experiment results conducted on a typical dataset indi- cate that the RBO artificial over-sampling technique offers a promising alternative to the current imbalanced dataset solutions. A three-way decision model (CTD) is introduced in [19] where the costs of choosing main samples are taken into consideration. First, the CTD uses Constructive Covering Algorithm (CCA) to sep- arate minority instances into multiple coverage. Each cover is then selected and divided into three regions based on the coverage den- sity. Finally, according to the model of cover distribution on minor-

The authors in [20] introduced generative adversarial minority oversampling (GAMO). The idea is that producing synthetic points near the borders of the minority class will let the classifier to learn class boundaries which are more robust to class imbalance. The convex generator produces synthetic points as convex combina- tions of the existing points from the minority class. They intro- duced also an additional discriminator which ensures that the generated points belong to the real distribution of the intended minority class.

The first step is training step which is utilized to train two clas- sifiers on the given data: a decision tree and a neural network. The decision tree is trained to get the minority class classification rules which will later be used to generate the synthetic samples. Then, a neural network is trained to classify the minority and majority classes data. The neural network will be later used for testing the generated samples. Despite the training on imbalanced data, the two classifiers are expected to extract and use all the available knowledge about the minority class in the data. This would guar- antee that the new generated samples follow the distribution of the minority class data even though its scarcity.

Training step details is described in Algorithm1. It contains two functions. First one is BuildDT, which takes the training data along with class label, uses Gini index for selecting the attribute that best classifies the instances and outputs arrays containing the upper and lower value for each attribute. The second function is NN which takes the majority and minority sets to be trained and then used later in verifying generated samples.

The last step is the testing step. In this step, the generated sam- ples are tested or verified using the trained neural network. If clas- sifier shows that the sample belongs to minority class, then it will be kept in new synthetic samples array. Otherwise, it will be dis- carded. The neural network is considered to be an unexplainable classifier that identify the class based on the finding the right weights of network neurons in order to find the right classification. Using such classifier would ensure that the generated sample of minority class through explainable classifier is verified by another classifier that works in a completely different and unexplained method.

The objective of the evaluation is to prove the effectiveness of the proposed adversarial guided oversampling technique. The pro- posed technique aims to balance the class distribution of data by making guided oversampling though using minority class rules dri- ven from training the decision tree on the dataset. And then check- ing the generated samples using well-trained neural network to assure they all belong to minority class. Towards this goal, the pro- posed technique is evaluated on different datasets over different classifiers against SMOTE method which is the baseline oversam- pling approach in the literature and also against one of its very recent smote variations which is Modified SMOTE. The following subsections describes the evaluation details.

[23,24]. Accuracy is the most common performance metric in prac- tice, particularly for binary and multi-class classification issues, as seen in various studies [25,26]. Sensitivity determines the amount of real positive that is correctly classified as such, while speci- ficity determines the amount of real negative that is correctly iden- tified. In other words, Specificity metric is used to measure the fraction of negative patterns that are classified correctly. Conse- quently, sensitivity takes into account the prevention of false neg- atives, and for false positive specificity does likewise [27].

Three different classifiers, K-Nearest, Fuzzy K-Nearest, and Sup- port Vector Machines classifications, were used for performance evaluation. The KNN classifier takes only one parameter and the best results got when k = 10 while the FKNN classifier takes two parameters k and m where k = 10 and m = 0.5. Classifiers were trained on different settings to compare the proposed oversam- pling technique against SMOTE and modified SMOTE. They were trained once on data without oversampling, once with data after oversampling it with SMOTE, once with data after oversampling it with modified SMOTE and finally with the data after oversam- pling it with TGT.

algorithm is variant on the five datasets. This may be related to the ratio between the majority and minority classes in the original data. The results show that the proposed technique performs extremely well when the ratio between the majority and minority classes in the original data is high as in Indian Diabetes and Kc2 Software Fault Prediction datasets.

The outperformance of the proposed technique against SMOTE algorithm [7] and modified SMOTE [13] may be related to the dif- ference between how the three methods work. Traditional SMOTE algorithm generates synthetic samples in the space of the minority data space, the Modified SMOTE algorithm generates synthetic samples in the space between minority and majority data. Both methods generate a new synthetic sample by analyzing a randomly chosen sample of minority class. On the other hand, TGT generates the synthetic samples guided by the classification rules of the minority class derived by training the dataset of the decision tree. After the guided generation, these generated samples will be veri- fied using the well-trained neural network to assure that all syn- thetic samples belong to minority class. Otherwise, they will be discarded. Those double-checked new samples generated by the guided adversarial oversampling technique have led to better clas- sifications which proves our initial argument. In addition, the interaction between explainable and non-explainable classifier and the analysis of the whole dataset has shown to be effective enough to generate new synthetic samples better than those gen- erated by SMOTE and modified SMOTE which depend on the anal- ysis of individual samples to generate new samples.

This paper presented an adversarial guided oversampling tech- nique (TGT) for handling the imbalanced datasets. The proposed technique utilizes two classifiers to extract and model the knowl- edge about the minority class data. A decision tree is trained on the given data to model the minority class data as set of classification rules where those rules are used to generate new samples of minority class. Then, a neural network is trained on the given data and used to verify that all the generated samples belong to the minority class data distribution. The proposed technique showed a higher performance when was evaluated against standard and recent data oversampling techniques over different datasets and using different classifiers.

