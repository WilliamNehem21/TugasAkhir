can be implemented, and techniques to address the overheads. Sec. 3 presents our approach to evaluating parallel state-space exploration algorithms, showing how overheads can be measured, how models can be constructed to assess the over- heads, and how the results from real models can be put into context. We round off the paper with a discussion of related work (cf. Sec. 4) and our conclusions (cf. Sec. 5).

One of the great difficulties of parallelising state-space exploration algorithms is understanding how to efficiently implement a devised parallel algorithm. The types of languages and libraries chosen for parallelisation can have a significant effect on the performance of the parallel algorithm, since their characteristics can influence overheads. The importance of these choices can be shown when inspecting the work of Inggs [19]: the use of Java led to high synchronisation overheads for memory allocation and garbage collection, and eventually Inggs used C to implement her parallel algorithm. Language selection can therefore be costly in time and effort if it hinders parallelisation.

Selecting an appropriate architecture is also an important decision when con- sidering how to parallelise a state-space exploration algorithm. Availability is a key issue when choosing hardware. Multi-processor, multi-core PC are becoming widely available for performing experimental analysis on parallel algorithms. The drawbacks of this type of machine are that they only offer a relatively small number of processors/cores and that secondary cores are approximately 30-40% less efficient than processors. Larger shared-memory machines can offer more processors for per- formance evaluation but are less readily available. PC clusters are easily available but incur communication overheads across the network. In addition, operating sys- tem choice is often tied to the machine under usage. Different operating systems schedule their threads in different ways, which can affect scheduling overhead [17]. Another operating system decision is related to tool support, where parallel tools that can aid the development of the algorithm may only be able to run on particular operating systems (see also Sec. 3).

When considering how to parallelise a state-space exploration algorithm, techniques for addressing parallel overheads must be well chosen to minimise their impact. The most common technique for addressing load imbalance caused by irregularity is dynamic load balancing, specifically workstealing techniques. These have been used in parallel state-space exploration algorithms to facilitate orders of magnitude improvements in time-efficiency [15,19]. We applied workstealing to parallel Satu- ration [8], and our results using this technique demonstrated speedups on several models including a super-linear speedup. Workstealing is based on the principle that, when one processor runs out of work to do, it steals work from another pro- cessor. For instance, if processors are given a number of states to enumerate, a processor completing its work can attempt to steal states from other processors. While this can be effective in spreading work to multiple processors efficiently, the technique introduces its own overhead from extra code and synchronisation. Thus, in order to improve the run-time of the parallel algorithm, sufficient parallel work must exist for the technique to spread fully across the available processors.

The quality of the measurement of parallel overheads, and how it reflects on the performance of a parallel state-space exploration algorithm, is a key issue that has yet to be addressed in the literature. The overheads of the algorithm are usually measured through some form of estimation, such as the distribution of states across processors to indicate load (im)balance. When we tried to estimate the amount of parallel work arising from our next-state function, we found that the influences on the parallelisation are much more complex than the factors we initially considered [9]. Thus, estimates of overheads may not be an accurate measurement of their true impact on a parallel algorithm, which brings their contribution towards an objective evaluation into question.

We now address the problem of measuring parallel overheads for each type of overhead in turn. Load balancing is difficult to measure. If we count the number of states enumerated on a processor and discover that this number is fairly even on different processors, we could argue that the load is well balanced amongst processors. However, what has not been taken into account here is the way in which work has been scheduled: due to the dependencies between states, the processors

While parallel overheads can be identified as a general influence on an algorithm, a subtlety of state-space exploration algorithms, and one which is not discussed in related literature, is the affect of the model on the severity of the overheads. We found this out the difficult way in Saturation, since some models showed good performance using the parallel algorithm, and others showed a lack of parallelisabil- ity [9]. At first glance these results can suggest that the parallelisation is inefficient, which is a highly frustrating point to consider when a lot of work has gone into the parallel algorithm. Our experience from investigating the effects of the underlying model can hopefully be used to alleviate future frustration relating to this point, by illustrating that the parallelisation efficiency is highly dependent upon the model as well as the techniques that have been used to address overheads.

(b) has high scheduling, and (c) is an ideal model that can be parallelised well with little overheads. Using models matching profile (a) one can ascertain whether the load balancing function of the parallel algorithm under investigation can be im- proved, by attempting to increase the amount of fully parallel processor utilisation. Using models matching profile (b) one can attempt to improve the scheduling tech- nique, by reducing the scheduling overhead. Using models matching profile (c) one can try to elaborate on various parallel overhead techniques, where any increase in fully parallel processor utilisation and decrease in scheduling and synchronisation overhead is desirable. These profiles highlight inefficiencies that can be used for the optimisation of parallel overhead techniques and allow for a quantitative comparison of the performance of different techniques. They also facilitate the understanding of how the effectiveness of the techniques can be challenged by the models under consideration.

Most of the work on parallel state-space exploration has focused on net- works of workstations (NOWs), primarily using static partitioning of state- spaces [2,3,4,6,11,24,25,26,27]. The evaluation of the proposed parallel algorithms usually involved benchmarking in terms of run-time and some estimation of the work distribution and communication overhead. To the best of our knowledge, only one paper gives a breakdown of the run-time overheads [18].

Our approach to evaluation is unique, firstly, with regards to the quality and thoroughness of parallel overhead measurement and, secondly, in the way we choose models to measure specific overheads. Using a profiler that provides a direct mea- surement of overheads, while taking into account the cost of its own instrumen- tation, assures the accuracy of measurement. Combining a thorough evaluation of a parallel state-space algorithm with a carefully selected benchmark of its run- time performance serves to provide a clear picture of how efficiently parallelised the algorithm is.

