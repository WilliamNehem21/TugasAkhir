Abstract In this paper, an algorithm is proposed to learn and evaluate different finite mixture models (FMMs) for data clustering using a new proposed criterion. The FMM corresponds to the minimum value of the proposed criterion is considered the most efficient FMM with compact and essential components for clustering an input data. The proposed algorithm is referred to as the EMCE algorithm in this paper. The selected FMM by the EMCE algorithm is efficient, in terms of its complexity and composed of compact and essential components. Essential components have minimum mutual information, that is, redundancy, among them, and therefore, they have minimum overlapping among them. The performance of the EMCE algorithm is compared with the perfor- mances of other algorithms in the literature. Results show the superiority of the proposed algorithm to other algorithms compared, especially with small data sets that are sparsely distributed or gen- erated from overlapping clusters.

[19]; it tends to overestimate the number of components when cluster shapes are not Gaussian [4]. On the other hand, it tends to underestimate the number of components when clusters are overlapping or when the number of feature vectors in the given data set is small [20]. Also, both of the BIC and the MML cri- teria have poor performance with sparsely distributed data [21]. Penalized-likelihood criteria compromise the goodness of fit- ting of the FMM to the input data set with the complexity of that FMM. Since the mixture complexity is a quadratic func- tion of the number of features (dimensions) in the input data set, these criteria are sensitive to the increase of the number of features in the input data set. In the rest of this paper, the algorithms that use the BIC and the MML criteria for deter- mining the number of FMM components are referred to as the BIC algorithm and the MML algorithm respectively.

Different criteria for estimating the number of FMM com- ponents include Adaptive Mixtures algorithm that is a recur- sive form of the EM algorithm [27]. This algorithm may overestimate the number of components when the given data set contains sparsely distributed data [20]. Also, it may under- estimate the number of components when some clusters in the data space are poorly separated [21]. In addition, this algo- rithm does not have a measure that compromises the increase in the FMM complexity with the goodness of fitting of that model to the given data. A cross-validated likelihood criterion is proposed to estimate the number of components in the FMM using large data sets [28]. However, this criterion re-

pendently and identically distributed in d-feature space. The values on each feature are scaled such that they range from 0 to 1. This reduces the sparsity of the data and increases the accuracy of estimating its cluster structure [35]. The cluster structure is revealed using the FMM that fits the input data set. Each component in this model is assumed to represent a cluster in the input data set. The components of the mixture model have non-restricted Gaussian distributions. Then, using a mixture model Mk that contains k components, the density function of this data set is defined as:

In this paper, a new algorithm that is referred to as the EMCE algorithm is proposed to integrate the unsupervised learning and the optimization of the FMM. It learns and eval- uates different FMMs for clustering an input data set using a new proposed criterion that is referred to as the EMCE crite- rion. The FMM corresponds to the minimum value of the EMCE criterion is considered the most efficient FMM, in terms of its complexity, that is composed of compact and essential components for clustering the input data set. This FMM has the minimum number of components, which have the minimum within-component variation and the least mutual information, that is, redundancy among them. The rest of this paper is organized as follows: Section 2 presents the proposed

The EMCE algorithm uses both the random parameter ini- tialization and the CEM algorithm [16] in order to reduce the effect of obtaining sub-optimal results or approaching the boundary of the parameter space while learning the FMM parameters. The algorithm starts with a mixture model with large number components kmax that is twenty in the experi- ments shown in this paper. The CEM algorithm is used to esti- mate parameters of the mixture model. After convergence of the CEM algorithm, the EMCE criterion value of the current FMM is computed. The component that has the smallest mix- ing weight in the FMM is considered unnecessary. Therefore, this component can be deleted from the FMM. Parameters of the new FMM are computed by the CEM algorithm. This process continues until there is one component in the model.

components using different data sets. All algorithms are implemented, and experiments are carried out using the MAT- LAB software. Data sets used are described in Section 3.1. The method of initialization and the convergence condition of the EM algorithm are described in Section 3.2. The measure used to quantify how good the clustering results obtained from the resulting FMM from each algorithm is described in Section

This data set is also commonly used in classification analysis [36]. It consists of 178 feature vectors each of which is a vector in 13-feature space. These feature vectors represent three clus- ters whose sizes are 59, 71, and 48 feature vectors. The clusters are separable in the data space. The purpose of using this data set is to test the algorithms compared when data clusters are separated and when the number of features is large compared to the number of feature vectors.

The mutual information is a symmetric measure to quantify the statistical information shared between two distributions [39]. Therefore, this measure is used to quantify how good the clustering results obtained using a FMM for a certain data set is by comparing it to the true classification of this data set [40]. Let X and Y be two random variables represent the true class labels [1 ... m] for a certain data set and the cluster labels [1 ... k] resulting from a FMM clustering for the same data set,

where H(X) and H(Y) denote the entropy of X and Y. The NMI has the value of 1 when there is a one to one mapping between the clusters obtained and the true classes (i.e., k = m) of a given data set. Since this measure is not biased to- ward large k, it is preferred to compare different data parti- tions [40,41].

algorithm results in the largest NMI criterion values and the correct number of mixture components with all data sets. These results show that the EMCE algorithm is less sensitive to the curse of dimensionality than all other algorithms com- pared. This is because the proposed EMCE criterion only de- pends on the characteristics of the FMM representing the input data set such as the within-component/cluster variation, the mutual information among components/clusters and the rela- tive mixing weights of components/clusters. On the other hand, the criteria used in the other algorithms compared de- pend explicitly on the dimensionality of the input data set

compact and essential components (EMCE) algorithm is pro- posed. It is based on a new proposed model selection crite- rion, called the EMCE criterion. This algorithm overcomes problems of the algorithms that use the penalized-likelihood or the Mutual Information criteria with small and sparse data. This algorithm produces a single frame for model esti- mation and selection for data clustering. Empirical analysis shows that the proposed algorithm outperforms the TUMI, the MML, and the BIC algorithms, especially with small and sparse data that may be generated from overlapping clusters.

