(Wand and Vu,2018) used the GRID dataset to train and test a deep neural network. (Wei et al. 2018)suggested a novel system for lipreading called a densely associated convolution network, which captures visual representations from color images. The authors worked on 3D lip physiological features based on the position and structure of the face. (Petridis et al. 2018) proposed a hybrid algorithm for audiovisual recognition. The authors applied CTC and the attention architecture to the LRS2 database for audiovisual speech recognition.

(Jadczyk,2018)conducted a Polish audiovisual voice recognition project wherein the work was divided into three subcategories: audio, visual speech, and integration. For the audio, they employed MFCC to extract the features, an HMM to extract the lip features, and multistream HMM for incorporation. (Shashidhar and Patilkulkarni,2021) worked on a traditional lipreading database. The authors used a custom 250-item dataset and applied a pre-trained model called VGG16 for better accuracy. (Cornejo and Pedrini,2019) proposed a deep CNN (DCNN) for audiovisual voice detection. Their approach first involved separating the audio from the video and then extracting audio data using two-dimensional CNN. They also extracted visual speech using principal component analysis and linear discriminant analysis and translated it into the census transform. Finally, audio and visual feature extractions were merged to improve the accuracy of audiovisual voice detection. Their method showed promising results on various benchmark datasets, demonstrating the effectiveness of DCNNs for audiovisual voice detection(Shashidhar and Sudarshan, 2022).

The authors introduced a new approach called a multimodal sparse transformer network (MMST). This method employs a sparse self-attention mechanism to focus selectively on important parts of the data and thus improve global information processing (Dupont and Luettin,2000) demonstrated the high performance of their proposed system on a large database of continuously spoken digits involving multiple speakers. The authors used a combination of the MSHMM, denoised MFCCs, and visual features to improve the results of multimodal isolated word recognition. They demonstrated that this approach can lead to better word recognition accuracy than using only audio features(Nefian et al. 2002).

In formation of the database videos are recorded at a resolution of 1080 pixles with a smooth frame rate of 60 Frams Per Second, resulting in high-quality footage. Each video has an average duration of 1 to 1.20 seconds, and their file size typically falls around 10 megabytes.

CNNs are a class of artificial neural networks inspired by the human visual system. They have proven to be extraordinarily effective in solving complex computer vision tasks, making them the cornerstone of modern machine learning and artificial intelligence. DCNNs are composed of multiple layers, each with a specific role in extracting and transforming visual features from input data.

Understanding and sensing the meanings behind what people communicate by simply observing visuals without audio is quite complex. Deep learning models are used to predict what someone is saying using visual information, primarily lip movements. Voice biometric applications, for example, are often used by office and university managers to verify individuals. Physically challenged people, such as those with audio and verbal speech impairments, can use such applications to manage conversations without the involvement of audio language. The audiovisual speech recognition method proposed in this study involves a new dataset to address the computational challenges of CNNs and deep learning.

The audiovisual speech recognition method proposed here involves a new dataset and a feed-forward neural network that addresses the computational challenges of CNNs and deep learning. The architecture includes a 1D CNN model for audio, an LSTM model for visual, and a DCNN for integration. The authors achieved a training accuracy of 94.67% and testing accuracy of 91.75% using a custom dataset. The results demonstrate that combining audio and visual inputs yields the best performance, as evidenced by the superior performance of the proposed method compared with existing methods.

