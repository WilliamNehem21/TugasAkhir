Abstract In this paper, a new algorithm is presented for unsupervised learning of finite mixture models (FMMs) using data set with missing values. This algorithm overcomes the local optima problem of the Expectation-Maximization (EM) algorithm via integrating the EM algorithm with Particle Swarm Optimization (PSO). In addition, the proposed algorithm overcomes the problem of biased estimation due to overlapping clusters in estimating missing values in the input data set by integrating locally-tuned general regression neural networks with Optimal Completion Strategy (OCS). A comparison study shows the superiority of the proposed algorithm over other algorithms commonly used in the literature in unsupervised learning of FMM parameters that result in mini- mum mis-classification errors when used in clustering incomplete data set that is generated from overlapping clusters and these clusters are largely different in their sizes.

from its classes [5]. When there is sufficiently large number of observed values, these algorithms outperform the EM algo- rithm combined with either the unconditional mean imputation or the conditional mean imputation of the missing values according to the classification performance of the resulting FMM [5]. It is shown that better results can be obtained by imputing missing values using the distribution of the input fea- ture vectors rather than using a priori probability distribution function used in the FMM [5,7]. However, these modified EM algorithms have poor performance in learning FMM parameters when clusters of the input data set are largely over- lapping and unbalanced in their numbers of feature vectors [5]. This is due to the fact that these algorithms estimate missing values in a certain feature vector only from either parameters or members of one component of the FMM to which this feature vector has the maximum posterior probability. The pos- terior probability is computed using complete values of each feature vector. This ignores the overlapping among clusters of the data set that is represented by FMM components. This overlapping means that feature vectors of the data set are generated from different components of FMM with different probabilities. Therefore, these algorithms produces inaccurate estimation of missing values which in turn leads to inaccurate estimation of FMM parameters learned from the whole data set.

In this paper, a new algorithm is proposed to overcome problems of the modified EM algorithms for unsupervised learning of the FMM parameters using incomplete data [4,5]. These problems are is the sensitivity to the occurrence of out- liers in the data, the overlap among classes in the data space, and the bias in generating the data from its classes i.e., clusters of the input data set are unbalanced in their numbers of feature vectors. The proposed algorithm is less sensitive to the learning problems of the EM algorithm in cases such as the occurrence of outliers in the data set, the overlapping among data classes, and the unbalanced representation of data classes. The rest of this paper is organized as follows. Section 2 presents the pro- posed algorithm. Section 3 shows results of the comparison study that is carried out to evaluate the performance of the proposed algorithm. These results are discussed in Section 4. Conclusions are presented in Section 5.

Step 1: Linearly scale the values of each feature in the input data set to make them lie in the interval [0,1]. This process removes the effect of different unit scales on different fea- tures, and therefore it is essential for finding the true cluster structure of this data set and the contribution of each fea- ture in the creation of this structure. A comparison of several methods of data normalization for cluster analysis has shown the superiority of the linear scaling method over many other normalization methods before applying cluster- ing algorithms in terms of the resulting cluster separation and error condition [16,17]. In addition, determine the opti-

GREM does not have the limitation of the general regression neural networks [20] when used for estimating missing values without local tuning with weakly correlated data [5]. In the experiments presented in this paper, the algorithm that uses general regression neural networks with the EM algorithm for learning FMM parameters is referred to as the GREM algo- rithm. In addition, local tuning of the general regression neural networks used in the POLGREM algorithm makes the algo- rithm be less dependent of FMM parameters. Therefore, the POLGREM algorithm overcomes the limitation of the MEM algorithm with small data sets which may contain outliers, overlapping clusters, or large differences in the sizes of their clusters [5]. Finally, due to the use of the OCS in estimating missing values the POLGREM algorithm produces accurate estimation of both the missing values in the data set and FMM parameters, especially when clusters of the data set are largely overlapping and different in their sizes. Therefore, the POLGREM algorithm overcomes problems of cluster overlap- ping and unbalanced cluster sizes that are limitations of the algorithm proposed in [5] and is referred to as the LGREM algorithm in the experiments presented in this paper.

estimating the missing values in the input data set using the unconditional mean imputation method (MENEM) and the nearest neighbor imputation method (NNEM). In the MENEM algorithm, the missing values in each feature are replaced with the mean of the observed values in that feature. In the NNEM algorithm, each missing value in a certain feature vector is replaced with the observed value in the same feature that is in the nearest feature vector according to the Euclidean distance in the subspace that is composed of the fully observed features. The comparison study in this paper uses three data sets. The missing values are randomly placed with different missing rates in two features of each data set. These features are selected such that the visual separation among data classes is maximum. The mechanism of the occurrence of the missing values in all data sets is missing completely at random (MCAR) [21]. These data sets are described as follows.

Correlations between different pairs of features of this data set are too weak. The missing values are put in the second and in the fifth features of the data set. Each one of the FMMs learnt by all algorithms compared in this study consists of two Gauss- ian components that have non-restricted covariance matrices.

The evaluation criterion used in this study to compare the different algorithms is the Mis-Classification Error (MCE). It is computed by comparing the clustering results, obtained using Bayes decision rule, of the learned FMM with the true classification of the data feature vectors, assuming each class is represented by a component in the FMM. Components of the FMM are allocated to different data classes such that the total number of misclassified feature vectors in the data set is minimum. Let the number of feature vectors belonging to class i be Ni, from which Nm feature vectors are not clustered into the component that represents this class in the FMM. Then

This data set is similar to the first data set but an outlier fea- ture vector is added to it. The outlier feature vector is [max (d1), min (d2), 0.5 max (d3), 0.5 max (d4)]T, where di, i = 1:4 are the features of the data set.

The Pima Indians Diabetes data set1 contains 768 feature vectors each of which is a vector in eight-feature space. These feature vectors represent two classes; the first class has 500 fea- ture vectors belonging to it; the second class has 268 feature vec- tors belonging to it. These classes are largely overlapping.

In this paper, the POLGREM algorithm is proposed to over- come the local optima problem of the EM algorithm and the bias problem in estimating missing values when the input data set contains overlapping clusters in learning FMM parameters for clustering using incomplete data sets. A comparison study shows the superiority of the POLGREM algorithm over other algorithms compared when the input data set may contain few outliers, clusters that are largely overlapping, or clusters that have large differences in their sizes. Examples of these algo- rithms are the LGREM algorithm [5], the MEM algorithm [4], the GREM algorithm and the common EM algorithm after estimating the missing values in the input data set using the unconditional mean imputation method (MENEM) and the nearest neighbor imputation method (NNEM).

