Particle swarm optimization (PSO) only uses primary mathe- matical operations and can obtain a high convergence speed. For PSO converges fast and can obtain high performance, it is widely applied to function optimization and real applications. Since the inception of PSO in 1995, a lot of PSO variants have been reported, the performance of PSO has been improved significantly. However, most of the current PSO variants can only perform well on a group of certain optimization problems, fewer PSO can achieve high per- formance on various types of optimization problems.

for hybrid DE and PSO algorithms. Developing high performance hybrid DE and PSO algorithm still attracts the attention of EC com- munity. To fully use the advantages of DE and PSO algorithms, this study proposed a mutual learning strategy for hybridizing DE and PSO. Furthermore, an elite mutation is introduced to accelerate the convergence speed of DE subswarm. The rest of this paper is orga- nized as follows: Section 2 reviews the related works, Section 3 proposes the methodology, section 4 conducts experiments and

information in both DE and PSO subswarm. In the mutual learning strategy, the DE and the PSO subswarm work in parallel, and the elite information is exchanged between two subswarm to improve the population quality. The position vector of PSO subswarm is employed for DE mutation to speed up the response of DE sub- swarm, furthermore, the elite DE mutation is employed for acceler- ating the convergence speed of DE subswarm.

In this study, three group test functions are employed to test the performance of MLDE-PSO. (1) 13 selected basic functions adopted by a few representative PSO variants [36] are employed to compare the performance of MLDE-PSO on basic optimization problems. (2) The aforementioned 13 basic functions are rotated to evaluate the performance of MLDE-PSO on rotated problems. (3) 30 CEC2017

Seven selective peer algorithms are compared with MLDE-PSO covering three state-of-the-art PSO algorithms, three recent PSO algorithm and one classic DE algorithms. CLPSO [6] adopts compre- hensive learning strategy to enhance the exploration of PSO. In comprehensive learning, each particle learns from the whole swarm and different dimensions learn from different particles. CLPSO performs well on multimodal problems. FDR-PSO [39] employs the nearby high fitness particles to guide the motion of learning particle. FDR-PSO avoids premature convergence at the cost of slowing down convergence speed in the early stage. UPSO

on six unimodal functions out of seven unimodal functions. Only on BF5, MLDE-PSO performs worse than other peer algorithms. MLDE-PSO achieves zero errors on BF1, BF2, BF3, and BF6. On BF4, MLDE-PSO achieves very high accuracy too. MLDE-PSO yields the lowest SP on BF1-BF4, BF6 and BF7, which means the convergence speed of MLDE-PSO on these functions are faster than other com- pared algorithms. DMSDL-PSO and UPSO achieve the lowest SP

rand/1 on six, ten, nine, seven, nine, six and eight functions, respec- tively. For DMSDL-PSO employs local search strategy to enhance convergence, DMSDL-PSO performs better than MLDE-PSO on mul- timodal functions. The overall performance of MLDE-PSO is better than DMSDL-PSO, DE/rand/1 and other peer algorithms. For MLDE- PSO employs PSO subswarm and Elite-DE to enhance exploitation,

four simple multimodal functions, seven hybrid functions and five composition functions. On functions F9, MLDE-PSO and DE/rand/1 tie for first, and on function F22,GGL-PSOD, DE/rand/1 tie for first. MLDE-PSO win the best performance on nineteen functions out of thirty CEC2017 functions. DMSDL-PSO, DE/rand/1, GGL-PSOD and CLPSO win the best performance on six, five, two and one func- tions, respectively. MLDE-PSO outperforms CLPSO, FDR-PSO, UPSO, TSL-PSO, GGL-PSO, DMSDL-PSO and DE/rand/1 on twenty five, twenty seven, thirty, twenty seven, twenty three, twenty two and twenty one functions, respectively. MLDE-PSO exhibits high performance on different types of CEC2017 functions. The advan- tage of MLDE-PSO on unimodal functions, simple multimodal func- tions and hybrid functions are more significant than on composition functions. The overall performance of MLDE-PSO is better than DE/rand/1, DMSDL-PSO and other peer algorithms. Though DMSDL-PSO employs DE to construct learning exemplar and local search strategy to enhance exploitation, it is still outper- formed by MLDE-PSO.

between two subswarms. The position information in PSO sub- swarm is employed for DE mutation and the DE individuals are employed for constructing learning exemplars for PSO subswarm. To further improve exploitation of DE subswarm, an elite DE muta- tion is adopted. The experiments on basic functions, rotated basic functions and CEC2017 functions indicated that the overall perfor- mance of MLDE-PSO is better than other compared classic PSO algorithms, recently PSO algorithms and classic DE. On complex functions and rotated functions, MLDE-PSO exhibits more advan- tages. MLDE-PSO is less sensitive to rotation transformation than compared algorithms. On penalty functions, the performance of MLDE-PSO is relatively weak. In the future research, MLDE-PSO will be extended to constraint test functions and real-life optimiza- tion problems.

The author thanks the reviewers and other scholars for provid- ing precious suggestion on this manuscript. This work is supported by the Scientific Research Fund of Hunan Provincial Education Department under Grant 20A460 and 20B536, the Scientific Research Start-up Fund for High-level Talents in Xiangnan Univer- sity, the Applied Characteristic Disciplines of Electronic Science and Technology of Xiangnan University. (Code is available https://www.researchgate.net/publication/360191796_Mutual_ learning_differential_particle_swarm_optimization).

