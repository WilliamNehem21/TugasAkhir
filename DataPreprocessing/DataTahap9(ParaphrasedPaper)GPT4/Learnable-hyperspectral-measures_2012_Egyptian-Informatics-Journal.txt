This academic paper discusses a new method for analyzing hyperspectral data to differentiate between spectral signatures. Hyperspectral measures, such as the spectral angle mapper (SAM), have traditionally provided a simple and computationally efficient way to compare spectral signatures, offering a scale from 0 (very similar) to 1 (very dissimilar).

The paper introduces a dynamic approach with learnable hyperspectral measures that adapt to various similarities across spectral regions. This adaptive method uses a classifier to determine a flexible similarity threshold, unlike the fixed thresholds of traditional methods.

Two new similarity patterns are described. The first is based on cosine similarity vectors derived from second-order spectral derivatives. The second combines various hyperspectral measures into a composite vector to distinguish spectrum pairs.

Two versions of this approach are implemented. Version 1.1 computes cosine similarity vectors for the second-order derivatives across full spectral spaces and classifies them using Support Vector Machines (SVM). Version 2.1 further develops this method by considering nine different similarity measures to generate the patterns classified by SVMs.

The software utilized includes MATLAB R2009b for hyperspectral measures and LIBSVM for SVM classification. The validity of these methods is tested on a challenging hyperspectral dataset with known issues, such as closely related spectral signatures between certain classes and a signal-to-noise ratio below the current standard.

Analysis revealed that one-versus-all (OVA) classification was inefficient for classes with mixed spectral signatures due to its high complexity. In contrast, one-versus-one (OVO) classification proved superior due to its focus on pairwise class comparisons, resulting in better performance.

The paper also explored performance with a partial dataset of nine classes, as Principal Component Analysis (PCA) and Neighbourhood Component Analysis (NCA) were not effective in classes with fewer samples. Using a statistical right-tail z-test, the authors compared the new approach's performance with NCA, illustrating that the novel method significantly outperformed the baseline in classification accuracy.

PCA was criticized for failing to consider bands with critical discriminatory information, which resulted in poor classification of smaller classes. NCA was more successful because it optimized the classification process by transforming the input data for better K-Nearest Neighbors (kNN) classification.

Concluding the study, learnable hyperspectral measures proposed in this research offer significant advantages over PCA and NCA, including reduced training complexity and computational cost, and improved classification accuracy. This makes them more suitable for practical applications in hyperspectral data analysis.