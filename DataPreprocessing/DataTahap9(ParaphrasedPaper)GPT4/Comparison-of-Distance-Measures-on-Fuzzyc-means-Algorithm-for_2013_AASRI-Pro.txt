This paper examines two distinct distance metrics: Euclidean distance and divergence distance, and their application in the Fuzzy C-means (FCM) algorithm for solving image classification tasks. The traditional FCM based on Euclidean distance only incorporates the mean values of image features, but when divergence distance is incorporated into FCM, it takes into account both variance and mean, potentially capturing richer feature information. The study tests these approaches on the Caltech database, focusing on a four-category classification challenge.

The findings indicate that FCM with divergence distance (termed divergence-based FCM or D-FCM) yields greater accuracy than the conventional FCM or other standard algorithms like Self-Organizing Maps (SOM) and K-means that rely solely on mean information. This superior performance suggests that incorporating both mean and variance information in the feature extraction process enhances the classification accuracy.

Unlike the conventional batch mode operation of FCM, which computes the parameters for centers and membership values in one go, the D-FCM algorithm iteratively updates these parameters with each data vector, similar to a gradient-based approach. This iterative process has been previously reported to offer advantages in updating center parameters.

The paper concludes that divergence-based features, which include both variance and mean information, play a crucial role in image classification tasks, and that classification schemes that utilize this richer feature set are preferable. However, the authors also recognize the need for further research to develop feature extraction methods that can reduce classification ambiguities, such as those between motorbikes and airplanes in the dataset.