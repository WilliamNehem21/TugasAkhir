Virtualization technology has facilitated the management and sharing of computational resources, paving the way for the development of cloud computing. Cloud computing offers an appealing solution to the increasing computational needs of scientists, overcoming the limitations of local computing infrastructure. The appeal of cloud computing for high-performance computing (HPC) applications lies in its resource elasticity and the ability to remove the need for costly and time-consuming cluster setups. In a study where virtualized HPC systems used different types of networks—namely Razi (Gigabit Ethernet) and Haitham (InfiniBand)—computational nodes were established using virtual machines. The performance was assessed with the help of benchmarking tools such as SKaMPI, IMB, and MPBench. They discovered outcomes can be influenced by the design of the clusters themselves, not just by the interconnections used.

Belgacem et al. linked Amazon EC2 cloud-based clusters from the USA with university clusters in Switzerland to run a sophisticated MPI-based application, observing the extra time cost (overhead) caused by expanding their HPC clusters with EC2. Their findings indicated that, without adjusting for CPU power and balancing the workload, cloud resources could lead to poorer performance. However, implementing load-balancing strategies could capitalize on the supplemental cloud resources.

The performance of the NAS Parallel Benchmarks (NPB) was measured in millions of operations per second (MOPS) and speedup, the latter quantifying how much faster a parallel program runs compared to its serial counterpart. A higher number in both metrics signifies better performance. A NPB Class C workload was chosen for its sensitivity to communication efficiency and network support.

The CG kernel (Conjugate Gradient) of NPB revealed optimal performance at four virtual machines (32 cores), but performance declined beyond this configuration. This suggested a bottleneck in virtualized network performance, addressable via enhanced NIC card virtualization methods like Single Root I/O Virtualization (SR-IOV) or faster network infrastructure such as InfiniBand. The IS (Integer Sort) kernel performed best on a single VM, while the FT (Fourier Transform) kernel showed peak performance both at a single VM and at 16 VMs with different MPI libraries, MPICH2 and OpenMPI, respectively.

Analysis showed that the NPB kernels ran with high efficiency on a single VM using shared memory with up to 8 cores, mainly due to the efficient scalability of intra-VM communications. In contrast, when expanding beyond a single VM, the performance suffered significantly due to network virtualization overheads, with the IS kernel scaling the least effectively, performing best on just one VM. The MG (Multi-Grid) kernel, in contrast, scaled best up to 8 VMs (128 cores), indicating more effective distributed scaling capabilities.