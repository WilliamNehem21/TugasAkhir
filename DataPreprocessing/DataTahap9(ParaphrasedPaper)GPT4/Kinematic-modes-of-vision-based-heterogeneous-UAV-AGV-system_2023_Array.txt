This paper discusses the use of Convolutional Neural Networks (CNNs) for the pose estimation of an automated ground vehicle (AGV), which is crucial for enabling an aerial vehicle to land on a moving AGV. It reviews work on vision-based landing systems for unmanned aerial vehicles (UAVs) that require detecting a visual marker placed on a ground vehicle using traditional machine vision techniques. This approach, along with autonomous landing on static targets, has been experimentally validated in several references.

The paper focuses on the development of control techniques for UAVs based on a kinematic model that accounts for the velocities of both the UAV and the AGV within the camera's frame of reference. A sliding mode control theory is utilized due to its robustness and straightforward implementation. This control technique works by bringing the system's errors to zero while maintaining a constant velocity vector that satisfies the kinematic model.

The study proposes a tracking control method that calculates necessary acceleration commands to enable the UAV or AGV to follow each other. Furthermore, formation control strategies are explored to allow the UAV and AGV to work together, maintaining a consistent separation and facilitating the UAV's landing on the AGV post-mission.

The paper highlights the natural formation behavior embedded within the system that benefits tasks such as UAV landing and AGV maneuvering within complex environments. This kinematic mode also helps to adjust the AGV's visual positioning in the camera frame, aiding navigation through complicated spaces.

Lastly, the paper addresses existing literature on UAV control based on the 3-dimensional positioning of targets, derived from their 2-dimensional representation in the camera's image plane. The control of the UAV is adjusted in three dimensions according to this 3D positioning, although the 2D target representation is inherently interrelated.