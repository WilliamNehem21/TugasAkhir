This paper discusses the challenges of parallelizing state-space exploration algorithms, focusing on implementation efficiency, architecture selection, and overhead management. The language and library choices for parallelization can significantly affect performance, as exemplified by past research demonstrating high overheads with Java, which were reduced by switching to C. Hardware availability is a critical factor, with multi-processor, multi-core PCs offering limited scalability due to the decreased efficiency of secondary cores and larger shared-memory machines being less accessible. PC clusters are more available but introduce network communication overheads, and the choice of operating system can impact thread scheduling and available development tools.

To tackle the challenges of load imbalances and irregularities, dynamic load balancing and work-stealing techniques are employed, enabling substantial time-efficiency improvements. However, the advantages of such techniques can be diminished by their intrinsic synchronization and additional code overheads, which necessitates sufficient workload for effective distribution across processors.

Measuring parallel overheads is complicated, and current models may not provide an accurate depiction of their impact, casting doubt on objective evaluation. The paper outlines how to gauge different overheads and considers the neglected influence of the model on overhead severity. The model itself could enhance or impede parallelization efforts, independent of algorithmic efficiency.

The paper proposes characterizing models by their load balancing efficiency, scheduling, and overheads to pinpoint specific areas for optimization. By examining models fitting these profiles, strategies can be developed to improve parallel processor utilization and reduce additional overheads.

Previous research focused on static partitioning in networked workstations, assessing runtime and workload distribution, often without detailing overheads. This paper sets itself apart through meticulous overhead measurement using sophisticated profiling tools and selects models strategically to highlight particular overheads, ultimately providing a transparent assessment of the algorithm's parallel efficiency.