This paper explores the challenge of finding the optimal artificial neural network (ANN) model for classification tasks when working with real-world data. The difficulty lies in achieving fast convergence and fine-tuning ANN weights using suitable learning algorithms to enhance classification accuracy. To address this, the paper introduces a variant of the Harmony Search (HS) optimization method, called Global-Best Harmony Search (GBHS), combined with gradient descent learning, applied to a Functional Link Artificial Neural Network (FLANN) for data mining classification tasks. GBHS adapts concepts from Particle Swarm Optimization to enhance the quality of potential solutions, or "harmonies". The combination of GBHS and gradient descent aims to optimize the weights of the FLANN more efficiently.

The proposed GBHS-Gradient Descent Learning-FLANN (GBHS-GDL-FLANN) model undergoes testing against various models, including standard FLANN, genetic algorithm-enhanced FLANN, particle swarm optimization-based FLANN, and others on UCI Machine Learning Repository benchmark datasets using 5-fold cross-validation. Statistical analysis, including the Friedman test, Holm and Hochberg procedures, and post-hoc ANOVA (Tukey and Dunnett tests), were applied to validate the results. The performance of the proposed method was found superior and statistically significant when compared to the alternatives.

Additionally, the paper outlines the applications and effectiveness of FLANN variants in different data mining tasks such as stock price prediction, gene disease prediction, analyzing online customer behavior, and forecasting stock exchange rates, demonstrating their advantages over conventional techniques like MLP, SVM, and other statistical approaches. The paper concludes by highlighting the importance of efficient weight initialization and learning algorithms for enhancing the performance of higher order ANNs, like FLANN, used in classification tasks.