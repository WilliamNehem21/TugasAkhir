The field of novelty detection is crucial in identifying new, unseen behaviors within systems, central to their maintenance and smooth functioning. Novelty detection allows models to recognize unfamiliar data classes not previously encountered during training. This discipline has broad applications, extending to sectors including manufacturing, cybersecurity, healthcare, and astronomy, among others.

In the process of novelty detection, autoencoders (AEs) can incur high reconstruction errors when faced with new data types, facilitating the detection of novel instances. Techniques such as mean-square-error (MSE), residual error, structural-similarity (SSIM), or feature consistency metrics are employed to quantify pixel-level disparities.

One notable issue with using autoencoders for this task is their tendency to generalize to new classes, weakening their effectiveness in pinpointing novelty. Traditional solutions involve intricate and costly training approaches with classifiers inserted into the training sequence of multi-discriminator AEs. In contrast, this paper introduces the Nearest-Latent-Neighbours (NLN) algorithm, which leverages nearest neighbors in the AE's latent space to address generalization problems.

A key differentiator in related work is the usage of supervised, semi-supervised, or unsupervised learning approaches. Supervised methods usually pertain more to anomaly detection, where both normal and anomalous data classes are predefined. However, this is not always practical, as anomalous classes might be rare or unidentified. In contrast, unsupervised methods operate without prior knowledge of the normality or abnormality of data. Semi-supervised methods are most frequent in real-world applications, where models are designed based on the presumption of regular operating conditions and any deviations signal potential novelty. This paper focuses on semi-supervised approaches, designating certain dataset classes as novel and others as normal.

Previous studies have found that relying solely on reconstruction-error metrics is not particularly resilient to noise or changes in background and viewpoint. To enhance performance, variational autoencoders (VAEs) incorporate reconstruction probability or attention mechanisms, while generative adversarial networks (GANs) utilize a comparison of training and generated image representations for error calculation. Self-supervised learning (SSL) methods have also shown promise in boosting novelty detection by leveraging tasks like in-painting or positional prediction.

For multi-class novelty detection, where multiple classes are treated as 'inliers' and one as 'novel,' more complex models are required that can still distinguish between the novel samples. This study investigates the performance of NLN-based models in contexts where there are either multiple 'inliers' and a single 'outlier' or one 'inlier' and multiple 'outliers'.

Research indicates that AEs, when used for one-class novelty detection, may inadvertently learn representations of certain classes, leading to false negatives. This suggests that reconstruction-based detectors could misidentify these unintentionally acquired classes.

In evaluating the proposed NLN algorithm, model adaptations were made for different datasets, employing architectures suitable for MVTec-AD as well as MNIST, CIFAR-10, and F-MNIST datasets. Results indicated that for certain data types like texture classes, the NLN-enhanced AE struggled to differentiate between texture patches, revealing a potential fundamental limitation of standard AE architectures.