Spiking neural networks (SNNs) have shown promise for AI applications, but they lack comprehensive benchmarks that consider both training and inference stages as well as the entire training process to reach certain accuracy levels. Existing benchmarks tend to focus on either the brain sciences or a single phase of the SNN lifecycle without a comprehensive approach.

A couple of benchmarks address SNN evaluation for AI. One, by Ostrau et al., looks at inference by converting pre-trained deep neural network (DNN) models to SNNs and reporting accuracy, but ignores the training phase. Another, by Kulkarni et al., covers both training and inference but uses simpler architectures and struggles to achieve state-of-the-art accuracy with the spike-timing-dependent plasticity (STDP) learning rule. The training accuracy with STDP is also more variable compared to other learning rules.

The researchers find that GPUs don't always outperform CPUs for SNNs. With smaller neuron counts per layer, CPUs can be more efficient than GPUs, likely due to overhead costs and non-optimized SNN simulation frameworks.

For benchmarking computational neuroscience, Brette et al. propose four benchmarks involving a network of 4000 neurons with various spiking neuron and synaptic types. In contrast, SNNbench, aimed at AI applications, focuses on image classification and speech recognition using datasets like MNIST and the Speech Commands V2 dataset. It encompasses widely-used spiking neural architectures and a variety of learning approaches (supervised, unsupervised, semi-supervised, and reinforcement learning).

The researchers emphasize the importance of including both the training and inference phases in benchmarks, as they differ significantly in their hardware requirements and workload characteristics. They argue against relying on indirect performance metrics, such as operations per second, which might not reflect real AI capabilities, and advocate for using model accuracy as a benchmark.

SNNbench aims to provide reproducible and usable benchmarks, accounting for AI's inherent randomness by using consistent random seeds, deterministic algorithms, and Docker containers to maintain consistent experiment environments.

The researchers investigate the performance of various SNN workloads, including STDP training and inference tasks, while also comparing SNNs with standard DNN architectures. They observe that accuracy and training speed differ substantially across learning rules and computing units, with traditional neural networks often outperforming SNNs in both accuracy and ease of use.

In conclusion, they establish that SNNbench can serve as an effective tool for evaluating SNNs' potential in AI, highlight the current limitations of SNNs when compared to DNNs, and suggest that optimal hardware configurations depend on operator type and data input size.