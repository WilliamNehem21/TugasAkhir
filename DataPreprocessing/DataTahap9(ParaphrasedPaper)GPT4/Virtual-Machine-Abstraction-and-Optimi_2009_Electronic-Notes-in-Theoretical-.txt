The paper compares the performance of various interpreters and compilers, establishing that interpreters generally run more slowly than compilers. While performance optimization is commonly ignored in interpreter design in favor of goals like portability and ease of use, the paper suggests that optimizing an interpreter can lead to substantial gains before resorting to compilation.

The research shows that optimization results vary by language and the level of abstraction in their virtual machines. For languages like Forth, Java, and OCaml, whose interpreters operate with low-level virtual machines close to the native machine code, the common belief that dispatch operations are the primary source of inefficiency is supported. However, high-abstraction-level interpreters, such as those of Python, Perl, and Ruby, do not conform to this belief.

Focusing on Java and Python, the paper details their respective features. Java's virtual machine, with its 205 typed instructions, is exemplified by the SableVM, whereas Python, a dynamic, multi-paradigm language, features 93 instructions in its 3.0rc1 version and supports ad-hoc polymorphism, as seen in how its binary add operation can perform different functions based on operand types.

The findings are not limited to just these languages. The authors hypothesize that interpreters of similar high-abstraction-level languages will exhibit analogous optimization characteristics to Python, including those of Perl and Ruby.