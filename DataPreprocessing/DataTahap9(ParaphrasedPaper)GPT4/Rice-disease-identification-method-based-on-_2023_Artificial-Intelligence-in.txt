The research explores the application of machine learning and hyperspectral imaging in identifying early stages of sugar beet diseases, achieving high levels of accuracy, specifically a 97% success rate in differentiating between healthy and diseased sugar beet leaves. Wang et al. (2019) conducted a study on wheat leaves afflicted by fungal diseases, including powdery mildew, rust, and leaf spots. They extracted various features related to color, shape, and texture and used a support vector machine to classify and identify the diseases. For wheat, a fuzzy c-means clustering algorithm was employed to extract 25 disease-related features, which were then processed using an artificial neural network to assess infection. It was noted that traditional machine learning techniques suffer from a dependency on manually extracted features and consequently exhibit reduced recognition efficiency. Although remote sensing has shown potential in crop stress monitoring, its utility is hampered by a lack of disease-specific spectral data which may introduce uncertainties.

As big data and deep learning research progress, plant phenotyping based on deep learning offers a novel avenue for crop disease identification, with notable achievements in enhancing crop monitoring and management. The study evaluated various identification models and found that ResNet50 and SVM showed better accuracy than others. An optimized convolutional neural network (CNN) model, DenseNet, was presented, reaching an impressive 98.06% accuracy, with lower parameter count and reduced computational demand compared to similar CNN architectures.

Additionally, the study delved into the external defects in tomatoes using transfer learning, revealing the ResNet18 model as the superior choice. Furthermore, an improved CNN that assimilates Inception and ResNet architectures was proposed. This network was refined to tackle high misidentification rates through the integration of image and contextual data from rice disease samples. The model was further enhanced with a Coordinate Attention (CBAM) module for refined feature extraction and a Bidirectional Gated Recurrent Unit (BiGRU) to capture the relationships between stress image features, thus improving understanding of rice disease structures. Experimental results showcased the model's higher accuracy and efficiency.

The success of CBAM and BiGRU modules was evident as they dramatically reduced the misclassification rates of various diseases. Despite the promising outcomes, the paper acknowledges the dataset limitation since it only contains four types of rice disease samples. Future efforts will concentrate on amassing a more diverse array of real-field samples and refining the model's structure to boost its adaptability to more complex situations.