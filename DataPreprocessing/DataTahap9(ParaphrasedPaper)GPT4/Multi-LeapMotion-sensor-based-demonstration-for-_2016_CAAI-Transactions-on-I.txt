This paper discusses the field of gesture recognition and its applications in robotic control, particularly focusing on noncontact methods suitable for hostile environments where traditional contact control methods are not feasible. Methods utilizing mechanical devices, optical tracking, and vision-based systems have been developed to create remote operation systems that are lower in cost and easier to adapt to different environments compared to mechanical alternatives.

For hand gesture recognition, data gloves are efficient but expensive and lack the ability to track the hand's position, necessitating additional systems like infrared optical tracking to enhance capabilities, which also adds complexity. Some researchers use vision-based methods alone for gesture recognition, but these are affected by environmental conditions such as lighting and background. Techniques like using skin color recognition or a uniform background color help improve accuracy. Others have used RGB-D data from Kinect, although it has been shown that Kinect, designed for full-body tracking, is less precise for hand movements than sensors like Leap Motion, which is specialized in tracking hands and fingers.

Leap Motion sensors are noted for their high accuracy in finger and hand tracking and come with software interfaces to aid in pose and gesture recognition. Preliminary studies have used Leap Motion sensors for robot manipulation, controlling grippers, and robotic arms. These studies highlight the occlusion problem when only one sensor is used, as it performs well only when the hand's orientation is ideal.

The paper introduces a new system using multiple Leap Motion sensors to address the shortcomings of a single-sensor setup. Analyzing tracking space and work area, the research establishes a two-sensor setup with a self-registering coordinate system. It proposes an algorithm to calibrate delays and combine data from both sensors, enhancing the stability of hand tracking and gesture recognition.

The paper then describes the development of a tele-operative demonstration system integrating a multi-Leap Motion system with a Kinect sensor and a robotic arm with a three-finger gripper, operating within the Robot Operating System (ROS). Experiments conducted show improvements in gesture recognition and hand tracking, ending with a scenario demonstrating the system's use in robotic applications.

Lastly, the paper remarks that while Leap Motion provides detailed palm position, orientation, and finger extension data through its APIs designed for a single Leap Motion, implementing such data from multiple sensors poses a challenge. The method used involves fusing original position and directional data from the sensors at a data level. It also acknowledges the need for future work in optimizing sensor setup and refining gesture recognition algorithms to improve performance, particularly for orientations where occlusions between sensors can occur.