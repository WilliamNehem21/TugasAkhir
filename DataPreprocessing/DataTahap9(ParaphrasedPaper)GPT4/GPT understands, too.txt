This paper introduces an approach called P-Tuning which combines continuous and discrete prompts to enhance the adaptability and training stability of pre-trained language models. This technique is useful regardless of whether the language model is being adjusted or kept fixed, and it works well in scenarios with limited as well as ample supervision. This method builds on existing research, such as Lester et al. (2021), which demonstrated that large pre-trained models could benefit from tuning only the continuous prompts while keeping the model parameters themselves unchanged.