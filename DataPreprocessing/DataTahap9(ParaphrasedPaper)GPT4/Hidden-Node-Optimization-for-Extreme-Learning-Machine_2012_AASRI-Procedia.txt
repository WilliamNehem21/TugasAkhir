This academic paper addresses the importance of selecting an optimal number of hidden nodes in Extreme Learning Machines (ELMs) for achieving good generalization capabilities. The current approach of trial-and-error for determining the optimal number of hidden nodes is time-consuming. The authors propose a new algorithm for optimizing this number by integrating the Particle Swarm Optimization (PSO) technique with the Structural Risk Minimization (SRM) principle. By incorporating the SRM principle, which combines empirical risk with the confidence derived from the Vapnik-Chervonenkis (VC) theory, the proposed SRM-ELM algorithm mitigates the risk of overfitting. The algorithm is shown to be superior to alternative optimization tools like Genetic Algorithm (GA) or Differential Evolution (DE) regarding operational time.

Furthermore, the VC dimension serves as an indicator of computational complexity in machine learning and influences neural network generalization. Although there is no general formula for computing the VC dimension for neural networks, the authors adjust existing formulae for networks with sigmoid activation functions to deduce the VC dimension.

In summary, the paper presents a new algorithm combining PSO with SRM to determine the optimal number of hidden nodes in ELMs. This algorithm modifies existing VC confidence formulae to form a concave objective function suitable for optimization with PSO. Experiments with six benchmark datasets demonstrate the algorithm's effectiveness in identifying an appropriate number of hidden nodes and enhancing generalization performance.