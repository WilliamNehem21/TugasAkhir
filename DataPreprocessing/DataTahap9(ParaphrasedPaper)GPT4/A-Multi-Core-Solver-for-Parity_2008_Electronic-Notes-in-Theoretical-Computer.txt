Our goal was to create an algorithm optimized for use on multi-core systems. These systems have multiple processors that communicate via shared memory. We aimed to design an algorithm that was memory-efficient and could achieve maximum parallelism. Notably, our design omitted the storage of predecessor edges and was designed to be both lock-free and almost wait-free, leading to restrictions on the heuristic used for vertex selection, particularly given the lack of direct predecessor information.

We describe a parallel implementation of CTL* on multi-core processors. This relies on hesitant alternating games to traverse the corresponding AND/OR graph utilizing a stack to identify cycles. Reported results indicate this method is nearly optimal. Shared-memory implementations' potential pitfalls are also discussed.

The algorithm calculates the least game parity progress measure as the minimal fixpoint of updates to the vertices' measures, starting with zero vectors. For distributed applications, the method allows lifting a set of vertices simultaneously. While choosing single vertices reverts to the original algorithm, Section 4 provides heuristics for selecting sets in a parallel context.

To implement the algorithm efficiently, a worker needs to maintain only a specific number of measure vectors at any time, saving memory. However, higher numbers of measures encountered suggest adding reference counting or garbage collection for practicality, with vector folding techniques possibly aiding larger measures' optimizations.

We tested our algorithm on a shared-memory system utilizing Intel Threading Building Blocks for a high-level concurrency interface. This facilitated a clean implementation that closely matches our pseudocode.

In our testing, we disregarded the sequence of measure updates by using a permutation with repetition allowed. We then evaluate different heuristic approaches for this permutation based on experimental performance.

Experiments were conducted on a dual quad-core Intel Xeon system with 8 GB RAM. We tested 1, 2, 4, and 8 worker threads and reported average wall clock time for different thread counts. Note that we focused only on computation times, not on the time needed to generate the parity game graph data.

We briefly mention the one-bit sliding window protocol, which allows limited parallel behavior and is contrasted with the alternating bit protocol due to its higher potential for parallel execution.

Data from Table 1 shows that leveraging more workers can lead to performance gains across various scenarios and selection heuristics; however, the gains vary. Complete worker asynchrony can lead to additional iterations before achieving a fixpoint. Moreover, the decreased workload per worker may introduce inefficiencies. Impacts of issues like false sharing, previously reported in the literature, are also considered.