Ekman and colleagues identified six fundamental emotions—happiness, sadness, surprise, fear, disgust, and anger—that are universally recognized across different cultures. Feldman and others highlighted that emotions are often understood along two primary dimensions: valence (ranging from pleasant to unpleasant) and arousal (ranging from calm to excited). These researchers noted that emotional expression is subjective, with individuals displaying emotions in unique ways. This variability becomes evident when asked to express emotions at varying periods. The study in reference aims to classify emotions based on binary combinations of valence and arousal, either as low/high valence or low/high arousal.

The research paper is structured as follows: Section 2 reviews previous studies, while Section 3 outlines the proposed approach's goals, which include three primary components for recognition through facial expressions, EEG, and a fusion of both. Section 4 evaluates the approach using the DEAP benchmark and compares it to leading methods, followed by a discussion and analysis. Section 5 summarizes the research and suggests directions for future study.

The proposed 3D-CNN architecture features six key layers, including convolution and max-pooling layers which process input volumes of EEG and facial signals, aiming to reduce computation time by down-sampling large-dimensional data. The input volume dimensions and convolution filter sizes are specified, denoting how EEG channels and face frame dimensions are represented.

The DEAP database, developed by Koelstra et al., consists of 32-channel EEG signals and 12 peripheral signals, which is processed down to a sample rate of 128 Hz from an original 512 Hz rate. To overcome the issue of limited samples, data augmentation techniques, such as adding Gaussian noise to EEG signals, are used during training.

The 3D-CNN system relies on input data and initial weights to achieve a trained model, subsequently tested for predictions. Transfer learning is employed, using pre-trained model weights to save training time and improve accuracy and processing times. Additionally, ensemble learning methods, like bagging and stacking, are used to combine predictions from different models to improve system performance.

The paper describes two fusion methods for emotional recognition based on facial and EEG data: stacking and bagging. Both methods outperform the modalities when used in isolation. The 3D-CNN network's capability of extracting shared representations showcased better accuracy in classifying the binary classes of arousal and valence.

The paper involves comparative studies indicating that the proposed deep learning framework directly maps video and EEG signal data to emotional states, outperforming traditional feature extraction and modeling approaches. Finally, an example of previous work is cited, where real-time mental state inference was conducted based on facial expressions and upper body gestures.