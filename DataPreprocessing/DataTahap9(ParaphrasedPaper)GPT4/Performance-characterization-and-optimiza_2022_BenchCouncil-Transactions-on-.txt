Deep Neural Networks (DNNs) are often over-parameterized, leading to challenges with excessive storage and computational demands. Pruning is a method used to address these concerns by removing superfluous parameters from DNNs, hence lowering storage and computational overhead. Pruning methods such as filter and element-wise pruning have been utilized, with the former typically providing more efficient inference and the latter better preserving accuracy.

This study explores various pruning techniques, evaluating their performance when applied to sparse DNNs. The pruning patterns examined include element-wise, vector-wise, block-wise, and group-wise pruning. Through analysis, the researchers highlight limitations with pre-existing libraries in fully leveraging GPU capabilities for these pruned models.

To improve upon this, an optimized approach to group-wise pruning is introduced, designed to harness GPU performance effectively. This is achieved by transforming key computations in pruned DNNs into General Matrix Multiplication (GEMM) operations, which are well-supported by current deep learning frameworks and hardware. The group-wise pattern presented allows for selective removal of entire groups of weights in a structured way, which can be combined with various importance criteria and pruning schedules to maintain model accuracy while benefiting from GPU optimizations.

Experimental results from evaluating this optimized group-wise pruning on models such as VGG, ResNet, BERT, and ViT show significant reductions in GPU inference latency compared to both other sparse patterns and previous group-wise pattern implementations. These optimizations lead to effective and more practical inference speeds, while still enjoying the benefits of model sparsity. Moreover, once model sparsity exceeds 75%, this group-wise implementation can sometimes outperform the inference speed of non-pruned, dense models.

In conclusion, this study contributes an efficient implementation of group-wise pruning that enhances the performance of pruned DNN inference on GPUs. The approach leverages broadly available GEMM library operations, simplifying implementation and increasing portability across different NVIDIA GPUs without the need for specific tuning. This work not only improves upon prior group-wise pruning implementations but also advances the overall capability of pruning methods to achieve faster inference times for DNNs with minimal accuracy loss.