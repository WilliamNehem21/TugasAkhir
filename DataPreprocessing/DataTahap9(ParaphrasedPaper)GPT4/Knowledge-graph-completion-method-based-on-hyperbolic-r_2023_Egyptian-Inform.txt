To tackle the challenges described, this article introduces ATTCL, a novel method combining hyperbolic representation learning with contrastive learning. ATTCL embeds knowledge into a hyperbolic space and uses adversarial noise to enhance samples lacking hierarchical characteristics or feature information. The loss from embedded samples is propagated in a way that adjusts embedded vectors through added perturbations. These perturbations are fine-tuned using a hyperparameter to better calibrate adversarial strength for sample construction.

Embedding, a widespread technique for knowledge graph completion, involves projecting entities and relations into a simpler vector space, which helps measure similarities and relations. This approach maintains entity connections and the original data's integrity. Various models like TransE, DistMult, ComplEx, and neural graph networks like ConvE differ in how they handle entities and relations within vector spaces, each with its own limitations such as handling complex relationships or computational efficiency.

Contrastive learning, which differentiates between positive and negative examples, enhances the ability to distinguish between various entities and relationships, providing more meaningful representations. Frameworks like SimCTG for text generation and SimKGC for knowledge graph completion have been proposed to address specific challenges in decoding and improve the learning process by introducing novel sample types and leveraging similarity measures.

Specifically, within knowledge graph completion, when fewer relationally connected entities are detected, ATTCL employs adversarial noise to create augmentative adversarial samples. These samples are used to enrich the contrastive learning process. The ATTCL model demonstrates more significant improvements on datasets with complex semantic relations, such as FB15K-237, compared to datasets with simpler semantic relations like WN18RR.

Improvements in Mean Reciprocal Rank (MRR), and Hits at multiple levels were observed. Integrating hyperbolic representation and contrastive learning can enhance the embedding space further, leading to better performance in knowledge graph completion tasks.