The authors of the paper extend gratitude to Richard Hudson, Don C. Mitchell, Owen Rambow, Lars Konieczny, Line Mikkelsen, Sten Vikner, Sabine Kirchmeyer-Andersen, and Alex Klinge for valuable discussions, as well as to two anonymous reviewers whose comments greatly contributed to the work. This research was supported by a grant from the Danish Research Council for the Humanities.

In the presented framework, lexical types in the lexicon are defined with ordered super type lists and variable declarations; variables inherit their declarations and values by default from super types and are categorized as atomic or set-valued. The paper provides an example where 'verb' is a subtype of 'word,' and further specifies how the values of set-valued and atomic variables are determined, either by specification or inheritance.

The paper also introduces 'dg constraints,' which are based on cost functions that calculate the syntactic violation cost by severity and frequency. These functions utilize real and set-valued operators, and their primary role is to define what constitutes a violation of a constraint. For simplicity, each cost function is considered to have a uniform cost, although in reality they would be weighted.

The authors clarify that different languages may assign varying penalty costs for specific syntactic violations, noting that a high cost represents ungrammaticality in languages such as Danish and English, while a lower cost might be used for similar 'marked' violations in German.

The paper discusses complexity in parsing, mentioning that certain formalisms are NP-hard due to their unrestricted constraints. CFG and TAG can be parsed in polynomial time but could become NP-hard with certain cost measures regarding semantic and pragmatic plausibility.

The Discontinuous Grammar (DG) parsing strategy is detailed, which uses a local search algorithm operating on a space of partial parses. Operations within this algorithm are defined, noting they should be both strong enough to correct suboptimal analyses and weak enough to reflect human parsing limitations. The authors suggest that parsing operations might be learned and can vary among different individuals or language communities.

To evaluate the parser's effectiveness, a distinction between grammar and parser failures is necessary, with parser failures being considered serious evidence against the parser unless similar difficulties are experienced by humans.

DG parsing employs 'monotonic operations' that include adding lexical items and edges to the graph. It lays out a theory for how humans may learn parsing operations and note k-change operations, especially those requiring more computational resources, might be used sparingly as repair strategies.

The results from implementing the DG parsing algorithm show that while the parser is heuristic, it is not significantly more error-prone than an exact one, offering optimal analyses except in challenging garden path sentences. The local cost functions in DG offer a flexible and efficient system for evaluating parses, and the optimality parser is rapid, potentially operating at a complexity of O(n log‚Å¥ n) with realistic linguistic assumptions. Despite strong performance, the parser does occasionally fail on complex sentences, akin to human parsing errors.

The paper concludes with an acknowledgment of unresolved issues, including handling ambiguities and enhancing the average-case complexity of the parser through advanced search techniques. Future work should aim to explore these areas, add new parsing operations to handle specific linguistic phenomena, and evaluate DG on a larger scale.