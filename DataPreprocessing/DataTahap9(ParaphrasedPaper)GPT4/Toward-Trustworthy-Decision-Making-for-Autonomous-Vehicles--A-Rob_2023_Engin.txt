In this paper, the authors present a comprehensive introduction to the implementation details of a new method. The RRL-SG algorithm, which is designed for ensuring dependable decision-making in autonomous driving, is laid out in Algorithm 2. The starting parameters for the various models involved—the actor, the adversary, and the critic—were initialized with values drawn from a random distribution. The configuration of the agent involved a critical design of the state, action, and reward functions tailored for the autonomous driving context.

Specifically, for the state representation, the paper focuses on the conditions affecting the ego vehicle—a term used for the self-driving car under consideration. The states are composed of several factors involving six nearby non-ego (social) vehicles in both the same and adjacent lanes. The autonomous driving agent then uses this information, which forms a 15-dimensional state space. This state space includes the relative positions and velocities of the nearby vehicles and the ego vehicle's own speed, acceleration, and the lane it occupies.

The scenario for the experiments conducted in the study was set up without any stationary or moving obstacles, which meant that the hunter—the term apparently used for the autonomous vehicle or agent—should ideally have been able to progress in a straight line without interruptions from adversarial interventions. The policy models were evaluated over 150 time steps, simulating the hunter's journey from one point to another. When testing the models against adversarial attacks, these were introduced at the midpoint of the evaluation period, specifically at the 75th time step. The autonomous agent had a repertoire of five potential behaviors it could decide to execute, including turning right, turning left, maintaining its current state, speeding up, or slowing down.