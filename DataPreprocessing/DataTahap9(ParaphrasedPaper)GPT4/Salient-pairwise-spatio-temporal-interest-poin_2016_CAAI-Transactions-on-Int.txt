This paper discusses various approaches used in the field of human action classification using video data. Various researchers have proposed methods using both holistic and local features to describe actions. Holistic features, treating actions as space-time pattern templates, have been utilized for action recognition where the problem is reduced to 3D object recognition. Prest et al. focused on human-object interactions by tracking both the object and the human.

Traditional 2D convolutional neural networks (CNNs) have been extended to handle video data through the development of 3D CNN models, operating directly on raw video inputs. Other approaches include the use of spatial-temporal correlograms to capture the co-occurrence of action descriptors in local regions. Naive Bayes methods augmented with quantized local features have also been used by researchers like Matikainen et al. to represent spatial-temporal relationships.

Ryoo et al. proposed a spatiotemporal relationship matching method that considers both temporal (such as before and during) and spatial (such as near and far) relationships among descriptors. Correlograms have been used to encode these co-occurrence relationships, relying on normalized Google-like distances.

The study proposes the Time Salient Pairwise Feature (TSP) to describe relationships between pairs of descriptors in the same frame, though it only considers pairs with different labels, which may cause ambiguity in distinguishing actions with similar geometric structures (G_SS). To overcome this, the authors introduce the Space Salient Pairwise Feature (SSP) to describe the geometric distributions of descriptors, which are necessary for accurate action classification.

The proposed methods were tested on various datasets, with the SSP demonstrating compatibility with traditional Bag of Words approaches. The paper reports an accuracy improvement over recent models and highlights the speed of their algorithm due to the use of fast feature detection and clustering methods.

In their tests, features like BOVW, TSP, and SSP were calculated in real-time using pre-trained models, and a non-linear SVM with a homogeneous kernel was employed to classify actions efficiently. The versatility of their algorithm suggests potential improvements in content-based video retrieval applications.

Experimental setup included using a Philips SPC900NC/97 camera, modified with a curved mirror to create a 360-degree panoramic view, mounted at 1.8 meters on a mobile robot.

In conclusion, the paperâ€™s robust multi-cue representation which includes TSP and SSP features, is able to distinguish similar actions. Future work will focus on improving the modeling of geometric distributions of descriptors, incorporating high-level models, such as human-object interactions and dense tracklets. The goal is to expand the applications to real-time situations.

The research was supported by several grants from Chinese funding bodies including the National Natural Science Foundation of China (NSFC), the National High Technology Research and Development Programme of China (863 Programme), the Scientific and Technical Innovation Commission of Shenzhen Municipality, and the Specialized Research Fund for the Doctoral Programme of Higher Education (SRFDP).