In the existing body of academic work, various numeric abstraction domains have been developed to uncover relationships among numeric variables within imperative programming. These domains vary in their ability to express constraints on program variables, the most prevalent being intervals, octagons, and polyhedra.

In terms of their capability to express information, the order of complexity is: polyhedra > octagons > intervals, and polyhedra > parallelotopes > intervals. Octagons and parallelotopes offer different but incomparable levels of expressiveness. Higher expressiveness theoretically enables more accurate tracking of variable values during execution, with the expectation that analyses using a more expressive domain ("A") would reveal tighter constraints than a less expressive one ("B").

However, expressiveness isn't the only factor; abstract operators and techniques like the widening operator also influence analysis outcomes. Widening operators are essential for dealing with infinitely increasing iterations in analysis, but they can sometimes reduce accuracy, resulting in less precise results than simpler domains like intervals.

Most interval operations scale linearly with the number of variables, while octagons and parallelotopes scale at most cubically, and polyhedra can scale exponentially. Even though theoretical behavior of abstract operators is well-understood, it doesn't fully capture the performance within actual analyses, especially since the operations' cost greatly depends on the specific instances encountered.

This paper contrasts the precision of analyses using different domains through the same algorithmic approach without comparing the direct results because they may be incomparable or overly complex. Instead, the assessment focuses on interval constraints, as these simpler constraints are easier to apply in practice and can be useful in error prevention. These constraints are common to all domains and relevant for practical applications.

The paper also discusses octagonal constraints, which are helpful for specific checks like out-of-bound array access when array sizes are known only at runtime. However, these can be transformed into interval checking problems with additional synthetic variables, which potentially downplays the relevance of the direct comparison with octagonal constraints.

Benchmarks were conducted using the Jandom static analyzer on the Alice benchmarks, a suite consisting of 108 linear transition systems with varying numbers of locations and loop heads. Standard two-phase analysis with both widening and narrowing was applied to each model. Different delays were tested for these two phases, and the results indicated that while polyhedra perform poorly without narrowing, delayed narrowing has limited benefits across other domains, and delayed widening can variably impact precision.

Performance-wise, intervals were the fastest, followed by octagons and then polyhedra, with parallelotope domains coming in lastâ€”partly due to implementation factors, as parallelotopes were not optimized in the same way as other domains within the tool.

The paper concludes that, despite polyhedra having the theoretical edge for defining linear relations, other less expressive domains can achieve more precise results in practice, especially when using sophisticated widenings with optimal delays. Delayed narrowing showed negligible effect on precision except in specific cases with the polyhedra domain.