The paper discusses Mobile-Assisted Language Learning (MALL), highlighting its advantage in allowing learners access to educational content anytime and anywhere. While AI-based language learning apps like Duolingo, Hello English, Babbel, Memrise, and Busuu offer learning tools for popular languages, there's a gap in offerings for indigenous languages of Bangladesh, such as Chakma, Marma, and Saotal. The paper underscores the need for a digital platform to help preserve these languages by applying AI to improve reading, writing, speaking, and listening skills.

The research also explores the use of heatmaps, derived from contemporary computer vision techniques, that show which parts of an input image influence the AI model's predictions. This helps users understand the decision-making process of the neural network.

In terms of data preparation, the paper elaborates on the cleaning process applied to the ISI numeral and Ekush datasets. The ISI numeral dataset was reduced from 27,500 to 23,392 images after cleaning, while the Ekush dataset ended up with a total of 17,745 training images, 5,053 test images, and 2,530 for validation.

The study presents results from the self-ChakmaNet optimization, indicating that despite early fluctuations, loss values decreased with continued training, showing a saturation trend and suggesting a well-fitted model. Self-ChakmaNet is compared to state-of-the-art MobileNet_v2 for recognizing handwritten Chakma characters, with the network's predictions being consistent with the stroke patterns of the characters. Heatmaps revealed that the model focused on important features rather than arbitrary ones for making predictions.

The paper also compares the model size of self-ChakmaNet, which has approximately 453,000 parameters, to BDNet and other models mentioned in literature, which have significantly more parameters, pointing towards an efficient design of the proposed self-ChakmaNet.