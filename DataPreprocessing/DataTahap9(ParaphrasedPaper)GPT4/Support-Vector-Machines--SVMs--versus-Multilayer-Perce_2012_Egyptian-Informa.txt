Support vector machines (SVMs) and kernel-based learning algorithms often outperform artificial neural networks (ANNs) and other computational approaches on common benchmark problems. This success is attributed to a localized learning process that adapts model capacity effectively to the complexity of the data, which ensures the model generalizes well to unseen data. Unlike ANNs, which may have multiple local minima, SVMs provide a single solution associated with the global minimum of the optimization problem. Furthermore, SVMs are not as dependent on heuristic methods, as they provide a more solid and adaptable framework.

This paper introduces a novel kernel function, the Gaussian Radial Basis Polynomial Function (GRPF), which has the potential to enhance SVM classification accuracy for both linear and non-linear datasets. The study compares the performance of SVMs trained with GRPF and other kernels, such as Gaussian and polynomial, against the back-propagation learning algorithm for classification tasks across various non-separable datasets with multiple features. Results indicate that the GRPF kernel achieves high classification accuracy, particularly on higher-dimensional datasets.

The study is outlined in several sections: Section 2 describes the SVM classifier. Section 3 details the design of the multi-layer perceptron (MLP) classifier. Section 4 introduces SVMs with the new GRPF kernel function, whose parameters are optimized in Section 5. Section 6 provides a comparison of SVMs and MLP classifiers, while conclusions are presented in Section 7.

MLP classifiers, a type of feedforward neural network, typically require multiple layers with one-directional connections and are trained using back-propagation. They learn from input-output data sample pairs, adjusting their weights incrementally to minimize the error between the network's output and the desired output. MLPs may require a greater number of hidden units when dealing with complex datasets, which can be controlled by limiting the number of these units.

Conversely, SVM complexity is not tied to data dimensionality. SVMs focus on minimizing structural risk, which aims for an optimal separating surface, providing strong performance on new data points. Although MLPs also aim for global approximation with few hidden neurons, SVMs use a larger number of support vectors and adopt a local approximation approach.

A key advantage of SVMs includes formulating their learning problem as a quadratic optimization task, which simplifies computations and speeds up the learning process, especially for large datasets. Overall, SVMs are depicted as efficient and capable of achieving high-quality classification results.