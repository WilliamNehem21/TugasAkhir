Classifying lithofacies in wellbores is essential to understanding reservoirs, commonly using well-log data. When distinctive lithofacies in core and well-log data are present, extensive, and homogenous, standard well logs are useful for classification. However, when these characteristics are absent, other geological information may be necessary to supplement the analysis.

Feature selection is critical in lithofacies analysis, especially when numerous well-log attributes are available, as trial-and-error becomes impractical. It is a complex problem where the number of combinations increases exponentially with the number of features. Selecting optimal features can enhance machine learning (ML) model efficiency.

Several ML algorithms are employed for lithofacies prediction, including neural networks, support vector classification, k-nearest neighbor, optimized data matching, and tree-ensemble methods. These algorithms vary in efficacy, but feature reduction can boost some ML models’ efficiency.

Attributes for lithofacies analysis should respond consistently to rock properties. Attributes calculated from gamma-ray, bulk density, and acoustic logs are preferred due to their consistent responses across wells, while resistivity and neutron logs may be less reliable because they are sensitive to fluid types in porous formations.

Feature selection can help improve the precision of ML models by eliminating less influential variables from consideration. Benchmark cases are used to assess different feature selections and include a simple case with just four well logs and a comprehensive case with all sixteen features.

In lithofacies classification studies, several ML models are evaluated, including adaptive boosting, decision trees, k-nearest neighbor, logistic regression, random forest, support vector regression, and extreme gradient boosting. These models differ in performance and are subject to misclassification risks, impacting error metrics and accuracy.

Feature selections with a high prediction accuracy use 7 to 10 of the 16 features. Three top-performing selections are compared in more detail with a case using just four logs and a case using all sixteen features. Factors such as numerical proximity of misclassified facies and correct predictions affect error metrics and accuracy measures.

Proper splits for training and validation datasets are settled using multi-k-fold cross-validation, with lower mean plus standard deviation indicating better splits. This technique provides mean and standard deviations for mean absolute error and root mean squared error for various k-fold configurations.

The study indicates that by calculating attributes for some logged variables and employing feature selection optimizers, more accurate lithofacies classification models can be created. Yet, there’s potential for further enhancement through more diversified well logs, alternative attributes, or addressing the challenging prediction in non-continuous clastic reservoirs.

A novel feature-selection approach using evolutionary optimization algorithms with a k-nearest neighbor model identified high-performing feature combinations with fewer features required for classification. Detailed evaluations of these combinations with seven different ML algorithms revealed their varying performance and emphasized the usefulness of derivative and volatility attributes in classification tasks.