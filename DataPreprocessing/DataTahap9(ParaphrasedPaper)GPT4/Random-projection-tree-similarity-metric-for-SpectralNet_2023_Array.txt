For SpectralNet to deliver precise clustering, it must work with a detailed affinity matrix that clearly distinguishes clusters. The matrix should assign high weights to edges between points in the same cluster and low or zero weights to edges between points in different clusters.

SpectralNet leverages siamese networks to learn these weights effectively. However, these networks require initial data points labeled as either similar (positive) or dissimilar (negative). This labeling can be achieved in a semi-supervised way, using known labels, or in an unsupervised way, where labels are inferred from the proximity of data points.

The research further explores using Gaussian Mixture Models (GMM) to cluster outputs, and compares this to other methods such as autoencoders combined with UMAP for dimensionality reduction and deep learning approaches that start with identifying central hub points within clusters.

To measure the storage efficiency, the study focuses on the number of total pairs used, avoiding machine-specific metrics like runtime. It also outlines the influence of RPTree parameters on a similarity metric and suggests improvements, such as alternative computations within siamese networks or different random projection methods like RPForests, to optimize clustering performance in deep networks.