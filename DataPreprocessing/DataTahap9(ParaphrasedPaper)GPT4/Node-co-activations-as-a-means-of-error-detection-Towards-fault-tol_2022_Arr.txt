Method:

We carried out training on four neural networks (NNs) and examined the frequency of co-activation for each pair of nodes. Using a distinct test dataset, we quantified the instances of rare co-activations associated with each input and categorized these inputs based on whether the neural network's prediction was accurate, inaccurate, or if the input belonged to a class not represented during training.

Conclusions:

Rare co-activations are prevalent in inputs from new, untrained classes, suggesting their potential for identifying concept drift in data. They may also be useful for recognizing inputs from classes absent in the training phase. However, the minimal difference in rare co-activations between correctly and incorrectly classified inputs suggests that more research is needed in this area. 

High numbers of rare co-activations might indicate prediction issues, with inputs from untrained classes having a higher average of these events, demonstrating their use in detecting data drift. Still, not all untrained inputs exhibit a high number of rare co-activations, and some trained inputs do. Therefore, pinpointing inputs that the network cannot accurately process may pose challenges. Although the average occurrence suggests the possibility of utilizing this method in certain contexts, systems should be equipped to handle false positives and negatives.

Errors, caused by system faults, pose a risk to system dependability. ReLU is a widely used activation function known for its effectiveness, operating on an "active or inactive" principle.

Neural networks feature node groups that handle different outcomes, with nodes within these groups often activating together. Disjoint, rare co-activations may indicate an unreliable prediction.

Our study focuses on examining if rare co-activations can signal untrustworthy predictions by analyzing the co-activation rates derived from networks post-training. We consider the relationship between these rare co-activations and the credibility of predictions.

Data from various networks were analyzed to deliver statistically significant insights. We used a separate test set that included inputs both within and outside of the trained classes. The networks included models with structural similarities and differences, utilizing Keras and NumPy for the experiments.

We evaluated statistical significance to gauge whether different prediction scenarios yield different patterns of rare co-activations. Our approach to identifying erroneous predictions and inputs from untrained classes based on rare co-activations showed promise in detecting shifts in data, but it might not be reliable for pinpointing individual incorrect predictions.

Finally, the study opens avenues for future research, particularly the exploration of rare co-activations in different network contexts and the need for empirical validation using data from real-world industrial settings.