Wand and Vu (2018) presented a novel lipreading system named Densely Associated Convolution Network (DACN), designed to interpret visual speech cues from color images by focusing on the 3D physiology of lips, influenced by facial structure and position. They proposed a combined audiovisual recognition algorithm and applied Connectionist Temporal Classification (CTC) and Attention mechanisms to the LRS2 database for advanced audiovisual speech recognition.

Jadczyk (2018) carried out a project on Polish audiovisual speech recognition, dividing the task into three parts: audio processing, visual speech analysis, and the fusion of both modalities. He utilized Mel-Frequency Cepstral Coefficients (MFCC) for audio features extraction, Hidden Markov Models (HMM) for lip features, and a multi-stream HMM approach for their integration.

Shashidhar and Patilkulkarni (2021) analyzed a standard lipreading dataset, employing a 250-item custom dataset with a VGG16 pre-trained model to enhance recognition accuracy.

Cornejo and Pedrini (2019) introduced a Deep CNN (DCNN) strategy for detecting audiovisual voices, separately processing audio and visual components and then merging them using a two-dimensional CNN for audio, Principal Component Analysis (PCA), Linear Discriminant Analysis (LDA), and the Census Transform for visual speech. Their approach demonstrated promising results on varied benchmark datasets, endorsing the efficacy of DCNNs in audiovisual voice detection.

Shashidhar and Sudarshan (2022) developed a Multimodal Sparse Transformer Network (MMST) leveraging sparse self-attention to selectively concentrate on pivotal data segments, enhancing the overall extraction of global information.

Dupont and Luettin (2000) reported superior results on a large dataset of continuously spoken digits by multiple speakers. Their approach combined Multi-State HMMs, denoised MFCCs, and visual data to refine the accuracy of multimodal isolated word recognition over audio-only features.

The database includes videos recorded at 1080 pixels resolution with a frame rate of 60 fps, averaging 1 to 1.20 seconds in duration, and about 10 MB in size.

CNNs mimic the human visual system and are fundamental in modern AI, especially in complex vision tasks. Using a new dataset and a network combining a 1D CNN for audio, an LSTM for visual input, and a DCNN for integration, the authors achieved training and testing accuracies of 94.67% and 91.75%, respectively, highlighting the advantages of audiovisual integration over traditional methods.