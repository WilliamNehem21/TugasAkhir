First-order methods have gained prominence over the past two decades for tackling convex optimization, driven by their successful implementation across various domains like machine learning and signal processing. These methods offer the advantage of rapidly producing solutions with moderate accuracy at low computational costs, ideal for large-scale optimization issues. This survey discusses significant advancements in gradient-based optimization, including non-Euclidean variants of the typical proximal gradient method and their accelerated forms, as well as recent progress in projection-free methods and proximal primal-dual algorithms. Detailed proofs of key findings are provided, shedding light on the similarities among different optimization techniques.

Traditionally, convex optimization problems were formulated as conic programs and addressed using primal-dual interior point methods, a protocol detailed in the pivotal work of Nesterov and Nemirovski. Despite their high accuracy and mathematical elegance for moderately-sized issues, first-order methods have taken precedence due to their scalability and efficiency in handling more extensive and intricate problems.

Proximal gradient methods, gaining vast interest in optimization and allied applications, follow a well-established formula for handling convex optimization issues. Non-smooth optimization has also made strides through methods like mirror descent, with numerous studies drawing comparisons and outlining variations among these algorithms. The survey also touches upon the alternating direction method of multipliers (ADMM), a potent tool for linearly constrained problems despite some constraints with parallelization and general linear constraints.

The survey examines standard and alternative step-size choices, noting that while multiple options exist for determining step sizes, they all provide comparable guarantees of effectiveness. It also discusses linear convergence rates and the restart technique for achieving these under specific conditions.

Concluding, the survey echoes Nesterovâ€™s sentiment that while general optimization problems are notoriously challenging to solve, convex programming is remarkably manageable due to the cohesive mathematical strategies devised for it. First-order methods stand out in the modern era of large-scale problems, offering a balance between efficiency and precision that has spurred many theoretical and practical innovations over the recent years.