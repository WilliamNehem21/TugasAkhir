The readability of counter-examples is greatly influenced by the model used and what is being verified. When models are more abstract, they lose interpretability since abstraction can mean omitting certain information from the modelâ€™s state space. This omission can make it challenging for application domain experts to understand the counter-examples, especially when information they expect to see is missing. Likewise, counter-examples become less comprehensible when they contain excessive, irrelevant data.

In our abstract model, the rules for changing signal aspects depend only on other state variables, not on the current aspect. For efficiency, we do not include signals in the state space; rather, we determine their state by checking the dependent variables. In this model, trains behave according to the rules, stopping at red signals and being able to stop on any track segment without a speed component.

The abstraction of our model, particularly the exclusion of signals, makes counter-examples harder to read because users can't see the signal aspects even though we've verified the model's accuracy through extensive testing. Signaling engineers may find this abstraction unsettling.

Typically, counter-examples highlight state variables that have changed from the previous state. When verifying railway interlockings and checking a single route, the inclusion of all opposing routes is necessary. If an error occurs, the counter-example will include numerous variables, even for a single route error, which can be overwhelming in a small verification area with up to 30 variables.

Our research suggests two main ways to present counter-examples to users: animation and interpretation. We chose interpretation due to the highly abstract nature of our model. We developed an algorithm based on signalling principles to automatically interpret counter-examples, identifying the specific error that caused the safety violation and presenting only the relevant data to the user in detail without being misleading.

Each type of counter-example's data is interpreted differently, and the format of the output can vary depending on some data values. Examples of how the interpretations may differ are provided for each type of counter-example.

Counter-examples pose usability issues not only for domain experts but also for engineers working on the models. Much of the existing research, including work by Clarke and others, has focused on eliminating false or spurious counter-examples over enhancing their readability. Our approach considered both animation and interpretation for presenting counter-examples, but we developed a prototype interpreter grounded in signalling principles to create a robust algorithm capable of handling any counter-example generated by our models.