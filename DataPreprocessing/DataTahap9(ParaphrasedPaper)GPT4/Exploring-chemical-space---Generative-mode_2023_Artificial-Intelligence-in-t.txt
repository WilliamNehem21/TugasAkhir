Recent breakthroughs in artificial intelligence, particularly in deep learning, have breathed new life into the exploration of chemical structures. Unlike classical strategies that build on known fragments, deep generative models create molecules in ways that are hard to interpret but offer the potential to discover completely new areas of chemical structures without relying on familiar patterns. This opacity in how molecules are formed and the complexity of training these models brings the evaluation of their novelty, distinctiveness, and diversity to the forefront.

This paper provides a summary of the latest methods for investigating chemical structures, with a focus on deep learning techniques. When evaluating generative models, it's important to consider the molecular representations used, the specific chemical spaces targeted, and how coverage of these spaces is validated.

For general exploration of a chemical space (CS), ideally, each molecule should have an equal chance of being produced, though some bias, such as favoring smaller molecules, might be acceptable. Sometimes the exploration has distinct objectives, such as optimizing compounds for certain biological or pharmacological properties.

Quantitative measures can gauge the validity and originality of the molecules created. Scores representing synthetic accessibility and estimated drug-likeness help determine the practicality of the proposed structures. Assessing to what extent a CS is representative involves looking at the distributions of structure and physical-chemical properties. Biological relevance is evaluated by the portion of molecules with desired activities.

For generative models (GMs) focused on particular subsets of CS, like those with desired biological activities, traditional benchmarking isn't suitable due to the vast size of these spaces. A GM's ability to recreate known molecules might suggest limited exploration ability.

In a set of generated molecules, you might expect to see about 37% repeated entries. Yet the ideal is that more than 99% of molecules are novel, highlighting the vastness of the targeted CS. When novelty is low, it can be a sign that a model is just repeating what it's seen (overfitting), while a lack of diversity (or uniqueness) might imply 'mode collapse,' where the model gets fixated on a narrow range of structures.

In one study, GMs sampled one billion compounds, and the best models covered 39% of the CS. If the models performed perfectly and every molecule had an equal chance of being generated, a 63% coverage might be possible. Since GMs can produce molecules not found in existing databases, it's clear they can navigate significant portions of chemical spaces.

Latent space analysis helps comprehend how deep GMs perceive similarity. For instance, one study examined the correspondence between latent space neighborhoods and structural similarity. Another innovative approach mapped an autoencoder's latent space into two dimensions to see how active compounds distributed in that space relate to adenosine A2A receptors.

Another advancement used input saliency maps to better understand the molecule generation process. These maps score each section of a molecular structure based on its importance for predicting subsequent sections, adding a layer of interpretability rooted in the chemistry.