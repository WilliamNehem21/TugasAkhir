The substantial progress in computational efficiency is highlighted by the fact that the ResNet-50 model's runtime has been dramatically reduced, with recent benchmarks indicating that it can now complete tasks in less than 30 seconds. However, from a benchmarking standpoint, such a rapid execution time is insufficient for a comprehensive and lasting assessment. Moreover, as high-performance computing (HPC) systems continue to expand, the fixed computational workload is spread thin across the system, leading to suboptimal resource utilization for individual nodes.

Current benchmarking initiatives like v2.0 and MLPerf(HPC) are criticized for not addressing scalability concerns, instead focusing on identifying typical HPC AI applications and enhancements based on parallel processing. On the other hand, HPL-AI and AIPerf have seen some success in achieving scalability, but they introduce new issues. For example, HPL-AI measures HPC systems' performance through mixed-precision LU decomposition at the kernel level, much like the original HPL.

Ensemble learning, which combines the outputs of multiple models to address a problem, leverages the 'wisdom of crowds' principle to reduce prediction variance. Bagging, or bootstrap aggregating, is a core strategy in ensemble learning, involving bootstrapping (sampling with replacement from the original dataset) and aggregating (combining predictions). This process results in 'bootstrapped' datasets, and the training of each base model in parallel allows for an aggregated final decision that averages the predictions of the individual base models.

Finally, the paper notes that data parallelism (DP) overhead is a significant factor because it requires global synchronization of all model copies at the end of each training step. In contrast, the HPC AI500 v3.0 model ensemble trains base models in an extremely independent manner, foregoing the need for such synchronization and thus avoiding communication overhead.