Numerous studies have investigated theoretical aspects of dropout in machine learning [6,23,31,49,55,58,75,79,81]. Baldi et al. demonstrated that dropout functions as a form of adaptive stochastic gradient descent, while Wager et al. treated it as an adaptive regularizer within the context of generalized linear models (GLMs). Ma et al. explored the discrepancy between training and testing when using dropout and proposed utilizing this difference to enhance the standard training loss function with regularizing effects.

We implemented logistic regression analysis on cancer-related datasets from the Expression Project for Oncology (EXPO) and trained feedforward neural networks (FNNs) on a dataset from the 1000 Genomes Project to predict individual ancestries based on genetic information, with individuals represented by their SNP profiles.

In the context of optimization and loss reduction, L1 regularization methods tend to simplify models by eliminating some parameters, thus acting as a feature selection process, while L2 techniques work differently.

Srivastava et al. posited that employing dropout in a neural network with 'n' units equates to sampling from 2^n possible sub-networks via weight sharing. During testing, where averaging over the numerous sub-networks is impractical, an approximation method is utilized by adjusting the weights of the single network to mimic the average of the ensemble's behavior. Although this approximation works well for models like logistic and linear regression, it may not perfectly represent the expected output of a complex deep neural network (DNN), with some uncertainty in the performance gap. Under certain conditions, according to Ma et al., this gap can be managed and used for regularizing the DNN.

Dropout has shown to enhance the performance of machine learning models, especially when combined with other regularization methods like batch normalization. Batch normalization is another popular technique that helps in accelerating training and enhancing the functionality of deep neural networks. In our experiments, we used stochastic gradient descent as the optimization strategy for both cancer datasets from EXPO and genetic datasets from the 1000 Genomes Project, applying them to logistic regression and FNNs respectively.

Moreover, introducing an autoencoder reconstruction path to a neural network improves performance. However, training such a combined autoencoder and classification network can be more challenging and potentially result in higher classification errors compared to training just a classification network. Regularization techniques can offer improvements, with traditional methods seemingly outperforming dropout in some cases.

In conclusion, our study discusses how to effectively train deep learning models using backpropagation and stochastic gradient descent to mitigate overfitting. We establish a theoretical link between dropout and L2 regularization and demonstrate through experimentation that combining dropout with other techniques such as batch normalization can deliver better results than solely relying on L1 and L2 regularization methods.