Research institutions and universities often boost the use of their computer workstations by setting up a system that redistributes unused processing power for large-scale computing tasks. This system assesses when computers are not in use and assigns them tasks until the user returns, boosting resource efficiency, albeit with the risk of wasted computation if tasks are frequently interrupted.

The paper describes the creation of a detailed stochastic model using Population Continuous Time Markov Chains (PCTMC) to simulate the competition for resources between users and computing tasks. This model is especially suited for the variable nature of user interactions and can quantify a range of performance and energy metrics. It leverages historical and real-time data to estimate future availability and apply rapid analysis for scenario exploration. This methodology could improve system optimization for energy and performance and aid users in managing their computational projects.

The study summarizes previous research on similar resource-sharing systems and positions its new method within this context, showing improvements in forecasting capabilities. It introduces a specific PCTMC model that takes into account different user behaviors, computer types, and tasks, and it applies these to an actual system of around 1400 machines on a university campus with varied usage patterns.

Data from these machines, collected through the HTCondor system, was condensed and categorized to fit within the PCTMC model framework. By examining user activity patterns, the model could predict how often computers transition between states. This is achieved by fitting multi-phase Coxian distributions to the data, which is more representative of the actual system dynamics compared to simpler exponential models.

The model also allows further refinement, like considering different strategies for managing tasks when a user becomes active, such as suspending rather than canceling a task. Using the Generalized Partial Aggregation (GPA) tool, the complex underlying ordinary differential equations are solved, enabling the prediction of future system performance.

Finally, the study showcases an implementation of this approach using a year's worth of HTCondor data from Newcastle University, aiming to provide a forecast tool that could guide users to optimally submit and plan their computing jobs. It suggests that the tool could help minimize the interruptions and wastage of computing resources while ensuring efficient job processing for the researchers. The paper envisions a practical application, where the tool could be integrated within the HTCondor system to assist both users and system administrators in optimizing the use of computational resources.