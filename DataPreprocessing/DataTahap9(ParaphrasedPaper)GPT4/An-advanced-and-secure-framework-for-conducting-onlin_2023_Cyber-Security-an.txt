Cluskey et al. examined strategies for administering online exams without proctor oversight, exploring prevalent cheating tactics used by students and how to curtail them. Techniques for designing online tests and measures to enhance exam integrity, such as scheduling exams simultaneously, employing the Respondus LockDown Browser, and confirming student identities, were recommended.

In their study, a system was introduced to create a database for storing answer scripts and assessing plagiarism. A paraphrasing tool powered by Python was utilized, leveraging synonyms, grammatical restructuring, and parts of speech changes. The system also incorporated various methods to detect plagiarism, including semantic, grammar-based, hybrid approaches, and clustering techniques.

The paper outlined a process to address plagiarism, calling for human review of answer scripts deemed to have high levels of plagiarism to determine further action.

The automated system proposed aims to meticulously review answer scripts, allowing quick and accurate result delivery. For subjective answers, human evaluators reviewed and scored responses.

In remote testing contexts, participants are expected to finish within a set time, with the testing window closing upon completion, and facilities receiving instantaneous reports. Results can then be communicated via email or website.

Addressing e-learning challenges in countries like Bangladesh, particularly during the COVID-19 pandemic, the paper discusses an AI and blockchain-enhanced framework to secure online examination systems, promising to emulate the experience of in-person exams within a digital e-learning environment.