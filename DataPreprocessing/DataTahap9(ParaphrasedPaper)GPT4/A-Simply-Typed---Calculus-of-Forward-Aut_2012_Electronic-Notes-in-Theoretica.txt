Automatic differentiation (AD) is a technique for computing exact derivatives of functions that are defined through computer programs. This method outperforms numerical divided differences, which suffer from approximation errors, and excels beyond the capabilities of symbolic differentiation when addressing highly complex code, delivering reliable computation time guarantees. While a range of AD tools exist, they predominantly integrate with imperative languages like Fortran and C/C++. However, AD concepts align more naturally with functional programming languages, where the differentiation operator could serve as a classic higher-order function. Despite extensive research and numerous implementations of AD, a comprehensive understanding of AD's semantics—particularly in connection with first-class functions within functional programming—is not yet established. This gap in understanding has hindered the integration of AD into functional programming environments. Siskind and Pearlmutter have explored the complexities involved in enhancing a functional programming language with first-class AD capabilities, highlighting the intricate nature of AD when applied to higher-order functions.