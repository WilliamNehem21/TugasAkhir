This research paper discusses a complex optimization challenge encountered in the market for computer services. A service provider makes use of a server cluster to offer various services to a user community. Users expect a certain quality of service when they pay for job processing, which may involve submitting a specific number of jobs of a distinct type at a predetermined rate. Simultaneously, the provider must consider the cost of energy consumption for each active server.

The provider is caught between the objectives of minimizing energy costs by limiting active servers and maximizing revenue by keeping more servers running to handle job streams and avoid penalties. Therefore, the paper suggests that it is advantageous to utilize dynamic policies to determine the timing to power servers on and off and whether to admit incoming streams. These policies need to be responsive to demand fluctuations, considering the risk of penalties when deciding to accept job streams.

Although the literature covers server allocation and energy-saving subjects, the specific problem discussed has not been sufficiently explored before. Previous work by Mazzucco and Mitrani, which defined the concepts of charge, obligation, and penalty related to individual jobs, is somewhat related. However, these works did not focus on user stream economics, server costs, and dynamic server management.

The paper acknowledges that accurately modeling system dynamics can be complex since the system's state depends on the history of job streams, thereby making accurate approximations a necessity.

For practical considerations, the paper assumes that the times to power servers on and off are negligible compared to stream lifetimes, and that policy decisions happen infrequently enough that the system reaches a steady state between them. The paper assumes a simplified scenario where arrival processes are close to Poisson, but this is not critical to the overall analysis.

To assess the effectiveness of various policies, simulations were carried out with a network of 40 servers, measuring profit against the rate of stream arrivals. Over a million jobs were processed in the simulations, with the data divided into portions to calculate confidence intervals for the observations.

The paper concludes with the suggestion that future research could explore scenarios where queues are separated by stream type, and the allocation of servers to each queue must be managed, along with considering the costs of reassigning servers between queues.