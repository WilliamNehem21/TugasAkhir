The paper is structured as follows: Section 2 describes the detailed semantics and leakage computation from previous research. Section 3 introduces a method for approximating this concrete leakage analysis and establishes some of its key properties. Related work and conclusions are discussed in Sections 4 and 5, respectively.

The paper refers to conditional mutual information that measures how much information is shared between the 'high' input and output of a program when the 'low' input is known. For deterministic programs, this information sharing is equivalent to the entropy of the 'public' output given the 'low' input.

They provide an example where only a minimal amount of information is disclosed to the public. This result aligns with our intuition, as the likelihood of the high input ('h') being zero is very low, and there is still considerable uncertainty about the value of 'h' when it is not zero, resulting in only a small portion of information being publicized.

The paper discusses the leakage analysis of loops, which involves partitioning state spaces and considering the entropy of these partitions. Specifically, the sum of the entropies of the loop body for each subset of the state space by applying the definition of entropy of partitions, as referenced in the literature.

They illustrate that the precise leakage in a simple example can be calculated. This shows that employing abstraction to unify objects does not compromise the safety of leakage computations since it never underestimates the potential leakage. The fineness of the partitioning strategy, which influences precision, depends on the initial distribution of sensitive (high security) inputs and program structure.

The example highlights that while their abstraction techniques can provide only an upper limit on leakage, they significantly enhance the efficiency of the analysis, reducing the analysis time from eight iterations to two, compared to the exact, concrete analysis.