This academic paper discusses the importance of texture features in identifying objects within images or video frames. The texture consists of unique visual patterns that can describe an object's surface independently from its color, as noted by Hu et al. (2011). When analyzing videos of flowers, there is a significant variation within the same class of flowers, especially in terms of color. Therefore, texture features are particularly useful in accurately describing regions containing flowers. In this study, two methods for capturing texture features are utilized: the gray level co-occurrence matrix (GLCM) and local binary patterns (LBP).

GLCM is a statistical method of analyzing texture by evaluating the frequency of pixel intensity pairs at certain distances and orientations within an image. In this research, 14 different statistical values derived from GLCM are used to create a feature vector from each frame of interest (FROI).

Linear Discriminant Analysis (LDA) is a technique for reducing the dimensionality of the data while still maintaining the ability to distinguish between classes. Originally proposed by Ronald Fisher in 1936, LDA aims to find a new feature space that maximizes the separation between classes while preserving differences within classes.

Support Vector Machine (SVM) is a powerful algorithm for supervised learning and is effective in classifying both linear and non-linear data. SVM works by finding an optimal separating hyperplane between classes, aiming to maximize the margin between different categories. For further insight into SVM, works by Vapnik (1998) and Duda et al. (1997) can be consulted.

In this research, a system for retrieving flower videos using a deep learning approach is proposed. The system uses a multiclass SVM to retrieve videos similar to a given query video. The authors suggest three different feature extraction strategies for this procedure: using the entire keyframe, the segmented flower region within a keyframe, and the gradient of the flower region, with features being extracted using a deep convolutional neural network (CNN).

The paper addresses the challenge of dealing with videos that may contain various classes of flowers during different segments. In such cases, a manual approach is used to cut the video into shots based on class distinctions. To improve this process, the authors highlight the need for an automated technique to detect boundaries between different classes or shots within the video.