The increasing prevalence of speech technology, such as Apple Siri and Amazon Alexa, has highlighted the importance of the speech separation problem. This problem is crucial as a preprocessing step in various applications including noise reduction in speech signals, automatic speech recognition (ASR), and the creation of speech databases. However, machine recognition of speech in noisy or crowded environments remains a challenge, with performance significantly declining in such conditions. Advances in speech separation are largely dependent on the development of high-quality speech databases.

Understanding speech in acoustically challenging settings, like a cocktail party where multiple speakers and background noises exist, is difficult. Research has suggested that incorporating visual streams alongside the audio signal can help analyze mixed sounds in noise-filled environments. Due to limited resources, training complex models on extensive datasets containing millions of recordings is not feasible, so subsets of the Oxford-BBC LRS2 dataset have been utilized.

The paper is structured as follows: Section 2 reviews various speech separation approaches, Section 3 details the proposed model for sound separation, Section 4 discusses the experimental setup, results, and findings, and Section 5 concludes the paper.

Research has been conducted using audio-only approaches for speech separation. Yannan et al. introduced an unsupervised DNN framework for co-channel speech separation which performed well in differentiating between two speakers of different genders using log power spectral as an input feature and minimizing the mean squared error under the MMSE criteria. Their system outperformed the Gaussian Mixture Model (GMM).

Other researchers like Quan Wang et al. created systems that can separate a target speaker's voice from multi-speaker signals using a reference signal from the target speaker itself, employing two separate neural networks to accomplish this task.

Ariel Ephrat et al. developed a model with a dilated convolution neural network that takes an audio-visual input—audio of mixed sounds and video frames showing detected faces—and separates the audio into individual speakers' audio streams. The model utilizes visual features to aid in speech separation and tracks speakers in the video, demonstrating higher Source to Distortion Ratio (SDR) compared to other visual methods and robustness in separating same-gender voices, emphasizing the importance of visual information for improving accuracy.

The proposed model in the paper provides a framework for speech separation with an available list of speakers, focusing on an audio-visual approach that relies on facial embedding features for each speaker, leading to better performance even with limited datasets.

The neural network architecture used is the U-Net, known for its applications in medical image segmentation. The structure of U-Net is simple and shallow, consisting of CNN blocks, ReLU, and max pooling in both encoding and decoding paths, allowing it to work well with small data quantities and showing promise in speech separation and enhancement.

In the process of separation, the model uses the face embeddings from a pre-trained FaceNet network as guidance. The network processes input images, detects faces and outputs a vector representing essential facial features. These vectors are then concatenated for each speaker to assist in the voice separation process.

Dilated CNNs are employed to reduce computational resources compared to classical CNNs, speeding up training by 12.09% and showing improved performance in image classification and segmentation.

The paper references a study by Quan Wang et al. presented at Interspeech 2019 in Graz, Austria, underscoring the relevance of speaker-conditioned spectrogram masking for targeted voice separation in their VoiceFilter project.