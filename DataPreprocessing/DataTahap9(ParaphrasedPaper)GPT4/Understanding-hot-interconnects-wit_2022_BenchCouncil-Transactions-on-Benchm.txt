As hardware innovation progresses and new interconnect technologies emerge, application developers are faced with multiple challenges that require their attention. These advancements necessitate the reevaluation of existing software designs to exploit improved data transfer rates offered by next-generation hardware. With the development of novel hardware features, a systematic examination of how these features affect application performance is crucial.

Different data center applications, including high-performance computing (HPC) workloads, deep learning training and inference, big data analytics, and cloud-based microservices, each exhibit unique performance characteristics. It is essential to individually and carefully assess the effects of new interconnect technologies on these varying workloads. Given these considerations, it is an opportune moment to study the performance characteristics of modern data centers and HPC interconnects through benchmarking experiments tailored to a variety of application contexts. This need has inspired us to conduct a comprehensive review of the prominent interconnects employed by modern data centers and HPC clusters, along with the relevant representative benchmarks, to enhance the understanding of these cutting-edge interconnects within the community.

Despite existing research on various benchmarks, such as deep learning and microservice benchmarks by Zhou et al., or comparisons of big data and AI benchmarks by Gao et al., there is a lack of comprehensive surveys covering a wide array of the latest advanced interconnects in modern data centers across different applications. This paper aims to fill that gap by surveying various high-profile interconnects and corresponding benchmarks, revealing their performance characteristics.

Ethernet, a traditional interconnect for HPC and data center clusters, has evolved from 1 Gb/s Ethernet (1-GigE) to versions supporting 10, 25, 50, and even 100 GigE to meet the demands for higher bandwidth, with 25-GigE being very prominent in the top500 list as of June 2022. RDMA over Converged Ethernet (RoCE) leverages the advantages of RDMA on Ethernet networks. RoCE can operate on both layer 2 and, with RoCE v2, layer 3 networks. Techniques like congestion control with ECN and QCN have been developed for Ethernet and RoCE, and Amazon's EFA offers high-speed (up to 100 Gbps) connections for its EC2 instances.

InfiniBand (IB), provided by Nvidia, stands as the second most common interconnect in the top500 list. It boasts higher bandwidth (up to 400 Gbps) and supports four transport modes, along with various network topologies. IB includes congestion control mechanisms utilizing FECN and BECN messages, and with CUDA 5.0, introduced support for GPU Direct RDMA (GDR), increasing data communication between GPUs significantly.

PerfTest, a tool that evaluates RDMA operation throughput and latency, along with specific transports such as RC, UC, UD, and extended transports like Mellanox DCT and AWS SRD, does not emulate real application traffic. Meanwhile, Network_Load_Test simulates network congestion in multi-tenant HPC environments and reports metrics to study congestion's impact.

With the advent of new storage technologies like NVMe SSDs, tools such as Intel's SPDK NVMe Perf test with minimal overhead become necessary for benchmarking. This tool supports various runtime options for different workloads.

This survey selects MPI and PGAS-based benchmarks to represent a range of data center applications. Their long history of community contributions and optimizations, along with ease of deployment and use, make them ideal candidates for assessing performance across different interconnects. Benchmarks for big data systems have been proposed to evaluate the growing number of big data platforms.

The research detailed in this paper is supported in part by NSF grants and the Exascale Computing Project, among other contributory sources, and utilizes resources from Argonne National Laboratory and the University of California, Merced.