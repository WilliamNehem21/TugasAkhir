The effectiveness of machine learning models for predicting protein-ligand binding affinities is influenced by the quality of the experimental data used for training. Typically, these models are trained using subsets of the PDBbind database, a primary source of publicly available protein-ligand complex data with associated binding affinities. Assessing the experimental uncertainty within this database is challenging due to the limited number of complexes with repeated measurements. This study examines bioactivity data from ChEMBL to estimate the experimental uncertainties linked to the three types of binding affinity measures within PDBbind (Ki, Kd, and IC50), including the uncertainty introduced when combining these measures. The combined uncertainty of these affinity metrics was characterized by a mean absolute error (MAE) of 0.78 logarithmic units, a root mean square error (RMSE) of 1.04, and a Pearson correlation coefficient (r) of 0.76. Comparing these uncertainty estimates with the performance of advanced machine learning models for affinity prediction reveals that these models may be overly optimistic, particularly when evaluated on the PDBbind core set.

Scoring functions for binding affinity prediction fall into two categories: classical and machine-learning based. Classical scoring functions use predefined relationships between protein-ligand features and affinity, while machine-learning based functions derive these relationships from data. Despite the superior performance of machine-learning scoring functions, these models face limitations, such as bias towards certain crystal structures and variability in affinity data stemming from different experimental sources. Such variability introduces experimental uncertainty, capping the potential accuracy of machine-learning models.

The study further delves into assessing experimental uncertainty in PDBbind by creating curated subsets for each binding affinity measure (pKi, pKd, pIC50), preserving only unique values for protein-ligand pairs, and considering pairs with at least two independent measurements. To examine the impact of combining affinity measures, pairwise and three-way combination datasets were generated, preserving the highest affinity value for overlaps and including pairs with at least one measurement of each type in the analysis.

When the predictive models derived from PDBbind data are compared against this experimental uncertainty framework, it appears that machine learning models have an overly optimistic assessment of their capabilities. However, due to differences in dataset size and composition, the comparison is not direct. Nevertheless, the statistics from the combined pKi-pKd-pIC50 dataset provide the closest approximation of what is achievable by these models.

Estimating the uncertainty of binding affinity from diverse sources demonstrated that combinations can increase uncertainty, with the least uncertainty arising from a mix of pKi and pIC50 measures. The study suggests that alternative metrics may offer a more nuanced understanding of model performance, as opposed to traditional metrics like Pearson's r, which may not be sensitive enough to account for noise in the data.

This brief exploration aims to contribute to the scientific debate on the limitations and utility of machine learning models trained on heterogeneous binding affinity data. It highlights the need for careful consideration of experimental uncertainty in developing and evaluating these predictive models.