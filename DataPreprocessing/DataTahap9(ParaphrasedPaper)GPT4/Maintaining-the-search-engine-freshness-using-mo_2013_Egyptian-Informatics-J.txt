The paper is structured as follows: Section 2 reviews the design of conventional search engines. Section 3 discusses previous research. Section 4 presents the design and process flow of the proposed distributed web crawling and indexing system (DWCIS). Section 5 outlines the experimental setup, while Section 6 presents and examines the results of the experiments. The paper concludes with Section 7.

Key components of search engines include:

- **Spiders/Crawlers:** Programs that download web pages by following links, adding new URLs to a queue, and downloading in parallel to enhance efficiency.
- **Web Page Repository:** A database or file storage system where compressed web pages retrieved by spiders are kept.
- **Indexers:** Systems that parse stored pages to create a searchable index by tokenizing content and calculating word occurrence metrics for ranking search results.
- **Query Engines:** These handle user search requests, search the indexes, rank results, summarize findings, and manage cached queries, finally displaying results through a user interface.

The paper mentions several related works:

- Some authors have initiated mobile crawling where mobile agents handle data retrieval and preliminary processing, although distributed indexing and updates are overlooked.
- Yadav et al. proposed a checksum-based content change detection that can overburden the network with unnecessary re-crawling due to any minor change.
- Artail and abi-Aad suggested a web page change detection based on similar HTML tag types, which is not efficient in terms of storage and is limited to certain page types.
- Bal et al. introduced a novel indexing system using mobile agents with change detection methods based on page size or last modification date, but this approach suffers from the same limitations as checksum-based techniques.

The authors propose a different approach with several advantages:

- Change detection methods that depend on page size, last modification date, and hash value.
- Returning only the updated document index to the search engine rather than the full page, reducing network traffic.
- Offloading computational demands from the search engine by pre-generating document indices at the web servers.

The paper discusses the use of the Terrier platform for indexing and retrieval, noting that HTML documents compress significantly (about 70%) using WinZip.

The performance evaluation aims to demonstrate the superiority of the authors' document index-based change detection technique over other methods. The distributed indexing approach is also shown to be more efficient than centralized indexing, as demonstrated with a dataset of 656 HTML pages from the Java programming tutorial, which can be further compressed to save network traffic.

The paper proposes mobile crawlers that use different change detection techniques (based on the last modification date, page size, and hash value) and send back only the modified and compressed web pages or document indices. In particular, the mobile crawler with the document index change detection technique (MC5) is highlighted for its ability to significantly reduce the computational load on the search engine.

Lastly, the authors simulate scenarios of insignificant page changes to analyze the effectiveness of their system in handling layout or structure changes and attribute changes in web pages.