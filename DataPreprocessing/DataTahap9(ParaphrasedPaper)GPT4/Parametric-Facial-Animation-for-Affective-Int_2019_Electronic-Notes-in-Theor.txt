The paper discusses various methods for animating avatars, with a specific focus on blend shapes and morph targets. This technique involves manually altering the avatar's mesh by adjusting vertices to morph from an initial to a final state, with the animation being created through the interpolation of these vertex positions. One of the key benefits of this approach is the freedom it provides animators, as they are not constrained by the limits of a rigging structure. This freedom is particularly useful for creating facial animations, although this method can require more computational resources than rigging.

Candide, a parametric computer graphics model of the human face, was first introduced by Rydfalk in 1987 and updated by Ahlberg in 2001. It's been developed based on the research of Ekman, who identified specific micro facial expressions that correspond with certain emotions. The Candide model features a series of action units (AUs) that replicate these expressions, though due to the model's polygon resolution limitations, 10 micro expressions are not included. The Candide model benefits from the work of Ekman since these micro expressions are powerful indicators of human emotions, which means that a subset of Candide's AUs can emulate particular emotions.

The process of avatar retargeting typically involves manipulating the facial mesh; therefore, it is essential to ensure that any face or head accessories like glasses or hats are separate 3D meshes that do not hinder the retargeting. The avatar's face must be a clean 3D mesh, without any attached accessories, to successfully complete this process.

To maintain resolution while working with avatars, 3D artists might use multiple texture maps. An alternative approach is to use a single high-resolution image map that still preserves the quality. This requires combining the various texture maps into one and then rebaking them to create a singular texture map for ease of use.

Animating the avatar involves precise control over the coordinates of facial vertices. By adjusting a parameter, vertices can be moved through 3D space to desired locations, creating facial expressions by simultaneously moving groups of vertices. These movements can be specified in an XML file, which allows for easy updating and modification of facial expressions on the fly.

To evaluate some assertions, experts were presented with a questionnaire, rating their agreement on a scale from 1 (strongly disagree) to 5 (strongly agree), each pertaining to a different category.

The paper proposes a workflow for generating avatars capable of displaying emotional expressions in real-time. This is achieved by swapping the original face of a pre-designed avatar with the Candide parametric mask, which bypasses the need for repetitive 3D animation processes. The workflow utilizes the Candide model due to its foundation in Ekman's emotional model, facilitating the generation of a wide range of facial expressions. This workflow has been applied to six avatars available under Creative Commons licensing.

The efficacy of the facial replacement process was assessed by a group of six digital artists who performed a subjective comparison between the original and retargeted avatars without knowledge of the method used to create each one, to prevent bias. The experts noted differences between the avatars but deemed these differences to be minimal. This suggests that the retargeting process effectively produces avatars resembling the originals.