This paper investigates a method to identify and differentiate attack-oriented web scans from regular user visits and benign crawlers using web server access logs. The authors developed a rule-based approach for detecting malicious scans, which includes examining past web server data from Apache or IIS and identifying SQL injection and XSS attacks. Tested against sample data sets and real-world logs, their method showed high accuracy and precision, suggesting it could be more effective than traditional detection techniques. The paper discusses the advantages of rule-based detection over machine learning in terms of memory usage, as their script can be run on Ubuntu with a simple command without consuming excess memory.

The proposed method is compared with related studies, emphasizing its unique aspects such as the inclusion of additional fields for detecting web crawlers, reliance on combined log format for more features, and non-dependence on user-agent header fields alone. One of the use-cases included studying the Damn Vulnerable Web Application (DVWA) to understand attacks and collect data. The paper acknowledges the limitations of the offline detection approach and suggests the development of more efficient real-time detection systems as future work.

In summary, the authors assert the effectiveness of their rule-based model, demonstrated by near-perfect detection rates, while suggesting improvements and additional avenues for further research like examining other types of web application attacks and log files.