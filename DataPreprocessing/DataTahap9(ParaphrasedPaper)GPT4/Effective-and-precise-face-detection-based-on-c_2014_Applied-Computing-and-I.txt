The paper describes a method for improving face detection accuracy by utilizing depth information, which reduces the number of incorrect detections (false positives) without compromising the ability to correctly identify faces (detection rate). The approach was evaluated on a dataset containing 180 samples with both 2D images and depth data, where face positions had been manually marked for testing.

Depth data from Kinect devices, while not helpful for distinguishing between individuals at a distance due to high similarity across different subjects, offer low variation for the same subject, aiding face detection robustness against challenges such as lighting, occlusions, and variations in expression and pose. The affordability and availability of Kinect have driven its popularity and collection of benchmark datasets.

This research follows similar literature efforts that leverage depth information to lower false positive rates and boost accurate detection percentages. A 2D multi-step algorithm is initially used for coarse-to-fine face classification, followed by 3D tracking to refine face localization.

A third filtering mechanism bases on the depth map to eliminate detections on flat surfaces (like walls) or irregular objects (such as tree leaves). By integrating color and depth data, the region of a candidate face can be segregated from the background, using depth and shape regularity as additional criteria to filter out false positives.

One drawback mentioned is the lack of freely available, comprehensive face detection datasets with both color and depth maps for challenging scenarios. Existing datasets mainly target face recognition and often present simple face detection tasks. To address this, the researchers evaluated their approach using a specially collected dataset which they intend to make freely available.

A joint calibration of Kinect's color and depth cameras is performed following Herrera et al.'s (2012) method. This allows each depth map sample to be associated with color information, facilitating the creation of aligned color images and depth maps.

Further, the depth map provides insights into the flatness or unevenness of the candidate face area. A segmentation step precedes the calculation of the standard deviation (STD) of depth map pixels within the main segment of the candidate region. Regions with STD outside the range of 0.15 to 4 are excluded.

Segmentation of both color and depth maps relies on a method by Dal Mutto et al. (2012), using the normalized cuts spectral clustering algorithm and considering both geometric and color information. To unify color and geometry, colors are converted to CIELAB space, creating a more perceptually uniform measure for the clustering algorithm. A parameter 'k' balances the importance of color versus geometry.

The dataset used for evaluation depicts people on a university campus in various daily activities, with varying lighting conditions and without posing for the camera. The diversity ensures a more challenging test than previous datasets.

Overall, the depth map proves essential in addressing false positives in difficult situations by providing the actual size of the candidate face and assisting with segmentation, a key process for subsequent steps in the method.