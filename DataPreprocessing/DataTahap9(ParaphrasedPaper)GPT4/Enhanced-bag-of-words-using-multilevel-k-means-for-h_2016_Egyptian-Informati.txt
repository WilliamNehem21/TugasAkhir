The paper discusses an improved method for recognizing human activities in videos using an extended bag of features approach, applied to the spatiotemporal domain. The standard four-stage human activity recognition process starts with detecting interest points, then describing them with descriptors like HOG, HOF, SIFT, and cuboids. Next, a bag of features algorithm creates a code from these descriptors. This study enhances this representation to boost recognition accuracy, validated on the KTH and Weizmann datasets, outperforming current methods.

The paper outlines video representation and encoding methods, activity recognition frameworks, and the proposed improvements in the bag of features representation with spatial relationship considerations. The study introduces hierarchical clustering and feature representation that captures local feature co-occurrences, limited by a predefined feature count per video.

Descriptors are utilized to capture shape and motion around interest points, with combinations like HOG and HOF fused early at the descriptor level or late at the classifier level. Both descriptors demonstrated high accuracy with the datasets, hence their use in this approach.

A novel multilevel k-means clustering approach is proposed to manage computational complexity, enabling the inclusion of all training data and enhancing recognition accuracy. This is done by first clustering features from individual videos, then actions, and finally all actions together.

Experiments on the KTH and Weizmann datasets show the proposed two-level and three-level clustering methodologies improve accuracy by creating more representative codebooks. Improved results and computation time comparisons with the state-of-the-art and previously proposed modifications indicate the benefits of the new approach.

In conclusion, the enhanced multilevel k-means clustering has been shown to significantly increase human activity recognition accuracy, with a two-level clustering achieving 96.28% accuracy and a three-level method pushing it further to 97.7%. Future work is suggested to build upon these findings.