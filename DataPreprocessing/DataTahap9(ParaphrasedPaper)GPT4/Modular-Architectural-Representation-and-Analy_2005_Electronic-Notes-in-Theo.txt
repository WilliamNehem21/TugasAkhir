In this research, we introduce a novel approach to software architecture modeling specifically aimed at understanding the failure behaviors of components, as opposed to typical models that focus on desired, correct functioning. By examining individual components for their failure modes, engineers can more effectively predict the overall system's failure behavior through a method we call the Failure Propagation and Transformation Calculus (FPTC). This compositional methodology enables automated analysis of the entire system's failure behavior based purely on its individual components, facilitating cost-effective predictions of changes in the system.

Although our research primarily targets software, we believe these methods could potentially be adapted for broader engineering systems, encompassing hardware and other physical elements. While this focus does not encompass all safety concerns, it contributes significantly to building a robust safety case.

We note that there are various interdependencies among components, such as software allocation to CPUs, which should also be considered in our failure model. In essence, we model the system as a network where tokens represent failure modes, which are generated, transformed, and potentially ceased within.

We adopt a cautious stance assuming the worst-case scenario in terms of failure propagation since detecting and correcting failures is typically not a standard feature of software components. Our preference for generality over specificity avoids semantically fragile models that can be disrupted by textual reordering.

Despite added complexity, the practice used in programming languages like ML and Haskell shows that managing overlapping patterns can enhance both syntax and readability.

We underline that our techniques are proactive, detecting issues before code implementation when fixing problems is less expensive. Our case study reveals the model to be partial, representing just the starting phase of the software without full design, which mirrors real-life project progression.

Our case study involved simulating failures in hardware sensors and using our tools to analyze the propagation of errors. Particularly, we examined potential failures impacting flight control actuators. This study also highlights how our approach can significantly reduce the cost and complexity of safety recertification when system changes occur, demonstrated through a simple alteration in sensor components.

We identified two limitations with the former FPTN method when compared to FPTC. FPTN was more ad-hoc, recording only known error transmissions and neglecting potential error routes, unlike FPTC, which is rooted in the actual system design, keeping the model closely aligned with reality and localizing update impacts.

Our technique found practical application in an avionics case study early in the project lifecycle to prevent unsafe designs and showed its effectiveness in handling design changes. We foresee the value of this method in incremental safety certification and product-line families.

Future ambitions extend to quality of service considerations. By redefining tokens to signify system data quality levels or failure severity, we aim to analyze the impact of components like voters or wiring schemes on system integrity. This could lead to deeper insights into whether system modifications introduce vulnerabilities, enhancing confidence in managing complex designs.