This academic paper introduces a Latency-Aware Automatic Channel Pruning (LACP) method designed to optimize neural network structures by automatically choosing the number of channels in convolutional layers to reduce latency while maintaining accuracy. Neural network pruning typically falls into two categories: weight pruning and channel pruning. Weight pruning zeroes out individual weights, creating a sparse model that can reduce size but may not speed up processing due to irregular memory access. Channel pruning, in contrast, cuts out entire channels but often uses conventional criteria like FLOPS and parameter count reduction to assess quality, failing to guarantee actual inference speedup.

The authors challenge the traditional approach in channel pruning by focusing on optimizing the channel count instead of identifying which channels to discard. They address the practical difficulty of exploring all possible pruned structures by narrowing down the search space using observations of a "staircase pattern" in GPU inference latency across different channel numbers. Guided by this, they employ an evolutionary algorithm to find efficient pruned structures.

To test LACP, they experiment with VGG and ResNet models on CIFAR-10, CIFAR-100, and TinyImageNet datasets. They evaluate the performance based on accuracy, reduction in latency, FLOPS, parameters, and channels. Results illustrate that LACP leads to better inference acceleration and enhanced accuracy compared to previous methods. However, the paper acknowledges limitations such as the empirical basis for assessing the GPU tail effect and the need for adapting the method to other hardware.

This study was supported by various Chinese institutions and used the supercomputer system at the University of Science and Technology of China for experiments. The paper concludes by suggesting future research avenues for a broader analysis of the GPU tail effect and the generalization of the LACP method to different hardware platforms.