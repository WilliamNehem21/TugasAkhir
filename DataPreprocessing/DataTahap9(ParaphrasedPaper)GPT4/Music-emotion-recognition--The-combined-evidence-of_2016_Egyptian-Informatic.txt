Feature extraction and classification are key steps in Music Emotion Recognition (MER). It's crucial for features extracted from music to represent it effectively while allowing for efficient computation. Much of the research in feature extraction has centered on timbre features. The Mel-Frequency Cepstral Coefficient (MFCC) is a notable feature derived from the timbre or spectral characteristics, and it's highly regarded in speech recognition. This study investigates MFCC and other potential new features to improve recognition of musical emotions.

Spectrum features, derived from the Short Time Fourier Transform (STFT) of an audio signal, are essential in speech and music recognition tasks. The MFCC, leveraging auditory perceptions and the decorrelation effect of the cepstrum, is a key spectrum feature. To compute MFCC for a music segment, prediction error over the analysis frame is used. This error, known as the Linear Prediction (LP) residual, contains valuable information about the excitation source. In this study, the phase of the analytic signal obtained from the LP residual is used to extract emotion-specific information.

The activation functions for the input and output layers of the model are linear, while the hidden layer can use either linear or non-linear functions. Research into three-layer Auto Associative Neural Network (AANN) models demonstrates that non-linear functions in hidden units can cluster input data within a linear subspace. If the network layers are increased, it can cluster data in a non-linear subspace. A particular five-layer AANN structure is optimal for training and testing residual phase features, where the input and output feature vectors are the same. The network adjusts weights to transform the input vector to the output, with the number of training epochs depending on the training error.

During testing, MFCC features from test utterances are input to a Support Vector Machine (SVM) model. The model calculates the distance between feature vectors and the SVM hyperplane, determining the emotion category from the scores. This process is repeated for all test utterances of emotions, yielding an average emotion recognition performance of 94.0%.

For deriving the LP residual, an LP filter order of 16 is applied. The LP residual is extracted from pre-emphasized music data using frames of 20 milliseconds with 50% overlap between frames. The highest Hilbert envelopes of around 40 samples are selected per frame. A distinct SVM model is developed for each emotion, resulting in five models for residual phase features. The overall emotion recognition performance after testing is about 56.0%.

In summary, 40-dimensional feature vectors extracted from music frames are input into an SVM model tailored for each emotion. A single category is created by merging all music emotions within the same group, with five SVM models developed altogether. The SVM is then trained to discriminate between acoustic features of one particular emotion and the rest.