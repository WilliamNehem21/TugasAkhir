As data volume grows, so does interest in harnessing its value despite frequent quality issues. One key challenge in data quality is the uneven distribution among different categories, known as class imbalance, which harms the effectiveness of analyses and data mining. Traditional classification techniques particularly struggle with this issue. This paper introduces a new over-sampling method called Train Generate Test (TGT) to address the challenge of imbalanced datasets. TGT ensures synthetic samples augment minority regions and improves classification accuracy across five diverse datasets.

Imbalanced datasets, where one class outnumbers the other, significantly impact classifier performance, favoring the majority class and reducing accuracy for minority classes. Sampling methods address this by adjusting the dataset to balance class distribution, with over-sampling generating synthetic minority instances and under-sampling reducing the majority class size.

The paper discusses related work, details the TGT method, and then evaluates its performance, concluding with the authors' findings. It highlights the limitations of SMOTE (Synthetic Minority Over-sampling Technique) and its adaptations when dealing with non-linear problems and parameter selection. Other approaches, such as SOMO (Self-Organizing Map Based Oversampling), RNS (Real Value Negative Selection), and RBO (Radial Based Oversampling), are also mentioned, each bringing its own advancements and challenges.

TGT employs two classifiers, a decision tree to derive minority class rules, and a neural network to verify synthetic samples. Only samples identified as minority class by the neural network are kept. This dual-verification approach appeared to generate more appropriate synthetic samples compared to SMOTE and its variations.

Performance was evaluated using three classifiers (KNN, Fuzzy KNN, and SVM) on five datasets and compared to SMOTE and Modified SMOTE. Accuracy, sensitivity, and specificity were considered in the analyses. TGT particularly excelled in datasets with a high majority-to-minority class ratio. The paper posits that TGT's strategy of generating samples based on decision tree-derived rules and validating them with a neural network resulted in superior performance over the SMOTE-based methods, which generate synthetic samples based on individual minority samples.

In summary, TGT leverages an adversarial guided over-sampling process that uses rule extraction and verification to improve classification outcomes for imbalanced datasets. The technique's effectiveness against standard and advanced over-sampling methods was demonstrated across varying datasets and classifiers.