Medical diagnosis using computer technology often hinges on accurately detecting lung abnormalities in digital chest X-rays, an essential early step in image analysis. Over recent decades, the evolution of digital imaging and chest radiography has led to vast collections of both labeled and unlabeled images. Research in semi-supervised learning (SSL) algorithms, which utilize both labeled and unlabeled images, has intensified, spurred by the potential these algorithms hold in medical imaging. The current study introduces a novel SSL algorithm rooted in ensemble techniques to classify lung abnormalities from X-rays, and the results show the strength and viability of this approach through numerous tests.

The paper provides a comprehensive analysis of existing techniques for lung segmentation in chest CT images, especially in pathological conditions. These techniques are broadly categorized and critically assessed in terms of their pros and cons.

In the subsequent sections of this paper, Section 2 outlines semi-supervised self-labeled (SSL) algorithms, and Section 3 provides an in-depth explanation of the new algorithm proposed. Section 4 describes experimental evaluations of this algorithm against well-established self-labeled classification algorithms. The final section, Section 5, discusses conclusions and potential avenues for future research.

Self-labeled methods in SSL help mitigate the scarcity of labeled data by allowing classifiers to self-train using their own predictions. These straightforward algorithms often use a wrapper approach. Several of these self-labeled strategies are reviewed in the literature, primarily divided into self-training and co-training approaches.

Co-training involves two classifiers training on separate views of the data and sharing their most confident predictions. While this method has shown promise, the assumption that there are sufficient independent views available is rarely met in practice. Tri-training is an enhancement of co-training for a single view, where three classifiers agree on the most reliable unlabeled data using bootstrap samples. Democratic co-learning and co-forest algorithms leverage ensemble learning for their decision-making processes, despite having a smaller set of labeled examples. These algorithms, along with co-bagging, form committees of classifiers based on bootstrapped samples for robust performance.

Building on earlier methods, the authors propose a semi-supervised learning algorithm, termed AAST, which dynamically selects the best learner for classifications. It uses multiple base learners in a self-training fashion and evaluates them for selection purposes.

The experimental setup tests the new algorithm, labeled ENSL, against other self-labeled algorithms (self-training, co-training, tri-training, co-bagging, CST-voting, co-forest, and democratic-co learning), utilizing a benchmark chest X-ray pneumonia dataset and a CT medical imaging dataset. The proposed ensemble of self-labeled algorithms in ENSL includes self-training, tri-training with C4.5, co-training with SMO, co-forest, and democratic-co learning. The ensemble outperformed individual methods, especially as the proportion of labeled data increased.

Comprehensive testing shows that ENSL, with its majority-voting ensemble of effective self-labeled algorithms, delivers high classification accuracy. This robust performance is statistically confirmed through the Friedman aligned ranks test and the Finner post-hoc test, indicating the potential of ensemble methods in semi-supervised learning frameworks.