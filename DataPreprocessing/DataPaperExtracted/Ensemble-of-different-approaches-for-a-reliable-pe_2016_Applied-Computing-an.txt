ORIGINAL ARTICLE
Ensemble of diﬀerent approaches for a reliableperson re-identiﬁcation system
Loris Nannia, Matteo Munaroa, Stefano Ghidonia, Emanuele Menegattia,
Sheryl Brahnamb,*
aDepartment of Information Engineering at the University of Padua, Via Gradenigo, 6-35131 Padova, Italy
bComputer Information Systems Department at Missouri State University, Springﬁeld, MO 65804, USAReceived 16 December 2014; revised 2 February 2015; accepted 17 February 2015
Available online 7 March 2015
KEYWORDSPerson re-identiﬁcation;Texture descriptors;Ensemble;Color space;Depth mapAbstract An ensemble of approaches for reliable person re-identiﬁcation is proposed in this paper.The proposed ensemble is built combining widely used person re-identiﬁcation systems using differ-ent color spaces and some variants of state-of-the-art approaches that are proposed in this paper.Different descriptors are tested, and both texture and color features are extracted from the images;then the different descriptors are compared using different distance measures (e.g., the Euclideandistance, angle, and the Jeffrey distance). To improve performance, a method based on skeletondetection, extracted from the depth map, is also applied when the depth map is available. The pro-posed ensemble is validated on three widely used datasets (CAVIAR4REID, IAS, and VIPeR),keeping the same parameter set of each approach constant across all tests to avoid overﬁttingand to demonstrate that the proposed system can be considered a general-purpose personre-identiﬁcation system. Our experimental results show that the proposed system offers signiﬁcantimprovements over baseline approaches. The source code used for the approaches tested in thispaper will be available athttps://www.dei.unipd.it/node/2357andhttp://robotics.dei.unipd.it/reid/.
ª2015 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This isan open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. Introduction
Person re-identiﬁcation is the task of recognizing a givenindividual when he or she is viewed across any number ofnon-overlapping views in a distributed network of camerasor at different time instants when captured by a single camera.Research in person re-identiﬁcation is motivated by the need ofautomating many surveillance activities in airports, metro sta-tions, etc. This task requires the creation of a model recordingmacroscopic characteristics, as many of the classic biometriccues (facial appearance and gait characteristics) are often not*Corresponding author. Tel.: +1 417 8739979.E-mail addresses:nanniemg@dei.unipd.it(L. Nanni),munaroemg@ dei.unipd.it(M. Munaro),ghidoniemg@dei.unipd.it(S. Ghidoni), sbrahnam@missouristate.edu(S. Brahnam).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2016) 12, 142–153
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2015.02.0022210-8327ª2015 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).available due to the low frame-rates and resolutions of manysurveillance cameras. Appearance-based, non-collaborativescenarios are challenging because the system must measurethe similarity between two person-centered bounding boxesand correctly identify the same person despite changes inillumination, pose, background, occlusions, and the variabilityin camera resolutions and viewpoints. New advances, however,such as using 3D sensors[16], are making it possible to extractsome soft-biometric features such as a person’s 3D shape,height, and the lengths of limbs.
This paper targets short-term re-identiﬁcation, which aimsat recognizing people within relatively short time frames, thusrelying on the assumption that the person is wearing the sameclothing during the training and testing phases. Unlike track-ing, we assume that no motion information is available for thistask.
In the literature on this topic, the features that are mostcommonly exploited are color, texture and shape. Forinstance, in[7,6,8], the body of each target is divided into smal-ler parts and evaluated with multiple color histograms, one foreach part. Even though this method is simple and effective, itfails in the case of strong illumination changes. Texture-basedand shape-based approaches, such as[1,11,24], usually make use of local features, which provide a detailed description oftargets. These approaches exploit descriptors evaluated on aset of keypoints to generate the signature of a target. Theperformance of this method is thus dependent on the capabil-ity of the keypoint detector to select stable features. In [17]a texture-based signature is proposed that consists of localdescriptors computed around the principal joints of the humanbody. To detect the body joints, 3D data from consumer depthsensors and state-of-the-art skeletal tracking algorithms areexploited. The resulting Skeleton-based Person Signature(SPS) has proved to be very robust in the presence of strongillumination changes. The main drawback of this approach,however, is its dependency on the skeletal tracker; when thisfails to recognize the body pose, the provided signature ismeaningless. For a recent survey on person re-identiﬁcation,see[23].
In this paper we improve the performance of state-of-the-art person re-identiﬁcation systems using an ensemble ofmethods combined by weighted sum rule. The differentsystems utilize different color spaces and several texture andcolor features for describing the images. To the best of ourknowledge, this is the ﬁrst work in which several differentstate-of-the-art person re-identiﬁcation systems, and theirvariants, are combined to obtain a more robust approach.
To demonstrate the generality of our system, we validateour approach on the following well-known datasets:CAVIAR4REID, IAS, and VIPeR. Moreover, we test our sys-tem on a dataset derived from VIPeR, which we call VIPeR45because it contains 45 image pairs from VIPeR that focus onsome of the most difﬁcult samples to re-identify images of per-sons containing strong pose changes, for instance, or wearingvery similar clothing. VIPeR45 was created because personre-identiﬁcation performance was tested in [7]using a dataset that was built in a similar fashion (i.e., using 45 difﬁcult imagepairs extracted from VIPeR); the human subjects obtained aRank(1) of 75% and a Rank(10) of/C24100%[7]. Thus, it is possible for other researchers in person re-identiﬁcation touse VIPeR45 for approximately comparing the performanceof their computer vision systems with the performance ofhuman beings at this same task. The VIPeR45 dataset will
be available athttp://robotics.dei.unipd.it /reid/.
The remainder of this paper is organized as follows. InSection2we describe the base approaches used in our systemand provide details of our weighted ensemble. In Section 3,w e describe the datasets used in our experiments, and in Section 4 we provide the experimental results. Finally, in Section 5we summarize the signiﬁcance of our work and highlight somefuture directions of exploration.
2. Methods
In this work we compare and combine different recent state-of-the-art person re-identiﬁcation systems, viz. a representationthat combines biologically inspired features and covariancedescriptors, called gBiCov[15], Symmetry-Driven Accumula-tion of Local Features (SDALF)[8], Custom PictorialStructures (CPS)[7]based on chromatic content and colordisplacement (CCD), Color Invariants (CI) [12], and the Skeleton-based Person Signature (SPS) technique [17]. More- over, we propose variants of such approaches, obtained byvarying the features used for describing the images and byusing different distance measures. Each of these state-of-the-art systems, our variants, and the different color spaces,distance measures (speciﬁcally, the Jeffery Divergencemeasure, which obtains the best performance), and the colorand texture descriptors used in our approaches are describedin this section.
The following descriptors (detailed in Section 2.8) are tested:
/C15Color: Color descriptor proposed in[3]. /C15WLD: Weber’s Law Descriptor proposed in [5]. /C15LPQT: Local Phase Quantization from Three OrthogonalPlane proposed in[18]./C15VLPQ: Volume Local Phase Quantization proposed in [19].
The best approach (seeFig. 1) is obtained by combiningseveral methods (detailed in this section) that utilize differentcharacteristics and can be described as follows:
/C15Convert the RGB image to XYZ./C15Extract the pictorial structures (PS) from both the RGBand XYZ image./C15Find skeleton joints from the RGB image in the 3D domainusing the tracker./C15Extract gBiCov and SDALF from the RGB image: severaldescriptors are used to describe the region found by PS andthe area around the skeleton joints./C15Match the two images using an appropriate distance mea-sure: different Matching Functions (MF) are used in thedifferent methods./C15Combine the set of matching scores by sum rule.
It is important to note the methods composing the ensem-ble schematized inFig. 1all work in parallel, i.e., each methodis performed independently of the others. The scores of eachmethod are simply summed (after normalization to mean 0and std = 1). Moreover, the proposed ensemble uses nooptimization algorithm: we simply combine the best methodsfor optimizing the average performance on the tested datasets.Ensemble of different approaches 1432.1. Color spaces
To improve the performance of each system, we utilize notonly the RGB color space but also several other color spaces.A color space is an abstract mathematical model describing theway colors can be represented[4]. The input images in thetested databases are given in the RGB color space. To exploreother spaces, the original images are transformed into thefollowing codings: YUV, HSV, HSL, and XYZ.
YUV deﬁnes color in terms of one luma/brightness (Y)component and two chrominance (UV) components, takinginto account human perception by reducing bandwidth forthe two chrominance components. HSV (hue-saturation-value) and HSL (hue-saturation-lightness) are commoncylindrical-coordinate representations of points in the RGBcolor space. The XYZ color space deﬁnes three primaries thatare not tied to any particular physical device but rather arepoints that lie outside the visible gamut, thereby completelyencoding all color perceptions possible in the real world.
2.2. Jeffery Divergence measure
Different distance measures were explored for comparingdescriptors. Those that performed best are the angle distance,the Euclidean distance, and the Jeffrey Divergence measure[13], the last being numerically stable and symmetric. JeffreyDivergence is an information-theoretic measure derived fromShannon’s entropy theory that treats objects as probabilisticdistributions. Thus, it is not applicable to features withnegative values.
Given two objectsA;B2RNtheir Jeffrey Divergence isdeﬁned as
JD A;BðÞ ¼Xni¼1ailog2ai
aiþb iþb ilog2bi
aiþb i/C18/C19 ð1Þ
2.3. gBiCov
Proposed in[15], gBiCov is a state-of-the-art personre-identiﬁcation method that combines biologically inspiredfeatures (BIF)[20]and covariance descriptors[22], speciﬁcally
by encoding the difference between BIF features at differentscales. This image representation efﬁciently measures the sim-ilarity between two persons without needing a preprocessingstep (e.g., to extract the background) since it is robust toillumination, scale, and background changes.
The extraction of the gBiCov descriptors is a three stepprocess:
In Step 1 BIF features are extracted using Gabor ﬁlters andthe max operator. Color images are split into the three HSVcolor channels and convolved with Gabor ﬁlters at 24 differentscales, with neighboring scales grouped into 12 different bands.The BIF magnitude images (B
iie[1,..., 12]) are obtained usingthe max operator within the same band of Gabor features.
In Step 2 similarity of BIF features is computed atneighboring scales using a covariance descriptor. The BIFmagnitude images are divided into small overlapping regionsto retain the spatial information, and the difference betweenthe corresponding regions of the different bands and thecovariance descriptors is computed, i.e., for each region thedifference of covariance descriptors between two consecutivebands is computed as
di;r¼dðC 2i/C01;r ;C2i;rÞ¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃX
Pp¼1In2kPðC2i/C01;r ;C2i;rÞvuut;ð2Þ
whereC i,ris the covariance descriptor (see[15]), i= [1,..., 6],ris the region, andk
PðC2i/C01;r ;C2i;rÞis thep-th generalized eigenvalues ofC
2i/C01,r,C2i,r.
In step 3 the BIF and covariance descriptors are combinedinto a single representation. Althoughd
i,rcan be taken as a direct gBiCov descriptor, they are nonetheless combined withthe BIF magnitude features. The BIF and covariance descrip-tors are two different levels of the entire representation: BIFincludes the appearance-based features while the covariancematrices are a description of the feature properties. Since colorimages in step 1 are split into three HSV color channels, thethree separately extracted gBiCov descriptors are ﬁnallyconcatenated into a single signature that is then reduced usinga dimensionality reduction method such as PrincipalComponent Analysis (PCA). In this paper PCA is not usedsince it needs a training set.CCD+ MF VLPQ+ MF LPQT+ MF Color+ MF WLD+ MF gBiCov+ MFConvert to XYX image
Extract pictorial structures Extract skeleton joints
Fusion by Sum ruleExtract pictorial structures
LPQT+ MF SDALF+ MFOriginal RGB image
Figure 1Flowchart of the proposed ensemble.144 L. Nanni et al.2.4. SDALF
Proposed in[8]SDALF is a method that models three aspectsof human appearance: (i) the overall chromatic content, (ii) thespatial arrangement of colors in speciﬁc regions, and (iii) thepresence of recurrent local motifs with high entropy. Thisinformation is derived from different body parts and weightedby exploiting symmetry and asymmetry perceptual principles.This combination makes SDALF robust against very low res-olution, occlusions, pose, viewpoint, and illumination. SDALFexploits both single-shot and multiple-shot approaches; inother words, the larger the number of images of a givenperson, the greater the expressivity of SDALF. In the descrip-tion of SDALF that follows, the harder case of a single-shotapproach will be described.
SDALF is a three-phase process. In phase 1, the back-ground (BG) is extracted and a silhouette mask Z(bounded by a box of size (I·J)) containing only foreground pixelvalues (FG) is obtained. Axes of asymmetry and symmetryare found for each pedestrian image using two operators: thechromatic bilateral operatorand the specialcovering operator.
The chromatic bilateral operator is deﬁned as
CHði;dÞ¼X
B½i/C0d;iþd/C138d2ðpi;^piÞ;ð3Þ
whered(/C15,/C15) is the Euclidean distance evaluated between theHSV pixel valuesp
i;^pilocated symmetrically with respect tothe horizontal heighti. The Euclidean distance is summed overB
½i/C0d;iþd/C138 , wheredis 1/4 the height of the image. In other words,Bis the FG region lying in the box ofJwidth and verticalextension [i/C0d,i+d]).
Thecovering operatorcalculates the difference for tworegions of a FG area and is deﬁned as
Sði;dÞ¼1=JdjAðB ½i/C0d;i/C138Þ/C0AðB ½i;iþd/C138Þj;ð4Þ
whereA(B [i/C0d,i]) is the FG area in the box of widthJand vertical extension [i/C0d,i].
CHandSare combined to give the axes of symmetry andasymmetry. The mainx-axis of asymmetryAx
TLis located at heighti
TLand is obtained asi TL=argmin i(1/C0CH(i,d)) + S(i,d), with values ofCHnormalized.Ax
TLusually separates the two biggest body portions deﬁned by different colors (e.g.,shirt and pants). The otherx-axis of asymmetryAx
HTis located at heighti
HTand is obtained asi HT=argmin i(/C0S(i,d)). Ax
HTseparates regions that greatly differ in area (e.g., betweenhead and shoulders).
The values ofi HTandi TLisolate three regionsR k, k= {0,1,2} that roughly correspond to the head, body, andlegs, respectively.R
0(the head) is discarded because its sizeis small and contains little information. Given R
1andR 2, they-axis of symmetry is located inj
LRk=argmin jCH(j,d)+ S(j,d), wherek= (1,2) anddis ﬁxed toJ/4.
In Phase 2 features are extracted from each part andaccumulated into a single signature. The following methodsfor extracting features are used: Weighted Color Histograms(WH), Maximally Stable Color Regions (MSCR) [10], and Recurrent High-Structured Patches (RHSP). For all features,their distance from thej
LRkis considered to minimize effectsof pose.
In WH, one histogram is made for each part and each pixelis weighted by a one-dimensional Gaussian Kernel ,ðl;1Þ,wherelis they-coordinate ofjLRk, and1is set toJ/4. In this
way, pixels nearjLRkare given more weight.
The MSCR operator detects blobs by iteratively clusteringneighboring pixels with similar color, considering somethreshold of maximal chromatic distance between colors.MSCR is extracted for each FG part and only within theGaussian kernel used in WH.
In RHSP texture characteristics that are highly recurrentare highlighted. First, patchespof size [I/6·J/6] are randomly extracted on each FG part, many aroundj
LRkto focus on sym- metries, again taking into consideration the Gaussian Kernelused in WH. Entropy of the patches is used to select thosepatches with the most information and is computed as thesumH
pof each RGB channel. Only those patches whose H p
values are higher than a ﬁxed threshold are selected. A set oftransformsT
i,i=1 ,2 ,...,N Tare then applied onp.
In phase 3 the matching of two signatures I AandI Bis performed by estimating the SDALF matching distance asfollows:
dSDALF ðIA;IBÞ¼bWH/C1dWHðWHðI AÞ;WHðI BÞÞþb
MSCR /C1dMSCR ðMSCRðI AÞ;MSCRðI BÞÞþb
RHSP /C1dRHSPðRHSPðI AÞ;RHSPðI BÞÞ ð5Þ
wherebare normalized weights.
2.5. CPS
Proposed in[7]CPS is inspired by studying how human beingsperform re-identiﬁcation (examining eye-tracker information)and focuses on body parts, looking for pictorial structures(PS) and then comparing them part-to-part.
In PS the body model is decomposed into a set of partsL¼fI
pgNp¼1, whereI p=(x p,yp,op,sp) encodes the position,orientation, and scale of partpin imageI, respectively. Givenimage evidenceD, the posterior ofLis modeled as p(L|D)/C181p(D|L)p(L), wherep(D|L) is the image likelihoodandp(L) is a prior modeling of the part’s connectivity. Thekinematic dependencies between body parts are mapped ontoa directed acyclic graph with edgesE. Image evidenceDis obtained with discriminatively trained part models, eachproviding an evidence mapb
p. PS factorizes the likelihood inpðDjLÞ¼
QNp¼1pðdpIpÞ, thereby making the posterior over theconﬁgurationLas follows:
pðLjDÞ/pðI 1ÞYNp¼1pðdpIpÞYNði;jÞ2EpðIiIjÞ;ð6Þ
whereI 1is the root node (the torso) andp(I iIj) models the joint between two connected parts.
PS is trained on a dataset of annotated images. For personre-identiﬁcation,N= 6 parts are selected that completelydescribe the chest, head, torso, thighs, and legs.
After ﬁtting PS the chromatic content and color displace-ment (CCD) in each of the six parts is considered. Chromaticcontent is computed using HSV color histograms, where hueand saturation are jointly taken by a two-dimensionalhistogram, along with a distinct count of full black to take intoaccount areas of low brightness. Since different parts havedifferent sizes (e.g., the torso is roughly three times larger thanthe head), part histograms are multiplied by a set of Nweights.Ensemble of different approaches 145The histograms are then normalized and concatenated to forma single feature vector. Color displacement is considered byextracting MSCR blobs (see Section2.4above) from the PSbody mask.
2.6. CI
Proposed in[12], CI uses shape context descriptors to representthe intra-distribution of structure and is based on the intuitionthat colors composing a person (say wearing a red shirt andblue jeans) form invariant color clusters, i.e., ‘‘color cloud’’shapes, that refer to speciﬁc parts of a person (torso/upperand limbs/lower). The claim of color invariance is based on anumber of intuitive arguments: e.g., that the different colorsobserved in an object are strongly invariant. Another argumentis that relative positions of colors remain invariant (if one parthas a stronger red component than another, that difference willhold for a wide range of illuminants and imaging devices).Color uniformity is also taken into account.
CI adds an invariant signature that exploits the distributionof color in different parts of an object. This signature is com-posed of three descriptors: (i)Cov, a variant of the covariancedescriptor deﬁned in (2), (ii) a color histogram over a log colorspace using histogram intersection[21]as the similarity measure, and (iii) what is called in[12]PartsSC, which uses spatial information regarding the observed colors. The colorhistogram is straightforward;CovandPartsSC, however, require further discussion.
Covuses only the dominant color in the original RGB colorspace of each part of the object when computing the covari-ance descriptor. This variant of the covariance descriptorcaptures the texture not accounted for by signatures describingabsolute colors or relations between colors.
PartsSCuses the Shape Context (SC) descriptor introducedin[2]. SC is a 2D log-polar histogram counting the number ofpoints falling in radius log(r) and orientationhfrom the reference point. Two cases are possible given a set of Ncolor observationsO={x
1,...,x N} within some color space: inthe ﬁrst case observations are given without spatial informa-tion. In the second case, observations are labeled l
i= 1, if they come from the upper part of the object, or l
i= 0 if they refer to the lower part. Two different signatures are extracted thatcorrespond to these two cases.
For case one, if we letO L={x i|li= 0} denote the observations generated from the lower part of an object andO
U={x i|li= 1} denote the observations generated from theupper part of an object, thenPartsSC(O
L,OU) can be deﬁned as
PartsSCðO L;OUÞ¼scðx;O UÞjx2O L fg ;ð7Þ
where sc(x,O) is the shape context descriptor of the points insetOwith respect to reference pointx.
Note that the colors in the upper part of an object areencoded with respect to the colors in the lower part. Thissignature captures the upper part color cloud and the shapeof the lower part color cloud, along with the relative positionsof the two color clouds.
For case two, which encodes no spatial information, thesignature is deﬁned as
SCðOÞ¼fscðx;OÞjx2Og;ð8Þ
whereOis the set of available observations for a given object.2.7. SPS
The Skeleton-based Person Signature (SPS) technique evaluates asignature vector for a given target based on the body pose. Ittakes as input the result of a skeletal tracker, namely a set of bodyjoints, and evaluates a set of local descriptors on the imagepatches around each joint. It should be noted that the best-performing skeletal tracker algorithms work on 3D data, whilewhen it comes to features the 2D approach performs better thanthe 3D counterpart. To improve the overall performance, the SPSis evaluated exploiting not only the 3D point cloud provided bythe 3D sensor, but also the 2D image of the scene, which is usu-ally also provided by 3D sensors. The skeleton joints are found inthe 3D domain by the tracker; they are then projected onto theimage plane (thanks to the calibration between 3D sensor and2D camera). Once the joints are available in the image domain,they are exploited as keypoints forevaluating the local features.Each feature evaluated around a keypoint provides a feature vec-tor (also called descriptor): the complete signature, describing thewhole target, is obtained by concatenating all the feature vectorsof the different body joints, following a pre-determined order.
The SPS for a given targetT kis given by
SPSJk¼[N/C01i¼0fDðJ i;TkÞg;ð9Þ
whereD(J i,Tk) is the descriptor evaluated using the chosenfeature on thei-th joint (J
i) for targetk(T k).
2.8. Texture/color descriptors
The following methods are coupled with state-of-the-artapproaches (CPS and SPS) for improving their performance:
/C15Color, where the following features are concatenated fordescribing a patch[13]: mean and homogeneity of the threechannels; mean, standard deviation and moments (3rd to5th) of the three channels; and marginal histograms (8 binsper channel). Marginal histograms estimate the colorcontent of an image through the probability distributionof colors as a function of each channel separately, thusdiscarding any information about the other channels [3]. /C15WLD[5], based on Weber’s law which states that a changeof stimulus that is just noticeable for human beings is aconstant ratio of the original stimulus; if the change is lessthan this constant ratio then it is considered backgroundnoise. For each pixel of the input image, the differentialexcitation component is computed based on the ratiobetween: (i) the relative intensity differences of a currentpixel against its neighbors, and (ii) the intensity of thecurrent pixel. From the differential excitation componentboth the local salient patterns in the input image and thegradient orientation of the current pixel are computed. Bycombining the WLD feature per pixel, an image (or imageregion) is represented with a histogram./C15LPQT, or LPQ-TOP[18], is the application of Local PhaseQuantization from Three Orthogonal Planes [25]. LPQ uses local phase information extracted using the two-dimensionalshort-term Fourier transform (STFT) computed over a rect-angularM·Mneighborhood centered at each pixel posi-tionxof an image. LPQT calculates LPQ histograms fromthree orthogonal planes (i.e., thexy,xt, andytplanes).146 L. Nanni et al./C15VLPQ is an extension of LPQ where the quantized phaseinformation of the Discrete Fourier Transform (DFT) iscomputed in pixel volume neighborhoods. The local Fouriertransform is computed efﬁciently using 1-D convolutionsfor each dimension in a 3-D volume (see[19], for details).
SIFT is also tested for SPS since it is the best descriptoramong those that were tested in[17], where SPS was ﬁrst pro-posed. SIFT[14]is widely used in robotics. It is a keypointdetector and a descriptor invariant to image scale and rotation;it is also robust to changes in illumination, noise, and minorafﬁne transformations. SIFT is computed as an 8-binnedhistogram of gradient distribution within the region aroundeach keypoint. The descriptor is normalized to unit length toobtain illumination invariance.
3. Datasets
To verify our approach and to build a general personre-identiﬁcation system, we exploited several datasets thatare widely used to test intelligent video surveillance systems:VIPeR, CAVIAR4REID, and IAS.
VIPeR (Viewpoint Invariant Pedestrian Recognition) is adataset composed of a large number of people (632) that areseen at different viewpoints and is available at http:// vision.soe.ucsc.edu/node/178. Only one image pair for eachperson is available, and people are framed at a distance. Thisdataset is a widely used benchmark. As reported in theliterature, results on VIPeR are produced by ten runs, eachconsisting of a partition of 316 randomly selected image pairs.Since this dataset is composed of 2D images, the skeletal trackeris unable to provide body joints; however, for a small subsetof images (45 image pairs), keypoints were manually added bya human operator (we call this subset of images VIPeR45).
CAVIAR is a dataset where 72 different people were col-lected for the EC funded CAVIAR project/IST 2001 37540and is available athttp://groups.inf.ed.ac.uk/vision/CAVIAR/CAVIARDATA1/. Caviar is a dataset in whichmultiple test cases are considered; the CAVIAR4REID(Caviar for Re-identiﬁcation) test case was used in our exper-iments. CAVIAR4REID is characterized by high level ofocclusions, pose changes, and low-resolution images. As inthe case of VIPeR, this dataset is composed by 2D images;keypoints were manually added by a human operator. Weperformed the same single-frame test described in [7], the frame selection is performed ﬁve times and the average resultsare reported.IAS is a dataset that was originally acquired for testing the
ﬁrst version of the SPS algorithm. It includes 33 sequences andinvolves 11 people. For every subject, the training and testingsequences were collected in different rooms. The entire trainingset is composed by 2146 images, with 999 images belonging tothe testing set. A 3D sensor was placed on a robot so imagesequences are seen from a robot’s perspective rather than fromthe perspective of a surveillance camera. The IAS datasetincludes sequences of the same target seen under very differentlighting conditions. IAS was used as a stress test for the SPSapproach. IAS is available athttp://robotics.dei.unipd.it/reid/index.php/8-dataset/5-overview-iaslabone.
4. Results
To verify our approach and to build a general personre-identiﬁcation system, we use the well-known datasetsdescribed in Section3. Across all databases the same parame-ters are maintained for each approach since the aim is not tooptimize the performance of the proposed system for eachdataset but rather to show that this generalized method workswell across all datasets without ad-hoc tuning.
Rank(1) and Rank(10) are used as the performance indica-tors. Rank(k) is the average person recognition rate computedwhen considering a classiﬁcation to be correct if the groundtruth person appears among the subjects who obtained the k best classiﬁcation scores.
In the ﬁrst experiment the aim was to test the different colorspaces applied to the different person re-identiﬁcation systems.Results are presented inTables 1–8. Each cell contains theRank(1) and Rank(10) values, except in IAS where onlyRank(1) is reported since IAS has only 11 individuals. In theapproaches where different distances are applied, all resultsare reported, with the ﬁrst row inside a cell reporting theJeffrey distance, the second row the angle distance, the thirdrow the Euclidean distance. Due to the computational issueof choosing the best methods to combine for fusion explo-ration, we ran experiments only on the VIPeR, VIPeR45,and CAVIAR4REID datasets. For IAS only the performanceof the approaches used to build the ensembles and the baselinemethods is reported. In the last column, the best averageensemble of the different color spaces is reported(a·X+b·Yis the fusion by weighted sum rule betweenthe color spaceX, with weighta, andY, with weightb). In the last row, we report the computation time (CT) in secondsfor extracting the descriptors from an image of size 128 ·64 using MATLAB R2013a (without the parallel toolbox) onan i5-3470 3.2 GHz processor with 8 GB of Ram.
It is interesting to note that the performance of gBiCov (seeTable 1) is clearly related to the color space; in the originalpaper[15], gBiCov is calculated on the HSV space only. Thefusion between HSV and YUV leads to a good performancein all the tested datasets. The performance obtained bythe three distances is very similar. SDALF (see Table 2) obtains the best performance using the RGB color space, butits fusions are not useful.
CI (seeTable 3) obtains the best improvement due to thefusion of the different color spaces. Unfortunately, CI workspoorly in the IAS dataset because of the strong illuminationchange between training and testing sets caused by the1It is not clear how the images are selected, for future faircomparison we have selected the frames in the following way:for ﬀ = 1:5 %for ﬁve timesTR = []; TE = [];for person = 1:max(label) %for each persona = ﬁnd(label== person); %ﬁnd the frames of that personTR = [TR a(ﬀ)]; %id frame to insert in TRTE = [TE a(NUM(person)/2 + ﬀ)]; %id frame to insert inTE, NUM contains the number of frames of each personend%the images of TR and TE will be compared...Ensemble of different approaches 147different auto-exposure levels of the Kinect sensor. Moreover,CI has a low computational time. Our recommendation is touse the CI ensemble with RGB and YUV only when low com-putational power is available and in cases where there are nopronounced illumination changes.
CPS experiments spanTables 4–8where results arereported for each texture/color descriptor (see Section 2.5). In general, CPS obtains very good results. It should be notedthat unlike the results reported in the original paper [7]we
did not change the parameters of this approach for thedifferent datasets. Moreover, examining the results of CPS, itis clear that the proposed approach for extracting featuresfrom the mask obtained by CPS works quite well. The differ-ent distances, however, are quite similar in performance,except that the Jeffrey distance outperforms the Euclideanand angle distance measures.Table 1gBiCov performance across different colorimetric spaces.
gBiCov RGB HSV HSL XYZ YUV RGB + HSV + YUV HSV + YUVIAS 86.7 57.4 –––– –85.9 58.385.9 58.3VIPeR 45 15.6 44.4 22.2 46.7 15.6 46.7 8.9 37.8 24.4 57.815.6 44.4 24.4 48.9 17.8 46.7 22.2 46.7 13.3 40.0 11.1 31.1 24.4 55.6 17.8 46.7 24.4 51.117.8 46.7 22.2 46.7 13.3 40.0 11.1 31.1 24.4 55.6 17.8 46.7 24.4 51.1VIPeR 3.2 14.2 12.9 37.3 8.6 31.7 1.6 8.3 10.7 31.8 12.8 33.9 13.5 39.7 3.2 14.2 12.9 38.0 6.7 26.3 1.5 7.8 8.8 25.6 11.2 30.0 13.3 35.83.2 14.2 12.9 38.0 6.7 26.3 1.5 7.8 8.8 25.6 11.2 30.0 13.3 35.8CAVIAR4REID9.734.4 7.8 31.4 6.1 23.3 8.6 29.4 6.7 31.7 8.6 35.3 8.1 31.79.4 33.9 7.2 31.7 5.3 22.2 7.8 29.7 8.3 33.6 8.9 37.8 9.2 35.69.4 33.9 7.2 31.7 5.3 22.2 7.8 29.7 8.3 33.6 8.9 37.8 9.2 35.6CT 7.87Note: gBiCov was always performed without a mask.
Table 2SDALF performance across different colorimetric spaces.
SDALF RGB HSV HSL XYZ YUV RGB + XYZ RGB + XYZ + YUVIAS 86.0 – – – – – –VIPeR 45 24.4 53.3 17.8 44.4 – 24.4 51.1 26.7 53.326.7 51.1 24.4 48.9 VIPeR 18.8 47.9 10.0 35.8 – 17.1 43.4 10.9 34.5 19.7 48.0 18.7 48.6CAVIAR4REID 9.439.44.7 30.0 – 11.4 37.2 4.2 28.3 11.938.9 10.3 38.9 CT 2.70
Table 4CPS based on different colorimetric spaces.
CPS RGB HSV HSL XYZ YUV RGB + HSLIAS 96.7 –––– – VIPeR45 24.4 55.6 15.6 40.0 26.7 40.0 33.3 60.0– 22.2 51.1 VIPeR 14.4 43.8 12.0 39.9 10.4 38.3 11.8 39.1 – 19.1 53.5 CAVIAR4REID18.1 50.37.8 35.0 6.4 26.9 16.7 48.3 – 13.3 44.4 CT 1.19Table 3CI performance across different colorimetric spaces.
CI RGB HSV HSL XYZ YUV RGB + YUVIAS 43.5 – – –VIPeR45 11.1 44.4 8.9 40.0 6.7 35.6 20.0 42.2 24.4 46.7 28.9 46.7 VIPeR 12.7 39.8 5.2 20.3 2.8 17.0 4.2 13.0 9.6 28.4 15.5 42.8 CAVIAR4REID 8.1 31.7 6.7 31.1 3.6 24.7 6.4 32.2 8.3 31.7 9.7 33.9 CT 0.33148 L. Nanni et al.Table 5CPS-color performance across different colorimetric spaces.
CPS-color RGB HSV HSL XYZ YUV RGB + HSVVIPeR45 17.857.833.3 55.6 13.3 44.4 15.6 48.9 – 33.353.3 24.4 55.6 17.8 40.0 13.3 48.9 17.8 46.7 24.4 55.624.4 53.3 15.6 40.0 13.3 51.1 17.8 46.7 24.4 55.6VIPeR 6.1 19.2 12.4 39.0 10.5 34.9 0.3 12.2 – 12.9 32.06.7 21.8 5.8 14.2 8.3 36.1 0.2 8.5 7.5 21.76.3 21.2 4.6 14.1 7.4 35.3 0.2 8.5 7.1 21.2CAVIAR4REID 11.7 45.6 8.3 31.4 3.3 23.1 4.7 33.6 – 13.342.8 11.946.77.2 29.2 3.9 23.6 4.4 31.9 12.5 45.812.246.77.2 28.6 3.6 23.9 4.4 31.9 12.2 45.8 CT 1.22
Table 6CPS-WLD performance across different colorimetric spaces.
CPS-WLD RGB HSV HSL XYZ YUV HSV + HSL RGB + HSV + HSLVIPeR45 13.3 42.2 33.3 46.7 37.8 46.7 13.3 51.1 – 35.6 46.7 31.1 48.911.1 40.0 20.0 46.7 26.7 48.9 11.1 46.7 24.4 48.9 28.9 48.913.3 35.6 20.0 44.4 20.0 51.1 6.7 48.9 24.4 51.1 22.2 48.9VIPeR 3.0 12.6 14.2 40.2 12.1 35.2 3.9 16.2 – 16.1 39.5 13.5 36.14.0 12.1 8.2 25.4 6.4 26.3 3.1 10.9 8.6 28.7 9.0 26.52.7 10.7 5.7 21.5 5.7 21.4 1.5 10.4 6.5 23.5 6.1 22.8CAVIAR4REID 10.3 37.5 10.3 32.5 6.9 28.1 8.9 38.1 – 8.1 34.2 9.7 38.3 9.2 32.8 8.6 30.6 6.4 26.1 7.2 34.2 6.4 26.4 10.832.5 8.9 32.8 6.7 28.6 5.8 24.4 6.1 33.9 6.9 26.9 9.7 33.9CT 2.59
Table 8CPS-LPQT performance across different colorimetric spaces.
CPS-LPQT RGB HSV HSL XYZ YUV RGB + XYZVIPeR45 17.8 44.4 22.2 48.920.0 46.726.7 48.9– 24.4 46.7 17.848.920.0 46.7 6.7 42.2 22.2 46.7 22.2 48.9 15.648.917.8 46.7 13.3 42.2 26.746.726.746.7 VIPeR 12.2 44.2 12.7 37.6 4.0 27.0 13.0 36.8 – 14.4 46.0 10.9 38.3 7.6 26.9 3.6 18.1 9.6 34.2 12.1 41.610.1 36.6 7.0 25.6 3.3 17.8 9.3 31.9 10.4 41.3CAVIAR4REID 11.1 32.8 8.1 33.6 4.7 21.7 8.9 35.6 – 10.3 36.7 8.9 35.0 8.9 33.1 5.0 20.0 8.3 33.3 9.2 36.47.2 35.8 8.3 33.1 5.0 22.8 7.5 33.3 8.6 36.4CT 1.72Table 7CPS-VLPQ performance across different colorimetric spaces.
CPS-VLPQ RGB HSV HSL XYZ YUV RGB + XYZVIPeR45 17.8 42.2 22.2 46.7 15.6 44.4 20.0 48.9 – 22.2 46.711.1 46.7 11.1 44.4 11.1 40.0 24.4 48.9 20.0 46.711.1 42.2 11.1 33.3 6.7 31.1 22.2 42.2 13.3 35.6VIPeR 8.3 37.4 7.6 31.3 2.8 15.9 9.5 33.0 – 12.8 42.86.7 31.5 4.9 21.3 2.2 13.1 8.9 31.3 9.5 36.53.7 19.8 2.3 16.2 1.5 9.4 4.9 21.3 5.1 23.4CAVIAR4REID 7.8 32.2 6.7 33.6 4.2 20.3 8.9 34.4 – 9.4 35.05.6 30.3 5.3 27.2 3.6 18.9 6.1 33.3 8.3 30.65.3 31.4 5.0 26.7 2.5 18.3 7.5 31.7 8.1 30.3CT 1.25Ensemble of different approaches 149The SPS experiments spanTables 9–13where results are reported for each texture/color descriptor (see Section 2.5). In general, SPS performs very well. This approach is very fast since the features are extracted only from a small set of patches
in the image. However, it does need to compute a depth mapand a skeleton point detection step.
Table 9SPS-SIFT performance across different colorimetric spaces.
SP-SIFT RGB HSV HSL XYZ YUV RGB + XYZIAS 92.6 67.4 71.8 91.7 92.1 92.2VIPeR45 6.7 37.8 13.3 53.3 15.6 51.1 11.1 42.2 4.4 37.8 8.9 37.8CAVIAR4REID 18.3 41.7 10.3 30.3 12.2 32.8 17.8 42.5 15.0 30.6 17.8 41.4CT 0.4
Table 10SPS-color performance across different colorimetric spaces.
SPS-color RGB HSV HSL XYZ YUV RGB + XYZ RGB + HSV + XYZVIPeR45 22.2 57.8 28.9 51.1 15.6 42.2 17.8 46.7 20.0 33.3 17.7 57.8 31.157.8 20.0 57.8 20.0 37.8 13.3 42.2 6.7 33.3 4.4 24.4 28.9 60.026.7 53.3 26.7 57.8 20.0 37.8 13.3 44.4 8.9 33.3 4.4 24.4 28.9 53.3 24.4 53.3CAVIAR4REID 18.3 57.2 12.8 41.7 4.4 26.1 15.3 51.4 5.8 31.9 19.2 58.1 19.758.9 17.261.112.5 36.9 4.4 23.6 17.2 49.7 2.5 17.5 17.5 60.3 17.2 57.816.7 60.3 12.5 36.9 4.2 24.4 17.2 49.7 2.5 17.5 16.7 59.4 17.2 56.4CT 0.028
Table 11SPS-WLD performance across different colorimetric spaces.
SPS-WLD RGB HSV HSL XYZ YUV RGB + XYZ RGB + HSV + XYZVIPeR45 2.2 44.4 13.3 42.2 17.848.9 15.6 48.9 15.6 48.9 6.7 53.3 17.851.1 6.7 44.4 13.3 42.2 13.3 33.3 17.8 57.8 17.8 57.811.1 48.9 11.1 51.1 8.9 33.3 11.1 46.7 8.9 46.7 11.1 40.0 11.1 40.0 13.3 37.8 22.2 48.9CAVIAR4REID 15.8 44.7 14.2 34.4 13.1 31.1 11.1 39.7 7.8 31.9 15.3 44.7 16.743.3 8.6 37.2 13.6 43.6 11.4 34.7 9.7 36.7 8.6 32.5 8.9 39.4 11.9 41.710.8 36.7 10.6 40.3 9.2 37.2 10.6 40.3 7.8 31.9 11.4 40.0 11.7 42.2CT 0.042
Table 12SPS-VLPQ performance across different colorimetric spaces.
SPS-VLPQ RGB HSV HSL XYZ YUV RGB + XYZ RGB + HSV + XYZVIPeR45 20.0 44.4 22.2 44.4 6.7 35.6 22.2 44.4 11.1 35.6 33.3 46.7 35.6 48.9 20.0 55.6 20.0 42.2 4.4 40.0 24.4 51.1 11.1 37.8 28.9 46.7 31.1 46.713.3 46.7 13.3 42.2 8.9 31.1 22.2 42.2 8.9 35.6 22.2 40.0 22.2 37.8CAVIAR4REID 13.9 42.8 12.8 40.6 5.3 18.6 15.0 38.9 14.2 43.1 16.743.316.745.8 13.9 42.8 9.7 36.9 4.4 21.4 14.2 38.9 12.5 41.9 16.743.616.7 47.5 3.6 21.9 6.1 25.6 1.7 18.1 5.3 28.3 8.9 33.3 6.1 24.4 4.4 24.7CT 0.33
Table 13SPS-LPQT performance across different colorimetric spaces.
SPS-LPQT RGB HSV HSL XYZ YUV RGB + XYZ RGB + HSV + XYZVIPeR45 24.4 44.4 24.4 44.4 17.8 42.2 26.7 44.4 15.6 48.926.7 42.235.644.4 20.048.926.7 44.4 8.9 40.0 22.2 40.0 8.9 46.7 22.2 42.2 35.6 42.217.846.7 20.0 46.7 11.1 37.8 17.8 40.0 11.1 42.2 22.2 40.0 26.7 42.2CAVIAR4REID 17.8 47.5 12.2 45.3 7.5 26.7 16.1 41.4 15.6 37.2 17.246.7 17.248.3 13.1 44.4 10.6 41.1 5.6 22.5 13.6 38.9 13.1 38.1 13.9 41.7 15.0 45.38.9 35.8 8.3 38.3 5.3 20.8 9.7 3 1.9 11.9 38.9 10.8 36.9 9.7 38.9CT 0.12150 L. Nanni et al.InTable 14we compared the performance of some ensem-bles (before the fusion the scores of the approaches are nor-malized to mean 0 and standard deviation 1) with the beststand-alone methods:
/C15BS is the best stand-alone approach considering bothRank(1) and Rank(10) on average in the tested datasets./C15BI is the best method considering the different datasetsseparately./C15FUS1 is the sum rule ensemble of CPS(RGB) +CPS_LPQT(XYZ)./C15FUS2(K) is the weighted sum rule ensemble ofK·CPS(RGB) + gBiCov(RGB) + SDALF(RGB) + CPS_LPQT(XYZ)./C15FUS3(K) is the weighted sum rule of K·CPS(RGB) + gBiCov(RGB) + SDALF(RGB) + CPS_LPQT(XYZ) +2·SPS_COLOR(RGB) + 2·SPS_LPQT(RGB) + 2· SPS_WLD(RGB) + SPS_VLPQ(RGB)./C15NogBiCov, is the method FUS3(12) without the expensive(from the computation time view) gBiCov(RGB).NogBiCov obtains a performance that is similar toFUS3(12) (it is slightly lower in CAVIAR4REID) withoutusing gBiCov, which takes several seconds to describe a givenimage.
For a deeper evaluation of the performance, Area UnderROC curve[9]is reported inTable 15. The Area Under theCurve (AUC) can be interpreted as the probability that a lowersimilarity is assigned to a randomly chosen positive match (i.e.,for the same person) rather than to a randomly chosen nega-tive match (i.e., for different persons). We have adopted theextension of AUC for multi-class datasets what is called a‘‘one versus all’’ approach, where, if you have three classes,you would calculate three AUCs. In the ﬁrst, you wouldchoose the ﬁrst class as the positive class, and group the othertwo classes together as the negative class, and so on. The aver-age result is reported.
Notice that in VIPeR45 the best method (considering theRank) obtains a lower AUC in respect to BS. Using AUC asthe performance indicator shows that the fusions clearly out-perform the stand-alone methods that built them.
The proposed combination is straightforward, but it has apotential drawback in computational speed, especially whencombining several methods that are already known to beexpensive, e.g. SDALF (foreground extraction) with CPS(parts detection). InTable 16, we report the computationtime of the ensemble methods using an i5-3470 – 3.2 GHzprocessor with 8 GB of Ram, running MATLAB codewith the parallel toolbox for exploiting the four cores.Descriptors are extracted from an image of size 128 ·64. However, as noted in the discussion ofFig. 1, the different approaches run independently of one another. Moreover,internally several approaches are highly parallelizable (e.g.,the descriptor of each part of the image could be extractedin parallel). In our ﬁrst tests using a better performingCPU (a Xeon E5 – 1620 v2.0) the computation time ofNogBiCov is/C242.5 s.
As in several other machine learning problems, it is verydifﬁcult to ﬁnd a stand-alone approach that works well onall the different datasets representing a speciﬁc problem. CPSworks very well in IAS and CAVIAR4REID but does notobtain the best performance in VIPeR.
Table 15Proposed ensemble approaches – AUC as performance indicator.
VIPeR VIPeR 45 CAVIAR4REID IASBS 0.86 0.76 0.77 0.95BI 0.91 0.60 0.77 0.95FUS10.920.70 0.79 0.96 FUS2(2)0.920.74 0.78 0.94 FUS2(6) 0.90 0.75 0.78 0.95FUS2(9) 0.89 0.75 0.78 0.95FUS2(12) 0.89 0.75 0.78 0.95FUS2(18) 0.88 0.76 0.78 0.95FUS3(2) – 0.74 0.85 0.91 FUS3(6) – 0.76 0.84 0.93FUS3(9) – 0.76 0.83 0.94FUS3(12) – 0.76 0.82 0.95FUS3(18) – 0.770.81 0.95 NogBiCov – 0.760.82 0.95Table 14Proposed ensemble approaches – rank as perfor-mance indicator.
VIPeR VIPeR 45 CAVIAR4REID IASBS 14.4 43.8 24.4 55.6 18.1 50.3 96.7BI 19.7 48.0 37.8 46.7 18.1 50.3 96.7FUS124.5 61.740.0 53.3 16.1 49.4 83.5FUS2(2) 22.9 56.2 33.3 53.3 18.3 55.3 95.1FUS2(6) 21.2 53.8 28.9 55.6 20.3 54.4 95.4FUS2(9) 20.3 51.6 26.7 57.8 20.3 52.8 95.7FUS2(12) 18.9 50.7 26.7 57.8 20.3 53.3 96.4FUS2(18) 17.9 49.2 28.9 60.0 20.3 51.9 96.7FUS3(2) –44.457.8 25.3 65.0 97.6FUS3(6) – 42.2 60.0 26.363.998.0 FUS3(9) – 42.2 60.0 26.1 63.3 97.8FUS3(12) – 37.8 64.4 25.6 61.9 97.6FUS3(18) – 33.366.724.2 59.4 97.4 NogBiCov – 37.8 64.4 25.0 61.4 97.5Ensemble of different approaches 151To obtain a more realistic validation of our approach, weused the same parameters in all the tested datasets. In otherwords, we did not optimize the performance of our systemsfor each dataset (to avoid overﬁtting). Nonetheless, our fusionmethod outperforms the average performance of all the stand-alone approaches. It should be noted that the results reportedfor SPS on the IAS dataset in[17]are not comparable withthose reported in this paper. In[17]the extracted skeletonsof low quality were removed; in the experiments reported here,the entire IAS dataset is used (i.e., no frames are pruned).
Finally, inTable 17we compared our best approach withseveral state-of-the-art methods proposed in the literature.The methods named OR_Xmean the performance as reportedin the papers where methodXis proposed. In several papersthe parameters of the methods evaluated are ﬁxed separatelyfor each dataset; in contrast, our method, as mentioned above,always uses the same parameters across the datasets to avoidoverﬁtting. With OR_CI we report the best approach reportedin[12], whereas OR_CI is obtained using semi-automaticallyextracted masks; in our tests, we use automatically extractedsilhouettes. EBICOV is an ensemble obtained combiningSDALF and gBiCov (using the performance reported in[15]). We have also reported the performance of approachesbased on the learnt metric (KBICOV, KISSME, andPCA-RBF), assuming a training set is available. Thus, theperformance comparison with our approach is not fair. Yetit is interesting to note that our method obtains a performancethat is very similar to methods based on learnt metrics alsoperformed without skeleton detection.
An interesting example of the difference in performancewhen a method is optimized for a dataset can be observed inthe performance of CPS. If CPS is optimized for VIPER, itobtains a Rank(10) of/C2457%, while CPS optimized forCAVIAREID obtains a Rank(10) of/C2453% (using our set ofimages
1). In contrast, if we use a set of parameters that remainthe same for both datasets, we obtain a rank(10) of 43.8% and50.3%, respectively, clearly lower than those obtained whenseparately optimizing the parameters for each dataset.
For a more exhaustive comparison of methods with theliterature, we suggest our results be compared to a recentsurvey[23]. ExaminingTable 4in[23], it is clear that our
proposed approach outperforms several other recent systemsnot compared in this paper.
5. Conclusion
In this paper we run experiments to develop an ensemble ofperson re-identiﬁcation systems that works well on differentdatasets without any ad-hoc dataset tuning. Therefore, weare quite sure that our approach is stable and could be usedin different image conditions.
For improving the state-of-the-art approaches, differentcolor spaces, texture, and color features for describing theimages were explored. We also considered different distancesfor comparing descriptors. Among the tested distances, thebest performance was obtained with the Jeffrey Divergencemeasure.
The new methods proposed in this paper were tested acrossseveral benchmark databases: CAVIAR4REID; VIPeR;VIPeR45; IAS. The experimental results demonstrate thatthe proposed approach provides signiﬁcant improvements overbaseline algorithms. The VIPER45 is a new dataset of 45image pairs taken from VIPeR that focus on difﬁcult sampleswith strong pose changes and with subjects wearing similarclothing. It was created because human beings were tested in[7]in a dataset that was built in a similar fashion (i.e., using45 difﬁcult image pairs extracted from VIPeR). It is thuspossible for other researchers in person re-identiﬁcation touse VIPeR45 for approximately comparing the performanceof their computer vision systems with the performance ofhuman beings.
A drawback of our approach is computational time, whichis not real-time, i.e., using MATLAB code. However, severalmethods used in our approach are internally highly paralleliz-able. The main focus of this paper was not on computationalspeed; our goal was to produce an approach that could matchhuman performance. Unfortunately our results show that thisgoal has not been achieved (our ensemble obtains a Rank(10)of/C2465%, while a human being obtains/C24100%). Nonetheless,we have succeeded in producing a stable general-purposeTable 16Computation time in seconds.
FUS1 FUS2 FUS3 NogBiCov1.55 8.15 10.25 4.82
Table 17Comparison with the Literature.
VIPeR CAVIAR4REID Here 22.9 56.2 25.3 65.0OR_CPS 21.84 57.21 /C249/C2447 OR_SDALF 19.87 49.37 –eBicov 24.34 58.48 –OR_CI 24.00 58.00 /C249/C2445 kBiCov 31.11 70.71 –MCC[1]15.19 57.59 – KISSME[11]19.60 62.60 – PCCA-rbf[2]19.27 64.91 –152 L. Nanni et al.person re-identiﬁcation system that offers signiﬁcant improve-ments over baseline approaches.
The MATLAB code of the approach described in this paperwill be freely available athttps://www.dei.unipd.it/node/2357as well as athttp://robotics.dei.unipd.it/reid/.
References
[1] M.S. Bauml, R. Stiefelhagen, Evaluation of local features forperson re-identiﬁcation in image sequences, Paper presented atthe 10th IEEE International Conference on Advanced Videoand Signal-Based Surveillance, Klagenfurt, Austria, 2011.
[2]S. Belongie, J. Malik, J. Puzicha, Shape matching and objectrecognition using shape contexts, IEEE Trans. Pattern Anal.Mach. Intell. 24 (24) (2002) 509–522
.
[3]F. Bianconi, A. Ferna´ndez, E. Gonza´lez, S.A. Saetta, Performance analysis of colour descriptors for parquet sorting,Expert Syst. Appl. 40 (5) (2013) 1636–1644
.
[4]L. Busin, N. Vandenbroucke, L. Macaire, Color spaces andimage segmentation, Adv. Imaging Electron Phys. 51 (2008) 65–168
.
[5]J. Chen, S. Shan, C. He, G. Zhao, M. Pietika ¨inen, X. Chen, W. Gao, WLD: a robust local image descriptor, IEEE Trans.Pattern Anal. Mach. Intell. 32 (9) (2010) 1705–1720
.
[6]D.C. Cheng, M. Cristani, Person re-identiﬁcation by articulatedappearance matching, in: S. Gong, M. Cristani, S. Yan, C.C.Loy (Eds.), Person re-identiﬁcation, Springer, London, 2014, pp.139–160
.
[7]D.S. Cheng, M. Cristani, M. Stoppa, L. Bazzani, V. Murino,Custom pictorial structures for re-identiﬁcation, in: J. Hoey, S.McKenna, E. Trucco (Eds.), Proceedings of the British MachineVision Conference, BMVA Press, University of Dundee, UK,2011, pp. 1–11
.
[8] M. Farenzena, L. Bazzani, A. Perina, V. Murino, M. Cristani,Person re-identiﬁcation by symmetry-driven accumulation oflocal features, Paper presented at the IEEE Computer Visionand Pattern Recognition, San Francisco, CA, 2010.
[9]T. Fawcett, ROC graphs: Notes and Practical Considerationsfor Researchers, HP Laboratories, Palo Alto, 2004
.
[10] P.-E. Forsse´n, Maximally stable colour regions for recognitionand matching, Paper presented at the IEEE Conference onComputer Vision and Pattern Recognition, Minneapolis, USA,2007.
[11] K. Jungling, A. Michael, Feature based person detection beyondthe visible spectrum, Paper presented at the IEEE Conference onComputer Vision and Pattern Recognition Workshop, MiamiBeach, FL, 2009.[12]I. Kviatkovsky, A. Adam, E. Rivlin, Color invariants for personreidentiﬁcation, IEEE Trans. Pattern Anal. Mach. Intell. 35 (7)(2013) 1622–1634
.
[13] H. Liu, D. Song, S. Ru¨ger, R. Hu, V. Uren, Comparingdissimilarity measures for content-based image retrieval, Paperpresented at the Information Retrieval Technology: 4th AsiaInformation Retrieval Symposium, AIRS 2008, Harbin, China,2008.
[14] D.G. Lowe, Object recognition from local scale-invariantfeatures, Paper presented at the 7th IEEE InternationalConference on Computer Vision, Kerkyra, 1999.
[15]B. Ma, Y. Su, F. Jurie, Covariance descriptor based on bio-inspired feature for person re-identiﬁcation and face veriﬁcation,Image Vision Comput. 32 (2014) 379–390
.
[16] M.B. Munaro, 3d reconstruction of freely moving persons forreidentiﬁcation with a depth sensor, Paper presented at theIEEE International Conference on Robotics and Automation,Hong Kong, China, 2014.
[17] M.B. Munaro, S. Ghidoni, D.T. Tartaro, E. Menegatti, Afeature-based approach to people re-identiﬁcation usingskeleton keypoints, Paper presented at the IEEE InternationalConference on Robotics and Automation, Hong Kong, China,2014.
[18] V. Ojansivu, J. Heikkila, Blur insensitive texture classiﬁcationusing local phase quantization, Paper presented at the ICISP,2008.
[19]J. Pa¨iva¨rinta, E. Rahtu, J. Heikkila¨, Volume Local Phase Quantization for Blur-insensitive Dynamic TextureClassiﬁcation Image Analysis, Springer, Berlin, Heidelberg,2011, pp. 360–369
.
[20]M. Riesenhuber, T. Poggio, Hierarchical models of objectrecognition in cortex, Nat. Neurosci. 2 (11) (1999) 1019–1025
.
[21] M. Swain, D. Ballard, Indexing via color histograms, Paperpresented at the IEEE International Conference on ComputerVision, Osaka, Japan, 1990.
[22]O. Tuzel, F. Porikli, P. Meer, Pedestrian detection viaclassiﬁcation on riemannian manifolds, IEEE Trans. PatternAnal. Mach. Intell. 30 (10) (2008) 1713–1727
.
[23]R. Vezzani, D. Baltieri, R. Cucchiara, People re-identiﬁcation insurveillance and forensics: a survey, ACM Comput Surveys 46(2) (2013) 29:21–29:23
.
[24]K. Yoon, D. Harwood, L. Davis, Appearance-based personrecognition using color/path-length proﬁle, J. Visual Commun.Image Represent. 17 (3) (2006) 605–622
.
[25]G. Zhao, M. Pietika¨inen, Dynamic texture recognition usinglocal binary patterns with an application to facial expressions,IEEE Trans. Pattern Anal. Mach. Intell. 29 (6) (2007) 915–928
.Ensemble of different approaches 153