Exact Conditioning of Regression Random Forest for Spatial Prediction
Francky Fouedjio
AngloGold Ashanti Australia Ltd., Growth and Exploration, 140 St. Georges Terrace, Perth, WA, 6000, Australia
ARTICLE INFO
Keywords:Exact conditioningMonte Carlo samplingMulti-GaussianSpatial predictionPrincipal component analysisRandom forestABSTRACT
Regression random forest is becoming a widely-used machine learning technique for spatial prediction that showscompetitive prediction performance in various geoscience ﬁelds. Like other popular machine learning methods for spatial prediction, regression random forest does not exactly honor the response variable ’s measured values at sampled locations. However, competitor methods such as regression-kriging perfectly ﬁt the response variable’s observed values at sampled locations by construction. Exactly matching the response variable ’s measured values at sampled locations is often desirable in many geoscience applications. This paper presents a new approachensuring that regression random forest perfectly matches the response variable ’s observed values at sampled locations. The main idea consists of using the principal component analysis to create an orthogonal representationof the ensemble of regression tree predictors resulting from the traditional regression random forest. Then, theexact conditioning problem is reformulated as a Bayes-linear-Gauss problem on principal component scores. Thisproblem has an analytical solution making it easy to perform Monte Carlo sampling of new principal componentscores and then reconstruct regression tree predictors that perfectly match the response variable ’s observed values at sampled locations. The reconstructed regression tree predictors ’average also precisely matches the response variable’s measured values at sampled locations by construction. The proposed method ’s effectiveness is illus- trated on the one hand using a synthetic dataset where the ground-truth is available everywhere within the studyregion, and on the other hand, using a real dataset comprising southwest England ’s geochemical concentration data. It is compared with the regression-kriging and the traditional regression random forest. It appears that theproposed method can perfectlyﬁt the response variable’s measured values at sampled locations while achieving good out of sample predictive performance comparatively to regression-kriging and traditional regression randomforest.
1. IntroductionA common problem in geosciences is predicting over the entire studyregion a physical quantity of interest measured at a few sampled loca-tions within the study region. The spatial prediction is used for impactfuldecision-making in many geoscience ﬁelds, including geology, geophysics, and geochemistry. There is an increasing interest in usingregression random forest for spatial prediction in various geoscienceﬁelds, when auxiliary information is available everywhere within thestudy region. Regression random forest is a machine learning ensemblemethod based on a collection of randomized regression trees, which arecombined through averaging (Breiman, 2001). Its popularity for spatial prediction relies on its ability to efﬁciently deal with many predictor variables, handle complex non-linear relationships and interactions, andrequire less data pre-processing. Regression random forest has provenrelevant for spatial prediction in several research works, includingFouedjio and Klump (2019),Szatm/C19ari and P/C19asztor (2019),Veronesi andSchillaci (2019),Hengl et al. (2018),Vaysse and Lagacherie (2017), Vermeulen and Niekerk (2017),Barzegar et al. (2017),Kirkwood et al. (2016a),Taghizadeh-Mehrjardi et al. (2016),Ballabio et al. (2016),Khan et al. (2016),Wilford et al. (2016),Hengl et al. (2015),Appelhans et al. (2015),Li (2013), andLi et al. (2011).Like other popular machine learning techniques for spatial predic-tion, regression random forest does not perfectly match the responsevariable’s observed values at sampled locations during the training stage.This characteristic is often undesirable in many geoscience applications,including mining and petroleum exploration, where it is of interest formodeling ore bodies and petroleum reservoirs. Competitor methods likeregression-kriging perfectlyﬁt the response variable’s observed values at sampled locations (Hengl et al., 2004;Chiles and Delﬁner, 2012). In this article, a new methodology ensuring the exact conditioning of regressionrandom forest is presented. The proposed method achieves the exactconditioning through a step-by-step approach. First, traditional regres-sion random forest is performed on data as usual, and an ensemble of
E-mail address:ffouedjio@anglogoldashanti.com.
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.01.001Received 18 November 2020; Received in revised form 7 January 2021; Accepted 9 January 2021Available online 13 January 20212666-5441/©2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23regression tree predictors is produced. Second, the principal componentanalysis (PCA) is carried out on the ensemble of regression tree pre-dictors. Third, the exact conditioning problem is reformulated as aBayes-linear-Gauss problem on principal component scores. This prob-lem has an analytical solution making it easy to carry out Monte Carlosampling of new principal component scores and then reconstructregression tree predictors that perfectly ﬁt the response variable’s measured values at sampled locations. A synthetic dataset where theground-truth is available within the region under study and a real datasetTable 1Synthetic data example - simulation parameters.
Mean Covariance functionType Scale SillX
1ð/C1 Þ5 Cubic 40 5X
2ð/C1 Þ5 Spherical 40 5X
3ð/C1 Þ5 Cardinal Sine 2 5X
4ð/C1 Þ5 K-Bessel (shape ¼1) 11 5
εð/C1 Þ0 Exponential 6.5 175
Fig. 1.Synthetic data example - (a), (b), (c), (d) explanatory variables, and (e) response variable.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
12Fig. 2.Synthetic data example - training dataset of n¼1000 observations.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
13including southwest England’s geochemical concentration data are usedto illustrate the proposed approach’s ability to perfectlyﬁt the response variable’s observed values at sampled locations.The remainder of the article is organized as follows. Section 2de- scribes the different ingredients required to apply the proposedapproach. Section3illustrates the proposed method’s effectiveness on a synthetic dataset as well as a real dataset. A comparison with theregression-kriging and the traditional regression random forest is carriedout. Section4offers concluding remarks.2. MethodologyLetfYðsÞ:s2D⊂R
dgbe a real-valued response variable deﬁned on a spatial domain of interestD⊂ℝ
d. Letfx1ðsÞ;…;xpðsÞ:s2D⊂ℝdgto be the set of predictor variables exhaustively known in the spatial domain D. The following terms are also used to refer to the response variable: targetvariable, dependent variable, outcome variable, explained variable. Apredictor variable is usually also referred as explanatory variable or in-dependent variable or covariate. Let fYðs
1Þ;…;Yðs nÞgbe the response variable’s measured values at sampled locations fs
1;…;s ng⊂D. The goal is to use the regression random forest for predicting the response variableover the spatial domainDrepresented by a number of grid locations N such that the response variable’s predicted values are the same as theresponse variable’s measured values at sampled locations, i.e.,bYðs
iÞ¼ Yðs
iÞ;i¼1;…;n. Different ingredients required to implement the pro-posed method are described in this section. The implementation is car-ried out in the R platform (R Core Team, 2020).2.1. Regression random forestRegression random forest is an ensemble learning approach thatcreates many regression tree models built from bootstrap samples of thetraining data (Breiman, 2001). It also injects some randomness into thetree-growing process by randomly selecting only a subset of predictorvariables to consider for split-point selection at each node. This operationreduces the chance of the same strong predictor variables to be selectedwhen a split is to be carried out, thus avoiding regression trees frombecoming overly correlated. The multiple regression tree predictors areknitted together to reduce the prediction variance and increase predic-tion accuracy. The method predicts the value that is the mean predictionof all individual regression tree predictors.Like most machine learning techniques, regression random forest hassome free parameters that can be optimized. There are among others, thenumber of trees, number of predictor variables randomly selected at eachnode, proportion of observations to sample in each regression tree, andminimum number of observations in a regression tree ’s terminal node. These free parameters are optimized through cross-validation. In prac-tice, there is no need to tune the number of decision trees; it is usuallyrecommend to set it to a large number, allowing the convergence of theprediction error to a stable minimum ( Hengl et al., 2018). The imple- mentation of the regression random forest is done using the R packagesranger(Wright and Ziegler, 2017) andtuneRanger(Probst et al., 2018). Letf~Y bðsÞ:s2Dgb¼1;…;B be the ensemble of regression tree pre-dictors resulting from the training of the traditional regression randomforest. At this stage, individual regression tree predictors and theiraverage do not necessarily match the response variable ’s measured
Fig. 3.Synthetic data example -B¼10000 unconditionalﬁrst PC scores andT¼1000 conditionalﬁrst PC scores.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
14values at sampled locations. The next steps consist of using this ensembleof regression tree predictorsf~Y
bðsÞ:s2Dgb¼1;…;B to reconstruct indi- vidual regression tree predictors that perfectly match the response vari-able’s observed values at sampled locations. By construction, theiraverage also exactlyﬁts the response variable’s observed values at sampled locations.2.2. Principal component analysisAfter training of the traditional regression random forest model, thenext step consists of performing principal component analysis on theensemble of regression tree predictors f~Y
bðsÞ:s2Dgb¼1;…;B arranged as a matrix of sizeðB/C2NÞwhose each row vector represents a singleregression tree predictorf~Y
bðsÞ:s2Dg. This results in the following decomposition inﬁnite dimensions:~Y
bðsÞ¼XLl¼1αb;lψlðsÞ;8s2D;b¼1;…;B;(1)wheref
αb;lgl¼1;…;L are principal component scores (or coefﬁcients) and f
ψlðsÞ:s2Dgl¼1;…;L are principal components factors (or eigenfunc-tions);L¼minðB;NÞ.f~Y
bðsÞ:s2Dgcan be interpreted as an image andf~Y
bðsÞ:s2Dgb¼1;…;B as an ensemble of images as well. Thus, Eq. (1)provides a decomposition of an ensemble of images into a set of eigen-imagesfψlðsÞ:s2Dgl¼1;…;L and a set of coefﬁcientsf αb;lgl¼1;…;L .I ti s essential to highlight that the eigenfunctions are considered ﬁxed in the PCA, while the coefﬁcients are considered random. It is also important toemphasize here that the PCA is used more as an orthogonal decomposi-tion method than a dimension reduction technique since all the eigen-functions are kept, as shown in Eq.(1). The bijective nature of PCA allowsthe reconstruction of regression tree predictors from coef ﬁcients. In other words, an image can be reconstructed back once all the principalcomponent factors and scores are used.2.3. Bayes-linear-Gauss problemGiven the PCA decomposition of the ensemble of regression treepredictors as shown the previous section, the next step consists of ﬁnding new principal component scores such that the reconstructed regressiontree predictors perfectly match the response variable ’s observed values at sampled locations. LetY
^ðsÞ¼XLl¼1βlψlðsÞ;8s2D (2)wherefβ
lgl¼1;…;L are random coefﬁcients;f ψlðsÞ:s2Dgl¼1;…;L are eigenfunctions deﬁned in Eq.(1). It is important to note that all theeigenfunctions are considered; so there is no truncation.
Fig. 4.Synthetic data example - response’s observed values vs response’s predicted values in the training dataset, for (a) regression-kriging, (b) traditional regression random forest, and (c) proposed regression random forest.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
15The objective here is to sample the vector of coef ﬁcientsβ¼ ðβ
1;…;βLÞ0such thatY^ðsÞ¼PLl¼1βlψlðsÞperfectlyﬁts the response var- iable’s observed values at sampled locations, i.e., Y
^ðsiÞ¼Yðs iÞ;i¼1;…; n. This exact conditioning can be rewritten as follows:Y¼Fβ (3)whereY¼ðYðs
1Þ;…;Yðs nÞÞ0is the response variable’s measured values at sampled locations andF¼½
ψlðsiÞ/C138is aﬁxed matrix of sizeðn/C2LÞwhose elements are eigenfunctions at sampled locations.To explore in a stochastic and convenient way the solution spaceassociated with Eq.(3), the vector of coefﬁcientsβ¼ðβ
1;…;βLÞ0is assumed to follow a multivariate Gaussian distribution de ﬁned by:βefðβÞ¼ð2
πÞ/C0L=2/C12/C12/C12/C12Σj
/C01=2exp/C18/C012ðβ/C0μÞ0Σ/C01ðβ/C0μÞ/C19(4)where the mean
μ¼E½β/C138and the covariance matrixΣ¼Var½β/C138are computed using PC scoresf
αb;lgderived from the PCA of regression treepredictors given in Eq.(1). Speciﬁcally,
μ¼"1BXBb¼1αb;l#
l¼1;…;L;Σ¼1B/C01XBb¼1ðαb/C0μÞðαb/C0μÞ0;with αb¼½αb;l/C138l¼1;…;L:(5)Equations(3) and (4)deﬁne a Bayes-linear-Gauss problem (Scheidt et al., 2018;Tarantola, 2013). This problem has a solution subject thatL¼minðB;NÞ>n. In practice, the number of grid locations is larger thanthe number of sampled locations ( N≫n). So there is a solution when B>n. It is important to highlight that given the number of sampled lo-cationsn, one can always haveB>nsinceBis free parameter in the regression random forest. LetfðβjYÞbe the probability density distribu-tion ofβjY. According to Bayes rule,fðβjYÞis also multivariate Gaussian with mean and covariance given by ( Scheidt et al., 2018;Tarantola, 2013):E½βjY/C138¼
μþΣF0ðFΣF0Þ/C01ðY/C0F μÞ(6)Var½βjY/C138¼Σ/C0ΣF
0ðFΣF0Þ/C01FΣ (7) SincefðβjYÞis a multivariate Gaussian distribution, it easy to drawMonte Carlo samples from this distribution. The generation of the sam-plesfβ
ctgt¼1;…;TefðβjYÞis carried out using the R package rockchalk (Johnson, 2019). Given the Monte Carlo samplesfβ
ctgt¼1;…;T , the recon- structed regression tree predictors are given by:
Fig. 5.Synthetic data example - spatial prediction map for (a) regression-kriging, (b) traditional regression random forest, (c) proposed regression ran dom forest, and (d) ground-truth.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
16Y⌣tðsÞ¼XLl¼1βct;lψlðsÞ;8s2D; withβ
ct¼/C16βct;1;…;βct;L/C170
;t¼1;…;T:(8) The prediction at an unsampled location s
02Dis made by averaging the predictions from all the individual reconstructed regression treepredictors:bYðs
0Þ¼1TXTt¼1Y^tðs0Þ (9)Since all the individual reconstructed regression tree predictorsfY
^tðsÞ:s2Dgt¼1;…;T perfectly match the response variable’s observed values at sampled locations, their mean fbYðsÞ:s2Dgdoes also. PC scoresf
αbgb¼1;…;B in Eq.(1)are called unconditional PC scores whilethosefβ
ctgt¼1;…;T in Eq.(8)are called conditional PC scores. It is importantto highlight that there no constraint on Trelatively toB;Tcan be less orgreater thanB.3. Practical examplesThe proposed method’s ability to perfectly match the response vari-able’s observed values at sampled locations is illustrated using bothsynthetic and real datasets. Prediction performance comparison is carriedout with the regression-kriging and the traditional regression randomforest using some well-known prediction accuracy criteria ( Hengl et al., 2018): the mean absolute error (MAE), the root mean square error(RMSE), the coefﬁcient of determination (R-square), and Lin ’s concor- dance correlation coefﬁcient (CCC). The lower are MAE and RMSE, thebetter is the model. The closer are R-square and CCC to 1, the better is themodel.
Fig. 6.Synthetic data example - response variable ’s observed values vs response variable ’s predicted values in testing dataset, for (a) regression-kriging, (b) traditional regression random forest, and (c) proposed regression random forest.
Table 2Synthetic data example - predictive performance statistics in the testing dataset.
Criteria Regression-Kriging Traditional Random Forest Proposed Random ForestMAE 12.69 12.01 11.05RMSE 16.86 15.66 14.40R-square 0.88 0.90 0.91CCC 0.93 0.94 0.95F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
17Fig. 7.Real data example - some predictor variables: (a) elevation, (b) landsat 8 band 6, (c) gravity survey high-pass ﬁltered Bouguer anomaly, (d) potassium counts from gamma ray spectrometry.
Fig. 8.Real data example - (a) response variable (Ga concentration), and (b) training and testing locations.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
18Fig. 9.Real data example -B¼10000 unconditionalﬁrst PC scores andT¼1000 conditionalﬁrst PC scores.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
193.1. Synthetic data exampleThe data-generating process is given by the following underlyingmodel:YðsÞ¼X
1ðsÞ2þ40tanhðX 1ðsÞX 2ðsÞÞþX 3ðsÞ2þ50sinðX 4ðsÞÞþ εðsÞ; 8s2½0;100/C138
2(10)whereYð/C1 Þis the response variable;X
1ð/C1 Þ,X 2ð/C1 Þ,X 3ð/C1 Þ, andX 4ð/C1 Þare predictor variables; and
εð/C1 Þis a latent (non-observed) variable.Predictor variables are simulated on the spatial domain ½0;100/C138
2as Gaussian isotropic stationary randomﬁelds with mean and covariance function given inTable 1. For background on Gaussian randomﬁelds, see Chiles and Delﬁner (2012). The simulation is performing using the RpackageRGeostatspackage (Renard et al., 2020). This simulated data example for which the ground-truth is available everywhere within thestudy domain refers to a situation where there is a non-linear relationshipbetween the response variable and predictor variables with some in-teractions between predictor variables. Also, the response variable showssome spatial auto-correlation and its distribution is not Gaussian.Fig. 1presents the synthetic data over a 100 /C2100 regular grid.n¼ 1000 observations are sampled randomly and taken as the training dataas shown inFig. (2). The rest of data (9000 observations) is kept aside forthe testing. The regression random forest is performed on the trainingdata with a large number of regression trees set to B¼10000. Thus, an ensemble ofB¼10000 regression tree predictorsf~YbðsÞ:s2½0;100/C1382gb¼1;…;10000 is constructed. According to the meth-odology described in Sect. 2, principal component analysis is performedon this ensemble, followed by the Monte Carlo sampling of new PC scoresensuring the exact conditioning.T¼1000 new PC scores are generated,thus giving an ensemble ofT¼1000 reconstructed (new) regression treepredictorsnY
⌣tðsÞ:s2½0;100/C1382o
t¼1;…;1000that perfectly match the response variable’s observed values at sampled locations. The meanprediction of reconstructed (new) regression tree predictors also exactlymatches the response variable’s observed values at sampled locations.PC scores before the conditioning (unconditional PC scoresf
αbgb¼1;…;B ) and after the conditioning (conditional PC scoresfβ
ctgt¼1;…;T ) are presented inFig. 3. In thisﬁgure, the points cloud of conditional PC scores is less scattered than those from unconditional PCscores due effectively to the conditioning. Indeed, unconditional PCscores have no constraints on their possible values. They can be any pointin the whole Euclidean spaceℝ
L. However, the set of linear constraintsdeﬁned by Eq.(3)produces a convex feasible region of possible values forconditional PC scores. So, conditional PC scores can not be any point inthe Euclidean spaceℝ
L.Fig. 4shows the response variable’s observed values versus predicted values in the training data, under the regression-kriging, the traditionalregression random forest, and the proposed one. One can effectivelynotice that the traditional regression random forest does not ﬁt the training data perfectly while the regression-kriging and the proposedregression random forest do.Fig. 5presents spatial prediction maps
Fig. 10.Real data example - response’s observed values vs response’s predicted values in the training dataset, for (a) regression-kigring, (b) traditional regression random forest, and (c) proposed regression random forest.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
20provided by the regression-kriging, the traditional regression randomforest, and the proposed one. The spatial prediction map resulting fromthe regression-kriging differs from the ones provided by the traditionalregression random forest and the proposed regression random forest. Inparticular, the spatial prediction map of regression-kriging is smootherthan the two others.The overall look of spatial prediction maps resulting from the tradi-tional regression random forest and the proposed regression randomforest looks very similar. However, there are some local differences dueto the exact conditioning of the proposed regression random forest. Thepredictive performance in the testing set for the regression-kriging, thetraditional regression random forest, and the proposed one is shown inFig. 6andTable 2. One can notice that the proposed regression randomforest performs better than the regression-kriging and the traditionalregression random forest.3.2. Real data exampleThe real dataset of interest comprises geochemical concentration dataof the southwest England (Kirkwood et al., 2016b). We are interested in the target variable Ga (Gallium) concentration which is measured at 568locations over the spatial domain of interest. Predictor variables includeelevation, gravity, magnetic, Landsat, radiometric, and their derivatives,totaling 26 predictor variables. Some predictor variables are shown inFig. 7.Fig. 8a presents measurements of the response variable. The dataare divided into a training set (/C2580%) and testing set (/C2520%) as showninFig. 8b.The estimated regression random forest model is an ensemble of B¼ 10000 regression tree predictors. PCA applied to this ensemble,following by the Monte Carlo sampling result to unconditional andconditional PC scores as shown inFig. 9;T¼1000 conditional PC scores are generated.Fig. 10shows the conditioning performance in thetraining data of the regression-kriging, the traditional regression randomforest, and the proposed regression random forest. As noticed in thesimulated data example, the regression-kriging and proposed regressionrandom forest perfectlyﬁt the data while the traditional regressionrandom forest does not.Spatial prediction maps provided by the regression-kriging, thetraditional regression random forest, and the proposed one are depictedinFig. 11. The spatial prediction map of the regression-kriging differsfrom the two others. The general appearance of the spatial predictionmaps of these later looks very similar. However, one notes some localdifferences due to the exact conditioning of the proposed regressionrandom forest.Fig. 12andTable 3present the predictive performance ofthe regression-kriging, the traditional regression random forest, and theproposed regression random forest on the testing data. The proposedregression random forest shows better predictive performance than thetwo other methods. Thus, the proposed approach can exactly match theresponse variable’s measured values at sampled locations whileachieving good out of sample predictive performance.
Fig. 11.Real data example - spatial prediction map for (a) regresssion-kriging, (b) traditional regression random forest, and (c) proposed regression rand om forest.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
214. Concluding remarksThis article presented a methodology guaranteeing that regressionrandom forest perfectly matches the response variable ’s observed values at sampled locations like competitor techniques such as regression-kriging. Given the ensemble of regression tree predictors resulting fromthe traditional regression random forest, the exact conditioning is ach-ieved by combining principal component analysis and Monte Carlosampling. As a result, a new ensemble of regression tree predictors thatperfectlyﬁt the response variable’s observed values at sampled locationsis obtained. The average of this new ensemble of regression tree pre-dictors also exactly matches the response variable ’s observed values at sampled locations by construction. The effectiveness of the proposedapproach has been demonstrated on both synthetic and real datasets. Itcan perfectlyﬁt the response variable’s measured values at sampled lo- cations while achieving good out of sample performance comparativelyto regression-kriging and traditional regression random forest. It is easyto implement since it combines well-known existing machine learningand Monte Carlo sampling methods.As highlighted previously, the exact conditioning is performed sub-ject that the number of regression trees is greater than the number ofsampled locations. Nonetheless, it will always be possible to meet thisconstraint because the number of regression trees is a free parameter. Thenumber of regression trees should also be large enough to allow goodcoverage of the solution space when performing the exact conditioning.The proposed approach relies on the regression random forest to performthe exact conditioning. Thus, one expects the proposed method to pro-vide better predictive performance in situations favorable to regressionrandom forest like a non-linear relationship between the response vari-able and predictor variables and some interactions between predictorvariables. In a situation where there is linear relationship between thedependent variable and predictor variables, competitor techniques likeregression-kriging could perform better. The proposed method can beused in any spatial dimension (e.g., 2D and 3D). It can be extended in thecase where the response variable is categorical. The exact conditioningcan be achieved following the idea developed in Fouedjio et al. (2020).
Fig. 12.Real data example - response variable ’s observed values vs response variable ’s predicted values in testing dataset, for (a) regression-kriging, (b) traditional regression random forest, and (c) proposed regression random forest.
Table 3Real data example - predictive performance statistics on the testing dataset.
Criteria Regression-Kriging Traditional Random Forest Proposed Random ForestMAE 2.87 2.74 2.63RMSE 3.79 3.59 3.46R-square 0.57 0.61 0.64CCC 0.74 0.75 0.78F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
22Declaration of competing interestThere is no conﬂict of interest.AcknowledgmentsThe authors is grateful to the anonymous reviewers and the editor fortheir helpful and constructive comments that greatly helped improve themanuscript.References
Appelhans, T., Mwangomo, E., Hardy, D.R., Hemp, A., Nauss, T., 2015. Evaluatingmachine learning approaches for the interpolation of monthly air temperature at mt.Kilimanjaro, Tanzania. Spatial Statistics 14, 91 –113. Ballabio, C., Panagos, P., Monatanarella, L., 2016. Mapping topsoil physical properties atEuropean scale using the Lucas database. Geoderma 261, 110 –123. Barzegar, R., Asghari Moghaddam, A., Adamowski, J., Fijani, E., 2017. Comparison ofmachine learning models for predicting ﬂuoride contamination in groundwater. Stochastic Environmental Research and Risk Assessment 31 (10), 2705 –2718. Breiman, L., 2001. Random forests. Mach. Learn. 45, 5 –32. Chiles, J.P., Delﬁner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley & Sons.Fouedjio, F., Klump, J., 2019. Exploring prediction uncertainty of spatial data ingeostatistical and machine learning approaches. Environmental Earth Sciences 78(1), 38.Fouedjio, F., Scheidt, C., Yang, L., Jef, C., 2020. Conditional simulation of categoricalspatial variables using Gibbs sampling of a truncated multivariate normal distributionsubject to linear inequality constraints. Stoch. Environ. Res. Risk Assess. https:// doi.org/10.1007/s00477-020-01925-7 . Hengl, T., Heuvelink, G.B., Stein, A., 2004. A generic framework for spatial prediction ofsoil variables based on regression-kriging. Geoderma 120, 75 –93. Hengl, T., Heuvelink, G.B.M., Kempen, B., Leenaars, J.G.B., Walsh, M.G., Shepherd, K.D.,Sila, A., MacMillan, R.A., Mendes de Jesus, J., Tamene, L., Tondoh, J.E., 2015.Mapping soil properties of Africa at 250 m resolution: random forests signi ﬁcantly improve current predictions. PloS One 10, 1 –26. Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., Gr €aler, B., 2018. Random forest as a generic framework for predictive modeling of spatial and spatio-temporal variables.PeerJ 6:e5518.https://doi.org/10.7717/peerj.5518 . Johnson, P.E., 2019. Regression estimation and presentation. URL. rockchalk. htt ps://CRAN.R-project.org/package ¼rockchalk. r package version 1.8.144.Khan, S.Z., Suman, S., Pavani, M., Das, S.K., 2016. Prediction of the residual strength ofclay using functional networks. Geoscience Frontiers 7, 67 –74. Kirkwood, C., Cave, M., Beamish, D., Grebby, S., Ferreira, A., 2016a. A machine learningapproach to geochemical mapping. J. Geochem. Explor. 167, 49 –61. Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016b. Stream sediment geochemistry asa tool for enhancing geological understanding: an overview of new data from southwest England. J. Geochem. Explor. 163, 28 –40. Li, J., 2013. Predictive Modelling Using Random Forest and its Hybrid Methods withGeostatistical Techniques in Marine Environmental Geosciences, in: 11-thAustralasian Data Mining Conference (AusDM1 ́3). Australia, Canberra, pp. 73–79. Li, J., Heap, A.D., Potter, A., Daniell, J.J., 2011. Application of machine learning methodsto spatial interpolation of environmental variables. Environ. Model. Software 26,1647–1659.Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and Tuning Strategies forRandom Forest. Wiley Interdisciplinary Reviews: Data Mining and KnowledgeDiscovery doi:10.1002/widm.1301 . R Core Team, 2020. R: a language and environment for statistical computing. R Foundationfor Statistical Computing. Vienna, Austria. https://www.R-project.org/
. (Accessed 15 November 2020).Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2020. Geostatisticalpackage. URL. RGeostats.http://cg.ensmp.fr/rgeostats. r package version 12.0.1. Scheidt, C., Li, L., Caers, J., 2018. Quantifying Uncertainty in Subsurface Systems.Geophysical Monograph Series. Wiley . Szatm/C19ari, G., P/C19asztor, L., 2019. Comparison of various uncertainty modelling approachesbased on geostatistics and machine learning algorithms. Geoderma 337, 1329 –1340. Taghizadeh-Mehrjardi, R., Nabiollahi, K., Kerry, R., 2016. Digital mapping of soil organiccarbon at multiple depths using different data mining techniques in Baneh region,Iran. Geoderma 266, 98–110. Tarantola, A., 2013. Inverse Problem Theory: Methods for Data Fitting and ModelParameter Estimation. Elsevier Science . Vaysse, K., Lagacherie, P., 2017. Using quantile regression forest to estimate uncertaintyof digital soil mapping products. Geoderma 291, 55 –64. Vermeulen, D., Niekerk, A.V., 2017. Machine learning performance for predicting soilsalinity using different combinations of geomorphometric covariates. Geoderma 299,1–12.Veronesi, F., Schillaci, C., 2019. Comparison between geostatistical and machine learningmodels as predictors of topsoil organic carbon with a focus on local uncertaintyestimation. Ecol. Indicat. 101, 1032 –1044. Wilford, J., de Caritat, P., Bui, E., 2016. Predictive geochemical mapping usingenvironmental correlation. Appl. Geochem. 66, 275 –288. Wright, M.N., Ziegler, A., 2017. ranger: a fast implementation of random forests for highdimensional data in Cþþand R. J. Stat. Software 77, 1–17.F. Fouedjio Artiﬁcial Intelligence in Geosciences 1 (2020) 11 –23
23