A survey on vulnerability assessment tools and databases for cloud-basedweb applications
Kyriakos Kritikos*, Kostas Magoutis, Manos Papoutsakis, Sotiris Ioannidis
ICS-FORTH, Heraklion, Crete, Greece
ARTICLE INFO
Keywords:VulnerabilityScanningAssessmentSurveyDatabaseToolsEvaluationAccuracyPerformanceABSTRACT
Due to its various offered beneﬁts, an ever increasing number of applications are migrated to the cloud. However,such a migration should be carefully performed due to the cloud ’s public nature. Further, due to the agile development cycle that applications follow, their security level might not be the best possible, exhibiting varioussorts of vulnerability. As such, to better support application migration and runtime provisioning, this articlesupplies three main contributions. First, it attempts to connect vulnerability management to the applicationlifecycle so as to highlight the exact moments where application vulnerability assessment must be performed.Second, it analyses the state-of-the-art open-source tools and databases so as to enable developers to make aninformed decision about which ones to select. In this sense, discovering such vulnerabilities will enable to bettersecure applications before or after migrating them to the cloud. The analysis conducted is quite rich, coveringvarious aspects and a rich sets of criteria. Third, it explores the claim that vulnerability scanning tools need to beorchestrated to reach the highest possible vulnerability coverage, both in terms of extend and breadth. Finally,this article concludes with some challenges that current vulnerability tools and databases need to face to increasetheir added-value and applicability level.
1. IntroductionCloud computing is a novel computing paradigm that has revolu-tionized the way applications are designed and provisioned. It promotesthe use of computational and networked resources on demand to supportapplication provisioning. As such, resource management is outsourced tocloud providers enabling to reduce or diminish operational and man-agement costs.The cloud computing success has also led to raising the abstractionlevel via the supply of extra service types above the infrastructural ones.In particular, platform services are offered to reduce the burden increating and managing the execution environments within the leashedresources. Further, the scope of such services has been also extended tofacilitate cloud application design and development.The result from the use of both infrastructure as a service (IaaS) andplatform as a service (PaaS) services was that existing applications weremigrated to the cloud while new ones popped up, exhibiting the bene ﬁt that they can inﬁnitely scale to handle the ever-increasing workloads.The plethora of services at the software level (SaaS) also enablescomposing such services to create added-value, advanced functionality.As such, nowadays software houses can acquire a plethora of tools andservices to rapidly build their applications and offer them in an ever-competing market.While migration of applications to the cloud continues with a fastpace, there are two prohibitive factors that make organisations hesitantto perform it. Theﬁrst factor is the lack of standardisation, as each cloudprovider offers its own APIs and tools to support application developmentand provisioning. Thus, adopting one cloud provider leads to a lock-ineffect as the application owner becomes bound to that provider andcannot easily migrate to a new one, when, e.g., new business opportu-nities arise. Fortunately, various frameworks have been proposed toconfront this lock-in issue, which enable applications to move from onecloud to another, thus becoming multi-cloud. Avoiding this lock-in effectintroduces greaterﬂexibility as there is plenty of space to optimise ap-plications by exploiting in a combined way those cloud services that moreoptimally satisfy the application requirements.The second migration factor is security. By giving more control onthird-parties, which might not offer services with suitable security gua-rantees, makes organisations to be reluctant to move to the cloud. Manyapplications already handle private or sensitive data which will never bemoved to the cloud as long as these guarantees are not satis ﬁed. Further, privacy laws might also forbid data movement from certain areas. Yet, a
* Corresponding author.E-mail address:kritikos@ics.forth.gr(K. Kritikos).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2019.100011Received 27 June 2019; Received in revised form 3 September 2019; Accepted 11 October 2019Available online 15 October 20192590-0056/©2019 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 3-4 (2019) 100011cloud provider might not possess datacentres in these areas or guaranteethat data might not be stored outside of them.Virtualisation also impacts security as physical hosts are shared byusers in the form of virtual machines (VMs). This can raise issues wheremalicious users can see the data stored in a VM or take control over allVMs by attacking the hypervisor. Further, as applications are distributedover public clouds, networked attacks can be performed over them whichcan, e.g., cause crucial information to be leaked during transmission orapplication overload based on denial-of-service attacks.While all these are valid security issues, research has led to particularachievements in the cloud security area such that we could even reasonthat public clouds might be even more secure than private ones. A newbreed of application development methodologies has been also recentlyworked out, enabling to secure an application by design, enabling toadopt security services that best match the designed application ’s nature and structure.Security services can be of a different nature, while they be alsoreactive or preventive. Reactive services can detect an attack and possiblyaddress it. For example, intrusion detection and protection systems canbe used to protect applications from network attacks by, e.g., detectingdenial-of-service attacks and blocking their origin IPs. Preventive ser-vices prevent a security incident from happening by detecting applicationareas that are vulnerable to certain attacks. The latter service kind cantake different forms.Vulnerability scanning tools enable to detect vulnerabilities indifferent application parts and kinds. Static (code) analysis tools can ﬁnd code bugs exploitable by security attackers. Audit tools can be utilised toﬁnd well-known rootkits, Trojans and backdoors as well as to unveilhidden processes and sockets. Finally, antivirus tools can detect viruseswhich either attempt to infect or have already infected the underlyingoperating system (OS).This article focuses on vulnerability scanning tools and databases. Thereason for this is that, while malware and antivirus software is well-known and adopted by both organisations and individuals, vulnera-bility scanning tools are not widely used in practice. Further, it is hard fora practitioner to choose the right vulnerability scanning tool due to thegreat tool diversity and varied coverage. As such, this article aims toguide the practitioner in making an informed decision about whichvulnerability scanning tool and database to select for his/her application.Such a selection relates also to particular challenges which have not beencurrently confronted in research, thus supplying an invaluable contri-bution to the academic community. In addition, this article contributestowards the identiﬁcation of those places in the application lifecyclewhere vulnerability scanning can be performed and supplies valuableinsights towards better securing cloud applications. Finally, this articleinvestigates and proves that vulnerability scanning tool orchestration canbe the answer towards higher vulnerability detection coverage.The rest of the article is structured as follows. The next sectionformally deﬁnes what is a vulnerability scanning tool while explicates itsarchitecture and the vulnerability management lifecycle. Section 3at- tempts to set up and explain the article ’s main research goals and methodology. Sections4 and 5evaluate the identiﬁed vulnerability scanning tools and databases, accordingly, based on a certain evaluationframework and analyse the main results produced. Finally, Section 6 explains the main research challenges involved in the quest for protectinga cloud application from security vulnerabilities. Finally, the last sectionconcludes the paper.2. Vulnerability management lifecycle and architectureA vulnerability scanning tool aims atﬁnding problematic parts of an application or its system that make it vulnerable to security attacks. Inthis sense, a vulnerability is a kind of malfunction, miscon ﬁguration or security gap in general that might affect the whole system hosting theapplication, including the VMs, OSs, application code or any other systemcomponent. For instance, an OS might suffer from well-known securityissues, while the application might use a third-party software sufferingfrom security bugs.Internally, a scanning tool employs a set of rules to perform thescanning which indicate how the scanning can be performed and when acertain vulnerability is raised. A scanning tool can either offer a rule setextensible by users, or it can cater for higher modularity by formulatingdifferent rule sets to match different application kinds or scanningsituations.The scanning via a tool can be performed in two different modes. Inthe internal mode, the tool is installed inside the application and attemptsto scan its components, hosted in VMs or containers. Such a mode canaddress any application component kind due to the potential use of bothstatic and dynamic scanning methods. Thus, its scanning accuracy isincreased. Further, it leads to reduced communication overhead as mostof the communication occurs within the same hosting environment.
However, it can have the greatest impact in terms of interfering with theapplication performance as it can steal precious resources from the samehosting environment. In the external mode, the scanning is performedexternally to the application and relies on the endpoints on which theapplication web-based components reside. As such, only the lattercomponent kind can be confronted. This scanning mode has the advan-tage of conducting the scanning without interfering internally with theapplication hosting environment. However, it might not be able to attainthe highest possible scanning accuracy as it cannot exploit static scanningmethods while it increases the communication overhead in the applica-tion system.Fig. 1depicts a reference architecture for a vulnerability scanningtool, showcasing both its main components and different operationmodes. The architectural variability that can be observed across allvulnerability scanning tools (in terms of two architecture classes) is alsodepicted, which is is explained in the next paragraph.In a client-server architecture, a server remotely orchestrates theexecution of different agents, either residing on the application VMs/containers to perform the scanning internally or on different VMs toconduct an external scanning. Such an architecture can more easily scalewhile it can accelerate the scanning by orchestrating the parallel agentexecution. In a standalone architecture, the scanning tool is installed inone place, either internal or external to an application, and then initiatedto perform the scanning. Such an architecture variant/class has the maindeﬁcit that the orchestration of the installation, execution and reportmerging of the respective vulnerability reports burdens the user.In any case, as also shown inFig. 1, each architecture alternative can map to a different scanning mode which means that the two main
Fig. 1.The different vulnerability scanning architecture alternatives.K. Kritikos et al. Array 3-4 (2019) 100011
2architecture classes, i.e., client-server and standalone, are orthogonal tothis mode. This is depicted through the use of different colours in therespective components involved mapping to the four possible architec-ture scenarios (associated with all possible combinations between thetwo scanning modes and architecture classes).A scanning tool might also exhibit the functionality to assess therelevant application exposure risk with respect to the kind of vulnera-bilities discovered and their criticality. Such a risk can not only indicatethe need to modify the application system but also those parts that needimmediate handling.Fig. 1depicts this tool capability with the solearchitectural difference being that in the standalone mode, the report isproduced directly while in the client-server mode, the report is compiledby the server after merging the vulnerability results produced by theagents/clients.As such, vulnerability databases (VDBs) are usually employed tocover the information to be included in a vulnerability scanning report.VDBs usually include some common attributes dedicated to properlyidentifying and explaining vulnerabilities. They might also include extrainformation, like which artefact is affected and how the vulnerabilitycould be addressed. Many VDBs also conform to standards that support akind of standardised identiﬁcation or determination of the information tobe covered.Fig. 1highlights two different connection kinds between a VDB and ascanning component. In case of a standalone scanner or server, thecontinuous lines indicate that the VDB is mainly used to produce the ﬁnal scanning report. In case of a standalone scanner, the VDB could be alsoused to deﬁne how the vulnerability can be detected. On the other hand,the dash lines between the scanning agents and VDB indicate that theVDB might not always be used by them. In some cases, two main optionalusage scenarios can hold: (a) the agent can obtain the scanning rules fromthe VDB; (b) it might use the VDB to construct its partial report to be sentto the server.Fig. 2showcases the lifecycle of vulnerability management whichinvolves the following activities:1.Scanning tool installation: in this activity a system admin also choosesthe right scanning architecture that more properly matches theapplication system architecture and the kind of application to bescanned.2.Scanning conﬁguration: the admin then attempts to conﬁgure appro- priately the scanning tool. She might use only a subset of all rules or acertain rule set in case of modular tools. She can also point the exactplaces where scanning needs to be performed.3.Scanning execution: the admin executes here vulnerability scanning.4.Vulnerability report inspection: the vulnerability report produced by thescanning tool isﬁrst inspected by the application owner to evaluatethe overall application risk and determine the strategy for reducing it.5.Vulnerability handling: this strategy is then materialised by the systemadmin who takes an actionﬁrst on the most critical vulnerabilities.Such an action either takes the form of applying recommendedremedies from the report, if they exist, on the affected applicationsystem component, orﬁrst searching toﬁnd ways to confront the vulnerability and then applying the most suitable one. The actionexecutor needs not be solely the system admin. The admin is mostlyresponsible for application deployment and provisioning, e.g., onvulnerabilities covering the OS level or the application executionenvironment in general. However, the application devop can also beinvolved, when issues regarding the application code must beconfronted.Once the handling is over, scanning can be re-executed to check thatthe vulnerabilities in focus have been corrected. Further, scanning mightalso need reconﬁguration. For instance, it might be possible that onevulnerability is too coarse-grained and a more focused scanning needs tobe performed to enable detecting more concrete vulnerabilities andsubsequently handling them. It might be also possible that the overallhandling strategy prescribes that the scanning frequency needs to bealigned with, e.g., the application modiﬁcation pace.
The above lifecycle analysis highlights that various actors or roles inthe user application management are also involved in the vulnerabilityhandling and management. Further, the application management life-cycle is connected to the vulnerability management lifecycle, a logicalconsequence of the fact that vulnerabilities are not only related to anapplication but also affect its development and production process. Fig. 3 depicts such a connection, where vulnerability management is part oftwo application lifecycle activities:/C15Testingin the sense that vulnerability scanning can be considered aspart of application security testing. Such a testing is performed oncean application is deployed and executed in an integration environ-ment such that it can be fully tested according to multiple aspectsapart from the security one, such as the (functional) integrationaspect./C15Application Monitoring&AdaptationOnce an application has been deployed and executed in a production environment, it must bemonitored and adapted according to the currentproblematic situa- tion. As such, scanning must be continuously performed as there canbe conﬁguration changes, OS updates or even security incidents thatmight have taken place such that they must be checked for vulnera-bilities to be then properly handled. In overall, an application is aliving organisation that can be affected by various changes that occurwithin its system. As such, we could consider scanning as part ofdynamic application testing at runtime, already promoted as a kind ofpreventive application monitoring and adaptation measure inliterature.In overall, we foresee two different paths via which vulnerabilitiescan be handled, connected to changes on the application based on itsmanagement lifecycle. Theﬁrst and simpler path indicates that
Fig. 2.The vulnerability management lifecycle.
 Fig. 3.The application management lifecycle.K. Kritikos et al. Array 3-4 (2019) 100011
3vulnerabilities only affect the application component code. As such, onlycertain application components will need to be modi ﬁed, thus going back to theDevelopmentlifecycle activity. Then, the physical management ﬂow will be followed leading to application redeployment and testing to checkwhether the vulnerabilities have been properly addressed withoutbreaking the application integration.The second path is followed when vulnerabilities affect multiplecomponents in the application system. In this case, we need to bothchange application components as well as the way the application ispackaged, deployed and conﬁgured. This then will affect bothDevelop- mentandDeploymentactivities. The changes can be so critical that mightaffect the application design while they can lead to producing a newapplication version. This is not so exceptional as we have seen in the pastholistic changes on applications that attempt to secure them more thanbefore.3. Survey goals&tool selection process3.1. Survey goalsAs Section1indicated, there is currently no guidance in properlyselecting the right tools to identify application vulnerabilities. Such aselection is usually performed via a manual process that can involvesearching the web toﬁnd the right tool, inspecting the tool features andpossibly getting in contact with the tool provider to ﬁnd the most suitable pricing model matching the application risk requirements.Based on this manual and time-consuming practice, this paper aims atevaluating vulnerability scanning tools against a criteria-based frame-work. Such an evaluation can unveil which tools prevail and according towhich aspects. We put special focus in the latter case on the trade-offbetween scanning accuracy and time to which we add the vulnerabilitycoverage dimension.Apart from the scanning tools, the selection of a database is crucial forobtaining the right information about a vulnerability which not onlyconforms to standards but is also frequently updated, by relying on thefact that new vulnerabilities are constantly identi ﬁed each year. Such information can then constitute the basis for formulating the most suit-able mitigation strategy.We concentrate mainly on open-source vulnerability scanning toolsand databases. Our rationale is twofold: (a) open-source model is welladopted in the market where more than 90% of software is availableeither in this or a dual open-source and proprietary form. This model iscritical for both maintaining the software ’s sustainability and extending it based on community contributions, especially for small or mediumenterprises; (b) for practical reasons, it is impossible to experimentallyevaluate a proprietary tool without purchasing it, while such an evalu-ation was required as Section5shows.In practice, each tool might come with a different performance overthe three aforementioned dimensions. It might be also specialised overdifferent application or component types. As such, it may be possible thatnot just one but multiple tools might be required to achieve a certainvulnerability detection goal. Thus, this article also aims at investigatingboth the tools differentiation and complementarity to assist in theirpossible orchestration.Based on the above analysis, the main research questions that thisarticle attempts to answer are the following:/C15Q
1: Which are the top open-source vulnerability databases and ac-cording to which aspects?/C15Q
2: Similarly, which are the top open-source vulnerability scanningtools and according to which aspects?/C15Q
3: How complementary are the open-source vulnerability scanningtools and is their orchestration meaningful?3.2. Survey methodologyTo answer the above questions, a survey over the state-of-the-art wasconducted through a methodology comprising the following steps:/C15tool search: search procedure for discovering and selecting the rightvulnerability scanning tools and databases/C15evaluation method&criteria determination: procedure for identifying the right evaluation methods and criteria/C15evaluation&reporting: evaluation, analysis&result reportingIn the following subsection, we focus more on the ﬁrst two steps. The last step is the subject of the next two sections of this article.3.2.1. Tool search&selectionA certain procedure for tool search and selection was followed,comprising two main steps: (a) actual search of the tools; (b) selection ofthe right tools from those discovered based on a set of inclusion criteria.The search relied on a two-front approach. In both fronts, we haverelied on a set of keywords deemed as most relevant for the search. Thisset was slightly differentiated depending on the concerned artefact(database or tool).For vulnerability databases, the keywords used and combined in alogical form were the following:vulnerability AND(database OR repository OR store). Similarly, for the scanning tools the keywords were thefollowing:vulnerability AND(scanning OR detection OR discovery ORsearch)AND(tool OR software OR prototype OR framework . As it can be seen, the keywords were split into two or three categoriesdepending on the artefact concerned. The ﬁrst category is related to the research subject so it maps to the sole term vulnerability. The second category concerned the characterisation of the searched artefact, i.e., theresearch object. In this case, different alternative terms were used tocapture all expression possibilities. The third category was onlyemployed in the context of tool search to explicate the artefact ’s specialisation, i.e., what it can perform in terms of the research subject. Inthis case, we focused on providing alternative terms that would explicatethe discovery of a vulnerability.Theﬁrst search front was the wide Internet. This front simulated howan application administrator would search for a vulnerability scanningtool or database. Thus, we have employed Google as the well-known andmost efﬁcient web search engine to apply the keyword search for eachartefact. This was proceeded by snowball search [ 1] by following the cross-references in a tool’s page to other related tools. While inspectingeach tool web page, we had already in mind recording information forthose criteria that will regulate the tool/database selection.The second search front was formulated to cover prototype tools ordatabases from academia that cannot be found easily from a web search.Such tools could be increasingly improved and extended due to therelative research and development tasks conducted by the research or-ganisations producing them. To discover such tools, we employed wellknown academic data sources, i.e., Web of Science and Scopus, enablingus to have a full literature coverage.Once all tools and databases were discovered, they were ﬁltered by employing a set of both inclusion and exclusion criteria. Such criteriakinds were split based on the two followed fronts due to the nature oftheirﬁndings.The inclusion criteria from the web front were the following:/C15A tool should perform at least vulnerability scanning/C15A database should cover at least the basic description ofvulnerabilities/C15The artefact (tool/database) should be open-source/C15The artefact has some sort of community behind, even if it is quitesmall. Thus, we do not favour tools developed by individuals as suchtools will tend to have limited capabilities and coverage.K. Kritikos et al. Array 3-4 (2019) 100011
4/C15The artefact last development must have been performed in last 8years.While theﬁrst four criteria cover quite logically the aforementionedresearch scope, the last criterion guarantees that only a non-outdated toolis selected by considering the rapid development and changes in thevulnerability area.On the other hand, the academic front inclusion criteria were asfollows:/C15Only peer-reviewed articles must be considered/C15The article should have been published since 8 years from now./C15The article should deal with the proposal of a vulnerability databaseor scanning tool/C15Quality assessment: we have maintained only articles that were easyto understand and had a suitable quality and contribution level.The exclusion criteria for the web front were the following:/C15The web page is not in English/C15Artefacts with suspicious web pages/C15Artefacts having web pages with no documentation/C15Artefacts for which the source code is not available or does notcompile any more or has been tamperedFinally, the exclusion criteria for the academic front were thefollowing:/C15Inaccessible articles: An email was sent to their authors to obtain aprivate copy of them. In case of no answer, the articles were rejected./C15Articles written in a different language than English/C15Very short articles (e.g., posters) with a limited contribution3.2.2. Evaluation criteria&method determinationFor the survey on vulnerability databases, we focused on a compar-ative evaluation based on the very nature of research question Q
1. Thus, we created a set of evaluation criteria which were either devised by us ordrawn from the literature. All criteria were produced by considering twoaspects:information coverage,capabilities&support. More details are supplied in Section4.To cover research questionQ
2, a similar approach was followed forthe vulnerability scanning tools, resulting in producing another set ofevaluation criteria. However, it was impossible to evaluate the selectedtools with respect to criteria like vulnerability coverage, scanning timeand accuracy based only on their documentation. To this end, it wasdecided to select a benchmark-based evaluation approach for thesecriteria, concentrating on a small subset of tools, with the rationale that:(a) we just need to prove the need for the tools ’orchestration according to research questionQ3as well as their complementarity; (b) it wasimpossible to evaluate all tools according to the benchmark as this wouldrequire a huge effort, involving: reading the technical documentation ofthe tools, downloading and installing them, running them over thebenchmark, and translating their results to a form processable by thebenchmark to evaluate the respective criteria.To summarise, two comparative evaluations were performed for theﬁrst two research questions plus a benchmark-based evaluation for thelast, thus adopting one evaluation method for each of the researchquestions posed.4. Vulnerability database analysis4.1. Evaluation criteriaAs the previous section indicated, we focused on devising a set ofcriteria concerning the aspects ofcapabilities&support andinformation coverage.Information coveragecriteria attempt to assess whether all necessaryinformation aspects are covered for a vulnerability. They include thefollowing:/C15scope–what kinds of vulnerabilities are covered by a VDB/C15impact&risk–what is the impact and risk of the vulnerability/C15resolution–how this vulnerability can be resolved/C15vendor–what is the vendor with products affected by thevulnerability/C15products–what are the main products affected by the vulnerability/C15exploit–information related to how the vulnerability can be exploited/C15categorisation–ability to provide an hierarchical vulnerability cate-gorisation to support a more user-intuitive browsing./C15relations–relationships with other vulnerabilities; they could be usedto, e.g., understand how vulnerabilities can be chained such that theycan be more effectively addressed by the respective defence system.The second aspect concerns what are the main VDB capabilities interms of interfacing, standards support, information freshness plus usersupport, which comes via not only VDB documentation but also an activecommunity, able to answer any enquiry related to the VDB and its usage.As such, the following criteria have been considered:/C15standards–the supported standards. The higher is the number ofstandards supported, the better is the VDB suitability./C15community–who is behind and updates the VDB. Maintaining a bigcommunity ensures VDB sustainability and extensive coverage whilecaters for high-quality entries due to cooperative curation by experts./C15interfacing–the offered VDB interfaces which should enable thesuitable search of the information provided plus information pushingmechanisms for retrieving up-to-date vulnerability information,catering for better securing a system against even the most recentvulnerabilities./C15freshness–the up-to-dateness of the supplied vulnerabilityinformation.4.2. Analysis of evaluation resultsThe evaluation results are summarised in the form of two tables,explicating how well a certain VDB satisﬁes the two considered aspects, respectively. In theﬁrst from these two tables, the following symbolshave been utilised: (a) ‘✓‘: indicates full satisfaction of a criterion; (b) ‘e‘: signiﬁes partial coverage of a criterion; (c) ‘-‘: indicates that the criterion is not supported at all.The results ofTable 1, mapping to theﬁrst criteria partition, indicate that the best approach in information coverage is Vulcan followed in 2ndplace by 2 other approaches: HPI-VDB and vFeed. Vulcan needs to betterhandle only theexploitcriterion which is partially satisﬁed. On the other hand, the other 2 approaches need to cover completely 2 criteria. Inoverall, we can see that Vulcan is very close to an ideal approach for thiscriteria category, quite normal if we consider that it is ontology-basedand thus designed to well integrate and correlate different informationpieces.To unveil some patterns from the results, we can indicate that mostapproaches focus on addressing theﬁrst 5 criteria while the rest are rarely covered. These 5 criteria can be regarded as the most essential forwell characterising vulnerabilities. The rest have an added-value focus onclarifying how a vulnerability can be exploited as well as in categorisingand relating it with other vulnerabilities. The former is an excellent in-formation source, leading to equipping a certain tool with extra vulner-ability detection capabilities. The latter criteria enable to nicelycategorise vulnerabilities for more user-intuitive browsing and explora-tion while also unveil relations between vulnerabilities, to be exploited toconduct more advanced exploit forms. This relationship knowledge canalso assist in detecting a vulnerability ’s root cause by, e.g., identifying theﬁrst vulnerability in the detected vulnerability chain.K. Kritikos et al. Array 3-4 (2019) 100011
5Most VDBs focus on any kind of vulnerability; very few have a morerestrained scope. For risk assessment, many VDBs adopt CommonVulnerability Scoring System (CVSS) while very few just provide animpact measure. Some VDBs supply both impact and risk information foreach vulnerability.Finally, only Common Weakness Enumeration (CWE) offers a verygood vulnerability categorisation in form of a deep hierarchy. Thiscontrasts the other approaches that focus mainly on supplying concretevulnerabilities at the leaf level of such hierarchy. For instance, Vuldbadvertises covering around 114198 leaf vulnerabilities. However, in ourview, all hierarchy levels are important. As such, it is imperative that aVDB integrates its content with CWE to enable producing such a richhierarchy. Such an approach is followed only by very few efforts,including Vulcan, asTable 2shows.Table 1Vulnerability databases evaluation summary according to information coverage.
DB Scope Impact &Risk Res. Vendor Products Exploit Cat. RelationsCVE [2] Any –– – – – – – Seven Pernicious SW security –– – – – ✓– Kingdoms [3] errorsOWASP Top App risks Risk ✓–– – – – Ten [4]NVD [5] Any CVSS [ 6]&version 3 metrics✓–✓–✓– CWE [7] SW security weaknesses Consequence effect incl. technical impact ✓–ee✓✓ OSVDB [8] Any – ✓–– – – – HPI-VDB [9,10] SW CVSS ✓✓ ✓ ––✓ Vulcan [11] Any CVSS ✓✓ ✓ e✓✓ Vulnerability&Exploit DB [12] Any Severity &CVSS✓–– – – – vFeed [13] Any CVSS ✓–✓✓ ✓ – Embedded Vulnerability Detector [ 14] jar&class–– ✓✓ –– – SecurityFocus Vulnerability DB [ 15]S W – ✓–✓e–– Vuldb [16] Any CVSS ✓–✓✓ –– SecurityTracker [
17] Any Impact ✓✓ ✓ –– – ZeroDayInitiative [18] Any CVSS ✓✓ ✓ –– – ExploitDatabase [19] Broad
aSeverity ––✓✓ –✓ ICS-CERT [20] Any CVSS ✓✓ ✓ –– – Japan Vulnerability Notes [21] Any CVSS ✓✓ ✓ –– – AusCERT Bulletins [22] Any Impact ✓✓ ✓ –– – CERT-EU Security Advisories [ 23] Any Impact &CVSS –✓✓ –– –
aRemote, web, application, local and privilege escalation, DDoS &PoC.
Table 2Vulnerability databases evaluation summary according to the remaining aspects.
DB Community Interfaces Freshness StandardsCVE CVE Numbering Authorities, CVE Board, CVE Sponsors, ITU-T,FIRST, NIST, DISA File, Twitter Feed, Change log,NVD, Web Search Daily NoSeven PerniciousKingdomsFortify Software Web search and hierarchical browsing Static NoOWASP Top Ten 46000þparticipants,>65 corporate organisations, manyacademic supporters github, wiki page Every 3 years NoNVD NIST web search, ﬁle, data feeds,hierarchical browsing, change log Daily CVSS, CPE [ 24],CVE, CWE CWE researchers&academic representatives, industry, USDepartment of Homeland security web search, hierarchical browsing,ﬁle Continuous updates NoOSVDB OpenSecurityFoundation, High-Tech Bridge, RiskBasedSecurityBlack Duck Software, RTS Labs, ISECOM, DVI open-source relational DB, alterting&logging Shutdown in 2016 NoHPI-VDB Hasso-Plattner Institute API, web search Daily CVE, CWE, CVSS, CPE Vulcan University of North Texas, NIST Text-based search over constructed KB via SNLP techniques On-demand viaNVD, XML feedCVE, CWE, CPE,CVSS Vulnerability&ExploitDBRapid 7 web search, web list Daily CVSSvFeed vFeed.io API, SQLite DB Daily Several
a
Embedded VulnerabilityDetectorredhat Canonical DB, clients (maven, ant, java jenkins, eclipse), API Not freq. updated CVESecurityFocusVulnerability DBSecurityFocus Web search Daily CVEVuldb crowd-based web search, API, RSS Daily CVE,CPE,CVSSSecurityTracker SecurityTracker Web search, RSS Daily CVEZeroDayInitiative TippingPoint, DVLabs, researchers Web search, RSS Freq. within a month CVE, CVSSExploitDatabase Offensive Security Web search, DB repository with linux client, RSS Daily CVEICS-CERT US Department of Homeland Security Web search, RSS Almost daily CVE, CVSSJapan VulnerabilityNotesJPCERT Coordination Centre, Information-technologyPromotion Agency Web search, RSS Almost daily CWE, CVE, CVSSAusCERT Bulletins AusCERT Web search, RSS Daily CVECERT-EU SecurityAdvisoriesCERT-EU Web search, RSS, social media Daily CVE, CVSS
aCVE, CWE, CPE, CVSS, CAPEC [25], OVAL [26], WASC [27].K. Kritikos et al. Array 3-4 (2019) 100011
6Table 2depicts the evaluation results for the capabilities&support criteria partition. These results indicate that there is no clear winner.NVD can be distinguished due to its community support and wideadoption. vFeed.io can be discerned due to its extensive support tostandards and its rich interfaces (API&database). Further, it supports rapid threat response development which comes via pinpointing securityscripts of vulnerability scan tools, referencing exploit information uti-lisable for automated testing via penetration tools like metasploit,applying effective defence rules (e.g., via Snort) and discovering relevantpatches and hotﬁxes via online web crawling.In the following, we analyse the results per each criterion in separateparagraphs while theﬁnal paragraph attempts to nominate the bestapproach according to all criteria partitions.Community.The best possible community support seems to come fromCVE while the second best is OWASP Ten. In rest of the efforts, thecommunity is smaller and sometimes con ﬁned in the form of a single organisation.Interfaces.The most common interface is web-based search. In somecases, we can see feed mechanisms and APIs. A database is scarcelysupplied.Freshness.Community support aligns with this criterion as VDBs withexcellent or good community support, also provide frequent updates totheir content. However, there is also correlation with the interfacemechanism provided as a feed mechanism unveils frequent VDB updat-ing. In overall, the need to provide fresh vulnerability content is recog-nised by all VDB efforts.Standards.We investigate both the most widely supported standardand the VDB that adopts most vulnerability assessment standards. Assuch, we can observe that the Common Vulnerability Enumeration (CVE)is a clear winner followed by CVSS. Such results are well expected asthese two standards concern major information aspects for vulnerabil-ities spanning their identiﬁcation and risk assessment. Common PlatformEnumeration (CPE) and CWE seem to come third. CPE is a standardsupporting platform enumeration. Correlating CPE with vulnerabilityinformation enables mapping vulnerabilities to their affected entities.While very few VDBs support CPE, many VDBs establish a correlation ofvulnerabilites to IT products.vFeed is the VDB supporting most standards; not only the afore-mentioned ones but also others like: (a) OVAL, the open vulnerability andassessment language; (b) CAPEC, the Common Attack Pattern Enumer-ation and Classiﬁcation; (c) the Web Application Security Consortium(WASC) threat classiﬁcation. vFeed is followed by NVD, HPI-VDB andVulcan, supporting the 4 most common standards: CPE, CWE, CPE & CVSS.Best Vulnerability Database Nomination. By considering both criteria partitions, Vulcan, theﬁrst partition winner, is not the best possible VDB.On the contrary, vFeed is nominated as the best as it offers a very goodinformation coverage and top support for the second partition criteria.Vulcan is positioned second due to its excellent information coverage andits good standards support. HPI-VBD and NVD are ﬁnally positioned as third. Theﬁrst is an academic effort with good information coverage byfocusing on correlating information via exploiting academic expertiseand knowledge, it supports a good number of standards and offers a goodinterface support with extra services on top, including a programmaticAPI or self-diagnosis and attack graph creation. NVD, on the other hand,is widely adopted, has the same standards coverage, a good interfacelevel and vast community support. Thus, while a plethora of interestingVDBs exists, only 4 of them can be distinguished as the most promising.5. Vulnerability assessment tool analysis5.1. Comparative evaluation5.1.1. Criteria frameworkFor comparatively evaluating the discovered tools, a set of criteriawere devised, clustered in 3 categories: support, functionality andconﬁguration.The support category includes the following set of criteria concerningthe level of support and updating of the vulnerability detection tool./C15standards: signiﬁes the standards supported by the tool. Such a sup-port enables not to develop everything from scratch plus interoper-
ability with external tools/software. It also relates to communitycreation/maintenance as usually some communities evolve aroundstandards./C15community: the community behind a tool can support it by identifyingbugs, supplyingﬁxes or extensions, and developing complementarytools, thus catering for its proper evolution./C15freshness: it indicates how frequently the tool is updated, either interms of improved versions,ﬁxing detected bugs, or extensions. In thecontext of vulnerability assessment, all these aspects are critical. Bugscan stop a vulnerability assessment tool from functioning. While thereduced or non-existing ability to evolve can mean that the tool is notable to rapidly provide support for detecting new vulnerabilities.The functionality category includes the following set of criteria,related to certain important and critical features that a tool must exhibit./C15categorisation: One or more categories that characterise a tool ’s functionality, derived from the OWASP taxonomy [ 28], detailed in the appendix. Depending on its functionality, a tool might belong tomultiple categories. This can occur not only for single tools but alsotool agglomerations, where each component tool might focus ondelivering a different kind of vulnerability detection functionality.Table 3Evaluation results on support aspect.
Tool/ApproachStandards Community FreshnessGrabber [29]–github 2015 last commit GoLismero[30]CWE, CVE github 6 monthsNikto2 [31]–github Very frequently Vega [32]–Subgraph, github 6 months Wapiti [33]–sourceforge 2013 last commit OWASPXenotic XSS[34]–OWASP, github 4 monthsOWASP ZAP[35]CWE, WASC OWASP, github Very frequently OpenVAS[36]OVAL, CVE, CVSS Greenbone, GermanFederal Ofﬁce forInformation Security (BSI),DFN-CERT, GithubVeryfrequentlyArachni [37] CWE Github Very frequently IronWASP[38]CWE Github 2015 last commit w3af [39]–Github Very frequently OpenSCAP[40,41]SCAP [42],OVAL, CVE CPE,CCE [43],XCCDF [44]Github, Red Hat, NIST Very frequentlyClair [45] CVE, CVSS Gitnub, CoreOS Very frequently Vulcan [11] SCAP, CVE, CWE,CPE, CVSSUniversity of North Texas,NIST Community-driven HPI-Vuln [46
] CVSS, CWE, CVE,CPEHasso-Plattner Institute Community- driven UC-OPS [47] CVSS, CVE, CWE,CPE, OVALUniversity of Marburg Community- driven AWS-Vuln[48]CVSS, CVE, CWE,CPEEURECOM, NortheasternUniversity Community-drivenK. Kritikos et al. Array 3-4 (2019) 100011
7Such tool agglomerations are further inspected by the Tool Coverage criterion. Finally, as all tools can perform vulnerability detection,they trivially map to thevulnerability scanningcategory which is omitted from the analysis./C15Object: this criterion investigates the kind of object/componentscanned by a tool. A tool might focus on one component kind ormultiple. Thus, the higher is the number of component kinds, thebetter is the tool./C15Vulnerability Coverage: attempts to evaluate a tool’s with respect to the vulnerability kinds it can detect. In other words, it assesses the per-centage of all possible vulnerability kinds covered. To derive thispercentage, we have followed an approach, detailed in Appendix B, comprising (3) main steps: (a) identiﬁcation of all possible web application vulnerability kinds by relying on SecToolMarket.com; (b) assessment of the percentage of vulnerability kinds covered by a tool;(c) mapping of the derived coverage percentages into a qualitativescale with values from“Very Low”to“Very High”going from a very low coverage below 16.66% up to a perfect coverage close to 100%./C15Inferencing: the ability to support inferencing that could, for instance,enable a tool to derive new vulnerabilities or correlate vulnerabilitiesto each other, possibly allowing it to support root-cause analysis./C15Counter-Measures: the ability to associate vulnerabilities to counter-measures. This could enable a security system to check the alterna-tives in addressing the current security situation and select the bestpossible./C15Risk Assessment: the capability to evaluate the individual and overallrisk across all vulnerabilities that have been detected./C15Tool coverage: it determines the sub-tools of a tool, in case it maps to atool agglomeration. This criterion is related to the categorisationone as a tool that utilises multiple tools can also cover multiple categories.The last category comprises the next set of criteria related to the toolconﬁguration, i.e., how a tool can be conﬁgured to work properly as desired, including architectural and modularity aspects, plus its resourcerequirements./C15architecture: identiﬁes the tool architecture. For instance, tools canfollow a client-server architecture, where the client is installed inplaces requiring scanning, while the logic and vulnerability de ﬁni- tions are taken from the server side. Each architecture kind can havespeciﬁc pros and cons. For instance, a tool working in standalonemode does not spend network bandwidth and resources to commu-nicate with a server. However, it cannot receive updates on logic orvulnerability deﬁnitions./C15usage level: indicates whether a tool can be executed internally orexternally to the scanning place. An external execution is non-intrusive, not spending precious resources from the running appli-cation component. However, it might not detect all possible vulner-abilities, especially those related to a component ’s internal execution environment. On the other hand, an internal execution can detectmore vulnerabilities but can also spend some precious resources,which might prevent the application from delivering a suitable ser-
vice level. So, toolsﬂexibly operating in different levels are morepreferable as they allow to detect only the most relevant vulnerabilitykinds and thus achieve a better trade-off between vulnerabilitycoverage and resource consumption./C15Vulnerability DB: ability to connect to a VDB from which freshvulnerability deﬁnitions and other crucial information, like counter-measures, can be drawn. While this enables to address properlynew vulnerabilities, it can also assist in suitably producing riskassessment reports with a detail level also correlated to the kind ofrisk assessment that can be supported (see risk assessment criterion)./C15Modularity: capability to operate under different modules where eachmodule could support detecting certain vulnerability kinds. This en-ables supporting a more focused, resource-ef ﬁcient, dynamic vulnerability assessment due to the one-demand scanning scopeadjustment via proper module selection. This can be quite bene ﬁcial for rapid security issue detection and addressing (via a more focusedvulnerability assessment) and for conducting periodic full scanscorrelated to periods of low resource usage in the application./C15OS support: A tool operating across different OSs should be preferredas it can cover all applications executing under these OSs. OS supportcomes into twoﬂavours: (a)operational supportmeaning that the tool can operate on that OS; (b)functional supportmeaning that the tool can also detect vulnerabilities at the OS on which it is run./C15Resource requirements: Each tool comes with its own minimumresource requirements that must be satis ﬁed to work properly. However, such requirements can affect the application ’s resource usage. Thus, in case of resource-intensive applications, it might bebetter to use tools either lightweight or supporting a non-intrusive,external usage level. Otherwise, especially in case of critical appli-cations, more heavy tools might be required to operate in full modesuch that the detection of a high number of different kinds ofvulnerability issues can be achieved./C15Access control mode: A tool might need to be executed based on certainaccess control rights to be operationally successful. Thus, if the goal istoﬁnd OS issues, the tool should have privileged access to check theOS-speciﬁc part of the hosting component (e.g., VM). This criterionalso correlates to theusage levelas when that level is non-intrusive,the access control mode can be the least critical.5.1.2. Analysis of comparative evaluation results5.1.2.1. Support category.The evaluation results for the support cate-gory can be seen inTable 3. In the following, we analyse these results pereach criterion involved in this category.Standards.Both CVE and CWE seem to be mostly supported. This islogical as these standards enable to better characterise a certain vulner-ability and classify it. Next comes CVSS followed by CPE which haveslightly more than one third of support. This indicates that the need toassociate vulnerabilities to the components concerned has not been wellrecognised yet while the capability to assess the risk related to avulnerability seems to be neglected.OVAL and SCAP are not very well supported. However, we believe inthese standards as they will enable to automate vulnerability detectionand addressing, thus allowing scanning tools to move forward byproviding a new breed of functionalities and extending their vulnera-bility coverage.It must be noted that one third of the tools do not support any stan-dard. Such tools do not also employ a VDB (see Table 5). This seems a logical correlation as the need to store information in a VDB would havecreated the requirement to supply a structured way to perform this viastandards. In our opinion, support to standards has been recognised bymost tool providers as: (a) it can allow any kind of integration or coop-eration between their tools; (b) it facilitates a better identi ﬁcation and assessment of vulnerabilities, thus enabling tool users to really bene ﬁt from the extra information retrieved.We must underline the good performance of academic approaches inthis criterion, indicating that researchers have well understood the powerof standards such that they tend to adopt them more easily than othertool providers. The sole exception is OpenSCAP which has considered thesupport to standards as one of its major design requirements. This welljustiﬁes the support of this tool to 8 security standards.Community.We have found that most tools have their own commu-nities, usually maintained by the tool provider. Such communities caninteract with the tool provider via emailing lists and blogs, while they canalso participate in the tool development. The latter is the cornerstone ofopen-source communities, usually built around interesting or innovativetools that become sustainable via their participation. As such, securitysoftware providers follow faithfully this model which is widely adoptedin the software world.K. Kritikos et al. Array 3-4 (2019) 100011
8Table 4Evaluation results on functional aspect.
Tool/ApproachCategorisation Object Vuln. CoverageInf. Counter-MeasuresRiskAssessmentTool CoverageGrabber Data Validation, Info. gathering, Source codeanalysis WebApplicationVery Low No No No Javascript Lint [ 50] PHP- SAT[51] GoLismero Data validation,Info. gathering, WebApplication,Network, DBVery High No Yes No OpenVAS, nmap [ 52], ssl-scan[53]Nikto2 Data validation Web Server Low No No No – Vega Data validation Web ApplicationLow No No No –Wapiti Data validation Web ApplicationMedium No No No –OWASP Data validation, Web Application,Very Low No No No –Xenotic XSS Info. gathering, NetworkOWASPZAPData validation,Info. gathering WebApplication,NetworkGood No No No DirBuster [ 54],JBroFuzz [55]OpenVAS Data validationInfo. gathering Web Server,DB, OS,WebApplication,Network, VM,ContainerVery High No Yes Yes Several
a
Arachni Data validation,Info. gathering WebApplicationGood No Yes Yes –IronWASP Data validation Web ApplicationMedium No No Yes Several
b
w3af Data validation,Info. gathering WebApplicationHigh No No No –OpenSCAP Data validation Web Server, DB, OS,WebApplication,Network, VM,ContainerVery High No Yes Yes –
Clair Data validation Image Low No No Yes – Vulcan Data validation Web ApplicationVery Low Yes Yes Yes –HPI-Vuln See OpenVAS See OpenVAS Very High No Yes Yes OpenVASUC-OPS See OpenVAS See OpenVAS Very High No Yes Yes OpenVAS, NessusAWS-Vuln Data validation,Info. gathering,Malware detectionSee OpenVAS Very High No Yes Yes Several
c
aGreenbone Security Assistant [56], Nikto, NMap, ike-scan [57], snmpwalk [58], amap [59], ldapsearch [60], slad [61], ovaldi [62], pnscan [63], portbunny [64], w3af.
bWiHawk [65], XmlChor [66], IronSAP [67], SSL Security Checker [68], OWASP Skanda [69], HAWAS [70], CSRF PoC Generator [71].
cNessus, Clam AV [72], Chkrootkit [73], RootkitHunter [74], Rootkit Revealer [75], NMap, John the Ripper [76], extundelete [77], WhatWeb [78].
Table 5Evaluation results on conﬁguration aspect.
Tool/Approach Architecture Usage LevelVDB Modularity OS Support Resource Reqs. AccessControlModeGrabber Standalone External No No Linux Low NormalGoLismero Client-Server Both Yes No Any See OpenVAS AdminNikto2 Standalone External No No Any Low NormalVega Standalone External No Yes Linux, OS-X, Windows Low NormalWapiti Standalone External No Yes OS with python support Low NormalOWASP XenoticXSSStandalone, API External No Yes Windows Low NormalOWASP ZAP Standalone, API External No Yes Any (4 GB RAM) NormalOpenVAS Client-Server Both Yes Yes Any (2vCPUs, 2 –4 GB) Both Arachni Standalone, API, Client-Server External No Yes Any (2BG RAM, 2 –4 GB Disk) Normal IronWASP Standalone External No Yes Windows, Linux, OS-X Low Normalw3af Standalone External Yes Yes Linux, OS-X, *BSD Low NormalOpenSCAP Standalone Both Yes Yes Windows, Linux Low BothClair Standalone, Client-Server External Yes No Debian, Ubuntu, CentOS Low NormalVulcan Standalone External Yes Yes Android Low NormalHPI-Vuln Client-Server External Yes Yes Any (2vCPUs, 2 –4 GB) Both UC-OPS Client-Server Both Yes Yes Linux (2vCPUs, 2 –4 GB) Both AWS-Vuln Client-Server Both Yes Yes Windows, Linux (1 dual-core 2 GHz CPU, 2–4 GB RAM,30 GB disk)BothK. Kritikos et al. Array 3-4 (2019) 100011
9We must appraise the existence of certain organisations, i.e., NIST andOWASP, that promote in the best possible way security software. NISTfocuses more on developing standards aiming to bridge interoperabilityissues, while OWASP focuses on promoting innovative tools, creatingtool benchmarks plus classifying both tools and vulnerabilities. Bothorganisations appear twice as community members in two respectivescanning tools. This also indicates that these organisations have a greatbelief in the dynamics and capabilities of the tools they support.We have also discovered that by following new software developmentpractices along with the open-source model, the tool providers invest ongithub as a wide source tank for potential community members while stilloffering the right facilities for agile software development. We believethat this is a trend which will be followed by most scanning tools in thenear future.We must also highlight that OpenVAS and OpenSCAP have more than2 communities targeted. This is not accidental by considering theirvulnerability coverage (see analysis below in functional category).OpenSCAP is supported by both NIST and RedHat while it has alreadyengaged multiple OSproviders as these providers have already speci ﬁed their vulnerabilities and policies in a format acceptable by OpenSCAP.On the other hand, OpenVAS is a fork of the well-known Nessus [ 49] vulnerability scanner, which has moved from an open-source to aclosed-source model. Fortunately, with the support from 3 organisations,OpenVAS has been quite successful as it can be witnessed by the greatamount of tools which exploit internally this scanner.Finally, we must stress that as the last 4 approaches are academicones, their community is more restricted, usually in form of one or twoorganisations, where one is afﬁliated to the other. This restriction, then,impacts the tools’update frequency which is quite scarce. This underlinesthe need for the involved organisations to have a more aggressive movetowards the open-source community to better support their tools sus-tainability and evolution.Freshness.More than half of the tools are updated frequently (everysome months) or very frequently (every some days) while the rest areeither not updated any more (tool provider has stopped investing onthem) or the update is infrequent and community-driven (esp. for aca-demic tools). This is a nice result, signifying that the need to update thetools to address the continuously increasing vast set of vulnerabilities hasbeen well recognised.This result can be correlated with the previous one by consideringthat frequently-updated open-source tools either rely on more than onecommunity or are pushed by their providers as they see great value inthem. In theﬁrst case, the update frequency can reach even higher levelsdepending on the tool community size. For instance, if we considerOWASP, this organisation is both big and well-known for its securityexpertise. As such, it does not only use its own resources to evolve itstools but it also incorporates an enormous in size member list supportingboth the tools updating and maintenance via two alternative feedbackmechanisms (merge requests&bug reporting).Best Tool Nomination for theSupportCategory.By considering the evaluation results across all criteria of this category, the best performersin theCommunitycriterion, OpenVAS and OpenSCAP, can be discernedbased on their community support and update frequency. We furtherdiscern OpenSCAP as the topmost performer, due to the great amount ofstandards it supports. Finally, academic approaches, especially Vulcanand UC-OPS, are placed third, with a good standards coverage.5.1.2.2. Functionality.The evaluation results for the functionality cate-gory can be seen inTable 4. In the following, we analyse these results pereach criterion involved in this category.Categories.All tools go beyond pure vulnerability scanning and offerextra functionality mapping to other OWASP categories from OWASPtaxonomy, spanningdata validation,information gathering, andapplication protection.All tools exhibit the functionality ofdata validationtesting. This is wellexpected as many vulnerabilities originate from improper or non-existentdata input validation. Thus, a tool must check against the target objectvarious kinds of data validation penetration scenarios. Three ways existto support this functionality: (a) develop it internally in the tool; (b) relyon the deﬁnitions of vulnerabilities which include the way to be detected;
(b) use another tool, like w3af, exhibiting this functionality.Similarly, more than half of the tools support information gathering. This is also logical as before any kind of vulnerability assessment isperformed, the target object needs to be scanned to ﬁnd the right places where vulnerability assessment will focus, collected in an informationgathering step. Depending on the object of focus, information gatheringcan take different forms. For pure web application scanners, spidersoftware is exploited toﬁnd all actual web pages from which the scan-ning can be performed. For more sophisticated scanners, informationgathering involves checking the whole host, ﬁnding open ports and attempting to infer the components that run on such ports.Finally, we must remark that only one tool exhibits application pro-tection capabilities, taking the form of antivirus/anti-malware detection.Such functionality can be considered as complementary to vulnerabilitydetection in the way it is employed. It has the added-value that it isactively enforced, enabling the system to immediately detect and react ona vulnerability that comes in form of a virus/malware. This is essential ascontinuous security assessment is highly required for most applications/systems, which constantly evolve over time. Further, we must stress thatin contrast to pure vulnerability detection, focusing on discovering butnot addressing vulnerabilities, antivirus detection can react on thevulnerability by, e.g., putting infected assets into quarantine or deletingthem. Based on this argument, it is recommended that either scanningtools are extended with application protection capabilities or are com-plemented with tools offering such capabilities. Further, by consideringapplication evolution, it is also advocated that vulnerability scanning isnot an one-shot but a continuous process with a frequency conforming tothe evolution frequency of the respective application.Object.Almost all vulnerability scanners can operate over a webapplication. However, as the next paragraph and Section 5.2will show, each scanner might have a different focus on the web application vul-nerabilities by, e.g., concentrating only on speci ﬁc kinds, like the most usual ones.Most old scanners focus solely on web applications as the targetscanning object. The main rationale is that application developers wouldcare most how web applications are vulnerable in their operation space(e.g., set of web pages) from the outside. However, as an applicationmight comprise multiple components and might run on vulnerablemedia, it is apparent that the focus should now move to holistically coverthe whole application system. This is actually grabbed by both recentscanners and academic approaches. A good example for the former case isOpenSCAP that covers not only pure web application but also cross-levelsystem vulnerabilities. In the latter case, academic approaches extend analmost complete vulnerability scanner, like OpenVAS, to make it perfect.Vulnerability Coverage.By considering all tools, it seems that onlyOpenVAS and OpenSCAP have the highest vulnerability coverage alongwith tools that re-use them. By considering that: (a) the percentage ofthese tools is less than half of the overall tools; (b) OpenVAS andOpenSCAP do not focus on scanning the whole web application opera-tional space, it is easy to understand that such a result marks the need toimprove most existing scanning tools.In fact, this might signify that security experts must now move to anew direction: as there exists a sophisticated state-of-the-art tool, thecommunity must focus on both improving and evolving it over time toalso detect new vulnerabilities via the production of respective plugins.In fact, this already occurs for the 2 aforementioned tools. In the contextof Network Vulnerability Tests (NVTs) or OVAL de ﬁnitions, OpenVAS and OpenSCAP, respectively, rely on a huge community effort, involvinga great amount of OS and software providers contributing to the de ﬁni- tion and detection of vulnerabilities.On the other hand, the academic community, while taking the sameK. Kritikos et al. Array 3-4 (2019) 100011
10direction on exploiting such state-of-the-art scanners, has a differentfocus and attempts to improve or orchestrate them with other tools toachieve both a better coverage extent and breadth plus capture a greaterset of vulnerability detection scenarios. For instance, as indicated in theCategorisationcriterion, one academic approach, AWS-Vuln, focuses oncombining vulnerability scanning with application protection to sustain asmooth, vulnerability- and malware-free application lifecycle. Thisapproach signiﬁes that due to the application system ’s complexity, it is not always possible to cover all possible vulnerability kinds and differenttools need to be employed to achieve a perfect coverage. Thus, it surelymoves to the right direction and paves the way via which vulnerabilityscanning tools should be utilised in the near future.Inferencing.Only one tool, Vulcan, supports some kind of inferencing.This academic tool employs ontology-based reasoning to support infer-encing. Restricted to the current tool functionality, this inferencing takesthe form of discovering vulnerabilities of component agglomerationsapart from those of individual components. However, this ontology-based reasoning approach is quite promising as it could be extended tocover the derivation of extra kinds of vulnerability-related knowledge;these kinds are explained in Section6.Counter-Measures.Also correlated to mitigation plan production, thiscriterion highlights the need to associate vulnerabilities with the waysthey can be addressed known as counter-measures. In contrast to theprevious criterion, though, the situation is much better as almost half ofthe tools support this criterion. This means that the need to report thiscorrelation kind to users has been well recognised and realised. Further,most tools exhibiting this feature support standards like CVE or CWEwhich do provide counter-measure information. This is another bene ﬁt related to the support of standards.OpenSCAP seems to be the sole tool going beyond counter-measurereporting to counter-measure enforcement. In particular, through sup-porting SCAP, this tool can mitigate the vulnerabilities found in somecases via automatic path application. This is the right direction to befollowed by all other scanners to really advance their usage and increasetheir added-value.Risk Assessment.More than half of the tools offer some risk assessmentform. So, they have well recognised this information ’s importance, which, by accompanying the vulnerability detection one, enables users toassess their application’s overall risk level and thus supports them inproducing a mitigation plan that prioritises more the vulnerabilities withthe highest risk level. Similarly to the previous criterion, many toolssupporting this criterion, also conform to CVSS, the most prevailing andde-facto standard.Note that 3 tools claim to support some risk assessment form but it isnot clear how. We suspect that they map vulnerability categories tocertain risk levels to address this. However, such an approach is toocoarse-grained and does not consider a vulnerability ’s severity with respect to the consequences it might have and the component(s) onwhich it applies.The 4 academic approaches support both this and the previous cri-terion which signiﬁes that they have well recognised the requirement tosupport the supply of the respective information. In most of the cases, thissupport comes from the adoption and possible extension of other scan-ning tools.Tool Coverage.Scanning tools re-use a great variety of sub-tools,spanning many OWASP taxonomy categories, including antivirus pro-tection, information gathering, network scanning, and data validationtesting. From these categories, we can discern Nessus and OpenVAS, asthose sub-tools mostly re-used due to their almost perfect coverageextent, as well as nmap, as the most popular and well-adopted informa-tion gathering/network scanning tool.However, more than half of the tools do not re-use any sub-tool. Thiscan be due to various factors: (a) they just do not report such a re-use – this then needs to be explored in the tool ’s source code; (b) no other sub- tool is re-used–this means that the scanning tool was developed fromscratch while its coverage level is inﬂuenced by its maturity level,developer base and level of support from its community.In fact, the second factor seems to apply to most (6 out of 9) pure webapplication scanning tools. This means that such tools have a focus thatcan be independently realised. This also signi ﬁes that possibly these tools’
providers do not want to invest on adopting other tools with themain rationale that this could restrict them technologically in the toolimplementation.Best Tool Nomination for the Functionality Category. In this criteria category, there is not a clear winner. We can actually see 2 main toolpartitions: (a) top approaches which do not support inferencing; (b) theVulcan academic approach which supports inferencing but has a very lowcoverage and focuses only on web applications as target objects. Thelatter also indicates that if Vulcan was able to use Nessus or OpenVAS, wewould surely nominate it as the best in this category, as this wouldimmediately improve in an ultimate manner its two main de ﬁcits. In theﬁrst partition, the academic tool AWS-Vuln is the clear winner.It not only has the best possible categorisation and vulnerability coveragebut also re-uses a great sub-tools set, which constitutes another proof ofits completeness. From the pure tool world, we can discern OpenVAS,which, however, does not cover additional security scanning categories.From the 2 best result partitions, academic tools can be discerned.This interesting result signiﬁes that the academia not only pioneersresearch but also attempts to be more innovative. As such, this is a goodparadigm for tool providers which need to either re-use academic tools orfollow their orchestration approach to improve their own tools and in-crease their uptaking.5.1.2.3. Conﬁguration.The evaluation results for the conﬁguration category can be seen inTable 5. In the following, we analyse these resultsper each criterion involved in this category.Architecture.A tool’s architecture along with its mode of operationimpacts both the vulnerability coverage and the ﬂexibility in the tool runtime administration and execution. In particular, a tool that works ina standalone manner and scans externally an application has the bene ﬁt of not interfering with the application ’s normal operation, thus taking a non-intrusive approach. On the other hand, this comes with the disad-vantage that vulnerability coverage is low. A tool working in client-servermode that is executed internally to an application is more intrusive buthas the beneﬁt of better vulnerability coverage even for the wholeapplication via the well conﬁgured concurrent deployment of tool clientsthat scan all application components in parallel.Table 5signiﬁes that most tools can be run in a standalone mode. Thisis a natural, well-expected result as: (a) this is the desired operation modefor simple, non-expert users who prefer to have a simpli ﬁed way of launching and interacting with the tool; (b) some tools have lowvulnerability coverage and can work only in non-intrusive mode. As such,it is not rationale to make these tools distributed and complicate theiradministration as this will affect their main competitive advantage: theirsimpliﬁed usage and administration.Tools that employ a client-server architecture are about one third innumber. They usually exhibit a better vulnerability coverage and focuson more advanced, IT-security expert users. As such, in contrast to theprevious paragraph argumentation, these tools should employ a client-server architecture as the most natural way to completely cover vulner-abilities across the whole application plus the most ﬂexible and capable way in terms of administering the scanning. Due to these advantages, it isworth sacriﬁcing slightly the simplicity to better attract the main targetusers, i.e., IT-security experts.There is also the case of tools (especially Arachni) which enable thealternative use of both architectures. This might be a very good move toextend a tool’s applicability towards all kinds of users and not only thenovice ones. As such, the adoption and market share of such tool could beincreased.In very limited cases, an API is offered by tools which either operate instandalone mode or in a client-server architecture. Such a mechanismK. Kritikos et al. Array 3-4 (2019) 100011
11targets developers of more extensive tools requiring a standardised wayto interact with the encompassing tool. An API supply can be also pref-erable in case that the user does not need to be burdened with the toolinstallation and deployment. In particular, a security organisation canmake the API available as a service such that the user can immediatelyuse it without spending resources to deploy, install and maintain it.However, an external API’s supply comes with a maintenance cost for itsoffering organisation which needs to be accompanied with a certainbusiness model to enable this organisation to gain from it. This businessmodel could take the form of an usage fee or the supply of API usagesupport or the paid subscription to more advanced API features. How-ever, none of the tools seems to be offered in a SaaS mode. This is naturalas all tools considered are open-source so only open-source-based busi-ness models would make sense in this case.Usage Level.The usage level directly impacts a tool ’s ability to cover a suitable coverage level. In particular, a tool operating in external modecannot access an application component ’s internal environment. As such, it might discover vulnerabilities that might jeopardise the component ’s operation from the outside but it will not detect other vulnerability kindsthat might concern the component (e.g., bugs or modelling mistakes inthe source code) and its environment ’s elements. On the other hand, for web application kinds with a simpliﬁed architecture, this external scan- ning mode might be more suitable as more focused and less intrusive. Assuch, the operation mode could rely on the type of application targeted,its architecture and its environment.The evaluation results unveil that most tools support only externalscanning. This is well-expected as: (a) the very nature of these tools fo-cuses on web application security only and has external scanning as itsdesign cornerstone; (b) in the past, web applications were offered inprivate, more difﬁcult to penetrate infrastructures. As such, there was noneed to supply an internal scanning mode to ﬁnd inner environment vulnerabilities. However, nowadays, due to the change of focus withrespect to the hosting environment and its public nature, this environ-ment is more vulnerable to attacks from adversaries, also due the fact thathardware resources are shared between user applications. This focus shiftthen requires supporting also an internal scanning mode. This real ne-cessity, quite demanding for cloud environments, seems to have beenpicked up by some tool providers. In particular, one third of the toolsseem to support both scanning modes. This concerns both the mostadvanced scanning tools, i.e., OpenVAS and OpenSCAP, and those aca-demic approaches adopting OpenVAS (or its parent tool, i.e., Nessus).Vulnerability Database.The existence of a VDB enhances a tool ’s reporting capability while assists in better detecting and identifyingvulnerabilities. Coupled with the support to standards, the amount andcredibility of reported information can reach high levels and thus makethe tool more attractive to users. The above arguments must havetouched the tool providers as almost half of the tools encompass a VDB inone or another form with varying capabilities. Most approaches encom-passing a VDB also support one or more standards, which enables toexhibit the advantages that were previously mentioned. On the otherhand, tools with no VDB seem to be old and clearly not supportingstandards. As such, these tools could be considered as outdated and notattractive any more to users. This has thus impacted their communities,also explaining their infrequent updating.The need to encompass a VDB has been well recognised by the aca-demic community as there is a 100% support for this feature by all ac-ademic efforts.Modularity.A modular tool can be individually extended as needed. Italso allows making a focused scanning by using only those modulesnecessary for detecting the right vulnerabilities for an application. Thisneed for modular tools seems to be picked up by the tool providers asmost tools are modular.Two different forms of modularity exist across the examined tools.Full modularity enables specifying scanning pro ﬁles including a conﬁg- urable scanning rule set. Such scanning pro ﬁles are special-purpose as they focus only on a certain set of vulnerability kinds which might beexhibited by particular kinds of applications or their components. Partialmodularity, on the other hand, is a limited modularity form that impliesthe existence of one proﬁle from which the user can select some of thescanning rules contained. In some cases, the addition of new rules mightbe also possible in such a proﬁle.
Based on the above deﬁnitions, we can observe that most tools arefully modular. This is a very good result for the prospective of tool usersas they have the best possibleﬂexibility in conﬁguring such tools with the right set of scanning rules. This also increases the usability level of thesetools.On the other side, non modular tools are mostly those with lowvulnerability coverage. Such tools might be special-purpose, focusing onfew vulnerability kinds, such that making them modular would not makesense.In most cases, tools which expose a VDB also exhibit modularity. Thisis a promising result, also enabling possible vulnerability scanningadopters to assess a tool’s coverage level by inspecting the vulnerabilitykinds it can cover from the available modules and their mapping to thevulnerabilities stored in the VDB. The VDB alone, though, cannot tell thewhole truth as it could be made as complete as possible (as in case of NVDVDB) without having a one-to-one mapping of vulnerabilities to thetool’s detection capabilities.OSSupport.The variety of OSs supported by a tool impacts theextensiveness in the tool’s usage and applicability. This support can comein different forms: (a) with respect to the tool ’s execution environment; (b) with respect to the scanned component ’s execution environment. Obviously, the latter form affects the former as it is expected that whenone tool is written to operate in one OS, it is rather strange to apply it toapplications written in other OSs. However, this expectation holdsmainly for internal-mode tools. As such, this criterion should not beinspected alone when attempting to discover OS restrictions on theapplication side. On the contrary, by knowing that a tool works inexternal mode, this tool can be applied for applications operating in anyOS. Further, a tool that works in both modes would then be able to coverexternally any application and internally only an application operatingon those OSs where this tool also operates.Some tools like Clair are able to operate on the level of images. Thisthen enables them to work on an “external”mode as they just inspect the images for vulnerabilities before they are deployed in the cloud. As such,this kind of tools can support the scanning of images of any OS.The evaluation results with respect to theﬁrst OSsupport type signify that most tools support Linux-based OSs. This is well expected byconsidering that such OSs have been deemed more secure (at initialdeployment time without any extra hardening support [ 48]). Windows come next with around two third of tools able to operate in Windowsenvironments. OS-X comes third but still having more than half of thetools supporting it.Around one third of the tools support any OS, which is an encouragingresult, signifying that the respective need has been well recognised bytool providers. However, contrary to other criteria, this need is not wellreﬂected in academic tools. This is well anticipated as the internal orcommunity resources dedicated to developing these tools are limited.Rather, a more focused development on improving these tools ’core ca- pabilities is expected.By combining the results from this and the Usage Levelcriterion, we can deduce that: (a) tools only operating on Linux have an external orboth modes; (b) tools operating on Windows and Linux exhibit bothmodes; (c) tools operating on any OStend to support only external usage.This result unveils possibly the fact that tools with external usage tend toincrease their applicability to improve their market share position bysupporting more OSs to alleviate for their non-high vulnerabilitycoverage. It further indicates that tools focusing on a limited OS set tendto be more extensive in terms of their usage level. This is natural as in thiscase such tools have a better coverage and applicability over these OSs asthey invest on these OSs’popularity.Hardware Requirements.This should be inspected in conjunction withK. Kritikos et al. Array 3-4 (2019) 100011
12theusage levelcriterion. This is due to the fact that as long as a tool is notintrusive, it can be regarded as not demanding for resource requirements.However, this should not be treated as a panacea as there exist non-intrusive tools like ZAP that have high hardware requirements. On theother hand, an intrusive tool should be either invoked irregularly or havelow resource requirements to not interfere with the scanned application ’s normal operation.The evaluation results indicate that more than half of the tools havelow resource requirements. These tools ’providers have decided to make them lightweight to be more appealing to prospective adopters. How-ever, this design choice seems to affect the tool capabilities, as most toolswith low requirements do not have a good vulnerability coverage. On theother hand, sophisticated tools, like OpenVAS, or tools that re-use themare more heavy-weight and require either high-disk VMs (Nessus) ormedium CPU&memory VMs (OpenVAS). However, these tools ’usage level is extensive such that they can be executed also in external mode. Assuch, they are not necessarily intrusive to the application such that theycan jeopardise its normal operation. Further, we are discussing mostlyrequirements at the server side. For the client side, the resource re-quirements can be signiﬁcantly less. So, the burden in using these toolscomes mainly with the operation cost of the server which can be outsidethe VMs hosting the scanned application.Access Control Mode.This mode impacts a tool’s vulnerability coverage plus its administration. It might also raise security concernsabout how safe are clients operating in admin mode in a client-serverarchitecture. Thus, tools operating in a normal, user mode are lessintrusive and do not access critical assets via internal scanning. However,as such assets are uncovered, vulnerability coverage is low. On the otherhand, an admin mode enables accessing (possibly infected) critical sys-tem assets and thus caters for a better vulnerability coverage. However,acting in this mode comes with issues that might involve the incautiousexchange of credentials and the possible scanner infection thus exposingthe respective system to great vulnerability.The evaluation signiﬁes that most tools support the normal accessmode. This is a natural result as: (a) a tool focusing solely on external webapplication scanning does not require using any credentials for authen-tication and authorisation purposes; (b) less vulnerability to attacks canbe caused by the tool’s compromise; (c) possible undesirability of users toprovide credentials for this scanning type. On the other hand, almost onequarter of tools support both access modes. We believe that such tools aremoreﬂexible in vulnerability scanning as they enable the user to choosethe most desirable access control mode according to the current contextand application kind. It is not surprising to see that the most represen-tative tools of this kind are OpenVAs and OpenSCAR, while the rest ofthis kind’s tools just reuse them. This indicates that these two tools aredesigned based on suitable requirement sets originating from actual usersand their needs. This answers well these tools ’excellent performance in many of the considered criteria.Best Tool Nomination for the Conﬁguration Category.By considering the overall evaluation results of this criteria category, OpenVAS andOpenSCAP can be considered as the best. OpenVAS seems better in termsof OS support while OpenSCAP is better with respect to hardware re-quirements. As the OS support can be less important than hardware re-quirements, we might then nominate OpenSCAP as the best. In bothcases, both tools seem to support only one kind of architecture (deploy-ment). Thus, we can deduce that there is still room for improvement ofthese tools.On the other hand, academic tools require signi ﬁcant improvement over this criteria category, especially for the criteria of usage level,OS support andhardware requirements. However, in contrast to non-academictools, we do not expect such improvement to take place soon due to thelimited resources available for developing the academic tools and theirdevelopment priorities.5.1.2.4. Overall evaluation results analysis. By considering the resultsacross all categories plus the derived results from the analysis of eachcategory, we can discern two main tools: OpenVAS and OpenSCAP.OpenVAS is the best in terms of the functional category while OpenSCAP
in the support category. While they score equally in the con ﬁguration category. However, in our opinion, while OpenVAS might seem better incategorisation and tool coverage as well as OSsupport, we would pro-mote OpenSCAP as the best based on the following justi ﬁcation: (a) OpenSCAP is the sole tool able to mitigate vulnerabilities; (b) it has lowerresource requirements; (c) it supports more standards while it has asimilar community support with OpenVAS. Further, it seems to have aslight better vulnerability coverage based on the quantitative results re-ported inAppendix B. To this end, OpenSCAP is the best possible tool thatcan be selected by respective practitioners while OpenVAS could beselected as the second best.If the analysis is restricted in the academic tool partition, differenttools can be distinguished as best in each criteria partition. HPI-Vuln isbetter in terms of conﬁguration, AWS-Vuln in terms of functionality andVulcan in terms of support. In overall, we would actually discern twotools for different reasons. AWS-Vuln is discerned not only due to itsperfect functional coverage but also as it seems to have good communitysupport and supports well-known standards. By considering its functionalcoverage, we should highlight its capability to complement vulnerabilitydetection with malware/antivirus protection, a highly-required andinnovative feature. However, it still needs to be improved in some aspectslike resource consumption, which comes with its design choice to selectNessus as its core vulnerability scanning sub-tool.On the other hand, we discern Vulcan for three main reasons: (a) itsexcellent support level to standards; (b) its inference capability; (c) itslow resource requirements. However, Vulcan still needs to be improvedwith respect to its low vulnerability and object coverage plus its limitedOSsupport. Further, it needs to be expanded to use a very good vulner-ability scanner, like OpenVAS or OpenSCAP. In any case, we believe thatall academic tools should be made publicly available as normal vulner-ability scanning tools to increase their sustainability and potential forimprovement and evolution.5.2. Benchmark evaluationAs indicated in Section3, research questionQ
3required a special handling to enable assessing the properties of scanning time, accuracyand coverage. This handling is also due to the fact that there is noreporting currently for most tools about their performance for theseproperties. Nevertheless, even if this reporting was available, it could beold or rely on simple benchmarks.This difﬁcult situation actually created the need to search for a suit-able benchmark to conduct the required tool assessment. Such a bench-mark should exhibit certain features, including the capability toobjectively assess all these properties while, in addition, catering forcreating an as realistic as possible assessment environment that coversmultiple vulnerability areas.As such, we have conducted a small web-based survey to identifycandidate vulnerability tool evaluation benchmarks. This led to discov-ering 6 benchmarks which are shortly presented in Table 6. From these benchmarks, we haveﬁnally selected the OWASP benchmark for thefollowing reasons: (a) it is as realistic as possible as it relies on a web-based application built according to well-known design patterns; (b) ithas the widest number of vulnerability areas covered; (c) it enables toassess all three properties. This includes a suitable scanning accuracymetric that considers a tool’s returned evaluation results across all thevulnerability areas covered; (d) it supplies code which employs an easyintegration point with respect to the adoption of a scanning tool. Thecode is also more frequently updated than the other benchmarks; (e) italready includes some scanning tools in its distribution.After selecting the right benchmark, we came to a certain dilemma:should we evaluate the plethora of selected scanning tools? As Section 3 indicated, this required a huge effort; so, we followed a differentK. Kritikos et al. Array 3-4 (2019) 100011
13approach. In particular, we decided to use the tools that the benchmarkalready provides plus open-source tools, like Zap, which provide guid-ance about how they can be assessed with this benchmark. This wouldenable us to maintain the assessment effort to the minimum while stillenabling to answer appropriately research question Q
3. From the reader’s perspective, we continue the assessment of the previous sub-section withthe capability to rank the top results against the three major properties ofscanning accuracy, time and coverage. To inspect the trade-offs betweenthe three main properties, we also tried to evaluate the scanning time andaccuracy by varying a tool’s vulnerability coverage. This was performedby activating or deactivating the respective vulnerability scanning rules,in case this possibility was available.In the following, weﬁrst acquaint the reader via the use of Table 7 with some of the assessed tools, which were not covered in the previoussubsection. Then, we present the evaluation results and analyse them.Tables 8 and 9show the results of the benchmark-based evaluation.Theﬁrst table focuses on depicting the tool involved, its operation mode,scanning accuracy and time while also supplies some extra informationrelated to the interpretation of each result. The second table reports theaccuracy of each tool per each vulnerability area covered by thebenchmark.Based on the above results, the best tool with respect to the overallscanning accuracy and time is FindSecurityBugs. In fact, this tool ach-ieves a high accuracy on many vulnerability areas. This might be regar-ded as unexpected as this tool only includes a small set of security rules.However, it can be well justiﬁed by the fact that this tool operatesdirectly on the source code so it can indicate with a higher accuracy if acertain issue holds.Similar results were expected for SonarQube. However, this occurredonly for scanning time but not accuracy. This could be due to the fact thatFindSecurityBugs focuses only on Java source code, on which the OWASPbenchmark is based, and is thus more optimal with respect to this pro-gramming language. Thus, as Sonarqube has put equal focus on differentlanguages, some required scanning rules for Java programs might be stillmissing. Sonarqube, though, was able to detect minor security issues, notforeseen in the OWASP benchmark. This led to a reduction in its accuracywhich was bypassed by removing the security rules detecting theseissues.ZAPProxy exhibited a much worse performance in both scanning timeand accuracy. The bad performance in scanning time can be justi ﬁed by the fact that ZAPProxy has to perform a kind of an extensive attack on thewhole application to detect vulnerabilities. In such an attack, eachvulnerability area testing requires executing a test set on each applicationpart. Thus, such an extensive attack takes a considerable amount of timeto execute in contrast to source code inspection that is much faster.This issue was considerably improved, but not still reaching the scaleof source code scanning time, by conﬁguring ZAPProxy to cover only certain vulnerability areas and running it at the user VM level. This led,for instance, to the case where in full scanning but local mode, thescanning time is much better than the remote (partial) scanning modeone.ZapProxy’s bad performance in scanning accuracy can be justi ﬁed as follows: (a) it is harder toﬁnd vulnerabilities when source code is notinspected–this justiﬁes the fact that some vulnerability areas were notcovered at all; (b) it is more difﬁcult to have a high conﬁdence for each vulnerability kind.Based on the results ofTable 8, a trade-off exists between scanningtime and accuracy only for dynamic vulnerability scanning. In fact, byvarying the number of scanning rules, we can reach different levels ofscanning time and accuracy. However, when the scanning is staticallyapplied on the application source code, scanning time is very similar,almost independently from the number of security rules considered. Thisis related to the fact that in all examined static analysis tools the numberof security rules is small, such that varying the partitions of the securityrule set does not signiﬁcantly impact scanning time. Thus, it is advocatedthat static analysis tools should always be used on full scanning mode. Onthe other hand, dynamic scanning should be more focused, mapping tothe need to create suitable scanning proﬁles to cover different kinds of applications or components. Due to the nature of such tools and theworkload they incur on the underlying resources, it is advocated that
such tools are used either remotely and infrequently due to their impacton application performance, or internally, when suf ﬁcient internal re- sources are available to enable the scanning to be normally performed.The results ofTable 9indicate that FindSecurityBugs covers allvulnerability areas. However, this coverage is not always deep. On theother hand, Sonarqube touches just three vulnerability areas and hasgood accuracy performance in only two of them. This signi ﬁes that FindSecurityBugs is recommended, due to its coverage, for use when theapplication is written in Java. While Sonarqube can be exploited forapplications written in different languages. However, in this case,Sonarqube requires to be coupled with another scanning tool to coveradditional vulnerability areas.In the context of dynamic analysis, ZAP covers mainly 5 vulnerabilityTable 6Vulnerability tool assessment benchmarks.
Name DescriptionOWASP Benchmark An open test suite focusing on evaluating the speed, accuracy and coverage of vulnerability detectiontools. This suite adopts the Youden Index, which is astandard way of measuring accuracy for test sets. Itstests are derived from coding patterns in real-worldapplications. WavSep Relies on supplying a vulnerable web application comprising a collection of vulnerable web pages to beused for testing the features, quality and accuracy ofscanning tools. The assessment is performed via a setof test cases which cover different vulnerabilityareas. DVWA This is a PhP/MySQL vulnerable web application that can assist in assessing scanning tool accuracy. Itexhibits some common vulnerabilities which comewith different levels of difﬁculty. Android App VulnerabilityBenchmarks [79]This benchmark repository captures 25 knownAndroid application vulnerabilities. Each benchmarkcaptures a unique vulnerability while maps to a pairof benign and malicious applications. The coveredareas include inter-component communication,storage, system and web. WebGoat It is a deliberately insecure J2EE application developed to teach web application security lessons.Once the application is installed and executed, theuser can go through a set of lessons, each focusing onidentifying a different issue mapping to a differentvulnerability. SQL Injection Framework[80]It can be used to evaluate web scanning andpenetration tools. Covers only the SQL injectionvulnerability area. It can dynamically create a testbedand map it to ideal assessment results. Tools can berun against this testbed to compare their results withthe ideal. 3 aspects can be evaluated: deploymentrequirement, SQL injection coverage, and certainevaluation parameters.
Table 7Additional (static analysis) tools in OWASP benchmark.
Name DescriptionFindSecurityBugs[81]The FindBugs tool [82] enables conducting static analysis toassess Java source code quality. This extension enhancesFindBugs with the capability toﬁnd security bugs. However, it does not enableﬁltering the security bug detection rules.Only explored possibility is to either apply only the securityrules or all bug detection rules, i.e., including those originallyconstituting the rule base of FindBugs.Sonarqube [83] It assesses quality of code implemented in manyprogramming languages. It has a more con ﬁgurable rule base, allowing toﬁlter some security rules, especially thosedetecting minor security issues.K. Kritikos et al. Array 3-4 (2019) 100011
14areas but exhibits good accuracy performance only in two of them.Compared to FindSecurityBugs, it is slightly better in two vulnerabilityareas:command injectionandpath traversalwhile much better incross-site scriptingandSQL injection. This signiﬁes that the agglomeration of these two tools will enable to have a better vulnerability area coverage andthus reach a much higher overall accuracy level. However, somevulnerability areas are not yet deeply covered, including: command in- jection,LDAP injection,path traversal,trust boundary violationandXPath injection. This outlines the need to consider another scanning tool tocomplementarily cover more deeply these areas.All the above observations signify that scanning tools should not beindividually used but orchestrated to reach a suitable vulnerabilitycoverage level. Further, it has been highlighted that there is a need for ascanning tool strategy or workﬂow to enable executing differentvulnerability tools based on the kind, nature and content of respectiveapplications. Finally, it has been indicated that tools might require beingconﬁgured with different rule proﬁles to enable a more focused scanningto cover only those vulnerability areas relevant for an application. Suchrule proﬁling would also guarantee a suitable scanning time withoutoverloading much the scanned application.6. Future work directions&challengesBased on the comparative and empirical evaluation results of thevulnerability scanning tools and databases selected, this section elabo-rates on the consequences of the mainﬁndings in terms of on-going challenges and future work directions. The presentation is separatedbased on the tool kind considered, i.e., a vulnerability DB or tool, in thefollowing two sub-sections.6.1. Vulnerability database challenges6.1.1. InterfacingMost VDBs exhibit just a limited web-search interface. In our opinion,there is a need to go beyond this and offer an API allowing to: (a) posemore advanced query forms; (b) subscribe to certain events (e.g., newvulnerabilities introduction); (c) manage vulnerability information. Suchan API could be quite beneﬁcial to both scanning tools and users. Theformer could retrieve the required information in a sophisticated andprecise manner. Further, they could supply different vulnerabilityreporting levels to users. On the other hand, users would bene ﬁt from the extra information that could be retrieved for a certain vulnerability aswell as from the existence of an interface for introducing or updatingvulnerabilities in case that they concern their own software. Further,such an API could enable to abstract away from the information sourcetechnology and enable the VDB to obtain and integrate information frommultiple, disparate sources.6.1.2. SemanticsWhile in terms of information coverage reasons a relational DBtechnology could be selected for implementing a VDB, we believe thatsemantic technology should be the norm instead due to the followingreasons: (a) a semantic representation can be richer, closer to humanperception, while enabling to better link different information perspec-tives and entities; (b) SPARQL as a query language can be more easy touse and manipulate for evaluating the relations between different vul-nerabilities; (c) semantic technology enables inferencing over the storedinformation which can be beneﬁcial to derive new knowledge that couldtake different forms (see details in next subsection); (d) in case thatdifferent information sources need to be integrated together, semantictechnology can be the most optimal way to achieve this integration.Due to the main beneﬁts that semantic technology brings about, weexpect a proliferation of semantic VDBs in the near future. Currently,only Vulcan offers a semantic VDB. However, this VDB is not rich enoughand coupled with the right rules to allow inferring various knowledgekinds. Further, it seems to manually and not automatically integrateTable 8Scanning time and accuracy benchmark results.
Tool OperationModeAccuracy Time CommentsFindSecurityBugs ONLY_SEC 39.10% 3:06 Only security rules applied FindSecurityBugs FULL 39.10% 3:41 All bug scanning rules applied Sonarqube FULL 10.00% 3:37 Found 23582 vulnerabilities,1176 bugs and15448 codesmells. Sonarqube PART_SEC 19.00% 3:41 Removed 13 security rulescreating falsepositives in twovulnerabilityareas. Thisenabled reachinga 100% accuracyin these two areas.Totalvulnerabilitieswere reduced to242. ZAPProxy FULL_REMOTE 15.65% 12:00:00 Remote scanning with allvulnerabilityscanning plugins.Reached 48%progress and thenthe benchmarkapp went down.Bad accuracy isdue to many falsenegatives. ZAPProxy PART_REMOTE 15.44% 501:40 Reduced version with only the mostappropriatevulnerabilityscanning plugins.Accuracy almostthe same.However, thecommandinjection categorywas additionallycovered. Thismight be due tothe previous modeinterruption,which did notenable to applythe correspondingplugin. ZAPProxy FULL_LOCAL 7.75% 2:42:00 Full scanning mode appliedinternally to thebenchmarkapplication.Scanning timewas quiteaccelerated.However,accuracy becameworse as theperformance onpath traversal andSQL injectionvulnerabilityareas was badwith respect toremote scanning.K. Kritikos et al. Array 3-4 (2019) 100011
15information from multiple sources.6.1.3. External source integrationIn some cases, VDBs attempt to draw information from various in-formation sources but only in the context of typical vulnerabilities. In ouropinion, a VDB should be equipped with the capability to automaticallycollect information from multiple sources that cover both vulnerabilitiesand software bugs. The latter could be drawn from well-known bug re-positories [46] and enable inspecting whether certain application com-ponents are safe to be exploited, unveiling bugs related to softwarevulnerabilities, and highlighting the most secure and reliable componentversions to adopt. The glue between vulnerabilities and bugs could beachieved by using semantic technology, realisable by adopting suitablesemantic models and mapping techniques to integrate information frommultiple, heterogeneous information sources.6.1.4. Vulnerability relation coverageCWE seems to propose a rich vulnerability and threat taxonomy.However, this taxonomy should be extended accordingly to become asemantic security meta-model able to both cover the most importantsecurity concepts and their relations. This coverage along with semanticrules incorporation would then enable inferring further relations be-tween security concepts which would never be acquired by a syntacticmodelling approach. Apart from this, such relations would improve thescanning accuracy as explicated below.6.2. Vulnerability scanning tools6.2.1. Tool supportA good support level already exists for many of the tools examined.However, we see some places for further improvement based on twomain directions: (a) additional support to standards; (b) communityengagement.Theﬁrst direction indicates that, as there are many special-purposestandards, most of them must be supported by the scanning tools so asto become more complete. Further, this promotes interoperability,especially in terms of tool re-use or orchestration, as it will be shown inthe next sub-section. For instance, support to CVE and CVSS can enable tomerge vulnerability reporting results from different scanning tools.However, as some standards seem already prevailing while otherspromising, possibly different priorities must be given to different stan-dards for their adoption. In our opinion, apart from supporting CVE, CWEand CVSS, there is a great need to also support OVAL and SCAP. The ﬁrst can enable a uniform way to specify and address vulnerabilities while thesecond their automatic mitigation. The support to SCAP will be furtherelaborated in a later sub-section.The second direction signiﬁes the need toﬁnd multiple ways via which communities can be engaged in tool usage and enhancement.Apart from the current adoption of well-known code development andsupport practices, the tool providers must think new ways communityengagement can occur: (a) organisation of specialised events to promotethe use of scanning tools and highlight the criticality of addressingapplication vulnerabilities; (b) organisation of conferences, workshopsand competitions to highlight recent vulnerability scanning advance-ments as well as the top tools possibly categorised under differentcompetition areas; (c) supply of benchmarks or scanner selection tools toassist users in evaluating and selecting the right scanners; (d) supply ofinterfaces and mechanisms via which valuable user input (e.g., feedback,plugins) can be provided in a more natural and user-intuitive way.While most tools provide some engagement support, this is notnecessarily the case for academic tools. Such tools, while presented inacademic forums, are not always made publicly available or offered asopen-source. Further, such tools do not supply usual engagement facil-ities like mailing lists and code development portals which hinders theirsustainability and further evolution.6.2.2. Tool orchestrationAs indicated by the evaluation results plus the existence of scanningtools that agglomerate different security tools, there is a high need toorchestrate scanning tools for various reasons. First, as their vulnerabilitycoverage and extent must be enhanced. We have already observed thatthere exist tools like OpenVAS and OpenSCAP which exhibit a very nicevulnerability coverage. However, due to the their current focus, thesetools’detection recall is not high, especially in terms of web applicationvulnerabilities. We have recently experienced this when using OpenVASin the OWASP benchmark where OpenVAS was not able to cover alloperation space parts of the benchmark application. This indicates thattools like OpenVAS must be complemented with traditional web appli-cation vulnerability scanners.Second, as witnessed by the empirical evaluation, even web appli-cation scanners are not so good and must be complemented by sourcecode analysis tools. If fact, as advocated in AWS-Vuln, extra tools (e.g.,antivirus) apart from vulnerability scanners are needed. Thus, it isactually advocated that there is a need to orchestrate a great number ofdifferent tool kinds to achieve the best possible coverage against thewhole application system in a continuous manner. Only in this way,applications can be permanently and fully protected during their wholelifetime as both applications, their components plus penetration methodsand techniques continuously evolve over time.Third, while the orchestration of scanning tools is exhibited in themarket, it is not perfect. In particular, such an orchestration does notconsider various conﬂicting factors, including: the complementarity ofthe tools, their integration level, the user requirements and preferences
and the application kinds to be addressed. By focusing on user re-quirements, we see a trade-off between the following properties: scan-ning time, accuracy and overhead. Traditionally, there is an usual trade-off between time and accuracy. Further, as accuracy can lead to a quiteintensive processing for retrieving all possible vulnerabilities, it can alsoaffect the scanning overhead in terms of resources needed to support thisscanning. This is especially true for the internal scanning mode asprecious application resources can be stolen by the respective scanner.We should also not ignore the scanning frequency which could beTable 9Scanning accuracy benchmark results per vulnerability area.
VulnerabilityArea Find SecurityBugs Sonarqube [Full] Sonarqube [PART] ZapProxy [FullRemote] ZapProxy [PartialRemote] ZapProxy [FullLocal]Command Injection 11.20% 0.00% 0.00% 0.00% 13.49% 12.70%Cross-Site Scripting 37.32% 0.00% 0.00% 55.69% 55.69% 8.13%Insecure Cookie 100% 55.56% 100.00% 55.56% 55.56% 55.56%LDAP Injection 15.63% 0.00% 0.00% 0.00% 0.00% 0.00%Path Traversal 9.57% 0.00% 0.00% 11.28% 11.28% 1.50%SQL Injection 9.48% 5.53% 5.53% 49.63% 33.82% 7.35%Trust Boundary Violation 18.60% 0.00% 0.00% 0.00% 0.00% 0.00%Weak Encryption Algorithm 54.31% 50.77% 100.00% 0.00% 0.00% 0.00%Weak Hash Algorithm 68.99% 0.00% 0.00% 0.00% 0.00% 0.00%Weak Random Number 100% 0.00% 0.00% 0.00% 0.00% 0.00%XPath Injection 5% 0.00% 0.00% 0.00% 0.00% 0.00%K. Kritikos et al. Array 3-4 (2019) 100011
16coupled with the scanning overhead and mode. For example, it might notbe suitable to frequently execute a heavy scanner in an internal mode.However, if this scanner was used externally, it could be exploitedfrequently.To address the suitable orchestration of vulnerability scanning soft-ware, we believe that the following directions should be followed:1. There is a need for complete vulnerability benchmarks that focus onevaluating scanners against different application kinds to betterexplore their complementarity. These benchmarks must also employsuitable scanning accuracy measures and be constructed by consid-ering the ideal vulnerability coverage per each application kind;2. There is a need for dynamically agglomerating vulnerability scannersaccording to the current context. Such a dynamic approach wouldenable addressing both the evolution of applications, of their re-quirements and of the components that comprise their system. Itwould also enable checking the current application context and thecurrent ways interference can be achieved to perform the requiredvulnerability scanning by using the best possible tool agglomeration.For instance, we could sense that the application is under a heavy loadsuch that we might attempt to either postpone its vulnerabilityscanning or perform it in a very lightweight manner. As anotherexample, if there is a great potential for application penetration, itmight be decided to perform a full scan, no matter what is the currentapplication workload.3. There is a need for a proper agglomeration strategy encompassing theappropriate use of the right tools of the right kind at the rightmoment. For instance, vulnerability scanning might be decided not tobe performed in conjunction with application protection as a veryheavy load could be put on the application, especially as protectiontesting is performed in an intrusive mode. Further, it could be decidedthat source code analysis could be performed each time the applica-tion is modiﬁed and in random moments in case we need to detectunexpected and irregular modiﬁcations in the application source orbinary code.4. In any case, we believe that vulnerability scanning should start fromthe very beginning with the checking of the application source codeand the images on which it can be deployed. The latter is actuallyadvocated by AWS-Vuln and Clair which indicate that images, nomatter who creates them, usually incorporate vulnerabilities thatcould be due to using old software or miscon ﬁguring system com- ponents (e.g., the OS).6.2.3. InferencingApart from the capability to reason about the vulnerabilities that canbe exhibited by component agglomerations, the following forms of extravulnerability-related knowledge should be captured [ 11]: (a) discovery of relations between vulnerabilities; (b) discovery of new vulnerabilities.Theﬁrst form enables producing a suitable mitigation plan, focusingonﬁxing core vulnerabilities. As such, vulnerabilities are addressed attheir very root before they propagate to other vulnerabilities, thusmaking the whole system highly vulnerable. To achieve the production ofthis knowledge kind, there is a need to employ and enhance catego-risations like CWE to check possible relations between vulnerabilitieswhich need to be coupled with knowledge about what constitutes theapplication system and how vulnerabilities can be propagated.The second form can enhance a tool ’s capability to discover new vulnerabilities and thus extend its current coverage. This can also enablethe IT world to beneﬁt from such a discovery to more rapidly react to newvulnerabilities before becoming exploitable via the development ofrespective code by adversaries. A tool, which exhibits such functionality,would become immediately the most utilised one as it would be adoptedby most systems in the IT world. To support new vulnerability infer-encing, there is a need to incorporate (semantic) rules attempting todeduce the existence of a vulnerability based on security facts or in-cidents. Such rules could be, for instance, derived by employing eventpattern mining techniques over security logs. While this is a practicecurrently followed in a manual manner by well-known software pro-viders, it could be an added-value to automate and integrate it in thecloud application management system. This automatic rule productionwould reduce detection costs and accelerate application evolution to-wards resolving the vulnerabilities detected. By also employing anapproach towards properly identifying and publishing vulnerabilities,the community will beneﬁt via: (a) the reduction in effort and time invulnerability publishing; (b) the rapid addressing of new vulnerabilities.6.2.4. MitigationThe vulnerability world is dynamically evolving constantly such thatnew vulnerabilities are detected each day. As such, a user is usually facedwith a great number of vulnerabilities inferred just for a single applica-tion. While a prioritisation of vulnerabilities based on their risk couldenable the user to focus more on the most critical ones, we believe thatthis should be complemented with the capability for automatic vulner-ability mitigation. The latter can enable the user to be burdened only bythose vulnerabilities still critical and not automatically solvable.The way to achieve this is to follow a twofold approach. First, anapproach based on SCAP must be followed via which vulnerabilities can
be automatically mitigated. This requires vulnerabilities to be coupledwith mitigation speciﬁcations which can be automatically executed byscanning tools once the vulnerabilities are detected with a high con ﬁ- dence. This requires a great community effort, similar to that devotedalready for OpenSCAP, involving major software providers and users.The main aim is to produce a global repository for vulnerability mitiga-tion, exploitable by any scanning tool.While, as indicated in a previous direction, proper engagement of allthese providers needs to be achieved, the existence of such repository isnot sufﬁcient until two extra correlated issues are addressed. First, avulnerability might not be always mitigated just with one piece of soft-ware or script. In fact, in different circumstances, a vulnerability might behandled by multiple mitigation actions. The selection of such actionscould depend on the current situation and especially the way the appli-cation is conﬁgured. As such, mitigations need to be assorted with de-scriptions about their pre- and post-conditions to allow vulnerabilityscanning tools to more optimally select the best possible mitigationalternative. Second, in some cases, a vulnerability might not be certain toexist. This is possible, e.g., in the context of external scanning where theexistence of a vulnerability can be inferred based on a respective con ﬁ- dence interval. This creates the need to both increase the con ﬁdence interval to a level acceptable for reaction as well as couple the vulnera-bility mitigation rules with conditions over this interval to avoid per-forming unnecessary mitigation actions. Such actions might lead toincreased costs and undesirable application interruptions. Thus, theyneed to be kept to the minimum and performed only when needed.6.2.5. Architecture&APIsThe evaluation results indicated that different tools employ differentarchitectures in vulnerability scanning, i.e., the standalone and client-service ones. In our opinion, both architectures are valid and must besupplied by a tool to cover all possible users and the diversi ﬁcation of their requirements. This can certainly improve a tool ’s applicability. It can also enable it to scale well to cover scanning big applications.Scanning tools also need to be accompanied with an API due to the greatbeneﬁts it offers, including: (a) ability to integrate the tool in a stand-ardised way with other tools with similar or complementary function-ality; (b) ability to abstract away from technical speci ﬁcities and enable a clearer and more user-intuitive tool usage and administration; (c) abilityto interface with a tool to create suitable visual vulnerability scanningand risk assessment UIs, enabling users to better browse and inspect in aclearer and user-intuitive way the scanning results.All such abilities would lead to the ability to integrate scanning toolsinto uniﬁed compositions that can better cover all possible applicationvulnerabilities. Thus, the use of APIs is more than recommended to toolK. Kritikos et al. Array 3-4 (2019) 100011
17providers which should move away from the previous practice ofproviding not easily integrable tools which are not even understandableby users in some cases.Apart from abstracting over different vulnerability scanners, the useof APIs could lead to Vulnerability Assessment as a Service scenarios. Insuch scenarios, users are not burdened any more with tool maintenance,administration and conﬁguration. They can also beneﬁt from this ser- vice’sﬂexible pricing model to reduce costs by using the scanner onlywhen needed. Such a service can be also easily con ﬁgurable by users based on their requirements by abstracting from any tool speci ﬁcities. Such a conﬁguration could come via proﬁles playing the role of tem- plates, further evolvable by users based on their requirements. Such aservice could also encompass an extra-priced tool agglomeration feature,enabling to optimally satisfy user requirements.6.2.6. Risk assessmentMost tools usually report a set of vulnerabilities assigned to a certainrisk based on either a manual mapping approach or the existence of alreadymodelled mapping knowledge in form of CVSS descriptions. In both cases,however, we see just the reporting of individual vulnerability risks with nocapability to aggregate them to deduce the overall application risk. Thelatter capability is already exhibited by some prototypes [ 84,85]b u tn o tt o the full possible extent covering the whole application system. As such, weexpect that research should advance the current risk computation algo-rithms to cover the whole application system while tool providers shouldborrow the main research results and incorporate them in their tools foradvanced reporting reasons. As such, this advanced reporting capabilitywill increase the tool’s added-value can be beneﬁcial for users as: (a) knowledge about the whole application risk can enable selecting the mostsuitable mitigation strategy; (b) a deeper root cause analysis could be fol-lowed to unveil further vulnerabilities and better resolve them.6.2.7. Composite vulnerability detectionWhile most tools focus on each application component individually,this is not sufﬁcient as illegal behaviour could be detected also incomponent interactions. As such, there is a need to consider the appli-cation topology to have a holistic view about the whole application, itsstructure and how its components interact with each other along withtheir dependencies (e.g., hosting, communication). Further, we shouldfocus on both the current topology description and its extension derivedvia reasoning. For instance, while examining a component that might behosted on a certain container, we should not focus just on both of thembut also on other components not currently covered by the topology, suchas improper or non-initiated system processes. Such information couldthen need to be exploited towards constructing suitable vulnerabilitydetection rules focusing on scanning the interactions between differentapplication components.The above direction requires to constantly observe the application ’s current, effective topology and to reason about its extension. Such arequirement could only be fulﬁlled by the proper cooperation betweenthe scanning tool and the (cloud) application management system. Thelatter, in particular, should have the right, standardised interface viawhich the tool could obtain the extended application topology. Further, itshould possess the right abilities and sensing mechanisms to sense theextra components involved in a certain system to correlate them to thecurrent application topology.Going beyond topology models, there can be cases where differentuser applications communicate to each other. Thus, one serious vulner-ability in one application might have the risk to be propagated toanother. However, by knowing the topology models of applications thatcommunicate to each other, the scanning process can focus on alsochecking cross-application vulnerabilities (e.g., side effects of wrongtransactions initiated by one, already infected application and handledby another application). This is a novel research direction, not consideredbefore in the literature.However, this direction might be hard to address as: (a) theknowledge about which application pairs communicate with each othermight not be given by the user. In fact, it is possible that each applicationis handled by a different management system or different instances ofsuch a system. As such, it should be the vulnerability assessment systemthat must infer this knowledge; (b) actual cross-application vulnerabil-ities might be hard to be deﬁned and detected while requiring specialknowledge to be given by the user (e.g., what is a wrong transaction and
how someone can detect it).The way this can be resolved is twofold. First, by usually inspectingeach application from the pair in an individual manner. This relies on therationale that each application could be considered as an end-user for theother. As such, this end-user could be considered as one source of vul-nerabilities, usually checked by data validation testing and other scan-ning technique kinds. Second, by allowing users to supply their ownchecks that focus on detecting those inconsistent system states whichcould make the whole system or an application as vulnerable. This can beconsidered as a kind of user-speciﬁc vulnerability detection, enabling ascanning tool to go beyond the detection of known vulnerabilities to-wards application/domain-speciﬁc ones. This would certainly contributeto a better tool completeness and suitability. Coupled also with the abilityto deﬁne certain mitigation actions when anticipating such in-consistencies would make the vulnerability scanning tool a full, advancedapplication security protection software, further enhancing its added-value and applicability.7. ConclusionsMalware and antivirus software is widely adopted by both organisa-tions and individuals due to the continuous and, in many cases, sophis-ticated risk mitigation support that it features. However, such softwaretakes a reactive approach in dealing with security issues. On the otherhand, while vulnerability scanning tools follow a proactive approach byidentifying those places in the application system that need improvementto avoid security issues from happening, they are not widely used inpractice. Further, even when decided to be adopted, the great tool di-versity and varied coverage makes it hard for a practitioner to choose theright vulnerability scanning tool. To this end, this article offers thefollowing contributions: (a) it guides practitioners towards making aninformed decision about which vulnerability scanning tools and data-bases to select for their applications. Such a guidance is supplied througha comparative evaluation approach that relies on a two-level, hierar-chical comparison criteria framework for both scanning tools and data-bases. The evaluation results supplied unveil which are the best scanningtools and databases per each criterion, category of criteria and in overall;(b) the framework could be adapted to include different weights(currently equal weights are assumed) on the respective criteria andcategories of criteria highlighting their relative importance. This couldthen enable to modify the evaluation results to make them ﬁt to the current usage context; (c) being one of the survey goals, the article provesthat vulnerability scanning tool orchestration can enable to reach ahigher vulnerability detection coverage; (d) by moving to the academicside, the evaluation results also highlight the exact places for toolimprovement, presented in the form of certain challenges which have notyet been confronted in research; (e)ﬁnally, this article explains in which application lifecycle places vulnerability scanning can be performed andsupplies valuable insights towards better securing cloud applications.Declaration of Competing InterestThe authors declare no conﬂict of interest.AcknowledgementsWe thankfully acknowledge the support of the UNICORN (GrantAgreement no. 731846) and CONCORDIA (Grant Agreement no.830927) H2020 European projects.K. Kritikos et al. Array 3-4 (2019) 100011
18Appendix A. OWASP TaxonomyOWASP has proposed a vulnerability tool categorisation/taxonomy which comprises three levels. At the ﬁrst level, the two top categories of: (a) web application vulnerability detectiontools and (b)web application protectiontools (e.g., malware and antivirus tools) exist. The former tools are then cat- egorised at the second level into: (i) threat modellingtools, (ii)source code analysistools (SAST), (iii)vulnerability scanningtools, (iv)interactive application security testingtools (IAST), and (v)penetration testingtools. Penetration testing tools are further classi ﬁed based on the following partitions: information gatheringtools,conﬁguration management testingtools,authentication testingtools,session management testingtools,authorisation testingtools,data validation testingtools,denial of service testingtools,web services testingtools,Ajax testingtools,HTTP trafﬁc monitoring,encoders/decoders, andweb testing frameworks.Appendix B. Approach for Evaluating Vulnerability Coverage of Scanning ToolsIn order to evaluate the coverage of each vulnerability scanning tool, we have relied on a three-step approach. This approach had the rationale thatcoverage means the actual percentage of the different kinds of vulnerabilities that can be detected by a certain tool.In this respect, theﬁrst approach step involved the proper identi ﬁcation of the different vulnerability kinds that can be detected. Towards realising this step, we have made an investigation of different frameworks that might be available in the literature. Such an investigation ended-up in selecti ng the framework in sectoolmarket. com which comprises a vast amount of the most frequent web application vulnerabilities categories. The focus of thisframework is on web application with the rationale that usually the focus is mainly on this kind of vulnerable object and not all possible ones. In fact, we can actually deduce that there is a lack of frameworks that attempt to categorise vulnerabilities beyond this object kind.As such, based on this framework, 33 vulnerability categories were identi ﬁed, for which the deﬁnition can be inspected in.
1These categories are shown in theﬁrst column ofTable 10.Once the right set of vulnerability categories was identi ﬁed, the second step of the followed approach involved the assessment of each vulnerability scanning tool considered based on whether it covers each of the categories identi ﬁed. Towards this goal, we have relied both: (a) on the actual ﬁndings
2
from a previously conducted research that is re ﬂected in sectoolmarket. com about the coverage of a certain set of open-source vulnerability scanners as well as (b) on an individual evaluation, conducted by us, of those scanners which were not included in the aforementioned evaluated set.
Table 10Mapping of scanning tools to the vulnerability kinds they cover.
Vulnerability Kind Vulnerability Scanning ToolSQL Injection WGrabber, Nikto2, Vega, Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAP, ClairTime-Based SQL Injection Grabber, Vega, Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAPServer-Side Javascript Injection OpenVAS, arachni, OpenSCAPReﬂected Cross-Site Scripting Grabber, Vega, Wapiti, Xenotic XSS, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAP, ClairPersisted Cross-Site Scripting Wapiti, Xenotic XSS, ZAP, OpenVAS, w3af, IronWASP, OpenSCAPDOM-based Cross-Site Scripting Xenotic XSS, OpenVAS, w3af, arachni, IronWASP, OpenSCAPJSON Hijacking OpenVAS, OpenSCAPLocal File Inclusion Grabber, Nikto, Vega, Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAP, ClairRemote File Inclusion Wapiti, Nikto, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAPCommand Injection Nikto2, Vega, Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAP, ClairUnrestricted File Upload Wapiti, OpenVAS, w3af, arachni, OpenSCAPOpen Redirect Vega, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAPCRLF Injection Vega, Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAP, ClairLDAP Injection Wapiti, ZAP, OpenVAS, w3af, arachni, IronWASP, OpenSCAPXPath Injection Wapiti, ZAP, w3af, arachni, IronWASPEmail Injection OpenVAS, w3af, OpenSCAPServer-Side Includes ZAP, OpenVAS, w3af, IronWASP, OpenSCAPFormat String Attack Vega, OpenVAS, w3af, OpenSCAPCode Injection ZAP, OpenVAS, arachni, IronWASP, OpenSCAPXML Injection Vega, OpenVAS, arachni, OpenSCAPEL Injection IronWASP, OpenSCAPBuffer Overﬂow OpenVAS, w3af, OpenSCAP, ClairInteger Overﬂow Vega, OpenVAS, OpenSCAP, ClairSource Code Disclosure Vega, OpenVAS, w3af, arachni, OpenSCAPBackup Files Grabber, Wapiti, ZAP, OpenVAS, w3af, arachni, OpenSCAPPadding Oracle OpenVAS, OpenSCAPAuthentication Bypass Nikto2, ZAP, OpenVAS, w3af, OpenSCAP, ClairPrivilege Escalation OpenVAS, OpenSCAP, ClairXXE Injection Wapiti, OpenVAS, arachni, OpenSCAPWeak Session Identiﬁer w3af, OpenVAS, IronWASP, OpenSCAPSession Fixation ZAP, OpenVAS, arachni, IronWASP, OpenSCAPCross-Site Resource Forgery ZAP, OpenVAS, w3af, arachni, OpenSCAP, ClairApplication Denial of Service Nikto2, Wapiti, OpenVAS, w3af, OpenSCAP, Clair
This second step resulted in the mapping of each vulnerability category into a set of vulnerability scanning tools that cover it which is re ﬂected in the second column ofTable 10. Based on the results of this step, the last approach step involved the calculation of the actual coverage percentage of this toolby computing the division between the vulnerability categories covered and all possible ones. This coverage percentage was then mapped to a
1http://www.sectoolmarket.com/audit-features-comparison-uni ﬁed-list.html#Glossary.
2http://www.sectoolmarket.com/price-and-feature-comparison-of-web-application-scanners-opensource-list.html.K. Kritikos et al. Array 3-4 (2019) 100011
19qualitative scale comprising 6 main values according to the following rule set:/C15percentage<¼16:66%→coverage¼“Very Low” /C1516:66%<percentage<¼33:33%→coverage¼“Low” /C1533:33%<percentage<¼50:00%→coverage¼“Medium” /C1550%<percentage<¼66:66%→coverage¼“Good” /C1566:66%<percentage<¼83:33%→coverage¼“High” /C1583:33%<percentage<¼100:00%→coverage¼“Very High”Theﬁnal result is depicted inTable 11
which is a subset ofTable 4focusing solely on theVuln. Coveragefunctional criterion.
Table 11Quantitative&Qualitative Vulnerability Coverage.
Tool/Approach Percentage CoverageGrabber 15.15% Very LowLismero 93.93% Very HighNikto2 18.18% LowVega 33.33% LowWapiti 42.42% MediumOWASP Xenotic XSS 9.09% Very LowOWASP ZAP 51.51% GoodOpenVAS 93.93% Very HighArachni 60.60% GoodIronWASP 51.51% Goodw3af 69.69% HighOpenSCAP 96.96% Very HighClair 33.33% LowVulcan % Very LowHPI-Vuln 93.93% Very HighUC-OPS 93.93% Very HighAWS-Vuln 93.93% Very High
It might be observed that some scanning tools have the same coverage. This is due to the fact that all these tools rely on OpenVAS. As we were notable to assess them easily over their actual coverage, we have considered that their accuracy could not be less than that of the sub-tool that they explo it. In this sense, as the coverage of OpenVAS is the best possible, mapping to the top coverage partition, our consideration is precise according to the sca le that has been adopted. Further, we should also note that we were not able to assess Vulcan as its code was not available. However, based on its respectivedocumentation, it relies on a certain limited scanning framework which is expected to have a very low coverage. So, our estimation should be correcthere.References
[1]Chromy JR. Encyclopedia of survey research methods. Thousand Oaks: SAGEPublications, Inc.; 2019.[2] Common vulnerabilities and exposures. 2019. https://cve.mitre.org. [3]Tsipenyuk K, Chess B, McGraw G. Seven pernicious kingdoms: a taxonomy ofsoftware security errors. IEEE Secur Priv 2005;3:81 –4. [4] OWASP top 10. OWASP_Top_Ten_Project; 2019. https://www.owasp.org/index.php /Category.[5] National vulnerability database. 2019. https://nvd.nist.gov/. [6] Common vulnerability scoring system. 2019. https://www.ﬁrst.org/cvss/. [7] Common weakness enumeration. 2019. https://cwe.mitre.org/. [8] Open source vulnerability database. 2019. https://en.wikipedia.org/wiki/Open_ Source_Vulnerability_Database . [9] Hasso-plattner vulnerability database. 2019. https://hpi-vdb.de/vulndb/. [10]Cheng F, Roschke S, Schuppenies R, Meinel C. Remodeling vulnerabilityinformation. In: Bao F, Yung M, Lin D, Jing J, editors. Information security andcryptology. Berlin, Heidelberg: Springer Berlin Heidelberg; 2010. p. 324 –36. [11]Kamongi P, Kotikela S, Kavi K, Gomathisankaran M, Singhal A. VULCAN:vulnerability assessment framework for cloud computing. In: Sere. Washington, DC,USA: IEEE Computer Society; 2013. p. 218 –26. [12] Rapid7 vulnerability and exploit database. 2019. https://www.rapid7.com/db. [13] Trusted vulnerability&threat intelligence database. 2019. https://vfeed.io/. [14] Redhat embedded vulnerability detector. 2019. https://access.redhat.com/labs info/jevd.[15] Securityfocus vulnerabilities. 2019. https://www.securityfocus.com/bid . [16] Vuldb. 2019.https://vuldb.com/. [17] Securitytracker. 2019.https://securitytracker.com/. [18] Zeroday published advisories. 2019. https://www.zerodayinitiative.com/adviso ries/published/.[19] Offensive security ’s exploit database archive. 2019. https://www.exploit-db.com/ . [20] NCCIC. 2019.https://ics-cert.us-cert.gov/. [21] Japan vulnerability notes. 2019. https://jvn.jp/en/. [22] Auscert security bulletins. 2019. https://www.auscert.org.au/bulletins/ .[23]CERT-EU security advisories. 2019. https://cert.europa.eu/cert/newsletter/en/latest_Security\’\%20Bulletins_.html. [24] Common platform enumeration. 2019. https://cpe.mitre.org. [25] Common attack pattern enumeration and classi ﬁcation. 2019.https://capec.m itre.org/.[26] Open vulnerability and assessment language. 2019. https://oval.mitre.org/. [27] Web application security Consortium project page. 2019. http://projects.webappse c.org/w/page/13246927/FrontPage . [28] OWASP taxonomy. Tools_Categories; 2019. https://www.owasp.org/index.php /Category.[29] Grabber security and vulnerability analysis. 2019. https://github.com/am oldp/Grabber-Security-and-Vulnerability-Analysis- . [30] Golismero group. 2019. http://www.golismero.com/. [31] Nikto2. 2019.https://cirt.net/Nikto2. [32] Vega vulnerability scanner. 2019. https://subgraph.com/vega/. [33] Wapiti vulnerability scanner. 2019. http://wapiti.sourceforge.net/ . [34] OWASP xenotix XSS exploit framework. 2019. https://www.owasp.org/index.php/ OWASP_Xenotix_XSS_Exploit_Framework . [35] OWASP zed attack proxy project. 2019. https://www.owasp.org/index.php/OWA SP_Zed_Attack_Proxy_Project. [36] Openvas vulnerability scanner. 2019. http://www.openvas.org/. [37] Arachni web application security scanner. 2019. http://www.arachni-sca nner.com/.[38] IronWASP. 2019.https://ironwasp.org/. [39] w3af web application attack and audit framework. 2019. http://w3af.org/. [40] OpenSCAP. 2019.https://www.open-scap.org/. [41]Scherf T. Openscap: security compliance with openscap. 2015. Admin . [42] NIST security content automation protocol. 2019. https://csrc.nist.gov/projects/sec urity-content-automation-protocol/ . [43] Common conﬁguration enumeration. 2019. https://cce.mitre.org/about/index.h tml.[44] Xccdf - the extensible conﬁguration checklist description format. 2019. https://csrc .nist.gov/projects/security-content-automation-protocol/scap-speci ﬁcations/xccdf. [45] Vulnerability static analysis for containers. 2019. https://github.com/coreos/clair . [46]Torkura KA, Cheng F, Meinel C. Aggregating vulnerability information for proactivecloud vulnerability assessment. J Internet Technol Secur Trans 2015;4:387 –95.K. Kritikos et al. Array 3-4 (2019) 100011
20[47]Schwarzkopf R, Schmidt M, Strack C, Martin S, Freisleben B. Increasing virtualmachine security in cloud environments. J Cloud Comput: Adv Syst Appl 2012;1:12 . [48]Balduzzi M, Zaddach J, Balzarotti D, Kirda E, Loureiro S. A security analysis ofamazon’s elastic compute cloud service. In: SAC, ACM; 2012. p. 1427 –34. Trento, Italy.[49] Nessus professional. 2019. https://www.tenable.com/products/nessus/nessus-professional.[50] Javascript lint. 2019.http://www.javascriptlint.com/ . [51] PHP-sat. 2019.http://program-transformation.org/PHP/PhpSat . [52] Nmap. 2019.https://nmap.org/. [53] SSLScan. 2019.https://github.com/DinoTools/sslscan . [54] OWASP DirBuster project. OWASP_DirBuster_Project; 2019. https://www.owasp .org/index.php/Category. [55] JBroFuzz project. 2019. https://www.owasp.org/index.php/JBroFuzz . [56] Greenbone security assistant. 2019. https://github.com/greenbone/gsa . [57] ike-scan. 2019.https://github.com/royhills/ike-scan . [58] snmpwalk. 2019.http://net-snmp.sourceforge.net/tutorial/tutorial-5/commands/snmpwalk.html.[59] thc-amap. 2019.https://www.thc.org/thc-amap/ . [60] Ldapsearch - LDAP search tool. 2019. http://www.openldap.org/software//ma n.cgi?query¼ldapsearch&apropos¼0&. [61] Security local auditing daemon. 2019. http://www.openvas.org/compendi um/security-local-auditing-daemon.html . [62] tOVAL Interpreter. 2019. https://sourceforge.net/projects/ovaldi/ . [63] Pnscan - a parallell network scanner. 2019. https://github.com/ptrrkssn/pnscan . [64] portbunny. 2019.http://www.recurity.de/portbunny/portbunny.shtml . [65] WiHawk tool. 2019.https://github.com/Anamika21/WiHawk .[66] XMLCHOR xpath injection exploitation tool. 2019. https://github.com/Harshal35/ XMLCHOR.[67] IronSAP. 2019.https://github.com/prasanna-in/IronSAP . [68] SSLSecurityChecker - IronWASP module. 2019. https://github.com/GDSSecurity /SSLSecurityChecker.[69] OWASP Skanda SSRF exploitation framework. 2019. https://www.owasp.org/ind ex.php/OWASP_Skanda_SSRF_Exploitation_Framework . [70] Hawas. 2019.https://github.com/lavakumar/hawas . [71] CSRF PoC generator. 2019. https://github.com/jayeshchauhan/csrf_poc_generator . [72] ClamAV antivirus engine. 2019. https://www.clamav.net/. [73] Chk-root-kit. 2019.
http://www.chkrootkit.org/. [74] Rkhunter - rootkit hunter project. 2019. http://rkhunter.sourceforge.net/ . [75] Sysinternals utilities index. 2019. https://docs.microsoft.com/el-gr/sysinternals/d ownloads/.[76] John the Ripper password cracker. 2019. http://openwall.com/john/. [77] Extundelete - recover deleted ﬁles. 2019.http://extundelete.sourceforge.net/ . [78] WhatWeb web scanner. 2019. https://github.com/urbanadventurer/WhatWeb . [79]Mitra J, Ranganath V. Ghera: a repository of android app vulnerability benchmarks,CoRR abs/1708. 2017, 02380 . [80]Tajpour A, Ibrahim S. A framework for evaluation of SQL injection detection andprevention tools. Int J Info Commun Technol Res 2013;5:55 –62. [81] Find security bugs scanner. 2019. https://ﬁnd-sec-bugs.github.io. [82] Find bugs scanner. 2019. http://findbugs.sourceforge.net/ . [83] Sonarqube scanner. 2019. https://www.sonarqube.org. [84]Li HC, Liang PH, Yang JM, Chen SJ. Analysis on cloud-based security vulnerabilityassessment. In: Icebe. IEEE Computer Society; 2010. p. 490 –4. [85]Saripalli P, Walters B. QUIRC: a quantitative impact and risk assessment frameworkfor cloud security. In: Cloud. IEEE Computer Society; 2010. p. 280 –8.K. Kritikos et al. Array 3-4 (2019) 100011
21