Assessing the performance of YOLOv5 algorithm for detecting volunteercotton plants in cornﬁelds at three different growth stages
Pappu Kumar Yadava,⁎,J .A l e xT h o m a s s o nb, Stephen W. Searcya, Robert G. Hardina, Ulisses Braga-Netoc, Sorin C. Popescu
d, Daniel E. Martine, Roberto Rodriguezf, Karem Mezag, Juan Encisoa, Jorge Solórzano Diaz
h,T i a n y iW a n gi
aDepartment of Biological & Agricultural Engineering, Texas A&M University, College Station, TX, United States of America
bDepartment of Agricultural & Biological Engineering, Mississippi State University, Mississippi State, MS, United States of America
cDepartment of Electrical & Computer Engineering, Texas A&M University, College Station, TX, United States of America
dDepartment of Ecology & Conservation Biology, Texas A&M University, College Station, TX, United States of America
eAerial Application Technology Research, U.S.D.A. Agriculture Research Service, College Station, TX, United States of America
fU.S.D.A.- APHIS PPQ S&T, Mission Lab, Edinburg, TX, United States of America
gDepartment of Civil & Environmental Engineering, Utah State University, Logan, UT, United States of America
hTexas A&M AgriLife Research & Extension Center, Weslaco, TX, United States of America
iCollege of Engineering, China Agricultural University, Beijing, China
abstract article info
Article history:Received 2 August 2022Received in revised form 25 November 2022Accepted 26 November 2022Available online 1 December 2022The feral or volunteer cotton (VC) plants when reach the pinhead squaring phase (5 –6l e a fs t a g e )c a na c ta sh o s t s for the boll weevil (Anthonomus grandisL.) pests. The Texas Boll Weevil Eradication Program (TBWEP) employs people to locate and eliminate VC plants growing by the side of roads or ﬁelds with rotation crops but the ones growing in the middle ofﬁelds remain undetected. In this paper, we demonstrate the application of computervision (CV) algorithm based on You Only Look Once version 5 (YOLOv5) for detecting VC plants growing in themiddle of cornﬁelds at three different growth stages (V3, V6 and VT) using unmanned aircraft systems (UAS)remote sensing imagery. All the four variants of YOLOv5 (s, m, l, and x) were used and their performanceswere compared based on classiﬁcation accuracy, mean average precision (mAP) and F1-score. It was foundthat YOLOv5s could detect VC plants with maximum classi ﬁcation accuracy of 98% and mAP of 96.3% at V6 stage of corn while YOLOv5s and YOLOv5m resulted in the lowest classi ﬁcation accuracy of 85% and YOLOv5m and YOLOv5l had the least mAP of 86.5% at VT stage on images of size 416 × 416 pixels. The developed CV algo-rithm has the potential to effectively detect and locate VC plants growing in the middle of corn ﬁelds as well as expedite the management aspects of TBWEP.© 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Boll weevilVolunteer cotton plantComputer visionYOLOv5Unmanned aircraft systems (UAS)Remote sensing
1. IntroductionThe boll weevil (Anthonomus grandisL.) is a serious pest that primar- ily feeds on cotton plants and has cost the U.S. cotton industry morethan 23 billion USD in economic losses since it ﬁrst entered to the U.S. from Mexico in the 1890s (Harden, 2018). The National Boll Weevil Eradication Program (NBWEP) has successfully eradicated the boll wee-vils from major parts of the U.S.; however southern parts of Texas (theLower Rio Grande Valley) remain prone to re-infestation each yeardue to its sub-tropical climatic conditions and proximity to the Mexicoborder (Roming et al., 2021). The sub-tropical climatic conditionsallow cotton plants to grow year-round and therefore the left-overcotton seeds during the harvest from previous season can grow eitherat the edges ofﬁelds or in the middle of rotation crops like corn ( Zea maysL.) and sorghum (Sorghum bicolorL.) (Wang et al., 2022;Yadav et al., 2019;Yadav et al., 2022a, 2022b). These feral or volunteer cotton (VC) plants when reach the pinhead squaring phase (5 –6 leaves) can serve as hosts for the boll weevil pests ( Yadav et al., 2022c). As per the management practices of The Texas Boll Weevil Eradica-tion Foundation (TBWEF), edges of rotation crop ﬁelds are inspected for the presence of VC plants and are eliminated if found, to avoid pestre-infestation in future. However, VC plants growing in the middle ofﬁelds remain undetected as it is practically not possible to detect and lo-cate them in thousands of acres ofﬁelds. Therefore, if any boll weevil isfound to be trapped in the pheromone traps of such ﬁelds, the whole ﬁeld is sprayed with chemicals like Malathion ULV (FYFANON® ULVAG) to kill the pests (National Cotton Council of America, 2012 ). UsuallyArtiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
⁎Corresponding author.E-mail address:pappuyadav@tamu.edu(P.K. Yadav).
https://doi.org/10.1016/j.aiia.2022.11.0052589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/spray capable unmanned aircraft systems (UAS) are used for this pur-pose and the typical spray rate ranges from 0.56 to 1.12 kg/ha (FMCCorporation, 2001). Malathion is classiﬁed as toxicity class III pesticide and therefore can be dangerous to humans when not applied wisely(United States Environmental Protection Agency, 2016 ). Apart from this, excessive spraying of the chemical may kill bene ﬁcial insects and pests in theﬁelds of rotation crops. Therefore, it has become a necessityto detect and locate VC plants growing in the middle of rotation crops.On doing this, a spot-spray capable UAS can precisely spray herbicides(before pinhead squaring phase) and pesticides (after pinhead squaringphase) only at the locations of VC plants. Hence, detecting the VC plantsis crucial for the management aspects of TBWEF because it can speed upthe management practices as well as help minimize the chemical costsassociated with herbicides and pesticides.Detecting VC plants growing in the middle of corn or sorghum ﬁelds is a two-step process i.e., classiﬁcation followed by localization whichnecessarily means object detection task ( Howard et al., 2019;Mustafa et al., 2020;Silwal et al., 2021;Zhao et al., 2019). In our past study, pixel-wise classiﬁcation method was used with classical machine learn-ing algorithms to detect the regions of VC plants in a corn ﬁeld (Yadav et al., 2019). This approach resulted in a classiﬁcation accuracy of around 70% which was far below our expectations and thereforecouldn't be used for practical applications. In another approach byWestbrook et al. (2016), traditional image processing method-linearspectral unmixing was used to detect individual cotton plants at anearly growth stage with fairly good accuracy. In recent past, manydeep learning-based algorithms have been developed and used success-fully for object detection tasks. Many of those algorithms use convolu-tion neural networks (CNNs), some of which are YOLOv3 ( Redmon and Farhadi, 2018), YOLOv5 (Jocher, 2020), Mask R-CNN (He et al., 2017), etc. The ability of CNNs to extract complex features automaticallyfrom images have made them quite popular in classi ﬁcation and detec- tion tasks. This is why they have also been widely used in agriculturalapplications for varieties of detection tasks like in the case of weed de-tection in turfgrass (Yu et al., 2019), weed detection in vegetable crops (Jin et al., 2021), plant seedling detection (Jiang et al., 2019)a n d bunch detection in white grapes (Sozzi et al., 2022). In this paper, we have shown the application of YOLOv5 for detect-ing VC plants growing in the middle of corn ﬁelds by using remote sens- ing multispectral imagery collected by unmanned aircraft systems(UAS).YOLOv5 is a one stage object detection algorithm that was origi-nally released in four different variants (YOLOv5s, YOLOv5m, YOLOv5land YOLOv5x) based on the network depth and number of parameters(Jocher, 2020;Jocher et al., 2021). The letterss, m, l,andxrepresent small, medium, large, and extra-large respectively depending uponthe network depth and parameter size used. The speci
ﬁc objective of this paper is to do a comparative analysis of the performances of allthe four variants of YOLOv5 for detecting VC plants in corn ﬁelds at three different growth stages (V3, V6 and VT) of corn plants. Thisstudy is an extension of our previous studies in which both YOLOv3and YOLOv5 were used successfully in detecting VC plants at a singlegrowth stage of corn plants (Yadav et al., 2022a, 2022b).2. Materials and methods2.1. Experiment sitesThe experiment sites were located at two different corn ﬁelds: one was based in Hidalgo County near Weslaco, Texas (97°56 ′21.538″W, 26°9′49.049″N) while the other was based in Burleson County near Col-lege Station, Texas (96°25′45.9″W, 30°32′07.4″N) (Fig. 1). Two types of soil (Harlingen clay and Raymondville clay loam) are primarily presentat the experiment plot based in Weslaco, TX while Weswood silty clayloam, Yaholaﬁne sandy loam and Belk clay are present at the experi-ment site of Burleson county (USDA-Natural Resources ConservationService, 2020). To mimic the presence of VC plants, 105 cotton seedsof variety Phytogen 350 W3FE (CORTEVA agriscience, Wilmington, Del-aware) were planted in the cornﬁeld of Weslaco. Some of these wereplanted in line with corn plants while the rest were planted in the fur-row middles. Similarly, a total of 180 cotton seeds-90 of the varietyPhytogen 340 W3FE (CORTEVA agriscience, Wilmington, Delaware)and another 90 of the variety Deltapine 1646 B2XF (Bayer AG, Leverku-sen, North Rhine-Westphalia, Germany) were planted at the test ﬁeld in Burleson County.2.2. Image data acquisitionAt the experiment site located in Weslaco, Texas, a three band (RGB:red, green, and blue) FC6310 camera (Shenzhen DJI Sciences and Tech-nologies Ltd., Shenzhen, Guangdong, China) integrated on DJI Phantom4 Pro quadcopter (Shenzhen DJI Sciences and Technologies Ltd.,Shenzhen, Guangdong, China) was used to collect aerial imagery froman altitude of 18.3 m (60 ft) above ground level (AGL). The images ac-quired by the FC6310 camera had a resolution of 5472 × 3648 pixelsand a spatial resolution of 0.5 cm/pixel (0.20 in./pixel). Data were col-lected on April 7, 2020, between 10 a.m. and 2 p.m. Central StandardTime (CST) at 80% sidelap and 75% overlap. The corn plants were atV3 stage when the data were acquired.At the second site located in Burleson County, RedEdge-MX (AgEagleAerial Systems Inc., Wichita, Kansas) camera was mounted on a customUAS (Fig. 2) for aerial data collection. Theﬁrst set of data were collected on May 5, 2021, between 11:00 a.m. and 2:00 p.m. central daylight-saving time (CDT) at an altitude of nearly 4.6 m (15 ft) above groundlevel (AGL) when the corn plants were at V6 growth stage. The secondset of data were collected on May 14, 2022, between 11:00 a.m. and2:00 p.m. CDT from an altitude of 4.6-m (15 ft) AGL when the cornplants had reached the VT stage. The acquired images on both thedays had approximate spatial resolution of 0.34 cm/pixel.2.3. YOLOv5YOLOv5 is theﬁfth version of YOLO series of object detection algo-rithm which was released in June of 2020 ( Jocher, 2020). Just like its predecessors, it is a single stage detector network which makes it fastercompared to other object detection algorithms ( Yan et al., 2021;Zhou et al., 2021). The simple architecture of YOLOv5 can be seen in Fig. 3 which was generated by a neural network visualization software toolNetron version 4.8.1 (Lutz, 2017). The overall architecture comprisesof 25 nodes which are named from model 0tomodel 24. Nodes 0 to 9 (i.e.,model/0tomodel/9) form the backbone network while nodes 10to 23 (i.e.,model/10tomodel/23) represent the neck network and thelast node i.e.,model/24forms the detection network. The last node com-prises of three layers to make detections at three different scales withbounding boxes and conﬁdence scores around the detected objects.The backbone network is comprised of focus, convolution,bottleneckCSP (Cross Stage Partial) and spatial pyramid pooling (SPP)modules. The focus module accepts input images of shape 3 × 640 ×640, where 3 represents the three channels (normally Red, Green, andBlue) while 640 × 640 represent image width and height in pixels.The focus module can process images of sizes other than 640 × 640pixels and can be customized for different numbers of channels aswell. The main objective of the focus module is to enhance the trainingspeed as it makes use of the “hard-swish”activation function. This acti- vation function is a modiﬁed version of the swish activation function, re-placing the sigmoid function with a piecewise-linear “hard”equivalent (eqs.1a n d2)(Howard et al., 2019).swish xðÞ ¼xσxðÞ ¼
x1þe
−xð1Þh−swish xðÞ ¼x
ReLU6 xþ3ðÞ6 ð2ÞP.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
293whereσis the sigmoid function and ReLU6 is a modi ﬁed version of rec- tiﬁed linear unit (ReLU) activation function in which the maximum valueis limited to 6. The bottleneckCSP module enhances the feature extrac-tion process by making use of a residual network, i.e., concatenatingdeeper features with shallow features. The SPP module is used to ensurethat aﬁxed-size feature vector is generated from any different sizefeature maps by using three parallel maxpooling layers.The neck network is used to ensure that detection accuracy of ob-jects is not compromised due to different scales and sizes. It makesuse of a path aggregation network (PANet) to enhance featurepropagation irrespective of scale (Liu et al., 2018). Modules 17, 20 and 23, which are part of the PANet and belong to the Neck network (alsocalled P3, P4 and P5), output three feature maps belonging to objectsat scales of small, medium, and large, respectively. P3 outputs a featuremap of 80 × 80 pixels, while P4 and P5 output feature maps of 40 × 40and 20 × 20, respectively.The detect network is comprised of three detect layers to make de-tections at three scales corresponding to the features output from P3,P4 and P5. This network applies three anchor boxes at each scale onthe three feature maps to output a vector containing information
Fig. 1.Experiment site of two cornﬁelds located in Burleson and Hidalgo counties.
Fig. 2.A customized unmanned aircraft systems (UAS) with on-board computing platform and MicaSense RedEdge-MX multispectral camera.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
294about the class probability, objectness score and predicted boundingbox (BB) coordinates.The original architecture of YOLOv5 was customized to detect a sin-gle class of VC plants and accept input images of shape 3 × 416 × 416 forthis study. YOLOv5 was originally released in four different variants:YOLOv5s, YOLOv5m, YOLOv5l and YOLOv5x based on network depthand number of parameters used. Here, s,m,l,andxrepresent small, me- dium, large, and extra-large respectively. All these four variants wereused in this study.2.4. Performance metrics of YOLOv5The performances of each trained YOLOv5 model were assessed byusing the metrices accuracy, precision (P), recall (R), mean average pre-cision (mAP) and F1-score as calculated in Eqs. 3, 4, 5, 6 and 7.Accuracy¼
TPþTNTPþTNþFPþFNð3ÞP¼
TPTPþFP ð4ÞR¼
TPTPþFN ð5ÞF1−Score¼2/C2
P/C2RPþR ð6ÞmAP¼1n∑k¼Nk¼1APk ð7ÞwhereTPis the number of true positives,TNis the number of true negatives,FPis the number of false positives,FNis the number of false negatives,AP
kis the average precision (AP) of class k(in our case N=1i.e., VC class), andnis the number of thresholds (n=1in our case i.e., 0.50 or 50%). This essentially means, the mAP and AP aresame for a single class case like in this study. The mAP values were cal-culated at 50% threshold value for intersection over union (IoU) mean-ing, all the predicted bounding boxes that resulted in ratios ofoverlapping areas to the union areas with ground truth boundinggreater than 50% were considered and the remaining were discarded(Gan et al., 2021;Padilla et al., 2021;Sharma, 2020;Wu et al., 2021). mAP is a metric based on the area under precision-recall curve (PRC)that is preprocessed to eliminate zig-zag behavior ( Padilla et al., 2021). Therefore, in case of class imbalance where there are more in-stances of one class than the other, mAP is a more reliable and powerfulmetric to analyze performance of a classiﬁer (Padilla et al., 2021;Saito and Rehmsmeier, 2015).2.5. Dataset preparation for training YOLOv5The RGB images collected at experiment plot located in Weslaco, TX,were split into 416 × 416-pixel after which augmentation techniqueswere applied to images that contained at least a VC plant using
Fig. 3.Simpliﬁed version of YOLOv5 architecture generated by Netron visualization software tool.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
295Augmentor Python library (Bloice, 2017) that generated a total of 409 images. To accomplish this,rotate,ﬂip_left_right,zoom_randomand ﬂip_top_bottomoperations were used with probability values of 0.80,0.40, 0.60 and 0.80 respectively as explained by Bloice et al. (2017). Roboﬂow, a web-based platform was used to annotate ground truthbounding boxes for VC plants (Roboﬂow, 2022)b yﬁrst dividing the im- ages in the ratio 16:3:1 for training, validation and test dataset and thenexporting them in YOLOv5 PyTorch format. A total of more than 1750instances for VC class were obtained in the training, validation, andtest datasets (Fig. 4A). It was also found that majority of VC plantswere of 4 to 6 cm in height and 5 cm in width ( Fig. 4A).Theﬁve-band multispectral images collected at experiment site lo-cated in Burleson County, Texas were radiometrically corrected andthen some preprocessing methods were applied using the source codefrom GitHub (GitHub, Inc., San Francisco, CA, U.S.A.) repository ofMicaSense (MicaSenseIncorporated, 2022). This generated RGB images of size 1207 × 923 pixels. Among the generated RGB images, only theones which had VC plants were chosen and then split into size 416 ×416 pixels. This again resulted in many images without VC plants whichwere again discarded. The resulting images with at least one VC plantswere then augmented to generate a total of 387 images at V6 stage and280 at VT stage. Roboﬂow was then used to annotate the ground truth
Fig. 4.VC class instances in training, validation and test datasets used for the YOLOv5 model at V3 (A), V6 (B) and VT (C) growth stages of corn plants.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
296bounding boxes for VC plants. Of these, training, validation, and test im-ages were divided in the ratio 16:3:1. A total of more than 800 instances(Fig. 4B) of VC class were obtained in the datasets for the V6 stage and 400for the VT stage (Fig. 4C). Most of the VC plants at V6 stage of corn were of6 to 8 cm in height and width while at the VT stage, they were mostly ofheight 8 cm and width 8 to 10 cm (Fig. 4B and C). The VC plants at V3 and VT stage of corn were almost equally distributed at all the regions of theimages while they were mostly located at the four corners of the imagesat V6 growth stage of corn (Fig. 4A, B and C).2.6. YOLOv5 trainingThe source code for YOLOv5 was obtained from the GitHub reposi-tory of Ultralytics Inc. (Jocher, 2020). All the four variants of YOLOv5were trained on Tesla P100-PCIE-16 GB (NVIDIA, Santa Clara, CA,U.S.A.) GPU using the Google Colaboratory (Google LLC, Melno Park,CA, U.S.A.) AI platform. Each of them was trained with initial learningrate of 0.01,ﬁnal learning rate of 0.1, momentum of 0.937, weightdecay of 0.0005 for a total of 200 inFig. 5.3. Results3.1. VC detection in aﬁe l dw i t hc o r na tV 3g r o w t hs t a g eThe results for different performance metrics that were obtainedduring the training process using the training and validation datasetsare shown inFig. 6.At V3 stage dataset, the convergence was achieved within the 200training iterations by using transfer learning approach in the trainingprocess i.e., by starting training from the pretrained weights ofYOLOv5 (COCO dataset weights). It was found that the maximum valuesof precision, recall, mAP, and F1-score reached 87%, 91%, 89% and 83%respectively. Similarly, 94% of the VC plants were correctly classi ﬁed while 6% were mistaken for background class i.e., either corn plants orweeds. However, none of the instances from the background classwere mistaken for VC plants (Fig. 7-left). Almost every image had more instances of background class (i.e., corn and weeds) than the VCclass, which means class instances were imbalanced. From past studies,it has been found that classiﬁer's performance can be biased towardsthe majority class, therefore it has been recommended to analyze per-formance based on the PRC as seen in Fig. 6(Saito and Rehmsmeier, 2015). Some of the detection results of VC plants are shown in Fig. 8 where the detected VC plants are enclosed within the red boundingboxes with corresponding conﬁdence scores.3.2. VC detection in aﬁe l dw i t hc o r na tV 6g r o w t hs t a g eInFig. 9, the graphs for different performance metrics can be seenthat were obtained after training YOLOv5s by using datasets belongingto corn plants at V6 growth stage.Due to lack of improvement in performance for 100 iterations, therewas early stopping of the training around the 130th iteration. Thismeans that convergence was achieved well before the 130th iteration.The maximum values of precision, recall, mAP, and F1-score werefound to be 97%, 97%, 96% and 93% respectively. In terms of classi ﬁcation accuracy, 98% of the VC plants were correctly classi ﬁed while only 2% of them were misclassiﬁed as corn or weeds and none of the backgroundclass (i.e., corn and weeds) were misclassi ﬁed as VC plants (Fig. 10-left).
Fig. 5.Workﬂowchart showing entire processes from data acquisition to YOLOv5 training.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
297Fig. 6.Example graphs showing performance metrics along with PR-curve (PRC) obtained after training YOLOv5s using the dataset belonging to the V3 growth st age of corn plants.
Fig. 7.Classiﬁcation results shown in confusion matrix (left) and F1-score calculated over a range of con ﬁdence scores (right) of VC plants after YOLOv5s was trained on dataset belonging to the V3 growth stage of corn plants.
Fig. 8.YOLOv5s detection results of VC plants in a ﬁeld of corn at V3 growth stage.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
298Fig. 9.Example graphs showing performance metrics along with PR-curve (PRC) obtained after training YOLOv5s using the dataset belonging to the V6 growth st age of corn plants.
Fig. 10.Classiﬁcation results shown in confusion matrix (left) and F1-score calculated over a range of con ﬁdence scores (right) of VC plants after YOLOv5s was trained on dataset belonging to the V6 growth stage of corn plants.
Fig. 11.YOLOv5s detection results of VC plants in a ﬁeld of corn at V6 growth stage.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
299Some of the detection results of VC plants are shown in Fig. 11where the detected VC plants are enclosed within the red bounding boxes withtheir corresponding conﬁdence scores.3.3. VC detection in aﬁeld with corn at VT growth stageFig. 12shows the graphs for different performance metrics that wereobtained after training YOLOv5s by using datasets belonging to cornplants at VT growth stage.As in the V6 case, there was early stopping in the training processaround 165th iteration for the VT case too as no improvement wasobserved in the last 100 iterations. This implies that convergencewas achieved well before the 165th iteration. The maximum valuesof precision, recall, mAP, and F1-score were found to be 99%, 86%,8 9 %a n d8 7 %r e s p e c t i v e l y .I nt e r m so fc l a s s i ﬁcation accuracy, 85% of the VC plants were correctly classi ﬁed while 15% of them were misclassiﬁed as corn or weeds and none of the backgroundclass (i.e., corn and weeds) were misclassi ﬁed as VC plants (Fig. 13-left).Some of the detection results of VC plants are shown in Fig. 14where the detected VC plants are enclosed within the red bounding boxes withtheir corresponding conﬁdence scores.
Fig. 12.Example graphs showing performance metrics along with PR-curve (PRC) obtained after training YOLOv5s using the dataset belonging to the VT growth st age of corn plants.
Fig. 13.Classiﬁcation results shown in confusion matrix (left) and F1-score calculated over a range of con ﬁdence scores (right) of VC plants after YOLOv5s was trained on dataset belonging to the VT growth stage of corn plants.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
3003.4. Comparison of detection results at three different growth stages of cornwith all the four variants of Yolov5Table 1shows the comparative results of all the four variants ofYOLOv5 in detecting VC plants growing in ﬁelds with corn plants at three different growth phases i.e., at V3, V6 and VT.It is evident that VC plants were most accurately classi ﬁed from the V6 growth stage corn than the V3 and VT growth stages by using all thefour variants of YOLOv5. In terms of detection accuracy based on mAP at50% IoU, similar results were observed ( Table 1). Between V3 and VT growth stage, VC plants were more accurately classi ﬁed as well as de- tected in the V3 than the VT stage.The average mAP for detecting VC plants in ﬁelds with corn plants at V3, V6 and VT growth stages by using all the four variants of YOLOv5were found to be 87%, 93% and 89% with standard deviations of 0.69,2.13 and 1.3 respectively. Similarly, the average classi ﬁcation accuracy of VC plants inﬁelds with corn plants at V3, V6 and VT growth stageswere found to be 92%, 95% and 87% with standard deviations of 1.7,2 . 4a n d2 . 9r e s p e c t i v e l y .4. DiscussionThe images acquired at V3 growth stage of corn plants were cap-tured by high resolution camera but at the same time they were cap-tured from an altitude that is four times more than the images thatwere captured at V6 and V8 growth stages. This resulted in a lowerspatial resolution (0.5 cm/pixel) than the images captured at theother two growth stages (0.34 cm/pixel). At this stage, there werenot many weeds in the background of images which is why the aver-age classiﬁcation accuracy of 92% was promising ( Fig. 15A). Even though this was an increment of more than 31% than our previousapproach (Yadav et al., 2019), the average classiﬁcation accuracy at the V6 stage was found to be even more at 95% (an increment ofnearly 36%). It was expected that the accuracy might be lower at V6stage as compared to the V3 stage due to larger canopy size of cornplants and relatively more weedsin the background. However, thehigher spatial resolution in the imagery by capturing images atlower altitude (4.6 m /15 ft) resulted in a better classi ﬁcation accu- racy (Table 1,Fig. 15A). The other reason for higher accuracy at V6s t a g ec o u l db ed u et ot h ef a c tt h a tt h ei m a g e sw e r ep r e p r o c e s s e dby using afﬁne transform, unsharp mask and gamma correction(MicaSenseIncorporated, 2022). However, these effects were notenough to improve classiﬁcation accuracy at the VT growth stage(Fig. 15A). We assume this was because at this stage the weeds inthe background were prominent and caused in more misclassi ﬁca- tions than at the V6 stage.In this study, at all the three growth stages of corn, the instancesof VC plants as compared to the instances of background class( i . e . ,c o r nw i t hw e e d s )w a sa l w a y sl e s sw h i c hm e a n st h e r ew a si m -balance between the positive (VC plants) and negative (background)classes. In such cases, performance of any classi ﬁers are found to be biased towards the majority class and therefore one cannot
Fig. 14.YOLOv5s detection results of VC plants in a ﬁeld of corn at VT growth stage.
Table 1Comparison of performance metrics of four variants of YOLOv5 in detecting VC plantsgrowing in the middle ofﬁelds with corn plants at three different growth phases.Corn Classiﬁcation_Acc mAP@50 F1-MaxGrowthYOLOv5 YOLOv5 YOLOv5Phase s m l x s m l x s m l xV3 (early)V6 (mid)VT (tassel)94 92 90 9398 95 92 9585 85 91 8687.9 86.5 86.5 87.496.3 93.5 91.6 92.087.6 90.3 89.0 87.683 82 83 8393 92 91 9187 88 85 88
Fig. 15.(A) Classiﬁcation accuracy distribution with all the four variants of YOLOv5 at all the three growth stages of corn plants. (B) mAP distribution with all the four v ariants of YOLOv5 at all the three growth stages of corn plants.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
301accurately make inference about the performance ( Saito and Rehmsmeier, 2015;Sofaer et al., 2019). In such cases, area under t h eP R Ca n dA P / m A Pa r ef o u n dt ob em o r ee f f e c t i v em e a s u r eo ft h eclassiﬁer performance (Miao and Zhu, 2021). Therefore, to make fair comparison without results being affected by class instance im-balance, mAP values are used from which the highest detection accu-racy was found at the V6 growth stage followed by VT and V2(Fig. 15B). This result was expected because images at V6 and VTstages were of higher spatial resolution as well had undergone pre-processing techniques which were absent in the case of images atV3 stage. The presence of prominence in weeds at VT stage resultedin lower detection accuracy as compared to the V6 stage.Theﬁndings reported in this paper has the potential to be usedfor near real-time VC detection in corn ﬁelds at V6 growth stage (for maximum accuracy) by deploying the trained YOLOv5 model on aspot-spray capable UAS (Yadav et al., 2022c) with a computer vision capability.5. Conclusions and future recommendationsThere is a tradeoff between detection accuracy and the amount ofarea surveyed because to survey larger area, images need to be capturedfrom higher altitude like in the case of V3 stage which eventually de-creases spatial resolution and a compromise in detection accuracy.However, if 87% mAP is not too low then we recommend of surveyingat V3 growth stage for detecting VC plants as it results in detectingthem in a larger area. We also expect the detection accuracy to improveand perhaps surpass that of V6 and VT stage when the images can bepreprocessed using the techniques described in the GitHub repositoryof MicaSense (MicaSenseIncorporated, 2022). This way, the system will be more practically viable and applicable by the personnel involvedin the boll weevil eradication program.CRediT authorship contribution statementPappu Kumar Yadav:Conceptualization, Data curation, Formalanalysis,Investigation,Methodology, Validation,Visualization,Writing – original draft, Writing–review & editing.J. Alex Thomasson: Conceptualization, Data curation, Investigation, Methodology, Formalanalysis, Project administration, Resources, Supervision, Writing – review & editing.Stephen W. Searcy:Formal analysis, Writing–review & editing.Robert G. Hardin:Project administration, Formal analysis, Re-sources, Supervision, Writing –review & editing.Ulisses Braga-Neto: Investigation, Methodology, Formal analysis, Writing –review & editing.Sorin C. Popescu:Formal analysis, Writing–review & editing. Daniel E. Martin:Writing–review & editing.Roberto Rodriguez: Data curation, Formal analysis, Writing –review & editing.Karem Meza:Resources, Writing–review & editing.Juan Enciso:Resources, Writing–review & editing.Jorge Solórzano Diaz:Resources, Writing –review & editing.
Tianyi Wang:Writing–review & editing.Declaration of Competing InterestThe authors declare that the research was conducted in the absenceof any commercial orﬁnancial relationships that could be construed as apotential conﬂict of interest.AcknowledgmentsThis material was made possible, in part, by Cooperative AgreementAP20PPQS&T00C046 from the United States Department ofAgriculture's Animal and Plant Health Inspection Service (APHIS). Itmay not necessarily express APHIS ’views. We would like to extend our sincere thanks to all the reviewers and people involved during ﬁeld work including Stephen P. Labar, Roy Graves, Madison Hodges, Dr.Thiago Marconi, and Uriel Cholula.ReferencesBloice, M.D., 2017. Augmentor: Image Augmentation Library in Python for MachineLearning.https://doi.org/10.5281/ZENODO.1041946 . Bloice, M., Stocker, C., Holzinger, A., 2017. Augmentor: An Image Augmentation Libraryfor Machine Learning.ArXiv Preprint ArXiv:1708.04680 .https://doi.org/10.21105/ joss.00432.FMC Corporation, 2001. FYFANON ULV AG. In FYFANON ULV AG.https://doi.org/10.1016/ b978-081551381-0.50007-5. Gan, T., Zha, Z., Hu, C., Jin, Z., 2021. Detection of Polyps during Colonoscopy Procedure Using YOLOv5 Network.Harden, G.H., 2018. Texas boll weevil eradication foundation cooperative agreement.https://www.usda.gov/sites/default/ ﬁles/33099-0001-23.pdf. He, K., Gkioxari, G., Dollar, P., Girshick, R., 2017. Mask R-CNN. IEEE International Confer- ence on Computer Vision (ICCV). pp. 2961 –2969. Howard, A., Sandler, M., Chen, B., Wang, W., Chen, L.C., Tan, M., Chu, G., Vasudevan, V.,Zhu, Y., Pang, R., Le, Q., Adam, H., 2019. Searching for mobileNetV3. Proceedings ofthe IEEE/CVF International Conference on Computer Vision. 1314 –1324.https://doi. org/10.1109/ICCV.2019.00140. Jiang, Y., Li, C., Paterson, A.H., Robertson, J.S., 2019. DeepSeedling: deep convolutional net-work and Kalmanﬁlter for plant seedling detection and counting in the ﬁeld. Plant Methods 15 (1), 1–19.https://doi.org/10.1186/s13007-019-0528-3 . Jin, X., Che, J., Chen, Y., 2021. Weed identi ﬁcation using deep learning and image process- ing in vegetable plantation. IEEE Access 9, 10940 –10950.https://doi.org/10.1109/AC- CESS.2021.3050296.Jocher, Glenn, 2020. YOLOv5. https://github.com/ultralytics/yolov5 . Jocher, G., Kwon, Y., guigarfr, perry0418, Veitch-Michaelis, J., Ttayu, Suess, D., Baltac ı,F . , Bianconi, G., IlyaOvodov, Marc, Lee, C., Kendall, D., Falak, Reveriano, F., FuLin,GoogleWiki, Nataprawira, J., ... Shead, T.M., 2021. ultralytics/yolov3: v9.5.0 -YOLOv5 v5.0 release compatibility update for YOLOv3. e96031413 https://doi.org/ 10.5281/ZENODO.4681234. Liu, S., Qi, L., Qin, H., Shi, J., Jia, J., 2018. Path aggregation network for instance segmenta-tion. Comp. Soc. Conf. Comp. Vision Pattern Recog. 8759 –8768.https://doi.org/10. 1109/CVPR.2018.00913.Lutz, R., 2017. Netron.https://github.com/lutzroeder/netron . Miao, J., Zhu, W., 2021. Precision –recall curve (PRC) classiﬁcation trees. Evol. Intel.https:// doi.org/10.1007/s12065-021-00565-2 . MicaSense Incorporated, 2022. MicaSense RedEdge and Altum Image Processing Tuto-rials.https://github.com/micasense/imageprocessing . Mustafa, T., Dhavale, S., Kuber, M.M., 2020. Performance analysis of inception-v2 andYolov3-based human activity recognition in videos. SN Comp. Sci. 1 (3), 1 –7. https://doi.org/10.1007/s42979-020-00143-w . National Cotton Council of America, 2012. Protocol for the Eradication of the Boll Weevilin the Lower Rio Grande Valley in Texas and Tamaulipas, Mexico. In cotton.org
. https://www.cotton.org/tech/pest/bollweevil/upload/ITAC-Protocol-d8-Eng-Dec-2012-LS-ed.pdf.Padilla, R., Passos, W.L., Dias, T.L.B., Netto, S.L., Da Silva, E.A.B., 2021. A comparative anal-ysis of object detection metrics with a companion open-source toolkit. Electronics(Switzerland) 10 (3), 1 –28.https://doi.org/10.3390/electronics10030279 . Redmon, J., Farhadi, A., 2018. YOLOv3:An Incremental Improvement. Computer Visionand Pattern Recognition.https://pjreddie.com/media/ﬁles/papers/YOLOv3.pdf. Roboﬂow, 2022. Roboﬂow.https://roboﬂow.com/. Roming, R., Leonard, A., Seagraves, A., Miguel, S.S., Jones, E., Ogle, S., 2021. Sunset Staff Re-ports with Final Results.www.sunset.texas.gov. Saito, T., Rehmsmeier, M., 2015. The precision-recall plot is more informative than theROC plot when evaluating binary classi ﬁers on imbalanced datasets. PLoS One 10 (3), 1–21.https://doi.org/10.1371/journal.pone.0118432 . Sharma, Vinay, 2020.Face Mask Detection using YOLOv5 for COVID-19 [California State University-San Marcos].https://scholarworks.calstate.edu/downloads/wp988p69r?locale=en.Silwal, A., Parhar, T., Yandun, F., Kantor, G., 2021. A Robust Illumination-Invariant CameraSystem for Agricultural Applications. http://arxiv.org/abs/2101.02190 . Sofaer, H.R., Hoeting, J.A., Jarnevich, C.S., 2019. The area under the precision-recall curveas a performance metric for rare binary events. Methods Ecol. Evol. 10 (4),565–577.https://doi.org/10.1111/2041-210X.13140 . Sozzi, M., Cantalamessa, S., Cogato, A., Kayad, A., Marinello, F., 2022. Automatic bunch de-tection in white grape varieties using YOLOv3, YOLOv4, and YOLOv5 deep learning al-gorithms. Agronomy 12 (2), 319. https://doi.org/10.3390/agronomy12020319 . UNITED ST ATES ENYLRONMENT AL PROTECTION AGENCY, 2016. Malathion:Human Health Draft Risk Assessment for Registration Review.USDA-Natural Resources Conservation Service, 2020. Web Soil Survey. https:// websoilsurvey.sc.egov.usda.gov/App/HomePage.htm . Wang, T., Mei, X., Alex Thomasson, J., Yang, C., Han, X., Yadav, P.K., Shi, Y., 2022. GIS-basedvolunteer cotton habitat prediction and plant-level detection with UAV remote sens-ing. Comput. Electron. Agric. 193 (December 2021), 106629. https://doi.org/10. 13031/aim.202000219.Westbrook, J.K., Eyster, R.S., Yang, C., Suh, C.P.C., 2016. Airborne multispectral identi ﬁca- tion of individual cotton plants using consumer-grade cameras. Remote Sens. Appl.Soc. Environ. 4, 37–43.https://doi.org/10.1016/j.rsase.2016.02.002 . Wu, W., Liu, H., Li, L., Long, Y., Wang, X., Wang, Z., Li, J., Chang, Y., 2021. Application of localfully convolutional neural network combined with YOLO v5 algorithm in small targetdetection of remote sensing image. PLoS One 16 (10), e0259283. https://doi.org/10. 1371/journal.pone.0259283. Yadav, P., Thomasson, J.A., Enciso, J., Samanta, S., Shrestha, A., 2019. Assessment of differ-ent image enhancement and classi ﬁcation techniques in detection of volunteerP.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
302cotton using UAV remote sensing. SPIE Defense + Commerc. Sens. https://doi.org/10. 1117/12.2518721May 2019, 20.Yadav, P.K., Thomasson, J.A., Hardin, R.G., Searcy, S.W., Braga-Neto, U.M., Popescu, S.C.,Martin, D.E., Rodriguez, R., Meza, K., Enciso, J., Solorzano, J., Wang, T., 2022a. Volun-teer cotton plant detection in corn ﬁeld with deep learning. Autonomous Air Ground Sens. Syst. Agricult. Optimiz. Phenotyp. VII 1211403 (June), 3. https://doi.org/10. 1117/12.2623032.Yadav, P.K., Thomasson, J.A., Hardin, R., Searcy, S.W., Braga-neto, U., Popescu, S.C., Martin,D.E., Rodriguez, R., Meza, K., Enciso, J., Solo rzano, J., Wang, T., 2022b. Detecting Volunteer Cotton Plants in a Corn Field with Deep Learning on UAV Remote-Sensing Imagery.ArXiv Preprint ArXiv:2207.06673 . , pp. 1–38https://doi.org/10.48550/arXiv.2207.06673 . Yadav, P.K., Thomasson, J.A., Searcy, S.W., Hardin, R.G., Popescu, S.C., Martin, D.E.,Rodriguez, R., Meza, K., Diaz, J.S., Wang, T., 2022c. Computer Vision for VolunteerCotton Detection in a Corn Field With UAS Remote Sensing Imagery and Spot-SprayApplications.ArXiv Preprint ArXiv :2207.07334 ,Vc. , pp. 1–39https://doi.org/10. 48550/arXiv.2207.07334.Yan, B., Fan, P., Lei, X., Liu, Z., Yang, F., 2021. A real-time apple targets detection method forpicking robot based on improved YOLOv5. Remote Sens. 13 (9), 1 –23.https://doi.org/ 10.3390/rs13091619.Yu, J., Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2019. Deep learning for image-basedweed detection in turfgrass. Eur. J. Agron. 104 (October 2018), 78 –84.https://doi. org/10.1016/j.eja.2019.01.004. Zhao, Z.Q., Zheng, P., Xu, S.T., Wu, X., 2019. Object detection with deep learning: a review.IEEE Trans. Neural Networks Learn. Syst. 30 (11), 3212 –3232.https://doi.org/10. 1109/TNNLS.2018.2876865. Zhou, F., Zhao, H., Nie, Z., 2021. Safety helmet detection based on YOLOv5. Int. Conf. PowerElectron. Comp. Appl. ICPECA 2021, 6 –11.https://doi.org/10.1109/ICPECA51329. 2021.9362711.P.K. Yadav, J.A. Thomasson, S.W. Searcy et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 292 –303
303