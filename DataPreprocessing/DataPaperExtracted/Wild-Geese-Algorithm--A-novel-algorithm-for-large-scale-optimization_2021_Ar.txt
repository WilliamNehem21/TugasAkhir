Wild Geese Algorithm: A novel algorithm for large scale optimization basedon the natural life and death of wild geese
Mojtaba Ghasemia, Abolfazl Rahimnejadb, Rasul Hemmatic, Ebrahim Akbarid, S. Andrew Gadsden
b,*
aDepartment of Electronics and Electrical Engineering, Shiraz University of Technology, Shiraz, Iran
bDepartment of Engineering Systems and Computing, University of Guelph, Guelph, Canada
cDepartment of Electrical and Computer Engineering, Marquette University, Milwaukee, USA
dDepartment of Electrical Engineering, Faculty of Engineering, University of Isfahan, Isfahan, Iran
ARTICLE INFO
Keywords:Large-scale global optimization (LSGO)Wild Geese Algorithm (WGA)Swarm-based methodEngineering optimizationABSTRACT
In numerous real-life applications, nature-inspired population-based search algorithms have been applied to solvenumerical optimization problems. This paper focuses on a simple and powerful swarm optimizer, named WildGeese Algorithm (WGA), for large-scale global optimization whose ef ﬁciency and performance are veriﬁed using large-scale test functions of IEEE CEC 2008 and CEC 2010 special sessions with high dimensions D¼100, 500, 1000. WGA is inspired by wild geese in nature and models various aspects of their life such as evolution, regularcooperative migration, and fatality. The effectiveness of WGA for ﬁnding the global optimal solutions of high- dimensional optimization problems is compared with that of other methods reported in the previous literature.Experimental results show that the proposed WGA has an ef ﬁcient performance in solving a range of large-scale optimization problems, making it highly competitive among other large-scale optimization algorithms despite itssimpler structure and easier implementation. The source code of the proposed WGA algorithm is publiclyavailable atgithub.com/ebrahimakbary/WGA .
1. IntroductionMany practical optimization problems, which are called Large ScaleGlobal Optimization (LSGO) problems, deal with a lot of decision vari-ables. Some practical LSGO problems are large-scale electronic systemsdesign, scheduling problems, vehicle routing in large-scale traf ﬁc net- works, and inverse problem chemical kinetics. Many real-world optimi-zation problems involve optimization of a large number of controlvariables with various constraints. However, the classical mathematicalprogramming methods do not generally provide good solutions fordifferent optimization problems with different real-world complexities,due to the huge size of the problems [ 1]. The global optimization per- formance of the population-based algorithms often becomes weaker insuch problems with increasing the dimension and complexity of theproblem [2–4]. The practical large-scale optimization problems havebeen modeled with different benchmark test functions such as thosepresented in the CEC 2008 [5] and CEC 2010 [6]. Recently, many nature-inspired and population-based meta-heuristicoptimization algorithms have been presented to deal with LSGOproblems with different real-world complexities such as nonlinearity,non-smoothness, non-convexity, mixed-integer nature, non-differentiability, etc. Some new nature-inspired optimization algo-rithms for solving the practical large-scale optimization problems arelisted inTable 1. It should be mentioned that, the boldface rows of thistable, show the methods which were used in the comparative study withthe proposed WGA.Wild geese have a long-distance, coordinated and organized travel,which can be used as an inspiration for a very appropriate optimizationalgorithm for high-dimension problems. Based on the general model ofwild geese’lives, a novel algorithm called Wild Geese Algorithm (WGA)is introduced in this paper, which have some main prominent charac-teristics compared to the previous algorithms including:/C15It is simple with low computational burden, and its implementation iseasily performed./C15It has proper and satisfactory power for different test functions, fromdifferent groups.
* Corresponding author.E-mail address:gadsden@uoguelph.ca(S.A. Gadsden).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100074Received 23 December 2020; Received in revised form 1 May 2021; Accepted 16 June 2021Available online 25 June 20212590-0056/©2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-
nc-nd/4.0/ ).Array 11 (2021) 100074It is worth mentioning that although the proposed WGA may seemsimilar to PSO, especially due to the existence of personal best and globalbest concepts, it has some thorough distinctions of structure andformulation, the main of which can be listed as follows:Table 1Summary of some new nature-inspired optimization algorithms for solving thepractical large-scale optimization problems.
Ref. Year Abbreviation Short Description Dimensions under studyReal-worldproblem[7]2008 MLCC MultilevelCooperativeCoevolution100, 500,1000No[8]2008 EPUS-PSO Efﬁcient PopulationUtilization Strategyfor Particle SwarmOptimizer (PSO)100, 500,1000No
[9]2008 sep-CMA-ES Covariance MatrixAdaptationEvolution Strategyhaving diagonalcovariance matrix100–1000 No
[10] 2010 SOUPDE Shuf ﬂe or updateparallel differentialevolution50, 100,200, 500,100No[11] 2010 CCVIL Cooperative Coevolution withVariable InteractionLearning1000 No
[12]2010/C15DECC-D/C15DECC-DML/C15DifferentialEvolution withCooperative Co-evolution usingDelta-Grouping/C15DifferentialEvolution withMultilevelCooperative Co-evolution usingDelta-Grouping100, 500,1000No
[13] 2010 GOBL Generalized Opposition-BasedLearning50, 100,200, 500,100No[14] 2011 TSVP Tabu Search with Variable Partitioning100, 400,1000No[15] 2011 SP-UCI Shuf ﬂed complexevolution withprincipalcomponentsanalysis–Universityof California at Irvine10, 50, 100,1000No
[16] 2012 LMDEa Differential Evolution withLandscape ModalityDetection and aDiversity Archive1000 No
[17
] 2012 DE-CCS Differential Evolution Algorithmwith CooperativeCoevolutionarySelection Operator500,1000 No
[2]2012 CCPSO2 A new CooperativeCoevolving ParticleSwarmOptimization with anew positionupdate rule basedon Cauchy andGaussiandistributions1000 No
[18] 2012 LSCBO Large Scale Optimization Basedon Co-ordinatedBacterial Dynamicsand OppositeNumbers100, 500,1000NoTable 1(continued)
Ref. Year Abbreviation Short Description Dimensions under studyReal-worldproblem[19] 2013 GOjDE A Generalized Opposition basedDifferentialEvolution enhancedwith a self-adaptingparameter tuningstrategy100, 200,500, 1000No
[20] 2013 EOEA A two-stage based ensembleoptimizationevolutionaryalgorithm1000 Yes
[21] 2014 FT-DNPSO PSO with dynamic neighborhood basedon kernel fuzzyclustering andvariable trust regionmethods30, 100,1000No
[22]2014 CBCC1-DGCBCC2-DGDECC-DGTwo differentversions ofContribution BasedCooperative Co-evolution andDifferentialEvolution withCooperative Co-evolution, all withdifferentialgrouping1000 No
[23] 2015 CDE Continuous DifferentialEvolution200, 500,1000No[24] 2015 CSO A Competitive Swarm Optimizer100, 500,1000, 2000,5000No[25] 2016 SOMAQI Self Organizing Migrating Algorithmwith QuadraticInterpolation100, 500,1000, 2000,3000No
[26] 2018 MWOA A Modi ﬁed WhaleOptimizationAlgorithm100, 300,500, 1000No[27] 2019 EHO Enhanced Elephant HerdingOptimization withNovel IndividualUpdating Strategies50, 100,200, 500,100No
[28] 2019 SFO Sail ﬁsh Optimizer 300 Yes [29] 2019 PRO Poor and rich optimizationalgorithm300 Yes[30] 2019 EBA Ensemble Bat Algorithm100, 500,1000No[31]2019 EO Equilibriumoptimizer10–200 Yes
[32] 2020 NPO Nomadic People Optimizer100, 500,2000No[33]2020 ISSA An improved SocialSpider Algorithm100, 500,1000NoM. Ghasemi et al. Array 11 (2021) 100074
21 In WGA, all solutions are sorted based on their objective values so thateach member of population moves using information from its adja-cent members in the sorted population.2 In the proposed method, the formulation for calculating the velocityof each goose is completely different from the PSO and is based on thepositions, velocities, and best positions of the goose and its adjacentgeese in the sorted population, as well as the global best solution'sposition. While in PSO, the only parameter that is shared among allsolutions is the position of the global best solution.3 In the proposed WGA, two different solutions are generated per so-lution and are used for creating the next iteration's goose based on amechanism similar to the crossover operator of differential evolution.4 Finally, in the proposed algorithm, a population reduction policy isimplemented which is accomplished by fatality (elimination) of theweakest goose of the population.The rest of this paper is organized as follows. Section 2presents the new proposed algorithm for large-scale optimization problems. Section 3 shows the experimental results. Finally, Section 4presents the conclusions.2. The proposed algorithm:Wild Geese AlgorithmIn recent years, some new algorithm inspired from group movementand group search by animals have been proposed for large-scale globalcontinuous optimization [1]. In this paper, based on the different phasesof wild geese's lives, including their rhythmic and coordinated groupmigration, reproduction and evolution and also deaths in the populationof geese, a new efﬁcient algorithm, named as Wild Geese Algorithm(WGA), is presented for high-dimensional optimization problems. InFig. 1, a group ordered migration based on the position of wild geese isshown. In general, the proposed WGA phases are as follows:1 Ordered and coordinated group migration (or migration anddisplacement velocity phase)2 Walking and searching for food by wild geese.3 Reproduction and evolution of wild geese.4 Death, migration and ordered evolution of wild geese.First, an initial population of wild geese are created, so that the po-sition vector of thei-th wild goose is equal tox
i. The best local position or personal best solutionp
iand migration velocityv iare determined. Then, all wild geese populations are sorted from the best to the worst accordingto their target function.In this modeling strategy, each wild goose exploits information fromits adjacent wild geese in the ordered population, and is directed by thoseindividuals. The phases of WGA are further discussed in the subsequentsubsections.2.1. An ordered and coordinated group migration (or migration anddisplacement velocity phase)As it is observed inFig. 1, migration of wild geese is a group, coor-dinated, ordered and under control migration, which is based on reach-ing the upfront and adjacent individuals in the sorted population.Velocity and displacement equations according to the coordinated ve-locity of the geese are given in Eq.(1)and Eq.(2).v
Iterþ1i;d¼/C16r 1;d/C2vIteri;dþr 2;d/C2/C16vIteriþ1;d/C0vIteri/C01;d/C17/C17þr
3;d/C2/C16pIteri;d/C0xIteri/C01;d/C17þr
4;d/C2/C16pIteriþ1;d/C0xIteri;d/C17þr
5;d/C2/C16pIteriþ2;d/C0xIteriþ1;d/C17/C0r
6;d/C2/C16pIteri/C01;d/C0xIteriþ2;d/C17(1)wherexi;d,pi;d, andv i;dare thedth dimension of the current position, thebest position, and the current velocity of the ith wild goose, respectively. Note that in this study,r
k;d;k¼1;2; :::;11 are uniformly distributed random numbers between 0 and 1.As observed in Eq.(1), the velocity and position changes of each wildgoose (for instancei-th wild goose) depend on the velocities of theirupfront and rear members, i.eðv
Iteriþ1/C0vIteri/C01Þ, and also to the positions of its adjacent members.According to the model from the migration of wild geese in Fig. 2and Eq.(1), the wild geese use information from their adjacent individuals inthe sorted population, as patterns for their movement and navigation,and tend to reach those members (reduce their distances), i.e. x
Iteri/C01→ p
Iteri;xIteri→pIteriþ1;xIteriþ1→pIteriþ2, andxIteriþ2→/C0pIteri/C01. Additionally, the global best member is used as another guide for themovements of the wholeﬂock; which is reﬂected in Eq.(2). This position change is carried out in an ordered form and coordinated with theupfront members in order to model the movement of all members as anordered series, as shown inFigs. 1 and 2.x
vi;d¼pIteri;dþr 7;d/C2r 8;d/C2/C16/C16gIterdþpIteriþ1;d/C02/C2pIteri;d/C17þv
Iterþ1i;d/C17(2)whereg
dis the global best position among all members.2.2. Walking and searching for food by wild geeseThis step is modeled in such a way that the i-th wild goose moves towards its upfront member, i.e. the ( iþ1)-th goose (p
Iteri→pIteriþ1). In another word, thei-th goose tries to reach the (iþ1)-th goose (p
Iteriþ1/C0pIteri). The equation for walking and searching for food by the wild goose, x
Wiis as follows:x
wi;d¼pIteri;dþr 9;d/C2r 10;d/C2/C16pIteriþ1;d/C0pIteri;d/C17(3)2.3. Reproduction and evolution of wild geeseAnother stage of wild geese's life is reproduction and evolution. In thispaper, its modeling is performed so that a combination between migra-tion equation (x
Vi) and walking and search for food equation ( xWi) is used. TheCrvalue for the proposed WGA algorithm is 0.5 in total simulations.x
Iterþ1i;d¼(xvi;dif r 11;d/C20Crx
wi;dotherwise: (4)
Fig. 1.An ordered and coordinated migration of wild geese.M. Ghasemi et al. Array 11 (2021) 100074
32.4. Death, migration and ordered evolutionThe previous experiments from the literature show that for differentoptimization algorithms the population number and the iteration numberdo not have the same level of inﬂuence on solving every types of prob-lems. For some functions, the size of algorithm's population is moreimportant and more effective than the number of algorithm's iterations(e.g. F2 and F3 functions), and for some other functions the number ofalgorithm's iterations is more important and more effective than the sizeof WGA algorithm's population (e.g. F7 and F8 functions). In this paper,to overcome this problem and establish a compromised solution, thedeath phase is employed in order to balance algorithm performance forall test functions. In this phase, the algorithm starts with the maximumpopulation numberNp
initialand during the algorithm iterations, theweaker members will be removed from the population based on Eq. (5) and the population size will decrease linearly so that it reaches its ﬁnal valueNp
finalin theﬁnal iteration.Np¼round0B@Np
initial
/C0/C18/C0Npinitial/C0Npfinal/C1*/C18 FEsFEs
max/C19/C191CA(5)whereFEsandFEs
maxare the number of function evaluations and itsmaximum.
Algorithm 1Demonstrates the optimization process of WGA.
Algorithm 1:1: to set values of the control parameters of WGA;2: to generate the initial population (whose number are equal to Np
initial) andVIter¼1i ¼ ½0/C138;3: to evaluate theﬁtness of each population individual and FEs¼Np
initial; 4: toﬁnd the personal best position of all particles Np
initial(i¼1, 2,…,Npinitial)i n swarmP
iand the global best position G; 5:whiletheFEstillFEs
maxdo6: Wild Goose populations are arranged from the best to the worst according toFig. 2;7:fori¼1 (best) toNp(worst)do 8: Select the sorted members i/C01th;iþ1th;andiþ2th; {** An ordered and coordinated group migration based on Eq. (1)and Eq.(2)**} 9:ford¼1t oDdo10:V
Iterþ1i←Eq.(1);11:end for12:ford¼1t oDdo13:x
Vi;d←Eq.(2);14:end for{** Walking and search geese Eq. (3)**} 15:ford¼1t oDdo16:x
Wi;d←Eq.(3);17:end for{** Reproduction and evolution Eq. (4)**} 18:ford¼1t oDdo (continued on next column)Algorithm 1(continued)
19:XIterþ1i←Eq.(4);20:end for21:ifx
Iterþ1i;d <xmind
22:xIterþ1i;d←xmind;23:end if24:ifx
Iterþ1i;d >xmaxd
25:xIterþ1i;d←xmaxd;26:end if27: to evaluate theﬁtness ofX
Iterþ1i
28:iffðXIterþ1iÞ/C20fðPIteriÞ29:P
Iterþ1i←XIterþ1i ;30:end if31:iffðP
Iterþ1iÞ/C20fðGÞ32:G←P
Iterþ1i ;33:end if34:end for35:FEs¼FEsþNp;36:Np←Eq.(5);37:end while
3. Results and analysis of experimental evaluation studiesIn this section, 20 widely used large scale test functions are exploitedto show the efﬁciency and performance of the proposed algorithm. Theformulation and characteristics of all CEC 2010 benchmark test functionsare listed in Ref. [6].The performance and robustness of WGA for solving real and large-scale optimization problems are characterized by two indices: 1) themean of best values of test function (Mean), and 2) the standard deviation(Std) indices.Test functions include 1. Separable functions ( F1/C0F3), 2. Single- groupm-nonseparable functions (F4/C0F8), 3.
D2m-groupm-nonseparable functions (F9/C0F13), 4.
Dm-groupm-nonseparable functions (F14/C0F18), and 5. non-separable functions (F19/C0F20), where m is the number of variables in each non-separable subcomponent, and Dandmare assumed as 1000 and 50, respectively. To show the ef ﬁciency of WGA, in all simulations of this paper, 25 independent simulations are used in eachsection for every test function, as in Refs. [ 6,22]. Furthermore, in all simulations, the maximum number ofﬁtness evaluationsFEs
maxis 3/C2 10
6. In all tables, theþsign means the algorithm outperforms WGA, the–sign means WGA outperforms the algorithm, and the ¼sign means WGA and the considered algorithm yield the same solution for the givenproblem. It should be mentioned that, in all results tables, the boldface isused to emphasize the algorithm that achieves the best Mean index valuefor each problem.3.1. Experimental setup3.1.1. Inﬂuence of death phase on WGA performanceAtﬁrst, to show the performance of the population reduction by death
Fig. 2.The model of ordered and coordinated group migration of wild geese.M. Ghasemi et al. Array 11 (2021) 100074
4of Wild Geese, WGA is tested without considering the death phase and istested with a large populationNp¼120 and a small populationNp¼30. The suitable results were compared with those of WGA (consideringpopulation reduction fromNp¼120 (Np
initial¼120) toNp¼30 (Np
final¼30) using Eq.(5), where the results obtained for each functionare listed inTable 2. The results demonstrate that the proposed deathphase improves the efﬁciency of WGA for high-dimensional problems.The positive inﬂuence of death phase can be especially observed for testfunctionsF3,F6,F7,F11,F12,F16, andF17. Moreover, the convergence characteristics of this algorithm for 6 different functions of various typesare depicted inFig. 3, which verify the effectiveness of implementingdeath phase in WGA.3.1.2. WhyCr¼0.5 in WGA for all test functions?In this paperCr¼0.5 is used for all simulations. To select a suitablevalue forCrfour different constant values other than 0.5, i.e. 0.1, 0.25,0.75 and 0.9 are tested, whose results are presented in Table 3.A s observed, the constant value 0.5 is the best value for different testfunctions of CEC 2010. It should be mentioned that in all simulationresults tables, three values are reported for optimizing each test functionwith each algorithm; theﬁrst two demonstrate the average and standarddeviation ofﬁtness values of the obtained results. The third value showsthe rank of that algorithm in terms of the mean index. Furthermore, threeparameters are reported for each algorithm in all tables, i.e. Nb,Nw,Mr. N
bandN ware the number of times the algorithm yields the best and theworst mean index, respectively; and M
ris the average rank of the algo- rithm achieved in solving all considered test functions.3.2. Comparing WGA with recent optimization algorithms3.2.1. CEC 2008 test functionsIn this section, the results of WGA are compared with those of a seriesof the recently proposed optimization algorithms for large-scale prob-lems from CEC 2008 test functions with different high dimensionsincludingD¼100,D¼500 andD¼1000. The formulation and char- acteristics of CEC
008 benchmark test functions are listed in Ref. [ 5] and Table 4:Two indices are exploited in this study to characterize the perfor-mance and robustness of WGA for solving real and large-scale optimi-zation problems with different dimensions: 1) mean of the best values oftest function (Mean), and 2) standard deviation (Std). Tables 5–7show theﬁnal best solutions of test functions’optimization by WGA and those of large scale optimization algorithms including CSO [ 24], CCPSO2 [2], sep-CMA-ES [9], MLCC [7], and EPUS-PSO [8]. As seen, the proposed WGA is able to provide very efﬁcient and competitive results in solvingreal and large-scale problems compared with the previously proposedalgorithms. WGA proves itself as a promising technique for real and largescale shifted unimodal and multimodal optimization problems.3.2.2. CEC 2010 test functionsAs mentioned in the introduction section of this paper, numerousresearches have been recently performed to achieve some algorithms andmeta-heuristic optimization methods for high-dimension optimizationproblems. These studies and many other methods have been introducedtoﬁnd a simple and quick method with the low computational burden. InTable 8, the results of previous researches for 20 different test functionsof CEC 2010 withD¼1000 are summarized [22], which was obtained with the same conditions as those of WGA. The summarized algorithms inTable 8include MLCC algorithm [7], differential evolution with coop- erative co-evolution and delta grouping DECC-D and DECC-DML [ 12], contribution based cooperative co-evolution and differential groupingCBCC1-DG and CBCC2-DG [22], differential evolution with cooperativeco-evolution and random grouping (DECC-DG) [ 22]. The last two rows of Table 8present the comparative indices for these algorithms.The WGA algorithm has achieved the best results in 12 of 20 func-tions, i.e.F4,F5,F6,F7,F9,F10,F13,F14,F15,F17,F18, andF19. In addition, for none of the test functions WGA has the worst results.Moreover, WGA reaches the best average rank ( M
r). The proposed al- gorithm (WGA) outperforms MLCC algorithm in 18 out of 20 functions;only for theﬁrst two functions MLCC algorithm performs better. For theﬁrst function the average value of WGA is very close to that of MLCCalgorithm. MLCC algorithm has different results for different test func-tions and has the worst results for 6 out of 20 functions. However, theproposed algorithm has acceptable and suitable results for most of theTable 2Averageﬁtness values and standard deviations of results for test functions over25 independent runs.
FWGA,Np¼30 WGA,Np¼120 WGAF1 1.68E-21 2.33E-24 1.05E-26 7.71E-22 1.58E-24 2.56E-26 32 1 F2 7.78Eþ032.18Eþ032.28Eþ03 7.95Eþ011.14Eþ014.58Eþ01 312 F3 1.00Eþ011.17E-131.47E-13 1.25Eþ017.40E-158.94E-15 312 F43.81Eþ119.99Eþ11 5.15Eþ11 1.63Eþ111.05Eþ11 7.89Eþ10 132 F5 9.55Eþ07 5.74Eþ075.47Eþ07 7.04Eþ06 3.68Eþ067.93Eþ06 32 1 F6 1.98Eþ01 3.56E-09 3.55E-09 2.50E-02 1.40E-15 5.48E-14 32 1 F7
8.01E-024.47Eþ03 4.60Eþ00 2.00E-021.69Eþ03 6.28Eþ00 132 F88.60Eþ064.30Eþ07 9.16Eþ06 3.16Eþ052.74Eþ07 8.79Eþ06 132 F9 2.54Eþ07 4.55Eþ072.21Eþ07 1.33Eþ06 5.50Eþ061.51Eþ06 23 1 F10 4.67Eþ031.76Eþ032.64Eþ03 1.60Eþ022.48Eþ012.70Eþ01 312 F11 8.94Eþ012.34E-133.06E-13 7.77Eþ001.07E-145.48E-14 312 F121.62Eþ033.25Eþ04 4.15Eþ03 1.30Eþ
021.40Eþ03 2.40Eþ02 132 F13 9.11Eþ02 9.87Eþ026.87Eþ02 1.93Eþ02 1.50Eþ022.63Eþ01 23 1 F147.51Eþ071.52Eþ08 7.67Eþ07 5.36Eþ061.24Eþ07 4.55Eþ06 132 F15 5.28Eþ03 4.21Eþ033.14Eþ03 3.79Eþ02 1.01Eþ025.42Eþ01 32 1 F16 2.69Eþ02 7.63Eþ003.79Eþ00 1.37Eþ01 2.95Eþ006.26E-01 32 1 F171.41Eþ041.47Eþ05 3.74Eþ04 6.23Eþ027.77Eþ03 1.36Eþ02
132 F18 2.11Eþ03 4.15Eþ031.52Eþ03 1.47Eþ03 1.56Eþ032.93Eþ02 23 1 F198.73Eþ051.35Eþ06 1.04Eþ06 1.03Eþ055.17Eþ04 2.85Eþ04 132 F20 1.58Eþ03 1.15Eþ031.04Eþ03 7.71Eþ01 2.42Eþ018.18Eþ01 32 1 Nb/Nw/Mr7/10/2.15 4/10/2.3 10/0/1.55M. Ghasemi et al. Array 11 (2021) 100074
5test functions and dispersion of its results are less than those of the otheralgorithms. The comparison between WGA and DECC-D algorithm showsthat WGA performs better for 18 out of 20 functions. Nonetheless, forfunctionsF2 andF20, it gives a worse result compared to that of DECC-D.For functionF2, the average value of WGA is very close to that obtainedfrom DECC-D algorithm. Furthermore, DECC-D algorithm does not pro-vide a good quality solution for different test functions, for example forF2 andF20 it has suitable results, but forF5/C0F8,F10/C0F12, andF15/C0 F17 its results are not acceptable compared to those of other algorithms.Although DECC-DML algorithm outperforms WGA for ﬁve test functions, it has the worst solution for six functions. CBCC1-DG and CBCC2-DGalgorithms are more successful than WGA for two and three functions,respectively; however, CBCC1-DG gives the best result for none of thefunctions and CBCC2-DG yields the best result for only function. DECC-DG algorithm performs better than WGA for 2 out of 20 test functions;however, it gives the worst solution for 4 test functions among allalgorithms.3.2.3. Test on real-world optimization problemsHere, the effectiveness of the proposed algorithm (WGA) was inves-tigated compared to genetic algorithm (GL-25) [ 34], DE with strategy adaptation (SaDE) [35], DE with control components and composite trialvector generation approaches (CoDE) [ 36], Standard particle swarm optimization (SPSO2013) [37], and heterogeneous comprehensivelearning PSO with improved exploitation and exploration (HCLPSO) [ 38] on real-world usages including estimating the factor forfrequency-modulated (FM) sound waves [ 39] and large-scale reliabili- ty-redundancy allocation optimization (RRAO) of a gas turbine [ 40].1) Estimating the factor for frequency modulated sound wavesThe greatly complex multimodal frequency-modulated (FM) soundsynthesis optimizing problem plays a key role in various modern musicsystems for estimating the optimal factors of a FM sound wave synthesis[39]. The estimation of optimal factors of an FM sound wave synthesis is
Fig. 3.Average convergence of WGA on nine selected test functions over 25 independent runs.M. Ghasemi et al. Array 11 (2021) 100074
6an optimization problem withDdecision variables. In this work, the caseofD¼6 is only considered in accordance with [ 41,42]. Six components are included in the 6-dimensional parameter vector as x¼[x
1(a1), x
2(ω1),x3(a2),x4(ω2),x5(a3),x6(ω3)] ranging between 6.35 and 6.5 forall variables. The equations provided for the target and approximatedsound waves fortdeﬁned in range of 1–100 are as follows [42]: yðtÞ¼x1sinðx 2tθþx 3sinðx 4tθþx 5sinðx 6tθÞÞÞ;(6)y
0ðtÞ¼1:0*sinð0:5tθ/C01:5*sinð4:8tθþ2:0*sinð4:9tθÞÞÞ;(7)whereθ¼
2π
100
The optimization problem objective function is considered as the sumof squared errors betweenyðtÞ(the approximated wave) andy
0ðtÞ(the target wave) with optimal valuef(x)¼0 as follows:fðxÞ¼X
100t¼0ðyðtÞ/C0y 0ðtÞÞ2: (8)2) RRAO constrained problem:The nonlinear reliability-redundancy constrained optimizationproblems are mainly aimed at enhancing the system reliability (maxi-mizing the overall system reliability) through optimizing element re-liabilities vector (r¼(r
1,r2,…,r m)) and redundancy assignment numbersvector (n¼(n
1,n2,…,n m)) for subsystems of the system. It is possible toformulate this problem as a nonlinear mixed-integer programming modelby choosing the system reliability as the objective function to be maxi-mized subjecting to several nonlinear constraints as follows [ 40]: MaximizeR
s¼fðr;nÞ; (9)subject togðr;nÞ/C20l;0/C20r
d/C201;n d2Zþ;0/C20d/C20m: (10)whereZ
þis the set of positive integers,R srepresents the reliability of various systems,f(.)andg(.)denote for the objective and constraintfunctions of RRAO problem for the total parallel-series systems, respec-tively, from whichg(.)is usually related to the system cost, weight andvolume.n¼(n
1,n2,…,n m) andr¼(r 1,r2,…,r m) show the redundancy allocation numbers and component reliabilities vectors for system'ssubsystems including m subsystems, respectively. Moreover, lshows the limit of the system resources.The overspeed detection was continually offered by the mechanicaland electrical systems. By occurring an overspeed, the fuel source mustbe stopped through control valves ( V
1toV m).Fig. 4represents a gas turbine's overspeed protection system for RRAO optimizing the mixed-integer non-linear problem. The large-scale test structure involves 40Table 3Averageﬁtness values and standard deviations on test functions over 25 inde-pendent runs.
FC r¼0.1Cr¼0.25Cr¼0.75Cr¼0.9Cr¼0.5F1 3.12Eþ07 3.77E-06 5.24Eþ09 5.00Eþ101.05E-26 7.41Eþ07 1.63E-07 1.03Eþ09 2.29Eþ102.56E-26 3,–2,–4,–5,–1 F4 1.17Eþ12 7.26Eþ11 4.96Eþ13 2.48Eþ145.15Eþ11 7.40Eþ11 9.64Eþ10 8.22Eþ13 5.31Eþ137.89Eþ10 3,–2,–4,–5,–1 F9 1.39Eþ10 9.04Eþ07 1.03Eþ10 6.71Eþ102.21Eþ07 8.22Eþ09 2.76Eþ08 7.50Eþ09 2.56Eþ101.51Eþ06 4,
–2,–3,–5,–1 F14 3.04Eþ10 2.71Eþ09 3.19Eþ09 1.55Eþ117.67Eþ07 2.61Eþ10 1.26Eþ09 4.54Eþ09 1.23Eþ114.55Eþ06 4,–2,–3,–5,–1 F20 1.50Eþ101.03Eþ031.17Eþ11 6.53Eþ11 1.04Eþ03 4.20Eþ095.15Eþ013.73Eþ09 3.90Eþ09 8.18Eþ01 3,–1,þ4,–5,–2 /C0/þ/¼5/0/0 4/1/0 5/0/0 5/0/0 – Nb/Nw/Mr0/0/3.4 1/0/1.8 0/0/3.6 0/0/5 4/0/1.2
Table 4Summary of CEC 08 Special Session benchmark test functions [ 5] for large scale global optimization.
Function Name Properties Search spaceGlobaloptimumf
1 Shifted Sphere Unimodal, separable,scalable[-100,100]0f
2 ShiftedSchwefel'sUnimodal, non-separable, scalable[-100,100]0f
3 ShiftedRosenbrock'sMultimodal, non-separable, scalable[-100,100]0f
4 ShiftedRastrigin'sMultimodal,separable, scalable[-5, 5] 0f
5 ShiftedGriewank'sMultimodal, non-separable, scalable[-600,600]0f
6 Shifted Ackley's Multimodal,separable, scalable[-32, 32] 0
Table 5Results obtained by optimization algorithms for dimension 100 over 25 independent runs.
FD¼100CCPSO2 [2] CSO [24] sep-CMA-ES [9] MLCC [7] EPUS-PSO [8] ISSA [33]E O [31] WGAF1 7.73E-14 9.11E-29 9.02E-15 6.82E-14 7.47E-01 01.31E-200 3.23E-14 1.10E-28 5.53E-15 2.32E-14 1.70E-01 05.01E-200 6,- 2,- 4,- 5,- 7,- 1,¼3.-1 F2 6.08Eþ00 3.35Eþ01 2.31Eþ01 2.53Eþ01 1.86Eþ01 8.31Eþ01 4.29Eþ012.14E-05 7.83Eþ00 5.38Eþ00 1.39Eþ01 8.73Eþ00 2.26Eþ0 1.91 Eþ01 3.69Eþ003.08E-05 2,- 6,- 4,- 5,- 3,- 8,- 7,- 1 F3 4.23Eþ02 3.90Eþ024.31Eþ001.50Eþ02 4.99Eþ03 1.68Eþ02 9.21Eþ01 1.04Eþ02 8.65Eþ02 5.53Eþ021.26Eþ015.72Eþ01 5.35Eþ03 9.46E
þ01 8.97Eþ01 4.01Eþ01 7,- 6,- 1,þ4,- 8,- 5,- 2, þ3 F4 3.98E-02 5.60E þ01 2.78Eþ024.39E-134.71Eþ02 5.00Eþ00 6.04Eþ02 1.25Eþ02 1.99E-01 7.48Eþ00 3.43Eþ019.21E-145.94Eþ01 6.60Eþ00 8.52Eþ01 1.41Eþ01 2,þ4,þ6,-1,þ7,- 3,þ8,- 5 F5 3.45E-03 02.96E-04 3.41E-14 3.72E-01 09.58E-020 4.88E-0301.48E-03 1.16E-14 5.60E-02 01.02E-010 4,-1,¼3,- 2,- 6,- 1,¼5,-1 F6 1.44E-13 1.20E-0142.12Eþ01 1.11E-13 2.06E þ00 2.09Eþ01 2.05Eþ01 1.39E-014 3.06E-141.52E-0154.02E-01 7.87E-15 4.40E-01 2.99E-02 1.73E-01 1.23E-015 4,-1,þ8,- 3,- 6,- 7,- 5,- 2 /C0/þ/¼5/1/0 3/2/1 5/1/0 5/1/0 6/0/0 3/1/2 5/1/0 – Nb/Nw/Mr
0/0/4.167 2/0/3.333 1/1/4.333 1/0/3.333 0/4/6.167 2/1/4.1670/1/53/0/2.333M. Ghasemi et al. Array 11 (2021) 100074
7decision variables (m*2¼40). The input factors and data for the large-scale test system are provided in Ref. [ 43] with 20 subsystems. It is possible to formulate this reliability optimization problem as:Maximizef
5ðr;nÞ¼Ymd¼1½1/C0ð1/C0r dÞnd/C138: 0:5/C20r
d/C20/C01/C010/C06/C1;0/C20d/C20m1/C20n
d/C2010;2Zþ: (11)The system constraints include:1) The combined weight, volume, and redundancy allocation con-straintg
1ðr;nÞ:g
1ðr;nÞ¼Xmd¼1v2dn2d/C20V (12)wherev
dshows the volume ofdth subsystem for all components and Vrepresents the upper volume limit of the products of the subsystem.2) The system cost limitationg2ðr;nÞ:g
2ðr;nÞ¼Xmd¼1Cðr dÞ/C2ndþe0:25n d/C3/C20C;Cðr
dÞ¼αd/C18/C0Tlnr
d/C19βd
: (13)where,Cshows the upper cost limit of the system, Cðr
dÞis the cost for all element with reliabilityr
datdth stage, andTis the operating time in which the components are working.3) The system weight limitationg
3ðr;nÞ:g
3ðr;nÞ¼Xmd¼1wdnde0:25n d/C20W (14)Table 6Results obtained by optimization algorithms for dimension 500 over 25 independent runs.
FD¼500CCPSO2 [2] CSO [24] sep-CMA-ES [9] MLCC [7] EPUS-PSO [8] ISSA [33]E O [31] WGAF1 3.00E-13 6.57E-23 2.25E-14 4.30E-13 8.45E þ01 9.90E-28 4.14E-04 0.00Eþ00 7.96E-14 3.90E-24 6.10E-15 3.31E-14 6.40E þ00 9.95E-28 3.87E-04 0.00Eþ00 5,- 3,- 4,- 6,- 8,- 2,- 7,- 1 F2 5.79Eþ012.60Eþ012.12Eþ02 6.67Eþ01 4.35Eþ01 2.66Eþ02 9.34Eþ01 5.73Eþ01 4.21Eþ012.40Eþ001.74Eþ01 5.70Eþ00 5.51E-01 1.92E þ01 3.01E-01 8.72E þ00 4,-1,þ7,- 5,- 2, þ8,- 6,- 3 F3 7.24Eþ02 5.74Eþ022.93Eþ029.25Eþ02 5.77Eþ04 8.31Eþ14 1.95Eþ03 5.22Eþ02 1.54Eþ02 1.67Eþ023.59E
þ011.73Eþ02 8.04Eþ03 3.11Eþ14 1.04Eþ03 3.60Eþ01 6,- 4,- 1,þ7,- 5,- 8,- 3,- 2 F4 3.98E-02 3.19E þ02 2.18Eþ031.79E-113.49Eþ03 2.07Eþ03 3.78Eþ03 1.25Eþ02 1.99E-01 2.16Eþ01 1.51Eþ026.31E-111.12Eþ02 5.38Eþ02 1.46Eþ02 1.41Eþ01 2,þ4,- 6,- 1,þ7,- 5,- 8,- 3 F5 1.18E-03 2.22E-167.88E-04 2.13E-13 1.64E þ00 4.48E-02 2.42E-01 4.12E-16 4.61E-030.00Eþ002.82E-03 2.48E-14 4.69E-02 1.29E-01 6.11E-01 5.36E-17 5,-1,þ4,- 3,- 8,- 6,- 7,- 2 F6 5.34E-13 4.13E-13 2.15E þ01 5.34E-13 6.64E þ00 2.14Eþ01 2.06Eþ015.77E-14 8.61E-14 1.10E-14 3.10E-01 7.01E-14 4.49E-01 1.70E-02 3.35E-01 1.58E-15 3,- 2,- 7,- 3,- 4,- 6,- 5,- 1,þ /C0/þ/¼5/1/0 4/2/0 5/1/0 5/1/0 5/1/0 6/0/0 6/0/0 – Nb/Nw/Mr0/0/4.167 2/0/2.5 1/2/4.833 1/1/4.167 0/3/5.667 0/2/5.833 0/1/6 2/0/2
Table 7Results obtained by optimization algorithms for dimension D¼1000 over 25 independent runs.
FD¼1000CCPSO2 [2] CSO [24] sep-CMA-ES [9] MLCC [7] EPUS-PSO [8] ISSA [33]E O [31] WGAF1 5.18E-13 1.09E-21 7.81E-15 8.46E-13 5.53E þ02 2.09E-18 1.35E þ041.75E-28 9.61E-14 4.20E-23 1.52E-15 5.01E-14 2.86E þ01 3.95E-18 6.94E þ031.27E-28 5,- 2,- 4,- 6,- 7,- 3,- 8,- 1 F2 7.82Eþ014.15Eþ013.65Eþ02 1.09Eþ02 4.66Eþ01 3.10Eþ02 1.64Eþ02 7.43Eþ01 4.25Eþ019.74E-019.02Eþ00 4.75Eþ00 4.00E-01 1.38E þ01 1.16Eþ02 4.89Eþ00 4,-1,þ8,- 5,- 2, þ7,- 6,- 3 F3 1.33Eþ03 1.01Eþ039.10Eþ021.80Eþ03 8.37Eþ05 2.17Eþ15 2.58Eþ09 1.00Eþ03 2.63Eþ02 3.02Eþ014.54E
þ011.58Eþ02 1.52Eþ05 6.89Eþ13 2.63Eþ09 8.25Eþ01 4,- 3,- 1,þ5,- 6,- 8,- 7,- 2 F4 1.99E-01 6.89E þ02 5.31Eþ031.37E-107.58Eþ03 1.49Eþ04 7.79Eþ03 2.52Eþ03 4.06E-01 3.10Eþ01 2.48Eþ023.37E-101.51Eþ02 1.93Eþ03 1.01Eþ02 1.34Eþ02 2,þ3,þ5,-1,þ6,- 8,- 7,- 4 F5 1.18E-03 2.26E-163.94E-04 4.18E-13 5.89E þ00 3.10E-01 4.07E þ01 1.22E-15 3.27E-032.18E-171.97E-03 2.78E-14 3.91E-01 4.51E-01 5.39E þ01 2.91E-16 5,-1,þ4,- 3,- 7,- 6,- 8,- 2 F6 1.02E-12 1.21E-12 2.15E þ01 1.06E-12 1.89E þ01 2.15Eþ01 2.05Eþ011.21E-13 1.68E-13 2.64E-14 3.19E-01 7.68E-14 2.49E þ00 7.70E-03 1.40E-01 5.18E-15 2,- 4,- 5,- 3,- 6,- 8,- 7,- 1 /C0/þ/¼5/1/0 3/3/0 5/1/0 5/1/0 5/1/0 6/0/0 6/0/0 – Nb/Nw/Mr0/0/3.667 2/0/2.33 1/1/4.5 1/0/3.833 0/0/5.667 0/3/6.667 0/2/7.167 2/0/2.167M. Ghasemi et al. Array 11 (2021) 100074
8The proposed WGA algorithm and the other 5 algorithms are appliedin these two real-world problems. For comparative studies, FEs
maxare adjusted to 5.00Eþ04 and a large enough population size is chosen for allalgorithms.Table 9presents the optimization results (mean and standarddeviation) of different algorithms executed in 30 runs for solving the twoproblems. The best results are shown in boldface, which indicate thatWGA provides efﬁcient and better performance compared to the other 5advanced algorithms for real-world optimization problems.Table 8Averageﬁtness values and standard deviations on CEC 2010 functions over 25 independent runs.
F MLCC [ 7] DECC-D [12] DECC-DML [12] CBCC1-DG [22] CBCC2-DG [22] DECC-DG [22] WGAF11.53E-271.01E-24 1.93E-25 1.32E þ04 8.34Eþ03 5.47Eþ03 1.05E-26 7.66E-271.40E-25 1.86E-25 6.25E þ04 3.41Eþ04 2.02Eþ04 2.56E-26 1,þ4,- 3,- 7,- 6,- 5,- 2 F25.57E-012.99Eþ02 2.17Eþ02 4.44Eþ03 4.44Eþ03 4.39Eþ03 2.28Eþ03 2.21Eþ001.92Eþ01 2.98Eþ01 1.60Eþ02 1.80Eþ02 1.97Eþ02 4.58Eþ01 1,þ3,þ2,þ6,- 6,- 5,- 4 F3 9.88E-13 1.81E-13 1.18E-131.66Eþ01 1.67Eþ01 1.67Eþ01 1.47E-13 3.70E-12 6.68E-15 8.22E-153.79E-01 3.28E-01 3.34E-01 8.94E-15 4,- 3,- 1,þ5,- 6,- 6,- 2 F4 9.61Eþ12 3.99Eþ12 3.58Eþ12 2.31Eþ12 2.36Eþ
12 4.79Eþ125.15Eþ11 3.43Eþ12 1.30Eþ12 1.54Eþ12 7.43Eþ11 7.92Eþ11 1.44Eþ127.89Eþ10 7,- 5,- 4,- 2,- 3,- 6,- 1 F5 3.84Eþ08 4.16Eþ08 2.98Eþ08 1.35Eþ08 1.36Eþ08 1.55Eþ085.47Eþ07 6.93Eþ07 1.01Eþ08 9.31Eþ07 2.18Eþ07 2.46Eþ07 2.17Eþ077.93Eþ06 6,- 7,- 5,- 2,- 3,- 4,- 1 F6 1.62Eþ07 1.36Eþ07 7.93Eþ05 1.65Eþ01 1.64Eþ01 1.64Eþ013.55E-09 4.97Eþ06 9.20Eþ06 3.97Eþ06 3.99E-01 3.46E-01 2.71E-01 5.48E-14 6,- 5,- 4,- 3,- 2,- 2,- 1 F7 6.89Eþ05 6.58Eþ07 1.39Eþ08 1.81Eþ04 1.35Eþ04 1.16Eþ044.60Eþ00
7.37Eþ05 4.06Eþ07 7.72Eþ07 4.59Eþ04 3.92Eþ04 7.41Eþ036.28Eþ00 5,- 6,- 7,- 4,- 3,- 2,- 1 F8 4.38Eþ07 5.39Eþ07 3.46Eþ07 3.34Eþ068.70Eþ053.04Eþ07 9.16Eþ06 3.45Eþ07 2.93Eþ07 3.56Eþ07 2.29Eþ061.71Eþ062.11Eþ07 8.79Eþ06 7,- 6,- 5.- 2, þ1,þ4,- 3 F9 1.23Eþ08 6.19Eþ07 5.92Eþ07 6.79Eþ07 7.97Eþ07 5.96Eþ072.21Eþ07 1.33Eþ07 6.43Eþ06 4.71Eþ06 6.92Eþ06 1.08Eþ07 8.18Eþ061.51Eþ06 7,- 4,- 2,- 5,- 6,- 3,- 1 F10 3.43Eþ03 1.16Eþ04 1.25Eþ04 4.01E
þ03 4.04Eþ03 4.52Eþ032.64Eþ03 8.72Eþ02 2.68Eþ03 2.66Eþ02 1.37Eþ02 1.21Eþ02 1.41Eþ022.70Eþ01 2,- 6,- 7,- 3,- 4,- 5,- 1 F11 1.98Eþ02 4.76Eþ011.80E-131.05Eþ01 1.03Eþ01 1.03Eþ013.06E-13 6.98E-01 9.53Eþ019.88E-159.31E-01 8.47E-01 1.01E þ005.48E-14 6,- 5,- 1,þ4,- 3,- 3,- 2 F12 3.49Eþ04 1.53Eþ05 3.79Eþ06 4.19Eþ03 4.00Eþ032.52Eþ034.15Eþ03 4.92Eþ03 1.23Eþ04 1.50Eþ05 1.25Eþ03 8.63Eþ024.86Eþ022.40Eþ02 5,- 6,- 7,- 4,- 2, þ1,þ3 F13 2.08Eþ03 9.87Eþ02 1.14E
þ03 9.10Eþ03 4.54Eþ03 4.54Eþ066.87Eþ02 7.27Eþ02 2.41Eþ02 4.31Eþ02 3.75Eþ03 1.91Eþ03 2.13Eþ062.63Eþ01 4,- 2,- 3,- 6,- 5,- 7,- 1 F14 3.16Eþ08 1.98Eþ08 1.89Eþ08 3.64Eþ08 3.69Eþ08 3.41Eþ087.67Eþ07 2.77Eþ07 1.45Eþ07 1.49Eþ07 2.61Eþ07 2.42Eþ07 2.41Eþ074.55Eþ06 4,- 3,- 2,- 6,- 7,- 5,- 1 F15 7.11Eþ03 1.53Eþ04 1.54Eþ04 5.89Eþ03 5.88Eþ03 5.88Eþ033.14Eþ03 1.34Eþ03 3.92Eþ02 3.59Eþ02 9.10Eþ01 8.81Eþ01 1.03Eþ025.42Eþ01 4,- 5,- 6,- 3,- 2,- 2,- 1 F16 3.76E
þ02 1.88Eþ02 5.08E-02 3.08E-12 4.44E-12 7.39E-133.79Eþ00 4.71Eþ01 2.16Eþ02 2.54E-01 3.19E-12 4.22E-13 5.70E-146.26E-01 7,- 6,- 4, þ2,þ3,þ1,þ5 F17 1.59Eþ05 9.03Eþ05 6.54Eþ06 4.50Eþ04 4.73Eþ04 4.01Eþ043.74Eþ04 1.43Eþ04 5.28Eþ04 4.63Eþ05 3.18Eþ03 2.77Eþ03 2.85Eþ031.36Eþ02 5,- 6,- 7,- 3,- 4,- 2,- 1 F18 7.09Eþ03 2.12Eþ03 2.47Eþ03 1.34Eþ09 3.47Eþ08 1.11Eþ101.52Eþ03 4.77Eþ03 5.18Eþ02 1.18Eþ03 4.94Eþ08 1.39Eþ08 2.04Eþ092.93Eþ02 4- 2,- 3,- 6,- 5,- 7,- 1 F19 1.36Eþ06 1.33Eþ07 1.59Eþ
07 1.74Eþ06 1.74Eþ06 1.74Eþ061.04Eþ06 7.35Eþ04 1.05Eþ06 1.72Eþ06 8.46Eþ04 8.46Eþ04 9.54Eþ042.85Eþ04 2,- 4,- 5,- 3,- 3,- 3,- 1 F20 2.05Eþ039.91Eþ02 9.91Eþ029.53Eþ04 8.42Eþ03 4.87Eþ07 1.04Eþ03 1.80Eþ022.61Eþ01 3.51Eþ011.02Eþ05 2.36Eþ03 2.27Eþ07 8.18Eþ01 3,-1,þ1,þ5,- 4,- 6,- 2 /C0/þ/¼18/2/0 18/2/0 15/5/0 18/2/0 17/3/0 18/2/0 - Nb/Nw/Mr2/6/4.5 1/1/4.45 3/6/3.95 0/2/4.05 1/3/3.9 2/4/3.95 12/0/1.75M. Ghasemi et al. Array 11 (2021) 100074
94. ConclusionThe proposed Wild Goose Algorithm (WGA) is a simple and effectivealgorithm that has been designed and proposed for optimization of high-dimensional problems. This algorithm, which is inspired by wild geesefound in nature, includes ordered and coordinated group migration,reproduction and evolution of geese, and also death in the population ofgeese. To show the performance of the proposed WGA algorithm foroptimization of high-dimension problems, it is tested and compared withsep-CMA-ES, CCPSO2, CSO, EPUS-PSO, MLCC, DECCD, DECC-DML,CBCC2-DG, CBCC1-DG and DECC-DG algorithms based on the func-tions of CEC 2008 and CEC 2010. One of the advantages of WGA is that ithas only one control parameter,Cr. It is experimentally shown that WGAhas better competitive results with respect to other mentioned algo-rithms, and outperforms all other algorithms for most of the test func-tions. Furthermore, WGA is a simple and basic algorithm for large-scaleoptimization which can be used for various real-world optimizationproblems. In recent years, numerous studies have been carried out in thearea of high-dimension optimization, the most of which focused oncooperative co-evolution technique. In future, WGA may be embeddedinto the frameworks of different CC methods with various categories inorder to improve its performance. Furthermore, WGA can be used forsolving other real-world large-scale optimization problems.Credit author statementMojtaba Ghasemi: Conceptualization, Methodology, Software,Writing–original draft preparation. Abolfazl Rahimnejad: Data curation,Software, Writing–original draft preparation. Rasul Hemmati: Writing – original draft preparation, Visualization, Investigation. Ebrahim Akbari:Software, Validation. S. Andrew Gadsden: Writing- Reviewing andEditing.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgementsWe would like to acknowledge funding and support from the NaturalSciences and Engineering Research Council (NSERC) Discovery Grant(Gadsden).References[1] Mahdavi S, Shiri ME, Rahnamayan S. Metaheuristics in large-scale global continuesoptimization: a survey. Inf Sci 2015;295:407 –28.https://doi.org/10.1016/ j.ins.2014.10.042.[2] Li X, Yao X. Cooperatively coevolving particle swarms for large scale optimization.IEEE Trans Evol Comput 2012;16:210 –24.https://doi.org/10.1109/ tevc.2011.2112662.[3] MacNish C, Yao X. Direction matters in high-dimensional optimisation. In: IEEEcongr evol comput (IEEE world congr comput intell 2008; 2008. https://doi.org/ 10.1109/cec.2008.4631115. [4] Ali MZ, Awad NH, Suganthan PN. Multi-population differential evolution withbalanced ensemble of mutation strategies for large-scale global optimization. ApplSoft Comput 2015;33:304–27.https://doi.org/10.1016/j.asoc.2015.04.019 . [5]Tang K, Y/C19ao X, Suganthan PN, MacNish C, Chen Y-P, Chen C-M, et al. Benchmarkfunctions for the CEC’2008 special session and competition on large scale globaloptimization. Nat Inspired Comput Appl Lab USTC, China 2007;24:1 –18. [6]Tang K, Li X, Suganthan PN, Yang Z, Weise T. Benchmark functions for the CEC2010 special session and competition on large-scale global optimization. NatInspired Comput Appl Lab USTC, China n.d 2010 . [7] Yang Z, Tang K, Yao X. Multilevel cooperative coevolution for large scaleoptimization. In: IEEE congr evol comput (IEEE world congr comput intell 2008;2008.https://doi.org/10.1109/cec.2008.4631014 . [8] Hsieh S-T, Sun T-Y, Liu C-C, Tsai S-J. Solving large scale global optimization usingimproved Particle Swarm Optimizer. IEEE Congr Evol Comput. 2008. https:// doi.org/10.1109/cec.2008.4631030 . IEEE World Congr Comput Intell 2008. [9] Ros R, Hansen N. A simple modi ﬁcation in CMA-ES achieving linear time and space complexity. Parallel Probl Solving from Nat –PPSN X 2008:296–305.https:// doi.org/10.1007/978-3-540-87700-4_30 . [10] Weber M, Neri F, Tirronen V. Shuf ﬂe or update parallel differential evolution for large-scale optimization. Soft Comput 2010;15:2089 –107.https://doi.org/ 10.1007/s00500-010-0640-9 . [11] Chen W, Weise T, Yang Z, Tang K. Large-scale global optimization using cooperativecoevolution with variable interaction learning. Parallel probl solving from nature.PPSN XI 2010.https://doi.org/10.1007/978-3-642-15871-1_31 . 300–9. [12] Omidvar MN, Li X, Yao X. Cooperative Co-evolution with delta grouping for largescale non-separable function optimization. In: IEEE congr evol comput; 2010.https://doi.org/10.1109/cec.2010.5585979 . [13] Wang H, Wu Z, Rahnamayan S. Enhanced opposition-based differential evolutionfor solving high-dimensional continuous optimization problems. Soft Comput 2010;15:2127–40.https://doi.org/10.1007/s00500-010-0642-7 . [14] Hedar A-R, Ali AF. Tabu search with multi-level neighborhood structures for highdimensional problems. Appl Intell 2011;37:189 –206.https://doi.org/10.1007/ s10489-011-0321-0.[15] Chu W, Gao X, Sorooshian S. A new evolutionary search strategy for globaloptimization of high-dimensional problems. Inf Sci 2011;181:4909 –27.
https:// doi.org/10.1016/j.ins.2011.06.024 . [16] Takahama T, Sakai S. Large scale optimization by differential evolution withlandscape modality detection and a diversity archive. In: IEEE congr evol comput;2012.https://doi.org/10.1109/cec.2012.6252911 . 2012. [17] Wang C, Gao J-H. A differential evolution algorithm with cooperativecoevolutionary selection operation for high-dimensional optimization. Opt Lett2012;8:477–92.https://doi.org/10.1007/s11590-012-0592-3 . [18] Chowdhury JG, Chowdhury A, Sur A. Large scale optimization based on Co-ordinated bacterial dynamics and opposite numbers. Swarm. Evol Memetic Comput2012;770–7.https://doi.org/10.1007/978-3-642-35380-2_90 . [19] Wang H, Rahnamayan S, Wu Z. Parallel differential evolution with self-adaptingcontrol parameters and generalized opposition-based learning for solving high-dimensional optimization problems. J Parallel Distr Comput 2013;73:62 –73. https://doi.org/10.1016/j.jpdc.2012.02.019 . [20] Wang Y, Huang J, Dong WS, Yan JC, Tian CH, Li M, et al. Two-stage based ensembleoptimization framework for large-scale global optimization. Eur J Oper Res 2013;228:308–20.https://doi.org/10.1016/j.ejor.2012.12.021 . [21] Fan J, Wang J, Han M. Cooperative coevolution for large-scale optimization basedon Kernel Fuzzy clustering and variable trust region methods. IEEE Trans Fuzzy Syst2014;22:829–39.https://doi.org/10.1109/tfuzz.2013.2276863 . [22] Omidvar MN, Li X, Mei Y, Yao X. Cooperative Co-evolution with differentialgrouping for large scale optimization. IEEE Trans Evol Comput 2014;18:378 –93. https://doi.org/10.1109/tevc.2013.2281543 . [23] Segura C, Coello Coello CA, Hern /C19andez-Díaz AG. Improving the vector generation strategy of Differential Evolution for large-scale optimization. Inf Sci 2015;323:106–29.https://doi.org/10.1016/j.ins.2015.06.029 . [24] Cheng R, Jin Y. A competitive swarm optimizer for large scale optimization. IEEETrans Cybern 2015;45:191–204.https://doi.org/10.1109/tcyb.2014.2322602 . [25] Singh D, Agrawal S. Self organizing migrating algorithm with quadraticinterpolation for solving large scale global optimization problems. Appl SoftComput J 2016;38:1040–8.https://doi.org/10.1016/j.asoc.2015.09.033 .
Fig. 4.The diagram block for a gas turbine's overspeed protection system.
Table 9Averageﬁtness values and standard deviations on real-world optimizationproblems.
Algorithms Problem 1 Problem 2Mean Std Mean StdGL-25 4.05Eþ000 9.83Eþ000 8.634E-001 8.114E-001 SaDE 2.72Eþ000 6.65Eþ000 8.898E-001 2.875E-002 CoDE 3.19Eþ000 8.54Eþ000 8.882E-001 6.155E-001 SPSO2013 7.64Eþ000 1.15Eþ001 8.730E-001 6.058E-001 HCLPSO 5.38Eþ000 1.29Eþ001 8.875E-001 1.464E-001 WGA 1.23E-007 1.08E-007 8.915E-001 9.628E-004M. Ghasemi et al. Array 11 (2021) 100074
10[26] Sun Y, Wang X, Chen Y, Liu Z. A modi ﬁed whale optimization algorithm for large- scale global optimization problems. Expert Syst Appl 2018;114:563 –77.https:// doi.org/10.1016/j.eswa.2018.08.027 . [27] Li J, Guo L, Li Y, Liu C. Enhancing elephant herding optimization with novelindividual updating strategies for large-scale optimization problems. Mathematics2019;7:395.https://doi.org/10.3390/math7050395 . [28] Shadravan S, Naji HR, Bardsiri VK. The Sail ﬁsh Optimizer: a novel nature-inspired metaheuristic algorithm for solving constrained engineering optimization problems.Eng Appl Artif Intell 2019;80:20–34.https://doi.org/10.1016/ j.engappai.2019.01.001. [29] Samareh Moosavi SH, Bardsiri VK. Poor and rich optimization algorithm: a newhuman-based and multi populations algorithm. Eng Appl Artif Intell 2019;86:165–81.https://doi.org/10.1016/j.engappai.2019.08.025 . [30] Cai X, Zhang J, Liang H, Wang L, Wu Q. An ensemble bat algorithm for large-scaleoptimization. Int J Mach Learn Cybern 2019;10:3099 –113.https://doi.org/ 10.1007/s13042-019-01002-8 . [31] Faramarzi A, Heidarinejad M, Stephens B, Mirjalili S. Equilibrium optimizer: anovel optimization algorithm. Knowl Base Syst 2020;191:105190. https://doi.org/ 10.1016/j.knosys.2019.105190 . [32] Salih SQ, Alsewari ARA. A new algorithm for normal and large-scale optimizationproblems: nomadic People Optimizer. Neural Comput Appl 2020;32:10359 –86. https://doi.org/10.1007/s00521-019-04575-1 . [33] Bas¸E, Ülker E. Improved social spider algorithm for large scale optimization. ArtifIntell Rev 2020:1–36.https://doi.org/10.1007/s10462-020-09931-5 . [34] García-Martínez C, Lozano M, Herrera F, Molina D, S /C19anchez AM. Global and local real-coded genetic algorithms based on parent-centric crossover operators. Eur JOper Res 2008;185:1088–113.https://doi.org/10.1016/j.ejor.2006.06.043 .[35] Qin AK, Huang VL, Suganthan PN. Differential evolution algorithm with strategyadaptation for global numerical optimization. IEEE Trans Evol Comput 2009;13:398–417.https://doi.org/10.1109/tevc.2008.927706 . [36] Wang Y, Cai Z, Zhang Q. Differential evolution with composite trial vectorgeneration strategies and control parameters. IEEE Trans Evol Comput 2011;15:55–66.https://doi.org/10.1109/tevc.2010.2087271 . [37] Zambrano-Bigiarini M, Clerc M, Rojas R. Standard particle swarm optimisation2011 at CEC-2013: a baseline for future PSO improvements. In: IEEE congr. Evol.Comput., IEEE; 2013; 2013. p. 2337 –44.https://doi.org/10.1109/ CEC.2013.6557848.[38] Lynn N, Suganthan PN. Heterogeneous comprehensive learning particle swarmoptimization with enhanced exploration and exploitation. Swarm Evol Comput2015;24:11–24.https://doi.org/10.1016/j.swevo.2015.05.002 . [39]Das S, Suganthan PN. Problem de ﬁnitions and evaluation criteria for CEC 2011 competition on testing evolutionary algorithms on real world optimizationproblems. Jadavpur Univ Nanyang Technol Univ Kolkata; 2010. p. 341
–59. [40] Chen T-C. IAs based approach for reliability redundancy allocation problems. ApplMath Comput 2006;182:1556–67.https://doi.org/10.1016/j.amc.2006.05.044 . [41] Das S, Abraham A, Chakraborty UK, Konar A. Differential evolution using aneighborhood-based mutation operator. IEEE Trans Evol Comput 2009;13:526 –53. https://doi.org/10.1109/tevc.2008.2009457 . [42] Wang H, Rahnamayan S, Sun H, Omran MGH. Gaussian bare-bones differentialevolution. IEEE Trans Cybern 2013;43:634 –47.https://doi.org/10.1109/ tsmcb.2012.2213808.[43] Zhang H, Hu X, Shao X, Li Z, Wang Y. IPSO-based hybrid approaches for reliability-redundancy allocation problems. Sci China Technol Sci 2013;56:2854 –64.https:// doi.org/10.1007/s11431-013-5372-5 .M. Ghasemi et al. Array 11 (2021) 100074
11