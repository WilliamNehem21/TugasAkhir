Low-cost livestock sorting information management system basedon deep learning
Yuanzhi Pana,b,∗, Yuzhen Zhanga,c, Xiaoping Wangd,X i a n gX i a n gG a ob, Zhongyu Houb,e,∗∗
aArtiﬁcial Intelligence Lab, Zhenjiang Hongxiang Automation Technology Co. Ltd., Zhenjiang 212050, Jiangsu Province, China
bSchool of Electronic Information and Electrical Engineering, Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200030, China
cSchool of Innovation and Entrepreneurship, Jiangsu University, 301 Xuefu Road, Zhenjiang 212013, Jiangsu Province, China
dShanghai Picowave Technology Co. Ltd., Shanghai 201802, China
eGuangzhou Institute of Advanced Technology, Chinese Academy of Science (GIAT), Guangzhou 511458, China
abstract article info
Article history:Received 22 April 2023Received in revised form 21 August 2023Accepted 23 August 2023Available online 24 August 2023Modern pig farming leaves much to be desired in terms of ef ﬁciency, as these systems rely mainly on electrome- chanical controls and can only categorize pigs according to their weight. This method is not only inef ﬁcient but also escalates labor expenses and heightens the threat of zoonotic diseases. Furthermore, con ﬁning pigs in large groups can exacerbate the spread of infections and complicate the monitoring and care of ill pigs. This re-search executed an experiment to construct a deep-learning sorting mechanism, leveraging a dataset infusedwith pivotal metrics and breeding imagery gathered over 24 months. This research integrated a Kalman ﬁlter- based algorithm to augment the precision of the dynamic sorting operation. This research experiment unveileda pioneering machine vision sorting system powered by deep learning, adept at handling live imagery for multi-faceted recognition objectives. The Individual recognition model based on Residual Neural Network (ResNet)monitors livestock weight for sustained data forecasting, whereas the Wasserstein Generative Adversarial Nets(WGAN) image enhancement algorithm bolsters recognition in distinct settings, fortifying the model's resilience.Notably, system can pinpoint livestock exhibiting signs of potential illness via irregular body appearances andisolate them for safety. Experimental outcomes validate the superiority of this proposed system over traditionalcounterparts. It not only minimizes manual interventions and data upkeep expenses but also heightens the accu-racy of livestock identiﬁcation and optimizes data usage. This ﬁndings reﬂect an 89% success rate in livestock ID recognition, a 32% surge in obscured image recognition, a 95% leap in livestock categorization accuracy, and a re-markable 98% success rate in discerning images of unwell pigs. In essence, this research augments identi ﬁcation efﬁciency, curtails operational expenses, and provides enhanced tools for disease monitoring.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Pig monitoringImage recognitionSorting systemWGANResNet
1. IntroductionWith large-scale production in the breeding industry, the use of arti-ﬁcial intelligence technology for automated breeding has become atrend. Livestock production in many parts of the world has become anindustrialized breeding business (Moekti, 2020), commonly referred to as industrialized animal husbandry. With increasing cost of labor inChina and rising instances of livestock disease caused by zoonotic dis-eases, larger-scale farms and enterprises have elevated their demandfor intelligent and automated technologies ( Latino et al., 2020). Adopting new technologies, such as the Internet of Things and deeplearning, provides more possibilities for the wider popularization of in-telligent and automated breeding (Neethirajan, 2020). Farms have adopted various technologies in view of the scale and nature of their op-eration. Large pig farms have adopted chip implantation technology byimplanting chips into pig ears to identify ( Marsot et al., 2020) and track (Pandey et al., 2021) pigs. The development process of the sorting sys-tem is depicted inFig. 1. Small pig farms generally employ statisticalmethods based on the use of traditional ear tag; however, this approachcan result in pigs biting each other's ears ( Calderón Díaz et al., 2018), and daily maintenance requires frequent manual intervention. There-fore, it is necessary to adopt a universally automated sorting system.With the development of new technologies, including pattern recog-nition, machine learning, artiﬁcial intelligence, and image recognition,more solutions are now available for the ef ﬁcient sorting of livestock.Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
∗Corresponding author at: Zhenjiang Hongxiang Automation Technology Co. Ltd.,Zhenjiang 212050, Jiangsu Province, China.
∗∗Corresponding author at: School of Electronic Information and Electrical Engineering,Shanghai Jiao Tong University, 800 Dongchuan Road, Shanghai 200030, China.E-mail addresses:yzpan@connect.hku.hk,yuanzhi_p@hotmail.com(Y. Pan), zhyhou@sjtu.edu.cn(Z. Hou).
https://doi.org/10.1016/j.aiia.2023.08.0072589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/By solving the many problems in previous sorting methods, labor costscan be effectively reduced, and production management ef ﬁciency can be improved. In our preliminary experimental study, it was found thatthe noise in theﬁeld environment, including the interference of visualacquisition and weighing equipment, affects the accuracy of the infor-mation acquisition, analysis, and judgment of the system. Ambientlight and image blockage lead to insufﬁcient or incorrect information contained in the image, thus affecting the detection accuracy. Fluctua-tions in the pressure of a moving hog as it passes through the weighingdevice can affect weight measurement, and thus the assessment of hoggrowth. Identiﬁcation is highly dependent on the environment, andwhen an image is obscured, the identiﬁcation accuracy is signiﬁcantly reduced.Image recognition, with its unique advantages of high automation,accuracy, and independence from additional consumables, has becomea crucial focus of sorting systems. Complex image quality issues, partic-ularly those involving glare spots or occlusions, necessitate auxiliarytechnologies associated with image completion to enhance image qual-ity. WGAN-based image completion algorithms have been previouslyproposed to address these challenges. The original generative adversar-ial network (GAN) faced issues such as unstable training and uncontrol-lable generation; The algorithm was later improved upon by replacingthe multilayer perceptron with a convolutional neural network (CNN),forming the deep convolutional GAN (DCGAN). While DCGAN offers in-creased stability in network training, it only mitigates some of the issuesplaguing the GAN. The GAN is further improved by introducing theWasserstein distribution distance GAN (WGAN), which provides amore stable training process (Radford et al., 2015). Algorithms that enhance model transferability are required in re-sponse to the abundance of unlabeled images and the demand for ro-bust generalization capabilities in real-world productionenvironments. Transfer learning uses previously acquired knowledge(source domain with ample labeled data) to facilitate the solution ofthe current task. For instance,Miller et al. (2000)applied shared density to the digital transformation problem ( Pan et al., 2011), and later de- signed a generative model based on the variational Bayesian framework(Miller et al., 2000). With the advent of CNNs in computer vision tasks,deep-learning methods are being increasingly employed to solve few-shot learning problems.Neural network algorithms have evolved to address technical chal-lenges and improve the reliability and accuracy of models in complexand diverse application scenarios. Neural network ﬁne-tuning, a type of deep transfer learning (Neethirajan, 2020), adapts pre-trained models with general feature extraction capabilities to target networks,adjusting the network structure based on speci ﬁc training tasks. Fine- tuned model parameters are then trained on target datasets and appliedto individual recognition tasks (Yosinski et al., 2014). Residual networks address the degradation problems in CNNs ( Tzeng et al., 2015). In terms of application,Pan et al. (2022)proposed a computer vision-based rec-ognition framework for distinguishing Neli-Ravi breeds from other buf-falo breeds, achieving an accuracy of 93% using support vector machinesand over 85% with recent variants.Kashiha et al. (2013)retained the pattern features of 10 pigs using Fourier descriptors with rotationaland translational invariance and achieved 88.7% accuracy in pig patternrecognition.Generative adversarial networks play a crucial role in generativemodels. The ResNet network is an effective deep-learning algorithm.Research on pigs encompasses various aspects including the processingof biological information (Wang et al., 2022), artiﬁcial intelligence breeding, and behavioral recognition. Furthermore, livestock researchincludes studies on mobile systems (Fuentes et al., 2022), digitalization of animal husbandry (
Neethirajan, 2022), and emotional state of live- stock (Wanga et al., 2015). Previous studies observe that the main char-acteristics of pigs are distributed in the skin, eyes, ears, nose, mouth, tail,and limbs, and their study can offer insights into the differences in pigcharacteristics. The main focus of this study is the extraction of thesefeatures and their application in recognition sorting. The feature de-scriptions are listed in theTable 1.Based on the study of pig characteristics, we constructed a datasetcontaining the disease, breeding, growth, and fattening status of pigfarms collected over two years of study to develop a deep-learningsorting system. The designed system processes ﬁeld images with multi- ple feature-recognition tasks, such as disease and illness, sex and breed-ing, growth, and fattening.
Fig. 1.Developmental history of livestock sorting.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
111This study identiﬁes the shortcomings of deep-learning-based ap-proaches applied in the livestock sorting ﬁeld and aims to design a pig sorting system that encompasses information collection, data process-ing, optimized algorithm training, and relevant applications. In thisstudy, a residual network algorithm model based on image completionthat provides a low-cost and tractable solution for classi ﬁcation and pre- diction by integrating weight and image data is proposed.2. Materials and methodsIn this study, a sorting system that uses image acquisition to sort pigsis designed. Brieﬂy, pigs enter the sorter from one end, and the sorterobtains signals toﬁx the pigs and collect data such as images andweights. After processing, the different gates are opened according tothe sorting requirements for partitioning.The sorting process:(1) The systemﬁrst enables the pigs to pass in sequence through theinfrared identiﬁcation device at the entrance by means of a ﬂex- ible position-limiting device that detects the imminent entry ofthe pigs and opens the front door. After the infrared detection de-vice detects that the pig has fully entered, the front door is closed.(2) The system weighs the weight of the pig through an electronictray and feeds the information back to the system. The pig im-ages are collected through an image acquisition module.(3) The system detects the health of the pigs through image acquisi-tion using a pre-trained model. Common pig diseases can causeabnormalities in the eyes, ears, nose, and skin. The system iso-lates sick pigs using image recognition, and the sick pigs are sub-sequently treated manually.(4) The system detects the collected images and compares differenttypes and stages of pigs for sorting using a deep-learningmodel. The corresponding sorting gates are opened to achievean orderly sorting of pig groups.(5) The ID weight data are managed to obtain the expected evalua-tion results.2.1. System modelThe proposed system consists of two parts: hardware and software.The hardware components includes weight and image detection de-vices and the necessary mechanical parts, such as ﬂexible limiters and sorting doors. The software encompasses image acquisition,Table 1Morphological features of ternary pig.Sr.#Characters Description Image1 Marking The skin of pigs usually exhibits a uniform pink or light pink color. There may be some black or dark brown spots on the skin,typically distributed on the ears, face, and legs.
2 Eye The eyes are typically black or brown in color, with a dark appearance. The eyes are typically located on the sides of the headand slightly inclined forward.
3 Ear The ears are usually medium-sized and have a slightly drooping shape. The color of the ears can vary, but they are usually pinkor light pink in color.
4 Nose The noses are usually ﬂat and snout-like in shape. The nostrils are typically large and oval-shaped, allowing for ef ﬁcient air intake.
5 Tail The tail is cylindrical in shape and tapers to a point. The tail is used for communication and can be moved to express different emotionssuch as excitement, happiness, or aggression.
6 Feet They typically have four toes on their front feet and three toes on their hind feet. The feet are relatively small in proportion to theirbody size. The feet are compact and rounded in shape, with short, sturdy toes.
7 Healthy Skin is smooth, elastic, and has well-arranged hair with normal sebum. Color varies from pink to light brown. Eyes are clear,secretion-free. Oral mucosa has normal color and saliva. Nose is moist without discharge. Limbs are strong, and joints lack redness,swelling, or inﬂammation.
8 Diseased Skin exhibits itching, redness, rashes, hair loss, and dullness. Eyes show congestion, edema, and increased secretions. Oral mucosa mayhave redness, ulcers, and bleeding; nose is dry or has discharge. Diseased pigs' limbs may have joint in ﬂammation and unsteady gait, or limping. Weight gain is unstable, appetite is poor, and weight loss is rapid or emaciation occurs.
Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
112recognition, and sorting, which are implemented using deep-learningalgorithms. A structural diagram of the system is shown in Fig. 2.I n the case of pigs, livestock enter the sorting system from the mixedpens and areﬁnally identiﬁed in the corresponding pens through theentry, identiﬁcation, and sorting modules of the sorter, as shown inFig. 2(A). The processes of sorting, modular device, and hardware orga-nization are described in this study, and the star network structure isshown inFig. 2(B). Modular channels, as shown inFig. 2(C), have also been designed to achieve innovation from a traditional sorter, severalof which together with the sorter, form an enclosed sorting area toachieve fully automated sorting. The hardware structure of the systemis shown inFig. 2(D). Aﬂowchart of the system is shown inFig. 2(E). The system has three components: detection and entry, data acquisi-tion, and sorting and exit. Each part is designed in a modular manner.During the detection and entry processes, the system detects the entryof pigs through an infrared transceiver in front of the entrance gateand opens the gate. After the weighing module detects that a pig has en-tered the system, it closes the gate. The data acquisition process collectsinformation such as pig weight and images.In the experiment, a modular assembly device was designed, and ashort-distance star communication network was built. It is controlledby a central control module, and each submodule has an independentcontrol unit responsible for circuit control within the module. Eachsubmodule is physically combined to form a sorting module consistingof at least one inlet, one detection, and several outlet modules. Similarly,connectors made of soft materials can be used to build pig passages withacoustic and optical devices to assist in the repelling of pigs and reducehuman intervention; the components are assembled into the pigsty in acombination of soft and hard materials. Pigs can automatically completethe sorting process when driven by sound and light devices throughguided passages.The sorting process is divided into two parts. First, health detectionwas performed using the collected pig photographs. Combining theweight and visual information with a pre-trained binary classi ﬁcation model and decision-level fusion determines whether the pig is abnor-mal or sick, and sick pigs are sorted into isolation pens for workers tocheck and treat.A fusion of the image neural network and weight-based classi ﬁca- tion results is used to improve the accuracy of the experiment and ob-tain theﬁnal target classiﬁcation and recognition results throughdecision fusion. The ID and category of the pig are identi ﬁed using the image, and its weight is measured using a weighing device. Sorting isperformed by fusing the historical data of the pig with the new data ac-cording to its ID and evaluating the growth status of the pig and itsgrowth level in the group. Simultaneously, image recognition detectswhether there is a possibility of disease in the phenotype of the pigand sorts the pig to the diseased pen when it is detected as having a pos-sible disease or abnormal weight.2.1.1. Mechanical deviceWhen the infrared detection device detects an obstacle and con ﬁrms that pigs are waiting to enter, the system controls the motor to open theentrance gate upward to guide the pigs in. After the electronic tray in-side the device conﬁrms that the pig has entered and detects no furtherobstacles in the infrared detection device, it closes the entrance gate andcollects the weight data of the pig. After the image acquisition devicehas collected pictures and completed the classi ﬁcation work, the me- chanical control device opens the corresponding partition exit gates, let-ting the pigs out of the sorting device. To sequentially guide the pigs intothe sorter, aﬂexible position-limiting device is designed, as shown inFig. 3. When the pig passes the threshold of ground pressure sensor B,theﬂexible limit device approaches and restricts the individual passing
Fig. 2.(A) Automatic sorting systems in pig houses. (B) Star network structure. (C) Schematic of the module components. (D) Schematic of the system control. (E) Systemﬂow chart.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
113of the pig. Side pressure sensor A detects contact and stops displace-ment; the pressure change curves of sensors A and B are shown inFigs. 3(C) and (B), respectively.To achieve the sequential entry of pigs into the sorter, the position-limiting device can adjust the limiting space for different pigs. The en-trance and detection modules are separated by a mechanically con-trolled gate that opens outward. The exit gate of the detection moduleopens into various branch channels. After the central processing modulemakes a sorting decision, the controller controls the opening of thegates on the corresponding sorting lanes via a star network communica-tion to guide the pigs and complete the fast sorting.2.1.2. Image acquisition deviceThis study considers the impact of the spectral characteristics of dif-ferent light sources on the image acquisition devices in terms of factorssuch as cost and lifespan (Yin et al., 2012). Incandescent lamps are the most common light source, producing large amounts of infrared energy,and are inexpensive. They can also extend their usage time by operatingat low voltages, but at the same time, they have the problem of low lu-minous efﬁciency (Dong et al., 2015); halogen lamps have a long servicelife and do not distort color but generate more heat; high-frequencyﬂuorescent lamps generate less heat and have a long service life, buttheir color rendering is unsatisfactory ( He et al., 2016a, 2016b); LED lamps produce less heat and have good monochromaticity. Theyachieve all colors in the visible light band with low power consumption.When facing moving objects, they exhibit good shock and impact resis-tance (He et al., 2017). Therefore, LED lights are used as the visual lightsource for the device (Kwon and Casebolt, 2006). Therefore, this exper- iment was designed using the image acquisition device, as shown inFigs. 4(A) and (B).The LEDﬁll light strip is located on both sides of the inner wall of thesorting device at a height of 1050 mm from the ground and acts an effec-tive supplementary light source. Two cameras are located 1300 mmabove the ground on the central axis of the sorting device and350 mm above the ground on the exit door. When entering, the camerais turned on after the gate is closed. Pictures of the front of the head ofthe pig, including the ears, nose, eyes, and mouth, and pictures of theneck and back from above are collected for sorting. The cameras are cen-tered and located 1200 and 400 mm above the pigpen. To verify the ro-bustness of the model, the light sources of the acquired images includenatural, indoor, LED, and incandescent lights, as shown in Fig. 4(C). It was found that the age range of 50–150 days is a fast-growing stagefor pigs with signiﬁcant research value. In addition, the classi ﬁcation of pigs according to their development can better evaluate their growth.Common pig diseases often present symptoms in the ears, nose,eyes, mouth, and skin (Ouyang and Ren, 2023), as shown inFig. 5(A). Images acquired at important locations can effectively extract essentialinformation (Robbins et al., 2014).Images were collected over a period of three years from farms inShandong Province, China. The image and weight data were tracked
Fig. 3.(A) Position-limiting device. (B) Operation of pressure sensor B. (C) Operation of pressure sensor A.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
114and summarized as statistics, as shown in Figs. 5(C). The age groups of the pigs used for data collection included nursery pigs and pigs aged50 days to farrowing. The species were mainly Chinese ternary cross-bred pigs, including gestating sows and male breeding pigs. Image en-hancement was also used to expand the dataset, including arbitrarilyrotated images, images enhanced by Gaussian blur, and arbitrarily cap-tured images, as shown inFig. 5(B). Owing to the relative lack and lim-ited variety of image samples collected from the sick pigs, sick pigimages were expanded using an internet dataset.2.1.3. Automatic weighing deviceA voltage-type weighing sensor was used as the weighing device.Because the irregular movement of live pigs entering the device inter-feres with the measurement of body weight, Kalman ﬁltering is used to predict the true weight of the live pigs. The voltage-type weighingsensor obtains real-time pressure and transfers it to the system. The sys-tem uses a Kalmanﬁltering algorithm to achieve dynamic weighting.The process is illustrated inFig. 6(A), and the operation of the Kalmanﬁlter is expressed as:X
k¼AX k−1þBU k−1ð1ÞP
k¼AP k−1ATþQ ð2ÞHere,X
kis the state of the system at time k; A is the state transitionmatrix related to the studied system; B is the input control matrix; U, anerror matrix, is the external effect on the system; and Q is the predictednoise covariance matrix (Kristensen et al., 2012). Eq.(1)represents the prediction of the state, wherein X
k−1estimates the state at time k by re-cursively using the state at time k−1. Eq.(2)represents the prediction of the error, wherein P
k−1uses the covariance at time k-1 to recursivelyobtain the covariance estimate at time k.K
k¼P kHTHPkHTþR/C16/C17−1
ð3ÞXk¼X kþK kZk/C0HX k ðÞ ð 4ÞP
k¼I/C0K kHðÞP k ð5ÞHere, K
krepresents the Kalman gain, H is the observation matrix, R isthe measurement noise covariance matrix, and Z
kis the observation value at time k. The Kalman gain can be calculated using Eq. (3). The corrected state and output of Kalmanﬁltering are described by Eq.(4). The updated covariance matrix is expressed in Eq. (5).A ss h o w ni n Fig. 6(B), the effect of the Kalmanﬁltering algorithm is signiﬁcantly bet- ter than that of the average valueﬁltering algorithm. The Kalmanﬁlter- ing results signiﬁcantly improve the sorting effect of the system.The experiments were analyzed and evaluated using simulationtests, and the simulations were conducted in the range according tothe actual weight. Let the state transfer matrix A be set as a unit array.The system does not inﬂuence the external input; U is zero, and H is aunit matrix. The gain matrix K does not need to be initialized, and theerror matrix P is initialized as a zero matrix. Q and R are the predictedand observed state covariance matrices, respectively, and Q and R areset to 0.000015 and 0.008, respectively ( Wang et al., 2017). A set of typ- ical waveform graphs that can be considered as irregular motion werecollected, leading toﬂuctuations in real-time weight information. Refer-ring to the actual scenario using different data between 20 and 100 kgfor the experiment, different sets of 20 data points were collected to cal-culate and compare the average and Kalman ﬁltering errors. The experimental results show that after the original weighing ﬂoat, as shown inFig. 6(C), is Kalmanﬁltered, the weighing error is main-tained within ±0.7%, as shown inFig. 6(D), which is clearly better than the averageﬁlter result and can meet the weighing requirementsof the system.2.2. Algorithm modelIn this study, the backbone network of the algorithmic model isResNet, as shown inFig. 7(A). For occluded images, a WGAN-basedimage completion algorithm was employed to ﬁll in the occlusions, thereby enhancing the overall recognition accuracy. Pre-trained models
Fig. 4.(A) Top view of the image acquisition device. (B) Front view of the image acquisition device. (C) Acquisition of collated matrix images. C series from C 1t oC 6a r e :s t u n t e d ,p o o r l y developed, normally developed, well developed, extremely well developed, and disease state. D series from D0 to D6 are: nursery pigs, 50 to 70 days old ,7 0t o9 0d a y so l d ,9 0t o1 1 0d a y s old, 110 to 130 days old, 130 to 150 days old, and 150 days old to farrowing.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
115were used for transfer learning on local datasets to facilitate the rapidtraining of the lower-level network. Additionally, techniques such ashyperparameter adjustment and freezing layers were adopted to im-prove the model performance; the application of the ResNet-50 net-work in this study is depicted inFig. 7(B).2.2.1. WGANSince their introduction byGoodfellow et al., 2020,t h ed e v e l o p - ment of GANs has been rapid. Gene rative adversarial networks have been widely used in variousﬁelds, such as style transfer, image restoration, and font generation. Nash equilibrium theory isthe core concept behind generating adversarial networks. In adver-sarial networks, both sides constantly confront each other to achievetheir respective goals of maximizing interests. When the result of theconfrontation reaches equilibrium, even if the optimization methodson both side are not necessarily globally optimal solutions, their re-sults have practical signiﬁcance. The architecture of GANs includesa generator and a discriminator network ( Goodfellow et al., 2020). The generator and discriminator networks continuously competewith each other to reach the Nash equilibrium, as shown in Fig. 8(A).Fig. 8(B) shows a generative network from a uniform distribu-tion to a normal distribution during the learning process.The generator is responsible for generating images that are close tothe real samples. By learning the data distribution of the real sample,the generator generates a new sample G(z) based on the acquirednoise Z, which determines whether the input/output data belong to areal sample or a generated sample. When the discriminator receivesimage data x, the output D xðÞis determined as the probability that x isreal. If D xðÞ= 1, x is a real sample, and if D xðÞ= 0, x is not a real sample. The GAN obtains a generative data distribution P
Gx;θðÞas close as pos- sible to the real data distribution P
dataxðÞbased on the random noise Z through a generator G.θ, which is a network parameter, adjusts thesimilarity between P
dataxðÞand P Gx;θðÞ. Adversarial training between the generator and discriminator aims to minimize the respective lossvalues. The generator generates a data distribution that is as close tothe real situation as possible and is agreed upon by the discriminatorsuch that D G zðÞðÞtends to 1. At this point, the loss function value ofthe generator is minimized. The discriminator is expected to distinguishas accurately as possible whether the acquired data originate from thetrue or generated distribution, that is, D G z ðÞðÞtends to zero and D xðÞ
Fig. 5.(A) Different types and degrees of diseased pigs (images of sick pigs have been collected from the internet). (B) Image enhancement. (C) Data collecti on column chart.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
116tends to one. The loss functions of the generator and discriminator areconstructed as follows:loss
G¼log 1/C0DGzðÞðÞðÞ ð6Þloss
D¼−logD xðÞ þlog 1−DGzðÞðÞðÞðÞð 7ÞGenerator networks seek to generate optimal generators G
∗Das: G
/C3D¼arg min GDiv P G;Pdata ðÞ; ð8ÞThe above equation indicates the scatter between the actual andgenerated data distributions, and the optimal generator refers to theminimum difference between the two distributions. In ﬁxing the gener- ator, the maximization objective function of the discriminator isexpressed as:VG , DðÞ ¼E
x∼PdataxðÞlogD xðÞ½/C138 þE z∼PGzðÞlog 1/C0DGzðÞðÞðÞ½/C138¼Z
xPdataxðÞlogD xðÞdxþZ
xPGxðÞlog 1/C0DxðÞðÞdx¼Z
xPdaazxðÞlogD xðÞdxþP GxðÞlog 1/C0DxðÞðÞ½/C138 dxð9ÞDiscriminator networks seek to generate optimal discriminatorsD
∗Gas:D
/C3G¼arg max DVG;DðÞ ð 10ÞBecause the expected values of the generator and discriminator forVG , DðÞare exactly opposite, the generator and discriminator iteratecontinuously to achieve the optimal objective function of the overallnetwork, solve the very small game, and achieve Nash equilibrium:minGmax DVG;DðÞ ¼E x∼PdtatxðÞlogD xðÞ½/C138þE
z∼PzzðÞlog 1−DGzðÞðÞðÞ½/C138 ð11ÞDuring the training process, the generator continuously improves itsgenerative power to generate images that are as similar as possible tothe real pattern to successfully fool the discriminator into thinkingthat it is the real image. The discriminator must continuously improveits discriminative power to avoid being fooled by the generator asmuch as possible. These two models iteratively play each other out tooptimize the network and achieve a balance between them. When thegenerator isﬁxed, the discriminator is trained iteratively until it reachesthe highest accuracy, achieving D
∗xðÞ ¼P dataxðÞ=P dataxðÞ þP GxðÞðÞ. After several iterations, the network model reaches the ideal state,where the distribution of the generated data is in ﬁnitely close to the ac- tual data, achieving P
dataxðÞ=P GxðÞ, and the discriminator cannot rec-ognize that the input data belong to the generator or dataset, thusachieving D G zðÞðÞ=0 . 5 .Although GANs have been greatly improved, they are characterizedby easy collapse and difﬁcult convergence owing to gradient disappear-ance and discriminator gradient invariance. Arjovsky et al. (2017)im- proved the GAN from the perspective of the loss function andproposed a novel Wasserstein distribution distance generative adver-sarial network (WGAN); they theoretically proved that the JS scatter(cross-entropy) interferes with the training stability of the originalGAN. The JS scatter measures the distance between two probability dis-tributions with non-overlapping regions and always obtains the con-stant log 2, which causes the network gradient to disappear. TheWasserstein distance is formulated as follows:WP
r,Pg/C0/C1¼inf
γ∼ΠP r,PgðÞEx,yðÞ∥x/C0y∥½/C138 ð 12Þ
Fig. 6.(A) Kalmanﬁltering process. (B) Comparison of Kalman ﬁltering results when the true value is 40. (C) Actual weight waveform when the weight value is 50. (D) Comparison of averageﬁltering error and Kalmanﬁltering error for 20 sets of data.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
117where P ris the real data distribution, P gis the generated data distribu- tion, andΠP
r,Pg/C0/C1represents the set of joint data distributions formedby the combination of former two distributions; further, x, y ðÞ∼γis a dis- tribution x from the data set and a distribution y from the generator ob-tained from the joint data distribution;∥x/C0y∥is the spatial distance between distribution x and distribution y; E
x,yðÞ∥x/C0y∥½/C138represents the expectation value of the joint distribution for this spatial distance;
infγ∼ΠP
r,P8 ðÞEx,yðÞ∥x/C0y∥½/C138represents the lower bound of the jointdistribution on the ideal value of the distance. The Wasserstein distanceallows the determination of the relationship between Prand P gwithout requiring an overlapping region. The use of the Wasserstein distance re-quires the Lipschitz continuity, and an additional limit is imposed on thecontinuity function such that it satisﬁes the Lipschitz continuity. Dis- criminator f
wis constructed with parameter w and a nonlinear activa-tion function at the end of the output layer to restrict parameter w toas p e c iﬁc range and maximize the objective function.
Fig. 7.(A) Overallﬂow chart of the feature extraction algorithm. (B) Network structure for livestock feature extraction using ResNet-50 as the backbone network.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
118L¼E x∼PrfwxðÞ½/C138 /C0E x∼PgfwxðÞ½/C138 ð 13ÞThe generator loss (G loss) function L
Gand discriminator loss (D loss) function L
Dof the WGAN are deﬁned as follows: L
G¼−E x∼PgfwxðÞ½/C138 ð 14ÞL
D¼E x∼PgfwxðÞ½/C138 /C0E x∼PrfwxðÞ½/C138 ð 15ÞThus, the WGAN can solve the problems of the GAN, namely, train-ing instability and collapse-mode diversity, thus ensuring diversity ofthe generated samples. In this study, a mask M was added to the inputimage as a broken image for the generator to generate the model, andthe WGAN algorithm was used. Furthermore, the contextual loss func-tion Lcontextual and the perceptual loss function Lperceptual wereadded to ensure that the same input image was obtained, and the con-textual and perceptual losses were combined with the argmin functionas Z. Herein,λfunctions as a hyperparameter and is used to control thedegree of importance of the contextual loss compared to perceptualloss. In this study, the default setting isλ= 0.1. The losses are calculated as follows:L
contextual ZðÞ ¼‖M⊙GzðÞ /C0M⊙y‖ð16ÞL
perceptual ZðÞ ¼log 1/C0DGZðÞðÞðÞDGZðÞðÞ Þ ð17ÞLzðÞ¼L
contextual zðÞþλL perceptual zðÞ ð18Þz
/C3¼arg min zLzðÞ ð 19ÞIn training, the weight of the GAN loss in the generator loss must beadjusted to ensure that the G loss and GAN loss are on equal footing, orfailing that, the G loss is one scale larger than the GAN loss. At the sametime, the ratio of the training time of the generator and to that of the dis-criminator is adjusted. Additionally, the discriminator is trained ﬁve times, while the generator is trained only once when conducting the ex-periment; the learning rate should not be excessively large, and trainingoptimization is performed using the Adam optimizer.2.2.2. Residual network modelHe et al. (2016a, 2016b)proposed a ResNet network structure, alsocalled a residual network, to solve the degradation problem of deep net-works. The basic idea is to add the concept of residual learning to thetraditional CNN to transform the learning of the output into the learningof the residuals to solve for the gradient dispersion and accuracy degra-dation in deep networks (He et al., 2016a, 2016b) and control the speed while ensuring accuracy (Krizhevsky et al., 2012). The residual network structure used is ResNet-50. The residual module ﬁrst reduces 256 channels to 64 channels by 1 × 1 convolution and then recovers forthe residual network unit by 1 × 1 convolution in 256 channels, asshown inFig. 9(A).Fig. 9(B) shows a single residual network unit.Deeper CNNs are used after applying the Visual Geometry Group(VGG) model. However, it has been experimentally shown that an in-crease in network depth causes the gradient to explode and disappear,resulting in a system that does not converge ( Lee et al., 2019). In the VGG model, the network is guaranteed to converge by using stochasticgradient descent (SGD) in backpropagation through a data normaliza-tion operation on the input data and intermediate layers; however,when the depth of the model network exceeds a dozen layers, SGD be-comes ineffectual. To address this problem, a deep residual network,ResNet, has been proposed, which allows the network to be as deep aspossible. However, it feeds a portion of the input data directly to the out-put without going through the convolutional network, and retains someof the original information. The ResNet network can add deeperconvolutional layers to enhance this effect, and the residual block struc-ture plays a key role in its functioning.If the dimension of the residual mapping F(x) is different fromthat of the jump connection X, it cannot be summed up; therefore,X must be up-dimensioned to ensure equality of dimensions beforesumming up; thus, a 1 × 1 speciﬁc convolutional kernel with stride =2 is usually added to the jump connection X to ensure that the outputof X is the same as that of the convolutional block. When residualblocks are stacked repeatedly, network structures of different depthscan be formed (Brito et al., 2019). After experimenting with variousdepths of networks, such as ResNet-18, ResNet-34, ResNet-50,ResNet-101, and ResNet-152, a 50-layer ResNet-50 network was se-lected for this study based on its practical effect and computationalvolume.The principle behind the ResNet network is the assumption that arelatively shallow network has reached saturation accuracy and the ad-dition of several constant mapping layers (identity mapping, y ¼x, out- put is equal to input) thereafter, increasing the network depth.However, the error does not increase; that is, a deeper network doesnot increase the error of the training set, and the residual structure iscalculated as follows:y
1¼hx lðÞ þFx l,Wl ðÞ, ð20Þx
lþ1¼fylðÞ, ð21Þwhere x
land x lþ1denote the input and output of theﬁrst residual unit, respectively, where each residual unit contains a multilayer structure; Fis the residual function, which denotes the learned residual; and H x ðÞ ¼ X
ldenotes the constant mapping; and f is the ReLU activation function.The learned features from shallow layer 1 to deep layer L can be ob-tained as:x
L¼xtþ∑L/C01i¼1Fxi,Wi ðÞ ð 22ÞUsing the chain rule, the gradient of the inverse process can beobtained as:
Fig. 8.(A) Generative adversarial network framework. (B) Generating a web-based learning process.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
119∂loss∂x
l¼∂loss∂x
L/C1∂xL
∂xl¼∂loss∂x
L/C11þ∂loss∂x
L∑L/C01i¼lFxi,Wi ðÞ ! ð23ÞTheﬁrst factor
∂loss∂x
Lin the above equation indicates the gradientreached by the loss function, and the “1”in parentheses indicates that the short-circuiting mechanism can propagate the gradient withoutloss, while the other residual gradient must pass through the layerwith weights, and the gradient is not passed directly. The residual gradi-ent is incomplete. Even if the value is small, the presence of 1 does notcause the gradient to vanish, making residual learning easier.The main structure of the ResNet-50 network consists of oneconvolutional layer and four residual blocks, each containing severalbottleneck residual units with different numbers of frozen residualunit parameters.2.2.3. Model trainingIn the identity classiﬁcation experiment, pig identiﬁcation was per- formed using the ResNet-50-based model pre-trained on the ImageNetdataset. After pre-training on the ImageNet dataset, the convolutionallayer of the ResNet-50 model was frozen, and the parameters of thefully connected layer were retrained to obtain the accuracy of the net-work. The captured images were classiﬁed and saved forﬁltering and cropping, with 6000 images as the training set, 2000 images as the val-idation set, and 2000 images as the test dataset.In the category classiﬁcation experiment, the pigs were classi-ﬁed intoﬁve categories according to their growth and development.Five groups of classiﬁed images, each containing 1800 images, wereused to divide the training, validation, and test sets in a 3:1:1 ratio.The initial learning rate used for the experimental training of allCNNs was 0.003, and the loss function was a cross-entropy lossfunction.This study expanded the dataset to 1200 images by screening 300images of sick pigs. The dataset was expanded to 800 images using arbi-traryﬂipping, Gaussian noise, and cropping and mixed with 1200 im-ages of healthy pigs to divide the training, validation, and test sets in aratio of 3:1:1. The pre-trained models were migrated to the target net-work for training. Most sick pigs develop lesions on their face andback, which can be sorted out to isolate the sick pigs through speci ﬁc fences. The camera unit of the sorting system captures images of theback and face in a model trained after pre-training and ﬁne-tuning. The two images detected simultaneously were assigned to the predic-tion model, and the results obtained were fused to obtain the classi ﬁca- tion results. The weights of the detection results for the face and bodyimages are 40% and 60%, respectively. The fused results were obtainedas the detection results of the diseased pigs.2.3. Data predictionIn this study, age and body weight data were obtained by aggregat-ing data collected fromﬁeld and public datasets. The main reference forprediction byﬁtting growth curves is the less disturbed and most vigor-ous developmental age of 50–150 days.Fig. 10(A) shows a scatter plot of the weight data used in this study, and Fig. 10(B) shows the cumula- tive weight and growth coefﬁcient of the pigs counted once every10 days.Growth curves for the body weight of the pigs were obtainedandﬁtted nonlinearly using a logistic model with the followingequation:W¼A=1þe
a−rtðÞ/C16/C17 ð24ÞR
2¼1/C0∑y/C0by/C0/C12=∑y/C0 yðÞ2ð25ÞHere, W is the weight at t days of age (kg), A is the limiting weightparameter (kg), and b is a constant scale, r is the growth rate parameter,and e is a constant. The pig coefﬁcient parameter equation is expressedas follows:c¼
Wt2/C0W t1
Wt2/C2Δt ð26ÞThe model calculates the following parameters in logistic regression:K = 154.60; a = 26.75;r= 0.022; R
2= 0.9956. The pigs were divided into six categories: low development level (C1), sub-low developmentlevel (C2), normal development level (C3), higher development level(C4), high development level (C5), and abnormal development (C6).The absolute weight and recent daily growth coef ﬁcients obtained from the last two statistics are examined in the study, both of whichare below 90% of the normal levels for low development level; one isbelow 90% of the normal level and the other is below average for thesub-low developmental level; both are above 90% of the normal levelsfor the high development level, and one is above 90% of the normallevel and the other is above average for the high developmental level.The rest were classiﬁed as having a normal development level, as wellas abnormal development owing to factors such as disease. Throughthe classiﬁcation ofﬁve categories characterizing the growth level ofthe pigs, a statistical analysis of the integration of the ﬁve categories of
Fig. 9.(A) ResNet network. (B) Residual network unit.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
120normal development and abnormal onset was performed. In the pres-ent study, aﬁve-category sorting mechanism was designed for pigs atdifferent developmental stages to evaluate and predict their growthusing weight and growth coefﬁcients.Byﬁtting the growth curves, scientiﬁc statistics and predictions were obtained for the studied pigs. A folded histogram of the statistics isshown inFig. 11(A). The bar chart shows the cumulative growth as anS-shaped curve, where the line graph denotes the growth coef ﬁcient,
Fig. 10.(A) Pig weight scatter plot. (B) Pig weight data.
Fig. 11.(A) Cumulative growth and growth coef ﬁcient of pigs from 50 to 150 days of age. (B) Predicted weight data (kg) of pigs in different classes at each stage after classi ﬁcation based on ﬁve categories. (C) Line graph of predicted weight data for each class. (D) Point plots of predicted weight data for each class.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
121whichﬁrst increases and then decreases. The weights of the pigs from50 to 180 days of age are also predicted based on the ﬁtted growth curve, and the data are shown inFig. 11(B). The line and points plots for the predicted weight are shown in Figs. 11(C) and (D), respectively. Five categories of normal pigs and one category of abnormal pigswere classiﬁed, and theﬁve statistical categories of normal develop-ment and abnormal morbidity were integrated and analyzed.Numerous pieces of information regarding pigs can be obtained usingthe above method, which are aggregated to design a sorting monitoringsystem.A visualization interface is shown inFig. 12. The visualization inter- face can display livestock data more intuitively and automate sortingmanagement.3. Results and discussion3.1. Image training resultsImage recognition comprises three components: individual recogni-tion, classiﬁcation recognition, and diseases recognition, all with goodrecognition accuracies. For the effect of occlusion on image recognition,the WGAN image complementation algorithm is used, and the feasibil-ity of the experimental design is demonstrated by comparing thesource, occlusion data, and complementation data.3.1.1. Identiﬁcation resultsThe images in the test dataset were fed into the trained pig identi ﬁ- cation model for pig identiﬁcation during testing, and the images suc-cessfully identiﬁed were deposited into the corresponding dataset.The highest accuracy was obtained in theﬁrst 25 frozen layers. Simulta- neously, it is necessary to make a timely update the dataset. As the pigsgrow and develop rapidly, increasing the sorting frequency guaranteesidentity recognition accuracy. The training was performed for 100epochs, 750 iterations, and a batch size of 8. The learning rate was0.005, and ReLU was used as the activation function. The loss functionversus accuracy curves for the training and test sets are shown inFigs. 13(A) and (B), respectively.Identity recognition is highly dependent on the environment, andthe recognition accuracy drops drastically when the image is occluded.The image complementation method of WGAN was used to add amask to the original input image, combining context and perceptuallosses. Context loss ensures that the obtained image is identical to theinput image, and perceptual loss ensures the authenticity of the imageused toﬁnd the complementary restored image. In this study, the pre-trained WGAN model wasﬁrst tested on the LSUN dataset for image re-construction, based on which, the pre-trained model was used to com-plete the image restoration task. The effect of image restoration isshown inFig. 14(A).In the experiment, the complementary images were retested, anda signiﬁcant improvement in the accuracy rate was observed, asshown inFig. 14(B). The overall recognition rate of the occludeddata is only 60.3%, whereas the recognition rate of thecomplemented image reaches 82.3%, which is close to the recogni-tion rate of 88.9% for the original image, thus satisfying the require-ments of the experimental design and improving the robustness ofthe classiﬁcation model. A sample of ten identiﬁed pigs was selected to plot the confusion matrix of the original and repaired data, asshown inFigs. 15(A) and (B), respectively.3.1.2. Sorting results and disease recognitionIn the experiment, the network model was trained according tothe pig classiﬁcation requirements. The accuracy of ResNet-50 wasveriﬁ
ed by freezing 3, 7, and 13 layers; the experimental results areshown inFig. 16(A). The results demonstrate that the highest train-ing results are obtained at the 91st cycle for 7 frozen layers. The av-erage accuracy of the experimental models exceed 95%, andoptimized ResNet-50 was used to achieve accurate predictions fordifferent classiﬁcations to meet the practical needs of sorting. Toshow the classiﬁcation effect more intuitively, tSNE values for D1to D5 are plotted based on the time series in Figs. 16(C) and (D) and tSNE values for C1 to C5 are plotted based on the level seriesinFigs. 16(E) and (F).The model was retrained on a disease recognition dataset, the un-derlying network was frozen, and the training was completed andtested on a test set. The accuracy of the disease test set is 97.81%,which satisﬁes the experimental requirements, as shown in Fig. 16(B). The confusion matrices for the training and test set predictions arealso plotted inFigs. 17(A) and (B), respectively.
Fig. 12.Designing visual interfaces.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
1223.2. Experimental resultsExperimental results are obtained based on ﬁne-tuning transfer learning using a residual CNN model for classi ﬁcation and sorting. The accuracy of the pig ID recognition reaches 89%. The proposedsystem collects pictures of different types of pig diseases and learnsbinary classiﬁcation from healthy pigs. Considering a speci ﬁc situation in which the image is incomplete owing to stray lightand railings in the application scenario, the WGAN algorithm isused to improve the image quality and sorting accuracy. TheResNet-50 network is used for individual identi ﬁcation. Transfer learning can quickly learn and deploy models with a limited samplesize, effectively improving system effectiveness. To ﬁtt h eg r o w t h curves of pigs, the absolute weight and growth between two detec-tions are considered, the pigs are categorized (six categories) basedon the growth and development stages, and the pig growth trendsare evaluated for farm management reference. The experimentalresults show that the classiﬁcation success rate for nursery pigs,piglets, and fattening pigs reaches 95%, whereas the classi ﬁcation success rate for sick and healthy pigs reaches 97%. Using imagecompletion algorithms to supplement occluded images cansigniﬁcantly improve accuracy by approximately 32% ( Table 3). To better evaluate the effects of model identi ﬁcation, the accuracy, precision, sensitivity, F1 value, and consistency test parameters werecalculated for the four sorting types, as listed in Table 2. The parameters of the model identiﬁcation results are consistent with the experimentalresults.Following equations describe the calculation of the test parameters:Precision¼100/C2
Tp
TpþF p/C18/C19 ð27ÞSensitivity¼100/C2
Tp
TpþF N/C18/C19 ð28Þ
F1/C0Score¼100/C22/C2Precision/C2SensitivityPrecisionþSensitivity/C18/C19/C18/C19 ð29ÞAccuracy¼100/C2
TpþT N
TpþT NþF pþF N/C18/C19 ð30ÞKappa is a statistical coefﬁcient used in evaluation measures to vali-date results. The kappa values illustrate the consistency of the predictedvalues with the actual values. The kappa values remain between zeroand one, indicating the conﬁdence level of the given rule. Lower kappavalues indicate no agreement; however, the low agreement obtainedin the experiment in this study is informative and within a reasonableinterval. Kappa is calculated as follows:kappa¼
po/C0pe
1/C0pe, ð31Þwhere p
0is the sum of the number of correctly classi ﬁed samples in each category divided by the total number of samples, that is, the overallclassiﬁcation accuracy, C is the total number of categories, and T
iis the
Fig. 13.(A) Loss value curve for identity model training. (B) Accuracy of model training.
Fig. 14.(A) Comparison of image complementation results. (B) Three types of image recognition results.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
123number of correctly classiﬁed samples in each category. Supposing thatthe number of true samples in each category is a1, a2, ..., aC, the num- ber of predicted samples in each category is b1, b2, ..., bC, and the total number of samples is n, then:p
o¼∑Ci¼1Ti
n ð32Þp
e¼∑Ci¼1ai/C2bi
n2ð33ÞCompared with other models, the livestock identi ﬁcation model based on the residual and generative adversarial networks designed inthis study has better features and more advantages. In the ﬁeld of pig recognition, the recognition rate obtained in this study for covered im-ages is signiﬁcantly higher than that obtained in previous studies andclose to the recognition accuracy of the original images. Additionally agood classiﬁcation effect in terms of classiﬁcation, sorting, and charac- terization of disease detection is obtained in this study.Based on the above results, it can be concluded that the livestockidentiﬁcation and information management system proposed in thisstudy provides satisfactory results in terms pig identi ﬁcation and
Fig. 15.(A) Confusion matrix for identiﬁcation of original data. (B) Confusion matrix for identi ﬁcation of repaired data.
Fig. 16.(A) Classiﬁcation results under different freezing layers. (B) Training curve for disease recognition. (C) Confused tSNE chart (time series) for ﬁve types of pigs. (D) tSNE chart (time series) forﬁve types of pigs after classiﬁcation. (E) Confused tSNE chart (level series) for ﬁve types of pigs. (F) tSNE chart (level series) for ﬁve types of pigs after classiﬁcation.Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
124information processing compared with those of similar systems, espe-cially with respect to occluded image recognition. The proposed deep-learning network model exhibits good results and may have signi ﬁcant implication in theﬁeld of in livestock sorting and information manage-ment.4. ConclusionIn this study, a low-cost livestock-sorting system for image recogni-tion was designed based on deep learning. Using only weights and im-ages as inputs, the proposed system accomplishes functions such aslivestock ID recognition, automated livestock sorting, disease detection,and data tracking and prediction, through the technical route of deeplearning and the industrial Internet of Things. The following conclusionsare drawn from this study.(1) A deep-learning-based machine vision sorting system is pro-posed and successfully tested for processing ﬁeld images with multiple feature recognition tasks, such as disease and illness,sex and reproduction, growth, and fattening. It uses deep learn-ing to complete the individual recognition of livestock, performscategory sorting, and detects diseases based on image data. It canbe fully applied to all aspects of sorting including improvingsorting efﬁciency, reducing the workload of manual sorting,and preventing bacterial infection.(2) Deep-learning technology is used to collect images and build dif-ferent datasets for different requirements. The ResNet-50 modelis used to ensure the accuracy and generalization of the resultsthrough pre-processing models and parameter ﬁne-tuning. Dif- ferent models are trained for different requirements to meetthe system requirements.(3) GANs are applied to the ResNet backbone model to improve therecognition accuracy against occluded images, allowing themodel to be applied to a wider range of scenarios.(4) A weight health evaluation system is designed that encom-passes modules such as data collection, individual recogni-tion, accuracy veriﬁcation, health and developmentevaluation, and weight prediction. The essential functions ofthe system are then completed.(5) A Kalmanﬁlter-basedﬁltering algorithm is developed to im-prove the accuracy of the dynamic process of the deep-
Fig. 17.(A) Confusion matrix forﬁve-class classiﬁcation recognition training set. (B) Confusion matrix for ﬁve-class classiﬁcation recognition test set.
Table 2Result evaluation parameter.Studies Year Species Objects Backbone AccuracyAndrew et al. 2021 cow Individual recognition CNN 93.80%Pan et al. 2022 cow Individual recognition CNN 93% (recent variants 85%)Xu et al. 2022 cow Individual recognition CattleFaceNet 91.35%Wang et al. 2022 pig Individual recognition DenseNet 121 94.04%Marsot et al. 2020 pig Individual recognition CNN 83.70%Wang et al. 2021 pig Individual recognition ResNet 67.58% (covered)Ours current study 2023 pigIndividual recognition ResNet+WGAN 82.9% (covered)Individual recognition ResNet+WGAN 88.90%Classiﬁcation recognition ResNet+WGAN 95%Disease recognition ResNet+WGAN 98%Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
125learning-based sorting system for multi-purpose applica-tions on pig farms.However, the generalizability of the model to different livestock isworth further investigation because of the signi ﬁcant cost of data collec- tion and tagging, as well as the need for a more robust and transferablemodel. More extensive data must be collected to train and validate thereliability of the model. The potential of heavy recognition and 3D rec-ognition technologies in theﬁeld of livestock sorting based on imagerecognition has been recognized, which may enable the modeling oflivestock body types and improve recognition accuracy. Because of thelack of collaboration in the veterinaryﬁeld at this stage, this study only focuses on the classiﬁcation and detection of diseases with obviouslivestock phenotypes; diseases related to livestock pathology and multi-information fusion detection should be the focus of future research. Fur-thermore, a collaboration with the Institute of Animal Husbandry andVeterinary Science on a cross-cutting project targeting livestock patho-genesis, processes, and image recognition is anticipated. Moreover, an-other collaboration with Ningxia Jinyuhaoxing Agriculture and AnimalHusbandry Co. Ltd. and Ningxia Yanchi Tan Sheep Industry Develop-ment Group Co. Ltd. is in the pipeline focusing on collecting data onother livestock, including dairy cows and tan sheep, to realize researchand innovation in sorting information management systems for otherlivestock breeds.FundingThis work wasﬁnancial supported by the Zhenjiang Science andTechnology Bureau, Jiangsu Science and Technology Department, ﬁ- nancial support was received from Zhenjiang HongxiangAutomationTechnology Co. Ltd.Institutional review board statementNot applicable.Informed consent statementNot applicable.CRediT authorship contribution statementYuanzhi Pan:Conceptualization, Data curation, Formal analysis,Funding acquisition, Methodology, Resources, Software, Validation, Vi-sualization, Writing–original draft, Writing–review & editing.Yuzhen Zhang:Data curation, Formal analysis, Methodology, Software, Valida-tion, Visualization, Writing–original draft.Xiaoping Wang:Project ad- ministration.Xiang Xiang Gao:Supervision.Zhongyu Hou:Writing– review & editing.Data availability statementDataset is available athttps://data.mendeley.com/datasets/vd5vmgr8kg(January 18, 2023)https://data.mendeley.com/datasets/jy6hngx7df(April 5, 2023).Declaration of Competing InterestThe authors declare no conﬂicts of interest.ReferencesArjovsky, M., Chintala, S., Bottou, L., 2017. Wasserstein generative adversarial networks [C]//international conference on machine learning. PMLR 214 –223. Brito, C., Machado, A., Sousa, A., 2019. Electrocardiogram beat-classiﬁcation based on a ResNet network. Stud. Health Technol. Informatics 30 (6), 264 –265. Calderón Díaz, J.A., García Manzanilla, E., Diana, A., et al., 2018. Cross-fostering implica- tions for pig mortality, welfare and performance[J]. Front. Vet. Sci. 5, 123.Dong, C., Loy, C.C., He, K., et al., 2015. Image super-resolution using deep convolutional networks. IEEE Trans. Pattern Anal. Mach. Intell. 38 (2), 295 –307. Fuentes, S., Viejo, C.G., Tongson, E., Dunshea, F.R., 2022. The livestock farming digital transformation: implementation of new and emerging technologies using arti ﬁcial intelligence. Anim. Health Res. Rev. 1 –13. Goodfellow, I., Pouget-Abadie, J., Mirza, M., Xu, B., Warde-Farley, D., Ozair, S., et al., 2020.Generative adversarial networks. Commun. ACM 63 (11), 139 –144. He, K., Zhang, X., Ren, S., et al., 2016a. Deep Residual Learning for Image Recognition. IEEE. He, K., Zhang, X., Ren, S., Sun, J., 2016b. Deep residual learning for image recognition. Pro- ceedings of the IEEE Conference on Computer Vision and Pattern Recognition (CVPR),Las Vegas, USA.He, K., Gkioxari, G., Dollar, P., et al., 2017. Mask R-CNN. Proceedings of the IEEE interna- tional conference on computer vision, pp. 2961 –2969. Kashiha, M., Bahr, C., Ott, S., et al., 2013. Automatic identiﬁcation of marked pigs in a pen using image pattern recognition[J]. Comput. Electron. Agric. 93, 111 –120. Kristensen, A.R., Nielsen, L., Nielsen, M.S., 2012. Optimal slaughter pig marketing with em- phasis on information from on-line live weight assessment. Livest. Sci. 145 (1 –3), 95–108.Krizhevsky, A., Sutskever, I., Hinton, G., 2012. ImageNet Classiﬁcation with Deep Convolutional Neural Networks. NIPS.Kwon, Y.H., Casebolt, J.B., 2006. Effects of light refraction on the accuracy of camera cali-bration and reconstruction in underwater motion analysis. Sports Biomechanics 5(2), 315–340.Latino, L.R., Pica-Ciamarra, U., Wisser, D., 2020. Africa: the livestock revolution urbanizes. Global Food Security 26.Lee, J.H., Kim, Y.J., Kim, Y.W., et al., 2019. Spotting malignancies from gastric endoscopic images using deep learning. Surg. Endosc. 89 (4), 806 –815. Marsot, M., Mei, J.Q., Shan, X.C., et al., 2020. An adaptive pig face recognition approach using convolutional neural networks. Comput. Electron. Agric. 173.Miller, E.G., Matsakis, N.E., Viola, P.A., 2000. Learning from one example through shared densities on transforms. Proceedings IEEE Conference on Computer Vision and Pat-tern Recognition (CVPR), pp. 464 –471 (Hilton Head). Moekti, G.R., 2020.Industrial livestock production: A review on advantages and disadvan-tages. IOP Conference Series: Earth and Environmental Science. vol. 492. IOP Publish-ing, p. 012094 No. 1.Neethirajan, S., 2020.The role of sensors, big data and machine learning in modern ani-mal farming. Sensing Bio Sens. Res. 29, 100367.Neethirajan, S., 2022.Affective state recognition in livestock —artiﬁcial intelligence ap- proaches. Animals 12 (6), 759.Ouyang, H., Ren, L., 2023.Special issue“state-of-the-art porcine virus research in China
”. Viruses 15 (2), 412.Pan, S.J., Tsang, I.W., Kwok, J.T., et al., 2011. Domain adaptation via transfer component analysis. IEEE Trans. Neural Netw. 22 (2), 199 –210. Pan, Y., Jin, H., Gao, J., Rauf, H.T., 2022. Identiﬁcation of Buffalo breeds using self-activated- based improved convolutional neural networks. Agriculture 12 (9), 1386.Pandey, S., Kalwa, U., Kong, T., Guo, B., Gauger, P.C., Peters, D.J., Yoon, K.J., 2021. Behavioral monitoring tool for pig farmers: ear tag sensors, machine intelligence, and technol-ogy adoption roadmap. Animals 11 (9).Radford, A., Metz, L., Chintala, S., 2015. Unsupervised representation learning with deep convolutional generative adversarial networks. arXiv preprint arXiv:1511.06434.Robbins, R.C., Almond, G., Byers, E., 2014. Swine diseases and disorders[J]. Encyclopedia Agricult. Food Syst. 261.Tzeng, E., Hoffman, J., Darrell, T., et al., 2015. Simultaneous deep transfer across domains and tasks. Proceedings 2015 IEEE International Conference on Computer Vision(ICCV). IEEE, Santiago, Chile, pp. 4068 –4076. Wang, H., Deng, Z., Feng, B., Ma, H., Xia, Y., 2017. An adaptive Kalmanﬁlter estimating process noise covariance. Neurocomputing 223, 12 –17. Wang, S., Jiang, H., Qiao, Y., Jiang, S., Lin, H., Sun, Q., 2022. The research Progress of vision- based artiﬁcial intelligence in smart pig farming. Sensors 22 (17).Wanga, H. Peter, Ghani, Nasir, Kalegele, Khamisi, 2015. “designing a machine learning – based framework for enhancing performance of livestock mobile application system. ” American. J. Softw. Eng. Appl. 4 (3), 56.Yin, H., et al., 2012.Analysis of factors on the error of the camera calibration. Informa.Commun. 1, 28–30.Yosinski, J., Clune, J., Bengio, Y., et al., 2014. How Transferable Are Features in Deep Neural Networks? In Proceedings International Conference on Neural Information Process-ing Systems. MIT Press, Kuching, Malaysia, pp. 3320 –3328.Table 3Comparison of livestock identiﬁcation under different models.Method Accuracy(%) Precision(%) Sensitivity(%) F1-Score(%) KappaID-Original 88.92 88.75 88.89 88.50 0.62ID-Repaired 83.34 83.26 83.49 83.27 0.60Classiﬁcation 95.20 94.92 95.12 94.88 0.65Health 97.81 97.89 97.66 97.56 0.67Y. Pan, Y. Zhang, X. Wang et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 110 –126
126