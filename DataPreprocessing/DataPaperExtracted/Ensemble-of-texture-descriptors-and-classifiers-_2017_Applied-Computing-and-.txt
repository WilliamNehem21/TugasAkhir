ORIGINAL ARTICLE
Ensemble of texture descriptors and classiﬁers forface recognition
Alessandra Luminia, Loris Nannib,*, Sheryl Brahnamc
aDISI, Universita`di Bologna, Via Venezia 52, 47521 Cesena, Italy
bDEI, University of Padua, viale Gradenigo 6, Padua, Italy
cComputer Information Systems, Missouri State University, 901 S. National, Springﬁeld, MO 65804, USAReceived 18 November 2015; revised 22 March 2016; accepted 12 April 2016Available online 21 April 2016
KEYWORDSFace recognition;Similarity metric learning;Ensemble of descriptors;Support vector machineAbstractPresented in this paper is a novel system for face recognition that works well in the wildand that is based on ensembles of descriptors that utilize different preprocessing techniques. Thepower of our proposed approach is demonstrated on two datasets: the FERET dataset and theLabeled Faces in the Wild (LFW) dataset. In the FERET datasets, where the aim is identiﬁcation,we use the angle distance. In the LFW dataset, where the aim is to verify a given match, we use theSupport Vector Machine and Similarity Metric Learning. Our proposed system performs well onboth datasets, obtaining, to the best of our knowledge, one of the highest performance rates pub-lished in the literature on the FERET datasets. Particularly noteworthy is the fact that these goodresults on both datasets are obtained without using additional training patterns. The MATLABsource of our best ensemble approach will be freely available at https://www.dei.unipd.it/node/ 2357.
/C2112016 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This isan open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionFace recognition has been an area of intense study since the1960s. Innovative applications making use of this technologyare continuously being developed at a rapid pace. Contempo-rary face recognition applications can be divided into threeareas that depend on the goal of the face recognition task:(1) face veriﬁcation, where the goal is to authenticate the iden-tity of a face image with a corresponding template; (2) faceidentiﬁcation, where the goal is to ﬁnd a match in a databaseof face images; and (3) face tagging (a relatively new variationof face identiﬁcation), where the goal is to label face imagesbased on identiﬁcation when matched. Face recognition isnow an essential component in biometric security, access man-agement, criminal identiﬁcation, and image sorting andretrieval.The main goal of face recognition is to compare twoimages of faces and solve the problem of determiningwhether both images are of the same person or of two differ-ent people. This problem is difﬁcult because two images of
*Corresponding author.E-mail addresses:alessandra.lumini@unibo.it(A. Lumini),loris. nanni@unipd.it(L. Nanni),sbrahnam@missouristate.edu(S. Brah- nam).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2017) 13,7 9–91
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2016.04.0012210-8327/C2112016 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).the same person can vary considerably in time, pose, facialexpression, illumination conditions, occlusions, and imagequality. Most state-of-the-art face recognition techniques per-form well when facial images are captured in optimal (labo-ratory) conditions where lighting is controlled and samplesprovide full frontal views, but when facial images are cap-tured in the wild – where pose, age, and facial expressionschange and where environmental conditions such as lightingare less than ideal – performance deteriorates. The difﬁcultylies in teasing out the speciﬁc features indicative of identityfrom the mass of features expressing other conditions. Eventhe best classiﬁer will fail if an insufﬁcient number of featuresindicative of identity are isolated. One way to tackle thisproblem is to usemultibiometrics, which recognizes individu-als via biometric fusion[1], whether multimodal, multi-instance, multisensorial[2], or multialgorithmic. Of particularimportance to both single trait biometrics and multibiomet-rics is the identiﬁcation of face descriptors that are discrimi-native yet insensitive to information having nothing to dowith identity, such as pose variations, changes in facialexpression, and lighting conditions.Some of the most notable face recognition techniques devel-oped the last ﬁve decades[3]include Principal ComponentAnalysis, Elastic Template Matching, Discriminant Analysis,Local Binary Patterns (LBPs), Algebraic moments, Gabor Fil-tering[4], and Neural Networks[5]. One way to categorize facerecognition techniques is to look at how a face is represented[3].Appearance based approachesutilize global texture featuressuch as Eigenfaces[6]or some other linear transformation. Inaddition to the information found in the texture of a faceimage,Model based approachestake into account the shapeof the face, whether 2D[7]or 3D.Geometry or template basedapproachescompare an input image with a set of templatesconstructed using either statistical tools or by analyzing localfacial features and their geometric relationships [8].Neural Networksinclude approaches based on ‘‘deep learning ”where the representation of faces is learned during the training pro-cess[5]. This last class includes approaches that are oftenreferred to as ‘‘deep methods”in opposition to ‘‘shallow meth- ods,”and differs from a second class of approaches where therepresentation of the face image is derived from ‘‘handcrafted ” image descriptors.Recent developments in the ﬁrst class of shallow methodsinclude the work of Pinto et al.[9]who describe a set of V1-like features that are composed of a population of Gabor ﬁl-ters. V1-like features are insensitive to view, lighting, and manyother image variations. The feature sets proposed by Cao et al.[10]that encode the local micro-structures of a face into a setof more uniformly distributed discrete codes are excellentexamples of a good tradeoff between discriminative powerand invariance, as are Patterns of Oriented Edge Magnitudes(POEM), a feature set proposed in[11,12]. POEM is an ori-ented spatial multiresolution descriptor that captures informa-tion about the self-similarity structure of an image. Somefeature sets that work well in the wild include those describedin[13]and more recently in[14], where monogenic binary cod-ing (MBC) is presented. MBC decomposes an original signalinto three components (amplitude, orientation, and phase) thatencode local variation. A histogram is then extracted from thelocal features. This efﬁcient descriptor signiﬁcantly lowers thetime and space complexity compared with other Gabor-transformation-based local feature methods.Another approach for overcoming variations in pose andillumination is to combine texture-based descriptors with othertechniques. For example, in[15]an accurate 3D shape modelworks by mapping images that vary in pose to a full frontalview. Discriminative models capable of handling aging, facialexpressions, low light, and over-exposure are then obtainedby comparing billions of faces. One approach described in
[16]trains binary classiﬁers on sixty-ﬁve describable visualtraits that were manually labeled on the training set. Anotherapproach based on ‘‘simile classiﬁers”removes the need for amanually labeled training set by training the binary classiﬁersto recognize the similarity of faces (using the whole image andpatches) to speciﬁc reference people. Both approaches exploitthe power of simple low-level features (such as image intensi-ties in RGB and HSV color spaces, edge magnitudes, and gra-dient directions). A drawback of these approaches, however, isthat they required using afﬁne warping to obtain pose invari-ance. In[17]an identity-preserving alignment is proposed. Inthis approach, face warping reduces differences in poses andexpressions while preserving differences indicative of identity.Binary classiﬁers are trained both to perform an ‘‘identity-preserving”alignment and to recognize people.The Multi-scale Local Phase Quantization (MLPQ) methodproposed in[18]is a blur-robust image descriptor. MLPQ iscomputed regionally and adopts a component-based frameworkto maximize the insensitivity to misalignment, a phenomenonfrequently encountered in blurring. Regional features are com-bined using kernel fusion. The MLPQ representation is com-bined with the Multiscale Local Binary Pattern (MLBP)descriptor according to a supervised fusion that is based on Ker-nel Discriminant Analysis (KDA). This step is necessary toincrease insensitivity to illumination. It should be pointed outhere, however, that the MLPQ representation in [18]was obtained using a supervised transform and a different testingprotocol. Thus, the results reported in[18]on the LFW datasetare not comparable with the approach proposed in this paper.A real breakthrough in the ﬁeld of face recognition was theintroduction of ‘‘deep methods,”which are based on the appli-cation of deep learning to this pattern recognition problem.The ﬁrst interesting paper in this area was[5], where a convo- lutional neural network (CNN) was employed to learn a metricbetween face images. This was a precursor to the recent highlysuccessful application of CNNs to face veriﬁcation. So power-ful is the deep learning approach that after a decade of studyresearchers[19]have recently announced that we are now ableto close the ‘‘gap to human-level performance in face veriﬁca-tion.”With an approach based on a 3D model for face align-ment and an ensemble of CNNs to ﬁnd a numerical descriptionof the forward-looking face, DeepFace has achieved 97.25%accuracy on the LFW dataset, which is very close to thehuman level accuracy of 97.53% in face veriﬁcation. Anotherwork[20]that is based on Gaussian Processes and multi-source training sets has achieved 98.52% accuracy on theLFW dataset, which is better than human performance.Many deep learning approaches[21–23]have also signiﬁ- cantly outperformed previous systems based on low levelfeatures in face recognition. There are two innovations of notein these deep learning approaches based on low-level features.The ﬁrst is in face identiﬁcation, thanks to the last hiddenlayer, which contains features highly discriminative inperforming large-scale face identiﬁcation. The second is inboth face identiﬁcation and veriﬁcation, thanks to supervising80 A. Lumini et al.deep neural networks that minimize the distance between fea-tures of the same identity while simultaneously decreasingintra-personal variations.Although one of the best benchmarks in face recognition isthe LWF dataset, there are some limitations of this datasetthat need some remarks: in particular the limitations discussedin[24], which investigated the availability of a big training setand its impact on recognition performance. During the historyof the LFW benchmark, the largest improvements have beenobtained the last few years by applying deep learning tech-niques to huge datasets containing outside labeled data. Theamount of training data expanded a hundred times from2010 to 2014, i.e., from about ten thousand training samplesin[25]to four million images in[19]. The best performanceusing a training set of less than 10,000 images with deep learn-ing was lower than 85%. According to the study in [24], the performance on large databases of faces seems to rise linearlyas data size increases, but a long-tail effect emerges when thenumber of individuals becomes greater than 10,000. Increasingindividuals (with a few instances per person) does not help toimprove performance. Moreover, it is worth noting that theMegvii Face Recognition System[24], which achieves99.50% accuracy on the LFW, did not reach acceptable per-formance in real-world security certiﬁcation scenarios thatcontend with a high range of age variation, proving that thereis still a real gap between machine recognition and human per-formance. The main drawback of these methods is that theyrequire millions of images for training. As a consequence theirresults on benchmarks are not directly comparable withapproaches obtained using a testing protocol based on a fewtraining samples.The approach presented in this paper can be referred to asshallow, since unlike deep methods, our proposed approach isbased on a representation of the face image using handcraftedlocal image descriptors. The system presented here is based onpreliminary results reported in[26]that demonstrate how theperformance of the POEM descriptor[12](one of the most efﬁ-cient and one of the highest performing descriptors recentlyproposed in the literature) can be enhanced with an ensembleof classiﬁers that combine different preprocessing techniquesthat vary a set of feature extraction parameters. However, herewe also test our proposed system using a set of ‘‘learned ”fea- tures, which have been obtained from the internal representa-tion of a deep method, especiﬁcally a Convolutional NeuralNetwork (CNN) trained for the face recognition problem.We want to underscore that the use of ‘‘learned features ”does not put the proposed approach in the category of a ‘‘deepmethod”approach since the training of the classiﬁer is per-formed in a traditional, shallow way.The key additions to[26], on which the approach proposedin this paper is based, are the following:/C15The combination of several feature extractors using differ-ent enhancement techniques./C15The improved performance obtained using two differentclassiﬁers for face veriﬁcation: Support Vector Machines(SVMs) and Similarity Metric Learning
1(SML)[25]./C15The utilization of the method proposed in[27]2for synthe- sizing a single frontal face view starting from an uncon-strained photo (useful because LFW images areunconstrained)./C15The combination of ‘‘learned”and ‘‘handcrafted”features. The resulting fusion based solely on the handcrafted fea-tures obtains, to the best of our knowledge, one of the highestmean accuracy ratings on the FERET datasets published in theliterature. Moreover, the fusion produces very good results onthe LFW dataset. The ensemble based on the fusion of learnedand handcrafted features improves performance on both theFERET and LFW datasets even further.It is important to point out that we have used the sameparameters for both the FERET and the LFW datasets toavoid any overﬁtting (in the literature a number of papersreport varying the parameters that are used in the two datasets,thereby increasing the likelihood of overﬁtting the givenmethod to that dataset).2. The proposed approachThe main idea of the proposed approach is to design an ensem-ble of classiﬁers trained on different descriptors extracted fromthe face image. Moreover, in order to perturb the informationgiven to the base classiﬁers and to make the ensemble stronger,we designed several perturbations at different steps in the clas-siﬁcation process: in the image preprocessing, feature transfor-mation, and matching steps. The general schema of thecomplete approach is illustrated inFig. 1. Detailed descrip-tions of the methods used in each step are provided below inthis section.As illustrated inFig. 1, the proposed approach can be bro-ken down into the following steps:/C15Face detection:ﬁrst the precise position of the face image isdetected as in[27], and the resulting face is cropped andaligned according to eye position./C15Frontalization:the approach proposed in[27]is used to syn- thesize frontal views of faces from the detected face (thisstep is useful in making the feature representation indepen-dent of pose changes)./C15Pose creation:to tackle pose variation, we make use of threeadditional poses obtained by vertically ﬂipping the image.In other words, we train four classiﬁers: the ﬁrst using theoriginal images for the two faces to be matched; the secondusing the vertical ﬂip of the second face; the third using thevertical ﬂip for the ﬁrst image; and the fourth using the ver-tical ﬂip for both images. The four systems are then simplycombined by sum rule./C15Preprocessing:several enhancing methods have been testedin this work in order to make the feature extraction morerobust to changes in illumination, noise, etc. The paralleluse of different approaches is performed as a perturbationstrategy in order to obtain diversity among the classiﬁers.The input of this step is the frontalized image, and the out-put is a set of images preprocessed according the followingapproaches: Adaptive single scale retinex[28], Anisotropic smoothing[29], Difference of Gaussians[30], an approach
1Code:http://secamlocal.ex.ac.uk/people/staff/yy267/code_sub- sml_iccv.zip.
2Code:http://www.openu.ac.il/home/hassner/projects/frontalize/ .Texture descriptors and classiﬁers 81based on the low-frequency discrete cosine transform [31], Oriented Local Histogram Equalization[32], Multi-Scale Retinex[33], Isotropic Smoothing Normalization[34,35], and Gradientfaces[36]./C15Feature extraction:this step is performed separately on eachimage resulting from the previous preprocessing method inorder to obtain different descriptors from each image. Thedescriptors extracted include the following: Local BinaryPatterns (LBPs)[37], Histogram of Gradients[38], POEM [11], Heterogeneous Auto-Similarities of Characteristics(HASC)[39], Gaussian of Local Descriptors (GOLD)[40], and Monogenic Binary Coding[14]./C15Feature transformation:before classiﬁcation the dimension-ality of each descriptor is reduced via Principal ComponentAnalysis (PCA)[41]./C15Classiﬁcation:a set of general-purpose classiﬁers is trainedon each reduced descriptor. The ﬁnal decision is then deter-mined according to the sum rule by summing up the scores/similarity values (SIM
i) obtained from each classiﬁer. Inthis work, the simple angle distance is used in the FERETdatasets, where the aim is identiﬁcation. Linear SVMs [42] and SML[25]are used on the LFW dataset, where theaim is to verify a given match.2.1. Hard Frontalization (HF)Unlike other frontalization methods that transform a 3D facialmodel to ﬁt a particular facial appearance, HF, proposed in[27], uses single 3D reference geometry to synthesize frontalviews of faces from different facial poses captured in the wild.This idea is based on the observation that for frontalization arough approximation to a single 3D facial shape is, for allpractical purposes, as good as any other, including a moreindividualized construction of a 3D structure.The HF process begins by utilizing standard facial featuredetectors to detect and crop a face when found in an image.This cropped image is rescaled to a standard coordinate sys-tem, where a set of 49 facial features are used to render a gen-eric 3D model. A 3/C24 projection matrix is then estimatedfrom the 2D query coordinates and the corresponding 3Dmodel coordinates. This matrix is used to back-project thequery intensities (facial colors) to the reference coordinatesystem, which are then overlaid on the frontalized model.Intensities are borrowed from corresponding symmetric partsof the face to ﬁll in missing areas in the generic 3D modelfor a ﬁnal result.HF requires four steps: (i) the pose estimation process,which is based on a synthetic rendered view of a texture 3Dmodel by means on a rotation matrix and a translation vector;(ii) frontal pose synthesis using bi-linear interpolation to sam-ple the intensities of the initial frontalized view produced byback projecting the query features onto the reference coordi-nate system of the 3D model; (iii) visibility estimation, whichis performed using a variation of the multiview 3D reconstruc-tion method where an approximation to the 3D reference faceand a single view (rather than multiple views) is employed toestimate visibility; and (iv) detection of problems introducedby conditional soft-symmetry using a standard representation(LBP) and a classiﬁer (SVM) to take advantage of the fact thatregardless of the actual shape of the face the same image regionin the frontal face always corresponds to the same areas.2.2. Preprocessing techniquesBefore the feature extraction step, it is possible to address theproblem of illumination variation by using some recentlydeveloped image enhancement techniques:/C15Adaptive single scale retinex (AR)[28]: a variant of the reti-nex technique, this approach was originally developed toimprove scene detail and color reproduction in the darkerareas of an image. This technique normalizes illuminationusing the spatial information between surrounding pixels(it should be noted that AR produced the best results inour experiments)./C15Anisotropic smoothing (AS)[29]: a simple automatic image-processing normalization algorithm, AS begins by estimat-ing the illumination ﬁeld and then compensates for it byenhancing the local contrast of the image in a fashion sim-ilar to human visual perception. This technique has provenhighly effective with standard face recognition algorithmsacross many face databases[29]./C15Difference of Gaussians (DG):this is a normalization tech-nique that relies on the difference of Gaussians to produce anormalized image. A band-pass ﬁlter is applied to an inputimage before the feature extraction step. In our experi-ments, the log transform is used before ﬁltering as in [30].
Figure 1Schema of the proposed face recognition ensemble.82 A. Lumini et al./C15Low-frequency discrete cosine transform (DCT) basedapproach[31]: this is an illumination normalization approachfor face recognition, where a discrete cosine transform (DCT)is employed to compensate for illumination variations in thelogarithm domain. The rationale is that since illuminationvariations mainly lie in the low-frequency band, an appropri-ate number of DCT coefﬁcients are truncated to minimizevariations under different lighting conditions./C15Oriented Local Histogram Equalization (OLHE) [32]: this is a histogram equalization that compensates illuminationwhile encoding rich information on the edge orientations./C15Multi Scale Retinex (MSR)[33]: this is a multiscale retinex(a model of the lightness and color perception of humanvision) that achieves simultaneous dynamic range compres-sion, color consistency, and lightness rendition with the aimof improving ﬁdelity of color images to human observation./C15Isotropic Smoothing Normalization (ISN)[34]: this method deals with the problem of face veriﬁcation across illumina-tion by means of isotropic smoothing normalization (a dif-fusion step which basically updates each pixel using anaverage of its neighboring pixels, regardless of the imagecontent surrounding the region under consideration)./C15Photometric Normalization (PN)[35]: this is a robust illu-mination normalization which operates as a simple and efﬁ-cient preprocessing chain based on gamma correction,difference of Gaussian ﬁltering, masking and contrastequalization, and photometric normalization, which elimi-nates most of the effects of changing illumination while stillpreserving the essential appearance details that are neededfor recognition./C15Gradientfaces (GFs)[36]: this is not properly an enhance-ment method but rather an illumination insensitive measurederived from the image gradient that is robust to illuminationchanges,includingthoseinuncontrollednaturallightingenvi-ronments. In this work we use Gradientfaces as a preprocess-ing approach to represent an image in the gradient domain.2.3. Feature extractionThe feature extraction step is performed on each preprocessedimage as detailed in the previous step in order to extract thefollowing descriptors: LBP, HoG, POEM, MBC, HASC,GOLD, RICLBP, and CLBP. Each of these feature extractionmethods is discussed below.2.3.1. Local Binary Patterns (LBPs)LBP[37]is a gray scale local texture operator with powerfuldiscrimination and low computational complexity. AmongLBPs many desirable properties are its invariant to monotonicgrayscale transformation; hence, it has low sensitivity tochanges in illumination.The LBP operator represents the difference between a pixelxand its symmetric neighbor set ofPpixels placed on a circleradius ofR(when a neighbor does not coincide with a pixel, itsvalue is obtained by interpolation). In this work we use P=8 andR= 1. We also use LBP with uniform bins. The LBPdescriptor is extracted from a set of subregions that areobtained by dividing each image cell into 9 /C210 equal nonoverlapping regions. The set of descriptors are concate-nated for describing the entire image.2.3.2. Histogram of Gradients (HoG)HoG[38]represents an image by a set of local histogramswhich count occurrences of gradient orientation in a local cellof the image. The HoG descriptor can be extracted in foursteps: (i) the computation of gradients of the image, (ii) thedivision of the image into small subregions, (iii) the buildinga histogram of gradient directions for each subregion, and(iv) the normalization of histograms to achieve better invari-ance to changes in illumination or shadowing. The subregionsare obtained by dividing each image cell into 8 /C28 equal nonoverlapping regions. The set of descriptors are concate-nated for describing the whole image.2.3.3. Patterns of gradient Orientations and Magnitudes(POEM)The POEM descriptor[11]relies on characterizing edge direc-tions of the local face appearance and its shape using the dis-tribution of local intensity gradients. It accomplishes this bymeasuring the edge/local shape information and the relationbetween the information in neighboring cells.Extracting POEM descriptors is a three step process:Step 1:Preform gradient computation and orientation quan-tization.This is accomplished by computing the gradientimage and then by discretizing the orientation of each pixel
over 0–p(for an unsigned representation) or 0–2p(for a signed representation). A soft assignment can be employedto avoid problems due to image degradation, where theoriginal magnitude of a pixel can be decomposed into twoparts and then assigned into its two nearest-neighbors ori-entation. In our experiments, we utilize the unsigned 0– p representation and soft assignment.Step 2.Calculate the magnitude accumulation.A local his- togram of orientations is calculated considering all pixelswithin a local image patch (cell). As a result, each pixel car-ries information about the distribution of the edge directionof a local cell.Step 3.Calculate self-similarity.In this step the accumulatedmagnitudes are encoded across different directions using theLBP-based operator within a larger patch (block). Based onprevious experimental results[26], the Dense LBP (DLBP) isused in our experiments instead of standard LBP.The result of the POEM extraction process is a set of ‘‘uni-directional”POEM maps. To incorporate spatial information,the POEM maps are divided into 8/C28 nonoverlappingregions. Histograms are then extracted from each region.The ﬁnal POEM-HS descriptor is the concatenation of all uni-directional descriptors at different orientations.The POEM descriptor depends on a large number ofparameters that need to be tuned speciﬁcally for each applica-tion. In our experiments the number of orientations dis-cretized, and the size of the cell, the size of the block, andthe number of neighbors considered in LBP have been setaccording to[43](i.e. to 3, 7, 5, and 8, respectively).2.3.4. Monogenic Binary Coding (MBC)MBC[14]is an efﬁcient texture descriptor. The monogenic sig-nal is a rotation-invariant representation that extracts thephase, amplitude, and orientation of a signal. Because itextracts multiple-orientation features without using steerableTexture descriptors and classiﬁers 83ﬁlters, it has a much lower time and space complexity than theGabor transformation (e.g. with time, there are three convolu-tions on each scale, and with space, there are three featuremaps on each scale). Monogenic signal representation is thecombination of an image and its Riesz transform. This repre-sentation decomposes an original signal into three compo-nents: amplitude, orientation, and phase. MultiresolutionMonogenic Signal Representation is obtained by performingband-pass ﬁltering on an image before applying the Riesztransforms by means of log-Gabor ﬁlters. In [14]three differ- ent resolutions are suggested that correspond to different scal-ing factors of the bandwidth. Monogenic Binary Codingencodes monogenic signal features in two complementarysteps: (i) the encoding of the variation between the central pixeland its surrounding pixels in a local patch (monogenic localvariation) and (ii) the encoding of the value of central pixelitself (monogenic local intensity coding). A monogenic binarycode (MBC) map is then calculated as the concatenation ofhistograms from each of the amplitude, phase, and orientationcomponents of the monogenic signal representation [14]. Lin- ear Discriminant Analysis (LDA)[44]is used as a ﬁnal stepto simultaneously reduce the histogram feature dimensionand enhance its discriminative power. This is accomplishedin three steps: (i) the MBC feature map is partitioned intoblocks, (ii) each block is further partitioned into subregions,and (iii) LDA is used in each block both to learn a projectionmatrix from the training set of feature maps of its subregionsand to reduce dimensionality of the histogram feature.Each step in MBC (the multiscale log-Gabor ﬁltering, sub-region histogram computing, and feature combination byLDA) involves several parameters. In our experiments, allparameters have been set according to those in the originalpaper. However, in this work an unsupervised feature trans-form (PCA), as described below, is used instead of LDA.The ﬁnal descriptor is composed of three feature vectors, onefor each component (amplitude, orientation, and phase) ofthe original signal, labeled in the experimental section asMBC
a, MBC o,MBC p, respectively. The three descriptors arenot fused at the feature level but rather at the score levelaccording to the weighed sum rule: MBC =(MBC
a+ MBC o+ MBC p)/3.2.3.5. Heterogeneous Auto-Similarities of Characteristics(HASC)HASC[39]is applied to heterogeneous dense feature maps andsimultaneously encodes linear relations by covariances (COV)and nonlinear associations through information-theoreticmeasures, speciﬁcally entropy combined with mutual informa-tion (EMI). The basic supposition behind HASC is that linearrelations alone are unable to capture the structural complexityof many objects. Using covariance matrices as region descrip-tors is advantageous because it is low-dimensional and robustto noise and pose changes; however, a single pixel outlier candramatically alter results, making the descriptor highly sensi-tive to impulsive noise. Moreover, the covariance among twofeatures is optimally able to encapsulate the features of thejoint PDF only if they are linked by a linear relation. EMIovercomes these limitations. The entropy ( E) of a random vari-able measures the uncertainty associated with the value of thevariable, and the mutual information (MI) of two randomvariables captures the generic dependencies (both linear andnonlinear). HASC takes advantage of these two propertiesby dividing an image into patches and creating an EMI matrix.Each diagonal entry of the EMI matrix captures the amount ofuncertainty or unpredictability related to a given featurewhereas off-diagonal entries capture the mutual dependencybetween two different features.HASC boosts discriminative performance because the com-bination of COV with EMI captures different features of thejoint underlying PDFs. Multiple experiments in [39]demon- strate that HASC is superior in performance to its individualcomponents COV and EMI. This makes HASC a versatiledescriptor for a large range of applications. HASC is extractedseparately from subregions of the whole image. The subregionsare obtained by dividing each image cell into 8 /C28 equal nonoverlapping regions. The set of descriptors are concate-nated for describing the entire image.2.3.6. Gaussian of Local Descriptors (GOLDs)GOLD[40]is a recent improvement of the well-known Bag ofWord (BoW) approach[45]for extracting features from animage. The canonical BoW descriptor generates a codebook(via clustering methods on the training set) from a set ofextracted local features that are then encoded into codes toform a global image representation. Instead of using a cluster-ing method, GOLD substitutes a ﬂexible local feature repre-sentation obtained by parametric probability densityestimation that does not require quantization. Quantizationhas the drawback of tightly tying dataset characteristics tothe feature representation since quantization is learned fromthe training set, and the cluster centers reﬂect the training datadistribution.GOLD is a four-step process: (i) feature extraction, wheredense SIFT descriptors are extracted on a regular grid of theinput image; (ii) spatial pyramid decomposition, where theimage is decomposed into subregions by a multilevel recursiveimage decomposition and where features are softly assigned toregions according to a local weighting approach; (iii) paramet-ric probability density estimation, where each region is repre-sented as a multivariate Gaussian distribution of theextracted local descriptors by inferring local mean and covari-ance, and (iv) projection on the tangent Euclidean space,where the ﬁnal region descriptor, the covariance matrix, is pro-jected on the tangent space and concatenated to the mean.2.3.7. Rotation Invariant Co-occurrence among adjacent LBP(RICLBP)The original LBP does not preserve structural informationamong binary patterns; therefore, a set of co-occurrencesamong adjacent LBPs (i.e. a co-occurrence matrix amongLBP pairs, or CoALBP) is extracted and converted to aCoALBP histogram feature. The rotation invariance ofCoALBP is obtained by attaching a rotation invariant labelto each LBP pair[46]. In this work the RICLBP descriptorhas been tested using the following LBP parameters: ( R=1 , P= 8), (R=2 ,P= 8) and (R=4 ,P= 8).2.3.8. Complete LBP (CLBP)CLBP[47]is an LBP variant which utilizes both the sign andmagnitude information in the difference between the centralpixel and some pixels in its neighborhood (the conventionalLBP operator only uses the sign component). CLBP also con-84 A. Lumini et al.siders the intensity of the central pixel; therefore, the ﬁnal codeis obtained from the combination of three codes: CLBP_S,which considers the sign component of the difference (i.e. thestandard LBP), CLBP_M, which considers the magnitudecomponent of the difference, and CLBP_C, which considersthe intensity of the central pixel. In this work the CLBPdescriptor has been tested using the following two LBP conﬁg-urations: (1,8) and (2,16).2.4. Feature transformWe tested several approaches for dimensionality reduction inour experiments to ﬁnd the best way of reducing the dimen-sionality of each descriptor before the classiﬁcation step.According to[48]nearly all spectral methods provide approx-imately the same accuracy when used with the same energy cut.In our experiments, however, the best performance wasobtained using PCA[41], one of the most popular methodsfor unsupervised dimensionality reduction. PCA maps featurevectors into a smaller number of uncorrelated directions calcu-lated to preserve the global Euclidean structure, and it alsoextracts an orthogonal projection matrix so that the varianceof the projected vectors is maximized.In this work we selected an orthogonal basis designed tomaintain enough components to explain 99% of the input vari-ance for LBP, HASC, GOLD, HOG and Deep Features (seeSection3.3) or the components where the eigenvalues are lar-ger than 10e/C04 for POEM and MBC[3]. When SML is used asthe classiﬁer, we retained the ﬁrst 300 components, as sug-gested in[26].2.5. ClassiﬁcationIn the classiﬁcation step, each preprocessed image togetherwith its extracted descriptor induces a different individual clas-siﬁer or distance measure. Therefore, for each descriptor wehave a different score or similarity measure SIM
ifor the refer- ence image. The ﬁnal decision of the ensemble is obtained bycombining all the scores by sum rule. This is a straightforwardmethod that was selected because the number of classiﬁers isquite high when including all the preprocessed images and allthe descriptors and artiﬁcial poses under consideration in thiswork. Moreover, the simple sum rule does not require a deepanalysis of the uncertainty space of ensemble classiﬁers, aswas performed in[49–51].In the FERET datasets, where the aim is identiﬁcation, weuse theangle distanceas the similarity function to compare twofaces. The angle distance between two vectors is the size of theangle between the two directions originating from the observerand pointing toward these two vectors. It can be calculated asthe angle whose cosine is the ratio between the dot product ofthe two vectors and the product of their magnitudes. In theLFW dataset, where the aim is to verify a given match, a gen-eral purpose binary classiﬁer can be used to distinguishbetween genuine and impostor matchings. In this work we testLinear SVM[40]and SML[25].Support Vector Machines (SVMs)[52]are a general pur-pose two-class classiﬁer that ﬁnds the equation of a hyperplanethat maximally separates all the points between the twoclasses. SVM handles nonlinearly separable problems usingkernel functions to project the data points onto a higher-dimensional feature space. We used different kernels in ourexperiments, but the best results were obtained with a linearkernel. The SVM classiﬁer is trained to distinguish betweengenuine and impostor matches. Therefore, a training patternis the combinationxof two descriptorsxiandx jand a label l. The two descriptors are combined in order to obtain the fol-lowing resulting vector:x=(x
i/C0x j)2./(xi+x j), where the element-wise power and the element-wise division (./) areperformed.Similarity Metric Learning (SML)[25]is a novel regulariza-tion framework to learn similarity metrics for unconstrainedface veriﬁcation where similarity metric learning over theintra-personal subspace is performed. The similarity functionbetween the imagesx
i,xjis deﬁned as follows:f
M;Gðxi;xjÞ¼s Gðxi;xjÞ/C0d Mðxi;xjÞwheres
Gðxi;xjÞandd Mðxi;xjÞare a weighed similarity and aweighed distance, respectively. The two weigh matrices G andMare learned from the training set such that f
M;Gðxi;xjÞ report a score proximal to 1 ifx
i,xjbelongs to the same classand a small score otherwise. The learning objective incorpo-rates the robustness to large intra-personal variations andthe discrimination power of novel similarity metrics.3. Experimental section3.1. DatasetsOur proposed system is evaluated on the FERET [53]and LFW[54]benchmark databases. The FERET database con-tains ﬁve datasets: Fa (1196 images), Fb (1195 images), Fc(194 images), Dup1 (722 images), and Dup2 (234 images).The gallery set is Fa, and the other datasets are used for test-ing. Fb contains pictures taken on the same day as the Faimages, using the same camera and under the same lightingconditions. Fc is a dataset of pictures taken on the same dayas Fa but with different cameras and under different illumina-tion conditions. The Dup1 and Dup2 datasets contain picturesthat were taken within the same year as Fa for Dup1 and laterthan one year for Dup2. The standard FERET evaluation pro-tocol involves comparing images in the testing sets to eachimage in the gallery set. In our experiments, all FERET grayscale images are aligned using the true eye positions andcropped to 110/C2110 pixels.The LFW[54]database contains 13,233 images of 5749celebrities that were collected from the internet (Yahoo news).A total of 1680 faces appear in more than two images. LFW iscommonly considered a very challenging dataset for face veri-ﬁcation since the faces were acquired in uncontrolled environ-ments. As a result, the images vary greatly in illumination,pose, and image quality, as well as in the age of the differentcelebrities. Two views are provided in the LFW database. View1 contains a training set of 2200 face pairs and a testing set of1000 face pairs and is used for model selection purposes only.View 2 contains 10 nonoverlapping sets of 600 matches and isfor performance reporting. View 2 images can be used for 10-fold cross-validation algorithms and for testing the parametersdeveloped on View 1. The classiﬁers are trained using onlyView 1. In this work we use preprocessed ‘‘prealigned ”and ‘‘funneled”images using commercial face alignment softwareavailable on the LFW website.Texture descriptors and classiﬁers 85The ofﬁcial testing protocols of both datasets are employedin the experiments reported in this section. For LFW, theImage-Restricted/No Outside Data Resultsis used. The perfor-mance indicator is the recognition rate in the FERET datasetsand accuracy in the LFW dataset. Accuracy is the proportionof true classiﬁcation results (both true positives and true neg-atives) in the population.3.2. ResultsThe ﬁrst experiment was aimed at evaluating the differentdescriptors when combined with the preprocessing methodslisted in Section2. The experiments were carried out on theLFW dataset using the complete approach described inFig. 1(including the steps of frontalization, pose creation,and feature transformation). The classiﬁer used in theseexperiments is SML. The results reported in Table 1showthe performance of each descriptor combined with eachpreprocessing method, withAlldenoting fusion by sum ruleof all the approaches in the same column.The results inTable 1clearly show that the fusion obtainedby combining all the preprocessing methods outperforms thebest single preprocessing method for each given descriptor.Another interesting ﬁnding is that the best descriptor for thisclassiﬁcation problem is MBC, and the best single preprocess-ing method is MSR.The second experiment was aimed at evaluating the utilityof some of the steps in the proposed system:/C15Frontalization:where [Y/N] denotes the presence/absence,respectively, of the frontalization step./C15Poses:where [Y/N] denotes the presence/absence, respec-tively, of the artiﬁcial poses for LFW dataset./C15Matching:where SML and SVM denote the classiﬁer used.To avoid a combinatorial explosion in computational com-plexity, the preprocessing methods, descriptors, and matchingclassiﬁers that were combined with pose creation and frontal-ization methods were ﬁxed to those inTable 2. These prepro- cessing methods, descriptors, and matchers produced the bestvalues in our previous experiments.The results reported inTable 2make evident the utility ofthe frontalization step: four approaches using frontalizationoutperform those without it. Also pose creation consistentlyproduces a performance improvement; its effectiveness isstronger, however, in the absence of frontalization. Finally,an examination of the results inTable 2shows an advantageTable 1Utility of combining the preprocessing methods.
LFW accuracy DescriptorsPreprocessing LBP HOG GOLD POEM MBC HASC RICLBP CLBPDCT 81.7 82.3 83.2 84.0 87.1 84.1 75.0 78.5OLHE 82.8 86.6 80.3 79.7 85.9 82.5 76.9 79.4MSR 86.7 87.7 88.787.7 88.7 85.9 78.2 81.6 AR 84.3 85.1 85.1 86.4 86.9 83.4 77.5 80.0ISN 87.5 83.4 88.2 84.1 88.6 85.2 78.8 82.2AS 82.3 85.1 85.7 83.2 86.8 83.3 75.5 79.8PN 86.4 86.4 87.1 86.7 88.4 86.0 76.8 81.1DG 84.9 84.0 87.4 86.4 88.5 85.3 76.7 80.5GF 84.2 84.0 85.4 85.6 87.2 83.1 76.1 80.0All88.2 88.388.288.2 89.0 87.2 79.9 83.5
Table 2Utility of various steps in the proposed method.
Frontalization Pose creation Preprocessing Descriptors Matching LFW accuracyN N MSR MBC SVM 80.6Y N MSR MBC SVM 83.0N Y MSR MBC SVM 81.2Y Y MSR MBC SVM 83.8N N MSR MBC SML 82.9Y N MSR MBC SML 88.7N Y MSR MBC SML 84.2Y Y MSR MBC SML 89.0
Table 3Analysis of the optimal dimension.
Retained variance (%) Final dimension LFW accuracy99 3288 86.42 97 3022 85.05 95 2789 84.23 93 2580 84.24 91 2390 84.67 70 1000 85.9 55 500 87.2 45 300 89.0 41 200 88.586 A. Lumini et al.in the choice of SML over SVM. However, since the perturba-tion of classiﬁers is one method for increasing the indepen-dence of classiﬁcation results, both classiﬁers prove useful inthe design of an ensemble (as is apparent in the fourthexperiment).Taking into consideration the best approach produced inthe second experiment (Table 2), the third experiment is aimedat tuning the dimension of the reduced space after the featuretransform. The results of this experiment are reported inTable 3: it is clear that a very strong dimensionality reductionis required to maximize performance. This is most likely due tothe curse of dimensionality.In the fourth set of experiments reported in Table 4,w e show the performance of some methods and fusions on boththe LFW and FERET datasets. To avoid displaying a hugetable, we report only the most interesting ensembles. The besttradeoff in performance on both the LFW and the FERETdatasets is given by the fusion between the two descriptorsPOEM and MBC.Table 4also reports the fusion of our bestapproach here with the approach proposed in [25]3(based on the SIFT and LBP descriptors coupled with SML classiﬁer):the resulting performance of this ensemble is better than thesingle approaches (seeTable 5for each of these performances).According toTable 4, the fusion between (FM +[25]) and FV produces a performance improvement if the weighing fac-tor is accurately tuned. InFig. 2we show the performance ofthe ensemble based on SML (FM +[25]) and the best ensem-ble based on SVM (labeled FV) combined by weight sum ruleas a function of the weighting factora. Before combining thetwo methods, the scores are normalized to mean 0 and stan-dard deviation 1. As can be observed, the fusion results in aslight performance gain (the result is up to 92.1% fora= 8). More experiments need to be performed (e.g. usingfurther training sets) to validate the usefulness of fusingSVM and SML.Table 4Accuracy obtained by our ensembles (in the ﬁrst column, a short name used to refer the ensemble is reported).
Fusion short name Preprocessing Descriptors Matching FERET recognition rate LFW accuracy Fb Fc Dup1 Dup2 FM All POEM + MBC SML – – – – 90.0FV All POEM + MBC SVM – – – – 86.6FM +[25]– – – – – – – 91.7FV +a(FM +[25]) – – – – – – – 92.1All POEM Angle 98.7 100 93.1 92.7 –All MBC Angle 98.5 100 91.8 89.3 –FA All POEM + MBC Angle 99.2 100 94.6 94.0 –
Table 5Comparison among the proposed ensemble with the state-of-the-arts approaches.
Methods FERET datasets LFW dataset Ref. Year Fb Fc Dup1 Dup2 Average[37]2004 93.0 51.0 61.0 50.0 63.8 –[56]2005 94.0 97.0 68.0 58.0 79.2 –[57]2005 96.3 99.5 78.8 77.8 88.1 –[58]2007 97.6 99.0 77.7 76.1 87.6 –[59]2007 98.0 98.0 90.0 85.0 92.8 –[60]2010 99.0 99.0 94.0 93.0 96.3 –[14]2012 99.7 99.5 93.6 91.5 96.07 –[11]2013 99.7 100 94.9 94.0 97.2 86.2[26]2013 98.7 100 94.6 93.6 96.7 76.9[61]2014 99.9 100 95.7 93.1 97.17 –[62]2007 – – – – – 73.9[63]2008 – – – – – 78.5[9]2009 – – – – – 79.35[64]2013 – – – – – 84.08[65]2013 – – – – – 79.08[66]2013 – – – – – 87.47[25]2013 – – – – – 88.5
a
[67]2015 – – – – – 88.97[55]2015 – – – – – 95.89[68]2015 – – – – – 91.10[69]2015 – – – – – 87.55Here 2015 99.2 100 94.6 94.0 97.0 91.7
aObtained using the source code shared by the authors of [25]and the testing protocol described in this work (which is slightly different from the one used in[25]).
3Code available athttp://empslocal.ex.ac.uk/people/staff/yy267/software.html.Texture descriptors and classiﬁers 87Finally, inTable 5a comparison with the state-of-art forboth the FERET and LFW datasets is reported. ExaminingTable 5, it is clear that system performance has signiﬁcantlyincreased the last few years. In the LFW database, we reportonly those methods that use no outside training data (as inour proposed approach and as is the case with approachesclassiﬁed in the Introduction as ‘‘shallow ”). Notice that ourproposed method is the second best approach on the LFWdataset after[55](whose results are not reproducible, sincethe method is not available).3.3. Deep featuresAs a ﬁnal experiment, we test the proposed approach using adifferent set of features, the ‘‘learned”features that were con-trasted in the introduction with ‘‘handcrafted ”features. Learned features are not deﬁned in a straightforward mannerto measure a speciﬁc property of the image (e.g. color and tex-ture) but are obtained from the internal representation of aCNN.In this work, the well-trained CNN parameters reported in[70]are used for representing the images in both the LFW andthe FERET datasets. In particular, the CNN outputs of the37th and 36th fully-connected layers are used for describingthe images. These descriptors, whose dimensionality is 6718,are labeled LF in the following. CNN was trained once in[70], and the same parameters are used in both our experimentson the LFW and the FERET datasets. Please note that in thepresent work CNN is used only for representation purposesand is trained on tiny cropped faces so that the backgroundis minimally involved in classiﬁcation. The matching step isperformed by training a general-purpose classiﬁer, viz., thosereported inTable 6. For classiﬁer training, the same protocolsused in the previous experiments and detailed in Section 3.1are employed (with no external images). Despite our attempts toline up settings for fair comparisons, it can be questionedwhether the results reported inTable 6for the ‘‘learned fea-tures”(LF) can be fairly compared to the results we reportabove on the LFW database; this is because the LF descriptorshave been obtained on a very large training set (even thoughexternal images were excluded from classiﬁer training). Sincethe FERET protocol does not contain any limitations regard-ing the use of external images, the reported results are note-worthy: they are the highest published in the literature onthe FERET datasets.The results inTable 6demonstrate that the learned featureshave good discriminant power for this problem and conﬁrm thelearning capabilities of CNN. The results for learned featureswere obtained by aligning the face images so that eyes are cen-tered and by performing a tiny crop of the face (for the hand-crafted features). Moreover, no preprocessing step was per-formed since CNN was used solely for the purpose of featureextraction and was trained using nonprocessed images.Before combining the two methods, the scores of the hand-crafted approaches (FM and FA) are normalized by dividingTable 6Accuracy obtained by the ‘‘learned-features ”LF.
Fusion short name Descriptors Matching FERET recognition rate LFW accuracy Fb Fc Dup1 Dup2 Deep LF SML – – – – 93.22FM + Deep – – – – – – 93.73Deep LF Angle 99.92 99.48 92.11 91.88 –FA + Deep – – 99.75 100 98.48 99.15 –
Table 7Computation time for the approach FM: Times aremeasured on an I5-3470 3.2 GHz – 8 GB Ram PC usingnonoptimized MATLAB code.
FMT(PP)T(D)T(M) Total Single core 1.44 10.8 1.02 13.26All the cores 0.42 2.92 1.02 4.36Accuracy 
Weight α
Figure 2Weighted sum rule between FM and FV on the LFW dataset, and the weight of FV is ﬁxed to 1, while the weight of FM variesbetween 1 and 10.S=F V+a/C1FM.88 A. Lumini et al.their scores by the number of classiﬁers that built them. Inboth cases the fusion results in a performance gain, whichreveals a partial independence among the features.3.4. Computational analysisIn this subsection, we perform an analysis on the computa-tional cost of the general approach described in Section 2.I n this analysis, we refer to the method named FM.The time complexityT(FM) of FM can be estimated as fol-lows:T(FM) =T(PP) +T(D) +T(M), whereT(PP),T(D),T (M) denote the computational costs for the preprocessingsteps, the extraction of descriptors, and matching, respectively.T(D) includes the extraction of both descriptors (POEM andMBC) from all nine preprocessed images. The computationtime of the feature transform, performed by PCA, is negligible.InTable 7computation times in seconds for the recognitionof a single 90/C290 image on an I5-3470 3.2 GHz – 8 GB RamPC using nonoptimized MATLAB code are reported.The computation time for extracting the deep features(using a single core) is 0.55 s.4. ConclusionIn this work, we proposed an ensemble of approaches thatobtain good results on the LFW dataset and that producethe best performance on the FERET datasets (see Table 6). Different preprocessing methods are coupled with two texturedescriptors to improve performance. In the FERET datasets,where the aim is identiﬁcation, we use the angle distance tomatch two faces. In the LFW dataset, where the aim is to ver-ify a given match, we use SVM and SML to match two faces.In the LFW dataset, the approach proposed here obtained92.1% accuracy, which is the second best result reported in theliterature without using outside training data. Unlike the sys-tem proposed in[55](which is the ﬁrst), the code of our fullsystem is freely available.Furthermore, the proposed system fused with a methodbased on a set of ‘‘learned”features, results in the highest per-formance published in the literature on the FERET datasets,and it also works well on the LFW dataset.In the future, we plan on testing new texture descriptors toenhance the performance of our approach. Future tests willalso be performed using outside training data for comparisonsof our approach with state-of-the-art deep learning methodstrained with millions of examples. Preliminary results alreadyreported in this work conﬁrm that the ‘‘learned features ”are a valid alternative to the ‘‘handcrafted features. ”Moreover, since our last experiments were related to features learnedfor the face recognition task, in the future we are interestedin evaluating the possibility of using features learned for differ-ent applications (i.e. object recognition, scene classiﬁcation,etc.) in order to evaluate the degree of independence of suchsets of features and their ability to work with different classiﬁ-cation problems.Author contributionsAll authors made signiﬁcant contributions.References
[1]A. Cernea, J.L. Fernandez-Martı´nez, Unsupervised ensembleclassiﬁcation for biometric applications, Int. J. Pattern Recog.Artif. Intell. 28 (4) (2014) 1456007-1–1456007-32
. [2]
L. Nanni et al, Effective and precise face detection based oncolor and depth data, Appl. Comput. Inf. 10 (1) (2014) 1–13
. [3]
S. Muruganantham, T. Jebarajan, A comprehensive review ofsigniﬁcant researches on face recognition based on variousconditions, Int. J. Comp. Theory Eng. 4 (1) (2012) 7–15
. [4]
A.-A. Bhuiyan, C.H. Liu, On face recognition using gaborﬁlters, World Acad. Sci., Eng. Technol. 28 (2007) 51–56
. [5]
S. Chopra, R. Hadsell, Y. LeCun, Learning a similarity metricdiscriminatively, with application to face veriﬁcation, in:Conference on Computer Vision and Pattern Recognition(CVPR), 2005, pp. 539–546
.[6]
M.A. Turk, A.P. Pentland, Eigenfaces for recognition, J. Cognit.Neurosci. 3 (1) (1991) 71–86
.[7]
L. Wiskott et al, Face recognition by elastic bunch graphmatching, IEEE Trans. Pattern Anal. Mach. Intell. 19 (7) (1997)775–779
.[8]
S. Lao et al, 3D template matching for pose invariant facerecognition using 3D facial model built with isoluminance linebased stereo vision, in: 15th IEEE International Conference onPattern Recognition, IEEE, 2000, pp. 911–916
. [9]
N. Pinto, J. DiCarlo, D. Cox, How far can you get with amodern face recognition test set using only simple features?, in:CVPR’09, 2009
[10] Z. Cao, et al., Face recognition with learning-based descriptor(2010) 2707–2714.[11]
N.-S. Vu, Exploring patterns of gradient orientations andmagnitudes for face recognition, IEEE Trans. Inf. Foren.Secur. 8 (2) (2013) 295–304
.[12]
N.-S. Vu, A. Caplier, Face recognition with patterns of orientededge magnitudes, in: ECCV, 2010
.[13]
Y. Liang et al, Exploring regularized feature selection for personspeciﬁc face veriﬁcation, in: ICCV, 2011
. [14]
M. Yang et al, Monogenic binary coding: an efﬁcient localfeature extraction approach to face recognition, IEEE Trans.Inf. Foren. Secur. 7 (6) (2012) 1738–1751
. [15] Y. Taigman, L. Wolf, Leveraging Billions of Faces to OvercomePerformance Barriers in Unconstrained Face Recognition, 2011<http://arxiv.org/pdf/1108.1122.pdf>. [16]
N. Kumar et al, Attribute and simile classiﬁers for faceveriﬁcation, in: International Conference on Computer Vision(ICCV), 2009
.[17]
T. Berg, P.N. Belhumeur, Tom-vs-pete classiﬁers and identity-preserving alignment for face veriﬁcation, in: British MachineVision Conference (BMVC), 2012
.[18]
C. Chan et al, Multiscale local phase quantisation for robustcomponent-based face recognition using kernel fusion ofmultiple descriptors, IEEE Trans. Pattern Anal. Mach. Intell.35 (5) (2013) 1164–1177
.[19] Y. Taigman, et al., Deepface: closing the gap to human-levelperformance in face veriﬁcation, in: Conference on ComputerVision and Pattern Recognition (CVPR), vol. 8, 2014. < http:// www.cs.tau.ac.il/~wolf/papers/deepface_11_01_2013.pdf >. [20]
C. Lu, X. Tang, Surpassing human-level face veriﬁcationperformance on LFW with GaussianFace, in: 29th AAAIConference on Artiﬁcial Intelligence (AAAI), 2014, pp. 3811–3819
.[21]
Y. Sun, X. Wang, X. Tang, Deep learning face representationfrom predicting 10,000 classes, in: Conference on ComputerVision and Pattern Recognition (CVPR), 2014, pp. 1–9
. [22]
Y. Sun, X. Wang, X. Tang, Deep learning face representationby joint identiﬁcation-veriﬁcation, in: NIPS, Montreal, 2014,pp. 1–9
.Texture descriptors and classiﬁers 89[23]Y. Sun, X. Wang, X. Tang, Deeply learned face representationsare sparse, selective, and robust, in: Conference on ComputerVision and Pattern Recognition (CVPR), 2015
. [24]
E. Zhou, Z. Cao, Q. Yin, Naive-Deep Face Recognition:Touching the Limit of LFW Benchmark or Not?, CornellUniversity Library, 2015
[25]Q. Cao, Y. Ying, P. Li, Similarity metric learning for facerecognition, in: IEEE International Conference on ComputerVision (ICCV), 2013, pp. 2408–2415
. [26]
L. Nanni et al, Ensemble of patterns of oriented edgemagnitudes descriptors for face recognition, in: InternationalConference on Image Processing, Computer Vision, and PatternRecognition (IPCV’13), 2013
.[27]
T. Hassner et al, Effective face frontalization in unconstrainedimages, in: Computer Vision and Pattern Recognition (CVPR),2015
.[28]
Y.K. Park, S.L. Park, J.K. Kim, Retinex method based onadaptive smoothing for illumination invariant face recognition,Sig. Process. 88 (8) (2008) 929–1945
. [29]
R. Gross, V. Brajovic, An image preprocessing algorithm forillumination invariant face recognition, in: 4th InternationalConference on Audio-and Video-Based Biometric PersonalAuthentication, 2003
.[30]
V. Sˇtruc, N. Pavesˇic, Photometric normalization techniques forillumination invariance, in: V. Sˇtruc, N. Pavesˇic (Eds.), Advances in Face Image Analysis: Techniques andTechnologies, IGI Global, Hershey, PA, 2011, pp. 279–300
. [31]
W. Chen, E. Meng-Joo, W. Shiqian, Illumination compensationand normalization for robust face recognition using discretecosine transform in logarithm domain, IEEE Trans. Syst., Man,Cybernet. Part B 36 (2006) 458–466
. [32]
P.H. Lee, S.W. Wu, Y.P. Hung, Illumination compensationusing oriented local histogram equalization and its applicationto face recognition, IEEE Trans. Image Process. (2012) 1–10
. [33]
D.J. Jobson, Z.u. Rahman, G.A. Woodell, A multiscale retinexfor bridging the gap between color images and the humanobservation of scenes, IEEE Trans. Image Process. 6 (7) (1997)965–976
.[34] G. Heusch, F. Cardinaux, S. Marcel, Lighting normalizationalgorithms for face veriﬁcation, IDIAP-com 05-03, 2005, p. 9.[35]
X. Tan, B. Triggs, Enhanced local texture feature sets for facerecognition under difﬁcult lighting conditions, Anal. Model.Faces Gestures (2007) 168–182, LNCS 4778
. [36]
T. Zhang et al, Face recognition under varying illuminationusing gradientfaces, IEEE Trans. Image Process. 18 (11) (2009)2599–2606
.[37]
T. Ahonen, A. Hadid, M. Pietikainen, Face recognition withlocal binary patterns, in: ECCV’04, 2004, pp. 469–481
. [38]
N. Dalal, B. Triggs, Histograms of oriented gradients for humandetection, in: 9th European Conference on Computer Vision,San Diego, CA, 2005
.[39]
M. San Biagio et al, Heterogeneous auto-similarities ofcharacteristics (hasc): exploiting relational information forclassiﬁcation, in: ICCV13, 2013
.[40]
G. Serra et al, Gold: Gaussians of local descriptors for imagerepresentation, Comput. Vis. Image Underst. 134 (May) (2015)22–32
.[41]
R.O. Duda, P.E. Hart, D.G. Stork, Pattern Classiﬁcation,second ed., Wiley, New York, 2000
. [42]
C.-C. Chang, C.-J. Lin, LIBSVM: a library for support vectormachines, ACM Trans. Intell. Syst. Technol. (TIST) 2 (2011) 1–39
.[43]
N.-S. Vu, H.M. Dee, A. Caplier, Face recognition using thePOEM descriptor, Pattern Recogn. Lett. 45 (7) (2012) 2478–2488
.[44]
P. Belhumeur, J. Hespanha, D. Kriegman, Eigenfaces vs.ﬁsherfaces: recognition using class speciﬁc linear projection,IEEE Trans. Pattern Anal. Mach. Intell. 19 (7) (1997) 711–720
.[45]G. Csurka et al, Visual categorization with bags of keypoints, in:ECCV International Workshop on Statistical Learning inComputer Vision, 2004, pp. 59–74
.[46]
R. Nosaka, C.H. Suryanto, K. Fukui, Rotation invariant co-occurrence among adjacent LBPs, in: ACCV Workshops, 2012,pp. 15–25
.[47]
Z. Guo, L. Zhang, D. Zhang, A completed modeling of localbinary pattern operator for texture classiﬁcation, in: IEEETrans. Image Process., 2010 (eoub.)
. [48]
J.L. Fernandez-Martı´nez, A. Cernea, Numerical analysis andcomparison of spectral decomposition methods in biometricapplications, Int. J. Pattern Recog. Artif. Intell. 28 (1) (2014)14560–14593
.[49]
J.L. Ferna´ndez-Martı´nez, A. Cernea, Exploring the uncertaintyspace of ensemble classiﬁers in face recognition, Int. J. PatternRecog. Artif. Intell. 29 (3) (2015)
.[50]
J.L. Ferna´ndez-Martı´nez et al, Aligned pso for optimization ofimage processing methods applied to the face recognitionproblem, in: Swarm, Evolutionary, and Memetic Computing,Springer, Berlin, 2013, pp. 642–651
.[51]
J.L. Fernandez-Martı´nez et al, From bayes to tarantola: newinsights to understand uncertainty in inverse problems, J. Appl.Geophys. 98 (2013) 62–72
.[52]
N. Cristianini, J. Shawe-Taylor, An Introduction to SupportVector Machines and Other Kernel-Based Learning Methods,Cambridge University Press, Cambridge, UK, 2000
. [53]
J. Phillips et al, The feret evaluation methodology for face-recognition algorithms, IEEE Trans. Pattern Anal. Mach. Intell.22 (2000) 1090–1104
.[54]
G.B. Huang et al, Labeled Faces in the Wild: A Database forStudying Face Recognition in Unconstrained Environments,University of Massachusetts, Amherst, 2007
. [55]
S.R. Arashloo, J. Kittler, Class-speciﬁc kernel fusion of multipledescriptors for face veriﬁcation using multiscale binarisedstatistical image features, IEEE Trans. Inf. Foren. Secur. 9(12) (2014) 2100–2109
.[56]
W. Zhang et al, Local Gabor binary pattern histogram sequence(LGBPHS): a novel non-statistical model for face representationand recognition, in: IEEE International Conference onComputer Vision, ICCV’05, Beijing, 2005
. [57]
W. Deng, J. Hu, J. Guo, Gabor-eigen-whiten-cosine: a robustscheme for face recognition, in: W. Zhao, S. Gong, X. Tang(Eds.), Second International Workshop, AMFG, Beijing, China,2005, pp. 336–349
.[58]
B. Zhang et al, Histogram of Gabor phase patterns (HGPP): anovel object representation approach for face recognition, IEEETrans. Image Process. 16 (1) (2007) 57–68
. [59]
X. Tan, B. Triggs, Fusing Gabor and LBP feature sets forkernel-based face recognition, in: Analysis and Modelling ofFaces and Gestures, LNCS, vol. 4748, Springer, NY, 2007, pp.235–249
.[60]
S. Xie et al, Fusing local patterns of Gabor magnitude and phasefor face recognition, IEEE Trans. Image Process. 19 (5) (2010)1349–1361
.[61]
Z. Chai et al, Gabor ordinal measures for face recognition,IEEE Trans. Inf. Foren. Secur. 9 (1) (2014) 14–26
. [62]
E. Nowak, F. Jurie, Learning visual similarity measures forcomparing never seen objects, in: Computer Vision and PatternRecognition (CVPR), 2007
.[63]
L. Wolf, T. Hassner, Y. Taigman, Descriptor based methods inthe wild, Science 6 (2008) 1–14
.[64]
H. Li et al, Probabilistic elastic matching for pose variant faceveriﬁcation, in: Computer Vision and Pattern Recognition,2013, pp. 3499–3506
.[65]
S.R. Arashloo, J. Kittler, Efﬁcient processing of mrfs forunconstrained-pose face recognition, in: IEEE SixthInternational Conference on Biometrics: Theory, Applicationsand Systems (BTAS), 2013, pp. 1–8
.90 A. Lumini et al.[66]K. Simonyan et al, Fisher vector faces in the wild, in: BritishMachine Vision Conference, 2013, pp. 8.1–8.11
. [67]
H. Li, G. Hua, Hierarchical-pep model for real-world facerecognition, in: IEEE Conference on Computer Vision andPattern Recognition (CVPR), 2015, pp. 4055–4064
. [68]
H. Li et al, Eigen-pep for video face recognition, in: ComputerVision (ACCV 2014), 2015, pp. 7–33
.[69]F. Juefei-Xu, K. Luu, M. Savvides, Spartans: single-sampleperiocular-based alignment-robust recognition techniqueapplied to non-frontal scenarios, Image Process. 24 (23) (2015)
. [70]
O.M. Parkhi, A. Vedaldi, A. Zisserman, Deep face recognition,in: British Machine Vision, 2015, p. 6
.Texture descriptors and classiﬁers 91