CactiViT: Image-based smartphone application and transformer networkfor diagnosis of cactus cochineal
Anas Berkaa,b,⁎,A d e lH aﬁaneb, Youssef Es-Saadya, Mohamed El Hajjia, Raphaël Canalsb, Rachid Bouharroudc
aIRF-SIC Laboratory, Ibnou Zohr University, BP 8106 —Cite Dakhla, Agadir 80000, Morocco
bINSA CVL, University of Orleans, PRISME Laboratory, EA 4229, Bourges 18022, France
cRegional Center for Agricultural Research of Agadir, National Institute of Agricultural Research, Avenue Ennasr, PoB 415 Rabat Principale, Rabat 1 0090, Morocco
abstract article info
Article history:Received 24 August 2022Received in revised form 14 June 2023Accepted 5 July 2023Available online 13 July 2023The cactus is a plant that grows in many rural areas, widely used as a hedge, and has multiple bene ﬁts through the manufacture of various cosmetics and other products. However, this crop has been suffering for some time fromthe attack of the carmine scaleDactylopius opuntia(Hemiptera: Dactylopiidae). The infestation can spread rapidly if not treated in the early stage. Current solutions consist of regular ﬁeld checks by the naked eyes carried out by experts. The major difﬁculty is the lack of experts to check all ﬁelds, especially in remote areas. In addition, this requires time and resources. Hence the need for a system that can categorize the health level of cacti remotely.To date, deep learning models used to categorize plant diseases from images have not addressed the mealybug infestation of cacti because computer vision has not suf ﬁciently addressed this disease. Since there is no pub- lic dataset and smartphones are commonly used as tools to take pictures, it might then be conceivable for farmersto use them to categorize the infection level of their crops. In this work, we developed a system called CactiVITthat instantly determines the health status of cacti using the Visual image Transformer (ViT) model. We also pro-vided a new image dataset of cochineal infested cacti.
1Finally, we developed a mobile application that delivers the classiﬁcation results directly to farmers about the infestation in their ﬁelds by showing the probabilities re- lated to each class. This study compares the existing models on the new dataset and presents the results obtained.The VIT-B-16 model reveals an approved performance in the literature and in our experiments, in which itachieved 88.73% overall accuracy with an average of +2.61% compared to other convolutional neural network(CNN) models that we evaluated under similar conditions.© 2023 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Keywords:CactusCochinealSmartphonesClassiﬁcation VITDeep learning
1. IntroductionCactus is a plant that is used in many communities around the worldas a land boundary or crop. It plays a fundamental role in improvingland protection against erosion while contributing to the preservationof natural resources and biodiversity ( Le Houérou, 1996). It is used for various purposes such as food, medicine, and cosmetics ( Stintzing and Carle, 2005). In addition, in several countries, the cactus is consideredan alternative crop with a high added value product for the develop-ment of arid and semi-arid areas (Grifﬁth, 2004). Most of their cultivars are found in North America (Tigano et al., 2020). Other countries such as Algeria, Argentina, Ethiopia, France, Italy, Mexico, Morocco, SouthAfrica, and Tunisia also cultivate it (Amani et al., 2019).Despite efforts to protect and maximize cactus production, it isthreatened by several diseases and pests. These include Phyllosticta opuntiaerust,Phytophthora cactorumcactus downy mildew,Ceratitis capitataceratitis andDactylopius opuntiae(Cockerell) opuntia cochineal scale (Nobel, 2002;Dodd et al., 1940;Donkin, 1977). Cochineal infesta- tion is the main current problem for the cactus; it was ﬁrst detected in Morocco in late 2014 (Bouharroud et al., 2016) and quickly spread to several provinces. There have also been other reports of this pest attackfrom other countries (Miller, 1996;Foldi, 2001;Aldama et al., 2005; Spodek et al., 2014). As for North and South America, the insect was ini-tially used to control the invasive spread of the opuntia ( Lotto, 1974; Foxcroft and Hoffmann, 2000): this pest causes severe damage to theplant by secreting a white wax on its leaves and fruits ( Lotto, 1974), thus the visual indication in images showing that the cactus has been in-fected. For example, an early-stage infestation can be seen in Fig. 1.a while inFig. 1.b, a late-stage infection with the cochineal is displayed.Early detection of diseases prevents damage to the crops in general(DiMiceli et al., 2021), so it is important for farmers to diagnose theArtiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
⁎Corresponding author.E-mail address:anas.berka@insa-cvl.fr(A. Berka).
1The dataset is open-source and available at: https://github.com/AnasBerka/CactiViT- materials.git
https://doi.org/10.1016/j.aiia.2023.07.0022589-7217/© 2023 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/disease. However, this requires regularﬁeld checks with naked eyes conducted by experts, which is generally not within their reach(Ramesh et al., 2018).On the other hand, the development of arti ﬁcial intelligence and mo- bile applications has opened up several opportunities for developingnew technological approaches to provide valuable assistance tools tofarmers (Li et al., 2020;Zhu et al., 2021). As smartphones are commonly used tools to acquire photos, they could be used to recognize plants, de-tect diseases, classify the health statuses and assess the infection level ofac r o p si nt h eﬁelds. Indeed, in recent years, more and more people havesmartphones (Carton et al., 2018). As a result, several mobile applica-tions have emerged in thisﬁeld (Pongnumkul et al., 2015), for example such as Pl@ntNet, which provides the characteristics of a plant from asmartphone image using deep learning approaches ( Goëau et al., 2013). Plantix is another diagnostic mobile app that uses plant imagesto detect diseases, pests, and nutritional de ﬁciencies (GmbH, 2019; Icrisat, 2019).E-agree can detect, from a picture of the plant's leaves,the type of disease (Reddy et al., 2015). The applications we cited have a common type of processing that is a cloud-base computation inwhich the system is split into two, a cloud-based computation and amobile server (Andrianto et al., 2020). Knowing that the internet is not always available, other apps offer local inference such as BioLeaf,in which the farmer can identify regions of injury on a leaf attacked byinsects (Machado et al., 2016). Similarly, Plant Disease App targetsgrapevine diseases on images with more options for the user to achievethe highest accuracy (Petrellis, 2017).Indeed, deep learning has enabled signiﬁcant advances in plant dis- ease identiﬁcation through various approaches (Martinelli et al., 2015; Ferentinos, 2018;Golhani et al., 2018;Ngugi et al., 2021;Ouhami et al., 2021). Starting with AlexNet, theﬁrst neural network architecture to go beyond the support vector machine known as SVM (a machinelearning model) (Krizhevsky et al., 2012), numerous models have been created and evaluated using the public PlantVillage dataset of54.306 images (Hughes et al., 2015;Yuan et al., 2022). The focus has al- ways been on proposing a better deep learning model to diagnose thediseases; models such as AlexNet combined with GoogleNet( Mohanty et al., 2016;Brahimi et al., 2017), SqueezeNet(Durmuşet al., 2017), or
even ResNet almost full (Zhang et al., 2018). InceptionV3 also showed a comparable performance (Brahimi et al., 2018;Qiang et al., 2019). DenseNet121 scored nearly perfect (Too et al., 2019), but EfﬁcientNet B4 and B5 (Atila et al., 2021) were the best performing models onPlantVillage, with 99.97% and 99.91% respectively. This raises manyquestions about the performance of deep learning on speci ﬁc crops with a smaller image dataset. In fact, many researchers have proventhe effectiveness of transfer learning using variety of models with trans-fer learning from The ImageNet dataset ( Krizhevsky et al., 2017). These techniques have been tested on crops for which data are available, suchas citrus (Shrivastava et al., 2019;Barman et al., 2020;Kaur et al., 2020), tomatoes (Fang and Ramasamy, 2015;Xie et al., 2017;El Massi et al., 2021;Ouhami et al., 2020), and grapevines (MacDonald et al., 2016; Wang et al., 2019;Kerkech et al., 2020;Reedha et al., 2022). Recent developments in computer vision ( Gao et al., 2021;Liang et al., 2021;Gheﬂati and Rivaz, 2022) show that the results from the Vi-sual image Transformers (ViT) using the transformers architecture andthe self-attention and multi-head mechanism in ( Vaswani et al., 2017) are promising. (Dosovitskiy et al., 2020) achieve optimal accuracy using ViT-H and ViT-B on ImageNet. This proved that transformers canfocus on the image, extract relevant information, and that the use oftransformers cannot be limited to the natural language processing(NLP) problem, which is revolutionary ( Wolf et al., 2020). Second, a higher accuracy of 90.45% on ImageNet classi ﬁcation was recorded after scaling the transformers architecture ( Zhai et al., 2021). But for ViT to achieve this performances, pre-training on an exceptionallylarge dataset is required (Touvron et al., 2020). The use of transformers in agriculture has successfully detected diseases in vineyards ( Reedha et al., 2022) and cassava (Thai et al., 2021). Within a few years, trans- formers will likely to be used for many more tasks in arti ﬁcial intelli- gence. To the best of our knowledge, few researchers have addressedthe cochineal infection problem from a computer vision perspective.In (Atitallah et al., 2021;Kaweesinsakul et al., 2021;Perez et al., 2022)the authors addressed the detection, classi ﬁcation of various cactus spe- cies, and segmentation of the plant cactus in complex environments.Since no public dataset is available, it is necessary to have a dataset ofimages acquired by smartphone for model training; therefore we haveﬁrst constructed our image dataset.In this paper, we propose an assistant system to diagnose the healthcondition of the cacti based on mobile images and Visual image Trans-formers (VIT). The system we propose is composed of two main compo-nents as described inFig. 2: a remote server and a mobile application onedge device. Theﬁrst one is for data collection and training the ViTmodels. The second one is operated on the mobile application, wherea user can take a photo of cactus, obtain information concerning thehealth status of the cacti and provide a feedback. To avoid running aheavy computations on the smartphone, a remote machine capable of
Fig. 1.Early-stage infection (a) and late-stage infection (b) of the cochineal on cactus.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
13performing the storage and training tasks, was used to store the datasetand regularly re-train the ViT model to perform updates. The latest up-date will then be sent to the individual users' s edge device via the appli-cation for better accuracy. On the other side, the smartphone willacquire new cactus photos or select some from the gallery and performthe classiﬁcation. We also contributed with new dataset for images ofcochineal infested cacti. The proposed system could be a valuable toolfor farmers growing cacti.This work is organized as follows; theﬁrst section provides a brief overview of the research. The second section presents a description ofthe dataset, the materials and the methodology used. The third sectiondetails the conduct of experiences. In the fourth section, results are re-ported with a discussion in the same section. Then conclusions ared r a w ni nt h eﬁnal section.2. Materials and methods2.1. Dataset descriptionThe main dataset for this case study was created manually from theﬁeld using a smartphone equipped with an f/1.9 5 mm ISO-101 cameraat several locations in Morocco, precisely in the Guelmim-Oued Nounregion, on two dates: 27/03/2021 and 23/05/2021. On each date, we ac-quired data from differentﬁelds as shown inTable 1. The raw images collected from the smartphone had many variationsin saturation, angle of shot, resolution, range and zoom. After acquiringthe images, they were resized into smaller images in the same shape(256 × 256 × 3) without data augmentation. These resulting imageswere labeled using our manual classiﬁcation process shown inFig. 3in order to perform a supervised learning approach. Our technique wasto some extent based on (Vasconcelos et al., 2009;Akroud et al., 2021) since the authors had classiﬁed the infestation from 0 to 5 to dif-ferentiate between the various heath statuses. For comparison to ourwork, 0 is healthy, 1 is early stage and from 2 to 5 late stage. The princi-ple is to look at the image and then answer the question to know whatthe class is for the given image. For example, if cacti are present in theimage and there is a whitish sticky residue on them and there aremany of them, then it is a late-stage infestation.From each image acquired at a different location labeled as “Fieldi” wherei∈1;8½/C138, we extracted several smaller images corresponding tothe relevant classes. Following our method of labeling, we created theﬁrst dataset indicating the level of infestation by the carmine cochinealDactylopius opuntiaein cacti. Manual classiﬁcation with expert veriﬁca- tion for each image should give one of the seven outputs shown in thediagram inFig. 3. Note that the“Confused”class is a set of images with too much noise, a lot of mixed information or simply confusionwhere we cannot deﬁne the exact class just by looking at the image.This is because this class is aﬁlter that eliminates any confusion that re-quires on-site checking to determine which class it belongs to. This classwill not contribute to the learning or testing phase. It is stored in an iso-lated folder out of the experiences. By having the “Confused”class out- side of the prepared dataset, we use 3921 images. In
Table 2we present the number of images generated in each class for the different locations.All the images were divided by the assigned class and not by the relativeﬁeld of the source. Theﬁnal veriﬁcation was done with the help of thesame expert to be sure of the correct labels for each image.An example of each class is presented in Fig. 4. The difference be- tween each class is the level of infestation and the health status of thecacti. In the Healthy class, there is both the spiny rackets and spinelessrackets. In the Damaged class, weﬁnd any infestation other then the co-chineal, while in the Old_Dead class, it is a class of dead cactus rackets.The classes NoCactus, EarlyStage and LateStage contain the most ofthe images. Our dataset is open-source and available at: https:// github.com/AnasBerka/CactiViT-materials.git .2.2. Model: Visual image transformersThe Visual image Transformer opted as the primary model for thisstudy is practically the same as the one proposed by ( Dosovitskiy et al., 2020)f o rt h e“ViT-Base”,a ss h o w ni nFig. 5.T h i sm o d e lw a so r i g - inally based on the transformer of (Vaswani et al., 2017). LetS¼X
i,yi fgni¼1be a set ofn¼16 images of the batch extractedfrom our prepared labeled dataset. The model takes as input i,a n imageX
iof size 224/C2224/C23ðÞto predict its true labelyi.W e r e y
i∈1;6½/C138corresponds to the encoded labels of the corresponding classin the data set. In theﬁrst step, at the embedding layer, the image X
i
was reshaped fromH/C2W/C2CðÞtoN/C2P/C2P/C2CðÞwhereH,WðÞ ¼ 224, 224ðÞthe resolution of the input image,C¼3 the number of chan- nels, P the size of the patch used which can be 16 or 32 and N¼ H/C2WðÞ=P/C2PðÞthe number of the new patches.Nwill be used as the number of effective input sequence lengths for the transformer. Afterthat, a learning embedding matrix zis created to be used as an input in the next layer.z
0is theﬁrst entry to the transformer encoder, the re-sulting patches as:
Fig. 2.System overview.
Table 1Dataset acquisition.Field N° Name of theﬁeld Acquisition date Number of original images1 Douar Tamgert Lmouden 27/03/2021 302 Douar Tigenda Mirleft 27/03/2021 143 Douar Tighratin Mirleft 27/03/2021 374 Route Taandit Mirleft 27/03/2021 425 Route Tigenda 27/03/2021 166 Douar Mrah 23/05/2021 1417 Douar Mtguayzin 23/05/2021 1478 Douar Taandit 23/05/2021 218A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
14z0¼x class ;x1pE;x2pE;⋯;xNpEhiþE pos ð1Þwithz
00¼x classis a reserved token for the output of the transformer en-coder layer,x
jpis a patch of the input image, withj∈1, 2,:,Nis the index ofx
pon positionpinX i,E∈RP2/C2Cis the same learning embedding ma-trix to all patches andE
pos∈RNþ1ðÞ /C2D,w h e r eD¼P2/C2C,t h a ta d d sp o s i - tion information for the patches. Afterwards the vector z
0is forwarded to the transformer encoder withL¼12 layers for the“ViT-Base”. The sequencez
lwherel¼1...12 is then updated as follows:z
0l¼MSA LN z l−1ðÞðÞ þz l−1 ð2Þz
l¼MLP LN z lðÞðÞ þz l ð3ÞAs shown inFig. 5, in the transformer encoder layer, the updates areperformed in two phases: a Multi-head Self-Attention (MSA) (Eq. (2)) and then a Multi-Layer Perceptron (MLP) (Eq. (3)). Both use residual skip connection and apply normalization to the sequence. In the MSAstage, the key elements of the transformer encoder, after getting thenormalized input, the model starts by calculating the heads followinghead
i¼Attention QWQi,KWKi,VWVi/C16/C17 ð4ÞwereQ,KandVare matrices ofq
i¼WQ/C1LNðzil−1Þ,k i¼WK/C1LNðzil−1Þ andv
i¼WV/C1LNðzil−1Þrespectively, andWQ,WKandWVare matrices to be learned.MultiHead Q,K,VðÞ ¼Concat head
1,...,head h ðÞWOð5ÞThen calculate the Multi-head function with WOanother matrix for the model to learn. After that, a two-layer MLP is applied. Note that thevariants proposed in (Dosovitskiy et al., 2020) extend the number of the statistics parameters for“ViT-Large”and“ViT-Huge”,h e n c et h en e e df o r more computational power for the ofﬁcial variants.3. Experiment setupIn this work, two main experiments were conducted. The ﬁrst exper- iment was limited to the minimum number of images in the folder of aclass. In our experiments, we set this number to 100 images per classsince the minimum was 100 images in the “Damaged”class. Since we have six classes, the total number of images for the ﬁrst experiment was 600 images to have the same number of images in each class, thisto avoid having an unbalanced dataset. For the second experiment, weused all the data in an unbalanced dataset but in a strati ﬁed distribution to evaluate the dataset of 3921 images.For the hyper-parameter search, a grid search was performed to se-lect the best optimizer to use and the corresponding learning rate (LR).Each time, models were reset and a unique random combination of theparameters was used for the search. At the end of the test, only param-eters that allow the model to achieve the best accuracy and lower losscompared to the previous results while searching were recorded andused for the experiments. The search uses 150 images from the datasetin a balanced conﬁguration, and the training was limited to 5 epochsdue to limited time and resources.Before feeding the model, a resize to a shape of (224x224x3) was ap-plied, with random horizontalﬂip and normalization. In addition, in allthe experiments, we varied the test size by changing the number of
Fig. 3.Architecture of the data labeling used.
Table 2Distribution of the 3921 manually labeled images.ClassesField 1 Field 2 Field 3 Field 4 Field 5 Field 6 Field 7 Field 8 TotalDamaged13 0 2 27 16 7 30 5 100 EarlyStage76 0 12 5 12 113 28 55 301 Healthy2 1 00934 7 1 8 2 5 123 LateStage82 71 329 256 57 507 79 1244 2625 NoCactus46 41 49 78 30 211 143 71 669 Old_Dead11 0 10 20 3 31 3 25 103 Total249 112 402 395 121 916 301 1425 3921A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
15folds used for cross-validation of the overall datasets to validate theevaluations on all images. For example, by choosing 3 folds for training,the data were divided into 3 sets. These sets were selected in a strati ﬁed method using the Sklearn method “StratiﬁedKFold”. Each is identical in the number of images per class without any data augmentation. Onefold was used for the test (33%). The other two folds were combinedand used for the training (67%) with the hyperparameters already se-lected. This training set was then split to search for the best set for val-idation using only ten epochs. Then we started the training process in100 epochs using the best conﬁguration for validation. When themodel converged, we trained it again using the remaining validationset. We repeated the process for the following train-test sets, and calcu-lated the overall performance of the results on the data.Experiences were uniﬁed under the same condition for alltests using a local machine equipped with a Xeon(R) W-2123 @3.60GHz × 8 CPU, 32 Gb RAM, 500 Gb SSD and NVIDIA CorporationGP104GL [Quadro P4000] 8 Gb GDDR5 for training and evaluation ofall experiences in the same global conﬁguration. As for software, the Py- thon 3 language was chosen because its libraries, such as Timm(Wightman, 2019), facilitate obtaining pre-trained models weights onpublic datasets to apply transfer learning.The comparison between the models was evaluated using the logﬁles of the evaluation of each model. Having the y
predandytruevalues, we were able to calculate the traditional metrics: accuracy, precision, re-call, F1-score and the Matthews correlation coef ﬁcient to evaluate the confusion matrix (Zhu, 2020). All the metrics were used to obtain themodel performance using the equations:Accuracy¼
TPþTNTPþTNþFPþFN ð6Þ
Fig. 4.Example of images from our dataset.
Fig. 5.The ViT-B architecture.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
16Precision¼TPTPþFP ð7ÞRecall¼
TPTPþFN ð8ÞF1¼
2∗Precision∗RecallPrecisionþRecall¼ 2∗TP2∗TPþFPþFNð9ÞMCC¼
TP⋅TN/C0FP⋅FNﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃTPþFPðÞ⋅TPþFNðÞ⋅TNþFPðÞ⋅TNþFNðÞp ð10ÞTP = True Positive, TN = True Negative, FP = False Positive, FN = FalseNegative.The setup we used for our CactiVIT mobile application was different.CactiVIT was built for Android 5.0 Lollipop (API 21) users to ensure thatall users could use our app. The software application used to create itwas Android Studio 2021.1.1 Patch 2 (Bumblebee). The coding wasdone using Python, Java, XML and PHP. The machine used for codingwas equipped with an AMD Ryzen-76800H CPU @ 4.10GHz × 16,16 Gb RAM, 500Gb SSD NVMe and NVIDIA RTX 3070Ti 8Gb GDDR6.For testing, we used virtual machines and an actual smartphone runnigandroid 11 equipped with a Mediatek Dimensity 700 2.2 GHz × 8 CPU,6 Gb RAM and 128Gb ROM.4. Results and discussion4.1. Evaluation of ViTSince ViT had multiple variations, in this subsection we focused oncomparing them to each other in order to select the best ones to thencompare with the rest of the CNN-based models. The criterion forchoosing one variety over another was the performance obtained onour local machine. These tests revealed that due to resource limitations,the“ViT-Lage”and“ViT-Huge”architectures could not be evaluated.Using pre-trained weights, the models achieved high accuracy eventhough it was theﬁrst time they were run on this data.After many experiences, the results in Fig. 6show that the average score gave the advantage to the “ViT-B/16 mill”model variation. The score is based on the average of the metrics: Accuracy, Precision, Recall,F1-score and MCC. The selected model “ViT-B/16 mill”performed the best because it has the original “ViT-Base”architecture that uses 16patches and was pre-trained by (Ridnik et al., 2021) on the ImageNet dataset: thus it performed better than the others due to the complexityof this dataset.4.2. Comparison of ViT with other modelsTo compare the selected model “ViT-B/16 mill”with other CNN models, we performed our experiences on the local machine. The crite-rion was the same: models that could be tested on the local machine.The results of theﬁrst experiment using transfer learning presentedinTable 3show the comparison of the selected models in various con-ﬁ
gurations. The ViT-B/16 model performed best using 50%, 33% and25% of the dataset as test. But the ViT-B/32 outperformed it whileusing only 20% as test. This is because the ViT-B/32 had a larger inputpatches size in this conﬁguration that required more data for training.Overall, for this balanced dataset of 600 images, the ViT-B/16 has an av-erage accuracy of 73%, followed by the ResNet26 with 70.66% while thethird was DenseNet169 with 70.25%.The results of the second experiment presented in Table 4,s h o wa higher performance due to the use of a larger dataset. We comparedthe same selected models in the various con ﬁguration, but this time using the large, unbalanced dataset of 3921 images. Once again, theViT-B/16 model performed the best with an average accuracy of88.73%. It was followed by the ViT-B/32_in21k model with 87.57%while the third is DenseNet169 with 87.04%.We can assume from the results inTables 3 and 4that ViT performed best because it was trained on more data. Also, looking at the classi ﬁca- tion metrics for each class inTable 5despite the low accuracy on half ofthe classes, the ViT had the best results we had seen. Conversely, CNNmodels like DenseNet can obtain acceptable results without transferlearning, but by adding this knowledge, additional performance wasproven, but still inferior to the performance obtained by ViT. We canconclude that ViT, in general, achieves signi ﬁcant improvements after using transfer learning (TL) and a maximum amount of data for training.4.3. DiscussionThe experiences revealed inTables 3 and 4that having a larger dataset can improve the models' performances signi ﬁcantly. Interest- ingly, ViT remains in the lead in both experiences. The main differenceis that ViT analyzes the entire image and then uses the attention
Fig. 6.Comparison of ViT variations.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
17mechanism to understand the semantics in the provided data, like nat-ural language processing (NLP), which leads to better results. CNN, onthe other hand, focuses locally on the neighbors of the pixels, so itmay miss useful information. The problem arises when we havemulti-features, such as having an early-stage cochineal infestation onthe top of the cactus with a spiny rackets, in this cases the model cannotpredict with a high accuracy. This precision is crucial for agents since wecannot wait for the cochineal to spread. The low accuracy mainly onDamaged, EarlyStage, and Healthy, is due to the similar features sharedbetween them. While cochineal has the main characteristic of whitewax on the leaves of cacti and is very bright in the late stage, in theearly stage, indications that a cactus is infected can easily be mistakenfor thorns with sun reﬂection or dust or even another infestationother than cochineal.Table 3Results of the experimentation N°1 using our dataset of cacti limited to 100 images per class.Experience N°1 20% Test 25% Test 33% Test 50% TestModel Acc F1 Acc F1 Acc F1 Acc F1DenseNet121 70.00 69.71 70.50 70.16 69.17 68.77 61.50 60.27DenseNet161 73.17 72.82 68.83 68.76 70.50 70.23 65.67 64.40DenseNet169 71.67 71.33 71.50 71.24 71.00 70.78 66.83 66.45DenseNet201 72.00 71.84 68.50 67.79 69.83 69.59 66.33 66.00EfﬁcientNet_b0 68.67 68.30 68.67 68.15 65.83 65.49 58.50 58.38EfﬁcientNet_b1 69.67 69.45 63.67 62.89 65.33 65.22 60.00 58.36EfﬁcientNet_b2 67.83 67.94 66.67 66.51 67.17 67.21 57.50 57.22Inception_Resnet_V2 69.67 69.51 63.17 63.28 67.50 67.13 61.00 61.02Resnet18 71.50 71.35 70.17 69.82 70.17 69.83 66.17 65.88Resnet26 71.17 70.76 73.00 72.61 70.17 69.95 68.33 67.92Resnet34 69.50 69.29 69.33 69.00 66.83 66.25 61.67 60.87ViT-B/16 mill 73.50 73.24 74.00 73.66 74.00 73.80 70.50 70.11 VIT-B32_in21k 73.67 73.50 70.50 70.46 72.50 72.31 63.67 62.79 Xception 72.67 72.37 70.00 69.50 69.67 69.47 65.33 64.99
Table 4Results of the experimentation N°2 using all images in our dataset of cacti with unbalanced data.Experience N°2 20% Test 25% Test 33% Test 50% TestModel Acc F1 Acc F1 Acc F1 Acc F1DenseNet121 87.57 87.53 87.19 87.04 86.81 86.61 85.00 84.48DenseNet161 87.22 86.94 87.14 87.11 86.94 86.41 85.69 85.15DenseNet169 87.68 87.62 87.75 87.45 87.32 86.92 85.43 84.86DenseNet201 87.19 87.29 85.92 86.05 85.97 85.96 83.98 83.95EfﬁcientNet_b0 87.80 87.20 86.99 86.41 85.43 84.86 84.69 83.98EfﬁcientNet_b1 87.65 87.32 87.57 87.19 86.35 85.67 83.59 82.98EfﬁcientNet_b2 87.24 87.16 86.43 86.42 85.79 85.32 83.92 83.38Inception_resnet_v2 83.26 83.37 84.41 84.13 84.05 83.48 82.39 82.13Resnet18 87.78 87.37 86.45 86.17 86.55 86.08 84.56 83.94Resnet26 86.94 87.10 86.12 86.01 85.97 85.62 84.49 84.18Resnet34 86.32 86.26 85.40 84.95 85.33 84.85 83.01 82.49ViT-B/16 mill 88.62 88.45 89.26 88.78 88.70 88.29 88.34 87.42VIT-B32_in21k 88.01 87.45 88.47 87.81 87.85 87.20 85.94 84.62Xception 87.55 87.40 87.65 87.49 86.35 86.28 80.10 80.70
Table 5ViT-B/16 mill performance.Class Experimentation N°1 Experimentation N°2N° Images Accuracy N° Images AccuracyDamaged 100 55% 100 50%EarlyStage 100 66% 301 73%Healthy 100 57% 123 45%LateStage 100 76% 2625 91%NoCactus 100 99% 669 97%Old_Dead 100 85% 103 53%
Fig. 7.Matrix confusion DensNet169.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
18InFigs. 7 and 8,w ec a no b s e r v et h a tt h em a i nc o n f u s i o ni sb e t w e e nthe classes Damaged, EarlyStage and Healthy; this is due to the similar-ities between the images of these classes.The main drawback of this data set, since it was acquired in the ﬁeld, is the possibility of confusing the waxy symptoms of cochineal withthorns. The existence of these thorns can be found on both healthya n di n f e c t e dc a c t i(Fig. 9). In addition, images were not acquiredunder the same conditions, the image scale was random, and theangle of the shots was notﬁxed at constant values.4.4. The mobile application: CactiViTThe proposed mobile application, “CactiViT,”is speciﬁcally designed to assess the health status of cactus based on images captured by theuser. It provides a more detailed classiﬁcation by displaying the accu- racy for every class for convenience use. Furthermore, the applicationallows users to provide valuable feedback on the classi ﬁcation results, facilitating ongoing improvements and re ﬁnements to the system. The process of using the app is easy for farmers. After logging in, theyjust take a picture and obtain results. The main advantage of our mobileapplication is that it uses the serialized model of the original model. So,the farmer does not need to be connected to internet to obtain results.All processing is performed on the phone.Fig. 10illustrates CactiViT with its variant interfaces. Fig. 10.a shows the main graphical user interface (GUI) with the application's homepage: the user can sign in, log in or just use the application as a guest.
Fig. 8.Matrix confusion ViT.
Fig. 9.Possibilities of confusion.
Fig. 10.Our CactiVIT GUI.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21InFig. 10.b, the user can either take a new picture, load an image fromthe gallery or simply review the previously obtained results to give afeedback. CactiViT, provides an insight into the classes probabilities as-sociated with each health status of an image. The example on Fig. 10.c CactiViT indicates that the image corresponds to a cactus in the earlystage of infection. While inFig. 10.d CaciViT predicts a late-stage infection.5. ConclusionDiagnosing the health statuses for vegetation product using onlymobile phone images is the main goal of preventing losses and improv-ing the quality and quantity of plant resources. The proposed applica-tion can help farmers to have an idea about the cactus in remote ﬁeld by showing the probabilities of the health statuses, in case of an infesta-tion exist the farmer can take decision for an early intervention. It has acomplete system that acquires data, train it on a remote machine usingthe ViT model, notiﬁes landowners of a developing infestation and helpsconserve resources. In this paper, we have contributed with the applica-tion and the dataset used so that researchers can make further improve-ments. In addition, we have compared the classi ﬁcation models on our dataset, and ViT-B/16 was the best performing model with a score of88.73%, followed by ViT-B/32_in21k with 87.57%; in comparison, theDenseNet169 model scored 87.04%. In comparison, the Densnet169scored 87.04%. To further our research, we plan to create a better per-forming model with low environmental impact. Further studies willfocus on serialization to deploy a better mobile application. In addition,it will be generalized to monitor other crops.CRediT authorship contribution statementAnas Berka:Software, Writing–original draft, Conceptualization, Methodology, Formal analysis, Data curation, Visualization, Investiga-tion.Adel Haﬁane:Supervision, Writing–review & editing, Conceptu- alization, Methodology, Validation. Youssef Es-Saady:Supervision, Writing–review & editing, Conceptualization, Methodology, Validation,Investigation.Mohamed El Hajji:Supervision, Writing–review & editing, Conceptualization, Methodology, Validation, Investigation.Raphaël Canals:Supervision, Writing–review & editing, Conceptuali- zation, Methodology, Validation.Rachid Bouharroud:Writing–review & editing, Validation, Data curation.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.AcknowledgementsThis research was made possible by project PHC Toubkal/21/121 – Campus France: 45942UG who funded the work and the mobility feebetween Morocco and France. We also thank Mr. Marouane EL KAOURIfor his ongoing collaboration to create the image database.References
Akroud, H., Sbaghi, M., Bouharroud, R., Koussa, T., Boujghagh, M., El Bouhssini, M., 2021.Antibioisis and antixenosis resistance to dactylopius opuntiae (hemiptera:Dactylopiidae) in moroccan cactus germplasm. Phytoparasitica 623 –631.https:// doi.org/10.1007/s12600-021-00897-w . Aldama, A.C., Celina, L., Soto Hernandez, M., Castillo-Márquez, L., 2005. Cochineal (dactylopius coccus costa) production in prickly pear plants in the open and inmicrotunnel greenhouses. Agrociencia 39, 161 –171. Amani, E., Marwa, L., Hichem, B.S., Amel, S.H., Ghada, B., 2019. Morphological variability of prickly pear cultivars (opuntia spp.) established in ex-situ collection in Tunisia. Sci.Hortic. (Amsterdam) 248, 163 –175.Andrianto, H., Suhardi Faizal, A., Armandika, F., 2020. Smartphone application for deep learning-based rice plant disease detection. 2020 International Conference on Infor-mation Technology Systems and Innovation (ICITSI). IEEE.Atila, U., Uçar, M., Akyol, K., Uçar, E., 2021. Plant leaf disease classi ﬁcation using EfﬁcientNet deep learning model. Ecol. Inform. 61, 101182. https://doi.org/10.1016/ j.ecoinf.2020.101182.Atitallah, S.B., Driss, M., Boulila, W., Koubaa, A., Atitallah, N., Ghézala, H.B., 2021. An en-hanced randomly initialized convolutional neural network for columnar cactus rec-ognition in unmanned aerial vehicle imagery. Proc. Comp. Sci. 192, 573 –581. https://doi.org/10.1016/j.procs.2021.08.059 . Barman, U., Choudhury, R.D., Sahu, D., Barman, G.G., 2020. Comparison of convolution neural networks for smartphone image based real time classi ﬁcation of citrus leaf dis- ease. Comput. Electron. Agric. 177, 105661.Bouharroud, R., Amarraque, A., Qessaoui, R., 2016. First report of the opuntia cochineal scale dactylopius opuntiae (hemiptera: Dactylopiidae) in Morocco. Bull. OEPP 46,308–310.Brahimi, M., Boukhalfa, K., Moussaoui, A., 2017. Deep learning for tomato diseases: classi-ﬁcation and symptoms visualization. Appl. Artif. Intell. 31, 299 –315.https://doi.org/ 10.1080/08839514.2017.1315516 . Brahimi, M., Arsenovic, M., Laraba, S., Sladojevic, S., Boukhalfa, K., Moussaoui, A., 2018.Deep learning for plant diseases: Detection and saliency map visualisation. Humanand Machine Learning. Springer International Publishing, pp. 93 –117. Carton, B., Mongardini, M.J., Li, Y., 2018. A New Smartphone for every Fifth Person on Earth: Quantifying the New Tech Cycle. International Monetary Fund.DiMiceli, C., Townshend, J., Carroll, M., Sohlberg, R., 2021. Evolution of the representationof global vegetation by vegetation continuous ﬁelds. Remote Sens. Environ. 254, 112271.https://doi.org/10.1016/j.rse.2020.112271 . Dodd, A.P., et al., 1940.The Biological Campaign against Prickly-Pear. CommonwealthPrickly Pear Board, Brisbane.Donkin, R.A., 1977. Spanish red: an ethnogeographical study of cochineal and the opuntiacactus. Trans. Am. Philos. Soc. 67, 1. https://doi.org/10.2307/1006195 . Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2020. AnImage is Worth 16x16 Words: Transformers for Image Recognition at Scale. http:// arxiv.org/abs/2010.11929.Durmuş, H., Güneş, E.O., Kırcı, M., 2017.Disease detection on the leaves of the tomato plants by using deep learning. 2017 6th International Conference on Agro-geoinformatics. IEEE, pp. 1 –5. El Massi, I., Es-saady, Y., El Yassa, M., Mammass, D., 2021. Combination of multiple classi-ﬁers for automatic recognition of diseases and damages on plant leaves. Sign. ImageVideo Proc.https://doi.org/10.1007/s11760-020-01797-y . Fang, Y., Ramasamy, R.P., 2015. Current and prospective methods for plant disease detec-tion. Biosensors 5, 537–561.https://doi.org/10.3390/bios5030537 . Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.Comput. Electron. Agric. 145, 311 –318.https://doi.org/10.1016/j.compag.2018.01. 009.Foldi, I., 2001. Liste des cochenilles de france (hemiptera, coccoidea). Bulletin de la Societeentomologique de France 106, 303 –308.https://doi.org/10.3406/bsef.2001.16768 . Foxcroft, L.C., Hoffmann, J.H., 2000. Dispersal ofDactylopius opuntiaecockerell (homoptera: Dactylopiidae), a biological control agent of Opuntia stricta(haworth.) haworth. (cactaceae) in the kruger national park. Koedoe 43.Gao, X., Qian, Y., Gao, A., 2021. COVID-VIT: classi ﬁcation of COVID-19 from CT chest im- ages based on vision transformer models. arXiv https://doi.org/10.48550/ARXIV. 2107.01682preprint.Gheﬂati, B., Rivaz, H., 2022. Vision transformers for classi ﬁcation of breast ultrasound im- ages. 2022 44th Annual International Conference of the IEEE Engineering in Medicine& Biology Society (EMBC). IEEE, pp. 480 –483https://doi.org/10.1109/EMBC48229. 2022.9871809.GmbH, P., 2019. Plantix. The Smart Crop Assistant on Your Smartphone. https://plantix. net/en/.Goëau, H., Bonnet, P., Joly, A., Baki ć, V., Barbe, J., Yahiaoui, I., Selmi, S., Carré, J., Barthélémy, D., Boujemaa, N., Molino, J.F., Duché, G., Péronnet, A., 2013. Pl@ntNet Mobile App. ,pp. 423–424https://doi.org/10.1145/2502081.2502251 . Golhani, K., Balasundram, S.K., Vadamalai, G., Pradhan, B., 2018. A review of neural net-works in plant disease detection using hyperspectral data. Inform. Proc. Agric. 5,354–371.https://doi.org/10.1016/j.inpa.2018.05.002 . Grifﬁth, M.P., 2004.The origins of an important cactus crop, opuntia ﬁcus-indica (cactaceae): new molecular evidence. Am. J. Bot. 91, 1915 –1921. Hughes, D., Salathé, M., et al., 2015. An open access repository of images on plant health toenable the development of mobile disease diagnostics. arXiv preprint http://arxiv. org/abs/1511.08060.Icrisat, 2019. Mobile App to Help Farmers Overcome Crop Damage. http://www.icrisat. org/mobile-app-to-help-farmers-overcome-crop-damage/ . Kaur, B., Sharma, T., Goyal, B., Dogra, A., 2020. A genetic algorithm based feature optimi- zation method for citrus HLB disease detection using machine learning. 2020 ThirdInternational Conference on Smart Systems and Inventive Technology (ICSSIT).IEEE, pp. 1052–1057.Kaweesinsakul, K., Nuchitprasitchai, S., Pearce, J., 2021. Open source disease analysis sys- tem of cactus by artiﬁ
cial intelligence and image processing. The 12th InternationalConference on Advances in Information Technology, pp. 1 –7. Kerkech, M., Haﬁane, A., Canals, R., 2020. VddNet: Vine Disease Detection Network Basedon Multispectral Images and Depth Map. http://arxiv.org/abs/2009.01708 . Krizhevsky, A., Sutskever, I., Hinto, G.E., 2012. 2012 AlexNet. Adv. Neural Inf. Proces. Syst. 281, 1–9.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
20Krizhevsky, A., Sutskever, I., Hinton, G.E., 2017. ImageNet classi ﬁcation with deep convolutional neural networks. Commun. ACM 60, 84 –90.https://doi.org/10.1145/ 3065386.Le Houérou, H.N., 1996. The role of cacti (opuntiaspp.) in erosion control, land reclama-tion, rehabilitation and agricultural development in the mediterranean basin. J. AridEnviron. 33, 135–159.https://doi.org/10.1006/jare.1996.0053 . Li, Z., Wallace, E., Shen, S., Lin, K., Keutzer, K., Klein, D., Gonzalez, J.E., 2020. Train Large,then Compress: Rethinking Model Size for Ef ﬁcient Training and Inference of Trans- formers.http://arxiv.org/abs/2002.11794 . Liang, J., Wang, D., Ling, X., 2021. Image classi ﬁcation for soybean and weeds based on ViT. J. Phys. Conf. Ser. 2002, 012068. https://doi.org/10.1088/1742-6596/2002/1/012068 . Lotto, G.D., 1974.On the status and identity of the cochineal insects (homoptera:Coccoidea: Dactylopiidae). J. Entomol. Soc. South. Afr. 37, 167 –193. MacDonald, S.L., Staid, M., Staid, M., Cooper, M.L., 2016. Remote hyperspectral imaging of grapevine leafroll-associated virus 3 in cabernet sauvignon vineyards. Comput. Elec-tron. Agric. 130, 109–117.Machado, B.B., Orue, J.P., Arruda, M.S., Santos, C.V., Sarath, D.S., Goncalves, W.N., Silva,G.G., Pistori, H., Roel, A.R., Rodrigues-Jr, J.F., 2016. BioLeaf: a professional mobile appli- cation to measure foliar damage caused by insect herbivory. Comput. Electron. Agric.129, 44–55.Martinelli, F., Scalenghe, R., Davino, S., Panno, S., Scuderi, G., Ruisi, P., Villa, P., Stroppiana,D., Boschetti, M., Goulart, L.R., Davis, C.E., Dandekar, A.M., 2015. Advanced methods ofplant disease detection. A review. Agron. Sustain. Dev. 35, 1 –25.https://doi.org/10. 1007/s13593-014-0246-1.Miller, D.R., 1996. Checklist of the scale insects (coccoidea: Homoptera) of mexico. Proc.Entomol. Soc. Wash. 98, 68 –86..https://www.biodiversitylibrary.org/part/66608 . Mohanty, S.P., Hughes, D.P., Salathé, M., 2016. Using deep learning for image-based plantdisease detection. Front. Plant Sci. 7, 1419. https://doi.org/10.3389/fpls.2016.01419 . Ngugi, L.C., Abelwahab, M., Abo-Zahhad, M., 2021. Recent advances in image processingtechniques for automated leaf pest and disease recognition –a review. Inform. Proc. Agric. 8, 27–51.https://doi.org/10.1016/j.inpa.2020.04.004 . Nobel, P.S., 2002.Cacti: Biology and Uses. University of California Press.Ouhami, M., Es-Saady, Y., Hajji, M.E., Ha ﬁane, A., Canals, R., Yassa, M.E., 2020. Deep trans- fer learning models for tomato disease detection. Lecture Notes in Computer Science.Springer International Publishing, pp. 65 –73. Ouhami, M., Haﬁane, A., Es-Saady, Y., El Hajji, M., Canals, R., 2021. Computer vision, IoTand data fusion for crop disease detection using machine learning: a survey and on-going research. Remote Sens. 13. https://doi.org/10.3390/rs13132486 . Perez, M.F., Bonatelli, I.A., Romeiro-Brito, M., Franco, F.F., Taylor, N.P., Zappi, D.C., Moraes,E.M., 2022.Coalescent-Based Species Delimitation Meets Deep Learning: Insightsfrom a Highly Fragmented cactus System. Molecular Ecology Resources 22,1016–1028. Publisher: Wiley Online Library.Petrellis, N., 2017.A smart phone image processing application for plant disease diagno-sis. 2017 6th International Conference on Modern Circuits and Systems Technologies(MOCAST). 1
–4. IEEE.Pongnumkul, S., Chaovalit, P., Surasvadi, N., 2015. Applications of smartphone-based sen-sors in agriculture: a systematic review of research. J. Sens. 2015, 1 –18.https://doi. org/10.1155/2015/195308. Qiang, Z., He, L., Dai, F., 2019. Identiﬁcation of plant leaf diseases based on inception v3 transfer learning andﬁne-tuning. Communications in Computer and Information Sci-ence. Springer Singapore, pp. 118 –127 (ISSN: 1865-0929). Ramesh, S., Hebbar, R., Niveditha, Pooja, Bhat, P., Shashank, Vinod, 2018. Plant disease de- tection using machine learning. In: 2018 International Conference on Design Innova-tions for 3Cs Compute Communicate Control (ICDI3C). IEEE.Reddy, S., Pawar, A., Rasane, S., Kadam, S., 2015. A survey on crop disease detection and prevention using android application. Int. J. Innov. Sci. Eng. Technol. 2, 621 –626. Reedha, R., Dericquebourg, E., Canals, R., Ha ﬁane, A., 2022. Transformer neural network for weed and crop classiﬁcation of high resolution UAV images. Remote Sens. 14,592.https://doi.org/10.3390/rs14030592 .Ridnik, T., Ben-Baruch, E., Noy, A., Zelnik-Manor, L., 2021. ImageNet-21k Pretraining for the Masses.Shrivastava, V.K., Pradhan, M.K., Minz, S., Thakur, M.P., 2019. Rice plant disease classiﬁca- tion using transfer learning of deep convolution neural network. Int. Archiv. Photo-gram. Rem. Sens. Spat. Inform. Sci. 3, 631 –635. Spodek, M., Ben-Dov, Y., Protasov, A., Carvalho, C.J., Mendel, Z., 2014. First record of dactylopius opuntiae (cockerell)(hemiptera: Coccoidea: Dactylopiidae) from Israel.Phytoparasitica 42, 377–379.Stintzing, F.C., Carle, R., 2005. Cactus stems (opuntia spp.): a review on their chemistry,technology, and uses. Mol. Nutr. Food Res. 49, 175 –194.https://doi.org/10.1002/ mnfr.200400071.Thai, H.T., Tran-Van, N.Y., Le, K.H., 2021. Arti ﬁcial cognition for early leaf disease detection using vision transformers. 2021 International Conference on Advanced Technologiesfor Communications (ATC), pp. 33 –38https://doi.org/10.1109/ATC52653.2021. 9598303.Tigano, A., Colella, J.P., MacManes, M.D., 2020. Comparative and population genomics ap- proaches reveal the basis of adaptation to deserts in a small rodent. Mol. Ecol. 29,1300–1314.Too, E.C., Yujian, L., Njuki, S., Yingchun, L., 2019. A comparative study ofﬁne-tuning deep learning models for plant disease identi ﬁcation. Computers and Electronics in Agri- culture 161, 272–279. Publisher: Elsevier.Touvron, H., Cord, M., Douze, M., Massa, F., Sablayrolles, A., Jégou, H., 2020. Training Data-efﬁcient Image Transformers & Distillation Through Attention. http://arxiv.org/abs/ 2012.12877.Vasconcelos, A.G.V.D., Lira, M.D.A., Cavalcanti, V.L.B., Santos, M.V.F.D., Willadino, L., 2009.Seleção de clones de palma forrageira resistentes Ã cochonilha-do-carmim(dactylopius sp). Rev. Bras. Zootec. 38, 827 –831.https://doi.org/10.1590/S1516- 35982009000500007.Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, L.,Polosukhin, I., 2017. Attention is All You Need. http://arxiv.org/abs/1706.03762 . Wang, A., Zhang, W., Wei, X., 2019. A review on weed detection using ground-based ma-chine vision and image processing techniques. Comput. Electron. Agric. 158,226–240.https://doi.org/10.1016/j.compag.2019.02.005 . Wightman, R., 2019. PyTorch Image Models. https://doi.org/10.5281/zenodo.4414861 . Wolf, T., Debut, L., Sanh, V., Chaumond, J., Delangue, C., Moi, A., Cistac, P., Rault, T., Louf, R.,Funtowicz, M., Davison, J., Shleifer, S., von Platen, P., Ma, C., Jernite, Y., Plu, J., Xu, C., LeScao, T., Gugger, S., Drame, M., Lhoest, Q., Rush, A., 2020. Transformers: State-of-the- art natural language processing. Proceedings of the 2020 Conference on EmpiricalMethods in Natural Language Processing: System Demonstrations, Association forComputational Linguistics, pp. 38 –45. Xie, C., Yang, C., He, Y., 2017.Hyperspectral imaging for classiﬁcation of healthy and gray mold diseased tomato leaves with different infection severities. Comput. Electron.Agric. 135, 154–162.Yuan, Y., Chen, L., Wu, H., Li, L., 2022. Advanced agricultural disease image recognitiontechnologies: a review. Inform. Proc. Agric. 9, 48 –59.https://doi.org/10.1016/j.inpa. 2021.01.003.Zhai, X., Kolesnikov, A., Houlsby, N., Beyer, L., 2021. Scaling Vision Transformers. http:// arxiv.org/abs/2106.04560.Zhang, K., Zhao, J., Zhu, Y., 2018. MPC case study on a selective catalytic reduction in apower plant. J. Process Control 62, 1 –10.https://doi.org/10.1016/j.jprocont.2017.11. 010.Zhu, Q., 2020. On the performance of Matthews correlation coef ﬁcient (MCC) for imbal- anced dataset. Pattern Recogn. Lett. 136, 71 –80.https://doi.org/10.1016/j.patrec. 2020.03.030.Zhu, M., Tang, Y., Han, K., 2021. Vision Transformer Pruning. http://arxiv.org/abs/ 2104.08500.A. Berka, A. Haﬁane, Y. Es-Saady et al. Artiﬁcial Intelligence in Agriculture 9 (2023) 12 –21
21