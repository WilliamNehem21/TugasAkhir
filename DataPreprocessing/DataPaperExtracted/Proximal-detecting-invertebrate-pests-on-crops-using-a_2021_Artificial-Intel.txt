Proximal detecting invertebrate pests on crops using a deep residualconvolutional neural network trained by virtual images
Huajian Liua,⁎, Javaan Singh Chahlb,c
aThe Plant Accelerator, Australian Plant Phenomics Facility, School of Agriculture, Food and Wine, University of Adelaide, Waite Campus, Building W T 40, Hartley Grove, Urrbrae, SA 5064, Australia
bSchool of Engineering, University of South Australia, Mawson Lakes 5095, Australia
cJoint and Operations Analysis Division, Defence Science and Technology Group, Australia
abstract article info
Article history:Received 9 November 2020Received in revised form 7 January 2021Accepted 8 January 2021Available online 10 January 2021
Keywords:Insect detectionDeep convolutional neural networkPrecision agricultureDetecting invertebrate pests on crops at early stages is essential for pest management. Traditionally, traps wereused to sample pests and then human experts undertook classi ﬁcation and counting to estimate the levels of infestation, which is subjective, error-prone and labour intensive. Recently, semi-automatic pest detection ispossible by using computer vision technologies to classify and count pest samples in laboratories or insecttraps, however, the decision made by the laboratory-based or trap-based approaches are still too late for moreoptimised pest management decisions. Today, precision agriculture needs detection of pests on crops so thatreal-time actions can be taken or optimised decision can be made based on accurate information of time andlocation pest occurs. In this study, we used computer vision and machine learning technologies to detect inver-tebrates on crops in theﬁeld. Weﬁrst evaluated the performances of the state-of-art convolutional neural net- works (CNNs) and proposed a standard training pipeline. Facing the challenge of rapidly developingcomprehensive training data, we used a novel method to generate a virtual database which was successfullyused to train a deep residual CNN with an accuracy of 97.8% in detecting four species of pests in farming environ-ments. The proposed method can be applied to a robotic system for proximal detection of invertebrate pests oncrops in real-time.© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an openaccess article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
1. IntroductionInvertebrate pests are difﬁcult to control and the losses caused bythem are huge (Oerke 2006). Invertebrate pests can feed on leaves, af-fect photosynthesis and infect diseases ( Nalam et al. 2019). In six major Australian grain crops, the estimated annual loss from inverte-brate pests is $359.8 million (Murray et al. 2013). Although there are bi- ological and chemical methods for pest control, biological control has tobe carefully used for speciﬁc species and heavy doses and uniform pes-ticide applications have caused serious problems of pesticide resistance,environmental pollution and killing of bene ﬁcial species, such as bees (Liu et al. 2016b).Increasing challenges for pest control and the need for better on-farm efﬁciencies are driving the implementation of integrated pestmanagement (IPM) which involves pest detection, application of appro-priate management methods and recording the result of the manage-ment action applied (Boissarda et al. 2008;Kogan and Hilton 2009). Early pest detection and estimation of potential infestation and yieldloss are essential for a successful IPM program. Traps, sweep net orbeet sheets (GRDC 2014) are widely adopted for systematic pestmonitoring. If implemented properly, these sampling methods can suc-cessfully estimate populations over the entire area of interest ( Yen et al. 2013). However, farmers have to manually collect and count samplesand evaluate infestation visually, introducing a degree of subjectivity.Further, manual counting is time-consuming, labour-intensive anderror-prone. Computer vision technologies for insect classi ﬁcation were initialized by entomologists (Martineau et al. 2017), and recently have been applied as semi-automatic methods for pest managementby counting pests in sample containers or traps ( Sun et al. 2017;Zhu et al. 2017). However, using the approach of “collect samples in traps and then estimate”, decisions tend to be late, leading to reduced ef ﬁ- ciency. Due to their short life cycle, pests might have ﬁnished reproduc- tion and the population could have exceeded the threshold for minimalintervention management before adults are detected ( Baker and Jennings 2015;Miles 2015). Also, traps cannot provide the exact timeand location that pestsﬁrst occur.Early detection of pests on crops can ensure appropriate and timelymanagement decisions. First, it can provide an accurate time whencrops are attacked; second, it can provide the exact location wherecrops are infested; third, based on the time, location and species, a ro-botic system can take actions in real-time, such as selective spraying,or the information could be analysed in an IPM system to make moreoptimal decisions (Liu et al. 2016b). However, detecting pests in naturalArtiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
⁎Corresponding author.E-mail address:huajian.liu@adelaide.edu.au(H. Liu).
https://doi.org/10.1016/j.aiia.2021.01.0032589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the C C BY license (http:// creativecommons.org/licenses/by/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/environments is challenging. Natural sunlight is unstable and changesfrom time to time. Even using artiﬁcial illumination, the leaves or branches could block the light, resulting in uneven illumination ( Liu et al. 2017c). The backgrounds of natural farmingﬁelds are complex and unpredictable with different types of soil and plant residues. To sur-vive, many pests are camouﬂaged and they have the same colour andmorphological features as their living environments ( Liu et al. 2016a; Liu et al. 2017a), which raises challenges for detection. There are verylimited studies that have been reported for detecting invertebratepests in natural farmingﬁelds. The recent studies ofChahl and Liu (2018)demonstrated the possibility of detecting pests in natural farm-ing environments using computer vision technologies. They could de-tect common invertebrate pests on green leaves using multispectralimages of ultraviolet, blue, green, red and near-infrared with the limita-tion of further identifying the species of the pests. Liu et al. (2017a)re- ported that three-dimensional (3D) vision could be used to detectrelatively large invertebrate pests on broad leaves. Xia et al. (2018) used the combination of a region proposal network ( Ren et al. 2017) and a pre-trained VGG19 network ( Simonyan and Zisserman 2014) for insect classiﬁcation and achieved an average precision of 89.22% on24 common insect species in cropﬁelds.This study aimed to use computer vision and machine learning tech-nologies to detect common pests on crops in South Australia. First, wecreated a ten-class database to evaluate the performance of the state-of-art CNNs for pest classiﬁcation and developed a standard trainingpipeline. Then we used a novel method to generate virtual images tocreate an optimisedﬁve-class database. The model trained using the vir-tual database had a high classiﬁcation accuracy and can be applied toproximally detect pests on crops.2. Materials and methods2.1. Create databasesTo compare the effects of the number of classes and size of databasesfor training CNNs models, two databases, 10C-database and 5C-database, were created to train and validate the models. The 10C-database included ten classes of background, bee and eight invertebratespecies hard to control in Australia. The database included a total of6073 images in which 5573 images were used for training and 500 im-ages for validation. The class names and quantity of images are listed inTable 1and the examples images are shown in Fig. A1. The class of back-ground included images of crop leaves, fresh grasses, ﬂowers, dry plant residuals and bare soil in different colours. The numbers of images indifferent classes were roughly balanced in the range of 450 to 627except the background. Because the class of background had largeintra-class variety, it included a relatively large number of images of984. Ideally, pests should be detected as early as possible, however,when their sizes are rather small, e.g., at egg stage, macro-lenses haveto be used for imaging, which is not practical in ﬁeld applications.Invertebrate animals present different and complex morphological, col-our and textural features at their different living stages and it would re-quire detection across orders of magnitude of scale to detect them in allof their living stages. To simplify the problem, the sizes of the inverte-brates for observation were larger than 5 mm (except aphid) and theywere at the same living stage in each class. A part of the image set wascollected in multipleﬁelds using smartphones. When collecting the im-ages, the working distances of the cameras was adjusted in the range of20 to 50 cm so that theﬁelds of view were as large as possible and at thesame time the pests presented enough pixels for human identi ﬁcation. Another part of the image set was collected from either ImageNet
(StandfordVisionLab 2018) or InsectImages (InsectImages 2018). In the original images, the invertebrates might distribute randomly andoccupied only a small portion of pixels and were not suitable for trainingCNN models. The regions of interests of the targets were cropped outmanually from the original images. The cropped images have sizesfrom 128 × 128 pixels to 300 × 300 pixels and the objects can be ob-served clearly in the images. In some of the cropped images, onlyparts of the invertebrates were visible, which was intentionally ar-ranged to simulate the real scenarios in natural farming environments.The 5C-database was created with fewer classes and more images ineach class than the 10C-database. It included ﬁve classes of background, snail, slug, earwig and worm and composed of a total of 29,507 imagesin which 22,773 images were used for training and 6743 images for val-idation. The class names and quantity of images are listed in Table 2and the example images are shown in Fig. A2. In contrast to the 10C-database, the background was mainly composed of green leaves withmoderate soil and branches of trees and did not include ﬂowers and grasses. Besides that, the intra-class variety of the class of worm was en-larged by including worms in different colour and textures. The imagesin the same classes of the 10C-database were copied to the 5C-databaseto create a part of the images. As the resources of the images of pests intheﬁeld environments were limited, to further increase the size of thedatabase was challenging. Since the images of plants without pestscan be easily collected, the database of background including 5554 im-ages wasﬁrst created. However, collecting the images of pests in theﬁelds is time-consuming because pests in the ﬁelds are difﬁcult toﬁnd and the data collection is limited by season. We used a novel methodto create virtual images of pests to obtain a large number of training im-ages while signiﬁcantly reduced labour. First, we collected an image of apest (or several pests). Then we used the Photoshop software (AdobePhotoshop, California, USA) to manually segment the pest from thebackground. Third, the image including only the pest was processed togenerate random transformation, including random rotation, randomcrop, random resize and random contrast adjustment. Lastly, the trans-formed image of the pest was positioned to an image of backgroundwhich was randomly selected from the class of background to createan image in the corresponding class of pest. Each original image ofpest can be used to generate 200 to 300 virtual images in the databaseso that it signiﬁcantly reduced the time and workload.2.2. Evaluate the performances of CNN modelsSeven deep CNN models achieved outstanding performances in theImageNet classiﬁcation were evaluated using the 10C-database and
Table 1Class name and quantity of images in 10C-database.No. Class name Train Validation0 Background (BK) 984 501 Aphid (AP) 530 502 Portuguese Millipede (PM) 465 503 Earwig (EA) 454 504 Vineyard snail and white garden snail (SN) 575 505 Slug (SL) 522 506 Honeybee (HB) 627 507 Locust (LO) 512 508 Orange striped oakworm (OO) 454 509 Redlegged earthmites (RE) 450 50Sub-total 5573 500Total 6073
Table 2Class name and quantity of images in 5C-database.No. Class name Train Validation0 Background (BK) 5554 15781 Vineyard snail and white garden snail (SN) 3718 11352 Slug (SL) 4763 16523 Earwig (EA) 4223 12744 Worm (WO) 4515 1095Sub-total 22,773 6734Total 29,507H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
14they were AlexNet, VGG16, VGG19, ResNet18_v2, ResNet34_v2,ResNet50_v2 and Resent152_v2. AlexNet architecture is one of theﬁrst deep CNNs to push ImageNet classiﬁcation accuracy by a signiﬁcant stride in comparison to traditional methodologies. It is composed of ﬁve convolutional layers followed by three fully connected layers(Krizhevsky 2014). The VGGNets made improvements over AlexNetby replacing large kernel-sizedﬁlters (11 and 5 in theﬁrst and second convolutional layer, respectively) with multiple 3 × 3 kernel-sized ﬁl- ters one after another (Simonyan and Zisserman 2014). A deep CNN could suffer from problems of vanishing gradient and degradation. Theﬁrst means that earlier layers are almost negligibly learned when aCNN is deep and the second refers to adding more layers leading tohigher training error. The residual network (ResNet) successfully solvedthese problems by constructing the network through modules called re-sidual modules. It achieved better accuracy than VGGNet andGoogLeNet (Szegedy et al. 2015) while being computationally more ef-ﬁcient (He et al. 2016).The modes were trained in four different training pipelines:(1) training the models from scratch using the original images in thetraining dataset; (2) training the models from scratch with data argu-ment; (3) training the models using transfer learning without data ar-gument; (4) training the models using both transfer learning and dataargument. The purpose of data argument is to increase the number ofimages in a dataset and it has proven to be useful to improve model ac-curacy. First, the input images were cropped with random size and as-pect ratio. The areas of the cropped images were 0.5 to 1 times of thatof the original images and the aspect ratio was from 0.5 to 2. Second,the images were randomlyﬂipped from left to right and top to bottom.Lastly, the brightness of the images was adjusted randomly. The ﬁnal input images were resized to 224 × 224 for input. Previous studieshave proven that transfer learning is effective in many remote sensingapplications (Hu et al. 2015;Fu et al. 2017;Maggiori et al. 2017). Gener- ally, transfer learning method discards the last layer of a pre-trainedmodel and appends a fully connected layer where the neurons corre-spond to the number of predicted classes. During the training stage,theﬁnal layer is trained from scratch, while the others are initializedfrom the pre-trained model and updated by back-propagation rule orkeptﬁxed (Huang et al. 2018). The models of 1000 classes fully trainedon ImageNet including 14,197,122 images were used for transfer learn-ing. The training and validation pipeline was realised using the MXNetmachine learning library written in Python 3.6 ( MXNet 2018). The modes were trained with batch size 10 and learn rate 0.002 on a com-puter with a GeForce GTX1080 GPU.Confusion matrices were calculated to provide a direct observa-tion of the classiﬁcation results. Four statistics were used to evaluatethe performance of the models. The accuracy of training a
tis thepercent of correctly classiﬁed samples in the training dataset, andsimilarly,avrepresents the accuracy of validation in the validationdataset.tandeare the time (hours) used and epochs run when a
v
start converging.2.3. Test model accuracyAfter evaluating the performances of the CNN models, the trainedmodel on the 10C-database had the highest a
vwas considered as the best model on this database and it was named M10C. The training pipe-line returning the M10C was considered as the standard training pipe-line. The standard training pipeline was applied to the 5C-database totrain another model M5C. M10C and M5C were tested using indepen-dent images collected by smartphones in real farming environments.The images were either collected in day time using solar illuminationor at night using artiﬁcial illumination and the background includedmultiple crops and fruit trees.T h ep e s t sd i s t r i b u t e dr a n d o m l yi nt h ei m a g e sa n do n l yo c c u p yasmall portion of the pixels. The locations and class names of thepests needed to be detected automatically in the images. The regionproposal network (Ren et al. 2017) has been widely used to detectthe locations of targets in images. However, this method needsextra training of bounding box, involving a large amount of manuallabelling work of bounding boxes. Besides this, aphids and snailscould cluster together or distribute randomly in an image, makingit hard to deﬁne the borders of bounding boxes; thus the boundingbox technique was not suitable for this study. We used a sliding win-dow technique which is simple and efﬁcient. The images were di- vided into consequent square windows and the classi ﬁcation model was applied to the windows one by one. The size of the windowsneeded to be adjusted so that the objects had a similar scale asthose in the training data. Besides confusion matrices and classi ﬁca- tion accuracya, true positive rater
tpand true negative rater tnwere used for evaluating the performances of the models. True positiverater
tpis the number of windows correctly classi ﬁed as the corre- sponding classes of pest to the number of windows have pests.True negative rater
tnis the number of windows correctly classi ﬁed as background to the number of windows have background only.3. Results3.1. Performances of the CNN models and training pipelinesThe seven CNN models were trained using the four different pipe-lines introduced insection 2.2, resulting in 28 models. The statistics ofthe training and validation are summarised in Tables 3to6, in whichTable 3The statistics of training from scratch. a
trepresents classiﬁcation accuracy of training,a vfor classiﬁcation accuracy of validation, tfor time (hour) used for training and efor epoch used.Model AlexNet VGG16 VGG19 ResNet18_v2 ResNet34_v2 ResNet50_v2 ResNet152_v2a
t 100.00% 100.00% 99.51% 100.00% 100.00% 99.92% 99.91%a
v 70.20% 64.00% 63.20% 71.80% 73.80% 70.40% 74.00% t0.70 1.30 1.50 0.40 0.41 1.20 2.50e160 50 50 40 40 60 50
Table 4The statistics of training from scratch using data argument. a
trepresents classiﬁcation accuracy of training,a vfor classiﬁcation accuracy of validation, tfor time (hour) used for training and efor epoch used.Model AlexNet VGG16 VGG19 ResNet18_v2 ResNet34_v2 ResNet50_v2 ResNet152_v2a
t 94.23% 94.93% 94.21% 95.00% 99.61% 99.03% 98.88%a
v 82.80% 87.00% 88.40% 89.80% 90.60% 91.20%89.20% t3 13.5 15.8 2.1 2.4 7.6 9.2e500 500 500 300 200 400 200H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
15the highest accuracies of validation are highlighted by bold font. InFigs. A3 to A6, the curves of accuracy-epoch corresponding to thehighest values ofa
vinTables 3to6are plotted on the left side and thecorresponding confusion matrices of validation are on the right side.Tables 3to6show that no matter what training pipeline was used,the ResNet models always returned the highest accuracy of validation;that isa
v74.00% for ResNet152_v2, 91.20% for ResNet 50_v2, 97.60%for ResNet152_v2 and 97.80% for ResNet 152_v2. Further, theResNet152_v2 are more accurate than ResNet34 and Resnet50. Thevalues ofa
valso show that data argument can increase the accuracy ofvalidation and transfer learning can signiﬁcantly improve the accuracy and reduce the time of training. The ResNet152_v2 converged in only0.86 h and had the highest validation accuracy of 97.80%. Thus, usingResNet152_v2 with data argument and transfer learning was consid-ered as the standard training pipeline.3.2. Testing resultsTen images for each class of pests were randomly selected from thetesting database for testing. The images were divided into small win-dows of size 400 × 400 and resized to 224 × 224 as input images,resulting in 1895 windows for testing M10C and 2133 windows for test-ing M5C. The overall test result of M10C is shown in Table 7and theTable 5The statistics of training using transfer learning. a
trepresents classiﬁcation accuracy of training,a vfor classiﬁcation accuracy of validation, tfor time (hour) used for training and efor epoch used.Model AlexNet VGG16 VGG19 ResNet18_v2 ResNet34_v2 ResNet50_v2 Resen152_v2a
t 100.00% 100.00% 100.00% 100.00% 100.00% 100.00% 100.00%a
v 92.20% 95.60% 96.60% 95.80% 96.40% 97.40% 97.60% t0.17 0.39 0.50 0.08 0.10 0.67 1.40e30 15 15 10 10 30 30
Table 6The statistics of training using both transfer learning and data argument. a
trepresents classiﬁcation accuracy of training,a vfor classiﬁcation accuracy of validation, tfor time (hour) used for training andefor epoch used.Model AlexNet VGG16 VGG19 ResNet18_v2 ResNet34_v2 ResNet50_v2 ResNet152_v2a
t 98.31% 99.33% 99.58% 99.94% 99.71% 99.83% 99.80%a
v 92.20% 95.60% 96.20% 96.00% 96.20% 97.00% 97.80% t0.17 0.68 1.29 0.17 0.23 0.37 0.86e40 25 40 20 20 20 20
Table 7Test results of M10C. The 11 columns on the left side compose the confusion matrix, inwhich theﬁrst row andﬁrst column show the class name of background (BK), aphid(AP), portuguese millipede (PM), earwig (EA), vineyard snail and white garden snail(SN), slug (SL), honeybee (HB), locust (LO), orange striped oakworm (OO), redleggedearthmites (RE). The column of SUM is for the sum of the number of windows of eachclass.r
tprepresents true positive rate, r tnfor true negative rate andafor classiﬁcation accuracy.BK AP PM EA SN SL HB LO OO RE SUM r
tp
BK 1126 45 78 103 38 56 17 157 67 13 1700 58.46%A P 5 1 2 0 00 0 0 00 0 1 7 r
tn
PM 9 0 17 6 0 0 0 0 0 0 32 66.24%EA 7 0 9 15 0 0 0 0 0 0 31 a SN 2 0 0 0 16 5 0 0 0 0 23 65.44%SL 6 0 2 1 4 17 0 0 0 0 30HB 4 0 0 0 0 0 9 2 0 0 15LO 5 2 0 0 0 0 2 8 0 0 17OO 4 0 0 0 0 0 0 3 9 0 16RE 3 0 0 0 0 0 0 0 0 11 14Table 8Test results of M5C. It shows the confusion matrix and accuracy of classi ﬁcation of each class and overall classes. In each sub-table, the six columns on the left side compose theconfusion matrix, in which theﬁrst row andﬁrst column show the class name of back- ground (BK), snail (SN), slug (SL), earwig (EA) and worm (WO). The column of SUM isfor the sum of the number of windows of each class. r
tprepresents true positive rate, r tn
for true negative rate andafor classiﬁcation accuracy.SnailBK SN SL EA WO SUM r
tp
BK 483 0 0 0 2 485 95.83%SN 1 23 0 0 0 24 r
tn
SL 0 0 0 0 0 0 99.59%EA 0 0 0 0 0 0 a WO 0 0 0 0 0 0 99.41%SlugBK SN SL EA WO SUM r
tp
BK 333 1 0 4 4 342 56.25%SN 0 0 0 0 0 0 r
tn
SL 5 2 9 0 0 16 97.37%EA 0 0 0 0 0 0 a WO 0 0 0 0 0 0 95.53%WormBK SN SL EA WO SUM r
tp
BK 641 1 4 1 2 649 73.33%SN 0 0 0 0 0 0 r
tn
SL 0 0 0 0 0 0 98.77%EA 0 0 0 0 0 0 a WO 7 1 0 0 22 30 97.64%EarwigBK SN SL EA WO SUM r
tp
BK 567 2 0 1 3 573 69.23%SN 0 0 0 0 0 0 r
tn
SL 0 0 0 0 0 0 98.95%EA 3 0 1 9 0 13 a WO 0 0 0 0 0 0 98.29%All classesBK SN SL EA WO SUM r
tp
BK 2025 4 4 6 11 2050 75.90%SN 1 23 0 0 0 24 r
tn
SL 5 2 9 0 0 16 98.78%EA 3 0 1 9 0 13 a WO 7 1 0 0 22 30 97.89%H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
16results for each class of pests and all classes of M5C are illustrated inTable 8. M10C had a poor classiﬁcation accuracyaof 65.44% with true positive rater
tp58.46% and true negative rater tn66.24%. The confusion matrix shows that most of the errors are the misclassi ﬁcation of back- ground to pests or vice versa. M5C had a high overall accuracy of97.89% with the moderate true positive rate of 75.90% and an excellenttrue negative rate of 98.78%. For an individual class of pests, the classi ﬁ- cation accuracy and true negative rate are high, ranging from 95.53% to99.59%. However, the true positive rates are relatively low for slug(56.25%), earwig (69.23%) and worm (73.33%).4. Discussion4.1. Deep CNNTraditional methods for image classiﬁcation need toﬁrst extract fea- ture vectors in images and then input the vectors to machine learningalgorithms for classiﬁcation, such as support vector machine or logisticmodel tree (LMT) (Landwehr et al. 2005). For invertebrates, feature vectors could be composed of colour, textural, spectral ( Liu et al. 2016a;Liu and Chahl 2018) and morphological features (Liu et al. 2017a) or abstract feature descriptors such as statistical moment ( Han and He 2013), SIFT descriptor (Lowe 2004;Larios et al. 2008) or bag- of-region features (Csurka et al. 2004;Larios et al. 2008). Compared to the feature-based classiﬁcation, CNN classiﬁcation uses relatively less pre-processing. CNN classiﬁcation combines feature extraction and clas-siﬁcation modules into one integrated system without involving thehard work of feature extraction. Recently, the outstanding perfor-mances of deep CNNs have driven image classi ﬁcation technologies to a new milestone. Deep CNNs naturally integrate low-, middle- andhigh-level features and classiﬁers in an end-to-end multilayer fashion,and the levels of features can be enriched by the number of stackedlayers (depth) (He et al. 2016).When evaluating the performances of the CNN models using the10C-database, the ResNets outperformed AlexNet and VGG nets, mean-while, the accuracy improved when the nets were deeper. The leadingCNN models that have been successful in the challenging ImageNet da-tabase all exploit“very deep”models, with a depth from sixteen to hun-dreds of layers (He et al. 2016), indicating that network depth is ofcrucial importance. However, deep CNNs could suffer the degradationproblem: with the network depth increasing, accuracy gets saturatedand then degrades rapidly (Simonyan and Zisserman 2014).He et al. (2016)successfully addressed this problem by using deep residuallearning. In this study, ResNet152_v2 with 152 layers did not sufferfrom the degradation problem and it achieved the highest validation ac-curacy of 97.80%; thus, it is an ideal CNN for invertebrate classi ﬁcation.4.2. Data argument and transfer learningThe 10C-database had limited training data, resulting in over- ﬁtting when training the models from scratch ( Table 3
and Fig. A3). The data argument suppressed the problem of over ﬁtting by increasing the size of the training data.Table 4shows that the data argument techniquesigniﬁcantly improved the accuracy of validation from the range of70.20% to 74.00% to the range of 82.80% to 89.20%. The transfer learningprovided the surprisingly high accuracies of validation from 92.20% to97.60% (Table 5). Another advantage of transfer learning is that it cansigniﬁcantly reduce training time. For example, Restnet152_v2 usingdata argument converged at 9.2 h, however, it converged at 1.4 hwhen using transfer learning (Table 5) and converged at 0.86 h when using the combination of transfer learning and data argument ( Table 6).
Fig. 1.Typical errors when testing the M10C model. The blue windows marked with “Clear”are correctly classiﬁed as background. The red windows with class name are correctly classi ﬁed as the corresponding pests. The misclassi ﬁed windows are highlighted with yellow colour.H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
174.3. DatabaseCreating a high-quality database for training purpose is one of themost expensive and time-consuming tasks and it is a bottleneck of ma-chine learning applications in the agricultural ﬁeld (Barbedo 2020). Ma- chine learning tools have evolved to a point in which the data fed tothem has a much more prominent role in their success than their intrin-sic characteristics, which explains the relatively similar performancesyielded by different models when the data used to train and test themis the same (Kamilaris and Prenafeta-Boldú 2018;Liakos et al. 2018). The classiﬁcation accuracy inTable 3toTable 6clearly show this trend. Conversely, image sets with different data distributions and var-iability can lead to signiﬁcantly disparate performances even when thesame machine learning model is employed, a fact that was re ﬂected in the M10C and M5C database.In testing, the M10C model got an unsatisﬁed classiﬁcation accuracy of 65.44%. First, the 10C-database was not large enough to represent thetrue scenarios of natural farming environments, especially for the classof background. Second, the intra-class variance of the background waslarge because the background included soil, green leaves, dry plant re-siduals andﬂowers. Most of the errors are the misclassi ﬁcation of back- ground to pests or pests to the background. For example, in Fig. 1a, the shadow of the leaf is misclassiﬁed as Locust. InFig. 1b, the windows of the green leaf with yellow spots are identi ﬁed as orange striped oakworm that has the similar texture to yellow spots. In Fig. 1c, the grass is misclassiﬁed as locust because locusts are camouﬂaged with the same colour and textural feature of the grass. In Fig. 1d, the stems are misclassiﬁed as earwigs because plant stems were not included inthe training data.The M5C model achieved a positive testing result with an overall ac-curacy of 97.89%, true positive rate of 75.90% and true negative rate of98.78%. This achievement was obtained by cutting down theintra-class variance of background, reducing the number of classesand enlarging the size of the database. The class of background in the5C-database was mainly composed of green leaves without ﬂowers and dry plant residuals so to simplify the classi ﬁcation. In real
Fig. 2.Typical errors when testing the M5C model. The blue windows marked with “Clear”are correctly classiﬁed as background. The red windows with class name are correctly classi ﬁed as the corresponding pests. The misclassi ﬁed windows are highlighted with yellow colour. The numbers in the square brackets are the values of con ﬁdence of classiﬁcation.H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
18agricultural applications, although detecting a large scale of pest specieson multiple host plants at different growing stages is attractive, a focuson detecting several pest species on a certain type of host plants is stillvaluable for pest management. Thus, simplifying the classi ﬁcation task to detect four types of pests on green leaves is reasonable and meetsthe requirements of most pest management tasks. The virtual imagesplayed a critical role in the high accuracy. By creating the virtual images,a large database with enough variance was created to represent the truescenario of the farming environments, at the same time, the cost and la-bour were signiﬁcantly reduced. The true positive rate of M5C was rel-atively low. In the test images, the numbers of windows havingbackground and pests were unbalanced, e.g., only a few windows hadpests in a testing image. Thus, even a small number of pestsmisclassiﬁed as background could cause a signiﬁcant drop in the true positive rate. However, for pest management, if a part of a pest is de-tected then the pest is considered as detected. Thus 75.90% true positiverate is acceptable for pest management applications.Although the 5C-database included a large number of green wormswithout notable textural or colour features, the M5C had a limitation todetect green worms because they are well camou ﬂaged in the green leaves. Green worms were often classiﬁed as background while stems or thin artiﬁcial materials were frequently classiﬁed as worm (Fig. 2). Using ultraviolet or near infrared images could improve the detectionof some camouﬂaged invertebrates on crops (Liu et al. 2016a;Liu et al. 2017a;Liu et al. 2017b;Liu et al. 2017c;Chahl and Liu 2018). Hyperspectral images having the capability to reveal the biologicaland physiological characters of plants ( Bruning et al. 2019;Bruning et al. 2020;Liu et al. 2020a, 2020b) have a potential to improve pestdetection.4.4. Feasibility of automationWe found that the wide-angle cameras of smartphones working at20 cm to 50 cm can capture high-quality images for pest detection.This imaging approach can be automated by using a ground-based plat-form with a robotic arm equipped with a low-cost camera. The process-ing speed was tested on a computer with a 4.2G Hz CPU and the averagetime for processing a window was 0.21 s. By using parallel processing,this processing rate can support real-time pest detection in the ﬁeld.5. ConclusionIn precision agriculture, detection of invertebrate pests at anearlier stage on crops in the natural farming environments is a nec-essary prerequisite of IPM, however, this task is challenging andcorresponding methods have not been well developed. We devel-oped a computer vision method which can achieve proximal detec-tion of invertebrate pests on crops. We found that ResNetsoutperformed AlexNet and VGG for pest classi ﬁcation and data ar- gument and transfer learning can signi ﬁcantly improve classiﬁca- tion accuracy and speed up convergence. We proposed a novelmethod to generate a virtual database which not only enabled train-ing a model with high accuracy but also signiﬁcantly saved the cost and time for collecting training data. Using three-dimensionalmodelling and gaming technologies to generate large-size trainingdata would boost the applications o fm a c h i n el e a r n i n g .I nt h ef u - ture, using a large virtual database including millions of imagesand tens of classes to train a model with high accuracy for pest de-tection will be further investigated. Using multispectral or
hyperspectral images to detect well-camou ﬂaged pests would be another research direction. Developing a ground-based robotic sys-tem to conduct proximal detection of pests on crops in real-time forpest management will be considered.Declaration of Competing InterestThis research is supported by Australian Plant Phenomics Facil-ity, The Plant Accelerator and School of Engineering, University ofSouth Australia. No potential conﬂict of interest was reported by the authors.AcknowledgementThe authors would like to thank The Australian Plant Phenomics Fa-cility funded by the Australian Government under the National Collabo-rative Infrastructure Strategy (NCRIS), School of Engineering, Universityof South Australia and Joint and Operations Analysis Division, DefenceScience and Technology Group, Australia.Appendix AH. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
19Fig. A1 Example images of 10C-database. The ﬁrst column shows the class names of background (BK), aphid (AP), portuguese millipede (PM), earwig (EA), vineyard snail and white garden snail (SN), slug (SL), honeybee (HB), locust (LO), orange striped oakworm (OO), redlegged earthmites (RE).H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
20Fig. A2 Example images of virtual 5C-database. The ﬁrst column shows the class names of background (BK), vineyard snail and white garden snail (SN), slug (SL), earwig (EA), worm (WO).
Fig. A3 ResNet152_v2 trained from scratch. The numbers in the ﬁrst row and column indicate classes (refer to Fig. A1).H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
21Fig. A4 ResNet50_v2 trained from scratch using data argument. The numbers in the ﬁrst row and column indicate classes (refer to Fig. A1).
Fig. A5 ResNet152_v2 trained using transfer learning. The numbers in the ﬁrst row and column indicate classes (refer to Fig. A1).
Fig. A6 ResNet152_V2 trained using both transfer learning and data argument. The numbers in the ﬁrst row and column indicate classes (refer to Fig. A1).H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
22References
Baker, G., Jennings, R., 2015. Growers Chase Pest-Control Answers. Grains Research andDevelopment Corporation. Adelaide, Australia https://grdc.com.au/Media-Centre/ Ground-Cover/Ground-Cover-Issue-117-July-August-2015/Growers-chase-pest-con-trol-answersLast accessed 8 Aug. 2016.Barbedo, J., 2020. Detecting and classifying pests in crops using proximal images and ma-chine learning: a review. Atiﬁcial Intelligen. 1, 312–328.https://doi.org/10.3390/ ai1020021.Boissarda, P., Martinb, V., Moisanb, S., 2008. A cognitive vision approach to early pest de- tection in greenhouse crops. Comput. Electron. Agric. 62, 81 –93. Bruning, B., Liu, H., Brien, C., Berger, B., Lewis, M., Garnett, T., 2019. The development ofhyperspectral distribution maps to predict the content and distribution of nitrogenand water in wheat (Triticum aestivum). Front. Plant Sci. 10. https://doi.org/ 10.3389/fpls.2019.01380.Bruning, B., Berger, B., Lewis, M., Liu, H., Garnett, T., 2020. Approaches, applications, andfuture directions for hyperspectral vegetation studies: an emphasis on yield-limiting factors in wheat. Plant Phenome J. https://doi.org/10.1002/ppj2.20007 . Chahl, J., Liu, H., 2018. Bioinspired invertebrate pest detection on standing crops. SPIE,Bioinspiration, Biomimetics, and Bioreplication VIII, Denver, Colorado, United States.SPIE, Colorado, United States, p. 105930B https://doi.org/10.1117/12.2296580 . Csurka, G., Dance, C.R., Fan, L., Willamowski, J., Bray, C., 2004. Visual categorization with bags of keypoints. ECCV International Workshop on Statistical Learning in ComputerVision, Prague, Czech Republic. CiteSeer, pp. 1 –22 doi:10.1.1.72.604. Fu, G., Liu, C., Zhou, R., Sun, T., Zhang, Q., 2017. Classi ﬁcation for high resolution remote sensing imagery using a fully convolutional network. Remote Sens. 9 (5), 498.https://doi.org/10.3390/rs9050498 . GRDC, 2014. Slugging slugs. Grains Research and Development Corporation, Barton, Can-berra, Australiahttp://grdc.com.au/Media-Centre/Hot-Topics/Slugging-slugs Last accessed 18, July 2016.Han, R., He, Y., 2013.Remote automatic identiﬁcation system ofﬁeld pests based on com- puter vision. Trans. Chin. Soc. Agric. Eng. 29 (3), 156 –162. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. IEEEConference on Computer Vision and Pattern Recognition (CVPR). IEEE https://doi. org/10.1109/CVPR.2016.90. Hu, F., Xia, G., Hu, J., Zhang, L., 2015. Transferring deep convolutional neural networks forthe scene classiﬁcation of high-resolution remote sensing imagery. Remote Sens. 7(11), 14680–14707.https://doi.org/10.3390/rs71114680 . Huang, H., Deng, J., Lan, Y., Yang, A., Deng, X., Zhang, L., 2018. A fully convolutional net-work for weed mapping of unmanned aerial vehicle (UAV) imagery. PLoS One 13(4), e0196302.https://doi.org/10.1371/journal.pone.0196302 . InsectImages, 2018. Insect Images. https://www.insectimages.org Last accessed Dec 1 2018.Kamilaris, A., Prenafeta-Boldú, F., 2018. Deep learning in agriculture: a survey. Comput.Electron. Agric. 147, 70–90.https://doi.org/10.1016/j.compag.2018.02.016 . Kogan, M., Hilton, R.J., 2009. Conceptual framework for integrated pest management(IPM) of tree-fruit pests. Biorational Tree-fruit Pest Management. vol. 1. Centre forAgriculture and Biosciences International, Oxfordshire, UK. https://doi.org/10.1079/ 9781845934842.0001ISBN:9781845934842.Krizhevsky, A., 2014. One weird trick for parallelizing convolutional neural networks.arXiv 1404.5997.https://arxiv.org/abs/1404.5997v2 arXiv:1404.5997v2. Landwehr, N., Hall, M., Frank, E., 2005. Logistic model trees. Mach. Learn. 59 (1
–2), 161–205.Larios, N., Deng, H., Zhang, W., Sarpola, M., 2008. Automated insect identiﬁcation through concatenated histograms of local appearance features: feature vector generation andregion detection for deformable objects. Mach. Vis. Appl. 19, 105 –123. Liakos, K., Busato, P., Moshou, D., Pearson, S., Bochtis, D., 2018. Machine learning in agri-culture: a review. Sensors (Basel, Switzerland) 18 (8), 2674. https://doi.org/10.3390/ s18082674.Liu, H., Chahl, J., 2018. A multispectral machine vision system for invertebrate detectionon green leaves. Comput. Elecron. Agric. 150, 279 –288.https://doi.org/10.1016/j. compag.2018.05.002.Liu, H., Lee, S.H., Chah, J.S., 2016a. An evaluation of the contribution of ultraviolet in fusedmultispectral images for invertebrate detection on green leaves. Precis. Agric. 17 (4).https://doi.org/10.1007/s11119-016-9472-7 .Liu, H., Lee, S.H., Chahl, J.S., 2016b. A review of recent sensing technologies to detect inver-tebrates on crops. Precis. Agric. 17 (4). https://doi.org/10.1007/s11119-016-9473-6 . Liu, H., Lee, S.H., Chahl, J.S., 2017a. A multispectral 3D vision system for invertebrate de-tection on crops. IEEE Sens., 1 –14https://doi.org/10.1109/JSEN.2017.2757049 . Liu, H., Lee, S.H., Chahl, J.S., 2017b. Registration of multispectral 3D points for plant inspec-tion. Precis. Agric.https://doi.org/10.1007/s11119-017-9536-3 . Liu, H., Lee, S.H., Chahl, J.S., 2017c. Transformation of a high-dimensional color space formaterial classiﬁcation. J. Opt. Soc. Am. A 34 (4), 523 –532.https://doi.org/10.1364/ josaa.34.000523.Liu, H., Bruning, B., Garnett, T., Berger, B., 2020a. Hyperspectral imaging and 3D technol-ogies for plant phenotyping: from satellite to close-range sensing. Comput. Electron.Agric. 175.https://doi.org/10.1016/j.compag.2020.105621 . Liu, H., Bruning, B., Garnett, T., Berger, B., 2020b. The performances of hyperspectral sen-sors for proximal sensing of nitrogen levels in wheat. Sensors 20 (16), 4550. https:// doi.org/10.3390/s20164550. Lowe, D., 2004.Distinctive image features from scale-invariant keypoints. Int. J. Comput.Vis. 60, 91–110.Maggiori, E., Tarabalka, Y., Charpiat, G., Alliez, P., 2017. Convolutional neural networks forlarge-scale remote-sensing image classi ﬁcation. IEEE Trans. Geosci. Remote Sens. 55 (2), 645–657.https://doi.org/10.1109/TGRS.2016.2612821 . Martineau, M., Conte, D., Raveaux, R., Arnault, I., Munier, D., Venturini, G., 2017. A surveyon image-based insect classiﬁcation. Pattern Recogn. 65 (C), 273 –284.https://doi. org/10.1016/j.patcog.2016.12.020 . Miles, M., 2015. Insect Pest Management in Faba Beans. Grains Research and Develop-ment Corporation. QLD, Australia https://grdc.com.au/Media-Centre/Ground-Cover/ Ground-Cover-Issue-117-July-August-2015/Insect-pest-management-in-faba-beansLast accessed 7 Oct 2015.Murray, D., Clarke, M., Ronning, D., 2013. Estimating invertebrate pest losses in six majorAustralian grain crops. Aust. J. Entomol. 52 (3), 227 –241.https://doi.org/10.1111/ aen.12017.MXNet, 2018. A Flexible and Efﬁcient Library for Deep Learning MXNet Scienti ﬁc Commu- nity.https://mxnet.apache.org/versions/1.7.0/ Last accessed August 2019. Nalam, V., Louis, J., Shah, J., 2019. Plant defense against aphids, the pest extraordinaire.Plant Sci. (Limerick) 279, 96 –107.https://doi.org/10.1016/j.plantsci.2018.04.027 . Oerke, E., 2006. Crop losses to pests. J. Agric. Sci. 144 (1), 31 –43.https://doi.org/10.1017/ S0021859605005708.Ren, S., He, K., Ross, G., Sun, J., 2017. Faster R-CNN: towards real-time object detectionwith region proposal networks. IEEE Trans. Pattern Anal. Mach. Intell. 39 (6),1137–1149.https://doi.org/10.1109/TPAMI.2016.2577031 . Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scale image recognition. arXiv 1409.1556 arXiv:1409.1556.StandfordVisionLab, 2018. ImageNet. http://www.image-net.orgLast accessed Jan 1 2019. Sun, Y., Cheng, H., Cheng, Q., Zhou, H., Li, M., Fan, Y., et al., 2017. A smart-vision algorithmfor counting whiteﬂies and thrips on sticky traps using two-dimensional Fouriertransform spectrum. Biosyst. Eng. 153, 82 –88.https://doi.org/10.1016/j. biosystemseng.2016.11.001. Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., et al., 2015. Going deeperwith convolutions. Proceedings of the IEEE Computer Society Conference on Com-puter Vision and Pattern Recognition, Boston, MA, USA. IEEE, pp. 1 –9https://doi. org/10.1109/CVPR.2015.7298594 ISBN:10636919. Xia, D., Chen, P., Wang, B., Zhang, J., Xie, C., 2018. Insect detection and classi ﬁcation based on an improved convolutional neural network. Sensors 18 (12), 4169. https://doi.org/ 10.3390/s18124169.Yen, A., Madge, D., Berry, N., Yen, J., 2013. Evaluating the effectiveness of ﬁve sampling methods for detection of the tomato potato psyllid, Bactericera cockerelli ( Šulc) (Hemiptera: Psylloidea: Triozidae). Aust. J. Entomol. 52 (2), 168 –174.https://doi. org/10.1111/aen.12006.Zhu, L., Ma, M., Zhang, Z., Zhang, P., Wu, W., Wang, D., et al., 2017. Hybrid deep learningfor automated lepidopteran insect image classi ﬁcation. Orient. Insects 51 (2), 79 –91. https://doi.org/10.1080/00305316.2016.1252805 .H. Liu and J.S. Chahl Artiﬁcial Intelligence in Agriculture 5 (2021) 13 –23
23