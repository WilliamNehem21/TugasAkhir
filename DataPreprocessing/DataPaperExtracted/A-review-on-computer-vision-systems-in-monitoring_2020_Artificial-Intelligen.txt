A review on computer vision systems in monitoring of poultry: Awelfare perspective
Cedric Okindaa, Innocent Nyalalaa, Tchalla Korohoua, Celestine Okindab,J i n t a oW a n ga, Tracy Achiengc, Patrick Wamalwa
d, Tai Manga, Mingxia Shena,⁎
aCollege of Artiﬁcial Intelligence, Laboratory of Modern Facility Agriculture Technology and Equipment Engineering of Jiangsu Province, Nanjing Agricultural Uni versity, Jiangsu 210031, PR China
bCollege of Veterinary Science and Agriculture, University of Nairobi, Lower Kabete, Nairobi, Kenya
cFaculty of Bioscience and Engineering, Ghent University, Ghent, Belgium
dFaculty of Engineering, Department of Agricultural Engineering, Egerton University, Njoro, Kenya
abstract article info
Article history:Received 7 August 2020Received in revised form 6 September 2020Accepted 6 September 2020Available online 9 September 2020
Keywords:Computer visionDeep learningMachine learningMonitoringPoultryWelfareMonitoring of poultry welfare-related bio-processes and bio-responses is vital in welfare assessment and man-agement of welfare-related factors. With the current development in information technologies, computer visionhas become a promising tool in the real-time automation of poultry monitoring systems due to its non-intrusiveand non-invasive properties, and its ability to present a wide range of information. Hence, it can be applied tomonitor several bio-processes and bio-responses. This review summarizes the current advances in poultry mon-itoring techniques based on computer vision systems, i.e., conventional machine learning-based and deeplearning-based systems. A detailed presentation on the machine learning-based system was presented,i.e., pre-processing, segmentation, feature extraction, feature selection, and dimension reduction, and modeling.Similarly, deep learning approaches in poultry monitoring were also presented. Lastly, the challenges and possi-ble solutions presented by researches in poultry monitoring, such as variable illumination conditions, occlusionproblems, and lack of augmented and labeled poultry datasets, were discussed.© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Contents1 . I n t r o d u c t i o n ............................................................... 1 8 51 . 1 . P r e c i s i o n l i v e s t o c k f a r m i n g a n d a n i m a l w e l f a r e ............................................ 1 8 52 . M a c h i n e l e a r n i n g - b a s e d p o u l t r y m o n i t o r i n g s y s t e m s............................................ 1 8 62 . 1 . I m a g e p r e - p r o c e s s i n g....................................................... 1 8 82 . 2 . R e g i o n o f i n t e r e s t s e g m e n t a t i o n .................................................. 1 8 82 . 2 . 1 . F e a t u r e s f o r s e g m e n t a t i o n a n d a p p r o a c h e s ......................................... 1 8 82 . 2 . 2 . R e g i o n o f i n t e r e s t v a l i d a t i o n ................................................ 1 8 92 . 3 . F e a t u r e e x t r a c t i o n ......................................................... 1 9 02 . 3 . 1 . M o r p h o l o g i c a l f e a t u r e s.................................................. 1 9 02 . 3 . 2 . L o c o m o t o r f e a t u r e s .................................................... 1 9 12.3.3. Opticalﬂo w m e a s u r e s ................................................... 1 9 12 . 3 . 4 . O t h e r f e a t u r e s...................................................... 1 9 22 . 4 . F e a t u r e s e l e c t i o n a n d d i m e n s i o n r e d u c t i o n .............................................. 1 9 32 . 5 . S t a t i s t i c a l a n a l y s i s ......................................................... 1 9 32 . 6 . M o d e l i n g t e c h n i q u e s ....................................................... 1 9 33 . D e e p l e a r n i n g - b a s e d p o u l t r y m o n i t o r i n g s y s t e m s .............................................. 1 9 73 . 1 . D e e p l e a r n i n g c a t e g o r i e s ...................................................... 1 9 73 . 1 . 1 . C o n v o l u t i o n a l N e u r a l N e t w o r k s ( C N N s ) ........................................... 1 9 83 . 1 . 2 . R e c u r r e n t a n d r e c u r s i v e n e u r a l n e t w o r k s.......................................... 1 9 9Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
⁎Corresponding author.E-mail address:mingxia@njau.edu.cn(M. Shen).
https://doi.org/10.1016/j.aiia.2020.09.0022589-7217/© 2020 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the C CB Y - N C - N Dl i c e n s e (http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/3 . 1 . 3 . P r e t r a i n e d U n s u p e r v i s e d N e t w o r k s ( P U N s )......................................... 1 9 93 . 2 . I m a g e p r e - p r o c e s s i n g....................................................... 2 0 03 . 3 . D a t a a u g m e n t a t i o n........................................................ 2 0 03 . 4 . D e e p l e a r n i n g a p p l i c a t i o n s ..................................................... 2 0 04 . C h a l l e n g e s a n d f u t u r e d i r e c t i o n ...................................................... 2 0 14 . 1 . L i v e w e i g h t e s t i m a t i o n s y s t e m s ................................................... 2 0 14 . 2 . L a m e n e s s d e t e c t i o n s y s t e m s .................................................... 2 0 24.3. Health status classiﬁc a t i o n s y s t e m s ................................................. 2 0 34 . 4 . P o u l t r y t r a c k i n g s y s t e m s ...................................................... 2 0 34 . 5 . B e h a v i o r m o n i t o r i n g s y s t e m s .................................................... 2 0 44 . 6 . A c t i v i t i e s a n d o t h e r m o n i t o r i n g s y s t e m s ............................................... 2 0 45 . C o n c l u s i o n s ............................................................... 2 0 5D e c l a r a t i o n o f c o m p e t i n g i n t e r e s t ........................................................ 2 0 5A c k n o w l e d g m e n t s.............................................................. 2 0 5R e f e r e n c e s .................................................................. 2 0 5
1. IntroductionFood security concerns have mandated an increase in agriculturalproduction due to the ever-growing world population with a projec-tion of over 9.6 billion people by the year 2050 ( Gerland et al., 2014). However, environmental production constraints have resulted in adecline in the global per-capita cereal production since the early1980s (Dyson, 1999). Additionally, there is an increasing preferencefor animal-based food proteins (FAO, 2018). Therefore, future global meat consumption is deemed to increase by 70% by 2050(Berckmans, 2017). This increasing demand has resulted in intensiveand extensive animal production. Currently, over 3535 million ani-mals are reared globally under extensive and intensive productionsystems, with a total annual production of 798 and 3029 milliontons of milk and meat, respectively ( Pulido et al., 2018;Wang et al., 2019b). A report byHenchion et al. (2014)on the trends of meat con- sumption indicated an increase in consumption of poultry meat andpoultry meat products, with a projected increase within the next de-cade due to preferences of white meat with chicken being the favor-ite (OECD-FAO, 2017;Okinda et al., 2019). With the intensiﬁcation of chicken production and the growing awareness of acceptable animalwelfare conditions, animal health, efﬁciency, and sustainable envi- ronmental conditions have become challenging factors to ful ﬁll (Berckmans, 2014). Hence, human surveillance has ceased to be a vi-able solution in livestock monitoring ( Okinda et al., 2019). Precision Livestock Farming (PLF) has been used as a solution to these chal-lenges by providing efﬁcient automated systems while at the sametime maintaining animal welfare (Lehr, 2014). PLF acts as a support system to the stockmen to monitor various bio-processes and bio-responses related to animal welf are, health, and productivity (Banhazi et al., 2012;Berckmans, 2017;Wathes et al., 2008). Vision-based PLF systems have become a rich research topic dueto the current development in technology and the advantages ofcomputer vision systems in animal monitoring. Computer vision sys-tems can provide non-intrusive, non-invasive, consistent, effective,and objective supervision. It provides an allowance for data record-ing for future usage and analysis. Additionally, computer vision re-duces tedious and labor-intensive processes. Furthermore, itprovides a robust sensing technology that can be used to monitor nu-merous aspects of the farm. Computer vision is the process of apply-ing mathematics, computer science, and software programming toprovide image-based automated process control. There are two cat-egories of computer vision-based systems, i.e., machine learning-based systems and deep learning-based systems. The former followsa typical image processing procedure (image acquisition, pre-processing, region of interest (ROI) extraction, feature extraction, and classiﬁcation or regression). While the later perform classi ﬁca- tion or regression from an object recognition point of view basedon Deep Neural Networks (DNN). General object recognition tasksin computer vision include image identi ﬁcation, object detection,image classiﬁcation, semantic segmentation, and speci ﬁc object rec- ognition (
Fujiyoshi et al., 2019).The image identiﬁcation problem involves verifying if an object in animage has the same pattern as a reference object. The veri ﬁcation is based on the difference (threshold distance) between the featurevectors of the reference pattern and the input image. Object detectionproblem involvesﬁnding the location of an object of a speciﬁcc a t e g o r y in an image. It can be a single category or a multi-class object detectionproblem. In deep learning (DL), multi-class object detection can be per-formed by a single network as opposed to a conventional machinelearning approach. The image classiﬁcation problem involvesﬁnding the category(s) in which an object(s) in an image belongs. The semanticsegmentation problem involves understanding the scene structure of animage, i.e.,ﬁnding pixel-wise object categories. This problem is chal-lenging using classical machine learning techniques but can be over-come by the application of DL algorithms. Speci ﬁc object recognition is a subtask of the general object detection problem. It involves the detec-tion of speciﬁcally deﬁned objects in an image by detecting featurepoints using scale-invariant feature transform (SIFT) ( Lowe, 2004)o r learned invariant feature transform (LIFT) ( LeCun et al., 1999). An over- view of computer vision applications in poultry monitoring is presentedinFig. 1.The main components of a computer vision system are the camerasensor, image processing board, software, and hardware. The camerasensor converts photons to electrical signals. In chicken monitoring sys-tems, visual light-based (charge-coupled devices (CCD) and comple-mentary metal-oxide-semiconductor (CMOS)), thermal and infrared(IR) depth-based sensors have been applied to acquire chicken imagesin different farm environments. The image processing board is alsoknown as the digitizer, converts the visual image into numerical form(pixels). The software is the underlying image analysis code that per-forms image manipulations to achieve the desired output. Different pro-cessing algorithms have been developed and applied to the acquiredimages to perform the objective tasks based on a speci ﬁc programming framework such as Matlab, ImageJ, and OpenCV, to mention a few. Thehardware refers to all the connected components that make up thecomputer vision system, i.e., a camera sensor, connecting cables, com-puters, etc.Despite the numerous advantages of computer vision systems, theperformance of any vision systems in the monitoring of animals isgreatly affected by the variation of ambient light conditions in thefarm environment, color contrast between background and foreground,and occlusion problems. Nevertheless, several studies have been carriedout to overcome these challenges.1.1. Precision livestock farming and animal welfareAnimal welfare is a complex, dynamic, multifaceted policy issuewith economic, scientiﬁc, ethical, and political dimensions that needto be addressed objectively in a scienti ﬁcally credible manner. TheC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
185term animal welfare has been deﬁned by several studies in differentways, depending on criteria and assessment. According to Bessei (2018), animal welfare ranges from a total perfect condition to extremesuffering. Characterized by diseases, physical damages (wounds andbone breakage), behavioral (displacement preening, stereotyped be-havior, feather pecking and cannibalism, aggression, and fear), physio-logical (stress), ethological, psychological, and positive feeling(dustbathing, preening, stretching and play) criteria. According toDawkins (2017), good animal welfare is deﬁned by good health (water, food, and lack of injury) and the animal having its needs andwants to be fulﬁlled at all times. Additionally, 12 criteria have been in-troduced as standards of good poultry welfare conditions ( Welfare- Quality®, 2009). These criteria include thermal comfort, absence of dis-ease, absence of prolonged hunger, absence of injuries, absence ofprolonged thirst, ease of movement, absence of pain due to manage-ment procedures, presence of comfort around resting, expression ofother natural behavior, good human-animal relations, expression of so-cial behavior, and positive emotional state. Generally, these welfarecriteria can be summarized as “animal health and animal want ” (Dawkins, 2017).Animal welfare is an emotive issue, but the question is, what is theimportance of animal welfare? In a general sense, good animal welfarebeneﬁts the stockman, the animal in question, and the consumer ofthe animal products. Without animal welfare, there would be no high-quality meat, eggs, or milk. In poultry production, if the birds arestressed, abused, and mistreated, the egg production by the layers willdecline (Alm et al., 2016). Similarly, if broilers and other animals keptfor meat are poorly handled and poor slaughtering practice, the meatwill be of poor quality or contaminated ( Faucitano, 2018; Shimokomaki et al., 2017). The beneﬁts of animal welfare can be pre- sented in terms of improved animal health ( Dawkins, 2017;Green et al., 2012;Salois and Baker, 2018), improved animal product quality(Dawkins, 2017;Llonch et al., 2015), reduced animal mortality rate(Dawkins, 2017;Salois and Baker, 2018), reduced risks of zoonotic dis- eases (Dawkins, 2017;Okinda et al., 2019), improved disease resistance (Dawkins, 2017;Hoerr, 2010), and farmers satisfaction (Hemsworth et al., 2015;Hemsworth and Coleman, 2010). However, with a good an- imal welfare practice, the prices of animal products have been seen torise, and some consumers aren't willing to pay for high welfare practices(Healy, 2018;Heise and Theuvsen, 2018).However, there are several hindrances in animal welfare evalua-tion, such as; the difﬁculty in measuring physiological and ethologi-cal responses in real-farm animal husbandry conditions (Alm et al.,
2016;Bessei, 2018), the difﬁculty in distinguishing between normal,abnormal, and disturbed behaviors ( Bessei, 2018). Furthermore, from the studies ofHughes et al. (2018)based on human and animal neural responses, under constant conditions, then the welfare situa-tion won't be considered as well-being even in good conditions andmanagement if the psychological balance isn't offset. Finally, animalsare complex individual and time-variant (CIT) systems that are indi-vidually different and respond differently at different moments.Thus, they can't be analyzed as a typical classical steady-state system(Berckmans, 2006). Moreover, welfare condition indicators can becontradicting, and according toAlm et al. (2016), presently, there is no consensus on the ideal technique to access animal welfare.PLF itself hasn't lived up to its expectation. Lehr (2014)presented the main obstacles to the implementation of PLF as, lack of consistentmarketing, lack of direct cooperation between farmers, biologists,engineers, and economists, little focus on data interpretation andcontrol, the technological gap between consumers and modernfarming, and lack of awareness, animal complexities, lack of robust-ness of developed techniques and technologies, and the dif ﬁculties and reliability of PLF systems in commercial farms. Despite the men-tioned hindrances on assessing animal welfare and adoption of PLF,studies have reported on several PLF techniques in the monitoringof various bio-responses and bio-processes aiming at improving ef ﬁ- ciency, animal health, welfare, and farm economy in large-scalechicken production.Several research efforts have been reported in literature on thedevelopment of computer vision systems for chicken monitoring ofwelfare-related issues such as weight, lameness, behaviors, temper-ature, activities, and health (Aydin, 2017a, 2017b;Mortensen et al., 2016;Okinda et al., 2019;Wang et al., 2019b;Zhuang et al., 2018; Zhuang and Zhang, 2019). This review aims at providing a propersynthesis of literature to provide clear guidance on the state-of-the-art techniques and the potential future direction on the monitor-ing of welfare-related bio-processes and bio-responses in chickenproduction. Therefore, this work will focus on up-to-date researchadvances to provide useful technical information for the develop-ment of more relevant and reliable computer vision techniques forthe monitoring of welfare-related bio-processes in chickenproduction.2. Machine learning-based poultry monitoring systemsAs already mentioned, the machine learning-based system follows atypical image analysis procedure with the application of conventionalmachine learning algorithms, as shown in Fig. 2.
Fig. 1.An overview of the application of computer vision in poultry monitoring.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
186Fig. 2.The general work
ﬂow of machine learning-based chicken monitoring systems.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
1872.1. Image pre-processingImage pre-processing operations are vital to obtaining a robust andefﬁcient ROI segmentation performance. The pre-processing operationsinvolve the following: resizing, color-space transformation, contrast en-hancement, normalization, and denoising. RGBis the widely applied color space, but due to the high correlation between R,G, andBcolor spaces, it's not suitable for object segmentation ( Cheng et al., 2001). Hence, several transformation techniques have been explored by sev-eral studies aiming at achieving accurate image object segmentation.The commonly used color space models are RGB(red, green, and blue),LAB(Lfor brightness,Afor values from red to green opponentcolors along A-axis, andBfor values from blue to yellow opponentcolors along B-axis),HIS(hue, saturation, and intensity),HSV(hue, sat- uration, value),YCrCb(Yfor luminance component,CbandCrfor blue- difference and red-difference chroma components, respectively) justto mention a few (Ibraheem et al., 2012). All these color spaces are com- puted fromRGB, as presented inTable 1.As mentioned above, color transformation operation is always per-formed to obtain better ROI segmentation results. Zhuang et al. (2018) applied theHSVandLAB(CIEL⁎a⁎b⁎) color spaces to extract poultryimage color features during background removal. The HSVcolor space is robust to variation in illumination and more aligned to human colorperception (Hamuda et al., 2017), while theLABcolor space is invariant to sensor sensitivity (Ireri et al., 2019). Based on the presentation by Zhuang et al. (2018),t h eSandVcolor spaces are not conducive forchicken segmentation because the resultant image intensities werewidely distributed and divergent. However, Hcolor space produced a clear visible broiler body segmentation, but the segmentation accuracywas somehow lower when compared to the a-bmap. Therefore, Zhuang et al. (2018)applied thea-b
map to describe the color space whilel-aas an auxiliary description in broiler body segmentation. To in-tensify the contrast of colored images, Pereira et al. (2013)applied the HIScolor space to contrast the background from the foreground. In HIS space,Hrepresents the speciﬁc color,Srepresents how saturated a color is in comparison to white, and Irepresent the brightness of the color. Additionally,Guo et al. (2020)compared the visualization effectofL⁎a⁎b⁎andRGB(RG,RB, andGB) and established that theGBspace had higher classiﬁcation and visualization efﬁciency. Most studies in chicken monitoring using visible light-based sensors are performed inRBGcolor space because of the numerous advantages of RGBcolormodel i.e., its suitable for color display, easy to use and it's an intuitivemodel for color creation and manipulation ( Chavolla et al., 2018). The image resizing operation is performed to minimize the compu-tation cost and complexities by reducing image resolution size. Imagecropping can also be implemented as an image resizing operation andROI extraction as performed byMehdizadeh et al. (2015)in the extrac- tion of the chicken head in beak and head motion analysis. Color imagescaptured under varying or insufﬁcient illumination have poor contrastand noise, which affects the performance of the subsequent image pro-cessing techniques. Contrast improvements can be performed duringthe image acquisition phase or as a pre-processing operation. A con-trasting background (darkﬂoor for white birds) can be manuallyinstalled to obtain a clear outline of the birds ( Amraei et al., 2018, 2017b, 2017a;Mollah et al., 2010;Pereira et al., 2013). The pre- processing contrast improvement technique is by grayscale pixel inten-sity enhancement by changing the grey-level between the range of 0 to255. Additionally, global histogram equalization has been used to en-hance contrast and alleviate light variations by normalization of imagehistograms.Pereira et al. (2013)appliedR,G,B,S,a n dIspace pixels nor- malization before a deduction of the inverse of randbmathematical op- erations on pixel matrices. However, in the tracking of birds,normalization operation is always performed to eliminate errors,i.e., noise occurring due to the difference in sizes of birds ( Aydin et al., 2010). Generally, normalization is performed to scale the data to a rea-sonable extent and to turn the images into normalized non-dimensionaldata (Wang et al., 2019a).2.2. Region of interest segmentationImage object segmentation can be referred to as a process of formingconnected objects with relatively homogenous properties by groupingrelated pixels together or partitioning an image into multiple segmentswith similar attributes (objects) (Ladický et al., 2009;Wang et al., 2019a). The main aim of the segmentation process is to transform animage to be more meaningful and easier to analyze and interpret ( Pal and Pal, 1993). The meaningful segments, also known as ROI, is the ini-tial step in transforming a color or a grayscale image from a low-levelimage processing task to a high-level image description task. Therefore,it is one of the most critical tasks in object detection by image process-ing. The success of image processing and analysis depends on the ef ﬁ- ciency and reliability of the segmentation process. However, theaccurate partitioning of an image is quite challenging and a lot of studieshave been performed to achieve efﬁcient and robust ROI extraction.2.2.1. Features for segmentation and approachesEfﬁcient discriminating features are fundamental in separating thebackground from the birds. For visual light-based sensors under con-stant light conditions, color is the most invariant feature against objecttranslation, rotation, and partial occlusion ( Wang et al., 2019a). Zaninelli et al. (2018)applied this approach in the processing of ther-mographic images to perform background color thresholds. However,for the IR depth-based sensors, since the pixel intensities are distance(depth) values, distance threshold is often performed to remove thebackground (Jana, 2012;Okinda et al., 2019). Additionally, the sensor being invariant to variation in ambient light conditions, the depth infor-mation can be applied to assist in the segmentation process in 2D sys-tems (Okinda et al., 2019). Generally, depth images have a less odioustask of background removal compared to color images. This can be ob-served in the study byMortensen et al. (2016), where multiple broilers could be detected in a depth image, based on a height function de ﬁned over a depth image based on the watershed segmentation method. Sim-ilarly, in the study byOkinda et al. (2019), the background was removed by a simple depth threshold and image subtraction.The main segmentation approaches can be grouped into three tech-niques: background subtraction, foreground detection, and learning-based techniques. Theﬁrst two techniques are a two-stage process
Table 1Color space transformation from RGB.Adopted fromWang et al. (2019a).ColormodelChannel Color space transformation from RGBXYZ X X= 0.607R+ 0.174G+ 0.200B YY= 0.299R+ 0.587G+ 0.114B ZZ= 0.066R+ 1.11B L
∗a∗b∗L∗
L∗¼116Y1=3if Y>k903:3Yi f Y≤k(k¼0:008856a
∗a∗= 500[f(X)−f(Y)] whereftðÞ¼ti f t>k7:787tþ0:1379if t≤k/C26b
∗b∗= 200[f(Y)−f(Z)] HSV HH¼pG−BðÞif M¼R120þpB−RðÞif M¼G240þpR−GðÞif M¼B8><>:whereM=max{R,G,B},m=min{R,G,B},p¼
60mM
SS¼M−mðÞM
VV=MHSL LL¼
MþmðÞ2
SS¼M−mðÞ;min;Mþm,;2−M−mfg
YCrCb Cr Cr= 0.713(R−Y) + 128 Cb Cb= 0.564(B−Y) + 128C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
188involving foreground detection and region validation, while the latter isa model development technique. Threshold-based and background sub-traction techniques are the most widely used technique for foregrounddetection in chicken monitoring systems ( Amraei et al., 2018, 2017a, 2017b;Aydin, 2017b;Aydin et al., 2015, 2013;De Wet et al., 2003; Leroy et al., 2006;Mollah et al., 2010;Sergeant et al., 1998;Zaninelli et al., 2018).Table 2presents an overview of the segmentation tech-niques applied in chicken monitoring systems.In the image background subtraction technique, the conventionalapproach is to obtain a background image depicting a scene without ob-jects of interest, and then to perform a frame-by-frame subtractionunder the condition that the camera is static. The pixels for which thedifference is above a set threshold are labeled as belonging to the ROI(animal). The background image can be manually captured or via an au-tomatically updating video frames, i.e., continuous frame averaging,loopy belief propagation, Gaussian mixture models, and frame copy out-side foreground neighborhoods (Li et al., 2019a). However,Sergeant et al. (1998)explained the shortcomings of the application of back-ground subtraction in chicken tracking as; high density of birds in apen may exceed the 1:1 ratio of moving objects to background. If thebackground image is obtained by automatically updating continuousframes, only moving birds can be identiﬁed. Thus, stationary birds will be detected as the background. Additionally, poor contract betweenbackground and foreground is a hindrance to background subtraction.The adaptive threshold technique based on Otsu (1979)is the most classical technique based on global intensity histograms (equalized) ofan image to determine the threshold value. Otsu's method establishesa threshold that maximizes the variance of the pixel intensities betweenclasses. To improve the threshold-based segmentation ef ﬁciency,background subtraction is often performed before threshold segmenta-tion (Li et al., 2019a).The model-based segmentation approach through computationalexpensive, but with proper training, can produce excellent segmenta-tion results. The template matching technique ﬁnds similar objects based on a visual template and image properties. Ellipse modeling andActive Shape Model (ASM) segmentation approaches are popular tem-plate matching techniques in object segmentation in animal studies(Leroy et al., 2006, 2005;Li et al., 2019a;Zhuang et al., 2018). An object's shape outline is tracked byﬁtting an ellipse in the ellipse modeling orPoint Distribution Model (PMD) in ASM modeling. Zhuang et al. (2018)applied a bird's color features based on a template image by el-lipse modeling and color features. DL approaches have also been madeas a segmentation technique, where a model is trained to detect partic-ular objects in an image. Convolution neural network (CNN) based de-tector and a correlationﬁlter-based tracker has also been applied inpig detection (Zhang et al., 2019). In poultry tracking,Fang et al. (2020)applied a deep regression network to detect and track birds ina pen. Similarly,Zhuang and Zhang (2019)used a CNN model to detect and predict sick birds. More details on DL techniques are presented inSection 3.2.2.2. Region of interest validationNot all the extracted regions after foreground detections are often
the ROI (may contain noise and misclassiﬁed pixels). A region validation is usually applied to improve quality by removing regions that aren'tconsistent with the features of the ROI. These are often artifacts presentin the background, such as droppings and shadows due to feathers andthe head within the foreground (Amraei et al., 2017a, 2017b;Mollah
Table 2Main segmentation techniques in poultry monitoring systems.MainsegmentationtechniqueAdditional technique Camera sensor Resolution Frame rateReferenceBackgroundsubtractionPixel intensity threshold, and ellipse modeling Sanyo VCB-3572IRP BW CCD ––Leroy et al. (2006)Pixel intensity threshold and region-basedhysteresis –– – Sergeant et al. (1998)Watershed algorithm, smoothening, and Sobelgradient operator Cambube3 –5 fpsNakarmi et al. (2014)Depth threshold, smoothening andmorphological opening Kinect v2 512 × 424 1 fps Okinda et al. (2019)Pixel intensity threshold and color ﬁlter Logitech Webcam Pro 9000 640 × 480 5 fps Aydin et al. (2013) Pixel intensity threshold Guppy F036C 1024 × 768 3.5 fps Aydin et al. (2010) Pixel intensity threshold and ellipse model Guppy F036C 1024 × 768 3.5 fps Aydin et al. (2015) Pixel intensity threshold and ellipse model Guppy F036C 1024 × 768 5 fps Aydin (2017b) Pixel intensity threshold CCD camera 640 × 480 1 fps Youssef et al. (2015) Pixel intensity threshold Monochrome CCD camera 256 × 256 –Bloemen et al. (1997) Threshold-based Contrast enhancements, erosion and dilation –– 16 fpsPereira et al. (2013) Color threshold and region crop Mikrotron EoSens –300fpsMehdizadeh et al. (2015)Pixel frequency distribution histogram Thermo GEAR-G120 (320 × 240) –Zaninelli et al. (2018) Color space conversion and morphologicalcorrosion PRO-1080MSFB 1440 × 1080 15 fps Guo et al. (2020)Color and, binarization threshold Sony Handycam Memory Flash PJ200 1080 × 1920 60 fpsNääs et al. (2018)Binarization threshold 3D Kinect camera 640 × 480 30 fps Aydin (2017a) Erosion and dilation Sony Cyber-shot ––Mollah et al. (2010) Smoothening, erosion, and dilation Samsung (SM-N9005) ––Amraei et al. (2018),Amraei et al. (2017b), Amraei et al. (2017a) Erosion and dilation –– – De Wet et al. (2003) Global threshold, morphological closing Aventura, CAM5D24DNVP 704 × 480 30 fps Kashiha et al. (2014) Pixel intensity threshold InterM, CCD Digital Color Camera VDC413 ––Kristensen et al. (2006)Morphological erosion image cropping Sony DCR-TRV330 ––Neves et al. (2015) Ellipse modeling Color space conversion, and K-means clustering Logitech C922 CCD 640 × 480 –Zhuang et al. (2018) Morphological closing, adaptive threshold Fluke TI32 320 × 240 –Xiong et al. (2019) Watershed Smoothening and morphological opening Kinect camera 640 × 480 15 fps Mortensen et al. (2016) Point DistributionModel– Sanyo VCB-35721RP BWCCD ––Leroy et al. (2005)C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
189et al., 2010). The most straightforward techniques for region validationare morphology operations and ellipseﬁtting (consistent with area and size constraints speciﬁed for the chickens).Morphological operations based on erosion and dilation are the basiccandidate validation processes performed to avoid discontinuities andisolated areas. Erosion and dilation functions remove and add pixelson an object boundary in an image, respectively. Erosion operationsmoothens the contour of an object by eliminating both narrow isth-muses and thin protrusions while dilation operation performs hole ﬁll- ing inside an object contour. The sizes and shapes of the structuralelements used in these morphological operations determine the num-ber of pixels removed or added to the objects in an image ( González et al., 2004). Additionally, these morphological operations assist in theremoval of unnecessary noises from the image (image smoothening).However, noise removal and smoothening can be performed by ﬁltering methods such as Gaussian, homomorphic, blur, adaptive median, andanisotropic diffusionﬁlters (Nakarmi et al., 2014;Tania and Rowaida, 2016). Furthermore, imageﬁltering can be performed as a precautionagainst over-segmentation in intensity based images (depth images)(Mortensen et al., 2016). In weight estimation, the head and the tail re-gions are often removed for an accurate estimation of weight by imageshape features. In intensity-based images, Mortensen et al. (2016)ap- plied morphological opening to eliminate local minima due to thehead while preserving local minima associated with the body. However,Amraei et al. (2017a),Amraei et al. (2017b), andAmraei et al. (2018) applied the Chen-Vese model to remove the chicken's head.Another candidate validation technique involves the incorporationof prior knowledge about the object's shape to achieve an accurate seg-mentation from noisy pre-processed data. Leroy et al. (2006)applied the ASM to model a 2D chicken shape and its deformations. However,the same study concluded that the main part of the chicken's bodycould be well approximated using a simple ellipse shape as Point Distri-bution Models (PDM), which was initially utilized in the previous studyofLeroy et al. (2005).2.3. Feature extractionFeatures are visual characteristics that can be used to correlate to aspeciﬁc bio-response or bio-process under investigation. These visualcharacteristics are extracted from the ROI. However, these features dif-fer from the segmentation features that are used to extract the chickenfrom the background. In machine vision systems, these features have tobe extracted manually from all images as opposed to DL techniques.Generally, in chicken monitoring systems, these features can be dividedinto three broad categories, i.e., morphological features, locomotor fea-tures, and opticalﬂow measures (Aydin, 2017b;Dawkins et al., 2012; Mortensen et al., 2016;Okinda et al., 2019).2.3.1. Morphological featuresMorphological features describe the shape and size of an object andhave been frequently applied in the description of several agriculturalproducts. The shape features have been applied in sick broiler detectionby posture analysis (Okinda et al., 2019;Zhuang et al., 2018). As changes in posture, such as depressed-bird-look-posture, indicates disease oc-currences. Size has long been an observed feature, even on humanhealth and growth. Animal size can be used to detect the occurrenceof several vitalities such as diseases, on feed conversion ratio, growth,and market readiness (Okinda et al., 2018a;
Wongsriworaphon et al., 2015). In object analysis, morphological properties are de ﬁned by the shape and structure of the object or parts of the object. Shape-based ob-ject analysis techniques are often preferred because of their stabilityagainst sensor noise since they are invariant to light and color variations(Kurnianggoro and Jo, 2018).Shape representation and description techniques can be divided intotwo broad groups, i.e., contour-based and region-based methods. Theseclassiﬁcations are categorized based on whether features are derivedfrom the whole shape or the contour region. Each section is furthersubdivided into structural and global, depending on whether theshape is represented as primitives or as a whole ( Zhang and Lu, 2004). The commonly applied shape morphological properties are the shapegeometric features, also known as shape simple descriptors ( Okinda et al., 2019). Shape geometric features can be simple shape measure-ments or shape indices. Shape measurements are the properties of theROI, such as area, perimeter, major and minor axis lengths, centroid,etc. Shape indices are a combination of shape measurements such as cir-cularity, eccentricity, average bending energy, convexity, etc. as illus-trated inTable 3. Shape indices have the advantage of being invariantto rotation, translation, and scale because they are dimensionless values(Zhang and Lu, 2004).Shape geometric features can be categorized as 2D and 3D featuresdepending on the dimensionality of the image to be analyzed. Severalshape geometric features have been successfully applied in chickenmonitoring systems, as presented inTable 4(Note that the 2D geomet- ric features can be projected to 3D). Besides the mentioned geometricshape features and indices,Okinda et al. (2019)proposed the use of shape complexity measure, which is a function of entropy of the shape'smedial axis transform (region-based shape descriptor) ( Okinda et al., 2018b;Panagiotakis and Argyros, 2016). The complexity feature was dependent on the shape structure (number of skeletons). The morethe shape structures, the higher the shape complexity; hence, thisTable 3Shape geometric parameters applied in shape analysis.Dimension GeometricfeaturesDeﬁnition2D Centroidg¼
1n∑ni¼1xi,;yi/C18/C19AreaA
shape ¼12∑ni¼1yiþ1xi−xiþ1yi/C0/C1PerimeterP
shape ¼∑ni¼1‖xiþ1,;yiþ1/C0/C1−x
i,;yi ðÞ‖Radialdistanceρ
i=‖p i−g‖Convexityδ
C¼Phull
Pshape
SolidityδS¼Ashape
Ahull
Aspect ratioδA¼xmax−xmin ðÞy
max−ymin ðÞ
Circularityratio (area)δCa¼Ashape
Acircle¼4πAshape
Pshape
Circularityratio(perimeter)δCp¼Ashape
P2circle
Circlevarianceδ
ρ¼σρ
μρwhereμP¼1k∑k−1i¼1ρiandσ P¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ 1k∑k−1i¼1ρi−μP ðÞ2sAveragebendingenergyδ
E¼1n∑n−1s¼0κsðÞ2
Hole arearatioδH¼Ahole
Ashape
Ellipsevarianceδ
d¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃρTiM−1ρiqEccentricityδ
e¼λ1
λ2
CompactnessδR¼Ashape
AboxwhereA boxis the area of a bounding box Elongationδ
l¼Hbox
Wbox
3D Centroidg
3D¼1n∑ni¼1xi,;yi,;zi ðÞSurface areaA
3Dshape ¼∑ki¼1ATk¼12∑ki¼1‖pi2−pi1 ðÞ∗pi3−pi1 ðÞ‖whereA Tkisthe area of thek
thtriangle withp kvverticesv=1 ,2 ,3 VolumeV
3Dshape ¼13∑ki¼1ATknTk∙pk1/C0/C1¼16∑ki¼1pi2−pi1 ðÞ∗pi3−pi1 ðÞ½/C138∙pi1
Sphericityγ
S¼Asphere
A3Dshape¼π136V3DshapeðÞ23
A3Dshape
WillmoreenergyW¼ 14Rk
1−k2 ðÞ2dAConvexityγ
C¼A3Dhull
A3DshapeC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
190feature successfully described a chicken posture shape. Additionally,Pereira et al. (2013)introduced the shape coefﬁcients features, which were indices computed from the area, perimeter, and radial distance(minimum and maximum radial distances).2.3.2. Locomotor featuresLocomotor features are one of the most important characteristicsused to identify poultry gait score (GS) and have been widely appliedin the monitoring of birds regarding lameness, activeness, and health.Generally, monitoring poultry mobility helps to detect the occurrenceof an infection and infestation, and provides a basis to evaluate if themanagement procedures and environmental conditions are conducive(Aydin, 2017b, 2017a;Okinda et al., 2019). Based on the idea by Winter (1985), the movement of a subject is an effort rather than thecause of the underlying problem. Hence, the locomotor features canbe subdivided into two categories, i.e., kinematic and kinetic features.Kinetic analysis was introduced as a technique to analyze the painlevels by the pressure a bird exerts on a particular foot. Kinetic featuresare calculated by analyzing the walking forces on the toes of a chickensuch as forces on the middle, medial, back, and lateral toes and metatar-sal pad as a chicken walk on a pedobarographic surface or a piezoelec-tric crystal sensor (Corr et al., 2007, 1998). Therefore, they are never applied in computer vision systems. Additionally, as Caplen et al. (2012)explained, few steps were analyzed in the kinetic analysis dueto bird pausing or sitting on the surface; hence, high levels of dataredundancy.Kinematic features are calculated by analyzing the walking motionand speed of a bird by computing its body displacement using referencebody positioned (markers are usually used) on the hook, knee, andmetatarsus. The relative displacement of these reference markers canbe tracked in both 2D space and 3D space. 3D space required a calibratedstereo camera to compute the 3D kinematic data, as performed byCaplen et al. (2012)andCaplen et al. (2013). However, these are consid- ered intrusive kinematic features (markers in contact with the bird),which would be infeasible in a real fam environment and consideringpoultry welfare criterion. The non-contact kinematic moments of abird include the walking velocity, acceleration, displacement, walkspeed, body oscillations, and movement frequency ( Aydin, 2017b; Dawkins et al., 2013;Nääs et al., 2018). In the analysis of feeding behav-ior, kinematic variables from mandibulations can be analyzed for an ef-fective feeder design and feeding behavior analysis ( Mehdizadeh et al., 2015).Table 5lists the mobility features that have been applied inchicken monitoring systems.2.3.3. Opticalﬂow measuresOpticalﬂow can be deﬁned as the pattern of visible motion of the ob-jects in the visual scene, or the distribution of the apparent velocity ofmotion of the brightness pattern in the image ( Horn and Schunck, 1981). Therefore, opticalﬂow analysis has been widely applied to detectmotion in several studies based on “noﬂow”and“ﬂow”analysis be- tween consecutive frames. Opticalﬂow can be categorized as sparse(ﬂow vectors of few pixels) and dense (ﬂow vectors of all pixels) opticalﬂ
ow. However, dense opticalﬂow is computationally expensive. There-fore, in most studies, image frames are often divided into pixel blocks(Colles et al., 2016;Dawkins et al., 2017, 2012). Opticalﬂow techniques can further be categorized as; differential techniques, energy-basedmethods, region-based matching, and phase-based techniques ( Barron et al., 1994;Horn and Schunck, 1981). Differential techniques computetheﬂow velocity from spatiotemporal derivatives of image intensity(Lucas and Kanade, 1981). Region-based matching deﬁnesﬂow velocityTable 4The applied shape morphological features in poultry monitoring systems.Classiﬁcation Features Description ReferenceGlobalregion-basedshapedescriptionsArea Number of pixels within the ROI Pereira et al. (2013),Zaninelli et al. (2018),Guo et al. (2020),Aydin et al. (2015),Aydin (2017b),De Wet et al. (2003),Mollah et al. (2010),Mortensen et al. (2016),Amraei et al. (2018),Amraei et al. (2017b),Amraei et al. (2017a), Neves et al. (2015) Minor axislengthLength of the minor axis of aﬁtted ellipse that has the same normalized second central moments as the ROI Leroy et al. (2006),Nakarmi et al. (2014),Amraei et al. (2018),Amraei et al. (2017b),Amraei et al. (2017a) Major axislengthLength of the major axis of aﬁtted ellipse that has the same normalized second central moments as the ROI Leroy et al. (2006),Nakarmi et al. (2014),Amraei et al. (2018),Amraei et al. (2017b),Amraei et al. (2017a) Centroid Center of mass of the ROI Leroy et al. (2006),Sergeant et al. (1998),Nakarmi et al. (2014),Aydin et al. (2015),Aydin (2017b),Nääs et al. (2018),Aydin et al. (2013) Orientation The angle between the x-axis and the major axis of aﬁtted ellipse that has the same second-moments as theROI.Leroy et al. (2006),Aydin et al. (2015),Aydin (2017b),Aydin (2017a)Elongation The ratio of height to width of the ROI's bounding box Okinda et al. (2019),Zhuang et al. (2018) Area-linearrateThe ratio of the area of the ROI to its perimeter Zhuang et al. (2018)Structuralregion-based
shapedescriptionsMedia axis An image skeleton Zhuang et al. (2018) RadialdistanceThe distance between a point on the boundary and thecentroid Pereira et al. (2013),Mortensen et al. (2016)Globalcontour-basedshapedescriptionsPerimeter The number of pixels around the boundary of a ROI Pereira et al. (2013),De Wet et al. (2003),Mortensen et al. (2016),Amraei et al. (2018),Amraei et al. (2017b),Amraei et al. (2017a) Eccentricity The ratio of the Eigenvalues ( λ
1andλ 2) of a covariance matrix of aﬁtted ellipse over a ROI. Okinda et al. (2019),Mortensen et al. (2016),Amraei et al. (2017b)CompactnessorrectangularityThe ratio between the size of shape compared to thesize of its bounding boxConcavity The measure of a shape's concaveness by how thederivative of its function is changing (curving in) Zhuang et al. (2018)Circularityratio(perimeter)The ratio between the size of the ROI compared to thearea of a circle that has the same perimeter with theROI's perimeterZhuang et al. (2018)CirclevarianceThe ratio between standard deviation and averagedvalue of the radial distance from all points in the ROI'sboundaryOkinda et al. (2019)Convexity The ratio between the perimeter of the convex hullfrom a ROI compared to its perimeter Okinda et al. (2019)C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
191as a change that provides the best match between image regions at dif-ferent times. This technique was introduced due to the impracticabilityof Differential techniques due to noise and aliasing during image-acquisition and few numbers of frames ( Anandan, 1989). Energy- based methods, also called frequency-based methods, apply velocitytunedﬁlters. Finally, in phase-based techniques, the ﬂow velocity is de- ﬁned in terms of the phase behavior of the output of a band-pass ﬁlter (Barron et al., 1994).The opticalﬂow measures include spatial mean, skewness, variance,and kurtosis of the estimatedﬂow velocities over the image. The de-scription of these features is given in Table 6. These measures are ob- tained from each frame in a time series. The average of these featuresis computed over a period of time to give a summary of the objectbeing monitored. In poultry monitoring, optical ﬂow analysis has been applied in monitoring of behavior and lameness ( Colles et al., 2016; Dawkins et al., 2013, 2012, 2009).2.3.4. Other featuresApart from the features mentioned above, several other featureshave been developed and derived in poultry monitoring. The behaviorsequence can be quantiﬁed to a fractal structure. Therefore, a correlationcan be developed between the fractal-like properties of behavior se-quences and a bio-response or bio-process by determining the mea-sures of complexity in those behavior sequences ( Marıa et al., 2004; Rutherford et al., 2004).Marıa et al. (2004)developed a fractal-like bi- nary behavior sequence for each chicken activity (observed). They per-formed a detrendedﬂuctuation analysis to quantify the correlationproperties of those fractals-like behavior sequences. This studyestablished that the fractal complexity of the behavior sequence de-creased with an increase in stress due to insuf ﬁcient energy to perform complex behaviors. Pixel proﬁle, moving pixels, or the proportion ofpixel changes have also been used as features in chicken tracking.Sergeant et al. (1998)performed an analysis of the frequency of thepixels of the ROI to determine the number of bids within the image. Ad-ditionally,Fraess et al. (2016)analyzed the percentage pixel changeusing EthoVision XT 10 (Noldus, Leesburg, VA, USA) to determinechicken activities from video frames. The difference in the intensityvalues between subsequent frames (at the same coordinates) can becomputed to determine the activity index. Similarly, Aydin et al. (2013)andAydin et al. (2010)applied this technique in their statisticalanalysis of chicken activity and GS. However, the latter applied theeYeNamic software to measure the bird's activities. Moreover, Van Hertem et al. (2018)determined theﬂock activity index and distribu- tion Index after image analysis by the eYeNamic software for GS predic-tion. In this sense, the activity index was a measure of the movement ofthe birds in the images, while the distribution Index was a measure ofthe occupiedﬂoor space in the house (Kashiha et al., 2013;Neves
et al., 2015). A similar technique was applied by Youssef et al. (2015)Table 5The applied locomotor features in poultry monitoring systems.KinematicfeaturesDescription Measurement technique Units ReferenceDisplacement The Euclidean distance moved by the bird or partof the body Image analysis (the Euclidean distance moved by the broiler's cen-troid between a pair of consecutive frames) mNääs et al. (2018),Image analysis (change in head's centroid position) mm Mehdizadeh et al.(2015) Stride duration The time taken by the bird to complete one stance(ground contact), and one swing (aerial) Qualisys Track Manager (QTM) software s Caplen et al. (2012),Caplen et al. (2013) Stride or steplengthThe distance moved during the stance and swingphase of a single leg Qualisys Track Manager (QTM) software mm Caplen et al. (2012),Caplen et al. (2013), Image analysis (centroid position and body location information) cm Aydin (2017b) PercentagestancePercentage of the stride duration when a foot is incontact with the ground Qualisys Track Manager (QTM) software % Caplen et al. (2012),Caplen et al. (2013) Step frequency The number of steps to cover a particular distance Image analysis (centroid position and body location information) n Aydin (2017b) Double-legsupportPercentage duration of each stride when both legsare weight-bearing Qualisys Track Manager (QTM) software % Caplen et al. (2012),Caplen et al. (2013) Vertical legdisplacementMaximum height leg lifted during a stride Qualisys Track Manager (QTM) software mm Caplen et al. (2012),Caplen et al. (2013) Lateral bodyoscillationThe chicken's body moving from one lateralposition to another and back to the originalpositionQualisys Track Manager (QTM) software mm Caplen et al. (2012),Caplen et al. (2013) Image analysis (orientation angle and body location information) cm
2Aydin (2017b) Vertical backdisplacementMaximum height back moved in a verticaldirection during a stride Qualisys Track Manager (QTM) software mm Caplen et al. (2012),Caplen et al. (2013) Speed The distance walked by the bird or moved by abird's body part per unit time. Image analysis (number of acquired images depending on thecapture frame rate as the bird moves across the walking testcorridor)ms
−1Okinda et al. (2019)mms
−1Aydin (2017b)Qualisys Track Manager (QTM) software mms
−1Caplen et al. (2012),Caplen et al. (2013) Manually measured the path of a bird that walked for over 10 s usingan acetate sheet was placed against the computer screen Dawkins et al.(2013) Image analysis (rate of change of beak displacement) mm ms
−1Mehdizadeh et al.(2015) Velocity Change in the chicken displacement per unit time Change in displacement between two consecutive periods, divided by the time difference ms
−1Nääs et al. (2018)Acceleration The rate of change of a bird's walking or a bird'sbody part velocity Change in the bird's velocity in a given amount of time ms
−2Nääs et al. (2018) Change in the beak's velocity with respect to time mm ms
−2Mehdizadeh et al.(2015)
Table 6The opticalﬂow measures.Opticalﬂow measures DescriptionMean μtðÞ¼
1B∑Ni¼1mitðÞVariance σ
2tðÞ¼1B−1∑Ni¼1mitðÞ−μtðÞðÞ2
Skewness γ
1tðÞ¼1B−1ðÞ ∑N
i¼1mitðÞ−μtðÞðÞ3
σ3tðÞ
Kurtosis γtðÞ¼ 1B−1ðÞ ∑N
i¼1mitðÞ−μtðÞðÞ4
σ4tðÞ
WhereBis the number of pixel blocks applied to compute the opticalﬂow.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
192andKristensen et al. (2006)in developing a close loop chicken behaviorcontrol system based on dynamic activity index, ambient temperature,air velocity, and light intensity.Regarding kinematic features and lameness, Reiter and Bessei (1997)suggested that lameness can be detected by analyzing thedifferences in the vertical and lateral movements of the left andright leg. Therefore, walking trajectory features has been used asdynamic features to assess lameness in broilers. Similarly, trajectoryand rotation features of sequences of a bird's image can be used inbehavior detection (Leroy et al., 2005). There are two scoring tech- niques in birds, i.e., GS (Kestin et al., 1992) and latency to lie down (LTL) (Berg and Sanotra, 2003;Weeks et al., 2002). However, GS is a subjective technique, while LTL is invasive as the bird has tocome in contact with water. Therefore, to develop a non-invasivetechnique (Aydin, 2017a) applied depth feature from a 3D depthimage to determine the LTL and number of lying events (NOL) to as-sess lameness using a 3D vision camera.2.4. Feature selection and dimension reductionFeature selection involves choosing a subset of relevant featuresafter feature extraction engineering before model development. The ex-tracted features may contain irrelevant and redundant variables thatwould inﬂuence the modeling task. A feature selection criterion is re-quired to measure the signiﬁcance of each feature and to remove extra-neous features, by selecting a subset of variables from an input data thatefﬁciently describes the input while reducing the effects of noise andother irrelevant variables but still capable of producing a generalizedmodel (Chandrashekar and Sahin, 2014). Therefore, feature selection helps to provide an in-depth understanding of the dataset, reducescomputational complexities, reduces the curse of dimensionality effects,and improves the general performance of the model. From a data-typeperspective,Li et al. (2017)categorized feature selection techniques assimilarity-based, hybrid feature selection, information-theoretical-based, statistical-based, sparse-learning-based, reconstruction-based,and deep-learning-based methods. In chicken monitoring systems,few features are always extracted; therefore, most studies don't per-form feature selection. However,Amraei et al. (2017a)andAmraei et al. (2018)performed feature selection based on a statistical-basedmethod (correlation analysis) to select the best predictors in chickenweight estimation.Dimension reduction isn't the same as feature selection. The dif-ference is that the resulting set of features after feature selection isalways a subset of the original set of features before the feature se-lection process. However, the resulting set after dimensionality re-duction techniques does not have to be a subset of the original setof features before the dimension reduction process (as in PrincipleComponent Analysis (PCA)). Thus, feature selection applies a subop-timal procedure to remove redundant data with tractable computa-tions. In summary, feature selection works on data attributes basedon variance, while dimension reduction works on Eigenvalue andEigenvector, making feature selection a special case of dimension re-duction. Considering that data becomes sparser in high-dimensionalspace (the curse of dimensionality), thus, affecting algorithms de-signed for low-dimensional space. Hence, dimension reductiontransfers the original dataset from high dimensional space to alower-dimensional space while preserving the essential features byﬁnding the optimal approximation of the original dataset. Dimensionreduction algorithms can be categorized according to their imple-mentation process, i.e., Feature Selection, Kernel Method, ProjectMethod, Manifold Learning, Dictionary Learning, Sparse Learning,and Artiﬁcial Neural Network (Huang et al., 2019). The eYeNamic software in the analysis of video frames in the study by Van Hertem et al. (2018)generated a large amount of data. Therefore,the PCA dimension reduction technique was applied while minimiz-ing information loss.2.5. Statistical analysisStatistical analysis is often performed to determine the statistical re-
lationship (inference) between the extracted features and the bio-process or bio-response being monitored. Generally, statistical infer-ence is a comparison of detailed statistics between an observationaldataset and an appropriate reference distribution to determine the sig-niﬁcance of those statistics in terms of mean values, standard deviation,differences among the means, etc. Statistical inference is a powerful toolfor drawing scientiﬁc conclusions that efﬁciently apply existing data or those collected for the speciﬁc purpose of testing hypotheses, providedvarious assumptions are met, and speciﬁc hypotheses are speciﬁed. Sta- tistical tests can be categorized into two groups, i.e., Parametric andNon-Parametric tests. The parametric statistical tests make assumptionson the parameters of the population distribution (data is assumed to benormally distributed). In contrast, non-parametric tests make no suchassumptions (Okinda et al., 2019). For every parametric test, there is ashadow non- parametric test. The choice of a statistical test is deter-mined by the underlying goal, as presented in Fig. 3. Statistical correlational analysis, i.e., Pearson correlation (parametrictest), was applied byDawkins et al. (2012)to determine the relation- ship between opticalﬂow and bird mortality. The same approach waspresented byDawkins et al. (2013)in determining the relationship be-tween opticalﬂow, behavior, and welfare. One-way ANOVA was alsoemployed byCaplen et al. (2012)andCaplen et al. (2013).T h ef o r m e r analyzed kinematic features regarding chicken weight and lameness,while the latter analyzed kinematic features to determine the responseof lame broilers to non-steroidal anti-in ﬂammatory drugs. Non- parametric tests such as the Friedman test, Dunn test, Spearman'sRanke Order correlation test, and Wilcoxon Signed-Rank Test havebeen successfully applied in chicken lameness ( Aydin, 2017b, 2017a; Aydin et al., 2015, 2013, 2010), health (Okinda et al., 2019)a n db e h a v i o r (Kristensen et al., 2007) monitoring systems. A description of the ap-plied statistical analysis in the monitoring of poultry is presented inTable 7.2.6. Modeling techniquesThe modeling step in poultry monitoring systems can be categorizedas regression and classiﬁcation tasks. Based on different modeling tech-niques, the tasks can be performed by conventional machine learning orDL techniques. Machine learning algorithms are computerized model-ing approaches based on sample data (use statistics to ﬁnd patterns in data) to make decisions or predictions without being reprogrammedtime and again (Bhargava and Bansal, 2018;Wang et al., 2019a). Ma- chine learning algorithms are grouped into three categories; SupervisedLearning, Unsupervised Learning, and Reinforcement Learning.Supervised Learning is a governed learning technique that incorpo-rates the use of example inputs and their desired outputs, and themain objective is to learn the pattern (training) that maps the inputsinto outputs (Alpaydin, 2020). Supervised Learning involves modeltraining and model testing tasks. The model training process is an essen-tial procedure performed using labeled data as the supervisory signal.Supervised learning algorithms learn a function that can map new in-puts into outputs (prediction) by iterative optimization of an objectivefunction. The model testing is the application of new or unknown datato the trained model and observing the accuracy of the predicted out-put. In chicken monitoring systems, the outputs are weight (regres-sion), health condition (classi
ﬁcation), or behavior (classiﬁcation), among others. Linear, nonlinear, and logit regression, Support vectormachine (SVM), Support vector regression (SVR), and arti ﬁcial neural networks (ANN) are the mostly applied supervised machine learning al-gorithms that have been applied in poultry monitoring systems.Linear regression applies a statistical approach to model the rela-tionship between one or more independent variables and a (one) de-pendent variable byﬁtting a linear equation. In linear modeling, theC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
193Fig. 3.Theﬂow chart for the selection of appropriate statistical tests.Adopted from
Jaykaran (2010)
.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
194Table 7Summary of the computer vision-based chicken monitoring systems in the literature.Monitored bio-process and bio-responses n Statistical analysis Model Software Accuracy ReferencesInvestigationBehavior Behavior in relation to stressconditions 7 ANOVA Linear regression –0.76 (R
2)Marıa et al.(2004) Behavior analysis of individuallycaged poultry 18– Transfer function (TF) –– Leroy et al.(2006) Relation of opticalﬂow patternsbetween behavior, mortality, GS,and leg health24 Pearson correlation test –– – Dawkinset al. (2012)Opticalﬂow patterns in broiler inbehavior classiﬁcation and GS40,000 Correlation analysis –– – Dawkinset al. (2009) The relationship between opticalﬂow, behavior and welfare 35,000 Pearson correlation test –– – Dawkinset al. (2013) Effects of different light sources andilluminances on behavior 16 Non-parametric analysis ofvariance – Statistical AnalysisSoftware (SAS) –Kristensenet al. (2007) Behavior classiﬁcation –– Classiﬁcation tree Weka® version3.4.1170.3%Pereira et al.(2013) ~350– YOLO v3 Darknet framework 92.09% Wang et al.(2020) Feeding behavior (beak and headmotion during feeding) 3 ANOVA Linear regression Minitab 17® 99.2% (R
2)Mehdizadehet al. (2015) Tracking and behavior classiﬁcation 15 Multi-regression – R statistical package 95% Nakarmiet al. (2014) Feeding and drinking behaviorclassiﬁcation 60 ANOVA Linear regression Statistical Analysis Software (SAS)96.5Li et al.(2019b) Behavior monitoring for earlydetection ofCampylobacter31 Multi-level models R statistical package Colles et al.(2016) Behavioral response to feedingevents 136 ANOVA EthoVision XT 10 SigmaPlot 11 Fraess et al.(2016) Behavior recognition 3087 – CNN Visual Studio OpenCV3.5.0Kinect for WindowsSDK96.4%Pu et al.(2018)Effects of wearing a backpack onbehavior, health, and productivity 60 Mixed logistic regression –– – Stadig et al.(2018) Effects of micro-environmentconditions on behavior and activity 45– Discrete transfer function –94%Youssefet al. (2015) Effects of feeder types onﬂockbehavior 14,000 General Linear Model (GLM) – MATLABMinitab 15 ® –Neves et al.(2015) Recognition of behaviorphenotypes of layers 5– Dynamic modeling Observer ® 70 –96%Leroy et al.(2005) Tracking Tracking of individual birds 13 –– – 95%Sergeantet al. (1998) 10– Deep regression Python 0.73 Fang et al.(2020) 10 Particle ﬁlter–– Fujii et al.(2009) Detection of multiple nestoccupations –ANOVA – R statistical package 95.5% Zaninelliet al. (2018) Monitoring broiler chickenﬂoordistribution 126 ANOVA BP neural network MATLAB 0.996 (R) Guo et al.(2020) Real-time malfunctioning in a
broiler house detector 28,000– Linear regression eYeNamic system MxControlCenterMATLAB95.24%Kashihaet al. (2013)Evaluation of a laying-hen tracking 6 – Hybrid Support VectorMachine (HSVM) OpenCV 0.79 Wang et al.(2016) Hen tracking in an environmentalpreference chamber 4– Ellipse-ﬁtting model MATLAB 95.9 ± 2.6%Kashihaet al. (2014) Health Monitoring of heat stress Correlation Faster R-CNN Caffe –Lin et al.(2018) Automatic detection of sick chickens –– Residual neural network –95%Zhang andChen (2020) Detection of sick broilers 20 – SVM, Bayesian classiﬁer,Random Forest and ANN Visual Studio 2013OpenCV 2.4.1399.469%Zhuang et al.(2018) Early detection and prediction ofsick chickens 280 Friedman test, Spearman'scorrelation test, WilcoxonSigned-Rank TestSVM, ANN, and logitregression Statistical Package forthe Social Sciences(SPSS)Kinect for WindowsSDKMATLAB0.978Okinda et al.(2019)
Classiﬁcation of broiler droppingsfor intestinal disease detection 10,000– Faster R-CNN and YOLO-V3 Tensor ﬂowframeworkDarknet framework93.3%Wang et al.(2019b)Detection of sick broilers 400,000 – Improved Feature FusionSingle Shot MultiBox OpenCV 99.7% Zhuang andZhang(continued on next page)C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
195relationships are developed by a linear predictor function ( Rencher and Christensen, 2012). However, for non-linear regression, the dependentvariable is modeled as a non-linear function of one or more independentvariables. In this case, data isﬁtted by successive approximationsmethods. The two regression techniques have been widely applied inchicken weight estimation.De Wet et al. (2003)developed two linear models to estimate chicken weight based on image object surface areaand perimeter. The same technique was applied by Mollah et al. (2010)but incorporated the age of the chicken. For comparative analy-sis with other regression algorithms, Mortensen et al. (2016)also ap- plied linear regression based on 2D, 3D image features and age. Thelogit regression is applied when the dependent variable is a binary (di-chotomous). It models the relationship between a dependent binaryvariable and one or more independent variables. Okinda et al. (2019) applied logit regression to classify broiler chicken as sick or healthyusing broiler morphological and locomotor features.SVR is an extension of SVM to solve regression problems. SVM canperform both linear (non-probabilistic binary linear classi ﬁer) and non-linear classiﬁcation by applying kernel functions to implicitly mapinputs into high-dimensional feature space ( Cortes and Vapnik, 1995). The kernel functions solve the quadratic programming problem ofseparating support vectors in the training data vectors by ﬁnding the appropriate hyperplane. The SVM kernels include linear, polynomial(quadratic and cubic), and radial basis function (RBF) kernels ( Nyalala et al., 2019;Okinda et al., 2020). RBF kernel was applied byAmraei et al. (2017b)in chicken weight estimation, whileOkinda et al. (2019) applied all the SVM kernels mentioned above in chicken health statusclassiﬁcation.Biological neural networks inspired the ANN machine learningtechnique. The ANN simulates the way the human brain analyzesand processes information. Therefore, it's a non-linear statisticalmodel. ANN consist of input, hidden, and output layer. The hiddenlayer transforms the input into output by solving an optimizationproblem by minimization of a loss function during optimization(Samarasinghe, 2016). The most basic types of ANN are thefeedforward neural network and recurrent neural network, whichare trained by the Backpropagation algorithms. The number of neu-rons in the hidden layer is often adjusted during training to minimizenetwork error. This can be observed in the study by Mortensen et al. (2016)in the selection of 3 and 10 neurons in the hidden layer. How-ever, these regression models performed reasonably similarly. TheBayesian ANN being a probabilistic model using an ANN regressionTable 7(continued)Monitored bio-process and bio-responses n Statistical analysis Model Software Accuracy ReferencesInvestigationDetector (IFSSD) (2019) Prediction of welfare outcomes forbroiler chickens 816,000– Bayesian multivariate linearmodel –– Robertset al. (2012) Head surface temperatureextraction 20–– MATLAB 92.77% Xiong et al.(2019) Activity Identiﬁcation of activities ofchickens with different GS 30 Friedman testDunn test – MATLAB –Aydin et al.(2013) Identiﬁcation of activities ofchickens with different GS 30 Friedman testDunn test – eYeNamic software –Aydin et al.(2010) Predicting broiler GS based onactivity monitoring andﬂock data196,000 ANOVABlande Altman method – eYeNamic software 0.53 –0.74(R
2)Van Hertemet al. (2018) Effects of micro-environmentconditions on behavior and activity 45– Discrete transfer function –94%Youssefet al. (2015) Image analysis to measure activityindex of poultry 15– Linear regression Turbo Pascal 0.5 (R
2)Bloemenet al. (1997) Effects of light intensity on thedynamic activity of broiler chickens 84 Spearman correlationANOVA Discrete transfer function MATLAB, Statistical Analysis Software(SAS)Kristensenet al. (2006)Lameness Classiﬁcation of lying event to assesslameness of broilers 250 Friedman testDunn test – MATLAB –Aydin et al.(2015) Kinematic analysis regarding GS –ANOVA – MLwiN v2.22 –Caplen et al.(2012) The response of lame broilers tonon-steroidal anti-inﬂammatorydrugs32 ANOVA – MLwiN v2.22 –Caplen et al.(2013)Early detection system for lamenessin broilers 250 Friedman testDunn test – MATLAB –Aydin(2017b) Estimating the GS of broilerchickens 300– Paraconsistent logic MATLAB 50 –100%Nääs et al.(2018) Assess the level of inactivity inbroiler chickens 250 Friedman testDunn test Linear regression Statistical analysis software (SAS)MATLAB94.49%Aydin(2017a)Walking behavior of heavy and lightbroilers 36 General Linear Model (GLM) – LabView –Bokkerset al. (2007) Weight Monitoring daily growth rates ofbroiler chickens 50– Non-linear regression –0.94–0.97(R
2)De Wet et al.(2003) Prediction of broiler chickensweight using 3D computer vision 48,000– Multivariate linearregressionANNBayesian ANNMATLAB 92.2% Mortensenet al. (2016)Boiler live weight estimation 100 Paired t-test Linear regression IDRISI 32 0.99 (R
2)Mollah et al.(2010) 30 Paired t-test, correlationanalysis ANN MATLAB 0.98 (R
2)Amraei et al.(2017a) 20 Paired t-test SVR LIBSVM 0.98 (R
2)Amraei et al.(2017b) 30 Correlation analysis Transform function MATLAB 0.98 (R
2)Amraei et al.(2018)C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
196function was also applied byMortensen et al. (2016)due to its ap- proach to outlier detection. Furthermore, its performance wasmore superior to linear regression and other ANN models in broilerweight estimation.To evaluate the performance of different backpropagation training al-gorithms, i.e., Gradient descent, Bayesian Regularization, Scaled Conju-gate Gradient, and Levenberg-Marquardt training algorithms, Amraei et al. (2017a)developed three ANN models for broiler weight estima-tion. Based on the training algorithms mentioned above, the Bayesianregulation training algorithm resulted in the best performing ANNmodel at an R
2of 0.983 and RMSE of 82.37 g on the testing data set.The performance of logit, SVM, and ANN classi ﬁers were evaluated and compared byOkinda et al. (2019)in broiler health classiﬁcation. The RBF SVM outperformed all the other models at an accuracy of 0.978. Sim-ilarly,Zhuang et al. (2018)reported the superiority of SVM at about99.5% accuracy in sick birds detection. In determining the chicken distri-bution,Guo et al. (2020)applied a backpropagati o nn e u r a ln e t w o r ka n d a normalized chicken image surface area. Other supervised Learning thathas been applied in chicken monitoring is the decision tree learning,which was implemented byPereira et al. (2013)in the classiﬁcation of broiler breeder behaviors whereby the extracted geometric featureswere the branches, while the leaves represented the behavior labels.Unsupervised Learning and Reinforcement Learning haven't beenapplied to poultry monitoring systems (modeling techniques). How-ever, this review will present a brief highlight of the two learning tech-niques. Unsupervised Learning has no training labels for trainingsamples, unlike supervised Learning. Unsupervised learning algorithmsﬁnd suitable structures and patterns in unlabeled data by modeling ofprobability densities over inputs (Hastie et al., 2009). The two main techniques used in Unsupervised Learning are cluster analysis and prin-cipal component (Duda et al., 2001). Nevertheless,Zhuang et al. (2018) applied K-means clustering as an ROI segmentation technique of birds ina farm environment. The reinforcement algorithm learns via a feedbackloop and focuses onﬁnding a balance between exploration and exploita-tion (Kaelbling et al., 1996). It works on the Markov decision process(MDP) environment. Therefore, basic reinforcement learning ismodeled as a Markov decision process. A detailed mathematical descrip-tion of these machine learning algorithms will not be presented in thisstudy. Please refer to the corresponding publications for more insight.3. Deep learning-based poultry monitoring systemsAs already mentioned, the conventional machine learning-basedpoultry monitoring follows the procedure image acquisition, pre-processing, segmentation (ROI extraction), feature extraction, andclassiﬁcation or regression, as presented in Section 2. However, seg- mentation, feature extraction, and selection engineering are arduoustasks. Furthermore, the performance of these algorithms is affectedby sensor sensitivity, making them challenging in a real farm envi-ronment. DL approaches eliminate these arduous tasks by directlyprocessing the image by the application of DNN, as shown in Fig. 4. Thus, DL is also considered as feature learning ( Kamilaris and Prenafeta-Boldú, 2018). Additionally, DL models have achievedhigher accuracy due to their ability to avoid errors associated withsegmentation and erroneous feature vectors. Furthermore, DL allowsmassive parallelization of computations due to the complex models.Therefore, complex problems can be solved at faster computationalspeeds (Pan and Yang, 2010). Therefore, more research is currentlyfocusing on optimum network architecture rather than on featureengineering in conventional image processing methodologies.In DL, both the local and inter-relationships of data are learned ina hierarchical structure through several levels of abstraction (eachlayer transforms the input data from the previous layer into a newrepresentation at a greater abstraction level). A non-linear functionin each layer of a DL model transforms the data into representationin each layer. This hierarchical feature representation learning al-lows DL models to be successfully applied in classi ﬁcation and pre- dictions in various artiﬁcial intelligence applications, i.e., audio,raster-based data, time-series data ( Kamilaris and Prenafeta-Boldú, 2018;Sehgal et al., 2017
;Song et al., 2016). DL monitoring systems follow the steps image pre-processing, data augmentation, model-ing, andﬁnally, classiﬁcation or regression, as shown inFig. 4. Similar to ANN, DL models are trained by a backpropagation algorithm to-gether with an optimization algorithm that updates the networkweights to minimize the loss function. This section will present a re-view of various DL model architectures, data pre-processing tech-niques, data augmentation methodologies, and DL systems in themonitoring of poultry.3.1. Deep learning categoriesThis study will present a brief discussion on some of the popular DLarchitectures, i.e., Convolutional Neural Networks (CNNs), Recurrentand Recursive Neural Networks, and Pretrained Unsupervised Net-works. Generally, each architecture has a speci ﬁc appropriate area of ap- plication, and some are already pre-trained to provide accurateclassiﬁcation in particular domains (Kamilaris and Prenafeta-Boldú, 2018;Pan and Yang, 2010). The popular platforms for development
Fig. 4.The general workﬂow of deep learning-based chicken monitoring systems.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
197and testing of DL models are TensorFlow, Keras, Theano, Matlab,Pylearn2, Caffe, TFLearn, and PyTorch.3.1.1. Convolutional Neural Networks (CNNs)CNN is the most popular architecture applied in computer visiontasks and natural language processing. CNN is a multi-layered networkthat can learn features of a target to perform an autonomous detection.It comprises several neural layers, i.e., convolutional, non-linear activa-tion layer, pooling, and fully connected layers. Each layer transforms theinput to output for neuron activation, which eventually leads to the ﬁnal fully-connected layers, thus resulting in the mapping of an input to a 1Dfeature vector. CNN perform convolution instead of standard matrixmultiplication in their layers as opposed to the conventional neural net-works. The main attributes of CNN are parameter sharing (tied weights,i.e., only a single set of parameters are learned for each location of animage) and sparse interactions (making the kernel smaller than thesize of the input, hence, reduced memory utilization and computationaloverhead) (Hosseini et al., 2020).Fig. 5presents the general structure of CNN architecture.In the convolutional layers, CNN applies various kernels to convolvethe entire image to generate feature maps. The non-linear activationlayer, i.e., the Rectiﬁed linear unit layer (ReLU), improves the trainingspeed and increases the non-linearity of the feature maps (inputs) byapplying a function. The pooling layer reduces the spatial dimensionsof the input volume. However, the pooling layer doesn't affect thedepth but only the width and height of the input volume. This operationis referred to as down-sampling or subsampling. This decrease in sizeleads to low computation complexity in the proceeding layers and pre-vents overﬁtting. The fully-connected layers perform the high-level rea-soning in the neural network by converting 2D feature maps to a 1Dfeature vector. The obtained vectors could be fed forward into catego-ries for classiﬁcation (object detection task) or as feature vectors for fur-ther processing. Several CNN architectures have been created over theyears, i.e., LeNet, AlexNet, ResNet, GoogLeNet (Inception), VGGNet,MobileNet, SqueezeNet, and Capsule Networks (CapsNet).ResNets, also known as Deep Residual Networks, presented a solu-tion to solve complex problems in CNNs as the network becomes deeper(vanishing gradients) (Balduzzi et al., 2017). ResNet consists of a series of residual modules (layers), and each layer is a function set to be per-formed on an input with the gradient signal capable of feedback to ear-lier layers via shortcut connections (Balduzzi et al., 2017;He et al., 2016;Kawaguchi and Bengio, 2019). ResNets have the advantages of beingmore accurate and require less weight in some cases and being highlymodular. Additionally, they can be designed to determine how deep anetwork can be. The main disadvantages of ResNets are that for a deepernetwork, the detection of errors becomes dif ﬁcult. Additionally, if the network is too shallow, the learning might be very inef ﬁcient. ResNets resulted in deeper networks, while Inception resulted inwider networks. Inception was intended to improve the computationalefﬁciency in the training of larger networks (scaling up neural networkswithout compromising the computational cost) ( Szegedy et al., 2016b). In a convolutional network, each layer extracts different types of infor-mation from the previous layer. An Inception module computes severaldifferent transformations over the same input map in parallel and con-catenates their results as a single output. To solve the computationalbottleneck, Inception performs dimensionality reduction by the use ofa 1 × 1 convolution across multiple channels to extract spatial informa-tion and compressing this information down to a lower dimension.Therefore, by reducing the number of input maps, Inception can stackdifferent layer transformations in parallel, thus, resulting in simulta-neously wide and deep networks. Inception has evolved from the ﬁrst version known as the GoogLeNet to Inception v2, v3, and v4. In v3, the5 × 5 convolution was replaced with two consecutive 3 × 3 convolu-tions. The current version v4 applied the residual connections withineach module resulting in an Inception-ResNet hybrid ( Szegedy et al., 2016a).The Xception stands for extreme inception. Consider that in a tradi-tional convolutional network, convolutional layers seek out correlationsacross both depth and space. While in Inception, a 1 × 1 convolution isused to project an original input into numerous separate, smaller inputspaces. From each input space, different types of
ﬁlters are applied to manipulate those smaller 3D blocks of data. However, in Xception, in-stead of partitioning the input data into several compressed primitives,the spatial correlations for each output channel is mapped separately,then performs a 1 × 1 depth-wise convolution to capture cross-channel correlations (Chollet, 2017). This operation can be referred toas depth-wise-separable-convolution, i.e., spatial convolution done in-dependently for each channel, followed by a point-wise convolution(1 × 1 convolution across channels) (Chollet, 2017). The VGGNet follows the typical layout of basic convolutional net-works, i.e., a series of convolutional, max-pooling, and activation layersbefore the fully-connected classiﬁcation layers at the end (Simonyan
Fig. 5.Convolutional neural network architecture.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
198and Zisserman, 2014). The MobileNet is essentially a rationalized ver-sion of the Xception architecture optimized for mobile applications.The SqueezeNet is powerful DL architecture that's ef ﬁcient in low band- width platforms. It is based on a CNN architecture but with 50 timesfewer parameters than AlexNet and maintains AlexNet-level accuracyon ImageNet (Iandola et al., 2016). The CapsNet, a multi-layer capsulesystem, is an advanced variation of CNNs that deepens in terms ofnesting or internal structure (Sabour et al., 2017). It's mainly used for accurate image recognition tasks because it is robust to geometric dis-tortions and transformations. Thus, it can exceptionally handle orienta-tions, rotations, and translations.CNN-based algorithms can be divided into two broad categories;two-stage target detection algorithms, i.e., R-CNN ( Girshick et al., 2014), Fast R-CNN (Girshick, 2015), Faster R-CNN (Ren et al., 2015), Mask R-CNN (He et al., 2017), that uses Region Proposal Network(RPN) to generate the anchor boxes, after which the detection networkperforms prediction. The one-stage target detection algorithm is thesecond category that includes OverFeat ( Sermanet et al., 2013), SSD (Liu et al., 2016), YOLO (Redmon et al., 2016), YOLO9000 (Redmon and Farhadi, 2017), YOLO v3 (Redmon and Farhadi, 2018). These algo- rithms predict the target location and category directly. Hence, theyare faster than two-stage target detection algorithms and are appliedin real-time detection systems.3.1.2. Recurrent and recursive neural networksThese are networks that can handle time-series data, i.e., RecurrentNeural Network (RNN), Recursive Neural Network, Attention, andLong Short-term Memory (LSTM).RNN is a network whose current output is based on both the presentinput data and the learning based on previous data. Therefore, RNN isapplied in applications where the sequence in which data is presentedis vital, i.e., machine translation, speech synthesis, and natural languageprocessing. Every computed information is stored (hidden state vector)and utilized to compute theﬁnal output. However, the same input canresult in different outputs depending on the previous inputs in thedata series. RNN is referred to as recurrent because the same task is per-formed for every element in the series, resulting in the generation of dif-ferentﬁxed-size output vectors where the hidden state vector isupdated for every input. Therefore, RNN captures both sequential andtime dependencies between data (Gulli and Pal, 2017;Haque and Neubert, 2020;Hosseini et al., 2020). RNNs are suitable for sequentialdata because they share weights across time steps and can performone to many, many to many, and many to one mapping. There aretwo varieties of RNN, the Bidirectional RNN (BRNN) and the Encoder-Decoder RNN (EDRNN). The output of a BRNN depends on both thepast and future outputs, i.e., RNNs makes inferences from the presentdata point in a sequence relative to both future and previous data points.The EDRNN can map the input data sequence into variable-length out-put sequences (Hosseini et al., 2020). RNNs can be made deeper (addingmultiple layers for faster learning and improved network performance)by adding more hidden state layers, adding more layers between thehidden state layer and the output layer, adding non-linear hidden layersbetween the input layer and the hidden state layer or applying all thethree (Haque and Neubert, 2020).Recursive neural networks have a return loop to feed the networkinto itself. This allows for the identiﬁcation of input data constituentsand their relationships through a binary tree structure and shared-weight matrix (Hosseini et al., 2020). Recursive neural networks arecharacterized by a top-down propagation method and a bottom-upfeed-forward method. There are two main types of Recursive neuralnetworks, i.e., supervised recursive neural tensor (applied in computer
vision) and the semi-supervised recursive autoencoder (applied in sen-tence deconstruction). The main advantage of Recursive neural net-works over RNNs is that they can capture long-term dependenciesefﬁciently. However, Recursive neural networks suffer from substantialcomputational overhead than the RNNs ( Goodfellow et al., 2016; Hosseini et al., 2020).The LSTM is a special RNN that applies recurrent edges as a solutionto the vanishing gradient problem. LSTM use memory cell to hold infor-mation and a set of gates (input, forget, and output gates) to indicate thestatus of the memory cell (Sundermeyer et al., 2015, 2012). The con- tents of the memory cell are modiﬁed by the input and forget gates con-ditions at each time step. The input gate selects the new informationthat should be added to the cell state. The forget gate selects which in-formation should be discarded from the cell state. The output state se-lects relevant information from the cell state as the output.3.1.3. Pretrained Unsupervised Networks (PUNs)PUNs are DL models whose hidden layers are trained by unsuper-vised learning to achieve an accurateﬁtting of the dataset. The layers are trained (unsupervised learning algorithm) independently sequen-tially, such that the input of a layer is the previously trained layer. Thewhole model is thenﬁne-tuned using supervised learning after eachlayer has been pre-trained. Types of PUNs include Generative Adversar-ial Networks (GAN), Autoencoder, and Deep Belief Networks (DBNs).An autoencoder neural network applies a backpropagation algo-rithm in an unsupervised environment. The input is compressed into alatent-space representation, and the output is the same or close to theinput values (learn a representation for dimensionality reduction).They are popular in anomaly detection applications, i.e., fraud detectioninﬁnancial transactions. The network comprises an encoder and de-coder parts, as shown inFig. 6. The input data is compressed by the en-coder into latent-space representation, while the decoder performs thedata reconstruction (output from the latent-space representation).Autoencoders cannot be applied as a generative model due to disconti-nuities in the latent space representations ( Haque and Neubert, 2020). Therefore, variational autoencoders were introduced as a solution.Whereby the encoder outputs two vectors (mean and standard devia-tion) rather than one. This allowance enables the decoder to correctlydecode values with small variations of the same input ( Haque and Neubert, 2020). There are four main types of autoencoders,i.e., Vanilla, Multilayer, Convolutional, and Regularized autoencoders.The vanilla is the simplest autoencoder with a neural network withone hidden layer. Multilayer is an autoencoder with more hidden layers.Convolutional is an autoencoder with convolution layers instead offully-connected layers. Lastly, the Regularized autoencoder applies aspecial loss function to improve performance.GANs involve the training of two DL models (the generator and thediscriminator) simultaneously that compete with each other. The gen-erator creates new instances by modeling a transform function duringtraining. In comparison, the discriminator classi ﬁes if an instance origi- nates from the generator or the training data, while the former maxi-mizes theﬁnal classiﬁcation error while the later minimizes the errorbetween the generated data and the training data. Thus, the two net-works are referred to as adversaries. Hence, the whole network im-proves with each iteration during training. GANs are widely applied incomputer vision, especially in image generation and also in speech,prose, and music because of GANs ability to mimic any distribution ofdata in any domain (Hosseini et al., 2020).GANs have the advantage that it requires no deterministic bias, un-like the variational encoders, they allow for ef ﬁcient training of models in a semi-supervised setting. However, the main drawbacks of GANs arethat the performance of the generator and discriminator are crucial inthe success of GAN, and the whole model fails if one system (generatorand discriminator) fails. Additionally, training GAN is computationallyexpensive with high training time due to the two-model training.DBNs are an extensive layered network structured by connectingseveral smaller unsupervised neural networks. A DBM is composed ofBelief Net and the Restricted Boltzmann Machine (RBM) ( Hosseini et al., 2020). Belief Net is composed of connected layers (binary unitlayers), each assigned a weight function (layer-by-layer learning). TheC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
199probability of the binary outcome depends on the weight factor and thebias inputs. RBM is a stochastic RNN designed on the principles ofenergy-based models (EBMs) (Haque and Neubert, 2020;Hosseini et al., 2020). Learning is performed by minimization of the energy func-tion, and prediction is achieved by determining the values of residualvariables that minimize the energy based on observed variables. TheRBM consists of one input layer and one hidden layer without an outputlayer. Another type of RBM is the Deep Boltzmann Machine (DBM),characterized by undirected connections. Hence, DBM is robust in han-dling certainty due to noisy inputs.3.2. Image pre-processingImage pre-processing is performed before the image is fed as aninput to the DL model. Image resizing is the most common image pre-processing procedure for the image to adapt to the DL model require-ments. In the deep regression network (AlexNet and ReLU activationfunction),Fang et al. (2020)resized the input image to 960 × 540from 1920 × 1080 resolution. Similarly, Zhuang and Zhang (2019)per- formed a resizing operation to have a 512 × 512 input image resolution.In a comparative analysis between Faster R-CNN and YOLO v3 in recog-nition and classiﬁcation of broiler droppings,Wang et al. (2019b) resized images from 5760 × 3240 resolution.Data labeling which involve the creation of bounding boxes is an-other vital pre-processing procedure. Data labeling is often performedmanually to reference the ground truth by a bounding box. Labelingsoftware such as LabelImg (Windows-based) is applied to draw thebounding boxes and extract their co-ordinate locations. Ground truthlabeling is a vital step in classiﬁcation tasks as it provides a basis for per-formance evaluation of the proposed detector ( Zhuang and Zhang, 2019). The procedures mentioned above are the main techniques thathave been applied in poultry monitoring DL modeling systems. Otherpre-processing operations include image segmentation to highlightthe ROI hence, facilitating the learning process as performed by Fanget al. (2020). Background removal or foreground pixel extraction canalso be performed to reduce the effect of noise in the dataset(Kamilaris and Prenafeta-Boldú, 2018).3.3. Data augmentationDL models need a lot of training data to achieve an appropriate con-vergence for better recognition accuracy while at the same time,avoiding over-ﬁtting. Therefore, data augmentation technique is per-formed to expand the training data by a dynamic transformation ofthe data without changing their classiﬁcation. Ifkis the number of aug- mentation techniques applied, then the total number of images used intraining will be (k+1)-fold of the original dataset. Additionally, theimage transformation effectively increases the training set without theneed to store a large augmented training set. Table 8presents the data augmentation techniques applied in the DL data processing.3.4. Deep learning applicationsSeveral studies have applied DL models in poultry monitoring sys-tems ranging from behavior classiﬁcation, tracking, sick birds' detection,to droppings classiﬁcation.Pu et al. (2018)developed a CNN detector to classify chickenﬂock behaviors at the feeders using color and depth im-ages (two parameter-sharing CNNs). His network consisted of threeconvolutional layers, each with a rectiﬁed linear unit (ReLU) activation function, a max-pooling layer with a local response normalizationstep. The system achieved an accuracy of 99.17% in the chicken behaviorclassiﬁcation. A Faster R-CNN chicken activity detector combined withthe temperature-humidity index (THI) was used to monitor heat stressin chicken (Lin et al., 2018). The detector applied the Zeiler and Fergusnetwork (
Zeiler and Fergus, 2014) as the base CNN. The chicken move-ment was determined by tracking the chicken location between subse-quent frames using the minimum distance matching and color featurematching techniques. As already mentioned, the detection speed of
Fig. 6.The autoencoder architecture.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
200YOLO v3 is faster (real-time) compared to other two-stage target detec-tion algorithms because it is an end-to-end target detection algorithm.Wang et al. (2020)presented a real-time behavior detector. This systemcould detect six chicken behaviors at the highest mean precision rate of94.72%. An improved SSD was introduced by Zhuang and Zhang (2019) for sick broiler detection. The introduced Improved Feature Fusion Sin-gle Shot MultiBox Detector (IFSSD) had the InceptionV3 architecture ofa 1 × 1 convolution of three different size layers and features generatedby a feature pyramid network. The detector could detect broilers andtheir health status simultaneously at a mean average precision (mAP)of 99.7%. As the number of network layers increases, there often arisesthe problems of difﬁculty of network optimization and disappearanceof gradient descent. Therefore,Zhang and Chen (2020)developed a sick chicken detector based on ResNet residual network. Taking advan-tage of ResNet having an excellent training performance even for deepnetworks and improving its network structure, the proposed networkadopted to different recognition environments. Fang et al. (2020)pre- sented the TBroiler tracker, whereby chicken tracking was performedas a regression task by developing a deep regression network composedofﬁve convolutional layers and three fully-connected layers. Addition-ally,Fang et al. (2020)pointed out that adding a local responsenormalization (LRN) layer and a pooling layer for max-pooling betweenthe 1st and 2nd layer and between the 2nd and 3rd layers effectivelyprevents overﬁtting. This technique achieved a mixed tracking perfor-mance evaluation of 0.730 at a processing speed of 30.53 fps.Apart from the mentioned machine learning and DL techniques,studies have applied other modeling techniques. The transfer function(TF) represents the relationship between the input and the output sig-nals of a control system for all possible input values. The parametersof a TF model can be estimated using several estimation techniquessuch as least squares (LS), state variable ﬁlters approach, instrument variable approach, generalized Poisson moment functions approach,etc. However, due to noise, the model parameters become asymptoti-cally biased when LS is applied. Therefore, Leroy et al. (2006)applied as i m p l iﬁed reﬁned instrumental variable (SRIV) to estimate TF modelparameters from an optimal shape posture parameters (ellipse shapemodel) to determine two dynamic parameters to predict a chicken be-havior depending on the previous behavior. The technique successfullyclassiﬁed scratching, walking, and standing behaviors. As already men-tioned, feature extraction engineering is an arduous task. Therefore,Zaninelli et al. (2018)performed bird recognition from a shape classi ﬁ- cation point of view. A normalized cross-correlation was performed be-tween a processed image and a template to detect multiple nestoccupancy (template comparison).Animals are CIT systems that are individually different and responddifferently at different moments. Therefore, they can't be analyzed as atypical classical steady-state system ( Berckmans, 2006). Additionally, Dawkins et al. (2009)andDawkins et al. (2012)reported that there was no simple association between acceleration and velocity, kinematicfeatures with a bird's GS. Therefore,Nääs et al. (2018)allowed for con- tradictions within a degree of certainty in the estimation of broilerchicken GS using the kinematic features by applying inconsistency-tolerant, Paraconsistent logic.4. Challenges and future directionIn an ideal environment and speciﬁc controlled chicken movements, the current computer vision methodologies provide auspicious results.However, in a real farm environment, the task of monitoring chicken be-comes complicated. Chickens inﬂocks are sometimes occluded by otherchicken, hence, changing their morphological parameters. Additionally,the variation of ambient light conditions and shadows signi ﬁcantly af- fect sensor stability. Therefore, further research and development arenecessary to establish the commercial viability and applicability ofthese poultry monitoring systems. These challenges need to be ad-dressed through combined approaches between livestock science andengineering for improved overall performance and robustness ofchicken monitoring in PLF.4.1. Live weight estimation systemsGood animal welfare is characterized by good health and productiv-ity in livestock production. In comparison to human health, measuringindividual weight and height is a common practice in a clinical check-up. Similarly, in livestock production, live body weight providesimportant information on feed conversion ef ﬁciency, growth, health, body uniformity, and market readiness ( Okinda et al., 2018a; Wongsriworaphon et al., 2015). Additionally, monitoring animal weightduring the entire growing period can be used to assess managementstrategies such as feeding rations and slaughter time. Hence, if the mea-sured weight doesn't coincide with the expected growth curve, then itwould be a clear indication of a problem such as disease occurrencesor other vitality issues for required counter-measures to be undertaken(Mollah et al., 2010;Mortensen et al., 2016). Therefore, animal live weight is an indicator of animal welfare conditions. Thus, it is necessaryto accurately estimate an animal weight and weight distribution of theentire population throughout the rearing period.Table 8Data augmentation techniques.Adopted fromShorten and Khoshgoftaar (2019) .Data augmentation techniques DescriptionsClassiﬁcation MethodGeometrictransformationsFlipping Horizontal and vertical mirroringRotation Rotating an image by ± θaround thecenter of the imageCropping Reducing the size of the input imageTranslation Shifting images left, right, up, or downto avoid positional bias in the dataNoise injection Injecting a matrix of random valuesdrawn from a Gaussian distribution(Gaussian noise)Color Isolating a single-color channel such asR,G,o rBand color histogramsmanipulation (brightness) PhotometrictransformationsColor spacetransformationConversion ofRGBspace to other colorspaces i.e.,HSV,YUV,CMYandLAB. Kernelﬁlters Sharpening Sharpen the image edges by use of high contrast vertical or horizontal edgeﬁlterBlurring Blurring the image by use of Gaussianblur, average blur, uniform blur, andmedianﬁlters Mixing images Mixing imagestogetherProducing a new image by averagingthe pixel values of images. Random erasing DropoutregularizationSelects ann×mpatch of an image andmask it with either 0 s, 255 s, meanpixel values, or random values Deeplearning-basedaugmentationFeature space Noise, extrapolating, and interpolatingby joiningknearest neighbors to formnew instances in lower-dimensionalrepresentations in high-level layersAdversarial training The use of adversarial attacking in arival network to learn augmentationsto images that result inmisclassiﬁcations in its rivalclassiﬁcation network.Generativeadversarial network(GAN)-basedCreation of artiﬁcial instances from adataset in a way that they retain similarcharacteristics to the original datasetNeural style transfer Manipulates the sequentialrepresentations across a CNN, i.e., thatthe style of an image can be transferredto another while preserving its originalcontentMeta-learningschemesApplies a prepended neural network tolearn augmentations via Neural StyleTransfer, mixing images, and geometrictransformations.C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
201Several studies have introduced chicken weighing systems based oncomputer vision based on 2D (Amraei et al., 2017b;D eW e te ta l . ,2 0 0 3; Mollah et al., 2010) and 3D (Mortensen et al., 2016) images. The basic principle of machine vision-based weighing systems is the correlationof image object shape geometric features to animal weight or volume.This is theoretically simple but quite challenging in a real farm environ-ment. As already mentioned,ﬁrstly, the bird's body must be segmentedfrom the background (ROI extraction). Secondly, the chicken's bodysegmented from the image is to be presented by describing characteris-tics (feature extraction). Thirdly, these describing characteristics arecorrelated to the bodyweight by a mathematical model.De Wet et al. (2003)pioneered the application of computer vision inchicken live weight estimation. The study developed linear and nonlin-ear regression models to correlate image area and perimeter geometricfeatures to the real chicken live weight and achieved a relative error of10% and 15% using surface-area and perimeter, respectively. Mollah et al. (2010)developed a similar system but accounted for the bird'sage in their model to achieve 0.999 R
2with the highest relative error of 16.47%. In a real farm environment, chickens always ﬂock together. Therefore, for a practical farm environment application, an automated,robust weighing system should be capable of estimating the live weightof each chicken in theﬂock.Amraei et al. (2017a)andAmraei et al. (2017b)reported having developed a multiple-bird weight estimationsystem based on an ellipseﬁtting technique to localize the chicken ina pen, after which 2D feature extraction was performed. Both studies re-ported an R
2of 0.98 based on ANN and SVM regression models, respec-tively. The mentioned systems were based on visible light-basedsensors (RGB images). However, these sensors are susceptible to varia-tion in ambient light; hence, they are prone to errors ( Okinda et al., 2019).Mortensen et al. (2016)applied the structured infrared-light(IR) based sensor, which is invariant to illumination conditions, to pre-dict the weight of broiler chicken based on both 2D and 3D image fea-tures at an average relative mean error of 7.8%.Poultry weight estimation systems are mainly challenged by varia-tion of ambient lighting conditions and the localization of a bird whenin aﬂock condition. To address the problem of variable light conditions,the solution would be the use of illuminant invariant cameras and ﬂex- ible image sensors in the farm environment. IR-based depth camerassuch as the Microsoft Kinect have been applied in weight estimationbyMortensen et al. (2016). However, IR depth cameras are sensitiveto sunlight, thus, limiting their application to an indoor environment.However, illuminant invariant visual light-based cameras ( Jansen-van Vuuren et al., 2016) are readily available in the market they haven'tbeen applied in poultry monitoring systems. Providing a controlledlighting environment for visual light-based sensors can be another solu-tion, although it's challenging if not infeasible due to farm structure,size, and other complexities. Therefore, the potential research and de-velopment area could be to provide a controlled illumination in thefarm environment for image acquisition in farmhouses and the use of il-luminant invariant cameras.Occlusion and overlap of birds signiﬁcantly affect morphological fea- tures, hence, affecting the performance of the regression model, as dem-onstrated in the study byMortensen et al. (2016), whereby the estimation errors increased as theﬂock density increased. Some studieshave approached this problem from a segmentation point of view. Forexample,Amraei et al. (2017b)applied the ellipseﬁtting technique to segment birds. However, in their presentation, the birds were not oc-cluded as depicted in the study byMortensen et al. (2016), who applied a watershed algorithm using depth distance as the height function. Ad-ditionally, a bird is a non-rigid shape; hence template matching and par-tial shape matching are quite challenging. More future research shouldfocus on efﬁcient ROI segmentation techniques under occlusion andno occlusion. These techniques can be based on both template andnon-template matching and DL techniques. Finally, the instantaneousexpression of chicken behavior leads to shape deformation, such as ﬂip- ping of wings, hence, affecting the model performance. The strategywould be to incorporate behavior with weight estimation. Such thatthe introduced system should be invariant to instantaneous behaviorexpressions.4.2. Lameness detection systemsThe occurrence of lameness affects the mobility of any legged crea-ture. The term mobility refers to the quality or the state of being mobileor the ability to move. Generally, mobility is associated with walking orlocomotion. In poultry, immobility is often a sign of chickens experienc-ing some discomfort. These discomforts may result from skeleton (leg)disorders, nutrition deﬁciencies and leg health (dermatitis), infestations(lice and mite), and diseases (Bessei, 2006;Bradshaw et al., 2002; Butcher et al., 1999;Knowles et al., 2008;Paul-Murphy and Hawkins, 2014). These factors can be categorized as genetical factors and environ-mental factors (illumination, bedding, ventilation, diseases, and stock-ing density) (Almeida Paz et al., 2010;Bessei, 2006;Knowles et al., 2008;Reiter and Bessei, 1997;Rozenboim et al., 2004;Tablante, 2013). Mobility is an important aspect of a living bio-organism. Being mo-bile is often perceived as beingﬁt and in good health. Moreover, difﬁ- culty in walking by birds can result in starvation, thus, affecting thefeed conversion ratio in terms of weight and growth, chest soiling,hock burns conditions, and being an easy target to be preyed on(Kestin et al., 2001;Paul-Murphy and Hawkins, 2014;Weeks et al., 2000). Additionally, leg disorders increase mortality culling, condemna-tions, and downgrades from trimming, which account for considerableeconomic losses. Furthermore, according to Welfare-Quality® (2009), the occurrence of the factors mentioned above is indicators of poor an-imal welfare conditions. Thus, monitoring the level of a bird's mobilityprovide an assessment for its welfare condition. Thorp and Duff (1988)described lameness as a range of injuries resulting from infectiveand non-infective sources. Additionally, based on pathological condi-tions resulting in lameness and leg weakness Bradshaw et al. (2002) classiﬁed poultry leg disorders as infectious, developmental, and degen-erative. Therefore, the general term describing the inability to walk nor-mally due to illness or injury affecting the foot or leg is lameness and legweakness.As already mentioned, despite the successes of the kinetic tech-niques, they were time-consuming, had a lot of data redundancy, andcould not provide continuous and automatic monitoring of birds.Hence, it can't be used as an early detection method. Kinematic moni-toring of birds was initially introduced by Abourachid (1991)to analyze the GS of turkeys. The same approach was applied by Caplen et al. (2012)to contrast the GS of broiler chickens and jungle fowl by theuse of a 3D temporospatial poultry walk information acquisition system.In comparison, the study established that the jungle fowl had a betterGS due to fast growth issues in broilers, which promotes a compensa-tory gait adaptation to minimize walking energy which triggers lame-ness. Additionally, lame broilers were observed to have a lowerwalking velocity and exhibited walking instability. A comparison ofthe effect of NSAID administration on lames by Caplen et al. (2013) established that there was an increase in the chicken walk speed afterNSAID administration and concluded that the model could be useful inassessing lameness-associated pain in broiler chickens. Using aparaconsistent logic,Nääs et al. (2018)tracked the centroid of a chicken to compute its kinematic features (velocity and acceleration) toestimate the GS of broilers. Despite the success of kinematic analysisas a computer vision technique, it suffered a couple of setbacks,i.e., markers on the skin locations being displaced during movement, re-quired the bird to walk parallel to the camera for accurate measure-ments to be taken, and this technique was both intrusive and invasiveto the birds.In the analysis of locomotor patterns of chickens, layers' body movesin a straight line because their legs are always under the center of grav-ity of their body. However, for broilers, their center of gravity moves lat-erally towards the supporting leg. Therefore, the differences inC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
202horizontal and vertical movements of the left and right legs, cycle periodof feet movements, irregularities in the motion of the center of gravity ofthe body, and frequency of body center oscillation can be evaluated todetect lameness in birds (Reiter and Bessei, 1997). Based on theseﬁnd- ings, alternatives to kinematic systems that could provide non-intrusive, non-invasive, automatic, and continuous systems for early de-tection of lameness in chicken were developed. A fully automated imagemonitoring technique byAydin et al. (2010)was capable of measuring the activities of broiler chickens and relating these activities with theirGS levels. This study established a signi ﬁcant relationship between manual GS and broiler activities, with higher GS having lower activitiesand GS3 being the most active. In the spatial use of mixed broilers, Aydin et al. (2013)returned the same results that GS0 to GS3 had more move-ments than all higher GS. Based on the LTL test and NOL, Aydin et al. (2015)introduced another automated image monitoring system forlameness detection. The two features (LTL and NOL) were then com-pared to the manual GS and established that NOL was positively signif-icantly correlated to GS. In contrast, LTL was negatively correlated withGS. These results indicated that this system could be used as a tool to as-sess lameness in broilers automatically. Taking the advantages of 3Ddepth sensorAydin (2017a)computed LTL and NOL using imagedepth information. The results obtained were almost similar to thelater study in that NOL was positively signi ﬁcantly correlated to GS (R
2= 0.934), while LTL was negatively signi ﬁcantly correlated to GS (R
2= 0.949). Theseﬁndings justiﬁed the use of a 3D vision monitoringtechnique as a method of assessing lameness in broiler chicken. Themost recent study ofAydin (2017b)based on kinematic parameters (lateral body oscillation, step length, step frequency, and walk speed)reported a correlation between the GS and these parameters at r= 0.861, 0.882, 0.831, 0.844, respectively. The study further established astatistical signiﬁcance in all the feature parameter as a measure of lame-ness (regarding GS), hence, this system can be used to provide an earlydetection of lameness in broilers.Variable light conditions and occlusion problems are also a hin-drance to lameness detection systems in birds. Additionally, as muchas kinematic posture trackers have yielded positive results, more re-search should be undertaken in the development of automated bodyposition trackers without the use of markers (non-intrusive and non-invasive systems). Moreover, the applicable camera position in a realfarm environment is still a challenge. Overhead camera positions aremost preferred, i.e., non-invasive.Aydin (2017a)andAydin (2017b)ap- plied overhead depth and RGB images, respectively. However, as al-ready mentioned, IR depth sensors are susceptible to sunlight, hence,limited to indoor applications or would limit the operation time of thesystem if it is installed in an outdoor environment. Additionally, RGBcameras are associated with visual light-based sensor errors.Furthermore, these experiments were conducted in a controlled en-vironment, whereby the birds' movements were restricted. Therefore,further research should focus on lameness detection of chickens in aﬂock setting such that occlusion problems and dynamic movementsare considered. Lastly, lameness in chicken is affected by several factors.However, the easiest to control from a stockman perspective are the en-vironmental factors such as illumination, bedding, ventilation, andstocking density. Therefore, more research should focus on the opti-mum environmental conditions for chicken regarding lameness.4.3. Health status classiﬁcation systemsAccording toWelfare-Quality® (2009), the health condition of a bird is a vital indicator of good welfare practice. The term being healthy canbe characterized by the absence of a disease, whereby disease is anycondition that causes a deviation from normal activities and functions.Poultry disease occurs due to the interaction between the birds, the en-vironment, and the infection agent (non-infectious and infectious). In-fectious agents include viruses, bacteria, fungi, and parasites, whilenon-infectious agents include chemical and physical toxins anddeﬁciency or excess of minerals and vitamins. The host factors includethe bird's age, sex, breed, and immune status. Environmental factors,which are also management factors, include air quality and ventilation,stocking density, sanitation, feed quality, lighting program, and medica-tion and vaccination programs (Tablante, 2013). Poultry diseases can be categorized as Respiratory (Newcastle Disease, Fowl Pox, Avian In ﬂu- enza, Infectious Bronchitis, Infectious Laryngotracheitis, Infectious Co-ryza, Aspergillosis, Swollen Head Syndrome), Viral Diseases (i.e., non-respiratory) (Marek's Disease, Infectious Bursal Disease, Lymphoid Leu-kosis, Avian Encephalomyelitis), and Non-respiratory Bacterial Diseases(Fowl Cholera, Necrotic Enteritis, Omphalitis, Ulcerative Enteritis, Botu-lism, Pullorum, Staphylococcus) (Butcher et al., 1999).Butcher et al. (1999)andTablante (2013)made a presentation on common poultrydiseases and infections, visual symptoms for each disease, and preven-tion and control. Furthermore,Damerow (2016)outlined how to recognize sick poultry by observation, dropping examination, and post-mortem examination. Current computer vision systems perform healthclassiﬁcation based on the behavior (posture) ( Okinda et al., 2019; Zhang and Chen, 2020;Zhuang et al., 2018;Zhuang and Zhang, 2019), chicken droppings (Wang et al., 2019b), locomotor (Okinda et al., 2019), and opticalﬂow (Roberts et al., 2012). Opticalﬂow measures were used as an early detection system topredict the mortality, hock burn, and GS of birds using a Bayesian re-gression model (Roberts et al., 2012).Zhuang et al. (2018)correlated skeleton features of broiler posture images taken from the side at an ac-curacy of 99.469%. Due to the applicable camera position in a real farmenvironment,Okinda et al. (2019)extracted overhead image postureand achieved an accuracy of 0.978. Nevertheless, these researcheswere performed in a controlled environment, where variation in illumi-nation was not factored in, and the birds' behaviors and activities werecontrolled. DL detectors byZhuang and Zhang (2019)andZhang and Chen (2020)gave out satisfactory results of mean average precision of99.7% and 93.7%, respectively. The main challenge in DL detection ofchicken is the lack of an appropriate dataset. Zhuang and Zhang (2019)expressed that the currently available bird's datasets do nothave a speciﬁc category called broilers; hence, it resulted in low recog-nition accuracy. Therefore, more research should be undertaken to de-velop a poultry dataset with speciﬁc categories such as broilers, layers,chicks, etc., that can be applied in poultry detection systems. However,this would be labor-intensive due to the retraining process of new appli-cations. Hence, much research should be focused on new learning tech-niques, i.e., adaptive learning and semi-supervised learning. In the studybyWang et al. (2019b)in the detection of digestive diseases in broilersbased on color and viscosity of the droppings. However, the color varia-tions of dropping can also result from the type of feed. Similarlydropping viscosity will also vary depending on water intake. Thus, thistechnique would be challenging to apply in free range chickens orchickens with a diverse feeding program. Furthermore, this would notprovide an early detection of disease occurrences. Body temperature isanother important parameter in the evaluation of an animal's healthstatus. However, temperature monitoring hasn't been widely appliedin poultry. Nevertheless,Xiong et al. (2019)presented a system that could extract the temperature of the head region of a broiler from ther-mal images. Therefore, more research should be directed towardsregion-based temperature detection for infection detection in poultry.4.4. Poultry tracking systemsTracking of poultry is an essential parameter in the assessment of be-havioral (types of activities) and physical (lameness and health) indica-tors in poultry welfare. There is a need to automatically record thebehavior and movement of birds continuously for welfare monitoringpurpose and behavior phenotyping.Noldus and Jansen (2004)catego- rized automated video tracking systems as analog and digital videotracking systems. Analog systems detected high peaks in the voltageof a video signal, i.e., regions of high contrast between the bird andC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
203background. However, these systems could only track one bird in a ded-icated experiment set up unit with restricted illumination and back-ground conditions. Digital systems allowed for pattern recognitiontechniques to be applied to image frames for the quantitative measure-ment of the birds. However, this system is limited by the computationalspeed and the complexity of the underlying software.Digital image animal tracking systems such as EthoVision have longbeen applied to several laboratory animals since the 1990s in trackingand behavior classiﬁcation (Noldus et al., 2001). The system's steps of operation include object identiﬁcation (size and color), feature extrac-tion, feature changes from the previous frame, and tracking and behav-ior detection. The chicken tracking system was pioneered by Sergeant et al. (1998), who performed tracking based on centroid detection andcurvature analysis to separate touching birds. However, centroid track-ing was challenging due to several factors such as; if a bird moves fasterthan the threshold value, the tracked centroid was lost on some occa-sions, the centroid was assigned to the noise region, total occlusion(bird squeezes under another bird) led to an ambiguity such as thetracked centroids interchanged.Fujii et al. (2009)applied a particleﬁl- ter algorithm to track poultry. The system applied two trackers,i.e., poultry trackers and exploring trackers. The former detected thechicken's location while the latter searched and corrected a failed poul-try tracker. Similar to the study bySergeant et al. (1998), absolute occlu- sion and quick movements of birds were signi ﬁcant problems in this technique. Additionally, heads and tails could be detected as the poultrybody during ellipse modeling. Furthermore, ambient light variation alsoinﬂuenced the performance of this system. Despite the ellipse ﬁtting technique byKashiha et al. (2014)reporting a superior performance,it lost track when the chicken moved very fast. Moreover, this techniquewas applied to individual birds and not as ﬂocks. For multiple birds tracking,Nakarmi et al. (2014)incorporated Radio Frequency Identiﬁ- cation (RFID) to identify and track birds when the vision system failedto maintain the identities of the tracked birds. However, tagging ofbirds with physical components is invasive and therefore affects theirnatural behaviors. In a comparative approach, Wang et al. (2016)intro- duced the hybrid support vector machine. They compared it to TLD(Tracking-Learning-Detection), the MeanShift Algorithm, the PLS (ob-ject tracking via partial least squares analysis), the Particle Filter Algo-rithm, and the Frag (fragment-based tracking method) in the trackingof chicken. A similar approach was presented by Fang et al. (2020), who applied deep regression network. The two techniques were robustand performed efﬁciently in chicken tracking in aﬂock setup. However, they were single object detection (only one bird tracking problem) andnot a multi-objects detection problem, hence, could only detect andtrack one bird in aﬂock. More research and developments should beperformed to develop a multi-object tracking such that several birdscan be tracked simultaneously in aﬂock. Multi-object detection has re-cently received a lot of recognition and has already been applied in sickbroiler detection byZhuang and Zhang (2019). Additionally, more re- search should be driven towards DL networks using non-visible light-based sensors to eliminate the illumination variation problems and toallow for tracking to be performed even during dark hours (lightingregime).4.5. Behavior monitoring systemsThe discipline that is closely related to animal welfare is the animalbehavior and is considered as behavioral indicators in welfare assess-ment. In a good welfare condition, the animals should be able to expresstheir natural behavior patterns due to no or minimal stress. However,studies have reported the dif
ﬁculty in differentiating between standardphysiological stress and productive indicators of stress, and at timescontradicted each other (Marıa et al., 2004). Additionally, sampling techniques or direct observation are invasive despite being the initialpoint for the development, validation, and implementation of non-invasive automated behavioral systems. Nevertheless, several poultrybehavior monitoring systems based on computer vision have beenintroduced.From a complex system approach,Marıa et al. (2004)expressed that the complexity of the behavior of animals reduces with stress. The studyapplied fractal analysis rather than a conventional Euclidean geometryto the quantiﬁcation of temporal heterogeneity of time series behavioralsequences. Opticalﬂow analysis has also been used to correlate the op-ticalﬂow measures to behavior and GS.Dawkins et al. (2009)presented that a higher mean opticalﬂow is associated with greater bird activitiesin terms of striding and walking rate. Additionally, behavior was highlysigniﬁcantly correlated to GS. In another study by Dawkins et al. (2012), mean opticalﬂow was negatively related toﬂock mortality, while kur- tosis and skew were both positively correlated to mortality, GS, andhock burn.Dawkins et al. (2013)pointed out that there exists no simpleconnection between opticalﬂow and behavior when they found thatthe mean opticalﬂow was not negatively correlated to birds sitting orlying nor a positive correlation between birds walking and opticalﬂow. However, mortality, GS, leg health were positively correlated tobirds sitting or lying and negatively correlated with birds walking.Colles et al. (2016)applied the same technique to detect Campylobacterinfected chicken; the study established a higher mean and lower kurto-sis for infected birds. Despite the success of the optical ﬂow analysis, they only presented the relationships between welfare indicators, be-haviors, and opticalﬂow measures but not the type of behavior.More research has been focused on developing behavior type recog-nition techniques based on developed ethograms as in the study byPereira et al. (2013)andMarıa et al. (2004).Leroy et al. (2005)devel- oped a dynamic model to recognize six different laying hens' behaviors(sleeping, standing, sitting, grooming, pecking, and scratching). A simi-lar approach was presented byPereira et al. (2013), who applied a clas- siﬁcation tree to identify nine chicken behaviors (Wing spreading,Drinking, Bristling, Resting, Scratching, Stretching, Mounting, Preening,and Inactivity). To eliminate the process of feature extraction engineer-ing and errors associated with visible light-based sensors, Pu et al. (2018)proposed two parameter-sharing CNNs for both RGB anddepth images to classifyﬂock behaviors at the feeders as non-crowded, a little crowded, and fairly crowded. A more straightforwardtechnique to determine the number of birds at the feeder and drinkerunder temporal and spatial preferences was presented by Li et al. (2019b). However, the technique suffered occlusion problems. Behaviorchanges are instantaneous; therefore, real-time monitoring systems areof great signiﬁcance.Wang et al. (2020)capitalized on the fast speed of YOLO v3 to develop a real-time behavior detector capable of classifyingsix behaviors mating, standing, feeding, spreading, ﬁghting, and drinking.More research and development should be focused on developingbehavior detection systems with illumination invariancy, factors in thebackground complexity, overlapping, and occlusion problems. The ap-plication of improved YOLO v3 has solved these problems, referred toas the YOLO v3-dense model (Tian et al., 2019), whereby DenseNet is used to process feature layers with compromised images (low resolu-tion, occluded objects). Similarly, the speed and performance of thereal-time YOLO v3 system can be improved by extending the detectionscale and down-sampling of feature fusion target detection layer ( Ju et al., 2019). The mentioned improvements haven't been applied inchicken monitoring CNN-based systems. Therefore, more researchshould be performed to improve the performance of real-time detectionmodels.4.6. Activities and other monitoring systemsAnimal activity is highly associated with behavior levels, GS, andhealth. In computer vision systems, activity is measured as percentagepixel change over the total area coverage over a period of time,i.e., the higher the activity levels, the higher the difference in pixelvalues. EthoVision XT and eYeNamic are software that can directlyC. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
204compute the activity of birds from surveillance video input and has beenapplied in several studies (Aydin et al., 2010;Fraess et al., 2016;Van Hertem et al., 2018). The activity index of birds was assessed with rela-tion to thermal stress byBloemen et al. (1997). During cold stress, the birds huddled together, while during thermal stress, the chickens occu-pied moreﬂoor space. A similar approach was presented by Kristensen et al. (2006)to evaluate the relation between illumination intensity andbroiler activity. The broilers' activities were signi ﬁcantly higher during high-intensity periods. eYeNamic software was applied by Aydin et al. (2010)to correlate the activity index to GS. The study established thathigher GS had signiﬁcantly lower activities. Similar results wereachieved byAydin et al. (2013)in the spatial use of mixed chickens.To control chicken activities in a pen, similar to the work by Bloemen et al. (1997),Youssef et al. (2015)compared the dynamic variations ofthe activity index of chickens to a 2D spatial pro ﬁle of airﬂow and the temperature pattern inside the pen. The study reported that duringcold stress, the birds occupied low air velocity zones, while during ther-mal stress, the birds occupied high air velocity zones. The activity levelandﬂock distribution data were used to determine the GS of birds byVan Hertem et al. (2018)based on eYeNamic software analysis. Thestudy reported that GS and activity were negatively correlated, whileGS andﬂock distribution was positively correlated. Therefore, ﬂock GS could be predicted from continuous monitoring of ﬂocks by video sur- veillance. Similarly, the animal distribution index was computed byeYeNamic software byKashiha et al. (2013)to detect any problem in a broiler house such as thermal discomfort, insuf ﬁcient feeds, and water. The studies mentioned above categorized any changes in aﬂock as activity without considering the intrinsic properties of changenor the type of changes occurring. Therefore, more research should befocused on activity deviation with behavior changes regarding welfareparameters.Other computer vision-based monitoring systems such as ﬂoor dis- tribution monitoring at drinking and feeding areas ( Guo et al., 2020), ef- fect of feeder types (Neves et al., 2015), type of light illuminance (Kristensen et al., 2007), backpack (Stadig et al., 2018) on bird's behav- ior have also been presented regarding activity and behavior monitor-ing. However, animal behavior is a complex bio-response to bothinternal and external stimuli. Therefore, more research should be di-rected towards the drivers of behavioral responses such as pen con-struction designs and materials and structures inside the pen as wellas health and micro-environment.5. ConclusionsThis review presents a summary of the current monitored bio-processes and bio-responses, how they qualify as welfare indicators,and the computer vision techniques applied in the surveillance andmonitoring of these bioprocesses and bio-responses, the challenges in-volved, and possible solutions to these challenges. Both machine visionand DL techniques were discussed.For conventional machine learning, the ﬁ
ve procedures, i.e., preprocessing, segmentation, feature extraction, feature selection,and classiﬁcation or regression, were discussed in detail. The dif ﬁculty of poultry monitoring lies in foreground detection due to the complexbackground, variations in illumination, and occlusion problems in areal farm environment. Several solutions have been proposed, throughthe application of non-visible light-based sensors, restriction of imageacquisition time, factoring in the animal behavior, and using depth-based sensors for easier separation of occluded birds. The extractedROI, i.e., the bird, can then be represented by the feature vectors; fourfeatures can be used for this task, i.e., morphological, locomotor, opticalﬂow, and other features. These features are always mixed, and a dimen-sion reduction or feature selection engineering is applied to create a ro-bust and more generalized model. The ﬁnal modeling procedure is conducted using regression or classiﬁcation-based machine learningapproaches. For DL, the tasks of segmentation, feature extraction, andfeature selection are eliminated by the use of CNNs.Much success has been achieved in animal monitoring systems.However, there exist several challenging factors for real farm applica-tions (occlusion, lighting condition, etc.). Several studies have presentedpossible solutions. These studies presented good results in dedicatedenvironments, hence, compromised a robustness and generalizationability of these systems. DL approaches have great potential. However,they require a vast amount of labeled dataset as an image dataset withground truth annotations, and samples are of great signi ﬁcance in model development and for testing of algorithms.Generally, appropriate image processing algorithms in computer vi-sion are essential for the poultry monitoring in the farm environmentfor precise localization of birds. This will be pivotal in the monitoringof several bioprocesses and bio-responses and also provide a solutionto occlusion problems. Even though several challenges still exist, moreresearches are being performed to improve the monitoring systems inpoultry.Declaration of competing interestAll authors declare that they have no conﬂict of interest.AcknowledgmentsThe project was funded by China National Key Research and Devel-opment Project (Grant No. 2017YFD0701602-2).ReferencesAbourachid, A., 1991. Comparative gait analysis of two strains of turkey, Meleagrisgallopavo. Br. Poult. Sci. 32, 271 –277.https://doi.org/10.1080/00071669108417350 . Alm, M., Tauson, R., Holm, L., Wichman, A., Kalliokoski, O., Wall, H., 2016. Welfare indica-tors in laying hens in relation to nest exclusion. Poult. Sci. 95, 1238 –1247.https://doi. org/10.3382/ps/pew100.Almeida Paz, I.C.L., Garcia, R., Bernardi, R., Nääs, I., Caldara, F.R., Freitas, L.W., Seno, L.,Ferreira, V., Pereira, D.F., Cavichiolo, F., 2010. Selecting appropriate bedding to reducelocomotion problems in broilers. Brazilian J. Poult. Sci. 12, 189 –195.https://doi.org/ 10.1590/S1516-635X2010000300008 . Alpaydin, E., 2020.Introduction to Machine Learning. MIT press.Amraei, S., Abdanan Mehdizadeh, S., Salari, S., 2017a. Broiler weight estimation based onmachine vision and artiﬁcial neural network. Br. Poult. Sci. 58, 200 –205.https://doi. org/10.1080/00071668.2016.1259530 . Amraei, S., Mehdizadeh, S.A., Sallary, S., 2017b. Application of computer vision and sup-port vector regression for weight prediction of live broiler chicken. Eng. Agric. Envi-ron. food 10, 266–271.https://doi.org/10.1016/j.eaef.2017.04.003 . Amraei, S., Mehdizadeh, S.A., Nääs, I. de A., 2018. Development of a transfer function forweight prediction of live broiler chicken using machine vision. Eng. Agrícola 38,776–782.https://doi.org/10.1590/1809-4430-eng.agric.v38n5p776-782/2018 . Anandan, P., 1989. A computational framework and an algorithm for the measurement ofvisual motion. Int. J. Comput. Vis. 2, 283 –310.https://doi.org/10.1007/BF00158167 . Aydin, A., 2017a. Using 3D vision camera system to automatically assess the level of inac-tivity in broiler chickens. Comput. Electron. Agric. 135, 4 –10.https://doi.org/10.1016/ j.compag.2017.01.024.Aydin, A., 2017b. Development of an early detection system for lameness of broilers usingcomputer vision. Comput. Electron. Agric. 136, 140 –146.https://doi.org/10.1016/j. compag.2017.02.019.Aydin, A., Cangar, O., Ozcan, S.E., Bahr, C., Berckmans, D., 2010. Application of a fully auto-matic analysis tool to assess the activity of broiler chickens with different gait scores.Comput. Electron. Agric. 73, 194 –199.https://doi.org/10.1016/j.compag.2010.05.004 . Aydin, A., Pluk, A., Leroy, T., Berckmans, D., Bahr, C., 2013. Automatic identi ﬁcation of ac- tivity and spatial use of broiler chickens with different gait scores. Trans. ASABE 56,1123–1132.https://doi.org/10.13031/trans.56.9987 . Aydin, A., Bahr, C., Berckmans, D., 2015. Automatic classi ﬁcation of measures of lying to assess the lameness of broilers. Anim. Welf. 24, 335 –343.https://doi.org/10.7120/ 09627286.24.3.335.Balduzzi, D., Frean, M., Leary, L., Lewis, J.P., Ma, K.W.-D., McWilliams, B., 2017. The Shattered Gradients Problem: If resnets are the answer, then what is the question?International Conference on Machine Learning, pp. 342 –350 B a n h a z i ,T . M . ,L e h r ,H . ,B l a c k ,J . L . ,C r a b t r e e ,H . ,S c h o ﬁeld, P., Tscharke, M., Berckmans, D., 2012. Precision livestock farming: an international review of scienti ﬁca n d
commercial aspects. Int. J. Agric. Biol. Eng. 5, 1 –9.https://doi.org/10.3965/j. ijabe.20120503.001.Barron, J.L., Fleet, D.J., Beauchemin, S.S., 1994. Performance of optical ﬂow techniques. Int. J. Comput. Vis. 12, 43–77.https://doi.org/10.1007/BF01420984 . Berckmans, D., 2006. Automatic on-line monitoring of animals by precision livestockfarming. Livest. Prod. Soc. 287. https://doi.org/10.3920/978-90-8686-567-3 .C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
205Berckmans, D., 2014. Precision Livestock Farming Technologies for Welfare Managementin Intensive Livestock Systems. https://doi.org/10.20506/rst.33.1.2273 . Berckmans, D., 2017. General introduction to precision livestock farming. Anim. Front. 7,6–11.https://doi.org/10.2527/af.2017.0102 . Berg, C., Sanotra, G.S., 2003.Can a modiﬁed latency-to-lie test be used to validate gait- scoring results in commercial broiler ﬂocks? Anim. Welf. 12, 655 –659. Bessei, W., 2006. Welfare of broilers: a review. Worlds. Poult. Sci. J. 62, 455 –466.https:// doi.org/10.1017/S0043933906001085 . Bessei, W., 2018. Impact of animal welfare on worldwide poultry production. Worlds.Poult. Sci. J. 74, 211–224.https://doi.org/10.1017/S0043933918000028 . Bhargava, A., Bansal, A., 2018. Fruits and vegetables quality evaluation using computer vi-sion: a review. J. King Saud Univ. Inf. Sci. https://doi.org/10.1016/j.jksuci.2018.06.002 . Bloemen, H., Aerts, J., Berckmans, D., Goedseels, V., 1997. Image analysis to measure activ-ity index of animals. Equine Vet. J. 29, 16 –19.https://doi.org/10.1111/j.2042- 3306.1997.tb05044.x.Bokkers, E.A.M., Zimmerman, P.H., Rodenburg, T.B., Koene, P., 2007. Walking behaviour ofheavy and light broilers in an operant runway test with varying durations of feeddeprivation and feed access. Appl. Anim. Behav. Sci. 108, 129 –142.https://doi.org/ 10.1016/j.applanim.2006.10.011 . Bradshaw, R.H., Kirkden, R.D., Broom, D.M., 2002. A review of the aetiology and pathologyof leg weakness in broilers in relation to welfare. Avian Poult. Biol. Rev. 13, 45 –103. https://doi.org/10.3184/147020602783698421 . Butcher, G.D., Jacob, J.P., Mather, F.B., 1999. Common poultry diseases. PS47-series Vet. Med. Anim. Clin. Sci. Dep. UF/IFAS Ext.Caplen, G., Hothersall, B., Murrell, J.C., Nicol, C.J., Waterman-Pearson, A.E., Weeks, C.A.,Colborne, G.R., 2012. Kinematic analysis quanti ﬁes gait abnormalities associated with lameness in broiler chickens and identi ﬁes evolutionary gait differences. PLoS One 7, e40800.https://doi.org/10.1371/journal.pone.0040800 . Caplen, G., Colborne, G.R., Hothersall, B., Nicol, C.J., Waterman-Pearson, A.E., Weeks, C.A.,Murrell, J.C., 2013. Lame broiler chickens respond to non-steroidal anti-inﬂammatory drugs with objective changes in gait function: a controlled clinicaltrial. Vet. J. 196, 477–482.https://doi.org/10.1016/j.tvjl.2012.12.007 . Chandrashekar, G., Sahin, F., 2014. A survey on feature selection methods. Comput. Electr.Eng. 40, 16–28.https://doi.org/10.1016/j.compeleceng.2013.11.024 . Chavolla, E., Zaldivar, D., Cuevas, E., Perez, M.A., 2018. Color spaces advantages and disad-vantages in image color clustering segmentation. Advances in Soft Computing andMachine Learning in Image Processing. Springer, pp. 3 –22https://doi.org/10.1007/ 978-3-319-63754-9_1.Cheng, H.-D., Jiang, X.H., Sun, Y., Wang, J., 2001. Color image segmentation: advances andprospects. Pattern Recogn. 34, 2259 –2281.https://doi.org/10.1016/S0031-3203(00) 00149-7.Chollet, F., 2017.Xception: deep learning with depthwise separable convolutions. Pro-
ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 1251–1258.Colles, F.M., Cain, R.J., Nickson, T., Smith, A.L., Roberts, S.J., Maiden, M.C.J., Lunn, D.,Dawkins, M.S., 2016. Monitoring chicken ﬂock behaviour provides early warning of infection by human pathogen Campylobacter. Proc. R. Soc. B Biol. Sci. 283,20152323.https://doi.org/10.1098/rspb.2015.2323 . Corr, S.A., McCorquodale, C.C., Gentle, M.J., 1998. Gait analysis of poultry. Res. Vet. Sci. 65,233–238.https://doi.org/10.1016/S0034-5288(98)90149-7 . Corr, S.A., McCorquodale, C., McDonald, J., Gentle, M., McGovern, R., 2007. A force platestudy of avian gait. J. Biomech. 40, 2037 –2043.https://doi.org/10.1016/j. jbiomech.2006.09.014.Cortes, C., Vapnik, V., 1995. Support-vector networks. Mach. Learn. 20, 273 –297.https:// doi.org/10.1007/BF00994018. Damerow, G., 2016.The Chicken Health Handbook: A Complete Guide to MaximizingFlock Health and Dealing With Disease. Storey Publishing.Dawkins, M.S., 2017. Animal welfare and ef ﬁcient farming: is conﬂict inevitable? Anim. Prod. Sci. 57, 201–208.https://doi.org/10.1071/AN15383 . Dawkins, M.S., Lee, H., Waitt, C.D., Roberts, S.J., 2009. Opticalﬂow patterns in broiler chickenﬂocks as automated measures of behaviour and gait. Appl. Anim. Behav.Sci. 119, 203–209.Dawkins, M.S., Cain, R., Roberts, S.J., 2012. Optical ﬂow,ﬂock behaviour and chicken wel- fare. Anim. Behav. 84, 219 –223.https://doi.org/10.1016/j.anbehav.2012.04.036 . Dawkins, M.S., Cain, R., Merelie, K., Roberts, S.J., 2013. In search of the behavioural corre-lates of opticalﬂow patterns in the automated assessment of broiler chicken welfare.Appl. Anim. Behav. Sci. 145, 44 –50.https://doi.org/10.1016/j.applanim.2013.02.001 . Dawkins, M., Roberts, S.J., Cain, R., Nickson, T., Donnelly, C., 2017. Early warning of footpaddermatitis and hockburn in broiler chicken ﬂocks using opticalﬂow, body weight and water consumption. Vet. Rec. 180. https://doi.org/10.1177/0967033520927519 . De Wet, L., Vranken, E., Chedad, A., Aerts, J.-M., Ceunen, J., Berckmans, D., 2003. Computer-assisted image analysis to quantify daily growth rates of broiler chickens. Br. Poult.Sci. 44, 524–532.https://doi.org/10.1080/00071660310001616192 . Duda, R.O., Hart, P.E., Stork, D.G., 2001. Unsupervised learning and clustering. Pattern Classif. 517–601.Dyson, T., 1999. World food trends and prospects to 2025. Proc. Natl. Acad. Sci. 96,5929–5936.https://doi.org/10.1073/pnas.96.11.5929 . Fang, C., Huang, J., Cuan, K., Zhuang, X., Zhang, T., 2020. Comparative study on poultry tar-get tracking algorithms based on a deep regression network. Biosyst. Eng. 190,
176–183.https://doi.org/10.1016/j.biosystemseng.2019.12.002 . FAO, 2018.World Food And Agriculture Statistical Pocketbook 2018.Faucitano, L., 2018. Meat science and muscle biology symposium: international perspec-tives on animal handling and welfare and meat quality preslaughter handling prac-tices and their effects on animal welfare and pork quality. J. Anim. Sci. https://doi. org/10.1093/jas/skx064.Fraess, G.A., Bench, C.J., Tierney, K.B., 2016. Automated behavioural response assessmentto a feeding event in two heritage chicken breeds. Appl. Anim. Behav. Sci. 179,74–81.https://doi.org/10.1016/j.applanim.2016.03.002 . Fujii, T., Yokoi, H., Tada, T., Suzuki, K., Tsukamoto, K., 2009. Poultry tracking system withcamera using particleﬁlters. 2008 IEEE International Conference on Robotics and Bio-mimetics. IEEE, pp. 1888 –1893https://doi.org/10.1109/ROBIO.2009.4913289 . Fujiyoshi, H., Hirakawa, T., Yamashita, T., 2019. Deep learning-based image recogni- tion for autonomous driving. IATSS Res. 43, 244 –252.https://doi.org/10.1016/j. iatssr.2019.11.008.Gerland, P., Raftery, A.E., Ševčíková, H., Li, N., Gu, D., Spoorenberg, T., Alkema, L., Fosdick,B.K., Chunn, J., Lalic, N., 2014. World population stabilization unlikely this century.Science (80-.) 346, 234 –237.https://doi.org/10.1126/science.1257469 . Girshick, R., 2015.Fast r-cnn. Proceedings of the IEEE International Conference on Com-puter Vision, pp. 1440–1448.Girshick, R., Donahue, J., Darrell, T., Malik, J., 2014. Rich feature hierarchies for accurate ob- ject detection and semantic segmentation. Proceedings of the IEEE Conference onComputer Vision and Pattern Recognition, pp. 580 –587. González, R.C., Woods, R.E., Eddins, S.L., 2004. Digital Image Processing Using MARLAB. Pearson.Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. MIT press. Green, L.E., Kaler, J., Wassink, G.J., King, E.M., Grogono Thomas, R., 2012. Impact ofrapid treatment of sheep lame with footrot on welfare and economics andfarmer attitudes to lameness in sheep. Anim. Welf. 21, 65 –71.https://doi.org/ 10.7120/096272812X13345905673728 . Gulli, A., Pal, S., 2017.Deep Learning With Keras. Packt Publishing Ltd.Guo, Y., Chai, L., Aggrey, S.E., Oladeinde, A., Johnson, J., Zock, G., 2020. A machine vision-based method for monitoring broiler chicken ﬂoor distribution. Sensors 20, 3179. https://doi.org/10.3390/s20113179 . Hamuda, E., Mc Ginley, B., Glavin, M., Jones, E., 2017. Automatic crop detection under ﬁeld conditions using the HSV colour space and morphological operations. Comput. Elec-tron. Agric. 133, 97–107.https://doi.org/10.1016/j.compag.2016.11.021 . Haque, I.R.I., Neubert, J., 2020. Deep learning approaches to biomedical image segmenta-tion. Informatics Med. Unlocked 18, 100297. https://doi.org/10.1016/j. imu.2020.100297.Hastie, T., Tibshirani, R., Friedman, J., 2009. Unsupervised learning. The Elements of Statis- tical Learning. Springer, pp. 485 –585. He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. Pro-ceedings of the IEEE Conference on Computer Vision and Pattern Recognition,pp. 770–778.He, K., Gkioxari, G., Dollár, P., Girshick, R., 2017. Mask r-cnn. Proceedings of the IEEE Inter- national Conference on Computer Vision, pp. 2961 –2969. Healy, S., 2018.Consumers, corporate policy and animal welfare. The Business of FarmAnimal Welfare. ROUTLEDGE in association with GSE Research, pp. 64 –72. Heise, H., Theuvsen, L., 2018. Citizens ’understanding of welfare of animals on the farm: an empirical study. J. Appl. Anim. Welf. Sci. 21, 153 –169.https://doi.org/10.1080/ 10888705.2017.1400439.Hemsworth, P.H., Coleman, G.J., 2010. Human-livestock interactions: the stockperson andthe productivity of intensively farmed animals. CABI. https://doi.org/10.1080/ 00480169.2014.966167.Hemsworth, P.H., Mellor, D.J., Cronin, G.M., Tilbrook, A.J., 2015. Scientiﬁc assessment of animal welfare. N. Z. Vet. J. 63, 24 –30. Henchion, M., McCarthy, M., Resconi, V.C., Troy, D., 2014. Meat consumption: trendsand quality matters. Meat Sci. 98, 561 –568.https://doi.org/10.1016/j. meatsci.2014.06.007.Hoerr, F.J., 2010. Clinical aspects of immunosuppression in poultry. Avian Dis. 54, 2 –15. https://doi.org/10.1637/8909-043009-Review.1 . Horn, B.K.P., Schunck, B.G., 1981. Determining optical ﬂow. Techniques and Applications of Image Understanding. International Society for Optics and Photonics,pp. 319–331https://doi.org/10.1016/0004-3702(81)90024-2 . Hosseini, M.-P., Lu, S., Kamaraj, K., Slowikowski, A., Venkatesh, H.C., 2020. Deep learningarchitectures. Deep Learning: Concepts and Architectures. Springer, pp. 1 –24 https://doi.org/10.1007/978-3-030-31756-0_1 . H u a n g ,X . ,W u ,L . ,Y e ,Y . ,2 0 1 9 .Ar e v i e wo nd i m ensionality reduction techniques. Int. J. Pattern Recognit. Artif. Intell. 33, 1950017. https://doi.org/10.1142/ S0218001419500174.Hughes, B.L., Leong, J.K., Shiv, B., Zaki, J., 2018. Wanting to like: motivation in ﬂuences be- havioral and neural responses to social feedback. bio Rxiv, 300657 https://doi.org/ 10.1101/300657.Iandola, F.N., Han, S., Moskewicz, M.W., Ashraf, K., Dally, W.J., Keutzer, K., 2016.SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <0.5 MBmodel size. arXiv Prepr. abs/1602.07360, arXiv1602.07360. https://arxiv.org/abs/ 1602.07360.Ibraheem, N.A., Hasan, M.M., Khan, R.Z., Mishra, P.K., 2012. Understanding color models: a review. ARPN J. Sci. Technol. 2, 265 –275. Ireri, D., Belal, E., Okinda, C., Makange, N., Ji, C., 2019. A computer vision system for defectdiscrimination and grading in tomatoes using machine learning and image process-ing. Artif. Intell. Agric.https://doi.org/10.1016/j.aiia.2019.06.001 . Jana, A., 2012.Kinect for Windows SDK Programming Guide. Packt Publishing Ltd.Jansen-van Vuuren, R.D., Armin, A., Pandey, A.K., Burn, P.L., Meredith, P., 2016. Organicphotodiodes: the future of full color detection and image sensing. Adv. Mater. 28,4766–4802.https://doi.org/10.1002/adma.201505405 . Jaykaran, 2010.How to select appropriate statistical test? J. Pharm. Negat. Results 1, 61.
Ju, M., Luo, H., Wang, Z., Hui, B., Chang, Z., 2019. The application of improved YOLO V3 inmulti-scale target detection. Appl. Sci. 9, 3775. https://doi.org/10.3390/app9183775 . Kaelbling, L.P., Littman, M.L., Moore, A.W., 1996. Reinforcement learning: a survey. J. Artif.Intell. Res. 4, 237–285.https://doi.org/10.1613/jair.301 .C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
206Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: a survey. Comput.Electron. Agric. 147, 70 –90.https://doi.org/10.1016/j.compag.2018.02.016 . Kashiha, M., Pluk, A., Bahr, C., Vranken, E., Berckmans, D., 2013. Development of an earlywarning system for a broiler house using computer vision. Biosyst. Eng. 116, 36 –45. https://doi.org/10.1016/j.biosystemseng.2013.06.004 . Kashiha, M.A., Green, A.R., Sales, T.G., Bahr, C., Berckmans, D., Gates, R.S., 2014. Perfor-mance of an image analysis processing system for hen tracking in an environmentalpreference chamber. Poult. Sci. 93, 2439 –2448.https://doi.org/10.3382/ps.2014- 04078.Kawaguchi, K., Bengio, Y., 2019. Depth with nonlinearity creates no bad local minima inResNets. Neural Netw. 118, 167 –174.https://doi.org/10.1016/j.neunet.2019.06.009 . Kestin, S.C., Knowles, T.G., Tinch, A.E., Gregory, N.G., 1992. Prevalence of leg weakness in broiler chickens and its relationship with genotype. Vet. Rec. 131, 190 –194. Kestin, S.C., Gordon, S., Su, G., Sørensen, P., 2001. Relationships in broiler chickens be- tween lameness, liveweight, growth rate and age. Vet. Rec. 148, 195 –197. Knowles, T.G., Kestin, S.C., Haslam, S.M., Brown, S.N., Green, L.E., Butterworth, A., Pope, S.J.,Pfeiffer, D., Nicol, C.J., 2008. Leg disorders in broiler chickens: prevalence, risk factorsand prevention. PLoS One 3, e1545. https://doi.org/10.1371/journal.pone.0001545 . K r i s t e n s e n ,H . H . ,A e r t s ,J . - M . ,L e r o y ,T . ,W a t h e s ,C . M . ,B e r c k m a n s ,D . ,2 0 0 6 .M o d e l l i n gthe dynamic activity of broiler chickens in response to step-wise changes in lightintensity. Appl. Anim. Behav. Sci. 101, 125 –143.https://doi.org/10.1016/j. applanim.2006.01.007.Kristensen, H.H., Prescott, N.B., Perry, G.C., Ladewig, J., Ersbøll, A.K., Overvad, K.C., Wathes,C.M., 2007. The behaviour of broiler chickens in different light sources and illumi-n a n c e s .A p p l .A n i m .B e h a v .S c i .1 0 3 ,7 5 –89.https://doi.org/10.1016/j. applanim.2006.04.017.Kurnianggoro, L., Jo, K.-H., 2018. A survey of 2D shape representation: methods, evalua-tions, and future research directions. Neurocomputing 300, 1 –16.https://doi.org/ 10.1016/j.neucom.2018.02.093 . Ladický, L., Russell, C., Kohli, P., Torr, P.H.S., 2009. Associative hierarchical crfs for objectclass image segmentation. 2009 IEEE 12th International Conference on Computer Vi-sion. IEEE, pp. 739–746https://doi.org/10.1109/ICCV.2009.5459248 . LeCun, Y., Haffner, P., Bottou, L., Bengio, Y., 1999. Object recognition with gradient-basedlearning. Shape, Contour and Grouping in Computer Vision. Springer, pp. 319 –345 https://doi.org/10.1007/3-540-46805-6_19 . Lehr, H., 2014.Recent advances in precision livestock farming. Int. Anim. Heal. J. 2, 44 –49. Leroy, T., Vranken, E., Struelens, E., Sonck, B., Berckmans, D., 2005. Computer vision basedrecognition of behavior phenotypes of laying hens. 2005 ASAE Annual Meeting.American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/2013.19471.Leroy, T., Vranken, E., Van Brecht, A., Struelens, E., Sonck, B., Berckmans, D., 2006. A com-puter vision method for on-line behavioral quanti ﬁcation of individually caged poul- try. Trans. ASABE 49, 795 –802.https://doi.org/10.13031/2013.20462 . Li, J., Cheng, K., Wang, S., Morstatter, F., Trevino, R.P., Tang, J., Liu, H., 2017. Feature selec-tion: a data perspective. ACM Comput. Surv. 50, 1 –45.https://doi.org/10.1145/
3136625.Li, B., Liu, L., Shen, M., Sun, Y., Lu, M., 2019a. Group-housed pig detection in video surveil-lance of overhead views using multi-feature template matching. Biosyst. Eng. 181,28–39.https://doi.org/10.1016/j.biosystemseng.2019.02.018 . Li, G., Zhao, Y., Chesser, G.D., Lowe, J.W., Purswell, J.L., 2019b. Image processing for analyz-ing broiler feeding and drinking behaviors. 2019 ASABE Annual International Meet-ing. American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/aim.201900165.Lin, C.-Y., Hsieh, K.-W., Tsai, Y.-C., Kuo, Y.-F., 2018. Monitoring chicken heat stress usingdeep convolutional neural networks. 2018 ASABE Annual International Meeting.American Society of Agricultural and Biological Engineers, p. 1 https://doi.org/ 10.13031/aim.201800314.Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016. Ssd: single shot multibox detector. European Conference on Computer Vision. Springer,pp. 21–37.Llonch, P., King, E.M., Clarke, K.A., Downes, J.M., Green, L.E., 2015. A systematic review ofanimal based indicators of sheep welfare on farm, at market and during transport,and qualitative appraisal of their validity and feasibility for use in UK abattoirs. Vet.J. 206, 289–297.https://doi.org/10.1016/j.tvjl.2015.10.019 . Lowe, D.G., 2004. Distinctive image features from scale-invariant keypoints. Int. J. Comput.Vis. 60, 91–110.https://doi.org/10.1023/B:VISI.0000029664.99615.94 . Lucas, B.D., Kanade, T., 1981.An Iterative Image Registration Technique With an Applica-tion to Stereo Vision.Marıa, G.A., Escós, J., Alados, C.L., 2004. Complexity of behavioural sequences and their re-lation to stress conditions in chickens (Gallus gallus domesticus): a non-invasivetechnique to evaluate animal welfare. Appl. Anim. Behav. Sci. 86, 93 –104.https:// doi.org/10.1016/j.applanim.2003.11.012 . Mehdizadeh, S.A., Neves, D.P., Tscharke, M., Nääs, I.A., Banhazi, T.M., 2015. Image analysismethod to evaluate beak and head motion of broiler chickens during feeding.Comput. Electron. Agric. 114, 88 –95.https://doi.org/10.1016/j.compag.2015.03.017 . Mollah, M.B.R., Hasan, M.A., Salam, M.A., Ali, M.A., 2010. Digital image analysis to estimatethe live weight of broiler. Comput. Electron. Agric. 72, 48 –52.https://doi.org/10.1016/ j.compag.2010.02.002.Mortensen, A.K., Lisouski, P., Ahrendt, P., 2016. Weight prediction of broiler chickensusing 3D computer vision. Comput. Electron. Agric. 123, 319 –326.https://doi.org/ 10.1016/j.compag.2016.03.011 . Nääs, I. de A., Lozano, L.C.M., Mehdizadeh, S.A., Garcia, R.G., Abe, J.M., 2018. Paraconsistentlogic used for estimating the gait score of broiler chickens. Biosyst. Eng. 173, 115 –123. https://doi.org/10.1016/j.biosystemseng.2017.11.012 .Nakarmi, A.D., Tang, L., Xin, H., 2014. Automated tracking and behavior quanti ﬁcation of laying hens using 3D computer vision and radio frequency identi ﬁcation technolo- gies. Trans. ASABE 57, 1455 –1472.https://doi.org/10.13031/trans.57.10505 . Neves, D.P., Mehdizadeh, S.A., Tscharke, M., de Alencar Nääs, I., Banhazi, T.M., 2015. Detec-tion ofﬂock movement and behaviour of broiler chickens at different feeders usingimage analysis. Inf. Process. Agric. 2, 177 –182.https://doi.org/10.1016/j. inpa.2015.08.002.Noldus, L., Jansen, R.G., 2004.Measuring Broiler Chicken Behaviour and Welfare: Pros-pects for Automation.Noldus, L.P.J.J., Spink, A.J., Tegelenbosch, R.A.J., 2001. EthoVision: a versatile video trackingsystem for automation of behavioral experiments. Behav. Res. Methods Instrum.Comput. 33, 398–414.https://doi.org/10.3758/BF03195394 . Nyalala, I., Okinda, C., Nyalala, L., Makange, N., Chao, Q., Chao, L., Yousaf, K., Chen, K., 2019.Tomato volume and mass estimation using computer vision and machine learning al-gorithms: cherry tomato model. J. Food Eng. 263, 288 –298.https://doi.org/10.1016/j. jfoodeng.2019.07.012.OECD-FAO, 2017.Organisation for Economic Co-operation and Development (OECD)/Food and Agriculture Organization of the United Nations (FAO). 2017. Agric. Outlook2017–2026 Spec. Focus Southeast Asia.Okinda, C., Liu, L., Zhang, G., Shen, M., 2018a. Swine live weight estimation by adaptiveneuro-fuzzy inference system. Indian J. Anim. Res., 52 https://doi.org/10.18805/ijar. v0iOF.7250.Okinda, C., Lu, M., Nyalala, I., Li, J., Shen, M., 2018b. Asphyxia occurrence detection in sowsduring the farrowing phase by inter-birth interval evaluation. Comput. Electron.Agric. 152, 221–232.https://doi.org/10.1016/j.compag.2018.07.007 . O k i n d a ,C . ,L u ,M . ,L i u ,L . ,N y a l a l a ,I . ,M u n e r i ,C . ,W a n g ,J . ,Z h a n g ,H . ,S h e n ,M . ,2 0 1 9 .Amachine vision system for early detection and prediction of sick birds: a broilerchicken model. Biosyst. Eng. 188, 229 –242.https://doi.org/10.1016/j. biosystemseng.2019.09.015. Okinda, C., Sun, Y., Nyalala, I., Korohou, T., Opiyo, S., Wang, J., Shen, M., 2020. Egg volumeestimation based on image processing and computer vision. J. Food Eng., 110041https://doi.org/10.1016/j.jfoodeng.2020.110041 . Otsu, N., 1979. A threshold selection method from gray-level histograms. IEEE Trans. Syst.Man. Cybern. 9, 62–66.https://doi.org/10.1109/TSMC.1979.4310076 . Pal, N.R., Pal, S.K., 1993. A review on image segmentation techniques. Pattern Recogn. 26,1277–1294.https://doi.org/10.1016/0031-3203(93)90135-J . Pan, S.J., Yang, Q., 2010. A survey on transfer learning. IEEE Transactions on Knowledgeand Data Engineering. 22, pp. 1345 –1359.https://doi.org/10.1109/TKDE.2009.191 . Panagiotakis, C., Argyros, A., 2016. Parameter-free modelling of 2D shapes with ellipses.Pattern Recogn. 53, 259 –275.https://doi.org/10.1016/j.patcog.2015.11.004 . Paul-Murphy, J.R., Hawkins, M., 2014. Bird-speci ﬁc considerations: recognizing pain be- havior in pet birds. Handbook of Veterinary Pain Management, Third edition ElsevierInc., pp. 536–554https://doi.org/10.1016/B978-0-323-08935-7.00026-0 . Pereira, D.F., Miyamoto, B.C.B., Maia, G.D.N., Sales, G.T., Magalhães, M.M., Gates, R.S., 2013.Machine vision to identify broiler breeder behavior. Comput. Electron. Agric. 99,194–199.https://doi.org/10.1016/j.compag.2013.09.012 . Pu, H., Lian, J., Fan, M., 2018. Automatic recognition of ﬂock behavior of chickens with convolutional neural network and kinect sensor. Int. J. Pattern Recognit. Artif. Intell.32, 1850023.https://doi.org/10.1142/S0218001418500234 . Pulido, M., Barrena-González, J., Badgery, W., Rodrigo-Comino, J., Cerdà, A., 2018. Sustain-able grazing. Curr. Opin. Environ. Sci. Heal. 5, 42 –46.https://doi.org/10.1016/j. coesh.2018.04.004.Redmon, J., Farhadi, A., 2017. YOLO9000: better, faster, stronger. Proceedings of the IEEE
Conference on Computer Vision and Pattern Recognition, pp. 7263 –7271https://doi. org/10.1007/978-3-319-46448-0_2 . Redmon, J., Farhadi, A., 2018. Yolov3: an incremental improvement. arXiv Prepr. abs/1804.02767, arXiv1804.02767. https://arxiv.org/abs/1804.02767 . Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You only look once: uniﬁed, real- time object detection. Proceedings of the IEEE Conference on Computer Vision andPattern Recognition, pp. 779 –788. Reiter, K., Bessei, W., 1997. Gait analysis in laying hens and broilers with and without legdisorders. Equine Vet. J. 29, 110 –112.https://doi.org/10.1111/j.2042-3306.1997. tb05067.x.Ren, S., He, K., Girshick, R., Sun, J., 2015. Faster r-cnn: towards real-time object detection with region proposal networks. Advances in Neural Information Processing Systems,pp. 91–99.Rencher, A.C., Christensen, W.F., 2012. Chapter 10, Multivariate Regression –Section 10.1, Introduction. Methods Multivar. Anal. Wiley Ser. Probab. Stat. 709 p. 19.Roberts, S.J., Cain, R., Dawkins, M.S., 2012. Prediction of welfare outcomes for broilerchickens using Bayesian regression on continuous optical ﬂow data. J. R. Soc. Interface 9, 3436–3443.https://doi.org/10.1098/rsif.2012.0594 . Rozenboim, I., Biran, I., Chaiseha, Y., Yahav, S., Rosenstrauch, A., Sklan, D., Halevy, O., 2004.The effect of a green and blue monochromatic light combination on broiler growthand development. Poult. Sci. 83, 842 –845.https://doi.org/10.1093/ps/83.5.842 . Rutherford, K.M.D., Haskell, M.J., Glasbey, C., Jones, R.B., Lawrence, A.B., 2004. Fractal anal- ysis of animal behaviour as an indicator of animal welfare. Anim. Welf. 13, 99 –103. Sabour, S., Frosst, N., Hinton, G.E., 2017. Dynamic routing between capsules. Advances in Neural Information Processing Systems, pp. 3856 –3866. Salois, M., Baker, K., 2018.Factors Affecting Broiler Livability: Implications for AnimalWelfare & Food Policy.Samarasinghe, S., 2016.Neural Networks for Applied Sciences and Engineering: FromFundamentals to Complex Pattern Recognition. Auerbach publications.Sehgal, G., Gupta, B., Paneri, K., Singh, K., Sharma, G., Shroff, G., 2017. Crop planning usingstochastic visual optimization. 2017 IEEE Visualization in Data Science (VDS). IEEE,pp. 47–51https://doi.org/10.1109/VDS.2017.8573443 .C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
207Sergeant, D., Boyle, R., Forbes, M., 1998. Computer visual tracking of poultry. Comput.Electron. Agric. 21, 1–18.https://doi.org/10.1016/S0168-1699(98)00025-8 . Sermanet, P., Eigen, D., Zhang, X., Mathieu, M., Fergus, R., LeCun, Y., 2013. Overfeat: inte-grated recognition, localization and detection using convolutional networks. arXivPrepr. abs/1312.6229, arXiv1312.6229. https://arxiv.org/abs/1312.6229 . Shimokomaki, M., Ida, E.I., Soares, A.L., Oba, A., Kato, T., Pedrão, M.R., Coró, F.A.G., Carvalho,R.H., 2017. Animal welfare and meat quality: methodologies to reduce pre-slaughterstress in broiler chicken. Global Food Security and Wellness. Springer, pp. 301 –313 https://doi.org/10.1007/978-1-4939-6496-3_16 . Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deeplearning. J. Big Data 6, 60.https://doi.org/10.1186/s40537-019-0197-0 . Simonyan, K., Zisserman, A., 2014. Very deep convolutional networks for large-scaleimage recognition. arXiv Prepr. abs/1409.1556, arXiv1409.1556. https://arxiv.org/ abs/1409.1556.Song, X., Zhang, G., Liu, F., Li, D., Zhao, Y., Yang, J., 2016. Modeling spatio-temporal distri-bution of soil moisture by deep learning-based cellular automata model. J. Arid Land8, 734–748.https://doi.org/10.1007/s40333-016-0049-0 . Stadig, L.M., Rodenburg, T.B., Ampe, B., Reubens, B., Tuyttens, F.A.M., 2018. An automatedpositioning system for monitoring chickens ’location: effects of wearing a backpack on behaviour, leg health and production. Appl. Anim. Behav. Sci. 198, 83 –88. https://doi.org/10.1016/j.applanim.2017.09.016 . Sundermeyer, M., Schlüter, R., Ney, H., 2012. LSTM neural networks for language model- ing. Thirteenth Annual Conference of the International Speech CommunicationAssociation.Sundermeyer, M., Ney, H., Schlüter, R., 2015. From feedforward to recurrent LSTM neuralnetworks for language modeling. IEEE/ACM Trans. Audio, Speech, Lang. Process. 23,517–529.https://doi.org/10.1109/TASLP.2015.2400218 . Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2016a. Inception-v4, inception-resnet andthe impact of residual connections on learning. arXiv Prepr. abs/1602.07261,arXiv1602.07261.https://arxiv.org/abs/1602.07261 . Szegedy, C., Vanhoucke, V., Ioffe, S., Shlens, J., Wojna, Z., 2016b. Rethinking the inception architecture for computer vision. Proceedings of the IEEE Conference on ComputerVision and Pattern Recognition, pp. 2818 –2826. Tablante, N.L., 2013.Common Poultry Diseases and Their Prevention. Univ. Maryl. Ext.Tania, S., Rowaida, R., 2016. A comparative study of various image ﬁltering techniques for removing various noisy pixels in aerial image. Int. J. Signal Process. Image Process.Pattern Recognit. 9, 113 –124.https://doi.org/10.14257/ijsip.2016.9.3.10 . Thorp, B.H., Duff, S.R.I., 1988. Effect of exercise on the vascular pattern in the bone ex-tremities of broiler fowl. Res. Vet. Sci. 45, 72 –77.https://doi.org/10.1016/S0034- 5288(18)30897-X.Tian, Y., Yang, G., Wang, Z., Wang, H., Li, E., Liang, Z., 2019. Apple detection during differ-ent growth stages in orchards using the improved YOLO-V3 model. Comput. Electron.Agric. 157, 417–426.https://doi.org/10.1016/j.compag.2019.01.012 . Van Hertem, T., Norton, T., Berckmans, D., Vranken, E., 2018. Predicting broiler gait scoresfrom activity monitoring andﬂock data. Biosyst. Eng. 173, 93 –102.https://doi.org/ 10.1016/j.biosystemseng.2018.07.002 . Wang, C., Chen, H., Zhang, X., Meng, C., 2016. Evaluation of a laying-hen tracking algo-rithm based on a hybrid support vector machine. J. Anim. Sci. Biotechnol. 7, 1 –
10. https://doi.org/10.1186/s40104-016-0119-3 . Wang, A., Zhang, W., Wei, X., 2019a. A review on weed detection using ground-based ma-chine vision and image processing techniques. Comput. Electron. Agric. 158,226–240.https://doi.org/10.1016/j.compag.2019.02.005 .Wang, J., Shen, M., Liu, L., Xu, Y., Okinda, C., 2019b. Recognition and classi ﬁcation of broiler droppings based on deep convolutional neural network. J. Sensors, 2019 https://doi. org/10.1155/2019/3823515. Wang, J., Wang, N., Li, L., Ren, Z., 2020. Real-time behavior detection and judgment of eggbreeders based on YOLO v3. Neural Comput. & Applic. 32, 5471 –5481.https://doi.org/ 10.1007/s00521-019-04645-4. Wathes, C.M., Kristensen, H.H., Aerts, J.-M., Berckmans, D., 2008. Is precision livestockfarming an engineer’s daydream or nightmare, an animal ’s friend or foe, and a farmer’s panacea or pitfall? Comput. Electron. Agric. 64, 2 –10.https://doi.org/ 10.1016/j.compag.2008.05.005 . Weeks, C.A., Danbury, T.D., Davies, H.C., Hunt, P., Kestin, S.C., 2000. The behaviour ofbroiler chickens and its modiﬁcation by lameness. Appl. Anim. Behav. Sci. 67, 111–125.https://doi.org/10.1016/S0168-1591(99)00102-1 . Weeks, C.A., Knowles, T.G., Gordon, R.G., Kerr, A.E., Peyton, S.T., Tilbrook, N.T., 2002. Newmethod for objectively assessing lameness in broiler chickens. Vet. Rec. 151, 762 –764. https://doi.org/10.1136/vr.151.25.762 . Welfare-Quality®, 2009.Welfare quality® assessment protocol for poultry (broilers, lay-ing hens). Welfare Quality® Consortium, Lelystad, NetherlandsQuality® (114 pp.).Winter, D.A., 1985.Concerning the scientiﬁc basis for the diagnosis of pathological gait and for rehabilitation protocols. Physiother. Can. 37, 245 –252. Wongsriworaphon, A., Arnonkijpanich, B., Pathumnakul, S., 2015. An approach based ondigital image analysis to estimate the live weights of pigs in farm environments.Comput. Electron. Agric. 115, 26 –33.https://doi.org/10.1016/j.compag.2015.05.004 . Xiong, X., Lu, M., Yang, W., Duan, G., Yuan, Q., Shen, M., Norton, T., Berckmans, D., 2019. Anautomatic head surface temperature extraction method for top-view thermal imagewith individual broiler. Sensors 19, 5286. https://doi.org/10.3390/s19235286 . Youssef, A., Exadaktylos, V., Berckmans, D.A., 2015. Towards real-time control of chickenactivity in a ventilated chamber. Biosyst. Eng. 135, 31 –43.https://doi.org/10.1016/j. biosystemseng.2015.04.003. Zaninelli, M., Redaelli, V., Luzi, F., Mitchell, M., Bontempo, V., Cattaneo, D., Dell ’Orto, V., Savoini, G., 2018. Development of a machine vision method for the monitoring of lay-ing hens and detection of multiple nest occupations. Sensors 18, 132. https://doi.org/ 10.3390/s18010132.Zeiler, M.D., Fergus, R., 2014. Visualizing and understanding convolutional networks.European Conference on Computer Vision. Springer, pp. 818 –833https://doi.org/ 10.1007/978-3-319-10590-1_53 . Zhang, H., Chen, C., 2020. Design of sick chicken automatic detection system based on im-proved residual network. 2020 IEEE 4th Information Technology, Networking, Elec-tronic and Automation Control Conference (ITNEC). IEEE, pp. 2480 –2485https:// doi.org/10.1007/978-3-319-50835-1_14 . Zhang, D., Lu, G., 2004. Review of shape representation and description techniques. Pat-tern Recogn. 37, 1–19.https://doi.org/10.1016/j.patcog.2003.07.008 . Zhang, L., Gray, H., Ye, X., Collins, L., Allinson, N., 2019. Automatic individual pig detectionand tracking in pig farms. Sensors 19, 1188. https://doi.org/10.3390/s19051188 . Zhuang, X., Zhang, T., 2019. Detection of sic k broilers by digital image processing and deep learning. Biosyst. Eng. 179, 106 –116.https://doi.org/10.1016/j. biosystemseng.2019.01.003. Zhuang, X., Bi, M., Guo, J., Wu, S., Zhang, T., 2018. Development of an early warning algo-rithm to detect sick broilers. Comput. Electron. Agric. 144, 102 –113.https://doi.org/ 10.1016/j.compag.2017.11.032 .C. Okinda, I. Nyalala, T. Korohou et al. Artiﬁcial Intelligence in Agriculture 4 (2020) 184 –208
208