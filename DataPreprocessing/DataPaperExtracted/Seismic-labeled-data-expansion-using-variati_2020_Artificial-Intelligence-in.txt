Seismic labeled data expansion using variational autoencoders
Kunhong Lia,b,*, Song Chena, Guangmin Hu, Ph.Da,b
aSchool of Resources and Environment and Center for Information Geoscience, University of Electronic Science and Technology of China (UESTC), Cheng du, 611731, China
bCenter for Information Geoscience, UESTC, Chengdu, 611731, China
ARTICLE INFO
Keywords:Deep learningVariational autoencodersData expansionABSTRACT
Supervised machine learning algorithms have been widely used in seismic exploration processing, but the lack oflabeled examples complicates its application. Therefore, we propose a seismic labeled data expansion methodbased on deep variational Autoencoders (VAE), which are made of neural networks and contains two parts-Encoder and Decoder. Lack of training samples leads to over ﬁtting of the network. We training the VAE with whole seismic data, which is a data-driven process and greatly alleviates the risk of over ﬁtting. The Encoder captures the ability to map the seismic waveform Yto latent deep featuresz, and the Decoder captures the ability to reconstruct high-dimensional waveformbYfrom latent deep featuresz. Later, we put the labeled seismic data into Encoders and get the latent deep features. We can easily use gaussian mixture model to ﬁt the deep feature distribution of each class labeled data. We resample a mass of expansion deep features z
*according to the Gaussian mixture model, and put the expansion deep features into the decoder to generate expansion seismic data.The experiments in synthetic and real data show that our method alleviates the problem of lacking labeled seismicdata for supervised seismic facies analysis.
1. IntroductionSupervised machine learning algorithms have been widely used forseismic interpretation in recent years, such as fault detection ( Wu et al., 2019), seismic facies (Wrona et al., 2018). A key to successful application of supervised machine learning is labeled training. Seismic label mainlycome from well loggings data and interpreters, both of which are limited.Manual interpretations are one of the most direct and intuitivelysimplest ways to increase labeled samples. Therefore, many seismicinterpretation methods use the results of manual interpretation astraining samples. However, manual interpretations require experienceand take up only a small percentage of the whole seismic data set. Inaddition, labeled sample data can be extended by forward simulationsbased on human prior knowledge. Above methods still require expertknowledge as well as signiﬁcant time, so we propose a data-drivenmethod for the seismic labeled data expansion.The proposed method is based on variational autoencoders (VAE),which are unsupervised models widely used for attribute extraction. AVAE is made of multilayer neural networks, which consist of one encoderand one decoder. The encoder codes the high-dimensional seismic datainto the low-dimensional deep feature space, and then the decoder mapsthe deep features to reconstruct the seismic waveform. The goal of VAE isto minimize the difference between the input waveform and the recon-structed waveform. Therefore, the VAE does not need labeled data and is
Fig. 1.The structure of VAE. The red font is the loss function. The KLð/C1 j/C1 Þis the KL divergence. Thejj /C1 jjis the 2-norm and theNð/C1 Þis the gaussian distribution. (For interpretation of the references to colour in this ﬁgure legend, the reader is referred to the Web version of this article.)
* Corresponding author. School of Resources and Environment and Center for Information Geoscience, University of Electronic Science and Technolog y of China (UESTC), Chengdu, 611731, China.E-mail addresses:lkhmoran@gmail.com(K. Li),2547546252@qq.com(S. Chen),hgm@uestc.edu.cn(G. Hu).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2020.12.002Received 31 August 2020; Received in revised form 20 November 2020; Accepted 12 December 2020Available online 13 January 20212666-5441/©2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license(http://creativecommons.org/licenses/by/4.0/ ).Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30Fig. 2.The workﬂow of the proposed method.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
25an unsupervised manner with VAE. Massive seismic data can be used astraining data, which can greatly alleviates the risk of over- ﬁtting. The VAE captures the deep feature distribution of seismic data whenit converges. We put the labeled samples into the decoder, and obtain thedeep feature distribution of each class. It is easy to ﬁt the deep feature distribution with the gaussian mixture model (GMM) for each class. Itshould be noted that each class of data following one gaussian mixture
Fig. 3.The synthetic data.
Fig. 4.The labeled data. (a) The labele data A. (b)The labele data B.
Fig. 5.The deep feature. (a) The deep feature of labeled data. (b)The expansiondeep feature.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
26distribution rather all the class data follow a gaussian mixture distribu-tion. In short, each class deep feature has an explicit gaussian distributionfunction. Subsequently, we get expansion deep feature by using thesefunctions. Finally, we put the expansion deep features through thedecoder and get expansion labeled data.We proposed a semi-supervised labeled data expansion method. Oneof the main reasons for network overﬁtting is the lack of training samples. This method uses whole seismic data for the training of feature extractionand alleviates the problem of over-ﬁtting. We use this method for syn-thetic and real data and the results demonstrate that the generatedexpansion labeled data enhance the seismic waveform classi ﬁcation performance. In the synthetic data experiment, the classi ﬁcation accu- racy is improved by 20 % with the proposed method when SNR is 3.2. Theory2.1. Variational autoencoderThe VAE is deﬁned by a neural network, which consists of two parts,the encoderEand the decoderD. The encoder codes the seismic signaly2RMinto deep feature meanz meanand deviationz dev, and the deep featurezequalsz
meanplusz dev, i.e.z¼z meanþzdev. Then the decoderDis used to decode the featurezto generate the seismic signalby, i.e.by¼ DðzÞ. The purpose of VAE is toﬁt the generatedbyto the inputy, while featureszneed to follow the prior gaussian distribution. Therefore, theloss function of the VAE includes two parts, one is the reconstructionerror, which is measured by the Euclidean distance. Another part of theloss function is about the featurez, and the KL divergence be used tomeasure the differences betweenzand prior gaussian distributions. Hence the VAE loss function is deﬁned as follows:
 Fig. 6.The expansion labeled data. (a) The expansion labeled data A. (b)Theexpansion labeled data B.
Fig. 7.Spectrum of synthetic data. The labeled data and expansion labeled dataalmost have the same spectrum curve.
Fig. 8.The SVM waveform classiﬁcation results with same parameters (kernel function is radial basis function and sigma is 3). (a) The results based on labeled data. (b)The results based on labeled data and expansion labeled data.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
27LVAE¼ylogðyÞþð1/C0yÞlogð1/C0yÞþ
12/C0/C0log/C0z 2dev/C1þz2devþz2dev/C01/C1(1)Due to space constraints, please refer to ( Doersch, 2016) for the de- tails of Eq.(1). The structure of VAE is shown inFig. (1). We use gradient descent to solve Eq.(1).2.2. Gaussian mixed modelThe probability density function of N-dimensional Gaussian distri-bution is deﬁned as follow:N y/C12/C12/C12/C12/C12μ;Σ!¼1ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃð2
πÞN/C12/C12Σ/C12/C12qexp /C012ðy/C0μÞTΣ/C01 y/C0
μ!!!(2)where
μ2Ris the mean vector andΣ2RN/C2Nis the covariance matrix. A distribution consisting ofKGaussian distributions is called a Gaussianmixed Model (GMM), and its probability density function is:f/C16yjΘÞ¼X
Kk¼1αkNðyjμk;Σk/C17 (3)where
αkis the weight coefﬁcient andPKk¼1αk¼1. TheΘrepresents all the parametersð
αk;μk;ΣkÞ. The expectation-maximization algorithm(Dempster et al., 1977) can be used to solve the Eq.(3). We chose GMM to ﬁt distribution of the deep featurezbecause of that GMM can theoreti-callyﬁts the probability distribution of any shape.2.3. Labeled samples expansionThe workﬂow of the proposed method is shown in Fig. 2.T h e r ea r e whole seismic data setY2R
M/C2mand the labeled setYk2RM/C2m k,w h e r e Mis the dimension,mis the number of samples,kis the index of category andm
kis the number ofkcategory labeled samples. The proposed methodhas a total four steps (2). Theﬁrst step is training the VAE with the wholedata setY. The Encoder of VAE will captures distribution of the deepfeaturezand the Decoder of VAE has the ability that generating waveformfrom the deep featurez. The second step is putting labeled samples Y
kinto the Encoder and getting the corresponding the deep feature z
k,w h i c hﬁts the each class deep feature distribution with GMM. We get the probabilitydensity functionfðzj
μk;ΣkÞfor each class. The third step is resamplingpseudo deep featurez
kkviafðzj μk;ΣkÞ, and theﬁnal step is to put thezk*into
Fig. 9.Classiﬁcation accuracy vs SNR.
Fig. 10.The information about the physical model. (a) Theoretical design drawing of the physical model. (b)The photo of the physical model. (c) The seismic pr oﬁle of line A. d) Spectrum of physical model data.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
28the Decoder to generate the pseudo labeled sample Yk*.3. Experiments3.1. Synthetic dataWe design a synthetic data set shown in the Fig. 3. The synthetic data hastotal 10,000 traces with added band-limited gaussian noise (SNR ¼3). The red line separates the synthetic data into two categories, the left is data Aand the right is data B. We take all the data as the training set and randomlyselect 500 traces from data A and B as labeled data respectively, i.e. Y
Aand Y
B.T h eFig. 4shows the labeled data. We use the proposed method toextend the labeled data. For better visualization, we set the deep latentfeaturezto2 dimensions. TheFig.5shows the deep latent features. The blue
Fig. 11.The fracture identiﬁcation results. (e) The fracture segment results by amplitude envelope. (f)The SVM classi ﬁcation results based on labeled data. (g) The SVM classiﬁcation results based on labeled data and expansion labeled data.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
29scatters (zA) correspond to the labeled data A (YA) and the green scatters (z
B) correspond to labeled data B (YB). Weﬁt the deep latent feature with GMM and resample 2000 expansion deep features for each class data ( z
A*; z
B*), which are shown inFig. 5b. We can see that the expansion deep feature(z
A*;zB*) is well in line with the labeled deep feature distribution and greatlyexpands the volume. Finally, we get the expansion labeled data Y
A*;YB*that is shown inFig. 6.Fig. 7shows the spectrum curves of labeled data andexpansion labeled data. The generated data has same character with thelabeled data, and meanwhile has less noise. The Fig. 8shows the seismic waveform classiﬁcation results via support vector machine (SVM). TheFig. 8a is based on labeled data (Y
AandYB) and theFig. 8b is based on the labeled data (Y
AandYB) and expansion data (YA*andYB*). For fairness, we set the same SVM parameters for both datasets, (kernel function is radialbasis function and sigma is 3).Fig. 9shows the accuracy of classiﬁcation under different SNR. Obviously,the result based on the labeleddata( Y
Aand Y
B) and expansion data (YA*andYB*) has better accuracy and robustness,which indicates the expansion data improves the performance ofclassiﬁcation.3.2. Real dataWe applied the proposed methods on a physical model data, whichare provided by China national petroleum corporation key laboratory ofgeophysical exploration. TheFig. 10shows the information about the physical model.Fig. 10a is the theoretical design drawing of the physicalmodel, andFig. 10b is the photo of the physical model. The main purposeof designing this physical model is to study fractures and fault. The areaenclosed by the blue dotted line is the fractures zone. Fractures withdifferent sizes, orientations, dip angles and densities are distributed inthis zone. The major frequency of seismic data is at 40 Hz and the intervalof seismic data is 1 ms.Simply, we just use the data of the fractures area. There is a red lineA cross the fractures area, and the seismic pro ﬁle of line A is shown in Fig. 10c. We cut the waveform data along a horizon with 40 ms window.In order to get labeled data, we calculate the amplitude envelopeattribute and then segment fracture via a threshold, as shown inFig. 11e. We take 2000 samples from fracture and background respec-tively as labeled data, and then expand the label data using the pro-posed method. Based on these labeled and expansion data, we use SVMto classify seismic waveforms. For fairness, we set the same SVM pa-rameters for both datasets, (kernel function is radial basis function andsigma is 5). TheFig. 11shows the results of classiﬁcation. TheFig. 11fi s based on the labeled data andFig. 11g is based on the labeled data andexpansion data. The classiﬁcation results based on expansion samplesand sample data are closer to the real fracture distribution, which in-dicates that expansion samples play a role in improving the classi ﬁca- tion results to some extent.4. ConclusionWe proposed a novel data-driven, semi-supervised label expansionmethod. We use whole survey seismic data as the training data for VAE,which provides enough training data and alleviates the risk of over-ﬁtting. The encoder of VAE projects seismic data into deep features,and the decoder of VAE generate seismic data from the deep features. Weuse GMM to extract deep feature distributions of labeled data, and thenresample a large number of deep features. In other words, the proposedmethod extend labeled data in the deep feature space. We test ourmethod via seismic waveform classiﬁcation. The results of theoretical and practical data demonstrate that expansion data can well enhance theclassiﬁcation results.Declaration of competing interestThe manuscript has not been published before and is not beingconsidered for publication elsewhere. All authors have contributed to thecreation of this manuscript for important intellectual content and readand approved theﬁnal manuscript. We declare there is no con ﬂict of interest.AcknowledgmentsThis work was supported by National Natural Science Foundation ofChina (41804126, 41604107). We would like to thank China nationalpetroleum corporation key laboratory of geophysical exploration forproviding the physical model data.References
Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood from incompletedata via the em algorithm. J. Roy. Stat. Soc. B 39, 1 –22. Doersch, C., 2016. Tutorial on Variational Autoencoders arXiv preprint arXiv:1606.05908.Wrona, T., Pan, I., Gawthorpe, R.L., Fossen, H., 2018. Seismic facies analysis usingmachine learning. Geophysics 83, O83 –O95. Wu, X., Liang, L., Shi, Y., Fomel, S., 2019. Faultseg3d: using synthetic data sets to train anend-to-end convolutional neural network for 3d seismic fault segmentation.Geophysics 84, IM35–IM45.K. Li et al. Artiﬁcial Intelligence in Geosciences 1 (2020) 24 –30
30