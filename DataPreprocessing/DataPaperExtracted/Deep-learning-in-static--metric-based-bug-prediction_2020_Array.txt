Deep learning in static, metric-based bug prediction
Rudolf Ferenca,*,D/C19enes B/C19ana, Tam/C19as Gr/C19oszb,c, Tibor Gyim/C19othya,c
aDepartment of Software Engineering, University of Szeged, Hungary
bDepartment of Computer Algorithms and Arti ﬁcial Intelligence, University of Szeged, Hungary
cMTA-SZTE Research Group on Arti ﬁcial Intelligence, Szeged, Hungary
ARTICLE INFO
Keywords:Neural networksDeep learningBug predictionCode metricsABSTRACT
Our increasing reliance on software products and the amount of money we spend on creating and maintainingthem makes it crucial toﬁnd bugs as early and as easily as possible. At the same time, it is not enough to knowthat we should be paying more attention to bugs; ﬁnding them must become a quick and seamless process in order to be actually used by developers. Our proposal is to revitalize static source code metrics –among the most easily calculable, while still meaningful predictors –and combine them with deep learning –among the most promising and generalizable prediction techniques –toﬂag suspicious code segments at the class level. In this paper, we show a detailed methodology of how we adapted deep neural networks to bug prediction, applied them to a largebug dataset (containing 8780 bugged and 38,838 not bugged Java classes), and compared them to multiple“traditional”algorithms. We demonstrate that deep learning with static metrics can indeed boost prediction ac-curacies. Our best model has an F-measure of 53.59%, which increases to 55.27% for the best ensemble modelcontaining a deep learning component. Additionally, another experiment suggests that these values could improveeven further with more data points. We also open-source our experimental Python framework to help other re-searchers replicate ourﬁndings.
1. IntroductionOur society’s ever increasing reliance on software products putspressure on developers that is close to unsustainable. With fast idea-to-market times, common overtime issues, and global competition, soft-ware faults–or bugs, as they are more commonly referred to –are easy to make, but hard and costly toﬁx[1]. Moreover, this cost increases pro-portionately with the time of discovery, so the earlier we can catch them,the better. Considering the scale of today ’s source code, however, this requires more automated support than ever. Even if we are not on thelevel of intelligentﬁxes or perfect recall yet, narrowing down the po-tential candidates or highlighting points of interest can be crucial forengineers to be able to keep up with demand.How these candidates are produced is still a heavily researched area,though. Dynamic or symbolic analyses could provide much more exactmatches, but they also require more time and resources per every piece ofsoftware under consideration. This is the main reason we are aiming torestrict our necessary analysis techniques to static only. Another issue canbe insufﬁcient data for generalizability, which is why we are using thelargest uniﬁed class level dataset [2] we are aware of.With the above constraints in place, most of the remaining researchfocuses on“traditional”machine learning approaches like decision trees,Bayesian models or Support Vector Machines for example. We, on theother hand, focus on deep learning and how it can be applicable to thesame problem, since it already showed promising general use in otherareas.Deep learning is a new and very successful area in machine learning;the name stems from the fact that it applies deep neural networks(DNNs). These deep networks differ from previously used arti ﬁcial neural networks in one key aspect, namely that they contain many hiddenlayers. Unfortunately, with these deep structures we have to face the factthat the traditional training algorithm encounters dif ﬁculties (“vanishing gradient effect”) and fails to train good models. As a solution to thisproblem, several new algorithms and modi ﬁcations have been proposed over the years. Of these, we opted for one of the simplest ones, the socalled deep rectiﬁer network [3]. With a simple modiﬁcation to the activation function, the DNN can be trained without any further changesusing the standard stochastic gradient descent (SGD) algorithm.Our plan was to take the above mentioned bug dataset and use it tocompare the performance of DNNs to other, more traditional machine
* Corresponding author. H-6720, Szeged, Dugonics t /C19er 13, Hungary. E-mail addresses:ferenc@inf.u-szeged.hu(R. Ferenc),zealot@inf.u-szeged.hu(D. B/C19an),groszt@inf.u-szeged.hu(T. Gr/C19osz),gyimothy@inf.u-szeged.hu (T. Gyim/C19othy).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100021Received 16 December 2019; Received in revised form 16 February 2020; Accepted 23 February 2020Available online 30 March 20202590-0056/©2020 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 6 (2020) 100021learning techniques within the domain of bug prediction –speciﬁcally, bug prediction from static source code metrics. We emphasize theinterdisciplinary aspect of this experiment by thoroughly detailing everystep we took on our way to training our optimal model, including thepossible data preprocessing, parameterﬁne-tuning, and further exami- nations regarding current or future expectations. Consequently, thecoming sections could be a useful tool for static analysts not familiar withdeep learning, while the nature and quantity of the data –along with the conclusions we can draw from them –could provide new insights for machine learning practitioners as well.Our best deep learning model achieved an F-measure of 53.59% usinga dynamically updated learning rate on the quite imbalanced bug dataset,which contains 8780 (18%) bugged and 38,838 (82%) not bugged Javaclasses. The only single approach capable of outperforming it was arandom forest classiﬁer with an improvement of 0.12%, while anensemble model combining these two reached an F-measure of 55.27%.Additionally, a separate experiment suggests that these deep learningresults could increase even further with more data points, as dataquantity seems to be more beneﬁcial for neural networks than it is forother algorithms.Contributions.The contributions of our work include:1. Adetailed methodology, that serves as an interdisciplinary guide-line for merging software quality and machine learning best practices;2. Alarge-scale case study, that demonstrates the applicability of bothdeep learning and static source code metrics in bug prediction; and3. Anadaptable implementation, that provides replicability, a lowerbarrier to entry, and facilitates the wider use of deep learning.Paper organization.The rest of the paper is structured as follows:Section2overviews the related literature, while Section 3contains a detailed account of our methodology. Then, we describe our process andour correspondingﬁndings in Section4, with the possible threats to the validity of our results being listed in Section 5. Finally, we summarize our conclusions and outline potential future work in Section 6.2. Related workDefect prediction has been the focus of numerous research efforts fora long time. In this section we give a high level overview of the trends weobserved in thisﬁeld and highlight the differences of our approach.Bug prediction features.Earlier work concentrated on static sourcecode metrics as the main predictors of software faults, including size,complexity, and object-orientation measures [ 4–8]. The common de- nominator in these approaches is the ability to look at a certain version ofthe subject system in isolation, and the relative ease with which thesemetrics are computable.Later research shifted its attention to process-based metrics likeadded or deleted lines, developer and history-related information, andvarious aspects of the changes between versions [ 9–13]. These features aim to capture bugs as they enter the source code, thereby having toconsider only a fraction of the full codebase. In exchange, however, amore complicated data collection process is required.In this work we utilize static source code metrics, only combined withdeep learning; a pairing that has not been suf ﬁciently explored in our opinion. We also note that more exhaustive surveys of defect detectionapproaches are published by Menzies et al. [ 14
] and D’Ambros et al. [15].Bug prediction methods.Once feature selection is decided, the nextcustomization opportunity is the machine learning algorithm used tobuild the prediction model. There have been previous efforts to adaptSupport Vector Machines [16], Decision Trees [17], or Linear Regression [15] to bug prediction. Comparative experiments [ 18,19] also incorpo- rate Bayesian models, K Nearest Neighbors, clustering, and ensemblemethods. In contrast, we rely on Deep Neural Networks –discussed below –and compare their performance to these more traditional approaches.Another aspect is the granularity of the collected data and, byextension, the predictions of the model. Many techniques stop at the ﬁle level, we–among others–use class-level features, and there are method-level studies as well.Deep learning and bug prediction. With the advent of more computing performance, deep learning [ 20] became practically appli- cable to a wide spectrum of problems. We have seen it succeed in imageclassiﬁcation [21,22], speech recognition [23,24], natural language processing [25,26], etc. It is reasonable, then, to try and apply it to theproblem of bug prediction as well.From the previously mentioned features, however, only the change-based ones seem to have“survived”the deep learning paradigm shift [27]. On the other hand, there are multiple recent studies focusing onsource code-based tokenization with vector embeddings, approachingthe problem from a language processing perspective [ 28,29]. Another use for these vector embeddings is bug localization, where the existenceof the bug is known beforehand and the task is automatically pairing it toits corresponding report [30–32].Although there are studies where static source code metrics andneural networks appear together, we feel that the relationship is notsufﬁciently explored. Therefore, our work aims to revitalize the use ofstatic source code metrics for bug prediction by combining it withmodern deep learning methodologies and a larger scale empiricalexperiment.A taxonomy of static bug prediction. To focus more exclusively on the closest“neighbors”of our approach, we examined a number ofpublications in order to build a local taxonomy of differences. The threeinclusion criteria were 1) static metric-based methods that are 2) con-cerned with bugs, and 3) utilize some type of machine learning. A sys-tematic review led toﬁve aspects of potential variations:/C15Deep Learning: whether the approach employed deep learning/C15Other Sources: whether it collected data from sources other thanstatic source code metrics/C15Quantity: the amount of training instances that were available(represented in powers of 10)/C15Granularity: the level of the source code elements that wereconsidered instances (Method,Class, orFile)/C15Prediction: whether there were any actual predictions, or only sta-tistical evaluationThe results are presented inTable 1.As the taxonomy shows, the novelty of our work lies in its speci ﬁc combination of aspects. While there are other studies using class-levelgranularity, the evaluation is usually on a much smaller scale, and doesnot involve a deep learning-based inference. On the other hand, whenthere is more data or neural networks are used, the granularity isdifferent. So as far as class-level bug prediction is concerned, this is thelargest scale experiment yet, and, to the best of our knowledge, the ﬁrst ever to investigate actual deep learning prediction. Additionally, none ofthe works from the table try ensemble models, nor do they consider thepossible effects of data quantity.Since not only our classiﬁer, but also our evaluation dataset is new,exact comparisons to other state-of-the-art results are meaningless –even if there were works that would conform to ours in all their taxonomyaspects, which we are not aware of. We would like to note, however, thata matching granularity usually leads to accuracies and F-measures in thesame ballpark, while signiﬁcantly better performances seem to dependon the method-level dataset in question. In the case of [ 33], for example, a (losing) stock Bayesian network produced better results than ourwinners, thereby showcasing the meaningful impact of the raw input.From our perspective, the relative performance differences of the variousapproaches–which can only be measured within an identical context – are much more relevant.R. Ferenc et al. Array 6 (2020) 100021
23. Methodology3.1. OverviewTo complete the experiment outlined in Section 1,w eﬁrst selected an appropriate dataset and applied optional preprocessing techniques(detailed in Section3.2). This was followed by a stratiﬁed 10-fold train/ dev/test split where the original dataset was split into 10 approximatelyequal bins in a way that each bin had roughly the same bugged/notbugged distribution as the whole. This allowed us to repeat every po-tential learning algorithm 10 times, separating a different bin pair for“dev”–a so called development set, reserved for gauging the effect oflater hyperparameter tweaks –and“test”, respectively. The remaining 8 bins were then merged together to form the training dataset.In an additional parametric resampling phase, we could even chooseto alter the ratio of bugged and not bugged instances –only in the current training set–in the hopes of enhancing the learning procedure. In thiscase, upsampling meant repeating certain bugged instances to increasetheir ratio, downsampling meant randomly discarding certain not bug-ged instances to decrease their ratio, and the amount of resamplingmeant how much of the gap between the two classes should be closed.Note that while a complete resampling (including even the dev and testsets) is not unheard of in isolated empirical experiments, it does notcorrectly indicate real world predictive power as we have no in ﬂuence over the distribution of the instances we might see in the future. Thisdistinction should be taken into account when comparing the magnitudeof our results to other studies ’.After all these preparations came the actual machine learning throughdeep neural networks and several other well-known algorithms, whichwe will discuss in Section3.3. These algorithms have many parameters,and multiple“constellations”were tried for each toﬁnd the best per- forming models. This arbitrary limiting and potential discretization ofparameter values and the evaluation of some (or all) tuples from theirCartesian product is commonly referred to as a grid search. Finally, we aggregated, evaluated, and compared the various results, based on theprinciples explained in Section3.4.3.2. Bug datasetThe basis for any machine learning endeavor is a large and repre-sentative dataset. Our choice is the class-level part of the Uni ﬁed Bug Dataset [2] which contains 47,618 classes. It is an amalgamation of 3preexisting sources (namely, PROMISE [ 43], the Bug Prediction Dataset [44], and the GitHub Bug Dataset [45]), which, in turn, consist of numerous open-source Java projects. Each class has 60 numeric metricpredictors–calculated by the OpenStaticAnalyzer toolset [ 46] and summarized inTable 2–and the number of bugs that were reported forit.As there are instances where multiple versions of the same projectappear, using the dataset as is could face the issue of “
the future pre- dicting the past”, where training instances from the more recent statehelp predict older bugs. We did not treat this as a threat, though, becausea) the whole metric-based approach to bug prediction relies on theassumption that the metrics are representative of the underlying faults, soit shouldn’t matter where they came from, and b) there can be legitimatecauses for trying to use insight gained in later versions and extrapolate itback to past snapshots of the codebase.As for preprocessing, the main step preceding every execution was the“binarization”of the labels, i.e., converting the number of bugs found in aclass to a boolean false or true (represented by 0 and 1), depending onwhether the number was 0 or not, respectively. This can be thought of asmaking a“bugged”and a“not bugged”class for prediction. Additional preprocessing options for the features included normali-zation–where metrics were linearly transformed into the [0,1] interval – and standardization–where each metric was decreased by its mean anddivided by its standard deviation, leading to a Gaussian distribution.These transformations can defend against predictors unjustly in ﬂuencing model decisions just because their range or scale is drastically different.For example, the predictor A being a few orders of magnitude larger thanpredictor B does not automatically mean that A ’s changes should affect predictions more than B’s.Table 1A taxonomy of static bug prediction.
Aspect [33][34][35][36][37][38][39][40][41][42] OurDeep Learning✓ ✓ ✓✓✓✓ ✓Other Sources✓✓✓ Quantity (10
x) 36253332334Granularity M M F M M F M C C C CPrediction✓✓✓✓✓✓✓ ✓✓✓
Table 2Features calculated by the OpenStaticAnalyzer toolset.
Abbr. Name Abbr. NameAD API Documentation NOA Number of AncestorsCBO Coupling Between Objectclasses NOC Number of ChildrenCBOI Coupling Between Objectclasses Inverse NOD Number of DescendantsCC Clone Coverage NOI Number of Outgoing Invocations CCL Clone Classes NOP Number of ParentsCCO Clone Complexity NOS Number of StatementsCD Comment Density NPA Number of Public AttributesCI Clone Instances NPM Number of Public MethodsCLC Clone Line Coverage NS Number of SettersCLLC Clone Logical Line Coverage PDA Public Documented APICLOC Comment Lines of Code PUA Public Undocumented APIDIT Depth of Inheritance Tree RFC Response set For ClassDLOC Documentation Lines of Code TCD Total Comment DensityLCOM5 Lack of Cohesion in Methods5 TCLOC Total Comment Lines of CodeLDC Lines of Duplicated Code TLLOC Total Logical Lines of CodeLLDC Logical Lines of DuplicatedCode TLOC Total Lines of CodeLLOC Logical Lines of Code TNA Total Number of AttributesLOC Lines of Code TNG Total Number of GettersNA Number of Attributes TNLA Total Number of Local Attributes NG Number of Getters TNLG Total Number of Local Getters NII Number of IncomingInvocations TNLM Total Number of LocalMethods NL Nesting Level TNLPA Total Number of Local Public Attributes NLA Number of Local Attributes TNLPM Total Number of Local Public Methods NLE Nesting Level Else-If TNLS Total Number of Local Setters NLG Number of Local Getters TNM Total Number of MethodsNLM Number of Local Methods TNOS Total Number of StatementsNLPA Number of Local PublicAttributes TNPA Total Number of PublicAttributes NLPM Number of Local PublicMethods TNPM Total Number of PublicMethods NLS Number of Local Setters TNS Total Number of SettersNM Number of Methods WMC Weighted Methods per ClassR. Ferenc et al. Array 6 (2020) 100021
33.3. Algorithms and infrastructureOnce the training dataset is given, machine learning can begin usingmultiple approaches. These approaches are implemented following theStrategy design pattern to be easily exchangeable and independentlyparameterizable. Our obvious main aim was proving the usefulness ofdeep neural networks–which we attempted with the help of TensorFlow–but we also utilized numerous “traditional”algorithms from the scikit- learn python package. To be able to experiment quickly, we relied on anNVIDIA Titan Xp graphics card to perform the actual low-level compu-tations. We note, however, that not having access to a dedicated graphicscard should not be considered a barrier to entry, because a CPU-basedexecution only makes the experiments slower, not infeasible.TensorFlow.TensorFlow [47] is an open, computation graph basedmachine learning framework that is especially suited for neural net-works. Our dependency is on at least the 1.8.0 version, but training canalso be run with anything more recent. We followed the setup steps of theDNNClassiﬁerclass which we laterﬁne-tuned using the Estimator API and custom model functions. One other important requirement wasrepeatability, so the Estimator ’s RunConﬁg object always contains an explicitly set random seed.The structure of the networks we train is always rectangular anddense (fully connected). Initial parameters can set the number of layers,the number of neurons per layer (which is the same for every hiddenlayer, hence the“rectangular”attribute), the batching (how many in-stances are processed at a time), and the number of epochs learningshould run for. The defaults for these values are 3, 100, 100, and 5,respectively. This algorithm will be referred to as sdnnc, for “simpledeep neuralnetworkclassiﬁer”. More complex parameters and approaches areexplained as our experiment unfolds step by step in Section 4. Scikit-learn.To make sure that going through the trouble of con ﬁg- uring and training deep neural networks is actually worth it, we have tocompare their results to“easier”–i.e., simpler, more quickly trainable – models. We did so using the excellent scikit-learn 0.19.2 module [ 48]. The 8 algorithms we included in our study (and the names we will use torefer to them from now on) are: KNeighborsClassi ﬁer (knn), GaussianNB (bayes), DecisionTreeClassiﬁer (tree), RandomForestClassiﬁer (forest), LinearRegression (linear), LogisticRegression (logistic), and SVC (svm).Note that from the above listed algorithms, LinearRegression is notreally a classiﬁer so we did an external binning on the output anddetermined the prediction bugged if the result was above 0.5. Thisthreshold was not considered a parameter hereafter. Also note thatLogisticRegression, despite its name, is indeed a classi ﬁer. Finally, each of these models started out with scikit-learn-provided defaults, but werelater fairlyﬁ
ne-tuned to make their competition with deep neural net-works unbiased.DeepBugHunter.DeepBugHunter is our experimental Python frame-work collecting the above-mentioned libraries and algorithms into a highabstraction level, parametric tool that makes it easy to either replicateour results, or to adapt the approach to other, possibly unrelated ﬁelds as well. We provide it as an accompanying, open-source contributionthrough GitHub [49]. Our experiments were performed using Python 3.6,and dependencies (apart from TensorFlow and scikit-learn) includednumpy v1.14.3, scipy v1.0.1, and pandas v0.22.0.3.4. Model evaluationAs mentioned at the beginning of this section, our main model eval-uation strategy is a 10-fold cross validation. We do not, however,compute accuracy, precision, or recall values independently for any fold,but collect and aggregate the raw confusion matrices (the true positive,true negative, false positive, and false negative values). This enables us tocalculate the higher level measures once, at the end. Our primary mea-sure and basis of comparison is the F-measure –i.e., the harmonic mean of a model’s precision and recall–but in the case of the best models peralgorithm, we calculated additional ROC curves (Receiver OperatingCharacteristics, mapping the relationship between false and true positiverates), AUC values (area under the ROC curve), as well as training andevaluation runtimes.We also note that due to the nature of cross validation, each fold gets achance to be part of both the development and the test set. This, however,does not mean that information from the test data “leaks”into the hyperparameter tuning phase, as each fold leads to a different model witha separate set of training data.4. ResultsThis section details the results we achieved, step by step as we re ﬁned our approach.4.1. PreprocessingTheﬁrst phase, even before a single machine learning pass, involvedexamining the available preprocessing strategies. Note that, asmentioned in Section3, the“binarization”of labels is already a given. Normalization vs. Standardization.As a preprocessing step for the 60 features–or, predictors–we compared the results of the default algo-rithms on the original data (none) vs. normalization and standardization,introduced in Section3.2. A comparison of the techniques is presented inTable 3.The results suggest that standardization almost always performs well–as expected from previous empirical experiments. Even when it doesnot, it is negligibly close, and it is also responsible for the largestimprovement in our deep neural network strategy. As there are alreadymany dimensions to cover in our search for the optimal bug predictionmodel, with many more still to come –and even more we could have added–we decided toﬁnalize this standardization preprocessing step forall further experimentation.Note that bold font is used to denote our chosen con ﬁguration for the given step while italic font (if any) denotes the previous state. Also notethat the“N/A”cell for the un-preprocessed svm means that execution hadto be shut down after even a single round of the 10-fold cross-validationfailed to complete in the allotted timeframe of 12 hours (while in theother 2 cases, an svm fold took mere minutes).Resampling.Similarly to preprocessing, we compared a few resam-pling amounts in both directions. The results in Table 4show the effect of altering the ratio of bugged and not bugged instances in the training seton predicting bugs in anunalteredtest set. The numbers in the headercolumn represent the percentage of resampling in the given direction, asdescribed in Section3.1.We ended up choosing the 50% upsampling because it was the bestperforming option for our sdnnc strategy and produced comparably goodresults for the other algorithms as well. Similarly to above, it is alsoconsidered aﬁxed dimension from here on out so we can concentrate onthe actual algorithm-speciﬁc hyperparameters. We do note, however,that while it was out of scope for this particular study, replicating theexperiments with different resampling amounts de ﬁnitely merits further research.
Table 3Preprocessing method comparison.
none normalize standardizeknn 44.38% 42.63% 46.47% bayes 34.35% 34.35% 34.35% forest 24.15% 24.15% 24.13% tree 25.95% 25.95% 25.95% linear 21.47% 21.58% 21.40% logistic 23.45% 24.44% 28.02% svm N/A 9.23% 9.88% sdnnc 19.56% 25.04% 34.07%R. Ferenc et al. Array 6 (2020) 100021
44.2. Hyperparameter tuningSimple Grid Search.In ourﬁrst pass at improving the effectiveness ofdeep learning, we triedﬁne-tuning the hyperparameters that werealready present in the default implementation, namely the number oflayers in the network, the number of neurons per layer (in the hiddenlayers), and the number of epochs –i.e., the number of times we traversethe whole training set. Note that the activation function of the neurons(rectiﬁed linear) and the optimization method (Adagrad) were constantthroughout this study, while the batching number could have been varied–and it will be in later stages–but were kept at aﬁxed 100 at this point. The performance of the different conﬁgurations is summarized in Table 5, where a better F-measure can help us select the most well-suitedhyperparameters.As the F-measures show, the best setup so far is 5 layers of 200neurons each, learning for 10 epochs. It is important to note, however,that these F-measures are evaluated on the devset, as the performance information they provide can factor into what path we choose in furtheroptimization. Were we to use the test set for this, we would lose theobjectivity of our estimations about the model ’s predictive power, so test evaluations should only happen at the very end.Initial Learning Rate.The next step was to consider the effects ofchanging the learning rate–i.e., the amount a new batch of informationinﬂuences and changes the model ’s previous opinions. These learningrates are set only once at the beginning of the training process and areﬁxed until the set number of epochs pass. Their effect on the resultingmodel’s quality are shown inTable 6.As we can see, lowering the learning rate to 0.05 –thereby making the model take“smaller steps”towards its optimum–helped itﬁnd a better overall conﬁguration.Early Stopping and Dynamic Learning Rates. Our most dramatic improvement was reached when we introduced validation duringtraining, and instead of learning for a set number of epochs, we imple-mented early stopping. This meant that after every completed epoch, weevaluated the F-measure of the in-progress model on the development setand checked whether it is an improvement or a deterioration. In the caseof a deterioration, we reverted the model back to the previous –and, so far, the best–state, halved the learning rate, and tried again; a strategycalled“new bob”in the QuickNet framework [50]. We repeated this loop until there were 4 consecutive “misses”, signaling that the model seemsunable to learn any further. The rationale behind this approach is that a)we start from a set learning rate and let the model learn while it can, and
b) if there is a“misstep”, we assume that it happened because thelearning rate is now too big and we overshot our target so we should retrythe previous step with a smaller rate.The performance impact of this change is meaningful, as shown inTable 7. Note that both the above limit of 4 for the consecutive missesand the halving of the learning rates come from previous experience andare considered constant. We will refer to this approach as cdnnc, for“customizeddeepneuralnetworkclassiﬁer”. Regularization.At this point, to decrease the gap between the trainingand dev F-measures and hopefully increase the model ’s generalization capabilities, we tried L2 regularization [ 51]. It is a technique that adds an extra penalty term to the model ’s loss function in order to discouragelarge weights and avoid over-ﬁtting.In our case, however, setting the coefﬁcient of the L2 penalty term (denoted byβ) to non-zero caused only F-measure degradation (as showninTable 8), so we decided against its use. Note that we also tried βvalues above 0.05, but those also lead to complete model failure.Another Round of Hyperparameter Tuning. Considering the meaningful jump in quality that cdnnc brought, we found it pertinent to repeat thehyperparameter grid search paired with the early stopping as well,netting us anotherþ0.45% improvement. The tweaked parameters were,again, the number of layers, the number of neurons per layer, thebatching amount, and the initial learning rate (that was still halved afterevery miss). The results, which are also our ﬁnal results for deep learning in this domain, are summarized inTable 9. The best model we were able to build, then, has 5 layers, each with250 neurons, gets its input in batches of 100, starts with a learning rate of0.1, and halves its learning rate after every misstep with backtrackingTable 4Resampling method and amount comparison.
down none up100 75 50 25 0255075 100knn 49.21% 51.10% 49.93% 48.46% 46.47%50.08%51.11%51.17% 51.04% bayes 34.70% 34.52% 34.39% 34.38% 34.35%34.39%34.62%34.65% 34.78% forest 47.91% 47.67% 41.17% 32.61% 24.13%44.22%48.43%49.39% 48.15% tree 46.83% 46.84% 44.28% 34.28% 25.95%45.19%47.42%48.18% 47.37% linear 46.34% 43.96% 36.02% 27.60% 21.40%38.89%45.57%46.70% 46.50% logistic 46.95% 45.10% 39.22% 33.44% 28.02%41.35%46.60%47.64% 47.12% svm 46.49% 41.13% 25.94% 15.69% 9.88%31.00%43.85%47.12% 46.53% sdnnc 48.25% 49.32% 46.04% 36.73% 34.07%50.67%52.03%51.66% 50.59%
Table 5Basic hyperparameter search.
Layers Neurons Epochs Result2 100 5 52.01%3 100 5 52.03%4 100 5 51.84%5 100 5 51.83%5 150 5 52.46%5 200 5 52.04%5 200 2 51.26%5 200 10 52.47%5 200 20 52.18%Table 6The effect of the initial learning rate.
Learning Rate Result0.025 52.69%0.05 52.70%0.1 52.47%0.2 52.36%0.3 51.76%0.4 52.37%0.5 51.87%
Table 7The effect of dynamic learning rates.
Learning Rate Result0.025 53.98%0.05 54.18%0.1 54.48%0.2 53.93%0.3 54.14%0.4 54.29%0.5 54.31%R. Ferenc et al. Array 6 (2020) 100021
5until 4 consecutive misses, thereby producing a 54.93% F-measure on thedevelopment set. Having decided to stop re ﬁning the model, we could also evaluate it on the test set, resulting in an F-measure of 53.59%. AlgorithmComparison.To get some perspective on how good theperformance of deep learning is, we needed to compare it to similarlyﬁne-tuned versions of the other, more “traditional”algorithms listed in Section3.3. Their possible parameters are listed in the of ﬁcial scikit-learn documentation [48], the method we used to tweak them is the same gridsearch we utilized for deep learning previously, and the best con ﬁgura- tions we found are summarized inTable 10in descending order of their test F-measures. Note that although we used F-measures to guide theoptimization procedure, we list additional AUC values belonging to theseﬁnal models for a more complete evaluation. We also measured modeltraining and test set evaluation times, which are given in the last twocolumns, respectively.The highest generalization on the independent test set goes to therandom forest algorithm, although the highest train and dev resultsbelong to our deep learning approach according to both F-measure andAUCﬁgures. The numbers also show a fairly relevant gap between theperformance of the two best models (forest and cdnnc) and the rest of thecompetitors. Additionally, while their evaluation times are at leastcomparable–with others meaningfully behind –training a neural network is two orders of magnitude slower.Despite the close second place, the reader might justi ﬁably discard deep learning as a viable option for bug prediction at this point. Whybother with the complex training procedure when a random forest canyield comparable results in a small fraction of the time? In the next twosections, however, we will attempt to show that deep learning can still beuseful (in its current form) with the potential of becoming even betterover time.4.3. Ensemble modelOne interesting aspect we noticed when comparing our cdnncapproach to random forest was that although they perform nearly iden-tically in terms of F-score, they arrive there in notably different ways.Taking a look at the separate confusion matrices of the two algorithms inTables 11 and 12shows a non-negligible amount of disagreement be-tween the models. Computing their precision and recall values (shown intheﬁrst two columns ofTable 14) conﬁrm their differences: cdnnc has higher recall (which is arguably more important in bug prediction any-way) at the price of lower precision, while forest is the exact opposite.This prompted us to try and combine their predictions to see how wellthey could complement each other as an “ensemble”[52]. The method of combination was averaging the probabilities each model assigned to thebugged class and seeing if that average itself was over or under 0.5 – instead of a simple logicaloron the class outputs. The thinking behindthis experiment was that if the two models did learn the same “lessons” from their training, then disregarding deep learning and simply usingforest is indeed the reasonable decision. If, on the other hand, theylearned different things, their combined knowledge might even surpassthose of the individual models ’.
Tables 13 and 14attest to the second theory, as the ensemble F-measure reached 55.27% (a 1.56% overallTable 8The effect of L2 regularization.
β Result0.0005 54.07%0.001 53.34%0.002 52.60%0.005 51.05%0.01 49.32%0.02 43.35%0.05þ0.00%
Table 9The effect of further hyperparameter tuning.
Layers Neurons Batch Learning Rate Result4 200 100 0.1 54.77%6 200 100 0.1 54.33%5 150 100 0.1 54.67%5 250 100 0.1 54.93%5 200 50 0.1 54.65%5 200 150 0.1 54.58%5 300 100 0.1 54.68%5 300 100 0.2 54.29%5 300 100 0.3 54.48%6 300 100 0.1 54.08%6 300 100 0.2 54.49%6 300 100 0.3 54.29%6 350 100 0.1 54.58%6 350 100 0.2 54.29%6 350 100 0.3 54.29%7 350 100 0.1 54.51%7 350 100 0.2 53.89%7 350 100 0.3 53.95%
Table 10The best version of each algorithm.
Alg. Parameters Train Dev Test TimeF-mes. AUC F-mes. AUC F-mes. AUC Train Eval.forest–max-depth 10 74.38% 89.19% 53.55% 83.23% 53.71% 82.98% 87.7s 0.5s –criterion entropy–n-estimators 100cdnnc–layers 579.10% 91.16% 54.93% 81.92% 53.59% 81.79% 2132.5s 12.7s –neurons 250–batch 100–lr 0.1 knn–n_neighbors 18 73.75% 89.17% 52.47% 81.36% 52.40% 81.14% 124.3s 273.2ssvm–kernel rbf–C 2.6 69.30% 75.87% 52.62% 70.96% 52.25% 70.75% 3142.0s 106.2s–gamma 0.02tree–max-depth 10 72.33% 87.04% 50.26% 77.85% 49.77% 77.34% 11.1s 0.1slogistic–penalty l2 58.23% 78.28% 46.66% 78.38% 46.43% 78.06% 58.4s 0.1s–solver liblinear–C 2.0–tol 0.0001linear 57.34% 77.64% 45.57% 77.74% 45.61% 77.47% 3.9s 0.1sbayes 39.78% 74.36% 34.62% 74.62% 34.84% 74.40% 0.5s 0.1sTable 11CDNNC confusion matrix.
PredictedBugged Not BuggedMeasured Bugged 5435 3345Not Bugged 6069 32,769R. Ferenc et al. Array 6 (2020) 100021
6improvement) while the AUC reached 83.99% (a 1.01% improvement).Moreover, the corresponding ROC curves provide a subtle (yet useful)visual support for this theory. As we can see in Fig. 1, CDNNC and Forest learned differently, hence the differences in their curves. CDNNC slightlyoutperforms Forest at lower false positive rates, but the relationship isreversed at higher rates. Combining their judgments leads to the dottedEnsemble curve, which outperforms both.This leads us to believe that deep neural networks might already beuseful for bug prediction–even if not by themselves but as parts of ahigher level ensemble model.4.4. The effect of data quantityAnother auxiliary experiment we tried was based on the assumptionthat“deep learning performs best with large datasets ”. And by“large”, we mean data points in at least the millions. While our dataset cannot beconsidered small by any measure, –it is the most comprehensive uniﬁed bug dataset we are aware of –it is still not on the“large dataset”scale. The question then became the following: how could we empiricallyshow that deep learning would perform better on more data withoutactually having more data? The answer we came up with inverts theproblem: we theorize that if data quantity is proportional to the “domi- nance”of a deep learning strategy then it would also manifest as a fasterdeterioration than the other algorithms when even less data is available.So we artiﬁcially shrank–i.e., did a uniform stratiﬁed downsampling on –the full dataset three times to produce a 25%, a 50%, and a 75% subsetto replicate our whole previous process on. The results are summarized inTable 15.The table consists of three regions, namely the various F-measuresevaluated on their test sets (left), the difference between the best deeplearning strategy and the current algorithm (middle), and the same dif-ference, only normalized into the [0,1] interval (right). The normalizedrelative differences are also illustrated in Fig. 2, where the slope of the lines represent the change in the respective differences. So we track theserelative differences over changing dataset sizes, and the steeperthe incline of the lines, thelessinﬂuence dataset sizes have over their cor-responding algorithms compared to neural networks.An imaginaryy¼xdiagonal line would mean that deep learning islinearly more sensitive to more data, which would lead us to believe thatif there were any more data, we could linearly increase our performance.And what we see inFig. 2is not far off from this theoretical indicator. Inthe case of logistic vs. cdnnc, for example, growth in the differencesmeans that cdnnc is leaving logistic farther and farther behind as moredata becomes available. While in the case of forest vs. cdnnc, it meansthat cdnnc is“catching up”–since theﬁgures are negative, but their absolute values are decreasing.As most tendencies of the changing differences empirically corrobo-rate, more data is good for every algorithm, but it has a bigger impact ondeep learning. Naturally, there are occasional swings like SVM
’s decrease at 75%–possibly due to the more“hectic”nature of the technique–or KNN’s“hanging tail”at 100%. If we assume a linear kind of relationship,however, even these cases show overall growth. This leads us to speculatethat deep neural networks could dominate their opponents –individu- ally, even without resorting to the previously described model combi-nation–when used in conjunction with larger datasets. We also note thatscalability should not be an issue, as larger input datasets would affectonly thetrainingtimes of the models–which is usually an acceptable up- front sacriﬁce–while leaving prediction speeds unchanged.5. Threats to validityThroughout this study, we aimed to remain as objective as possible bydisclosing all our presuppositions and publishing only concrete, repli-cable results. However, there are still factors that could have skewed theconclusions we drew. One is the reliability of the bug dataset we used asour input. Building on faulty data will lead to faulty results –also known as the“garbage in, garbage out”principle–but we are conﬁdent that this is not the case here. The dataset is independently peer reviewed,accepted, and is compiled using standard data mining techniques.Another factor might be–ironically–bugs in our bug prediction framework. We tried to combat this by rigorous manual inspections,tests, and replications. Additionally, we are also making the source codeopenly available on GitHub and invite community veri ﬁcation or comments.Yet another factor could be the study dimensions we decided to ﬁx– namely, the preprocessing technique, the preliminary resamplig, thenumber of consecutive misses before stopping early, the 0.5 multiplierfor the learning rate“halving”, and even the random seed, which was thesame for every execution. Analyzing how changes to these parameterswould impact the results–if at all–was out of the scope of this study.Finally, the connections and implications we discovered from theobjectiveﬁgures might just be coincidences. Although there are perfectlylogical and reasonable explanations for the unveiled behavior –which we discussed–there is still much to be examined and con ﬁrmed in this domain.Table 12Forest confusion matrix.
PredictedBugged Not BuggedMeasured Bugged 5098 3682Not Bugged 5105 33,733
Table 13Ensemble confusion matrix.
PredictedBugged Not BuggedMeasured Bugged 5360 3420Not Bugged 5255 33,583
Table 14Comparison of individual and ensemble results.
CDNNC Forest EnsemblePrecision 47.24% 49.97% 50.49% Recall 61.90% 58.06% 61.05% F-Measure 53.59% 53.71% 55.27% AUC 81.79% 82.98% 83.99%
Fig. 1.ROC comparison for CDNNC, Forest, and their Ensemble.R. Ferenc et al. Array 6 (2020) 100021
76. Conclusions and future workIn this paper, we presented a detailed approach on how to apply deepneural networks to predict the presence of bugs in classes from staticsource code metrics alone. While neither deep learning nor bug predic-tion are new topics in themselves, we aim to bene ﬁt their intersection by combining ideas and best practices from both.Our greatest contribution is the thorough, step by step description ofour process which–apart from the underexplored coupling of concepts – leads to a deep neural network that is on par with random forests anddominates everything else. Additionally, we unveiled that an ensemblemodel made from our best deep neural network and forest classi ﬁers is actually better than either of its components individually, –suggesting that deep learning is applicable right now –and that more data is likely to make our approach even better. These are two further convincing argu-ments supporting the assumption that the increased time and resourcerequirements of training a deep learning model are worth it. Moreover,we open-sourced the experimental tool we used to reach these conclu-sions and invite the community to build on our ﬁndings. Our future plans include comparing the effectiveness of static sourcecode metrics to change-based and vector embedding-based featureswhen utilized with the same deep learning techniques, and to quantifythe effects of different network architectures. We would also like toreplicate the outlined experiments with extra tweaks to the parameterswe consideredﬁxed thus far (e.g., the random seed or the preprocessingmethodology), thereby examining how stable and resistant to noise ourresults are. Additionally, we plan to expand the dataset –ideally some- what automatically to be able to reach an of ﬁcial“large dataset”status in the near future–and to integrate the current best bug prediction modelinto the OpenStaticAnalyzer toolchain to issue possible bug warningsalongside the existing source code metrics. In the meantime, we considerourﬁndings a successful step towards understanding the role deep neuralnetworks can play in bug prediction.AcknowledgmentThis work was partially supported by grant 2018 –1.2.1-NKP-2018- 00004“Security Enhancing Technologies for the IoT ”funded by the Hungarian National Research, Development and Innovation Of ﬁce. Ministry for Innovation and Technology, Hungary grant TUDFO/47138–1/2019-ITM is acknowledged. The Titan Xp used for this researchwas donated by the NVIDIA Corporation.References
[1]Zhivich M, Cunningham RK. The real cost of software errors. IEEE Secur. Priv. 2009;7(2):87–90.[2] Ferenc R, T/C19oth Z, Lad/C19anyi G, Siket I, Gyim/C19othy T. A public uniﬁed bug dataset for java. In: Proceedings of the 14th international conference on predictive models anddata analytics in software engineering. New York, NY, USA: PROMISE ’18, ACM; 2018. p. 12–21.https://doi.org/10.1145/3273934.3273936 . URL http:// doi.acm.org/10.1145/3273934.3273936.Table 15F-measures across different data quantities.
Algorithm Test results Relative difference Normalized relative difference25 50 75 100 25 50 75 100 25 50 75 100sdnnc 47.66% 50.73% 51.27% 53.37%cdnnc 48.19% 51.01% 52.84% 53.59%forest 50.02% 51.96% 53.31% 53.71% /C01.83%/C00.95%/C00.47%/C00.12% 0.00% 51.88% 79.92% 100.00% knn 48.09% 49.55% 50.88% 52.40% 0.10% 1.46% 1.96% 1.19% 0.00% 73.04% 100.00% 58.27%linear 43.91% 45.55% 45.16% 45.61% 4.28% 5.46% 7.68% 7.98% 0.00% 31.81% 91.93% 100.00%logistic 44.75% 45.77% 46.02% 46.43% 3.44% 5.24% 6.82% 7.16% 0.00% 48.50% 91.02% 100.00%svm 47.49% 49.77% 51.80% 52.25% 0.70% 1.24% 1.04% 1.34% 0.00% 83.59% 52.35% 100.00%tree 45.96% 47.52% 48.82% 49.77% 2.23% 3.49% 4.02% 3.82% 0.00% 70.22% 100.00% 89.02%bayes 35.48% 35.96% 35.25% 34.84% 12.71% 15.05% 17.59% 18.75% 0.00% 38.80% 80.79% 100.00%
Fig. 2.The tendencies of the normalized relative differences.R. Ferenc et al. Array 6 (2020) 100021
8[3]Glorot X, Bordes A, Bengio Y. Deep sparse recti ﬁer neural networks. In: Proceedings of the fourteenth international conference on arti ﬁcial intelligence and statistics; 2011. p. 315–23.[4]El Emam K, Melo W, Machado JC. The prediction of faulty classes using object-oriented design metrics. J Syst Software 2001;56(1):63 –75. [5]Subramanyam R, Krishnan MS. Empirical analysis of ck metrics for object-orienteddesign complexity: implications for software defects. IEEE Trans Software Eng2003;29(4):297–310.[6]Gyimothy T, Ferenc R, Siket I. Empirical validation of object-oriented metrics onopen source software for fault prediction. IEEE Trans Software Eng 2005;31(10):897–910.[7]Nagappan N, Ball T. Static analysis tools as early indicators of pre-release defectdensity. In: Proceedings of the 27th international conference on Softwareengineering. ACM; 2005. p. 580 –6. [8]Nagappan N, Ball T, Zeller A. Mining metrics to predict component failures. In:Proceedings of the 28th international conference on Software engineering. ACM;2006. p. 452–61.[9]Nagappan N, Ball T. Use of relative code churn measures to predict system defectdensity. In: Proceedings of the 27th international conference on Softwareengineering. ACM; 2005. p. 284 –92. [10]Hassan AE. Predicting faults using the complexity of code changes. In: Proceedingsof the 31st international conference on software engineering. IEEE ComputerSociety; 2009. p. 78–88. [11]Moser R, Pedrycz W, Succi G. A comparative analysis of the ef ﬁciency of change metrics and static code attributes for defect prediction. In: Proceedings of the 30thinternational conference on Software engineering. ACM; 2008. p. 181 –90. [12]Bernstein A, Ekanayake J, Pinzger M. Improving defect prediction using temporalfeatures and non linear models. In: Ninth international workshop on Principles ofsoftware evolution: in conjunction with the 6th ESEC/FSE joint meeting. ACM;2007. p. 11–8.[13]Kamei Y, Shihab E, Adams B, Hassan AE, Mockus A, Sinha A, Ubayashi N. A large-scale empirical study of just-in-time quality assurance. IEEE Trans Software Eng2013;39(6):757–73.[14]Menzies T, Milton Z, Turhan B, Cukic B, Jiang Y, Bener A. Defect prediction fromstatic code features: current results, limitations, new approaches. Autom SoftwareEng 2010;17(4):375–407. [15]D’Ambros M, Lanza M, Robbes R. An extensive comparison of bug predictionapproaches. In: Mining software repositories (MSR), 2010 7th IEEE workingconference on. IEEE; 2010. p. 31 –41. [16]Shuai B, Li H, Li M, Zhang Q, Tang C. Software defect prediction using dynamicsupport vector machine. In: Computational intelligence and security (CIS), 2013 9th
international conference on. IEEE; 2013. p. 260 –3. [17]Wang J, Shen B, Chen Y. Compressed c4. 5 models for software defect prediction.In: Quality software (QSIC), 2012 12th international conference on. IEEE; 2012.p. 13–6.[18]Ghotra B, McIntosh S, Hassan AE. Revisiting the impact of classi ﬁcation techniques on the performance of defect prediction models. In: Proceedings of the 37thinternational conference on software engineering, vol. 1. IEEE Press; 2015.p. 789–800.[19]Rajbahadur GK, Wang S, Kamei Y, Hassan AE. The impact of using regressionmodels to build defect classiﬁers. In: Proceedings of the 14th international conference on mining software repositories. IEEE Press; 2017. p. 135 –45. [20]Hinton GE, Salakhutdinov RR. Reducing the dimensionality of data with neuralnetworks. Science 2006;313(5786):504 –7. [21] D. Cires¸an, U. Meier, J. Schmidhuber, Multi-column deep neural networks for imageclassiﬁcation, arXiv preprint arXiv:1202.2745.[22]Krizhevsky A, Sutskever I, Hinton GE. Imagenet classi ﬁcation with deep convolutional neural networks. In: Advances in neural information processingsystems; 2012. p. 1097 –105. [23]T/C19oth L. Phone recognition with deep sparse recti ﬁer neural networks. In: Acoustics, speech and signal processing (ICASSP), 2013 IEEE international conference on.IEEE; 2013. p. 6985–9.[24]Mohamed A-r, Dahl GE, Hinton G, et al. Acoustic modeling using deep beliefnetworks. IEEE Trans. Audio Speech Lang. Process 2012;20(1):14 –22. [25]Mnih A, Hinton GE. A scalable hierarchical distributed language model. In:Advances in neural information processing systems; 2009. p. 1081 –8. [26]Sarikaya R, Hinton GE, Deoras A. Application of deep belief networks for naturallanguage understanding. IEEE/ACM Trans. Audio Speech Lang. Process.(TASLP)2014;22(4):778–84.[27]Yang X, Lo D, Xia X, Zhang Y, Sun J. Deep learning for just-in-time defectprediction. QRS; 2015. p. 17 –26. [28]Wang S, Liu T, Tan L. Automatically learning semantic features for defectprediction. In: Software engineering (ICSE), 2016 IEEE/ACM 38th internationalconference on. IEEE; 2016. p. 297 –308. [29] M. Pradel, K. Sen, reportDeep learning to ﬁnd bugs, [Technical Report]. [30]Lam AN, Nguyen AT, Nguyen HA, Nguyen TN. Combining deep learning withinformation retrieval to localize buggy ﬁles for bug reports (n). In: Automatedsoftware engineering (ASE), 2015 30th IEEE/ACM international conference on,IEEE; 2015. p. 476–81.[31]Lam AN, Nguyen AT, Nguyen HA, Nguyen TN. Bug localization with combination ofdeep learning and information retrieval. In: Program comprehension (ICPC). IEEE:IEEE/ACM 25th International Conference on; 2017. p. 218 –29. 2017. [32]Huo X, Li M, Zhou Z-H. Learning uni ﬁed features from natural and programming languages for locating buggy source code. IJCAI; 2016. p. 1606 –12. [33]Manjula C, Florence L. Deep neural network based hybrid approach for softwaredefect prediction using software metrics. Cluster Comput 2019;22(4):9847 –63. [34]Pascarella L, Palomba F, Bacchelli A. Re-evaluating method-level bug prediction. In:2018 IEEE 25th international conference on software analysis, evolution andreengineering (SANER). IEEE; 2018. p. 592 –601. [35]Clemente CJ, Jaafar F, Malik Y. Is predicting software security bugs using deeplearning better than the traditional machine learning algorithms?. In: 2018 IEEEinternational conference on software quality, reliability and security (QRS). IEEE;2018. p. 95–102.[36]Giger E, D’Ambros M, Pinzger M, Gall HC. Method-level bug prediction. In:Proceedings of the ACM-IEEE international symposium on Empirical softwareengineering and measurement. ACM; 2012. p. 171 –80. [37]Jayanthi R, Florence L. Software defect prediction techniques using metrics basedon neural network classiﬁer. Cluster Comput 2019;22(1):77 –88. [38]Li J, He P, Zhu J, Lyu MR. Software defect prediction via convolutional neuralnetwork. In: 2017 IEEE international conference on software quality, reliability andsecurity (QRS). IEEE; 2017. p. 318 –28. [39]Capretz LF, Xu J. An empirical validation of object-oriented design metrics for faultprediction. J Comput Sci 2008;4(7):571 . [40]Arar€OF, Ayan K. Software defect prediction using cost-sensitive neural network.Appl Soft Comput 2015;33:263 –77. [41]Gyim/C19othy T, Ferenc R, Siket I. Empirical validation of object-oriented metrics onopen source software for fault prediction. IEEE Trans Software Eng 2005;31(10):897–910.[42]Gupta DL, Saxena K. Software bug prediction using object-oriented metrics.S/C22adhan/C22a 2017;42(5):655–69. [43] Menzies T, Krishna R, Pryor D. The promise repository of empirical softwareengineering data. 2015.http://openscience.us/repo. [44]D
’Ambros M, Lanza M, Robbes R. An extensive comparison of bug predictionapproaches. In: 7th working conference on mining software repositories (MSR).IEEE; 2010. p. 31–41.[45]T/C19oth Z, Gyimesi P, Ferenc R. A public bug database of github projects and itsapplication in bug prediction. In: International conference on computational scienceand its applications. Springer; 2016. p. 625 –38. [46] OpenStaticAnalyzer static code analyzer. 2018. https://github.com/sed-inf-u-s zeged/OpenStaticAnalyzer. [47] Abadi M, Agarwal A, Barham P, Brevdo E, Chen Z, Citro C, Corrado GS, Davis A,Dean J, Devin M, Ghemawat S, Goodfellow I, Harp A, Irving G, Isard M, Jia Y,Jozefowicz R, Kaiser L, Kudlur M, Levenberg J, Man /C19e D, Monga R, Moore S, Murray D, Olah C, Schuster M, Shlens J, Steiner B, Sutskever I, Talwar K, Tucker P,Vanhoucke V, Vasudevan V, Vi /C19egas F, Vinyals O, Warden P, Wattenberg M, Wicke M, Yu Y, Zheng X. TensorFlow: large-scale machine learning onheterogeneous systems. software available from: tensor ﬂow.org. 2015. URL,htt p://tensorﬂow.org/.[48]Pedregosa F, Varoquaux G, Gramfort A, Michel V, Thirion B, Grisel O, Blondel M,Prettenhofer P, Weiss R, Dubourg V, Vanderplas J, Passos A, Cournapeau D,Brucher M, Perrot M, Duchesnay E. Scikit-learn: machine learning in Python.J Mach Learn Res 2011;12:2825 –30. [49] DeepBugHunter–experimental python framework for deep learning. 2019. https ://github.com/sed-inf-u-szeged/DeepBugHunter . [50] Johnson D. Quicknet. 2019. URL, http://www1.icsi.berkeley.edu/Speech/qn.html . [51] Goodfellow I, Bengio Y, Courville A. Deep learning. MIT Press; 2016. http://www.d eeplearningbook.org.[52]Opitz D, Maclin R. Popular ensemble methods: an empirical study. J Artif Intell Res1999;11:169–98.R. Ferenc et al. Array 6 (2020) 100021
9