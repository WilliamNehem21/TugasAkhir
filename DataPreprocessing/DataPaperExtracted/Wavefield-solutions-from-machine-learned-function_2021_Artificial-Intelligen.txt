Waveﬁeld solutions from machine learned functions constrained by theHelmholtz equation
Tariq Alkhalifaha,*, Chao Songa, Umair bin Waheedb, Qi Haoc
aPhysical Sciences and Engineering Division, King Abdullah University of Science and Technology, Thuwal, 23955, Saudi Arabia
bDepartment of Geosciences, King Fahd University of Petroleum and Minerals, Dhahran, 31261, Saudi Arabia
cCenter for Integrated Petroleum Research, King Fahd University of Petroleum and Minerals, Dhahran, 31261, Saudi Arabia
ARTICLE INFO
Keywords:Helmholtz equationWaveﬁeldsModelingNeural networksDeep learningABSTRACT
Solving the wave equation is one of the most (if not the most) fundamental problems we face as we try to illu-minate the Earth using recorded seismic data. The Helmholtz equation provides wave ﬁeld solutions that are dimensionally reduced, per frequency, compared to the time domain, which is useful for many applications, likefull waveform inversion. However, our ability to attain such wave ﬁeld solutions depends often on the size of the model and the complexity of the wave equation. Thus, we use here a recently introduced framework based onneural networks to predict functional solutions through setting the underlying physical equation as a loss functionto optimize the neural network (NN) parameters. For an input given by a location in the model space, the networklearns to predict the waveﬁeld value at that location, and its partial derivatives using a concept referred to asautomatic differentiation, toﬁt, in our case, a form of the Helmholtz equation. We speci ﬁcally seek the solution of the scattered waveﬁeld considering a simple homogeneous background model that allows for analytical solutionsof the background waveﬁeld. Providing the NN with a reasonable number of random points from the model spacewill ultimately train a fully connected deep NN to predict the scattered wave ﬁeld function. The size of the network depends mainly on the complexity of the desired wave ﬁeld, with such complexity increasing with increasing frequency and increasing model complexity. However, smaller networks can provide smootherwaveﬁelds that might be useful for inversion applications. Preliminary tests on a two-box-shaped scatterer modelwith a source in the middle, as well as, the Marmousi model with a source at the surface demonstrate the potentialof the NN for this application. Additional tests on a 3D model demonstrate the potential versatility of theapproach.
1. IntroductionA fundamental part of using surface seismic recorded data to illumi-nate the Earth is solving the wave equation ( Claerbout, 1985). Solving the wave equation numerically constitutes the majority of the compu-tational cost and complexity in applications like seismic modeling, im-aging, and waveform inversion. Time-domain solutions of the waveequation dominate seismic applications as they are often ef ﬁcient and comply with our natural understanding of wave evolution ( Alterman and Karal, 1968;Richards and Aki, 1980). However, frequency-domain so- lutions, providing a reduction in dimensionality, recently gained addi-tional attention with the rise of waveform inversion ( Pratt, 1999;Sirgue and Pratt, 2004). Such solutions are obtained by inverting the stiffnessmatrix of the Helmholtz wave equation. However, the cost andcomplexity of such a matrix inversion are intolerable as the model sizeincreases, like for high frequencies or 3D applications ( Cl/C19ement et al., 1990), or the wave equation is complex, like those in anisotropic media(Wu and Alkhalifah, 2018a). This led (Sirgue et al., 2008) to suggest using time-domain modelling to obtain wave ﬁelds in the frequency domain for waveform inversion applications. However, such solutionsare vulnerable to dispersion and stability errors ( Wu and Alkhalifah, 2018b).In recent years, researchers in ourﬁeld have utilized machine learning algorithms to predict and classify fault locations, horizons, saltboundaries, facies, as well as, velocity models ( R€oth and Tarantola, 1994; Wrona et al., 2018;Araya-Polo et al., 2019;Holm-Jensen and Hansen, 2020). Whether supervised or semi or unsupervised training, neuralnetworks have shown incredibleﬂexibility in adapting to various
* Corresponding author.E-mail address:tariq.alkhalifah@kaust.edu.sa(T. Alkhalifah).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.08.002Received 2 June 2021; Received in revised form 13 August 2021; Accepted 17 August 2021Available online 8 September 20212666-5441/©2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19geophysical tasks. Supervised learning was instrumental in predictinglow frequencies to help full waveform inversion (FWI) converge to anaccurate solution (Ovcharenko et al., 2019). Deep learning was also utilized to develop”a priori”models from well information to be used inFWI (Mosser et al., 2018;Zhang and Alkhalifah, 2019). Even wave propagation and wave equation solutions were facilitated using deepneural networks (Sorteberg et al., 2018;Hughes et al., 2019). Within the framework of utilizing deep neural networks as universalfunction approximators (Liu and Nocedal, 1989) and under the banner of physics-informed neural networks (PINN), Raisse et al. (2019)demon- strated the network'sﬂexibility in learning how to extract desired func-tional solutions to nonlinear partial differential equations, utilizing theconcept of automatic differentiation ( Baydin et al., 2018). PINN has found considerable traction in solving partial differential equations(linear and nonlinear ones) ranging from cardiac activation mapping(Sahli Costabal et al., 2020) to steady-state Navier-Stokes equation(Dwivedi et al., 2021). Even within the framework of one dimensionalwave propagation, PINN was utilized to establish ﬂexible domain solu- tions of the wave equation (Kissas et al., 2020). In all of these applica- tions, the predicted solutions were smooth, which is a requirement of NNas a universal function approximator ( Pinkus, 1999). Waveﬁelds are generally smooth, but they are often more complex in nature than otherphysical phenomena. The complexity of the wave ﬁeld increases at the source, as it represents a singularity in the solution. Thus, to use ML topredict waveﬁeld solutions will require larger neural networks, whichwill eventually require larger computational resources. It will alsorequire, like most numerical methods, careful sampling of the sourceregion. As a resultSong et al. (2021)suggested that we seek such NN functions for the scattered waveﬁeld instead of the full waveﬁeld. Spe- ciﬁcally, to avoid the need for adaptive training points for the neuralnetwork (NN) to handle the expected source singularity bias, they solvefor the scattered waveﬁeld in the frequency domain and, thus, utilize thecorresponding Lippmann–Schwinger equation as the loss function totrain a deep fully connected neural network with inputs given by(randomly chosen) points in space (within the domain of interest) andoutputs given by the complex scattered wave ﬁeld at these points. In their implementation, they focus on the application of their method onanisotropic media. Our objective in this paper is to evaluate what exactlyan NN can predict of the waveﬁeld solution, especially at a reasonablecost that can be utilized in practical applications.Thus, here, we focus on the role of the model size, the solver, andfrequency of the waveﬁeld in predicting such scattered waveﬁeld solu- tions. We willﬁ
rst compare solutions for the Helmholtz equation ( McFall and Mahan, 2009) to those obtained for the scattered version of theHelmholtz equation for the same network size and hyperparameters. Wewill compare solutions at two different frequencies to assess the ability ofthe NN model in handing higher frequencies. This is followed by inves-tigating the role of the NN model size in smoothing the wave ﬁeld solu- tions by evaluating the corresponding velocity for the predictedwaveﬁelds. We test the performance of the NN on a two box-shapedscatterer model, as well as, the Marmousi model, and in the processshow the sensitivity of the approach to model size and frequency. Furthertesting on 3D will demonstrate the role of the NN parameters optimizer.2. The Helmholtz equationThe wave equation is often solved in the time domain, and such so-lutions are attained by extrapolating the wave ﬁeld in time simulating what happens in nature (Richards and Aki, 1980). Waveﬁelds in the time domain, however, are large, as they are given by a four dimensionalfunction in 3D media or a three dimensional function in 2D media for agiven source. In addition, the time axis, often, requires ﬁne sampling to avoid aliasing, and an evenﬁner sampling of time is required to avoidinstability when solving the wave equation using ﬁnite-difference methods (Courant, 1928).As a result of its linear nature, the wave equation can be easilyformulated in the frequency domain. In this case, the resulting Helmholtzequation can be solved per frequency, with no requirements on frequencysampling, admitting a reduction in dimensionality of the wave ﬁeld so- lution. The Helmholtz equation in an acoustic, isotropic, constant densitymedium, described by the velocity, v, is given by:/C0r2þk2/C1uðxÞ¼fðxÞ;where k¼ ω
v:(1)In this case, the solution of such an equation is a complex wave ﬁeld, u¼{u
r,ui}, deﬁned in the Euclidean space, withx¼{x,y,z}, and a function of the angular frequency,
ω. As a result, our time-domain solu-tion is nothing but a superposition of frequency-domain solutions.The point source nature of the source function, f, admits a singularity in the waveﬁeld solution at the point source location. Such a singularityoften causes inaccuracies in numerical solutions of the Helmholtz equa-tion near the source. As suggested by Song et al. (2021), such a limitation can be addressed by solving the Lippmann –Schwinger form of the wave equation (Lippmann and Schwinger, 1950), instead. This equation is exact as we do not apply the Born approximation, as we maintain the truevelocity on the right hand side of the equation. Thus, to somewhatmitigate the source singularity, we solve for the scattered wave ﬁeld, δu¼u/C0u
0, whereu 0is the background waveﬁeld satisfying the same wave equation (equation(1)) for the background velocityv
0.D eﬁning the velocity model perturbation,δm¼
1v2/C01v20, the scattered waveﬁeld satisﬁes/C18r
2þω2
v2/C19δu¼/C0
ω2δmu 0: (2)For the scattered waveﬁeld, the source function is no longer conﬁned in space, like the point source. It now depends on the perturbation model,which may extend the full space domain. To allow for ef ﬁcient evaluation of the background waveﬁeld, we consider the background velocity, v
0,t o be constant. For marine acquisition, we may choose this constant velocityto equal the water velocity to reduce the effect of the source singularityeven further. The waveﬁeld in an acoustic isotropic medium in 3D for aconstant velocity and a point source located at x
s, is given by:u
0ðxÞ¼eiω
v0jx/C0x sj
4πjx/C0x sj; (3)whereiis the imaginary identity. For 2D applications, the solution foracoustic isotropic media is given byu
0ðxÞ¼i4Hð2Þ0/C18ω
v0jx/C0x sj/C19; (4)whereH
ð2Þ0is the zero-order Hankel function of the second kind, herex¼{x,z}(Richards and Aki, 1980).Solving for the scattered waveﬁeld will allow us later to utilize random samples of the space domain to train the neural network toprovide the functional solution representing the scattered wave ﬁeld in the frequency domain. The analytical solution for the backgroundwaveﬁeld allows us to evaluate the waveﬁeld instantly at any random point in the domain of interest. Next, we will see exactly how theseformulations help the training of a functional neural network (NN).3. The neural network solutionBased on the physics-informed neural network (PINN) frameworkintroduced byRaissi et al. (2019), we utilize a neural network architec-ture using fully connected layers to approximate a function. This functionis the scattered waveﬁeld solution of equation(2).Hornik et al. (1989). have shown the ability of neural networks in approximating functionsT. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
12that are smooth, like what we would expect from solving the waveequation. The input to the network, like a function, is a location in space,given in 2D byxandzcoordinate values, and in 3D byx,y, andzco- ordinate values. The output of the network consists of the real andimaginary values of the complex scattered wave ﬁeld at the input loca- tion.Fig. 1shows, in detail, the PINN concept for our application. We usethe network to evaluate the waveﬁeld and its second-order partial de-rivatives inxandz, which is needed to evaluate the Laplacian operatorand the loss function. Thus, to train the network with equation (2),w e use the following loss function:f¼
1NXNj¼1 /C12/C12ω2mðjÞδuðjÞrþr2δuðjÞrþω2δmðjÞuðjÞr0/C12/C1222þ/C12/C12ω2mðjÞδuðjÞi
þr2δuðjÞiþω2δmðjÞuðjÞi0/C12/C1222; (5)whereNis the number of training samples, and jis the training sample index. The two terms in the loss function correspond to the losses for thereal (δu
r) and imaginary (δu i) parts of the scattered waveﬁeld, using the real (u
r0) and imaginary (u i0) parts of the background waveﬁeld. For the loss function, we chose the background model to be simple enough(homogeneous) so that the background wave ﬁeld can be evaluated analytically on theﬂy. The details of the fully connected deep networkwill be shared in the examples. The activation function between layers,other than the last layer, in all the examples is an inverse tangent. The lasthidden layer connected to the output layer is linear. We chose to optimizethe loss function using an Adam optimizer followed by limited memoryBFGS iterations, all full-batch, gradient-based optimization algorithm(Liu and Nocedal, 1989). The L-BFGS admits smoother and more robustupdates at a higher cost. We will show later the performance of bothoptimizers separately for comparison.The NN functional provides a continuous representation of thewaveﬁeld, as opposed to a grid based representation, and such acontinuous representation offers many bene ﬁts. We can attain the solu- tion at any point, no interpolation is needed, and the domain of coveragecan be of any shape. This can be beneﬁcial in the presence of topography. However, this continuous functional representation has its limitationsthat appear mainly when the waveﬁeld is complex, requiring larger networks and more advanced training. This appears to be the case whenwe have strong scattering and high frequencies. As a result, in thefollowing tests, and as we introduce the approach, we will focus on lowerfrequencies and smoothed models. We will, nevertheless, also demon-strate these limitations as we compute the velocity model that corre-sponds to the predicted scattered waveﬁeld. This can be achieved by solving the wave equation for the velocity model, and from equation (1), this implies:v2approxðxÞ¼Rω2uðxÞfðxÞ/C0r
2uðxÞ;where uðxÞ¼u0ðxÞþδu NNðxÞ;(6)δu
NNis the predicted neural network scattered wave ﬁeld solution, andRcorresponds to the real part. The process of solving equation (6) must be handled with care and there are many ways to do so includingmultiplying the numerator and denominator with the complex conjugateof the denominator and adding a small positive number to the denomi-nator to avoid dividing over zero (Song and Alkhalifah, 2020).4. Testing the NNWe will test this NN framework initially on two 2D examples trying tohighlight some of its features and weaknesses. In the ﬁrst example, we use a two box-shaped scatterer model with the source in the middle andwe look at the dependency of the prediction on the frequency. Then, weapply the approach on the Marmousi model with the source on the sur-face, and we test the dependency of the solution on the size of the neuralnetwork. Finally, we apply the approach on a small 3D model and focuson the role of the optimizer. The objective of these tests is to study theability of an NN to learn to a functional solution of the wave equation forthe scattered waveﬁeld as opposed to the waveﬁeld itself.4.1. A two-scatterer modelIn theﬁrst model, we place two box-shaped perturbations in anotherwise homogeneous background as shown in Fig. 2(a). The model has 100 samples in both thexandzdirections, with a sampling interval of20 m. The corresponding (real part) of the 5 Hz wave ﬁeld for a point source (a delta function, one sample) in the center of the model is showninFig. 2(b). The background model is given by a constant velocity of2 km/s, and the corresponding waveﬁeld for the same source and fre- quency is shown inFig. 2(c). If we subtract the two waveﬁelds, we obtain the true scattered waveﬁeld with the real part shown inFig. 3(a), where the energy, as expected, reﬂects scattering from the two box-shapedscatterers. Using the loss function in equation (5), we train an 8-layer deep fully connected neural network with 20 neurons in each layer torepresent the scattered waveﬁeld solution. We randomly chose 5000samples from the space domain (x
i,zi) for the training, and train for 100000 epochs of Adam updates and 20000 of LBFGS updates. Thisnumber of samples used represents one fourth of the grid samples used tosolve the Helmholtz equation and it was necessary to arrive to the scat-tered waveﬁeld solution shown inFig. 3(b). The difference between the true scattered waveﬁeld and the NN predicted one is shown in Fig. 3(c). There are differences, but they are generally mild. The imaginary part ofthe scattered waveﬁeld, not shown here, had similar accuracy.
Fig. 1.The neural network architecture (left side dashed box) with inputs ( x,z) and outputs given by the real and imaginary parts of the (scattered) wave ﬁeld. The network is trained using a loss function given by the scattered wave equation (right side dashed box), in which the Laplacian components ( δu
r,xx.δur,zz,δui,xx,δui,zz) are evaluated using automatic differentiation of the NN. The loss function can be supported by boundary conditions.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
13To justify inverting for the scattered wave ﬁeld instead of the wave- ﬁeld directly using the Helmholtz wave equation, we repeat the exactexperiment with the same number of randomly chosen training samples.The loss function, in this case, is given by the Helmholtz wave equationand to lessen the effect of a point source bias, we use an isotropicGaussian source with a variance of 2.5. Fig. 4(a) and (b) show the real and imaginary parts, respectively, of the true wave ﬁeld for the two- scatterers model shown inFig. 2(a).Fig. 4(c) and (d) show the real and imaginary parts, respectively, of the NN predicted wave ﬁeld for the same model. The difference is large and this is attributed to the source singu-larity in the Helmholtz equation, which requires better sampling of thesource area in the training data.For an 8 Hz waveﬁeld, we use a larger network given by 40 neurons ineach of the 8 layers and we use 10000 samples in the training. The realpart of the predicted scattered waveﬁeld is shown inFig. 5(a). The dif- ference between this predicted waveﬁeld and the considered true nu- merical solution, plotted at the same scale as in Fig. 5(a), is small as shown inFig. 5(b). To arrive to this solution, we used 150000 epochs ofAdam updates and 20000 of LBFGS updates as demonstrated in Fig. 5(c). We use LBFGS at the end as it admits smoother updates we can rely on,but it is generally more expensive. The sudden change in the behaviour ofthe loss curve reﬂects the transition from Adam to LBFGS. Despite thelarger network, compared to the 5Hz case, and additional epochs, thecost increase was less than 100%, and that is much smaller than theadditional cost we experience in solving for high frequencies using ﬁnite difference methods, which tend to increase exponentially.4.2. The Marmousi modelNow, we test the utilization of the NN PDE in solving for the wave ﬁeld for a slightly smoothed Marmousi model ( Fig. 6(a)). A point source is placed, this time, on the surface at location 4.5 km. We solve theHelmholtz equation numerically to obtain the 3 Hz frequency wave ﬁeld.The background model is homogeneous with a velocity of 1.5 km/s inwhich we can solve the waveﬁeld analytically. The difference betweenthe true and the background waveﬁelds, constituting the scattered waveﬁeld is shown inFig. 6(b) (real part) and 6(c) (imaginary part). Thebackground waveﬁeld and the model perturbations (difference betweenthe true model and the homogeneous background) are used in the costfunction given by equation(5)to invert for the NN parameters. We use,this time, a 10-layer network with {128, 128, 64, 64, 32, 32, 16, 16, 8, 8}neurons in the layers, respectively. Weﬁnd that this conﬁguration, given by larger dimensional layers early, is generally more effective. We use10000 random sample points for the training and the resulting loss over20000 epochs of training is shown in Fig. 7(a). The trained network is then used to evaluate the scattered wave
ﬁeld on a regular grid and the resulting real part is shown inFig. 7(b) and the imaginary part is showninFig. 7(b).The difference between the true scattered wave ﬁeld and the NN predicted one is shown inFig. 8(a) (real part) and 8(b) (imaginary part).The difference is generally small again, but here it seems to include morecoherent energy corresponding to some of the scattering. In other words,the resulting NN predicted scattered waveﬁeld is smoother than the true waveﬁeld. This is an expected feature of NN when we avoid over ﬁtting, the network acts as a smoother (Neal et al., 2018). We can further verify this smoothness feature by using the wave equation (equation ( 6)) to compute the velocity model corresponding to the predicted wave ﬁeld as shown inFig. 8(c).If we use a smaller network of 8 layers with {64, 64, 32, 32, 16, 16, 8,8} neurons in the layers from left to right (we dropped the ﬁrst two layers from the previous network), and use the same number of epochs, theresulting real part of the predicted scattered wave ﬁeld tends to be smoother as shown inFig. 9(a). The difference between the predicted andtrue scattered waveﬁeld is shown inFig. 9(b) plotted at the same scale. The difference includes more energy than before. The increasedsmoothness of the waveﬁeld can be veriﬁed by the resulting velocity
Fig. 2.a) A two-scatter model. b) The real part of a 5 Hz wave ﬁeld for the velocity in (a) for a source in the middle, computed numerically, and considered true. c) The real part of the 5 Hz waveﬁeld for the background model given by a velocity of 2 km/s, computed analytically.
Fig. 3.a) The scattered waveﬁeld given by the difference between the two wave ﬁelds inFig. 2(b) and (c) (True and background wave ﬁelds). b) The NN predicted scattered waveﬁeld on a regular grid. c) The difference between the actual and predicted scattered wave ﬁelds.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
14model calculated from the waveﬁeld and shown inFig. 9(c). The velocity model is smooth compared to the true model, re ﬂecting the smooth na- ture of the waveﬁeld.On the other hand, if we actually use a 12-layer network by addingtwo layers at the beginning of the original network with 256 neurons ineach of them, we end up with a network given by {256, 256, 128, 128,64, 64, 32, 32, 16, 16, 8, 8} neurons in the layers from left to right. Usingthe same number of epochs in the training of the same random samples inspace, we end up with the predicted scattered wave ﬁeld with the real part shown inFig. 10(a). The difference between the predicted and truescattered waveﬁeld is shown inFig. 10(b) plotted at the same scale. It contains less energy and that again can be veri ﬁed by the resulting ve- locity model calculated from the waveﬁeld and shown inFig. 10(c). Thevelocity model is clearly sharper and it is reasonably close to the truevelocity model shown inFig. 6(a).Thus, a larger network can provide more accurate wave ﬁelds, but for applications in gradient calculation for velocity model update, a perfectscattered waveﬁeld is not necessary. The cost of training the 12-layernetwork is 50%higher than the 10-layer one in spite that the numberof network model parameters increased by 4. Meanwhile, the cost of thetraining the 8-layer network is two-third the cost of training the 10-layernetwork, while the number of network parameters is one-fourth of that ofthe 10-layer network. So, in summary, using this network architecture,the cost of the training will increase by about 50 %with the addition of two layers of double the size of theﬁrst (largest) layer, and we end up with a higher resolution waveﬁeld.
Fig. 4.a) The real part of the true waveﬁeld. b) The imaginary part of the true wave ﬁeld. c) The real part of the NN predicted wave ﬁeld. d) The imaginary part of the NN predicted waveﬁeld. The waveﬁelds correspond to the velocity model in Fig. 2(a).
Fig. 5.a) The NN predicted scattered 8Hz wave ﬁeld for a source in the middle. b) The difference between the predicted scattered wave ﬁeld and the one computed numerically (true) plotted at the same scale as in a). c) The NN training loss function, which displays the loss using Adam followed by LBFGS.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
15Fig. 6.a) The Marmousi model. b) The real part of the resulting 3 Hz wave ﬁeld for a source on the surface located in the middle. c) The imaginary part of the waveﬁeld.
Fig. 7.a) The loss function for the training of the NN. b) The real part of the predicted scattered wave ﬁeld from the NN network. c) The imaginary part.
Fig. 8.a) The difference betweenFig. 6(b) and 7(b)(True and predicted real parts of the scattered wave ﬁelds). b) The difference betweenFig. 6(c) and 7(c)(True and predicted imaginary parts of the scattered wave ﬁelds). c) The velocity model computed from the predicted wave ﬁeld.
Fig. 9.a) The real part of the predicted scattered wave ﬁeld using an 8-layer network. b) The difference between Fig. 6(b) and 9(a)(True and predicted real parts of the scattered waveﬁelds). c) The velocity model computed from the predicted wave ﬁeld.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
164.3. A 3D example and the optimizerWe consider a 3D cube extracted from the SEG/EAGE Overthrustmodel (Aminzadeh et al., 1994) and slightly smoothed as shown inFig. 11(a). In this test, we also test the performance of two NN optimi-zation algorithms, speciﬁcally Adam and LBFGS. The background ho-mogeneous model has a velocity of 3.2 km/s. The difference between theHelmholtz computed waveﬁeld for 10 Hz and the background waveﬁeld for the same frequency provides us with the true scattered wave ﬁeld for a source located in the middle, with the real part of this scattered wave ﬁeld shown inFig. 11(b). Since the Adam optimizer admits, as we saw earlier,turbulent loss functions, for this example, we will test the performance ofthe two network optimization algorithms separately: The Adam opti-mizer and the limited memory BFGS algorithm. In this comparison, forthe Adam optimizer we use 150000 epochs and for the LBFGS we use50000 epochs in the training, and the neurons in each of the 8 hiddenlayers are {64, 64, 32, 32, 16, 16, 8, 8}, as shown in Fig. 11(c). The loss function for the Adam optimizer is shown in Fig. 12(a) and, in average, the loss reduces per epoch, but with Adam we notice the lossfunction is bumpy and this has been realized by others in the applicationof PINN. Considering the log scale of the vertical axis, such alterations areslightly exaggerated in the Figure. The Adam update can be madesmoother by using a smaller learning rate, but that increases the numberof epochs. The real part of the resulting predicted scattered wave ﬁeld is shown inFig. 12(b), and the difference between it and the true scatteredwaveﬁeld is shown inFig. 12(c). The difference, plotted at the same scaleas the scattered waveﬁeld, is small. On the other hand, using an LBFGSoptimizer, the loss function is smoother as shown in Fig. 13(a), and seemingly admits a lower loss. However, the predicted scattered wave-ﬁeld from the network, shown inFig. 13(b), looks almost identical to the one obtained from the Adam optimizer. This can be further veri ﬁed by observing the difference between the predicted scattered wave ﬁeld and the true one shown inFig. 13(c), plotted again at the same scale. Theerrors seem to be similar to that observed with the Adam optimizer. Sothe effective differences in the loss has limited effect on the wave ﬁeld.Considering that the results from using the Adam and the LBFGSoptimizers are similar, and since the LBFGS optimizer is more expensivedepending on the memory parameters, we suggest, as Raissi et al. (2019) suggested, using initially the Adam optimizer, which is extremely pop-ular in ML optimizations.5. DiscussionsMachine learning provides a platform for predicting outputs bymainly recognizing the corresponding patterns of the inputs through atraining process. Waveﬁelds are by deﬁnition smooth and differentiable other than at the source, which is a requirement for a functional NNsolution output (Hornik et al., 1989). The input to the proposed neuralnetwork is a location in space and the output is the wave ﬁeld (or the scattered waveﬁeld) that satisﬁes a cost function given by the Helmholtz
equation or its variant. These equations depend on the velocity and thesource function, or in our implementation, the background wave ﬁeld and the velocity perturbations. So the NN weights and biases are expected toabsorb the velocity and source information in their efforts to learn topredict the solution of the wave equation. As the NN tries to learn thewaveﬁeld, the source location is, especially, inﬂuential as it determines the epicentre of the waveﬁeld. The velocity has generally a second-ordereffect on the wave shape, compared to the source location. Meanwhile,the frequency mainly controls the wavelength. These facts can help usdecide on how to use the network for any successive wave ﬁeld solutions. For example, solutions for any additional velocity perturbations thatmaybe extracted from any velocity model update procedure like migra-tion velocity analysis or full waveform inversion. Speci ﬁcally, the current NN model can be used as an initial model for training on the updatedvelocity.By using the Born (Lippmann–Schwinger equation) version of thewave equation instead of the Helmholtz solver, we avoided the biasrequired in better sampling the source region in the training of thenetwork, necessary to mitigate the effect of the source singularity. So byusing a homogeneous background model in which the wave ﬁeld can be
Fig. 10.a) The real part of the predicted scattered wave ﬁeld using a 12-layer network. b) The difference between Fig. 6(b) and 10(a)(True and predicted real parts of the scattered waveﬁelds). c) The velocity model computed from the predicted wave ﬁeld.
Fig. 11.a) A 3D model. b) The real part of the resulting 10 Hz wave ﬁeld for a source on the surface located in the middle. c) The NN architecture with dimensions of the 8 hidden layers given by (64,64,32,32,16,16,8,8) from shallow to deep.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
17solved analytically (and instantly), the perturbations (difference betweentrue and background models) will often extend the model domain, andrandom samples of the model space can be used in the training. Toremove the signature of the source singularity from the scattered wave-ﬁeld, the background velocity should be chosen equal to the velocity atthe source. For marine data, this is given by a velocity of approximately1.5 km/s. In general, the accuracy of predicting the wave ﬁeld depends mainly on the complexity of the waveﬁeld. Near the source, the wave-ﬁeld is complex, but it could also be complex in many other areasdepending on the velocity model. In this case, we will need a largerneural network model, as well as better sampling of these complex re-gions. The random training points will often sample the domainreasonably well, but it does not take into account the complexity of thewaveﬁeld. From our observation, and especially with the Marmousimodel, for aﬁxed neural network model size and random sampling of thetraining, the NN provides a uniformly smooth wave ﬁeld compared the true one. We can also utilize the concept of collocation points, as well, asadaptively adding points in regions requiring more emphasis (i.e., withhigh residuals) (McFall and Mahan, 2009). These options have their own cost. The number of training samples used to train the NN to predict thescattered waveﬁeld is a delicate matter (Zhou and Wu, 2011). It directly affects the cost of the training and yet it is necessary that we have enoughsamples to accurately train the network to predict the wave ﬁeld. Training examples and their inﬂuence on the training is an ongoing research topicin the machine learning community.Another feature of using a cost function for NN training, like in Raissi et al. (2019), we canﬁt the boundary condition or even the data as part ofthe objective and, thus, include two or more terms in the cost function.For the dataﬁtting case, this amounts to something like the wave ﬁeld reconstruction method (Van Leeuwen and Herrmann, 2013), which is also solved in the frequency domain and faces similar challenges withregard to data and model sizes (Song et al., 2021). Thus, an importantfeature of such neural network waveﬁeld solutions is theﬁxed memory requirements, mainly controlled by the architecture of the network. It is,thus, independent of the size of the gridded velocity model. As we saw,the errors associated with reducing the size of the network are not of thedispersion kind, like for conventional numerical solvers considering thevelocity model discretization, but they manifest themselves in smoothingthe waveﬁeld.The cost of training the neural network depends on the number ofrandom samples used in the training (the training set) to optimize thenetwork parameters, as well as, the size of the network. As the trained NNtries toﬁt the loss function given by the wave equation or its Born formand any boundary conditions, the size of the network, including thenumber of layers and neurons, deﬁnes the details of the predicted waveﬁeld. As we saw, smaller networks, cheaper to train, admitsmoother waveﬁelds. Thus, the size of the neural network will depend onthe application and the objective involved in the application. For moreaccurate waveﬁelds, the cost can be much higher than conventionalnumerical methods, as we are solving an optimization problem startingfrom random initialization. Transfer learning, in which we use the pre-viously trained network as an initial NN model for a slightly updatedwave
ﬁeld due to a small shift in the source location or a change in thevelocity model, can reduce some of this cost ( Song and Alkhalifah, 2021). On the other hand, an application like waveform inversion may requiresmoother waveﬁelds in the early iterations as we build up the back-ground low wavenumber model and, thus, a neural network wave ﬁeld can help us obtain smoother velocities and/or gradients without the needfor smoothing or spatialﬁltering. Intuitively, the smoother the waveﬁeld, the less NN parameters we will need, which bodes well for effecient lowfrequencies. Interestingly, the cost increase of enlarging the network topredict waveﬁelds for higher frequencies, is less severe than that neededforﬁnite difference methods. However, we noticed that for high fre-quencies the training is harder as we use inverse tangent activation
Fig. 12.a) The loss function for the training of the NN using an Adam optimizer. b) The real part of the predicted scattered wave ﬁeld from the NN. c) The difference between the predicted waveﬁeld and the true one inFig. 11(b).
Fig. 13.a) The loss function for the training of the NN using an LBFGS optimizer. b) The real part of the predicted scattered wave ﬁeld from the NN. c) The difference between the predicted scattered wave ﬁeld and the true one inFig. 11(b).T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
18functions to develop the sinusoidal waveﬁeld. An alternative is to use sine activation functions, which we plan to investigate in the near future.Though, at this early stage of using this functional ML solution, thecost may not justify replacing the regular Helmholtz solvers for 2D, andmaybe even 3D, isotropic examples, the potential and ﬂexibility of the approach will induce more interesting applications. This includes appli-cations on more complex physics, like anisotropy (i.e. ( Song et al., 2021)) and elasticity, where the regular Helmholtz solver becomes impractical.This includes applications involving complex wave ﬁelds in 3D, like those for orthorhombic anisotropy, even if the perturbations are small. Con-ventional frequency-domain solutions for such complex physics are hardand somewhat beyond our capability, especially if the model size is large.Machine learning is an optimal platform for large problems and largedata as it adapts to this objective and learns the appropriate solution byrecognizing patterns. Thus, for complex physics, our cost function willchange, and possibly include more terms, but the training machinery isthe same and does not involve solving for the inverse of a large matrix.6. ConclusionsWe trained a neural network to provide functional solutions to theHelmholtz equation. To avoid the point source singularity, we use a fullyconnected network that takes in space coordinates within the domain ofinterest and outputs the real and imaginary parts of the scattered (insteadof the full) waveﬁeld in the frequency domain. The background velocityis homogeneous, which admits analytical solutions of the backgroundwaveﬁeld. With automatic differentiation, the network is capable, aswell, of evaluating the partial derivatives of the scattered wave ﬁeld necessary to evaluate the loss function given by the Lippmann Schwingerform of the wave equation. This loss function is used to update thenetwork parameters. With a scattered wave ﬁeld corresponding to per- turbations spanning the space domain, the training of the network can beperformed with less random samples. However, the network and thenumber of samples should increase with an increase in frequency. Thisincrease in cost is far less than the exponential increase we experience inthe case of increasing frequency forﬁnite difference methods. Overall, the output waveﬁelds are somewhat smoother than the exact ones, andthis can be attributed to the compromise feature of our relatively smallnetwork and this feature might be useful for applications like waveforminversion. In fact, the smaller the network, the smoother the outputscattered waveﬁeld.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsWe thank KAUST and KFUPM for their support, and the seismic waveanalysis group (SWAG) for constructive discussions.Appendix A. Supplementary dataSupplementary data to this article can be found online at https://do i.org/10.1016/j.aiig.2021.08.002.References
Alterman, Z., Karal Jr., F., 1968. Propagation of elastic waves in layered media by ﬁnite difference methods. Bull. Seismol. Soc. Am. 58, 367 –398. Aminzadeh, F., Burkhard, N., Nicoletis, L., Rocca, F., Wyatt, K., 1994. SEG/EAGE 3-Dmodeling project: 2nd update. Lead. Edge 13, 949 –952.Araya-Polo, M., Farris, S., Florez, M., 2019. Deep learning-driven velocity model buildingworkﬂow. Lead. Edge 38, 872a1 –872a9. Baydin, A.G., Pearlmutter, B.A., Radul, A.A., Siskind, J.M., 2018. Automaticdifferentiation in machine learning: a survey. J. Mach. Learn. Res. 18 . Claerbout, J.F., 1985. Imaging the Earth's Interior, vol. 1. Blackwell scienti ﬁc publications Oxford.Cl/C19ement, F., Kern, M., Rubin, C., 1990. Conjugate gradient type methods for the solutionof the 3D Helmholtz equation. In: Proceedings of the First Copper MountainConference on Iterative Methods . Courant, R., 1928. On the partial difference equations of mathematical physics. Math.Ann. 100, 32–74.Dwivedi, V., Parashar, N., Srinivasan, B., 2021. Distributed learning machines for solvingforward and inverse problems in partial differential equations. Neurocomputing 420,299–316.Holm-Jensen, T., Hansen, T.M., 2020. Linear waveform tomography inversion usingmachine learning algorithms. Math. Geosci. 52, 31 –51. Hornik, K., Stinchcombe, M., White, H., 1989. Multilayer feedforward networks areuniversal approximators. Neural Network. 2, 359 –366. Hughes, T.W., Williamson, I.A., Minkov, M., Fan, S., 2019. Wave physics as an analogrecurrent neural network. Science advances 5, eaay6946 . Kissas, G., Yang, Y., Hwuang, E., Witschey, W.R., Detre, J.A., Perdikaris, P., 2020.Machine learning in cardiovascular ﬂows modeling: predicting arterial blood pressure from non-invasive 4Dﬂow MRI data using physics-informed neural networks. Comput. Methods Appl. Mech. Eng. 358, 112623 . Lippmann, B.A., Schwinger, J., 1950. Variational principles for scattering processes. I.Phys. Rev. 79, 469.Liu, D.C., Nocedal, J., 1989. On the limited memory BFGS method for large scaleoptimization. Math. Program. 45, 503 –528. McFall, K.S., Mahan, J.R., 2009. Arti ﬁcial neural network method for solution of boundary value problems with exact satisfaction of arbitrary boundary conditions.IEEE Trans. Neural Network. 20, 1221 –1233. Mosser, L., Dubrule, O., Blunt, M.J., 2018. Stochastic reconstruction of an ooliticlimestone by generative adversarial networks. Transport Porous Media 125, 81 –103. Neal, B., Mittal, S., Baratin, A., Tantia, V., Scicluna, M., Lacoste-Julien, S., Mitliagkas, I.,2018. A Modern Take on the Bias-Variance Tradeoff in Neural Networks, arXivPreprint arXiv:1810.08591. Ovcharenko, O., Kazei, V., Kalita, M., Peter, D., Alkhalifah, T., 2019. Deep learning forlow-frequency extrapolation from multioffset seismic data. Geophysics 84,R989–R1001.Pinkus, A., 1999. Approximation theory of the MLP model. Acta Numerica 1999 88,143–195.Pratt, R.G., 1999. Seismic waveform inversion in the frequency domain, Part 1: theoryand veriﬁcation in a physical scale model. Geophysics 64, 888 –901. Raissi, M., Perdikaris, P., Karniadakis, G.E., 2019. Physics-informed neural networks: adeep learning framework for solving forward and inverse problems involvingnonlinear partial differential equations. J. Comput. Phys. 378, 686 –707. Richards, P.G., Aki, K., 1980. Quantitative Seismology: Theory and Methods, vol. 859.Freeman, New York.R€oth, G., Tarantola, A., 1994. Neural networks and inversion of seismic data. J. Geophys.Res.: Solid Earth 99, 6753 –6768. Sahli Costabal, F., Yang, Y., Perdikaris, P., Hurtado, D.E., Kuhl, E., 2020. Physics-informedneural networks for cardiac activation mapping. Frontiers in Physics 8, 42 . Sirgue, L., Pratt, R.G., 2004. Efﬁcient waveform inversion and imaging: a strategy forselecting temporal frequencies. Geophysics 69, 231 –248. Sirgue, L., Etgen, J., Albertin, U., 2008. 3D frequency domain waveform inversion usingtime domainﬁnite difference methods. In: 70th EAGE Conference and ExhibitionIncorporating SPE EUROPEC 2008. European Association of Geoscientists & Engineers.Song, C., Alkhalifah, T.A., 2020. Ef ﬁcient waveﬁeld inversion with outer iterations and total variation constraint. IEEE Trans. Geosci. Rem. Sens. 58, 5836 –5846.https:// doi.org/10.1109/TGRS.2020.2971697 . Song, C., Alkhalifah, T., 2021. Wave ﬁeld Reconstruction Inversion via Physics-Informed Neural Networks, arXiv.Song, C., Alkhalifah, T., Waheed, U.B., 2021. Solving the frequency-domain acoustic VTIwave equation using physics-informed neural networks. Geophys. J. Int. 225,846–859.Sorteberg, W.E., Garasto, S., Pouplin, A.S., Cantwell, C.D., Bharath, A.A., 2018.Approximating the Solution to Wave Propagation Using Deep Neural Networks, arXivPreprint arXiv:1812.01609. Van Leeuwen, T., Herrmann, F.J., 2013. Mitigating local minima in full-waveforminversion by expanding the search space. Geophys. J. Int. 195, 661 –667. Wrona, T., Pan, I., Gawthorpe, R.L., Fossen, H., 2018. Seismic facies analysis usingmachine learning. Geophysics 83. O83 –O95. Wu, Z., Alkhalifah, T., 2018a. An ef ﬁcient Helmholtz solver for acoustic transversely isotropic media. Geophysics 83, C75 –C83. Wu, Z., Alkhalifah, T., 2018b. A highly accurate ﬁnite-difference method with minimum dispersion error for solving the helmholtz equation. J. Comput. Phys. 365, 350 –361. Zhang, Z.-D., Alkhalifah, T., 2019. Regularized elastic full-waveform inversion using deeplearning. Geophysics 84, R741 –R751. Zhou, Y., Wu, Y., 2011. Analyses on in ﬂuence of training data set to neural network supervised learning performance. In: Advances in Computer Science, IntelligentSystem and Environment. Springer, pp. 19 –
25.T. Alkhalifah et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 11 –19
19