 A A S R I  P r o c e d i a    6   (  2 0 1 4  )   8 2  –  8 8  Available online at www.sciencedirect.com
2212-6716 © 2014 The Authors. Published by Elsevier B. V. Open access underCC BY-NC-ND license.  
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute
doi: 10.1016/j.aasri.2014.05.012 
ScienceDirect
D
 c,
Abst
Due 
perfo
eithe
Field
neura
imag
benc
using
© 20
Sele
Keyw
* 
E2013 2n d
Design an
Subadra M
aProfe
bAssistant P
dPG Scholar,Ap p
tract
to advanceme n
orm "intelligen t
er by analog ha r
d P r o g r a m m a b l
al networ k. By 
ge processing, p
hmark XOR p r
g VHDL. The d
013 The Auth
ection and/or p
words: FPGA; O n
Subadra Murug a
E-mail address: * sd AASRI C o
nd Impl e
Murugana*,
essor, Departme n
Professo r, Depa r
plied Electronics,
nts in technol o
t" tasks simila r
rdware or mas s
le Gate Array 
this fast protot y
pattern recogni
roblem using b
design works at 
ors Publishe d
peer review u n
n-chip learning; B
an.. Tel.: +0-979- 0
subadra_m@yah oonference o
ementat i
Chip L
, Packia L a
nt of ECE, Einste i
rtment of ECE, E
 Department of E
ogy, many inte g
r to those perfo
sively by parall
(FPGA) as thi s
yping is possib l
tion and classi
ack propagatio
5.332 MHz an
d by Elsevier B
nder responsi b
Back propagatio n
096-8632; $ Pac k
oo.com, $krishba gon Comput a
ion of M
Learnin g
akshmi Kb$
in College of Eng
Einstein College of
ECE, Einstein Co
grated circuits
ormed by the h
lel computers. T
s h e l p s  i n  l e a r n
le for real-time 
fication. In thi s
n based multil a
d the total gate 
B.V. 
bility of Ame r
n; Multilayer perc
kia Lakshmi  K
gya@gmail.comational Intel l
Multila ye
in Virt e
$, Jeyanthi 
gineering,Tirunel v
of Engineering,Ti r
llege of Enginee r
are fabricated 
human brain. M
This proposed w
ning capabilit y
applications, s u
s work on-chi p
ayerperceptro n
count is 4, 73, 2
rican Applied 
ceptron.  ligence and 
r Perce p
ex-E 
Sunda rc, M
veli, Tamil Nadu ,
runelve li,Tamil N
ring,Tirunelve li, T
to develop an 
Many of them u
work is about a
y b y  e x p l o i t i n g
uch as speech r
p learning met h
n and is imple m
237.  
Science Res eBioinform a
ptron wi t
MathiVath a
, India-627 012. 
Nadu, India-627 0
Tamil Nadu, India
artificial syst e
use off-chip lea r
a trainable neu r
g t h e  i n h e r e n t  p
recognition, sp e
hod is designe d
mented in VIR T
earch Institute atics 
th On-
ani Kd 
012. 
a-627 012,. 
em that could 
rning method 
ral chip using 
parallelism of 
eech synthesis, 
d for standard 
TEX-E FPGA 
© 2014 The Authors. Published by Elsevier B. V. Open access under CC BY-NC-ND license.  
Peer-review under responsibility of Scientific Committee of American Applied Science Research Institute83  Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
1. Introduction  One of the emerging applications of Very Large Scal e of Integration (VLSI) is standalone neural network chip. This stand alone neural network chip could surpass the capabilities of conventional computer-based pattern recognition systems and they are used in  pattern classification, data processing, electrical load forecasting, power control systems, quantitative weather forecasting, games development, optimization problems etc. [1]. Artificial Neural Networks (ANNs) are powerful tool for modeling especially when underlying data relationship is unknown. It offers a co mpletely different approach to solve the real-time problems and they are known as sixth generation of computing techniques [2] . Most of the existing neural network applications in commercial use are normally devel
 oped by software and sequentially simulated on a general-purpose processor [3]. This is the easiest but least favoured method, as the time taken for training is long. It is suitable for the networks which need not to be adapted to new data and used only for investing the capability of modeling the network [4]. However, there is some specific real- time application such as streaming video compression, bioinformatics which demands high volume adaptive real-time processing and learning of large non-lin ear dataset within a stipulated time [5]. Therefore, it necessitates the design of neural network hardware with truly parallel processing capabilities and reconfigurability for future extendable applications [6]. This can be achieved by on-chip learning method. It is a desirable method since it develops a way to make a stand-alone neural network chips. In on-chip learning method, hardware keeps itself ready to fix the archite cture depending on the weight, to obtain the required performance by taking the full advantage of their in herent parallelism and runs faster in the order of magnitude than software simulation [7]. Chandrashekhar et al. (2013) compared the software and hardware implementation methods. Particularly they compared different hardware implementation methods like VLSI (analog/digital), Application Specific Integrated Circuit (ASIC) and FPGA. They have chosen FPGA implementation for their work because of high degree of programmability [8]. Suhap Sahin et al. (2006) compared the hardware implementation method using VLSI neural chips and FPGA. They have explaine d the usage of FPGA over VLSI chips for real-time applications. They described the FPGA features and concluded that FPGAs has higher speed and smaller size for real-time application than the VLSI design especially  for classification [9]. Janardan Misra et al. (2010) also analyzed the different hardware implementation like an alog neural chip, digital neural chip, RAM-based implementation method and FPGA implementation method. Finally they have concluded that FPGAs are low cost, readily available, and reconfigurable offering software like flexibility. They have further stated that the partial and online reconfiguration capabilities in the latest generation of FPGAs offer additional advantages [5].  This paper aims at the design of on-chip learning Mu ltilayer Perceptron (MLP) based neural network with Back Propagation (BP) algorithm for learning to solve XO R problem. Section 1.1 reviews the architecture and Section 1.2 describes the learning algorithm of neural ne twork. Section 2 deals with the implementation of on- chip learning followed by discussion of results. 1.1. Network architecture Multilayer perceptron is one of the widely used universal approximator and is suitable to solve the non-linear separable problem [10]. Architectural parameters such as the number of inputs per neuron, weights, activation function, synaptic interconnection and number of layers are to be specified in ANN structure as they play significant role while learning [11]. Logi cal XOR function has two inputs and one output based upon which MLP is structured as in Fig 1.The ANN topology requires solving a non-linearly separable 84   Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
problem consisting of at least one hidden layer [7]. Henc e, in this work 2-2-1 MLP structure is designed with sigmoid activation function to solve XOR problem.   
Fig. 1. Architecture of MLP for XOR problem  
1.2. Learning process The main objective of learning is to achieve good generalization that is able to predict the output values that are associated with a given input value [12]. The back propagation method is a supervised learning algorithm that is widely used for training the multilayer perceptron. It adjusts the weight values that are calculated from input-output mappings and minimize the error between the correct output value and target value. It iteratively computes the values of weight using gradient descent algorithm [10].  For the
 MLP with input vector x
i, output vector y j the weight is calculated by gradient descent rule and is given by (1) 
'wi =  Į  yj(1-yj)  (tj-yj)   xi                                                                                                                       (1) where, Į is learning rate, y
j (1-y j) is the derivative of activation function and  (t j-yj) is error with t i as target. BP algorithm is based on repeated application of two passes namely- Forward pass and Feedback pass.  1.2.1. Forward pass In forward pass, the input data travels from the input la yer to output layer to obtain the output value at each processing element and the corresponding error value is calculated at the output layer.  The output error computation is calculated as the difference between the actual output (y1, y2, … yn) and the desired output (t)  and  is calculated by equation (2) and (3) 
Gk = Yk (1-yk) (tk-yk)   for each output neuron(k)                                                                                     (2) 
        In put  la yer  
    Hidden la yer
 Output layer
Bias (b2) 
Input  
Input     
 Bias (b1)
Output 
 Bias (b3) 85  Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
- Gh = Yh (1-yh) 6kwh,k Gkfor each hidden unit(h)                                                                                    (3)1.2.2. Feedback pass In this step the error value is propagated backward th rough the network, layer by 
 layer in order to update the weight for the expected output. For each network weight w
i,j, weight updation can be calculated as (4) wi,j(new) = wi,j(old) + 
'wi                                                                                                                       (4) where 'w
i,j= K G j xi,j which is  called as magnitude of  weight updates. If the output is correct (t=y) the weights are not changed ('w
i,j =0) otherwise the weights w i are updated. This training process is repeated until the output error signal falls below a predetermined th reshold value or Mean Squared Error (MSE).  2. Implementation As the logical XOR problem is used to benchmark the learning ability of  ANN, this work utilizes on-chip propagation learning algorithm for solving XOR using VIRTEX FPGA. Fig 2 shows the complete BP algorithm implementation module. Forward pass is controlled by forward phase controller and backward pass of the learning process is controlled by backward phase controller. Global controller is used to synchronize the all modules.  
Fig. 2. BP algorithm implementation module 1 
1 
1
311
Global Controller 
Error Com parison block
Forward Phase Controller 
 Backward Phase Controller 
 Gradient Phase Controller 
3
33
 
 Input La yer
 Input ROM 
  Target ROM 
 
 Hidden Layer
Neuron Module
Gradient Module
Weight updating Module 
 
 Output La yer
Neuron Module 
Gradient Module 
Weight updating Module 
Error Calculation Module 
RAM
RAM1 3186   Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
Each neuron in all layers is stimulated simultaneously and forwards the output to next layer. Finally the output is produced at the output layer by applying th e activation function to calculate the value of weighted sum. This final output is given to the error calculation module to find MSE. Error comparison module of global controller is used for checking the condition of learni ng. If the error value is greater than the threshold value, global controller sends enabling signal to backward phase controller. The backward phase of BP algorithm consists of two main important phase  xGradient calculation phase - calculation of new weight using gradient desc
 ent method xWeight updating phase - new weight value is updated in all layer neurons At fir
st gradient value of output neuron is calculated  and back propagated to previous hidden layer for its gradient calculation. After the gradient calculation, weight updating process takes place simultaneously for each layer and e
very weight of neuron. The new updated value is stored in RAM for future use. Thus completion of weight updating process indicates one co mplete training set and these two stages are repeated for all other input patterns until the network is sufficiently trained. VHDL code is written for complete module of XOR problem using IEEE 754 standard single precision floating point arithmetic package and sigmoid activation function package. Hardware implementation of activation function is done using the approximati on function using the following equation (5) [13].   ݂ሺݐ݁݊ሻൌͳ Ȁ ʹ ʹቂ
௡௘௧ȁ௡௘௧ȁ൅ͳቃ                                                                                                                  (5) 3. Implementation Results For on-chip training of XOR problem, the complete module is coded using VHDL and realized in VIRTEX -E using Xilin14.5 ISE. Once the design is completed, the top module is synthesized and verified for its timing and functionality. The Fig 3 & Fig 4 illustrates the training phase of the neural network in forward and backward pass respectively. From the figure it can be noted that the weight updating process is completed according to the error calculated in forward phase. The MSE ensures that the network is sufficiently trained.  The resource utilization is given in the Table 1.        
Table 1. Device utilized by the complete XOR module Logic Utilization Used Available Utilization 
Total Number Slice Registers 2,851 64,896 4% 
Number of occupied slices 32,015 32,448 98% Number of slice containing only related logic 32,015 32,015 100% Total Number 4 input LUTs 61,386 64,896 94% Number used as logic 56,958 -- -- Number of GCLKs 4 4 100% Number of GCLKOBs 1 4 25% Total equivalent gate count for design 473,237 -- -- 87  Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
 
Fig. 3. forward phase 
Fig. 4. backward phase 
4. Conclusion FPGA retains a high degree of flexibility for device r econfiguration and reduces the hardware development cycle. The proposed work is coded using VHDL and successfully simulated in MODELSIM 6.3f simulator and implemented in VIRTEX E FPGA using XILINX ISE 14.5 ISE. This work sustains the internal parallelism of artificial neural network and the design works at 0.6053315 μs. The design have utilised 87% of LUTs and 98% of slices. The result showcase the suitability of enhancing the MLP architecture with back propagation learning that can suit for real-time applications.  Future work will be focused on designing the complex ANN to solve real-time problems.   Acknowledgements This work is supported by the Department of Scie nce and Technology (DST), Government of India under SERB  Fast track Scheme for Young Scientists. Th e authors would like to acknowledge DST, India and Management and Principal of Einstein College of Engi neering, Tamil Nadu, India for providing the facility and necessary support to design and implement this work. References [1] Ayman Youssef, Karim Mohammed, Amin Nassar. A Reconfigurable, Generic and Programmable Feed Forward Neural-network. IEEE 14th Internationa l Conference on Modelling and Simulation: 2012. [2] Pinjare, Arun Kumar M. Implementation of Neural Netw ork Back Propagation Training Algorithm on       FPGA. International Journal of Computer Applications, 2012: (0975 – 8887). [3] Packia Lakshmi.K, M. Subadra, PhD. Design and Realization of FPGA based Off-Chip Trained MLP for 
88   Subadra Murugan et al.  /  AASRI Procedia   6  ( 2014 )  82 – 88 
Classical XOR Problem and Need of On-Chip Training. Sp ecial Issue of International Journal of Computer Applications (0975 – 8887) on International Conference on Electronics, Communication and Information Systems (ICECI 12): 2012. [4] Jagath C, Rajapakse & Amos R. Omondi. FPGA implementation of Neural Networks”. ISBN-10 0-387-28487-7, Spriger: 2006 [5] Janardan Misra, Indranil Saha b. Artificial neural networks in hardware:  A survey of two decades of progress. Elsevier, Neurocomputing 2010: pp:239–255. [6] Lorena P. Vargas1, Leiner Barba, Torres & L Mattos. Sign Language Recognition System using Neural Network for Digital Hardware Implementation.  Journal of Physics: Conference Series: 2011. [7] Packia Lakshmi.K , M. Subadra, PhD. A survey on FPGA based MLP realization for on-chip learning”, International Journal Of Scientific & Engineering Research. 2012; Volume 4:ISSN 2229-5518. [8] Chandrashekhar Kalbande, Anil Bavaskar. Implementation of FPGA based general purpose artificial neural network. ITSI Transactions on Electrical and Electronics Engineering.2013: 2320 – 8945. [9] Suhap Sahin, Yasar Becerikli, Suleyman Yazic. Neural Network Implementation in Hardware Using FPGA. Springer-Verlag Berlin Heidelberg, ICONIP, 2006:1105 – 1112. [10] Faycal benrekia, mokhtar & mounir bouheda. Gas sensor characterization and multilayer  perceptron (MLP) hardware implementation for gas identificati on using a field programmable gate  array (FPGA) programmable Gate Array(FPGA). Sensors journal 2013: pp: 2967-2985.  [11] Medhat Moussa and Shawki Areibi & Kristian Nichols . Arithmetic precision for implementing BP networks on FPGA", Springer, 2006 : pp.37-61. [12] Kuno Kollmann, Karl-Ragmar Riemschneider , Ha ns Christoph Zeidler . On-Chip Backpropagation Training Using Parallel Stochastic Bit Streams. IEEE:1996. [13] Thamer & khammas.  Implementation of a sigmoi d activation function for neural network using FPGA. 13
th scientific conference of al-ma’moon uninersity college, Iraq; 2012. 