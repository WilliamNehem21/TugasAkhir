Artificial Intelligence in Geosciences 3 (2022) 162â€“178
Available online 24 December 2022
2666-5441/Â© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC
BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available at ScienceDirect
Artificial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artificial-intelligence-in-geosciences
Original research articles
Geostatistical semi-supervised learning for spatial prediction
Francky Fouedjioa,âˆ—, Hassan Talebib
aRio Tinto, Data & Analytics, 152-158 St Georges Terrace, Perth, WA 6000, Australia
bRio Tinto, Development & Technology, 152-158 St Georges Terrace, Perth, WA 6000, Australia
A R T I C L E I N F O
Keywords:
Labeled spatial data
Unlabeled spatial data
Spatial autocorrelation
Pseudo labeling
Spatial predictionA B S T R A C T
Geoscientists are increasingly tasked with spatially predicting a target variable in the presence of auxiliary
information using supervised machine learning algorithms. Typically, the target variable is observed at a
few sampling locations due to the relatively time-consuming and costly process of obtaining measurements.
In contrast, auxiliary variables are often exhaustively observed within the region under study through the
increasing development of remote sensing platforms and sensor networks. Supervised machine learning
methods do not fully leverage this large amount of auxiliary spatial data. Indeed, in these methods, the training
dataset includes only labeled data locations (where both target and auxiliary variables were measured). At the
same time, unlabeled data locations (where auxiliary variables were measured but not the target variable)
are not considered during the model training phase. Consequently, only a limited amount of auxiliary spatial
data is utilized during the model training stage. As an alternative to supervised learning, semi-supervised
learning, which learns from labeled as well as unlabeled data, can be used to address this problem. However,
conventional semi-supervised learning techniques do not account for the specificities of spatial data. This
paper introduces a spatial semi-supervised learning framework where geostatistics and machine learning are
combined to harness a large amount of unlabeled spatial data in combination with typically a smaller set
of labeled spatial data. The main idea consists of leveraging the target variableâ€™s spatial autocorrelation to
generate pseudo labels at unlabeled data points that are geographically close to labeled data points. This
is achieved through geostatistical conditional simulation, where an ensemble of pseudo labels is generated
to account for the uncertainty in the pseudo labeling process. The observed labels are augmented by this
ensemble of pseudo labels to create an ensemble of pseudo training datasets. A supervised machine learning
model is then trained on each pseudo training dataset, followed by an aggregation of trained models. The
proposed geostatistical semi-supervised learning method is applied to synthetic and real-world spatial datasets.
Its predictive performance is compared with some classical supervised and semi-supervised machine learning
methods. It appears that it can effectively leverage a large amount of unlabeled spatial data to improve the
target variableâ€™s spatial prediction.
1. Introduction
The prediction of a variable of interest over the whole study region,
in the presence of auxiliary variables observed everywhere within the
area of interest, has become a ubiquitous task in various geoscience
fields ( Giaccone et al. , 2021 ; Du et al. , 2020 ; Maxwell et al. , 2018 ;
Kanevski , 2008 ; Kanevski et al. , 2009 ). Supervised machine learning
methods (e.g., random forest, support vector machines, neural net-
works) have been extensively used for this purpose in a variety of
geoscience applications like geochemical mapping ( Kirkwood et al. ,
2022 , 2016a ; Wilford et al. , 2016 ), soil mapping ( Wadoux et al. ,
2020 ; Taghizadeh-Mehrjardi et al. , 2016 ; Ballabio et al. , 2016 ; Khan
et al. , 2016 ; Hengl et al. , 2015 ), hydrological mapping ( Barzegar et al. ,
2016 ; Appelhans et al. , 2015 ), environmental mapping ( Li, 2013 ; Li
âˆ—Corresponding author.
E-mail address: migrainefrancky.fouedjiokameni@riotinto.com (F. Fouedjio).et al. , 2011 ), and geological mapping ( Albrecht and GonzÃ¡lez-Ãlvarez ,
2021 ; Kumar et al. , 2020 ; Latifovic et al. , 2018 ; Sahoo and Jha , 2017 ;
Othman and Gloaguen , 2017 ; Cracknell and Reading , 2015 , 2014 ;
Yu et al. , 2012 ). In particular, supervised machine learning methods
tailored to spatial data have gained much interest due to their ability
to account for the specificities of spatial data, such as spatial autocor-
relation ( Fouedjio , 2021b ; Talebi et al. , 2021 ; Fouedjio , 2021a ; SekuliÄ‡
et al. , 2020 ; Hengl et al. , 2018 ; Fouedjio and Klump , 2019 ; Fouedjio ,
2020 ). This latter plays a crucial role in the realm of spatial data.
The target variableâ€™s observations are commonly available at a
few sampling locations due to relatively high acquisition costs and
time to obtain measurements. In contrast, auxiliary variables are of-
ten available everywhere within the study region through the wider
https://doi.org/10.1016/j.aiig.2022.12.002
Received 3 November 2022; Received in revised form 21 December 2022; Accepted 21 December 2022Artificial Intelligence in Geosciences 3 (2022) 162â€“178
163F. Fouedjio and H. Talebi
Fig. 1. Synthetic case study â€” (a)â€“(d) spatially exhaustive auxiliary variables and (e) spatially exhaustive target variable.
development of remote sensing platforms and sensor networks. Thus,
spatial data available consists of a very small amount of labeled spatial
data (primary spatial data) and a very large amount of unlabeled spatial
data (secondary or auxiliary spatial data). This situation is common
as acquiring auxiliary spatial data is relatively cheap while collecting
primary spatial data is expensive. Supervised machine learning tech-
niques do not fully exploit this vast amount of auxiliary spatial data.
Indeed, in these techniques, the training dataset includes only labeled
data locations (i.e., data locations at which both the target variableâ€™s
observations and auxiliary variablesâ€™ observations are available). At
the same time, unlabeled data locations (i.e., data locations at which
the target variableâ€™s observations are not available but auxiliary vari-
ablesâ€™ observations are known) are not considered during the model
training stage. In other words, only colocated auxiliary spatial data are
considered during the training phase. Thus, the information provided
by auxiliary spatial data beyond those colocated is ignored. As a
consequence, the amount of auxiliary spatial data (unlabeled spatial
data) used during the model training stage is limited.
Machine learning approaches that effectively exploit large quan-
tities of unlabeled data are gaining interest. One such approach issemi-supervised learning, which learns from both labeled and unla-
beled data to build better predictive models ( Chapelle et al. , 2010 ;
Zhu and Goldberg , 2009 ). It contrasts supervised learning (data all la-
beled) and unsupervised learning (data all unlabeled). Semi-supervised
learning is of great interest in machine learning because, in addition
to labeled data, it can use readily available unlabeled data to improve
supervised learning tasks when labeled data are scarce or expensive.
One can distinguish two general forms of semi-supervised learning:
inductive and transductive semi-supervised learning. They differ by the
data availability for which one wants to obtain predictions during the
training phase. The goal of inductive semi-supervised learning is to
find the function that maps auxiliary variables to the target variable
from a combined set of labeled and unlabeled data. This function
can then be used to make predictions on new data points that are
not available during the training phase. Transductive semi-supervised
learning aims to infer the correct labels for the given unlabeled data,
which is already present during the training phase. Semi-supervised
learning methods can be classified into six categories : self-training,
co-training, multi-view learning, expectationâ€“maximization with gener-
ative mixture models, graph-based methods, and transductive supportArtificial Intelligence in Geosciences 3 (2022) 162â€“178
164F. Fouedjio and H. Talebi
Fig. 2. Synthetic case study â€” (a)â€“(d) auxiliary variables and (e) target variable at ğ‘›= 1,000sampling locations.
vector machines ( Chapelle et al. , 2010 ; Zhu and Goldberg , 2009 ). In
particular, self-training combines information from unlabeled data with
labeled data to iteratively identify labels for unlabeled data within the
dataset. Thus, the labeled training dataset is enlarged at each iteration
until the entire dataset is labeled. The self-training algorithm can be
applied as a wrapper to any given supervised base learner. The co-
training method basically trains two models, while self-training trains
one. Thus, self-training can be seen as a particular case of co-training
where just one model is trained.
In order to make any use of unlabeled data, some relationship
to the underlying distribution of data must exist. A necessary condi-
tion of semi-supervised learning is that the underlying marginal data
distribution over the attribute space contains information about the
posterior distribution. Semi-supervised learning algorithms make use of
at least one of the following assumptions: the smoothness assumption
(if two data points are close in the attribute (feature) space, their labels
should be similar), the low-density assumption (the decision boundary
should not pass through high-density areas in the attribute space), and
the manifold assumption (data points on the same low-dimensionalmanifold should have the similar label). These assumptions are the
foundation of most, if not all, semi-supervised learning algorithms,
which generally depend on one or more of them being satisfied, either
explicitly or implicitly. Thus, if sufficient unlabeled data is available
and under certain assumptions about the distribution of the data, the
unlabeled data can help in the construction of a better predictive model.
If, on the other hand, this condition is not met, it is inherently impos-
sible to improve the accuracy of predictions based on the additional
unlabeled data ( Zhu and Goldberg , 2009 ). It is worth pointing out that
blindly selecting a semi-supervised learning method for a specific task
will not necessarily improve performance over supervised learning. In
fact, unlabeled data can lead to worse performance with the wrong link
assumptions. For a detailed review on semi-supervised learning, refer
to Van Engelen and Hoos (2020 ), Pise and Kulkarni (2008 ).
While semi-supervised learning for non-spatial data has been gain-
ing much attention, semi-supervised learning dedicated to spatial data
has so far been scarce ( Kobs et al. , 2021 ; Asghar et al. , 2020 ; Vat-
savai et al. , 2007 ). Classical semi-supervised learning techniques have
been developed in a non-spatial context. As a consequence, they doArtificial Intelligence in Geosciences 3 (2022) 162â€“178
165F. Fouedjio and H. Talebi
Fig. 3. Synthetic case study â€” experimental and fitted variograms of the target
variableğ‘Œin the original training dataset ( ğ‘›= 1,000). The fitted variogram model cor-
responds to an isotropic exponential model with a practical range and sill respectively
equal to 155.010 and 346.489.
not account for the characteristics of spatial data, especially spatial
autocorrelation, which is widely ignored. In the spatial framework, in
addition to the attribute space, the geographical space needs to be taken
into account. Indeed, spatial data often show the property of spatial
dependency over the study region. Observations located close to one
another in the geographical space might have similar characteristics.
Considering geographical coordinates as other auxiliary variables is
not the best way to account for the spatial information, as shown
in many research works ( Hengl et al. , 2018 ). Applying traditional
semi-supervised learning methods in the realm of spatial data would
be liable to produce less accurate predictions. Indeed, the success of
semi-supervised learning depends critically on some underlying as-
sumptions ( Chapelle et al. , 2010 ; Zhu and Goldberg , 2009 ). A basic
approach to exploit unlabeled spatial data consists of using unlabeled
spatial data as additional predictor variables in a supervised learn-
ing task. Thus, predictor variablesâ€™ observations surrounding (at the
nearest locations of) a labeled data location are defined as additional
covariates. However, under this approach, the number of covariates
increases drastically, while the number of observations (samples) is still
the same and limited. This situation may lead to the well-known curse
of dimensionality ( Keogh and Mueen , 2017 ). As the dimensionality
increases, the number of data points required for a good performance
of any supervised machine learning algorithm increases exponentially.
Thus, a marginal rise in dimensionality also requires a significant
increase in the data volume to maintain the same level of performance.
This paper presents a geostatistical semi-supervised learning frame-
work that allows harnessing a large amount of unlabeled spatial data
available in many geoscience use cases in combination with typi-
cally smaller sets of labeled spatial data. It focuses on semi-supervised
regression (i.e., where the target variable is continuous), but the pro-
posed framework can be easily adapted to semi-supervised classifica-
tion (i.e., for a categorical target variable). It follows the general idea of
pseudo labeling (label generation) inherent to classical semi-supervised
learning methods such as self-training and co-training. The core idea
consists of leveraging the target variableâ€™s spatial autocorrelation to
generate pseudo labels at unlabeled data points that are geographically
close to labeled data points. This is achieved through geostatistical
conditional simulation, where an ensemble of pseudo labels is gener-
ated to account for the uncertainty in the pseudo labeling process. This
ensemble of simulated (pseudo) labels is augmented by the observed
(true) labels to create an ensemble of pseudo training datasets. Asupervised machine learning model is then trained on each pseudo
training dataset, followed by an aggregation of trained models. As a
byproduct, the target variableâ€™s prediction uncertainty is provided. The
proposed geostatistical semi-supervised learning method is illustrated
on synthetic spatial data for which ground truth is available everywhere
within the study region. It is applied to real-world spatial data for
geochemical mapping. Its predictive performance is compared with
some classical supervised and semi-supervised learning methods.
The rest of the paper is organized as follows. Section 2 describes
the different ingredients and steps of the proposed geostatistical semi-
supervised learning approach. Section 3 illustrates the proposed semi-
supervised machine learning method on synthetic spatial data. An
application example with real-world spatial data is given in Section 4.
Sections 3 and 4 also include a comparison with classical supervised
and semi-supervised learning methods. Concluding remarks are sum-
marized in Section 5.
2. Methodology
Let{ğ‘Œ(ğ¬) âˆ¶ ğ¬âˆˆğ·}be the target variable (continuous) defined
on a fixed continuous spatial domain of interest ğ· âŠ‚Rğ‘‘(ğ‘‘âˆˆNâˆ—).
There are ğ‘predictor (auxiliary or explanatory) variables {ğ—(ğ¬) =
(X1(ğ¬),â€¦,Xğ‘(ğ¬)) âˆ¶ ğ¬âˆˆğ·}exhaustively known in the spatial do-
mainğ·. Suppose that we have labeled spatial data îˆ¸(ğ¬1,â€¦,ğ¬ğ‘›) ={(ğ—(ğ¬1),ğ‘Œ(ğ¬1)),â€¦,(ğ—(ğ¬ğ‘›),ğ‘Œ(ğ¬ğ‘›))}
, with {ğ¬ğ‘–âˆˆğ·}ğ‘–=1,â€¦,ğ‘›representing the
target variableâ€™s sampling locations. îˆ¸(ğ¬1,â€¦,ğ¬ğ‘›)depicts the original
training dataset. In addition to labeled spatial data, unlabeled spatial
dataî‰(ğ¬ğ‘›+1,â€¦,ğ¬ğ‘) ={
ğ—(ğ¬ğ‘›+1),â€¦,ğ—(ğ¬ğ‘)}
are available. Data loca-
tions {ğ¬ğ‘–âˆˆğ·}ğ‘–=1,â€¦,ğ‘›will refer to labeled data locations and {ğ¬ğ‘—âˆˆ
ğ·}ğ‘—=ğ‘›+1,â€¦,ğ‘will denote unlabeled data locations. We are dealing with
the situation where relatively few labeled spatial data are available, but
a large number of unlabeled spatial data are given (ğ‘ â‰«ğ‘› ). The goal is
to leverage labeled and unlabeled spatial data in an attempt to improve
the target variableâ€™s spatial prediction over the spatial domain ğ·. This
section describes the different steps and ingredients to implement the
proposed geostatistical semi-supervised learning for spatial prediction.
The implementation is carried out in the R platform ( R Core Team ,
2021 ).
2.1. Generating Pseudo training data
A basic approach to take advantage of unlabeled data is first to pre-
dict their labels and add the most confident predicted labels back to the
labeled data. This process is known as pseudo labeling in classical semi-
supervised learning methods such as self-training and co-training ( Van
Engelen and Hoos , 2020 ). In these later, pseudo labeling consists of first
training a supervised machine learning model on labeled data. Then,
use the predictions of the trained machine learning model to generate
additional labeled data. Finally, the original labeled data (observed)
and pseudo labeled data (generated) are combined for a final model
retraining. The term pseudo labeling is utilized because these pseudo
labels are not real (observed) labels. We adopt a different strategy to
generate pseudo labels. In the spatial context, one can leverage the
target variableâ€™s spatial autocorrelation to generate more confidently
pseudo labels at unlabeled data locations. The spatial autocorrelation
refers to Toblerâ€™s first law of geography ( Tobler , 1970 ), meaning that
everything is related to everything else, but near things are more
related than distant things. In the presence of spatial autocorrelation,
closer things tend to be more predictable and have less variability. In
contrast, distant things tend to be less predictable and are less related.
Thus, not all unlabeled data locations can be labeled with high confi-
dence. The inaccuracy of pseudo labels for unlabeled data locations that
are too far from labeled data locations may introduce errors into the
machine learning model and cause its degradation. So, not all unlabeled
data locations are beneficial for the model training. Our approachArtificial Intelligence in Geosciences 3 (2022) 162â€“178
166F. Fouedjio and H. Talebi
Fig. 4. Synthetic case study â€” auxiliary variables at selected unlabeled data locations for pseudo labeling.
Fig. 5. Synthetic case study â€” four randomly selected realizations of pseudo labeling through geostatistical conditional simulation at selected unlabeled data locations.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
167F. Fouedjio and H. Talebi
Fig. 6. Synthetic case study â€” prediction maps provided by (a) geostatistical semi-supervised random forest, (b) classical random forest, (d) random forest with unlabeled spatial
data as additional covariates, and (e) self-training random forest.
Fig. 7. Synthetic case study â€” prediction uncertainty maps provided by (a) geostatistical semi-supervised random forest, (b) classical random forest, (c) random forest with
unlabeled spatial data as additional covariates. The black dots represent training data locations. The prediction uncertainty corresponds to the width of the 95% prediction interval.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
168F. Fouedjio and H. Talebi
Fig. 8. Synthetic case study â€” target variableâ€™s observed values vs. target variableâ€™s predicted values at unlabeled data locations, for (a) geostatistical semi-supervised random
forest, (b) classical random forest, (c) random forest with unlabeled spatial data as additional covariates, and (d) self-training random forest. The red line indicates the 1:1 line.
involves leveraging the target variableâ€™s spatial autocorrelation to gen-
erate pseudo labels at unlabeled data locations that are geographically
close to labeled data locations. The target variableâ€™s spatial dependence
structure is used to define the proximity to the labeled data locations.
This latter is usually described using geostatistical tools such as the
variogram ( Chiles and Delfiner , 2012 ). One of the main features of the
variogram, the range, is used to define the proximity to the labeled data
locations. The range is the distance after which the variogram levels
off. The physical meaning of the range is that pairs of locations that
are this distance or greater apart are not spatially correlated. In other
words, points further apart than the range are spatially independent.
The starting point of the geostatistical semi-supervised learning
approach consists of estimating and modeling the target variableâ€™s
spatial dependence structure (variogram) using the target variableâ€™s
observations. This is achieved by calculating the sample (experimental)
variogram through the method of moments, followed by the fitting
(e.g., manual or automated fitting) to a class of theoretical paramet-
ric variogram models (refer to Chiles and Delfiner (2012 ), for more
details about variogram estimating and modeling). Given labeled data
locations, any unlabeled data location that falls within a neighbor-
hood centered at a labeled data location with a radius equal to a
quarter of the variogramâ€™s range of the target variable is selected for
the pseudo labeling. Thus, only unlabeled data locations that have astrong spatial dependency with labeled data locations are considered.
Let{ğ¬ğ‘¡ğ‘˜âˆˆğ·}ğ‘˜=1,â€¦,ğ‘€;ğ‘¡ğ‘˜âˆˆ{ğ‘›+1,â€¦,ğ‘}be the set of unlabeled data locations
selected for the pseudo labeling. The pseudo labeling is carried out
through geostatistical conditional simulation. Geostatistical simulation
is a spatial extension of the Monte Carlo simulation concept. In addition
to reproducing the data histogram, geostatistical simulation also hon-
ors the dataâ€™s spatial dependence structure (variogram). A simulation
method that honors the data in the sense that the simulated value
at a sampling location exactly matches the observed value is termed
a conditional simulation. In contrast, a simulation method that does
not honor the data is called unconditional simulation. It is worth
highlighting that geostatistical simulation accounts for non-Gaussian
data. In this latter case, a Gaussian transformation of the target variable
is part of the simulation process (refer to Chiles and Delfiner (2012 )
and Lantuejoul (2013 ) for more details about geostatistical simulation).
The geostatistical conditional simulation method used for gener-
ating the pseudo labels is the well-known conditioning by kriging,
which is the combination of a non-conditional simulation method
(e.g., spectral turning bands method) and kriging ( Chiles and Delfiner ,
2012 ). Let{
ğ‘Œğ‘™(ğ¬ğ‘¡1),â€¦,ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€)}
ğ‘™=1,â€¦,ğ¿be the ensemble of pseudo
(plausible) labels generated at the selected unlabeled data locations.
Geostatistical simulation allows obtaining multiple pseudo labels at
each selected unlabeled data location instead of a single pseudo label.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
169F. Fouedjio and H. Talebi
Fig. 9. Synthetic case study â€” prediction interval coverage probability plot (accuracy
plot). Method 1 refers to the geostatistical semi-supervised random forest; Method
2 refers to the classical random forest; Method 3 refers to the random forest with
unlabeled spatial data as additional covariates.
Thus, the uncertainty in the pseudo labeling process is taken into
account. Given the set of pseudo labels{
ğ‘Œğ‘™(ğ¬ğ‘¡1),â€¦,ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€)}
ğ‘™=1,â€¦,ğ¿,
one forms the following ensemble of pseudo labeled spatial data{(
ğ—(ğ¬ğ‘¡1),ğ‘Œğ‘™(ğ¬ğ‘¡1))
,â€¦,(
ğ—(ğ¬ğ‘¡ğ‘€),ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€))}
ğ‘™=1,â€¦,ğ¿. This latter ensemble is
used to augment the original training dataset îˆ¸(ğ¬1,â€¦,ğ¬ğ‘›). This results
in the following ensemble of pseudo training datasets:{
îˆ¸ğ‘™(ğ¬1,â€¦,ğ¬ğ‘›,ğ¬ğ‘¡1,
â€¦,ğ¬ğ‘¡ğ‘€)}
ğ‘™=1,â€¦,ğ¿where each îˆ¸ğ‘™(ğ¬1,â€¦,ğ¬ğ‘›,ğ¬ğ‘¡1,â€¦,ğ¬ğ‘¡ğ‘€) ={
(ğ—(ğ¬1),ğ‘Œ(ğ¬1)),
â€¦,(ğ—(ğ¬ğ‘›),ğ‘Œ(ğ¬ğ‘›)),(
ğ—(ğ¬ğ‘¡1),ğ‘Œğ‘™(ğ¬ğ‘¡1))
,â€¦,(
ğ—(ğ¬ğ‘¡ğ‘€),ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€))}
,ğ‘™= 1,â€¦,ğ¿.
Thus, the size of the original training dataset is increased by ğ‘€,
the number of selected unlabeled data locations. The training of a
supervised machine learning model can be improved by exposing it
to as much data as possible. The pseudo labeled spatial data provide
context that will aid the training of a supervised machine learning
model. It is important to note that each pseudo training dataset contains
the original training dataset. Geostatistical conditional simulation is
used here as a label generator and data augmentation approach. In
many cases, at least ğ¿= 50 simulations will provide sufficient infor-
mation to account for the uncertainty in the pseudo labeling. A larger
number of simulations allows a better account for the uncertainty but
requires more computing time. However, it is essential to note that the
generation of pseudo labels can be performed in parallel.
2.2. Training supervised machine learning model
Given the ensemble of pseudo training datasets{
îˆ¸ğ‘™(ğ¬1,â€¦,ğ¬ğ‘›,
ğ¬ğ‘¡1,â€¦,ğ¬ğ‘¡ğ‘€)}
ğ‘™=1,â€¦,ğ¿generated at the previous section, this step consists
of training a supervised machine learning model on each pseudo
training dataset. We opt for the regression random forest, but any
other supervised machine learning method can be used as well. The
regression random forest popularity for spatial prediction relies on
its ability to efficiently deal with many predictor variables, handle
complex nonlinear relationships and interactions, require less data pre-
processing, and be a non-parametric method (model-free). Regression
random forest is a type of ensemble machine learning method that
constructs a multitude of regression tree models on various subsets ofthe training dataset (bootstrap samples) using different subsets of avail-
able predictor variables, followed by aggregation. Under random forest,
each built regression tree model is unique (less correlated with others)
due to the bootstrapping of the training data and the random selection
of subsets of predictor variables. The multiple regression tree models
knitted together reduce the prediction variance and increase prediction
accuracy. The regression random forestâ€™s prediction is obtained by
averaging all of the regression treeâ€™s predictions.
For each pseudo training dataset îˆ¸ğ‘™(ğ¬1,â€¦,ğ¬ğ‘›,ğ¬ğ‘¡1,â€¦,ğ¬ğ‘¡ğ‘€), a random
forest regressor {Ì‚ğ‘“ğ‘™(ğ—(ğ¬)) âˆ¶ ğ¬âˆˆğ·}is built (ğ‘™= 1,â€¦,ğ¿). The number
of trees is set to 1,000, and the other hyper-parameters are optimized
through cross-validation. They include the number of predictor vari-
ables randomly selected at each node, the proportion of observations
to sample in each regression tree, and the minimum number of obser-
vations in a regression treeâ€™s terminal node. The ensemble of random
forest regressors{
Ì‚ğ‘“ğ‘™(ğ—(ğ¬))âˆ¶ğ¬âˆˆğ·}
ğ‘™=1,â€¦,ğ¿is aggregated to provide the
final model as follows:
Ì‚ğ‘Œ(ğ¬) =Ì‚ğ‘“(ğ—(ğ¬))=1
ğ¿ğ¿âˆ‘
ğ‘™=1Ì‚ğ‘“ğ‘™(ğ—(ğ¬)),âˆ€ğ¬âˆˆğ·. (1)
The ensemble of random forest regressors{
Ì‚ğ‘“ğ‘™(ğ—(ğ¬))âˆ¶ğ¬âˆˆğ·}
ğ‘™=1,â€¦,ğ¿
can be also used to generate the prediction uncertainty. This can be
achieved by calculating the inter-percentile range of the predictions
ensemble. Let Ì‚ğ¹(â‹…;ğ¬)be the predictive distribution of the target variable
ğ‘Œat the target location ğ¬âˆˆğ·, obtained from the predictions ensem-
ble{
Ì‚ğ‘“ğ‘™(ğ—(ğ¬))}
ğ‘™=1,â€¦,ğ¿. A100(1 âˆ’ğ›¼)%prediction interval of the target
variableğ‘Œat location ğ¬âˆˆğ·is expressed as [Ì‚ğ‘„ğ›¼âˆ•2(ğ¬),Ì‚ğ‘„1âˆ’ğ›¼âˆ•2(ğ¬)](0<
ğ›¼ < 1); whereÌ‚ğ‘„ğ›¼âˆ•2(ğ¬)denotes the ğ›¼-quantile of Ì‚ğ¹(â‹…;ğ¬)and is defined
asÌ‚ğ‘„ğ›¼âˆ•2(ğ¬) = inf{ğ‘¦âˆ¶Ì‚ğ¹(ğ‘¦;ğ¬)â‰¥ğ›¼}. Thus, the inter-percentile range
Ì‚ğ‘„1âˆ’ğ›¼âˆ•2(ğ¬)âˆ’Ì‚ğ‘„ğ›¼âˆ•2(ğ¬)can be viewed as a measure of prediction uncertainty.
In particular, Ì‚ğ‘„0.975(ğ¬) âˆ’Ì‚ğ‘„0.025(ğ¬)which corresponds to the width of the
95% prediction interval [Ì‚ğ‘„0.025(ğ¬),Ì‚ğ‘„0.975(ğ¬)]will be used as a measure
of prediction uncertainty in the case studies presented at Sections 3 and
4.
The following R packages are used to perform the regression random
forest: ranger (Wright and Ziegler , 2017 ) and tuneRanger (Probst et al. ,
2018 ). It is important to remark that the construction of the ensemble
of random forest regressors{
Ì‚ğ‘“ğ‘™(ğ—(ğ¬))âˆ¶ğ¬âˆˆğ·}
ğ‘™=1,â€¦,ğ¿can be conducted
in parallel. To summarize, the geostatistical semi-supervised learning
approach is implemented using the pseudo algorithm 1.
2.3. Evaluating machine learning model performance
To assess the proposed spatial semi-supervised learning approachâ€™s
ability to predict the target variable accurately, a comparison will
be conducted with classical supervised and semi-supervised machine
learning methods. The comparison will be performed using testing
data, i.e., data kept aside for the whole analysis. The criteria used to
evaluate the prediction accuracy quantitatively are: the mean absolute
error (MAE), the root mean square error (RMSE), the coefficient of
determination (R-square), and Linâ€™s concordance correlation coefficient
(CCC) ( Steichen and Cox , 2002 ). The lower are MAE and RMSE, the
better the model is. R-square and CCC close to 1 indicate a perfect
model. These criteria are computed as follows:
ğ‘€ğ´ğ¸ =1
ğ‘Ÿğ‘Ÿâˆ‘
ğ‘—=1|ğ‘Œğ‘—âˆ’Ì‚ğ‘Œğ‘—|, (2)
ğ‘…ğ‘€ğ‘†ğ¸ =âˆšâˆšâˆšâˆš1
ğ‘Ÿğ‘Ÿâˆ‘
ğ‘—=1(ğ‘Œğ‘—âˆ’Ì‚ğ‘Œğ‘—)2, (3)
ğ‘…2= 1 âˆ’âˆ‘ğ‘Ÿ
ğ‘—=1(ğ‘Œğ‘—âˆ’Ì‚ğ‘Œğ‘—)2
âˆ‘ğ‘Ÿ
ğ‘—=1(ğ‘Œğ‘—âˆ’ğœ‡ğ‘Œ)2, (4)
ğ¶ğ¶ğ¶ =2 Ã—ğœŒÃ—ğœÌ‚ğ‘ŒÃ—ğœğ‘Œ
ğœ2
Ì‚ğ‘Œ+ğœ2
ğ‘Œ+ (ğœ‡Ì‚ğ‘Œâˆ’ğœ‡ğ‘Œ)2(5)Artificial Intelligence in Geosciences 3 (2022) 162â€“178
170F. Fouedjio and H. Talebi
Fig. 10. Real case study â€” spatially exhaustive auxiliary variables: (a) elevation, (b) Landsat 8 band 5, (c) Landsat 8 band 6, (d) gravity survey Bouguer anomaly, (e) gravity
survey high-pass filtered Bouguer anomaly, and (f) Uranium counts from gamma-ray spectrometry.
Algorithm 1 Geostatistical Semi-Supervised Learning for Spatial Prediction
Input : labeled spatial data îˆ¸(ğ¬1,â€¦,ğ¬ğ‘›) ={(ğ—(ğ¬1),ğ‘Œ(ğ¬1)),â€¦,(ğ—(ğ¬ğ‘›),ğ‘Œ(ğ¬ğ‘›))}
and unlabeled spatial data î‰(ğ¬ğ‘›+1,â€¦,ğ¬ğ‘) ={
ğ—(ğ¬ğ‘›+1),â€¦,ğ—(ğ¬ğ‘)}
;
ğ¬ğ‘–âˆˆğ·âŠ‚Rğ‘‘, ğ‘–= 1,â€¦,ğ‘;ğ‘ â‰«ğ‘› .
1.Estimate and model the target variableâ€™s variogram using observations {ğ‘Œ(ğ¬ğ‘–)}ğ‘–=1,â€¦,ğ‘›;
2.Select unlabeled data locations for which the distance to the closest labeled data location is less than the quarter of the variogram range of
the target variable; the outcome is a set {ğ¬ğ‘¡ğ‘˜âˆˆğ·}ğ‘˜=1,â€¦,ğ‘€;ğ‘¡ğ‘˜âˆˆ{ğ‘›+1,â€¦,ğ‘};
3.Generate pseudo labels at the selected unlabeled data locations through geostatistical conditional simulation; the output is an ensemble of
pseudo labeled spatial data{(
ğ—(ğ¬ğ‘¡1),ğ‘Œğ‘™(ğ¬ğ‘¡1))
,â€¦,(
ğ—(ğ¬ğ‘¡ğ‘€),ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€))}
ğ‘™=1,â€¦,ğ¿;
4.Form the ensemble of pseudo training datasets{
(ğ—(ğ¬1),ğ‘Œ(ğ¬1)),â€¦,(ğ—(ğ¬ğ‘›),ğ‘Œ(ğ¬ğ‘›)),(
ğ—(ğ¬ğ‘¡1),ğ‘Œğ‘™(ğ¬ğ‘¡1))
,â€¦,(
ğ—(ğ¬ğ‘¡ğ‘€),ğ‘Œğ‘™(ğ¬ğ‘¡ğ‘€))}
ğ‘™=1,â€¦,ğ¿;
5.Train a supervised machine learning model for each pseudo training dataset; this results to an ensemble of regressors{
Ì‚ğ‘“ğ‘™(ğ—(ğ¬))âˆ¶ğ¬âˆˆğ·}
ğ‘™=1,â€¦,ğ¿;
6.Aggregate the ensemble of regressors through averaging: Ì‚ğ‘“(ğ—(ğ¬))=1
ğ¿âˆ‘ğ¿
ğ‘™=1Ì‚ğ‘“ğ‘™(ğ—(ğ¬)),âˆ€ğ¬âˆˆğ·;
Output : predicted target variable {Ì‚ğ‘Œ(ğ¬) =Ì‚ğ‘“(ğ—(ğ¬))âˆ¶ğ¬âˆˆğ·}over the spatial domain ğ·.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
171F. Fouedjio and H. Talebi
Fig. 11. Real case study â€” auxiliary variables at sampling locations: (a) elevation, (b) Landsat 8 band 5, (c) Landsat 8 band 6, (d) gravity survey Bouguer anomaly, (e) gravity
survey high-pass filtered Bouguer anomaly, and (f) Uranium counts from gamma-ray spectrometry.
Fig. 12. Real case study â€” (a) target variable at sampling locations; (b) training and testing data locations.
where {ğ‘Œğ‘—}ğ‘—=1,â€¦,ğ‘Ÿare actual values of the target variable at testing
locations {ğ®ğ‘—âˆˆğ·}ğ‘—=1,â€¦,ğ‘Ÿ;{Ì‚ğ‘Œğ‘—}ğ‘—=1,â€¦,ğ‘Ÿare predicted values of the target
variable at testing locations {ğ®ğ‘—âˆˆğ·}ğ‘—=1,â€¦,ğ‘Ÿ;ğ‘Ÿis the total number of
testing locations; ğœ‡ğ‘Œandğœ2
ğ‘Œare respectively, the mean and variance ofactual values of the target variable at testing locations; ğœ‡Ì‚ğ‘Œandğœ2
Ì‚ğ‘Œare
respectively, the mean and variance of predicted values of the target
variable at testing locations; ğœŒis the correlation coefficient between
predicted and observed values at testing locations {ğ®ğ‘—âˆˆğ·}ğ‘—=1,â€¦,ğ‘Ÿ.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
172F. Fouedjio and H. Talebi
Fig. 13. Real case study â€” variograms (experimental and fitted) of the target variable
in the original training dataset. The fitted variogram model corresponds to an isotropic
stationary exponential model with a practical range and sill respectively equal to
22,551.842and 12,983.605.
The tool used to evaluate the prediction uncertainty accuracy is the
prediction interval coverage probability plot, also referred to as the
accuracy plot ( Fouedjio and Klump , 2019 ). Given the target variableâ€™s
measurements {ğ‘Œğ‘—}ğ‘—=1,â€¦,ğ‘Ÿat testing locations {ğ®ğ‘—âˆˆğ·}ğ‘—=1,â€¦,ğ‘Ÿ, the accu-
racy plot (which is a scatter-plot) compares the proportion of values of
the testing dataset falling into the symmetric ğ‘-probability interval (PI)
with the expected probability ğ‘. By construction, there is a probability
ğ‘(0â‰¤ğ‘â‰¤1)that the true value of the target of variable falls into the
symmetric ğ‘-probability interval[
Ì‚ğ‘„(1âˆ’ğ‘)
2(ğ‘—),Ì‚ğ‘„(1+ğ‘)
2(ğ‘—)]
;Ì‚ğ‘„(1âˆ’ğ‘)
2(ğ‘—)and
Ì‚ğ‘„(1+ğ‘)
2(ğ‘—)are the(1âˆ’ğ‘)
2and(1+ğ‘)
2quantiles of the predictive distribution
of the target variable at testing location ğ®ğ‘—. The fraction of target
variableâ€™s true values falling into the symmetric ğ‘-probability interval
is given as: Ì„ ğœ…(ğ‘) =1
ğ‘Ÿâˆ‘ğ‘Ÿ
ğ‘—=1ğœ…ğ‘—(ğ‘), whereğœ…ğ‘—(ğ‘) = 1 ifÌ‚ğ‘„(1âˆ’ğ‘)
2(ğ‘—)< ğ‘Œğ‘—<
Ì‚ğ‘„(1+ğ‘)
2(ğ‘—)and0otherwise. The more points are closer to the 45 degree
line in the accuracy plot the well-calibrated the model is. Specifically,
the closeness of points to the bisector of the accuracy plot is quantified
using the goodness statistic (G-statistic): ğº= 1âˆ’âˆ«1
0[3ğ‘(ğ‘)âˆ’2][Ì„ ğœ…(ğ‘)âˆ’ğ‘]ğ‘‘ğ‘,
withğ‘(ğ‘) = 1Ì„ ğœ…(ğ‘)>ğ‘. The value of ğºlies in the interval [0,1].ğº= 1for
maximum goodness corresponding to the case Ì„ ğœ…(ğ‘) =ğ‘,âˆ€ğ‘âˆˆ [0,1].
ğº= 0when no true values are contained in any of the PIs, i.e. Ì„ ğœ…(ğ‘) =
0,âˆ€ğ‘âˆˆ [0,1]. The higher is the value of ğºthe well-calibrated the model
is.
3. Synthetic case study
The geostatistical semi-supervised learning based on random forest
is first applied to synthetic spatial data for which the ground truth is
exhaustively available over the study region. The comparison is carried
out with classical random forest, random forest with unlabeled spatial
data treated as additional covariates, and self-training random forest.
The first two machine learning methods are supervised, while the
last one is semi-supervised. All competing machine learning methods
are based on random forest. The number of decision trees (1,000) is
the same for all the competing machine learning methods. The other
hyper-parameters are optimized through cross-validation. The classical
random forest is implemented in the R packages ranger (Wright and
Ziegler , 2017 ) and tuneRanger (Probst et al. , 2018 ). The self-training
random forest is implemented in the R package ssr(Garcia-Ceja , 2019 ).Table 1
Synthetic case study â€” simulation parameters.
Variables Mean Variogram type Variogram scale Variogram sill
ğ‘‹1(â‹…) 0 Cubic 300 1
ğ‘‹2(â‹…) 0 Spherical 300 1
ğ‘‹3(â‹…) 0 Cardinal Sine 15 1
ğ‘‹4(â‹…) 0 Linear 0.003 â€“
ğœ(â‹…) 0 Exponential 130 400
The synthetic spatial data is generated using the following model:
ğ‘Œ(ğ¬) = 5ğ‘‹1(ğ¬) Ã—ğ‘‹2(ğ¬) + 5ğ‘‹3(ğ¬)2+ 15 sin(ğ‘‹4(ğ¬))+ğœ(ğ¬),
âˆ€ğ¬âˆˆ [0,800] Ã— [0,500], (6)
whereğ‘‹1(â‹…),ğ‘‹2(â‹…),ğ‘‹3(â‹…), andğ‘‹4(â‹…)are auxiliary variables, ğœ(â‹…)is the
latent (unobserved) variable, and ğ‘Œ(â‹…)is the target variable. ğ‘‹1(â‹…),ğ‘‹2(â‹…),
ğ‘‹3(â‹…), andğ‘‹4(â‹…), andğœ(â‹…)are independent Gaussian random fields with
mean and isotropic stationary variogram model specified in Table 1.
For background on Gaussian random fields, refer to Chiles and Delfiner
(2012 ). The auxiliary, latent, and target variables are generated over
a 400 Ã—250 regular grid in the spatial domain [0,800] Ã— [0,500]. The
simulation is carried out via the turning bands method implemented in
the R package RGeostats (Renard et al. , 2021 ). This simulated example
depicts a situation where there is a non-linear relationship between
the target variable and auxiliary variables with some interactions be-
tween auxiliary variables. Also, the target variable shows some spatial
autocorrelation, and its distribution is non-Gaussian.
Fig. 1 displays the target and auxiliary variables maps. There are
ğ‘= 100,000data locations. Fig. 2 depictsğ‘›= 1,000data loca-
tions sampled randomly to form labeled spatial data îˆ¸(ğ¬1,â€¦,ğ¬ğ‘›) ={(ğ—(ğ¬1),ğ‘Œ(ğ¬1)),â€¦,(ğ—(ğ¬ğ‘›),ğ‘Œ(ğ¬ğ‘›))}
(original training dataset). The re-
mainder of the data locations ( ğ‘âˆ’ğ‘›=99,000) is considered as un-
labeled spatial data î‰(ğ¬ğ‘›+1,â€¦,ğ¬ğ‘) ={
ğ—(ğ¬ğ‘›+1),â€¦,ğ—(ğ¬ğ‘)}
, after hiding
the corresponding target variableâ€™s observations {ğ‘Œ(ğ¬ğ‘›+1),â€¦,ğ‘Œ(ğ¬ğ‘)}.
Thus, labeled and unlabeled spatial data represent respectively 1%
and 99% of the data. So, we are in a situation where ğ‘ â‰« ğ‘› .
The goal is to predict the target variable over the same 400 Ã—250
regular grid using the labeled and unlabeled spatial data. The pre-
dictions will be compared to the hidden target variableâ€™s observations
{ğ‘Œ(ğ¬ğ‘›+1),â€¦,ğ‘Œ(ğ¬ğ‘)}.
The first step of the geostatistical semi-supervised learning method
consists of estimating and modeling the target variableâ€™s variogram
from the target variableâ€™s observations. The corresponding variograms
(experimental and fitted) are presented in Fig. 3. One can clearly see
that the target variable shows some spatial autocorrelation. The fitted
variogram model corresponds to an isotropic exponential model with
a practical range and sill respectively equal to 155.010 and 346.489.
The second step consists in selecting unlabeled data locations for which
the distance to the closest labeled data location is less than a quarter
of the variogram range of the target variable. The selected unlabeled
data locations ( ğ‘€=82,912) are shown in Fig. 4. They represent
approximately 84% of the ensemble of unlabeled data locations. In the
third step, pseudo labels at the selected unlabeled data locations are
generated through geostatistical conditional simulation. Pseudo labels
are generated not once but several times ( ğ¿= 50) to account for the
pseudo labeling processâ€™s uncertainty. The pseudo labeling performed
through geostatistical conditional simulation is such that pseudo labels
at labeled data locations match exactly the true (observed) labels.
Four randomly selected realizations of pseudo labels at the selected
unlabeled data locations are given in Fig. 5. The resulting ensemble of
pseudo labels is augmented to the labeled spatial data (original training
dataset) to create an ensemble of pseudo training datasets. The next
step consists in training a random forest model for each pseudo training
dataset, followed by an aggregation of the models through averaging.
Fig. 6 presents prediction maps provided by the geostatistical semi-
supervised random forest, the classical random forest, the randomArtificial Intelligence in Geosciences 3 (2022) 162â€“178
173F. Fouedjio and H. Talebi
Fig. 14. Real case study â€” auxiliary variables at selected unlabeled data locations for pseudo labeling: (a) elevation, (b) Landsat 8 band 5, (c) Landsat 8 band 6, (d) gravity
survey Bouguer anomaly, (e) gravity survey high-pass filtered Bouguer anomaly, and (f) Uranium counts from gamma-ray spectrometry.
forest with unlabeled spatial data defined as additional covariates, and
the self-training random forest. It is important to highlight that all
competing machine learning methods rely on random forest with the
same number of decision trees set to 1,000. The other hyper-parameters
have been selected via cross-validation. In the random forest with
unlabeled spatial data treated as additional predictor variables, the
ten closest predictor variablesâ€™ observations surrounding a labeled data
location are defined as additional predictor variables. So, there are
10 Ã— 4 = 40 additional covariates. The general appearance of prediction
maps resulting from the classical random forest, the random forest
with unlabeled spatial data defined as additional covariates, and the
self-training random forest looks similar. The proposed methodâ€™s pre-
diction map differs from the other methods. In particular, it is more
similar to the ground truth than the other methods. The proposed
methodâ€™s prediction map shows some patterns existing in the target
variableâ€™s reference map that are missing in other methodsâ€™ prediction
maps. Fig. 7 shows the prediction uncertainty maps (corresponding
to the width of the 95% prediction interval) of the different machine
learning methods except for the self-training random forest. Available
R packages for self-training random forest do not return individual
predictions for each tree which is necessary to compute the predictionuncertainty; only aggregated predictions for all trees are provided. The
prediction uncertainty map resulting from the proposed method differs
notably from the others. In particular, under the proposed method, the
prediction uncertainty tends to be lower at the original training data
locations than at other locations, which is not the case for the other
methods.
Table 2, Figs. 8 and 9 show the predictive performance of the dif-
ferent machine learning methods on the unlabeled spatial data (99,000
observations). One can observe that the geostatistical semi-supervised
random forest provides better predictive performance than the other
methods. In particular, the self-training random forest (classical semi-
supervised learning) is the worst. The random forest with unlabeled
spatial data treated as additional predictor variables performs slightly
better than the classical random forest. The underperformance of the
classical semi-supervised learning method (self-training random forest)
can be explained by the nature of the data. Classical semi-supervised
learning methods have been designed for non-spatial data. As high-
lighted in Section 1, unlabeled data can lead to worse performance if
the underlying assumptions are not met. In particular, the smoothness
assumption as defined in the classical semi-supervised learning may not
be valid in the spatial framework. Two data points that are close in theArtificial Intelligence in Geosciences 3 (2022) 162â€“178
174F. Fouedjio and H. Talebi
Fig. 15. Real case study â€” examples of pseudo labeling through geostatistical conditional simulation at selected unlabeled data locations.
Fig. 16. Real case study â€” prediction maps provided by (a) geostatistical semi-supervised random forest, (b) classical random forest, (d) random forest with unlabeled spatial
data treated as additional covariates, and (e) self-training random forest.
attribute space can have dissimilar labels as they may be geographically
distant. In terms of prediction uncertainty accuracy, the accuracy plotin Fig. 9 and the G-statistic in Table 2 show that the geostatistical semi-
supervised regression provides a better uncertainty estimates than theArtificial Intelligence in Geosciences 3 (2022) 162â€“178
175F. Fouedjio and H. Talebi
Fig. 17. Real case study â€” prediction uncertainty maps provided by (a) geostatistical semi-supervised random forest, (b) classical random forest, and (c) random forest with
unlabeled spatial data treated as additional covariates. The black dots represent training data locations. The prediction uncertainty corresponds to the width of the 95% prediction
interval.
Table 2
Synthetic case study â€” predictive performance statistics for unlabeled data locations
(ğ‘âˆ’ğ‘›= 99 000 ). Method 1 refers to the geostatistical semi-supervised random forest;
Method 2 refers to the classical random forest; Method 3 refers to the random forest
with unlabeled spatial data as additional covariates; Method 4 refers to the self-training
random forest.
Criteria Method 1 Method 2 Method 3 Method 4
MAE 6.169 9.696 9.542 10.163
RMSE 8.121 12.256 12.008 12.796
R-SQUARE 0.823 0.597 0.613 0.5613
CCC 0.897 0.735 0.739 0.700
G-statistic 0.995 0.984 0.969 â€“
other methods. Thus, the spatial prediction performance of the geosta-
tistical semi-supervised regression suggests that the spatial prediction
task should be considered in a semi-supervised learning process instead
of a conventional supervised learning process to exploit a larger set of
unlabeled spatial data.
4. Real case study
The geostatistical semi-supervised learning with random forest is
applied to real-world spatial data for geochemical mapping. The target
variable is Barium (Ba) concentration (in mg/kg) measured at 568
sampling locations across the region of interest in southwest Eng-
land ( Kirkwood et al. , 2016b ). A detailed description of the study
area and data can be found in Kirkwood et al. (2016a ). The auxiliary
variables selected include elevation, Landsat 8 band 5, Landsat 8 band
6, gravity survey Bouguer anomaly, gravity survey high-pass filtered
Bouguer anomaly, and Uranium counts from gamma-ray spectrometry.
The spatially exhaustive predictor variables are displayed over a grid
containing 689,065 unsampled locations ( Fig. 10). Fig. 11 shows pre-
dictor variables at sampling locations. The target variable at sampling
locations (labeled spatial data) are shown in Fig. 12. Labeled spatialdata represent less than 0.1% of the available data (unlabeled and
labeled data). The target variableâ€™s observations are split into a training
set (âˆ¼ 75% ) and a testing set ( âˆ¼ 25% ). This latter set is used to evaluate
the different machine learning methods: geostatistical semi-supervised
random forest, classical random forest, random forest with unlabeled
spatial data defined as additional predictor variables, and self-training
random forest. The ten closest predictor variablesâ€™ observations at a
labeled data location are defined as additional covariates in the ran-
dom forest with unlabeled spatial data treated as additional predictor
variables. So, there are 10 Ã— 6 = 60 additional covariates. All of the
machine learning methods are based on random forest with the same
number of decision trees set to 1,000. The other hyper-parameters have
been selected via cross-validation.
Empirical and adjusted variograms of the target variable in the
training dataset are given in Fig. 13. The target variable variogram is
estimated from the isotropic stationary exponential family. The practi-
cal range and sill respectively correspond to 22,551.842and12,983.605.
The selected unlabeled data locations for pseudo labeling are shown
in Fig. 14, according the selection method described in Section 2.1.
They represent approximately 88% of the unlabeled data locations.
Note that unlabeled data locations that are more than a quarter of
the variogram range away from the labeled data locations are ignored.
Fig. 15 shows examples of pseudo labeling of the target variable at the
selected unlabeled data locations. As described in Section 2.1, pseudo
labeling is done through geostatistical conditional simulation, allowing
for generating not just a single pseudo label at a selected data location
but an ensemble of pseudo labels ( ğ¿= 50). Thus, the uncertainty in the
pseudo labeling process is accounted for. The ensemble of pseudo labels
is combined with the original training dataset to form an ensemble
of pseudo training datasets in which the size of each pseudo training
dataset is much larger than the original training dataset. Then, a ran-
dom forest model is trained for each pseudo training dataset, followed
by an aggregation of the models through averaging.
Prediction maps associated with the different machine learning
methods are shown in Fig. 16. The classical random forest, the randomArtificial Intelligence in Geosciences 3 (2022) 162â€“178
176F. Fouedjio and H. Talebi
Fig. 18. Real case study â€” target variableâ€™s observed values vs. target variableâ€™s predicted values at testing data locations, for (a) geostatistical semi-supervised random forest,
(b) classical random forest, (c) random forest with unlabeled spatial data treated as additional covariates, and (d) self-training random forest. The red line indicates the 1:1 line.
forest with unlabeled spatial data defined as additional covariates,
and the self-training random forest provide similar prediction maps.
Whereas there is a marked difference between the prediction maps
provided by the geostatistical semi-supervised random forest and the
other techniques. Prediction uncertainty maps of the different machine
learning techniques except for the self-training random forest are given
in Fig. 17. The prediction uncertainty is derived as the width of the
95% prediction interval. As mentioned earlier, available R packages for
self-training random forest do not return individual predictions for each
tree which is necessary to calculate the prediction uncertainty. The gen-
eral appearance of the prediction uncertainty map associated with the
proposed method differs from the others. As pointed out in the synthetic
case study, the proposed approachâ€™s prediction uncertainty tends to be
lower at the original training data locations than at other locations. In
particular, one can note that the highest prediction uncertainties are
concentrated in those areas not surveyed, which is more realistic than
the prediction uncertainty map provided by the other methods. These
latter provide relatively low prediction uncertainties in not surveyed
areas.
To evaluate the predictive ability of the different machine learning
methods, their predictions are compared with the target variableâ€™s ob-
served values at 142testing locations ( Fig. 18). Predictive performance
measures described in Section 2.3 are calculated and summarized inTable 3. One can note that the geostatistical semi-supervised random
forest performs the best while the self-training random forest provides
the worst prediction performance. The random forest with unlabeled
spatial data treated as additional predictor variables and the classical
random forest have similar prediction performances. As highlighted in
the synthetic case study in Section 3, the poor prediction performance
of the conventional semi-supervised learning technique (self-training
random forest) can be explained by the specificities of spatial data and
the underlying assumptions related to classical semi-supervised learn-
ing. Classical semi-supervised learning methods, such as self-training,
are based on the smoothness assumption that data points that are close
by in the input space should have similar labels. This assumption,
though works for non-spatial data, is not valid in the realm of spatial
data. Two data points that are close in the feature space can have
dissimilar labels as they may be far away in the geographical space.
Concerning the prediction uncertainty accuracy, the accuracy plot in
Fig. 19 and the G-statistic in Table 3 show that the geostatistical semi-
supervised regression provides a more reliable uncertainty estimates
than the other methods. In particular, the uncertainty tend to be
underestimated under the other methods.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
177F. Fouedjio and H. Talebi
Fig. 19. Real case study â€” prediction interval coverage probability plot (accuracy
plot). Method 1 refers to the geostatistical semi-supervised random forest; Method
2 refers to the classical random forest; Method 3 refers to the random forest with
unlabeled spatial data as additional covariates.
Table 3
Real case study â€” predictive performance statistics in the testing dataset containing
142 observations. Method 1 refers to the geostatistical semi-supervised random forest;
Method 2 refers to the classical random forest; Method 3 refers to the random forest
with unlabeled spatial data treated as additional covariates; Method 4 refers to the
self-training random forest.
Criteria Method 1 Method 2 Method 3 Method 4
MAE 54.124 56.393 56.168 59.138
RMSE 70.473 73.852 72.797 78.333
R-SQUARE 0.552 0.492 0.506 0.428
CCC 0.692 0.661 0.662 0.622
G-statistic 0.955 0.908 0.943 â€“
5. Concluding remarks
This article proposed a geostatistical semi-supervised learning
method that leverages a large amount of unlabeled spatial data in
combination with typically a smaller set of labeled spatial data to
improve the target variableâ€™s spatial prediction. This is achieved by
combining aspects of geostatistics and machine learning such as spatial
autocorrelation, geostatistical conditional simulation, and supervised
machine learning. The central idea consists of taking advantage of the
target variableâ€™s spatial autocorrelation to generate pseudo labels at
unlabeled data locations that are geographically close to labeled data
locations. This mechanism allows for the selection of unlabeled data
locations that are useful for the learning task. Different synthetic and
real-world spatial datasets used in this paper showed the ability of
the proposed spatial semi-supervised learning method to outperform
classical supervised and semi-supervised machine learning methods.
The proposed spatial semi-supervised learning technique allows better
use of unlabeled spatial data than classical semi-supervised learning
techniques. It also provides more realistic spatial prediction uncertainty
than classical supervised machine learning methods.
The proposed geostatistical semi-supervised learning approach has
some attractive features. It is easy to implement as it combines ex-
isting techniques in geostatistics and machine learning. Methods of
geostatistics are used for pseudo labeling (label generation) and data
augmentation, while machine learning methods focus on the supervised
learning task. It can be used with any given supervised base learner,allowing unlabeled spatial data to be introduced in a straightforward
manner. In particular, it allows a better training of data-hungry su-
pervised base learners such as neural networks as the size of pseudo
training datasets is much larger than the original training dataset. It
can be easily adapted to semi-supervised classification (i.e., when the
target variable is categorical). In this latter case, the pseudo labeling
will be performed using a geostatistical conditional simulation method
for categorical data (e.g., sequential indicator simulation, plurigaussian
simulation) ( Chiles and Delfiner , 2012 ). The aggregation of models
can be done through the majority vote. Contrary to classical semi-
supervised machine learning approaches (self-training and co-training),
the proposed spatial semi-supervised learning technique accounts for
uncertainty in the pseudo labeling process.
The proposed spatial semi-supervised learning technique assumes
the presence of spatial autocorrelation in the target variable. Thus,
one should expect the proposed semi-supervised learning method to
perform best when the target variableâ€™s observations depict some spa-
tial dependency. However, the proposed modeling method will not be
suitable if the target variable shows a very weak or no spatial autocorre-
lation. Classical supervised and semi-supervised learning methods may
be appropriate in this latter case as the target variableâ€™s observations
can be considered independent. There are many tools that can be used
to check spatial autocorrelation in the target variable, including the
variogram. A variogram with a monotonic increasing slope indicates
that the target variable is spatially dependent or autocorrelated. In the
proposed spatial semi-supervised learning, the supervised base learner
used was the traditional regression random forest. Still, any super-
vised machine learning model can also be used, including the existing
ones dedicated to spatial data. In the proposed spatial semi-supervised
learning framework, modeling the target variableâ€™s variogram and the
geostatistical conditional simulation have been performed under the
stationarity setting ( Fouedjio , 2021c ). This can also be carried out in
the non-stationarity framework for better accounting for local charac-
teristics of the target variable subject to having enough data ( Fouedjio ,
2017 ). Like traditional semi-supervised learning methods, the proposed
one is computationally more intensive than classical supervised learn-
ing methods as it accounts for a large amount of unlabeled spatial data
and the uncertainty in the pseudo labeling process. However, it has
easily parallelizable components such as generating pseudo training
datasets and training supervised machine learning models.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
References
Albrecht, T., GonzÃ¡lez-Ãlvarez, J., 2021. Using machine learning to map Western
Australian landscapes for mineral exploration. ISPRS Int. J. Geo-Inf. 10.
Appelhans, T., Mwangomo, E., Hardy, D.R., Hemp, A., Nauss, T., 2015. Evaluating
machine learning approaches for the interpolation of monthly air temperature at
Mt. Kilimanjaro, Tanzania. Spat. Stat. 14, 91â€“113.
Asghar, S., Choi, J., Yoon, D., Byun, J., 2020. Spatial pseudo-labeling for
semi-supervised facies classification. J. Pet. Sci. Eng. 195, 107834.
Ballabio, C., Panagos, P., Monatanarella, L., 2016. Mapping topsoil physical properties
at European scale using the LUCAS database. Geoderma 261, 110â€“123.
Barzegar, R., Moghaddam, A.Asghari., Adamowski, J., Fijani, E., 2016. Comparison of
machine learning models for predicting fluoride contamination in groundwater.
Stoch. Environ. Res. Risk Assess. 1â€“14.
Chapelle, O., Scholkopf, B., Zien, A., 2010. Semi-supervised learning. In: Adaptive
Computation and Machine Learning Series. MIT Press.
Chiles, J.P., Delfiner, P., 2012. Geostatistics: Modeling Spatial Uncertainty. John Wiley
& Sons.
Cracknell, M.J., Reading, A.M., 2014. Geological mapping using remote sensing data:
A comparison of five machine learning algorithms, their response to variations in
the spatial distribution of training data and the use of explicit spatial information.
Comput. Geosci. 63, 22â€“33.Artificial Intelligence in Geosciences 3 (2022) 162â€“178
178F. Fouedjio and H. Talebi
Cracknell, M., Reading, A., 2015. Spatial-contextual supervised classifiers explored: A
challenging example of lithostratigraphy classification. IEEE J. Sel. Top. Appl. Earth
Obs. Remote Sens. 8, 1â€“14.
Du, P., Bai, X., Tan, K., Xue, Z., Samat, A., Xia, J., Li, E., Su, H., Liu, W., 2020.
Advances of four machine learning methods for spatial data handling: a review. J.
Geovisualization Spat. Anal. 4.
Fouedjio, F., 2017. Second-order non-stationary modeling approaches for univariate
geostatistical data. Stoch. Environ. Res. Risk Assess. 31, 1887â€“1906.
Fouedjio, F., 2020. Exact conditioning of regression random forest for spatial prediction.
Artif. Intell. Geosci. 1, 11â€“23.
Fouedjio, F., 2021a. Classification random forest with exact conditioning for spatial
prediction of categorical variables. Artif. Intell. Geosci. 2, 82â€“93.
Fouedjio, F., 2021b. Random forest for spatial prediction of censored response variables.
Artif. Intell. Geosci. 2, 115â€“127.
Fouedjio, F., 2021c. Stationarity. In: Daya Sagar, B., Cheng, Q., McKinley, J., Agter-
berg, F. (Eds.), Encyclopedia of Mathematical Geosciences. In: Encyclopedia of
Earth Sciences Series, pp. 1â€“5.
Fouedjio, F., Klump, J., 2019. Exploring prediction uncertainty of spatial data in
geostatistical and machine learning approaches. Environ. Earth Sci. 78 (38).
Garcia-Ceja, E., 2019. ssr: semi-supervised regression methods. https://cran.r-project.
org/web/packages/ssr/index.html. r package.
Giaccone, E., Oriani, F., Tonini, M., Lambiel, C., MariÃ©thoz, G., 2021. Using data-
driven algorithms for semi-automated geomorphological mapping. Stoch. Environ.
Res. Risk Assess. 1â€“17.
Hengl, T., Heuvelink, G.B.M., Kempen, B., Leenaars, J.G.B., Walsh, M.G., Shep-
herd, K.D., Sila, A., MacMillan, R.A., Jesus, J.Mendes.de., Tamene, L., Tondoh, J.E.,
2015. Mapping soil properties of Africa at 250 m resolution: Random forests
significantly improve current predictions. PLoS One 10, 1â€“26.
Hengl, T., Nussbaum, M., Wright, M., Heuvelink, G., GrÃ¤ler, B., 2018. Random forest
as a generic framework for predictive modeling of spatial and spatio-temporal
variables. PeerJ 6, e5518.
Kanevski, M., 2008. Advanced Mapping of Environmental Data: Geostatistics, Machine
Learning and Bayesian Maximum Entropy. John Wiley & Sons.
Kanevski, M., Pozdnoukhov, A., Timonin, V., 2009. Machine Learning for Spatial
Environmental Data: Theory, Applications, and Software. EPFL Press.
Keogh, E., Mueen, A., 2017. Curse of dimensionality. In: Sammut, C., Webb, G. (Eds.),
Encyclopedia of Machine Learning and Data Mining. pp. 314â€“315.
Khan, S.Z., Suman, S., Pavani, M., Das, S.K., 2016. Prediction of the residual strength
of clay using functional networks. Geosci. Front. 7, 67â€“74.
Kirkwood, C., Cave, M., Beamish, D., Grebby, S., Ferreira, A., 2016a. A machine
learning approach to geochemical mapping. J. Geochem. Explor. 167, 49â€“61.
Kirkwood, C., Economou, T., Pugeault, N., Odbert, H., 2022. Bayesian deep learning
for spatial interpolation in the presence of auxiliary information. Math. Geosci. 54,
507â€“531.
Kirkwood, C., Everett, P., Ferreira, A., Lister, B., 2016b. Stream sediment geochemistry
as a tool for enhancing geological understanding: An overview of new data from
South West England. J. Geochem. Explor. 163, 28â€“40.
Kobs, K., SchÃ¤fer, M., Krause, A., Baumhauer, R., Paeth, H., Hotho, A., 2021. Semi-
supervised learning for grain size distribution interpolation. In: Del Bimbo, A.,
Cucchiara, R., Sclaroff, S., Farinella, G.M., Mei, T., Bertini, M., Escalante, H.J., Vez-
zani, R. (Eds.), Pattern Recognition. ICPR International Workshops and Challenges.
Springer International Publishing, Cham, pp. 34â€“44.
Kumar, C., Chatterjee, S., Oommen, T., Guha, A., 2020. Automated lithological mapping
by integrating spectral enhancement techniques and machine learning algorithms
using AVIRIS-NG hyperspectral data in gold-bearing granite-greenstone rocks in
Hutti, India. Int. J. Appl. Earth Obs. Geoinf. 86, 102006.Lantuejoul, C., 2013. Geostatistical Simulation: Models and Algorithms. Springer Berlin
Heidelberg.
Latifovic, R., Pouliot, D., Campbell, J., 2018. Assessment of convolution neural networks
for surficial geology mapping in the South Rae geological region, Northwest
territories, Canada. Remote Sens. 10.
Li, J., 2013. Predictive modelling using random forest and its hybrid methods with geo-
statistical techniques in marine environmental geosciences. In: 11-Th Australasian
Data Mining Conference. AusDM Â´13, Canberra, Australia, pp. 73â€“79.
Li, J., Heap, A.D., Potter, A., Daniell, J.J., 2011. Application of machine learning
methods to spatial interpolation of environmental variables. Environ. Model. Softw.
26, 1647â€“1659.
Maxwell, A.E., Warner, T.A., Fang, F., 2018. Implementation of machine-learning
classification in remote sensing: an applied review. Int. J. Remote Sens. 39,
2784â€“2817.
Othman, A.A., Gloaguen, R., 2017. Integration of spectral, spatial and morphomet-
ric data into lithological mapping: A comparison of different machine learning
algorithms in the Kurdistan region, NE Iraq. J. Asian Earth Sci. 146, 90â€“102.
Pise, N.N., Kulkarni, P., 2008. A survey of semi-supervised learning methods. In: 2008
International Conference on Computational Intelligence and Security. pp. 30â€“34.
http://dx.doi.org/10.1109/CIS.2008.204.
Probst, P., Wright, M., Boulesteix, A.L., 2018. Hyperparameters and Tuning Strategies
for Random Forest. Wiley Interdisciplinary Reviews: Data Mining and Knowledge
Discovery, http://dx.doi.org/10.1002/widm.1301.
R Core Team, 2021. R: A Language and Environment for Statistical Computing. R
Foundation for Statistical Computing, Vienna, Austria, https://www.r-project.org/.
Renard, D., Bez, N., Desassis, N., Beucher, H., Ors, F., Freulon, X., 2021. RGeostats:
Geostatistical package. http://rgeostats.free.fr/. r package version 12.1.0.
Sahoo, S., Jha, M.K., 2017. Pattern recognition in lithology classification: Modeling
using neural networks, self-organizing maps and genetic algorithms. Hydrogeol. J.
25, 311â€“330.
SekuliÄ‡, A., Kilibarda, M., Heuvelink, G., NikoliÄ‡, M., Bajat, B., 2020. Random forest
spatial interpolation. Remote Sens. 12 (1687).
Steichen, T.J., Cox, N.J., 2002. A note on the concordance correlation coefficient. Stata
J. 2, 183â€“189.
Taghizadeh-Mehrjardi, R., Nabiollahi, K., Kerry, R., 2016. Digital mapping of soil
organic carbon at multiple depths using different data mining techniques in Baneh
region, Iran. Geoderma 266, 98â€“110.
Talebi, H., Peeters, L.J., Otto, A., Tolosana-Delgado, R., 2021. A truly spatial random
forests algorithm for geoscience data analysis and modelling. Math. Geosci. 1â€“22.
Tobler, W.R., 1970. A computer movie simulating urban growth in the Detroit region.
Econ. Geogr. 46, 234â€“240.
Van Engelen, J.E., Hoos, H.H., 2020. A survey on semi-supervised learning. Mach.
Learn. 109, 373â€“440.
Vatsavai, R.R., Shekhar, S., Burk, T.E., 2007. An efficient spatial semi-supervised
learning algorithm. Int. J. Parallel Emergent Distrib. Syst. 22, 427â€“437.
Wadoux, A.M.C., Minasny, B., McBratney, A.B., 2020. Machine learning for digital soil
mapping: Applications, challenges and suggested solutions. Earth Sci. Rev. 210,
103359.
Wilford, J., Caritat, P.de., Bui, E., 2016. Predictive geochemical mapping using
environmental correlation. Appl. Geochem. 66, 275â€“288.
Wright, M.N., Ziegler, A., 2017. ranger: A fast implementation of random forests for
high dimensional data in C++ and R. J. Stat. Softw. 77, 1â€“17.
Yu, L., Porwal, A., Holden, E.J., Dentith, M.C., 2012. Towards automatic lithological
classification from remote sensing data using support vector machines. Comput.
Geosci. 45, 229â€“239.
Zhu, X., Goldberg, A., 2009. Introduction to semi-supervised learning. In: Synthesis
Lectures on Artificial Intelligence and Machine Learning. Morgan & Claypool.