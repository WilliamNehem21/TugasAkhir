Empirical autopsy of deep video captioning encoder-decoder architecture
Nayyer Aafaq*, Naveed Akhtar, Wei Liu, Ajmal Mian
The University of Western Australia, 35 Stirling Hwy, 6009, WA, Australia
ARTICLE INFO
Keywords:CNN architectureEncoder-decoderWord embeddingsLanguage modelVideo captioningLanguage and visionVideo to textNatural language processingRecurrent neural networksABSTRACT
Contemporary deep learning based video captioning methods adopt encoder-decoder framework. In encoder,visual features are extracted with 2D/3D Convolutional Neural Networks (CNNs) and a transformed version ofthose features is passed to the decoder. The decoder uses word embeddings and a language model to map visualfeatures to natural language captions. Due to its composite nature, the encoder-decoder pipeline provides thefreedom of multiple choices for each of its components, e.g., the choices of CNN models, feature transformations, word embeddings, and language models etc. Component selection can have drastic effects on the overall video captioning performance. However, current literature is void of any systematic investigation in this regard. Thisarticleﬁlls this gap by providing theﬁrst thorough empirical analysis of the role that each major component plays in a widely adopted video captioning pipeline. We perform extensive experiments by varying the constituentcomponents of the video captioning framework, and quantify the performance gains that are possible by merecomponent selection. We use the popular MSVD dataset as the test-bed, and demonstrate that substantial per-formance gains are possible by careful selection of the constituent components without major changes to thepipeline itself. These results are expected to provide guiding principles for research in the fast growing directionof video captioning.
1. IntroductionRecent years have seen rising research interests in automaticdescription of images and videos in natural language using deep learningtechniques. Recent methods are inspired by the encoder-decoderframework used in machine translation [ 1]. These techniques use Con- volutional Neural Networks (CNNs) as encoders to computeﬁxed/variable-length vector representations of the input images orvideos. A Recurrent Neural Network (RNN), e.g., vanilla RNN [2], Gated Recurrent Units (GRU) [3] or Long Short Term Memory (LSTM) networks[4] are then used as decoders to generate natural language descriptions.In the encoder-decoder pipeline, visual features are extracted with2D/3D CNNs from the input videos. These features are then transformedthrough Mean Pooling (MP) [5–7], Temporal Encoding (TE) [8], and/or Semantic Attributes Learning (SAL) [7,9,10] before feeding to a language model for natural language caption generation. Most of the existingcaptioning methods [11–16] mainly differ from each other in terms of theadopted visual feature extraction ( i.e., CNN models), types of visual feature transformations, language models, and the word embeddingsincorporated in the language models. Despite the aforementioned dif-ferences, these four core components are common to nearly alltechniques that follow the encoder-decoder framework for videocaptioning.InFig. 1, we take a modular decomposition approach and depict theencoder-decoder captioning framework in terms of its four core compo-nents. Multiple choices are available to instantiate each component. Forinstance, one can choose either a 2D-CNN or a 3D-CNN as the CNN model. Availability of numerous 2D/3D-CNN models provides further ﬂexibility in the choices of the visual feature extraction component. The choice ofmodels may directly affect the overall performance of the videocaptioning system. Similarly, the choice of MP or TE for Feature Trans- formationcan also have signiﬁcant effects on the system performance.This modular view of the encoder-decoder pipeline for video captioningis critical to assess how each component contributes to the captionqualities. For many existing methods, it is often unclear whether theperformance gain is a result of some novel sophisticated enhancements,or simply due to better component selection. This calls for a systematicinvestigation to quantify performance gains against various componentselections across the pipeline. Such analysis would establish a betterunderstanding of the encoder-decoder framework and in turn helpidentify the most promising components and their best instances.Moreover, it can guide future research in video captioning to focus more
* Corresponding author.E-mail address:nayyer.aafaq@research.uwa.edu.au (N. Aafaq).
Contents lists available atScienceDirect
Array
journal homepage:www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2020.100052Received 5 August 2020; Received in revised form 25 October 2020; Accepted 1 December 2020Available online 29 December 20202590-0056/©2020 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).Array 9 (2021) 100052on improving the critical components of the framework.In this work, we present theﬁrst systematic analysis of the encoder-decoder framework components with the aim of revealing the contri-bution of each component on the quality of the generated captions. Ouranalysis is performed by studying the effects of popular choices for eachcomponent while keeping the remaining components ﬁxed. We also include the choices of important hyper-parameters in our analysis. Themain contributions of this paper are as follows:1. We evaluate and quantify the role of CNN architecture employed toextract features in the video captioning framework. We analyze 5CNN models encompassing varying depths and structures. Weobserve that networks with stronger expressive ability perform better.We empirically demonstrate that the choice of CNN model in thevideo captioning framework can lead to performance gain up to16:61% in METEOR (seeTable 5for complete details).2. We analyze and quantify the performance gain achieved by employ-ing temporal encoding over inferior mean pooling strategy to repre-sent the whole video. It is further observed that temporal encodingapplied on 2D-CNN, depending upon the technique, may evenoutperform most widely used C3D which is computationally muchmore expensive.3. We further explore and quantify the effects of pre-trained word em-beddings utilized in language models and report that FastTextout- performs the contemporary popular word embeddings e.g.,Word2Vec [17]o rGloVe[18].4. Lastly, we analyze the effects of language model depth and varioushyper-parameters choicese.g., internal state size, number of frames,ﬁne tuning the pre-trained word embeddings, and dropout regulari-zation in video captioning framework.2. Related work2.1. Classical methodsEarly works on video captioning [19–21] follow the template based approaches. These methods involve handcrafted features of limitedpre-deﬁned entitiesi.e., subject, object, action, and place. The entities ina video are detected [22] separately by employing classical methods foreach. Subsequently, to describe the detected components in naturallanguage, most important entities areﬁt into a pre-deﬁned sentence template. Typically, a sentence template comprises of three componentsnamely lexicons, grammar and template rules. Lexiconrepresents vo- cabulary,template rulesare user-deﬁned rules to help select appropriatelexicons andgrammardeﬁnes linguistic rules to ensure that generatedsentence is syntactically correct. Due to advancement in deep neuralnetworks, the encoder-decoder based framework was ﬁrstly introduced to overcome the limitations of classical approaches [ 5].2.2. Encoder-decoder methodsContemporary methods in video captioning rely on neural networksbased framework with encoder-decoder architecture being the mostpopular [1,
5,8,9,23–26]. The encoder (i.e., 2D/3D-CNN),ﬁrst encodes the video frames and decoder (i.e., vanilla-RNN, GRU, LSTM), decodesthe visual information into natural language sentences. These methodstrain the encoder and decoder into an end-to-end manner. This frame-work exploits the power of CNNs and RNNs in video representation andsequential learning respectively. Later methods, improve the perfor-mance by replacing the mean pooling with temporal encoding andintroducing attention [8,9,27,28] in the encoder-decoder architecture.Most recent methods incorporate various adjustments in the encoder-decoder framework. Wang et al. [29] proposed multi-model memory tomodel the long-term visual-textual dependency. Chen et al. [ 30] proposes a frame picking module to select a compact frame subset to represent thevideo. This enables the encoder-decoder architecture more applicable toprocess the real world videos. GRU-EVE [ 25] employs Short Fourier Transform on the encoder output to enrich the video representation withtemporal information. Zhang et al. [31] models salient objects with their temporal dynamics to improve the architecture performance. Zheng et al.[32] propose syntax aware action targetting module to explicitly learnactions. In order to understand the strength of each technique employingencoder-decoder architecture, it is important to understand the role ofeach module in encoder-decoder. For that matter, we put together theperformance contribution of each module, forming the basis ofencoder-decoder architecture, towards overall framework performance.3. Setup for analysis3.1. ComponentsWeﬁrst introduce the setup used in our empirical analysis of thevideo captioning framework. For evaluation, we divide the frameworkinto four core components, namely CNN model- that encodes visual features of videos,feature transformation- that transforms visual features to be used as inputs by the language model component, word embeddings- that provides numerical representation of words in the vocabulary, andthelanguage modelcomponent, which decodes the visual features intonatural language descriptions. Extensive experiments are carried out byvarying the methods for each component of the framework and analyzethe captioning performance of the overall pipeline.3.2. Evaluation metricsWe measure the performance in terms of most commonly used eval-uation metrics in the contemporary captioning literature, namely Bilin-gual Evaluation Understudy (BLEU) [33], Recall Oriented Understudy for Gisting Evaluation (ROUGE) [34], Metric for Evaluation of Translationwith Explicit Ordering (METEOR) [ 35], and Consensus based Image
Fig. 1.The encoder-decoder framework of contem-porary deep video captioning techniques has fourmajor modules where selections can be made. It ispossible to choose from a variety of 2D/3D-CNNs toencode visual features of videos. These features arethen transformed to feed into the language model,which can be done by temporal encoding or meanpooling of the features. Multiple choices are alsoavailable for selecting the word embeddings that mapwords in a vocabulary to dense vector representationsto be used by the language model. Language modelscan have different complexity, governed by e.g., the number of network layers. In this paper, we vary thechoices for each of the four major components, andanalyze its effects on the overall captioning perfor-mance of the framework.N. Aafaq et al. Array 9 (2021) 100052
2Description Evaluation (CIDEr) [36]. These metrics are known to comprehensively evaluate the quality of automatically generated cap-tions. We brieﬂy discuss each metric in the below text. For details on eachmetric, pros and cons, and for their comparisons, we refer the readers tooriginal papers and survey paper [37].BLEU measuresn–grambased exact matches of the words as well astheir order in the reference and generated sentences. Hence, with highernumber of reference sentences BLEU is more likely to score high.Therefore, more realistic usage of BLEU would be at a corpus level.Formally BLEU is computed as;logðBLEUÞ¼min/C181/C0
lgt
lc;0/C19þXKk¼1wklogp k;wherel
gt=lcrepresent the ratio between the lengths of ground truth andthe candidate descriptions,w
kare positive weights, andp kis the geo- metric average of the modiﬁedn–gramprecisions. Theﬁrst term in above equation computes brevity penalty that penalizes descriptions shorterthan the ground truth description (refer to original paper for furtherdetails).ROUGE
Lcomputes the recall score of the generated sentences using n- ^a
€“grams. It computes the common subsequences between the candidateand reference sentences.Unlike, words match may not be consecutivehowever should be in sequence.METEOR addresses many of the BLEU shortcomings. For example,instead of exact word match, it incorporates synonym matching andperforms semantic matching. It is also found to be more robust andclosely correlated to human judgments [ 38]. To compute the METEOR score,ﬁrstuni–grambased recallRand precisionPscores are computed as;R¼m
cg/C14m
gt;P¼m cg/C14m
ct;wherem
cg, in both equations, refer to the number of uni–gramsco- occurring in both candidate and ground truth sentences, m
gtis the total number ofuni–gramsin the ground truth sentences andm
ctcorresponds to total number ofuni–gramsin the candidate sentences. METEOR scorefor a sentence is computed using F-score and penalty psuch as;F
score¼10PRRþ9P;p¼ 12/C18Nch
Nuni/C192
;M¼F scoreð1/C0pÞ;whereN
chandN unirepresents the number of chunks and number ofuni–gramsgrouped together respectively.N
chincludes adjacentuni–grams in candidate as well as reference sentences. The ratio
Nch
Nuniwill be 1 in case of generated sentence is an exact match to the reference sentence. Inorder to compute corpus level score, aggregated values of the constituentcomponentsi.e.,P,R, andpare taken. Moreover, highest METEOR scoreis selected if there are multiple reference sentences against a generatedsentence.Lastly, CIDEr
Devaluates the consensus between a generated andreference sentences. For that matter, itﬁrst performs stemming to all the candidate and reference words followed by measuring the n–grambased co-existence frequency using Term Frequency Inverse Document Fre-quency (TF-IDF) [39]. score is computed as;CIDEr
nðgi;SiÞ¼1mX
jκnðgiÞ:κn/C0s
ij/C1jjκ
nðgiÞjj:/C12/C12/C12/C12κn/C0s
ij/C1/C12/C12/C12/C12;whereκ
nðgiÞrepresentsn–gramsof lengthn,κnðgiÞdenote magnitude of κ
nðgiÞ. Same is the case forκnðsijÞ. CIDEr uses higher ordern–gramsto capture the grammatical properties and semantics of the text. It combinesscores of differentn–gramsusing the following equation:CIDErðgi;SiÞ¼XNn¼1wnCIDEr nðgi;SiÞ:With slight modiﬁcations in the original metric, CIDEr-D is the mostpopular variant in the image and video captioning evaluation. Firstly, itremoves the stemming to ensure correct form of words are used. Sec-ondly, by clippingn–gramscount, it ensures high score is not produced byrepeating high conﬁdence words even if the sentence does not makesense.We used the Microsoft COCO server [ 40] to compute our results. To clearly analyze the contribution of each component in the overall pipe-line, we follow the strategy of freezing all other components whenevaluating a particular module. The metric has also been found to bemore robust to distractionse.g., scene or person changes [38].3.3. DatasetWe perform experiments on the popular video captioning datasetMSVD [41]. This dataset comprises 1 ;970 YouTube short video clips, primarily containing single action/event in a video. Each clip durationvaries from 10 to 25 s. Each video is associated with multiple humanannotated captions. On average, there are 41 captions per video clip. Forbench-marking, we follow the data split of 1 ;200, 100, and 670 videos for training, validation and testing respectively. This is a widelyemployed protocol for evaluation using MSVD dataset [ 8,9,29].3.4. Training detailsIn our experiments, we employ 5 popular CNN architectures ( i.e., VGG16, VGG19, Inception-v3, InceptionResNet-v2, and C3D) in theencoder to extract the visual features. The last ‘fc2’and‘avgpool’layer of the 2D-CNNs and ‘fc6’layer of 3D-CNN are considered as the extractionlayers. All the 2D-CNNs are pretrained on ImageNet [ 42] whereas 3D-CNN on sports 1 M dataset [43]. For the 2D-CNN, we process allframes and for 3D-CNN 16 frame clips as inputs with an 8 frame overlapare selected. For decoder training, we add ‘start’and‘end’tokens to deal with the dynamic length of the captions. We set maximum length ofsentence to 30 words and truncate/zero pad in case of length of thesentence exceeds or fall short respectively. We employ RMSProp algo-rithm and set the learning rate 2/C210
/C04to train the models. We select the batch size of 60 and train the models for 50 epochs. We use sparse crossentropy loss to train our model. We use NVIDIA Titan XP 1080 GPU in ourexperiments. TensorFlow framework is used for the development.4. Analysis of framework components4.1. PreliminaryIn this section we brieﬂy describe the vanilla encoder-decoder ar-chitecture which is backbone of most video captioning methods. Encoderencodes the input video frames by employing pre-trained convolutionalneural network(s). The extracted features from encoder are then utilizedas input to RNN decoder to generate the caption. Formally, for a givenvideoV¼ff
1;f2;…;f Ng;whereNrepresents the number of frames in video V. To generate the features for all frames from videoV, deep Convolutional Neural Networks(CNNs) pretrained for object recognition and detection are employed.These deep CNN models generate a feature representation for each framef. Formally:F
V¼φðVÞ¼ff 1;f2;…;f Ng;whereφdenote pretrained CNN model and f
i2Rmrepresent the featureN. Aafaq et al. Array 9 (2021) 100052
3vector forithframe. The visual representationF Vmay further be enriched by employing additional modalities such as attention, temporalmodeling, semantic attributes, audio and last but not the least objects andaction modeling.F
encd¼ΨðF VÞ;whereΨrepresents the additional modalities applied and F
encddenote the encoder output. The encoder output is utilized to initialize the initialhidden state of the language model. The word at time step tis generated based on the previous hidden state and sequence of words generated tilltime stept. This process continues until it gets the end token of thesentence. Formally, the decoder takes F
encdas input and outputs the sentence (i.e., word sequence) represented as:S¼fw
1;w2;…;w Tg;where the decoderΦ,i.e. a language model in this case, generates worddistributionw
tat any time step:w
t¼ΦðF encdjðw1;w2;…;w t/C01ÞÞ:4.2. CNN selectionConvolutional Neural Networks (CNNs) can be readily applied toimages and videos. In deep learning based encoder-decoder frameworkfor captioning, CNNs dominate the encoder part. Due to the signi ﬁcance of a encoder role, the choice of CNN models can affect the overallcaptioning performance signiﬁcantly. Hence, weﬁrst analyze theﬁve most commonly used CNN models in captioning, namely; C3D [ 44], VGG-16 [45], VGG-19 [45], Inception-v3 [46], and InceptionResNet-v2 [47]. Among these models, C3D - a popular example of 3D-CNN, is acommon choice [23,48] because it can not only process individualframes, but also short video clips. This is possible due to its ability toprocess tensors with an extra time dimension.While performing these experiments to compare different CNNmodels, weﬁx all components of the pipeline, except the visual features.For the remaining components, the popular Mean Pooling is used totransform the extracted visual feature into a ﬁxed length vector to represent a complete video; word2vec word embeddings are used for a2/C0layer GRU as the language model. The results of this set of experi-ments are summarized inFig. 2. We can see a signiﬁcant variation in captioning performance due to changes in the CNN models, ascertainingthat better visual features (obtained from more sophisticated models)lead to better video captions. Hence, a careful selection of CNN model iscritical for effective video captioning. Interestingly, the spatial visualfeatures of 2D-CNN (VGG16, VGG19, Inc-V3,andInc-Res-V2) are able to outperform the spatio-temporal features of C3Dfor the video captioning task, indicating that the extra dimension of 3D-CNNs may not beparticularly effective in this case.Moreover, we observe that the effect of CNN architecture in the videocaptioning framework follows the similar trend as seen in image classi-ﬁcation tasks. It is generally believed that deeper networks tend toperform better, we see that VGG-Nets from 16 to 19 layers slightlyimprove the performance of captioning framework. However, after acertain depth is reached in the CNN, increase in number of layers do notincrease the classiﬁcation accuracy of the model. The problem wasresolved by introducing residual blocks that resulted in networks withstronger expressive ability and hence performed better in classi ﬁcation tasks. Similarly, we see the same trend in our experiments where weachieve better performance by employing these networks in the videocaptioning framework.4.3. Features transformationMost existing video captioning methods [ 5–7,9] perform mean pooling to combine individual frame features into a feature vector for thewhole video. However, this practice is bound to inferior performance asmean pooling can result in signiﬁcant loss of temporal information andthe order of events in videos. Such information often plays a crucial rolein video understanding for humans. Inspired by this observation, weconduct another series of experiments that compares a temporal encod-ing strategy to the mean pooling strategy for feature transformation in
video captioning.For temporal encoding, we follow our previous work [ 25] and compute Short Fourier Transform [49] of the frame level features of the video. These features are combined in a hierarchical manner that cap-tures the local, intermediate and high level temporal dynamics in thevideo. Interested readers are referred to our work [ 25] for exact details of the temporal feature transformation. The core insight relevant to ouranalysis here is that instead of compromising on the temporal informa-tion through mean pooling, we capture high ﬁdelity temporal dynamics over whole videos with our temporal encoding. Similar to the othersections of this paper, weﬁx all the remaining components of the un-derlying framework when analyzing the transformation strategy. Theresults of our experiments are summarized in Fig. 3. It is evident that the models employing temporal encoded features, have outperformed all of
Fig. 3.Performance comparison ofﬁve 2D/3D CNNs models with two types of feature transformationsi.e., Mean Pooling and Temporally Encoding. It isevident that under all circumstances, temporally encoded features outperformthe mean pooled features across all networks and among all metrics.
Fig. 2.Performance ofﬁve 2D/3D CNNs architectures (C3D, VGG-16, VGG-19,Inception-V3 (Inv-V3), and Inception-ResNet-V2 (Inv-Res-V2)) when used asvisual encoder in the captioning framework. Results are achieved by using MeanPooling for feature transformation, word2vec as word embedding, and a 2-layerGRU as the language model.N. Aafaq et al. Array 9 (2021) 100052
4their mean pooling based counterparts across all evaluation metrics.Considering the widespread use of mean pooling as the feature trans-formation strategy in video description, these results are signi ﬁcant. This experiment clearly establishes the supremacy of temporal encoding overmean pooling for the encoders in video description. This temporalencoding does not require any training, as it is applied after the featuresare extracted, therefore little computational overhead is introduced.It is also evident from the results that for temporal encoding, theperformance of different models show a similar behavior relative to eachother, which is also the case for the mean pooled features. For instance,with temporally encoded features, the best performing architecture stillremains the best and vice versa is also true. The temporal encoding isproviding a signiﬁcant positive offset to the performance.4.4. Word embeddings in language modelIn this encoder-decoder framework, a word embedding is a vectorrepresentation for each word in the available vocabulary for videocaption generation. Word embeddings are much more powerful low-dimensional representations for words as compared to the sparse one-hot vectors. More importantly, unlike one-hot vectors, word embed-dings can be learned for the captioning tasks. In captioning literature,two methods are commonly used to compute these vectors. The ﬁrst approach is to learn the vectors from the training dataset while the lan-guage model is trained. In this case, one can initialize the embeddingvectors randomly and compute the embeddings tailored to the captioningtask. However, such vectors often fail to capture rich semantics due to thefact that captioning corpus size is often small for the purpose of training alanguage model. The second way to obtain these vectors is to use pre-trained embeddings that are learned for a different task and selectthose according to the vocabulary of the current task.We follow both of the aforementioned methods to compute embed-dings in our analysis. For theﬁrst method, random initialization is per-formed. For the second, we obtain the most commonly used four pre-trained word embeddings in the contemporary video description litera-ture, namely Word2Vec [17], two variants of Glove (i.e.,glove6Band glove840B)[18] and FastText [50]. In our analysis, we select the bestperforming CNN model from our experiments in Section 4.3that uses temporal encoding for feature transformation, i.e., Inception-ResNet-V2. The results of our experiments for word embeddings are summarized inFig. 4. From theﬁgure, we can conclude that FastText currently providesmuch more effective word embeddings for video captioning than theother techniques. Moreover, learning the embeddings with randominitialization is still a useful option for the MSVD dataset. This is true tothe extent thatglove6Bshows comparable performance to ourrandomly initialized learned word embeddings.We attribute the superior performance of FastText to its ability togenerate vectors for out of vocabulary words according to the contextualvectors. The other word embeddings do not have this property. Forinstance, with 9;914 words of corpus vocabulary size in FastText, 8 ;846 tokens are extracted from the pre-trained embeddings and the embed-dings for the remaining 1 ;068 tokens are generated using character n-grams of out-of-vocabulary words. The resulting vectors are then mergedto produce theﬁnal embedding vector. This strategy is certainly betterthan random initialization of the out-of-vocabulary words. With FastTextat the top,glove840Band Word2Vec performs almost at par. Among allthe pre-trained embeddings,glove6Bproved to be the weakest.4.5. Language model depth selectionIn language models, given the type and size of data, depth of the
model plays the pivotal role in effective learning. Where lower layers of amodel learn to represent thesyntactic information(parts of speech, grammatical role of words in each sentence etc.), semantic information (meaning of the words, contextual information) is better captured at thehigher layers. As each layer learns different type of information, depth ofmodels becomes important for effective language modelling. However,the modelling performance may start to deteriorate at a certain depth dueto the data size limitation.In our experiments, Gated Recurrent Units (GRUs) based languagemodels are used. Long Short Term Memory (LSTM) networks are alsopopular for language modeling, however, there is a consensus in theliterature that the performance and behavior of the two do not deviatesigniﬁcantly for the same task [51,52]. In our analysis, we vary the number of layers in the language model to observe the performancechange. Our empirical evaluation shows that two-layers language modelperforms best under our settings (type and size of dataset,encoder-decoder framework), as compared to a one or a three layermodel. The results are summarized in Table 1. Increasing layers from one to two generally improves the captioning performance. However,increasing the layers further to three does not result in performance gain.In fact, it slightly deteriorates the scores across all metrics. The experi-ments are performed using visual features of Inception-ResNet-V2 thatare transformed with temporal encoding [ 25] for the language model.4.6. Hyperparameter settingsAppropriate hyper-parameter setting and model ﬁne-tuning are well- known for their role in achieving the improved performance with deepnetworks. Here, we provide a study of a few important hyper-parametersrelevant to the captioning task under the encoder-decoder framework.The reported results andﬁndings can serve as guidelines for the com-munity for training effective captioning models.State Size: In the language model, deciding a suitable state size iscritical. We tested captioning performance for various state sizes, i.e., 512, 1024, 2048, and 4096. These results are reported in Table 2and Fig. 5.W eﬁnd a direct relation between the state size and the modelperformance. We compute Pearson’s correlation between state sizes andeach metric as shown inTable 4. It is evident from the results that there issigniﬁcant correlation between state size and all the metrics. The rela-tionship is even stronger in the lower n–grams of BLEU metric. It can be observed that the model performance enhances gradually when wechange the state size from 512 until 2048. Further increase in the statesize results in negligible or no improvement in the performance of theBLEU-4, METEOR, CIDEr, and ROUGE
Lmetrics. However, lowern–grams of BLEU metric (B1, B2, B3) show slight improvements in such cases.
Fig. 4.Performance of four popular pre-trained word embeddings and thelearned embedding with random initialization.Table 1Results of depth variation in GRU-based language model.
Model Depth B-4 M C R1 layer 49.6 34.9 75.8 71.32 layers 47.9 35.0 78.1 71.53 layers 47.7 34.6 77.4 70.8N. Aafaq et al. Array 9 (2021) 100052
5Number of Frames: Frame selection can be treated as a function oftime for any video. Though smaller number of frames reduces thecomputation cost of the model however it potentially loses someimportant spatio-temporal information. In this set of experiments, weﬁrst process signiﬁcantly low number of frames,i.e., using every sixteenth (16
th) frame in the video. In the next experiment, we process allframes of the video. A signiﬁcant gain in model performance is observedin the case where all the frames are processed. These experiments areperformed using C3D model (Tran et al., 2015) for visual encoding. Thetwo experiments follows the same settings except in terms of the numberof frames used. The results of these experiments are shown in Table 3.A s evident from the results, a signiﬁcant improvement in model perfor-mance across all metrics can be observed when all frames are used forcaptioning.Fine Tuning Pre-Trained Word Embeddings : It is also observed in our experiments that using pre-trained word embeddings often outper-form random initialization based learned embeddings.We also experimented byﬁne tuning the model for 10 epochs on thepre-trained word embeddings. It was observed that in this case, theperformance on BLEU and CIDEr metrics improved slightly with the ﬁne tuning. However, performance on ROUGE
Lmetric remained negligible. METEOR metric value showed mixed behaviour with no regular patterns.Dropout in Recurrent Layers: Dropout is a technique used in neuralnetworks to prevent overﬁtting of the model during training. In recurrentnetworkse.g., in GRU, input and recurrent connections to GRU units areprobabilistically excluded from activation and weight updates whiletraining the network. Using dropout is typically effective for traininglanguage models with large dataset. However, with the MSVD dataset,the caption corpus is rather small (e48K captions withe9K unique to- kens), dropout therefore does not have a signi ﬁcant effect on language model performance for this dataset, or the datasets of similar scale. Weemployed dropout in the recurrent layers of language model. However, itwas observed that application of dropout did not improve the perfor-mance. In fact, it sometimes resulted in slight deterioration of the modelperformance. Based on the observed behavior, we can con ﬁdently recommend to avoid the use of recurrent dropout in a GRU languagemodel, given the training data of MSVD size (or comparable) and modelcomplexity of 2/C03 GRU layers.5. System level discussion and analysisWith Section4focusing on‘ablation study’of individual components, in this section, we further discuss and analyze the results of the pipelineas a whole, at the system level. First, we discuss the results in terms ofMin–Max improvements in captioning score for each metric, respec-tively, as shown inTable 5. Here,‘Min’denotes the minimum percentage gain in the performance which is computed as the difference between thelowest score and the second lowest score in our experiments. ‘Max’de- notes the percentage gain achieved by comparing the lowest and thehighest values achieved. The Min–Max ranges provide an estimate of theperformance gain that is possible by varying the selection of componentvariants.InTable 5,ﬁrst two rows depict improvements by selecting superioror inferior CNNs (in terms of their original results on ImageNet classi ﬁ- cation accuracies). These results are obtained using mean pooling strat-egy over the frame level features of the corresponding networks. Whencompared across 2D/3D CNNs (ﬁrst row) we see a drastic obtainableimprovement in the model performance i.e., up to 44 % in BLEU and 60 % for CIDEr metric, if we choose the right visual feature encodingmodel. Similarly, when comparing among 2D CNNs only (second row),we see there are signiﬁcant performance variations. These variationsonly resulted from varying the CNN model. Hence, we can conclusivelyargue that superior CNNs (with better representation power) can result insigniﬁcant performance improvement for the captioning techniques.The evaluation results for the word vector representations are shownin row 3 ofTable 5. We can observe that there are a few instances ofsigniﬁcant performance variations across all metrics when we usedifferent word embeddings. Among the used popular embeddings, Fast-Text performs the best and glove6B the weakest. Note that the results alsoinclude the learned word vectors obtained during language modelTable 2Results on the state size choices of the GRU language model.
State Size B1 B2 B3 B4 M C R4096 77.3 62.9 51.6 40.7 30.8 59.2 66.72048 77.1 62.8 51.5 41.0 31.3 61.9 67.61024 74.9 59.4 47.5 36.6 30.0 55.9 65.6512 71.8 54.9 42.8 32.0 28.4 46.5 62.4
Fig. 5.Performance evaluation of language model using four state sizes. Eachtrend line show each metric used to compute the captions score.
Table 3Results on number of frames vs captioning quality.
# Frames B1 B2 B3 B4 M C R16–F 65.4 46.7 34.0 23.0 24.5 31.6 57.2All–F 69.6 52.1 39.6 28.8 27.7 42.6 61.6
Table 4Results of Pearson’s correlation of state sizes with each metric.
B1 B2 B3 B4 M C RPearson’s Correlation 0.8110 0.8028 0.8107 0.7947 0.6897 0.6635 0.6714p-Value 0.1889 0.1972 0.1893 0.2053 0.3103 0.3365 0.3286Table 5Percentage improvements (Min–Max) achievable with careful selection of components. First row compared performance including 2D/3D networks. Row 2demonstrates performance variation with 2D networks only. Subsequent rowsshow improvements due to word embeddings and the depth of language model.
B-4 (%) M (%) C (%) R (%)CNN (2D/3D) 32.64 –44.10 6.86–16.61 23.0–60.09 6.17–9.74 CNN (2D only) 3.93–8.64 3.38–9.12 1.53–30.15 0.92–3.36 Word Vectors 8.27–14.99 0.64–7.03 0.81–9.69 0.60–5.37 Depth of LanguageModel0.42–3.98 0.87–1.16 2.11–3.03 0.71–0.99N. Aafaq et al. Array 9 (2021) 100052
6training with random initialization. Moreover, we also experimentedwithﬁne tuning of the pre-trained embeddings for 10 epochs for thecaptioning task. However, we observed that ﬁne tuning does not result in any drastic performance gain. We noticed that the performance ofword2vec and glove840B mostly remain at par with each other.Compared to the visual feature encoder selection, we can see the per-formance gain by the informed selection of word embeddings are notnegligible either. However, the right CNN model does have a dominanteffect on the performance gain as compared to the word embeddingselection.The last row ofTable 5provides language model depth analysis.Relative to the gain obtainable by varying other components in thepipeline, altering the depth from1-layerto3-layers, does not boost the performance signiﬁcantly. The metric scores generally improve whenmodel depth is varied from 1/C0to/C02layers. However, further increase in the depth degrades the model performance. We can con ﬁdently claim that under the employed popular pipeline, 2-layersGRU network per- forms better as compared to the single or three layers RNNs.InTable 6, we report the maximum percentage gains caused by thefeature transformation techniques in our experiments. Each row reportsthe metric score improvement resulted when mean pooled features arereplaced with the temporal encoding features [ 25] of same CNN network. As can be observed, there is signiﬁcant improvement in the model per-formance across all metrics and all networks with the temporal encoding.The largest performance gain results in the case of C3D. We conjecturethat a major reason behind this phenomenon is that there are always lessnumber of clips as compared to the number of frames in videos. C3Dexploits clips which reduces the number of unit data samples containingdistinct pieces of information for the task at hands. The temporalencoding strategy is able make up for this discrepancy. Moreover, spatialfeature capturing with a 2D-CNN followed by temporal encoding result inmore discriminative video-level features as compared to thespatio-temporal features of 3D-CNNs.Based on the evaluations performed with all components of thecaptioning framework, we can order the components in terms of theirimportance/contribution to the overall captioning performance. To thatend, in our experiments, the most signiﬁcant contribution comes from the feature transformation techniquei.e., Temporal Encoding. The secondsigniﬁcant performance variation is possible through the selection ofappropriate CNN model. At the third position in terms of contribution tocaptioning quality, we can place the word embedding vectors. Thenumber of network layers in captioning model had less signi ﬁcant role to play in our experimental results. A simple 2-layer GRU seems a reason-able baseline choice for the captioning models. Similar to the networklayers, other hyper-parameters choices also contribute to the captioningperformance, as mentioned in Sec.4.6. However, assuming a reasonable default hyper-parameter settings, their role is far less signi ﬁcant than the variation in the major components of the pipeline.6. ConclusionIn this paper, we empirically evaluate and quantify the performancecontribution of encoder-decoder architecture components towards videocaptioning framework. For that matter we decompose the architectureinto four core components namely, CNN architecture ( i.e., employed for visual feature extraction), features transformation technique ( e.g., mean pooling, temporal encoding), word embeddings (utilized in languagemodel), and language model itself. This allows us to carry out acomprehensive and fair ablation study at both the component level andthe system level on a most popular MSVD dataset. Various model hyper-parameters (e.g., depth of language model, it’s internal state size, and dropout in recurrent layers) are also included in our empirical study.Exhaustive experiments are carried out to capture and quantify the per-formance gain by each constituent component of the overall videocaptioning framework. In particular, we use 5 popular CNNs ( i.e., C3D, VGG-16, VGG-19, Inception-V3, and Inc-ResNet-V2), 2 feature trans-formation algorithms (i.e., mean pooling, temporal encoding), 5 pre-trained word embeddings (i.e., learned, word2vec, glove6B, glove840B,and fasttext) and an RNN language model with depth of 1, 2, and 3 layers.It is demonstrated that with a well-informed selection of the componentsin the encoder-decoder based video captioning framework, a signi ﬁcant performance gain can be achieved. In our experiments, the best per-forming framework comprises Inception-ResNet-V2 as the visualencoder, followed by temporal encoding for feature transformation,fasttext word embeddings and a 2 layers language model.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgementThis work is supported by Australian Research Council Grant ARCDP19010244. The GPU used for this work was donated by NVIDIACorporation.References
[1]Donahue J, Anne Hendricks L, Guadarrama S, Rohrbach M, Venugopalan S,Saenko K, Darrell T. Long-term recurrent convolutional networks for visualrecognition and description. In: Proceedings of the IEEE conference on computervision and pattern recognition; 2015. p. 2625 –34. [2]Elman JL. Finding structure in time. Cognit Sci 1990;14:179 –211. [3]Cho K, Van Merri€enboer B, Gulcehre C, Bahdanau D, Bougares F, Schwenk H,Bengio Y. Learning phrase representations using rnn encoder-decoder for statisticalmachine translation. 2014. arXiv preprint arXiv:1406.1078 . [4]Hochreiter S, Schmidhuber J. Long short-term memory. Neural Comput 1997;9:1735–80.[5]Venugopalan S, Xu H, Donahue J, Rohrbach M, Mooney R, Saenko K. Translatingvideos to natural language using deep recurrent neural networks. 2014. arXivpreprint arXiv:1412.4729. [6]Pan Y, Mei T, Yao T, Li H, Rui Y. Jointly modeling embedding and translation tobridge video and language. In: Proceedings of the IEEE conference on computervision and pattern recognition; 2016. p. 4594 –602. [7]Pan Y, Yao T, Li H, Mei T. Video captioning with transferred semantic attributes. In:IEEE CVPR; 2017.[8]Yao L, Torabi A, Cho K, Ballas N, Pal C, Larochelle H, Courville A. Describing videosby exploiting temporal structure. In: Proceedings of the IEEE internationalconference on computer vision; 2015. p. 4507 –15. [9]Gan Z, Gan C, He X, Pu Y, Tran K, Gao J, Carin L, Deng L. Semantic compositionalnetworks for visual captioning. In: IEEE CVPR; 2017 . [10]Yao X, Han J, Cheng G, Qian X, Guo L. Semantic annotation of high-resolutionsatellite images via weakly supervised learning. IEEE Trans Geosci Rem Sens 2016;54:3660–71.[11]Pei W, Zhang J, Wang X, Ke L, Shen X, Tai YW. Memory-attended recurrent networkfor video captioning. In: The IEEE conference on computer vision and patternrecognition (CVPR); 2019. [12]Pan B, Cai H, Huang DA, Lee KH, Gaidon A, Adeli E, Niebles JC. Spatio-temporalgraph for video captioning with knowledge distillation. 2020. arXiv preprint arXiv:2003.13942.[13]Yan C, Tu Y, Wang X, Zhang Y, Hao X, Zhang Y, Dai Q. Stat: spatial-temporalattention mechanism for video captioning. In: IEEE transactions on multimedia;2019.[14]Liu S, Ren Z, Yuan J. Sibnet: sibling convolutional encoder for video captioning. In:IEEE transactions on pattern analysis and machine intelligence; 2020 .Table 6Percentage improvement achieved in BLEU (B-4), METEOR (M), CIDEr (C), andROUGE
L(R) metrics when mean pooled visual features are replaced withtemporally encoded features of corresponding networks.
B-4 (%) M (%) C (%) R (%)C3D 40.97 11.91 30.75 9.42VGG-16 13.09 7.77 20.04 4.59VGG-19 12.59 6.86 18.80 4.70Inception-V3 10.84 4.04 8.82 2.98Inception-ResNet-V2 10.14 4.33 8.80 3.25N. Aafaq et al. Array 9 (2021) 100052
7[15]Wang X, Chen W, Wu J, Wang YF, Yang Wang W. Video captioning via hierarchicalreinforcement learning. In: Proceedings of the IEEE conference on computer visionand pattern recognition; 2018. p. 4213 –22. [16]Wang B, Ma L, Zhang W, Liu W. Reconstruction network for video captioning. In:Proceedings of the IEEE conference on computer vision and pattern recognition;2018. p. 7622–31.[17]Mikolov T, Sutskever I, Chen K, Corrado GS, Dean J. Distributed representations ofwords and phrases and their compositionality. In: Advances in neural informationprocessing systems; 2013. p. 3111 –9. [18]Pennington J, Socher R, Manning C. Glove: global vectors for word representation.In: Proceedings of the 2014 conference on empirical methods in natural languageprocessing. EMNLP); 2014. p. 1532 –43. [19]Kojima A, Tamura T, Fukunaga K. Natural language description of human activitiesfrom video images based on concept hierarchy of actions. IJCV 2002;50:171 –84. [20]Das P, Xu C, Doell RF, Corso JJ. A thousand frames in just a few words: lingualdescription of videos through latent topics and sparse object stitching. In: IEEECVPR; 2013. p. 2634–41. [21]Krishnamoorthy N, Malkarnenkar G, Mooney RJ, Saenko K, Guadarrama S.Generating natural-language video descriptions using text-mined knowledge. In:AAAI; 2013. p. 2.[22]Yao X, Han J, Zhang D, Nie F. Revisiting co-saliency detection: a novel approachbased on two-stage multi-view spectral rotation co-clustering. IEEE Trans ImageProcess 2017;26:3196–209. [23]Yu H, Wang J, Huang Z, Yang Y, Xu W. Video paragraph captioning usinghierarchical recurrent neural networks. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition; 2016. p. 4584 –93. [24]Cheng G, Yang C, Yao X, Guo L, Han J. When deep learning meets metric learning:remote sensing image scene classi ﬁcation via learning discriminative cnns. IEEE Trans Geosci Rem Sens 2018;56:2811 –21. [25] Aafaq N, Akhtar N, Liu W, Gilani SZ, Mian A. Spatio-temporal dynamics andsemantic attribute enriched visual encoding for video captioning. In: IEEE CVPR;2019. URL:http://arxiv.org/abs/1902.10322 . [26]Park JS, Darrell T, Rohrbach A. Identity-aware multi-sentence video description. In:Proceedings of the ECCV; 2020 . [27]Li X, Zhao B, Lu X, et al. Mam-rnn: multi-level attention model based rnn for videocaptioning. In: IJCAI; 2017. p. 2208 –14. [28]Xu K, Ba J, Kiros R, Cho K, Courville A, Salakhudinov R, Zemel R, Bengio Y. Show,attend and tell: neural image caption generation with visual attention. In:International conference on machine learning; 2015. p. 2048 –57. [29]Wang J, Wang W, Huang Y, Wang L, Tan T. M3: multimodal memory modelling forvideo captioning. In: Proceedings of the IEEE conference on computer vision andpattern recognition; 2018. p. 7512 –20. [30]Chen Y, Wang S, Zhang W, Huang Q. Less is more: picking informative frames forvideo captioning. In: Proceedings of the European conference on computer vision.ECCV); 2018. p. 358
–73. [31]Zhang J, Peng Y. Object-aware aggregation with bidirectional temporal graph forvideo captioning. In: Proceedings of the IEEE conference on computer vision andpattern recognition; 2019. p. 8327 –36. [32]Zheng Q, Wang C, Tao D. Syntax-aware action targeting for video captioning. In:Proceedings of the IEEE/CVF conference on computer vision and patternrecognition; 2020. p. 13096–105.[33]Papineni K, Roukos S, Ward T, Zhu WJ. Bleu: a method for automatic evaluation ofmachine translation. In: Proceedings of the 40th annual meeting on ACL; 2002.p. 311–8.[34]Lin CY. Rouge: a package for automatic evaluation of summaries. In: Textsummarization branches out: proceedings of the ACL-04 workshop; 2004[Barcelona, Spain].[35]Banerjee S, Lavie A. Meteor: an automatic metric for mt evaluation with improvedcorrelation with human judgments. In: Proceedings of the ACL workshop onintrinsic and extrinsic evaluation measures for machine translation and/orsummarization; 2005. p. 65–72. [36]Vedantam R, Lawrence Zitnick C, Parikh D. Cider: consensus-based imagedescription evaluation. In: IEEE CVPR; 2015 . [37]Aafaq N, Mian A, Liu W, Gilani SZ, Shah M. Video description: a survey of methods,datasets, and evaluation metrics. ACM Comput Surv 2019b;52:115 . [38]Kilickaya M, Erdem A, Ikizler-Cinbis N, Erdem E. Re-evaluating automatic metricsfor image captioning. 2016. arXiv preprint arXiv:1612.07600 . [39]Robertson S. Understanding inverse document frequency: on theoretical argumentsfor idf. J Doc Oct 2004;60:503–20. [40]Chen X, Fang H, Lin TY, Vedantam R, Gupta S, Doll /C19ar P, Zitnick CL. Microsoft coco captions: data collection and evaluation server. 2015. arXiv preprint arXiv:1504.00325.[41]Chen DL, Dolan WB. Collecting highly parallel data for paraphrase evaluation. In:ACL: human language technologies-volume 1. ACL; 2011. p. 190 –200. [42]Russakovsky O, Deng J, Su H, Krause J, Satheesh S, Ma S, Huang Z, Karpathy A,Khosla A, Bernstein M, et al. Imagenet large scale visual recognition challenge. Int JComput Vis 2015;115:211–52. [43]Karpathy A, Toderici G, Shetty S, Leung T, Sukthankar R, Fei-Fei L. Large-scalevideo classiﬁcation with convolutional neural networks. In: Proceedings of the IEEEconference on computer vision and pattern recognition; 2014. p. 1725 –32. [44]Tran D, Bourdev L, Fergus R, Torresani L, Paluri M. Learning spatiotemporalfeatures with 3d convolutional networks. In: Proceedings of the IEEE internationalconference on computer vision; 2015. p. 4489 –97. [45]Simonyan K, Zisserman A. Very deep convolutional networks for large-scale imagerecognition. In: ICLR; 2015. [46]Szegedy C, Vanhoucke V, Ioffe S, Shlens J, Wojna Z. Rethinking the inceptionarchitecture for computer vision. In: Proceedings of the IEEE conference oncomputer vision and pattern recognition; 2016. p. 2818 –26.[47]Szegedy C, Ioffe S, Vanhoucke V, Alemi AA. Inception-v4, inception-resnet and theimpact of residual connections on learning. In: Thirty- ﬁrst AAAI conference on artiﬁcial intelligence; 2017. [48]Krishna R, Hata K, Ren F, Fei-Fei L, Carlos Niebles J. Dense-captioning events invideos. In: Proceedings of the IEEE international conference on computer vision;2017. p. 706–15.[49]Oppenheim AV. Discrete-time signal processing. Pearson Education India; 1999 . [50]Bojanowski P, Grave E, Joulin A, Mikolov T. Enriching word vectors with subwordinformation. TACL 2017:135–46. [51]Chung J, Gulcehre C, Cho K, Bengio Y. Empirical evaluation of gated recurrentneural networks on sequence modeling. 2014. arXiv preprint arXiv:1412.3555 . [52]Jozefowicz R, Zaremba W, Sutskever I. An empirical exploration of recurrentnetwork architectures. In: International conference on machine learning; 2015.p. 2342–50.N. Aafaq et al. Array 9 (2021) 100052
8