Original Article
A look at the time delays in CVSS vulnerability scoring
Jukka Ruohonen
Department of Future Technologies, University of Turku, FI-20014 Turun yliopisto, Finland
article info
Article history:Received 5 October 2017Revised 23 November 2017Accepted 21 December 2017Available online 27 December 2017Keywords:Software vulnerabilityVulnerability severitySeverity scoringDatabase maintenanceCyber securityabstract
This empirical paper examines the time delays that occur between the publication of CommonVulnerabilities and Exposures (CVEs) in the National Vulnerability Database (NVD) and the CommonVulnerability Scoring System (CVSS) information attached to published CVEs. According to the empiricalresults based on regularized regression analysis of over eighty thousand archived vulnerabilities, (i) theCVSS content does not statistically inﬂuence the time delays, which, however, (ii) are strongly affected bya decreasing annual trend. In addition to these results, the paper contributes to the empirical researchtradition of software vulnerabilities by a couple of insights on misuses of statistical methodology./C2112017 The Author. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionSoftware vulnerabilities are software bugs that expose weak-nesses in software systems. The CVSS standard is used to classifythe severity of known and disclosed vulnerabilities. Once the clas-siﬁcation and evaluation work has been completed for a vulnera-bility identiﬁed with a CVE, the structured and quantiﬁedseverity information is stored to vulnerability databases. Moti-vated by a recent empirical evaluation [16], this paper examines the time delays between the publication of CVEs and the usuallylater publication of CVSS information. The scope is restricted toNVD and the second revision of the CVSS standard.The use of CVSS is mandated and recommended by manystate agencies for assessments in different security-criticaldomains[36], including but not limited to medical devices [38] and the payment card industry[2]. The standard has been alsoincorporated into different governmental security risk, threat,and intelligence systems. Furthermore, CVSS information is usedin numerous different commercial products [16], ranging from vul- nerability scanners and compliance assessment tools to automatedpenetration testing and intrusion detection systems.CVSS is also widely used in academic research. Typical applica-tion domains include risk analysis [2,14], security audit frame- works[4], so-called attack graphs[7,26], and empirical assessments using CVSS for different purposes [1,25,31,33].T o these ends, a lot of work has been done to improve CVSSwith different weighting algorithms [17,40], among other techniques[9,30]. With some rare exceptions[13], limited atten- tion has been given for examining how severity assessments aredone in practice.Practical approaches are important because CVSS has faced alsochallenges. Analogous to problems that have affected CVE assign-ments[33,34], different practical problems have inﬂuenced theseverity assignments for CVE-stamped vulnerabilities. Excludingthe actual content of the standard, the historical problems relatedto classiﬁcation inconsistencies, time delays, and the proliferationof classiﬁcation standards[5,24]. Some of these problems havecontinued to exist. For instance, proliferation has continued inrecent years; new standards have been introduced for classifyingsoftware misuse and conﬁguration vulnerabilities [3]. Some coun- tries[45]and companies[43]have also introduced their ownseverity metrics. To examine whether also the problem with timedelays is still present—as has been suspected [18], a brief remark is required about the CVE and CVSS publication processes in the con-text of NVD. Although the available documentation about theseprocesses is limited[28], the sketch presented inFig. 1is not a far-fetched analytical speculation.The process starts when security researchers, vendors, andother related actors request CVEs for vulnerabilities they have dis-covered or made aware of. These request-response dynamics arehandled by the non-proﬁt MITRE corporation. As is common in
https://doi.org/10.1016/j.aci.2017.12.0022210-8327//C2112017 The Author. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).Peer review under responsibility of King Saud University.
Production and hosting by Elsevier
E-mail address:juanruo@utu.ﬁApplied Computing and Informatics 15 (2019) 129–135
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
software engineering, MITRE presumably maintains a backlog forthe CVEs assigned, some of which may be even rejected for inclu-sion to NVD. Although the structure of the backlog is unknown, asimple FIFO (ﬁrst-in, ﬁrst-out) might be considered in order to con-nect the speculation to a recent theoretical work [10]. In any case, eventually the vulnerabilities accepted for archiving are publishedin NVD. In parallel to the coordination and archiving work relatedto CVEs, vulnerabilities are evaluated for their severity by the NVDteam, which largely operates independently from others carryingsimilar evaluations[16]. Once the evaluation has been completed,the CVE-referenced vulnerability information is updated in NVD.The time lags between the initial CVE publications and the laterCVSS updates constitute the empirical phenomenon examined.There is another viewpoint to the abstract CVE backlog. Thisviewpoint originates from the so-called switching costs, whichare often high for information technology standards [37]. Such the- oretical costs cover also database maintenance: even smallchanges made to standards may imply a lot of evaluation work par-ticularly in case old information needs to be updated. This concernwas raised also during the 2007 introduction of the second revisionof the CVSS standard[36]. In other words, updates can be costly interms of time and resources—given the nearly ninety thousand vul-nerabilities currently archived in NVD. Therefore, it is relevant toask the following research question (RQ) about the time lags affect-ing CVSS scoring.RQ
1Do the time delays between CVE publications and CVSSupdates vary systematically according to an annual year-to-year trend?Another question relates to the content of the CVSS standard interms of the vulnerabilities scored. Reﬂecting the disagreementsamong experts about the severity of some vulnerabilitytypes[13], it can be hypothesized that the CVSS content itselfaffects the time delays. Not all vulnerabilities are equally easy (orhard) to classify in terms of severity; hence, some vulnerabilitiesmay take a relatively short (long) time to classify. This reasoningis presented as a second research question, stated as follows.RQ
2Do the time delays vary systematically according to thecontent of the CVSS severity information?Finally, a third and ﬁnal question can be postulated for control-ling the answers to the earlier two questions:RQ3Does the answer to RQ 2hold when also the annual trend iscontrolled for?According to the empirical results, only the answer to RQ
1is positive. For predicting the time delays, the CVSS content is largelynoise. The statistical effect (RQ
2) also fades away once the annualtrend is controlled for (RQ
3). To elaborate how these conclusionsare reached, the remainder of this paper is structured into threesections. Namely: Section2introduces the dataset and the opera-tionalization of the variables used, Section 3outlines the statistical methodology and presents the empirical results along the way, andSection4ﬁnally discusses the ﬁndings.2. SetupTo outline the setup for the analysis, the following discussionwill address the operationalization of the delay metric examinedthe covariates used to model the metric.2.1. ResponseFollowing the so-called vulnerability life cycle research tradi-tion[25,33], the interest relates to a time difference
Di¼sCVSSi/C0sCVEai;given ð1Þ
sCVSSiPsCVEaifor alli¼1;...;n:
The integer sCVSSidenotes the day (timestamp) at which a CVSSentry was generated for thei:th CVE that was published at
sCVEai.I n practice, the two timestamps map to the ﬁelds cvss:generated- on-datetimeandvuln:published-datetimein the NVD’s extensible markup language schema. Although the exact meaningof the ﬁelds is undocumented, the time differences can be inter-preted as delays between CVE and CVSS publications.Of the 89,465 archived vulnerabilities with both CVE and CVSSentries, the condition
sCVSSiPsCVEaifails to satisfy only for 1,375 vulnerabilities. Without loss of generality, these cases wereexcluded. The same applies to CVEs without severity records. Atthe time of retrieving the NVD content [27], there were 2,218 vul- nerabilities that were published but still lacked CVSS records. Mostof these cases relate either to new vulnerabilities that are still inthe pipeline for severity assessments, or to already published CVEsthat were later rejected as inappropriate for archiving. Either way,these had to be also excluded in order for
Dito be deﬁned for all cases observed. In total, the dataset examined containsn¼89;465/C01;375¼88;090 archived cases. Given these cases,the distribution of the time delays observed is shown in Fig. 2. The timelines exhibit a heavy-tailed distribution with extre-mely long right tail. A half of the vulnerabilities observed have seen
Fig. 2.CVE-CVSS publication time delays (Eq. 1).Fig. 1.A simpliﬁed model for CVSS processing.130 J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135severity assignments already a day after CVEs were published, butthe standard deviation is still over a year. Most of this deviation iscaused by a few extreme outliers for which the severity scoreswere assigned even a decade after the CVEs were originallypublished.To brieﬂy probe these outliers further, Fig. 3. displays the distri- bution of another time difference
di¼sCVSSi/C0sCVEbi; ð2Þ
where sCVEbidenotes thevuln:last-modified-datetimeﬁeld in NVD. The large amount of negative values indicate that CVEs areoften updated after these were already published with CVSS infor-mation. Interestingly, 187 outlying cases satisfy d
i>0, which may point toward some inconsistencies in database maintenance; CVSSinformation was generated without updating the corresponding
sCVEbitimestamps. About a quarter of the cases observed satisfyd
i¼0, meaning that the latest CVE modiﬁcations matched the gen-eration of severity information.
2.2. CovariatesTwo types of covariates are used for modeling the timedelays in(1). The ﬁrst contains the CVSS information itself. TheCVSS (v. 2) standard[6]classiﬁes the impact of vulnerabilitiesaccording to conﬁdentiality, integrity, and availability (CIA). Eachletter in the CIA acronym further expands into three categories thatcharacterize the impact upon successfully exploiting the vulnera-bility in question. Thus, the analytical structure behind the impactdimension can be illustrated with a diagram:
IMPACT2CONFIDENTIALITY2NONE/C3
PARTIALCOMPLETE8><>:INTEGRITY2NONE
/C3
PARTIALCOMPLETE8><>:AVAILABILITY2NONE
/C3
PARTIALCOMPLETE8><>:8>>>>>>>>>>>>>>>>><>>>>>>>>>>>>>>>>>:
The three impact metrics measure the severity of a vulnerabilityon a system after the vulnerability has already been exploited.However, not all vulnerabilities can be exploited; therefore, theCVSS standard speciﬁes also an exploitability dimension for vul-nerabilities. Like with the impact dimension, exploitabilityexpands into three metrics (access vector, complexity, and authen-tication) that can each take three distinct values. The analyticalmeaning can be again summarized with the following diagram:EXPLOITABILITY2VECTOR2LOCAL/C3
NETWORKADJACENT8><>:COMPLEXITY2LOW
/C3
MEDIUMHIGH8><>:AUTHENTICATION2NONE
/C3
SINGLEMULTIPLE8><>:8>>>>>>>>>>>>>>>>><>>>>>>>>>>>>>>>>>:
The rationale for the impact and exploitability metrics relate todifferent combinatory relationships between the different valuesthe metrics can take. For instance, it is probable that mass-scaleattacking tools target less complex vulnerabilities that can beexploited through a network without performing authentication,possibly regardless of the impact upon conﬁdentiality, integrity,and availability. There exists also some empirical evidence alongthese lines[1]. However, the impact and exploitability dimensionsboth relate to intrinsic characteristics of vulnerabilities; they areconstant across time and environments. For instance, EXPLOIT-ABILITY cannot answer to a temporal question about whetheran exploit is known to exists for the vulnerability inquestion[30,43]. The same point extends toward NVD in general[8]. For these and other reasons, the new (v. 3) standard for CVSSenlarges the dimensions toward temporal and environmentalmetrics.For the present purposes, however, the impact and exploitabil-ity dimensions are sufﬁcient for soliciting an answers to RQ
2. This choice is also necessitated by the paper’s focus on NVD, which doesnot currently provide full CVSS v. 3 information [29]. Despite of this limitation, a correlation between the six CVSS metrics and
Di
could be expected due to the fairly detailed criteria used for themanual classiﬁcation. Complex vulnerabilities with severe impactmay require more evaluation work than trivial vulnerabilities; aremote buffer overﬂow vulnerability is usually more difﬁcult tointerpret compared to a trivial cross-site scripting vulnerability.Also the reverse direction is theoretically possible; more effortmay be devoted for high-proﬁle vulnerabilities [18]. Either way, RQ
2seems like a sensible hypothesis worth asking.With regard to statistical modeling, the three impact metricsand the three exploitability metrics are included in the models asso-called dummy variables. For each metric, the reference categoryis marked with a star in the previous two diagrams. For instance,INTEGRITY is expanded into two dummy variables, INTEGRITY(PARTIAL) and INTEGRITY(COMPLETE), say, the effects of whichare compared against INTEGRITY(NONE), which cannot beincluded in the models due to multicollinearity. The same strategyapplies to the metrics used for evaluating RQ
1. Namely, the annual effects are proxied through 18 dummy variables that record theyear at which a vulnerability was published according to
sCVEai. Because only ﬁve vulnerabilities were published in the 1980s anda negiligle amount (about 1.8 %) in the 1990s, the reference cate-gory for the annual dummy variables is formed by collapsing allvulnerabilities published prior to 2000 into a single group. Giventhe two CVSS dimensions and the dummy variable approximationfor the annual trend, three model matrices ( X
1;X 2, andX 3) are used in the statistical computation:
M1:X1¼1;X IMPACT½/C138;M
2:X2¼X 1;XEXPLOITABILITY ½/C138;M
3:X3¼X 2;XANNUAL ½/C138:8><>: ð3Þ
Fig. 3.CVE-CVSS modiﬁcation time delays (Eq. 2).J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135 131The ﬁrst model M 1regressesD¼½ D1;...;Dn/C1380against a constant represented by an-length vector of ones,1, and the six impact dummy variables present in theðn/C26ÞmatrixX
IMPACT . The second model is identical except that further six dummy variables areincluded for measuring the exploitability dimension. The thirdand ﬁnal model includes all information used.Despite of the growing number of CVEs processed from the circamid-2000s onward[32], the time delays for CVSS processing havesteadily decreased over the years. As can be seen from Fig. 4, there have been no extreme outliers in recent years, meaning that mostof the right tail inFig. 2is attributable to older CVEs. A possible butspeculative explanation is that the work done to update old CVEswith CVSS (v. 2) information has mostly been completed.The strong decreasing trend is likely to support a positiveanswer to the research question RQ
1. Given this prior expectation, the main interest in the forthcoming analysis relates to the statis-tical effect of the impact and exploitability metrics when also theannual trend is modeled. One strategy for evaluating the researchquestion RQ
3is to compare the models M 1and M 2against the full information model M
3. If the CVSS metrics provide statisticalpower for predictingD, this power should be visible also whenthe decreasing annual trend is controlled for.3. ResultsThe responseDrepresents a count data vector; each observationin the vector counts the days between CVE and CVSS publicationsin NVD. Thus, a Poisson regression model provides a natural start-ing point for modeling the time delays. The expected value of theresponse thus is
EDjX j/C0/C1¼eXjb; ð4Þ
whereX jis a given model matrix from(3)andbak-length vector of regression coefﬁcients, including the intercept b
1. This conditional mean is always positive.
However, the model assumes thatDis distributed from the Poisson distribution, which, in turn, implies that the mean of thetime delays should equal the variance of the delays. As can be con-cluded from the numbers shown in Fig. 2, this assumption is clearly problematic in the current setting. While bis still consis- tently estimated, the apparent overdispersion, Var ðDÞ>EðDÞ, affects the standard errors of the regression coefﬁcients, and,hence, the statistical signiﬁcance of the coefﬁcients. A commonsolution to tackle the overdispersion problem is to estimate a so-called negative binomial model (NBM) instead, although theconventional ordinary least squares (OLS) regression oftenworks well in applied problems when the response is suitablytransformed[15]. Thus, instead of(4), consider that the conditional mean is given by an OLS regression.Eðln½Dþ1/C138jX jÞ¼Eð~DjX jÞ¼X jb;ð5Þ
such that
^ba¼min
bð~D/C0X jbÞ0ð~D/C0X jbÞð6Þ
When applied to the full model matrix X 3, the adjusted coefﬁ- cient of determination is 0.64 for this OLS regression. In otherwords, the general model performance is quite decent, given thelimited amount of information used to model the severity assign-ment timelines. Moreover, only three coefﬁcients in ^b
aare not sig- niﬁcant at the conventionalp<0:05 threshold. By further testingthe joint signiﬁcance of the dummy variable groups with a F-test, all groups are signiﬁcant at ap<0:001 level. Also the combined forward-stepwise and backward-stepwise algorithm (as imple-mented in thestepfunction for R) retains all coefﬁcients in ^b
a. As is common in applied problems[35], the~D¼lnðDþ1Þtransfor- mation does not account for the high positive skew; therefore,another test can be computed by using an R implementation [44] for a consistent covariance matrix estimator [42]. However, the results do not diverge much from the plain OLS estimates; onlyone additional coefﬁcient is insigniﬁcant at a p<0:05 threshold. Finally, analogous conclusions can be reached by estimatinga negative binomial regression model with the assumptionthat VarðDÞ¼EðDÞþ/½EðDÞ/C138
2, where/is a parameter to be estimated[19,41]. By again using an R implementation [20], only two coefﬁcients attainpP0:05.Thus, based on statistical signiﬁcance, positive answers wouldbe given to all three research questions. This conclusion wouldbe unwarranted, however. Most of the coefﬁcients in the M
3model are close to zero, irrespective of the estimation strategy. Since allcovariates are dummy variables (and, hence, have the same scale),this observation can be illustrated in the form of Fig. 5, which plots the OLS coefﬁcients (y-axis) against the corresponding NBM coefﬁ-cients (x-axis), omitting the constant ^b
1. As can be seen, there are some differences between the two regression coefﬁcient vectors,but these differences apply mostly to the annual effects. In partic-ular, the coefﬁcients for the impact and exploitability dimensionsare very close to zero without notable differences between theOLS and NBM estimates. The largest absolute coefﬁcient valuesare obtained for the annual effects from 2005 to 2017. These coef-ﬁcients exhibit also the largest differences between the OLS andthe negative binomial estimates.To examine these observations further, the so-called least abso-lute shrinkage and selection operator (LASSO) provides a good tool.Fig. 4.Annual time delays (based on sCVEai).
Fig. 5.Coefﬁcients from the OLS and NBM regressions (M 3).132 J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135The LASSO method is a regression model that uses regularization inorder to improve prediction accuracy and feature selection. Whencompared to other regularized regression models, such as the so-called Ridge regression, LASSO can shrink some coefﬁcients exactlyto zero. Although the feature selection properties are not entirelyideal for hypothesis testing[21], this property is desirable for fur-ther examining whether regularization pushes the coefﬁcients forall of the CVSS metrics toward zero. It should be noted that drop-ping individual dummy variables based on feature selection is usu-ally unwarranted because interpretation of the coefﬁcientschanges—but if all of the impact and exploitability dummy vari-ables are regularized toward zero, there is not much to interpret.If this is the case, there is also no particular reason to considermore complex estimation strategies, such as the so-called groupLASSO method[39]. A brief elaboration is required also about themore classical LASSO regressions.Instead of minimizing the residual sum of squares in (6), LASSO minimizes penalized sum of squares given by
^bb¼min
b12nXni¼1ð~Di/C0x0jibÞ2þkXks¼2jbsj();ð7Þ
wherekP0 is known as the shrinkage factor, and the scalingbyð1=2nÞis done to ease comparisons with different sample
Fig. 6.Gaussian LASSO estimates ( ^b b). Fig. 7.Poisson LASSO estimates ( ^b c).J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135 133sizes[12]. The penalty is given by theL 1norm, that is, the sum of the absolute coefﬁcient values, omitting the constant present inX
j.I fkis zero, the solution reduces to the OLS estimates, and whenk!1, all coefﬁcients in^b
btend to zero. Despite of the overdisper-sion, the Gaussian LASSO in(7)can be accompanied with a PoissonLASSO as an additional robustness check.
The so-called quasi log-likelihood for Poisson regression can beobtained by left-multiplying the logarithm of the expected valuesin(4)byDand subtracting EDjX
j/C0/C1from the result[23]. Given this quasi log-likelihood, for the Poisson regression [12].
LbjD;X j/C0/C1¼DX
jb/C0expðX jbÞ;ð8Þ
LASSO optimizes
^bc¼min
b/C0LbjD;X j/C0/C1nþkXks¼2jbsj();ð9Þ
By again using an R implementation [11], the results from the LASSO computations are shown inFigures 6 and 7for the Gaussian and Poisson speciﬁcations. The coefﬁcient magnitudes are shown inthey-axes, the lowerx-axes represent different values of kin loga- rithm scale, and the upperx-axes denote the number of coefﬁcientsnot regularized to zero. The shaded region is based on a 10-foldcross-validation: in each plot, the left endpoint of the region corre-sponds with the value ofkthat gives the minimum cross-validationerror, while the right endpoint is one standard error from thisminimum.
In both ﬁgures, the models M 1and M 2yield large absolute coef- ﬁcient magnitudes for the CVSS metrics. Furthermore, the coefﬁ-cients retain their magnitudes rather long as the shrinkage factorincreases. For instance, the upper-left plot indicates that none ofthe impact metrics are regularized to zero in the Gaussian speciﬁ-cation until aboutk¼expð/C06Þ. However, when the annual affectsare included in M
3, all of the CVSS metrics are very close to zeroparticularly with respect to ^b
b. Although a couple of exploitabilitymetrics retain their magnitudes within the cross-validation regionshown in the lower-right plot inFig. 7, the same conclusion applies more or less also to the Poisson LASSO model. Furthermore, withinthe cross-validation regions, both ^b
band^b ccompare well to the OLS and NBM coefﬁcient vectors illustrated in Fig. 5. To conclude: when predicting the time delay from CVE publications to CVSSassignments, the actual CVSS content is largely noise; the most rel-evant readily available information comes with the decreasingannual trend.4. DiscussionThis short empirical paper examined the time delays that affectCVSS scoring work in the context of NVD. Three research questionswere presented for guiding the empirical analysis based on regres-sion methods. The results are easy to summarize. The CVSS contentis correlated with the time delays (RQ
2), but the correlations are spurious; the decreasing annual trend affecting the time delays(RQ
1) also makes the effects of the CVSS content negiligle (RQ 3). Three points are worthwhile to raise about the signiﬁcance of theseempirical ﬁndings.First, the negative answers to RQ
2and RQ 3are positive ﬁndings in terms of practical applications using CVSS information. Whetherthe application context is governmental security intelligence sys-tems or commercial security assessment tools, there is currentlyno particular reason to worry that a NVD data feed would showsigniﬁcant delays for the CVSS information. Likewise, in 2017,there is no reason to suspect that information for severe vulnera-bilities would tend to arrive later (or earlier) than informationfor mundane vulnerabilities. However, this conclusion does notapply to historical contexts, and, moreover, the historically longdelays affect also academic research.Second, the positive answer to RQ1is a negative ﬁnding in terms of existing academic research; the historically long time delayspresumably translate into selection biases in some existing empir-ical studies using CVSS information. Without naming any particu-lar academic study, consider that a hypothetical article publishedin the late 2000s used a NVD-based dataset of CVE-referenced vul-nerabilities published between 2000 and 2007, say. The long timedelays during this period imply that a lot of the vulnerabilities inthe dataset could not have had CVSS information. Consequently,some existing academic studies are exposed to difﬁcult questionsrelated to sample selection and missing values, among otherissues. This concern is particularly pronounced regarding studiesthat examine time-sensitive topics such as vulnerability disclosure.Third, the results echo the recently raised concern about themisuse of statistical signiﬁcance in the software vulnerability con-text[22]. It seems that the size of archival material stored to vul-nerability databases has surpassed a point after which statisticalsigniﬁcance starts to lose its usefulness for inference in appliedresearch. The current rate of new vulnerabilities archived—about17 per day in 2016—implies that the problem with statistical sig-niﬁcance is only going to get worse. The point is particularlyimportant in case CVEs are referenced with other datasets, includ-ing big data outputted by intrusion detection and related systems.The regularized regression models used in this paper offer onesolution to consider in further applications, but more research isrequired to assess the existing biases and the potential means formoving forward.References
[1]L. Allodi, F. Massacci, Attack potential in impact and complexity, in:Proceedings of the International Conference on Availability, Reliability andSecurity (ARES 2017), ACM, Reggio Calabria, 2017, pp. 32:1–32:6
. [2]
L. Allodi, F. Massacci, Security events and vulnerability data for cybersecurityrisk estimation, Risk Anal. 37 (8) (2017) 1606–1627
. [3]
M.N. Alsaleh, E. Al-Shaer, Enterprise risk assessment based on compliancereports and vulnerability scoring systems, in: Proceedings of the Workshop onCyber Security Analytics, Intelligence and Automation (SafeConﬁg 2014), ACM,Scottsdale, 2014, pp. 25–28
. [4]
M. Aslam, C. Gehrmann, M. Björkman, ASArP: automated security assessment& audit of remote platforms using TCG-SCAP Synergies, J. Inform. Secur. Appl.22 (2015) 28–39
.[5] C. Eiram, B. Martin, The CVSSv2 Shortcomings, Faults, and FailuresFormulation, Risk Based Security and the Open Security Foundation (OSF),2013. Available online in September 2017, < http://www. riskbasedsecurity.com/reports/CVSS-ShortcomingsFaultsandFailures.pdf >. [6] FIRST, A Complete Guide to the Common Vulnerability Scoring System Version2.0, FIRST.ORG, 2007. Available online in June 2015: < https://www.ﬁrst.org/ cvss/cvss-v2-guide.pdf>.[7]
L. Gallon, J.-J. Bascou, CVSS attack graphs, in: Proceedings of the SeventhInternational Conference on Signal Image Technology & Internet-BasedSystems (SITIS 2011), IEEE, Dijon, 2011, pp. 24–31
. [8]
M. Garcia, A. Bessani, I. Gashi, N. Neves, R. Obelheiro, Analysis of operatingsystem diversity for intrusion tolerance, Software: Pract. Exp. 44 (6) (2014)735–770
.[9]
J. Geng, D. Ye, P. Luo, Predicting severity of software vulnerability based ongrey system theory, in: Proceedings of the International Conference onAlgorithms and Architectures for Parallel Processing (ICA3PP), Lecture Notesin Computer Science, vol. 9532, Springer, Zhangjiajie, 2015, pp. 143–152
. [10]
K. Haldar, B.K. Mishra, Mathematical model on vulnerability characterizationand its impact on network epidemics, Int. J. Syst. Assur. Eng. Manage. 8 (2)(2017) 378–392
.[11] T. Hastie, J. Qian, Glmnet Vignette, 2014. Available online in September 2017:<https://web.stanford.edu/hastie/glmnet/glmnet_alpha.html >. [12]
T. Hastie, R. Tibshirani, M. Wainwright, Statistical Learning with Sparsity: TheLasso and Generalizations, CRC Press, Taylor & Francis, Boca Raton, 2015
. [13]
H. Holm, K.K. Afridi, An expert-based investigation of the commonvulnerability scoring system, Comput. Secur. 53 (2015) 18–30
. [14]
S.H. Houmb, V.N.L. Franqueira, E.A. Engum, Quantifying security risk level fromCVSS estimates of frequency and impact, J. Syst. Software 83 (2010) 1622–1634
.[15]
A.R. Ives, For testing the signiﬁcance of regression coefﬁcients, go ahead andlog-transform count data, Meth. Ecol. Evol. 6 (7) (2015) 828–835
.134 J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135[16] P. Johnson, R. Lagerström, M. Ekstedt, U. Franke, Can the common vulnerabilityscoring system be trusted? A Bayesian analysis, IEEE Trans. Depend. Secur.Comput. (2017). Published online in December 2016.[17]J. Ko, S. Lee, T. Shon, Towards a novel quantiﬁcation approach based on smartgrid network vulnerability score, Int. J. Energy Res. 40 (3) (2016) 298–312
. [18] B. Ladd, The Race Between Security Professionals and Adversaries, RecordedFuture Blog, 2017. Available online in November 2017: < https://www. recordedfuture.com/vulnerability-disclosure-delay/ >. [19]
J.F. Lawless, Negative binomial and mixed Poisson regression, Can. J. Stat. 15(3) (1987) 209–225
.[20] M. Lesnoff, R. Lancelot, aod: Analysis of Overdispersed Data, R Package Version1.3, 2012. Available online in September 2017: < https://cran.r-project.org/ web/packages/aod/index.html >. [21]
Z. Li, M.J. Sillanpää, Overview of LASSO-related penalized regression methodsfor quantitative trait mapping and genomic selection, Theor. Appl. Gen. 125 (3)(2012) 419–435
.[22] F. Massacci, How do you know that it works? The curses of empirical securityanalysis, in: T.W. Moore, C.W. Probst, K. Rannenberg, M. van Eeten (Eds.),Assessing ICT Security Risks in Socio-Technical Systems (Dagstuhl Seminar16461), vol. 6, Schloss Dagstuhl-Leibniz-Zentrum fuer Informatik, Dagstuhl,2017, pp. 77–78. Available online in September 2017: < http://drops.dagstuhl. de/opus/volltexte/2017/7039>. [23]
P. McCullagh, Quasi-likelihood functions, Ann. Stat. 11 (1) (1983) 59–67 . [24]
P. Mell, K. Scarfone, S. Romanosky, Common vulnerability scoring system, IEEESecur. Privacy 4 (6) (2006) 85–89
. [25]
P.J. Morrison, R. Pandita, X. Xiao, R. Chillarege, L. Williams, Are vulnerabilitiesdiscovered and resolved like other defects?, Emp Software Eng. (2017) 1–39,Published online in September 2017
. [26]
L. Muñoz-González, D. Sgandurra, M. Barrère, E.C. Lupu, Exact inferencetechniques for the analysis of bayesian attack graphs, IEEE Trans. Depend.Secure Comput. (2017), Published online in March 2017
. [27] NIST, NVD Data Feed and Product Integration, National Institute of Standardsand Technology (NIST), Annually Archived CVE Vulnerability Feeds: SecurityRelated Software Flaws, NVD/CVE XML Feed with CVSS and CPE Mappings(Version 2.0), 2017a. Retrieved in 23 September 2017 from: < https://nvd. nist.gov/download.cfm>.[28] NIST, NVD Frequently Asked Questions. National Institute of Standards andTechnology (NIST), 2017b. Available online in November 2017: < https://nvd. nist.gov/general/faq>.[29] NIST, Vulnerability Metrics. National Institute of Standards and Technology(NIST), 2017c. Available online in November 2017: < https://nvd.nist.gov/vuln- metrics>.[30]
D.M. Ross, A.B. Wollaber, P.C. Trepagnier, Latent feature vulnerability rankingof CVSS vectors, in: Proceedings of the Summer Simulation Multi-Conference(SummerSim 2017), ACM, Washington, 2017, pp. 19:1–19:12
.[31]J. Ruohonen, Classifying web exploits with topic modeling, in: Proceedings ofthe 28th International Workshop on Database and Expert SystemsApplications (DEXA 2017), IEEE, Lyon, 2017, pp. 93–97
. [32]
J. Ruohonen, S. Hyrynsalmi, V. Leppänen, An outlook on the institutionalevolution of the European union cyber security apparatus, Govern. Inform.Quart. 33 (4) (2016) 746–756
. [33]
J. Ruohonen, S. Hyrynsalmi, V. Leppänen, Modeling the delivery of securityadvisories and CVEs, Comput. Sci. Inform. Syst. 14 (2) (2017) 537–555
. [34]
J. Ruohonen, S. Rauti, S. Hyrynsalmi, V. Leppänen, Mining social networks ofopen source CVE coordination, in: Proceedings of the 27th InternationalWorkshop on Software Measurement and 12th International Conference onSoftware Process and Product Measurement (IWSM Mensura 2017), ACM,Gothenburg, 2017, pp. 176–188
. [35]
J. Rydberg, D.M. Carkin, Utilizing alternate models for analyzing countoutcomes, Crime Delinq. 61 (1) (2017) 61–76
. [36]
K. Scarfone, P. Mell, An analysis of CVSS version 2 vulnerability scoring, in:Proceedings of the 3rd International Symposium on Empirical SoftwareEngineering and Measurement (ESEM 2009), IEEE, Lake Buena Vista, 2009,pp. 516–525
.[37]
D.-H. Shin, H. Kim, J. Hwang, Standardization revisited: a critical literaturereview on standards and innovation, Comput. Stand. Interf. 38 (2015) 152–157
.[38] I. Stine, M. Rice, S. Dunlap, J. Pecarina, A cyber risk scoring system for medicaldevices, Int. J. Crit. Infrastruct. Protect (2017). Published online in April 2017.[39]
D. Vidaurre, C. Bielza, P. Larrañaga, A survey of L1regression, Int. Stat. Rev. 81 (3) (2013) 361–387
.[40]
J.A. Wang, M. Guo, H. Wang, L. Zhou, Measuring and ranking attacks based onvulnerability analysis, Inform. Syst. e-Bus. Manage. 10 (4) (2012) 455–490
. [41]
F. Wei, G. Lovegrove, An empirical tool to evaluate the safety of cyclists:community based, macro-level collision prediction models using negativebinomial regression, Accid. Anal. Prevent. 61 (2013) 129–137
. [42]
H. White, A heteroskedasticity-consistent covariance matrix estimator and adirect test for heteroskedasticity, Econometrica 80 (4) (1980) 817–838
. [43]
A.D. Younis, Y.K. Malaiya, Comparing and evaluating CVSS base metrics andMicrosoft rating system, in: Proceedings of the IEEE International Conferenceon Software Quality, Reliability and Security (QRS 2015), IEEE, Vancouver,2015, pp. 252–261
.[44]
A. Zeileis, Econometric computing with HC and HAC covariance matrixestimators, J. Stat. Software 11 (10) (2004) 1–17
. [45]
X. Zhu, C. Cao, J. Zhang, Vulnerability severity prediction and risk metricmodeling for software, Appl. Intell. 47 (3) (2017) 828–836
.J. Ruohonen / Applied Computing and Informatics 15 (2019) 129–135 135