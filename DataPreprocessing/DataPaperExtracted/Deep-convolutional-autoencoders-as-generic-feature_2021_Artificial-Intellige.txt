Deep convolutional autoencoders as generic feature extractors inseismological applications
Qingkai Kong*, Andrea Chiang, Ana C. Aguiar, M. Giselle Fern /C19andez-Godino, Stephen C. Myers, Donald D. Lucas
Lawrence Livermore National Laboratory, USA
ARTICLE INFO
Keywords:Machine learningAutoencoderABSTRACT
The idea of using a deep autoencoder to encode seismic waveform features and then use them in differentseismological applications is appealing. In this paper, we designed tests to evaluate this idea of using autoencodersas feature extractors for different seismological applications, such as event discrimination (i.e., earthquake vs.noise waveforms, earthquake vs. explosion waveforms), and phase picking. These tests involve training anautoencoder, either undercomplete or overcomplete, on a large amount of earthquake waveforms, and then usingthe trained encoder as a feature extractor with subsequent application layers (either a fully connected layer, or aconvolutional layer plus a fully connected layer) to make the decision. By comparing the performance of thesenewly designed models against the baseline models trained from scratch, we conclude that the autoencoderfeature extractor approach may only outperform the baseline under certain conditions, such as when the targetproblems require features that are similar to the autoencoder encoded features, when a relatively small amount oftraining data is available, and when certain model structures and training strategies are utilized. The modelstructure that works best in all these tests is an overcomplete autoencoder with a convolutional layer and a fullyconnected layer to make the estimation.
1. IntroductionMachine learning models, especially recently developed deeplearning models, have the capability to extract features, which aremeasurable properties or characteristics of the studied phenomenon,from images, texts, time series and many other types of data ( Goodfellow et al., 2016;LeCun et al., 2015). Many good reviews are available thatdescribe deep learning applications in seismology and the broad geo-sciences (Bergen et al., 2019;Karpatne et al., 2019;Kong et al., 2019; Lary et al., 2016). In seismology speciﬁcally, great performance has been achieved in event detection and discrimination ( Kong et al., 2016;Li et al., 2018;Linville et al., 2019;Meier et al., 2019;Perol et al., 2018), seismic phase picking (Mousavi et al., 2020;Ross et al., 2018;Zhou et al., 2019;Zhu and Beroza, 2019), denoising (Chen et al., 2019;Saad and Chen, 2020;Tibi et al., 2021;Zhu et al., 2019), and lab experiment predictions (Rouet-Leduc et al., 2017). To highlight a few applications related to the work presented here,Ross et al. (2018)andZhu and Beroza (2019)developed deep learning based approaches for phase picking,which are now adopted widely to estimate the P and S arrivals ( Chaiet al., 2020;Graham et al., 2020;Park et al., 2020;Wang et al., 2020a). They designed deep learning models that automatically extract thewaveform characteristics distinguishing the P phase, S phase and noise tomake decisions about P and S arrivals on the seismic waveform. Aftertraining with large amounts of seismic data, the two models generalizewell with new input data.Linville et al. (2019)explored using convolu- tional and recurrent neural networks to discriminate explosive and tec-tonic sources at local distances, they showed that developed models cansuccessfully determine the source type of the events at an accuracy above99%.An autoencoder is a machine learning model that can be used to learnefﬁcient representations (encoding) from a set of data, and then recoverthe data from these encoded representations. Deep autoencoders havebeen used in many different applications, such as compression, denoising,dimensionality reduction, and feature extraction ( Baldi, 2012;Liu et al., 2017). Particularly, using autoencoders to extract features for differenttasks show great promise (Ditthapron et al., 2019;Gogna and Majumdar, 2019;Kunang et al., 2018;
Xing et al., 2015). In seismology, applications use autoencoders to extract compressed feature representations for
* Corresponding author.E-mail address:kong11@llnl.gov(Q. Kong).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage:www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2021.12.002Received 22 October 2021; Received in revised form 13 December 2021; Accepted 13 December 2021Available online 16 December 20212666-5441/©2021 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/ ).Artiﬁcial Intelligence in Geosciences 2 (2021) 96–106different uses, such as clustering and regression ( Bianco et al., 2020; Jenkins et al., 2021;Mousavi et al., 2019b;Snover et al., 2021;Spurio Mancini et al., 2021). For example,Snover et al. (2021)use convolutional autoencoders to learn latent features from the spectrograms of the ambientseismic noise data from the Long Beach dense seismic array, and then usedeep clustering algorithms to identify the source of the different clusters.Mousavi et al. (2019b)use the features extracted from the autoencoders todiscriminate local waveforms from teleseismic waveforms and determineﬁrst motion polarities. Inspired by these applications, we test whetherencoded features from autoencoders may subsequently be used for appli-cations of seismological interest, such as event discrimination and phasepicking, among other applications. Taking this approach has the followingbeneﬁts compared to training the deep learning models from scratch: ﬁrst, extracting features in this way would streamline the processing pipelineand improve the usage of these deep learning models, because we onlyneed one big generic waveform dataset for the autoencoder model thatlearns the waveform characteristics in an unsupervised way (because itonly reconstructs the waveforms). Second, after training, the model can beused in many different applications where labeled training data are sparse.We only need a small dataset forﬁne tuning to accommodate the new usecases. In a sense, this is a special case of transfer learning ( Bengio, 2012; Shin et al., 2016;Tan et al., 2018), where we train a deep neural networkmodel on a problem with large amounts of data expecting that theextracted features will be transferable to other tasks by ﬁne tuning of newly added layers or previously learned layers. In this case, we use theencoder portion of the deep convolutional autoencoder to transfer thelearned features to different problems.In order to test the effectiveness of extracting and transferring seismicfeatures, we systematically evaluate this method using different datasetsfor three different seismological applications: noise vs. earthquake clas-siﬁcation, P wave arrival picking, and explosion vs. earthquakediscrimination. We tested the use of overcomplete and undercompleteautoencoders, using a different number of feature maps in the mainencoder layers, adding an extra convolutional layer before the fullyconnected layer to make the decision, and training with different ap-proaches to evaluate the performance. By comparing the performance ofthese newly designed models against the baseline models (same struc-ture) trained from scratch, we conclude that the autoencoder featureextractor approach may have advantages in certain circumstances: suchas when the target problems require features that are similar to theautoencoder features, when a relatively small amount of training data isavailable for the target problem, and when certain model structures andtraining strategies are utilized. The model structure that works best in allthese tests is an overcomplete autoencoder with a convolutional layerand a fully connected layer to make the estimation. In almost all the othercases, a model trained speciﬁcally for the problem (a.k.a. baseline model)outperforms the autoencoder-based approaches.2. Methods2.1. OverviewThe idea behind this paper is toﬁrst train an autoencoder on a large number of seismic waveforms to encode features then reconstruct the
Fig. 1.The workﬂow of the experiments. The above solid blue boxe shows the structure of the designed autoencoder. Black text labels on the top of the layersrepresent the overcomplete autoencoder, orange color text labels on the bottom represent the undercomplete autoencoder, and the dotted blue box aro und the input and bottleneck layers contains the encoder. The format of the text in m /C21@n indicates the feature map (or input) is m pixels wide and 1 pixel in height, with n channels (or the number of feature maps). The encoder output (green block) contains the learned features from the so-called bottleneck layer. These l earned features are fed into the application layers containing an optional convolutional network (CNN) that provides another layer of feature extraction (tested wi th and without), a fully connected (FC) layer with 100 neurons is applied to the ﬂattened features, and a sigmoid or ReLU activation function for different applications.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
97signals. The trained encoder should capture the main characteristics ofthe seismic waveforms, and thus can work as a feature extractor. Byconcatenating more convolutional layers and/or fully connected layers(so called application layers) to the encoded layer, the combined modelcan be used in different applications.Fig. 1shows the whole workﬂow of the method. The top big blue solid boxe in Fig. 1illustrates the autoen- coder structures that are utilized here. After training, the encoder portionof the autoencoder (i.e., the layers from the input to the bottleneck layer,blue dotted box) is cut out and combines with the application layers,which contain an optional convolutional layer, a fully connected layer,and a decision layer using a sigmoid or recti ﬁed linear unit (ReLU) for making decisions. To be able to apply this approach to different appli-cations, we used two different training approaches. In the ﬁrst approach, only the application layers at the end of the model were trained, with allthe encoder layers locked. In the second approach, both the applicationlayers as well as the encoder layers were tuned with a much smallerlearning rate than originally used. The following sections will explainthese training approaches in more detail.2.2. AutoencodersAn autoencoder is a neural network that is trained to attempt to copyits input to its output (Goodfellow et al., 2016). It generally comprises two parts, the encoder and decoder. The encoder frequently contains aseries of layers to extract features of the input, and passes these featuresto another series of layers, the decoder, to reconstruct the input. To makeautoencoders useful at learning features and not simply copying the in-puts to the outputs, we can follow two paths. One path is to constrain thelast encoder layer to have smaller total dimensions than the input di-mensions, essentially making a bottleneck in the middle of the wholeautoencoder. For example, inFig. 1, our input dimension is 1620 (540/C2 3), while the bottleneck layer dimensions are 544 (68 /C28), which essentially compresses the data to about one third of the input di-mensions. This type of autoencoder is called an undercomplete autoen-coder, since the bottleneck layer is smaller than the input dimension.Squeezing the dimensions in this way forces the autoencoder to capturethe most useful features of the training data. Another path is to use theovercomplete autoencoders, which extending the bottleneck layer of theencoder to have more dimensions than the input, in our case, 1620 inputversus 8704 (68/C2128) output dimensions, but adding a regularizationterm to force the bottleneck layer to be sparse, thus constraining theautoencoder to learn useful features instead of simply copying the input.We use a L1 regularization term (10e-5) at the bottleneck layer. Con-trastingly, overcomplete autoencoders have been developed becausethey have greater robustness in the presence of noise and have greaterﬂexibility in learning useful features from the data ( Goodfellow et al., 2016).In our models, the encoder layers use 2D convolutional operationswith a kernel size of (3, 1) and strides of (2, 1) to shrink the size of thefeature maps to half in theﬁrst axis. The decoder upscales the size of thefeature maps using transpose 2D convolutional layers with a kernel sizeof (3, 1) and strides of (2, 1) to reconstruct the waveforms to 540 /C23. To train the model, we utilized the Adam optimizer ( Kingma, 2015) with mean absolute error as the loss function and implemented an earlystopping criterion to avoid overﬁtting: training stops if performance didnot improve over 20 epochs.2.3. Application layersAfter training, the encoder (dotted blue box in Fig. 1) can be used as a general feature extractor. By combining the encoder with applicationlayers, we can test whether extracted features are useful for differentproblems. Essentially, the structure of the model used here is similar tosome of the existing models that used the standard convolutional neuralnetwork with shrinking dimensions with deeper layers ( Ross et al., 2018). The differences between them are the training processes, with themodel we designed here using a training approach like transfer learning.We tested two architectures of the application layers: The ﬁrst one has a single fully connected layer where 100 neurons were added to the
encoder and theﬂattened output of the encoder serves as the input. Thesecond one has a convolutional layer with 32 kernels before the fullyconnected layer. If we do not use this CNN layer, the features extracteddirectly from the encoder will be used for making decisions. With theCNN layer added, it provides another mechanism to update and extractthe features to make it more adaptable to the new problems, thusachieving better results as will be shown in the results section. A sigmoidor ReLU activation function was used as the output depending onwhether it is a classiﬁcation or regression problem.2.4. Baseline modelA baseline model refers to a simple or existing model that can serve asa performance reference. In this case, we use a model with the samestructure as the encoder and application layers, but trained from scratchinstead of relying on pre-trained encoder, as our baseline model.2.5. Training setupTwo training schemas were tested: (1) train only the applicationlayers, so the encoder parameters are locked. This is similar to thestandard transfer learning approach (Tan et al., 2018), or a special case of transfer learning that only tunes the last fully connected layers. Thisapproach assumes that the locked layers trained on a large amount ofdata are considered as good feature extractors. In this case, tuning the lastlayer will help the model accommodate the new patterns in the newdataset that the model will be applied on. This approach usually workswell when the features extracted from the locked layers are similar tothose in the new dataset, and it works best if the problems are similar orthe same. (2) After tuning the application layers, we also ﬁne tune the parameters of the encoder layers (i.e. unlock them), using a very smalllearning rate. The features trained on a different dataset or task may be sodifferent from the ones in the new problem that ﬁne-tuning these pre-trained layers with a very small initial learning rate can help improvethe model performance. Some of the chosen training parameters areshown inTable 1.3. DataTo test the three different applications, as well as building theautoencoder, we used data from multiple sources. In this section weinclude a detailed description of the data used in the applications.Example waveforms are shown inFig. 2.3.1. AutoencoderThe data for training the autoencoder comes from the STEAD dataset(Mousavi et al., 2019a), which contains about ~1.2 million local earth-quake waveforms (with P and S arrival labels). We only used the earth-quake waveforms to train the autoencoders. The earthquake waveformswere resampled to 20 samples per second for a total length of 540 (27s)and amplitude normalized from/C01 to 1. 576434, 144109, and 308805waveforms were used for training, validation and testing purposes,respectively. The magnitude, distance and depth distribution are showninFigs. S17–S19(the blue bars), for more details about the dataset,please refer toMousavi et al. (2019a).3.2. Noise vs. earthquakeFor this problem, we take advantage of the LEN_DB dataset ( Magrini et al., 2020), which contains 629,095 three-component earthquakewaveforms generated by 304,878 local events and 615,847 noise wave-forms. The noise records in the LEN_DB dataset were randomly selectedQ. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
98from the period when no earthquake events occurred nearby the stations(seeMagrini et al., 2020for the criterion used). Each waveform issampled at 20 samples per second with a total of 540 data points. Themagnitude, distance and depth distribution are shown in Figs. S20–S22.3.3. Explosion vs. earthquakeThis dataset is assembled from 3 different experiments: SPE (SourcePhysics Experiment) Phase I (Pyle and Walter, 2019), iMUSH/MSH(Imaging Magma Under St. Helens) ( Hansen and Schmandt, 2015), and BASE (The Bighorn Arch Seismic Experiment) ( Wang et al., 2020b). Overall, it has 9728 three-component explosion records and 23,645earthquake records. The SPE Phase I conducted between 2011 and 2016is a series of underground chemical high-explosive detonations in satu-rated granite of various sizes and depths. There were ﬁve borehole shots (M
L1.2–2.1, with explosive yields between about 0.1 and 5 ton TNTequivalent (Snelson et al., 2013). The MSH dataset contains 23 explosivesources (M
L0.9–2.3) and 91 earthquakes (M L1.5–3.3) within 75 km ofTable 1Training parameters of the models with Adam optimizer. We only save the best model based on the monitored metrics with the corresponding mode, the batc h size used is n/100, which is the 1% number of training samples. Validation dataset is 20% of the total training data, shuf ﬂe was used in each Epoch. Early stopping was used as a regularization to avoid overﬁtting, with 20 epochs as the patience parameter, which means if the monitored metric does not improve for 20 epochs, we stop training.Training baseline is the model training from scratch, schema 1 is the model only ﬁne tune the last layers, while schema 2 includes ﬁne tuning encoder layers.
Problem Training Monitor metrics Monitoring Mode Learning rate Optimizer and LossNoise vs EQ LEN_DB baseline Validation accuracy max 0.01 Adam optimizer Sparse Categorical Crossentropy schema1 0.01schema 2 0.00005 Explosion vs EQ Multiple baseline Validation accuracy max 0.01 Adam optimizer Sparse Categorical Crossentropy schema 1 0.01schema 2 0.00005 P phase picking STEAD baseline Validation loss min 0.01 Adam optimizer Mean Absolute Error schema 1 0.01schema 2 0.00005
Fig. 2.Example input waveforms for the three examples. (a) and (b) are used in the earthquake vs. noise problem, where the earthquake waveform in (a) is from aM3.0 earthquake recorded at 36.7 km on station 319A in AE network. (c) and (d) are used in the earthquake vs. explosion problem, where (c) is a waveform fr om a M2.2 earthquake recorded at 58.7 km on station SWF2 in the MSH experiment and (d) is a waveform from a M1.3 explosion recorded on station SM34 at 22.5 km inthe BASE experiment. (e) is a waveform from a M3.7 earthquake recorded by a station 109C in the TA (Transportable Array) network at 75.8 km.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
99the volcano during the 2014–2016 iMUSH project (Hansen and Schmandt, 2015;Wang et al., 2020b). The explosive events are shallowborehole shots and are distributed relatively uniformly in this region. TheBASE experiment (Worthington et al., 2016;Yeck et al., 2014) was conducted in 2010 to image the Bighorn Arch in Wyoming. 21 explosivesources (M
L0.7–1.7, loads 113–907 kg) and 19 earthquakes (M L0.3–2.7) recorded by ~90 broadband stations and ~180 short-period stations arein the dataset. These explosion records have different P/S ratios ( Pyle and Walter, 2019) and local/code magnitudes (Koper et al., 2021) than natural earthquakes. To make the data consistent with the 20 samples persecond (540 data points), weﬁrst low-passﬁltered data at 10 Hz and resampled it to 20 samples per second. For these records, we cut theoriginal explosion waveforms ten times and the earthquake records fourtimes with a start time randomly selected from/C022.5s to the origin of the earthquake to augment the data. A total of 97,280 explosion records and94,580 earthquake records were obtained for training purposes.Figs. S23–S25show the magnitude, distance and depth distribution ofthese records.3.4. P phase pickingFrom the STEAD dataset (Mousavi et al., 2019a), we selected all the earthquake waveforms within 100 km with magnitude larger than M2.0.For each earthquake waveform, weﬁrst low-passﬁltered data at 10 Hz and resampled it to 20 samples per second. Then we cut a window of 540data points (because the raw waveform is longer) with the start timerandomly selected before the P wave arrival. This process returned 168,859 waveforms for training and testing purposes. Note that we also testedwith more data as shown later in the results section. In this test, we useddata with magnitude larger than M1.5 within 200 km, which returned425,552 waveforms. The magnitude, distance and depth distribution areshown inFigs. S17–S19as green and orange bars.4. Results4.1. Autoencoder resultsThe mean absolute error results for the test data and the differentautoencoder architectures are shown in Fig. 3. In thisﬁgure, we can observe four main results: (1) the performance is getting better when themodel has fewer layers (i.e. it has a larger feature map dimension at thebottleneck layer) using the same number of feature maps. Since thebottleneck extracted features are the building blocks of the decoder toreconstruct the waveforms, at the same number of the feature maps, thelarger ones are easier for the model to reconstruct. (2) Models with morelayers extract smaller features, therefore, we need more feature maps toreconstruct the signal, consistent with point 1. As a result, we see betterperformance when we increase the number of feature maps (red line isthe best performance with the lowest errors while the purple line is thepoorest performance). (3) The overcomplete models perform better thanthe undercomplete model, because there are more features that can beextracted in the bottleneck layer. (4) We also notice that when thenumber of feature maps is smaller than 32, such as the blue and purplelines, the pattern of the lines changed compared to the rest of the models(i.e., the direction of the curvature). We think this is due to the smallernumber of feature maps used in the bottleneck layer, which limits thepower of combining these extracted features, but this needs furthertesting to conﬁrm. To compare the performance of the reconstruction ofthe undercomplete and overcomplete models, Fig. 4shows the outputs of the autoencoders compared against the inputs. We can see the matchingof the reconstructed and input waveforms for the overcomplete model isbetter than those from the undercomplete model, which illustrates thefeatures extracted from the overcomplete model captures more of thecharacteristics of the waveforms.4.2. Results for applicationsTo achieve a good balance between performance and computationcost, we selected 128 feature maps for the undercomplete and over-complete models for testing. We tested the models when the bottleneckfeature map dimensions are 68 and 34 with different amounts of trainingdata. For each model-speciﬁc conﬁguration, we ran 5 different training/testing instances varying the initialization of the model weights as well asthe resampling of the training data. The main averaged results for theindividual tasks are summarized in Figs. 5–7
, and discussed in the following paragraphs (for individual test curves with uncertainties,
Fig. 3.The test data performance using mean absolute error for various trained autoencoders. Solid lines are from the overcomplete autoencoders while the d otted line model is from the undercomplete autoencoder. Models with 17, 34, and 68 feature map dimensions in the bottleneck layer were tested. The 2nd row in t he x axis labels is showing the number of layers used in the encoder. Different colors of the lines represent the number of feature maps used in the 3rd and deeper l ayers.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
100please refer toFig. S2S3, S4 and S5 in the supplementary material thataccompanies this paper). We choose to show the results from models withan extra CNN layer in the application layers here, because they performbetter. Results from the models without the CNN layer in the applicationlayers are inFig. S6throughFig. S11.FromFigs. 5–7, we can see some interesting common trends. (1) As
Fig. 4.Waveforms from the autoencoders with bottleneck dimension 68, blue and orange waveforms are the inputs and outputs (reconstructed), respectively. The three rows in each panel are the east-west, north-south and vertical components. (a) Outputs from the undercomplete autoencoder. (b) Outputs from th e overcomplete autoencoder.
Fig. 5.Averaged accuracy for the test data set for binary classi ﬁcation, i.e. noise vs. earthquake, with designed models trained against increasing training data (the corresponding training data percentages are also shown with the maximum number of data used as 100%). The test data size for noise samples is 492,759, w hile the earthquake data size is 503,194. Each data point represents the average of ﬁve training runs with different sampling of training data and new initiation of all weights. Panel (a) compares the results for the undercomplete models, while (b) shows the results for the overcomplete models. Different colors represent dif ferent training method, solid and dotted lines are for bottleneck dimensions 68 and 34. Panels (c) and (d) show the results from the bottleneck dimension point of view, with (c) comparing models with bottleneck feature map dimension 34 and (d) with dimension 68. Different colors represent different training method, solid an d dotted lines are for overcomplete and undercomplete models, respectively. The x axis is in log scale.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
101expected, with more training data the performance of the trained modelsis better. (2) In almost all test cases, training the encoder plus applicationlayers performs poorer than freshly training a model with the samestructure directly using the available training dataset. The only exceptionis when training data is small for the two classi ﬁcation problems, espe- cially when the training dataset is limited to less than 1500 waveforms.This indicates that the features extracted from the autoencoders, thoughgeneric to the waveform itself, may not optimally extract all relevantinformation for speciﬁc applications. When the training dataset is small,the features extracted from the encoders provide additional informationto those extract directly from the apparently undertrained baselinemodel, thus we see better performance. (3) Generally, overcompletemodels outperform the undercomplete models (the green and orangesolid lines work better than the corresponding dotted lines) in all panelsof (c) and (d). Since the bottleneck extracted feature maps are thebuilding blocks for various applications, the models that extract more ofthem (the overcomplete models) are more likely to include features thatare useful to subsequential applications. (4) Overall, the trainingapproach thatﬁne tunes all the layers outperform the approach that onlyupdates the last application dense layers, i.e., the better performance ofthe green lines if compared with the orange lines. This makes sense,becauseﬁne tuning all the encoder layers helps the feature extractionlayers adapt to new data sets and different applications. (5) It is not clearwhether the bottleneck dimension 34 is better than the 68. Though thesolid lines in all panels (a) and (b) are slightly better than the dotted lines,the gaps are small. The next few paragraphs will go over these ﬁguresindividually, and highlight their differences.Fig. 5shows the test results for the noise vs. earthquake classi ﬁcation. First, from panels (a) and (b), we can see that using bottlenecks of 34 or68 has a larger effect on the undercomplete models (the gaps between thesame color solid and dotted lines). Also, when the training data isincreased, the undercomplete models improve little compared to over-complete models. We think this is due to the smaller number of learnedfeature maps in the undercomplete models, which limits their perfor-mance. For panels (c) and (d), we can see that for the overcompletemodels, even when only the application layers are tuned, the perfor-mance can be better than if all the encoder layers are tuned in theundercomplete models. This is additional evidence that the overcompletemodels can learn more features than the undercomplete models. We alsocan see from these panels that the performance improvement initiallygrows faster, but enters into a slow growing area and then into a plateauwhen the training data size is above 15,000.Similarly,Fig. 6summarizes the explosion vs. earthquake classi ﬁca- tion accuracy for the test dataset. Though it is a similar classi ﬁcation problem as noise vs. earthquake, the essential features that distinguishthe two classes are substantially different, with more subtle featuresbetween explosion and earthquake waveforms. In panel (b), the perfor-mances of the overcomplete models are very similar. This indicates thatthe encoders from the overcomplete models did a good job of extractingthe features that can be used to distinguish the earthquake and explo-sions, and thusﬁne tuning all the layers didn't improve the results. In thisapplication, we also do not see the performance plateau as before and we
Fig. 6.Averaged accuracy results on the test dataset for binary classi ﬁcation, i.e. explosion vs. earthquake, with the autoencoder based models trained against different amount of training data (the percentages of the training data are also shown with the maximum number of data used as 100%). The test data sizes for explosion and earthquake are 23,601 and 26,603 respectively. Panels (a) compares the results for the undercomplete models, while (b) shows the resul ts for the overcomplete models. Different colors represent different training methods, solid and dotted lines are for bottleneck dimensions 68 and 34. Panels (c) and (d) show the results from the bottleneck dimension point of view, with (c) comparing models with bottleneck feature map dimension 34 and (d) with dimension 68. Dif ferent colors represent different training method, solid and dotted lines are for overcomplete and undercomplete models. The x axis is in log scale.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
102observe that the accuracy improvement is almost linear in the log scale.Fig. 7shows the test results for the regression problem, i.e., the es-timate of the P wave arrival. The features used in this problem are morelocalized than the previous two examples. We used the standard devia-tion of the errors as a measure of performance. With suf ﬁcient data, the mean of the error distribution approaches zero (see panel d in Figs. S2–S5 in the supplementary material), thus the standard deviation is a goodapproximation of the performance. We can see from Fig. 7(d), the per- formances of the designed encoder plus application layers in the over-complete models with feature map dimension 68 do not exceed theperformance of the baseline models as the previous two examples ofclassiﬁcation when training dataset is small, but the difference is notlarge. InFig. S3(c), we can also see that the shaded area (the standarddeviation of the 5 tests) for the green line has regions lower than the bluebaseline, which means that there are cases among the ﬁve runs that performed better than the baseline model. We can also see that with moretraining data available, the performance of the models with a ﬁne tuning of all the layers are getting closer to the baseline performance, until thereis a constant gap. One interesting thing in panel (c) and (d) of Fig. 7is that, when data sizes are large, the undercomplete models with a ﬁne tuning of all the layers have a comparable performance to that of theovercomplete models, unlike the two classi ﬁcation cases. We attribute this to relevant features being more localized in the P phase pickingproblem and can be captured with fewer feature maps. Therefore, morefeature maps in the overcomplete models do not necessarily improve theresults if compared with the undercomplete models with a ﬁne tuning of all layers.FromFig. 7, we can see the errors still seem to be decreasing. Since wehave more training data in the STEAD, we continued the training withmore data up to 300,000.Fig. 8shows the performance of the models onthe test data with more training data (note, in this case, the x-axis is linearscale to avoid label overlap). As expected, the green line ( ﬁne tune of all layers) and blue line (baseline) inFig. 8shows further improvement, although the improvement rate is smaller compared with training datasize of 100,000. Besides, the gap between the green and blue line con-tinues to decrease.Fig. 9also shows the distributions of estimated errors(predicted time–labeled time) with different training data sizes. We cansee that with more training data available, the performance of the modelswith theﬁne tuning of all the layers are approaching to the baselinemodel. We also see that the performance of the model with a ﬁne tuning of only the last layer improves slowly.We also tested to see if the SNR (signal to noise ratio) will affect theresults, i.e., whether the autoencoder-based model can outperform thebaseline model when SNR is low (or noisier data) using a large amount oftraining data.Fig. S26shows the STD of the absolute errors versus SNRon the test data, and we do not see the autoencoder based model performbetter for noisy data. As shown inFig. 7, when training data is small, the autoencoder based model will perform slightly better than the baselinemodel trained from scratch. This can be seen in Fig. S27as well.
Fig. 7.Averaged standard deviation of the absolute estimation errors for the regression problem, i.e. P arrival estimation, with autoencoder based models trained against different amounts of training data (the percentages of the training data are also shown with the maximum number of data used as 100%). Test data size is 50,658. Panels (a) compares the results from the undercomplete models, while (b) shows the results from the overcomplete models. Different colors re present training methods, solid and dotted lines are for bottleneck dimensions 68 and 34. Panels (c) and (d) show the results from the bottleneck dimension point of view , with (c) comparing models with bottleneck feature map dimension 34 and (d) with dimension 68. Different colors represent training method, solid and dotted li nes are for overcomplete and undercomplete models. The x axis is in log scale.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
1034.3. Computational costFig. 10shows the computational cost of training different types ofmodels using various amounts of training data sizes. Due to trainingprocess takes most of the time, while the test only takes fractions ofseconds to run on a single waveform, we only show the computation costof the training here. Times were measured on two Nvidia Quadro RTX6000 GPUs from the beginning of the training until the model conver-gence (the validation accuracy or loss did not improve for 20 epochs).The timing roughly increases exponentially. When the data sizes arerelatively small, the different methods have similar timing cost (or smalldifferences). In fact, many of the encoder plus the application layermodels converge faster than the baseline model (see Fig. S16in the supplementary material for more details). When data sizes are becominglarger, roughly around 8,000, we start to see that the training times in-crease dramatically, especially for the encoder schema with ﬁne tuning. Overall, as expected, aﬁne tuning of all the layers with a small learningrate takes the longest time to converge, while training the baseline modelconsumes the least time in all these models.5. ConclusionsWe evaluate the idea of using an autoencoder as a generic featureextractor for different seismological applications. The main conclusion isthe autoencoder using the overcomplete model structure with an extraconvolutional layer in the application layer is achieving the best per-formance when trained with theﬁne tuning of all layers. In most cases,the autoencoder approach does not outperform a baseline model that is
Fig. 8.Standard deviation of the absolute error when training with more data for the P wave arrival estimation using STEAD (Magnitude /C211.5 and distance within 200 km). Each dot is the mean value of the ﬁve models trained with different initialization of weights and randomly sampled training data, the shaded areas represent one standard deviation. The x axis is in linear scale.
Fig. 9.P wave arrival time error (Prediction–Label) distribution on test data with different training data sizes, the test data size for each panel is 106,388. Test error distribution with (a) training data size 500; (b) training data size 5000; (c) training data size 25,000; (d) training data size 50,000; (e) training d ata size 100,000; (f) training data size 300,000.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
104tailored to the speciﬁc application. However, the autoencoder-basedmodels can perform slightly better than a baseline model when thetraining dataset is small.Declaration of competing interestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂuence the work reported in this paper.AcknowledgmentsThe views expressed in the article do not necessarily represent theviews of the U.S. Department of Energy or the U.S. Government. Thisresearch was funded by the National Nuclear Security Administration,Defense Nuclear Nonproliferation Research and Development (NNSA DNNR&D). The authors acknowledge the important interdisciplinary collabo-ration with scientists and engineers from Los Alamos National Laboratory(LANL), Lawrence Livermore National Laboratory (LLNL), Mission Supportand Test Services, LLC (MSTS), Paciﬁc Northwest National Laboratory (PNNL), and Sandia National Laboratories (SNL). This research was per-formed in part under the auspices of the U.S. Department of Energy by theLLNL under Contract Number DE-AC52-07NA27344. This is LLNLContribution LLNL-JRNL-828227. We also thank the authors of the data-sets used in this study, including STEAD, LEN_DB, SPE, BASE, and iMush,especially we thank Brandon Schmandt and Ruijia Wang for providing alist of stations for downloading data for SPE ( https://www.nnss.gov/doc s/fact_sheets/NNSS-SPE-U-0034-Rev01.pdf ), BASE (https://www.passc al.nmt.edu/content/bighorn-arch-seismic-experiment-results ), and iMush (http://geoprisms.org/education/report-from-the- ﬁeld/imush-sp ring2015/). We also thank IRIS (https://www.iris.edu/hq/)t oh o s tt h e seismic data for research purposes. We thank useful discussions from BillWalter at the Lawrence Livermore National Laboratory. All the analysesare done in Python and the deep learning framework used here is Ten-sorFlow (Abadi et al., 2016), and seismological related analysis are usingObspy (Beyreuther et al., 2010;Krischer et al., 2015), we thank the awesome Python communities to make everything openly available.Appendix A. Supplementary dataSupplementary data to this article can be found online at https://do i.org/10.1016/j.aiig.2021.12.002.References
Abadi, M., Barham, P., Chen, J., Chen, Z., Davis, A., Dean, J., et al., 2016. TensorFlow: asystem for large-scale machine learning. In: 12th USENIX Symposium on OperatingSystems Design and Implementation (OSDI 16), pp. 265 –283. Retrieved from. https://www.usenix.org/system/ﬁles/conference/osdi16/osdi16-abadi.pdf . Baldi, P., 2012. Autoencoders, unsupervised learning, and deep architectures. In:Proceedings of ICML Workshop on Unsupervised and Transfer Learning, pp. 37 –49. JMLR Workshop and Conference Proceedings. Retrieved from. http://proceedings .mlr.press/v27/baldi12a.html . Bengio, Y., 2012. Deep learning of representations for unsupervised and transfer learning.In: Proceedings of ICML Workshop on Unsupervised and Transfer Learning,pp. 17–36. JMLR Workshop and Conference Proceedings. Retrieved from. http ://proceedings.mlr.press/v27/bengio12a.html . Bergen, K.J., Johnson, P.A., Hoop, M. V. de, Beroza, G.C., 2019. Machine learning fordata-driven discovery in solid Earth geoscience. Science 363 (6433). https://doi.org/ 10.1126/science.aau0323. Beyreuther, M., Barsch, R., Krischer, L., Megies, T., Behr, Y., Wassermann, J., 2010.ObsPy: a Python toolbox for seismology. Seismol Res. Lett. 81 (3), 530 –533.https:// doi.org/10.1785/gssrl.81.3.530 . Bianco, M.J., Gannot, S., Gerstoft, P., 2020. Semi-supervised source localization with deepgenerative modeling. In: 2020 IEEE 30th International Workshop on MachineLearning for Signal Processing (MLSP), pp. 1 –6.https://doi.org/10.1109/ MLSP49062.2020.9231825. Chai, C., Maceira, M., Santos-Villalobos, H.J., Venkatakrishnan, S.V., Schoenball, M.,Zhu, W., et al., 2020. Using a deep neural network and transfer learning to bridgescales for seismic phase picking. Geophys. Res. Lett. 47 (16), e2020GL088651.https://doi.org/10.1029/2020GL088651 . Chen, Y., Zhang, M., Bai, M., Chen, W., 2019. Improving the signal-to-noise ratio ofseismological datasets by unsupervised machine learning. Seismol Res. Lett. 90 (4),1552–1564.https://doi.org/10.1785/0220190028 . Ditthapron, A., Banluesombatkul, N., Ketrat, S., Chuangsuwanich, E., Wilaiprasitporn, T.,2019. Universal joint feature extraction for P300 EEG classi ﬁcation using multi-task autoencoder. IEEE Access 7, 68415–68428.https://doi.org/10.1109/ ACCESS.2019.2919143.Gogna, A., Majumdar, A., 2019. Discriminative autoencoder for feature extraction:application to character recognition. Neural Process. Lett. 49 (3), 1723 –1735. https://doi.org/10.1007/s11063-018-9894-5 . Goodfellow, I., Bengio, Y., Courville, A., 2016. Deep Learning. MITPress. Retrieved from.https://mitpress.mit.edu/books/deep-learning . Graham, K.M., Savage, M.K., Arnold, R., Zal, H.J., Okada, T., Iio, Y., Matsumoto, S., 2020.Spatio-temporal analysis of seismic anisotropy associated with the Cook Strait and
Fig. 10.Total time in seconds for the models to converge (if the validation performance does not improve for 20 epochs, the training process stops), dots are me an values and shaded areas are one standard deviation from the ﬁve runs. The models were trained on 2 Nvidia Quadro RTX 6000 GPUs. Overcomplete models are shown in the top row panels (a), (b) and (c), while undercomplete models are shown in the bottom row panels. Noise vs. earthquake problem is shown in the ﬁrst column, panels (a) and (d). Explosion vs. earthquake problem is shown in the 2nd column, panels (b) and (e). P arrival estimation problem is shown in the last col umn, panels (c) and (f). We used aﬁxed batch size of 256 in all the tests for comparison purposes.Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
105Kaik/C22oura earthquake sequences in New Zealand. Geophys. J. Int. 223 (3),1987–2008.https://doi.org/10.1093/gji/ggaa433 . Hansen, S.M., Schmandt, B., 2015. Automated detection and location of microseismicityat Mount St. Helens with a large-N geophone array. Geophys. Res. Lett. 42 (18),7390–7397.https://doi.org/10.1002/2015GL064848 . Jenkins, W.F., Gerstoft, P., Bianco, M.J., Bromirski, P.D., 2021. Unsupervised deepclustering of seismic data: monitoring the Ross ice shelf, Antarctica. J. Geophys. Res.Solid Earth 126 (9), e2021JB021716. https://doi.org/10.1029/2021JB021716 . Karpatne, A., Ebert-Uphoff, I., Ravela, S., Babaie, H.A., Kumar, V., 2019. Machinelearning for the geosciences: challenges and opportunities. IEEE Trans. Knowl. DataEng. 31 (8), 1544–1554.https://doi.org/10.1109/TKDE.2018.2861006 . Kingma, D.P., 2015. Adam: A Method for Stochastic Optimization. Retrieved from. http:// arxiv.org/abs/1412.6980. Kong, Q., Allen, R.M., Schreier, L., Kwon, Y.-W., 2016. MyShake: a smartphone seismicnetwork for earthquake early warning and beyond. Sci. Adv. 2 (2), e1501055.https://doi.org/10.1126/sciadv.1501055 . Kong, Q., Trugman, D.T., Ross, Z.E., Bianco, M.J., Meade, B.J., Gerstoft, P., 2019. Machinelearning in seismology: turning data into insights. Seismol Res. Lett. 90 (1), 3 –14. https://doi.org/10.1785/0220180259 . Koper, K.D., Holt, M.M., Voyles, J.R., Burlacu, R., Pyle, M.L., Wang, R., Schmandt, B.,2021. Discrimination of small earthquakes and buried single- ﬁred chemical explosions at local distances (<150 km) in the western United States from comparison of local magnitude (ML) and coda duration magnitude (MC). Bull.Seismol. Soc. Am. 111 (1), 558–570.https://doi.org/10.1785/0120200188 . Krischer, L., Megies, T., Barsch, R., Beyreuther, M., Lecocq, T., Caudron, C.,Wassermann, J., 2015. ObsPy: a bridge for seismology into the scienti ﬁc Python ecosystem. Comput. Sci. Discov. 8 (1), 014003. https://doi.org/10.1088/1749-4699/ 8/1/014003.Kunang, Y.N., Nurmaini, S., Stiawan, D., Zarkasi, A., Firdaus, Jasmir, 2018. Automaticfeatures extraction using autoencoder in intrusion detection system. In: 2018International Conference on Electrical Engineering and Computer Science (ICECOS),pp. 219–224.https://doi.org/10.1109/ICECOS.2018.8605181 . Lary, D.J., Alavi, A.H., Gandomi, A.H., Walker, A.L., 2016. Machine learning ingeosciences and remote sensing. Geosci. Front. 7 (1), 3 –10.https://doi.org/10.1016/ j.gsf.2015.07.003.LeCun, Y., Bengio, Y., Hinton, G., 2015. Deep learning. Nature 521 (7553), 436 –444. https://doi.org/10.1038/nature14539 . Li, Z., Meier, M.-A., Hauksson, E., Zhan, Z., Andrews, J., 2018. Machine learning seismicwave discrimination: application to earthquake early warning. Geophys. Res. Lett. 45(10), 4773–4779.https://doi.org/10.1029/2018GL077870 . Linville, L., Pankow, K., Draelos, T., 2019. Deep learning models augment analystdecisions for event discrimination. Geophys. Res. Lett. 46 (7), 3643 –3651.https:// doi.org/10.1029/2018GL081119 . Liu, W., Wang, Z., Liu, X., Zeng, N., Liu, Y., Alsaadi, F., 2017. A survey of deep neuralnetwork architectures and their applications. Neurocomputing 234, 11 –26.https:// doi.org/10.1016/j.neucom.2016.12.038 .
Magrini, F., Jozinovi/C19c, D., Cammarano, F., Michelini, A., Boschi, L., 2020. Localearthquakes detection: A benchmark dataset of 3-component seismograms built on aglobal scale. Artif. Intell. Geosci. 1, 1–10.https://doi.org/10.1016/ j.aiig.2020.04.001.Meier, M.-A., Ross, Z.E., Ramachandran, A., Balakrishna, A., Nair, S., Kundzicz, P., et al.,2019. Reliable real-time seismic signal/noise discrimination with machine learning.J. Geophys. Res. Solid Earth 124 (1), 788 –800.https://doi.org/10.1029/ 2018JB016661.Mousavi, S.M., Sheng, Y., Zhu, W., Beroza, G.C., 2019a. STanford EArthquake dataset(STEAD): a global data set of seismic signals for AI. IEEE Access 7, 179464 –179476. https://doi.org/10.1109/ACCESS.2019.2947848 . Mousavi, S.M., Zhu, W., Ellsworth, W., Beroza, G., 2019b. Unsupervised clustering ofseismic signals using deep convolutional autoencoders. Geosci. Rem. Sens. Lett. IEEE16 (11), 1693–1697.https://doi.org/10.1109/LGRS.2019.2909218 . Mousavi, S.M., Ellsworth, W.L., Zhu, W., Chuang, L.Y., Beroza, G.C., 2020. Earthquaketransformer—an attentive deep-learning model for simultaneous earthquakedetection and phase picking. Nat. Commun. 11 (1), 3952. https://doi.org/10.1038/ s41467-020-17591-w.Park, Y., Mousavi, S.M., Zhu, W., Ellsworth, W.L., Beroza, G.C., 2020. Machine-learning-based analysis of the guy-greenbrier, Arkansas earthquakes: a tale of two sequences.Geophys. Res. Lett. 47 (6), e2020GL087032. https://doi.org/10.1029/ 2020GL087032.Perol, T., Gharbi, M., Denolle, M., 2018. Convolutional neural network for earthquakedetection and location. Sci. Adv. 4 (2), e1700578. https://doi.org/10.1126/ sciadv.1700578.Pyle, M.L., Walter, W.R., 2019. Investigating the effectiveness of P/S amplitude ratios forlocal distance event discrimination. Bull. Seismol. Soc. Am. 109 (3), 1071 –1081. https://doi.org/10.1785/0120180256 . Ross, Z.E., Meier, M., Hauksson, E., Heaton, T.H., 2018. Generalized seismic phasedetection with deep learning. Bull. Seismol. Soc. Am. 108 (5A), 2894 –2901.https:// doi.org/10.1785/0120180080 . Rouet-Leduc, B., Hulbert, C., Lubbers, N., Barros, K., Humphreys, C.J., Johnson, P.A.,2017. Machine learning predicts laboratory earthquakes. Geophys. Res. Lett. 44 (18),9276–9282.https://doi.org/10.1002/2017GL074677 . Saad, O.M., Chen, Y., 2020. Deep denoising autoencoder for seismic random noiseattenuation. Geophysics 85 (4), V367–V376.https://doi.org/10.1190/geo2019- 0468.1.Shin, H.-C., Roth, H.R., Gao, M., Lu, L., Xu, Z., Nogues, I., et al., 2016. Deep convolutionalneural networks for computer-aided detection: CNN architectures, datasetcharacteristics and transfer learning. IEEE Trans. Med. Imag. 35 (5), 1285 –1298. https://doi.org/10.1109/TMI.2016.2528162 . Snelson, C.M., Abbott, R.E., Broome, S.T., Mellors, R.J., Patton, H.J., Sussman, A.J., et al.,2013. Chemical explosion experiments to improve nuclear test monitoring. Eos,Transactions American Geophysical Union 94 (27), 237 –239.https://doi.org/ 10.1002/2013EO270002.Snover, D., Johnson, C.W., Bianco, M.J., Gerstoft, P., 2021. Deep clustering to identifysources of urban seismic noise in Long Beach, California. Seismol Res. Lett. 92 (2A),1011–1022.https://doi.org/10.1785/0220200164 . Spurio Mancini, A., Piras, D., Ferreira, A.M.G., Hobson, M.P., Joachimi, B., 2021.Accelerating Bayesian microseismic event location with deep learning. Solid Earth 12(7), 1683–1705.https://doi.org/10.5194/se-12-1683-2021 . Tan, C., Sun, F., Kong, T., Zhang, W., Yang, C., Liu, C., 2018. A survey on deep transferlearning. In: Kůrkov/C19a, V., Manolopoulos, Y., Hammer, B., Iliadis, L., Maglogiannis, I.(Eds.), Artiﬁcial Neural Networks and Machine Learning –ICANN 2018. Springer International Publishing, Cham, pp. 270–279.https://doi.org/10.1007/978-3-030- 01424-7_27.Tibi, R., Hammond, P., Brogan, R., Young, C.J., Koper, K., 2021. Deep learning denoisingapplied to regional distance seismic data in Utah. Bull. Seismol. Soc. Am. 111 (2),775–790.https://doi.org/10.1785/0120200292 . Wang, R., Schmandt, B., Zhang, M., Glasgow, M., Kiser, E., Rysanek, S., Stairs, R., 2020a.Injection-induced earthquakes on complex fault zones of the raton basin illuminatedby machine-learning phase picker and dense nodal array. Geophys. Res. Lett. 47 (14),e2020GL088168.https://doi.org/10.1029/2020GL088168 . Wang, R., Schmandt, B., Kiser, E., 2020b. Seismic discrimination of controlled explosionsand earthquakes near mount St. Helens using P/S ratios. J. Geophys. Res. Solid Earth125 (10), e2020JB020338.https://doi.org/10.1029/2020JB020338 . Worthington, L.L., Miller, K.C., Erslev, E.A., Anderson, M.L., Chamberlain, K.R.,Sheehan, A.F., et al., 2016. Crustal structure of the Bighorn Mountains region:precambrian inﬂuence on Laramide shortening and uplift in north-central Wyoming.Tectonics 35 (1), 208–236.https://doi.org/10.1002/2015TC003840 . Xing, C., Ma, L., Yang, X., 2015. Stacked denoise autoencoder based feature extractionand classiﬁcation for hyperspectral images. Journal of Sensors. https://doi.org/ 10.1155/2016/3632943, 2016.Yeck, W.L., Sheehan, A.F., Anderson, M.L., Erslev, E.A., Miller, K.C., Siddoway, C.S.,2014. Structure of the Bighorn Mountain region, Wyoming, from teleseismic receiverfunction analysis: implications for the kinematics of Laramide shortening. J. Geophys.Res. Solid Earth 119 (9), 7028–7042.https://doi.org/10.1002/2013JB010769 . Zhou, Y., Yue, H., Kong, Q., Zhou, S., 2019. Hybrid event detection and phase-pickingalgorithm using convolutional and recurrent neural networks. Seismol Res. Lett. 90(3), 1079–1087.https://doi.org/10.1785/0220180319 . Zhu, W., Beroza, G.C., 2019. PhaseNet: a deep-neural-network-based seismic arrival-timepicking method. Geophys. J. Int. 216 (1), 261 –273.https://doi.org/10.1093/gji/ ggy423.Zhu, W., Mousavi, S.M., Beroza, G.C., 2019. Seismic signal denoising and decompositionusing deep neural networks. IEEE Trans. Geosci. Rem. Sens. 57 (11), 9476 –9488. https://doi.org/10.1109/TGRS.2019.2926772 .Q. Kong et al. Artiﬁcial Intelligence in Geosciences 2 (2021) 96 –106
106