ORIGINAL ARTICLE
An optimization approach for thesatisﬁability problem
S. Noureddine
Prince Sultan University, Riyadh, P.O. Box 66833, Saudi Arabia
Available online 9 January 2012
KEYWORDSSatisﬁability;NP-completeproblem;Optimization;GeometricprogrammingAbstract We describe a new approach for solving the satisﬁability problem bygeometric programming. We focus on the theoretical background and givedetails of the algorithmic procedure. The algorithm is provably efﬁcient as geo-metric programming is in essence a polynomial problem. The correctness of thealgorithm is discussed. The version of the satisﬁability problem we study is exactsatisﬁability with only positive variables, which is known to be NP-complete.
ª2012 Production and hosting by Elsevier B.V. on behalf of King Saud University.
1. Introduction
The satisﬁability problem is still the most important open problem in computerscience. Many scientiﬁc disciplines highly depend on efﬁcient solutions of this coreproblem. These span computer science itself, mathematical logic, pure and appliedmathematics, physics, chemistry, economics, and engineering, just to mentionsome. Approximations and heuristics are more than welcome in applications, since
E-mail address:Souﬁane@psu.edu.saPeer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2015) 11,4 7–59
Saudi Computer Society, King Saud University
Applied Computing and Informatics
(http://computer.org.sa)www.ksu.edu.sawww.sciencedirect.com
http://dx.doi.org/10.1016/j.aci.2011.11.0022210-8327ª2012 Production and hosting by Elsevier B.V. on behalf of King Saud University.they oftentimes provide efﬁcient solutions for special cases. Theoretically, how-ever, the computer science community is still lacking a good understanding ofthe computational difﬁculties of this problem though many facts about it arealready known. Complexity theory tells us that the actual problem is related totheP=NP? question. Indeed, this question is now becoming a complexoffered to us by complexity theory. Much effort to solve it has been in vain so far. Regret-tably, no real progress in the discipline of computer science can be made until theproblem is solved. Most (senior) computer scientists today rather believe that Pis not equal toNP. That is, no matter how much we try, the satisﬁability problem, asa prominent NP-complete problem, will persist as a challenging problem withexponential known deterministic worst-case complexity.
It is also disappointing that the NP-complete problems are too many and essen-tial in real applications. It is rather simple to ﬁnd a new NP-complete problem:just try to solve one of them and describe the main difﬁculty you encounter as anew problem if possible. So, adding a new problem to the list is relatively easy,but removing one from the list seems to be very hard. The reason is that the listof NP-complete problems degenerates to the empty list as soon as one of the prob-lems is discovered to be outside the list, that is, as soon as one of the problems isproved to be P-complete. This very fact is the main achievement of the theory ofNP-completeness.
Prevalent approaches for the satisﬁability problem have concentrated so far on:logical manipulation of formulas, graph-theoretic algorithms, probabilistic algo-rithms, and mathematical optimization. Two methods based on logical manipula-tions are worth mentioning: the Davis Putnam (DPLL) procedure and the methodof resolution and reduction. DPLL (Bacchus, 2002) proved extremely efﬁcient formost SAT instances arising in practice. Propositional resolution ( Robinson, 1965) is a well-understood powerful method. Propositional reduction ( Noureddine, submitted for publication) is also powerful and works well in conjunction with res-olution. Graph-theoretic algorithms (Aspvall et al., 1979) are extremely helpful for analyzing special algorithms (e.g. algorithms for 2SAT). Probabilistic algorithms(Schoening, 1999) are also of extreme importance in practice and might deliverthe best results for complex instances. Finally, the method of mathematical opti-mization (Fletcher, 1987) treats the satisﬁability problem as an optimization prob-lem and not as a decision problem. The method is very promising in a real settingthough implementing reliable optimization code is a very challenging task.
We favor in this paper the method of mathematical optimization. In anotherpaper, we focused on formulating the satisﬁability problem as non-convex, exact,exterior, penalty-based problem with a coercive objective function. The methodfocused on exact satisﬁability (XSAT), which is NP-complete. The method fallsinto the category of approximation schemes for solving the satisﬁability problemand is sub-optimal and partially heuristic in nature. In this paper, we treat theproblem by way of geometric programming. We still focus on XSAT or more pre-cisely on 3XSAT, which has the following properties:48 S. Noureddine1. Each clause includes exactly three literals.2. Each satisfying assignment satisﬁes exactly one literal in each clause.3. No variable in the formula is negated, that is, all literals are positive.
The third property simpliﬁes the formulation of the problem via geometric pro-gramming and preserves NP-completeness (Schaefer, 1978).
To the knowledge of the author, geometric programming has not been used toattack the satisﬁability problem yet. Certainly, other optimization schemes arewell-known, but geometric programming theory is very promising in the contextof satisﬁability as we shall see. The method we provide is new and, thus, interestingby its own means. The characteristic feature of geometric programming is that ithelps overcome the non-convex nature of the optimization problem under study.This property is of extreme utility in our setting, since direct optimization methodsfor the satisﬁability problem tend to be non-convex. Knowing that general non-convex optimization problems are (still) not tractable, the method of geometricprogramming offers new perspectives for attacking the problem.
We will need to brieﬂy review the basic theory of geometric programming inSection2. The focus will be on the reﬁned theory based onstrong dualitytheorems as described by the inventors of geometric programming in Dufﬁn et al. (1967). Strong duality optimization is not the standard way of geometric programming.However, we shall see that theweak dualitytheory is not sufﬁcient in our case.Applications of the strong duality theory are rare, if any, in the literature. Thisis why the algorithm we shall outline in Section3is of importance only if the the-ory behind it is proved to be efﬁcient in practice. We shall not focus on experi-menting with the algorithm in this paper. The aim is rather to outline theabstract approach and underpin it by known theoretical results. As any other opti-mization algorithm, the implementation of our algorithm is not straightforward.This is even more complicated in the case of the satisﬁability problem, inasmuchas testing the algorithm for small instances can be very well misleading and is byno means sufﬁcient to draw sound conclusions. Sufﬁcient evidence will be given,however, that the algorithm we present is always of polynomial complexity inworst case and correct in most cases. Section4discusses the approach, its limita-tions, and highlights related to research perspectives.
2. Theoretical background
This section is devoted to the introduction of the needed theory of optimizationand to ﬁxing our notation. Vector quantities are written in bold if not obviousfrom the context (e.g.x). Vector components are indexed accordingly (e.g. x
i). Unless stated differently, we assume throughout the paper continuity and differ-entiability of used functions. This assumption is not restrictive in our contextand is beneﬁcial computationally. An optimization problem ( P) is to ﬁnd the min- imum (or maximum) of a real-valued functionf(x), so-calledobjective function,An optimization approach for the satisﬁability problem 49satisfying a set inequality constraintsg i(x)60 fori2Iand a set of equality con- straintsh
j(x) = 0 forj2E, whereIandEare possibly empty index sets. Formally:
ðPÞminimizefðxÞsubject tog
iðxÞ60 fori2Ih
jðxÞ¼0 forj2E:
Thefeasibility setIðPÞof the problem (P) is the set of vectors satisfying the con-
straints, that is:
IðPÞ¼fx2Rn:giðxÞ608j2Eandh jðxÞ¼08i2Ig:
A solution of (P) is any vectorx*2Rnsuch that:
fðx/C3Þ¼minffðxÞ;x2I g:
In our context, we will focus on a special type of optimization problems, namely,geometric programs. In geometric programs only positive polynomials, so-calledposynomials, are permissible as objective functions. Posynomials have the form:
fðxÞ¼X
k2KukðxÞwhereu
kðxÞ¼c kY
l2Lkxal
l
ckP0:
The termsu k(x) are calledmonomials. As an example, the function deﬁned by:
fðxÞ¼2x21x/C052xe3þx 2þpx/C021x32
is a posynomial and, therefore, permissible as an objective function for ( P). In geo-
metric programs, an inequality constraint has the form g i(x)61 and any such functiong
i(x) must be a posynomial. The equality constraints are of the formh
j(x) = 1 and the functionsh j(x) have to be monomials. Finally, in any geometricprogram all mentioned variablesx
khave to be positive. Thus, a geometric pro-gram (P) has the form:
ðPÞminimizefðxÞsubject tog
iðxÞ618i2Ih
jðxÞ¼18j2Ex
k>08k2f1;...;ngwherefðxÞandg
iðxÞare posynomials and theh jðxÞare monomials:50 S. NoureddineThe original theory of geometric programming lies essentially on the simpleArithmetic-Geometric-Meaninequality (AG). Though other inequalities can alsobe used to develop the same theory, we focus here on the AG inequality due toits simplicity and widespread use. The AG inequality says:(AG)
Let 2nnumbersx i> 0 andd i> 0 fori2{1,...,n} be given with:P
ni¼1di¼1
Then:Qni¼1xdi
i6Pni¼1dixi
with equalityiﬀ x 1=/C1/C1/C1=x n.
A related inequality (necessary for the constrained case) is the General Arithmetic- Geometric-Meaninequality (GAG). The GAG inequality is:
(GAG) Let 2nnumbersx i> 0 andd ifori2{1,...,n}, where thed iare all
positive or all zero, be given with:P
ni¼1di¼k
Then:k
kQni¼1 xi
di/C16/C17di/C18/C196Pni¼1xi/C0/C1k
with 00¼ðx i=0Þ0/C171 and equalityiﬀd 1¼/C1/C1/C1¼d n¼0o rx i¼di
kPnj¼1xj.
With the two inequalities in hand, a duality theory can be easily developed. Theprocess issketched(see e.g.Dufﬁn et al., 1967; Peressini et al., 1988for more details) in the proof of the following main theorem of geometric programming.
Theorem 2.1.For every geometric program (P) with E¼£, calledprimal program, there exists adualprogram (D) of the form:
ðDÞmaximizevðdÞ¼Yni¼1ci
di/C16/C17di !Y
i2IkiðdÞkiðdÞ
subject toX
k2I0dk¼1ðaÞX
k2Daijdk¼08j2f1;...;ngðbÞd
kP08k2Dwherek
iðdÞ¼X
k2Iidk8i2IandD¼[ kP0Ik:
Moreover, f(x)Pv(d) for any two feasible vectorsx2I ðPÞandd2I ðDÞ.hAn optimization approach for the satisﬁability problem 51Proof.First, the requirementE¼£can be omitted. We will focus on inequalityconstraints only. The index setsI
kfollow from the structure of the used posyno-mials. We will prove the theorem in the case of a single inequality constraintg
1(x)61. The case of multiple constraints follows in the same manner. By(AG), we have:
fðxÞPYn0
i¼1ci
di/C18/C19di !Y
i2I0xLi
i:ð2:1Þ
On the other hand, a lower bound for the function g 1(x)k(fork> 0) is obtained
by (GAG):
g1ðxÞkPkkYn1
i¼n0þ1ci
di/C18/C19di !Y
i2I1xLi
i:ð2:2Þ
Here, the index setsI 0andI 1capture the indexes of the different variablex imen-
tioned inf(x) andg 1(x), respectively, and eachL iis a linear combination of theform (a).Sinceg
1(x)61, it follows that:
fðxÞPfðxÞg1ðxÞkPYn0
i¼1ci
di/C18/C19di !Y
i2I0xLi
ikkYn1
i¼n0þ1ci
di/C18/C19di !Y
i2I1xLi
i:ð2:3Þ
By requiring that eachL i= 0 as in (a), the right-hand side of(2.3)becomes a func-
tion of the variablesd k. Calling this functionv(d),(2.3)becomes:
fðxÞPvðdÞ:
Obviously, the above derivation is valid only if xanddare feasible as
required.h
The above theorem is the corner stone of the computational procedure for solv-ing geometric problems. The main facts in this respect are summarized in the fol-lowing theorem, which we state without proof (see e.g. Dufﬁn et al., 1967).
Theorem 2.2
(i)The logarithm of dual objective function ln(v(d)) is concave.(ii)If the interior ofIðPÞis not empty and (P) has a (ﬁnite) solutionx
*, then (D) also has a solutiond
*. Moreover:
fðx/C3Þ¼vðd/C3Þ:
h52 S. NoureddineIn other words, fact (i) says (as ln(x) is monotone-increasing) that the dual pro-gram (D) is efﬁciently solvable, since it is in essence a (simple) convex program.Polynomial algorithms for convex programs are known (e.g. interior point meth-ods (Forsgren et al., 2002)). Fact (ii) is saying that once we have solved the dualproblem (D), the primal problem (P) is practically solved, too. Although only theminimum off(x
*) is determined by solving the dual problem, a method for deter-mining a minimizerx
*is known to be equivalent to a linear program and, there-fore, the problem of determiningx
*is solvable in polynomial time.
The theory outlined above is calledweak duality theory. It is weak in the sense that fact (ii) requires the feasible setIðPÞto have a non-empty interior. Such pro-grams are calledsuper-consistent. On the other hand, ifIðPÞis non-empty the pro- gram is calledconsistent. However, under further mild requirements, the super-consistency assumption can be omitted. This leads to the so-called strong duality theory. The following theorem is the main theorem of strong duality.
Theorem 2.3.If the primal program (P) is consistent, then its dual (D) is consistentthe its inﬁmum, if it exists, coincides with the supremum of (D).h
This theorem relaxes the requirement of super-consistency: it only requires thatthe primal is consistent. The price of this relaxation is reﬂected in the conclusionthat just the primal inﬁmum and the dual supremum coincide (if existent). Com-pared withTheorem 2.2this is a weaker conclusion, since there the primal mini-mum and dual maximum coincide if they exist. However, Theorem 2.3is only theoretically weaker. In practice, the conclusion is sufﬁcient for devising an algo-rithmic procedure to approximate the minimum of the primal problem and/or themaximum of the dual problem.
Theorem 2.3opens a new way for geometric programming if used effectively.The idea we now introduce shall try to attack the open problem of solving geomet- ric programs with equality constraints for posynomials . So far, we have allowed equality constraints for monomials only. This requirement is related to the efﬁ-cient solvability of the problem via convex techniques. Theorem 2.3tells us, how- ever, that consistency of the primal program is sufﬁcient for the mentionedinﬁmum–supremum correspondence if a suitable dual program is deﬁned. As amatter of fact, we are facing the question: Is there a dual program for geometricprograms with posynomial equality constraints? The answer to this question is yesas seen in the following theorem.
Theorem 2.4.Let (P) be a primal program with posynomial equality constraints.Then (D) as deﬁned above is the dual program of (P). Moreover, Theorem2.1 applies for (P) and (D), in particular for feasible xanddwe have: f(x)Pv(d).hAn optimization approach for the satisﬁability problem 53Proof.The proof ofTheorem 2.1can be reused here without modiﬁcation forposynomial inequality constraints. For posynomial equality constraints, we justpoint out that line(2.3)of that proof should include the equality symbol insteadof the leftmost inequality symbol. However, this does not impair the validity ofother inequalities. Thus, the main conclusion of the theorem, namely, thatf(x)Pv(d) for suitablexandd, remains true here, too.h
The following theorem connects the preceding theorem to the task of ﬁnding theinﬁmum of the primal program.
Theorem 2.5.Let (P) be a consistent primal program with posynomial equalityconstraints and (D) be its dual program. Then (D) is consistent and the inﬁmum of(P) coincides with supremum of (D).h
Proof.Since perTheorem 2.4the dual program (D) is well-deﬁned,Theorem 2.3 can be used to infer the inﬁmum–supremum correspondence as required.h
Observe that the requirement of consistency is crucial in Theorem 2.5. Just super-consistency would not sufﬁce here, since the primal program is allowed tohave posynomial equalities.
Corollary 2.6.Let (P) be as inTheorem2.5, then its inﬁmum coincides with theconstrained maximum of (D).
Proof.As (D) is equivalent to a convex program, its supremum is actually its max-imum, whence the desired proposition.h
The last claim is of no theoretical importance but useful in practice. It helps dealwith the maximization algorithm of the dual (concave) program in the classical(efﬁcient) way.
One remark is here in order. Paradoxically, the preceding theorems show thatequality constraints are dealt with as inequality constraints if only the primal isconsistent and the optimization is changed so as to ﬁnd the inﬁmum instead ofthe minimum. This is so, sinceTheorem 2.4shows that the same lower bound isused for equality and inequality constraints. This is rather strange, as the dualprogram would lose information about the constraint types of the primal pro-gram. This is why the new introduced theorems are rather in the status of con-junctures until more theoretical and/or experimental evidence about them isdemonstrated. One way of accepting this paradox, however, is to imagine thatproving consistency of the primal program with posynomial equalities is byitself a serious difﬁculty in practice, so as to make the conclusions of the lasttheorems of theoretical interest only save perhaps for special (non-)interestingcases.54 S. Noureddine3. The algorithm
We ﬁnally come back to the original problem of the paper, the satisﬁability prob-lem. In the last section, we proved that equality constraints act theoretically likeinequality constraints, if the original primal program is consistent. Algorithmsfor handling equality constraints are known. Actually, one of the inventors of geo-metric programming, Dufﬁn pointed out (Dufﬁn, 1970) that the method ofposy- nomial condensationcould be of great use in practice to approximate geometricprograms by linear ones. This method is useful in our setting, too. The idea isto replace any posynomial equality constraint, e.g.g(x) = 1, by the (in)equalities:
g0ðxÞ¼1;
whereg0(x) is the monomial obtained by direct use of (AG) for g(x) i.e.: g
0(x)6g(x).
It is clear that the method of condensation only approximates the original prob-lem. However, good sub-optimal solutions can be achieved by this method.
The method of condensation does not rely on the strong duality theory. It cir-cumvents the difﬁculty of equalities by direct approximation. We will not use it inthis paper. We rather intend to apply the strong duality results of the last section.However, to really beneﬁt from these results, we need to require that the consis-tency problem is (efﬁciently) solvable by some means. Though this requirementseems to be unachievable in general, there are special cases where it is easilyachieved. A noteworthy case is when all constraints are posynomial equality con-straints where each term is a simple term (i.e. consists of a single variable with 1 asexponent). We call such a function alinear posynomial. In this case, the consis- tency problem boils down to a linear programming problem, which is efﬁcientlysolvable by conventional techniques. With the help of the strong duality theory,an efﬁcient method for minimization is also at hand. Though, this scheme onlydelivers inﬁmum information, in applications this information does not differmuch from minimum information, as optimization algorithms are approximativein nature. This is the way we are going to go for solving the satisﬁability problem.
Formally, let the primal optimization problem (P) to be solved be of the form:
ðPÞminimizefðxÞsubject tog
iðxÞ¼18i2Ex
k>08k2f1;...;ngwherefðxÞis a posynomial and theg
iðxÞare linear posynomials:
An algorithm for ﬁnding the inﬁmum of the problem is as follows:An optimization approach for the satisﬁability problem 55Algorithm 3.1
Input : ProblemðPÞOutput : Infimum ofðPÞif it exists:
Step1:Consistency CheckSolve the linear programming problem (LP):
giðxÞ¼18i2Ex
k>08k21;...;n:If (LP) is not solvable then (P) is not solvable; stop.
Step 2:Solve the consistent (P) by its dual (D)GetM: = constrained maximum of (D);ReturnM;
Claim 3.1.The algorithm is of polynomial complexity.h
Proof.Step 1 is essentially a perturbed linear programming problem (due to theexistence of strict inequalities). So, it is solvable in polynomial time. Step 2 is aconvex problem, which is harder to solve, but is still of polynomialcomplexity.h
Claim 3.2.The algorithm is correct.h
Proof.The correctness of the algorithm relies directly on the results of Section 2. We need to emphasize, however, that only the inﬁmum is approximated by themethod and to refer to the remarks at the end of last section.h
The presented algorithm is of use for the exact satisﬁability problem (XSAT), ifwe can convert (XSAT) into an optimization problem of the form ( P). WithClaim 3.1, we are guaranteed to have a polynomial-time algorithm then. In view of theremarks of the last section,Claim 3.2is still to be justiﬁed experimentally. Inany case, a conversion of (XSAT) to (P) is of great importance in our setting.To this end, we ﬁrst recall that our XSAT instances are required to be positive(i.e. without negated literals). LetC
1=x 1+x 2+x 3be a clause of such an XSAT formulaF. We ﬁrst notice that the following system of equations:
x1þx 2þx 3¼1x
1x2¼0x
1x3¼0x
2x3¼08>>><>>>:56 S. NoureddinehasS= {(1,0,0),(0,1,0),(0,0,1)} as a solution set. Remarkably, this is exactly thesolution set of a permissible 3XSAT clause in a satisﬁable formula F. We might better see the previous system of equations as an optimization problem ( P
C1) for clauseC
1, where (P C1) is:
Minimizex 1x2þx 1x3þx 2x3
subject tox
1þx 2þx 3¼1x
1P0x
2P0x
3P0:8>>>>>>>><>>>>>>>>:
Let the above objective function of clauseC 1be calledf 1(x) and the equality
constraint function be calledg 1(x). Obviously, the same procedure can be usedfor any clauseC
i(1 </C0i6m) if the formulaFincludesmclauses, and for each clauseC
ifunctionsf i(x) andg i(x) can be deﬁned accordingly. To solve the XSATproblem, we need to combine the equations of the different programs ( P
Ci)’s. This yields to the following formulation of the XSAT problem as an optimizationproblem
ðXSATÞminimizeXmi¼1
fiðxÞsubject tog
iðxÞ¼18i2f1;...;mgx
kP08k2f1;...;ng:
By construction off i(x) andg i(x), we evidently see that these are posynomials
and linear posynomials, respectively. Also, the positivity constraints of the vari-ables can be easily perturbed to strict positivity constraints. Thus, in fact, we needto solve (XSAT) for positivex
konly. But now all assumptions of aforementionedalgorithm are satisﬁed, whence the following claim.
Claim 3.3.The optimization problem (XSAT) with strict positivity constraints issolvable in polynomial time.h
We emphasize that by ‘‘solvable’’ we mean that the solution can be approximatedto any desired degree of accuracy. This so because our algorithm delivers the inf-imum and (XSAT) needs to be perturbed numerically.
Claim 3.4.The 3XSAT with positive literals is solvable in polynomial time.h
Proof.A procedure for solving positive XSAT is straightforward:An optimization approach for the satisﬁability problem 571. Solve the corresponding geometric programming problem (XSAT)
byAlgorithm 3.1and get its inﬁmumM.2.IfM/C250then
printsatisﬁableelseprintunsatisﬁable.
h
The previous procedure is a decision procedure. To obtain a solution vector,bisection in variables’ vector space is the immediate approach. Thus, if Fis satis- ﬁable, the procedure in the previous proof needs to be called O( n) times, wherenis the number of variables mentioned inF. We cannot rely here on the approach ofgeometric programming for determining minimizers (via linear programming),since this method relies on the super-consistency assumption of the problem tobe minimized, which is not allowed in our setting.
4. Conclusion
This paper has two main contributions:
We revived the theory of strong duality in geometric programming. We easilyextended the theory to handle posynomial equality constraints. At this seemedto be only of theoretical interest, but we argued that in special cases (linear posy-nomials) the theory extremely useful in practice and we outlined a polynomial-time algorithm for solving this sort of problems.
The second main contribution is related to satisﬁability research. We proposeda new format of the (positive) 3XSAT problem as a geometric program. We thenproved that our format adheres to the requirement of linear posynomial equalities.Thus, we were able to apply the proposed optimization algorithm for XSAT.
There is no doubt that bothAlgorithm 3.1and the procedure outlined in theproof ofClaim 3.4are polynomial-time algorithms. However, the issue that stillneeds further investigation is the (practical) correctness of these procedures. Weintend to verify correctness experimentally. The main drawback of this approachis that faults in the implementation of mentioned optimization algorithms inevita-bly lead to erroneous conclusions. The paper shows, however, that the theorybehind the algorithms is sound and that it predicts polynomial-time performance(with exact upper bounds). Despite this fact, a warning is here in order. Experiencewith other optimization methods has shown that theoretical investigation is notthe whole story in this domain. One need only consider the Ellipsoid Method ofoptimization, which is provably of polynomial-time complexity, and which, how-ever, has poor performance in practice compared to the theoretically inferior Sim-plex Method with its exponential worst-case complexity.
We ﬁnally want to point out that the optimization problem for XSAT asdeﬁned in this paper calls for another totally different way of solution. We could,58 S. Noureddinenamely, try to attack the problem via quasi-convex programming. Quasi-convex-ity of constraint functions is sufﬁcient for minimization algorithms based on theKarush–Kuhn–Tucker theory and may be implemented efﬁciently, if the objectivefunction is convex or at least pseudo-convex. The performance and reliability (i.e.performability) of this approach will be part of our future research.
References
Aspvall, B. et al., 1979. A linear-time algorithm for testing the truth of certain quantiﬁed Boolean formulas.Information Processing Letters 8 (3), 121–123
.
Bacchus, F., 2002. Enhancing Davis Putnam with extended binary clause reasoning. In: 18th NationalConference on Artiﬁcial Intelligence.
Dufﬁn, R.J., 1970. Linearizing geometric programs. SIAM Review 12 (2), 211–227 .
Dufﬁn, R.J. et al., 1967. Geometric Programming: Theory and Applications. Wiley, New York .
Fletcher, S.R., 1987. Practical Methods of Optimization, second ed. Wiley .
Forsgren, A. et al., 2002. Interior methods for nonlinear optimization. SIAM Review 44 (4), 925–997 .
Noureddine, S., submitted for publication. An approach for the satisﬁability problem via exterior penaltyoptimization, Journal of Computer Science.
Peressini, A.L. et al., 1988. The Mathematics of Nonlinear Programming. Springer-Verlag, New York .
Robinson, J.A., 1965. A machine-oriented logic based on the resolution principle. Journal of the Association forComputing Machinery 12, 23–41
.
Schaefer, T.J., 1978. The complexity of satisﬁability problems. In: Proceedings of the 10th Annual ACMSymposium on Theory of Computing, San Diego, California, USA, pp. 216–226.
Schoening, U., 1999. A probabilistic algorithm for k-SAT and constraint satisfaction problems. In: Proceedingsof the 40th Annual IEEE Symposium on Foundations of Computer Science, FOCS’99, pp. 410–414.An optimization approach for the satisﬁability problem 59