Retrieval ofﬂower videos based on a query with multiple speciesofﬂowers
V.K. Jyothia,⁎, V.N. Manjunath Aradhyab, Y.H. Sharath Kumarc, D.S. Gurua
aDepartment of Studies in Computer Science, Manasagangothri, University of Mysore, Mysore 570 006, Karnataka, India
bDepartment of Computer Applications, JSS Science and technology University, Mysore, Karnataka, India
cDepartment of Information Science and Engineering, Maharaja Institute of Technology Mysore (MITM), Manday 571438, Karnataka, India
abstract article info
Article history:Received 25 August 2020Received in revised form 6 November 2021Accepted 6 November 2021Available online 14 November 2021Searching, recognizing and retrieving a video of interest from a large collection of a video data is an instantaneousrequirement. This requirement has been recognized as an active area of research in computer vision, machinelearning and pattern recognition. Flower video recognition and retrieval is vital in the ﬁeld ofﬂoriculture and hor- ticulture. In this paper we propose a model for the retrieval of videos of ﬂowers. Initially, videos are represented with keyframes andﬂowers in keyframes are segmented from their background. Then, the model is analysed byfeatures extracted fromﬂower regions of the keyframe. A Linear Discriminant Analysis (LDA) is adapted for theextraction of discriminating features. Multiclass Support Vector Machine (MSVM) classi ﬁer is applied to identify the class of the query video. Experiments have been conducted on relatively large dataset of our own, consistingof 7788 videos of 30 different species of ﬂowers captured from three different devices. Generally, retrieval of ﬂower videos is addressed by the use of a query video consisting of a ﬂower of a single species. In this work we made an attempt to develop a system consisting of retrieval of similar videos for a query video consistingofﬂowers of different species.© 2021 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Flower region of interest (FRoI)Linear discriminant analysis (LDA)Retrieval ofﬂower videosMulticlass support vector machine
Contents1 . I n t r o d u c t i o n ............................................................... 2 6 32 . R e l a t e d w o r k s .............................................................. 2 6 32 . 1 . P r e v i o u s w o r k .......................................................... 2 6 32 . 2 . C o n t r i b u t i o n s o f t h e p r o p o s e d w o r k ................................................. 2 6 33 . P r o p o s e d w o r k ............................................................. 2 6 43 . 1 . P r e p r o c e s s i n g ........................................................... 2 6 43 . 1 . 1 . G a u s s i a n M i x t u r e m o d e l ( G M M ) .............................................. 2 6 43.1.2. Extraction ofﬂo w e r r e g i o n o f i n t e r e s t ( F R o I ) ......................................... 2 6 53 . 2 . E x t r a c t i o n o f f e a t u r e s....................................................... 2 6 53 . 2 . 1 . T e x t u r e f e a t u r e s ..................................................... 2 6 53 . 2 . 2 . S c a l e i n v a r i a n t f e a t u r e t r a n s f o r m ( S I F T ) ........................................... 2 6 53 . 2 . 3 . E n t i r e k e y f r a m e ...................................................... 2 6 73.2.4. Allﬂo w e r r e g i o n s o f i n t e r e s t ................................................ 2 6 73.2.5. Maximumﬂo w e r r e g i o n o f i n t e r e s t ............................................. 2 6 73 . 2 . 6 . L i n e a r d i s c r i m i n a n t a n a l y s i s ( L D A ) ............................................. 2 7 13 . 3 . R e t r i e v a l : q u e r y c l a i m i n g i d e n t i t y o f c l a s s .............................................. 2 7 34 . E x p e r i m e n t s a n d r e s u l t s......................................................... 2 7 34 . 1 . D a t a s e t s............................................................. 2 7 34 . 2 . A l l F R o I ' s ............................................................. 2 7 44 . 3 . M a x . F R o I ............................................................. 2 7 4Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
⁎Corresponding author.E-mail addresses:jyothivk.mca@gmail.com(V.K. Jyothi),aradhya@sjce.ac.in(V.N.M. Aradhya),dsg@compsci.uni-mysore.ac.in (D.S. Guru).
https://doi.org/10.1016/j.aiia.2021.11.0012589-7217/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/4 . 4 . M a x . F R o I w i t h L D A ........................................................ 2 7 54 . 5 . C o m p a r a t i v e s t u d y b e t w e e n p r o p o s e d w o r k a n d p r e v i o u s w o r k .................................... 2 7 54 . 6 . R e s u l t a n a l y s i s a n d d i s c u s s i o n................................................... 2 7 54.7. Query with multiple classﬂo w e r s i n v i d e o s............................................. 2 7 6 5 . C o m p a r a t i v e s t u d y b e t w e e n p r o p o s e d w o r k a n d d e e p l e a r n i n g m o d e l ..................................... 2 7 66 . F u t u r e w o r k ............................................................... 2 7 67 . C o n c l u s i o n............................................................... 2 7 7R e f e r e n c e s .................................................................. 2 7 7
1. IntroductionDue to the ease of availability of recent video capturing devices suchas cameras, mobiles, storage media, users can easily capture and store alarge number of videos. Video contains a large information than images.A single video can capture the reality better than thousands of images.Recently, video databases have become much larger, hence there isneed for automatic analysis and retrieval system with the minimum in-tervention of human is essentially required. Video has become a signif-icant element of communication environment. Users can search andshare desired videos due to networking technology, which has madedeveloping an automated system to search and retrieve videos. And itis an interesting and active research ( Shen et al., 2016). Videos are cat- egorized into different domains for example sports, news, surveillance,commercials, medical etc., again domain speci ﬁc videos are categorized into different subcategories/classes (Geetha et al., 2009). Data acquisition tools with recent technological advancementsallowed researchers/scientists to acquire data from different applicationdomains in the form of images and videos, these are a large and complexin nature (Mufti et al., 2021). One of the important aspects of organic lifeis its outstanding diversity. There exists a very large number of speciesofﬂowers in the world and the estimation of ﬂower species ranges be- tween 2,20,000 and 4,20,000 (Chaitra et al., 2021). Specialized knowl- edge is required to recognize the taxonomic information of ﬂowers. Plant Identiﬁcation skills and Taxonomic knowledge is restricted to alimited number of individuals (Jyothi et al., 2018,Wäldchen et al., 2018). To address the taxonomist'sﬂower species identiﬁcation re- quirement, a signiﬁcant amount of research work has been carried outin theﬁeld of Artiﬁcial Intelligence and Video/Image Processing for au-tomaticﬂower recognition and retrieval.Developing aﬂower video retrieval system is a domain speci ﬁc with many applications. It is an application in the ﬁeld ofﬂoriculture for com- mercial trades. Floriculture is one of the important commercial trades inagriculture (Guru et al., 2010). Day to day there is an increase in the de-mand forﬂowers. Floriculture involves nursery,ﬂower trade, seed pro- duction fromﬂowers (Guru et al., 2011). Further, it is found useful in horticulture, interest in knowing theﬂower names for decoration, cos- metics and medicinal use etc., (Das et al., 1999a, 1999b). Due to the de- velopment of technology in business, trader can store a large volume ofvideos. Instead of visiting the nurseries for their desired ﬂowers, users can analyse the entireﬂower before purchasing it and its seeds. Also,they can view different species of
ﬂowers along with different variantsavailable in each species. Further itﬁnds applications such as medicinal,cosmetics, industrial use for the extraction of oils from ﬂowers and dec- oration etc., (Das et al., 1999a, 1999b). In such cases, it is essential to de-velop an automated system to search and retrieve videos of ﬂowers of user's interest. Therefore, the proposed research motivates to designan automated system for the retrieval of users desired videos of ﬂowers. The challenges involved inﬂower videos to design a retrieval system areillumination: light variations differ from different angles and varied sea-sonal time; variation in viewpoint: videos with varying viewpoint ofﬂowers changes appearance of theﬂower in size, shape, pose and rota-tion; cluttered background, variation among intra class and inter class,multiple instances ofﬂowers in videos etc. To design a video retrievalsystem, two main prominent methods are used to increase retrievalperformance. First is toﬁnd more appropriate features to describevideos and second is an appropriate dimensionality reduction methodfor selecting most discriminative features.2. Related worksGenerally, the video retrieval system retrieves similar videos basedon query by example. An example may be an image, keywords, sketch,object, video, video frame etc., (Hu et al., 2011). In the literature we found retrieval of videos based on an object ( Morand et al., 2010), frame (Shekar et al., 2016), video (Geetha et al., 2009;Gao et al., 2009;Han et al., 2014;Liang et al., 2012), keywords (Priya and Domnic, 2014). For the retrieval of videos the features and algorithmssuch as opticalﬂow tensor and Hidden Markov Models (HMMs) ( Gao et al., 2009), the multi-modal spectral clustering and ranking algorithm(Han et al., 2014), block wise intensity comparison (Geetha et al., 2009), Scale Invariant Feature Transform (SIFT) ( Zhu et al., 2016), Bag-of- Features (Cui et al., 2016), dynamic weighted similarity measure withcolor and edge descriptors (Liang et al., 2012) are used. When a set of features are used to represent of a video, then the dimension of featuresmay be high. If the dimension of the feature vector is high, the video re-trieval system consumes more computational time. It can be reducedwith the feature dimensionality reduction techniques. The dimensional-ity reduction techniques such as Principal Component Analysis (PCA)(Geetha et al., 2009), Fisher Discriminant Ratio (Shen et al., 2016), Lin- ear Discriminant Analysis (Gao et al., 2009), semi-supervised linear dis- criminant analysis (Wang et al., 2016), supervised linear dimensionalityreduction (Cui et al., 2016), nonparametric discriminant analysis ( Khanet al., 2012) are utilized to reduce the feature dimension in other videoretrieval systems.2.1. Previous workIn proposed work, to design aﬂower video retrieval system the fea-tures of previous work (Guru et al., 2018a, 2018b)s u c ha sG L C M (Haralick et al., 1973), LBP (Ojala et al., 2002) and SIFTLowe (2004) are utilized. Instead of extracting features from entire keyframe, fea-tures are extracted in two different modes from each keyframe of thevideo. Initially, from all Flower Region of Interest (FRoI), secondly,from maximum Flower Region of Interest (Max.FRoI). A dimensionalityreduction method is introduced for the features extracted from Max.FRoI, to improve the performance of the system with greater extent,which leads the fast accessing of videos. In the previous work ( Guru et al., 2018a, 2018b) the query video consists of a single class of ﬂowers. In the present work along with single class of ﬂower videos query video also consists of multiclassﬂowers. The dataset considered in the presentwork is relatively large. The comparative study is made with previouswork to show the effectiveness of the proposed work.2.2. Contributions of the proposed workThe contributions are summarized as follows.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
263a) Creation of a reasonably large dataset of videos of ﬂowers which shall be made available public for research purpose.b) Proposal of fusion of features strategy to improve the performance ofthe existing model.c) Proposal of an algorithmic model for the retrieval of videos ofﬂowers using allﬂower regions of interest.d) Proposal of a model for the retrieval of videos of ﬂowers with maxi- mumﬂower regions of intereste) Adoption of a dimensionality reduction approach to improve the ef-ﬁciency of the system.f) Addressed retrieval of videos ofﬂowers even when a query videocontainsﬂowers of more than one class.g) Compared the proposed model with earlier proposed model and adeep learning model.3. Proposed workThe proposed model comprises three stages namely, preprocessing,extraction of features and retrieval. The block diagram of the proposedﬂower video retrieval system using Flower Region of Interest (FRoI) isas shown inFig. 1.( S e eTable 1.)3.1. PreprocessingThe preprocessing stage involves the processes such as selection ofkeyframes, segmentation and extraction of ﬂower region of Interest (FRoI). The proposed system initially converts video to frames. Supposethat theﬂower video dataset‘X'consists of‘vn’number of samples and it is stated asX¼x
v1,xv2,xv3,...,x vi,...,x vn fg ð1ÞLet theﬂower videox
viconsists of aﬁnite set of‘F N’number of frames and it is deﬁned asx
vi¼F 1,F2,F3,...,F i,...,F N fg ð2ÞThen the keyframes of the videoxviare selected using GMM cluster based algorithmic model (Guru et al., 2018a, 2018b). Here, Block wise entropy feature is extracted from each frame of the video and similarframes are grouped together using Gaussian Mixture Model and theframe near to each cluster centroid are selected as keyframes of thevideo. GMM is explained in section 3.1.1. When the set of keyframesare selected fromx
vi, then the videox viis represented as‘K y’number of keyframes and is deﬁned as,K
y¼k 1,k2,k3,...,k i,...,k y/C8/C9 ð3ÞTheﬂowers in keyframes are segmented from their backgroundusing statistical region merging algorithm ( Nock and Nielsen, 2004). The keyframes after segmentation can be de ﬁned asSK
y¼sk 1,sk2,sk3,...,sk i,...,sk y/C8/C9 ð4Þ3.1.1. Gaussian Mixture model (GMM)Gaussian Mixture model (GMM) is a statistical and unsupervisedlearning model. GMM (Stauffer and Grimson, 1999), preserves content of the scene, the idea behind GMM is to describe pixels, some ofwhich represent the background while the others represent the fore-ground in the scene. Aﬁnite number of mixtures of Gaussian distribu-tions are used to generate data points. It preserves the sub-samplingproperty; it leads for clustering data points. The GMM parameters areestimated from data using the maximum expectation algorithm. AGMM is a weighted sum of several Gaussian densities. Therefore, inthe present work to create clusters GMM is used for the selection ofkeyframes. Clusters are created byﬁtting the Gaussian distribution ondata (x)w i t h‘n’features, the Gaussian function is deﬁned as (Chen et al., 2015).fxðÞ ¼
1σﬃﬃﬃﬃﬃﬃ2πpe −x−μðÞ22σ2ð5Þ
Fig. 1.Block diagram of the proposed class based ﬂower video retrieval system.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
264Whereμis the mean andσis the standard deviation of data (fea-tures)‘x’.3.1.2. Extraction ofﬂower region of interest (FRoI)After the process of segmentation of keyframes, all ﬂower regions are selected using connected component analysis and the selectedﬂower regions are named as Flower Regions of Interest (FRoI's) (referFig. 1). Then from FRoI's of each keyframe, features such as GLCM, LBPand SIFT are extracted for further processing.3.2. Extraction of featuresVideo visual features such as color, texture, local invariant features,etc., play an important role in the retrieval of videos ( Hong et al., 2014;Li et al., 2015). Some of the different species ofﬂowers are similar in color. For example, we canﬁnd red colored rose, hibiscus, bougainvil-lea belongs to three different species. Therefore, color feature many notdiscriminateﬂowers from one species to another. There exists a largeintra class variability and inter class similarity in the dataset. Due tothis there are two prime motivations in the selection of features to de-scribeﬂowers in videos. Primarily, the texture of an individual speciesofﬂowers are similar, therefore textural features are used to describetheﬂowers in videos. Secondly, theﬂowers in videos consists of varia- tion in view point and illumination, in such cases features extractedfrom Scale Invariant Feature Transform Lowe (2004)are considered.3.2.1. Texture featuresTexture of an image/frame contain unique visual patterns. Texturefeatures describes the object surface, these features are independentof object colorHu et al. (2011). The videos ofﬂowers consist of large intra class variation such as variation in color of ﬂowers. Therefore, to describe theﬂower region, texture features play a vital role. In thiswork, texture features namely, Gray Level Co-occurrence Matrix andLocal Binary Pattern are used.3.2.1.1. Gray level co-occurrence matrix (GLCM). GLCM describes the tex- ture ofﬂower in terms of statistical information. In the current work, thesystem extracts 14 different gray level co-occurrence of statistical values(Haralick et al., 1973) are extracted from each FRoI. These features arerepresented as a feature vector.3.2.1.2. Local binary pattern (LBP).LBP describes the texture descriptionin terms of local features of theﬂower region. An approach to recog-nize local binary patterns of image texture, and their occurrence histo-gram proved that LBP is a powerful texture feature ( Ojala et al., 2002). It is robust in terms of variation and transformation of the gray scale.In the proposed work, the system extracts LBP features ( Ojala et al., 2002) which are invariant to local grayscale variations in the FRoI.LBP texture features are extracted using 3 × 3 neighbourhood by thevalue of centre pixel, the pixels of eight neighbors are thresholded.In 3 × 3 neighbourhood, the centre pixel LBP value is obtained bythresholded binary values are weighted by powers of two andsummed up.3.2.2. Scale invariant feature transform (SIFT)SIFT plays a vital role in video retrieval for the analysis of the video
content (Zhu et al., 2016). In SIFT the set of image features are generatedin 4 stagesLowe (2004).I nt h eﬁrst stage, the model searched over allscales and image locations to identify interest points that are invariantto orientation and scale. In the second stage, at each location, model isdetermined scale and location which is named as keypoint localization.In the third stage, based on local image gradient directions, orientationsare assigned to each keypoint location. Finally, at the selected scale inthe region around each keypoint, it generates descriptors, with a kernelof 4 × 4 histogram of 8 bins. These histograms compute the directionand magnitude of the gradient in the region of 16 × 16 pixels. The histo-grams results are represented in the form of descriptors. In the currentwork these feature descriptors are used to describe the FRoI's Lowe (2004).To design the proposed model, the features such as Gray LevelCo-occurrence Matrix (GLCM) (Haralick et al., 1973), Local Binary Pattern (LBP) (Ojala et al., 2002) and Scale Invariant Feature Trans-form (SIFT) features proposed byLowe (2004)are extracted. Initially we propose to accomplish extracting these features by consideringan entire keyframe after segmentation ( Guru et al., 2018a, 2018b). Subsequently, we employ the extraction of features on all ﬂower re- gions of each keyframe of the video. And ﬁnally, we accomplish extraction of these features by selecting the Maximum Flower Re-gion among allﬂower regions of the keyframe for the purpose ofretrieval.Table 1Summary of mentioned technologies and applications in related works.Sl.No.Algorithms Applications References1 opticalﬂow and Hidden Markov Models retrieval of videos Gao et al., 2009 2 multi-modal spectral clustering and ranking algorithm retrieval of videos Han et al., 2014 3 Principal Component Analysis Feature dimensionality reduction Geetha et al., 2009 4. Fisher Discriminant Ratio, Linear Discriminant Analysis, semi-supervised linear discriminantanalysis, supervised linear dimensionality reduction, nonparametric discriminant analysis Feature dimensionality reduction Shen et al., 2016Gao et al., 2009Wang et al., 2016Cui et al., 2016Khan et al., 2012
Fig. 2.Extraction of features from allﬂower regions of interest.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
265Fig. 3.Extraction of Maximum Flower Region of Interest (Max. FRoI).
Fig. 4.Samples ofﬂower videos with large intraclass variation from 30 classes of videos.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
2663.2.3. Entire keyframeIn this method (Guru et al., 2018a, 2018b), the model extracts the features such as Gray Level Co-occurrence Matrix (GLCM) ( Haralick et al., 1973), Local Binary Pattern (LBP) (Ojala et al., 2002)a n dS c a l eI n - variant Feature Transform (SIFT)Lowe (2004)from an entire keyframe after segmentation and generates feature vector. Then, in the proposedmodel these features are fused like GLCM+LBP, GLCM+SIFT, LBP + SIFT,GLCM+LBP + SIFT to improve the performance of the system. Thevideox
viis represented as a set of features and is de ﬁned as, x
vi¼f1,f2,f3,...,fi,...,fN fg ð 6ÞThen,x
vi=F iMi, whereF iMi={ f 1,f2,f3,…,f i,…,f N}, similarly, features for all videos of a data base‘X' of Eq.(1)is deﬁned as,
RD¼F 1M1xv1ðÞ;F 2M2xv2ðÞ;F 3M3xv3ðÞ;…;F iMixviðÞ;…;F nMnxvnðÞ fg ð7ÞWhereF
1M1(xv1), F2M2(xv2), F3M3(xv3),…,F iMi(xvi),…,F nMn(xvn)are the feature matrices of the videosx
v1,xv2,xv3,…,x vi,…,x vnrespectively in Eq.(1).3.2.4. Allﬂower regions of interestThe proposed system extracts features such as GLCM ( Haralick et al., 1973), LBP (Ojala et al., 2002)a n dS I F TLowe (2004)from allﬂower re- gions of keyframes and is as shown in Fig. 2. In the proposed model these features are fused like GLCM+LBP, GLCM+SIFT, LBP + SIFT,GLCM+LBP + SIFT to improve the performance of the system. Let Rr
be the number of selectedﬂower regions of a keyframesk iin Eq.(4). Then,sk
iwith number ofﬂower regions is deﬁned as. sk
i¼R 1SKi,R2SKi,R3SKi,...,R rSKi fg ð8ÞThen, the feature vector of all regions is represented asR
1SKi¼f11,f12,f13,...,f1M ½/C138,R 2SKi
¼f21,f22,f23,...,f2M ½/C138,...,R 1SKi¼fr1,fr2,fr3,...,frM ½/C138Finally, the feature vector of all the regions of a keyframe sk
ias s h o w ni nE q .(8)is represented as,
F1M1d¼f11;f12;f13;…;f1M ½/C138 ;f21;f22;f23;…;f2M ½/C138 ;…;fr1;fr2;fr3;…;frM ½/C138 fg
Then, all regions of a keyframesk iis deﬁned as,F
1Md1¼∀R iSKi∈sk i ð9ÞWhereF
1M1dis the feature matrix of the video x viof the Eq.(1) consists of∀R
iSKiall regions of a keyframesk iin Eq.(8). Then, the feature vector of all FRoI's of all keyframes of a video x
vican be deﬁned as.FM
dxviðÞ ¼∀F jMdj∈SK y ð10ÞWhereFM
d(xvi)is the feature matrix of the video x viof Eq.(1) consists of all feature matrices of all ‘y’keyframes of a video as shown in Eq.(4).The feature dimension of a video x
vii.e.,FMd(xvi)consists of the features extracted from all regions of each keyframe of the video x
vi. Similarly, the feature vectors obtained for all videos of a database ‘X' can be deﬁned as,R
D¼FMdxv1ðÞ;FMdxv2ðÞ;FMdxv3ðÞ;…;FMdxviðÞ;…;FMdxvnðÞð11Þ3.2.5. Maximumﬂower region of interestIn this method, features such as GLCM ( Haralick et al., 1973), LBP (Ojala et al., 2002)a n dS I F TLowe (2004)are extracted from the Maxi- mum Flower Region of Interest (Max. FRoI) among all ﬂower regions of each keyframe of a video, then features are fused like GLCM+LBP,GLCM+SIFT, LBP + SIFT, GLCM+LBP + SIFT to improve the perfor-mance of the system..Fig. 3shows the selectedﬂower region. Max. FRoI is obtained by selecting the maximum ﬂower region i.e., theﬂower region having high density of pixels and is the largest area among all theregions in each keyframe. When there is only one ﬂower region in the keyframe then that will be considered as Max. FRoI as shown in Fig. 3. It reduces the dimension of the features of the proposed retrieval sys-tem as compared with all FRoI's. Through Max. FRoI model, the ef ﬁ- ciency can be improved. The features are extracted, after selectingMax. FRoI from each keyframe of Eq. (4). Therefore, the feature vector deﬁned in Eq.(8)can be redeﬁned in this case as,MF
iMdi¼Max R iSKi ðÞ∈sk i ð12ÞWhereMF
iMidis the feature matrix of the video x viof the Eq.(1) consists ofMax(R
iSKi) maximumﬂower region of a keyframesk iin Eq.(8).Then the feature matrix of Max. FRoI of all keyframes of a video x
vi
can be deﬁned asFM
dxviðÞ ¼∀MF jMdj/C16/C17∈SK
y ð13Þ
Sl. No.No. of Classes in a videoMulti classFlower VideoFlowerRegion ofInterestCorrectly Identified?
12 YesYes
22 YesNo
32 YesYes
42 YesNo
52 YesYes
62 YesYes
72 YesYes
83 YesYes
No
Fig. 5.Query acquiring the identity of the class for multiclass ﬂower video.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
267Fig. 6.Features extracted from all FRoI's for SGGP dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.
Fig. 7.Features extracted from all FRoI's for Sonycyber Shot dataset: (a) 30% Train - 70% Test, (b) 50% Train - 50% Test, (c) 70% Train - 30% Test.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
268Fig. 8.Features extracted from all FRoI's for Canon dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.
Fig. 9.Features extracted from Max FRoI for SGGP dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
269Fig. 10.Features extracted from Max FRoI for Sonycyber Shot dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.
Fig. 11.Features extracted from Max FRoI for Canon dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
270WhereFMd(xvi) is the feature matrix of the video x viof Eq.(1) consists of maximumﬂower region feature matrices of all ‘y’ keyframes of a video as shown in Eq. (4).The feature dimension of a videox
vii.e.,FMd(xvi)i nE q .(13)consists of the features extracted from maximum ﬂower region of each keyframe of the videox
vi. Similarly feature vectors for all videos of data-base‘X' are obtained and are deﬁned as,R
D¼FMdxv1ðÞ;FMdxv2ðÞ;FMdxv3ðÞ;…;FMdxviðÞ;…;FMdxvnðÞ ð14ÞFurther, even though the Max.FRoI reduces the dimension of the fea-tures of the proposed retrieval system as compared with all FRoI's, toimprove the efﬁciency of the retrieval system, the most discriminantfeatures from Max. FRoI are obtained using LDA and is discussed in sec-tion 3.2.4. The feature dimension of a video x
vias shown in Eq.(13)is represented as the reduced discriminant features obtained from Max.FRoI using LDA and it can be deﬁned asFM
dxv1ðÞ ¼∀MF jMdj/C16/C17∈SK
y ð15ÞWherej=1to‘y’keyframes of a videox
vias shown in Eq.(4). Finally, the reduced feature vectors for all videos of the database ‘X', are deﬁned as,R
D¼/C26DR FMdxv1ðÞ/C16/C17DR FMdxv2ðÞ/C16/C17DR FMdxv3ðÞ/C16/C17…DR FMdxviðÞ/C16/C17…DR FM
dxvnðÞ/C16/C17/C27 ð16Þ3.2.6. Linear discriminant analysis (LDA)LDA is a supervised dimensionality reduction method ( Belhumeur et al., 1999). Ronald Fisher in 1936 proposed discriminant analysis, toﬁnd a new feature space from original feature space. LDA plays a vitalrole in order to maximize class separability and preserves the withinTable 2(a). SGGP Dataset: Train 30% - Test 70%. (b). SGGP Dataset: Train 50% - Test 50%. (c). SGGPDataset: Train 70% - Test 30%.(a)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.2 0.18 0.15 0.17LBP (Ojala et al., 2002) 0.13 0.11 0.07 0.09SIFTLowe (2004)0.99 0.99 1 0.99 GLCM+LBP 0.16 0.15 0.1 0.12GLCM+SIFT 0.99 0.99 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.99 0.99 0.99 0.99(b)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.22 0.21 0.18 0.19LBP (Ojala et al., 2002) 0.13 0.12 0.08 0.09SIFTLowe (2004)0.99 0.99 0.99 0.99 GLCM+LBP 0.19 0.18 0.12 0.14GLCM+SIFT 0.99 0.99 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.98 0.98 0.98 0.98(c)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.33 0.31 0.3 0.29LBP (Ojala et al., 2002) 0.15 0.16 0.1 0.11SIFTLowe (2004) 0.99 0.99 1 0.99 GLCM+LBP 0.2 0.2 0.1 0.16GLCM+SIFT 0.99 0.99 1 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.99 0.99 1 0.99
Table 3(a). Sonycyber Shot Dataset: Train 30% -Test 70%. (b). Sonycyber Shot Dataset: Train 50%-Test 50%. (c). Sonycyber Shot Dataset: Train 70% -Test 30%.(a)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.21 0.2 0.17 0.18LBP (Ojala et al., 2002) 0.38 0.41 0.37 0.37SIFTLowe (2004)0.99 0.99 0.99 0.99 GLCM+LBP 0.45 0.44 0.43 0.42GLCM+SIFT 0.99 0.99 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.97 0.99 0.96 0.97(b)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.24 0.24 0.2 0.22LBP (Ojala et al., 2002) 0.37 0.38 0.4 0.35SIFTLowe (2004)0.99 0.99 1 1 GLCM+LBP 0.49 0.51 0.5 0.47GLCM+SIFT 0.99 0.99 1 1LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.99 0.99 1 0.99(c)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.28 0.44 0.2 0.3LBP (Ojala et al., 2002) 0.39 0.37 0.5 0.43SIFTLowe (2004)0.99 0.99 1 1 GLCM+LBP 0.54 0.54 0.5 0.53GLCM+SIFT 0.99 0.99 1 1LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 0.99 0.99 1 1Table 4(a). Canon Shot Dataset: Train 30% -Test 70%. (b). Canon Shot Dataset: Train 50% -Test 50%.(c). Canon Shot Dataset: Train 70% -Test 30%.(a)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.54 0.53 0.51 0.5LBP (Ojala et al., 2002) 0.63 0.68 0.64 0.64SIFTLowe (2004)0.99 0.99 0.99 0.99 GLCM+LBP 0.81 0.83 0.78 0.8GLCM+SIFT 0.99 0.99 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 1 1 1 1(b)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.56 0.55 0.52 0.53LBP (Ojala et al., 2002) 0.66 0.7 0.67 0.67SIFTLowe (2004)0.99 0.99 0.99 0.99 GLCM+LBP 0.82 0.86 0.8 0.82GLCM+SIFT 0.99 1 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 1 1 1 1(c)Features Accuracy Precision Recall F-MeasureGLCM (Haralick et al., 1973) 0.63 0.64 0.61 0.6LBP (Ojala et al., 2002) 0.69 0.73 0.71 0.7SIFTLowe (2004)0.99 0.99 0.99 0.99 GLCM+LBP 0.85 0.87 0.86 0.86GLCM+SIFT 0.99 1 0.99 0.99LBP + SIFT 1 1 1 1GLCM+LBP + SIFT 1 1 1 1V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
271Fig. 12.Features extracted from entire keyframe for SGGP dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.
Fig. 13.Features extracted from entire keyframe for Sonycyber Shot dataset (a)30%Train-70%Test (b)50%Train-50% Test (c)70%Train-30% Test.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
272class similarity. It maximizes the distance between the projected data ofinter classes and minimizes the distance between the predictable dataof the intra class (Gyamﬁet al., 2018;Wang et al., 2016) and hence in the current work we have applied LDA for the reduction of feature di-mension.The reduced dimension of the feature vector is de ﬁned as follows,DR FMV
i ðÞ ¼f1,f2,f3,...,fdr fg ð17ÞFor the retrieval of videos, the proposed model utilizes reduced fea-tures obtained after dimensionality reduction. The reduced feature vec-tor ofFMV
iconsists of 30 features.3.3. Retrieval: query claiming identity of classInitially, for a given query video‘Q
V’,the system acquires the identity of the class using Multiclass Support Vector Machine (MSVM). Then thesimilar videos are retrieved from the predicted class. For the retrieval ofa query video, the model is trained with two different set of featuresexplained in section 3.2.2 and section 3.2.3 and the experimentalresults are shown in section 4.Support vector machine (SVM) is a computationally powerful toolfor supervised learning (Kumar and Gopal, 2011,a n dKhan et al., 2012). Support vector machine is a vector-space-based classi ﬁcation method for both linear and non-linear data. The fundamental idea ofSVM classiﬁer is toﬁnd the optimal separating hyperplane betweentwo classes. For more information please refer ( Vapnik (1998)and Duda et al., 1997).4. Experiments and results4.1. DatasetsDataset is a fundamental requirement to test the ef ﬁciency of any automatic system designed. To conduct experiments, relatively largedataset is required. Since the standardﬂower video dataset is not pub- licly available, we createdﬂower video datasets. To createﬂower video datasets, we used three devices namely, Samsung Galaxy GrandPrime (SGGP) mobile, Sonycyber Shot camera and Canon camera.
Fig. 14.Features extracted from entire keyframe for Canon dataset: (a) 30% Train –70% Test, (b) 50% Train–50% Test, (c) 70% Train–30% Test.
Table 5Accuracy obtained for feature combinations with different modes of extraction of features with 70% training and 30% testing.Sl. No. Modes of extraction of features Feature combination Datasets (results in %)SGGP Sonycyber Shot Canon1 Entire keyframe GLCM+LBP + SIFT 53.83 60.18 65.732 All FRoI's GLCM+LBP + SIFT 53.83 63.56 52.363 Max. FRoI GLCM+LBP + SIFT 60.59 67.07 75.794 Max. FRoI with LDA LBP + SIFT 100 100 100V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
273SGGP dataset consists of 2611 videos of 8 M pixels. Sonycyber Shot cam-era dataset consists of 2521videos of 14.1 M pixels. And Canon cameracosists of 2656 videos of 16 M pixels. Videos captured with the durationranges from 4 to 60 s. We have captured 30 different species of ﬂowers from all the three devices. There exists a small inter class and large intraclass variations. Videos captured in the real environment during sum-mer, rainy and winter seasons. Videos involved the challenges such asviewpoint variations, illumination, cluttered background, and multipleinstances of theﬂowers. Flower video samples with large intra-classvariations from the dataset we created are shown in Fig. 4. Along with the above mentioned three datasets, we created adataset with multiple classes ofﬂowers in a video for querying. Thedataset contains two and three different classes of ﬂowers. The samples of theseﬂower videos are shown inFig. 5. The performance of the proposed model is analysed in differentmodes of extraction of features. Results of the features extracted fromall FRoI's as shown in the section 4.2, the features extracted from Max-imum FRoI (MFRoI) is as shown section 4.3 and the features extractedfrom Maximum FRoI (MFRoI) with LDA is as shown in section 4.4. Andalso, the results obtained in previous work of extracting features fromentire keyframe (Guru et al., 2018a, 2018b) are shown in section 4.5. The dataset we created is used to conduct experiments. In order to eval-uate the system, metrices such as accuracy, precision, recall and F-measure are used and are given below. The results are tabulated withvarying training and testing videos.Accuracy¼
Sum of videos retrieved correctlyTotal number of query videos ð18ÞPrecision¼Total number of videos retrieved are relevantTotal number of videos retrieved ð19ÞRecall¼
Total number of videos retrieved are relevantTotal number of similar videos in the database ð20ÞF−Measure¼
2∗Precision∗RecallPrecisionþRecallðÞ ð21Þ4.2. All FRoI'sThe result analysis of proposed retrieval system trained with the fea-tures extracted from all FRoI's are shown in the following ﬁguresFig. 6, Fig. 7andFig. 8for SGGP, Sonycyber Shot and Canon datasets respec-tively. From the results we can observe that the accuracy of the systemin this approach achieved 53.83% for SGGP dataset, 52.36% for SonycyberShot and 63.56% for Canon dataset for 70% training and 30% testing.4.3. Max. FRoIThe result analysis of proposed retrieval system trained with the fea-tures extracted from maximumﬂower region of interest are shown inthe followingﬁgures:Fig. 9,Fig. 10andFig. 11for SGGP, Sonycyber Shot and Canon datasets respectively. From the results we can observethat the accuracy of the system in this approach is achieved 60.59% forSGGP dataset, 67.07% for Sonycyber Shot dataset and 75.79% for Canondataset for 70% training and 30% testing. Further, from the results we
Fig. 15.Comparative study between proposed work and deep learning model ( Jyothi et al., 2018) for SGGP dataset.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
274can observe that the Max. FRoI give improved results than all FRoI's forall the three datasets.4.4. Max. FRoI with LDAIn this section we obtain discriminant features from Max. FRoI usingLDA are passing to the model. It improves the retrieval performance byidentifying the class of the query video. Table 2(a) toTable 2(c),Table 3 (a) toTable 3(c) andTable 4(a) toTable 4(c) show the result analysis of proposed retrieval system trained with the reduced features extractedfrom Max. FRoI is as shown in Eq.(14)for SGGP, Sonycyber Shot and Canon datasets respectively. Further, the tables show that the resultsobtained from Max. FRoI with LDA gives good results than the resultsobtained from other proposed modes.4.5. Comparative study between proposed work and previous workIn the previous work (Guru et al., 2018a, 2018b) the features such as Gray Level Co-occurrence Matrix (GLCM) ( Haralick et al., 1973), Local Binary Pattern (LBP) (Ojala et al., 2002) and Scale Invariant Feature Transform (SIFT)Lowe (2004)are extracted from entire keyframe.With the fusion of these features the model achieved good performance.The retrieval accuracy of previous work ( Guru et al., 2018a, 2018b) achieved 53.83%, 60.18% and 65.73% are shown in Fig. 12,Fig. 13and Fig. 14for SGGP, Sonycyber Shot and Canon datasets respectively. Inthe proposed work, to further improve the retrieval performance,GLCM (Haralick et al., 1973), LBP (Ojala et al., 2002) and SIFTLowe (2004)features are extracted in two different modalities as mentionedin section 3.2.2 and 3.2.3. The features extracted from proposed retrievalsystem using, Max. FRoI and Max. FRoI with LDA these two methodsgive good results when compared to the previous work ( Guru et al., 2018a, 2018b). The comparison between the results obtained fromprevious and proposed approaches namely, features extracted froman entire keyframe, all FRoI's, Max. FRoI and Max.FRoI with LDA aresummarized inTable 5for all datasets.4.6. Result analysis and discussionWe have the following observations from the proposed system of ap-proaches namely, features extracted from an entire keyframe, all FRoI's,Max. FRoI and Max.FRoI with LDA. Features extracted from entirekeyframes of a video provide good results with the fusion of the featuresGLCM+LBP + SIFT as shown inFig. 12toFig. 14. All FRoI's approach generates almost similar results for the combination of features GLCM+LBP + SIFT as compared to the features extracted from an entirekeyframe as shown inFig. 6toFig. 8for SGGP, Sonycyber Shot and Canon datasets respectively. Max. FRoI's approach generates good resultsfor the combination of features GLCM+LBP + SIFT as shown in Fig. 9to Fig. 11for SGGP, Sonycyber Shot and Canon datasets respectively. Fromthe results we can observe that, this approach generates improvedresults than the features extracted from entire keyframe. The proposedapproach Max. FRoI with LDA resultsshow the effectiveness of the selec-tion of more discriminating feature subset from original set using LDA.The efﬁciency of the proposed system using Max. FRoI with LDA is im-proved and achieved 100% performance for SGGP, Sonycyber Shot andCanon datasets.
Table 2andTable 3show the combination of features
Fig. 16.Comparative study between proposed work and deep learning model ( Jyothi et al., 2018) for Sonycyber Shot dataset.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
275LBP + SIFT achieves good performance for SGGP and Sonycyber Shotdatasets.Table 4show that the combinations of features LBP + SIFTand GLCM+LBP + SIFT achieves good pe rformance for Canon dataset.4.7. Query with multiple classﬂowers in videosQuery video may contain multiple classes of ﬂowers in video. There are two cases at this point. First, a query video consists of multiclassﬂowers in all frames, then the system retrieves similar videos from da-tabase by considering Flower Regions of Interest. Second, a query videoconsists of multiclassﬂowers not in the same frame, video consists ofone class in some duration and then other classes in next duration. Insuch case, we manually split (cut) the video into shots based on classboundary, then for each shot the system retrieves similar videos basedon FRoI using MSVM.Fig. 5shows the query acquiring the identity ofthe class for multiclassﬂowers in a video.5. Comparative study between proposed work and deep learningmodelIn (Jyothi et al., 2018), authors have proposed aﬂower video re- trieval system using deep leaning approach, here the similar videos fora given query video are retrieved using Multiclass Support Vector Ma-chine. For the extraction of features in ( Jyothi et al., 2018), authors have proposed three different modalities; entire keyframe, segmentedﬂower region of a keyframe, and the gradient of ﬂower region are con- sidered for feature extraction using Deep Convolutional Neural Networkof AlexNet architecture. Among these three modalities, the segmentedﬂower region of a keyframe is achieved better results for smallerdataset. In (Jyothi et al., 2018), the query video consists of a singleclass ofﬂowers. In the present work along with single class of ﬂower videos query video also consists of multiclass ﬂowers. The dataset con- sidered in the present work is relatively large. The presented model iscompared against deep learning model ( Jyothi et al., 2018) which re- veals that the proposed one is superior to the existing in terms of re-trieval results. The proposed system Max. FRoI with LDA is improvedand achieved 100% performance for larger sized datasets namelySGGP, Sonycyber Shot and Canon. The retrieval results in terms of Accu-racy, Precision, Recall and F-measure of existing work ( Jyothi et al., 2018) are compared with present work and the results are shown inFig. 15,Fig. 16andFig. 17for SGGP, Sonycyber Shot and Canon datasetsrespectively.6. Future workThe research work presented in this paper can be further extendedin following ways:a) An attempt on shot boundary or class boundary detection when avideo consists of multiple species ofﬂowers can be further explored.Explanation: In the present work, when a video consists of one classofﬂowers in some duration and then other classes in next duration. Insuch case, we manually split (cut) the video into shots. To overcomethis drawback a video shot/class boundary detection is essential.
Fig. 17.Comparative study between proposed work and deep learning model ( Jyothi et al., 2018) for Canon dataset.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
276Further, the research can be automized for shot boundary detection in-stead of manually split the video shots.b) The current research work limits the species of ﬂowers to 30.Explanation: The current research work limits the class size ofﬂowers to 30. There is scope for extending the class size and explore dif-ferent methodologies to retrieveﬂower videos in real time.7. ConclusionThe main aim of this work is to discover the solution to a problem ofretrieval of videos ofﬂowers through query by video mechanism. Thepresented system works based on keyframes represented for eachvideo. Keyframes are segmented using statistical region merging algo-rithm. From the segmented keyframes features are extracted in threedifferent modalities namely, all regions ofﬂowers in the keyframe, max- imumﬂower region in the keyframe andﬁnally, features of maximum ﬂower region with a set of discriminating features generated by LDA.The presented system is compared against our previous work ( Guru et al., 2018a, 2018b) and the deep learning retrieval system ( Jyothi et al., 2018), which reveals that the proposed system with Max. FRoIwith LDA is superior to the existing models in terms of retrieval results.The proposed system retrieves similar videos when the query videocontains multiple classes ofﬂowers in a video. In proposed work,when video consists of one class ofﬂowers in some duration and thenother classes in next duration. In such case, we manually split (cut)the video into shots. Further, the research can be extended byautomizing the shot boundary detection instead of manually split thevideo shots. And also, the current research work limits the species ofﬂowers to 30, further the class size can be extended and can explore dif-ferent methodologies to retrieveﬂower videos in real time.Declaration of Competing InterestWe don't have any conﬂict of interest.References
Belhumeur, P.N., Hespanha, J.P., Kriegman, D.J., 1999. Eigenfaces vs.ﬁsherfaces: recogni- tion using class speciﬁc linear projection. IEEE Trans. Pattern Anal. Mach. Intell. 19,711–720.Chaitra, K.N., Jyothi, V.K., Chandrajit, M., Guru, D.S., 2021. Flower classiﬁcation in videos: a hog-PCA-NN method. 2nd International Conference on Arti ﬁcial Intelligence: Ad- vances and Applications, ICAIAA 2021.Chen, W., Tian, Y., Wang, Y., Huang, T., 2015. Fixed-point gaussian mixture model for anal- ysis friendly surveillance video coding. Comput. Vis. Image Underst. 142, 65 –79. Cui, M., Cui, J., Li, H., 2016.Dimensionality reduction for histogram features: a distance-adaptive approach. Neurocomputing 173, 181 –195. Das, M., Manmatha, R., Riseman, E., 1999a. Indexingﬂower patent images using domain knowledge. IEEE Intell. Syst. 14 (5), 24 –33. Das, M., Manmatha, R., Riseman, E.M., 1999b. Indexingﬂower patent images using do- main knowledge. IEEE Intell. Syst. 14 (5), 24 –33. Duda, R.O., Hart, P.E., Stork, D.G., 1997. Pattern Classiﬁcation. Second edition. .Gao, X., Xuelong, L., Jun, F., Dacheng, T., 2009. Shot-based video retrieval with optical ﬂow tensor and hmms. Pattern Recogn. Lett. 30, 140 –147. Geetha, M.K., Palanivel, S., Ramalingam, V., 2009. A novel block intensity comparison code for video classiﬁcation and retrieval. Expert Syst. Appl. 36, 6415 –6420. Guru, D.S., Sharath, Y.H., Manjunath, S., 2010. Texture features and knn in classi ﬁcation of ﬂower images. IJCA special Issue on Recent Trends in Image Processing and PatternRecognition, RTIPPR, pp. 21–29. Guru, D.S., Sharath, Y.H., Manjunath, S., 2011. Texture features inﬂower classiﬁcation. Math. Comput. Model. 54, 1030–1036. Guru, D.S., Jyothi, V.K., Kumar, Y.H.S., 2018a. Features fusion for retrieval ofﬂower videos. Proceedings of DAL 2018, LNNS 43, pp. 221 –234. Guru, D.S., Jyothi, V.K., Kumar, Y.H.S., 2018b. Cluster based approaches for keyframe selec- tion in naturalﬂower videos. Springer AISC 736, ISDA 2018, pp. 474 –484. Gyamﬁ, K.S., Brusey, J., Hunt, A., Gaura, E., 2018. Linear dimensionality reduction for clas- siﬁcation via a sequential bayes error minimisation with an application to ﬂow meterdiagnostics. Expert Syst. Appl. 91, 252 –262. Han, J., Ji, X., Hu, X., Han, J., Liu, T., 2014. Clustering and retrieval of video shots based on natural stimulus FMRI. Neurocomputing 144, 128 –137. Haralick, R.M., Shanmugam, K., Dinstein, I., 1973. Textural features for image classi ﬁca- tion. IEEE Trans. Syst. Man Cybernet. SMC-3 (6), 610 –621. Hong, R., Pan, J., Hao, S., Wang, M., Xue, F., Wu, X., 2014. Image quality assessment based on matching pursuit. Inf. Sci. 273, 196 –211. Hu, W., Xie, N., Li, L., Xianglin, Z., Maybank, S., 2011. A survey on visual content-based video indexing and retrieval. IEEE Trans. Syst. Man Cybernet. 41 (6), 797 –819. Jyothi, V.K., Guru, D.S., Sharath Kumar, Y.H., 2018. Deep learning for retrieval of natural ﬂower videos. Elsevier Proc. Comput. Sci. 132, 1533 –1542. Khan, N.M., Ksantini, R., Ahmad, I.S., Boufama, B., 2012. A novel SVM+NDA model for classiﬁcation with an application to face recognition. Pattern Recogn. 45, 66 –79. Kumar, M.A., Gopal, M., 2011. A hybrid SVM based decision tree. Pattern Recogn. 43,3977–3987.Li, J., Li, X., Yang, B., Sun, X., 2015. Segmentation-based image copy move forgery detec- tion scheme. IEEE Trans. Inf. Foren. Secur. 10 (3), 507 –518. Liang, B., Xiao, W., Liu, X., 2012. Design of video retrieval system using MPEG-7 descrip-tors. Proc. Eng. 29, 2578–2582.Lowe, D.G., 2004.Distinctive image features from scale-invariant keypoint. Int. J. Comput.Vis. 60, 91–110.Morand, C., Benois-Pineau, J., Domenger, J.P., Zepeda, J., Kijak, E., Guillemot, C., 2010. Scal- able object-based video retrieval in hd video databases. Signal Process. ImageCommun. 25, 450–465.Mufti, M., Shamim, K.M., McGinnity, Martin, Hussain, Amir, 2021. Deep learning in mining biological data. Cogn. Comput. 13, 1 –33. Nock, R., Nielsen, F., 2004.Statistical region merging. IEEE Trans. Pattern Anal. Mach.Intell. 26 (11), 1–7.Ojala, T., Pietikainen, M., Maenpaa, T., 2002. Multiresolution gray scale and rotation in- variant texture classiﬁcation with local binary patterns. IEEE Trans. Pattern Anal.Mach. Intell. 24 (7), 971–987.Priya, G.G.L., Domnic, S., 2014. Shot based keyframe extraction for ecological videoindexing and retrieval. Ecol. Inform. 23, 107 –117. Shekar, B.H., Uma, K.P., Holla, K.R., 2016. Video clip retrieval based on lbp variance. Proc. Comput. Sci. 89, 828–835.Shen, X.-J., Mu, L., Li, Z., Wu, H.-X., Gou, J.-P., Chen, X., 2016. Large-scale support vector machine classiﬁcation with redundant data reduction. Neurocomputing 172,189–197.Stauffer, C., Grimson, W.E.L., 1999.
Adaptive background mixture models for real-time tracking. Proceedings of IEEE Conference on Computer Vision and Pattern Recogni-tion.Vapnik, V.N., 1998.Statistical Learning Theory. John wiley and sons, New York, USA.Wäldchen, Jana, Rzanny, Michael, Seeland, Marco, Mäder, Patrick, 2018. Automated plantspecies identiﬁcation—Trends and future directions. https://doi.org/10.1371/journal. pcbi.1005993.Wang, S., Lu, J., Gu, X., Du, H., Yang, J., 2016. Semi-supervised linear discriminant analysis for dimension reduction and classi ﬁcation. Pattern Recogn. 57, 179–189. Zhu, Y., Huang, X., Huang, Q., Tian, Q., 2016. Large-scale video copy retrieval with tempo- ral concentration sift. Neurocomputing 187, 83 –91.V.K. Jyothi, V.N.M. Aradhya, Y.H. Sharath Kumar et al. Artiﬁcial Intelligence in Agriculture 5 (2021) 262 –277
277