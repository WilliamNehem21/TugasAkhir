Original Article
Assessing the suitability of soft computing approaches for forest ﬁresprediction
Samaher Al_Janabia, Ibrahim Al_Shourbajib,⇑, Mahdi A. Salmana
aDepartment of Computer Science, Faculty of Science for Women (SCIW), University of Babylon, Iraq
bDepartment of Computer Network, Faculty of Computer Science and Information System, University of Jazan, Jazan, Saudi Arabia
article info
Article history:Received 5 May 2017Revised 7 September 2017Accepted 16 September 2017Available online 20 September 2017Keywords:Forest ﬁresSoft computingPredictionPrinciple component analysisParticle swarm optimizationCascade correlation networkMultilayer perceptron neural networkPolynomial neural networksRadial basis functionSupport vector machineabstract
Forest ﬁres present one of the main causes of environmental hazards that have many negative results indifferent aspect of life. Therefore, early prediction, fast detection and rapid action are the key elements forcontrolling such phenomenon and saving lives. Through this work, 517 different entries were selected atdifferent times for montesinho natural park (MNP) in Portugal to determine the best predictor that hasthe ability to detect forest ﬁres, The principle component analysis (PCA) was applied to ﬁnd the criticalpatterns and particle swarm optimization (PSO) technique was used to segment the ﬁre regions (clus-ters). In the next stage, ﬁve soft computing (SC) Techniques based on neural network were used in par-allel to identify the best technique that would potentially give more accurate and optimum results inpredicting of forest ﬁres, these techniques namely; cascade correlation network (CCN), multilayer percep-tron neural network (MPNN), polynomial neural network (PNN), radial basis function (RBF) and supportvector machine (SVM) In the ﬁnal stage, the predictors and their performance were evaluated based onﬁve quality measures including root mean squared error (RMSE), mean squared error (MSE), relativeabsolute error (RAE), mean absolute error (MAE) and information gain (IG). The results indicate thatSVM technique was more effective and efﬁcient than the RBF, MPNN, PNN and CCN predictors. The resultsalso show that the SVM algorithm provides more precise predictions compared with other predictorswith small estimation error. The obtained results conﬁrm that the SVM improves the prediction accuracyand suitable for forest ﬁres prediction compared to other methods./C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University. This is anopen access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
1. IntroductionForest ﬁres is one of the most important environmental risksthat have several negative inﬂuences in different aspect of life suchas economic, natural environment and health [38]. However, early detection, fast prediction and rapid actions can play an essentialrole in controlling and saving lives from forest ﬁres danger andtheir negative consequences.According to Zadeh[66], soft computing (SC) is a collection ofadvanced computing methodologies, which aim to accomplishrobustness, tractability, total low cost and better solution for aproblem under investigation. This makes SC widely used tech-niques in solving complex problems in many different applications[14]and appropriate techniques selection is the ﬁrst step to solveany problem in real scenarios[29]. These techniques can be typi-cally divided into two groups: hard and soft computing. Fig. 1 shows the problem solving technologies and their classiﬁcations.During the last years, several works have proved that SC tech-niques play a vital role in monitoring of forest ﬁres [4,37,43].I n another works, SC was used to discover and predict valuable infor-mation about the relationships between variables for forest ﬁresoccurrence[1,17,44]and for estimating burned areas under futureclimate conditions[3,9,60]. Several recent papers have alsofocused on SC techniques applied to forest ﬁres prediction[5,13,18,27,45,50,65].Recently, many approaches have adopted artiﬁcial neural net-work (ANN) to improve forest ﬁres detection in early stages. Bis-quert et al.[10]investigated the potential of ANN and logisticregression to estimate forest ﬁre danger. The results of this workindicated that good classiﬁcation accuracy achieved by ANN
https://doi.org/10.1016/j.aci.2017.09.0062210-8327//C2112017 The Authors. Production and hosting by Elsevier B.V. on behalf of King Saud University.This is an open access article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).⇑Corresponding author.E-mail addresses:samaher@uobabylon.edu.iq(S. Al_Janabi),alshourbajiibrahim@ gmail.com(I. Al_Shourbaji),mahdi.salman@uobabylon.edu.iq (M.A. Salman). Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics 14 (2018) 214–224
Contents lists available atScienceDirect
Applied Computing and Informatics
journal homepage: www.sciencedirect.com
technique. Similar results conﬁrmed by the work of [24].I n another work, Maeda et al.[36]employed ANN to identify theareas of high risks of forest ﬁres in Brazil. Their results showed thatthe ANN approach detects forest ﬁres efﬁciently. A set of differentlearning parameters require to be identiﬁed in ANN includinglearning rate, hidden layers number and the nodes number in thehidden layer[48]. A huge number of training iterations may inﬂu-ence to over-train, and therefore, they could affect the accuracy ofthe prediction model[8].A wide range of studies have reported for the adaptation of sup-port vector machine SVM)[34,49,61,67]and they found that the SVM achieved good results and it is able to effectively predict for-est ﬁre. This is because SVM does not require determining proba-bilities before analysis and therefore, makes the method morepreferable.This paper aims to investigate the performance of ﬁve SC tech-niques to determine the best and suitable predictor for forest ﬁres.These methods include cascade correlation network (CCN), multi-layer perceptron neural network (MPNN), polynomial neural net-work (PNN), radial basis function (RBF) and support vectormachine (SVM). To attain this goal, 517 different entries wereselected at different times for montesinho natural park (MNP)dataset. The principle component analysis (PCA) was applied toﬁnd the critical patterns and particle swarm optimization (PSO)technique was used to segment the ﬁre regions (clusters). Theresulted data from PCA and PSO methods will be used as inputsin these predictors. The performance of these techniques will beevaluated based on ﬁve quality measures, which include root meansquared error (RMSE), mean squared error (MSE), relative absoluteerror (RAE), mean absolute error (MAE) and information gain (IG).The results from this work will assist and provide additional aidsfor systems operators to effectively monitor their system.2. Material and methodsForest ﬁres detection require deployment a number of sensorsscattered over a large area. The main function of these sensors isto collect various meteorological data such as moisture, pressure,temperature and humidity that can be used by the systems opera-tors to take proper actions[26]. To infuse the gathered informationfrom the different sensors and get meaningful information poses asigniﬁcant challenge to the systems operators. These problemsinvolve, ﬁnding the hidden patterns, reduce superﬂuous informa-tion and generate effective rules from the data [6,11]. This chal-lenge would have been easier if the system has the ability topredict the state of the system based on the previously obtainedvalues. For this, there is a prime importance to have an appropriatemethod, which can deﬁne the data easily according to the ﬁreinformation.Fig. 2illustrates the details of the study workﬂow.From the above ﬁgure, the ﬁrst phase is to preprocess the data-set. For this purpose, the principle component analysis (PCA) andparticle swarm optimization (PSO) techniques were used to ﬁndthe critical features and to segment the ﬁre regions (clusters)respectively. Then, ﬁve predictors were applied to identify the bestand suitable predictor that has the ability to predict forest ﬁres.The ﬁnal phase is to evaluate the predictors’ performance basedon ﬁve quality measures.
Fig. 1.Taxonomy of problem solving technologies.
Fig. 2.Workﬂow process.S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224 2152.1. Case study and data collectionThe montesinho natural park (MNP) was selected as a casestudy, which is located in the northeast of Portugal. Fig. 3shows the geographical view of the park. The meteorological datasetwas collected from the period of January 2000 to December whichhas an average temperature of 6–12 degree Celsius 2003. The dataobtained from UCI machine learning repository, which have 517different entries taken at different times. The input features consistof 12 attributes included under spatial and temporal components,ﬁre weather index (FWI) component and weather conditions. Theoutput feature represents the total burnt area. Data collectiondetails can be founded in[16,54]. Several recent papers have usedmontesinho dataset in their works for forest ﬁres prediction[15,23,25,30,46,57,62].2.2. Principle component analysis (PCA)PCA is one of the most wildly used dimension reductionmethod, which is often applied to identify patterns in datasets ofhigher dimensions[28]. In PCA, a group of p-associated variablesis transformed into a smaller set principal components (PCs)which can be used to ﬁnd the dependences and associations thatexist between variables in a compact manner. The PCA removesunimportant information from the variables and keeps thevariation presents in the original dataset. PCA transforms the datavariables for diagonalizing estimation of a covariance matrixx
p;p¼1;2;/C1/C1/C1;I;x p2RNwhich can be given by:
C¼1IXii¼1xjxTj ð1Þ
2.3. Particle swarm optimization (PSO)PSO[33]is a widely used optimization technique to ﬁnd opti-mal solutions in various applications [7,31,51,64]. Each candidate solution termed as particle that holds the previous path coordinatewhen it has the best results. Every particle has its own position,velocity and best solution. The PSO method can be presented asfollows:The swarm position in a D space of the ith particle can be pre-sented by:Xi¼X I1;Xi2;/C1/C1/C1X id fg ð2Þ
Each particle maintains a memory of its previous best position.The best swarm position is given by:
Pgbest¼fP g1;Pg2;/C1/C1/C1P gdgð 3Þ
As well as velocity of each particle can be represented
Vi¼V i1;Vi2;/C1/C1/C1V id fg ð4Þ
A particle velocity should be updated and the following equa-tion can be used
Vid¼wvidþc 1r1ðpid/C0x idÞþc 2r2ðpgd/C0x idÞð5Þ
where c 1,c2parameters are constants and r 1,r2are random num- bers within 0–1. p
idis the best local solution of the ith particle forthe iteration number up to the ith iteration. The best global solutionof all particles is p
gd. The ‘‘inertia weight” w controls the effect ofthe previous velocity of the particle on its current one. If the valueof w is greater than 1, then the particle favored searching overexploitation, else w is less than 1, the particle gave more impor-tance to the current best positions.
2.4. Cascade correlation network (CCN)CCN[21]is a supervised learning architecture that constructs anear multi-layer network topology with a high level of accuracy.CCN network structure holds no hidden nodes and each input con-nected to each output with an adjustable weight corresponding toeach connection. When the network is incapable to build a correla-tion between the input and output, a hidden unit is added to thenetwork from a pool of candidate units. The process continues untilthe error in the output units reaches to an acceptable level [52]. The output units are trained to reduce the familiar sum-squarederror measure
E¼12X
O;Pyop/C0top/C0/C12ð6Þ
where yopis the observed value of outputofor the training pattern p. The target output is t
op. E is minimized by gradient descent using
Fig. 3.Map of Portugal illustrating the location of montesinho natural park.216 S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224@E@w
oi¼X
pðyop/C0topÞf0pIip ð7Þ
where f0prepresents derivative of the activation function of the out-put unit for pattern p. I
ipis the value of hidden unit i. w oipresents the weight connecting input i to output unit o [47].
2.5. Multilayer perceptron neural network (MPNN)MPNNis a widely used feed forward networks due to its cleararchitecture, fast operation, easy to implement and its capabilityto solve complex classiﬁcation problems [32,55]. The MPNN net- work includes three main layers: input, hidden and output. Theselayers are used for data input, data transmitting and data outputrespectively. The hidden layer function is to pass the results tothe output layer[22,40]. The output of each neuron can be writtenas
yi¼fXw ijxi/C16/C17 ð8Þ
where yirepresents the input that a single node j receives. The func-tion f can be a simple threshold, sigmoidor hyperbolic tangent func-tion. The weights between the nodes i and j is denoted by w
ij:.x i
represents the output from nodei.
2.6. Polynomial neural networks (PNNs)PNN is based on group method of data handling (GMDH) thatuses a class of polynomials named partial descriptions (PDs). Bypicking the polynomial order between these numerous types ofavailable forms and the most essential input variables, the bestPDs can be obtained based on choosing the nodes of each layerand producing additional layers until the best performance isattained[41,68]. The association between input and output canbe described as
y/C0fðx 1;x2;/C1/C1/C1x nÞð 9Þ
The estimated output yˆcan be read as
^y¼^fðx1;x2;/C1/C1/C1;x nÞ¼c
0þX
k1ck1xk1þX
k1k2ck1k2xk1k2þX
k1k2k3ck1k2k3 xk1k2k3 þ/C1/C1/C1;ð10Þ
where c k’s refer to the coefﬁcients of the optimized model [42].
2.7. Radial basis function (RBF)RBF neural network has a feed forward structure that containsan input layer, a single hidden and an output layer [12]. Whereas the hidden layers collects the data from the input layers and passthem to the Gaussian transfer function to transform and regulatethe data nonlinearly. The function responses are then linearlymerged to create the data of the output layer. RBF is widely appliedin several applications such as system control, time series predic-tion, dynamic system problems and data classiﬁcation because ofits ability to forecast the behavior directly from input and outputdata[19,35,63]. RBF network tries to minimize the following errorfunction (training error)
E¼12Xkt¼1Xpj¼1e2jðtÞð 11Þ
wheree jðtÞ, is the error of each output unit[39].2.8. Support vector machine (SVM)SVM[58]algorithm is commonly employed to solve classiﬁca-tion and regression problems. It performs the classiﬁcation by sep-arating the data into two categories to determine the output andconstruct N-dimensional hyperplane. The main reason behindusing SVM lies in dealing and addressing various nonlinear issues,ﬂexible use of kernel functions to increase the ability to convertdata from some high dimensional space. The results of the higherdimensional space are similar to the results in dimensional inputspace. The SVM algorithm is considered as an alternative techniquefor polynomial, radial basis function and multi-layer perceptronclassiﬁers, the weights of the network are found and used to solvea quadratic programming problem with linear constraints. SVMaims to ﬁnd the hyperplanes that have high generalization abilityby separating the training data without errors and developing arobust and effective solution. The SVM classiﬁer tries to minimizethe following function
Lp¼12jj~wjj /C0Xni¼1/iyi~w:xi!þb/C16/C17þXni¼1/i ð12Þ
where the number of training data is and / i,i¼1/C1/C1/C1n are the non- negative numbers such that the derivatives of Lp to /
iare zero,/ i
are called the Lagrange multipliers, while the hyperplane is deﬁnedby Lagrangian function Lp, the vectors ~w and constant b.
2.9. Quality measuresThe following quality measures were selected in the perfor-mance evaluation of CCN, MPNN, PNNs, RBF and SVM models./C15Root mean squared error (RMSE):
RMSE¼ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃPni¼1ðyi/C0y0iÞ2
ns ð13Þ
/C15Mean squared error (MSE):
MSE¼Pni¼1ðyi/C0y0iÞ2
n ð14Þ
/C15Relative absolute error (RAE):
RAE¼Pni¼1jyi/C0y0jP
ni¼1jyi/C0y00j ð15Þ
/C15Mean absolute error (MAE):
MAE¼Pni¼1jyi/C0y0ijn ð16Þ
where n is the number of data rows, yipresents the actual target value of record i and y
0iis the predicted target value of record i.
/C15Information gain (IG)
Entropyðp1;p2/C1/C1/C1pnÞ¼/C0Xni¼1pilog2pi
InformationðAttributeÞ¼Xni¼1 FreSubAttributeTotal Number of Records/C3Entropy InformationGainðIGÞ¼IBS/C0IASð17Þ
whereIBSrefers to the information before splitting and IASindi- cates to the information after splitting [2].
3. Results and discussionTo apply the SC techniques, a program was developed usingpython programming language. Python is an environment forS. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224 217machine learning, text mining and business analysis. It is alsowidely used for research and education.3.1. Used dataThe collected meteorological dataset have 517 different entrieswere taken at different points of time and 13 input features arepreprocessed thoroughly. The preprocessing phase was executedaccording to the following steps:3.1.1. Preprocess of the datasetIn this stage, description features are converted into numericalusing Min-Max normalization technique. This was done by extract-ing the input features from the dataset and preprocessing them.The normalized value,e
ifor variableEin the ith row can be com- puted as:
Normalizedðe iÞ¼ei/C0E min
Emax/C0E minð18Þ
whereE minpresents the minimum value for variable EwhileE max
represents the maximum value for variable E[53]. The procedure of normalization and coding are as follows:
3.1.1.1. Normalization.The data value can be scaled by using Min-Max method and the Linear transformation (L
0) of the original input ranged to a newly speciﬁed data range.
L0¼½ ðLminÞ=ðmax/C0minÞ/C138 /C3 ðmax0/C0min0Þþmin0ð19Þ
whereminis the old minimum value,min0is new minimum,maxis old maximum,max
0is new maximum.
For example,consider an old data that ranged from [0–100], weobtain an equation to migrate the data to range from [5–10].L
0= [(L/C00)/(100/C00)]⁄(10/C05) + 5L
0= [L/100]⁄5+5L
0= (L/20) + 5Let L = 0 Then L
0=5If L = 10 Then L
0= (1/2) + 5 = (1 + 10)/2 = 5.53.1.1.2. Coding (Convert linguistic terms to numeric form). To encode the attributes of linguistic variable (e.g. day and month), the fol-lowing procedure was used/C15Create:The repetition table by determining the repetition timesfor each linguistic term./C15Rearrange:The table by making the large value repeated in themiddle and the lower one in the right and left. This process wasrepeated until the minimum repetition becomes at most leftand most right./C15Assign:Code for each linguistic term depending on its new orderin the repetition table.3.1.2. Find the correlation measuresThe main goal from this step is to ﬁnd the correlation betweenthe features which represent the most important variables in thedataset.Table 1provides the correlation among these featuresusing the following:
r¼NPxy/C0PxPyﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃNðPx
2Þp/C0ðPxÞ2ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃNðPy
2Þ/C0ðPyÞ2q ð20Þ
whereNpresent the vector size and/C016r61, the positive sign (+) denotes to positive correlation and the negative sign ( /C0) refers to negative correlation.The value of the correlation that is greater than 0.8 is consid-ered as a strong value while the ones less than 0.5 is contemplatedas a weak value. The values depend on the data from one dataset toanother and all other features have a strong correlation to eachother except in RH column as shown in Table 1. In this work, the correlation was only used as an initial processto ﬁnd associations between the features. It was found that RH hasa weak correlation with other variables. The PSO method was usedto cluster the data without RH column and the resulted data wereemployed as inputs to the predictors. However, both the computa-tional cost and the generated error are increased in the predictorsremarkably. For this reason, we decide to apply PCA technique tooptimize the selection of the most correlated variables in thedataset.3.1.3. Find the eigenvalues and eigenvectors among featuresBased on the eigenvalue criterion, each component shouldexplain at least one variable’s worth of the variability, and there-fore only components with eigenvalues >1 should be selected.Every eigenvector has a corresponding eigenvalue and for the pro-portion of variance criterion, the analysis simply selects the com-ponents one by one until the desired proportion of variability isachieved. The minimum communality criterion states that enoughcomponents should be extracted, so that the communalities foreach of these variables exceed a certain threshold, for example50%. The maximum number of components that should beextracted is just prior to where the scree plot begins to straightenout into a horizontal line. The following illustrates the PCA pseu-docode that was used to ﬁnd eigenvalues and eigenvectors.PCA procedureInput:Process datasetOutput:Principal Components (PC)-databaseStep1:Compute standardized data matrix Z¼½Z
1;Z2;/C1/C1/C1;Z m/C138 based onZ
i¼ðX i/C0liÞ=riifrom the original datasetStep 2: Compute eigenvaluesletBbe an m/C2m matrix andIbe m/C2m identity matrix (diagonal matrix with 1
0s on the diagonal)then the scalars (numbers of dimension 1 /C21)k
1;k2;/C1/C1/C1;k mare said to be the eigenvalues ofBif theysatisfyjB/C0kIj¼0Step 3:Compute eigenvectorsletBbe an m/C2m matrix andkbe an eigenvalues ofBthen nonzero m/C21 vector e is said to be an eigenvector of Bif Be =ke.Step 4:Compute ith PC, the ith PC of the standardized datamatrixZ¼½Z
1;Z2;/C1/C1/C1;Z m/C138is given byY i¼eT iZ End procedure
The obtained eigenvalues, eigenvectors and cumulative variabilityof the dataset are presented.Table 2provides the eigenvalues, while the eigenvectors among the features are summarized in Table 3. The variance of components is shown in Fig. 4.
Based on the results from the PCA method and for the purposeof improving the accuracy and speed up the models generationprocess, all the variables will be used in the next phase since thereis a strong correlation between all variables in the MNP collecteddataset.3.1.4. Apply PSO techniqueThe PSO method is used to divide the correlated data after usingthe PCA method into clusters, so that items in the same group areas similar as possible and items in different groups are as dissimilar218 S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224as possible. The PSO selects the best seeds for each cluster whereeach of them has 13 values and each value represents a feature.In order to get the shortest and largest distances betweenclusters, weights were given to each cluster and the traditionalEuclidean distance between the seeds and clusters was computed.Tables 4and5provide the initial and ﬁnal values of centroidclasses. Five optimal groups were generated by the PSO methodand the ﬁnal centroid classes are provided in Table 6.Table 1Correlation among the features.
Variables X Y MONTH DAY FMMC DMC DC ISI temp RH wind rain areaX10.540/C00.065 0.009/C00.021/C00.048/C00.086 0.006/C00.051 0.085 0.019 0.065 0.063 Y 0.5401/C00.066 0.046/C00.046 0.008/C00.101/C00.024/C00.024 0.062/C00.020 0.033 0.045 MONTH/C00.065/C00.06610.073 0.291 0.467 0.869 0.187 0.369 /C00.095/C00.086 0.013 0.056 DAY 0.009 0.046 0.073 10.081 0.052 0.054 0.028 0.085 /C00.145/C00.077 0.022 0.007 FFMC/C00.021/C00.046 0.291 0.081 10.383 0.331 0.532 0.432 /C00.301/C00.028 0.057 0.040 DMC/C00.048 0.008 0.467 0.052 0.383 10.682 0.305 0.470 0.074 /C00.105 0.075 0.073 DC/C00.086/C00.101 0.869 0.054 0.331 0.682 10.229 0.496/C00.039/C00.203 0.036 0.049 ISI 0.006/C00.024 0.187 0.028 0.532 0.305 0.229 10.394/C00.133 0.107 0.068 0.008 Temp/C00.051/C00.024 0.369 0.085 0.432 0.470 0.496 0.394 1/C00.527/C00.227 0.069 0.098 RH 0.085 0.062 /C00.095/C00.145/C00.301 0.074/C00.039
/C00.133/C00.52710.069 0.100/C00.076 Wind 0.019/C00.020/C00.086/C00.077/C00.028/C00.105/C00.203 0.107/C00.227 0.06910.061 0.012 Rain 0.065 0.033 0.013 0.022 0.057 0.075 0.036 0.068 0.069 0.100 0.061 1/C00.007 Area 0.063 0.045 0.056 0.007 0.040 0.073 0.049 0.008 0.098 /C00.076 0.012/C00.0071
Table 2Eigenvalues among the different features.
F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 1F13Eigenvalue 3.33 1.57 1.44 1.25 1.01 0.96 0.91 0.77 0.54 0.46 0.43 0.212 0.07Variability 25.61 12.08 11.14 9.62 7.82 7.42 6.99 5.94 4.19 3.56 3.34 1.634 0.58C. V. 25.61 37.70 48.84 58.46 66.29 73.72 80.72 86.67 90.87 94.44 97.7 99.42 100.0C. V. = Cumulative Variability.
Table 3Eigenvectors among the features.
F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13X/C00.067 0.679/C00.015/C00.055 0.032/C00.126 0.017 0.116 /C00.313/C00.214/C00.595/C00.047/C00.017 Y/C00.061 0.673/C00.017/C00.140/C00.011/C00.145 0.040 0.076 0.289 0.231 0.595 0.024 0.058 MONTH 0.408/C00.014 0.331/C00.096 0.069 0.024 0.193 0.378 /C00.368/C00.058 0.247 0.006 /C00.574 DAY 0.078 0.066 /C00.180/C00.280/C00.497 0.463 0.611 /C00.185 0.027/C00.067/C00.042 0.057 0.013 FFMC 0.359 0.060 /C00.296 0.255/C00.035/C00.121 0.033/C00.237/C00.367 0.675/C00.077 0.214 0.023 DMC 0.404 0.095 0.283 0.085 0.010 /C00.007 0.023/C00.235 0.556 0.194 /C00.317/C00.430/C00.229 DC 0.462/C00.015 0.371/C00.101 0.030/C00.006 0.068 0.168 /C00.095/C00.061 0.018/C00.005 0.770 ISI 0.292 0.098 /C00.272 0.463/C00.021/C00.174 0.081/C00.334
/C00.106/C00.567 0.293/C00.221 0.017 Temp 0.421 0.028 /C00.275/C00.134 0.002/C00.016/C00.258 0.126 0.377 /C00.252/C00.131 0.641/C00.108 RH/C00.183 0.107 0.620 0.259 /C00.057/C00.007 0.080/C00.452/C00.018/C00.079 0.012 0.528 /C00.065 Wind/C00.112 0.017/C00.069 0.611 0.218 0.142 0.462 0.477 0.253 0.061 /C00.101 0.132 0.069 Rain 0.039 0.170 0.083 0.348 /C00.416 0.572/C00.536 0.184/C00.078 0.033 0.069 /C00.088 0.004 Area 0.060 0.133 /C00.077/C00.118 0.721 0.596 /C00.039/C00.268/C00.077/C00.016 0.063 0.001 0.014
020406080100
00.511.522.533.5
F1 F2 F3 F4 F5 F6 F7 F8 F9 F10 F11 F12 F13
Cumula/g415ve variability (%)Eigenvalue 
AxisScreen plot
Fig. 4.The variance of components.S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224 2193.2. Apply the predictorsThe parameters for each predictor used in this work wereselected using trial and error method which can be a fundamentaltechnique in problem solving and obtaining knowledge. To evalu-ate the predictors and their performance based on the 5 qualitymeasures were used in this paper, the dataset was split into twoparts: 50% of the data were used for training phase while theremaining data were utilized for the testing phase.3.2.1. RBF networkThe main parameters of this predictor include: the number ofneurons = 100, the minimum radius = 0.01, the maximum radius= 519.669, the minimum Lambda = 0.01328, the maximum Lambda= 9.95337 and Regularization Lambda for ﬁnal weights =6.3458e/C0005 after 14 iterations.3.2.2. Multilayer perceptron NNThe main parameters of this predictor include: the number ofinputs = 12, the number of layers = 3 (1 hidden), hidden layer 1neurons: search from 2 to 20, the hidden layer activation functionis logistic, the output layer activation function is linear, the min.weight of hidden layer =/C03.636e/C0001, the max. Weight of hiddenlayer = 3.409e/C0001; the min. weight of output layer = 9.638e /C0005, and the max. Weight of output layer = 1.358e /C0001.3.2.3. Support vector machine (SVM)The main parameters of this predictor include: the kernel func-tion is radial basis function (RBF), the Epsilon = 0.001, C =84.1830278, Gamma = 3800.28665 and =6.4728203.3.2.4. Polynomial neural networkThe main parameters of this predictor include: activation kernelfunction = Gaussian (one variable) y = p1 + p2 ⁄exp (/C0((x1/C0p3) ^2)/p4), p1 = 31.30268, p2 = /C027.3003, p3 = 1.137834, p4 = 6.307772 and output area = 31.30268–27.3003 ⁄exp (/C0((temp + 1.137834)^2)/6.307772)3.2.5. Cascade correlation networkThe main parameters of this predictor include: the minimumneurons in hidden layer is 0, the maximum neurons in hidden layeris 50, the hidden neuron kernel are sigmoid and Gaussian func-tions, the output neuron kernel function: Sigmoid, the number ofneurons in input layer = 12, the number of neurons in hiddenlayer = 2 (Sigmoid neurons = 1 and Gaussian neurons = 1), theminimum weight in hidden layer = /C025.040615, the maximum weight in hidden layer = 13.892605, the number of neurons in out-put layer = 1, the minimum weight in output layer = /C00.217332 and the maximum weight in output layer = 0.150124.Table 4Initial centroid classes.
Class X Y MONTH DAY FFMC DMC DC ISI Temp RH Wind Rain Area1 4.491 4.296 7.389 3.324 90.402 106.962 537.491 9.244 18.483 45.981 4.033 0.011 7.5042 4.929 4.347 7.571 3.837 90.224 118.626 553.101 9.241 18.905 44.643 3.946 0.002 14.1393 4.179 4.221 7.632 3.979 91.023 117.019 569.926 9.005 19.122 43.284 4.053 0.008 20.4724 5.055 4.418 7.418 3.509 90.597 109.219 548.775 8.348 18.740 45.582 3.741 0.071 15.7945 4.651 4.208 7.396 3.830 90.991 103.895 533.243 9.306 19.234 41.792 4.324 0.011 7.206
Table 5Final centroid classes.
Class X Y MONTH DAY FFMC DMC DC ISI Temp RH Wind Rain Area Sum of weights Within class variance1 4.92 4.42 3.06 3.32 86.72 26.69 68.01 6.47 12.28 46.20 4.58 0.00 5.38 89.00 2154.202 4.43 4.17 8.15 3.99 91.73 127.75 626.58 10.80 21.11 43.53 4.08 0.05 10.68 149.00 4438.873 4.48 4.20 8.79 3.60 91.73 140.85 734.60 8.97 20.18 44.21 3.61 0.01 8.93 217.00 6809.944 5.46 4.73 7.53 3.70 89.71 83.36 384.76 8.47 18.24 44.18 4.48 0.00 13.22 60.00 9643.065 7.00 5.50 8.50 4.50 93.65 171.75 686.50 11.25 26.30 27.00 4.45 0.00 918.56 2.00 64819.93
Table 6Distance between the centroid classes.
12 3 4 51 000.000 567.812 676.403 321.983 1112.7102 567.812 000.000 108.840 245.932 911.0793 676.403 108.840 000.000 354.582 911.6104 321.983 245.932 354.582 000.000 958.5835 1112.710 911.079 911.610 958.583 000.000
Table 7Comparison among different predictors based on the quality measures.
Predictor Testing phase Training phaseRMSE MSE RAE MAE IG RMSE MSE RAE MAE IGCNN 62.6 3926.7 21.9 551.4 1.803 68.3 4665.5 25.2 673.9 1.174MPNN 63.1 3993.2 18.5 617.4 1.419 63.8 4076.4 19.6 652.2 1.146PNN 63.2 3996.5 17.5 516.5 1.508 63.7 4069.3 17.2 466.0 0.982RBF 54.2 2939.9 23.7 911.2 2.160 68.1 4638.2 26.2 887.9 1.077SVM54.0 2926.4 10.5 282.4 2.656 63.5 4042.0 18.3 502.0 0.817220 S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224Fig. 5.Evaluation the predictors based on the quality measures, (a)–(d) and (e). Comparison between predictors and the quality measures.S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224 2213.3. Performance analysisThe results inTable 7show that SVM predicts ﬁre probabilitywell. A comparison of SVM results with other predictors revealsthat the SVM outperforms the RBF, MPNN, PNN and CCN predic-tors. SVM has the smallest RMSE of 54.0, MSE of 2926.4, RAE of10.5, and MAE of 2.656 and the highest IG of 2.656 in the testingstage. In general, the results indicate that SVM has the best predic-tion ability for forest ﬁre compared to other selected SC methods.Results of the ﬁve SC for forest ﬁres prediction based on eachstatistical measure are shown inFig. 5. According to the bar charts, the SVM has achieved the highest gain in (e) with minimal error in(a, b, c and d). Therefore, the SVM algorithm is the best predictorand it has the capability to effectively predict forest ﬁres comparedto other predictors. Similar results were obtained by the work of[16,61]. The difference between the expected and actual valuesfor the MNP dataset that generated by the best predictor SVM tech-niques provided inFig. 6. Green color represents the predicted val-ues while the black color presents the actual values.4. ConclusionIn this article, several SC techniques were applied on MNP data-set to determine the effective predictor that would potentially givemore accurate results for forest ﬁres. The PCA was used to capturethe correlation between features and PSO technique was applied toﬁnd the number of clusters existing in the resulted data after usingthe PCA technique. In the next stage, the obtained clustered datawas used as inputs in the SC techniques. The quality measures usedin this work make it obvious that SVM attained best results with ahigh level of accuracy compared to other methods. The ﬁndings in
Fig. 5(continued)
Fig. 6.Predicted values of burnt area based on the best predictor (SVM).222 S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224this study would assist and provide additional aids for systemsoperators’ to monitor their system effectively.References
[1] W. Aertsen, V. Kint, J. Van Orshoven, K. Ozkan, B. Muys, Performance ofmodelling techniques for the prediction of forest site index: a case study forpine and cedar in the Taurus mountains. Turkey XIII World Forestry Congress,2009, pp. 18–23.[2]
A.M. Al-Bakary, H.A. Samaher, Data construction using genetic programmingmethod to handle data scarcity problem, Int. J. Adv. Comput. Technol. (2010)
. [3]
G. Amatulli, A. Camia, J. San-Miguel-Ayanz, Estimating future burned areasunder changing climate in the EU-Mediterranean countries, Sci. Total Environ.450 (2013) 209–222
.[4] V.D. Anezakis, K. Demertzis, L. Iliadis, S. Spartalis, A hybrid soft computingapproach producing robust forest ﬁre risk indices, in: IFIP InternationalConference on Artiﬁcial Intelligence Applications and Innovations, SpringerInternational Publishing, 2016, pp. 191–203.[5] T. Artés, A. Cencerrado, A. Cortés, T. Margalef, Time aware genetic algorithm forforest ﬁre propagation prediction: exploiting multi-core platforms,Concurrency and Computation: Practice and Experience, 2016.[6]
Y.E. Aslan, I. Korpeoglu, Ö. Ulusoy, A framework for use of wireless sensornetworks in forest ﬁre detection and monitoring, Comput. Environ. Urban Syst.36 (6) (2012) 614–625
.[7]
E. Assareh, M.A. Behrang, M.R. Assari, A. Ghanbarzadeh, Application of PSO(particle swarm optimization) and GA (genetic algorithm) techniques ondemand estimation of oil in Iran, Energy 35 (12) (2010) 5223–5229
. [8]
I. Basheer, A. Hajmeer, M, Artiﬁcial neural networks: fundamentals computingdesign and application, J. Microbiol. Methods 43 (1) (2000) 3–31
. [9]
J. Bedia, S. Herrera, A. Camia, J.M. Moreno, J.M. Gutiérrez, Forest ﬁre dangerprojections in the Mediterranean using ENSEMBLES regional climate changescenarios, Climatic Change 122 (1–2) (2014) 185–199
. [10]
M. Bisquert, E. Caselles, J.M. Sánchez, V. Caselles, Application of artiﬁcial neuralnetworks and logistic regression to the prediction of forest ﬁre danger inGalicia using MODIS data, Int. J. Wildland Fire 21 (8) (2012) 1025–1029
. [11]
K. Bouabdellah, H. Noureddine, S. Larbi, Using wireless sensor networks forreliable forest ﬁres detection, Proc. Comput. Sci. 19 (2013) 794–801
. [12] D.S. Broomhead, D. Lowe, Radial basis functions, multi-variable functionalinterpolation and adaptive networks (No. RSRE-MEMO-4148). Royal signalsand radar establishment Malvern (United Kingdom), 1988.[13]
D.T. Bui, Q.T. Bui, Q.P. Nguyen, B. Pradhan, H. Nampak, P.T. Trinh, A hybridartiﬁcial intelligence approach using GIS-based neural-fuzzy inference systemand particle swarm optimization for forest ﬁre susceptibility modeling at atropical area, Agric. For. Meteorol. 233 (2017) 32–44
. [14]
D.K. Chaturvedi, Soft Computing: Techniques and Its Applications in ElectricalEngineering, Springer, 2008
. [15]
T. Cheng, J. Wang, Integrated spatio-temporal data mining for forest ﬁreprediction, Trans. GIS 12 (5) (2008) 591–611
. [16]
P. Cortez, A.D.J.R. Morais, A data mining approach to predict forest ﬁres usingmeteorological data, in: Proceedings of the 13th Portuguese Conference onArtiﬁcial Intelligence (EPIA) Guimarães Portugal, 2007, pp. 512–523
. [17]
A. De Angelis, C. Ricotta, M. Conedera, G.B. Pezzatti, Modelling themeteorological forest ﬁre niche in heterogeneous pyrologic conditions, PloSone 10 (2) (2015) e0116875
. [18]
M. Denham, A. Cortés, T. Margalef, E. Luque, Applying a dynamic data drivengenetic algorithm to improve forest ﬁre spread prediction, in: InternationalConference on Computational Science, Springer Berlin Heidelberg, 2008, pp.36–45
.[19]
H. Du, N. Zhang, Time series prediction using evolving radial basis functionnetworks with new encoding scheme, Neurocomputing 71 (7) (2008) 1388–1400
.[21] S.E. Fahlman, C. Lebiere, The cascade-correlation learning architecture,Gardner (1990).[22]
M.W. Gardner, S.R. Dorling, Artiﬁcial neural networks (the multilayerperceptron)—a review of applications in the atmospheric sciences, Atmos.Environ. 32 (14) (1998) 2627–2636
. [23]
A. Felber, P. Bartelt, The use of the Nearest Neighbour Method to predict forestﬁres, Proceedings of 4th International Workshop on Remote Sensing and GISApplications to Forest Fire Management: Innovative Concepts and Methods inFire Danger Estimation, Ghent, Belgium, 2003
. [24]
Y.J. Goldarag, A. Mohammadzadeh, A.S. Ardakani, Fire risk assessment usingneural network and logistic regression, J. Indian Soc. Rem. Sens. (2016) 1–10
. [25]
J. Han, K. Ryu, K. Chi, Y. Yeon, Statistics based predictive geo-spatial datamining: forest ﬁre hazardous area mapping application, Web Technol. Appl.(2003), 602-602
.[26]
M. Hefeeda, M. Bagheri, Forest ﬁre modeling and early detection using wirelesssensor networks, Ad Hoc & Sensor Wireless Networks 7 (3–4) (2009) 169–224
. [27]
H. Hong, S.A. Naghibi, M.M. Dashtpagerdi, H.R. Pourghasemi, W. Chen, Acomparative assessment between linear and quadratic discriminant analyses(LDA-QDA) with frequency ratio and weights-of-evidence models for forestﬁre susceptibility mapping in China, Arab. J. Geosci. 10 (7) (2017) 167
. [28]
H. Hotelling, Analysis of a complex of statistical variables into principalcomponents, J. Educ. Psychol. 24 (6) (1933) 417–441
.[29]S. Hussein Ali, Designing a Software for Classifying Objects for Air Photos &Satellite Images using Soft Computing M.Sc. Thesis, Babylon University, 2005
. [30]
S. Jain, M.P.S. Bhatia, Performance investigation of support vector regressionusing meteorological data, Int. J. Database Theory Appl. 6 (4) (2013) 109–118
. [31]
M. Jiang, Y.P. Luo, S.Y. Yang, Stochastic convergence analysis and parameterselection of the standard particle swarm optimization algorithm, Inform.Process. Lett. 102 (1) (2007) 8–16
. [32]
T. Kavzoglu, P.M. Mather, The use of backpropagating artiﬁcial neuralnetworks in land cover classiﬁcation, Int. J. Rem. Sens. 24 (23) (2003) 4907–4938
.[33]
J. Kennedy, R. Eberhart, Particle swarm optimization, Proceedings of IEEEInternational Conference on Neural Networks (1995) 1942–1948
. [34]
B.C. Ko, K.H. Cheong, J.Y. Nam, Fire detection based on vision sensor andsupport vector machines, Fire Saf. J. 44 (3) (2009) 322–329
. [35]
J. Liu, Radial Basis Function (RBF) Neural Network Control for MechanicalSystems: Design, Analysis and Matlab Simulation, Springer Science & BusinessMedia, 2013
.[36]
E.E. Maeda, A.R. Formaggio, Y.E. Shimabukuro, G.F.B. Arcoverde, M.C. Hansen,Predicting forest ﬁre in the Brazilian Amazon using MODIS imagery andartiﬁcial neural networks, Int. J. Appl. Earth Obs. Geoinf. 11 (4) (2009) 265–272
.[37]
E. Mahdipour, C. Dadkhah, Automatic ﬁre detection based on soft computingtechniques: review from 2000 to 2010, Artif. Intell. Rev. 42 (4) (2014) 895–934
.[38] A.G. Motazeh, E.F. Ashtiani, R. Baniasadi, F.M. Choobar, Rating and mappingﬁre hazard in the hardwood Hyrcanian forests using GIS and expert choicesoftware, Acknowledgement to Reviewers of the Manuscripts Submitted toForestry Ideas in 2013, p. 141.[39]
R. Neruda, P. Kudová, Learning methods for radial basis function networks,Future Gener. Comput. Syst. 21 (7) (2005) 1131–1142
. [40]
S.N. Og˘ulata, C. S/C223ahin, R. Erol, Neural network-based computer-aided diagnosisin classiﬁcation of primary generalized epilepsy by EEG signals, J. Med. Syst. 33(2) (2009) 107–112
.[41]
S.K. Oh, W. Pedrycz, The design of self-organizing polynomial neural networks,Inf. Sci. 141 (3) (2002) 237–258
. [42]
S.K. Oh, W. Pedrycz, B.J. Park, Polynomial neural networks architecture:analysis and design, Comput. Electr. Eng. 29 (6) (2003) 703–725
. [43]
J.A. Olivas, Forest ﬁre prediction and management using soft computing, in:Proceedings of the International Conference on Industrial Informatics (INDIN),2003, pp. 338–344
.[44]
S. Oliveira, F. Oehler, J. San-Miguel-Ayanz, A. Camia, J.M. Pereira, Modelingspatial patterns of ﬁre occurrence in Mediterranean Europe using MultipleRegression and Random Forest, Ecol. Manage. 275 (2012) 117–129
. [45]
A.M. Özbayog˘lu, R. Bozer, Estimation of the burned area in forest ﬁres usingcomputational intelligence techniques, Proc. Comput. Sci. 12 (2012) 282–287
. [46]
K.S.N. Prasad, S. Ramakrishna, An autonomous forest ﬁre detection systembased on spatial data mining and fuzzy logic, Int. J. Comput. Sci. NetworkSecur. 8 (12) (2008) 49–55
. [47] D.E. Rumelhart, G.E. Hinton, R.J. Williams, Learning internal representations byerror propagation (No. ICS-8506), California Univ San Diego La Jolla Inst forCognitive Science, 1985.[48]
Y. Saﬁ, A. Bouroumi, Prediction of forest ﬁres using artiﬁcial neural networks,Appl. Math. Sci. 7 (6) (2013) 271–286
. [49]
G.E. Sakr, I.H. Elhajj, G. Mitri, Efﬁcient forest ﬁre occurrence prediction fordeveloping countries using two weather parameters, Eng. Appl. Artif. Intell. 24(5) (2011) 888–894
.[50]
O. Satir, S. Berberoglu, C. Donmez, in: Mapping Regional Forest Fire ProbabilityUsing Artiﬁcial Neural Network Model in a Mediterranean Forest Ecosystem,Geomatics, Natural Hazards and Risk, 2015, pp. 1–14
. [51]
M. Shourian, S.J. Mousavi, A. Tahershamsi, Basin-wide water resourcesplanning by integrating PSO algorithm and MODSIM, Water Resourc.Manage. 22 (10) (2008) 1347–1366
. [52]
J.K. Spoerre, Application of the cascade correlation algorithm (CCA) to bearingfault classiﬁcation problems, Comput. Ind. 32 (3) (1997) 295–304
. [53] J.J. Storer, Computational Intelligence and Data Mining Techniques Using theFire Data Set (MS Thesis), Bowling Green State University.[54] J. Storer, R. Green, PSO trained Neural Networks for predicting forest ﬁre size: acomparison of implementation and performance, in: Neural Networks (IJCNN),2016 International Joint Conference on, 2016, pp. 676–683.[55]
A. Subasi, EEG signal classiﬁcation using wavelet feature extraction and amixture of expert model, Expert Syst. Appl. 32 (4) (2007) 1084–1093
. [57]
C.E. Van Wagner, P. Forest, Development and Structure of the Canadian ForestFireWeather Index System, For. Serv., Forestry Tech. Rep, in Can, 1987
. [58]
V.N. Vapnik, The Nature of Statistical Learning, Theory Springer-Verlag, NewYork, USA, 1995
.[60]
A.M. West, S. Kumar, C.S. Jarnevich, Regional modeling of large wildﬁres undercurrent and potential future climates in Colorado and Wyoming, USA, ClimaticChange 134 (4) (2016) 565–577
. [61] D.W. Xie, S.L. Shi, Prediction for burned area of forest ﬁres based on SVMmodel, in: Applied Mechanics and Materials, vol. 513, Trans Tech Publications,2014, pp. 4084–4089.[62]
L. Yang, C.W. Dawson, M.R. Brown, M. Gell, Neural network and GA approachesfor dwelling ﬁre occurrence prediction, Knowl. – Based Syst. 19 (4) (2006)213–219
.S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224 223[63]H. Yu, T. Xie, S. Paszczynski, B.M. Wilamowski, Advantages of radial basisfunction networks for dynamic system design, IEEE Trans. Industr. Electron. 58(12) (2011) 5438–5450
.[64] T. Yu, L. Wang, X. Han, Y. Liu, L. Zhang, Swarm Intelligence OptimizationAlgorithms and Their Application. WHICEB 2015 Proceedings, 2015, p. 3.[65]
C. Yuan, Y. Zhang, Z. Liu, A survey on technologies for automatic forest ﬁremonitoring detection and ﬁghting using unmanned aerial vehicles and remotesensing techniques, Can. J. Forest Res. 45 (7) (2015) 783–792
.[66]L.A. Zadeh, Soft computing and fuzzy logic, IEEE Software 11 (6) (1994) 48 . [67]
J. Zhao, Z. Zhang, S. Han, C. Qu, Z. Yuan, D. Zhang, SVM based forest ﬁredetection using static and dynamic features, Computer Sci. Inform. Syst. 8 (3)(2011) 821–841
.[68]
L. Zjavka, Wind speed forecast correction models using polynomial neuralnetworks, Renew. Energy 83 (2015) 998–1006
.224 S. Al_Janabi et al. / Applied Computing and Informatics 14 (2018) 214–224