ORIGINAL ARTICLE
Eﬀective and precise face detectionbased on color and depth data
Loris Nannia,*, Alessandra Luminib, Fabio Dominioa,Pietro Zanuttigh
a
aDEI, University of Padova, via Gradenigo, 6, 35131 Padova, Italy
bDISI, University of Bologna, via Venezia, 52, 47521 Cesena, ItalyReceived 3 February 2014; revised 31 March 2014; accepted 7 April 2014
Available online 21 April 2014
KEYWORDSFace detection;Skin detection;Depth map;Viola–jones detectorAbstract In this work an effective face detector based on the well-known Viola–Jones algorithm is proposed. A common issue in face detection is that for max-imizing the face detection rate a low threshold is used for classifying as face aninput image, but at the same time using a low threshold drastically increases thenumber of false positives. In this paper several criteria are proposed for reducingfalse positives: (i) a skin detection step is used to reject a candidate face regionthat does not contain the skin color, (ii) the size of the candidate face region iscalculated according to the depth data, removing the too small or the too largefaces, (iii) images of ﬂat objects (e.g. candidate face found in a wall) or unevenobjects (e.g. candidate face found in the leaves of a tree) are removed usingthe depth map and a segmentation approach based both on color and depthdata.
The above criteria permit to drastically reduce the number of false positives with-out decreasing the detection rate. The proposed approach has been validated onthree datasets composed of 180 samples including both 2D and depth images.The face position inside samples has been manually labeled for testing.
*Corresponding author. Tel.: +39 3493511673.E-mail address:nanni@dei.unipd.it(L. Nanni).Peer review under responsibility of King Saud University.
Production and hosting by ElsevierApplied Computing and Informatics (2014) 10, 1–13
King Saud University
Applied Computing and Informatics
www.ksu.edu.sawww.sciencedirect.com
2210-8327ª2014 King Saud University. Production and hosting by Elsevier B.V. All rights reserved.http://dx.doi.org/10.1016/j.aci.2014.04.001A Matlab version of the system for face detection and the full testing dataset willbe freely available fromhttp://www.dei.unipd.it/node/2357.
ª2014 King Saud University. Production and hosting by Elsevier B.V. All rights reserved.
1. Introduction
Face detection has attracted the attention of many research groups due to itswidespread application in many ﬁelds as surveillance and security systems, ashuman–computer interface, face tagging, behavioral analysis, content-based imageand video indexing, and many others (Zeng et al., 2009). Face detection is the ﬁrst crucial step for facial analysis algorithms (i.e. face recognition/veriﬁcation, headtracking, and facial expression recognition) whose goal is to determine whetheror not faces are present in an image and eventually return their location and extent(i.e. a bounding box). It is a more challenging problem than face localization inwhich a single face is assumed to be inside an image.
M o s to ft h el i t e r a t u r ei nt h i sﬁ e l dd e a l sw ith frontal face detection from two-dimen-sional (2D) images: the problem is often formulated as a two-class pattern recognitionproblem aimed at classifying each sub-window of a given size of the input image aseither containing or not containing a face (Jin et al., 2004). Then the classiﬁcation is performed by common technologies for 2Dfacial recognition such as Eigenface,Fisherface, waveletface, PCA (Principal Component Analysis), LDA (Linear Dis-criminant Analysis), Haar wavelet transform, and so on. The Viola–Jones detector(Viola and Jones, 2001) is probably the most famous approach for frontal 2D detec-tion: it involves exhaustively searching an entire image for faces, with multiple scalesexplored at each pixel using Haar-like rectangle features boosting classiﬁcation. Twodifferent face detection strategies based on slightly modiﬁed Viola–Jones are proposedinAnisetti (2009).I nKu¨blbeck and Ernst, 2006boosting has also been used in con-junction with Modiﬁed census transform (MCT), to improve illumination invariance.In (Huang et al., 2007) a method able to detect faces witharbitrary rotation-in-plane and rotation–off-plane angles in still images or video sequences is proposed. In(Jianxin et al., 2008) is designed a classiﬁer that explicitly addresses the difﬁcultiescaused by the asymmetric learning goal (th em i n o r i t yc l a s si st h ef a c ec l a s s ) .
Despite the success of these and of several other methods, designed to provideaccurate detection performance under variable conditions ( Zhang and Zhang, 2010), most of difﬁculties in precise face detection still arises in the presence ofillumination changes and occlusions. One possible way to improve algorithmsfor face detection is to incorporate models of the image processing that efﬁcientlyintegrates multiple cues, such as stereo disparity, texture and motion.
For example, Microsoft Kinect is a depth sensing device that couples the 2DRGB image with a depth map (RGB-D) which can be used to determine the depthof every object in the scene. Each pixel in Kinect’s depth map has a value2 L. Nanni et al.indicating the relative distance of that pixel from the sensor at the time of imagecapture. Depth information captured by Kinect is not useful to differentiateamong different individuals at a distance, due to its very high inter-class similarity,but thanks to its low intra-class variation may be useful to improve the robustnessof a face detector by reducing sensitivities to illumination, occlusions, changing ofexpression and pose. Kinect devices have been extremely popular recently, due totheir low-cost and availability, and the ﬁrst benchmark datasets have beencollected for 3D face recognition (Tsalakanidou et al., 2003) or detection (Hg et al., 2012).
Several recent approaches use depth map or other 3D information for facedetection. For instance, the classic Viola–Jones face detection algorithm isextended in (Dixon et al., 2007; Burgin et al., 2011) to simultaneously considerdepth and color information in face detection. In (Shieh and Hsieh, 2013) Haar wavelets on 2D are ﬁrst used to detect the human face and then its position isreﬁned by structured light analysis. Other depth-based detectors are proposed inShotton et al. (2011), Mattheij et al. (2012): the approach byShotton et al. (2011)employs depth-comparison features deﬁned as pixel pairs in depth images,to quickly and accurately classify body joints and parts from single depth images,a similar method based on square regions comparison is coupled to Viola Jonesface detector inMattheij et al. (2012)for robust and accurate face detection. In(Jiang et al., 2013) biologically inspired integrated representation of texture andstereo disparity information is used for a multi-view facial detection task withthe valuable result of improved detection performance and reduced computationalcomplexity. Disparity information extracted from stereo images allows to stronglyreduce the number of locations to be evaluated during the search process. In(Goswami et al., 2013) the authors use the additional information obtained bythe depth map to improve face recognition: their approach extracts texturaldescriptors (the Histogram of Oriented Gradients) from four entropy maps corre-sponding to RGB and depth information with varying patch sizes and uses Ran-dom Forest as a classiﬁer. Another face recognition approach designed speciﬁcallyfor low resolution 3D sensors is proposed inLi et al. (2013), which uses efﬁcient Iterative Closest Point method and facial symmetry for estimating a canonicalfrontal view from non-frontal views.
This work, similar to other approaches proposed in the literature ( Anisetti et al., 2008), is aimed at using depth information to reduce the number of false positivedetections and improve the percentage of correct detections. In ( Anisetti et al., 2008) the authors use a 2D multi-step algorithm to obtain a coarse-to-ﬁne classiﬁ-cation, then reﬁne the quality of the face location by a 3D tracking approach.
In this paper an effective and precise face detector designed for upright andfrontal faces is presented based on both gray-level image and depth map: depthinformation is used to ﬁlter the regions of the image where a candidate face regionis found by the Viola–Jones (VJ) detectorViola and Jones, 2001. A main drawback of VJ is that several false positives occur setting a low threshold for faceEffective and precise face detection based on color and depth data 3classiﬁcation; in this work several criteria, mainly evaluated on the depth map, areused to drastically reduce the number of false positives:
- the ﬁrst ﬁltering rule is deﬁned on the color of the region; since some false positiveshave colors not compatible with the face (e.g. shadows on jeans) a skin detector isapplied to remove the candidate face regions that do not contain skin pixels;- the second ﬁltering rule is deﬁned on the size of the face: using the depth mapit is quite easy to calculate the size of the candidate face region, which is use-ful to discard smallest and largest faces from the ﬁnal result set;- the third ﬁltering rule is deﬁned on the depth map to discard ﬂat objects (e.g.candidate faces found in a wall) or uneven objects (e.g. candidate face foundin the leaves of a tree). Combining color and depth data the candidate faceregion can be extracted from the background and measures of depth and reg-ularity are used for ﬁltering out false positives.
Unfortunately, in the literature there are no freely available large datasets forface detection (with difﬁcult images as complex background, the datasets usedin (Shieh and Hsieh, 2013; Mattheij et al., 2012) are quite easy) that contains boththe color and the depth map. There are several datasets for face recognition usingthe depth map, but the face detection step in those datasets is easy. Therefore, theproposed approach has been evaluated on a collected dataset that will be madefreely available for further comparisons.
2. Base face detector
The proposed method is based on the widely used VJ face detector ( Viola and Jones, 2001), characterized by a slow training, but very fast classiﬁcation. VJinvolves a very simple image representation, based on Haar wavelets, an integralimage for rapid feature detection, the AdaBoost machine-learning method forselecting a small number of important features, and a cascade combination of weaklearners for classiﬁcation. The detection performance of VJ strictly relies on thethreshold used to classify an input region as face, this value deﬁnes the criteria todeclare a ﬁnal face detection in an area where there are multiple detections aroundan object. Groups of candidate face regions that meet the threshold are merged toproduce one bounding box around the target object. Increasing this threshold mayhelp suppress false detections by requiring that the target object be detected multi-ple times during the multiscale detection phase. Since the original VJ implementa-tion is designed for upright frontal image, in order to handle non upright faces inthe work, the original images are also rotated of {20/C176,/C020/C176} before detection.
In the complete system, outlined inFigure 1, ﬁrst the VJ detector is applied toinput and rotated images using a low classiﬁcation threshold, then all thecandidate face regions are ﬁltered out according to three criteria (detailed in thefollowing sub-sections) with the aim of reducing false positives:4 L. Nanni et al.- skin detection;- size of the image;- ﬂatness/unevenness of the image.
Figure 2shows the result of the three ﬁltering steps on two sample images.
Depth data acquired by the Kinect are projected over the color images contain-ing the faces in order to obtain a set of aligned color images and depth maps. Forthis purpose, the calibration data for the depth and color cameras of the Kinect arecomputed using the method proposed inHerrera et al. (2012). This approach com- putes both the intrinsic parameters of the depth and color cameras and the extrinsicparameters between the two cameras. The depth samples positions of the image inthe 3D space are ﬁrst computed by using the intrinsic parameters of the depth cam-era and the 3D samples are then reprojected in the 2D color image reference systemby using both the color camera intrinsic parameters and the extrinsic ones. By theend of this procedure, a color and a depth value are associated to each sample.
2.1. Skin detection ﬁlter
The presence of skin is a good indicator to assert the detection of a face. In thiswork a skin ﬁlter is applied to candidate face regions which is a simpliﬁed versionof the ensemble proposed inNanni et al. (2013). It is the combination by the sumrule of the methods proposed inJones (2002), O´Conaire et al. (2007)with three other skin detectors based on the idea proposed inKhan et al. (2011); after the
VJ Face detector(low threshold)
Size ﬁltered regions ﬂatness\unevennessﬁltered regions
Input image ()±20° Rotated images
Depth map alignmentSet of candidate face regions
Skin ﬁlteredregions3 steps ﬁlteringDetected faces
Figure 1Outline of our complete system.Effective and precise face detection based on color and depth data 5training phase performed according to the above cited approaches, pixels areclassiﬁed by a set of lookup tables, built using SVMs trained considering differentfeatures (i.e., different pre-processing and color spaces):
/C15max-RGB color constancy and RGB color space;/C15max-RGB color constancy and YUV
1color space;/C15RGB color space.
The color constancy problem is the ability to estimate the unknown light of ascene from an image. The max-RGB color constancy approach is based on theassumption that the reﬂectance which is achieved for each of the three colorchannels is equal (van de Weijer et al., 2007).
Since lookup tables2are used for the classiﬁcation task this method allows toclassify skin pixels of a given image in real time. Please note that since lookuptables have been calculated from several large datasets ( Nanni et al., 2013), the system is not over-trained in the dataset tested in this paper.
2.2. Filter by the size of the image
The size criteria simply remove the candidate faces not included in a ﬁxed rangesize ([12.5,30] cm). The size of a candidate face region is extracted from the depthmap according to the following approach.
Assuming that the face detection algorithm returns the 2D position anddimension in pixels (w
2D,h2D) of a candidate face region, its 3D physicaldimension in mm (w
3D;h3D) can be estimated as:
Candidate face region ﬁltered by skin  
True posi/g415ve discovered only in the rotated version of the input image 
Candidate face regions ﬁltered by size 
Candidate face regions ﬁltered by size 
 Candidate face region ﬁltered by low std 
True faces correctly labelled 
Figure 2Result of the ﬁltering steps on some sample images.
1The color space conversion is performed using the ‘‘Colorspace’’ Matlab toolbox http://www.math- works.com/matlabcentral/ﬁleexchange/28790-colorspace-transformations .
2A lookup table is the result of the pre-calculation by SVM of classiﬁcation scores of all the combinations ofpixel values (2563) from a given color space.6 L. Nanni et al.w3D¼w 2D/C22df
x
h3D¼h 2D/C22df
y
wheref xandf yare the Kinect camera focal lengths computed by the calibration
algorithm ofHerrera et al. (2012)and /C22dis the average depth of the samples withinthe face candidate bounding box. Note how
/C22dis indeed deﬁned as the median ofthe depth samples in order to reduce the impact of noisy samples in the averagecomputation.
2.3. Filter by ﬂatness/unevenness of the image
Another signiﬁcant information that can be obtained from the depth map is theﬂatness/unevenness of the candidate face regions. For this ﬁlter ﬁrst a segmenta-tion procedure is applied, then from each face candidate region the standard devi-ation (std) of the pixels of the depth map that belongs to the larger segment iscalculated. Regions having astdout of a ﬁxed range [0.15,4] are removed.
The segmentation of both color and depth map is performed according to theapproach ofDal Mutto et al. (2012). This segmentation scheme is based on thenormalized cuts spectral clustering algorithm (Shi and Malik, 2000) and jointly exploits the geometry and color information for optimal performances.
The basic architecture of the segmentation scheme is shown in Figure 3. The procedure has two main stages: ﬁrstly a six-dimensional representation of thescene samples is built from the geometry and color data and then the obtainedpoint set is segmented using spectral clustering.
Each sample in the acquired depth map corresponds to a 3D point of the scenep
i;i¼1;...;N. After the joint calibration of the depth and color cameras it is pos-sible to compute the 3D coordinatesx,yandzofp
iand to associate to it a 3-Dvector containing theR,G, andBcolor components. Geometry and color thenneed to be uniﬁed in a meaningful way. Color values are converted to a perceptu-ally uniform space in order to give a perceptual signiﬁcance to the distancebetween colors that will be used in the clustering algorithm. The CIELab spacehas been used for this purpose, i.e., the color information of each scene point isthe 3-D vector:
pci¼LðpiÞaðp
iÞbðp
iÞ264375
;i¼1;...;N
The geometry is simply represented by the 3-D coordinates of each point, i.e.:
pgi¼xðpiÞyðp
iÞzðp
iÞ264375
;i¼1;...;NEffective and precise face detection based on color and depth data 7The scene segmentation algorithm should be insensitive to the relative scaling ofthe point-cloud geometry and should bring geometry and color distances into aconsistent framework. Therefore all the components of p
giare normalized with respect to. the average of the standard deviations of the point coordinatesr
g¼ðr xþr yþr zÞ=3. The adopted geometry representation is thus the vector:
/C22xðpiÞ/C22yðp
iÞ/C22zðp
iÞ264375
¼3r
xþr yþr zxðpiÞyðp
iÞzðp
iÞ264375
¼1r
gxðpiÞyðp
iÞzðp
iÞ264375
In order to balance the relevance of color and geometry in the merging process,
the color information vectors are also normalized by the average of the standarddeviations of theL,aandbcomponents. The ﬁnal color representation therefore is:
/C22LðpiÞ/C22aðp
iÞ/C22bðp
iÞ264375
¼3r
Lþr aþr bLðpiÞaðp
iÞbðp
iÞ264375
¼1r
cLðpiÞaðp
iÞbðp
iÞ264375
From the above normalized geometry and color information vectors, each point
is ﬁnally represented as
pfi¼/C22LðpiÞ/C22aðp
iÞ/C22bðp
iÞk
/C22xðpiÞk
/C22yðpiÞk
/C22zðpiÞ26666666643777777775
wherekis a parameter balancing the contribution of color and geometry. High
values ofkincrease the relevance of geometry, while low values of kincrease the relevance of color information. A complete discussion on the effect of thisparameter and on how to automatically set it to the optimal value is presentedinDal Mutto et al. (2012).
Figure 3Architecture of the proposed segmentation scheme.8 L. Nanni et al.The computed vectorspfiare then clustered in order to segment the acquiredscene. Among the various clustering techniques, methods based on pairwise afﬁn-ity measures computed between all the possible couples of points allows to obtainvery accurate and robust results because they do not assume a Gaussian model forthe distribution of the points. On the other side they have the drawback that theyneed to compare all the possible pairs of points and are so very expensive in termsof both CPU and memory resources. Normalized cuts spectral clustering ( Shi and Malik, 2000) is an effective example of this family. This method is based on thepartition of a graph representing the scene according to spectral graph theory cri-teria. The minimization is done using normalized cuts and accounts both for thesimilarity between the pixels inside the same segment and the dissimilarity betweenthe pixels in different segments. The minimization problem is very computation-ally expensive and several methods have been proposed for its efﬁcient approxima-tion. In the method based on the integral eigenvalue problem proposed in Fowlkes et al. (2004), the set of points is ﬁrst randomly subsampled and then the subset ispartitioned and the solution is propagated to the whole points set by a speciﬁctechnique called the Nystro¨m method. In order to avoid small regions due to noisea ﬁnal reﬁnement stage removing regions smaller than a pre-deﬁned threshold isﬁnally applied. InFigure 4an example of segmented image is reported.
3. Experimental results
The experimental evaluation of the proposed face detection system has been car-ried out on a dataset composed of three subsets, all containing frontal images:
/C0Microsoft hand gesture (Ren et al., 2011), it is composed of images of 10 dif-ferent people performing gestures; each image contains only one face; sincethe images in the datasets are quite similar to each other we have chosenand labeled for face detection a subset of 42 images./C0Padua Hand gesture (Dominio et al., 2013), it is another gesture datasetcomposed of images from 10 different people; each image contains onlyone face; since the images are quite similar we have chosen a subset of 59images.
Figure 4Segmentation map, color image and depth map.Effective and precise face detection based on color and depth data 9/C0Padua FaceDec, it is a new dataset collected and labeled for the purpose ofthis work. It contains 132 images acquired with the Kinect sensor at the Uni-versity campus in Padova. It includes both outdoor and indoor scenes,framed in different hours during the day, in order to account for the varyinglighting conditions. The images capture one or several people performingvarious daily activities, e.g., working, studying, walking, chatting and soon. Note how most people are not directly looking into the camera, i.e., theydid not pose for the frame acquisition but they were doing their activitieswithout being aware of the camera shooting them. Some faces are also par-tially occluded by objects or other people. For these reasons, this dataset ismore challenging than the previous ones.
The three sets have been merged to form a single dataset consisting of 233images containing 251 faces
3(only upright frontal faces with a maximum rotationof ±30/C176have been considered). Notice that the parameters of the method havebeen manually selected and are the same for all the testing images despite their dif-ferent origin. The dataset is not ‘‘easy’’: inFigure 5some samples which are notdetected by the VJ method
4are shown.
The aim of the experiment reported inTable 1is to evaluate the effectiveness ofthe proposed approach considering the different ﬁltering steps and the use ofdepth image; the following approaches are compared according to the detectionrate (percentage of faces detected), the number of false positives and the F-mea- sure
5evaluated on the whole dataset:
/C15VJ(k), Viola–Jones detector in the 2D image with a threshold of k; /C15VJ(k)-Sz, the above approach ﬁltered considering the size of the candidateface region;
Figure 5Sample images from the dataset which contain faces not detected from the VJ method.
3Some images contain more than one face and some no faces.
4VJ is executed with a very low recognition threshold ( k= 2).
5The F-measure is the harmonic mean of recall and precision, often used in document retrieval, it is deﬁned as:2·precision·recall/(precision + recall).10 L. Nanni et al./C15VJ(k)-SzSk, the base VJ detector ﬁltered considering size and skin;/C15VJ(k)-SzSkStd, the base VJ detector ﬁltered considering size, skin and pres-ence of ﬂat/uneven regions (considering the depth map). In this method stdis calculated on the whole candidate face region (without the segmentation, toreduce the computation time)./C15VJ(k)-Fin, the whole approach described in this paper (including segmenta-tion to calculatestd)/C15VJ(k)-Fin-No, as VJ(k)-Fin but without considering the skin ﬁlter step.
It is clear that the size is a useful criterion for removing the high number of falsepositives candidates found by VJ using a low threshold (required to reach a highdetection rate). It allows to greatly reduce the number of false positives, the othertwo ﬁltering criteria allow to further reducing the number of false positives. Theproposed approach allows to diminish the number of false positives on the consid-ered dataset from 1063 to just 143 almost without affecting the detectionperformances.
The depth map allows to remove the false positives in many critical situations.In particular it allows to get the actual size of the candidate face allowing toremove objects too small or too large to be a face. It also aids the segmentationstep in the proposed method that is a critical point to ensure proper processingin the remaining steps.
Finally, even if the experiments reported in this paper are related to dataacquired by the Kinect, there are several other depth acquisition schemes and sen-sors that can be exploited. For example stereo vision systems that get the 3D datafrom two standard cameras, allow to work at big distances by choosing a suitablebaseline. Also there is a wide range of 3D sensors that can work at different dis-tances and with different accuracies. The Kinect is one of the most widespread andthe cheapest acquisition sensors but not the best.Table 1Comparison of methods in terms of detection rate and number of false positives. Rowscorresponding to the optimal setting of the VJ threshold have been highlighted. Bold is the best performance.Detection rate (%) # False positives F-measure VJ(4) 88.05 193 0.665VJ(3) 92.03 375 0.539VJ(2) 95.62 1063 0.308VJ(1) 95.62 8017 0.056VJ(3)-Sz 92.03 1230.725 VJ(2)-Sz 95.22 310 0.601VJ(1)-Sz 95.62 2062 0.189VJ(2)-SzSk 95.22 196 0.706VJ(2)-SzSkStd 95.22 165 0.730VJ(2)-Fin 94.82 143 0.758 VJ(2)-Fin-No 94.82 212 0.679Effective and precise face detection based on color and depth data 114. Conclusion
In this work a face detector for frontal faces is proposed. The Viola–Jones facedetector is coupled with three heuristic criteria calculated using the depth mapwith the main goal of obtaining accurate face detection with few false positives.
The proposed system makes use of several criteria for ﬁltering the false positivesfound by the face detector:
/C0A skin detection ﬁlter is used to remove the candidate face regions that con-tain enough skin pixels;/C0The size of the candidate face is calculated using the depth map to removeregions whose size is out of a ﬁxed range;/C0The depth map is used to design a ﬁlter rule to discard ﬂat objects (e.g. can-didate faces found in a wall) or uneven objects (e.g. candidate face found inthe leaves of a tree).
Experimental results show that the proposed system works well in a collecteddataset built by color images and depth map.
We are aware that the dataset used for testing is small with respect to thoseavailable for 2D face detection, but in our opinion the results clearly conﬁrm thatthe depth map permits to deﬁne criteria for a drastical reduction of the number offalse positives. However, it is our intention to collect new images to build a largerdataset.
Future works include collecting a larger dataset and extend the system to dealalso with non frontal upright faces. Another future work will be testing differentand more performing face detectors, as (Nanni and Lumini, 2012), for reducing the number of false negatives.
References
Anisetti, M., 2009. ‘‘Fast and robust Face Detection’’, Multimedia Techniques for Device and AmbientIntelligence, ISBN: 978-0-387-88776-0, Springer, US (Chapter 3).
Anisetti, M., Bellandi, V., Damiani, E., Arnone, L., Rat, B., 2008. A3FD: accurate 3D face detection. In: SignalProcessing for Image Enhancement and Multimedia Processing. Springer, US, pp. 155–165
.
Burgin, W., Pantofaru, C., Smart, W.D., 2011.‘‘Using depth information to improve face detection’’. In:Proceedings of the 6th ACM/IEEE International Conference on Human-Robot Interaction (HRI ’11), pp.119–120.
Dal Mutto, C., Zanuttigh, P., Cortelazzo, G.M., 2012. ‘‘Fusion of geometry and color information for scenesegmentation’’. In: IEEE Journal of Selected Topics in Signal Processing, vol. 6, No. 5, pp. 505–521.
Dixon, M., Heckel, F., Pless, R., Smart, W.D., 2007. Faster and more accurate face detection on mobile robotsusing geometric constraints. In: IEEE/RSJ International Conference on Robots and Systems (IROS 2007), pp.1041–1046.
Dominio, F., Donadeo, M., Zanuttigh, P., 2013. Combining multiple depth-based descriptors for hand gesturerecognition, Pattern Recogn. Lett., (accepted for publication), available online 24 October 2013.
Fowlkes, C., Belongie, S., Chung, F., Malik, J., 2004. Spectral grouping using the Nystro ¨m method. IEEE Trans. Pattern Anal. Mach. Intell. 26 (2), 214–225
.
Goswami, G., Bharadwaj, S., Vatsa, M., Singh, R., 2013. On RGB-D face recognition using Kinect. In: IEEESixth International Conference on Biometrics: Theory, Applications and Systems (BTAS), pp. 1–6.12 L. Nanni et al.Herrera, D., Kannala, J., Heikkila¨, J., 2012. Joint depth and color camera calibration with distortion correction.IEEE Trans. Pattern Anal. Mach. Intell. 34, 2058–2782
.
Hg, R.I., Jasek, P., Roﬁdal, C., Nasrollahi, K., Moeslund, T.B., Tranchet, G., 2012. An RGB-D database usingMicrosoft’s Kinect for Windows for face detection. In: Eighth International Conference on Signal ImageTechnology and Internet Based Systems (SITIS), pp. 42–46.
Huang, Chang, Ai, Haizhou, Li, Yuan, Lao, Shihong, 2007. High-performance rotation invariant multiview facedetection. IEEE Trans. Pattern Anal. Mach. Intell., 671–686
.
Jiang, F., Fischer, M., Ekenel, H.K., Shi, B.E., 2013. Combining texture and stereo disparity cues for real-timeface detection. Sig. Process. 28 (9), 1100–1113
.
Wu, Jianxin, Charles Brubaker, S., Mullin, Matthew D., Rehg, James M., 2008. Fast asymmetric learning forcascade face detection. IEEE Trans. Pattern Anal. Mach. Intell. 30, 369–382
.
Jin, H.L., Liu, Q.S., Lu, H.Q., 2004. ‘‘Face detection using one-class based support vectors’’. In: Proc. 6th IEEEInt. Conf. Autom. Face Gesture Recog., pp. 457–462.
Jones, M.J. et al, 2002. Statistical color models with application to skin detection. IJCV 46 (1), 81–96 .
Khan, R., Hanbury, A., Sto¨ttinger, J., Bais, A., 2011. Color based skin classiﬁcation. Pattern Recogn. Lett., 157–163
.
Ku¨blbeck, C., Ernst, A., 2006. Face detection and track in video sequences using the modiﬁed censustransformation. Image Vision Comput. 24 (6), 564–572
.
Li, B.Y., Mian, A.S., Liu, W., Krishna, A., 2013. Using kinect for face recognition under varying poses,expressions, illumination and disguise. In: IEEE Workshop on Applications of Computer Vision (WACV),pp. 186–192.
Mattheij, R., Postma, E., Van den Hurk, Y., Spronck, P., 2012. Depth-based detection using Haarlike features.In: Proceedings of the BNAIC 2012 conference. Maastricht University, The Netherlands, pp. 162–169
.
Nanni Loris, Lumini Alessandra, 2012. ‘‘Combining face and eye detectors in a high-performance face-detectionsystem’’. In: IEEE Multimedia, vol. 19, No. 4, Oct.–Dec. 2012, pp. 20–27. doi:10.1109/MMUL.2011.57.
Nanni, L., Lumini, A., Migliardi, M., 2013. Learning based Skin Classiﬁcation, submitted to Applied SoftComputing.
O´Conaire, Ciara´n, O’Connor, Noel E., Smeaton, Alan F., 2007. ‘‘Detector adaptation by maximising agreementbetween independent data sources’’. In: IEEE International Workshop on Object Tracking and ClassiﬁcationBeyond the Visible, Spectrum.
Ren, Z., Meng, J., Yuan. J., 2011. Depth camera based hand gesture recognition and its applications in human–computer-interaction. In: Proc. of ICICS, pp. 1–5.
Shi, J., Malik, J., 2000. Normalized cuts and image segmentation. IEEE Trans. Pattern Anal. Mach. Intell. 22 (8),888–905
.
Shieh, M.Y., Hsieh, T.M., 2013. Fast facial detection by depth map analysis. Math. Prob. Eng., 1–10 .
Shotton, J., Fitzgibbon, A., Cook, M., Sharp, T., Finocchio, M., Moore, R., Kipman, A., Blake, A., 2011. Real-time human pose recognition in parts from single depth images. CVPR 2, 3
.
Tsalakanidou, F., Tzovaras, D., Strintzis, M.G., 2003. Use of depth and colour Eigenfaces for face recognition.Pattern Recogn. Lett. 24 (910), 1427–1435
.
van de Weijer, J., Gevers, T., Gijsenij, A., 2007. Edge-based color constancy. IEEE Trans. Image Process. 16,2207–2214
.
Viola, Paul, Michael, J. Jones, 2001. ‘‘Rapid Object Detection using a Boosted Cascade of Simple Features’’,CVPR.
Zeng, Z., Pantic, M., Roisman, G.I., Huang, T.S., 2009. A survey of affect recognition methods: audio, visual,and spontaneous expressions. IEEE Trans. Pattern Anal. Mach. Intell. 31 (1), 39–58
.
Zhang C., Zhang Z., 2010. ‘‘A Survey of Recent Advances in Face Detection’’, Microsoft Research TechnicalReport, MSR-TR-2010-66.Effective and precise face detection based on color and depth data 13