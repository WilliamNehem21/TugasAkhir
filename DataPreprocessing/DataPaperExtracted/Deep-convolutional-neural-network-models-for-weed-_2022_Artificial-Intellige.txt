Deep convolutional neural network models for weed detectionin polyhouse grown bell peppers
A. Subeesh ⁎, S. Bhole, K. Singh, N.S. Chandel, Y.A. Rajwade, K.V.R. Rao, S.P. Kumar, D. Jat
ICAR-Central Institute of Agricultural Engineering (CIAE), Bhopal, Madhya Pradesh, India
abstract article info
Article history:Received 30 October 2021Received in revised form 18 January 2022Accepted 27 January 2022Available online 10 February 2022Conventional weed management approaches are inef ﬁcient and non-suitable for integration with smart agricul- tural machinery. Automatic identiﬁcation and classiﬁcation of weeds can play a vital role in weed management contributing to better crop yields. Intelligent and smart spot-spraying system's ef ﬁciency relies on the accuracy of the computer vision based detectors for autonomous weed control. In the present study, feasibility of deeplearning based techniques (Alexnet, GoogLeNet, InceptionV3, Xception) were evaluated in weed identi ﬁcation from RGB images of bell pepperﬁeld. The models were trained with different values of epochs (10, 20,30), batch sizes (16, 32), and hyperparameters were tuned to get optimal performance. The overall accuracy of theselected models varied from 94.5 to 97.7%. Among the models, InceptionV3 exhibited superior performance at30-epoch and 16-batch size with a 97.7% accuracy, 98.5% precision, and 97.8% recall. For this Inception3 model,the type 1 error was obtained as 1.4% and type II error was 0.9%. The effectiveness of the deep learning modelpresents a clear path towards integrating them with image-based herbicide applicators for precise weedmanagement.© 2022 The Authors. Publishing servic es by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an openaccess article under the CC BY-NC-ND license ( http://creativecommons.org/licenses/by-nc-nd/4.0/ ).
Keywords:Bell pepperComputer visionConvolutional neural networksDeep learningWeed identiﬁcation
1. IntroductionBell pepper (Capsicum annum L.) is an important crop cultivated allover the world owing to its commercial value and medicinal uses ( De, 2003;Gonzalez-Diaz et al., 2009). Bell pepper is also known as ‘sweet pepper’,‘pepper’or‘capsicum’and is cultivated for its fruit. India con-tributes one-fourth of the world's production of bell peppers with an av-erage annual production of 0.9 MT from an area of 0.885 millionhectares (Kumar et al., 2016). Like any other crop, bell peppers are sub-ject to biotic and abiotic factors that negatively affect the yield, weedbeing the major biotic factor (Chandel et al., 2021b). Being a poor com- petitor with weeds, immediately after transplanting, the supply of irri-gation for the crop may stimulate weed growth leading to yield lossup to 97% (Amador-Ramírez, 2002;Campiglia et al., 2012). Pepper pro- ductivity has decreased over the period in open ﬁelds compared to polyhouse due to frequent pests and diseases ( J a te ta l . ,2 0 2 0). Polyhouse cultivation in India needs to be enhanced in terms of tech-nological intervention and reduction of operating costs ( Singh et al., 2019). Weeds not only cause a reduction in yield but also lower cropquality, act as a reservoir for pests and diseases, and reduction inhuman efﬁciency. This issue requires the utilization of a weed detectionframework (Hasan et al., 2021). Weeds are persistent problems inpolyhouses and detract from the perceived quality of crops beinggrown. Herbicides are a means to control weed growth, but they haveseveral negative consequences for the environment and human health(Bah et al., 2018). The adoption of chemical and cultural control mea-sures might have negative environmental consequences, when not ap-propriately controlled. As most of the countries are facing labourshortages and increased labour costs, automation of weed control sys-tems is a need of the hour (Liu and Bruch, 2020;Subeesh and Mehta, 2021). Weed management in covered structures like polyhousesneeds much precision as the vapours from the chemicals can be trappedin closed structures, resulting in damage to both workers as well as thecrop.Weed recognition is one of those essential that requires digitizationand automation. Therefore, data-driven and image processing-basedtechniques using Internet of things and variable rate application of in-puts for real-time automation of system needs to be developed(Mehta et al., 2021). Weed detection in crops is an inherently challeng-ing problem to solve using digital technologies especially, using tradi-tional image processing techniques ( Wang et al., 2019). This is due to the shapes and textures of both weed and crop. Varying illuminatingcondition is another major challenge, when traditional image process-ing techniques have been applied to solve this problem. If weeds areidentiﬁed at an earlier stage it leads to reduced cost of herbicides(Espejo-Garcia et al., 2020). A typical weed identiﬁcation system followsArtiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
⁎Corresponding author.E-mail address:subeesh18@gmail.com(A. Subeesh).
https://doi.org/10.1016/j.aiia.2022.01.0022589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http:// creativecommons.org/licenses/by-nc-nd/4.0/ ).
Contents lists available atScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage:http://www.keaipublishing.com/en/journals/artificial-intelligence-in-agriculture/four key steps: image data collection, pre-processing of images, featureextraction and classiﬁcation (Shanmugam et al., 2020). Different ad- vanced technologies are in vogue to execute these steps. The mostvital part of these steps is weed identiﬁcation and classiﬁcation. For the last few years, Artﬁcial Intelligence (AI) and deep learning strategieshave been utilized for weed management with progressions in innova-tion replacing the traditional approaches ( Sharpe et al., 2019, 2020;Su, 2020).In recent years, numerous studies have been carried for the automa-tion of the process of identiﬁcation and classiﬁcation of weeds. Deep learning based image processing in lettuce crops, three methods forweed estimation were proposed byOsorio et al. (2020), which included machine learning and deep learning models such as Support Vector Ma-chines (SVM), YOLO and Mask R-CNN. The models achieved an F1 scoreof 88%, 94%, and 94%, respectively for the crop detection. The SVM in thisstudy used histograms of oriented gradients (HOG) as feature descrip-tors and YOLOV3 for object detection and Mask R-CNN for instance seg-mentation. SVM also found to be successful in identifying aquatic weeds(Pereira et al., 2012)a n dc l a s s iﬁcation of chilli and weeds in imagestaken from single plants (Ahmed et al., 2012).Yu et al. (2019)reported the efﬁciency of several deep convolutional neural network (DCNN)models such as VGGNet, GoogLeNet and DetectNet for detection ofweeds in bermudagrass. The majorﬁnding was that DetectNet exhib- ited high performance for detection of weeds while growing in dormantbermudagrass, with F1 score of 0.99. There were numerous attemptswere made by researchers in identifying weeds in rice ( Ashraf and Khan, 2020;Barrero et al., 2016;Cheng and Matson, 2015), wheat (Golzarian and Frick, 2011;Hameed and Amin, 2018), onion (Kim et al., 2018;Parico and Ahamed, 2020;Sanchez et al., 2021), etc.Hu et al. (2020)proposed a novel graph-based ‘Graph Weeds Net’architec- ture to recognize various types of weeds from RGB images collectedfrom complex rangelands and the model achieved good performancewith 98.1% accuracy. In the present study, we investigated the feasibilityof use of DCNN models such as Alexnet, GoogLeNet, InceptionV3 andXception in detecting weeds in the bell pepper cultivation.2. Materials and methods2.1. Image acquisitionThe images were collected at ICAR-Central Institute of AgriculturalEngineering, Bhopal, Madhya Pradesh (longitude —77°24′11.28″Ea n d latitude—
23°18′35.67”N). A hybrid variety of bell pepper known as‘Indra’was cultivated on raised beds according to standard package ofpractices (Jat et al., 2020). The images were captured between 9 AMto 5 PM inside a polyhouse under varying lighting conditions with a dig-ital camera (Xiaomi Mi 11× mobile device's rear camera, which has atriple camera setup with 48 MP, f/1.8, 26 mm (wide), 1/2 “,0 . 8μm, PDAF 8 MP, f/2.2, 119° (ultrawide) 5 MP, f/2.4, 50mm (macro), 1/5.0 ”, 1.12μm) at a ratio of 4:3, with a resolution of 4000 × 3000 pixels. Atotal of 1106 individual images were collected from the polyhouse,with 685 images of bell pepper and 421 images of various weeds. Thecollected images were grouped into weed and crop categories. Thenoise and lighting variations have been removed while pre-processingthe images. Further, data augmentation has been applied over thedataset to enhance the size and quality of training datasets and preventoverﬁtting (Shorten and Khoshgoftaar, 2019). The presence of weeds in bell pepper grown in polyhouse is shown in Fig. 1. The images from the dataset were further divided into training, test-ing and validation. The modeling was performed with 80% of the imagesfor training, 10% for testing and 10% for validation from the total cap-tured images.2.2. Deep learning and CNN modelsIn the initial years, artiﬁcial intelligenceheavily relied on the rule-based engines that can make predictions based on the ﬁxed and predeﬁned rule sets generated by a human expert. However, as thedata got massive, a more data driven approach was required and ma-chine learning was into the action. Machine learning is a collection of al-gorithms and tools by which machines can understand patterns withinthe data and perform reasoning about a speci ﬁc task. Machine learning has the capability to extract meaningful information from the data usingvarious algorithms. Deep learning (DL) can be considered as a next fron-tier of machine learning; it is a subset of machine learning that makesextensive use of neural networks. The advent of deep learning hasbrought a revolution in the area of image analysis and computer vision(Chandel et al., 2021a;Hemanth and Estrela, 2017). Deep learning techniques applied to digital photographs can helpdistinguish between crops and weeds beyond the limitations of conven-tional image processing. A deep convolutional neural network (DCNN)is a type of artiﬁcial intelligence that is extensively utilized in recentyears. In the initial days, the most that CNN could do was recognizehandwritten digits. At present, DCNN models are the most eccentric ap-paratus in computer vision capable of analysing huge complex datasets,having high computational limits. Numerous DL architectures exist forimage classiﬁcation like AlexNet (Krizhevsky et al., 2012), DenseNet (Huang et al., 2017), EfﬁcientNet (Tan and Le, 2020), GoogLeNet (Szegedy et al., 2015), InceptionNet (Szegedy et al., 2016), NASNet/ PNASNet/ENASNet (Adam and Lorraine, 2019), ResNeXt (Xie et al., 2017), ResNet50 (He et al., 2016
), XceptionNet (Chollet, 2017), SENet (Hu et al., 2018), VGGNet (Simonyan and Zisserman, 2015) and ZFNet (Howard et al., 2017). In this study for identifying weeds, the perfor-mance of four different CNN architectures AlexNet, GoogleNet,
Fig. 1.Presence of weeds in bell pepper grown in polyhouse.A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
48InceptionV3 and Xception are investigated. The models were selectedbased on the complexity and computational cost. AlexNet being the rel-atively simplest models and InceptionV3 (48 layers) and GoogleNet, areof moderate complexity and Xception with high complexity and depth.Table 1shows the architectural characteristics of these models.AlexNet is an 8-layer CNN that outperformed all other models in alarge margin and won the ImageNet large scale visual recognition chal-lenge held in 2012 (Krizhevsky et al., 2012). Although LeNet (LeCun et al., 1995) had achieved decent results on early datasets of smallersize, the performance while training CNNs on larger datasets were notthat impressive. AlexNet has achieved huge improvement over LeNetand it was theﬁrst architecture to adopt consecutive convolutionallayers with kernel sizes of (11 × 11), (5 × 5), and (3 × 3). VGG16Model (Simonyan and Zisserman, 2015), which achieved 92.7% top-5 test accuracy in ImageNet Large-Scale Visual Recognition Challenge. Itwas submitted to ILSVRC in 2014 where it became one of the popularmodels. VGG16 showed improved performance over AlexNet by replac-ing large kernel-sizedﬁlters (11 and 15 sizes present in AlexNet) with anumber of 3 × 3 sized kernels. The introduction of inception networkshas made a signiﬁcant impact in theﬁeld of neural networks. Theﬁrst version of inception model i.e. inceptionV1 is termed as GoogLeNet.This had set a new state of the art for both classi ﬁcation and object de- tection problems. The GoogLeNet Architecture has a depth of 22 layerswith 27 pooling layers and 9 inception modules stacked in totalTable 1Architectural characteristics of selected Convolutional Neural Network (CNN) models.Model Parameters (in millions) Depth Image input size Model characteristicsAlexNet 61.0 8 227 × 227 Simple architecture with 5 Convolutional layers and 3 Fully connected layersGoogLeNet 7.0 22 224 × 224 Use of auxiliary classi ﬁers, Going deeper with convolutions InceptionV3 23.9 48 299 × 299 Extended network of GoogLeNet, Reduced computational complexityXception 22.9 71 299 × 299 Linear stack of depthwise separable convolutions and residual connections, Modular architecture
Fig. 2.Deep Learning based Image classi ﬁcation pipeline used for weed identi ﬁcation.
Fig. 3.Components of Image classiﬁcation using DCNN.A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
49(Szegedy et al., 2015). GoogLeNet had achieved a top-5 error rate of6.67% in ILSVRC, which is very close to the human level performance.In comparison to the AlexNet and GoogLeNet, InceptionV3 modelfocuses on consuming less computational power. InceptionV3 modeloptimizes the network with the help of factorized convolution, dimen-sionality reduction, regularization and parallelized computations(Szegedy et al., 2016). Xception is an extreme version of the inception.Depth wise separable convolutional layers make Xception differentfrom the rest of the models and it follows a modular architecture(Chollet, 2017).2.3. Deep learning based image classi ﬁcation framework for weed identiﬁcationThe image classiﬁcation for weed detection began with the imagedata collection. The data has been collected from the precision farming
Fig. 4.Accuracy and loss function plots (trained with epoch =30, batch size =16) a) AlexNet, b) GoogLeNet, c) InceptionV3, d) Xception.A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
50development centre located at ICAR –Central Institute of Agricultural Engineering Bhopal, India.The images collected have been pre-processed before handing overto the classiﬁcation task (Fig. 2). The labels for each image have been de-ﬁned and a binary classiﬁcation was performed in the experiment. Themodels AlexNet, GoogLeNet, InceptionV3, and Xception models werechosen for identifying the bell pepper and weed. The pre-trainedmodels speciﬁed here for image classiﬁcation are generally composed of two components: a convolutional base and a classi ﬁer (Fig. 3). The convolutional base is responsible for the feature extraction and the clas-siﬁer classiﬁes the input image based on the features extracted by theconvolutional base. In the classiﬁer part, the standard approach is touse fully connected layers followed by an activation layer, which is gen-erally a softmax activation. The softmax layer generates the output indi-cating the probability of each class. The most probable class is chosen asthe predicted class.The model was trained with the help of training data and before thetraining got started, the model parameters like number of epochs, batchsize, learning rate, etc. were set. Validation set was used for unbiasedevaluation of the model. This helped to optimize the hyperparametersof the model. After identifying the right parameters, the model wastrained again by setting these parameters. Once the model was
Fig. 5.Confusion Matrices indicating the performance of the selected models (trained on epoch = 30, batch size = 16) a) AlexNet, b)GoogLeNet, c) InceptionNe t, d) Xception.
Table 2Performance of inceptionV3 model.Epoch Batch size Precision (%) Recall (%) F1 score (%) Accuracy (%)10 16 92.0 99.2 95.5 94.632 93.4 99.2 96.2 95.520 16 94.9 99.2 97.0 96.432 97.8 97.8 97.8 97.330 16 98.5 97.8 98.1 97.732 96.4 99.2 97.8 97.3A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
51completely trained, test set was used to provide a ﬁnal real-world check of the unseen data points to conﬁrm the proper functioning of themodel.2.4. Training, validation and testingThe images were pre-processed and trained using AlexNet,GoogLeNet, Xception and Inception V3. During the training phase, themodel automatically extracts the relevant features of ‘bell pepper’ plant and‘weed’in general. This process can be applied directly to theimage captured from theﬁelds, which are unseen to the model, andthe model can produce an accurate classi ﬁcation. During the training process, the loss function was set to stochastic gradient descent algo-rithm and batch sizes were set to 16 and 32. The initial learning ratewas set to 0.001 and momentum to 0.9000. For all these models,epoch and batch size were varied and performance across the modelswere compared. The validation data used to validate our model thathelps in adjusting and tuning the hyperparameters. The main reasonfor using the validation data is to prevent the model from over ﬁtting. Confusion matrix is effective in estimating the performance of the ma-chine learning classiﬁcation models. In this study, we have a binary clas-siﬁcation problem and each input sample was assigned to one of theclasses‘weed’or‘bell pepper’. The row of the confusion matrix denotesthe predicted class and the column represents the actual class of the in-stances.Accuracy¼TPþTNTPþTNþFPþFN ð1ÞPrecision¼
TPTPþFP ð2ÞSensitivity=Recall¼
TPTPþFN ð3ÞF1 Score¼
2∗Precision∗RecallPrecisionþRecall ð4ÞAccuracy, precision, recall and F1 score matrices are used for evalu-ating the model performance (Eqs. (1)–(4)). Here, TP denotes the True Positive, indicating the number/percentage of instances of bellpeppers accurately predicted by the model, FP denotes the Fall Posi-tive/type I error that shows the percentage of bell peppers misclassi ﬁed as weeds. FN or False Negative denotes the Type II error that shows thenumber/percentage of weed images classi ﬁed as bell pepper. TN or True Negative indicates the number percentage of weed images that areaccurately predicted by the model.3. Results and discussionIn this study, the performances of various deep learning modelswere evaluated for identifying the weeds among the bell peppers. Theresults indicated that all the models have performed satisfactorilywith an overall accuracy varying between 94.5 and 97.7%. The experi-ment has been repeated with a number of epochs 10, 20, 30 and signif-icant improvement has been observed in the accuracy. The number ofbatch size should be a power of 2, to take the complete advantage ofthe GPU processing (Kandel and Castelli, 2020). Two different values of batch size 16 and 32 were selected for training the model. Fig. 4 shows the accuracy and loss function variation when the model istrained with 30 epochs and 16 batch size. The plots indicate the lossfunction for all the models has started converging from early epochswithout having largeﬂuctuations. There were no overﬁtting or
Fig. 6.Performance of Deep Learning Models with respect to parameters epoch and batch size.
Table 3Performance of models AlexNet, GoogLeNet, InceptionV3 and Xception (Epoch = 30,batch size = 16).Model Precision (%) Recall (%) F1 Score (%) Accuracy (%)AlexNet 95.6 98.5 97.0 96.4GoogleNet 94.9 98.5 96.7 95.9InceptionV3 98.5 97.8 98.1 97.7Xception 95.6 99.2 97.4 96.8A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
52underﬁtting observed in AlexNet, GoogLeNet, InceptionV3, andXception models.In AlexNet, for a mini batch size of 16, the accuracy obtained for 10,20, and 30 epochs were 96.8%, 95.6%, and 96.4% respectively. With minibatch size 32, the accuracies for the model were 95.9%, 96.8%, 95.4% re-spectively. Thus there was no steady improvement is observed while in-creasing the number of epochs as well as the batch size for Alexnet. Theoverall accuracies in alexnet were varied between 95.4% and 96.8%. ForGoogLeNet, the highest accuracy of 97.2% was obtained at 20 epochsand batch size 16. In the Xception model, there is a slight improvementin accuracy has been observed after increasing the number of epochsfrom 10 to 30. The accuracies of Xception were varied between 95.9and 96.9%.The confusion matrices for weed and bell pepper identi ﬁcation using the selected deep learning models have been shown in Fig. 5. The diag- onal values indicate the true estimations. The highest accuracy observedwas 97.7% for InceptionV3 when it was trained for 30 epochs with abatch size of 16. For these hyperparameter values of batch size 16,with epoch 30, the inceptionV3 model was able to successfully classify135 bell pepper images out of 138 bell pepper images and only 3 imagesof bell pepper were misclassiﬁed. Also, the model successfully identiﬁed 81 weed images out of 83 weed images and only 2 weed images weremisclassiﬁed. InceptionV3 model showed a steady rise in the accuracyon increasing the number of epochs. The accuracies observed forInceptionV3 for mini batch size 16 and epochs 10, 20, 30 were 94.6,96.4 and 97.7%, respectively (Table 2). This improved performance of InceptionV3 over other models is due to factorization into smaller con-volutions and the use of auxiliary classiﬁers as regularizes. InceptionV3 architecture has made major improvements over the GoogLeNet archi-tecture without compromising on the performance. Even though thereis an increase, the same trend of steady increase was not observed inbatch size 32 and the accuracies were 95.5, 97.2, and 97.3 respectively.There was no further improvement in accuracy after increasing theepochs from 20 to 30 (Fig. 6).Table 3compares the precision, recall, F1 scores of all the modelswith the best performing which was trained with 30 epochs and abatch size of 16. The recall was very high for Xception, which indi-cated that the model was highly capable of making correct positivepredictions. The precision, accuracy, and F1 score of inceptionV3 at30 epochs and 16 batch size are higher than the other modelsand found to be the most effective model for weed and crop iden-tiﬁcation. The Type I error for inceptionV3 at these parameterssetting was 1.4% and Type II error was observed as 0.9%. The sameinceptionV3 model was tested with random testing images and theresults are as shown inFig. 7. The obtained results show that themodel is able to successfully identify the weed among the bell pepperwith very high conﬁdence.4. ConclusionDeep convolutional neural network-based weed detection is prom-ising and supports automation of agricultural operations. This workdemonstrated the capability of using DCNN models for weed identi ﬁca- tion in bell pepperﬁeld. In this study, four deep learning models(Alexnet, GoogLeNet, InceptionV3, Xception) have been appliedfor the identiﬁcation of weeds present among the bell pepper ﬁeld. InceptionV3 outperformed others in terms of precision, accuracy, andrecall. The potential future work includes detection of crop and weedin real-time and execution of the weeding action by intelligent weedersand/or site-speciﬁc herbicide applicators based on the decision made bythe DCNN models.
Fig. 7.Predictions and accuracy of InceptionV3 model (trained on epoch = 30, batch size =16) on the test images (The percentage in the ﬁgurehead represents the conﬁdence of the model in predicting the speciﬁcc l a s s ) .A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
53Credit author statementA. Subeesh: Conceptualization, Data curation, Methodology, Formalanalysis, Writing–original draft. Sameer Bhole: Investigation, Datacuration, Validation, Writing-original draft. Karan Singh: Software, Su-pervision, Writing - review & editing. NS Chandel: Data curation, Meth-odology, Validation, Writing - original draft, Writing - review & editing.YA Rajwade: Data curation, Methodology, Writing - original draft, Writ-ing - review & editing. K.V.R Rao –Investigation, Resources, Supervision,Validation, Writing - review & editing. SP Kumar - Validation, Writing -review & editing. Dilip Jat: Data curation, Writing - review & editing.Declaration of Competing InterestThe authors declare that they have no known competing ﬁnancial interests or personal relationships that could have appeared to in ﬂu- ence the work reported in this paper.References
Adam, G., Lorraine, J., 2019. Understanding Neural Architecture Search TechniquesArXiv190400438 Cs Stat.Ahmed, F., Al-Mamun, H.A., Bari, A.S.M.H., Hossain, E., Kwan, P., 2012. Classi ﬁcation of crops and weeds from digital images: a support vector machine approach. CropProt. 40, 98–104.https://doi.org/10.1016/j.cropro.2012.04.024 . Amador-Ramírez, M.D., 2002. Critical period of weed control in transplanted chilli pepper.Weed Res. 42, 203–209.https://doi.org/10.1046/j.1365-3180.2002.00278.x . Ashraf, T., Khan, Y.N., 2020. Weed density classi ﬁcation in rice crop using computer vision. Comput. Electron. Agric. 175, 105590. https://doi.org/10.1016/j.compag.2020. 105590.Bah, M.D., Haﬁane, A., Canals, R., 2018. Deep learning with unsupervised data labeling forweed detection in line crops in UAV images. Remote Sens. 10, 1690. https://doi.org/ 10.3390/rs10111690.Barrero, O., Rojas, D., Gonzalez, C., Perdomo, S., 2016. Weed detection in rice ﬁelds using aerial images and neural networks. 2016 XXI Symposium on Signal Processing, Im-ages and Artiﬁcial Vision (STSIVA), pp. 1 –4https://doi.org/10.1109/STSIVA.2016. 7743317.Campiglia, E., Radicetti, E., Mancinelli, R., 2012. Weed control strategies and yield re-sponse in a pepper crop (Capsicum annuum L.) mulched with hairy vetch (Viciavillosa Roth.) and oat (Avena sativa L.) residues. Crop Prot. 33, 65 –73.https://doi. org/10.1016/j.cropro.2011.09.016 . Chandel, N., Chakraborty, S., Rajwade, Y., Dubey, K., Tiwari, M.K., Jat, D., 2021a. Identifyingcrop water stress using deep learning models. Neural Comput. Applic. 33. https://doi. org/10.1007/s00521-020-05325-4 . Chandel, N.S., Chandel, A.K., Roul, A.K., Solanke, K.R., Mehta, C.R., 2021b. An integratedinter- and intra-row weeding system for row crops. Crop Prot. 145, 105642.https://doi.org/10.1016/j.cropro.2021.105642 . Cheng, B., Matson, E.T., 2015. A feature-based machine learning agent for automatic Riceand weed discrimination. In: Rutkowski, L., Korytkowski, M., Scherer, R.,Tadeusiewicz, R., Zadeh, L.A., Zurada, J.M. (Eds.), Arti ﬁcial Intelligence and Soft Com- puting. Lecture Notes in Computer Science. Springer International Publishing, Cham,pp. 517–527https://doi.org/10.1007/978-3-319-19324-3_46 . Chollet, F., 2017. Xception: deep learning with depthwise separable convolutions. 2017IEEE Conference on Computer Vision and Pattern Recognition (CVPR),pp. 1800–1807https://doi.org/10.1109/CVPR.2017.195 . De, A.K., 2003.Capsicum: The Genus Capsicum. CRC Press.Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., 2020. Improving weeds identi-ﬁcation with a repository of agricultural pre-trained deep neural networks. Comput.Electron. Agric. 175, 105593.https://doi.org/10.1016/j.compag.2020.105593 . Golzarian, M.R., Frick, R.A., 2011. Classi ﬁcation of images of wheat, ryegrass and brome grass species at early growth stages using principal component analysis. PlantMethods 7, 28.https://doi.org/10.1186/1746-4811-7-28 . Gonzalez-Diaz, L., Martinez Jimenez, M.P., Bastida, F., Gonzalez-Andujar, J., 2009. Decisionsupport system for integrated plant protection in pepper. Expert Syst. Appl. 36,8975–8979.https://doi.org/10.1016/j.eswa.2008.11.038 . Hameed, S., Amin, I., 2018. Detection of weed and wheat using image processing. 2018
IEEE 5th International Conference on Engineering Technologies and Applied Sciences(ICETAS), pp. 1–5https://doi.org/10.1109/ICETAS.2018.8629137 . Hasan, A.S.M.M., Sohel, F., Diepeveen, D., Laga, H., Jones, M.G.K., 2021. A survey of deeplearning techniques for weed detection from images. Comput. Electron. Agric. 184,106067.https://doi.org/10.1016/j.compag.2021.106067 . He, K., Zhang, X., Ren, S., Sun, J., 2016. Deep residual learning for image recognition. 2016IEEE Conference on Computer Vision and Pattern Recognition (CVPR), pp. 770 –778 https://doi.org/10.1109/CVPR.2016.90 . Hemanth, D.J., Estrela, V.V., 2017. Deep Learning for Image Processing Applications. IOS Press.Howard, A.G., Zhu, M., Chen, B., Kalenichenko, D., Wang, W., Weyand, T., Andreetto, M.,Adam, H., 2017.MobileNets: Efﬁcient Convolutional Neural Networks for Mobile Vision Applications ArXiv170404861 Cs.Hu, J., Shen, L., Sun, G., 2018. Squeeze-and-excitation networks. 2018 IEEE/CVF Confer-ence on Computer Vision and Pattern Recognition, pp. 7132 –7141https://doi.org/ 10.1109/CVPR.2018.00745. Hu, K., Coleman, G., Zeng, S., Wang, Z., Walsh, M., 2020. Graph weeds net: a graph-baseddeep learning method for weed recognition. Comput. Electron. Agric. 174, 105520.https://doi.org/10.1016/j.compag.2020.105520 . Huang, G., Liu, Z., Van Der Maaten, L., Weinberger, K.Q., 2017. Densely connectedconvolutional networks. 2017 IEEE Conference on Computer Vision and Pattern Rec-ognition (CVPR), pp. 2261 –2269https://doi.org/10.1109/CVPR.2017.243 . Jat, D., Rajwade, Y.A., Chandel, N.S., Dubey, K., Rao, K.V.R., 2020. Embedded system for reg- ulating abiotic parameters for Capsicum cultivation in a polyhouse with comparisonto open-ﬁeld cultivation. Int. J. Veg. Sci. 26, 487 –497. Kandel, I., Castelli, M., 2020. The effect of batch size on the generalizability of theconvolutional neural networks on a histopathology dataset. ICT Express 6, 312 –315. https://doi.org/10.1016/j.icte.2020.04.010 . Kim, S., Lee, J.S., Kim, H.S., 2018. Deep learning-based automatic weed detection on onionﬁeld. Smart Media J. 7, 16 –21.https://doi.org/10.30693/SMJ.2018.7.3.16 . Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012. ImageNet classiﬁcation with deep convolutional neural networks. Advances in Neural Information Processing Systems.Curran Associates, Inc.Kumar, P., Chauhan, R.S., Grover, R.K., 2016. Economic analysis of capsicum cultivation under polyhouse and openﬁeld conditions in Haryana. Int. J. Farm Sci. 6, 96 –100. LeCun, Y., Jackel, L.D., Bottou, L., Brunot, A., Cortes, C., Denker, J., Drucker, H., Guyon, I.,Muller, U.A., Sackinger, E., 1995. Comparison of learning algorithms for handwritten digit recognition. International Conference on Arti ﬁcial Neural Networks. Perth, Australia, pp. 53–60.Liu, B., Bruch, R., 2020. Weed detection for selective spraying: a review. Curr. Robot. Rep.1, 19–26.https://doi.org/10.1007/s43154-020-00001-w . Mehta, C.R., Chandel, N.S., Rajwade, Y., 2021. Smart farm mechanization for sustainable Indian agriculture. Ama Agric. Mech. Asia Afr. Lat. Am. 50, 99 –105. Osorio, K., Puerto, A., Pedraza, C., Jamaica, D., Rodríguez, L., 2020. A deep learning ap-proach for weed detection in lettuce crops using multispectral images.AgriEngineering 2, 471–488.https://doi.org/10.3390/agriengineering2030032 . Parico, A.I.B., Ahamed, T., 2020. An aerial weed detection system for green onion cropsusing the you only look once (YOLOv3) deep learning algorithm. Eng. Agric. Environ.Food 13, 42–48.https://doi.org/10.37221/eaef.13.2_42 . Pereira, L.A.M., Nakamura, R.Y.M., de Souza, G.F.S., Martins, D., Papa, J.P., 2012. Aquatic Weed Automatic Classiﬁcation Using Machine Learning Techniques.Sanchez, P.R., Zhang, H., Ho, S.-S., De Padua, E., 2021. Comparison of one-stage object de-tection models for weed detection in mulched onions. 2021 IEEE International Con-ference on Imaging Systems and Techniques (IST), pp. 1 –6https://doi.org/10.1109/ IST50367.2021.9651352.Shanmugam, S., Assunção, E., Mesquita, R., Veiros, A., Gaspar, P.D., 2020. Automated weed detection systems: a review. KnE Eng. 271 –284. Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2019. Detection of Carolina Geranium (Gera-nium carolinianum) growing in competition with strawberry using convolutionalneural networks. Weed Sci. 67, 239 –245.https://doi.org/10.1017/wsc.2018.66 . Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2020. Goosegrass detection in strawberry andtomato using a convolutional neural network. Sci. Rep. 10, 9548. https://doi.org/10. 1038/s41598-020-66505-9. Shorten, C., Khoshgoftaar, T.M., 2019. A survey on image data augmentation for deeplearning. J. Big Data 6, 60.https://doi.org/10.1186/s40537-019-0197-0 . Simonyan, K., Zisserman, A., 2015. Very Deep Convolutional Networks for Large-Scale Image Recognition ArXiv14091556 Cs.Singh, M.C., Singh, G.K., Singh, J.P., 2019. Nutrient and water use efﬁciency of cucumbers grown in soilless media under a naturally ventilated greenhouse. J. Agric. Sci. Technol.21, 193–207.Su, W.-H., 2020. Advanced machine learning in point spectroscopy, RGB- andhyperspectral-imaging for automatic discriminations of crops and weeds: a review.Smart Cities 3, 767–792.https://doi.org/10.3390/smartcities3030039 . Subeesh, A., Mehta, C.R., 2021. Automation and digitization of agriculture using arti ﬁcial intelligence and internet of things. Artif. Intell. Agric. 5, 278 –291.https://doi.org/10. 1016/j.aiia.2021.11.004.Szegedy, C., Liu, W., Jia, Y., Sermanet, P., Reed, S., Anguelov, D., Erhan, D., Vanhoucke, V.,Rabinovich, A., 2015. Going deeper with convolutions. 2015 IEEE Conference on Com-puter Vision and Pattern Recognition (CVPR), pp. 1 –9https://doi.org/10.1109/CVPR. 2015.7298594.Szegedy, C., Ioffe, S., Vanhoucke, V., Alemi, A., 2016. Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning ArXiv160207261 Cs.Tan, M., Le, Q.V., 2020.EfﬁcientNet: Rethinking Model Scaling for Convolutional NeuralNetworks ArXiv190511946 Cs Stat.Wang, A., Zhang, W., Wei, X., 2019. A review on weed detection using ground-based ma-chine vision and image processing techniques. Comput. Electron. Agric. 158,226–240.https://doi.org/10.1016/j.compag.2019.02.005 . Xie, S., Girshick, R., Dollár, P., Tu, Z., He, K., 2017. Aggregated Residual Transformations for Deep Neural Networks ArXiv161105431 Cs.Yu, J., Sharpe, S.M., Schumann, A.W., Boyd, N.S., 2019. Deep learning for image-basedweed detection in turfgrass. Eur. J. Agron. 104, 78 –84.https://doi.org/10.1016/j.eja. 2019.01.004
.A. Subeesh, S. Bhole, K. Singh et al. Artiﬁcial Intelligence in Agriculture 6 (2022) 47 –54
54