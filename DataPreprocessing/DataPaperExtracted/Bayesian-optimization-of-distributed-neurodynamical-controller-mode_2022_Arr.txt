Array 15 (2022) 100218
Available online 15 July 2022
2590-0056/¬© 2022 The Author(s). Published by Elsevier Inc. This is an open access article under the CC BY license ( http://creativecommons.org/licenses/by/4.0/ ).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/locate/array
Bayesian optimization of distributed neurodynamical controller models for
spatial navigation
Armin Hadzica,‚àó, Grace M. Hwanga,b,1, Kechen Zhangc, Kevin M. Schultza, Joseph D. Monacoc
aThe Johns Hopkins University Applied Physics Laboratory, Laurel, 20723, MD, USA
bKavli Neuroscience Discovery Institute, Johns Hopkins University, Baltimore, 21218, VA, USA
cDepartment of Biomedical Engineering, Johns Hopkins University School of Medicine, Baltimore, 21205, MD, USA
A R T I C L E I N F O
Keywords:
Bayesian optimization
Multi-agent control
Swarming
Dynamical systems models
Spatial navigation
UMAPA B S T R A C T
Dynamical systems models for controlling multi-agent swarms have demonstrated advances toward resilient,
decentralized navigation algorithms. We previously introduced the NeuroSwarms controller, in which agent-
based interactions were modeled by analogy to neuronal network interactions, including attractor dynamics
and phase synchrony, that have been theorized to operate within hippocampal place-cell circuits in navigating
rodents. This complexity precludes linear analyses of stability, controllability, and performance typically used
to study conventional swarm models. Further, tuning dynamical controllers by manual or grid-based search is
often inadequate due to the complexity of objectives, dimensionality of model parameters, and computational
costs of simulation-based sampling. Here, we present a framework for tuning dynamical controller models of
autonomous multi-agent systems with Bayesian optimization. Our approach utilizes a task-dependent objective
function to train Gaussian process surrogate models to achieve adaptive and efficient exploration of a dynamical
controller model‚Äôs parameter space. We demonstrate this approach by studying an objective function selecting
for NeuroSwarms behaviors that cooperatively localize and capture spatially distributed rewards under time
pressure. We generalized task performance across environments by combining scores for simulations in multiple
mazes with distinct geometries. To validate search performance, we compared high-dimensional clustering for
high- vs. low-likelihood parameter points by visualizing sample trajectories in 2-dimensional embeddings. Our
findings show that adaptive, sample-efficient evaluation of the self-organizing behavioral capacities of complex
systems, including dynamical swarm controllers, can accelerate the translation of neuroscientific theory to
applied domains.
1. Introduction
Collective biological behaviors of animal groups, including swarm-
ing, flocking, and schooling behaviors [ 1‚Äì6] have long inspired robotics
and computer science research into problems of decentralized control
and coordination for autonomous groups of artificial agents [ 7‚Äì12].
In particular, advancing the autonomous spatial capabilities of multi-
agent swarm control has been a key objective of simulation studies
and analyses of artificial swarms based on dynamical systems mod-
els [13]. Complementarily, the impressive recent progress of artificial
intelligence based on deep learning [ 14] has demonstrated the im-
portance of adopting key biological inspirations from neuroscience
and the brain. However, it has been unclear how to integrate com-
plex temporal features of brain dynamics thought to support crucial
‚àóCorresponding author.
E-mail addresses: arminhadzic@outlook.com (A. Hadzic), grace.hwang@jhuapl.edu (G.M. Hwang), kzhang4@jh.edu (K. Zhang), kevin.schultz@jhuapl.edu
(K.M. Schultz), joe@selfmotion.net (J.D. Monaco).
1This material is based on work supported by (while serving at) the National Science Foundation. Any opinion, findings, and conclusions or recommendations
expressed in this material are those of the authors and do not necessarily reflect the views of the National Science Foundation.mechanisms of neural computation [ 15]. Thus, addressing critical ques-
tions in autonomous robotics and artificial intelligence may depend
on efficient exploration and optimization of dynamical systems models
with complex interactions among many units. In both domains, major
gaps in state-of-the-art capabilities are highlighted by tasks involving
autonomous spatial navigation and foraging [ 16‚Äì19] in complex, novel,
or changing environments.
Bayesian optimization provides a probabilistic framework for adap-
tive, sample-efficient optimization of ‚Äòblack box‚Äô models with moderate
dimensionality (up to ‚àº20 parameters) and expensive sample evalua-
tions. In this framework, a task-dependent objective function signifies
the output performance of the complex underlying model, and the
optimizer traces parameter-space trajectories of candidate points from
https://doi.org/10.1016/j.array.2022.100218
Received 1 March 2022; Received in revised form 1 July 2022; Accepted 2 July 2022Array 15 (2022) 100218
2A. Hadzic et al.
acquisition functions operating on a simpler surrogate model. The
typical surrogate model is a Gaussian process that populates the pa-
rameter space of interest with multivariate normal distributions and
which serves as a prior distribution for candidate-point updates [20,
21]. Bayesian optimization with Gaussian process surrogate models
has enabled applications including the hyperparameter tuning and
optimization of evolutionary algorithms, multi-modal functions, robotic
controllers, and other complex systems [22‚Äì27].
The collective behavioral states of some swarming models are
tractable to linear analysis of stability, density, and clustering prop-
erties [28‚Äì32]. However, for dynamical systems that preclude such
analysis due to nonlinearity, nonstationarity, stochasticity, or other
complications, the computational budget for parameter exploration or
optimization with simulation-based samples is a limiting factor for
translation to engineered designs. Indeed, standard methods based on
gradient descent have two main drawbacks in this context: they can dis-
cover local optima, but resist exploration of system behaviors for other
purposes; and their basic operation is massively sample-inefficient,
which can be prohibitive for expensive simulation-based sample evalu-
ations. Moreover, emergent collective behaviors like swarming outstrip
conventional agent-based learning methods based on the restrictive
action and policy spaces of reinforcement learning, particularly for
uncertain, changing, or open-ended tasks.
We previously introduced the NeuroSwarms framework for model-
ing emergent high-level navigation and foraging in a brain-inspired
multi-agent metacontroller [33‚Äì35]. NeuroSwarms addressed decen-
tralized, distributed control by analogy to neural circuit dynamics,
including oscillations [36‚Äì39] and attractors [40‚Äì42], and associative
synaptic plasticity [43] related to rodent spatial cognition; the result-
ing collective behaviors of NeuroSwarms models included swarming,
patroling, and goal-finding in simulated maze environments with com-
plex, irregular, or fragmented geometry [34]. These behaviors enabled
NeuroSwarms to complete cooperative multiple reward-capture tasks
without pretraining across distinct environments [34]. However, the
nonlinearities inherent in NeuroSwarms‚Äô oscillatory phase-coupled self-
organization precluded analytic approaches to global identification,
exploration, or optimization of system behaviors. Thus, this class of
dynamical systems model can provide insights into key aspects of brain
structure and function that may inspire theoretical advances as well as
new directions for systems engineering designs. This insight depends
crucially on devising a task-dependent objective function that can guide
the efficient discovery of system behaviors and optimal performance.
In this paper, we demonstrate that Bayesian optimization can utilize
such an objective function to efficiently and usefully find paths through
otherwise prohibitive model spaces. In particular, we show that a
neurodynamical controller model with emergent properties can be
characterized and tuned using Bayesian optimization with Gaussian
process surrogate models.
2. Models and methods
2.1. NeuroSwarms model
Monaco et al. (2020) [34] introduced the NeuroSwarms frame-
work and described a model implementation with 300 agents; baseline
wall-avoiding, momentum-carrying motion-vector updates; maze en-
vironments whose geometry occluded agents‚Äô line-of-sight; interagent
communication between mutually visible agents; cosine-coupling of
internal phase variables driving interagent attraction and repulsion;
and 9 key dynamical parameters (Table 1) that had required intensive
manual fine-tuning to balance swarming and reward capture.Table 1
Tunable parameters that governed the spatiotemporal dynamics of the example Neu-
roSwarms model implementation [34]. ‚ÄòRange‚Äô indicates the limits of the parameter
subspace made available for Bayesian optimization. All other NeuroSwarms parameter
values and constants were fixed at the defaults in Table 1 of Monaco et al. (2020) [34].
Name Range Description
ùúé [10‚àí3,4] Normalized interagent spatial scale
ùúÖ [10‚àí3,4] Normalized reward-approach spatial scale
ùúÇùë† [10‚àí3,4] Recurrent interagent learning rate
ùúÇùëü [10‚àí3,4] Feedforward reward-approach learning rate
ùúî0 [0,1] Baseline agent oscillation frequency
ùúîùêº [0,1] Max. activation-based frequency increase
ùúèùëû [0,1] Recurrent interagent time-constant
ùúèùëü [0,1] Feedforward reward time-constant
ùúèùëê [0,1] Sensory input time-constant
2.2. Bayesian optimization
Bayesian optimization constructs and performs sequential optimiza-
tion on a surrogate model that represents the objective performance
of a more complex model [44‚Äì46]. Learning surrogate models can be
beneficial if directly optimizing a complex model is not computation-
ally tractable given resource constraints. These surrogate models can
then be deployed to predict the performance of the underlying model
at untested parameter points without requiring a full model simulation
of those parameter values (Fig. 1).
We implemented Bayesian optimization with surrogate models de-
fined as Gaussian processes [20,48,49]. Gaussian processes are para-
metric models that iteratively learn a probabilistic mapping ùëì‚à∂X‚Ü¶R
such that the density estimate p(ùë¶ùëñ|ùê±ùëñ) =ùëì(ùê±ùëñ,ùë¶ùëñ), where X‚äÜRùëùis the
bounded parameter subspace being optimized, ùê±ùëñ‚ààXis a parameter
point, and ùë¶ùëñ‚ààRis an objective function output value [21,50,51];
e.g.,ùëù= 9NeuroSwarms parameters in this paper. Thus, the under-
lying ‚Äòblack box‚Äô objective function ùëìtrueis assumed to be distributed
according to a Gaussian process,
ùëìtrue‚àºÓà≥Óàºùúá,ùëò(ùëã),
whereùúá(‚ãÖ)andùëò(‚ãÖ)are mean and covariance kernels applied to an input
parameter set, ùëã ‚äÇX. The posterior distribution of a ùëû-sized batch of
candidate points ÃÇùëã= {ÃÇ ùë•1,‚Ä¶, ÃÇ ùë•ùëû}conditioned on the observed training
dataÓà∞= {( ùê±ùëñ,ùë¶ùëñ)}ùëõ
ùëñ=1takes the form of a ùëù-dimensional multivariate
normal distribution, i.e., P(Óà≥Óàº(ùëã)|Óà∞) ‚àºÓà∫ùëù(ùúá(ùëã), ùëò(ùëã)).
2.3. Acquisition functions
Bayesian optimization relies on acquisition functions to provide the
candidate parameter points that navigate the underlying model space.
Acquisition functions define a strategy to manage the trade-off between
exploring the parameter space and exploiting regions that yielded
improvement for previous samples [52]. An acquisition function can be
evaluated on the Gaussian process posterior P(Óà≥Óàº(ùëã)|Óà∞)by averaging
a set of Monte Carlo (MC) samples, e.g.,
ÃÇ ùõºùëõ(ùëã;Óà∞) =1
ùëõùëõ‚àë
ùëñ=1ùëé(Óà±ùëñ
Óà∞(ùëã)), (1)
whereùëõis the sample count and ùëé(‚ãÖ)is the net utility function provid-
ing objective function output. Thus, ÃÇ ùõºùëõis an expectation of posterior
samples Óà±Óà∞‚àº P(Óà≥Óàº(ùëã)|Óà∞). We study a pair of MC-based acquisition
functions:ùëû-Expected Improvement (qEI) [53] and Noisy ùëû-Expected
Improvement (qNoisyEI) [54]. We compare qEI and qNoisyEI to ran-
dom sampling of candidate parameters. First, similar to ÃÇ ùõºùëõ(Eq. (1)),
qEI calculates an expectation over posterior samples,
qEI(ùëã) ‚âà1
ùëõùëõ‚àë
ùëñ=1ùëûmax
ùëó=1[
Óà±ùëñ
ùëó‚àíùëå‚àó]
+,
where [‚ãÖ]+indicates linear rectification and ùëå‚àóis the best observed
objective function value. Thus, qEI estimates a noise-free expectedArray 15 (2022) 100218
3A. Hadzic et al.
Fig. 1. Computation flow for optimization and simulation-based sampling. A, Step 1: The posterior distribution is computed from the Gaussian process surrogate model (GP Model)
based on the training data Óà∞. Step 2: The acquisition function‚Äôs Quasi Monte Carlo sampling process uses the posterior distribution to select new candidate parameters ÃÇùëã(Step 3)
based on the acquisition function‚Äôs estimated objective function value ÃÇùëå(Step 4). Step 5: The NeuroSwarms model [ 33,34] is simulated with candidate parameter points ÃÇùëãto
generate the observed objective value ùëå(see B). Step 6: The initial Gaussian process model‚Äôs marginal log-likelihood (MLL) is then calculated and used to optimize the Gaussian
process using the L-BFGS-B algorithm [ 47]. Step 7: The resulting Óà∞(from Step 5) and MLL (from Step 6) update the Gaussian process model for the next iteration of the outer
loop. B, Flow diagram of simulation-based candidate-point evaluation. For each sample (see Step 5 in A), the optimizer executes play-throughs in both the Hairpin (top) and Tunnel
(bottom) maze environments. The sample‚Äôs objective value ùëåis computed as the average of the respective loss values ùêøùêªandùêøùëá(Eq. (3)).Array 15 (2022) 100218
4A. Hadzic et al.
improvement of the posterior with respect to the best value. Second,
qNoisyEI approximates improvement relative to the expected best ob-
jective value conditioned on the observed MC sampling history Óà±obs
within each batch [55]; simplistically, the constrained batch-sampling
performed by qNoisyEI [54,56] approximates
qNoisyEI (ùëã;Óà∞) ‚âà1
ùëõùëõ‚àë
ùëñ=1ùëûmax
ùëó=1[
Óà±ùëñ
ùëó‚àí max Óà±obs]
+,
but more detailed treatments of this complex optimization problem
provide critical analyses and caveats (cf. [54‚Äì56]).
Throughout our study, Bayesian optimization with any of the three
acquisition functions employed 512 MC samples, 30 training epochs
(with a batch size of 3), and 8 random training samples to initialize
the Gaussian process surrogate model.
2.4. Objective function
We constructed an objective function to evaluate the performance of
the example NeuroSwarms model [34] in a time-pressured cooperative
foraging task. The objective function quantifies how quickly the swarm
of agents collectively capture several spatially distributed rewards in
a given maze. Let ùëõcap(ùë°)be the cumulative number of cooperatively
captured rewards by time ùë°. A reward is captured if, at any timestep,
at leastùëõùë†‚àïùëõùëüagents were simultaneously colocated within a defined
radius from the reward, where ùëõùë†= 300 agents andùëõùëü= 3and 5 rewards
in the Tunnel and Hairpin mazes, respectively. For a given simulated
play-through, this objective function can be expressed as a loss which
is updated at every timestep until all rewards are captured,
ùêø= ‚àíùë°‚àï(ùëõùë°ùëõcap(ùë°) + 1), (2)
whereùëõùë°is the total number of time steps. The agent group‚Äôs behav-
ior is time-pressured by ùë°growing continuously until all rewards are
captured. If the swarm is not able to capture all the rewards in the
environment, ùë°will be set to the maximum number of timesteps allowed
for the simulation ùëõùë°and the loss will reflect the number of missed
rewards. Loss values range from [‚àí1,0], with better task performance
closer to zero.
To account for the generalizability of spatial task performance
across distinct environmental geometries, each simulation-based sam-
ple constitutes play-throughs of both the Hairpin and Tunnel mazes,
respectively providing loss values ùêøùêªandùêøùëáas calculated in Eq. (2)
(see Fig. 1 B). Thus, the generalized performance at a given parameter
point ùê±ùëñis indicated by the objective value ùëå, computed as the average
ùë¶ùëñ(ùê±ùëñ)‚âêùëå=ùêøùêª+ùêøùëá
2. (3)
2.5. Gaussian process training
The means and variances of the Gaussian process surrogate model
are updated with each sample evaluation to reflect the expected values
and uncertainty, respectively, of the underlying model‚Äôs performance.
We use the Bayesian optimization library BoTorch [51] to implement
the outer loop of surrogate model training based on iteratively updating
a Gaussian process following initialization with sample data Óà∞. The
posterior distribution P(Óà≥Óàº(ùëã)|Óà∞)is then sampled from a batched
MC sampling process using an acquisition function to determine the
candidate parameter points ÃÇùëãfrom the subspace bounded by the ranges
listed in Table 1. The candidate points are selected based on predictive
estimates of utility value ÃÇùëå(Fig. 1 A) and evaluated by simulating the
NeuroSwarms model to generate loss values (Eq. (2)) and objective
function output ùëå(Eq. (3)) (Fig. 1 B). Lastly, the resulting ( ÃÇùëã,ùëå) tuple
is appended to training data Óà∞to update the Gaussian process for the
next iteration.
The surrogate model hyperparameters were tuned by first comput-
ing the marginal log-likelihood (MLL) of the Gaussian process appliedto observed parameters ùëãand fitting hyperparameters with the limited-
memory Broyden‚ÄìFletcher‚ÄìGoldfarb‚ÄìShanno algorithm with simple
bounds (L-BFGS-B) [47]. The fitting process provides an updated MLL
for the next optimization step.
2.5.1. Convergence metrics
This hyperparameter tuning process described above was repeated
until convergence according to two metrics: maximum posterior vari-
ance and minimum candidate dissimilarity. First, maximum posterior
variance for training epoch ùëÄwas computed following
max Var(P(Óà≥Óàº(ùë•ùëÄ) ‚à£Óà∞ùëÄ))
to indicate whether the Gaussian process‚Äô posterior variance was no
longer increasing and that training should cease. Second, minimum
candidate dissimilarity measures the stabilization of candidate selection
as an inverse cosine similarity; i.e., we calculated the metric following
ùëÄ‚àí1
min
ùëñ=1[
1 ‚àíùë•ùëñ‚ãÖùë•ùëÄ
‚Äñ‚Äñùë•ùëñ‚Äñ‚Äñ‚ãÖ‚Äñ‚Äñùë•ùëÄ‚Äñ‚Äñ]
to confirm whether epoch ùëÄselected for similar neighborhoods of
parameter points as in previous training epochs. These convergence
metrics determined hyperparameter convergence and enabled the re-
sulting Gaussian process surrogate model to efficiently adapt to the
NeuroSwarms parameter space.
2.6. Parameter visualization
The low-dimensional representations produced by the uniform man-
ifold approximation and projection (UMAP) [57] result from a locality-
preserving embedding that serves to spatially cluster higher-
dimensional vectors such as ùëù-dimensional parameter points. A 2D
UMAP projection allows these point clusters to be simply visualized
as images or scatter plots, for which the ùë•-axis andùë¶-axis constitute
an arbitrary coordinate frame. For UMAP scatter plots, as in Figs. 3
and 6, the marker for each point can be colored for convenient visual
inspection of associated values, including vector elements or com-
puted output. We use this visual clustering to qualitatively inspect the
parameter-dependence and structure of the Gaussian process surrogate
model by selecting a UMAP data point with, e.g., high performance
indicated by its loss value ùë¶ùëñ(Eq. (3)), and assessing that point‚Äôs other
values in the context of its location and neighborhood relative to
UMAP-based clusters.
3. Results and discussion
3.1. Overview
We demonstrate Bayesian optimization methods (see Section 2.2)
for tuning the parameters of a neuroscience-inspired swarming model,
NeuroSwarms [33,34,39] (see Section 2.1), to find cooperative for-
aging behaviors for capturing multiple rewards in distinct maze en-
vironments under time pressure (see Section 2.4). We train Gaussian
process surrogate models (see Section 2.5) to characterize the Neu-
roSwarms parameter space using noise-free (i.e., qEI) and observed
sampling history-dependent (i.e., qNoisyEI) acquisition functions (see
Section 2.3). Then we show how the locality-preserving dimensionality
reduction provided by UMAP embeddings (see Section 2.6) can be used
to evaluate the surrogate model and identify system behaviors.Array 15 (2022) 100218
5A. Hadzic et al.
3.2. Training the surrogate model for swarming performance
Small variations in the ùëù= 9dynamical NeuroSwarms parameters
(Table 1) can substantially impact collective behaviors. Optimal pa-
rameters that allow NeuroSwarms models to accomplish generalized
cooperative foraging may not be limited to a single set of parameters
due to the complexity and potential degeneracy of emergent collective
behaviors in a distributed multi-agent system. Thus, we constructed
a simple time-pressured objective function to measure the progress
of reward-capture (Section 2.4) and guide Bayesian optimization us-
ing Gaussian process surrogate models ( Fig. 1A). We utilized acqui-
sition functions to sample candidate parameter points and optimize
the Gaussian process‚Äô predictive performance compared to observed
NeuroSwarms simulations (Section 2.5). We evaluated the surrogate
models in two environments for each sample: a Hairpin maze and
a Tunnel maze ( Fig. 1B). By simultaneously assessing mazes with
distinct geometries, the surrogate model optimization was allowed to
find swarming and navigational dynamics resulting in time-efficient
cooperative foraging that may generalize across environments.
We started training with an initial set of 24 randomly selected
parameter points with corresponding simulation results. Each Gaussian
process was trained by an acquisition function for selecting candidate
points:ùëû-batched Expected Improvement (qEI), ùëû-batched Noisy Ex-
pected Improvement (qNoisyEI), or random parameter sampling (Sec-
tion 2.3). Gaussian process modeling and training was implemented
using BoTorch [ 51] and optimized with 512 MC samples over 30
training epochs (Section 2.5). We verified that the EI-based acqui-
sition functions converged based on metrics of minimum candidate
dissimilarity and maximum posterior variance (Section 2.5.1 ). The
EI-based acquisition functions approached zero dissimilarity during
training ( Fig. 2A). Similarly, the maximum posterior variance for each
surrogate model had converged by the end of training ( Fig. 2B).
We evaluated how effective each acquisition function was at finding
regions of the parameter space that optimize the NeuroSwarms ob-
jective function ( Eqs. (2) and (3)). Both qEI and qNoisyEI discovered
more parameter points with high-performance values than random
sampling ( Fig. 2C). Both random sampling and the default parameters
from Monaco et al. (2020) [ 34] were outperformed by the EI-based ac-
quisition functions. Thus, qEI and qNoisyEI demonstrated the strongest
utility improvement of best observed values during training as the Neu-
roSwarms parameter space was learned by the corresponding surrogate
models ( Fig. 2D).
3.3. Evaluating UMAP-clustering of selected parameters
Understanding the results of the above Bayesian optimization pro-
cess requires a visual representation of the parameter space, yet it can
be challenging to represent data with >3 dimensions. We considered
that visualizing parameter points in lower dimensions could facilitate
the discovery of critical surrogate model structures, including clusters
of high-performing parameters that potentially yield distinct behav-
ioral solutions to the cooperative foraging task. Thus, we used UMAP
(Section 2.6) to reduce sets of 9-dimensional NeuroSwarms parameters
(Table 1) into locality-preserving 2D representations. For qEI-selected
parameters, we assigned colors to the resulting 2D UMAP-clustered data
points according to posterior mean estimates of objective values (top,
left plot) or individual parameter values ( Fig. 3). The resulting visual
representation in Fig. 3 shows where the highest utility (i.e., best
posterior mean estimate of objective value) data points cluster into
groups based on the values of NeuroSwarms parameters.
Given that qEI demonstrated the largest utility improvement
(Fig. 2D) and consistently identified high-performing parameters
(Fig. 2C), we consider its UMAP representation for further analysis. The
qEI-based parameter samples formed two clusters of data points with
the highest utility ( Fig. 3). In the (top, left) posterior mean plot, we
selected one of these points from the lower, left cluster and matched
Fig. 2. Convergence metrics and objective function values for acquisition functions
across training. A+B, Training convergence metrics: minimum candidate dissimilarity
(A) and maximum posterior variance ( B).C+D, The training performance of Gaussian
process models based on the qEI and qNoisyEI acquisition functions, compared to a
baseline of random sampling, was quantified by objective function values shown as his-
tograms of losses for the sampled parameter trajectories ( C) and as the improvement in
best observed values ( D), where values closer to 0 indicate better performance (Eq. (2))
in the time-pressured cooperative foraging task.Array 15 (2022) 100218
6A. Hadzic et al.
Fig. 3. UMAP-clustered parameter points selected by the noise-free qEI acquisition function. The dimensional reduction computed by the UMAP transformation (Section 2.6)
preserves locality of neighboring parameter points. As a result, high-dimensional clusters can be revealed by scatter plots of 2D UMAP data. Each of the 10 scatter plots shows
the same UMAP projection of qEI-sampled parameter points, using the same (arbitrary) 2D coordinate frame. In the first plot (top, left), the color of each point indicates the
expected posterior mean of the trained Gaussian process surrogate model according to the colorbar legend to the right of the plot; e.g., a group of adjacent blue points reflects
a high-performing cluster of NeuroSwarms parameters. The top-left colorbar additionally serves to provide a reference for how colors are mapped to the respective value ranges
(i.e., [min, max]) specified in the label above the remaining ùëù= 9plots. These 9 plots show the individually sampled parameter values (cf. Table 1) associated with each UMAP
point. (For interpretation of the references to color in this figure legend, the reader is referred to the web version of this article.)
it with the numerical values of its associated parameters, which we
subsequently evaluated in NeuroSwarms simulations.
We simulated the qEI-optimized NeuroSwarms model on both the
Hairpin and Tunnel mazes (see Fig. 1B). Trajectory-trace plots for
the Hairpin ( Fig. 4, blue traces) depict the movement of each agent
that contributed to reward capture throughout the simulation, up to
the timestep at which cooperative capture of each reward goal was
achieved. Likewise, trajectory traces in orange ( Fig. 4) reflect the
behavior of the reward-capturing agents after the reward had been
captured. For example, the transition from swarming and goal-directed
dynamics to post-capture exploration is depicted by the capture of
Reward 3 (R3) in the third row of Fig. 4, in which a subset of
agents converged on and captured R3 and immediately dispersed, thus
permitting the search for and capture of subsequent reward goals.
Agents recommenced exploration following reward-capture because
NeuroSwarms relies on local, line-of-sight communication between
agents, meaning that agent motion may not be influenced by nearby
rewards if they are occluded by walls of the maze. The qEI-tuned
swarms were able to quickly capture all five rewards on the Hairpin en-
vironment ( ùë°= 25.38s), as shown in Fig. 4, whereas the original default
parameters of NeuroSwarms‚Äîdetermined by hand-tuning as described
in our previous work [ 34]‚Äîproduced relatively slow reward capture
(ùë°= 41.02s). Reward-capture speed using the default parameters was
additionally exacerbated in the Tunnel maze ( ùë°= 175.42s). In contrast,
the qEI-tuned swarm captured all three rewards ( Fig. 5) faster than the
default swarm captured two rewards ( ùë°= 34.88s). We attribute the
worse performance of the hand-tuned default parameters to longer dy-
namical time-constants and thus slower behavioral responsivity. Thus,
compared to manual parameter tuning for each maze environment, our
Bayesian batch-optimization process (Section 2.3; Fig. 1A) with joint
objective sampling (Section 2.4; Fig. 1B) was able to simultaneously,
jointly, and efficiently discover distinct high-performing dynamical
parameters for multiple mazes.
A key feature of our Bayesian optimizer is that the objective in-
directly quantifies (i.e., as a ‚Äòblack box‚Äô model) cooperative foraging
without directly modifying NeuroSwarms‚Äô underlying mechanisms. In
general, this feature allows a task-dependent objective to evaluate
multi-agent performance in collective tasks involving, e.g., social co-
ordination or distributed consensus. In contrast to the regular butfragmented geometry of the Hairpin maze ( Fig. 4), the Tunnel maze
required the swarm to distribute through an irregular geometry to
complete the foraging task ( Fig. 5). Additionally, whereas agents were
initialized at uniform random locations in the Hairpin maze, all agents
in the Tunnel maze were initialized to points inside a small disc
circumscribed within its Southwest quadrant. As a result, the agents
rapidly capture R2 ( Fig. 5, top row) and then split into subgroups
to capture the remaining two rewards ( Fig. 5, lower two rows). An
additional challenge of the Tunnel maze is that R3 is initially visible
to all agents and closer than R1, yet the tunnel constricts access
to it. Conversely, R1 is initially visible and accessible, yet further
away and partially occluded once agents have converged onto R2‚Äôs
location. The fast capture of R1 ( ùë°= 5.46s) vs. R3 ( ùë°= 31.78s)
reflects the characteristic time-scale differences between coordinated
reward-approach trajectories and exploratory swarming trajectories,
respectively. Comparing the pre-capture (blue, left) and post-capture
(orange, right) trajectories for each reward ( Fig. 5), the agents began
using the large opening in the center of the map only once R2 and R1
were both captured. This behavioral transition suggests that exploration
traded off with goal-directed exploitation by adaptively forming and
regrouping subgroups of agents. Thus, distinct challenges presented by
the Tunnel maze, in concert with our optimizer‚Äôs objective function
definition (Section 2.4), may have induced collective behaviors that can
flexibly adapt to diverse foraging problems.
3.4. Exploring the future parameter space
Trained acquisition functions can be used to predict the perfor-
mance of unobserved regions of the parameter space. To test predictive
selection, we generated 500 samples from the qEI acquisition function
and the posterior distribution of its trained Gaussian process surrogate
model. The qEI sample means from the posterior ( Fig. 6, top-left plot)
were similar across most data points because qEI had adapted to
parameter regions with the highest likelihood of utility improvement.
As in the previous Section 3.3, we selected candidate points from these
anticipated future qEI parameters to simulate in the Hairpin and Tunnel
mazes, but we chose points that featured mid-range parameter values,
i.e., whose vector elements were not at or near the range limits of theArray 15 (2022) 100218
7A. Hadzic et al.
Fig. 4. NeuroSwarms trajectories depicting reward capture in the Hairpin maze. The Hairpin maze presents a large, fragmented arena to assess the swarm‚Äôs foraging performance
given the uncertain localization inherent in environments with symmetrically repeating geometric patterns. Five reward goals are spatially distributed at maze locations indicated
by gold stars (R1‚ÄìR5, top-left maze plot). The 10 maze plots show segments of spatial trajectories traced out by NeuroSwarms agents during a sample simulation. Maze plots on
the left show agent paths (blue traces) from either the beginning of the simulation or the most recent previous reward capture to the time of the reward capture indicated by the
text label to the left of the plot. Traces are shown for only those agents that contributed to cooperative capture of the given reward (see Section 2.4). Conversely, maze plots on
the right show agent paths (orange traces) from the time of reward capture until the end of the simulation. From top to bottom, each row presents a pre-capture and post-capture
pair of swarm trace plots in the order in which rewards were captured in the simulation. Individual traces are translucent; thus, the degree to which the trajectories of multiple
agents superposed upon the same observed paths is indicated by the relative saturation of the trace color. As a result, visual inspection yields information about the swarming
and reward-approach dynamics with respect to the spatial convergence and divergence of agents over time.Array 15 (2022) 100218
8A. Hadzic et al.
Fig. 5. NeuroSwarms trajectories depicting reward capture in the Tunnel maze. The Tunnel maze presents an irregular arena to assess the swarm‚Äôs foraging performance given a
loop-like environment with substantial geometric occlusion of visibility and passageways with large vs. constrictive (e.g., the eponymous ‚Äòtunnel‚Äô connecting the Southwest to the
Southeast quadrants) apertures. Three reward goals are spatially distributed at maze locations indicated by gold stars (R1‚ÄìR3, top-left maze plot). The 6 maze plots show agent
paths before (left, blue traces) and after (right, orange traces) the cooperative reward capture (see Section 2.4) indicated by the label to the left of the plots. Additional details
are as described in the caption for Fig. 4.
respective parameter ( Table 1). In particular, we selected parameters
where the time-constants were greater than the minimum of their
ranges (1 ms), constituting a parameter regime that was distinct from
clusters of qEI samples which minimized their respective time-constants
in response to the time-pressure imposed by our objective function
(Eq. (2)). We chose these points, with corresponding simulations shown
in Fig. 7, to demonstrate the distinct behavioral solutions to the for-
aging task that can be discovered by the same acquisition function and
associated surrogate model. Trajectory-trace plots of reward-capturing
agents before and after rewards were cooperatively captured on the
Hairpin and Tunnel mazes show that the selected parameters resulted in
slower reward capture for the Hairpin ( ùë°= 47.44s; Fig. 7A) and Tunnel
(ùë°= 66.96s; Fig. 7B) mazes compared with the optimized parameters
in Fig. 4 (Hairpin, ùë°= 25.38s) and Fig. 5 (Tunnel,ùë°= 31.78s).
Additionally, the default parameters from Monaco et al. (2020) [ 34]
entailed strong reward-approach exploitation (e.g., ùúÖ= 6.6), but weak
swarming-based exploration (e.g., ùúé= 2.0). This combination of be-
havioral forces increased the time-to-capture for all five rewards. Thus,we attribute slow reward-capture to a combination of longer dynami-
cal time-constant parameters and exploration‚Äìexploitation mismatches.
Moreover, if the energy budget of agent locomotion (e.g., speed, turn-
ing, etc.) were to be taken into account by the objective function, a
slower behavioral repertoire enabled by these parameter regimes could
help to minimize energetic or inefficient navigational patterns.
4. Concluding remarks
Neuroscience-inspired learning and control methods have seen in-
creased interest from robotics, artificial intelligence, and multi-agent
control. Here, we presented a demonstration of exploring and visu-
alizing the parameter space of a multi-agent model with complex
dynamical behaviors using sample-efficient Bayesian optimization with
Gaussian process surrogate models. We introduced an objective func-
tion for a spatial cooperative foraging task in NeuroSwarms simula-
tions [ 34] to predict reward-capture performance across two distinct
maze environments. Training the surrogate model was facilitated by theArray 15 (2022) 100218
9A. Hadzic et al.
Fig. 6. Anticipated future qEI-sampled parameter points. As in Fig. 3, a UMAP projection is shown across a series of plots: the top-left scatter plot assigns colors to each 2D UMAP
point based on the colorbar to the right of the plot as indexed by the surrogate model‚Äôs expected posterior mean for each associated parameter point; the remaining ùëù= 9plots
depict the same UMAP transformation except that the color of each point is mapped to the specified range (i.e., [min, max]) of the given NeuroSwarms parameter (cf. Table 1).
A large batch of 500 qEI-based parameter samples is shown to facilitate visual inspection of the local structure of the trained surrogate model. For instance, these plots show that
posterior sample means (top, left) have converged to similar high-performing values, and that most of the discovered system behaviors rely on short time-constants in the neural
controller‚Äôs dynamics (viz., the prevalence of red data points in the three ùúè‚àóplots). (For interpretation of the references to color in this figure legend, the reader is referred to
the web version of this article.)
Fig. 7. Example reward-capture trajectories from selected future qEI-sampled NeuroSwarms parameters. Pre-capture (left, blue traces) and post-capture (right, orange traces) pairs
of trajectory-trace plots are shown relative to example reward-capture events from qEI-selected simulations in the Hairpin ( A; cf. Fig. 4) and Tunnel ( B; cf. Fig. 5) mazes. Parameters
were selected for mid-range values (i.e., away from parameter range limits) from predictive (anticipated future) samples generated by the trained qEI-based surrogate model. Our
Bayesian batch-optimizer naturally produces diverse output parameters that allow for the selection of distinct high-performing solutions and system behaviors, all of which have
been equivalently constrained and guided by the high-dimensional shape of its task-dependent objective function.Array 15 (2022) 100218
10A. Hadzic et al.
qEI and qNoisyEI acquisition functions. In particular, qEI was shown to
guide optimizer trajectories towards parameter regions with high utility
improvement, outperforming random sampling and manual tuning.
By learning UMAP embeddings [57], we demonstrated visualiza-
tion of 9-dimensional parameter points to identify and select high
performing clusters of parameters. We illustrated the identification of
parameters that generalized across environments by jointly evaluating
the NeuroSwarms metacontroller in two distinct maze environments.
Overall, our study serves as an example application of Bayesian op-
timization of complex multi-agent models to explore and select for
complex behaviors like goal-directed spatial navigation in a system
with distributed neural control.
As parameter size grows, the computational cost of the matrix
inversions required to calculate updated Gaussian process parameters
increases exponentially and eventually outweighs the gains in adaptive
search efficiency provided by computing the acquisition function over
the surrogate model to advance the sample trajectory [20]. This limita-
tion on model dimensionality does not, in general, prohibit analysis of
complex dynamics, particularly in systems of homogeneous particles,
but it would reasonably detract the feasibility of Bayesian optimization
for modeling systems with nontrivial heterogeneity in agent/particle
behaviors. Within that moderate limit on model complexity‚Äîe.g., for ùëù
up to ‚àº20‚ÄîBayesian optimization may facilitate adaptive and efficient
computational exploration of dynamical parameter spaces, resulting in
the identification of distinct and complex system behaviors.
Future work is needed to develop new controller models and crit-
ical spatial tasks to explore the capabilities of multi-agent objective
functions that adapt efficiently to the characteristics of diverse environ-
ments (e.g., occlusive geometry, dynamic change, reward distribution,
cue richness, etc.). We theorize that heterogeneous variation of swarm
spatial structure and intertemporal coordination dynamics will be able
to support a form of swarm metacognition that allows adjustment to
the available goals in an environment, without initial knowledge of the
goals or their locations. This approach could extend the flexibility of
Bayesian optimization to operate in diverse environments and adapt
efficiently to tasks with difficult or uncertain goals.
CRediT authorship contribution statement
Armin Hadzic: Methodology, Software, Validation, Investigation,
Data curation, Writing ‚Äì original draft, Visualization. Grace M. Hwang:
Conceptualization, Methodology, Resources, Writing ‚Äì review & edit-
ing, Supervision, Project administration, Funding acquisition. Kechen
Zhang: Writing ‚Äì review & editing, Project administration, Funding ac-
quisition. Kevin M. Schultz: Conceptualization, Methodology, Formal
analysis, Resources, Writing ‚Äì review & editing, Supervision, Project
administration, Funding acquisition. Joseph D. Monaco: Conceptual-
ization, Methodology, Software, Validation, Formal analysis, Writing ‚Äì
review & editing, Supervision, Funding acquisition.
Declaration of competing interest
The authors declare that they have no known competing finan-
cial interests or personal relationships that could have appeared to
influence the work reported in this paper.
Acknowledgments
Funding for this work was provided by the National Science Foun-
dation (NCS/FO Award No. 1835279 to GMH, KZ, KMS, and JDM), the
NIH National Institute for Neurological Disorders and Stroke (NINDS
R03NS109923 to KZ and JDM), and Johns Hopkins University Applied
Physics Laboratory (JHUAPL) internal research and development pro-
grams (AH, GMH, and KMS). Additional support was provided to GMH
by the Johns Hopkins University Kavli Neuroscience Discovery Institute
and the JHUAPL Innovation and Collaboration Janney Program.References
[1] Passino KM. Biomimicry for optimization, control, and automation. Springer
Science & Business Media; 2005.
[2] Seeley TD, Morse RA, Visscher PK. The natural history of the flight of honey
bee swarms. Psyche 1979;86(2‚Äì3):103‚Äì13.
[3] Boinski S, Garber PA. On the move: How and why animals travel in groups.
University of Chicago Press; 2000.
[4] Couzin ID. Collective cognition in animal groups. Trends Cogn Sci
2009;13(1):36‚Äì43. http://dx.doi.org/10.1016/j.tics.2008.10.002.
[5] Sumpter DJ. Collective animal behavior. Princeton University Press; 2010.
[6] Herbert-Read JE, Perna A, Mann RP, Schaerf TM, Sumpter DJ, Ward AJ.
Inferring the rules of interaction of shoaling fish. Proc Natl Acad Sci USA
2011;108(46):18726‚Äì31. http://dx.doi.org/10.1073/pnas.1109355108.
[7] Beni G. From swarm intelligence to swarm robotics. In: International workshop
on swarm robotics. Springer; 2004, p. 1‚Äì9. http://dx.doi.org/10.1007/978-3-
540-30552-1_1.
[8] ≈ûahin E. Swarm robotics: From sources of inspiration to domains of application.
In: International workshop on swarm robotics. Springer; 2004, p. 10‚Äì20. http:
//dx.doi.org/10.1007/978-3-540-30552-1_2.
[9] Brambilla M, Ferrante E, Birattari M, Dorigo M. Swarm robotics: A review
from the swarm engineering perspective. Swarm Intell 2013;7(1):1‚Äì41. http:
//dx.doi.org/10.1007/s11721-012-0075-2.
[10] Bayƒ±ndƒ±r L. A review of swarm robotics tasks. Neurocomputing 2016;172:292‚Äì
321. http://dx.doi.org/10.1016/j.neucom.2015.05.116.
[11] Hasselmann K, Robert F, Birattari M. Automatic design of communication-based
behaviors for robot swarms. In: International conference on swarm intelligence.
Springer; 2018, p. 16‚Äì29. http://dx.doi.org/10.1007/978-3-030-00533-7_2.
[12] Brown DS, Turner R, Hennigh O, Loscalzo S. Discovery and exploration of novel
swarm behaviors given limited robot capabilities. In: Distributed autonomous
robotic systems. Springer; 2018, p. 447‚Äì60. http://dx.doi.org/10.1007/978-3-
319-73008-0_31.
[13] Coppola M, de Croon GC. Optimization of swarm behavior assisted by an
automatic local proof for a pattern formation task. In: International conference on
swarm intelligence. Springer; 2018, p. 123‚Äì34. http://dx.doi.org/10.1007/978-
3-030-00533-7_10.
[14] LeCun Y, Bengio Y, Hinton G. Deep learning. Nature 2015;521(7553):436‚Äì44.
http://dx.doi.org/10.1038/nature14539.
[15] Monaco JD, Rajan K, Hwang GM. A brain basis of dynamical intelligence for AI
and computational neuroscience. 2021, http://dx.doi.org/10.48550/arXiv.2105.
07284, ArXiv preprint.
[16] Price IC, Lamont GB. GA directed self-organized search and attack UAV swarms.
In: Winter simulation conference. IEEE; 2006, p. 1307‚Äì15. http://dx.doi.org/10.
1109/WSC.2006.323229.
[17] Quijano N, Passino KM. Honey bee social foraging algorithms for resource
allocation: Theory and application. Eng Appl Artif Intell 2010;23(6):845‚Äì61.
http://dx.doi.org/10.1016/j.engappai.2010.05.004.
[18] Lu Q, Hecker JP, Moses ME. Multiple-place swarm foraging with dynamic depots.
Auton Robot 2018;42(4):909‚Äì26. http://dx.doi.org/10.1007/s10514-017-9693-
2.
[19] Talamali MS, Bose T, Haire M, Xu X, Marshall JA, Reina A. Sophisticated
collective foraging with minimalist agents: A swarm robotics test. Swarm Intell
2020;14(1):25‚Äì56. http://dx.doi.org/10.1007/s11721-019-00176-9.
[20] Rasmussen CE. Gaussian processes in machine learning. In: Summer school on
machine learning. Springer; 2003, p. 63‚Äì71.
[21] Snoek J, Larochelle H, Adams RP. Practical Bayesian optimization of machine
learning algorithms. In: Pereira F, Burges C, Bottou L, Weinberger K, editors.
Advances in neural information processing systems 25. Curran Associates, Inc.;
2012, p. 2951‚Äì9.
[22] Roman I, Ceberio J, Mendiburu A, Lozano JA. Bayesian optimization for
parameter tuning in evolutionary algorithms. In: IEEE congress on evolution-
ary computation. IEEE; 2016, p. 4839‚Äì45. http://dx.doi.org/10.1109/CEC.2016.
7744410.
[23] Nguyen V. Bayesian optimization for accelerating hyper-parameter tuning. In:
IEEE second international conference on artificial intelligence and knowledge
engineering. IEEE; 2019, p. 302‚Äì5. http://dx.doi.org/10.1109/AIKE.2019.00060.
[24] Roman I, Mendiburu A, Santana R, Lozano JA. Bayesian optimization approaches
for massively multi-modal problems. In: International conference on learning and
intelligent optimization. Springer; 2019, p. 383‚Äì97. http://dx.doi.org/10.1007/
978-3-030-38629-0_31.
[25] Kieffer E, Rosalie M, Danoy G, Bouvry P. Bayesian optimization to enhance
coverage performance of a swarm of UAV with chaotic dynamics. In: Inter-
national workshop on optimization and learning. 2018, http://hdl.handle.net/
10993/35500.
[26] Rai A, Antonova R, Meier F, Atkeson CG. Using simulation to improve sample-
efficiency of Bayesian optimization for bipedal robots. J Mach Learn Res
2019;20(1):1844‚Äì67.
[27] Berkenkamp F, Krause A, Schoellig AP. Bayesian optimization with safety
constraints: Safe and automatic parameter tuning in robotics. Mach Learn
2021;20(1):1‚Äì35. http://dx.doi.org/10.1007/s10994-021-06019-1.Array 15 (2022) 100218
11A. Hadzic et al.
[28] Iwasa M, Iida K, Tanaka D. Hierarchical cluster structures in a one-dimensional
swarm oscillator model. Phys Rev E 2010;81(4):046220. http://dx.doi.org/10.
1103/PhysRevE.81.046220.
[29] Iwasa M, Tanaka D. Dimensionality of clusters in a swarm oscillator model. Phys
Rev E 2010;81(6):066214. http://dx.doi.org/10.1103/PhysRevE.81.066214.
[30] O‚ÄôKeeffe KP, Hong H, Strogatz SH. Oscillators that sync and swarm. Nature
Commun 2017;8(1):1504. http://dx.doi.org/10.1038/s41467-017-01190-3.
[31] O‚ÄôKeeffe K, Bettstetter C. A review of swarmalators and their potential in bio-
inspired computing. In: Proceedings of the international society for optics and
photonics (SPIE): Micro-and nanotechnology sensors, systems, and applications
XI, Vol. 10982. 2019, p. 383‚Äì94. http://dx.doi.org/10.1117/12.2518682.
[32] O‚ÄôKeeffe K, Ceron S, Petersen K. Collective behavior of swarmalators on a
ring. Phys Rev E 2022;105(1):014211. http://dx.doi.org/10.1103/PhysRevE.105.
014211.
[33] Monaco JD, Hwang GM, Schultz KM, Zhang K. Cognitive swarming: An approach
from the theoretical neuroscience of hippocampal function. In: Proceedings of the
international society for optics and photonics (SPIE): Micro-and nanotechnology
sensors, systems, and applications XI, Vol. 10982. 2019, p. 373‚Äì82. http://dx.
doi.org/10.1117/12.2518966.
[34] Monaco JD, Hwang GM, Schultz KM, Zhang K. Cognitive swarming in complex
environments with attractor dynamics and oscillatory computing. Biol Cybern
2020;114(2):269‚Äì84. http://dx.doi.org/10.1007/s00422-020-00823-z.
[35] Hwang GM, Schultz KM, Chalmers RW, Monaco JD, Zhang K. Autonomous
navigation technology. U.S. Patent 11,378,975 2022.
[36] Buzs√°ki G. Theta rhythm of navigation: Link between path integration
and landmark navigation, episodic and semantic memory. Hippocampus
2005;15(7):827‚Äì40. http://dx.doi.org/10.1002/hipo.20113.
[37] Monaco JD, Knierim JJ, Zhang K. Sensory feedback, error correction, and
remapping in a multiple oscillator model of place-cell activity. Front Comput
Neurosci 2011;5:39. http://dx.doi.org/10.3389/fncom.2011.00039.
[38] Blair HT, Wu A, Cong J. Oscillatory neurocomputing with ring attractors: A
network architecture for mapping locations in space onto patterns of neural
synchrony. Philos Trans R Soc Lond B Biol Sci 2014;369(1635):20120526.
http://dx.doi.org/10.1098/rstb.2012.0526.
[39] Monaco JD, De Guzman RM, Blair HT, Zhang K. Spatial synchronization codes
from coupled rate-phase neurons. PLOS Comput Biol 2019;15(1):e1006741. http:
//dx.doi.org/10.1371/journal.pcbi.1006741.
[40] Samsonovich A, McNaughton BL. Path integration and cognitive mapping in a
continuous attractor neural network model. J Neurosci 1997;17(15):5900‚Äì20.
http://dx.doi.org/10.1523/JNEUROSCI.17-15-05900.1997.
[41] Zhang K. Representation of spatial orientation by the intrinsic dynamics of
the head-direction cell ensemble: a theory. J Neurosci 1996;16(6):2112‚Äì26.
http://dx.doi.org/10.1523/JNEUROSCI.16-06-02112.1996.
[42] Knierim JJ, Zhang K. Attractor dynamics of spatially correlated neural activity
in the limbic system. Annu Rev Neurosci 2012;35:267‚Äì85. http://dx.doi.org/10.
1146/annurev-neuro-062111-150351.
[43] Lansner A. Associative memory models: from the cell-assembly theory to
biophysically detailed cortex simulations. Trends Neurosci 2009;32(3):178‚Äì86.
http://dx.doi.org/10.1016/j.tins.2008.12.002.
[44] O‚ÄôHagan A. Curve fitting and optimal design for prediction. J R Stat Soc
Ser B Methodol 1978;40(1):1‚Äì24. http://dx.doi.org/10.1111/j.2517-6161.1978.
tb01643.x.
[45] Jones DR, Schonlau M, Welch WJ. Efficient global optimization of expensive
black-box functions. J Global Optim 1998;13(4):455‚Äì92. http://dx.doi.org/10.
1023/A:1008306431147.
[46] Osborne MA. Bayesian Gaussian processes for sequential prediction, optimisation
and quadrature (Ph.D. thesis), UK: Oxford University; 2010.
[47] Zhu C, Byrd RH, Lu P, Nocedal J. Algorithm 778: L-BFGS-B: Fortran subrou-
tines for large-scale bound-constrained optimization. ACM Trans Math Software
1997;23(4):550‚Äì60. http://dx.doi.org/10.1145/279232.279236.
[48] Williams CK. Prediction with Gaussian processes: From linear regression to linear
prediction and beyond. In: Learning in graphical models. Springer; 1998, p.
599‚Äì621. http://dx.doi.org/10.1007/978-94-011-5014-9_23.
[49] MacKay DJC. Gaussian processes‚Äîa replacement for supervised neural
networks? In: Lecture notes from NeurIPS. 1997.
[50] Krauth K, Bonilla EV, Cutajar K, Filippone M. AutoGP: Exploring the capabilities
and limitations of Gaussian process models. 2016, http://dx.doi.org/10.48550/
arXiv.1610.05392, ArXiv preprint.
[51] Balandat M, Karrer B, Jiang DR, Daulton S, Letham B, Wilson AG, Bakshy E.
Botorch: A framework for efficient Monte-Carlo Bayesian optimization. In:
Larochelle H, Ranzato M, Hadsell R, Balcan MF, Lin H, editors. Advances in
neural information processing systems, 33. Curran Associates, Inc.; 2020, p.
21524‚Äì38.[52] Shahriari B, Swersky K, Wang Z, Adams RP, De Freitas N. Taking the human out
of the loop: A review of Bayesian optimization. Proc IEEE 2015;104(1):148‚Äì75.
http://dx.doi.org/10.1109/JPROC.2015.2494218.
[53] Wilson JT, Hutter F, Deisenroth MP. Maximizing acquisition functions
for Bayesian optimization. 2018, http://dx.doi.org/10.48550/arXiv.1805.10196,
ArXiv preprint.
[54] Letham B, Karrer B, Ottoni G, Bakshy E, et al. Constrained Bayesian optimization
with noisy experiments. Bayesian Anal 2019;14(2):495‚Äì519. http://dx.doi.org/
10.1214/18-BA1110.
[55] Scott W, Frazier P, Powell W. The correlated knowledge gradient for simulation
optimization of continuous parameters using Gaussian process regression. SIAM
J Optim 2011;21(3):996‚Äì1026. http://dx.doi.org/10.1137/100801275.
[56] Frazier PI. A tutorial on Bayesian optimization. 2018, http://dx.doi.org/10.
48550/arXiv.1807.02811, ArXiv preprint.
[57] McInnes L, Healy J, Melville J. UMAP: Uniform manifold approximation and
projection for dimension reduction. 2018, http://dx.doi.org/10.48550/arXiv.
1802.03426, ArXiv preprint.
Mr.Armin Hadzic is a computer vision researcher at the Johns Hopkins University
Applied Physics Laboratory. He has a B.S. in Computer Engineering, a B.S. in Electrical
Engineering, and an M.S. in Computer Science from the University of Kentucky. His
research interests include developing deep learning methods to address challenges in
latent information representation from multiple sources, as well as computer vision,
reinforcement learning, remote sensing, and brain-inspired robotics.
Dr.Grace M. Hwang is a senior staff engineer at the Johns Hopkins University Applied
Physics Laboratory in Laurel, Maryland, and faculty member of the Johns Hopkins Uni-
versity Kavli Neuroscience Discovery Institute. Dr. Hwang currently serves as a Program
Director at the National Science Foundation in Alexandria, Virginia. She received a B.S.
in Civil and Environmental Engineering from Northeastern University, an M.S. from
the Massachusetts Institute of Technology, and an M.S. and Ph.D. in Biophysics and
Structural Biology from Brandeis University. Her areas of expertise include biophysics,
biosensors, biophotonics, brain‚Äìcomputer interface technologies, brain-derived artificial
intelligence, computational neuroscience, and disability/rehabilitation engineering.
Dr.Kechen Zhang is an Associate Professor in the Department of Biomedical Engi-
neering at the Johns Hopkins University School of Medicine in Baltimore, Maryland.
He has a B.S. in Biophysics and Physiology, an M.S. in Neurobiology from Peking
University, and a Ph.D. in Cognitive Science from the University of California San
Diego. He completed a postdoctoral fellowship in computational neurobiology at the
Salk Institute for Biological Studies in La Jolla, California. His research interests span
the breadth of theoretical and computational neuroscience, with a particular focus on
sensory coding and spatial representations related to the hippocampal system.
Dr.Kevin M. Schultz is an applied mathematician, senior scientist, and project
manager in the experimental and computational physics group at the Johns Hopkins
University Applied Physics Laboratory in Laurel, Maryland. He has a B.S. in Math-
ematics, and a B.S., M.S., and Ph.D. in Electrical and Computer Engineering from
Ohio State University. His research interests include quantum characterization, control,
sensing, and the application of signal processing and statistics to the domain of quantum
information. Dr. Schultz‚Äô research interests additionally span distributed control and
signal processing for applications including UAV swarming, sensor networks, critical
infrastructure resilience, and neuroscience.
Dr.Joseph D. Monaco is formerly a Research Associate faculty member of the
Department of Biomedical Engineering at the Johns Hopkins University School of
Medicine in Baltimore, Maryland. He received B.A. degrees in Cognitive Science and
Mathematics from the University of Virginia, and an M.A. and Ph.D. in Neurobiology
& Behavior from the Columbia University Center for Theoretical Neuroscience. His
research has examined the neural computations of spatial cognition by modeling the
cellular and network dynamics of the hippocampal complex. Dr. Monaco is currently
conducting independent research toward the theoretical integration of dynamical
neuroscience and embodied cognition to broadly advance the science of intelligence.