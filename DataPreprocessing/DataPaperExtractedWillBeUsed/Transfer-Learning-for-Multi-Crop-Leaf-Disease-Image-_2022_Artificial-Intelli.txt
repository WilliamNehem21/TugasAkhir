Transfer Learning for Multi-Crop Leaf Disease Image Classiﬁcation using
Convolutional Neural Network VGG
Ananda S. Paymode ⁎, Vandana B. Malode
MGM's Jawaharlal Nehru Engineering College, Aurangabad 431001, Maharashtra, India
a b s t r a c t
a r t i c l e
i n f o
Article history:
Received 9 October 2021
Received in revised form 8 December 2021
Accepted 30 December 2021
Available online 7 January 2022
In recent times, the use of artiﬁcial intelligence (AI) in agriculture has become the most important. The technol-
ogy adoption in agriculture if creatively approached. Controlling on the diseased leaves during the growing stages
of crops is a crucial step. The disease detection, classiﬁcation, and analysis of diseased leaves at an early stage, as
well as possible solutions, are always helpful in agricultural progress. The disease detection and classiﬁcation of
different crops, especially tomatoes and grapes, is a major emphasis of our proposed research. The important ob-
jective is to forecast the sort of illness that would affect grapes and tomato leaves at an early stage. The
Convolutional Neural Network (CNN) methods are used for detecting Multi-Crops Leaf Disease (MCLD). The fea-
tures extraction of images using a deep learning-based model classiﬁed the sick and healthy leaves. The CNN
based Visual Geometry Group (VGG) model is used for improved performance measures. The crops leaves images
dataset is considered for training and testing the model. The performance measure parameters, i.e., accuracy, sen-
sitivity, speciﬁcity precision, recall and F1-score were calculated and monitored. The main objective of research
with the proposed model is to make on-going improvements in the performance. The designed model classiﬁes
disease-affected leaves with greater accuracy. In the experiment proposed research has achieved an accuracy of
98.40% of grapes and 95.71% of tomatoes. The proposed research directly supports increasing food production in
agriculture.
© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Keywords:
Convolutional Neural Network (CNN)
Artiﬁcial Intelligence (AI)
Visual Geometry Group (VGG)
Multi-Crops Leaf Disease (MCLD)
1. Introduction
To contribute to the development of nations, knowledge of agricul-
ture sectors is crucial. Agriculture is a one-of-a-kind source of wealth
that develops farmers. For a strong country, the development of farming
is a necessity and a need in the global market. The world’s population is
growing at an exponential rate, necessitating massive food production
in the next 50 years. Information about different types of crops and dis-
eases occurring at each level and its analysis at an early stage play a key
and dynamic role in the agriculture sector. A farmer's main problem is
the occurrence of various diseases on their crops. The disease classiﬁca-
tion and analysis of illnesses is a crucial concern for agriculture's
optimum food yield. Food safety is a huge issue due to a lack of infra-
structure and technology, so crop disease classiﬁcation and identi-
ﬁcation are important to be considered in the coming days. This is
necessary for yield estimation, food security, and disease management.
Detection and recognition of crops illnesses is an important study topic
because it could be capable of monitoring huge ﬁelds of crops and de-
tecting disease symptoms as soon as they occur on plant leaves. As a
result, ﬁnding a quick, efﬁcient, least inexpensive, and effective
approach to determine crops diseases instances is quite important
(C. J. Chen et al., 2021).
Artiﬁcial intelligence (AI) provides considerable assistance to agri-
culture, which enhances a nation's gross domestic product (GDP)
mostly through this sector. Climate change, labour scarcity, rainy season
uncertainty, natural disasters, and various diseases on plant leaves are
all major issues in agriculture. The plant leaves recognition and detec-
tion studies with edge intelligence applied to agriculture. There is a
new advancement with different deep learning models that overcomes
the challenge. The YOLOv3 neural network model is based on deep
learning and is built on an embedded system and the NVIDIA Jetson
TX2. The system is implemented on a drone, and photographs of plants
are taken, pest positions are identiﬁed, and pesticides are applied as
needed; this is a novel approach based on deep learning (Al Hiary
et al., 2011).
Hyper spectral and multispectral knowledge acquisition techniques
and applications have exhibited their utility in improving agricultural
production and practises by providing farmers and agricultural manage-
ment with crucial data on the elements impacting crop condition and
growth. This technology has been widely employed in a variety of agri-
cultural applications, including sustainable agriculture (Ang, 2021).
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
⁎ Corresponding author.
E-mail addresses: anandpaymode@gmail.com (A.S. Paymode),
vandanamalode@jnec.ac.in (V.B. Malode).
https://doi.org/10.1016/j.aiia.2021.12.002
2589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
Weed detection in vegetable plantations is more difﬁcult than in crop
plantations due to uneven plant spacing. Deep Learning technology is
a novel method that blends with image processing. This approach con-
centrates solely on recognising plants, avoiding the handling of numer-
ous plant species. Furthermore, by reducing the amount of training
image collection and even the complexity of weed detection, this tech-
nique can improve plant diagnosis accuracy and performance (Jin et al.,
2021).
The massive crop loss occurred because of the failure to predict dis-
ease at an early stage, which always results in lower crop production. As
a result, identifying and analysing crop diseases is a critical step in en-
suring crop quality (Wu, 2020). As high computing speed and power
have recently improved, the availability of massive datasets improves
the system's efﬁciency.
In this section, there are various techniques for detecting and classi-
fying crop leaf disease. We present the related survey as a system that
employs a variety of classiﬁer techniques. There are two types of combi-
nations: serial and hybrid, with the combination of serial and parallel
achieving the signiﬁcant performance parameter within 600 images
(Massi et al., 2020). The hybrid combination has a recognition rate of
91.11%, which is higher than the serial, parallel, and deep learning ap-
proaches. For identifying and analyzing leaf illness, a deep learning
convolutional neural network (CNN) model was used to classify healthy
and sick images. The model train contained 25 different plants, 58 clas-
ses’ sets, including healthy and diseased plants, and had 87,848 images.
Using several (Ferentinos, 2018) model architectures, the best perfor-
mance success rate was 97.53%. The Multi-Context Fusion Network
(MCFN), a deep learning-based method, is built and prepared for crop
disease detection. The MCFN aids in the extraction of visual information
from 50,000 crop photos. The MCFN produced 77 common crops in-
fected using a deep fusion model, with a 97.50% identiﬁcation accuracy
(Jin et al., 2021).
The identiﬁcation of weeds in crops using the CovNet algorithm is
also a potent and cutting-edge approach. In recent research, bounding
boxes were drawn across cropped images and the model was trained.
Colour-based segmentations are applied to images and colour informa-
tion, and visual categorization is calculated for weed images. The colour
index was examined with a genetic algorithm and Bayesian categoriza-
tion (Jin et al., 2021). The deep residual network and the deep dense
network are combined in the hybrid deep learning model. The hybrid
deep learning model reduces training parameters while increasing ac-
curacy by up to 95.00 % (Zhou et al., 2021).
Deep transfer learning is an amazing performance methodology
for identifying plant diseases. For pre-trained datasets, Inception
and ImageNet modules were utilized (Chen et al., 2020). The perfor-
mance of pepper, vegetable, potato, and tomato leaf images in the
plantvillage database was studied and enhanced using support vec-
tor machine (SVM) and multi-layer perceptron. After training the
model the system achieves a higher performance accuracy of 94.35
% (Kurmi et al., 2020).
To detect and recognize corn dietary sickness, a Deep Convolutional
Neural Network was deployed., The recognition of corn leaf diseased ac-
curacy was 88.46 %, and the usage of hardware, such as a raspberry pi3
with an Intel Movidius Neural Compute Stick and a system GPU that
pre-trained the CNN Model, resulted in superior metric accuracy perfor-
mance (Sun et al., 2020).
With the rapid growth of artiﬁcial intelligence and deep learn-
ing technology, computer vision (CV) made a breakthrough. The
CV-based approaches are commonly utilized for diagnosing grape
leaf diseases. The principle component analysis (PCA) and back
propagation methods aid in the diagnosis of grape diseases such
as downy mildew and powdery mildew, with a research accuracy
of 94.29 % (Xie et al., 2020), using VGGNet. The weights are initial-
ized using ImageNet pre-trained datasets, and over through the
real - world dataset, such approaches had a validation accuracy of
91.83 %.
2. Material & methods
2.1. Datasets
To support our research in the area of collection of images available
from Pennsylvania state university named plantvillage dataset. The
dataset plant-village included 152 crop solutions, 38 crop classes, and
19 crop categories, for 54,303 crop leaves images. In the datasets, high
quality JPEG image format with 5471width and 3648 height pixels are
available. In the pre-processing, de-nosing, segmentation and after
images are 256 X 256 pixels (Gandhi et al., 2018). The plantvillage is a
well-known dataset for crop disease, with a large number of public
datasets available. A plantvillage dataset images were captured in the
lab, thus they are used as training datasets. Our model tested on real
ﬁeld captured images, As a result, we must concentrate on developing
our own ﬁeld database. The test images were captured with a separate
Megapixel camera and stored in a database. The datasets prepared in
the ﬁeld are available and be used in the proposed research. The agro-
deep mobile application was used to capture some on-ﬁeld crops images.
The ﬁeld photographs were taken with the redmi Note 5 Pro MIUI
Global 11.0.5.0(PRIMEXM), Android Version PKQ1.180904.001, and a
camera frame 4:3 high picture quality on 16 MP+5MP with f/2.2 aper-
ture pixel, in a variety of natural environments. The disease-affected and
healthy photos are the most common image categories collected for re-
search purposes. Healthy spot contaminated, mosaic virus, yellow leaf
curl virus, septoria leaf spot bacterial spot, early blight, late blight, leaf
mould, septoria leaf spot, and spider mites are examples of tomato
imagery.
2.2. Proposed research
The schematic in Fig 1 depicts a potential view for multi-crop leaf
disease classiﬁcation and analysis. Initially, plant leaf disease images
are collected and classiﬁed into several categories. Picture ﬁltering,
grey transformation, picture sharpening, and scaling are some of the
image-processing techniques. By using data augmentation methods,
new sample photos are created from available photos to enhance and
prepare the dataset. Augmentation procedures like turning, translation,
and randomized transformation are employed to enhance the size of
the dataset. The photos are then used as input to the suggested approach
for training the model in the following stage. The newly trained architec-
tural model is used to anticipate previously unseen images. Eventually,
the ﬁndings of plant disease detection and identiﬁcation are achieved.
Finally, complete details of these steps are depicted in later parts (Table 1).
2.3. Sample images category
The sample images of crops shown in Fig. 2 depict the category of
ﬁeld’s tomato leaf images of various disease and healthy classes. The im-
ages are one-of-a-kind for each type of disease symptom, pattern, spot,
and colour mark. Speciﬁc tomato plant leaf diseases such as bacterial
wilt, leaf mold, and grey spots are identiﬁed and detected as disease im-
pacted recognition traits (Paymode et al., 2020).
Fig. 3 depicts ﬁeld images of grape vine leaves obtained in the Nashik
district of Maharashtra, India. A grape category, Healthy 423, Black Rot
1180, Black Measles 1383 and Leaf Blight 1076 images were recorded,
recognized, and captured. The datasets for grape plant leaves were gen-
erated by adjusting the brightness and hue of images from the A to D
category (See Figs. 4-5).
The second crop of tomatoes sampled Early blight 1000, Mosaic virus
373, Bacterial spot 2127, Late blight 1909, Leaf mould 952, Septoria leaf
spot 1771, 1404 spot, spider mites 1676 and Yellow leaf curl 3209. The
deep learning based methods are state-of-the-art in computer vision,
whichisusedinimagerecognitionandclassiﬁcation.Ingeneral,datasetcol-
lecting, data pre-processing, image segmentation, feature extraction, and
classiﬁcation are the four stages of Artiﬁcial Intelligence (AI) in agriculture
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
24
approaches for crop leaf disease detection and classiﬁcation utilising
Convolutional Neural Network (CNN). A Google Colaboratory platform
was used to pre-process the image, extraction of features, and classify it.
2.4. Image augmentation
The large number of datasets improves the learning algorithms' perfor-
mance and prevents overﬁtting. Obtaining a real-time dataset for use as
input to a training model is a complex and time-consuming operation. As
a result, data augmentation broadens the range of training data available
to deep learning models. Deep learning-based augmentation approaches
include image ﬂipping, cropping, rotation, colour transformation, PCA
colour augmentation, noise rejection, Generative Adversarial Networks
(GANs), and Neural Style Transfer (NST) (Arun Pandian et al., 2019). The
Faster DR-IACNN approach for detecting grape leaf diseases is based on
deep learning. The automatic extraction of spots on leaves has a high de-
tection speed and accuracy. There are 4449 original photographs and
62,286 photos developed using data augmentation techniques.
Fig 1. Proposed research system ﬂow diagram.
Table 1
A study of deep learning techniques with classiﬁcation and recognition rate (See Fig. 12).
Approach
Classiﬁcation
Model
Recognition rate (%)
Hybrid Combination (Massi et al., 2020)
Three SVM
SVM
91.11
Deep Learning (Ferentinos, 2018)
CNN
VGG
97.53
Multi-Context Fusion Network (MCFN) (Wu, 2020)
CNN
AlexNet &VGG16
97.50
Deep Transfer Learning (DTL) (Chen et al., 2020)
CNN
VGG
91.83
Machine Learning (Kurmi et al., 2020)
SVM
MLP
94.35
Deep Learning (Sun et al., 2020)
DCNN
DCNN
88.46
Deep Learning (Xie et al., 2020)
Faster DR-IACNN
Inception-v1 ResNet-v2
81.11
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
25
The images are converted into a vector of ﬁxed features through fea-
ture extraction in segmentation. The color, texture, and shape are the
system-adopted features. A means, conﬁdence intervals, and sleekness
have been employed as colored methods, with HSV and RGB color
spaces being retrieved. The gray-level co-occurrence matrix is preferred
when extracting texture features from a colour image. This approach is
used to identify plant diseases.
2.5. Transfer learning
The model's optimization and training is a tough and time-
consuming operation. A powerful graphical processing unit (GPU)
is required for the training, as well as millions of training examples.
However, transfer learning, which is employed in deep learning,
solves all of the problems. The pre-trained Convolutional Neural
Network (CNN) used in transfer learning is optimized for one task
and transfers knowledge to different modes (Nevavuori et al.,
2019). The multi-crop image dataset model comprises a size of
224 X 224. The residual network (ResNet) needed to be tweaked.
In all ResNet models, the ﬁnal layer before the softmax is a 7 X 7
average-pooling layer. A smaller image can ﬁt into the network
when the pooling size decreases. The basic picture preparation is
necessary for the transfer learning considerations with the multi-
cropped image dataset.
3. Results & discussion
3.1. Convolutional neural network
The convolutional layers, pooling layers, fully-connected layers, and
dense layers constitute the architecture of the Convolutional Neural
Network (CNN) (See Fig. 6). The layers' description is shown below.
3.1.1. Convolutional layer
Convolutional layers' fundamental function is to extract unique fea-
tures from images. The implementation of convolutional layers on a
normal basis facilitates the extraction of input features (Chen et al.,
2020), The features extraction (Hi) among several layers in CNN is com-
puted using the formula below.
Hi ¼ φ Hi−1 Wi þ bi
ð
Þ
ð1Þ
Where, Hi - Feature map, Wi –Weight, bi is offset and φ – Rectiﬁed Lin-
ear Unit (RELU)
3.1.2. Pooling layers
The pooling layers are a crucial component of a Convolutional Neural
Network (CNN). It shrinks the size of convolved features in dimension
while simultaneously minimizing the computational resources neces-
sary for image processing. Pooling arise categorized into two types:
max pooling and average pooling. Max pooling returns the maximum
value of images, whereas an average pooling returns the average value
of the image section.
3.1.3. Drop-out layers
The dropout layers improve the capability of a trained model. It pro-
vides regularization and prevents the model from over-ﬁtting by de-
creasing the correlation between the neurons. The drop out process is
used in all the activation functions but it is scaled by factor (Liu, 2020).
3.1.4. Flatten layers
It collapses the spatial dimensions of the mapped pooled features
while retaining the channel dimensions. The ﬂattened layer adds extra
dimensions and after it is transformed into a vector. The vectored feed
Fig 2. Sample tomato leaf images (A: Mosaic Virus, B: Healthy, C: Target Spot, D: Late Blight, E: Bacterial Spot, F: Septoria Spot, G: Spider Mite, H: Leaf Mold, I: Early Blight, J: Yellow Leaf.
Fig 3. Sample grapes plant leaf images. (A: Grape Black Rot, B: Grape Esca (Black Measles), C: Healthy, D Grape Leaf blight (Isariopsis Leaf Spot).
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
26
to fully connected layers also known as the dense layer or fully con-
nected layers.
3.1.5. Fully-connected layers
Fully connected layers are needed for extracted images classiﬁcation
features because of their special purpose. The softmax function predicts
earlier extracted image attributes from preceding layers. Softmax is a
multiclass classiﬁcation activation function in the output layers. The
neural network layer uses a multilayer perceptron model (MLP) as a
classiﬁer for two-class classiﬁcation. The model with nonlinearity,
which is introduced in the full vectors using rectiﬁed linear unit
(RELU) activation. The versatility of class separation is greater when
employing a support vector machine (SVM). The essentials of SVM are
as described in the following:
Minimize 1=2 ∑
n
j¼1
W2
1 þ C ∑
N
j¼1
ξ j
ð2Þ
Where C is the tuning measure, subject to the constraint yj (W∙X + b) ≥
1 – ζ, j = 1, 2, 3… N. The softmax parameter γ = 1 and C = 1 are used
throughout training and test sets of the classiﬁcation algorithm.
Fig 4. Multi-crops image augmentation (a) (A: Original B: Rotate, C: Color, D: Image Point, E: Hstack, F: Size G: Gaussian Noise, H: Shape).
Fig 5. A B: H Stack, C: Original, D: Augmentation, E: Batch H stack, F: Adaptive Gaussians Noise.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
27
The ConvNet architecture design's main component is its depth. By
deﬁning additional design parameters and growing the network depth
continuously, by adding more convolutional layers that are doable by
using extremely small (3 x 3) convolution ﬁlters in all layers. As a result,
they've developed substantially more accurate ConvNet architectures
that not only reach state-of-the-art accuracy on ﬁxed dataset classiﬁca-
tion and localisation tasks, but are also applicable to other image recog-
nition datasets, where they perform admirably even when utilised as
part of relatively simple pipelines(Simonyan and Zisserman, 2015).Our
ConvNets are fed a ﬁxed-size 224 x 224 RGB picture during training.
The only pre-processing we perform is removing each pixel from the
mean RGB value determined on the training set. We apply ﬁlters with
a very small receptive ﬁeld 3 x 3 to send the image through a stack of
convolutional layers. We also use 1 x 1 convolution ﬁlters in one of the
conﬁgurations, which are a linear change of the input channels (followed
by non-linearity). The convolution stride is set to 1 pixel, and the spatial
padding of the convolutional layer input is set to 1 pixel for 3 conv. layers
so that the spatial resolution is kept after convolutional. Five max-
pooling layers, which follow part of the convolutional layers, do spatial
pooling (not all the convolutional layers are followed by max pooling)
Max-pooling is done with stride 2 over a 2 x 2 pixel window.
3.2. VGG16
The Convolutional Neural Network based VGG16 pre-trained
models are used to improve the performance and classify the crop
images as healthy and disease images. For quality detection and
analysis of crop leaf images, the initial model transfers information
from pre-trained VGG16 models. The Convolutional Neural Network
(CNN) model retained new images of the ﬁeld and learned to per-
form a model for disease detection and classiﬁcation (Alencastre-
Miranda et al., 2021).
The VGG model improved with large kernel-sized ﬁlters, with 11
and 5 convolutional layers with a 3 x 3-kernel ﬁlter size. The input
image size is ﬁxed at 224 x 224. Following image pre-processing, images
were passed through a convolutional layer with a ﬁlter size of (3 x 3).For
linear transformation of the input channel, the ﬁlter size is set to (1 x 1).
The stride size is ﬁxed to 1 and max pooling is performed with 2 x 2 sizes
and stride set to 2. In the next steps, fully connected layers have the
same conﬁguration with 4096 channels in each layer. The ﬁnal layer is
the softmax activation layers, followed by the RELU activation functions
(See Fig. 7).
3.3. Performance measure
The F1 score, accuracy matrix, and Receiver operating characteristic
(ROC), as well as the area under the curve (AUC), are being used to eval-
uate segmentation performance (AUC). The performance of the classi-
ﬁer is measured using evaluation metrics.
3.3.1. Accuracy metrics
The model performance for all classes is accurately measured. The
accuracy is calculated by adding the total number of correct predictions
to the total number of predictions. The performance parameter calcula-
tion of precision and recall and F1-Score are measured. The accuracy is
expressed in terms as follows.
AC ¼
ðTP þ TN
Þ
ðTP þ FP þ FN þ TN
Þ
ð3Þ
Where, TP is True Positive, TN True Negative, FN False Negative and
FP False Positive Samples. The classiﬁer performance measure using
evaluation metrics are gives as;
Fig 6. Proposed convolutional neural network CNN architecture.
Fig 7. Proposed convolutional neural network (CNN) VGG16 architecture.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
28
TPR ¼ Sensitivity
ð
Þ ¼
TP
ðTP þ FN
Þ , TNR ¼ Specificity
ð
Þ
TN
ðTN þ FP
Þ
ð4Þ
FPR ¼
FP
ðFP þ TN
Þ
ð5Þ
Where, TPR is True Positive Rate, TNR True Negative Rate, and FPR
False Positive Rate.
Precision ¼
TP
ðTP þ FP
Þ , Recall ¼
TP
ðTN þ FN
Þ
ð6Þ
G−Mean ¼
∏
m
K¼1
RecallK
�
� 1
m
ð7Þ
Here m represents the number of categories and G denotes the TNR
and FPR accuracy ratio.
Mean average precision (mAP), which consists of Precision, Recall,
and Mean, is the algorithm assessment standard employed. Image pro-
cessing and detection rely heavily on the mAP. From the entire results,
the accuracy has classiﬁed correctly. From the complete ﬁndings, the re-
call is correctly classiﬁed.
The F1 score is another important metric for evaluating the algo-
rithm. It's precision and recall fundamental that's presented as follows:
F1 Score ¼ 2 � Precision � Recall
ðPrecision þ Recall
Þ
ð8Þ
3.3.2. Receiver operating characteristic
The receiver operating characteristic (ROC) curve is used to understand
deterministic indications of categorization sorting as well as computational
modeling challenges. The curve is a graph that shows the ratio of false pos-
itives to true positives under different standard limits (See Fig. 8).
A prototype also with largest true negative rate values was used to cor-
rectly categorize defectives, and the model with the highest true positive
rate values was used to correctly classify healthily. To boost productivity
by reducing processing time for training and testing, the MCC (Matthews
Correlation Coefﬁcient) is employed for the total computation. MCC is a
criterionfor categorizing complex data into distinct categories. MCC is a su-
perior method to accuracy which only has signiﬁcant importance if the
true positives, true negatives, false negatives, and false positives outcomes
are all positive. The MCC ranges from 1 (poorest judgment) to 1 (perfect
predictions), with an MCC of 0 suggesting a random guess.
The model is tuned by the number of epochs, hidden layers, hidden
nodes, activation functions, dropout, learning rates, and batch size. The
model performance is affected by hyper parameter tuning. The term
"hyper parameter tuning" refers to the process of repeatedly adjusting hid-
den layers, epochs, activation function, or learning rate. The model is ﬁne-
tuned to achieve the best accuracy while minimising the average loss.
The experimental analysis was carried out on Google research prod-
ucts on Google Colaboratory. The Colaboratory platform supports python
programming, and nearly all of the Python libraries are uploaded and
installed for research purposes. The Python 3 Google Compute Engine
backend (GPU) with RAM of 12.72 GB and disc space of 68.40 GB is avail-
able while experimenting. The dataset is uploaded with the drive
mounted, and the model is trained on the Google platform with high
conﬁgurations. A Python convert to image function is used for converting
all the images to an array and fetching images from the directory.
The processed images come from a directory, and all label images are
transformed using the label binarized sklearn python package. The total
number of images is divided into train and test using train-test-split py-
thon functions. The model parameters were set as shown in Table 2, and
the model was trained to calculate all trainable and non-trainable pa-
rameters. The Adam optimization algorithm is used to train the deep
learning convolutional neural network model. The algorithm optimized
the sparse gradient noise issue.
The input network uses 224 X 224 images, and the batch size is 30
for grapes and 25 for tomatoes, respectively, and the same test is per-
formed for different epochs with batch size and learning rate. In every
polling layer with a 2 x 2-pool size and the RELU function utilized in
the network, the model performs a max-pooling operation. The output
of the last layer is a oftmax-activation multi-crop-developed prediction.
During the network's training phase, hyper parameters such as learning
rate and epoch size were adjusted. The average accuracy achieved was
98.40% for grapes and 95.71% for tomatoes, respectively. The learning
rate is tested at different values to optimize targeted performance mea-
sured. The validation process is based on a total number of images from
the multi-crop dataset. With the setting of different epochs and batch
size, the accuracy improved and grew.
The crops-leaf images datasets are used to train the model and iden-
tiﬁcation of class and category of disease with transfer learning tech-
niques including VGG16. The original datasets are divided into
training data 80%, validation data 10% and testing data 10%.
Fig 8. Receiver operating characteristics FP versus TP
Table 2
Parameter setting for trained the model
Hyperparameter
Value Setting
Crops
Grapes & Tomatoes
Convolutional Layers
13
Max Pooling Layer
5
Dropout Rate
015/0.25/0.50
Activation Function
Relu, Softmax
Epochs
20/25/30/40/45
Learning Rate
0.00001/0.0001
Image Size
224 x 224 x 3
Table 3
Experimental results of the grape model for setting different parameters.
No. of epochs
Learning rate
Dropout rate
No. of images
Training loss
Training accuracy
Validation loss
Validation accuracy
40
0.00001
0.25
450
0.0897
0.9840
0.0486
0.9889
30
0.0001
0.50
450
0.1136
0.9585
0.0686
0.9867
45
0.00001
0.25
400
0.0995
0.9796
0.0529
0.9858
45
0.0001
0.25
750
0.0875
0.9696
0.0521
0.9853
30
0.0001
0.50
450
0.1326
0.957
0.0686
0.9843
40
0.001
0.30
600
0.1139
0.9606
0.0529
0.9831
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
29
3.4. Training and validation accuracy
Training and validation accuracy is measured by setting differ-
ent values while training the model. The experiments were carried
out at Google Colaboratory on the available RAM of 12.50 GB. While
performing the experiment, different values are set for the follow-
ing: the number of epochs, learning rate, dropout rate, and the
number of images noted as training loss, training accuracy, valida-
tion loss, and validation accuracy. A model's performance is mea-
sured and veriﬁed on the grape and tomato crops' leaves. Table 3
and Table 4 show the details of the results of experiments carried
on grapes and tomatoes, respectively.
3.5. Figures and graphs
A model's performance is measured and veriﬁed with training, test-
ing, and validation methods for grapes and tomatoes leaves. Fig 9 and
Fig. 10 show the training and validation accuracy and loss of the grape
leaves and tomatoes, respectively.
The confusion matrix has been used to measure the performance pa-
rameter for grapes and tomatoes leaves, as shown in Fig. 11. Experiment
with the facts collected. The suggested approach is tested using our
grapes and tomatoes image datasets, which were taken in a real-ﬁeld
with various backdrop and light intensities, similar to the tests done in
Section 4.4.
Table 4
Experimental results of the tomatoes model for setting different parameters.
No. of epochs
Learning rate
Dropout rate
No. of images
Training loss
Training accuracy
Validation loss
Validation accuracy
25
0.0001
0.25
200
0.1643
0.9571
0.2627
0.9432
35
0.00001
0.20
180
0.2203
0.9281
0.3143
0.9013
30
0.00001
0.15
200
0.2624
0.9097
0.3345
0.8926
30
0.00001
0.25
200
0.3042
0.8983
0.3736
0.8849
30
0.00001
0.25
180
0.4871
0.8354
0.4149
0.8671
30
0.00001
0.50
200
0.5226
0.8255
0.4508
0.8538
Fig 9. Training and validation. (a) Accuracy and (b) loss of VGG16 grapes.
Fig 10. Training and validation. (a) Accuracy and (b) Loss of VGG16 tomatoes.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
30
To assure the diversity of sample images and avoid the over ﬁtting
problem, data augmentation techniques such as random rotation, ﬂip-
ping, and scale transform, as well as associated pre-processing activities,
are used to extend the training samples. The processes are described in
more detail below.
1. Image resize: The total images scaled into size of 224 x 224 pixels, for
the model ﬁt and minimum 200 images taken from each healthy and
unhealthy category are augmented with data augmentation methods.
2. Image pre-processing: Image pre-processing is used to darken the
different lengths of the image data, going to bring them into ratio
Fig 11. Confusion matrix. (a) Tomatoes and (b) grapes.
95.71%
95.00%
91.83%
86.10%
81.11%
80.30%
70%
75%
80%
85%
90%
95%
100%
Accuracy in Percentage
Tomatoes Methods
Fig 13. Comparison between different model vs accuracy in percentage (%) with proposed VGG16 tomatoes.
98.40%
97.53%
97.50%
91.83%
88.46%
81.11%
0%
20%
40%
60%
80%
100%
120%
Proposed
VGG16
Deep Learning
VGG
Mul�-Context
Fusion Network
AlexNet &
VGG16
Deep Transfer
Learning VGG
Deep Learning
DCNN
DL
Incep�on-v1
ResNet-v2
Accuracy in Percentage
Grapes Methods 
Fig 12. Comparison between different model vs accuracy in percentage(%) with proposed VGG16 grapes.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
31
and retaining the initial images' knowledge formation while
attempting to prevent image deformation.
3. The dataset partition and training. In this section a selection of ran-
dom sample images for proposed experiments and calculated with
carried out the result as per Section 4.4.
4. Validation and testing. The testing is done on the images that were
used to evaluate the model, and new images from outside modeling
are used to check the model effectiveness. The output results are
compared to the real categories, the effectiveness of the control
that goes with them is computed.
The residual block collection and DesnseNet used in task of tomato
leaf disease identiﬁcation with RDN restructured model. After input
image normalizing and adding the convolutional layer residual modules
dense layer classify the tomato disease images with 95% accuracy dis-
ease dataset (Zhou et al., 2021). The public data set of the AI Challenger
Competition in 2018 used the Inception-ResNet-v2 model using the
RELU activation function, with an accuracy of 86.1%. (Ai et al., 2020),
under complex background conditions, the accuracy of VGG Net is
91.83 %. One more approach to INC-VGGN rice disease detection with
an average accuracy of 80.38% for both "Phaeo- sphaeria Spot" and
"Maize Eyespot" diseases (J. Chen et al., 2020) (See Fig. 13).
4. Conclusion
In this paper, there are two types of crop disease leaves were collected
and prepared as a dataset with available data. The techniques of data aug-
mentation, dataset pre-processing, training, and testing are applied to the
convolutional neural network-based VGG16 model. The proposed model
is built and tested to improve the performance measured and compared.
The evaluation metrics parameters are higher and increased as compared
to other available datasets and methods. Therefore, our proposed re-
search work increased accuracy for grapes by 98.40% and for tomatoes
by 95.71%. Always improving the performance of on-ﬁeld crops, leaf im-
ages and diseases classiﬁcation and analysis is a critical step, but with our
model achieved the highest performance, which supported agricultural
development. The major focus of research is to provide advancement in
the agriculture sector and an increase in food production. The collection
and preparation of genuine datasets and applying to the deep learning
models with multiple crops leaves images is a future target. In the future,
the use of Inception V3 and ResNet-based CNN models for much deeper
analysis of crop images is anticipated. Our work encourages and stimu-
lates farmers, which ultimately raises farm income and helps to build
up powerful countries.
Acknowledgements
Farmers from Nashik and Aurangabad, Maharashtra [India],
contributed to the collection of real ﬁeld crop images for research
purposes. We would like to thank you Dr. Panjabrao Deshmukh
Krushi Vidyapeet (Dr. PDKV), Akola, Maharashtra [India], for their
encouragement and assistance.
Declaration of competing interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂu-
ence the work reported in this paper.
Credit author statement
We the undersigned declare that this manuscript is original, has not
been published before and is not currently being considered for publica-
tion elsewhere.
We would like to draw the attention of the Editor to the following
publications of one or more of us that refer to aspects of the manuscript
presently being submitted. Where relevant copies of such publications
are attached
We wish to draw the attention of the Editor to the following facts
which may be considered as potential conﬂicts of interest and to signif-
icant ﬁnancial contributions to this work. [OR]
We wish to conﬁrm that there are no known conﬂicts of interest as-
sociated with this publication and there has been no signiﬁcant ﬁnancial
support for this work that could have inﬂuenced its outcome.
We conﬁrm that the manuscript has been read and approved by
all named authors and that there are no other persons who satisﬁed
the criteria for authorship but are not listed. We further conﬁrm that
the order of authors listed in the manuscript has been approved by
all of us.
We conﬁrm that we have given due consideration to the protection
of intellectual property associated with this work and that there are no
impediments to publication, including the timing of publication, with
respect to intellectual property. In so doing we conﬁrm that we have
followed the regulations of our institutions concerning intellectual
property.
We further conﬁrm that any aspect of the work covered in this man-
uscript that has involved either experimental human patients has been
conducted with the ethical approval of all relevant bodies and that such
approvals are acknowledged within the manuscript.
We understand that the Corresponding Author is the sole contact
for the Editorial process (including Editorial Manager and direct
communications with the ofﬁce). He/she is responsible for commu-
nicating with the other authors about progress, submissions of revi-
sions and ﬁnal approval of proofs. We conﬁrm that we have provided
a current, correct email address which is accessible by the Corre-
sponding Author and which has been conﬁgured to accept email
from biomaterials@elsevier.com.
References
Ai, Y., Sun, C., Tie, J., Cai, X., 2020. Research on recognition model of crop diseases and insect
pests based on deep learning in harsh environments. IEEE Access 8, 171686–171693.
https://doi.org/10.1109/access.2020.3025325.
Alencastre-Miranda, M., Johnson, R.M., Krebs, H.I., 2021. Convolutional neural networks
and transfer learning for quality inspection of different sugarcane varieties. IEEE
Trans. Indust. Inform. 17 (2), 787–794. https://doi.org/10.1109/TII.2020.2992229.
Ang, K.L.M., Seng, J.K.P., 2021. Big data and machine learning with hyperspectral informa-
tion in agriculture. IEEE Access 9, 36699–36718. https://doi.org/10.1109/ACCESS.
2021.3051196.
Arun Pandian, J., Geetharamani, G., Annette, B., 2019. Data augmentation on plant leaf dis-
ease image dataset using image manipulation and deep learning techniques. Pro-
ceedings of the 2019 IEEE 9th International Conference on Advanced Computing,
IACC 2019, pp. 199–204 https://doi.org/10.1109/IACC48062.2019.8971580.
Chen, J., Chen, J., Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020. Using deep transfer learning
for image-based plant disease identiﬁcation. Comput. Electr. Agricult. 173 (November
2019) 105393. https://doi.org/10.1016/j.compag.2020.105393.
Chen, C.J., Huang, Y.Y., Li, Y.S., Chen, Y.C., Chang, C.Y., Huang, Y.M., 2021. Identiﬁcation of
fruit tree pests with deep learning on embedded drone to achieve accurate pesticide
spraying. IEEE Access 9, 21986–21997. https://doi.org/10.1109/ACCESS.2021.
3056082.
Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.
Comput. Elect. Agric. 145 (September 2017), 311–318. https://doi.org/10.1016/j.
compag.2018.01.009.
Gandhi, R., Nimbalkar, S., Yelamanchili, N., Ponkshe, S., 2018. Plant disease detection using
CNNs and GANs as an augmentative approach. 2018 IEEE International Conference on
Innovative Research and Development, ICIRD 2018, no. May: 1–5 https://doi.org/10.
1109/ICIRD.2018.8376321.
Hiary, H., Al, S.B., Ahmad, M., Reyalat, M. Braik, ALRahamneh, Z., 2011. Fast and accurate
detection and classiﬁcation of plant diseases. Int. J. Comput. Appl. 17 (1), 31–38.
https://doi.org/10.5120/2183-2754.
Jin, X., Che, J., Chen, Y., 2021. Weed identiﬁcation using deep learning and image process-
ing in vegetable plantation. IEEE Access 9, 10940–10950. https://doi.org/10.1109/AC-
CESS.2021.3050296.
Kurmi, Y., Gangwar, S., Agrawal, D., Kumar, S., Srivastava, H.S., 2020. Leaf image analysis-
based crop diseases classiﬁcation. Signal Image Video Process. https://doi.org/10.
1007/s11760-020-01780-7.
Liu, S.Y., 2020. Artiﬁcial intelligence (AI) in agriculture. IT Profes. 22 (3), 14–15. https://
doi.org/10.1109/MITP.2020.2986121.
Massi, I.E., Mostafa, Y.E.-s., Yassa, E., Mammass, D., 2020. Combination of multiple classi-
ﬁers for automatic recognition of diseases and damages on plant leaves. Signal Image
Video Process. https://doi.org/10.1007/s11760-020-01797-y.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
32
Nevavuori, P., Narra, N., Lipping, T., 2019. Crop yield prediction with deep convolutional
neural networks. Comput. Elect. Agric. 163 (June) 104859. https://doi.org/10.1016/j.
compag.2019.104859.
Paymode, A.S., Malode, V.B., Shinde, U.B., 2020. Artiﬁcial intelligence in agriculture for leaf
disease detection and prediction: a review 13 (4), 3565–3573.
Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale
image recognition. 3rd International Conference on Learning Representations, ICLR
2015 - Conference Track Proceedings . http://www.robots.ox.ac.uk/.
Sun, J., Yang, Y., He, X., Xiaohong, W., 2020. Northern maize leaf blight detection under
complex ﬁeld environment based on deep learning. IEEE Access 8, 33679–33688.
https://doi.org/10.1109/ACCESS.2020.2973658.
Wu, W., Yang, T.L., Li, R., Chen, C., Liu, T., Zhou, K., Sun, C.M., Li, C.Y., Zhu, X.K., Guo, W.S.,
2020. Detection and enumeration of wheat grains based on a deep learning method
under various scenarios and scales. J. Integr. Agric. 19 (8). https://doi.org/10.1016/
S2095-3119(19)62803-0.
Xie, X., Ma, Y., Liu, B., He, J., Li, S., Wang, H., 2020. A deep-learning-based real-time detec-
tor for grape leaf diseases using improved convolutional neural networks. Front. Plant
Sci. 11 (June) 1–14. https://doi.org/10.3389/fpls.2020.00751.
Zhou, C., Zhou, S., Xing, J., Song, J., 2021. Tomato leaf disease identiﬁcation by restructured
deep residual dense network. IEEE Access 9, 28822–28831. https://doi.org/10.1109/
ACCESS.2021.3058947.
A.S. Paymode and V.B. Malode
Artiﬁcial Intelligence in Agriculture 6 (2022) 23–33
33
