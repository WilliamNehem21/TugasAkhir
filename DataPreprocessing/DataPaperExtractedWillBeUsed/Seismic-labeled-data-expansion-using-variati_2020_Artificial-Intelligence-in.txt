Seismic labeled data expansion using variational autoencoders
Kunhong Li a,b,*, Song Chen a, Guangmin Hu, Ph.D a,b
a School of Resources and Environment and Center for Information Geoscience, University of Electronic Science and Technology of China (UESTC), Chengdu, 611731,
China
b Center for Information Geoscience, UESTC, Chengdu, 611731, China
A R T I C L E I N F O
Keywords:
Deep learning
Variational autoencoders
Data expansion
A B S T R A C T
Supervised machine learning algorithms have been widely used in seismic exploration processing, but the lack of
labeled examples complicates its application. Therefore, we propose a seismic labeled data expansion method
based on deep variational Autoencoders (VAE), which are made of neural networks and contains two parts-
Encoder and Decoder. Lack of training samples leads to overﬁtting of the network. We training the VAE with
whole seismic data, which is a data-driven process and greatly alleviates the risk of overﬁtting. The Encoder
captures the ability to map the seismic waveform Y to latent deep features z, and the Decoder captures the ability
to reconstruct high-dimensional waveform bY from latent deep features z. Later, we put the labeled seismic data
into Encoders and get the latent deep features. We can easily use gaussian mixture model to ﬁt the deep feature
distribution of each class labeled data. We resample a mass of expansion deep features z* according to the
Gaussian mixture model, and put the expansion deep features into the decoder to generate expansion seismic data.
The experiments in synthetic and real data show that our method alleviates the problem of lacking labeled seismic
data for supervised seismic facies analysis.
1. Introduction
Supervised machine learning algorithms have been widely used for
seismic interpretation in recent years, such as fault detection (Wu et al.,
2019), seismic facies (Wrona et al., 2018). A key to successful application
of supervised machine learning is labeled training. Seismic label mainly
come from well loggings data and interpreters, both of which are limited.
Manual interpretations are one of the most direct and intuitively
simplest ways to increase labeled samples. Therefore, many seismic
interpretation methods use the results of manual interpretation as
training samples. However, manual interpretations require experience
and take up only a small percentage of the whole seismic data set. In
addition, labeled sample data can be extended by forward simulations
based on human prior knowledge. Above methods still require expert
knowledge as well as signiﬁcant time, so we propose a data-driven
method for the seismic labeled data expansion.
The proposed method is based on variational autoencoders (VAE),
which are unsupervised models widely used for attribute extraction. A
VAE is made of multilayer neural networks, which consist of one encoder
and one decoder. The encoder codes the high-dimensional seismic data
into the low-dimensional deep feature space, and then the decoder maps
the deep features to reconstruct the seismic waveform. The goal of VAE is
to minimize the difference between the input waveform and the recon-
structed waveform. Therefore, the VAE does not need labeled data and is
Fig. 1. The structure of VAE. The red font is the loss function. The KLð �j �Þ is the
KL divergence. The jj � jj is the 2-norm and the Nð �Þ is the gaussian distribution.
(For interpretation of the references to colour in this ﬁgure legend, the reader is
referred to the Web version of this article.)
* Corresponding author. School of Resources and Environment and Center for Information Geoscience, University of Electronic Science and Technology of China
(UESTC), Chengdu, 611731, China.
E-mail addresses: lkhmoran@gmail.com (K. Li), 2547546252@qq.com (S. Chen), hgm@uestc.edu.cn (G. Hu).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2020.12.002
Received 31 August 2020; Received in revised form 20 November 2020; Accepted 12 December 2020
Available online 13 January 2021
2666-5441/© 2021 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY license
(http://creativecommons.org/licenses/by/4.0/).
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
Fig. 2. The workﬂow of the proposed method.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
25
an unsupervised manner with VAE. Massive seismic data can be used as
training data, which can greatly alleviates the risk of over-ﬁtting.
The VAE captures the deep feature distribution of seismic data when
it converges. We put the labeled samples into the decoder, and obtain the
deep feature distribution of each class. It is easy to ﬁt the deep feature
distribution with the gaussian mixture model (GMM) for each class. It
should be noted that each class of data following one gaussian mixture
Fig. 3. The synthetic data.
Fig. 4. The labeled data. (a) The labele data A. (b)The labele data B.
Fig. 5. The deep feature. (a) The deep feature of labeled data. (b)The expansion
deep feature.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
26
distribution rather all the class data follow a gaussian mixture distribu-
tion. In short, each class deep feature has an explicit gaussian distribution
function. Subsequently, we get expansion deep feature by using these
functions. Finally, we put the expansion deep features through the
decoder and get expansion labeled data.
We proposed a semi-supervised labeled data expansion method. One
of the main reasons for network overﬁtting is the lack of training samples.
This method uses whole seismic data for the training of feature extraction
and alleviates the problem of over-ﬁtting. We use this method for syn-
thetic and real data and the results demonstrate that the generated
expansion labeled data enhance the seismic waveform classiﬁcation
performance. In the synthetic data experiment, the classiﬁcation accu-
racy is improved by 20 % with the proposed method when SNR is 3.
2. Theory
2.1. Variational autoencoder
The VAE is deﬁned by a neural network, which consists of two parts,
the encoder E and the decoder D. The encoder codes the seismic signal
y 2 RM into deep feature mean zmean and deviation zdev, and the deep
feature z equals zmean plus zdev, i.e. z ¼ zmean þ zdev. Then the decoder D is
used to decode the feature z to generate the seismic signal by, i.e. by ¼
DðzÞ. The purpose of VAE is to ﬁt the generated by to the input y, while
features z need to follow the prior gaussian distribution. Therefore, the
loss function of the VAE includes two parts, one is the reconstruction
error, which is measured by the Euclidean distance. Another part of the
loss function is about the feature z, and the KL divergence be used to
measure the differences between z and prior gaussian distributions.
Hence the VAE loss function is deﬁned as follows:
Fig. 6. The expansion labeled data. (a) The expansion labeled data A. (b)The
expansion labeled data B.
Fig. 7. Spectrum of synthetic data. The labeled data and expansion labeled data
almost have the same spectrum curve.
Fig. 8. The SVM waveform classiﬁcation results with same parameters (kernel function is radial basis function and sigma is 3). (a) The results based on labeled data.
(b)The results based on labeled data and expansion labeled data.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
27
LVAE ¼ ylogðyÞ þ ð1 � yÞlogð1 � yÞþ
1
2
�
� log
�
z2
dev
�
þ z2
dev þ z2
dev � 1
�
(1)
Due to space constraints, please refer to (Doersch, 2016) for the de-
tails of Eq. (1). The structure of VAE is shown in Fig. (1). We use gradient
descent to solve Eq. (1).
2.2. Gaussian mixed model
The probability density function of N-dimensional Gaussian distri-
bution is deﬁned as follow:
N
 
y
�����μ; Σ
!
¼
1
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
ð2πÞN��Σ
��
q
exp
 
� 1
2ðy � μÞTΣ�1
 
y � μ
!!!
(2)
where μ 2 R is the mean vector and Σ 2 RN�N is the covariance matrix. A
distribution consisting of K Gaussian distributions is called a Gaussian
mixed Model (GMM), and its probability density function is:
f
�
yjΘÞ ¼
X
K
k¼1
αkNðyjμk; Σk
�
(3)
where αk is the weight coefﬁcient and PK
k¼1αk ¼ 1. The Θ represents all
the parameters ðαk; μk; ΣkÞ. The expectation-maximization algorithm
(Dempster et al., 1977) can be used to solve the Eq. (3). We chose GMM to
ﬁt distribution of the deep feature z because of that GMM can theoreti-
cally ﬁts the probability distribution of any shape.
2.3. Labeled samples expansion
The workﬂow of the proposed method is shown in Fig. 2. There are
whole seismic data set Y 2 RM�m and the labeled set Yk 2 RM�mk, where
M is the dimension, m is the number of samples, k is the index of category
and mk is the number of k category labeled samples. The proposed method
has a total four steps (2). The ﬁrst step is training the VAE with the whole
data set Y. The Encoder of VAE will captures distribution of the deep
feature z and the Decoder of VAE has the ability that generating waveform
from the deep feature z. The second step is putting labeled samples Yk into
the Encoder and getting the corresponding the deep feature zk, which ﬁts
the each class deep feature distribution with GMM. We get the probability
density function fðzjμk; ΣkÞ for each class. The third step is resampling
pseudo deep feature zk
k via fðzjμk;ΣkÞ, and the ﬁnal step is to put the zk
* into
Fig. 9. Classiﬁcation accuracy vs SNR.
Fig. 10. The information about the physical model. (a) Theoretical design drawing of the physical model. (b)The photo of the physical model. (c) The seismic proﬁle
of line A. d) Spectrum of physical model data.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
28
the Decoder to generate the pseudo labeled sample Yk
*.
3. Experiments
3.1. Synthetic data
Wedesign asyntheticdata set showninthe Fig. 3.The syntheticdata has
total 10,000 traces with added band-limited gaussian noise (SNR ¼ 3). The
red line separates the synthetic data into two categories, the left is data A
and the right is data B. We take all the data as the training set and randomly
select 500 traces from data A and B as labeled data respectively, i.e. YA and
YB. The Fig. 4 shows the labeled data. We use the proposed method to
extend the labeled data. For better visualization, we set the deep latent
featurez to2dimensions.TheFig.5showsthedeeplatentfeatures.Theblue
Fig. 11. The fracture identiﬁcation results. (e) The fracture segment results by amplitude envelope. (f)The SVM classiﬁcation results based on labeled data. (g) The
SVM classiﬁcation results based on labeled data and expansion labeled data.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
29
scatters (zA) correspond to the labeled data A (YA) and the green scatters
(zB) correspond to labeled data B (YB). We ﬁt the deep latent feature with
GMM and resample 2000 expansion deep features for each class data (zA
* ;
zB
*), which are shown in Fig. 5b. We can see that the expansion deep feature
(zA
* ;zB
*) is well in line with the labeled deep feature distribution and greatly
expands the volume. Finally, we get the expansion labeled data YA
* ; YB
* that
is shown in Fig. 6. Fig. 7 shows the spectrum curves of labeled data and
expansion labeled data. The generated data has same character with the
labeled data, and meanwhile has less noise. The Fig. 8 shows the seismic
waveform classiﬁcation results via support vector machine (SVM). The
Fig. 8a is based on labeled data (YA and YB) and the Fig. 8b is based on the
labeled data (YA and YB) and expansion data (YA
* and YB
*). For fairness, we
set the same SVM parameters for both datasets, (kernel function is radial
basis function and sigma is 3). Fig. 9 shows the accuracy of classiﬁcation
underdifferentSNR.Obviously,theresultbasedonthelabeleddata(YA and
YB) and expansion data (YA
* and YB
*) has better accuracy and robustness,
which indicates the expansion data improves the performance of
classiﬁcation.
3.2. Real data
We applied the proposed methods on a physical model data, which
are provided by China national petroleum corporation key laboratory of
geophysical exploration. The Fig. 10 shows the information about the
physical model. Fig. 10a is the theoretical design drawing of the physical
model, and Fig. 10b is the photo of the physical model. The main purpose
of designing this physical model is to study fractures and fault. The area
enclosed by the blue dotted line is the fractures zone. Fractures with
different sizes, orientations, dip angles and densities are distributed in
this zone. The major frequency of seismic data is at 40 Hz and the interval
of seismic data is 1 ms.
Simply, we just use the data of the fractures area. There is a red line
A cross the fractures area, and the seismic proﬁle of line A is shown in
Fig. 10c. We cut the waveform data along a horizon with 40 ms window.
In order to get labeled data, we calculate the amplitude envelope
attribute and then segment fracture via a threshold, as shown in
Fig. 11e. We take 2000 samples from fracture and background respec-
tively as labeled data, and then expand the label data using the pro-
posed method. Based on these labeled and expansion data, we use SVM
to classify seismic waveforms. For fairness, we set the same SVM pa-
rameters for both datasets, (kernel function is radial basis function and
sigma is 5). The Fig. 11 shows the results of classiﬁcation. The Fig. 11f is
based on the labeled data and Fig. 11g is based on the labeled data and
expansion data. The classiﬁcation results based on expansion samples
and sample data are closer to the real fracture distribution, which in-
dicates that expansion samples play a role in improving the classiﬁca-
tion results to some extent.
4. Conclusion
We proposed a novel data-driven, semi-supervised label expansion
method. We use whole survey seismic data as the training data for VAE,
which provides enough training data and alleviates the risk of over-
ﬁtting. The encoder of VAE projects seismic data into deep features,
and the decoder of VAE generate seismic data from the deep features. We
use GMM to extract deep feature distributions of labeled data, and then
resample a large number of deep features. In other words, the proposed
method extend labeled data in the deep feature space. We test our
method via seismic waveform classiﬁcation. The results of theoretical
and practical data demonstrate that expansion data can well enhance the
classiﬁcation results.
Declaration of competing interest
The manuscript has not been published before and is not being
considered for publication elsewhere. All authors have contributed to the
creation of this manuscript for important intellectual content and read
and approved the ﬁnal manuscript. We declare there is no conﬂict of
interest.
Acknowledgments
This work was supported by National Natural Science Foundation of
China (41804126, 41604107). We would like to thank China national
petroleum corporation key laboratory of geophysical exploration for
providing the physical model data.
References
Dempster, A.P., Laird, N.M., Rubin, D.B., 1977. Maximum likelihood from incomplete
data via the em algorithm. J. Roy. Stat. Soc. B 39, 1–22.
Doersch, C., 2016. Tutorial on Variational Autoencoders arXiv preprint arXiv:
1606.05908.
Wrona, T., Pan, I., Gawthorpe, R.L., Fossen, H., 2018. Seismic facies analysis using
machine learning. Geophysics 83, O83–O95.
Wu, X., Liang, L., Shi, Y., Fomel, S., 2019. Faultseg3d: using synthetic data sets to train an
end-to-end convolutional neural network for 3d seismic fault segmentation.
Geophysics 84, IM35–IM45.
K. Li et al.
Artiﬁcial Intelligence in Geosciences 1 (2020) 24–30
30
