Emotion recognition from EEG-based relative power spectral topography
using convolutional neural network
Md. Asadur Rahman a, Anika Anjum b, Md. Mahmudul Haque Milu c, Farzana Khanam c,
Mohammad Shorif Uddin d,*, Md. Nurunnabi Mollah e
a Department of Biomedical Engineering, Military Institute of Science and Technology (MIST), Dhaka, 1216, Bangladesh
b Department of Biomedical Engineering, Khulna University of Engineering & Technology (KUET), Khulna, 9203, Bangladesh
c Department of Biomedical Engineering, Jashore University of Science and Technology (JUST), Jashore, 7408, Bangladesh
d Department of Computer Science and Engineering, Jahangirnagar University (JU), Dhaka, 1342, Bangladesh
e Department of Electrical and Electronic Engineering, Khulna University of Engineering & Technology (KUET), Khulna, 9203, Bangladesh
A R T I C L E I N F O
Keywords:
Electroencephalography (EEG)
Emotion recognition
Relative power spectral density
Convolutional neural network (CNN)
SEED dataset
A B S T R A C T
Emotion recognition, a challenging computational issue, ﬁnds interesting applications in diverse ﬁelds. Usually,
feature-based machine-learning methods have been used for emotion recognition. However, these conventional
shallow machine learning methods often ﬁnd unsatisfactory results as there is a tradeoff between feature di-
mensions and classiﬁcation accuracy. Besides, extraction and selection of features from the spatial and frequency
domains could be an additional issue. This work proposes a method that transforms EEG (electroencephalog-
raphy) signals to topographic images that contain the frequency and spatial information and utilizes a convolu-
tional neural network (CNN) to classify the emotion, as CNN has improved feature extraction capability.
According to the proposed method, the topographic images are prepared from the relative power spectral density
rather than power spectral density that shows remarkable improvement in classiﬁcation accuracy. The proposed
method is applied to the well-known SEED database and has given outperforming results than the current state-of-
the-art.
1. Introduction
Emotion is a complex state of mind that is associated with someone's
surroundings, thoughts, feelings, and circumstances and it results in
physical and psychological changes. Some researchers have found that
emotion is a cognitive task [1–3]. A human can understand emotion
through the behavior, mood, and temperament of another person, but it
is hard for a machine to understand human emotion unless the human
makes the machine intelligent enough to decode emotions. This method
for the machines can be named as human emotion recognition. In the
case of the treatment of patients with expression problems, recognition of
a real emotional state by a machine is helpful for providing better
medical treatment. There are some psychophysiological studies [4–6]
where they found a strong correlation between emotion recognition and
brain activities.
In recent years, EEG modality has gained much attention for
measuring brain activity in a precise manner along with setting a
communicational pathway between humans and machines. Among
various modalities, this method is very much promising because of its
high accuracy and objective evaluation comparing with other external
appearances like facial expression and gesture [7]. But, the raw EEG
signal is contaminated with artifacts and this signal is complex due to its
variation with time and space. The feature extraction and classiﬁcation of
EEG signal have two important aspects which must be ensured in
emotional state recognition. Speciﬁed domain knowledge is required for
conventional manual feature extraction and feature selection. However,
the cost of conventional feature selection increases at a quadratic rate
with the increase of the number of features [8]. In most of the studies for
feature extraction, researchers have only focused on time [9–14], fre-
quency [15–17], or time-frequency [18,19] domains of the EEG signal,
and rarely focused on the spatial dimension. A multichannel EEG system
acquires data from the different spatial locations of the human brain. So,
the motivation of this work is to combine time, frequency, and spatial
domains to classify the emotional state from the multichannel EEG
* Corresponding author.
E-mail addresses: bmeasadur@gmail.com (Md.A. Rahman), anikaanjum123@gmail.com (A. Anjum), mahmudhmilu@gmail.com (Md.M.H. Milu), farzanabme@
just.edu.bd (F. Khanam), shorifuddin@gmail.com, shorifuddin@juniv.edu (M.S. Uddin), nurunnabim12@gmail.com (Md.N. Mollah).
Contents lists available at ScienceDirect
Array
journal homepage: www.elsevier.com/journals/array/2590-0056/open-access-journal
https://doi.org/10.1016/j.array.2021.100072
Received 24 October 2020; Received in revised form 7 May 2021; Accepted 31 May 2021
Available online 10 June 2021
2590-0056/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Array 11 (2021) 100072
signals with high accuracy.
Several studies have considered body movement, voice tone or facial
expressions, and physiological activities to recognize emotions [20–23].
These physical changes had some common complications as they did not
show reliable emotion. Our state of mind could only be a reliable way to
understand the emotional state that could have been revealed by the brain
signal like EEG. Some research works [24–26] are accomplished so far to
construct machine learning-based predictive models to classify the
emotional states from the EEG signals. A study led by Wang et al. [24]
reported the alpha, beta, and gamma bands as features to classify four
emotions (joy, relax, sad, and fear) and found 66.5% (in average) accuracy
by support vector machine (SVM) using different kinds of emotional video
as stimuli. Another study done by Lin et al. [25] achieved 82.29% accuracy
using the same classiﬁer. Their experiment included 26 subjects and used
30 s long music samples as stimuli. To achieve a highly accurate predictive
model, recent studies prefer the deep neural network as a feature extractor
and classiﬁer. Zheng et al. [26] extracted density entropy of the multi-
channel EEG signals using a short-time Fourier transform and used deep
belief network (DBN) as a classiﬁer to classify positive, negative, and
neutral emotions and obtained 86.65% accuracy. A recent study [27] has
proposed to construct topographic images from the multichannel EEG
signals to feed them into the deep neural network which proves to achieve
a high classiﬁcation accuracy than the current state of the arts. The method
describes in Ref. [27] used power spectral density (PSD) to form the
topographic image which is slightly questionable to attain high-level ac-
curacy, as the value of the PSD can vary with the individuals, time, and
system. Therefore, using relative PSD (RPSD) is more technically sound to
ensure the power level variation measurement of the concerned band with
respect to the full band power in case of EEG signals [27,28]. Therefore,
feature extraction, feature selection, and setting of the proper classiﬁer are
the most important issues to construct a machine learning-based predictive
model for emotional state classiﬁcation.
To overcome these issues, the present work proposes a mechanism to
combine the time, frequency, and spatial domain of the multichannel
EEG emotional signals into topographic images. The topographic image
represents the RPSD of each trial of an emotional state that needs time,
frequency, and spatial information. There may arise an argument for
using the RPSD instead of PSD. Although a beautiful explanation is given
in Refs. [27,28], a graphical realization is presented in the Results and
Discussions Section to explain the effect of PSD and RPSD through power
level variation. This graphical presentation clariﬁes how RPSD improves
the feature quality of the EEG signal power distribution.
Many recent research works [29–31] recommend that the convolu-
tional neural network (CNN) method is very effective in feature extrac-
tion and classiﬁcation tasks. For this, utilizing the automated feature
extraction and classiﬁcation facilities of the CNN, the topographical
images are classiﬁed. The main contributions of this work are as follows:
� A method is proposed that utilizes the RPSD of each spatial position of
EEG data on the brain (channel) to prepare the topographic images.
� A CNN has been applied for the feature extraction and emotion
classiﬁcation from the topographic images.
� We compared our work with the other recent works on emotion
recognition using the SEED dataset.
The article is organized as follows: Section 2 describes the data
collection, representation, and the proposed methodology, Section 3
presents the results of this research work and compares its ﬁndings with
the ﬁndings of the recent works, ﬁnally, Section 4 concludes the paper
with future research directions.
2. Materials and methods
2.1. Dataset description
The proposed methodology of this work has been conducted on the
SEED dataset [32]. In the data acquisition, there are ﬁfteen subjects,
among them eight are females and seven are males (Age ¼ 23.27�2.37
years). All participants are right-handed native Chinese students of
Shanghai Jiao Tong University. They are reported as normal or corrected
to normal vision and normal hearing.
Various stimuli can be utilized in emotion-related research such as
movie clips, music, and verbal command, etc. Among them, the movie
clip has greater efﬁciency and reliability [33] as it contains both audio
and video. In this experiment, Chinese movie clips were selected since all
the participants were native Chinese. The videos are of 4 min long and
can be categorized as: positive, negative, and neutral. The following se-
lection criteria were maintained-
� The total time of the experiment should be small otherwise subjects
might become fatigued.
� The movie clips should be well understood without clariﬁcation.
� The clips should stimuli a single target emotion. It should not contain
mixed emotions.
Each ﬁlm is edited to create coherent emotion-eliciting and maximize
emotional meanings. The details of the ﬁlm clips used in the experiments
are given in Table 1.
Each subject experienced three experiments: positive, negative, and
neutral. So, 15 subjects experienced 45 experiments in total. There is a
total of 15 trials for each experiment. Each trial is started with 5 s hint of
starting. Then, ﬁlm clips are played for 4 min and 45 sec is allotted after
each clip for feedback. Within this period, participants are asked to report
their emotional reactions toward the shown video clips. The order of
presentation is arranged in such a way that two clips targeting the same
emotion are not shown consecutively. The detailed protocol is given in
Fig. 1.
2.2. Data acquisition and processing
The experiments were performed in three sessions for each subject.
The time interval between each session is one week or longer. This
process ensured stable patterns of neural activities across sessions and
individuals. Facial videos and EEG data are recorded simultaneously.
Subjects are seated in front of a big screen where movie clips are shown.
EEG data is recorded using an ESI NeuroScan System at a sampling rate of
1000 Hz from a 62-channel active AgCl electrode cap following the In-
ternational 10–20 System [34]. The SEED dataset contains a down-
sampled, preprocessed, and segmented version of EEG data. It is
downsampled to 200 Hz. A bandpass ﬁlter of 0–75 Hz is applied to
remove any artifacts. There are 45 mat ﬁles in total, one per experiment.
Each subject ﬁle contains 15 arrays. In this work, we have used a
band-pass ﬁlter (3–30 Hz) to remove unnecessary frequencies. In the EEG
signal, up to 3 Hz represents the Delta Band, which is present in deep
sleep or coma. As subjects were awake and alert during trials, Delta Band
is rejected in our processing. Frequencies higher than 30 Hz are also
removed as it does not represent any brain activity regarding emotional
effects.
2.3. Base construction of the EEG topography
An illustration of 62-electrode placement regarding the SEED dataset
is given in Fig. 2. It can be generalized through a matrix of m � n. Here, m
and n represent the maximum-point number in horizontal test points and
vertical test points, respectively. In this work, m � n equals to 9 � 8
matrix. The red circled points in Fig. 3 are the EEG electrodes of the SEED
dataset. These points indicate the value of relative PSD of the EEG signals
of corresponding electrodes. The gray points are added to form a com-
plete 9 � 8 matrix. These points are the interpolation of the surrounding
red points. The method of EEG topography construction is shown step-
wise in Fig. 4. A pseudo-code for the topographic image formation
from the multichannel EEG signals is added in Appendix A.
Md.A. Rahman et al.
Array 11 (2021) 100072
2
The construction of some new points using speciﬁc neighborhood
discrete data points is found by interpolation. To make 72 points of the 9
� 8 matrix, 10 additional points are calculated through interpolating the
neighborhood points. The interpolation process of gray points can be
calculated as:
Γðm; nÞ ¼ Γ
0ðm þ 1; nÞ þ Γ
0ðm � 1; nÞ þ Γ
0ðm; n þ 1Þ þ Γ
0ðm; n � 1Þ
K
;
ð0 � m; n � 8; m; m 2 NÞ
(1)
where, Γ (m, n) presents the required value of the gray points, Γ’ (m, n)
are the values of the point surrounding Γ (m, n). Here, K is the number of
non-zero elements in the numerator.
2.4 RPSD calculation and normalization
PSD of a signal means the distribution of power over its frequency
components that represents the impact of the frequency components
included in the signal. Let, P is the average power of a signal x(t), then the
power for the total time period T is,
P ¼ lim
T→∞
1
T
ZT
0
jxðtÞj2dt
(2)
The frequency content of the signal xðtÞ is x
_ðωÞ which is calculated by
Fourier transformation. Then, the PSD can be calculated as follows [35,
36]:
SxxðωÞ ¼ lim
T→∞E
�
jbxðωÞj2�
(3)
The ratio of the PSD of the band of interest (PSDBOI) and the PSD of
the total frequency band (PSDtotal) is called RPSD. The value of the PSD
varies from person to person and in the case of the same individual, it
varies from time to time. Thus PSD is not a reliable source of information
irrespective of person and time [27,28]. RPSD solves this problem by
comparing the PSD of the concerned band with respect to the PSD value
of the total frequency range of the signal [27,28]. Therefore, RPSD can be
presented as (4).
RPSD ¼ PSDBOI
PSDtotal
(4)
To reduce inter-participant variability, the RPSD of each subject can
be normalized by scaling between 0 and 1 [34] as given in (5).
π
0 ¼ π � πmin
πmax � πmin
(5)
Whereπ
0is the normalized value of the feature; πmax , πmin are the
maximum and minimum value of the subject features, respectively. Using
Fig. 1. Each subject faced 45 trials in total. No consecutive trial was targeted at the same emotion.
Table 1
Exemplary movie clips for the positive, negative, and neutral emotional stimulation used in the experiment.
Serial no
Emotion label
Film clips' sources
01
Negative
Tangshan Earthquake
02
Negative
Back to 1942 (War movie)
03
Positive
Lost in Thailand
04
Positive
Flirting Scholar
05
Positive
Just Another Pandora's Box
06
Neutral
World Heritage in China
Examples of the movie clips of Positive, Negative, and Neutral emotions
Positive Clips
Negative Clips
Neutral Clips
Flirting Scholar
Back to 1942
World Heritage in China
Lost in Thailand
Tangshan Earthquake
World Heritage in China
Md.A. Rahman et al.
Array 11 (2021) 100072
3
Fig. 3. Feature matrix of EEG is made considering RPSD as a feature. The shown gray points are interpolated from the surrounding red points. Ten new points are
interpolated during this topographic image construction.
Fig. 2. The positions of the 62 electrodes of the data acquiring device according to the international 10–20 method.
Md.A. Rahman et al.
Array 11 (2021) 100072
4
normalized RPSD values of 72 positions (as shown in Fig. 3) the EEG
topographic images were constructed.
2.5. Construction of CNN-based classiﬁer
For automatic feature extraction and classiﬁcation CNN is used in this
work. CNN is a type of machine learning system in which a model learns
automatically to classify objects from images, numerical values, or
videos. It is highly capable of learning from the input data by optimizing
the weight parameters of each ﬁlter by minimizing the classiﬁcation
error. CNN consists of an input layer and an output layer, along with
multiple hidden layers. It takes an image as an input, then processes it
Fig. 4. Flow diagram of the procedure to construct the topographic image from multichannel EEG.
Md.A. Rahman et al.
Array 11 (2021) 100072
5
through multiple hidden layers. It gives the output as a probable class
name. The hidden layers of CNN typically consist of convolutional layers,
ReLU layers, pooling layers, a fully connected layer, a batch normaliza-
tion layer [37]. A generalized pattern of the layers in a CNN is presented
in Fig. 5. The design details of the different layers regarding this work are
discussed brieﬂy in the following.
In this work, topographic images are considered as the input of the
CNN. A color image can be represented as m � n � c where m is the width
and n is the height of the image and c is the number of channels e.g. RGB
image has c ¼ 3. Here, the input image size is 192 � 192 � 3. The
convolutional layer works for feature extraction. It is considered the core
building block of the CNN architecture [38]. Convolutional layers
Fig. 7. Comparison between the raw EEG data and the ﬁltered EEG signal. EEG signal is ﬁltered with a band-pass ﬁlter of band 8–32 Hz.
Fig. 6. The values of the different parameters of the layers in the proposed CNN along with the clariﬁcation of the regarding layers.
Fig. 5. Different layers of a convolutional neural network consisting of an image as an input layer, different ﬁlters in convolution, ReLU and pooling layer, and ﬁnally
output layer with the ﬁnal score.
Md.A. Rahman et al.
Array 11 (2021) 100072
6
Fig. 9. Comparison among topographic images of (a) negative, (b) neutral, and (c) positive emotions. The center part of the brain which contains a limbic system is
always active during these emotions as this part is responsible for creating the emotions.
Fig. 8. An example of an EEG topographic image. AF7, AF5, AF6, AF8, PO9, PO10, CB5, CB3, CB4, and CB6 are the interpolated points from surrounding points.
Md.A. Rahman et al.
Array 11 (2021) 100072
7
transform the input data by using a patch of locally connecting neurons
from the previous layer. This convolutional layer convolves the 192 �
192 � 3 image by moving a ﬁlter along with the vertical and horizontal
input image. While creating a convolutional two-dimensional layer, the
ﬁlter size of the input argument is speciﬁed as 8 � 8. Stride is known as
the step size with which the ﬁlter moves. For 8 � 8 ﬁlter scanning
through the input image, the stride of 1 is used in this research.
In dilated convolution, the ﬁlters are expanded by inserting spaces
between the elements without increasing the number of parameters or
computation. We apply ([3,3,4,4]) zero padding to input image borders
to add zero values vertically and horizontally. The output size of this
designed convolutional layer can be equated by:
Output Size ¼
1
S þ 1 ðIS � ððFS � 1Þ * DF þ 1Þ þ 2PÞ
(6)
Here, IS ¼ Input size of image; S ¼ Stride; FS ¼ Filter size; DF ¼
Dilation factor; P ¼ Padding. So, the mathematical formula of the two-
dimensional convolutional layer is:
G½m; n� ¼
X
∞
i¼�∞
X
∞
j¼�∞
h½i; j�:g½m � i; n � j�
(7)
Here, g represents the input image matrix to be convolved with the
kernel matrix h to result in a new matrix G.
In our proposed CNN structure, the input channel is normalized by a
batch normalization layer. It is generally used between the convolutional
layer and the ReLU layer to speed up the training process of CNN and
reduce the sensitivity to network initialization. A batch normalization
layer normalizes its inputs by calculating the mean and variance over a
mini-batch and each input channel. Then it calculates the normalized
activations as [39],
bpi ¼ pi � μB
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
σ2
B þ ε
p
(8)
Here, pi¼ inputs; μB ¼ mean; σ2
B¼ variance; ε improves numerical
stability when the mini-batch variance is very small. The rectiﬁed linear
unit or ReLU was used as an element-wise activation function over the
input data thresholding. Running ReLU over the input volume changes
the pixel values but does not change the spatial dimension of the input
data in the output. ReLU is more preferred than other functions because it
makes the neural network faster [39].
The details of these layers in our proposed CNN model are shown in
Fig. 6. The fully connected layer is used to compute class scores that can
Fig. 11. (a) Accuracy and (b) loss of the training and validation of the proposed CNN structure with respect to iterations.
Fig. 10. RPSD- and PSD-based topographs presenting the variation of EEG signals' spatial power level distribution for positive, negative, and neutral emotions. These
are the corresponding results of Subject 3.
Md.A. Rahman et al.
Array 11 (2021) 100072
8
be used as the output of the network. The dimension of the output vol-
ume is 1 � 1 � N, where N is the number of output classes (here the
number of classes N ¼ 3).
3. Results and discussions
All steps of the processing, image formation, feature extraction, and
data classiﬁcation were performed in Matlab 2018a. Data utilized in this
research work has been collected from the SEED dataset. Some pre-
processing was already applied to the dataset e.g. data were down-
sampled to 200 Hz, bandpass ﬁlter was applied from 0 to 75 Hz to remove
artifacts. Despite this preprocessing, some other processing were also
done. A bandpass ﬁlter was applied from 8 to 32 Hz to remove noise. A
graphical representation of raw and bandpass ﬁltered EEG signals is
shown in Fig. 7.
Fig. 8 shows one sample of EEG topographic image, where 10 new
data points are interpolated from the neighbor points: AF7, AF5, AF6,
AF8, PO9, PO10, CB5, CB3, CB4, and CB6. The RPSD of each electrode is
calculated and normalized within the range of 0–1. Each RPSD value is
mapped in a two-dimensional plot according to the locations of each
electrode. These locations are given speciﬁc names. A color bar is also
shown in Fig. 8 to represent the activity level of the different positions of
our brain during different emotions.
Different emotions create different effects on the human brain. A sad
movie can make someone emotionally unstable while a natural scene can
create a soothing effect on our brain. The state of an emotional condition
changes the EEG result taken during different emotions. A comparison
among topographic images of negative, neutral, and positive emotions is
shown in Fig. 9. One thing to notice in those images is that the central
part of the brain is always active in all three emotions. The central part of
the brain or medial temporal lobe is consists of the limbic system [40]. It
has been said that the limbic system is responsible for emotion formation
[41]. Thus, despite the emotion type, the limbic system is always active.
Another important issue is to use the RPSD instead of PSD for making
the topographical images. It is a hypothesis that power level distribution-
based images are better in the case of RPSD than that of the PSD. Based
on this argument, we have presented PSD- and RPSD-based topograph-
ical images in Fig. 10. It is clearly observable that power level distribu-
tion is more precise in the case of RPSD than that of the PSD.
The topographic images are given as input to the CNN for feature
extraction and emotion classiﬁcation. To feed the CNN, there were three
types of EEG data corresponding to three emotional states of the brain.
Since each participant took part in 3 different emotional states (positive,
negative, and neutral) and there are 15 trials for an individual state. So,
there are 15 � 3 ¼ 45 multichannel EEG data from each participant.
The accuracy and loss of the proposed CNN structure with respect to
iterations are presented in Fig. 11(a) and Fig. 11(b), respectively. Here,
the maximum epoch is considered 15. We have selected the ﬁlter size of
the convolutional layer based on the trial and error method. Each result is
checked using the Matlab simulator several times to select the best
pattern. The convolutional layer, batch normalization layer, ReLU layer,
pooling layer are used twice in this work. The ﬁnal results are the clas-
siﬁcation accuracies against the three classes: positive, negative, and
neutral emotional states. Therefore, the fully connected layer produces 3
output classes.
We have done experimentation with the proposed CNN in two sce-
narios. In the ﬁrst scenario, 25% of data are used for training, and the rest
75% are used for testing. Similarly, in the second scenario, 50% of data
are used for training, and the rest 50% are used for testing. The training
and testing data were chosen randomly ﬁve times for each participant
and the resultant classiﬁcation accuracy is tabulated from the average
value of the corresponding ﬁve classiﬁcation accuracy of each partici-
pant. The classiﬁcation accuracy of the proposed method is presented in
Table 2. We found that the average classiﬁcation accuracy achieved by
our proposed work is 89% for the ﬁrst scenario and 94% for the second
scenario. This outcome is the highest classiﬁcation accuracy with respect
to the other recent methods applied to the SEED dataset so far. The
comparison of the methods and their classiﬁcation accuracies are shown
in Table 3.
4. Conclusion
In this research work, human emotion has been detected from
multichannel EEG signals. The multichannel emotional EEG signals were
mapped into two-dimensional tomographic images using RPSD. These
images combined frequency and spatial domain information of the EEG
Table 3
Comparison with other recent methods with SEED dataset.
Author & Study
Method
Classiﬁer
Average Accuracy (%)
W. Zheng et al., 2017 [13]
Group Sparse Canonical Correlation Analysis
86.65
W. L. Zheng et al., 2017 [14]
Differential Entropy as Features
Discriminative Graph regularized Extreme Learning
Machine
79.28
W. L. Zheng et al., 2015 [17]
Critical Frequency Band Investigation
Deep Belief Network
86.08 � 8.34
Y. M. Jin et al., 2020 [30]
Differential Entropy
Domain Adaptation Network
79.19
Y. Yang, 2018 [42]
Differential Entropy as Features
Hierarchical Network with Subnetwork Nodes
86.42
M. A. Rahman, 2019 et al.
[43]
PCA and t-statistics-Based Feature Selection
Method
SVM and ANN
85.85 � 5.72 and 86.57 � 4.08
Proposed Method
RPSD-Based Topographic Image for Emotional EEG
Data
CNN
89.056 � 4.32 (25% data for
training)
94.63 � 3.68 (50% data for
training)
SVM: Support Vector Machine [44,45], ANN: Artiﬁcial Neural Network [44,45].
Table 2
Classiﬁcation accuracy for 15 subjects using CNN.
Subject
Accuracy with 25% data for
training and 75% data for
testing (%)
Accuracy with 50% data for
training and 50% data for
testing (%)
1
82.79
93.48
2
90.91
100
3
84.85
92.48
4
90.85
95.24
5
84.70
100
6
90.91
90.71
7
83.82
90.71
8
84.85
95.24
9
88.82
100
10
93.94
90.48
11
92.91
95.24
12
85.73
88.67
13
90.88
94.48
14
96.97
94.48
15
92.91
98.24
Average �
Standard
Deviation
89.056 � 4.32
94.63 � 3.68
Md.A. Rahman et al.
Array 11 (2021) 100072
9
signals. For feature extraction and classiﬁcation CNN was used in this
work and found a signiﬁcant enhancement in the classiﬁcation accuracy.
Compared with the other recent methods that were applied on the SEED
dataset, the proposed method achieved the highest classiﬁcation accu-
racies 89.056% � 4.32 (using 25% data in training and the rest 75% in
testing) and 94.63% � 3.68 (using 50% data in training, and rest 50% in
testing). From this convincing output, it is expected that the proposed
expert system would work efﬁciently in other types of EEG signal clas-
siﬁcation, which will be our next focus.
Declaration of competing interest
The authors have no conﬂict of interest regarding this publication.
Funding Acknowledgement
No funding organization/institute supported this research work.
Appendix A
Pseudo Code:
EEG topographic image (used in MATLAB R2018a):
data¼ load the RPSD values of total estimated channel; % Load your
2D EEG data
xc¼ [horizontal axis coordinates]; % get the x-axis
yc¼ [vertical axis coordinates]; % get y-axis points
trlen¼ normalized value of each electrode; % ﬁnd the normalized
value
xi¼linspace(min(xc),max(xc),30);
yi¼linspace(min(yc),max(yc),30);
[XI, YI]¼meshgrid(xi,yi); % Create a mesh with xi and yi
zc ¼ griddata(xc,yc,trlen,XI,YI,'natural’); % relative power of each
electrode
[cs,hh]¼contourf(XI,YI,ZI,20,'LineStyle','none’); % contour plot of
the data
set(hh,'EdgeColor','none')
shading interp
set(gca,'Visible','off’); colormap(jet);
set(gcf,'PaperUnits','inches','PaperPosition',[0 0 2 2]);
print(1,'-dpng’, '.png','-r00);
Author statement
Md. Asadur Rahman: Conceptualization, Methodology, Formal anal-
ysis, Software, Investigation, Resources, Draft preparation. Anika Anjum:
Methodology, Formal analysis, Software, Investigation, Draft prepara-
tion. Md. Mahmudul Haque Milu: Formal analysis, Data curation,
Investigation, Draft preparation. Farzana Khanam: Formal analysis, Data
curation, Investigation, Draft preparation. Mohammad Shorif Uddin:
Validation, Visualization, Review & Editing. Md. Nurunnabi Mollah:
Supervision, Review & Editing
Funding
No funding was received for this research.
Informed consent
The studies reported in this work did not require any informed
consent.
Ethical approval
This article does not contain any studies with human participants or
animals performed by any of the authors. It used a publicly available
dataset. Proper acknowledgments with citation guidelines are main-
tained for the use of this dataset.
References
[1] Lazarus R. Emotion and adaptation. USA: Oxford University Press; 1991.
[2] Mueller SC. The inﬂuence of emotion on cognitive control: relevance for
development and adolescent psychopathology. Front Psychol 2011;2:327. https://
doi.org/10.3389/fpsyg.2011.00327.
[3] Tyng CM, Amin HU, Saad MNM, Malik AS. The inﬂuences of emotion on learning
and memory. Front Psychol August 2017;8(1454):24. https://doi.org/10.3389/
fpsyg.2017.01454.
[4] Sammler D, Grigutsch M, Fritz T, Koelsch S. Music and emotion:
electrophysiological correlates of the processing of pleasant and unpleasant music.
Psychophysiology 2007;44(2):293–304. https://doi.org/10.1111/j.1469-
8986.2007.00497.x.
[5] Knyazev GG, Slobodskoj-Plusnin JY, Bocharov AV. Gender differences in implicit
and explicit processing of emotional facial expressions as revealed by event-related
theta synchronization. Emotion 2010;10(5):678–87. https://doi.org/10.1037/
a0019175.
[6] Mathersul D, Williams LM, Hopkinson PJ, Kemp AH. Investigating models of affect:
relationships among EEG alpha asymmetry, depression, and anxiety. Emotion 2008;
8(4):560–72. https://doi.org/10.1037/a0012811.
[7] Ahern GL, Schwartz GE. Differential lateralization for positive and negative emotion
in the human brain: EEG spectral analysis. Neuropsychologia 1985;23(6):745–55.
https://doi.org/10.1016/0028-3932(85)90081-8.
[8] Dash M, Liu H. Feature selection for classiﬁcation. Intell Data Anal 1997;1(3):
131–56. https://doi.org/10.1016/S1088-467X(97)00008-5.
[9] Murugappan M, Ramachandran N, Sazali Y. Classiﬁcation of human emotion from
EEG using discrete wavelet transform. J Biomed Sci Eng 2010;3:390–6. https://
doi.org/10.4236/jbise.2010.34054.
[10] Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from EEG using higher-
order crossings. IEEE Trans Inf Technol Biomed 2010;14:186–97. 0.1109/
TITB.2009.2034649.
[11] Petrantonakis PC, Hadjileontiadis LJ. Emotion recognition from brain signals using
hybrid adaptive ﬁltering and higher-order crossings analysis. IEEE Transaction on
Affective Computing 2010;1:81–97. https://doi.org/10.1109/T-AFFC.2010.7.
[12] Murugappan M, Rizon M, Nagarajan R, Yaacob S. Inferring of human emotional
states using multichannel EEG. Eur J Sci Res 2010;48(2):281–99.
[13] Zheng W. Multichannel EEG-based emotion recognition via group sparse canonical
correlation analysis. IEEE Transactions on Cognitive and Developmental Systems
Sept. 2017;9(3):281–90. https://doi.org/10.1109/TCDS.2016.2587290.
[14] Zheng WL, Zhu JY, Lu BL. “Identifying stable patterns over time for emotion
recognition from EEG. IEEE Transaction on Affective Computing 2017;1. https://
doi.org/10.1109/TAFFC.2017.2712143.
[15] Thammasan N, Moriyama K, Fukui K, Numao M. Continuous music-emotion
recognition based on electroencephalogram99. ” IEICE Transaction on Information
System; 2016. p. 1234–41. https://doi.org/10.1587/transinf.2015EDP7251.
[16] Jirayucharoensak S, Pan-Ngum S, Israsena P. ““EEG-based emotion recognition
using deep learning network with principal component based covariate shift
adaptation. Sci World J 2014:1–10. https://doi.org/10.1155/2014/627892. 2014,
627892.
[17] Zheng WL, Lu BL. Investigating critical frequency bands and channels for EEG-based
emotion recognition with deep neural networks. IEEE Transactions on Autonomous
Mental Development 2015;7(3):162–75. https://doi.org/10.1109/TAMD.2015.
[18] Yin Z, Wang Y, Liu L, Zhang W, Zhang J. Cross-subject EEG feature selection for
emotion recognition using transfer recursive feature elimination. Front Neurorob
2017;11:19. https://doi.org/10.3389/fnbot.2017.00019.
[19] Li X, Qi XY, Sun XQ, Xie JL, Fan MD, Kang JN. An improved multi-scale entropy
algorithm in emotion EEG features extraction. Journal of Medical Imaging & Health
Informatics 2017;7:436–9. https://doi.org/10.3772/j.issn.1002-0470.2015.10-
11.001.
[20] Schirmer A, Adolphs R. Emotion perception from face, voice, and touch:
comparisons and convergence. Trends Cognit Sci 2017;21(3):216–28. https://
doi.org/10.1016/j.tics.2017.01.001.
[21] Rigoulot S, Pell MD. Seeing emotion with your ears: emotional prosody implicitly
guides visual attention to faces. PloS One 2012;7(1):1–11. https://doi.org/
10.1371/journal.pone.0030740.
[22] Banziger T, Grandjean D, Scherer KR. Emotion recognition from expressions in face,
voice, and body: the multimodal emotion recognition test (MERT). Emotion 2009;
9(5):691–704. https://doi.org/10.1037/a0017088.
[23] Stathopoulou IO, Tsihrintzis GA. “Emotion recognition from body movements and
gestures,” Intelligent Interactive Multimedia Systems and Services. July 2011.
p. 295–303. https://doi.org/10.1007/978-3-642-22158-3_29.
[24] Wang XW, Nie D, Lu BL. EEG based emotion recognition using frequency domain
features and support vector machines. Neural Information Processing 2011;7062:
734–43. https://doi.org/10.1007/978-3-642-24955-6_87.
[25] Lin YP, Wang CH, Jung TP. EEG-based emotion recognition in music listening. IEEE
(Inst Electr Electron Eng) Trans Biomed Eng 2010;57(7):1798–806. https://
doi.org/10.1109/TBME.2010.2048568.
[26] Li Y, Huang J, Zhou H, Zhong N. Human emotion recognition with
electroencephalographic multidimensional features by hybrid deep neural
networks. Appl Sci 2017;7(10). https://doi.org/10.3390/app7101060.
[27] Rahman MA, Rashid MMO, Khanam F, Alam MK, Ahmad M. EEG based brain
alertness monitoring by statistical and artiﬁcial neural network approach. Int J Adv
Md.A. Rahman et al.
Array 11 (2021) 100072
10
Comput Sci Appl January 2019;10(1). https://doi.org/10.14569/
IJACSA.2019.0100157.
[28] Khanam F, Rahman MA, Ahmad M. Evaluating alpha relative power of EEG signal
during psychophysiological activities in Salat. In: International conference on
innovations in science, engineering and Technology 2018 (ICISET). Bangladesh:
International Islamic University Chittagong (IIUC); 2018. p. 1–6. https://doi.org/
10.1109/ICISET.2018.8745614. 27-28 October.
[29] Mahmud M, Kaiser MS, Hussain A, Vassanelli S. Applications of deep learning and
reinforcement learning to biological data. in IEEE Transactions on Neural Networks
and Learning Systems June 2018;29(6):2063–79. https://doi.org/10.1109/
TNNLS.2018.2790388.
[30] Noor MBT, Zenia NZ, Kaiser MS, Mamun SA, Mahmud M. “Application of deep
learning in detecting neurological disorders from magnetic resonance images: a
survey on the detection of Alzheimer's disease, Parkinson's disease and
schizophrenia. Brain Informatics 2020;7(11). https://doi.org/10.1186/s40708-
020-00112-2.
[31] Rahman MA, Uddin MS, Ahmad M. Modeling and classiﬁcation of voluntary and
imagery movements for brain-computer interface from fNIR and EEG signals
through convolutional neural network. Health Inf Sci Syst 2019;7(22). https://
doi.org/10.1007/s13755-019-0081-5.
[32] Emotional EEG Dataset. Available in: http://bcmi.sjtu.edu.cn/home/seed/index.ht
ml.
[33] Jin Y, Luo Y, Zheng W, Lu B. EEG-based emotion recognition using domain
adaptation network. International Conference on Orange Technologies. Singapore:
ICOT); 2017. p. 222–5. https://doi.org/10.1109/ICOT.2017.8336126.
[34] Homan RW, Herman J, Purdy P. “Cerebral location of international 10–20 system
electrode placement. Electroencephalogr Clin Neurophysiol 1987;66(4):376–82.
https://doi.org/10.1016/0013-4694(87)90206-9.
[35] Rieke F, Bialek W, Warland D. Spikes: Exploring the Neural Code (Computational
Neuroscience). MIT Press; 1999, ISBN 978-0262681087.
[36] Scott Millers, Childers Donald. Probability and random processes. Academic Press;
2012. p. 370–5.
[37] Krizhevsky A, Sutskever I, Hinton GE. Imagenet classiﬁcation with deep
convolutional neural networks. Adv Neural Inf Process Syst 2012:1097–105.
https://doi.org/10.1145/3065386.
[38] Cun YL, Bengio Y. “Convolutional networks for images, speech, and time series,”
The handbook of brain theory and neural networks. 1995.
[39] Acharya UR, Oh SL, Hagiwara Y, Tan JH, Adeli H. Deep convolutional neural
network for the automated detection and diagnosis of seizure using EEG signals.
Comput Biol Med 2018;100:270–8. https://doi.org/10.1016/
j.compbiomed.2017.09.017.
[40] Morgane PJ, Galler JR, Mokler DJ. A review of systems and networks of the limbic
forebrain/limbic midbrain. Prog Neurobiol 2005;75(2):143–60. https://doi.org/
10.1016/j.pneurobio.2005.01.001.
[41] Pessoa L. “Emotion and cognition and the amygdala: from “what is it?” to “what's to
be done”. Neuropsychologia 2010;48(12):3416–29. https://doi.org/10.1016/
j.neuropsychologia.2010.06.038.
[42] Yang Y, Wu QMJ, Zheng W, Lu B. EEG-based emotion recognition using hierarchical
network with subnetwork nodes. IEEE Transactions on Cognitive and
Developmental Systems June 2018;10(2):408–19. https://doi.org/10.1109/
TCDS.2017.2685338.
[43] Rahman MA, Hossain MF, Hossain M, Ahmmed R. Employing PCA and t-statistical
approach for feature extraction and classiﬁcation of emotion from multichannel
EEG signal. Egyptian Informatics Journal 2019. https://doi.org/10.1016/
j.eij.2019.10.002.
[44] Rahman MA, Khanam F, Ahmad M, Uddin MS. “Multiclass EEG signal classiﬁcation
utilizing R�enyi min-entropy-based feature selection from wavelet packet
transformation. Brain Informatics 2020;7(7). https://doi.org/10.1186/s40708-020-
00108-y.
[45] Rahman MA, Rashid MA, Ahmad M, Kuwana A, Kobayashi H. Modeling and
classiﬁcation of voluntary and imagery movements from the prefrontal fNIRS
signals. IEEE Access 2020;8:218215–33. https://doi.org/10.1109/
ACCESS.2020.3042249.
Md.A. Rahman et al.
Array 11 (2021) 100072
11
