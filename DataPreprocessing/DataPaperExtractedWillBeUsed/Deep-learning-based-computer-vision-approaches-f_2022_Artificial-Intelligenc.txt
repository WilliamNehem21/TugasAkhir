Deep learning based computer vision approaches for smart
agricultural applications
V.G. Dhanya a,⁎, A. Subeesh b,⁎⁎, N.L. Kushwaha c, Dinesh Kumar Vishwakarma d, T. Nagesh Kumar e,
G. Ritika c, A.N. Singh a
a ICAR- Indian Institute of Seed Science, Mau, Uttar Pradesh 275101, India
b ICAR- Central Institute of Agricultural Engineering, Bhopal, Madhya Pradesh 462038, India
c ICAR- Indian Agricultural Research Institute, New Delhi 110012, India
d Govind Ballabh Pant University of Agriculture and Technology, Pantnagar, Uttarakhand 263145, India
e ICAR - National Institute of Natural Fibre Engineering and Technology, Kolkata 700040, India
a b s t r a c t
a r t i c l e
i n f o
Article history:
Received 2 June 2022
Received in revised form 22 September 2022
Accepted 25 September 2022
Available online 30 September 2022
The agriculture industry is undergoing a rapid digital transformation and is growing powerful by the pillars of
cutting-edge approaches like artiﬁcial intelligence and allied technologies. At the core of artiﬁcial intelligence,
deep learning-based computer vision enables various agriculture activities to be performed automatically with
utmost precision enabling smart agriculture into reality. Computer vision techniques, in conjunction with
high-quality image acquisition using remote cameras, enable non-contact and efﬁcient technology-driven solu-
tions in agriculture. This review contributes to providing state-of-the-art computer vision technologies based on
deep learning that can assist farmers in operations starting from land preparation to harvesting. Recent works in
the area of computer vision were analyzed in this paper and categorized into (a) seed quality analysis, (b) soil
analysis, (c) irrigation water management, (d) plant health analysis, (e) weed management (f) livestock manage-
ment and (g) yield estimation. The paper also discusses recent trends in computer vision such as generative ad-
versarial networks (GAN), vision transformers (ViT) and other popular deep learning architectures. Additionally,
this study pinpoints the challenges in implementing the solutions in the farmer’s ﬁeld in real-time. The overall
ﬁnding indicates that convolutional neural networks are the corner stone of modern computer vision approaches
and their various architectures provide high-quality solutions across various agriculture activities in terms of pre-
cision and accuracy. However, the success of the computer vision approach lies in building the model on a quality
dataset and providing real-time solutions.
© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open
access article under the CC BY-NC-ND license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Key words:
Agriculture automation
Computer vision
Deep learning
Machine learning
Smart agriculture
Vision transformers
Contents
1.
Introduction. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
212
2.
Computer vision and deep learning models. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
213
2.1.
Image classiﬁcation with CNN and Object detection models . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
Abbreviations: AI, Artiﬁcial Intelligence; ANN, Artiﬁcial Neural Network; BP, Back Propagation; C-GAN, Conditional Generative Adversarial Network; CNN, Convolutional Neural
Network; COCO, Common Objects in Context; CV, Computer Vision; DCNN, Deep Convolutional Neural Network; DL, Deep Learning; DNA, Deoxyribo Nucleic Acid; RCNN, Region-
based Convolutional Networks; FCN, Fully Convolutional Networks; FLDA, Fisher's Linear Discrimination Analysis; GAN, Generative Adversarial Network; GLCM, Grey Level Co-
occurrence Matrix; GPU, Graphic Processing Units; HOG, Histogram of Oriented Gradients; KNN, K- Nearest Neighbour; LBP, Local Binary Patterns; LCTF, Liquid Crystal Tunable Filters;
LDA, Linear Discriminant Analysis; LIDAR, Light Detection and Ranging; LSTM, Long Short-Term Memory; MHA, Multi Headed Attention; ML, Machine Learning; MLP, Multi-Layer
Perceptron; NASNet, Neural Search Architecture Network; NLP, Natural Language Processing; OCR, Optical Character Recognition; PEAT, Progressive Environmental and Agricultural
Technologies; PLF, Precision Livestock Farming; ResNet, Residual Network; RF, Random Forest; RGB, Red Green Blue; SegNET, Semantic Segmentation Network; SSD, Single Shot
Multibox Detector; SVM, Support Vector Machine; UAV, Unmanned Aerial Vehicle; VGG, Visual Geometry Group; ViT, Vision Transformers; WSN, Wireless Sensor Network; YOLO, You
Only Look Once.
⁎ Corresponding author to: V.G Dhanya, ICAR - Indian Institute of Seed Science, Mau, Uttar pradesh 275101, India.
⁎⁎ Corresponding author to: A. Subeesh, ICAR - Central Institute of Agricultural Engineering (CIAE), Bhopal, Madhya Pradesh 462038, India.
E-mail addresses: dhanya.vg@icar.gov.in (V.G. Dhanya), subeesh.a@icar.gov.in (A. Subeesh).
https://doi.org/10.1016/j.aiia.2022.09.007
2589-7217/© 2022 The Authors. Publishing services by Elsevier B.V. on behalf of KeAi Communications Co., Ltd. This is an open access article under the CC BY-NC-ND license (http://
creativecommons.org/licenses/by-nc-nd/4.0/).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Agriculture
journal homepage: http://www.keaipublishing.com/en/journals/artificial-
intelligence-in-agriculture/
2.2.
Generative adversarial network (GAN) and Vision Transformers (ViT) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
214
3.
Deep learning driven computer vision – Application areas in agriculture. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
3.1.
Seed quality analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
215
3.2.
Soil analysis
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
216
3.3.
Irrigation management
. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
217
3.4.
Plant health analysis. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
218
3.5.
Weed management . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
219
3.6.
Livestock management. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
220
3.7.
Yield estimation. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
4.
Practical implications. . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
221
5.
Challenges and way forward . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
223
6.
Conclusions . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
References . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . .
224
1. Introduction
The UNDP 2021 report on “leveraging digital technology for sustain-
able agriculture” states that global food production needs to be in-
creased by 98 percent to feed a burgeoning human population of 9.9
billion by 2050 (Burra et al., 2021). This target needs to be accomplished
through the effective utilization of available resources viz land, labor,
capital, and technology (Ranganathan et al., 2018). Present status on
precision agriculture aims to deﬁne the decision support system for
farm management by optimizing the output while consecutively pre-
serving the resources applied. Constructively pointing out, the emerging
trend of food security needs to be handled with data-driven farming
that can increase productivity, efﬁciency, and proﬁts. The key challenges
such as food demand, labor shortage, water shortage, climate change
(Badrzadeh et al., 2022; Elbeltagi et al., 2022a; Kaack et al., 2022) and in-
creasing energy demands lead to the need for technology intervention.
The opportunity offered by smart agriculture, which encompasses pre-
cision agriculture, digital agriculture as well as modern agricultural
practices, needs prime validation at this point. Smart agriculture is pri-
marily based on three platforms viz, science, innovation, and ICT (Infor-
mation and Communication Technology) (Khanna and Kaur, 2019). The
traditionally used information and knowledge management system for
collecting and monitoring agricultural data is not only laborious but is
also time-consuming and error-prone. Therefore, the technical ad-
vancement in remote sensing, digital applications, sensors, advanced
imaging systems, cloud data storage along with intelligent data analysis
using the decision support systems need to be well utilized in making
the farming sector smarter (Fig. 1). Smart agriculture can leverage
cutting-edge technologies like the Internet of Things, Machine learning,
Cloud computing, Blockchain, etc., and beneﬁt from these opportunities
in improving food production and addressing the emerging challenges
in this sector (Sami et al., 2022).
Recently, the inﬁltration of computer/ mobile technology even to the
most rural pockets, has provided an inimitable facility in connecting the
rural producers with the city-consumers or the international investors,
thereby facilitating better investments and knowledge transfer in agri-
culture (Aker, 2011; Karim et al., 2013). Artiﬁcial intelligence (AI) is a
game-changing technology that already has proven track records across
various industries, including agriculture (Adnan et al., 2021; Bhagat
et al., 2020; Jamei et al., 2022b; Kumar et al., 2019; Subeesh et al.,
2019). The use of machine learning, a subset of artiﬁcial intelligence,
has been covered extensively by researchers in delivering innovative
solutions for modelling complex relationships and further, making pre-
dictions on agriculture data (Bhavsar and Panchal, 2012; Heramb et al.,
2022; Jamei et al., 2022c; Karbasi et al., 2022; Malik et al., 2022a; Rai
et al., 2022; Rehman et al., 2019; Tantalaki et al., 2019). Computer vi-
sion, a ﬁeld of artiﬁcial intelligence, is making a machine “see”, using
the modern technologies involving a camera and computer instead of
human vision, empowering extensive automation capabilities to AI sys-
tems. Computer vision collects necessary visual data regarding crops,
livestock, farm or garden, allowing us to identify, detect and track spe-
ciﬁc objects using visual elements and comprehend complex visual
data for automation tasks. In the past decades, expert and intelligent
systems based on computer vision technology have been well utilized
for agricultural operations (Foglia and Reina, 2006; Gomes and Leta,
2012; Rico-Fernández et al., 2019). Further, the development of modern
technologies and hardware supports like Graphic Processing Units
(GPUs) and edge devices have diversiﬁed the application of computer
Fig. 1. Components of smart agricultural solutions.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
212
vision, thereby making strands to efﬁcient agricultural production (Li
et al., 2019; Mochida et al., 2019; Rehman et al., 2019; Vázquez-
Arellano et al., 2016). Modern computer vision techniques can help in
the digital quantiﬁcation of different morphological and physiological
plant parameters along with the qualitative assessment of the same
and are expected to rapidly improve the accuracy of plant phenotyping
(Araus and Cairns, 2014; Ghanem et al., 2015). Further, combining the
computer vision techniques with the high throughput molecular meth-
odologies of DNA sequencing provides an opportunity for genome-wide
exploration of useful genes and molecular modeling of the same to un-
derstand the complex traits such as plant yield and productivity, stress
tolerance, biotic and abiotic stress management etc (Araus et al., 2018;
Shakoor et al., 2017). Thus imaging with computer vision technology
aided by various imaging sensors and algorithms can indeed play a
major role in precision agriculture and in paving the way for smart ag-
riculture (Araus et al., 2018). A data driven precision agriculture system
architecture consists of sensors deployed on the ﬁelds (sensing layer),
network layer that provides connectivity, storage and other services
(service layer) and application layer consisting of the end user accessing
the services through mobile/web-based applications (Fig. 2). Integrative
and multi-mode Artiﬁcial Intelligence (AI) models can be deployed to
predict crop behaviour under differing ﬁeld conditions (Shrivastava
and Marshall-Colon, 2018; Waldhoff et al., 2017). The yield perfor-
mance of major crops in various regions, along with the ﬁeld conditions
for crop production, environmental impact and economic outcome,
have been assessed using the algorithms of deep learning and machine
learning (Tantalaki et al., 2019). Deep learning permits the computa-
tional models with multiple processing layers to indicate the data in
multiple levels of abstraction (Schmidhuber, 2015). The main applica-
tion of deep learning in the ﬁeld of agriculture are building models to
derive meaningful insights from agriculture data (Jamei et al., 2022a;
Malik et al., 2022b), image analysis including classiﬁcation and object
detection, such as the detection of diseases, weed identiﬁcation, soil
analysis, plant disease detection, etc. (Kamilaris and Prenafeta-Boldú,
2018).
For this study, we have collected more than 100 research papers
from scientiﬁc databases, including PubMed, Web of Science, and
Scopus, in the area of deep learning-based computer vision. Further,
we investigated all these works that leveraged deep learning-based
computer vision technologies to address key agriculture tasks such as
plant health monitoring, disease and weed identiﬁcation, irrigation
management, soil analysis, livestock management, yield estimation,
etc. The main objective of this study is to evaluate the penetration of
deep learning-based computer vision approaches in key agricultural
problems, and this review is intended to be useful to agriculture re-
searchers as well as general computer vision researchers who are inter-
ested in the application of computer vision solutions to automate and
solve potential agricultural problems. The practical implications of
these technologies along with major challenges in implementing
large-scale applications were also constructively pointed out in this
study.
2. Computer vision and deep learning models
Computer vision possesses dual and interrelated goals. In biological
science, computer vision aims to represent the human visual system
using computational models, and in the engineering perspective, com-
puter vision attempts to create autonomous systems that can do tasks
that often human visual systems cannot perform (Huang, 1993). Com-
puter vision imparts visual capability to machines through cameras,
data, models, and algorithms rather than retinas and the visual cortex.
Optical character recognition (OCR) technology and intelligent charac-
ter recognition were some major tasks that employed computer vision
to accomplish tasks such as document and invoice processing, vehicle
plate detection, etc. In the early stages of computer vision research,
the main focus was to build algorithms to detect edges, curves, corners,
Fig. 2. Data-driven precision agriculture system architecture.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
213
and other basic shapes. Before the era of deep learning, image process-
ing relied on gray level segmentation and this approach wasn’t robust
enough to represent complex classes. Modern computer vision algo-
rithms rely extensively on artiﬁcial neural networks that provide a dra-
matic improvement in performance and accuracy compared to
traditional approaches for image processing. Deep learning-based com-
putation models allow multiple processing layers to learn and infer
complex patterns mimicking the human brain (O’Mahony et al., 2020;
Schmidhuber, 2015; Zhong et al., 2016). It runs and inspects the data
over several iterations until it discerns distinctions and identiﬁes or rec-
ognizes the features in the images. The recent surge of interest in deep
learning is due to the fact that it can handle massive amounts of hetero-
geneous data (visual, audio, text, etc.) and is capable of embedding so-
lutions into several hardwares. DL allows automatic feature extraction
and can be utilized in numerous image processing tasks and is well
known for its effectiveness in handling vision-based activities like
image classiﬁcation, object detection, semantic segmentation, etc. In
fact, these tasks are the backbone for modeling and automating agricul-
tural activities such as disease identiﬁcation, weed detection, yield esti-
mation, etc (Jha et al., 2019; Subeesh and Mehta, 2021; Tian et al., 2020).
2.1. Image classiﬁcation with CNN and Object detection models
Convolutional neural network-based deep learning architectures are
popular for computer vision tasks like image classiﬁcation. A
convolutional neural network is a type of neural network architecture
that takes input images and extracts relevant features to efﬁciently
identify and classify images. CNN uses labels to perform convolutions
and generate feature maps. The introduction of imageNet dataset that
contained millions of tagged images had laid a foundation and
benchmark for building advanced computer vision-based models
(Kriegeskorte and Golan, 2019; Miikkulainen et al., 2019; Yoo, 2015).
LeNet-5 was one of the earliest CNN proposed by Yann LeCun (LeCun
et al., 1998), led to the development of various CNN models (Fig. 3). In
2012, AlexNet architecture (Krizhevsky et al., 2012a) was found prom-
ising for image recognition, and numerous new architectures such as
VGGNet (Simonyan and Zisserman, 2015), ResNet (He et al., 2015),
etc. were also introduced by researchers, reducing the error rate and im-
proving the performance. Image segmentation approaches are quite
useful for understanding what an image consists of, by dividing the im-
ages into several segments. Image segmentation creates a pixel-
oriented mask for each object present inside the image. This eases the
image processing tasks as the important segments alone can be consid-
ered for processing tasks.
The image classiﬁcation mainly identiﬁes the class, a speciﬁc image
belongs to. The image classiﬁcation approach is often not successful
when there are multiple objects in the same image. Object detection
aims to detect the location of objects in the image/video. Object detection
task comprises two major components; class information and location in-
formation. The location information is described by bounding boxes
around the target object. Object detection architectures such as YOLO
(You Only Look Once) (Redmon et al., 2016), SSD (Single Shot Multibox
Detector) (Liu et al., 2016), Faster-RCNN (Region Convolutional Net-
works) (Ren et al., 2016) are widely used for object detection and auto-
mation across different domains including agriculture.
2.2. Generative adversarial network (GAN) and Vision Transformers (ViT)
A generative adversarial network (GAN) is a special type of neural
network used for unsupervised learning. GAN is an approach to genera-
tive modeling that can learn to mimic a given distribution of data. These
models effectively reduce the data into its fundamental properties or
generate new data points with varied properties. The application
of GANs has achieved state-of-the-art performance in many image
generation tasks, such as text-to-image synthesis (Xu et al., 2017),
super-resolution (Ledig et al., 2017), and image-to-image translation
Fig. 3. Architecture of (a) Convolutional neural networks (b) LeNet-5 architecture.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
214
(Zhu et al., 2020b). Generally, GAN has two main building blocks (two
neural nets) which compete with each other and are capable of captur-
ing, copying and analyzing the variations in a dataset (Fig. 4). The two
networks are usually called Generator and Discriminator. The generator
neural network helps to generate new instances, while the discrimina-
tor neural network evaluates the authenticity of the generated images.
The discriminator decides whether or not every instance of the data it
evaluates belongs to the actual training set and penalizes the generator
for generating implausible outcomes. The loss of the discriminator is
used for improving the generator (Reimers and Requena-Mesa, 2020).
The discriminator tries to identify the fake data from the real data, and
both networks work simultaneously to learn complex data. GANs are
a panacea for the data scarcity problem, which is a serious hurdle in de-
veloping robust deep neural network models (Hiriyannaiah et al.,
2020). The realistic images produced by GAN that are different from
the original training data are attractive in data augmentation of DL-
computer vision to reduce the model overﬁtting.
Transformer models have become the de-facto status quo in text
processing, and recently, the computer vision community has extended
the concept of NLP (Natural Language Processing) transformer to apply
to the image domain with slight modiﬁcation in the implementation to
process multiple modalities (e.g., images, videos, etc.) using similar pro-
cessing blocks (Dosovitskiy et al., 2021; Khan et al., 2021; Vaswani et al.,
2017). Even though the general architecture used in both cases are sim-
ilar, ViT uses different approaches for tokenization and embedding
(Fig. 5). The overall architecture consists of 3 main components, viz.,
patch embedding, feature extraction by stacked transformer encoders
and the classiﬁcation head. In ViT, initially, the input image of shape
(height, width, channels) is embedded into a feature vector of shape
(n+1, d), using a set of transformations. The input image is split into a
group of image patches. Later, these groups of image patches are em-
bedded into encoded vectors and fed into transformer encoder network.
The transformer encoder learns the features from the embedded
patches using a stack of transformer encoders (Wu et al., 2021). The en-
coder mainly comprises multi-headed attention (MHA) and a 2-layer
MLP with layer normalization and residual connections. The ﬁnal MLP
block, called the MLP head is used as an output of the transformer. In
the case of image classiﬁcation, a softmax on the output generates the
classiﬁcation outputs. ViTs are useful in several vision applications such
as image classiﬁcation, image-to-text, text-to-image generation, image
segmentation, object detection, etc (Bazi et al., 2021; Li et al., 2022).
3. Deep learning driven computer vision – Application areas in
agriculture
3.1. Seed quality analysis
The commercial seed industry is focused on the supply of the right
quality seeds to the farmers at the right time in the right quantity. Filter-
ing out low-quality of seeds from high-quality ones, is not only labori-
ous, but it requires sophisticated equipments, infrastructure, and time
(Kannur et al., 2011). The testing of seeds for their quality can indeed
gain momentum by the use of computer vision technology which can
extract the morphological information of different seed lots and grade
it according to the internationally prescribed quality standards (Bao
and Bambil, 2021). The different seed testing modules are likely to ad-
dress their physical purity, genetic purity, seed health, vigour, patterns
of deterioration etc., which in general may indeed cover the physical
or visually attributable characters such as the seed length, shape, size,
visual impairments, and presence of foreign bodies which can indeed
be captured by the advanced computer vision technology (Granitto
et al., 2005).
Performance issues of traditional computer vision have greatly been
improved by deep learning-based computer vision, resulting in larger
adoption for seed variety identiﬁcation. The seed quality evaluation pro-
cess using computer vision is shown in Fig. 6. Often, spectral imaging
techniques are also merged with these approaches to enhance the accu-
racy (Qiu et al., 2018; Zhu et al., 2019). In a study conducted by Zhu et al.
(2019), combining spectroscopy and machine learning – CNN models
were found to be effective in identifying the seed varieties. The machine
learning models showed an accuracy of more than 80% in classifying the
cotton seeds based on the feature extracted by the CNN and ResNet
models. In another investigation, SeedSortNet built from computer vi-
sion CNN models, was found to be promising, with accuracies 97.33%
and 99.56% in sorting the maize and sunﬂower seeds (Li et al., 2021).
CNN deep learning was also utilized for cognizing the viable and
non-viable seeds and was found to be successful with 90% viability
prediction accuracy for naturally aged seeds (Ma et al., 2020).
Fig. 4. Overview of training process in GAN (Generative Adversarial Network).
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
215
Taheri-Garavand et al. (2021) developed models for automatic identiﬁ-
cation of chickpea varieties using seed images in the visible spectrum. A
modiﬁed VGG16 model was used for the identiﬁcation purpose. As
sorting high-quality seeds are vital for increasing yield in the breeding
industry, Zhao et al. (2021) employed seven different computer vision
models to accurately detect and identify surface defects. MobileNet-V2
model had shown excellent detection accuracy for the soybean dataset.
There are numerous such studies done by various researchers and the
seed industry is hugely getting beneﬁted from advanced computer vi-
sion models, achieving a higher level of automation capabilities. Some
of the studies in this area are precisely summarized Table 1.
3.2. Soil analysis
The preservation and improvement of dynamic soil characteristics is
the main emphasis of soil management in agriculture for increasing
crop productivity (Kushwaha et al., 2022; Suchithra and Pai, 2020). Tra-
ditional soil texture analysis entails taking soil samples and bringing
them to a laboratory, where they are dried, crushed, and sieved before
being used. For coarse textured or sandy soils, sieving is the most typical
laboratory analytical method, while for smaller textured particles, a hy-
drometer or pipette approach based on sedimentation theory is used
(Kushwaha et al., 2022; Sudarsan et al., 2016).
With the advancement of image processing power and the develop-
ment of image acquisition (e.g., cameras) systems in recent years, com-
puter vision-based image analysis approaches have gotten a lot of
interest in a lot of sectors, including soil science. This method collects
soil images (dynamic or static) with cameras and then uses simple com-
puter programmes to classify and categorise them (Fig. 7). For example,
after matching textural patterns, the size of the soil particles might
be estimated straight from the image. In several investigations,
various image analysis-based computer vision approaches were tried.
Fig. 5. The architecture of Vision Transformer Model for image classiﬁcation (Dosovitskiy et al., 2021; Vaswani et al., 2017).
Fig. 6. Seed quality analysis using data-driven models.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
216
Haralick et al. (1973) attempted to classify images received from an ae-
rial or satellite source using entropy and angular moment-based tex-
tural classiﬁcation. Since then, the grey level co-occurrence matrix
(GLCM) and its analogues have been used in a variety of remote sensing
applications (Dell’Acqua and Gamba, 2003; Kuplich et al., 2005). How-
ever, the greatest resolution satellite can only provide a maximum res-
olution of 10 m/square pixel, which is insufﬁcient to understand soil
particle sizes. Riese and Keller (2019a) implemented three 1-
dimensional (1D) convolutional neural networks: the LucasCNN, the
LucasResNet and the LucasCoordConv. In addition, for the classiﬁcation
problem at hand, the study tweaks two existing 1D CNN techniques and
compares the CNN techniques against a random forest classiﬁer to see
how well they do. Thereby, study uses the LUCAS topsoil dataset,
which is freely available. The CNN method with the least amount of
depth turns out to be the most effective classiﬁer. In terms of average ac-
curacy, the LucasCoordConv has the best results.
Similarly, Zhang et al. (Zhang et al., 2003) proposed a soil texture
classiﬁcation system that uses the wavelet transform approach to dis-
tinguish between different types of soil. Wavelet transform, which is a
strong image and signal analysis method due to its multi-resolution ca-
pabilities, is used to extract features. A set of training instances is used to
create a maximum likelihood (ML) classiﬁer. This method of ML
parameter estimation produces the best results. At the time of training
and classiﬁcation, the Fisher's Linear Discrimination Analysis (FLDA) is
used to optimize and reduce the dimension of the vector. Soil textures
such as clay, sand, and silt are employed for training and classiﬁcation.
Clay, sand, and silt have 60 percent, 100 percent, and 100 percent cate-
gorization rates, respectively. In instance segmentation, Zhang et al.
(Zhang et al., 2020) suggested a mask reﬁned R-CNN for reﬁning object
details. The goal is to ﬁgure out how semantic segmentation of high-
level and low-level features affects instance segmentation. The COCO
(common objects in context) and cityscapes datasets were used to col-
lect the trial results. This approach is reported to be simple to use and
effective. Some of the previous signiﬁcant studies in soil analysis using
DL computer vision have been summarized in Table 2.
3.3. Irrigation management
Irrigation water management in agricultural production necessitates
considerable effort and is crucial in maintaining hydrological, climato-
logical, and agronomic equilibrium. Several studies have thus been un-
dertaken in gaining knowledge of the biophysical processes included
in the uptake of water through the root zone of the soil and the pro-
cesses of transpiration through the plant canopy (Elbeltagi et al.,
Table 1
Previous studies on seed quality analysis through application of computer vision and deep learning.
Reference
Objectives and scenario
of application
Methodology
Crop
Results
(Javanmardi
et al., 2021)
Corn variety
classiﬁcation using 9
different varieties
CNN as a generic feature extractor. Classiﬁcation
using ANN, SVM, kNN, boosted tree, bagged tree
and LDA
Corn
CNN ANN classiﬁcation has a classiﬁcation accuracy of 98.1%,
precision 98.2%, recall 98.1% and F1 score of 98.1%.
(Qiu et al., 2018)
Variety identiﬁcation in
rice
KNN, SVM and CNN models
Rice
CNN outperformed other models with 89.6% accuracy on the
training set and 87% accuracy on the testing set.
(Gulzar et al.,
2020)
Seed classiﬁcation
using 14 types of seeds
VGG16 architecture for classiﬁcation
-
99.9% accuracy over test set with 234 images
(Wu et al., 2019)
Variety identiﬁcation in
oats
DCNN model
Oats
99.19% accuracy on testing set.
(Gulzar et al.,
2020)
Seed classiﬁcation in
maize and sunﬂower
CNN model
Maize and
Sunﬂower
CNN based visual model - SeedSortNet developed with 97.33
percent accuracy on maize and 99.56% accuracy on sunﬂower
dataset respectively.
(Liu et al., 2015)
Soyabean seed sorting
BP neural network
Soyabean
97.25% average recognition accuracy over 857 images of soybean
seeds with pest and insect damage.
(Veeramani
et al., 2018)
Corn seed defect
detection
VGG 19 and GoogleNet
Maize
-
(Dolata and
Reiner, 2018)
Varietal identiﬁcation
in barley
CNN
Barley
Increase in average classiﬁcation accuracy by 0.6% and sensitivity
by 2.3% with respect to view point ignorant architecture of the said
study.
(Kurtulmuş,
2021)
Seed classiﬁcation in
sunﬂower
AlexNet, GoogleNet and ResNet
Sunﬂower
95% accuracy with GoogleNet algorithm for classiﬁcation of 4800
sunﬂower seeds.
(Ni et al., 2019)
Seed grading in maize
DCNN
Maize
98.2% prediction accuracy for 408 test images in maize.
Fig. 7. Soil texture analysis using image processing.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
217
2022b; Kushwaha et al., 2021). For an effective irrigation schedule, it is
necessary to know the precise amount of water required by the crop
(Kushwaha et al., 2016; Vishwakarma et al., 2022). The application of
computer vision technologies, as well as the integration and deploy-
ment of automated crop production management, plant irrigation, and
yield evaluation, thus become critical. Zhang et al. (Zhang et al., 2018)
performed identiﬁcation and monitoring of centre pivot irrigation sys-
tems using a Convolutional Neural Networks (CNNs) approach to the al-
location of irrigation water. The CNNs with various structures were built
and compared and for data augmentation,training, a sampling strategy
was developed. In the testing region, the CNN with the best perfor-
mance and the shortest training time was used. To further pinpoint
the centre of each centre pivot system, a variance-based technique
was presented. The proposed approach performed well in the centre
pivot irrigation systems identiﬁcation challenge, with a precision of
95.85% and a recall of 93.33% of the identiﬁcation ﬁndings.
Similarly, Chang and Lin (Chang and Lin, 2018) developed a compact
intelligent agricultural machine which is capable of autonomous
weeding and variable watering on the cultivated ground, using a combi-
nation of computer vision and multitasking. The system classiﬁes the
plants and weeds in real-time so that it can weed and water while main-
taining an average herbicidal rate of 90% and a deep soil moisture level
of 80%. This strategy has a lot of potential because it allows for not only
multitasking integration but also resource utilization in its entirety.
Kamyshova et al. (Kamyshova et al., 2022) proposed a computer
vision-based technology for optimizing the watering process of crops
utilizing a phyto indication system in low latency mode, the study sug-
gested an algorithm-based system for obtaining a maize irrigation map.
The system, which comprises 8 IP cameras coupled to a DVR connected
to a laptop, can be mounted on a centre pivot irrigation system. There
are three steps to the algorithm. Using an integrated excess green and
excess red difference (ExGR) index during the image preprocessing
stage. The application of the approach that the study chose based on
the system's operational conditions is the categorization stage. A neural
network trained using the Resilient Propagation method is utilised in
the ﬁnal stage to calculate the rate of watering of plants in the cur-
rent sector of the sprinkler site. Plant identiﬁcation accuracy was
up to 93 percent, and growth stages were up to 92 percent. Low-
cost cameras are now being used in all sectors of technology, partic-
ularly in agricultural applications. The soil water balance may be
precisely assessed to enable accurate irrigation planning by acquir-
ing relevant information on the growth of horticulture crops through
photographs (Koech and Langat, 2018). Table 3 shows the irrigation
water management through the application of computer vision and
deep learning technologies.
3.4. Plant health analysis
With the advancement in computer vision and deep learning, new
promising solutions for identifying overall health status of the plants
were introduced. The intelligent decision support system for identifying
crop diseases (Fig. 8), water stress, and nutrient deﬁciencies would lead
to timely control of the panic situations and eradicating the huge losses,
ultimately leading to improved plant quality.
Plant stress induced by biotic and abiotic factors is expressed in the
plant canopy as multiple symptoms. In case of water stress, the plant
closes stomata and delays photosynthesis and transpiration activities
indicating colour changes in the leaf and temperature (Nilsson, 1995).
Similarly, nutrient deﬁciencies-related symptoms are typically visible
in leaves color and texture (Xu et al., 2011). Image analysis can detect
these changes in a pattern quite effectively. Deep learning-based com-
puter vision approaches are viable solutions in addressing timely dis-
ease identiﬁcation and avoiding consultation of human experts. The
availability of a large number of public image datasets such as
PlantVillage (Hughes and Salathe, 2016), PlantDoc (Singh et al., 2020)
have proliferated the research in the area of disease identiﬁcation and
many works have taken encouraging steps towards disease-free agri-
culture (Hassan and Maji, 2022; Ji and Wu, 2022; Nagasubramanian
et al., 2019).
The PlantVillage dataset has been extensively utilized by various re-
searchers for solving disease identiﬁcation problems using deep learn-
ing (Amara et al., 2017; Brahimi et al., 2017; Ferentinos, 2018;
Mohanty et al., 2016). Several studies reveal that pre-trained models
quickly and accurately identifying the diseases in terms of precision, re-
call and F1 scores (Abbas et al., 2021; Chen et al., 2020b; Coulibaly et al.,
2019; Mukti and Biswas, 2019; Thakur et al., 2021). Abbas et al. (Abbas
et al., 2021) used synthetic images generated using the Conditional
Generative Adversarial Network (C-GAN) to build tomato leaf disease
detection. C-GAN can address the issue of data insufﬁciency and provide
more generalization to the models (Mirza and Osindero, 2014). It is
worth noting that some investigations were focused on the localization
of the disease spots, giving precise information about the diseases (Cen
et al., 2016; Liu and Wang, 2020; Mathew and Mahesh, 2022; Son,
2021). Several other studies reported research on DL-computer vision
based identiﬁcation of crop stresses, including water stress and nutrient
deﬁciencies (Abdalla et al., 2021; Anami et al., 2020; Jahagirdar and
Table 2
Previous studies on computer vision and deep learning technologies for soil properties analysis and management.
Reference
Objectives and
scenario of
application
Methodology
Results
(Riese and Keller,
2019b)
Soil texture
analysis
The CNN architectures LucasCNN, the LucasResNet and theLucasCoordConv
Models
The CNN method with the least amount of depth turns
out to be the most effective classiﬁer
(Omondiagbe
et al., 2022)
Soil texture
prediction
Employed automated deep convolutional neural networks and
population-based learning by replacing the random search with a Bayesian
Optimization.
Results show improvements of 5% to 26% for all three soil
properties such as sand, silt and clay.
(Pyo et al., 2020)
Estimation of
heavy metal
concentration
From the soil reﬂectance images, CNN with convolutional autoencoders was
trained to estimate As, Cu and Pb metals.
The highest accuracies reported for As, Cu, and Pb
estimates were with R2 values of 0.86, 0.74, and 0.82.
(Zhong et al.,
2021)
Soil properties
The DCNN architectures LucasResNet-16 and LucasVGGNet-16 models
When compared to a single-task DCNN model, the
performance of a multi-task DCNN model created based
on LucasResNet-16 was enhanced.
(Yu et al., 2019)
Soil
Classiﬁcation
Lquid crystal tunable ﬁlters (LCTF)-based system and three-dimensional
convolutional neural network (3D-CNN) for soil classiﬁcation
The overall accuracy of 99.59% for 3D-CNN-SD-PCA.
(Azadnia et al.,
2022)
Texture Analysis
Portable smartphone-based machine vision system using CNN was
developed. The features were extracted using CNN and classiﬁcation is
performed using ANN, SVM, RF and KNN classiﬁers.
Model accuracies at distances of 20, 40 and 60 cm were
of 99.89, 99.81 and 99.58%,
(Azadnia et al.,
2022)
Texture analysis
Deep learning models VggNet16, ResNet50, and Inception-v4 models were
used to classify soil aggregates
Overall accuracy obtained for CNN networks was 96.2%,
97.1%, and 98.7%
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
218
Budihal, 2021). Table 4 shows the previous studies on deep learning
based computer vision technology on plant health analysis.
3.5. Weed management
Weeds are among the major factors that affect agricultural produc-
tion negatively. With the focus on improving agricultural productivity,
it is evident that more and more chemicals are being dumped into the
environment with the aim of managing the weed growth. But for im-
proving the productivity, it also requires the optimum utilization of re-
sources which can only be achieved by the precise spraying on weeds.
The traditional robotic weeders generally function by detecting crop
row patterns and they do not rely on crop recognition for the weeding
operation. If the weed density and population are large, they may ob-
scure the row pattern leading to reduced efﬁciency of the weeders.
Computer vision approaches come to rescue at this point by accurately
identifying the objects as precise spraying of weeds depends on the ac-
curate identiﬁcation and location of weeds. Recently, several studies
were carried out by researchers on adaptability of computer vision tech-
nology for the agronomic classiﬁcation of plant species at the ﬁeld level,
viz the classiﬁcation of crops from weeds, off types etc. (Sau and
Ucchesu, 2019; Sau et al., 2018; Subeesh et al., 2022). Detailed applica-
tion of the same in the automatic identiﬁcation of plant species based on
the leaf recognition pattern has been proposed for preserving and
cataloguing plant species (Putzu et al., 2016) along with the botanical
characterization of germplasm (Lo Bianco et al., 2017). Methods of
achieving weed detection at the ﬁeld level mainly include the utilization
of computer vision technology using the traditional image processing
and deep learning. When, the conventional methods of computer vision
are used, extracting the different features such as colour, shape, texture
Table 3
Previous studies on irrigation water management through application of computer vision and deep learning approaches.
References
Objectives and scenario of application
Methodology
Results
(Albuquerque
et al., 2020)
Identiﬁcation of malfunctioning in the
irrigation systems
Mask R-CNN based segmentation on UAV captured
images
Given dataset sizes, the results are satisfactory.
(Chen et al.,
2020a)
Identiﬁcation of water pollution for
agricultural irrigation resources
Shallow CNN model in combination with decision
tree algorithm trained on NIR data
Validation results were 25.47 of RMSEV and 0.914 of
Rv.
(Zhang et al.,
2018)
Monitoring and identiﬁcation of canter pivot
irrigation system to supply irrigation water
CNN based segmentation on UAV captured images
Precision and recall of 95.85% and 93.3 percent,
respectively, were attained.
(Tang et al., 2021)
Monitoring the distribution of center pivot
irrigation systems
Lightweight real-time object detection network
(PVANET) based on GoogLeNet and Hough
transform
Experiments with Sentinel-2 images achieved a
precision of 95% and a recall of 95.5%,
(Kumbi and Birje,
2022)
Irrigation efﬁciency
Sun-ﬂower Atom Optimization-based Deep
convolution neural network (SFAO-DeepCNN)
algorithm
Maximal accuracy of 92%, speciﬁcity of 91.2% and
sensitivity of 94.1%
(Kim et al., 2022)
Water Level Estimation of Irrigation Channel
ResNet-50 image classiﬁcation and U-Net
segmentation models on irrigation canal's CCTV
images
The image segmentation model showed a Dice score
of 0.998 and predicted water levels showed R2 of 0.97
Fig. 8. Deep learning based computer vision approach for plant health analysis.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
219
etc., and combining them with the machine learning methods such as
the SVM becomes necessary. But with the improvement in computing
power, the deep learning algorithms can beneﬁcially extract multidi-
mensional and multi-scale spatial and semantic feature information of
weeds through AlexNet, VGGNet, ResNet, etc due to their enhanced ca-
pability for image data expression thereby avoiding the disadvantages
of traditional methods of feature extraction. The application of deep
learning in agronomic classiﬁcation of plant species has gained momen-
tum after the outbreak of CNN and AlexNet (Krizhevsky et al., 2012b).
Hall et al. (2015) have utilized the CNN architecture in classifying leaves
of 32 species of crops and weeds by capturing nearly 1900 images of the
same.
Utilization of CNN architecture in the classiﬁcation and differentia-
tion of weeds from different species of wheat, sugarbeet, corn, soybean,
sunﬂower, etc. has been proposed by Kussul et al. (2017), while the
modiﬁed version of VGG16 for the classiﬁcation of barley, grass, oil
crops and weeds have been proposed by Mortensen et al. (2016).
Table 5 shows the previously applied computer vision technology for
weed management.
3.6. Livestock management
Computer vision approaches are leveraged extensively in precision
livestock farming (PLF), ensuring optimum output and health of each
individual animal. Livestock monitoring systems provide real-time in-
formation and assist farmers in making strategic decisions (Fig. 9). The
non-invasive computer vision technology has been widely researched
for its use in recognition of livestock behaviour over the past few
years (Bello et al., 2021; Kumar et al., 2017; Qiao et al., 2019a; Shen
et al., 2020). Xiao et al. (2022) employed a modiﬁed Mask-RCNN
model and trained a fusion of Mask-RCNN and SVM to identify cows
in unconstrained barn. Hansen et al. (2018) trained a CNN to recognize
pigs via the face using a data set with 1,553 images. The VGG-face model
used in this study achieved an accuracy of 96.7%. Some of the
Table 4
Previous studies on computer vision and deep learning technologies for crop health analysis.
References
Objectives and scenario of application
Methodology
Crop
Results
(Hassan and Maji,
2022)
Plant disease identiﬁcation
Novel lightweight CNN based on Inception
and Residual connections with fewer
parameters
Rice,
Cassava
The testing accuracies of the proposed model is
99.39%,99.66% and 76.59% on Plantvillage, Rice, and
Cassava dataset
(Hati and Singh,
2021)
Species Recognition (SR) and
Identiﬁcation of Healthy and Infected
Leaves (IHIL)
Residual network (ResNet) based
convolutional neural network (CNN)
architecture
12 different
plant
species
Species identiﬁcation: Precision 91.84%, Recall 91.67%
and F191.49%. IHIL : Precision 84%, Recall 83.14% and
F1 83.19%
(Ji and Wu, 2022)
Black measles disease identiﬁcation
in grape
Plant disease evaluation. Image
segmentation using DeepLabV3 with
ResNet50 backbone
Grape
Overall classiﬁcation accuracy of 97.75% on the
hold-out test dataset.
(Syed-Ab-Rahman
et al., 2022)
Citrus diseases classiﬁcation using
leaf images
Two-stage deep CNN model
Citrus
Detection accuracy of 94.37% and an average precision
of 95.8%.
(Li and Li, 2022)
Leaf disease identiﬁcation
Vision Transformer-based lightweight
apple leaf disease- identiﬁcation model
(ConvViT)
Apple
ConvViT achieved an accuracy of 96.85% on the apple
leaf disease dataset
(Mkonyi et al.,
2020)
Early identiﬁcation of Tuta absoluta
disease
Pre-trained CNN architectures VGG16,
VGG19 and ResNet50 Models
Tomato
VGG16 attained the highest accuracy of 91.9%
(Azimi et al.,
2021)
Stress level detection due to nitrogen
deﬁciency
Custom Deep learning architecture with 23
layers.
Sorghum
8.25% better accuracy than traditional machine
learning techniques
(Joshi et al., 2021)
Viral disease diagnosis
Convolutional neural network - VirLeafNet
Vigna
mungo
Accuracies of VirLeafNet-1, VirLeafNet-2, and
VirLeafNet-3 were 91.234%, 96.429%, and 97.403%
(Shah et al., 2021)
Plant disease detection
ResTS Architecture with residual
connection
14 crops
F1-Score: 0.991
(Singh et al., 2021)
Pest and disease detection
2D-CNN model with segmented images
Coconut
tree
Accuracy of 96.94% with a Kappa value 0.91
Table 5
Previous studies on computer vision and deep learning technologies for weed management.
References
Objectives and scenario of
application
Methodology
Crop
Results
(Le et al., 2020)
Weed identiﬁcation in
Canola, corn and raddish
Filtered Local Binary Pattern withContour Mask
and Coefﬁcient k (k-FLBPCM), VGG-16,
VGG-19,ResNet-50, Inception-v3
Canola, corn,
radish
K-FLBPCM method outperformed other state of the
art CNN models.
(Osorio et al., 2020)
Weed detection in lettuce
Compared Mask R-CNN with HOG SVM and YOLO
V3
Lettuce
98% accuracy for Mask R-CNN
(Chavan and
Nandedkar, 2018)
Weed identiﬁcation in paddy
ﬁeld
Comapred SegNET with FCN and U-Net
Rice
92.7% accuracy for SegNet
(Chavan and
Nandedkar, 2018)
Weed classiﬁcation at ﬁeld
level
Comapred Hybrid network with VGGNet and
AlexNet
Maize,wheat,
sugarbeet
98.23% accuracy for Hybrid network
(Fawakherji et al.,
2020)
Crop/weed segmentation
using synthetic images
Synthetic image generation using GAN and
segmentation models (UNET, BONNET,SEGNET,
UNET-RESNET)
Sugar beet
All models were performed well with synthetic
images generated using GAN and IoU increased
drastically using synthetic dataset.
(Wang et al., 2020)
Weed detection in sugarbeet
and oilseeds
FCN architecture employed
Sugarbeet
and oilseeds.
Best MIoU value (pixel-wise segmentation) 88.91%
and object-wise segmentation 96.12%
(Espejo-Garcia
et al., 2020)
Detection of balck night
shade and velvet leaf in
tomato and cotton ﬁleds
Compared Modiﬁed Xception, with Inception -
ResNet, VGG-Net, MobileNet and DenseNet
Tomato and
cotton
Combination of ﬁne tuned Densenet and SVM.micro
F1 score of 99.29%.F1 score ≥ 95% over repeated tests.
(Huang et al., 2020)
Weed in rice ﬁeld
FCN
Rice
Highest accuracy- VGG Net based FCN
(Veeranampalayam
Sivakumar et al.,
2020)
Weed in soybean ﬁled
Compared Single-Shot Detector (SSD), Faster
R-CNN
Soybean
Faster RCNN as the best model for weed detection
performance and inference time
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
220
investigations relied on data collection using unmanned aerial vehicles
to accurately detect and count the cattle (Andrew et al., 2019;
Chamoso et al., 2014; Rahnemoonfar et al., 2019; Rivas et al., 2018).
Such detection and counting approach problems, in general, have
adopted either CNN-based probability heat map generation on the loca-
tion of the animals or generation of bounding boxes for detection of the
animals. An improved Yolo model called ‘FLYOLOv3’ (FilterLayer
YOLOv3) based on Filter layer was introduced by Jiang et al. (2019) to
ensure accurate detection of key parts of dairy cows. The performance
of this approach was superior to the Faster-RCNN and Yolov3
algorithms.
Daily activity patterns, food intake, and ruminating are some key in-
dicators closely bound to the health and productivity of dairy cows
(Huzzey et al., 2007; Weary et al., 2009). Some recent studies underline
that traditional methods of direct observation and time-lapse video re-
cording are slowly getting replaced by computer vision approaches.
Yang et al. (2018) used a Faster-RCNN model to identify individual
pigs from a group and subsequently assess the feeding area occupation
rate to identify their feeding behaviour. To improve the accuracy of
feeding behavior analysis, identify and exclude the non-nutritive visits
(NNV) to the feeding area, Alameer et al. (2020) developed a
GoogLeNet-based approach. The detection of feeding behaviour was
highly accurate with 99.4% accuracy. CNN architectures are also found
to be promising for early cattle disease detection in the animal hus-
bandry farm (Rony et al., 2021). Table 6 shows the previous studies on
computer vision technology for livestock management.
3.7. Yield estimation
Early and accurate yield estimation is essential for farmers and other
stakeholders in making strategic decisions on post-harvest planning,
policy-making
and
crop
management
(Al-Gaadi
et
al.,
2016;
Chlingaryan et al., 2018; Wei et al., 2020). Some of the studies underline
that yield estimation using deep learning-based computer vision on ae-
rial images is superior to traditional approaches. In a study conducted by
Yang et al. (2019) rice grain yield from low-altitude remote sensing data
was used to estimate the rice grain yield using convolutional neural net-
works. The models were trained on both RGB and multispectral images
collected by UAV, and results showed that the CNN trained on these im-
ages outperformed the VIs-based traditional regression models for grain
yield estimation at the ripening stage.
You et al. (2017) employed a combination of convolutional neural
networks and recurrent neural networks based on the remotely sensed
images to predict the soybean yield. Another investigation carried out
by Russello (2018) utilized satellite images in combination with
convolutional neural networks for crop yield prediction. In case of or-
chard crops like citrus, computer vision approaches are quite straight-
forward (Fig. 10). The yield can be estimated by directly counting the
number of ﬂowers or fruits prior to the harvesting stages (Cheng et al.,
2017; Dorj et al., 2017; Kanwal et al., 2019). With an objective of esti-
mating yield from citrus orchards, Apolo-Apolo et al. (Apolo-Apolo
et al., 2020)developed a Faster-RCNN model for the fruit detection.
The data collected through UAV was used for the model development.
In their study, based on the count, yield from orchards was modelled
using the Long Short-Term Memory (LSTM) model. An attempt was
made by Zhou et al.(Zhou et al., 2020)to deploy the yield estimation
models in smartphones as android applications. In his investigation,
four different computer vision models; SSD with MobileNetV2, quan-
tized MobileNetV2, InceptionV3, and quantized InceptionV3 were
trained and converted to TensorFlow Lite models. As reported by stud-
ies, fruit occlusion caused by leaves and twigs and varying illumination
conditions are some challenging factors in implementing fruit yield es-
timation systems based on computer vision (Maheswari et al., 2021).
Table 7 shows the previous studies on computer vision technology for
yield estimation.
4. Practical implications
Despite being late for digitization, the agriculture sector has ﬁnally
seen good momentum for the practical implementation of several arti-
ﬁcial intelligence applications, including deep learning-based computer
vision approaches. Computer vision-powered disease identiﬁcation ap-
plications merge the expertise of genetic resources and artiﬁcial intelli-
gence, allowing farmers and extension workers to act quickly and
rescue the crop. This disease detecting computer vision-enabled soft-
ware is also being installed inside greenhouses, drones, and other
equipment to identify the issues and provide a faster response in taking
preventive measures. Advancement in computer vision technology has
been used by agricultural startups for building solutions that assist
farmers in harvesting, plant health monitoring, pest-weed control, etc.
For pesticide application, blue river technology (Blue river technology)
developed a see and spray’ technology that works based on camera
Fig. 9. Major applications of DL-Computer vision for livestock management
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
221
inputs and computer vision algorithms. The algorithm can distinguish
weeds from plants and perform targeted pesticide applications. The
startup, cromai (Cromai) developed AI-driven land and crop diagnostic
information. They provide a technological solution for georeferenced
identiﬁcation of weeds in the sugarcane ﬁeld using advanced artiﬁcial
intelligence approaches. Harvesting robots are widely used in open
ﬁeld conditions, integrating with machine visions and achieving im-
proved precision. Harvest CROO robotics (Harvest croo robotics) devel-
oped a fully autonomous harvester, employing a harvester-mounted
LIDAR system to avoid collisions and accurate navigation. The computer
vision system scans each berry on the plant and determines the ripeness
and health before harvesting. ‘Plantix’, the crop damage diagnosis mo-
bile application (Plantix) developed by German startup PEAT (Progres-
sive Environmental and Agricultural Technologies), uses deep learning
and computer vision to help farmers to combat pests and diseases
(Goncharov et al., 2018; Tibbetts, 2018). The application’s functionality
enables the end-user to upload crop images and get guidance on the dis-
ease affected, symptom descriptions, treatment information, preventive
measures, etc. With the same objective of identifying a large number of
plant diseases, other applications such as Agrio (Agrio) were also
introduced to the farming community. Several technology-driven solu-
tions were introduced into precision livestock farming to ensure opti-
mal health and output of animals also. The technology startup
Cainthus (Cainthus) offers a computer vision-driven AI system for
dairy farmers to monitor their cows and send timely alerts and reports
via associated applications. Smart cameras are deployed to watch over
the activities of the cows to provide the right amount of feed available
on a timely basis. Similar to this, Piguard (Piguard), an innovative live-
stock management software, leverages deep learning-based computer
vision approach to monitor the health status and behavioral patterns
of animals.
Computer vision technology covers a broad spectrum of solutions for
farmers, from small AI-enabled mobile apps for decision support, over
in-ﬁeld imaging sensors and remote sensing technologies for data col-
lection, and to drones and robots for the automation of processes. Across
the globe, farming community has realized the potential of digital tech-
nologies and for the past few years, there has been an increase in its
adoption. Some of the key factors inﬂuencing the transformation
of farms into digital farms include farm characteristics, operator
characteristics, interactions, institutions, attributes to technology, and
Table 6
Previous studies on computer vision and deep learning technologies for livestock management.
References
Objectives and scenario of
application
Methodology
Livestock
Results
(Qiao et al.,
2019b)
Cattle Segmentation and Contour
extraction
Mask R-CNN based cattle instance
segmentation and contour line extraction
Cattle
Cattle segmentation performance with 0.92 Mean Pixel
Accuracy (MPA)
(Achour et al.,
2020)
Identiﬁcation and feeding
behavior monitoring
CNN coupled to Support Vector Machine
(SVM)
Cow
Accuracy 97% for individual identiﬁcation of cows using
multi-CNN.
(Xu et al., 2020)
Livestock classiﬁcation and
counting
Mask RCNN based segmentation on UAV
captured images
Cattle
and
Sheep
Classiﬁcation Accuracy: 96% and Counting accuracy: 92%
(Jung et al., 2021)
Cattle Vocal Classiﬁcation and
Livestock Monitoring
Convolutional neural network (CNN)
based cattle vocal classiﬁcation
Cattle
Accuracy of 81.96% after the sound ﬁltering.
(Qiao et al., 2022)
Behaviour classiﬁcation
C3D-ConvLSTM based cow behaviour
classiﬁcation using video data
Cow
Classiﬁcation accuracy of 90.32% and 86.67% in calf and cow
datasets of 30-frame video length
(Abu Jwade et al.,
2019)
Breed Classiﬁcation
VGG16 model for breed classiﬁcation
Sheep
Maximum classiﬁcation accuracy of 95.8% with 1.7 standard
deviation.
(Shojaeipour
et al., 2021)
Automated Muzzle Detection and
Biometric Identiﬁcation
Two-stage YOLOv3-ResNet50 algorithm
Cattle
Muzzle detection accuracy was 99.13% and biometric
identiﬁcation of 99.11% testing accuracy
(Brand et al.,
2021)
Pregnancy status prediction from
mid-infrared spectroscopy
Genetic algorithm and DenseNet model
Cow
DenseNet was superior over GA with prediction sensitivity 0.89,
speciﬁcity of 0.86, and prediction accuracy of 0.88%.
(Ayadi et al.,
2020)
Rumination behavior
identiﬁcation
Convolutional Neural Networks
Cow
Average accuracy, recall and precision were 95%, 98% and 98%
respectively
(Riekert et al.,
2020)
Position and posture detection
Faster R-CNN object detection
Pig
Pig position detection: Average Precision (AP) 87.4% Pig
position, and pig position and posture: mAPof 80.2%.
Fig. 10. Orchard yield estimation using computer vision.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
222
psychological factors (Shang et al., 2021). Larger farms are more likely
to adopt these technologies by taking advantage of economies of scale,
and they can afford the higher initial investment cost. Use of comple-
mentary technologies can also lead to better adoption of technologies.
For example, the variable rate technology and yield mapping are inter-
related, and farmers who are using variable rate technologies are more
likely to adopt yield mapping technologies. Operator characteristics
such as end user’s education level, age, on-farm digital device such as
computer usage are also signiﬁcant (Isgin et al., 2008). Operators having
higher education levels and innovativeness could adopt the new tech-
nologies faster (Aubert et al., 2012). Isgin et al. (2008) found signiﬁcant
evidence relating to the impact of urban inﬂuences on adoption of pre-
cision farming technologies in their empirical analysis. Mohr and Kühl
(2021) investigated the behavioral factors inﬂuencing the acceptance
of artiﬁcial intelligence technologies using a theoretical framework.
The results showed that behavioral control and personal attitude of
the farmers are the two most inﬂuential factors in the acceptance of
artiﬁcial intelligence in agriculture.
5. Challenges and way forward
Deep learning for computer vision, the spearhead of artiﬁcial intelli-
gence, is perhaps one of the most promising technologies for meeting
the ever-growing food demand. Several intractable problems in agricul-
ture are being solved with the support of DL-computer vision. However,
high innovation capability always comes along with some challenges.
One major challenge in computer vision using deep learning includes
the requirement of massive processing power, and most deep learning
applications are data-intensive. A possible solution to this is the adop-
tion of cloud-based solutions that offer auto-scaling, load balancing, eas-
ier maintenance, and high availability features. However, cloud
solutions limit real-time processing due to the latency in access and re-
trieval of the data from the cloud. The increased cost of immense data
processing and privacy issues are also other concerns. Advanced edge
devices with accelerators are capable of analyzing real-time video in-
puts and providing inferences in near real-time. Deployment of the
computer vision solutions in edge devices can reduce the latency limita-
tions. Sophisticated computer vision models in a variety of agricultural
use cases often do not perform as expected in the production environ-
ment. To ensure that a promising model is not becoming a costly liabil-
ity, several aspects like data quality check, code inspection, hyper-
parameter tuning, code versioning, setting up the right deployment en-
vironment, rigorous training and re-training, etc, need to be closely
evaluated.
Quality of data is another major concern for developing efﬁcient
data-driven solutions (Cai and Zhu, 2015; Carletto, 2021). Programmat-
ically generating synthetic data is one of the approaches for enhancing
the data quality in deep learning-based computer vision solutions
(Fig. 11). Generative adversarial networks and their variations like
CGAN can generate synthetic data for agricultural applications quite ef-
fectively (Cui et al., 2021; Olatunji et al., 2020; Zhu et al., 2020a). The
performance of a DL-CV model relies heavily on the right hyper-
parameter conﬁgurations. There are no simple ways to set hyper-
parameters such as learning rate, batch size, momentum, weight
decay, etc, and it demands expertise and extensive trial and error to
achieve the best performance. The process of conﬁguring the hyper-
parameter in a high-dimensional space is not a trivial challenge. Com-
puter vision problems, more speciﬁcally object detection approaches
face practical implementation challenges such as viewpoint variation,
deformation, occlusion, varying illumination conditions, complex back-
grounds, and speed. Viewpoint variation is very common in object de-
tection, and segmentation problems, as the object may look at
different viewing angles. For e.g., a crop may look different when cap-
tured from different angles. The additional complication appears due
to the occlusion.
In fruit yield estimation systems, this is a major concern and causes
sharp declination in the overall accuracy of the system. Varying illumi-
nation conditions and extraction of data from complex overlapped
and textured backgrounds also make the computer vision task challeng-
ing. In real-time video applications, performance in terms of detection
speed and accuracy are crucial for detecting objects in motion. Research
in computer vision is growing at a faster pace in the agriculture domain.
Building a robust computer vision system requires quality data genera-
tion, transfer, and processing. The system should have adequate security
to block attacks. Heterogeneity of resources involved in CV solutions in-
troduces a lot of security concerns, such as data integrity, privacy issues,
Table 7
Previous studies on computer vision and deep learning technologies for yield estimation.
References
Objectives and scenario of
application
Methodology
Crop
Results
(Khaki et al.,
2020)
Image-based corn kernel counting
and yield estimation
Truncated VGGNet backbone and semi
supervised deep learning.
Corn
MAE and RMSE of 41.36 and 60.27 respectively.
(Apolo-Apolo
et al., 2020)
Yield map Generation
Region-CNN (RCNN) Model using UAV
imagery
Apple
R-squared value: 0.86, MAE: 10.35 and RMSE: 13.56
(Palacios et al.,
2020)
Detection of ﬂower at bloom for
yield estimation
CNN SegNet architecture with a VGG19
network encoder
Grape
A determination coefﬁcient (R2) of 0.91 between the
actual and detected ﬂowers.
(Faisal et al.,
2020)
Intelligent harvesting decision
system based on date fruit maturity
level.
VGG-19, Inception-v3, and NASNet Models
Date
Performance metrics of IHDS were 99.4%, 99.4%,
99.7%, and 99.7% for accuracy, F1 score, sensitivity
(recall), and precision, respectively.
(Yang et al.,
2019)
Rice grain yield forecasting using
UAV images
CNN models with RGB and multispectral
datasets
Rice
Prediction accuracy: MAPE: 20.4%, RMSE: 0.658 and
R-squared: 0.585
(Tedesco-Oliveira
et al., 2020)
Yield estimation using object
detection models
Faster RCNN, SSD and SSD Lite Models
Cotton
Mean percentage error of 8.84%
(Chen et al.,
2019)
Yield prediction by counting
number of ﬂowers and maturity
analysis, using aerial ortho images
Faster RCNN model
Strawberry
The average deep learning counting accuracy was
84.1% with average occlusion of 13.5%.
(Bargoti and
Underwood,
2017)
Yield Estimation using fruit
detection and counting
CNN and Watershed algorithm
Apple
The count estimates using CNN and WS with
R-squared value of 0.826
(Zhou et al.,
2020)
Real-time fruit detection and yield
estimation through smartphones.
Single shot Multibox Detector with
MobileNetV2, quantized MobileNetV2,
InceptionV3, and quantized InceptionV3
Models
Kiwi
MobileNetV2, quantized MobileNetV2, InceptionV3,
and quantized InceptionV3 obtained TDR of 90.8%,
89.7%, 87.6%, and 72.8%, respectively.
(Rahnemoonfar
and Sheppard,
2017)
Fruit counting based on deep
simulated learning
Modiﬁed version of the Inception-ResNet
Model
Tomato
91% average test accuracy on real images and 93% on
synthetic images
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
223
reliability, etc. As these solutions integrate several digital technologies
starting from the internet, IoT, cloud computing or edge computing,
and wireless sensor networks, the system should accommodate security
features for all these technologies and ensure data and device integrity,
data accuracy, and availability. From land preparation to harvesting, dif-
ferent stakeholders are leveraging new ways to improve the ability to
derive insights from images, object detection and tracking, etc. Deep
learning - computer vision models will undoubtedly continue to expand
and become more innovative and intelligent, handling more complex
computations in agriculture with utmost precision. Above all, for
obtaining efﬁcient and desirable outputs, strong business cases with
the capability to scale on a larger scale is necessary.
6. Conclusions
The surge of deep learning coupled with computer vision over the
past few years has brought automation capabilities to traditional agri-
culture practices. In this paper, we have extensively discussed the role
of deep learning-based computer vision in different agriculture applica-
tions. More speciﬁcally, the paper emphasizes seven different applica-
tion areas such as seed quality analysis, soil analysis, irrigation
management, plant health analysis, weed management, livestock man-
agement, and yield estimation. Review of the application of deep learn-
ing particularly, the assessment and planning of water resources
revealed that the water sector would continue to embrace deep learning
at an accelerated rate, and it will play a signiﬁcant role in the future of
water-related research and the wide range of application areas. Tech-
nologies powered by deep learning have created a myriad of application
and research opportunities that have the potential to change hydrolog-
ical science and workﬂow. Recent advances in deep learning-assisted
image analysis involving algorithms for image classiﬁcation, object de-
tection, segmentation, etc., have expanded their applications across dif-
ferent pre-and post-harvesting activities in agriculture.
The following conclusions can be drawn from the study.
• Deep learning-based computer vision has tremendous automation ca-
pabilities across different applications such as automated plant health
monitoring, weed detection, irrigation management, livestock man-
agement, yield estimation, etc.
• Integration of the deep learning computer vision approaches with the
UAV, and spectral data can help in building advanced-intelligent solu-
tions.
• Despite the beneﬁts computer vision and deep learning brought to ag-
riculture, signiﬁcant challenges do remain, especially the data quality
issues, the computation power requirement, etc.
• The extensive automation across various agriculture activities will
continue to attract the interest of the deep learning research commu-
nity in the years to come.
The adoption rate of advanced technologies in agriculture is rela-
tively slow, owing to the high initial investment required, lack of techni-
cal expertise, and growing concerns about data privacy. However at
present, the rate of the adoption of these digital solutions has seen a ris-
ing curve, thus suggesting that these would not be concerns in moving
forward.
Declaration of Competing Interest
The authors declare that they have no known competing ﬁnancial
interests or personal relationships that could have appeared to inﬂu-
ence the work reported in this paper.
Fig. 11. Challenges in implementation of deep learning based computer vision.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
224
References
Abbas, A., Jain, S., Gour, M., Vankudothu, S., 2021. Tomato plant disease detection using
transfer learning with C-GAN synthetic images. Comput. Electron. Agric. 187,
106279. https://doi.org/10.1016/j.compag.2021.106279.
Abdalla, A., Cen, H., Wan, L., Mehmood, K., He, Y., 2021. Nutrient Status Diagnosis of Inﬁeld
Oilseed Rape via Deep Learning-Enabled Dynamic Model. IEEE Trans. Ind. Inform. 17,
4379–4389. https://doi.org/10.1109/TII.2020.3009736.
Abu Jwade, S., Guzzomi, A., Mian, A., 2019. On farm automatic sheep breed classiﬁcation
using deep learning. Comput. Electron. Agric. 167, 105055. https://doi.org/10.1016/j.
compag.2019.105055.
Achour, B., Belkadi, M., Filali, I., Laghrouche, M., Lahdir, M., 2020. Image analysis for indi-
vidual identiﬁcation and feeding behaviour monitoring of dairy cows based on
Convolutional Neural Networks (CNN). Biosyst. Eng. 198, 31–49. https://doi.org/10.
1016/j.biosystemseng.2020.07.019.
Adnan, R.M., Mostafa, R.R., Islam, A.R.Md.T., Kisi, O., Kuriqi, A., Heddam, S., 2021. Estimat-
ing reference evapotranspiration using hybrid adaptive fuzzy inferencing coupled
with heuristic algorithms. Comput. Electron. Agric. 191, 106541. https://doi.org/10.
1016/j.compag.2021.106541.
Agrio, 2022o. Agrio. https://agrio.app/. (Accessed 6 July 2022).
Aker, J.C., 2011. Dial “A” for agriculture: a review of information and communication tech-
nologies for agricultural extension in developing countries. Agric. Econ. 42, 631–647.
https://doi.org/10.1111/j.1574-0862.2011.00545.x.
Alameer, A., Kyriazakis, I., Dalton, H.A., Miller, A.L., Bacardit, J., 2020. Automatic recogni-
tion of feeding and foraging behaviour in pigs using deep learning. Biosyst. Eng.
197, 91–104. https://doi.org/10.1016/j.biosystemseng.2020.06.013.
Albuquerque, C.K.G., Polimante, S., Torre-Neto, A., Prati, R.C., 2020. Water spray detection
for smart irrigation systems with Mask R-CNN and UAV footage. 2020 IEEE Interna-
tional Workshop on Metrology for Agriculture and Forestry (MetroAgriFor),
pp. 236–240 https://doi.org/10.1109/MetroAgriFor50201.2020.9277542.
Al-Gaadi, K.A., Hassaballa, A.A., Tola, E., Kayad, A.G., Madugundu, R., Alblewi, B., Assiri, F.,
2016. Prediction of Potato Crop Yield Using Precision Agriculture Techniques. PLOS
ONE 11, e0162219. https://doi.org/10.1371/journal.pone.0162219.
Amara, J., Bouaziz, B., Algergawy, A., 2017. A deep learning-based approach for banana
leaf diseases classiﬁcation. Datenbanksysteme Für Bus. Technol. (Web BTW 2017-
Work).
Anami, B.S., Malvade, N.N., Palaiah, S., 2020. Deep learning approach for recognition and
classiﬁcation of yield affecting paddy crop stresses using ﬁeld images. Artif. Intell.
Agric. 4, 12–20. https://doi.org/10.1016/j.aiia.2020.03.001.
Andrew, W., Greatwood, C., Burghardt, T., 2019. Aerial Animal Biometrics: Individual Frie-
sian Cattle Recovery and Visual Identiﬁcation via an Autonomous UAV with Onboard
Deep Inference. https://doi.org/10.48550/arXiv.1907.05310.
Apolo-Apolo, O.E., Martínez-Guanter, J., Egea, G., Raja, P., Pérez-Ruiz, M., 2020. Deep learn-
ing techniques for estimation of the yield and size of citrus fruits using a UAV. Eur.
J. Agron. 115, 126030. https://doi.org/10.1016/j.eja.2020.126030.
Araus, J.L., Cairns, J.E., 2014. Field high-throughput phenotyping: the new crop breeding
frontier. Trends Plant Sci. 19, 52–61. https://doi.org/10.1016/j.tplants.2013.09.008.
Araus, J.L., Kefauver, S.C., Zaman-Allah, M., Olsen, M.S., Cairns, J.E., 2018. Translating High-
Throughput Phenotyping into Genetic Gain. Trends Plant Sci. 23, 451–466. https://
doi.org/10.1016/j.tplants.2018.02.001.
Aubert, B.A., Schroeder, A., Grimaudo, J., 2012. IT as enabler of sustainable farming: An
empirical analysis of farmers’ adoption decision of precision agriculture technology.
Decis. Support Syst. 54, 510–520. https://doi.org/10.1016/j.dss.2012.07.002.
Ayadi, S., Ben Said, A., Jabbar, R., Aloulou, C., Chabbouh, A., Achballah, A.B., 2020. Dairy
Cow Rumination Detection: A Deep Learning Approach. In: Jemili, I., Mosbah, M.
(Eds.), Distributed Computing for Emerging Smart Networks, Communications in
Computer and Information Science. Springer International Publishing, Cham,
pp. 123–139 https://doi.org/10.1007/978-3-030-65810-6_7.
Azadnia, R., Jahanbakhshi, A., Rashidi, S., Khajehzadeh, M., Bazyar, P., 2022. Developing an
automated monitoring system for fast and accurate prediction of soil texture using an
image-based deep learning network and machine vision system. Measurement 190,
110669. https://doi.org/10.1016/j.measurement.2021.110669.
Azimi, S., Kaur, T., Gandhi, T.K., 2021. A deep learning approach to measure stress level in
plants due to Nitrogen deﬁciency. Measurement 173, 108650. https://doi.org/10.
1016/j.measurement.2020.108650.
Badrzadeh, N., Samani, J.M.V., Mazaheri, M., Kuriqi, A., 2022. Evaluation of management
practices on agricultural nonpoint source pollution discharges into the rivers under
climate change effects. Sci. Total Environ. 838, 156643. https://doi.org/10.1016/j.
scitotenv.2022.156643.
Bao, F., Bambil, D., 2021. Applicability of computer vision in seed identiﬁcation: deep
learning, random forest, and support vector machine classiﬁcation algorithms. Acta
Bot. Bras. 35, 17–21. https://doi.org/10.1590/0102-33062020abb0361.
Bargoti, S., Underwood, J.P., 2017. Image Segmentation for Fruit Detection and Yield Esti-
mation in Apple Orchards. J. Field Robot. 34, 1039–1060. https://doi.org/10.1002/rob.
21699.
Bazi, Y., Bashmal, L., Rahhal, M.M.A., Dayil, R.A., Ajlan, N.A., 2021. Vision Transformers for
Remote Sensing Image Classiﬁcation. Remote Sens. 13, 516. https://doi.org/10.3390/
rs13030516.
Bello, R.-W., Mohamed, A.S.A., Talib, A.Z., 2021. Contour Extraction of Individual Cattle
From an Image Using Enhanced Mask R-CNN Instance Segmentation Method. IEEE
Access 9, 56984–57000. https://doi.org/10.1109/ACCESS.2021.3072636.
Bhagat, M., Kumar, D., Haque, I., Munda, H.S., Bhagat, R., 2020. Plant Leaf Disease Classiﬁ-
cation Using Grid Search Based SVM. 2nd International Conference on Data, Engi-
neering and Applications (IDEA). Presented at the 2nd International Conference on
Data, Engineering and Applications (IDEA), pp. 1–6 https://doi.org/10.1109/
IDEA49133.2020.9170725.
Bhavsar, H., Panchal, M.H., 2012. A review on support vector machine for data classiﬁca-
tion. Int. J. Adv. Res. Comput. Eng. Technol. IJARCET 1, 185–189.
Blue river technology, 2022y. https://bluerivertechnology.com/. (Accessed 5 July 2022).
Brahimi, M., Boukhalfa, K., Moussaoui, A., 2017. Deep Learning for Tomato Diseases: Clas-
siﬁcation and Symptoms Visualization. Appl. Artif. Intell. 31, 299–315. https://doi.org/
10.1080/08839514.2017.1315516.
Brand, W., Wells, A.T., Smith, S.L., Denholm, S.J., Wall, E., Coffey, M.P., 2021. Predicting
pregnancy status from mid-infrared spectroscopy in dairy cow milk using deep learn-
ing. J. Dairy Sci. 104, 4980–4990. https://doi.org/10.3168/jds.2020-18367.
Burra, D.D., Hildebrand, J., Giles, J., Nguyen, T., Hasiner, E., Schroeder, K., Treguer, D.,
Juergenliemk, A., Horst, A., Jarvis, A., Kropff, W., 2021. Digital Agriculture Proﬁle:
Viet Nam (Report). Food and Agriculture Organization of the United Nations.
Cai, L., Zhu, Y., 2015. The Challenges of Data Quality and Data Quality Assessment in the
Big Data Era. Data Sci. J. 14, 2. https://doi.org/10.5334/dsj-2015-002.
Cainthus, 2022s. Cainthus. https://www.cainthus.com. (Accessed 6 July 2022).
Carletto, C., 2021. Better data, higher impact: improving agricultural data systems for so-
cietal change. Eur. Rev. Agric. Econ. 48, 719–740. https://doi.org/10.1093/erae/
jbab030.
Cen, H., Lu, R., Zhu, Q., Mendoza, F., 2016. Nondestructive detection of chilling injury in cu-
cumber fruit using hyperspectral imaging with feature selection and supervised clas-
siﬁcation. Postharvest Biol. Technol. 111, 352–361. https://doi.org/10.1016/j.
postharvbio.2015.09.027.
Chamoso, P., Raveane, W., Parra, V., González, A., 2014. UAVs applied to the counting and
monitoring of animals. Ambient Intelligence-Software and Applications. Springer
71–80.
Chang, C.-L., Lin, K.-M., 2018. Smart Agricultural Machine with a Computer Vision-Based
Weeding and Variable-Rate Irrigation Scheme. Robotics 7, 38. https://doi.org/10.
3390/robotics7030038.
Chavan, T.R., Nandedkar, A.V., 2018. AgroAVNET for crops and weeds classiﬁcation: A step
forward in automatic farming. Comput. Electron. Agric. 154, 361–372. https://doi.org/
10.1016/j.compag.2018.09.021.
Chen, Y., Lee, W.S., Gan, H., Peres, N., Fraisse, C., Zhang, Y., He, Y., 2019. Strawberry Yield
Prediction Based on a Deep Neural Network Using High-Resolution Aerial Orthoim-
ages. Remote Sens. 11, 1584. https://doi.org/10.3390/rs11131584.
Chen, H., Chen, A., Xu, L., Xie, H., Qiao, H., Lin, Q., Cai, K., 2020a. A deep learning CNN ar-
chitecture applied in smart near-infrared analysis of water pollution for agricultural
irrigation resources. Agric. Water Manag. 240, 106303. https://doi.org/10.1016/j.
agwat.2020.106303.
Chen, J., Chen, Jinxiu, Zhang, D., Sun, Y., Nanehkaran, Y.A., 2020b. Using deep transfer
learning for image-based plant disease identiﬁcation. Comput. Electron. Agric. 173,
105393. https://doi.org/10.1016/j.compag.2020.105393.
Cheng, H., Damerow, L., Sun, Y., Blanke, M., 2017. Early Yield Prediction Using Image Anal-
ysis of Apple Fruit and Tree Canopy Features with Neural Networks. J. Imaging 3, 6.
https://doi.org/10.3390/jimaging3010006.
Chlingaryan, A., Sukkarieh, S., Whelan, B., 2018. Machine learning approaches for crop
yield prediction and nitrogen status estimation in precision agriculture: A review.
Comput. Electron. Agric. 151, 61–69. https://doi.org/10.1016/j.compag.2018.05.012.
Coulibaly, S., Kamsu-Foguem, B., Kamissoko, D., Traore, D., 2019. Deep neural networks
with transfer learning in millet crop images. Comput. Ind. 108, 115–120. https://
doi.org/10.1016/j.compind.2019.02.003.
Cromai, 2022i. Cromai. https://www.cromai.com/. (Accessed 6 July 2022).
Cui, X., Ying, Y., Chen, Z., 2021. CycleGAN based confusion model for cross-species plant
disease image migration. J. Intell. Fuzzy Syst. 41, 6685–6696. https://doi.org/10.
3233/JIFS-210585.
Dell’Acqua, F., Gamba, P., 2003. Texture-based characterization of urban environments on
satellite SAR images. IEEE Trans. Geosci. Remote Sens. 41, 153–159. https://doi.org/
10.1109/TGRS.2002.807754.
Dolata, P., Reiner, J., 2018. Barley Variety Recognition with Viewpoint-Aware Double-
Stream Convolutional Neural Networks. 2018 Federated Conference on Computer
Science and Information Systems (FedCSIS)., pp. 101–105
Dorj, U.-O., Lee, M., Yun, S., 2017. An yield estimation in citrus orchards via fruit detection
and counting using image processing. Comput. Electron. Agric. 140, 103–112. https://
doi.org/10.1016/j.compag.2017.05.019.
Dosovitskiy, A., Beyer, L., Kolesnikov, A., Weissenborn, D., Zhai, X., Unterthiner, T.,
Dehghani, M., Minderer, M., Heigold, G., Gelly, S., Uszkoreit, J., Houlsby, N., 2021. An
Image is Worth 16x16 Words: Transformers for Image Recognition at Scale.
https://doi.org/10.48550/arXiv.2010.11929.
Elbeltagi, A., Kumar, M., Kushwaha, N.L., Pande, C.B., Ditthakit, P., Vishwakarma, D.K.,
Subeesh, A., 2022a. Drought indicator analysis and forecasting using data driven
models: case study in Jaisalmer, India. Stoch. Environ. Res. Risk Assess https://doi.
org/10.1007/s00477-022-02277-0.
Elbeltagi, A., Kushwaha, N.L., Rajput, J., Vishwakarma, D.K., Kulimushi, L.C., Kumar, M.,
Zhang, J., Pande, C.B., Choudhari, P., Meshram, S.G., Pandey, K., Sihag, P., Kumar, N.,
Abd-Elaty, I., 2022b. Modelling daily reference evapotranspiration based on stacking
hybridization of ANN with meta-heuristic algorithms under diverse agro-climatic
conditions. Stoch. Environ. Res. Risk Assess. https://doi.org/10.1007/s00477-022-
02196-0.
Espejo-Garcia, B., Mylonas, N., Athanasakos, L., Fountas, S., Vasilakoglou, I., 2020. Towards
weeds identiﬁcation assistance through transfer learning. Comput. Electron. Agric.
171, 105306. https://doi.org/10.1016/j.compag.2020.105306.
Faisal, M., Alsulaiman, M., Arafah, M., Mekhtiche, M.A., 2020. IHDS: Intelligent Harvesting
Decision System for Date Fruit Based on Maturity Stage Using Deep Learning and
Computer Vision. IEEE Access 8, 167985–167997. https://doi.org/10.1109/ACCESS.
2020.3023894.
Fawakherji, M., Potena, C., Prevedello, I., Pretto, A., Bloisi, D.D., Nardi, D., 2020. Data Aug-
mentation Using GANs for Crop/Weed Segmentation in Precision Farming. 2020 IEEE
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
225
Conference on Control Technology and Applications (CCTA). Presented at the 2020
IEEE Conference on Control Technology and Applications (CCTA), pp. 279–284
https://doi.org/10.1109/CCTA41146.2020.9206297.
Ferentinos, K.P., 2018. Deep learning models for plant disease detection and diagnosis.
Comput. Electron. Agric. 145, 311–318. https://doi.org/10.1016/j.compag.2018.01.
009.
Foglia, M.M., Reina, G., 2006. Agricultural robot for radicchio harvesting. J. Field Robot. 23,
363–377. https://doi.org/10.1002/rob.20131.
Ghanem, M.E., Marrou, H., Sinclair, T.R., 2015. Physiological phenotyping of plants for crop
improvement. Trends Plant Sci. 20, 139–144. https://doi.org/10.1016/j.tplants.2014.
11.006.
Gomes, J.F.S., Leta, F.R., 2012. Applications of computer vision techniques in the agricul-
ture and food industry: a review. Eur. Food Res. Technol. 235, 989–1000. https://
doi.org/10.1007/s00217-012-1844-2.
Goncharov, P, Ososkov, G, Nechaevskiy, A, Uzhinskiy, A, Nestsiarenia, I, 2018. Disease De-
tection on the Plant Leaves by Deep Learning. In: Kryzhanovsky, Boris, Dunin-
Barkowski, Witali, Redko, Vladimir, Tiumentsev, Yury (Eds.), Advances in Neural
Computation, Machine Learning, and Cognitive Research II. Springer, pp. 151–159.
Granitto, P.M., Verdes, P.F., Ceccatto, H.A., 2005. Large-scale investigation of weed seed
identiﬁcation by machine vision. Comput. Electron. Agric. 47, 15–24. https://doi.
org/10.1016/j.compag.2004.10.003.
Gulzar, Y., Hamid, Y., Soomro, A.B., Alwan, A.A., Journaux, L., 2020. A Convolution Neural
Network-Based Seed Classiﬁcation System. Symmetry 12, 2018. https://doi.org/10.
3390/sym12122018.
Hall, D., McCool, C., Dayoub, F., Sunderhauf, N., Upcroft, B., 2015. Evaluation of Features for
Leaf Classiﬁcation in Challenging Conditions. 2015 IEEE Winter Conference on Appli-
cations of Computer Vision. Presented at the 2015 IEEE Winter Conference on Appli-
cations of Computer Vision, pp. 797–804 https://doi.org/10.1109/WACV.2015.111.
Hansen, M.F., Smith, M.L., Smith, L.N., Salter, M.G., Baxter, E.M., Farish, M., Grieve, B., 2018.
Towards on-farm pig face recognition using convolutional neural networks. Comput.
Ind. 98, 145–152. https://doi.org/10.1016/j.compind.2018.02.016.
Haralick, R.M., Shanmugam, K., Dinstein, I., 1973. Textural Features for Image Classiﬁca-
tion. IEEE Trans. Syst. Man Cybern. SMC-3, 610–621. https://doi.org/10.1109/TSMC.
1973.4309314.
Harvest croo robotics, 2022s. Harvest croo robotics. https://www.harvestcroorobotics.
com/technology. (Accessed 5 July 2022).
Hassan, S.M., Maji, A.K., 2022. Plant Disease Identiﬁcation Using a Novel Convolutional
Neural Network. IEEE Access 10, 5390–5401. https://doi.org/10.1109/ACCESS.2022.
3141371.
Hati, A.J., Singh, R.R., 2021. Artiﬁcial Intelligence in Smart Farms: Plant Phenotyping for
Species Recognition and Health Condition Identiﬁcation Using Deep Learning. AI 2,
274–289. https://doi.org/10.3390/ai2020017.
He, K., Zhang, X., Ren, S., Sun, J., 2015. Deep residual learning for image recognition.
ArXiv151203385 Cs.
Heramb, P., Kumar Singh, P., Ramana Rao, K.V., Subeesh, A., 2022. Modelling reference
evapotranspiration using gene expression programming and artiﬁcial neural network
at Pantnagar. Inf. Process. Agric, India https://doi.org/10.1016/j.inpa.2022.05.007.
Hiriyannaiah, S., Srinivas, A.M.D., Shetty, G.K., Srinivasa, K.G., 2020. Chapter 4 - A compu-
tationally intelligent agent for detecting fake news using generative adversarial net-
works. In: Bhattacharyya, S., Snášel, V., Gupta, D., Khanna, A. (Eds.), Hybrid
Computational Intelligence, Hybrid Computational Intelligence for Pattern Analysis.
and Understanding. Academic Press, pp. 69–96 https://doi.org/10.1016/B978-0-12-
818699-2.00004-4.
Huang, T., 1993. Computer vision. Evolution and promise.
Huang, H., Lan, Y., Yang, A., Zhang, Y., Wen, S., Deng, J., 2020. Deep learning versus Object-
based Image Analysis (OBIA) in weed mapping of UAV imagery. Int. J. Remote Sens.
41, 3446–3479. https://doi.org/10.1080/01431161.2019.1706112.
Hughes, D.P., Salathe, M., 2016. An open access repository of images on plant health to en-
able the development of mobile disease diagnostics. ArXiv151108060 Cs.
Huzzey, J.M., Veira, D.M., Weary, D.M., von Keyserlingk, M., 2007. Prepartum behavior and
dry matter intake identify dairy cows at risk for metritis. J. Dairy Sci. 90, 3220–3233.
https://doi.org/10.3168/jds.2006-807.
Isgin, T., Bilgic, A., Forster, D.L., Batte, M.T., 2008. Using count data models to determine
the factors affecting farmers’ quantity decisions of precision farming technology
adoption. Comput. Electron. Agric. 62, 231–242. https://doi.org/10.1016/j.compag.
2008.01.004.
Jahagirdar, P., Budihal, S.V., 2021. Framework to Detect NPK Deﬁciency in Maize Plants
Using CNN. In: Panigrahi, C.R., Pati, B., Mohapatra, P., Buyya, R., Li, K.-C. (Eds.), Prog-
ress in Advanced Computing and Intelligent Engineering, Advances in Intelligent Sys-
tems and Computing. Springer, Singapore, pp. 366–376 https://doi.org/10.1007/978-
981-15-6353-9_33.
Jamei, M., Karbasi, M., Malik, A., Abualigah, L., Islam, A.R.M.T., Yaseen, Z.M., 2022a. Compu-
tational assessment of groundwater salinity distribution within coastal multi-
aquifers of Bangladesh. Sci. Rep. 12, 11165. https://doi.org/10.1038/s41598-022-
15104-x.
Jamei, M., Maroufpoor, S., Aminpour, Y., Karbasi, M., Malik, A., Karimi, B., 2022b. Develop-
ing hybrid data-intelligent method using Boruta-random forest optimizer for simula-
tion of nitrate distribution pattern. Agric. Water Manag. 270, 107715. https://doi.org/
10.1016/j.agwat.2022.107715.
Jamei, Mehdi, Karbasi, M., Malik, A., Jamei, Mozhdeh, Kisi, O., Yaseen, Z.M., 2022c. Long-
term multi-step ahead forecasting of root zone soil moisture in different climates:
Novel ensemble-based complementary data-intelligent paradigms. Agric. Water
Manag. 269, 107679. https://doi.org/10.1016/j.agwat.2022.107679.
Javanmardi, S., Miraei Ashtiani, S.-H., Verbeek, F.J., Martynenko, A., 2021. Computer-vision
classiﬁcation of corn seed varieties using deep convolutional neural network. J. Stored
Prod. Res. 92, 101800. https://doi.org/10.1016/j.jspr.2021.101800.
Jha, K., Doshi, A., Patel, P., Shah, M., 2019. A comprehensive review on automation in ag-
riculture using artiﬁcial intelligence. Artif. Intell. Agric. 2, 1–12. https://doi.org/10.
1016/j.aiia.2019.05.004.
Ji, M., Wu, Z., 2022. Automatic detection and severity analysis of grape black measles dis-
ease based on deep learning and fuzzy logic. Comput. Electron. Agric. 193, 106718.
https://doi.org/10.1016/j.compag.2022.106718.
Jiang, B., Wu, Q., Yin, X., Wu, D., Song, H., He, D., 2019. FLYOLOv3 deep learning for key
parts of dairy cow body detection. Comput. Electron. Agric. 166, 104982. https://
doi.org/10.1016/j.compag.2019.104982.
Joshi, R.C., Kaushik, M., Dutta, M.K., Srivastava, A., Choudhary, N., 2021. VirLeafNet: Auto-
matic analysis and viral disease diagnosis using deep-learning in Vigna mungo plant.
Ecol. Inform. 61, 101197. https://doi.org/10.1016/j.ecoinf.2020.101197.
Jung, D.-H., Kim, N.Y., Moon, S.H., Jhin, C., Kim, H.-J., Yang, J.-S., Kim, H.S., Lee, T.S., Lee, J.Y.,
Park, S.H., 2021. Deep Learning-Based Cattle Vocal Classiﬁcation Model and Real-
Time Livestock Monitoring System with Noise Filtering. Animals 11, 357. https://
doi.org/10.3390/ani11020357.
Kaack, L.H., Donti, P.L., Strubell, E., Kamiya, G., Creutzig, F., Rolnick, D., 2022. Aligning ar-
tiﬁcial intelligence with climate change mitigation. Nat. Clim. Change 12, 518–527.
https://doi.org/10.1038/s41558-022-01377-7.
Kamilaris, A., Prenafeta-Boldú, F.X., 2018. Deep learning in agriculture: A survey. Comput.
Electron. Agric. 147, 70–90. https://doi.org/10.1016/j.compag.2018.02.016.
Kamyshova, G., Osipov, A., Gataullin, S., Korchagin, S., Ignar, S., Gataullin, T., Terekhova, N.,
Suvorov, S., 2022. Artiﬁcial neural networks and computer vision’s-based
phytoindication systems for variable rate irrigation improving. IEEE Access 10,
8577–8589. https://doi.org/10.1109/ACCESS.2022.3143524.
Kannur, Anil, Kannur, Asha, Rajpurohit, V.S., 2011. Classiﬁcation and grading of bulk seeds
using artiﬁcial neural network. Int. J. Mach. Intell. 3, 62–73. https://doi.org/10.9735/
0975-2927.3.2.62-73.
Kanwal, Z., Basit, A., Jawad, M., Ullah, I., Ali, A., 2019. Overlapped apple fruit yield estima-
tion using pixel classiﬁcation and hough transform. Int. J. Adv. Comput. Sci. Appl. 10.
https://doi.org/10.14569/IJACSA.2019.0100271.
Karbasi, M., Jamei, M., Ali, M., Malik, A., Yaseen, Z.M., 2022. Forecasting weekly reference
evapotranspiration using Auto Encoder Decoder Bidirectional LSTM model hybrid-
ized with a Boruta-CatBoost input optimizer. Comput. Electron. Agric. 198, 107121.
Karim, L., Anpalagan, A., Nasser, N., Almhana, J., 2013. Sensor-based M2M Agriculture
Monitoring Systems for Developing Countries. State and Challenges 5. https://doi.
org/10.5296/npa.v5i3.3787.
Khaki, S., Wang, L., Archontoulis, S.V., 2020. A CNN-RNN Framework for Crop Yield Predic-
tion. Front. Plant Sci. 10. https://doi.org/10.3389/fpls.2019.01750.
Khan, S., Naseer, M., Hayat, M., Zamir, S.W., Khan, F.S., Shah, M., 2021. Transformers in Vi-
sion: A Survey. ACM Comput. Surv. https://doi.org/10.1145/3505244.
Khanna, A., Kaur, S., 2019. Evolution of Internet of Things (IoT) and its signiﬁcant impact
in the ﬁeld of Precision Agriculture. Comput. Electron. Agric. 157, 218–231. https://
doi.org/10.1016/j.compag.2018.12.039.
Kim, K.-H., Kim, M.-G., Yoon, P.-R., Bang, J.-H., Myoung, W.-H., Choi, J.-Y., Choi, G.-H., 2022.
Application of CCTV Image and Semantic Segmentation Model for Water Level Esti-
mation of Irrigation Channel. J. Korean Soc. Agric. Eng. 64, 63–73. https://doi.org/10.
5389/KSAE.2022.64.3.063.
Koech, R., Langat, P., 2018. Improving irrigation water use efﬁciency: a review of ad-
vances, challenges and opportunities in the Australian context. Water 10, 1771.
https://doi.org/10.3390/w10121771.
Kriegeskorte, N., Golan, T., 2019. Neural network models and deep learning. Curr. Biol. 29,
R231–R236. https://doi.org/10.1016/j.cub.2019.02.034.
Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012a. ImageNet classiﬁcation with deep
convolutional neural networks, in: Proceedings of the 25th International Conference
on Neural Information Processing Systems - Volume 1, NIPS’12. Curran Associates
Inc., Red Hook, NY, USA, pp. 1097–1105.
Krizhevsky, A., Sutskever, I., Hinton, G.E., 2012b. ImageNet Classiﬁcation with Deep
Convolutional Neural Networks. Advances in Neural Information Processing Systems.
Curran Associates, Inc.
Kumar, S., Singh, S.K., Singh, R., Singh, A.K., 2017. Recognition of cattle using face images.
Anim. Biom. 79–110.
Kumar, A., Sarkar, S., Pradhan, C., 2019. Recommendation System for Crop Identiﬁcation
and Pest Control Technique in Agriculture. 2019 International Conference on Com-
munication and Signal Processing (ICCSP). Presented at the 2019 International Con-
ference on Communication and Signal Processing (ICCSP), pp. 0185–0189 https://
doi.org/10.1109/ICCSP.2019.8698099.
Kumbi, A.A., Birje, M.N., 2022. Deep CNN based sunﬂower atom optimization method for
optimal water control in IoT. Wirel. Pers. Commun. 122, 1221–1246. https://doi.org/
10.1007/s11277-021-08946-7.
Kuplich, T.M., Curran, P.J., Atkinson, P.M., 2005. Relating SAR image texture to the biomass
of regenerating tropical forests. Int. J. Remote Sens. 26, 4829–4854. https://doi.org/10.
1080/01431160500239107.
Kurtulmuş, F., 2021. Identiﬁcation of sunﬂower seeds with deep convolutional neural net-
works. J. Food Meas. Charact. 15, 1024–1033. https://doi.org/10.1007/s11694-020-
00707-7.
Kushwaha, N.L., Bhardwaj, A., Verma, V.K., 2016. Hydrologic response of Takarla-Ballowal
watershed in Shivalik foot-hills based on morphometric analysis using remote sens-
ing and GIS. J Indian Water Resour Soc 36, 17–25.
Kushwaha, N.L., Rajput, J., Elbeltagi, A., Elnaggar, A.Y., Sena, D.R., Vishwakarma, D.K., Mani,
I., Hussein, E.E., 2021. Data Intelligence model and meta-heuristic algorithms-based
pan evaporation modelling in two different agro-climatic zones: a case study from
Northern India. Atmosphere 12, 1654. https://doi.org/10.3390/atmos12121654.
Kushwaha, N.L., Elbeltagi, A., Mehan, S., Malik, A., Yousuf, A., 2022. Comparative study on
morphometric analysis and RUSLE-based approaches for micro-watershed
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
226
prioritization using remote sensing and GIS. Arab. J. Geosci. 15, 564. https://doi.org/
10.1007/s12517-022-09837-2.
Kussul, N., Lavreniuk, M., Skakun, S., Shelestov, A., 2017. Deep learning classiﬁcation of
land cover and crop types using remote sensing data. IEEE Geosci. Remote Sens.
Lett. 14, 778–782. https://doi.org/10.1109/LGRS.2017.2681128.
Le, V.N.T., Ahderom, S., Alameh, K., 2020. Performances of the LBP Based Algorithm over
CNN Models for Detecting Crops and Weeds with Similar Morphologies. Sensors 20,
2193. https://doi.org/10.3390/s20082193.
LeCun, Y., Bottou, L., Bengio, Y., Haffner, P., 1998. Gradient-based learning applied to doc-
ument recognition. Proc. IEEE 86, 2278–2324.
Ledig, C., Theis, L., Huszar, F., Caballero, J., Cunningham, A., Acosta, A., Aitken, A., Tejani, A.,
Totz, J., Wang, Z., Shi, W., 2017. Photo-Realistic Single Image Super-Resolution Using a
Generative Adversarial Network. https://doi.org/10.48550/arXiv.1609.04802.
Li, X., Li, S., 2022. Transformer Help CNN See Better: A Lightweight Hybrid Apple Disease
Identiﬁcation Model Based on Transformers. Agriculture 12, 884. https://doi.org/10.
3390/agriculture12060884.
Li, Y., Randall, C.J., Van Woesik, R., Ribeiro, E., 2019. Underwater video mosaicing using to-
pology and superpixel-based pairwise stitching. Expert Syst. Appl. 119, 171–183.
https://doi.org/10.1016/j.eswa.2018.10.041.
Li, C., Li, H., Liu, Z., Li, B., Huang, Y., 2021. SeedSortNet: a rapid and highly efﬁﬁcient light-
weight CNN based on visual attention for seed sorting. PeerJ Comput. Sci. 7, e639.
https://doi.org/10.7717/peerj-cs.639.
Li, Y., Wu, C.-Y., Fan, H., Mangalam, K., Xiong, B., Malik, J., Feichtenhofer, C., 2022. MViTv2:
improved multiscale vision transformers for classiﬁcation and detection. Presented at
the Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recog-
nition, pp. 4804–4814.
Liu, J., Wang, X., 2020. Tomato diseases and pests detection based on improved yolo V3
convolutional neural network. Front. Plant Sci. 11.
Liu, D., Ning, X., Li, Z., Yang, D., Li, H., Gao, L., 2015. Discriminating and elimination of dam-
aged soybean seeds based on image characteristics. J. Stored Prod. Res. 60, 67–74.
https://doi.org/10.1016/j.jspr.2014.10.001.
Liu, W., Anguelov, D., Erhan, D., Szegedy, C., Reed, S., Fu, C.-Y., Berg, A.C., 2016. SSD: Single
Shot MultiBox Detector. In: Leibe, B., Matas, J., Sebe, N., Welling, M. (Eds.), Computer
Vision – ECCV 2016. Lecture Notes in Computer Science. Springer International Pub-
lishing, Cham, pp. 21–37 https://doi.org/10.1007/978-3-319-46448-0_2.
Lo Bianco, M., Grillo, O., Cañadas, E., Venora, G., Bacchetta, G., 2017. Inter- and intraspeciﬁc
diversity in Cistus L. (Cistaceae) seeds, analysed with computer vision techniques.
Plant Biol. 19, 183–190. https://doi.org/10.1111/plb.12529.
Ma, T., Tsuchikawa, S., Inagaki, T., 2020. Rapid and non-destructive seed viability predic-
tion using near-infrared hyperspectral imaging coupled with a deep learning ap-
proach. Comput. Electron. Agric. 177, 105683. https://doi.org/10.1016/j.compag.
2020.105683.
Maheswari, P., Raja, P., Apolo-Apolo, O.E., Pérez-Ruiz, M., 2021. Intelligent fruit yield esti-
mation for orchards using deep learning based semantic segmentation techniques—a
review. Front. Plant Sci. 12, 1247. https://doi.org/10.3389/fpsyg.2020.513474.
Malik, A., Saggi, M.K., Rehman, S., Sajjad, H., Inyurt, S., Bhatia, A.S., Farooque, A.A., Oudah,
A.Y., Yaseen, Z.M., 2022a. Deep learning versus gradient boosting machine for pan
evaporation prediction. Eng. Appl. Comput. Fluid Mech. 16, 570–587. https://doi.
org/10.1080/19942060.2022.2027273.
Malik, A., Tikhamarine, Y., Sihag, P., Shahid, S., Jamei, M., Karbasi, M., 2022b. Predicting
daily soil temperature at multiple depths using hybrid machine learning models for
a semi-arid region in Punjab. Environ. Sci. Pollut. Res, India https://doi.org/10.1007/
s11356-022-20837-3.
Mathew, M.P., Mahesh, T.Y., 2022. Leaf-based disease detection in bell pepper plant using
YOLO v5. Signal Image Video Process. 16, 841–847. https://doi.org/10.1007/s11760-
021-02024-y.
Miikkulainen, R., Liang, J., Meyerson, E., Rawal, A., Fink, D., Francon, O., Raju, B., Shahrzad,
H., Navruzyan, A., Duffy, N., Hodjat, B., 2019. Chapter 15 - Evolving Deep Neural Net-
works. In: Kozma, R., Alippi, C., Choe, Y., Morabito, F.C. (Eds.), Artiﬁcial Intelligence in
the Age of Neural Networks and Brain Computing. Academic Press, pp. 293–312
https://doi.org/10.1016/B978-0-12-815480-9.00015-3.
Mirza, M., Osindero, S., 2014. Conditional Generative Adversarial Nets. ArXiv14111784 Cs
Stat.
Mkonyi, L., Rubanga, D., Richard, M., Zekeya, N., Sawahiko, S., Maiseli, B., Machuve, D.,
2020. Early identiﬁcation of Tuta absoluta in tomato plants using deep learning. Sci.
Afr. 10, e00590. https://doi.org/10.1016/j.sciaf.2020.e00590.
Mochida, K., Koda, S., Inoue, K., Hirayama, T., Tanaka, S., Nishii, R., Melgani, F., 2019. Com-
puter vision-based phenotyping for improvement of plant productivity: a machine
learning perspective. GigaScience 8. https://doi.org/10.1093/gigascience/giy153.
Mohanty, S.P., Hughes, D.P., Salathé, M., 2016. Using Deep Learning for Image-Based Plant
Disease Detection. Front. Plant Sci. 7.
Mohr, S., Kühl, R., 2021. Acceptance of artiﬁcial intelligence in German agriculture: an ap-
plication of the technology acceptance model and the theory of planned behavior.
Precis. Agric. 22, 1816–1844. https://doi.org/10.1007/s11119-021-09814-x.
Mortensen, A.K., Dyrmann, M., Karstoft, H., Nyholm Jørgensen, R., Gislum, R., 2016. Se-
mantic Segmentation of Mixed Crops using Deep Convolutional Neural Network.
Mukti, I.Z., Biswas, D., 2019. Transfer Learning Based Plant Diseases Detection Using
ResNet50. 2019 4th Int. Conf. Electr. Inf. Commun. Technol. EICT. https://doi.org/10.
1109/EICT48899.2019.9068805.
Nagasubramanian, K., Jones, S., Singh, A.K., Sarkar, S., Singh, A., Ganapathysubramanian,
B., 2019. Plant disease identiﬁcation using explainable 3D deep learning on
hyperspectral images. Plant Methods 15, 98. https://doi.org/10.1186/s13007-019-
0479-8.
Ni, C., Wang, D., Vinson, R., Holmes, M., Tao, Y., 2019. Automatic inspection machine for
maize kernels based on deep convolutional neural networks. Biosyst. Eng. 178,
131–144. https://doi.org/10.1016/j.biosystemseng.2018.11.010.
Nilsson, H., 1995. Remote Sensing and Image Analysis in Plant Pathology. Annu. Rev.
Phytopathol. 33, 489–528. https://doi.org/10.1146/annurev.py.33.090195.002421.
O’Mahony, N., Campbell, S., Carvalho, A., Harapanahalli, S., Hernandez, G.V., Krpalkova, L.,
Riordan, D., Walsh, J., 2020. Deep Learning vs. Traditional Computer Vision. In: Arai,
K., Kapoor, S. (Eds.), Advances in Computer Vision, Advances in Intelligent Systems
and Computing. Springer International Publishing, Cham, pp. 128–144 https://doi.
org/10.1007/978-3-030-17795-9_10.
Olatunji, J.R., Redding, G.P., Rowe, C.L., East, A.R., 2020. Reconstruction of kiwifruit fruit ge-
ometry using a CGAN trained on a synthetic dataset. Comput. Electron. Agric. 177,
105699. https://doi.org/10.1016/j.compag.2020.105699.
Omondiagbe, O.P., Lilburne, L., Licorish, S., MacDonell, S., 2022. Soil Texture Prediction
with Automated Deep Convolutional Neural Networks and Population Based Learn-
ing (SSRN Scholarly Paper No. 4003387). Social Science Research Network, Rochester,
NY https://doi.org/10.2139/ssrn.4003387.
Osorio, K., Puerto, A., Pedraza, C., Jamaica, D., Rodríguez, L., 2020. A deep learning ap-
proach
for
weed
detection
in
lettuce
crops
using
multispectral
images.
AgriEngineering 2, 471–488. https://doi.org/10.3390/agriengineering2030032.
Palacios, F., Bueno, G., Salido, J., Diago, M.P., Hernández, I., Tardaguila, J., 2020. Automated
grapevine ﬂower detection and quantiﬁcation method based on computer vision and
deep learning from on-the-go imaging using a mobile sensing platform under ﬁeld
conditions. Comput. Electron. Agric. 178, 105796. https://doi.org/10.1016/j.compag.
2020.105796.
Piguard, 2022d. Piguard. https://www.serket-tech.com/Products. (Accessed 5 July 2022).
Plantix, 2022x. Plantix. https://plantix.net/en/. (Accessed 5 July 2022).
Putzu, L., Di Ruberto, C., Fenu, G., 2016. A Mobile Application for Leaf Detection in Com-
plex Background Using Saliency Maps. In: Blanc-Talon, J., Distante, C., Philips, W.,
Popescu, D., Scheunders, P. (Eds.), Advanced Concepts for Intelligent Vision Systems.
Springer International Publishing, Cham, pp. 570–581 https://doi.org/10.1007/978-3-
319-48680-2_50.
Pyo, J., Hong, S.M., Kwon, Y.S., Kim, M.S., Cho, K.H., 2020. Estimation of heavy metals using
deep neural network with visible and infrared spectroscopy of soil. Sci. Total Environ.
741, 140162. https://doi.org/10.1016/j.scitotenv.2020.140162.
Qiao, Y., Su, D., Kong, H., Sukkarieh, S., Lomax, S., Clark, C., 2019. Individual cattle identiﬁ-
cation using a deep learning based framework. IFAC-Pap. 52, 318–323.
Qiao, Y., Truman, M., Sukkarieh, S., 2019b. Cattle segmentation and contour extraction
based on Mask R-CNN for precision livestock farming. Comput. Electron. Agric. 165,
104958. https://doi.org/10.1016/j.compag.2019.104958.
Qiao, Y., Guo, Y., Yu, K., He, D., 2022. C3D-ConvLSTM based cow behaviour classiﬁcation
using video data for precision livestock farming. Comput. Electron. Agric. 193,
106650. https://doi.org/10.1016/j.compag.2021.106650.
Qiu, Z., Chen, J., Zhao, Y., Zhu, S., He, Y., Zhang, C., 2018. Variety Identiﬁcation of Single Rice
Seed Using Hyperspectral Imaging Combined with Convolutional Neural Network.
Appl. Sci. 8, 212. https://doi.org/10.3390/app8020212.
Rahnemoonfar, M., Sheppard, C., 2017. Deep count: fruit counting based on deep simu-
lated learning. Sensors 17, 905. https://doi.org/10.3390/s17040905.
Rahnemoonfar, M., Dobbs, D., Yari, M., Starek, M.J., 2019. DisCountNet: Discriminating and
Counting Network for Real-Time Counting and Localization of Sparse Objects in High-
Resolution UAV Imagery. Remote Sens. 11, 1128. https://doi.org/10.3390/
rs11091128.
Rai, P., Kumar, P., Al-Ansari, N., Malik, A., 2022. Evaluation of machine learning versus em-
pirical models for monthly reference evapotranspiration estimation in Uttar Pradesh
and Uttarakhand States. India. Sustainability 14, 5771. https://doi.org/10.3390/
su14105771.
Ranganathan, J., Waite, R., Searchinger, T., Hanson, C., 2018. How to Sustainably Feed 10
Billion People by 2050, in 21 Charts.
Redmon, J., Divvala, S., Girshick, R., Farhadi, A., 2016. You Only Look Once: Uniﬁed, Real-
Time Object Detection. 2016 IEEE Conference on Computer Vision and Pattern Recog-
nition (CVPR), pp. 779–788 https://doi.org/10.1109/CVPR.2016.91.
Rehman, T.U., Mahmud, Md.S., Chang, Y.K., Jin, J., Shin, J., 2019. Current and future appli-
cations of statistical machine learning algorithms for agricultural machine vision sys-
tems. Comput. Electron. Agric. 156, 585–605. https://doi.org/10.1016/j.compag.2018.
12.006.
Reimers, C., Requena-Mesa, C., 2020. Chapter 13 - Deep Learning – an Opportunity and a
Challenge for Geo- and Astrophysics. In: Škoda, P., Adam, F. (Eds.), Knowledge Dis-
covery in Big Data from Astronomy and Earth Observation. Elsevier, pp. 251–265
https://doi.org/10.1016/B978-0-12-819154-5.00024-2.
Ren, S., He, K., Girshick, R., Sun, J., 2016. Faster R-CNN: towards real-time object detection
with region proposal networks. ArXiv150601497 Cs.
Rico-Fernández, M.P., Rios-Cabrera, R., Castelán, M., Guerrero-Reyes, H.-I., Juarez-
Maldonado, A., 2019. A contextualized approach for segmentation of foliage in differ-
ent crop species. Comput. Electron. Agric. 156, 378–386. https://doi.org/10.1016/j.
compag.2018.11.033.
Riekert, M., Klein, A., Adrion, F., Hoffmann, C., Gallmann, E., 2020. Automatically detecting
pig position and posture by 2D camera imaging and deep learning. Comput. Electron.
Agric. 174, 105391. https://doi.org/10.1016/j.compag.2020.105391.
Riese, F.M., Keller, S., 2019a. SOIL TEXTURE CLASSIFICATION WITH 1D CONVOLUTIONAL
NEURAL NETWORKS BASED ON HYPERSPECTRAL DATA, in: ISPRS Annals of the Pho-
togrammetry, Remote Sensing and Spatial Information Sciences. Presented at the
ISPRS Geospatial Week 2019 (Volume IV-2/W5) - 10–14 June 2019. Copernicus
GmbH, Enschede, The Netherlands, pp. 615–621 https://doi.org/10.5194/isprs-an-
nals-IV-2-W5-615-2019.
Riese, F.M., Keller, S., 2019b. Soil texture classiﬁcation with 1D convolutional neural net-
works based on hyperspectral data. ISPRS Annals of the Photogrammetry, Remote
Sensing and Spatial Information Sciences. Copernicus GmbH, pp. 615–621 https://
doi.org/10.5194/isprs-annals-IV-2-W5-615-2019.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
227
Rivas, A., Chamoso, P., González-Briones, A., Corchado, J.M., 2018. Detection of Cattle
Using Drones and Convolutional Neural Networks. Sensors 18, 2048. https://doi.
org/10.3390/s18072048.
Rony, Md., Barai, D., Riad Hasan, Z., 2021. Cattle External Disease Classiﬁcation Using Deep
Learning Techniques. 2021 12th International Conference on Computing Communi-
cation and Networking Technologies (ICCCNT), pp. 1–7 https://doi.org/10.1109/
ICCCNT51525.2021.9579662.
Russello, H., 2018. Convolutional neural networks for crop yield prediction using satellite
images. IBM Cent. Adv. Stud.
Sami, M., Khan, S.Q., Khurram, M., Farooq, M.U., Anjum, R., Aziz, S., Qureshi, R., Sadak, F.,
2022. A Deep Learning-Based Sensor Modeling for Smart Irrigation System. Agron-
omy 12, 212. https://doi.org/10.3390/agronomy12010212.
Sau, S., Ucchesu, M., D'hallewin, G., Bacchetta, G., 2019. Potential use of seed morpho-
colourimetric analysis for Sardinian apple cultivar characterisation. Comput. Electron.
Agric. 162, 373–379. https://doi.org/10.1016/j.compag.2019.04.027.
Sau, S., Ucchesu, M., Dondini, L., De Franceschi, P., D'hallewin, G., Bacchetta, G., 2018. Seed
morphometry is suitable for apple-germplasm diversity-analyses. Comput. Electron.
Agric. 151, 118–125. https://doi.org/10.1016/j.compag.2018.06.002.
Schmidhuber, J., 2015. Deep learning in neural networks: An overview. Neural Netw. 61,
85–117. https://doi.org/10.1016/j.neunet.2014.09.003.
Shah, D., Trivedi, V., Sheth, V., Shah, A., Chauhan, U., 2021. ResTS: Residual Deep interpret-
able architecture for plant disease detection. Inf. Process. Agric. https://doi.org/10.
1016/j.inpa.2021.06.001.
Shakoor, N., Lee, S., Mockler, T.C., 2017. High throughput phenotyping to accelerate crop
breeding and monitoring of diseases in the ﬁeld. Curr. Opin. Plant Biol., 38 Biotic in-
teractions 2017. 38, pp. 184–192. https://doi.org/10.1016/j.pbi.2017.05.006.
Shang, L., Heckelei, T., Gerullis, M.K., Börner, J., Rasch, S., 2021. Adoption and diffusion of
digital farming technologies - integrating farm-level evidence and system interaction.
Agric. Syst. 190, 103074. https://doi.org/10.1016/j.agsy.2021.103074.
Shen, W., Hu, H., Dai, B., Wei, X., Sun, J., Jiang, L., Sun, Y., 2020. Individual identiﬁcation of
dairy cows based on convolutional neural networks. Multimed. Tools Appl. 79,
14711–14724.
Shojaeipour, A., Falzon, G., Kwan, P., Hadavi, N., Cowley, F.C., Paul, D., 2021. Automated
muzzle detection and biometric identiﬁcation via few-shot deep transfer learning
of mixed breed cattle. Agronomy 11, 2365. https://doi.org/10.3390/agron-
omy11112365.
Shrivastava, S., Marshall-Colon, A., 2018. Big data in agriculture and their analyses. Ency-
clopedia of Food Security and Sustainability. Elsevier, pp. 233–237 https://doi.org/10.
1016/B978-0-08-100596-5.22191-4.
Simonyan, K., Zisserman, A., 2015. Very deep convolutional networks for large-scale
image recognition. ArXiv14091556 Cs.
Singh, D., Jain, N., Jain, P., Kayal, P., Kumawat, S., Batra, N., 2020. PlantDoc: a dataset for
visual plant disease detection. Proc. 7th ACM IKDD CoDS 25th COMAD,
pp. 249–253 https://doi.org/10.1145/3371158.3371196.
Singh, P., Verma, A., Alex, J.S.R., 2021. Disease and pest infection detection in coconut tree
through deep learning techniques. Comput. Electron. Agric. 182, 105986. https://doi.
org/10.1016/j.compag.2021.105986.
Son, C.-H., 2021. Leaf spot attention networks based on spot feature encoding for leaf dis-
ease identiﬁcation and detection. Appl. Sci. 11, 7960. https://doi.org/10.3390/
app11177960.
Subeesh, A., Mehta, C.R., 2021. Automation and digitization of agriculture using artiﬁcial
intelligence and internet of things. Artif. Intell. Agric. 5, 278–291. https://doi.org/10.
1016/j.aiia.2021.11.004.
Subeesh, A., Kumar, P., Chauhan, N., 2019. Flood early detection system using internet of
things and artiﬁcial neural networks. International Conference on Innovative Com-
puting and Communications. Springer, pp. 297–305.
Subeesh, A., Bhole, S., Singh, K., Chandel, N.S., Rajwade, Y.A., Rao, K.V.R., Kumar, S.P., Jat, D.,
2022. Deep convolutional neural network models for weed detection in polyhouse
grown bell peppers. Artif. Intell. Agric. 6, 47–54. https://doi.org/10.1016/j.aiia.2022.
01.002.
Suchithra, M.S., Pai, M.L., 2020. Improving the prediction accuracy of soil nutrient classiﬁ-
cation by optimizing extreme learning machine parameters. Inf. Process. Agric. 7,
72–82. https://doi.org/10.1016/j.inpa.2019.05.003.
Sudarsan, B., Ji, W., Biswas, A., Adamchuk, V., 2016. Microscope-based computer vision to
characterize soil texture and soil organic matter. Biosyst. Eng., Proximal Soil Sensing –
Sensing Soil Condition and Functions 152, 41–50. https://doi.org/10.1016/j.
biosystemseng.2016.06.006.
Syed-Ab-Rahman, S.F., Hesamian, M.H., Prasad, M., 2022. Citrus disease detection and
classiﬁcation using end-to-end anchor-based deep learning model. Appl. Intell. 52,
927–938. https://doi.org/10.1007/s10489-021-02452-w.
Taheri-Garavand, A., Nasiri, A., Fanourakis, D., Fatahi, S., Omid, M., Nikoloudakis, N., 2021.
Automated in situ seed variety identiﬁcation via deep learning: a case study in chick-
pea. Plants 10, 1406. https://doi.org/10.3390/plants10071406.
Tang, J., Arvor, D., Corpetti, T., Tang, P., 2021. Mapping Center Pivot Irrigation Systems in
the Southern Amazon from Sentinel-2 Images. Water 13, 298. https://doi.org/10.
3390/w13030298.
Tantalaki, N., Souravlas, S., Roumeliotis, M., 2019. Data-driven decision making in preci-
sion agriculture: the rise of big data in agricultural systems. J. Agric. Food Inf. 20,
344–380. https://doi.org/10.1080/10496505.2019.1638264.
Tedesco-Oliveira, D., Pereira da Silva, R., Maldonado, W., Zerbato, C., 2020. Convolutional
neural networks in predicting cotton yield from images of commercial ﬁelds.
Comput. Electron. Agric. 171, 105307. https://doi.org/10.1016/j.compag.2020.
105307.
Thakur, P., Chug, A., Singh, A.P., 2021. Plant disease detection of bell pepper plant using
transfer learning over different models. 2021 8th International Conference on Signal
Processing and Integrated Networks (SPIN), pp. 384–389 https://doi.org/10.1109/
SPIN52536.2021.9565945.
Tian, H., Wang, T., Liu, Y., Qiao, X., Li, Y., 2020. Computer vision technology in agricultural
automation —A review. Inf. Process. Agric. 7, 1–19. https://doi.org/10.1016/j.inpa.
2019.09.006.
Tibbetts, John, 2018. The Frontiers of Artiﬁcial Intelligence: Deep learning brings speed,
accuracy to the life sciences. BioScience 68 (1), 5–10. https://doi.org/10.1093/
biosci/bix136.
Vaswani, A., Shazeer, N., Parmar, N., Uszkoreit, J., Jones, L., Gomez, A.N., Kaiser, Ł.,
Polosukhin, I., 2017. Attention is All you Need, in: Advances in Neural Information
Processing Systems. Curran Associates, Inc.
Vázquez-Arellano, M., Griepentrog, H.W., Reiser, D., Paraforos, D.S., 2016. 3-D Imaging
Systems for Agricultural Applications—A Review. Sensors 16, 618. https://doi.org/
10.3390/s16050618.
Veeramani, B., Raymond, J.W., Chanda, P., 2018. DeepSort: deep convolutional networks
for sorting haploid maize seeds. BMC Bioinformatics 19, 289. https://doi.org/10.
1186/s12859-018-2267-2.
Veeranampalayam Sivakumar, A.N., Li, J., Scott, S., Psota, E., Jhala, J., Luck, J.D., Shi, Y., 2020.
Comparison of object detection and patch-based classiﬁcation deep learning models
on mid- to late-season weed detection in UAV imagery. Remote Sens. 12, 2136.
https://doi.org/10.3390/rs12132136.
Vishwakarma, D.K., Pandey, K., Kaur, A., Kushwaha, N.L., Kumar, R., Ali, R., Elbeltagi, A.,
Kuriqi, A., 2022. Methods to estimate evapotranspiration in humid and subtropical
climate conditions. Agric. Water Manag. 261, 107378. https://doi.org/10.1016/j.
agwat.2021.107378.
Waldhoff, G., Lussem, U., Bareth, G., 2017. Multi-Data Approach for remote sensing-based
regional crop rotation mapping: A case study for the Rur catchment, Germany. Int.
J. Appl. Earth Obs. Geoinformation 61, 55–69. https://doi.org/10.1016/j.jag.2017.04.
009.
Wang, A., Xu, Y., Wei, X., Cui, B., 2020. Semantic segmentation of crop and weed using an
encoder-decoder network and image enhancement method under uncontrolled out-
door illumination. IEEE Access 8, 81724–81734. https://doi.org/10.1109/ACCESS.
2020.2991354.
Weary, D.M., Huzzey, J.M., von Keyserlingk, M., 2009. Board-invited review: Using behav-
ior to predict and identify ill health in animals. J. Anim. Sci. 87, 770–777. https://doi.
org/10.2527/jas.2008-1297.
Wei, M.C.F., Maldaner, L.F., Ottoni, P.M.N., Molin, J.P., 2020. Carrot yield mapping: a preci-
sion agriculture approach based on machine learning. AI 1, 229–241. https://doi.org/
10.3390/ai1020015.
Wu, N., Zhang, Y., Na, R., Mi, C., Zhu, S., He, Y., Zhang, C., 2019. Variety identiﬁcation of oat
seeds using hyperspectral imaging: investigating the representation ability of deep
convolutional neural network. RSC Adv. 9, 12635–12644. https://doi.org/10.1039/
C8RA10335F.
Wu, H., Xiao, B., Codella, N., Liu, M., Dai, X., Yuan, L., Zhang, L., 2021. Cvt: Introducing con-
volutions to vision transformers. Presented at the Proceedings of the IEEE/CVF Inter-
national Conference on Computer Vision, pp. 22–31.
Xiao, J., Liu, G., Wang, K., Si, Y., 2022. Cow identiﬁcation in free-stall barns based on an im-
proved Mask R-CNN and an SVM. Comput. Electron. Agric. 194, 106738. https://doi.
org/10.1016/j.compag.2022.106738.
Xu, G., Zhang, F., Shah, S.G., Ye, Y., Mao, H., 2011. Use of leaf color images to identify nitro-
gen and potassium deﬁcient tomatoes. Pattern Recognit. Lett. 32, 1584–1590. https://
doi.org/10.1016/j.patrec.2011.04.020.
Xu, T., Zhang, P., Huang, Q., Zhang, H., Gan, Z., Huang, X., He, X., 2017. AttnGAN: Fine-
Grained Text to Image Generation with Attentional Generative Adversarial Networks.
https://doi.org/10.48550/arXiv.1711.10485.
Xu, B., Wang, W., Falzon, G., Kwan, P., Guo, L., Sun, Z., Li, C., 2020. Livestock classiﬁcation
and counting in quadcopter aerial images using Mask R-CNN. Int. J. Remote Sens. 41,
8121–8142. https://doi.org/10.1080/01431161.2020.1734245.
Yang, Q., Xiao, D., Lin, S., 2018. Feeding behavior recognition for group-housed pigs with
the Faster R-CNN. Comput. Electron. Agric. 155, 453–460. https://doi.org/10.1016/j.
compag.2018.11.002.
Yang, Q., Shi, L., Han, J., Zha, Y., Zhu, P., 2019. Deep convolutional neural networks for rice
grain yield estimation at the ripening stage using UAV-based remotely sensed im-
ages. Field Crops Res. 235, 142–153. https://doi.org/10.1016/j.fcr.2019.02.022.
Yoo, H.-J., 2015. Deep convolution neural networks in computer vision: a review. IEIE
Trans. Smart Process. Comput. 4, 35–43.
You, J., Li, X., Low, M., Lobell, D., Ermon, S., 2017. Deep Gaussian process for crop yield pre-
diction based on remote sensing data. Proceedings of the Thirty-First AAAI Confer-
ence on Artiﬁcial Intelligence, AAAI’17. AAAI Press, San Francisco, California, USA,
pp. 4559–4565.
Yu, Y., Xu, T., Shen, Z., Zhang, Y., Wang, X., 2019. Compressive spectral imaging system for
soil classiﬁcation with three-dimensional convolutional neural network. Opt. Express
27, 23029–23048. https://doi.org/10.1364/OE.27.023029.
Zhang, X., Younan, N., King, R., 2003. Soil texture classiﬁcation using wavelet transform
and Maximum Likelihood Approach. IGARSS 2003. 2003 IEEE International Geosci-
ence and Remote Sensing Symposium. Proceedings (IEEE Cat. No.03CH37477),
pp. 2888–2890.
Zhang, C., Yue, P., Di, L., Wu, Z., 2018. Automatic identiﬁcation of center pivot irrigation
systems from landsat images using convolutional neural networks. Agriculture 8,
147. https://doi.org/10.3390/agriculture8100147.
Zhang, Y., Chu, J., Leng, L., Miao, J., 2020. Mask-Reﬁned R-CNN: A Network for Reﬁning Ob-
ject Details in Instance Segmentation. Sensors 20, 1010. https://doi.org/10.3390/
s20041010.
Zhao, G., Quan, L., Li, H., Feng, H., Li, S., Zhang, S., Liu, R., 2021. Real-time recognition sys-
tem of soybean seed full-surface defects based on deep learning. Comput. Electron.
Agric. 187, 106230. https://doi.org/10.1016/j.compag.2021.106230.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
228
Zhong, G., Wang, L.-N., Ling, X., Dong, J., 2016. An overview on data representation learn-
ing: From traditional feature learning to recent deep learning. J. Finance Data Sci. 2,
265–278. https://doi.org/10.1016/j.jfds.2017.05.001.
Zhong, L., Guo, X., Xu, Z., Ding, M., 2021. Soil properties: Their prediction and feature ex-
traction from the LUCAS spectral library using deep convolutional neural networks.
Geoderma 402, 115366. https://doi.org/10.1016/j.geoderma.2021.115366.
Zhou, Z., Song, Z., Fu, L., Gao, F., Li, R., Cui, Y., 2020. Real-time kiwifruit detection in orchard
using deep learning on AndroidTM smartphones for yield estimation. Comput. Elec-
tron. Agric. 179, 105856. https://doi.org/10.1016/j.compag.2020.105856.
Zhu, S., Zhou, L., Gao, P., Bao, Y., He, Y., Feng, L., 2019. Near-infrared hyperspectral imaging
combined with deep learning to identify cotton seed varieties. Molecules 24, 3268.
https://doi.org/10.3390/molecules24183268.
Zhu, F., He, M., Zheng, Z., 2020a. Data augmentation using improved cDCGAN for plant
vigor rating. Comput. Electron. Agric. 175, 105603. https://doi.org/10.1016/j.
compag.2020.105603.
Zhu, J.-Y., Park, T., Isola, P., Efros, A.A., 2020b. Unpaired Image-to-Image Translation using
Cycle-Consistent Adversarial Networks. https://doi.org/10.48550/arXiv.1703.10593.
V.G. Dhanya, A. Subeesh, N.L. Kushwaha et al.
Artiﬁcial Intelligence in Agriculture 6 (2022) 211–229
229
