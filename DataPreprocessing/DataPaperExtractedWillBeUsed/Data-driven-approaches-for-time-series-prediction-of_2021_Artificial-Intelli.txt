Data-driven approaches for time series prediction of daily production in the
Sulige tight gas ﬁeld, China
Qi Zhang a, Ziwei Chen b, Yuan Zeng a, Hang Gao a, Qiansheng Wei a, Tiaoyu Luo b,
Zhiguo Wang b,*
a The Third Gas Production Plant of PetroChina Changqing Oilﬁeld Branch, Wushenqi, Inner Mongolia, 017000, PR China
b School of Mathematics and Statistics, Xi'an Jiaotong University, Xi'an, Shaanxi, 710049, PR China
A R T I C L E I N F O
Keywords:
Prediction of production time series
Long short-term memory neural network
Random forest
Support vector machine
A B S T R A C T
The Sulige tight gas ﬁeld is presently the largest gas ﬁeld in China. Owing to the ultralow permeability and strong
heterogeneity of the reservoirs in Sulige, the number of production wells has exceeded 3,000, keeping the stable
gas supply in the decade. Thus, the daily production prediction of gas wells is signiﬁcant for monitoring pro-
duction and for implementing and evaluating stimulation measures. Therefore, on the basis of the three data-
driven time series approaches, the daily production of 1692 wells over 10 years was mining for the daily pro-
duction prediction of wells in Sulige. The jointed deep long short-term memory and fully connected neural
network (DLSTM-FNN) model was proposed by introducing the recurrent neural network's sequential expression
ability and was compared with random forest (RF) and support vector regression (SVR). After the daily production
predictions of thousands of wells in Sulige, the proposed DLSTM-FNN model signiﬁcantly improved the time
series prediction accuracy and efﬁciency in the short training samples and had strong availability and practica-
bility in the Sulige tight gas ﬁeld.
1. Introduction
The Sulige gas ﬁeld is located northwest of the Ordos Basin, China.
Since 2011, it has become China's largest gas ﬁeld (H. Yang et al., 2012).
Owing to sedimentation, diagenesis, and accumulation processes (T.
Yang et al., 2012; Zou et al., 2012; Jiang et al., 2007), the Sulige gas ﬁeld
reservoirs show ultralow permeability and strong heterogeneity, result-
ing in the low production of wells. Hence, in the last decade, the number
of wells in the Sulige gas ﬁeld had to be increased rapidly for the
continuous
expansion
of
productivity.
Subsequently,
liquid-accumulation, intermittent, and low production wells are gradu-
ally increasing, making the production management of the entire gas
ﬁeld more complicated. Therefore, it is crucial to explore the production
rules of various gas wells in different stages, especially predicting pro-
duction dynamics and monitoring production anomalies for the whole
life cycle management of gas wells, which helps in maintaining long-term
stable production and supply of natural gas.
The commonly used production prediction methods of gas wells can
be divided into numerical simulation based on the physical model and
production decline curve analysis based on the statistical law.
Reservoir numerical simulation utilizes computer models to estimate
the ﬂuids dynamics in porous media. The numerical simulation requires a
large amount of static geological and dynamic production data to achieve
accurate geological modeling and high-quality history matching. There-
fore, the dynamic simulation takes a lot of time, and there are deviations
between the predicted results and the actual gas ﬁeld production results.
In reservoir engineering, many decline curve analysis models have
been proposed to predict oil and gas production. As early as 1945, Arps
proposed a classical exponential decline model, hyperbolic decline
model, and harmonic decline model (Arps, 1945). Chen et al. proposed a
generalized decline model, linear decline model, and pan-exponential
decline model in China (Chen and Tang, 2016; Chen and Zhou, 2015;
Chen and Fu, 2019). However, it is still challenging to select the pa-
rameters for tight sandstone gas in the abovementioned decline models,
thus limiting its application in production dynamic prediction.
Recently, on the basis of the available production data, various ma-
chine learning methods, especially neural networks, have been success-
fully used to select the featured parameters that affect well production
and then predict oil and gas well production under a supervised learning
framework (Cao et al., 2016; Hou, 2019; Liu et al., 2019, 2020; Gu et al.,
* Corresponding author.
E-mail address: emailwzg@gmail.com (Z. Wang).
Contents lists available at ScienceDirect
Artiﬁcial Intelligence in Geosciences
journal homepage: www.keaipublishing.com/en/journals/artiﬁcial-intelligence-in-geosciences
https://doi.org/10.1016/j.aiig.2022.02.005
Received 15 December 2021; Received in revised form 25 February 2022; Accepted 26 February 2022
Available online 3 March 2022
2666-5441/© 2022 The Authors. Publishing Services by Elsevier B.V. on behalf of KeAi Communications Co. Ltd. This is an open access article under the CC BY-NC-ND
license (http://creativecommons.org/licenses/by-nc-nd/4.0/).
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
2019).
However, in the abovementioned oil and gas well production pre-
diction methods, whether physical model-driven or data-driven algo-
rithms,
they
focused
more
on
medium-to-long-term
production
prediction in the unit of month or year and were less involved in time
series prediction of a single well's daily production and abnormal
monitoring in the whole life cycle of a tight gas ﬁeld (Dong and Yang,
2009; Sagheer and Kotb, 2019; Jiang et al., 2021). Therefore, considering
the current and past statuses of daily production time series, we proposed
a deep network prediction method based on long short-term memory
(LSTM) neural network and further compared it with traditional
data-driven machine learning approaches such as random forest (RF) and
support vector regression (SVR).
2. Data-driven approaches
2.1. Random forest and support vector regression
A decision tree (DT) is a supervised learning algorithm that selects the
best decision scheme by constructing the decision process and calculating
the corresponding mathematical expectation under the condition of
known probabilities of all events. It can be expressed as a tree structure,
in which the internal nodes represent different levels of decision schemes
and the leaf nodes (endpoints) represent the ﬁnal decision results.
However, they are prone to overﬁtting. Therefore, ensemble learning
methods are often used to replace a single DT, that is, RF (Breiman,
2001). Assuming that the prediction results of each DT are good, there is
overﬁtting for different datasets. The RF notes that by constructing many
DTs and averaging their prediction results, the robustness of the DTs can
be improved while maintaining their prediction ability. The RF algorithm
ﬁrst conducts random sampling with replacement of the dataset to obtain
several new datasets to create as many different DTs as possible. For these
newly created datasets, each DT is constructed, and the weighted average
is conducted according to the different importance of features to obtain
the prediction results of the RF.
The model function of SVR is a linear function. The SVR constructs an
interval band on both sides of the linear function. The loss is not calcu-
lated for the data samples falling into the interval band but for the
samples located outside the interval band, and it is calculated according
to a speciﬁc loss function. The SVR also introduces the kernel function
method from the support vector machine to expand the algorithm to
higher dimensional data space. There are many kinds of kernel functions.
In this paper, the Gaussian kernel was used to map data samples
x1 and x2 into high-dimensional space, which is deﬁned as
krbf ðx1; x2Þ ¼ exp
�
� γjjx1 � x2jj2�
;
(1)
where γ is the parameter controlling the width of the Gaussian kernel.
2.2. Recurrent neural network
For time series such as the daily production of gas wells, the recurrent
neural network (RNN) (Hopﬁeld, 1982) was introduced to capture the
correlation between the current and past moments. The RNN model has
the following characteristics: the output at one moment will be a part of
the input at the next moment; for the multilayer stacked RNN, the pa-
rameters of different layers can be shared, thus reducing the number of
training parameters; and the length of the input data can be variable.
Fig. 1 shows the basic structure of RNN. Let the input data X ¼ ½x0;x1;
…;xN�1�. A represents a structural neuron unit, xt represents the input of
the t-th network, ot represents the output of the t-th network, and ht
represents the hidden state of the output of the t-th network. The
input–output relationship can be expressed as
� ht ¼ σðWxhxt þ Whhht�1 þ bhÞ
ot ¼ Whyht�1 þ bo
;
(2)
where σ is the activation function, Wxh is the weight matrix applied to xt
to compute the hidden state ht, Whh is the weight matrix applied to the
current hidden state ht to calculate the next hidden state ht�1, Why is the
weight matrix applied to the hidden state ht to calculate the output ot,
and bh and bo are offset items belonging to the hidden state and output,
respectively. According to equation (2), ht from any layer of the network
can affect the subsequent output, which has strong practical signiﬁcance
for processing time series.
However, the RNN networks are challenging to achieve long-term
transmission of the input. When the back-propagation mechanism is
adopted to update the weights in the RNN, the gradient of the latter layer
will be multiplied by that of the former network layer as the number of
network layers increases gradually. Hence, the RNN is prone to gradient
explosion/disappearance (Bengio et al, 1994). To solve the problem of
long-term dependencies, many network structures have been proposed,
such as LSTM neural networks and gated recurrent units.
2.3. LSTM neural network
LSTM is one of the most widely used RNN structures (Hochreiter and
Schmidhuber, 1997). Compared with the traditional RNN structure, the
LSTM adds memory and gating units on the original basis, which can
effectively avoid the disappearance/explosion of the gradient and can
extract the long-term correlation that may exist in the data. Fig. 2 depicts
a schematic of an LSTM unit. The LSTM unit introduces three types of
gating units, which are the input, forget, and output gates. The general
equation of the input and output of the gating unit can be expressed as
y ¼ σðWx þ bÞ;
(3)
where W and b are the weight matrix and bias of the gating unit,
respectively, and σ is the activation function. In the LSTM, the sigmoid
function is generally used, and its expression is
σðxÞ ¼
1
1 þ e�x
(4)
At time t, the input data of the LSTM are denoted by xt, the output
data are denoted by ht, the memory unit is ct, � represents the
Fig. 1. The architecture of a recurrent neural network.
Fig. 2. A block of the long short-term memory.
Q. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
166
multiplication of the corresponding bit elements, and the input gate i is
used to determine how much input data xt are allocated to the memory
unit ct. The forward propagation process is
it ¼ σ
�
Wxixt þ Whf ht�1 þ bi
�
(5)
Considering some features in the time series may not be meaningful
for the target task. Thus, the forgetting gate is introduced to reduce the
inﬂuence of these features at subsequent time t þ n. The forgetting gate f
mainly selects the discarded sample features by changing the memory
unit ct as follows:
f t ¼ σ
�
Wxf xt þ Whiht�1 þ bf
�
;
(6)
ct ¼ ft � ct�1 þ it � tanhðWxcxt þ Whcht�1 þ bcÞ
(7)
The output gate o determines the output data ht of the network
through the memory unit ct and the input data xt. At this time, the output
ht of the network takes into account both the current input xt and the
content learned by the previous network (that is, the memory unit ct�1),
and its expression is
ot ¼ σðWxoxt þ Whoht�1 þ boÞ;
(8)
ht ¼ ot � tanhðctÞ
(9)
In summary, for a sample si of the training set, its data and label are xi
and yi, respectively. In this paper, the architecture of a deep network
based on the deep LSTM and a fully connected network, that is, DLSTM-
FNN, is ﬁnally proposed, as shown in Fig. 3. The forward propagation
expression is
8
>
>
>
>
<
>
>
>
>
:
f1 ¼ RNNðxiÞ
f2 ¼ Reluðf1Þ
f3 ¼ Dropoutðf2Þ:
f4 ¼ Linearðf3Þ
f5 ¼ Poolðf4Þ
(10)
In equation (10), RNN represents the recurrent neural network. In this
study, we selected the LSTM unit, where the depth of hidden layers was
30, which means the deep LSTM (DLSTM). Relu represents the linear
rectiﬁcation activation function, which is used to enhance the nonlinear
expression capability of the network. Dropout represents the drop layer,
and the inactivation rate was set to 0.5, which aims to alleviate the
overﬁtting problem of the model. Linear and Pool represent the full
connection layer (FNN) and the average pooling layer, respectively, to
make the output f5 have the same shape as the label yi. The root mean
square error was used as the loss function, and adaptive moment esti-
mation (Adam) was used as the weight optimization algorithm.
3. Time series prediction
The reservoir indicators of gas wells generally include static geolog-
ical and dynamic production data. Static geological data refer to well-
head position coordinates, depth, resistivity, sonic interval transit time,
rock density, compensated neutrons, shale content, gas layer thickness,
total porosity, permeability, and hydrocarbon saturation. Dynamic pro-
duction data mainly refer to the daily gas production, gas pressure, casing
pressure, and production time of a gas well under different production
dates. To facilitate the comparison of different methods, this study ig-
nores the remaining dynamic production characteristics and only con-
siders the daily gas production time series. Next, the RF, SVR, and
DLSTM-FNN models were applied to dynamically predict the daily gas
production time series for 1692 tight gas wells in the Sulige gas ﬁeld.
3.1. Sample set construction
In the daily production time series of 1692 tight gas wells in Sulige,
the earliest production date started on November 5, 2006, and the
longest production time was 5405 days as of August 28, 2021. Let the
daily gas production of gas well vi be datai. Owing to the different pro-
duction times and durations of each gas well, the length of datai in each
gas well was not completely consistent. To enable large quantities of gas
well data to be inputted into the DLSTM-FNN for training and prediction
simultaneously, all datai must have the same length. One method is to ﬁll
in with zero values, that is, to make the value of datai at all nonmining
times be zero. Another method is zero removal, that is, delete the zero
values existing in datai and splice it. After splicing, the timing sequence
length of the dataset is the minimum length of datai after zero removal.
However, both methods have advantages and disadvantages. The zero-
ﬁlling method retains the consistency of the daily gas production of
each gas well in the production time, but it will result in a considerable
number of zeros in the dataset, which reduces the convergence speed of
the neural network. The zero removal method reduces the difﬁculty of
neural network training because it retains effective production data, but
it leads to inconsistencies in the corresponding production times of
different gas wells. Additionally, because the effective production data of
individual gas wells are relatively small, the time sequence length of the
combined dataset will be very low, limiting the expressive ability of the
neural network.
Therefore, if the zero-ﬁlling method is adopted to construct dataset D
for all gas wells, its dimension Dshape is ð1692; 5405Þ. In this study, it was
unnecessary to consider all gas wells. Supposing the number of gas wells
to be considered is n, for the zero-ﬁlling method, Dshape is equal to ðn;
5405Þ; for the zero removal method, Dshape is equal to ðn;lminÞ; and lmin is
the minimum length after zero removal of datai.
Let the dimension of the time series D be ðn;lÞ. First, D was divided
Fig. 3. The schematic of the DLSTM-FNN architecture.
Q. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
167
into the training set Dtrain and the test set Dtest. That is, for a time series of
length l, assign the ﬁrst ltrain information of D to Dtrain and the last ltest
information to Dtest. Next, construct input data x and input label y for
Dtrain and Dtest, respectively. At time t, we hope to use the sequence x for
some time before t to predict the sequence y for some time after t.
Assuming that the sequence length of x is lx, the sequence length of y is ly,
and the sample formed by a pair of x and y at time t is st, the sample stþ1
can be generated at time t þ 1, as shown in Fig. 4. Following Fig. 4, for the
training set Dtrain, the sample dataset Strain can be constructed as ½s0;s1;…;
smtrain�, and the number of samples mtrain is ltrain � lx � ly þ 1. Similarly, for
the test set Dtest, Stest can be constructed as ½s0;s1;…;smtest �, and the number
of test sets mtest is ltest � lx � ly þ 1. If Strain and Stest are divided into
training set data xtrain, training set label ytrain, test set data xtest, and test
set label ytest, their dimensions are ðmtrain;lx;nÞ, ðmtrain;ly;nÞ, ðmtest;lx;nÞ,
and ðmtest;ly;nÞ, respectively.
3.2. Result analysis
Using the information of 1692 gas wells in Sulige as the dataset, the
RF, SVR, and DLSTM-FNN models were constructed. First, we reduced
the dimensions of x and y to ðm; lxÞ and ðm;lyÞ, respectively. Then, model
training was performed on the basis of xtrain and ytrain. The length of the
training sample was only 10% of the total sample data, which belonged
to the short training sample set. On the basis of the data-driven ap-
proaches, we used the trained model to predict xtrain and xtest to obtain
ytrain
pred and ytest
pred, and then, we compared them with the observed values
ytrain and ytest to analyze regression accuracy. As a deep learning model,
the DLSTM-FNN needs to balance prediction accuracy and computational
time. By constant optimizations of the network hyperparameters, ﬁnally,
the shortest sequence length lx was set to 1, the label sequence length ly
was 1, the learning rate was 0.0001, the weight attenuation was 0.0005,
and the number of iterations was 150. Under optimal hyperparameters,
the average predicting speed of the DLSTM-FNN model was 67.59 s per
well, which can fulﬁll the real-time requirement of the actual production
abnormality monitoring in Sulige. Here the root mean square error
(RMSE) was used as an accuracy measurement, which can be given as
follows
RMSE ¼
ﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃﬃ
1
n
X
n
i¼1
�
yi � yi
pred
�2
s
;
(11)
where yi is the current observed value and yi
pred is its predicted value. For
the 1692 gas wells in the Sulige gas ﬁeld, Table 1 compares the RMSE of
the three gas well production dynamic prediction models: the RF, SVR,
and DLSTM-FNN on the training set and testing set.
Although the RF had the smallest error in the short training sample
set, which was 0.0884904, it had the largest error in the test set, which
was 0.1878669. The DLSTM-FNN had the smallest error in the testing set,
which was 0.1378165 and 0.03 less than the SVR prediction error.
Additionally, its prediction was the best.
To further analyze the life cycle of the gas well, typical gas wells S1
and S2 that have produced for 3922 and 1145 days, respectively, were
taken as examples. Fig. 5 shows the gas well production prediction results
of the three data-driven models, and Table 2 shows their corresponding
RMSE. For gas wells S1 and S2, DLSTM-FNN had the smallest RMSE on
the testing set, which was 0.0722189 and 0.1025749, respectively. By
comparing Fig. 5a, c, and 5e, the DLSTM-FNN showed more accurately
predicted production trends and mutation locations in the measured
continuous production period (1000–1500 days), the intermittent pro-
duction period (1500–2000 days), and the economic inefﬁciency period
(>2000 days) of gas well S1. During the 1145-day production cycle, the
daily production of gas well S2 was zero due to multiple shutdowns, and
the daily production ﬂuctuated sharply because of multiple production
stimulation measures. In the prediction results of gas well S2 (Fig. 5b, d,
and 5f), the DLSTM-FNN model better predicted the locations of zero
daily production abnormalities and severe ﬂuctuations (Fig. 5f). There-
fore, compared with the traditional RF and SVR, the data-driven DLSTM-
FNN model had the advantages of short training sample sets, quick speed
of large-scale predictions, and more accurate results. Furthermore, the
prediction of DLSTM-FNN is expected to be used for monitoring pro-
duction anomalies, for providing a speciﬁc time window, and for a
quantitated production evaluation for the stimulation measures.
4. Conclusion
This paper studied the daily production prediction of the Sulige tight
gas wells. Three data-driven approaches of time series prediction,
including the RF, SVR, and LSTM, were applied. Considering the poten-
tial information of current and past statuses in the time series, we pro-
posed an optimal set of the DLSTM-FNN hyperparameters suitable for the
daily production prediction of tight gas wells.
The availability and practicability of the three data-driven models in
Fig. 4. The sample composition of the daily production time series.
Table 1
The RMSE of the production prediction models for 1692 gas wells in Sulige.
Model
Training Set
Testing Set
RF
0.0884904
0.1878669
SVR
0.1409583
0.1636799
DLSTM-FNN
0.1663612
0.1378165
Q. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
168
dynamic daily production prediction were veriﬁed using 1692 time series
of wells in the Sulige gas ﬁeld. Among them, the DLSTM-FNN deep
learning model had a good ability in time series modeling. For the short
training sample set and the rapid prediction of thousands of wells, the
quantitated prediction performances revealed that the DLSTM-FNN
model was more accurate and reliable and was more in line with the
actual dynamic trend of the daily production of gas wells. Furthermore,
the proposed DLSTM-FNN model can be applied to the time series pre-
diction of other tight gas ﬁelds based on transfer learning.
Funding
This work was supported by the National Key R&D Program of China
(2020YFA0713404).
Declaration of competing interest
The authors declare that they do not have any commercial or asso-
ciative interest that represents a conﬂict of interest in connection with
the work submitted.
References
Arps, J.J., 1945. Analysis of decline curves. Transact. Am. Inst. Min. Metal. Petrol. Eng.
160 (1), 228–247.
Bengio, Y., Simard, P., Frasconi, P., 1994. Learning long-term dependencies with gradient
descent is difﬁcult. IEEE Trans. Neural Network. 5 (2), 157–166.
Breiman, L., 2001. Random forests. Mach. Learn. 45, 5–32.
Cao, Q., Banerjee, R., Gupta, S., et al., 2016. Data driven production forecasting using
machine learning. In: SPE Argentina Exploration and Production of Unconventional
Resources Symposium. SPE, Buenos Aires.
Fig. 5. Data-driven methods to predict daily productions of gas wells S1 and S2. (a): The prediction result of RF on S1. (b): The prediction result of RF on S2. (c): The
prediction result of SVR on S1. (d): The prediction result of SVR on S2. (e): The prediction result of DLSTM-FNN on S1. (f): The prediction result of DLSTM-FNN on S2.
Table 2
The RMSE of the production prediction models for wells S1 and S2.
Model
S1 training set
S1 testing set
S2 training set
S2 testing set
RF
0.1090542
0.1285114
0.1269109
0.1587212
SVR
0.2011788
0.0957835
0.1945521
0.1073256
DLSTM-FNN
0.2042352
0.0722189
0.19246629
0.1025749
Q. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
169
Chen, Y., Fu, L., 2019. Establishment, comparison and application of power function
decline model. Petrol. Geol. Recov. Efﬁc. 26 (6), 87–91.
Chen, Y., Tang, W., 2016. Establishment and application of generalized decline model.
Acta Pet. Sin. 37 (11), 1410–1413.
Chen, Y., Zhou, C., 2015. Establishment, comparison and application of the linear decline
type. Acta Pet. Sin. 36 (8), 983–987.
Dong, W., Yang, S., 2009. Factors affecting production decline performance and
production forecasting. Nat. Gas Geosci. 20 (3), 411–415.
Gu, J., Zhou, M., Li, Z., et al., 2019. Oil well production forecast with long- short term
memory network model based on data mining. Special Oil Gas Reservoirs 26 (2),
81–85, 135.
Hochreiter, S., Schmidhuber, J., 1997. Long short-term memory. Neural Comput. 9 (8),
1735–1780.
Hopﬁeld, J.J., 1982. Neural networks and physical systems with emergent collective
computational abilities. In: Proceedings of the National Academy of Sciences of the
United States of America, 79. National Academy of Sciences of the United States of
America, pp. 2554–2558, 8.
Hou, C., 2019. New well oil production forecast method based on long-term and short-
term memory neural network. Petrol. Geol. Recov. Efﬁc. 26 (3), 105–110.
Jiang, F., Pang, X., Jiang, Z., et al., 2007. Physical simulation experiment of gas charging
in tight sandstone. Geol. Rev. 53 (6), 844–849.
Jiang, Y., Chen, X., Bao, H., 2021. A new model for rapid prediction of horizontal well
production decline in shale gas staged fracturing: case study of Fuling shale gas ﬁeld.
Nat. Gas Geosci. 32 (6), 845–850.
Liu, W., Liu, W.D., Gu, J., Shen, X., 2019. Predictive model for water absorption in
sublayers using a machine learning method. J. Petrol. Sci. Eng. 182, 106367.
Liu, W., Liu, W., Gu, J., et al., 2020. Oil production prediction based on a machine
learning method. Oil Drill. Product. Technol. 42 (1), 70–75.
Sagheer, A., Kotb, M., 2019. Time series forecasting of petroleum production using deep
LSTM recurrent networks. Neurocomputing 323, 203–213.
Yang, H., Fu, J., Liu, X., Meng, P., 2012. Accumulation conditions and exploration and
development of tight gas in the upper paleozoic of the Ordos basin. Petrol. Explor.
Dev. 39 (3), 295–303.
Yang, T., Zhang, G., Liang, K., 2012. The exploration of global tight sandstone gas and
forecast of the development tendency in China. Strat. Study CAE 14 (16), 64–68.
Zou, C., Yang, Z., Tao, S., et al., 2012. Nano-hydrocarbon and the accumulation in
coexisting source and reservoir. Petrol. Explor. Dev. 39 (1), 13–26.
Q. Zhang et al.
Artiﬁcial Intelligence in Geosciences 2 (2021) 165–170
170
