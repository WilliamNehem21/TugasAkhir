Array 13 (2022) 100110
Available online 2 December 2021
2590-0056/© 2021 The Authors. Published by Elsevier Inc. This is an open access article under the CC BY license (http://creativecommons.org/licenses/by/4.0/).
Physics-informed neural network method for solving one-dimensional 
advection equation using PyTorch 
Shashank Reddy Vadyala a,*, Sai Nethra Betgeri a, Naga Parameshwari Betgeri, Ph.D b 
a Department of Computational Analysis and Modeling, Louisiana Tech University, Ruston, LA, United States 
b Department of Business and Administration, Dr. B.V.Raju Institute of Technology, Narsapur, Medak, Telangana, India   
A R T I C L E  I N F O   
Keywords: 
Data-driven scientific computing 
Partial differential equations 
Physics 
Machine learning 
Finite element method 
A B S T R A C T   
Numerical solutions to the equation for advection are determined using different finite-difference approxima-
tions and physics-informed neural networks (PINNs) under conditions that allow an analytical solution. Their 
accuracy is examined by comparing them to the analytical solution. We used a machine learning framework like 
PyTorch to implement PINNs. PINNs approach allows training neural networks while respecting the Partially 
differential equations (PDEs) as a strong constraint in the optimization as apposed to making them part of the loss 
function. In standard small-scale circulation simulations, it is shown that the conventional approach incorporates 
a pseudo diffusive effect that is almost as large as the effect of the turbulent diffusion model; hence the numerical 
solution is rendered inconsistent with the PDEs. This oscillation causes inaccuracy and computational uncer-
tainty. Of all the schemes tested, only the PINNs approximation accurately predicted the outcome. We assume 
that the PINNs approach can transform the physics simulation area by allowing real-time physics simulation and 
geometry optimization without costly and time-consuming simulations on large supercomputers.   
1. Introductions 
PDEs, whose states exist in infinite-dimensional spaces, often model 
physical science and engineering processes [1]. Due to the lack of 
computational methods in most situations, finite-dimensional approxi-
mations are used instead, based on conventional numerical techniques 
developed and improved over the years. Traditional numerical solvers, 
on the other hand, frequently necessitate considerable computational 
effort, particularly for complex systems with multiscale/multiphysics 
features, and may not be feasible in real-time or many-query applica-
tions, such as optimization, inverse problem, and uncertainty quantifi-
cation (UQ), which necessitate many repeated simulations [2]. Solving 
PDE structures for the best possible combination between precision and 
reliability [3]. Some data-driven approaches to solve PDEs with deep 
neural networks (DNNs) exist nowadays. The benefits of using DNNs to 
approximate PDE solutions [4]. DNNs can Identify nonlinear in-
teractions, which are confirmed mathematically by universal approxi-
mation theorems. 
Forward assessments of trained DNNs are quick, which is ideal for 
real-time or multiple-query applications. Furthermore, DNN models are 
analytically differentiable; derivative information can be easily derived 
for optimization and control problems using automatic differentiation 
[5]. Recently, researchers have attempted to use DNNs in numerous 
sectors, including education [6], media [7], power [8], and healthcare 
[9]. However, there are several problems in various fields for which 
there is no analytical approach. Since certain constants are believed to 
be fixed, even problems with analytical solutions have them. The 
analytical solution to a simplistic dilemma, on the other hand, teaches us 
a lot about the system’s actions. On the other hand, numerical methods 
can rapidly investigate problems if no analytical solution approach is 
available. However, extreme caution must be exercised to ensure that a 
converged solution is achieved. This means we need to figure out if the 
step sizes are minimal enough to discover the solutions to the equations 
we are trying to understand. For example, the numerical method FEM is 
an excellent tool for solving complicated geometrical shapes with a 
boundary and load condition that is difficult to describe with available 
analytical expressions. There are generally three approaches by which 
scientific problems/equations are solved: Analytical, Numerical, and 
Experimental. However, we cannot perform the experimental method 
every time because of cost and time constraints. The traditional ap-
proaches for solving problems are analytical methods. However, we 
cannot solve equations analytically due to limitations imposed by 
* Corresponding author. 
E-mail address: vadyala.shashankreddy@gmail.com (S.R. Vadyala).  
Contents lists available at ScienceDirect 
Array 
journal homepage: www.sciencedirect.com/journal/array 
https://doi.org/10.1016/j.array.2021.100110 
Received 17 May 2021; Received in revised form 21 September 2021; Accepted 19 November 2021   
Array 13 (2022) 100110
2
complex geometry, boundary conditions, and other factors. 
Consequently, we have been pushing towards numerical methods for 
several years because they can produce almost reliable results in addi-
tion to analytical methods, and they can do so in a much shorter and 
easier period. For example, the famous Navier-Stoke equation has never 
been solved analytically, but it can be solved quickly using Numerical 
Schemes. On the other hand, the disadvantages of traditional Numerical 
Schemes are they are challenging in representing irregular boundaries, 
not optimized for unstructured meshes, and momentum, energy, and 
mass are not conserved [10]. Lastly, they are not well suited for turbu-
lent flow slow for significant problems, and they tend to be biased to-
wards edges and one-dimensional physics. 
To solve the drawbacks of traditional methods, we investigate the 
use of PINNs as a solution approximation for PDEs in this paper. PINNs 
trained to solve supervised learning tasks while respecting any given 
physics law described by general nonlinear PDEs Fig. 1 and Eq. (1) 
summarizes the PINNs [11]. We will talk about PyTorch as a Python 
implementation for PINNs. We assume these PINNs would affect 
real-world applications where reduced-order physics models are 
possible and widely used. The numerical efficiency of reduced-order 
models usually comes at the expense of physical consistency. We illus-
trate how PINNs can be used in reduced-order models to reduce 
epistemic (model-form) ambiguity for the advection equation. PINNs 
architectures can help close the distance between forecasts and 
observable results. Our method begins with an abstract formulation and 
then moves to numerical integration methods before implementing a 
neural network. 
PINNs ​ = ​ Data ​ + ​ Neural ​ Networks ​ + ​ Physical ​ Laws
(1) 
The rest of the paper is structured in the following way. First, section 
2, the background of the advection equation, section 3.1 specifies the 
implementation choices in language, libraries, and public repositories 
(needed for replicating results). Then, section 3.2 presents the formu-
lation and implementation for integrating the first-order ordinary dif-
ferential equation with the simple Euler’s forward method. Next, section 
4 presents results, and section 5 closes conclusions and future work. 
Finally, Appendix summarizes concepts about neural networks used in 
this paper. 
2. Background 
Whether it is waves on a lake, sound waves, or an earthquake, 
everybody has seen traveling waves. Writing down a reasonable-looking 
finite difference approximation to a wave equation is not complicated. 
Most of the time is spent attempting to decide if the process succeeds. 
Since numerical wave propagation has its own set of complexities, we 
will start with one of the most basic mathematical equations for 
generating moving waves [12]. The advection equation is written as 
∂u/∂t + a ∂u/∂x = 0, for
{
−∞ < x < ∞
0 < t
(2) 
The convenient feature of model Eq. (2) is that it has an analytical 
solution in Eq. (3): 
u = u0f(x − at)
(3) 
Which represents a wave propagating with a constant velocity with 
an unchanged shape. When a>0, the wave propagating in the positive x- 
direction, whereas for a<0, the wave propagates in the negative x-di-
rection. Eq. (2) is significant in various applications should not be 
overlooked due to its mathematical simplicity. The momentum equation 
for gas motion, for example, is an inhomogeneous version of Eq. (2) that 
is dependent on u. Therefore, a nonlinear version of the advection 
equation occurs in the physical model of interstellar gas and its motion 
due to the solar wind [13]. Another application is in analyzing traffic 
flow along a highway [14]. The density of cars along the road is rep-
resented by u (x, t). Their speed is represented by a. Eq. (2) may serve as 
a model-equation for a compressible fluid, e.g., if u denote pressure, it 
represents a pressure wave propagating. The advection equation is often 
used to model pressure or flow transmission in a compliant pipe, such as 
a blood vessel. The applications of fractional advection-dispersion equa 
tions for anomalous solute transport in surface and subsurface water. 
The generalization of equation ∂u/∂t + ∂F/∂x = 0, where for the linear 
advection equation F(u) = au. 
PDEs are equations that contain unknown multivariate functions and 
their partial derivatives in Eq. (4). 
∅t + ∇ ⋅ (u ⋅ ∅ ) = ∇⋅(Γ∇ ∅ )
(4) 
Where ∅ is the dependent variable, ∅t = ∂∅/∂t is the derivative of ∅t 
concerning time, t, ∇ =
(
∂∅/∂t, ∂∅/∂t, ∂∅
∂t
)
is the nabla operator, u =
(u, v, w) is the velocity, and Γ is the diffusivity. The independent vari-
ables are space, x = (x, y, z) and time, t. Eq. (4) is known as the 
convection-diffusion equation. Diffusion and convection are two 
mechanisms that explain how particles, electricity, and other physical 
quantities are transported within a physical structure. Concerning 
diffusion ∇⋅(Γ∇ ∅), The concentration of a chemical can be assumed by 
∅. As concentration is minimal in one region relative to the surrounding 
areas (e.g., a local minimum of concentration), the substance diffuses 
Fig. 1. Schematic of a PINNs for solving the diffusion equation (∂u/∂t = ⋏∂2u/
∂t2).  
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
3
from the surrounding areas, increasing concentration. The net diffusion 
is proportional to the Laplacian (or second derivative) of concentration. 
If the diffusivity Γ is a constant, Γ∇2∅. On the other hand, concerning 
convection, ∇⋅(u ⋅ ∅), Assume the chemical is being carried down a 
canal, and we are calculating the concentration of the water per second. 
Then, someone spills a barrel of the chemical into the river upstream. 
The concentration will suddenly rise and fall as the field with the 
elevated chemicals passes. The problems presented in this study as ex-
amples will consist of finding the function ∅(x, t) that, for a given ge-
ometry, original, and boundary conditions, satisfies the PDE. In a typical 
approach such as finite volume, we divide the statistical domain into 
small regions and consider the average volume size ∅  at a given moment 
Eq. (5). 
∅n
i = 1
Vi
∫
Vi
∅(x, tn)
(5) 
Where Vi denotes the volume of the ith discretized element in the 
computational domain. The global solution can then be obtained by 
combining the different solutions for all volumes Eq. (6). 
∅n =
∑
Vi
∅n
i
(6) 
This function is piecewise constant and cannot be deduced, demon-
strating one of the first drawbacks of conventional numerical methods. 
Many applications need derivable PDE solutions. Specific exciting ef-
fects, such as heat fluxes and mass transfer, are computed with de-
rivatives; a derivable solution would be more reliable than a derivatives 
approximation using piecewise functions. We discretize the various 
operators in equation one and use a time integration scheme to change 
the time. In its most basic form, the first-order Euler algorithm Eq. (7). 
∅n+1
i
= ∅n
i − Δt
Vi
∑
fi
Ff Af
(7)  
where Δt is the time step, Ff is the flux ∅ across the face f of the volume 
Vi and Af is the area of the face. Additional computational schemes to 
calculate these unknown values are used to calculate the fluxes at the 
faces. Some popular choices are central difference or upwind methods. 
Two more aspects are required to complete the problem: the original and 
boundary conditions. To begin, keep in mind that an initial state is 
simply a time-dimensional boundary condition. This is important 
because initial and boundary conditions are typically considered inde-
pendently, but our approach will treat them similarly. The initial con-
dition sets the value for ∅(x, t = 0) and functions as the first values to 
start the computation in the previously described time-marching algo-
rithm. Boundary conditions set the value for ∅(x ∈ D, t > 0) where D is 
the total number of points on the domain’s boundaries. We must specify 
a specific set of rules to change these points, not to see values outside the 
scope. There are many boundary conditions, but the ones used in our 
examples are as follows:  
• Periodic: The domain folds itself to bind boundaries when periodic 
conditions are assumed.  
• Dirichlet: For this type of boundary condition, we will fix ∅  it at 
boundaries.  
• Newmann: For this type of boundary condition, we will fix ∇∅  at 
boundaries. 
For the treatment of walls, inflows, and outflows, specific boundaries 
may be needed. For example, in the temporal dimension, an initial state 
is a Dirichlet boundary condition. The approach used for other con-
ventional approaches such as finite difference or finite elements differs 
slightly. However, the basic principle remains the same: discretize the 
computational domain into small regions where the solution is assumed 
and bring them back together to retrieve the global solution. Therefore, 
there are no derivable piecewise alternatives. Furthermore, since we use 
time-marching algorithms, new computations are needed once the free 
parameters, current, or boundary conditions are changed. 
This study presents a strategy to combine physics-based modeling 
with data-driven ML to solve 1D advection equation. The equation of 1D 
advection is directly encoded into the PINNs to guide its training and 
prediction. The presented case studies show that encoding physics can 
significantly improve the generalization ability of the PINNs for struc-
tural response prediction and reduce its reliance on the quality and 
quantity of the training data. The PINNs model can effectively reduce 
the computational costs compared with the numerical method of the 
same order, making it a good candidate for surrogate modeling of 
structural, dynamical systems. 
3. Methods 
3.1. Implementation 
This part will teach you how to solve Eq. (2) using a neural network. 
Python is being used to solve the PDEs. Python, a programming lan-
guage, has grown in popularity in recent years. For the following rea-
sons: Since there is no need for an explicit declaration of variables or a 
separate compilation period, it is quick and simple to code and use for 
small “prototyping” tasks. It is available for free on most computing 
systems and has a vast repository of packages covering a wide range of 
applications. Python also has features that make developing and doc-
umenting massive, well-structured program structures easier. Python is 
not appropriate for running extended computational computations since 
it is an interpreted language. However, calls to precompiled library 
routines are often used in the code for such computations. Many of these 
routines are available directly from Python using the PyTorch [15], 
NumPy [16], and SciPy [17] packages. Most operating systems (OS), 
including Linux, OSX, and MSWindows, have these kits available for 
free. 
The aim is to achieve a trained multilayer perceptron (MLP) that can 
give the output φ(x, t), where x and t are set as inputs that satisfy Eq. (1). 
The basic principle is that a forward transfer on the network using the 
independent variables as PINNs inputs gives one the evaluated depen-
dent variables. Since PINNs are derivable, we can compute the depen-
dent variables (outputs). The dependent variables (inputs) to calculate 
the different derivatives that appear in the original PDEs. We create a 
loss function that fits the PDEs with this derivative throughout the 
training phase. We will assume that our PINNs solve the PDEs if the loss 
function approaches a near-zero value. The teaching is done in an un-
supervised setting. Thus, there are continuous and derivable solutions 
that can be applied throughout the whole domain. PINNs also have the 
advantage of allowing one to use the PDEs-free parameters in the solu-
tion. Therefore, a solution trained for various values of these parameters 
will generalize to different conditions rather than a single scenario, 
eliminating the need for new calculations any time a parameter is 
modified. This property is of particular interest in optimization studies. 
In more depth, we describe a collection of points within our domain the 
same way as standard approaches would. These points are divided into 
two categories: preparation and validation after training. We also 
differentiate between internal and external points. This will be dealt 
with in compliance with the boundary conditions that have been 
established. Then, we define the MLP architecture: several layers and a 
number of hidden units in each layer. The number of inputs would be the 
same as the number of independent variables in the PDEs with any free 
parameters we want to add. The number of outputs would be the same as 
the number of unknowns that need to be solved. Table 1 summarizes the 
PINNs algorithm. Once we have training data and the PINNs defined, 
follows the steps: 
• We compute the network’s outputs at all points, ∅(x, t) and the de-
rivatives concerning the inputs: ∅t,∅x, ∅xx. 
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
4
• We use a loss function that fits our PDE for internal points. This is the 
role we’d like to improve: ∅t + ∇⋅(u ⋅ ∅) − ∇⋅(Γ∇ ∅) = 0  
• We can create an MSE loss function to fulfill the given condition for 
boundary points since we fix values.  
• Update the parameters of the PINNs for each loss function. 
3.2. Solving one-dimensional advection equation using PINNs 
We aim to create a qualified multilayer perceptron (MLP) that can 
output (x, t) when given x and t as inputs and satisfy Eq. (2). Consider 
the one-dimensional advection Eq. (8), which is a simplification of Eq. 
(1) for a 1D, 
∅t + u∅x = 0
(8) 
Where ∅(x, t) is the unknown function, x and t are the independent 
variables, u is a constant parameter and ∅t and ∅x are the derivatives of 
∅ concerning t and x, respectively. This PDE has an analytical solution, 
which is ∅(x,t) = ∅(x,x − ut). We may assume that the initial condition 
was physical ∅(x, t = 0) moves in x at speed u. In the case that ∅ (x,t =
0) = exp
(
− 1
2
(
x−at
0.4
)2)/
L the solution is ∅(x,t) = exp
(
− 1
2
(
x−at
0.4
)2)/
L 
as illustrated in Fig. 2. To solve the equation, we define a set of points for 
training. 
Discretized ​ equation: ​ un+1
i
= un
i − a Δt
Δx
(
un
i − un
i−1
)
(9) 
Defining a Δx and Δt allows us to build a uniform grid of points in the 
entire domain used in the discretized Eq. (9). We describe internal and 
boundary points, each of which would have a separate loss function 
associated with it. When t = 0, the initial condition will use a Mean 
Square Error (MSE) loss function, which will equate the known initial 
condition with the PINNs output. For the spatial boundary condition, we 
use a periodic condition that compares the solutions using an MSE loss 
function at x = 0 and x = L for any t and forces them to be equal. As seen 
in Fig. 3, we characterize our solution as an MLP with two inputs 
(number of independent variables), hidden layers, and one output. Fig. 3 
shows a general scheme’s flowchart for converting PDEs into the PINNs 
structure. 
The training steps are as follows:  
• Compute 
the 
network 
outputs 
for 
the 
internal 
points: 
∅(0 < x < L; t > 0).
• Compute the gradients of the outputs concerning the inputs: ∅x, ∅t.  
• Create the internal point loss function: L1 = MSE (∅x + ∅t)  
• Calculate 
boundary 
state 
outputs: 
∅(0 < x < L; t =
0), 
∅(x = 0; t) and ∅(x = L,t).  
• Establish a failure function to account for boundary conditions: L2 =
MSE (∅(0 < ​ x ​ < ​ L; ​ t ​ = 0) − exp
(
− 1
2
(
x−at
0.4
)2)/
L , L3 = MSE 
(∅(x = 0; t)− ( ∅(x = L; t))
• Update the PINNs parameters for the other losses. 
MLP with two inputs, x and t, one output ∅(x,t), sigmoid activation 
function, and five hidden layers with 32 neurons shown in Fig. 4. 
Moreover, we here assume that t is a function of ∅(x, t), the first de-
rivative of ∅(x, t) concerning the set of inputs and physical parameters, 
θ. The training points are randomly uniformly drawn from their corre-
sponding domains. {xn,i,un,i} corresponding to data at tn and Euler 
scheme Eq. (7) allows us to infer the latent solution u(t, x) in a sequential 
method. Starting from initial data {xn,i,un,i} at time tn and data at the 
domain boundaries x = 0 and x = 2 shown in Fig. 5, we can use the loss 
function to train the networks and predict the solution at the time tn+1. 
An Euler time-stepping scheme would then use this prediction as initial 
data for the next step and proceed to train again and predict. We use an 
optimization algorithm to perform well in applying the PINNs method 
for parameter estimation. The Adam optimizer with a learning rate of 
0.001 is used until the optimization problem’s solution converges with 
the prescribed tolerance. We set the maximum number of Adam’s epoch 
Table 1 
PINNs algorithm.  
PINNS Algorithm 
Input: dynamics parameters ∅, start time t0, stop time t1, final state z (t1).
• Specify the two training sets Tf and 
Tb for ​ the ​ equation ​ and ​ boundary/initial ​ conditions.   
• Define dynamics on the augmented state.  
• Specify a loss function for the PDE equation and boundary condition.  
• Train the PINNs to find the best parameters ∅ by ​ minimizing ​ the ​ loss ​ function.   
Fig. 2. Example of the solution to the 1D advection equation with the initial condition.exp
(
− 1
2
(
x−at
0.4
)2)
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
5
to be 10,000 and use the mini-batch training; the PINNs weights are 
randomly initialized using the Xavier scheme [18]. 
These steps are usually small in the classical numerical analysis due 
to stability constraints for explicit schemes or computational complexity 
constraints for implicit formulations. If the number of Euler scheme 
stages increases, these restrictions become more serious. Thousands to 
millions of such measures are needed for most realistic interest problems 
before the solution is resolved up to a desired final period. We can use an 
implicit Euler scheme with an infinitely large number of stages at no 
added expense compared to conventional approaches. This helps one 
overcome the entire Spatio-temporal solution in a single step while 
ensuring consistency and high predictive precision. Table 2 shows the 
parameters and constants used implementation of PINNs and different 
hyperbolic schemes. 
4. Results 
A one-dimensional advection equation is solved to understand the 
main process of solving a PDE with PINNs. Compared PINNs with a 
traditional FTBS [19], Lax Wendroff [20], and Lax Friedrich [21] all 
time-integrated with an Euler scheme. Our trained PINNs, unlike the 
FVM solution, are continuous and derivable over the entire domain. 
Since physical effects do not limit PINNs, our experiments demonstrate 
that a much larger mesh will provide better results in this specific case. 
In comparison to conventional methods, we find that PINNs are capable 
of minimizing error. Our findings are summarized in Table 3 below. 
The computing time needed to integrate the problem in a small grid 
of points is a few seconds. Still, if the domain’s discretization contains 
about 10,000 points for each variable, the computing time of the 10,000 
Euler method is not like that of the PINNs training. However, the 
computing time requirement of the PINNs method is not comparable 
with traditional numerical methods. Any numerical method gives solu-
tion points over a selected grid. The computing time, in this case, in-
creases as the number of grid points increases. So, computing the 
solution for an infinite number of points will need infinite time. On the 
other hand, the PINNs find a continuous solution over the real domain in 
a finite time. 
The PINNs were tested with points that did not participate in the 
training. Fig. 5 shows the MSE of the trained network as a function of the 
number of testing points. It can be observed that the averaged error is 
stable for an increasing number of testing points, which indicates that 
the solution found is a good approximation of the real solution not only 
for the training points but also over the whole domain of the problem. 
Fig. 3. Flowchart for designing the PINNs for a general PDE.  
Fig. 4. Two distributions of training points of PINNs for one-dimensional advection equation (Left). PINNs Multilayer perceptrons (Right).  
Table 2 
Constants and parameters.  
Constants and parameters 
Values 
a (Wave Speed) 
1.0 
tmin, tmax (start and stop time of simulation)  
0.0,2.0 
Nx (N umber of spatial points)  
80 
C (Courant number, need C ≤ 1 for stability) 
0.9  
Table 3 
MSE reported. MSE between the predicted ∅(x, t)
and the exact solution u(x, y) for the different 
methods and PINNs.  
Methods 
MSE 
FTBS 
3.87e-03 
Lax Wendroff 
4.32e-03 
Lax Fredrich 
4.78e-03 
PINNs 
1.65e-03  
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
6
Fig. 6 shows the errors in the resulting PINNs solution ∅ (x, t) for the 
reference one-dimensional advection equation u(x, t) field with the 
maximum point errors provided in Table 4 for the prediction in 0 < t < 
2. The FTBS, Lax Wendroff, and Lax Friedrich approximation produced a 
less accurate solution and had more intense oscillation than PINNs. As 
can be seen, training the PINNs requires optimization concerning many 
Fig. 5. MSE of the trained PINNs vs the number of testing points into the integration domain.  
Fig. 6. A range of hyperbolic schemes are implemented 1D advection equation with the initial condition exp
(
− 1
2
(
x−at
0.4
)2)
. The colored lines with the marker "*" 
denote different hyperbolic schemes. The solid black line denotes the exact solution. 
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
7
loss functions (as many as PDEs and different boundary conditions), 
challenging complex problems. We see that PINNs more sharply resolve 
discontinuities at the cost of adding oscillations, which leads to a net 
reduction in error. We can use this result to hypothesize that PINNs will 
improve the solution when the discontinuity is more significant. We 
verify our hypothesis by plotting the error ratio against discontinuity 
width in 0 < t < 2 and demonstrating that more significant disconti-
nuities lead to lower error ratios. In the solutions obtained with Lax 
Friedrich, FTBS, and Lax Wendroff, an anomalous oscillation is present 
when the grid spacing is too large to follow the quantity’s variations 
advected closely. Lax Friedrich, FTBS, and Lax Wendroff’s results 
converge faster and produce more significant predictive errors than 
PINNs at almost all training data points. The accuracy of the predictions 
Lax Friedrich, FTBS, and Lax Wendroff is very good at the initial con-
ditions t = 0, and the boundary conditions are controlled or well known, 
but they struggle at t = 2. MSE indicates that the PINNs loss can effi-
ciently approximate the solution (0 < t < 2) without explicit 
regularization. 
Even in the presence of partially unknown physics, the PINNs 
approach can reliably anticipate the reaction of a one-dimensional 
advection equation and has high generalization capacity. The perfor-
mance comparison of the PINNs and the Deep Residual Recurrent Neural 
Networks (DR-RNN) [2] using the parameters as shown in Table .5. 
Fig. 7 and Fig. 8 show that incorporating physics into the design of ML 
models may considerably enhance prediction performance, particularly 
for extrapolation, while also lowering training costs. The PINNs 
approach may efficiently decrease computing costs by employing a more 
significant time step than an explicit integration method of the same 
order while retaining high prediction accuracy; the PINNs method’s 
resilience is proven by training the PINNs with noisy/noisy/limited 
data, as shown in Table .6. Furthermore, it is found that the quantity and 
quality of the training data do not significantly influence the prediction 
performance. The prediction accuracy of the PINNs method does not 
vary significantly with the prediction time length. 
5. Conclusions 
By comparing numerical solutions of advection equation with known 
analytic solutions and PINNs have been demonstrated. The FTBS, Lax 
Wendroff, and Lax Friedrich introduced a pseudo diffusion effect that 
led to a large error in t = 2 and would make accurate turbulent diffusion 
modeling impossible. In those cases where the value of the property 
being advected varied rapidly in the space of a few grid intervals, var-
iations which seem to be typical in many models, the Lax Friedrich, 
FTBS, and Lax Wendroff introduced an anomalous oscillation into the 
distribution of the quality being advected at t = 2. The oscillations grew 
steadily with time and could easily introduce instability into a numerical 
model. Of the methods investigated, Only the PINNs approximation 
moved the advected field correctly at 0 < t < 2. Traditional numerical 
methods to find a consistent solution with more straightforward and 
faster methods led to inaccurate results. Only the PINNs method pro-
duces an accurate and consistent approximation with the PDEs. Tradi-
tional numerical methods also require a large amount of computer core 
storage. PINNs approach can transform the physics simulation area by 
allowing real-time physics simulation and geometry optimization 
without 
costly 
and 
time-consuming 
simulations 
on 
large 
supercomputers. 
Credit author statement 
Shashank Reddy Vadyala: Conceptualization, Methodology, Data 
curation, Writing – original draft preparation. Dr B. Naga Para-
meshwari: Writing- Reviewing, Editing and Supervision. Sai Nethra 
Betgeri: Visualization,Validation. 
Declaration of competing interest 
The authors declare that they have no known competing financial 
interests or personal relationships that could have appeared to influence 
the work reported in this paper.  
Appendix. PINNs method 
PINNs are a simple multilayer feed-forward neural network with depth D that contains an input layer, D − 1 hidden layers, and output layer. 
Without loss of generalization, we assume that there are Nd neurons in the dth hidden layer. Then, the dth hidden layer receives the post-activation 
output xd−1 ε RNd−1 of the previous layers as its input, and the specific affine transformation is of the form shown in Eq. (A.1): 
ρd
(
Xd−1) Δ
≡Wdxd−1 + bd
(A.1)  
where the network weight Wd ε RNd−1 and the bias term bd ∈ RNd be learned are initialized using some special strategies, such as Xavier initialization or 
initialization. The nonlinear activation function σ(⋅) is applied component-wise to the affine output ρd of the present layer. This nonlinear activation is 
not used in the output layer for specific regression issues, or we may assume that the identity activation is used in the output layer. As a result, the 
neural network can be denoted as q(x, Θ) = (ρd 
◦ σ ◦ ρd−1 
◦ ⋅ ⋅ ⋅ ◦ σ ◦ ρ1) (x). Where the operator " ◦ " is the composition operator, Θ = {Wd, bd}D ε P 
represents the learnable parameters to be optimized later in the 
network, and P is the parameter space, q, and x0 = x is the output and 
input of the network, respectively. The residual networks fu (x, t): = ut +
Nu (u, ux, uxx, uxxx, ⋅ ⋅ ⋅), and fv (x, t): = vt + Nv (v, vx, vxx, vxxx, ⋅ ⋅ ⋅). Then 
the solution q (x, t) will be trained to satisfy these two physical 
constraint conditions. Which play a vital role in regularization and have 
been embedded into the mean-squared objective function, that is, the 
loss function lossΘ = lossu + lossv + lossfu + lossfv 
Appendix. Multilayer perceptrons (MLP) shown in Fig. A.1 
Table 4 
1D advection equation: Relative final prediction error measure in the L2 norm 
for different hyperbolic schemes. Here, the time-step size to t = 0.5.  
Model 
t = 0 min 
t = 0.5 
t = 1 
t = 1.5 
t = 2 
FTBS 
2.33e-04 
5.29e-04 
5.46e-03 
6.01e-03 
7.12e-03 
Lax Wendroff 
2.03e-04 
4.90e-04 
5.26e-03 
6.71e-03 
8.92e-03 
Lax Friedrich 
2.53e-04 
5.80e-04 
6.56e-03 
7.21e-03 
9.32e-03 
PINNs 
1.03e-02 
1.10e-3 
1.26e-03 
1.65e-03 
2.12e-03  
Table 5 
Parameters used in PINNs and DR-RNN.  
Parameter 
Value 
Activation 
Adam 
Hidden Layers 
5 
Neurons 
300 
Batch Size 
64 
Learning Rate 
5e-4 
Epochs 
4000  
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
8
Fig. 7. Performance of PINNs.  
Fig. 8. Performance of DR-RNN.  
Table 6 
Comparison of computation time and MSE between the DR-RNN and the PINNs 
model.   
PINNs 
DR-RNN 
Computation time (s) 
25.4 
134.7 
MSE 
2.12e-04 
4.03e-04  
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
9
Fig. A.1. illustrates the popular MLP.  
Each layer can have one or more perceptrons (nodes in the graph). A perceptron applies a linear combination to the input variables followed by an 
activation function. 
v ​ = ​ f(z) ​ and ​ z ​ = ​ WTu + b  
where v is the perceptron output, u is the inputs; w and b are the perceptron hyperparameters, and f (.) is the activation function. Throughout this 
paper, we used the hyperbolic tangent (tanh), sigmoid, and the exponential linear unit (elu) activation functions (although others could also be used, 
such as the rectified exponential linear unit): 
tanh(z) ​ = ​ ez − e−z
ez + e−z , ​ sigmoid(z) ​ = ​
1
1 + e−z and ​ elu(z) ​ =
{
z when z > 0
ez − 1    
1. Alblawi, A.S. and A.A. Alhamed. Big data and learning analytics in higher education: Demystifying variety, acquisition, storage, NLP and analytics. 
In 2017 IEEE Conference on Big Data and Analytics (ICBDA). 2017. IEEE.  
2. Kani, J.N. and A.H. Elsheikh, DR-RNN: A deep residual recurrent neural network for model reduction. arXiv preprint arXiv:1709.00939, 2017. 
References 
[1] Sirignano J, Spiliopoulos K. DGM: a deep learning algorithm for solving partial 
differential equations. J. Comput. Phys. 2018;375:1339–64. https://doi.org/ 
10.1016/j.jcp.2018.08.029. Dec. 
[2] Rüde U, Willcox K, McInnes LC, Sterck HD. Research and education in 
computational science and engineering. SIAM Rev. 2018;60(3):707–54. https:// 
doi.org/10.1137/16M1096840. Jan. 
[3] Gunzburger M. Finite element solution of boundary value problems—theory and 
computation (O. Axelsson and V. A. Barker). SIAM Rev. 1988;30(1):143–4. https:// 
doi.org/10.1137/1030025. Mar. 
[4] Mahmoudabadbozchelou M, Caggioni M, Shahsavari S, Hartt WH, Em 
Karniadakis G, Jamali S. Data-driven physics-informed constitutive metamodeling 
of complex fluids: a multifidelity neural network (MFNN) framework,. J. Rheol. 
2021;65(2):179–98. https://doi.org/10.1122/8.0000138. Feb. 
[5] Xu J, Tao Q, Li Z, Xi X, Suykens JAK, Wang S. Efficient hinging hyperplanes neural 
network and its application in nonlinear system identification. ArXiv190506518 
Cs, Nov. 2019, Mar. 14,, http://arxiv.org/abs/1905.06518; 2021 [Online]. 
Available. 
[6] Albalowi A, Alhamed A. Big data and learning analytics in higher education: 
Demystifying variety, acquisition, storage. NLP and analytics 2017. 
[7] Denecke K. Extracting medical concepts from medical social media with clinical 
NLP tools : a qualitative study. 2014. /paper/Extracting-Medical-Concepts-from- 
Medical-Social-NLP-Denecke/8ed7609f41aa772a93c4a46f5ae7a9266559376e, . 
[Accessed 14 March 2021]. 
[8] Strubell E, Ganesh A, McCallum A. Energy and policy considerations for deep 
learning in NLP. ArXiv190602243 Cs, Jun. 2019, Mar. 14, http://arxiv.org/abs/1 
906.02243; 2021 [Online]. Available. 
[9] Vadyala SR, Betgeri SN, Sherer EA, Amritphale A. Prediction of the number of 
COVID-19 confirmed cases based on K-Means-LSTM. ArXiv 200614752 phys Q-Bio 
2021. Jun. 2020, Accessed: Mar. 14,, [Online]. Available: http://arxiv.org/abs/2 
006.14752. 
[10] Loredo C, Banks D, Roque˜ní N. Evaluation of analytical models for heat transfer in 
mine tunnels. Geothermics 2017;69:153–64. https://doi.org/10.1016/j. 
geothermics.2017.06.001. Sep. 
[11] Raissi M, Perdikaris P, Karniadakis GE. Physics-informed neural networks: a deep 
learning framework for solving forward and inverse problems involving nonlinear 
partial differential equations. J. Comput. Phys. 2019;378:686–707. https://doi. 
org/10.1016/j.jcp.2018.10.045. Feb. 
[12] Dutykh D, Dias F. Water waves generated by a moving bottom. In: Kundu A, editor. 
Tsunami and nonlinear waves. Berlin, Heidelberg: Springer; 2007. p. 65–95. 
[13] Bruno R, Carbone V. The solar wind as a turbulence laboratory. Living Rev. Sol. 
Phys. 2013;10(1):2. https://doi.org/10.12942/lrsp-2013-2. May. 
[14] L. Romero and F. G. Benitez, "Outline OF diffusion-advection IN traffic flow 
modelling," p. 17. 
[15] Stevens E, Antiga L, Viehmann T, Chintala S. Deep learning with PyTorch 2020. 
[16] Oliphant T. Guide to NumPy 2006. 
[17] FJ Blanco-Silva"Learning SciPy for numerical and scientific computing," Packt. 
https://www.packtpub.com/product/learning-scipy-for-numerical-and-scientific- 
computing/9781782161622 (accessed Mar. 14, 2021). 
S.R. Vadyala et al.                                                                                                                                                                                                                             
Array 13 (2022) 100110
10
[18] Kumar SK. On weight initialization in deep neural networks," ArXiv 170408863 Cs. 
May 2017, Mar. 14,, http://arxiv.org/abs/1704.08863; 2021 [Online]. Available. 
[19] Long W, Kirby J, Shao Z. A numerical scheme for morphological bed level 
calculations. Coast. Eng Times 2008;55:167–80. https://doi.org/10.1016/j. 
coastaleng.2007.09.009. Feb. 
[20] Roe PL. Generalized formulation of TVD lax-wendroff schemes. Nat Aeronau Space 
Admin Langlry Res Center 1984. 
[21] Chen G. Convergence of the LAX–FRIEDRICHS scheme for isentropic gas dynamics 
(III). Acta math. Sci. 1986;6(1):75–120. https://doi.org/10.1016/S0252-9602(18) 
30535-6. Jan. 
S.R. Vadyala et al.                                                                                                                                                                                                                             
