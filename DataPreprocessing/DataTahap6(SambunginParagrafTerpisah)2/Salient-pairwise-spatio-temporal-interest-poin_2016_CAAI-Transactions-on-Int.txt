and local features have been proposed [1,2]. Holistic features have been employed in Refs. [3e5], where actions were treated as spaceetime pattern templates by Blank et al. [3] and the task of human action classification was reduced to 3D object recognition. Prest et al. [4] focused on the actions of humaneobject interactions, and explicitly represented an ac- tion as the tracking trajectories of both the object and the person. Recently, traditional convolutional neural networks (CNNs) which are limited to handle 2D inputs were extended, and a novel 3D CNN model was developed to act directly on raw videos [5].

[20] utilized spatial-temporal correlograms to capture the co- occurrences of pairwise words in local spatio-temporal re- gions. To represent spatio-temporal relationships, Matikainen et al. [21] formulated this problem in a Nave Bayes manner, and augmented quantized local features with relative spatial- temporal relationships  between  pairs  of features. From

another point of view, both local and global relationships of pairwise words were explored in Refs. [22,23]. A spatio- temporal relationship matching method was proposed by Ryoo et al. [22] which explored temporal relationships (e.g. before and during) as well as spatial relationships (e.g. near and far) among pairwise words. In Ref. [23], co-occurrence relationships of pairwise words were encoded in correlo- grams, which relied on the computation of normalized google- like distances.

directional information from them to reflect the natural structure of human actions that our motion parts are direc- tional. Time Salient Pairwise feature (TSP) is proposed to describe the relationships between pairwise STIPs on the same frame, and only the pairs with different labels are considered. Obviously, TSP ignores the relationships between pairwise STIPs with same labels in G ss, and brings ambiguous to distinguish actions with similar G st. Thus, this paper proposes another descriptor called Space Salient Pairwise feature (SSP) to describe G ss.

ieves 92.50% accuracy, which is 4.9% higher than recent work [38]. Since [39] mainly focus on the speed of the algorithm, the local feature detector and clustering steps are implemented using more fast method like V-FAST interest point detector and semantic texton forests. To ensure a fair comparison with

[14]. Then BoVW, TSP and SSP features are calculated in real-time using offline trained models. Finally, non-linear SVM with homogeneous kernel generates the type of ac- tion efficiently. Since the proposed algorithm are not limited to human actions, it can be utilized to improve the performance of content based video retrieval.

the PHILIPS SPC900NC/97 camera and place it on the head of the robot with a height of 1.8 m. Additionally, a curve mirror is utilized to change the camera into a 360 degree panoramic camera. The mobile robot works in a

Since richer information of spatial-temporal distribution is involved, TSP outperforms baseline BoVW. Additionally, a Space Salient Pairwise feature (SSP) is designed to describe geometric distribution of STIPs which is ignored by TSP. The SSP achieves compatible results with BoVW model on different datasets which proves the effect of spatio-temporal distribution for action classification without lying on content of  STIPs.  Finally,  a  multi-cue  representation  called mentary nature of these three methods. Experimental results on four challenging datasets show that salient motions are robustness against distracted motions and efficient to distin- guish similar actions. Future work focus on how to model geometric distribution of STIPs more accurately. As only STIPs are involved in current work, high level models and features like explicit models of human-object [4] and dense tracklets in Ref. [43] can be considered. Additionally, more real-time applications will be designed to apply our algorithm.

This work is supported by the National Natural Science Foundation of China (NSFC, nos. 61340046), the National High Technology Research and Development Programme of China (863 Programme, no. 2006AA04Z247), the Scientific and Technical Innovation Commission of Shenzhen Munici- pality (nos. JCYJ20130331144631730), and the Specialized Research Fund for the Doctoral Programme of Higher Edu- cation (SRFDP, no. 20130001110011).

